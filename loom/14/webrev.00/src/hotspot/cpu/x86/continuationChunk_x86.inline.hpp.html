<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/x86/continuationChunk_x86.inline.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef CPU_X86_CONTINUATION_CHUNK_X86_INLINE_HPP
 26 #define CPU_X86_CONTINUATION_CHUNK_X86_INLINE_HPP
 27 
 28 #include &quot;memory/iterator.inline.hpp&quot;
 29 #include &quot;runtime/frame.inline.hpp&quot;
 30 
 31 #if INCLUDE_ZGC
 32 #include &quot;gc/z/zAddress.inline.hpp&quot;
 33 #define FIX_DERIVED_POINTERS true
 34 #endif
 35 
 36 static inline void* reg_to_loc(VMReg reg, intptr_t* sp) {
 37   assert (!reg-&gt;is_reg() || reg == rbp-&gt;as_VMReg(), &quot;&quot;);
 38   return reg-&gt;is_reg() ? (void*)(sp - frame::sender_sp_offset) // see frame::update_map_with_saved_link(&amp;map, link_addr);
 39                        : (void*)((address)sp + (reg-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 40 }
 41 
 42 #ifdef ASSERT
 43 static bool is_in_oops(const ImmutableOopMap* oopmap, intptr_t* sp, void* p) {
 44   for (OopMapStream oms(oopmap); !oms.is_done(); oms.next()) {
 45     if (oms.current().type() != OopMapValue::oop_value)
 46       continue;
 47     if (reg_to_loc(oms.current().reg(), sp) == p)
 48       return true;
 49   }
 50   return false;
 51 }
 52 
 53 static bool is_in_frame(CodeBlob* cb, intptr_t* sp, void* p0) {
 54   intptr_t* p = (intptr_t*)p0;
 55   int argsize = cb-&gt;is_compiled() ? (cb-&gt;as_compiled_method()-&gt;method()-&gt;num_stack_arg_slots() * VMRegImpl::stack_slot_size) &gt;&gt; LogBytesPerWord : 0;
 56   int frame_size = cb-&gt;frame_size() + argsize;
 57   return p == sp - frame::sender_sp_offset || ((p - sp) &gt;= 0 &amp;&amp; (p - sp) &lt; frame_size);
 58 }
 59 #endif
 60 
 61 // We replace derived pointers with offsets; the converse is done in fix_stack_chunk
 62 template &lt;bool concurrent_gc&gt;
 63 static void iterate_derived_pointers(oop chunk, const ImmutableOopMap* oopmap, intptr_t* sp, CodeBlob* cb) {
 64   for (OopMapStream oms(oopmap); !oms.is_done(); oms.next()) {
 65     OopMapValue omv = oms.current();
 66     if (omv.type() != OopMapValue::derived_oop_value)
 67       continue;
 68     
 69     intptr_t* derived_loc = (intptr_t*)reg_to_loc(omv.reg(), sp);
 70     intptr_t* base_loc    = (intptr_t*)reg_to_loc(omv.content_reg(), sp); // see OopMapDo&lt;OopMapFnT, DerivedOopFnT, ValueFilterT&gt;::walk_derived_pointers1
 71     assert (is_in_frame(cb, sp, base_loc), &quot;&quot;);
 72     assert (is_in_frame(cb, sp, derived_loc), &quot;&quot;);
 73     assert (derived_loc != base_loc, &quot;Base and derived in same location&quot;);
 74     assert (is_in_oops(oopmap, sp, base_loc), &quot;not found: &quot; INTPTR_FORMAT, p2i(base_loc));
 75     assert (!is_in_oops(oopmap, sp, derived_loc), &quot;found: &quot; INTPTR_FORMAT, p2i(derived_loc));
 76     
 77     // The ordering in the following is crucial
 78     OrderAccess::loadload();
 79     oop base = Atomic::load((oop*)base_loc);
 80     assert (oopDesc::is_oop_or_null(base), &quot;not an oop&quot;);
 81     assert (Universe::heap()-&gt;is_in_or_null(base), &quot;not an oop&quot;);
 82     if (base != (oop)NULL) {
 83       assert (!CompressedOops::is_base(base), &quot;&quot;);
 84 
 85 #if INCLUDE_ZGC
 86       if (concurrent_gc) { //  &amp;&amp; UseZGC // TODO: this is a ZGC-specific optimization
 87         if (ZAddress::is_good(cast_from_oop&lt;uintptr_t&gt;(base))) 
 88           continue;
 89       }
 90 #endif
 91 
 92       OrderAccess::loadload();
 93       intptr_t derived_int_val = Atomic::load(derived_loc); // *derived_loc;
 94       if (derived_int_val &lt; 0) {
 95         continue;
 96       }
 97 
 98       if (concurrent_gc) {
 99         jdk_internal_misc_StackChunk::set_gc_mode(chunk, true);
100         OrderAccess::storestore(); // if you see any following writes, you&#39;ll see this
101       }
102 
103       // at this point, we&#39;ve seen a non-offset value *after* we&#39;ve read the base, but we write the offset *before* fixing the base,
104       // so we are guaranteed that the value in derived_loc is consistent with base (i.e. points into the object).
105       intptr_t offset = derived_int_val - cast_from_oop&lt;intptr_t&gt;(base);
106       assert (offset &gt;= 0 &amp;&amp; offset &lt;= (base-&gt;size() &lt;&lt; LogHeapWordSize), &quot;offset: %ld size: %d&quot;, offset, (base-&gt;size() &lt;&lt; LogHeapWordSize));
107       Atomic::store((intptr_t*)derived_loc, -offset); // there could be a benign race here; we write a negative offset to let the sign bit signify it&#39;s an offset rather than an address
108     } else {
109       assert (*derived_loc == 0, &quot;&quot;);
110     }
111   }
112   OrderAccess::storestore(); // to preserve that we set the offset *before* fixing the base oop
113 }
114 
115 static void fix_derived_pointers(oop chunk, const ImmutableOopMap* oopmap, intptr_t* sp, CodeBlob* cb) {
116   for (OopMapStream oms(oopmap); !oms.is_done(); oms.next()) {
117     OopMapValue omv = oms.current();
118     if (omv.type() != OopMapValue::derived_oop_value)
119       continue;
120     
121     intptr_t* derived_loc = (intptr_t*)reg_to_loc(omv.reg(), sp);
122     intptr_t* base_loc    = (intptr_t*)reg_to_loc(omv.content_reg(), sp); // see OopMapDo&lt;OopMapFnT, DerivedOopFnT, ValueFilterT&gt;::walk_derived_pointers1
123     
124     // The ordering in the following is crucial
125     OrderAccess::loadload();
126     oop base = Atomic::load((oop*)base_loc);
127     if (base != (oop)NULL) {
128       assert (!CompressedOops::is_base(base), &quot;&quot;);
129       ZGC_ONLY(assert (ZAddress::is_good(cast_from_oop&lt;uintptr_t&gt;(base)), &quot;&quot;);)
130 
131       OrderAccess::loadload();
132       intptr_t offset = Atomic::load(derived_loc); // *derived_loc;
133       if (offset &gt;= 0)
134         continue;
135 
136       // at this point, we&#39;ve seen a non-offset value *after* we&#39;ve read the base, but we write the offset *before* fixing the base,
137       // so we are guaranteed that the value in derived_loc is consistent with base (i.e. points into the object).
138       if (offset &lt; 0) {
139         offset = -offset;
140         assert (offset &gt;= 0 &amp;&amp; offset &lt;= (base-&gt;size() &lt;&lt; LogHeapWordSize), &quot;&quot;);
141         Atomic::store((intptr_t*)derived_loc, cast_from_oop&lt;intptr_t&gt;(base) + offset);
142       }
143   #ifdef ASSERT 
144       else { // DEBUG ONLY
145         offset = offset - cast_from_oop&lt;intptr_t&gt;(base);
146         assert (offset &gt;= 0 &amp;&amp; offset &lt;= (base-&gt;size() &lt;&lt; LogHeapWordSize), &quot;offset: %ld size: %d&quot;, offset, (base-&gt;size() &lt;&lt; LogHeapWordSize));
147       }
148   #endif
149     }
150   }
151   OrderAccess::storestore(); // to preserve that we set the offset *before* fixing the base oop
152   jdk_internal_misc_StackChunk::set_gc_mode(chunk, false);
153 }
154 
155 template &lt;class OopClosureType&gt;
156 static bool iterate_oops(OopClosureType* closure, const ImmutableOopMap* oopmap, intptr_t* sp, CodeBlob* cb) {
157   DEBUG_ONLY(int oops = 0;)
158   bool mutated = false;
159   for (OopMapStream oms(oopmap); !oms.is_done(); oms.next()) { // see void OopMapDo&lt;OopFnT, DerivedOopFnT, ValueFilterT&gt;::iterate_oops_do
160     OopMapValue omv = oms.current();
161     if (omv.type() != OopMapValue::oop_value &amp;&amp; omv.type() != OopMapValue::narrowoop_value)
162       continue;
163 
164     assert (UseCompressedOops || omv.type() == OopMapValue::oop_value, &quot;&quot;);
165     DEBUG_ONLY(oops++;)
166 
167     void* p = reg_to_loc(omv.reg(), sp);
168     assert (p != NULL, &quot;&quot;);
169     assert (is_in_frame(cb, sp, p), &quot;&quot;);
170 
171     // if ((intptr_t*)p &gt;= end) continue; // we could be walking the bottom frame&#39;s stack-passed args, belonging to the caller
172 
173     // if (!SkipNullValue::should_skip(*p))
174     log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack narrow: %d reg: %s p: &quot; INTPTR_FORMAT &quot; sp offset: %ld&quot;, omv.type() == OopMapValue::narrowoop_value, omv.reg()-&gt;name(), p2i(p), (intptr_t*)p - sp);
175     // DEBUG_ONLY(intptr_t old = *(intptr_t*)p;)
176     intptr_t before = *(intptr_t*)p;
177     omv.type() == OopMapValue::narrowoop_value ? Devirtualizer::do_oop(closure, (narrowOop*)p) : Devirtualizer::do_oop(closure, (oop*)p);
178     mutated |= before != *(intptr_t*)p;
179   }
180   assert (oops == oopmap-&gt;num_oops(), &quot;oops: %d oopmap-&gt;num_oops(): %d&quot;, oops, oopmap-&gt;num_oops());
181   return mutated;
182 }
183 
184 template &lt;class OopClosureType, bool concurrent_gc&gt;
185 void Continuation::stack_chunk_iterate_stack(oop chunk, OopClosureType* closure) {
186     // see sender_for_compiled_frame
187 
188   assert (Continuation::debug_is_stack_chunk(chunk), &quot;&quot;);
189   log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack requires_barriers: %d&quot;, !Universe::heap()-&gt;requires_barriers(chunk));
190 
191   int num_frames = 0;
192   int num_oops = 0;
193 
194   bool do_destructive_processing; // should really be `= closure.is_destructive()`, if we had such a thing
195   if (concurrent_gc) {
196     do_destructive_processing = true;
197   } else {
198     if (SafepointSynchronize::is_at_safepoint() &amp;&amp; !jdk_internal_misc_StackChunk::gc_mode(chunk)) {
199       do_destructive_processing = true;
200       jdk_internal_misc_StackChunk::set_gc_mode(chunk, true);
201     } else {
202       do_destructive_processing = false;
203     }
204     assert (!SafepointSynchronize::is_at_safepoint() || jdk_internal_misc_StackChunk::gc_mode(chunk), &quot;gc_mode: %d is_at_safepoint: %d&quot;, jdk_internal_misc_StackChunk::gc_mode(chunk), SafepointSynchronize::is_at_safepoint());
205   }
206 
207   CodeBlob* cb = NULL;
208   intptr_t* start = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk);
209   intptr_t* end = start + jdk_internal_misc_StackChunk::size(chunk) - jdk_internal_misc_StackChunk::argsize(chunk);
210   for (intptr_t* sp = start + jdk_internal_misc_StackChunk::sp(chunk); sp &lt; end; sp += cb-&gt;frame_size()) {
211     address pc = *(address*)(sp - 1);
212     log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack sp: %ld pc: &quot; INTPTR_FORMAT, sp - start, p2i(pc));
213     assert (pc != NULL, &quot;&quot;);
214 
215     int slot;
216     cb = ContinuationCodeBlobLookup::find_blob_and_oopmap(pc, slot);
217     assert (cb != NULL, &quot;&quot;);
218     assert (cb-&gt;is_compiled(), &quot;&quot;);
219     assert (cb-&gt;frame_size() &gt; 0, &quot;&quot;);
220     assert (!cb-&gt;as_compiled_method()-&gt;is_deopt_pc(pc), &quot;&quot;);
221 
222     assert (slot &gt;= 0, &quot;&quot;);
223     const ImmutableOopMap* oopmap = cb-&gt;oop_map_for_slot(slot, pc);
224     // if (LIKELY(slot &gt;= 0)) {
225     //   oopmap = cb-&gt;oop_map_for_slot(slot, pc);
226     // } else {
227     //   CompiledMethod* cm = cb-&gt;as_compiled_method();
228     //   assert (cm-&gt;is_deopt_pc(pc), &quot;&quot;);
229     //   pc = *(address*)((address)sp + cm-&gt;orig_pc_offset());
230     //   oopmap = cb-&gt;oop_map_for_return_address(pc);
231     // }
232     assert (oopmap != NULL, &quot;&quot;);
233     log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack slot: %d codeblob:&quot;, slot);
234     if (log_develop_is_enabled(Trace, jvmcont)) cb-&gt;print_value_on(tty);
235 
236     if (Devirtualizer::do_metadata(closure) &amp;&amp; cb-&gt;is_nmethod()) {
237       // The nmethod entry barrier takes care of having the right synchronization
238       // when keeping the nmethod alive during concurrent execution.
239       cb-&gt;as_nmethod_or_null()-&gt;run_nmethod_entry_barrier();
240     }
241 
242     num_frames++;
243     num_oops += oopmap-&gt;num_oops();
244     if (closure == NULL) {
245       continue;
246     }
247     
248     if (do_destructive_processing) { // evacuation always takes place at a safepoint; for concurrent iterations, we skip derived pointers, which is ok b/c coarse card marking is used for chunks
249       iterate_derived_pointers&lt;concurrent_gc&gt;(chunk, oopmap, sp, cb);
250     }
251 
252     bool mutated_oops = iterate_oops(closure, oopmap, sp, cb);
253 
254   #ifdef FIX_DERIVED_POINTERS
255     if (concurrent_gc &amp;&amp; mutated_oops &amp;&amp; jdk_internal_misc_StackChunk::gc_mode(chunk)) { // TODO: this is a ZGC-specific optimization that depends on the one in iterate_derived_pointers
256       fix_derived_pointers(chunk, oopmap, sp, cb);
257     }
258   #endif
259   }
260 
261   assert (num_frames &gt;= 0, &quot;&quot;);
262   assert (num_oops &gt;= 0, &quot;&quot;);
263   if (do_destructive_processing || closure == NULL) {
264     jdk_internal_misc_StackChunk::set_numFrames(chunk, num_frames);
265     jdk_internal_misc_StackChunk::set_numOops(chunk, num_oops);
266   }
267 
268   if (closure != NULL) {
269     Continuation::emit_chunk_iterate_event(chunk, num_frames, num_oops);
270   }
271 
272   // assert(Continuation::debug_verify_stack_chunk(chunk), &quot;&quot;);
273   log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack ------- end -------&quot;);
274   // tty-&gt;print_cr(&quot;&lt;&lt;&lt; stack_chunk_iterate_stack %p %p&quot;, (oopDesc*)chunk, Thread::current());
275 }
276 
277 template &lt;class OopClosureType&gt;
278 void Continuation::stack_chunk_iterate_stack_bounded(oop chunk, OopClosureType* closure, MemRegion mr) {
279   assert (false, &quot;&quot;); // TODO REMOVE
280   log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack_bounded&quot;);
281   intptr_t* const l = (intptr_t*)mr.start();
282   intptr_t* const h = (intptr_t*)mr.end();
283 
284   CodeBlob* cb = NULL;
285   intptr_t* start = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk);
286   intptr_t* end = start + jdk_internal_misc_StackChunk::size(chunk);
287   if (end &gt; h) end = h;
288   for (intptr_t* sp = start + jdk_internal_misc_StackChunk::sp(chunk); sp &lt; end; sp += cb-&gt;frame_size()) {
289     intptr_t* next_sp = sp + cb-&gt;frame_size();
290     if (sp + cb-&gt;frame_size() &gt;= l) {
291       sp += cb-&gt;frame_size();
292       continue;
293     }
294 
295     address pc = *(address*)(sp - 1);
296     int slot;
297     cb = ContinuationCodeBlobLookup::find_blob_and_oopmap(pc, slot);
298     const ImmutableOopMap* oopmap = cb-&gt;oop_map_for_slot(slot, pc);
299     assert (slot &gt;= 0, &quot;&quot;);
300 
301     log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack_bounded sp: %ld&quot;, sp - start);
302     if (log_develop_is_enabled(Trace, jvmcont)) cb-&gt;print_on(tty);
303 
304     for (OopMapStream oms(oopmap); !oms.is_done(); oms.next()) { // see void OopMapDo&lt;OopFnT, DerivedOopFnT, ValueFilterT&gt;::iterate_oops_do
305       OopMapValue omv = oms.current();
306       if (omv.type() != OopMapValue::oop_value &amp;&amp; omv.type() != OopMapValue::narrowoop_value)
307         continue;
308       
309       oop* p = (oop*)reg_to_loc(omv.reg(), sp);
310       assert (p != NULL, &quot;&quot;);
311 
312       if ((intptr_t*)p &lt; l || (intptr_t*)p &gt;= end) continue;
313 
314       log_develop_trace(jvmcont)(&quot;stack_chunk_iterate_stack_bounded p: &quot; INTPTR_FORMAT, p2i(p));
315       // if (!SkipNullValue::should_skip(*p))
316         omv.type() == OopMapValue::narrowoop_value ? Devirtualizer::do_oop(closure, (narrowOop*)p) : Devirtualizer::do_oop(closure, p);
317     }
318   }
319 }
320 
321 #endif // CPU_X86_CONTINUATION_CHUNK_X86_INLINE_HPP
    </pre>
  </body>
</html>