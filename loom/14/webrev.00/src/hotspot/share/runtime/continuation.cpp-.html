<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/runtime/continuation.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2018, 2020 Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/javaClasses.inline.hpp&quot;
  27 #include &quot;classfile/vmSymbols.hpp&quot;
  28 #include &quot;code/codeCache.inline.hpp&quot;
  29 #include &quot;code/compiledMethod.inline.hpp&quot;
  30 #include &quot;code/scopeDesc.hpp&quot;
  31 #include &quot;code/vmreg.inline.hpp&quot;
  32 #include &quot;compiler/oopMap.hpp&quot;
  33 #include &quot;compiler/oopMap.inline.hpp&quot;
  34 #include &quot;jfr/jfrEvents.hpp&quot;
  35 #include &quot;gc/shared/memAllocator.hpp&quot;
  36 #include &quot;gc/shared/oopStorage.hpp&quot;
  37 #include &quot;gc/shared/threadLocalAllocBuffer.inline.hpp&quot;
  38 #include &quot;interpreter/interpreter.hpp&quot;
  39 #include &quot;interpreter/linkResolver.hpp&quot;
  40 #include &quot;interpreter/oopMapCache.hpp&quot;
  41 #include &quot;logging/log.hpp&quot;
  42 #include &quot;logging/logStream.hpp&quot;
  43 #include &quot;metaprogramming/conditional.hpp&quot;
  44 #include &quot;oops/access.inline.hpp&quot;
  45 #include &quot;oops/instanceStackChunkKlass.inline.hpp&quot;
  46 #include &quot;oops/objArrayOop.inline.hpp&quot;
  47 #include &quot;oops/weakHandle.hpp&quot;
  48 #include &quot;oops/weakHandle.inline.hpp&quot;
  49 #include &quot;prims/jvmtiThreadState.hpp&quot;
  50 #include &quot;runtime/continuation.inline.hpp&quot;
  51 #include &quot;runtime/deoptimization.hpp&quot;
  52 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  53 #include &quot;runtime/frame.hpp&quot;
  54 #include &quot;runtime/frame.inline.hpp&quot;
  55 #include &quot;runtime/javaCalls.hpp&quot;
  56 #include &quot;runtime/jniHandles.inline.hpp&quot;
  57 #include &quot;runtime/prefetch.inline.hpp&quot;
  58 #include &quot;runtime/sharedRuntime.hpp&quot;
  59 #include &quot;runtime/vframe_hp.hpp&quot;
  60 #include &quot;utilities/copy.hpp&quot;
  61 #include &quot;utilities/debug.hpp&quot;
  62 #include &quot;utilities/exceptions.hpp&quot;
  63 #include &quot;utilities/macros.hpp&quot;
  64 
  65 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  66 
  67 // #define NO_KEEPALIVE true // Temporary: Disable keepalive to test ZGC
  68 #define CONT_JFR false
  69 
  70 #define SENDER_SP_RET_ADDRESS_OFFSET (frame::sender_sp_offset - frame::return_addr_offset)
  71 
  72 static void fix_stack_chunk(oop chunk, intptr_t* start, intptr_t* end);
  73 
  74 static const bool TEST_THAW_ONE_CHUNK_FRAME = false; // force thawing frames one-at-a-time from chunks for testing purposes
  75 // #define PERFTEST 1
  76 
  77 #ifdef PERFTEST
  78 #define PERFTEST_ONLY(code) code
  79 #else
  80 #define PERFTEST_ONLY(code)
  81 #endif // ASSERT
  82 
  83 #ifdef __has_include
  84 #  if __has_include(&lt;valgrind/callgrind.h&gt;)
  85 #    include &lt;valgrind/callgrind.h&gt;
  86 #  endif
  87 #endif
  88 #ifndef VG
  89 #define VG(X)
  90 #endif
  91 #ifdef __has_include
  92 #  if __has_include(&lt;valgrind/memcheck.h&gt;)
  93 #    include &lt;valgrind/memcheck.h&gt;
  94 #    undef VG
  95 #    define VG(x) x
  96 #  endif
  97 #endif
  98 
  99 #ifdef CALLGRIND_START_INSTRUMENTATION
 100   static int callgrind_counter = 1;
 101   // static void callgrind() {
 102   //   if (callgrind_counter != 0) {
 103   //     if (callgrind_counter &gt; 20000) {
 104   //       tty-&gt;print_cr(&quot;Starting callgrind instrumentation&quot;);
 105   //       CALLGRIND_START_INSTRUMENTATION;
 106   //       callgrind_counter = 0;
 107   //     } else
 108   //       callgrind_counter++;
 109   //   }
 110   // }
 111 #else
 112   // static void callgrind() {}
 113 #endif
 114 
 115 // #undef log_develop_info
 116 // #undef log_develop_debug
 117 // #undef log_develop_trace
 118 // #undef log_develop_is_enabled
 119 // #define log_develop_info(...)  (!log_is_enabled(Info, __VA_ARGS__))   ? (void)0 : LogImpl&lt;LOG_TAGS(__VA_ARGS__)&gt;::write&lt;LogLevel::Info&gt;
 120 // #define log_develop_debug(...) (!log_is_enabled(Debug, __VA_ARGS__)) ? (void)0 : LogImpl&lt;LOG_TAGS(__VA_ARGS__)&gt;::write&lt;LogLevel::Debug&gt;
 121 // #define log_develop_trace(...) (!log_is_enabled(Trace, __VA_ARGS__))  ? (void)0 : LogImpl&lt;LOG_TAGS(__VA_ARGS__)&gt;::write&lt;LogLevel::Trace&gt;
 122 // #define log_develop_is_enabled(level, ...)  log_is_enabled(level, __VA_ARGS__)
 123 
 124 // #undef ASSERT
 125 // #undef assert
 126 // #define assert(p, ...)
 127 
 128 #ifdef ASSERT
 129 template&lt;int x&gt; NOINLINE static bool verify_continuation(oop cont) { return Continuation::debug_verify_continuation(cont); }
 130 template&lt;int x&gt; NOINLINE static bool verify_stack_chunk(oop chunk) { return Continuation::debug_verify_stack_chunk(chunk); }
 131 #endif
 132 
 133 #ifdef ASSERT
 134 extern &quot;C&quot; void pns2();
 135 extern &quot;C&quot; void pfl();
 136 #endif
 137 
 138 int Continuations::_flags = 0;
 139 int ContinuationEntry::return_pc_offset = 0;
 140 nmethod* ContinuationEntry::continuation_enter = NULL;
 141 address ContinuationEntry::return_pc = NULL;
 142 
 143 void ContinuationEntry::set_enter_nmethod(nmethod* nm) {
 144   assert (return_pc_offset != 0, &quot;&quot;);
 145   continuation_enter = nm;
 146   return_pc = nm-&gt;code_begin() + return_pc_offset;
 147 }
 148 
 149 PERFTEST_ONLY(static int PERFTEST_LEVEL = ContPerfTest;)
 150 // Freeze:
 151 // 5 - no call into C
 152 // 10 - immediate return from C
 153 // 15 - return after count_frames
 154 // 20 - all work, but no copying
 155 // 25 - copy to stack
 156 // 30 - freeze oops
 157 // &lt;100 - don&#39;t allocate
 158 // 100 - everything
 159 //
 160 // Thaw:
 161 // 105 - no call into C (prepare_thaw)
 162 // 110 - immediate return from C (prepare_thaw)
 163 // 112 - no call to thaw0
 164 // 115 - return after traversing frames
 165 // 120
 166 // 125 - copy from stack
 167 // 130 - thaw oops
 168 
 169 
 170 // TODO
 171 //
 172 // Nested continuations: must restore fastpath, held_monitor_count, cont_frame-&gt;sp (entrySP of parent)
 173 // Add:
 174 //  - method/nmethod metadata
 175 //  - compress interpreted frames
 176 //  - special native methods: Method.invoke, doPrivileged (+ method handles)
 177 //  - compiled-&gt;intrepreted for serialization (look at scopeDesc)
 178 //  - caching h-stacks in thread stacks
 179 //
 180 // Things to compress in interpreted frames: return address, monitors, last_sp
 181 //
 182 // See: deoptimization.cpp, vframeArray.cpp, abstractInterpreter_x86.cpp
 183 //
 184 // For non-temporal load/store in clang (__builtin_nontemporal_load/store) see: https://clang.llvm.org/docs/LanguageExtensions.html
 185 
 186 //
 187 // The data structure invariants are defined by Continuation::debug_verify_continuation and Continuation::debug_verify_stack_chunk
 188 //
 189 
 190 #define YIELD_SIG  &quot;java.lang.Continuation.yield(Ljava/lang/ContinuationScope;)V&quot;
 191 #define YIELD0_SIG &quot;java.lang.Continuation.yield0(Ljava/lang/ContinuationScope;Ljava/lang/Continuation;)Z&quot;
 192 #define ENTER_SIG  &quot;java.lang.Continuation.enter(Ljava/lang/Continuation;Z)V&quot;
 193 #define ENTER_SPECIAL_SIG &quot;java.lang.Continuation.enterSpecial(Ljava/lang/Continuation;Z)V&quot;
 194 #define RUN_SIG    &quot;java.lang.Continuation.run()V&quot;
 195 
 196 static bool is_stub(CodeBlob* cb);
 197 static void set_anchor(JavaThread* thread, intptr_t* sp);
 198 static void set_anchor_to_entry(JavaThread* thread, ContinuationEntry* cont);
 199 
 200 // static void set_anchor(JavaThread* thread, const frame&amp; f); -- unused
 201 
 202 // debugging functions
 203 frame sp_to_frame(intptr_t* sp);
 204 void do_verify_after_thaw(JavaThread* thread, intptr_t* sp, intptr_t **rbp_pos);
 205 static void print_oop(void *p, oop obj, outputStream* st = tty);
 206 static void print_vframe(frame f, const RegisterMap* map = NULL, outputStream* st = tty);
 207 static void print_chunk(oop chunk, oop cont = (oop)NULL, bool verbose = false) PRODUCT_RETURN;
 208 
 209 #ifdef ASSERT
 210   static void print_frames(JavaThread* thread, outputStream* st = tty);
 211   // template&lt;int x&gt; static void walk_frames(JavaThread* thread);
 212   static bool assert_frame_laid_out(frame f);
 213   static bool assert_entry_frame_laid_out(ContinuationEntry* cont);
 214   //static void print_blob(outputStream* st, address addr);
 215   static jlong java_tid(JavaThread* thread);
 216   // static bool is_deopt_pc(const frame&amp; f, address pc);
 217   // static bool is_deopt_pc(address pc);
 218   // void static stop();
 219   // void static stop(const frame&amp; f);
 220   // static void print_JavaThread_offsets();
 221   // static void trace_codeblob_maps(const frame *fr, const RegisterMap *reg_map);
 222 
 223   static RegisterMap dmap(NULL, false); // global dummy RegisterMap
 224 #endif
 225 
 226 #define ELEMS_PER_WORD (wordSize/sizeof(jint))
 227 // Primitive hstack is int[]
 228 typedef jint ElemType;
 229 const BasicType basicElementType   = T_INT;
 230 const int       elementSizeInBytes = T_INT_aelem_bytes;
 231 const int       LogBytesPerElement = LogBytesPerInt;
 232 const int       elemsPerWord       = wordSize/elementSizeInBytes;
 233 const int       LogElemsPerWord    = 1;
 234 
 235 STATIC_ASSERT(elemsPerWord &gt;= 1);
 236 STATIC_ASSERT(elementSizeInBytes == sizeof(ElemType));
 237 STATIC_ASSERT(elementSizeInBytes == (1 &lt;&lt; LogBytesPerElement));
 238 STATIC_ASSERT(elementSizeInBytes &lt;&lt;  LogElemsPerWord == wordSize);
 239 
 240 // #define CHOOSE1(interp, f, ...) ((interp) ? Interpreted::f(__VA_ARGS__) : NonInterpretedUnknown::f(__VA_ARGS__))
 241 #define CHOOSE2(interp, f, ...) ((interp) ? f&lt;Interpreted&gt;(__VA_ARGS__) : f&lt;NonInterpretedUnknown&gt;(__VA_ARGS__))
 242 
 243 static const unsigned char FLAG_LAST_FRAME_INTERPRETED = 1;
 244 static const unsigned char FLAG_SAFEPOINT_YIELD = 1 &lt;&lt; 1;
 245 
 246 static const int SP_WIGGLE = 3; // depends on the extra space between interpreted and compiled we add in Thaw::align; maybe needs to be Argument::n_int_register_parameters_j TODO PD
 247 
 248 void continuations_init() {
 249   Continuations::init();
 250 }
 251 
 252 #define USE_STUBS (ConfigT::_compressed_oops &amp;&amp; ConfigT::_post_barrier &amp;&amp; (!ConfigT::_slow_flags || LoomGenCode))
 253 #define FULL_STACK (ConfigT::_slow_flags &amp;&amp; !UseContinuationLazyCopy)
 254 #define USE_CHUNKS (UseContinuationChunks)
 255 
 256 class SmallRegisterMap;
 257 class ContMirror;
 258 class hframe;
 259 
 260 template &lt;typename ConfigT&gt;
 261 class CompiledMethodKeepalive;
 262 
 263 #ifdef CONT_DOUBLE_NOP
 264 class CachedCompiledMetadata;
 265 #endif
 266 
 267 DEBUG_ONLY(template&lt;typename FKind&gt; static intptr_t** slow_link_address(const frame&amp; f);)
 268 
 269 class Frame {
 270 public:
 271   template&lt;typename RegisterMapT&gt; static inline intptr_t** map_link_address(const RegisterMapT* map);
 272   static inline intptr_t** callee_link_address(const frame&amp; f);
 273   static inline Method* frame_method(const frame&amp; f);
 274   static inline address real_pc(const frame&amp; f);
 275   static inline void patch_pc(const frame&amp; f, address pc);
 276   static address* return_pc_address(const frame&amp; f);
 277   static address return_pc(const frame&amp; f);
 278 
 279   DEBUG_ONLY(static inline intptr_t* frame_top(const frame &amp;f);)
 280 };
 281 
 282 template&lt;typename Self&gt;
 283 class FrameCommon : public Frame {
 284 public:
 285   static inline Method* frame_method(const frame&amp; f);
 286 
 287   template &lt;typename FrameT&gt; static bool is_instance(const FrameT&amp; f);
 288 };
 289 
 290 class Interpreted : public FrameCommon&lt;Interpreted&gt; {
 291 public:
 292   DEBUG_ONLY(static const char* name;)
 293   static const bool interpreted = true;
 294   static const bool stub = false;
 295   static const int extra_oops = 0;
 296   static const char type = &#39;i&#39;;
 297 
 298 public:
 299 
 300   static inline intptr_t* frame_top(const frame&amp; f, InterpreterOopMap* mask);
 301   static inline intptr_t* frame_bottom(const frame&amp; f);
 302 
 303   static inline address* return_pc_address(const frame&amp; f);
 304   static inline address return_pc(const frame&amp; f);
 305   static void patch_return_pc(frame&amp; f, address pc);
 306   static void patch_sender_sp(frame&amp; f, intptr_t* sp);
 307 
 308   static void oop_map(const frame&amp; f, InterpreterOopMap* mask);
 309   static int num_oops(const frame&amp;f, InterpreterOopMap* mask);
 310   static int size(const frame&amp;f, InterpreterOopMap* mask);
 311   static inline int expression_stack_size(const frame &amp;f, InterpreterOopMap* mask);
 312   static bool is_owning_locks(const frame&amp; f);
 313 
 314   typedef InterpreterOopMap* ExtraT;
 315 };
 316 
 317 DEBUG_ONLY(const char* Interpreted::name = &quot;Interpreted&quot;;)
 318 
 319 template&lt;typename Self&gt;
 320 class NonInterpreted : public FrameCommon&lt;Self&gt;  {
 321 public:
 322   static inline intptr_t* frame_top(const frame&amp; f);
 323   static inline intptr_t* frame_bottom(const frame&amp; f);
 324 
 325   template &lt;typename FrameT&gt; static inline int size(const FrameT&amp; f);
 326   template &lt;typename FrameT&gt; static inline int stack_argsize(const FrameT&amp; f);
 327   static inline int num_oops(const frame&amp; f);
 328 
 329   template &lt;typename RegisterMapT&gt;
 330   static bool is_owning_locks(JavaThread* thread, RegisterMapT* map, const frame&amp; f);
 331 };
 332 
 333 class NonInterpretedUnknown : public NonInterpreted&lt;NonInterpretedUnknown&gt;  {
 334 public:
 335   DEBUG_ONLY(static const char* name;)
 336   static const bool interpreted = false;
 337 
 338   template &lt;typename FrameT&gt; static bool is_instance(const FrameT&amp; f);
 339 };
 340 
 341 DEBUG_ONLY(const char* NonInterpretedUnknown::name = &quot;NonInterpretedUnknown&quot;;)
 342 
 343 struct FpOopInfo;
 344 
 345 typedef int (*FreezeFnT)(address, address, address, address, int, FpOopInfo*, void * /* FreezeCompiledOops::Extra */);
 346 typedef int (*ThawFnT)(address /* dst */, address /* objArray */, address /* map */, address /* ThawCompiledOops::Extra */);
 347 
 348 typedef void (*MemcpyFnT)(void* src, void* dst, size_t count);
 349 
 350 static inline void copy_from_stack(void* from, void* to, size_t size);
 351 static inline void copy_to_stack(void* from, void* to, size_t size);
 352 
 353 class Compiled : public NonInterpreted&lt;Compiled&gt;  {
 354 public:
 355   DEBUG_ONLY(static const char* name;)
 356   static const bool interpreted = false;
 357   static const bool stub = false;
 358   static const int extra_oops = 1;
 359   static const char type = &#39;c&#39;;
 360 
 361   typedef FreezeFnT ExtraT;
 362 };
 363 
 364 DEBUG_ONLY(const char* Compiled::name = &quot;Compiled&quot;;)
 365 
 366 class StubF : public NonInterpreted&lt;StubF&gt; {
 367 public:
 368   DEBUG_ONLY(static const char* name;)
 369   static const bool interpreted = false;
 370   static const bool stub = true;
 371   static const int extra_oops = 0;
 372   static const char type = &#39;s&#39;;
 373 };
 374 
 375 DEBUG_ONLY(const char* StubF::name = &quot;Stub&quot;;)
 376 
 377 static bool is_stub(CodeBlob* cb) {
 378   return cb != NULL &amp;&amp; (cb-&gt;is_safepoint_stub() || cb-&gt;is_runtime_stub());
 379 }
 380 
 381 static bool requires_barriers(oop obj) {
 382   return Universe::heap()-&gt;requires_barriers(obj);
 383 }
 384 
 385 enum op_mode {
 386   mode_fast,   // only compiled frames
 387   mode_slow,   // possibly interpreted frames
 388 };
 389 
 390 // Represents a stack frame on the horizontal stack, analogous to the frame class, for vertical-stack frames.
 391 
 392 // We do not maintain an sp and an unexetended sp. Instead, sp represents frame&#39;s unextended_sp, and various patching of interpreted frames is especially handled.
 393 template&lt;typename SelfPD&gt;
 394 class HFrameBase {
 395 protected:
 396   int     _sp;
 397   int     _ref_sp;
 398   address _pc;
 399 
 400   bool _is_interpreted;
 401   mutable void* _cb_imd; // stores CodeBlob in compiled frames and interpreted frame metadata for interpretedd frames
 402   mutable const ImmutableOopMap* _oop_map; // oop map, for compiled/stubs frames only
 403 
 404   friend class ContMirror;
 405 private:
 406   const SelfPD&amp; self() const { return static_cast&lt;const SelfPD&amp;&gt;(*this); }
 407   SelfPD&amp; self() { return static_cast&lt;SelfPD&amp;&gt;(*this); }
 408 
 409   const ImmutableOopMap* get_oop_map() const { return self().get_oop_map(); };
 410 
 411   void set_codeblob(address pc) {
 412     if (_cb_imd == NULL &amp;&amp; !_is_interpreted) {// compute lazily
 413       _cb_imd = ContinuationCodeBlobLookup::find_blob(_pc);
 414       assert(_cb_imd != NULL, &quot;must be valid&quot;);
 415     }
 416   }
 417 
 418 protected:
 419   HFrameBase(bool dummy) {}
 420 
 421   HFrameBase() : _sp(-1), _ref_sp(-1), _pc(NULL), _is_interpreted(true), _cb_imd(NULL), _oop_map(NULL) {}
 422 
 423   HFrameBase(int sp, int ref_sp, address pc, void* cb_md, bool is_interpreted)
 424     : _sp(sp), _ref_sp(ref_sp), _pc(pc),
 425       _is_interpreted(is_interpreted), _cb_imd((intptr_t*)cb_md), _oop_map(NULL) {}
 426 
 427   HFrameBase(int sp, int ref_sp, address pc, const ContMirror&amp; cont)
 428     : _sp(sp), _ref_sp(ref_sp), _pc(pc),
 429       _is_interpreted(Interpreter::contains(pc)), _cb_imd(NULL), _oop_map(NULL) {
 430       set_codeblob(_pc);
 431     }
 432 
 433   static address deopt_original_pc(const ContMirror&amp; cont, address pc, CodeBlob* cb, int sp);
 434 
 435 public:
 436   inline bool operator==(const HFrameBase&amp; other) const;
 437   bool is_empty() const { return _pc == NULL; }
 438 
 439   inline int       sp()     const { return _sp; }
 440   inline address   pc()     const { return _pc; }
 441   inline int       ref_sp() const { return _ref_sp; }
 442 
 443   inline void set_sp(int sp) { _sp = sp; }
 444 
 445   inline CodeBlob* cb()     const { assert (!Interpreter::contains(pc()), &quot;&quot;); return (CodeBlob*)_cb_imd; }
 446   void set_cb(CodeBlob* cb) {
 447     assert (!_is_interpreted, &quot;&quot;);
 448     if (_cb_imd == NULL) _cb_imd = cb;
 449     assert (cb == slow_get_cb(*this), &quot;&quot;);
 450     assert (_cb_imd == cb, &quot;&quot;);
 451     assert (((CodeBlob*)_cb_imd)-&gt;contains(_pc), &quot;&quot;);
 452   }
 453   inline bool is_interpreted_frame() const { return _is_interpreted; } // due to partial copy below, this may lie in mode_fast
 454 
 455   template&lt;op_mode mode&gt;
 456   void copy_partial(const SelfPD&amp; other) {
 457     _sp = other._sp;
 458     _ref_sp = other._ref_sp;
 459     _pc = other._pc;
 460     if (mode != mode_fast) {
 461       _is_interpreted = other._is_interpreted;
 462     }
 463     self().copy_partial_pd(other);
 464   }
 465 
 466   inline void set_pc(address pc) { _pc = pc; }
 467   inline void set_ref_sp(int ref_sp) { _ref_sp = ref_sp; }
 468 
 469   template&lt;typename FKind&gt; address return_pc() const { return *self().template return_pc_address&lt;FKind&gt;(); }
 470 
 471   const CodeBlob* get_cb() const { return self().get_cb(); }
 472 
 473   const ImmutableOopMap* oop_map() const {
 474     if (_oop_map == NULL) {
 475       _oop_map = get_oop_map();
 476     }
 477     return _oop_map;
 478   }
 479 
 480   template&lt;typename FKind&gt; int frame_top_index() const;
 481   template&lt;typename FKind&gt; int frame_bottom_index() const { return self().template frame_bottom_index&lt;FKind&gt;(); };
 482 
 483   address real_pc(const ContMirror&amp; cont) const;
 484   void patch_pc(address pc, const ContMirror&amp; cont) const;
 485   template&lt;typename FKind&gt; inline void patch_return_pc(address value); // only for interpreted frames
 486 
 487   int compiled_frame_size() const;
 488   int compiled_frame_num_oops() const;
 489   int compiled_frame_stack_argsize() const;
 490 
 491   DEBUG_ONLY(int interpreted_frame_top_index() const { return self().interpreted_frame_top_index(); } )
 492   int interpreted_frame_num_monitors() const         { return self().interpreted_frame_num_monitors(); }
 493   int interpreted_frame_num_oops(const InterpreterOopMap&amp; mask) const;
 494   int interpreted_frame_size() const;
 495   void interpreted_frame_oop_map(InterpreterOopMap* mask) const { self().interpreted_frame_oop_map(mask); }
 496 
 497   template&lt;typename FKind, op_mode mode&gt; SelfPD sender(const ContMirror&amp; cont, int num_oops) const {
 498     assert (mode != mode_fast || !FKind::interpreted, &quot;&quot;);
 499     return self().template sender&lt;FKind, mode&gt;(cont, num_oops);
 500   }
 501   template&lt;typename FKind, op_mode mode&gt; SelfPD sender(const ContMirror&amp; cont, const InterpreterOopMap* mask, int extra_oops = 0) const;
 502   template&lt;op_mode mode /* = mode_slow*/&gt; SelfPD sender(const ContMirror&amp; cont) const;
 503 
 504   template&lt;typename FKind&gt; bool is_bottom(const ContMirror&amp; cont) const;
 505 
 506   address interpreter_frame_bcp() const                             { return self().interpreter_frame_bcp(); }
 507   intptr_t* interpreter_frame_local_at(int index) const             { return self().interpreter_frame_local_at(index); }
 508   intptr_t* interpreter_frame_expression_stack_at(int offset) const { return self().interpreter_frame_expression_stack_at(offset); }
 509 
 510   template&lt;typename FKind&gt; Method* method() const;
 511 
 512   inline frame to_frame(ContMirror&amp; cont) const;
 513 
 514   void print_on(const ContMirror&amp; cont, outputStream* st) const { self().print_on(cont, st); }
 515   void print_on(outputStream* st) const { self().print_on(st); };
 516   void print(const ContMirror&amp; cont) const { print_on(cont, tty); }
 517   void print() const { print_on(tty); }
 518 };
 519 
 520 // defines hframe
 521 #include CPU_HEADER(hframe)
 522 
 523 template&lt;typename Self&gt;
 524 template &lt;typename FrameT&gt;
 525 bool FrameCommon&lt;Self&gt;::is_instance(const FrameT&amp; f) {
 526   return (Self::interpreted == f.is_interpreted_frame()) &amp;&amp; (Self::stub == (!Self::interpreted &amp;&amp; is_stub(slow_get_cb(f))));
 527 }
 528 
 529 template &lt;typename FrameT&gt;
 530 bool NonInterpretedUnknown::is_instance(const FrameT&amp; f) {
 531   return (interpreted == f.is_interpreted_frame());
 532 }
 533 
 534 // Mirrors the Java continuation objects.
 535 // This object is created when we begin a freeze/thaw operation for a continuation, and is destroyed when the operation completes.
 536 // Contents are read from the Java object at the entry points of this module, and written at exists or intermediate calls into Java
 537 class ContMirror {
 538 private:
 539   JavaThread* const _thread;
 540   ContinuationEntry* _entry;
 541   oop _cont;
 542 
 543   oop _tail;
 544 
 545   int  _sp;
 546   intptr_t _fp;
 547   address _pc;
 548 
 549   typeArrayOop _stack;
 550   int _stack_length;
 551   ElemType* _hstack;
 552 
 553   size_t _max_size;
 554 
 555   int _ref_sp;
 556   objArrayOop _ref_stack;
 557 
 558   unsigned char _flags;
 559 
 560   short _num_interpreted_frames;
 561   short _num_frames;
 562 
 563   // Profiling data for the JFR event
 564   short _e_size;
 565   short _e_num_interpreted_frames;
 566   short _e_num_frames;
 567   short _e_num_refs;
 568 
 569 private:
 570   ElemType* stack() const { return _hstack; }
 571 
 572   template &lt;typename ConfigT&gt; bool allocate_stacks_in_native(int size, int oops, bool needs_stack, bool needs_refstack);
 573   void allocate_stacks_in_java(int size, int oops, int frames);
 574   static int fix_decreasing_index(int index, int old_length, int new_length);
 575   inline void post_safepoint_minimal(Handle conth);
 576   int ensure_capacity(int old, int min);
 577   bool allocate_stack(int size);
 578   typeArrayOop allocate_stack_array(size_t elements);
 579   bool grow_stack(int new_size);
 580   static void copy_primitive_array(typeArrayOop old_array, int old_start, typeArrayOop new_array, int new_start, int count);
 581   template &lt;bool post_barrier&gt; bool allocate_ref_stack(int nr_oops);
 582   template &lt;bool post_barrier&gt; objArrayOop  allocate_refstack_array(size_t nr_oops);
 583   template &lt;typename ConfigT&gt; bool grow_ref_stack(int nr_oops);
 584   template &lt;typename ConfigT&gt; void copy_ref_array(objArrayOop old_array, int old_start, objArrayOop new_array, int new_start, int count);
 585   template &lt;bool post_barrier&gt; void zero_ref_array(objArrayOop new_array, int new_length, int min_length);
 586   objArrayOop  allocate_keepalive_array(size_t nr_oops);
 587   oop raw_allocate(Klass* klass, size_t words, size_t elements, bool zero);
 588 
 589 public:
 590   inline void post_safepoint(Handle conth);
 591   oop allocate_stack_chunk(int stack_size);
 592 
 593 public:
 594   // TODO R: get rid of these:
 595   static inline int to_index(int x) { return x &gt;&gt; LogBytesPerElement; }
 596   static inline int to_bytes(int x)    { return x &lt;&lt; LogBytesPerElement; }
 597   static inline int to_index(const void* base, const void* ptr) { return to_index((const char*)ptr - (const char*)base); }
 598 
 599 private:
 600   ContMirror(const ContMirror&amp; cont); // no copy constructor
 601 
 602   DEBUG_ONLY(bool sp_unread();)
 603 
 604 public:
 605   ContMirror(JavaThread* thread, oop cont); // does not automatically read the continuation object
 606   ContMirror(oop cont);
 607   ContMirror(const RegisterMap* map);
 608 
 609   intptr_t hash() {
 610     #ifndef PRODUCT
 611       return Thread::current()-&gt;is_Java_thread() ? _cont-&gt;identity_hash() : -1;
 612     #else
 613       return 0;
 614     #endif
 615   }
 616 
 617   void read();
 618   inline void read_minimal();
 619   void read_rest();
 620 
 621   inline void write_minimal();
 622   void write();
 623 
 624   oop mirror() { return _cont; }
 625   oop parent() { return java_lang_Continuation::parent(_cont); }
 626   void cleanup();
 627 
 628   ContinuationEntry* entry() const { return _entry; }
 629   intptr_t* entrySP() const { return _entry-&gt;entry_sp(); }
 630   intptr_t* entryFP() const { return _entry-&gt;entry_fp(); }
 631   address   entryPC() const { return _entry-&gt;entry_pc(); }
 632 
 633   int argsize() const { return _entry-&gt;argsize(); }
 634   void set_argsize(int value) { _entry-&gt;set_argsize(value); }
 635 
 636   bool is_mounted() { return _entry != NULL; }
 637 
 638   oop tail() const         { return _tail; }
 639   void set_tail(oop chunk) { _tail = chunk; }
 640 
 641   int sp() const           { return _sp; }
 642   intptr_t fp() const      { return _fp; }
 643   address pc() const       { return _pc; }
 644 
 645   void set_sp(int sp)      { _sp = sp; }
 646   void set_fp(intptr_t fp) { _fp = fp; }
 647   void clear_pc()  { _pc = NULL; set_flag(FLAG_LAST_FRAME_INTERPRETED, false); }
 648   void set_pc(address pc, bool interpreted)  { _pc = pc; set_flag(FLAG_LAST_FRAME_INTERPRETED, interpreted);
 649                                                assert (interpreted == Interpreter::contains(pc), &quot;&quot;); }
 650 
 651   unsigned char flags() { return _flags; }
 652   bool is_flag(unsigned char flag) { return (_flags &amp; flag) != 0; }
 653   void set_flag(unsigned char flag, bool v) { _flags = (v ? _flags |= flag : _flags &amp;= ~flag); }
 654 
 655   int stack_length() const { return _stack_length; }
 656 
 657   JavaThread* thread() const { return _thread; }
 658 
 659   template &lt;typename ConfigT&gt; inline bool allocate_stacks(int size, int oops, int frames);
 660 
 661   template &lt;typename ConfigT&gt;
 662   void make_keepalive(Thread* thread, CompiledMethodKeepalive&lt;ConfigT&gt;* keepalive);
 663 
 664   inline bool in_hstack(void *p) { return (_hstack != NULL &amp;&amp; p &gt;= _hstack &amp;&amp; p &lt; (_hstack + _stack_length)); }
 665 
 666   bool valid_stack_index(int idx) const { return idx &gt;= 0 &amp;&amp; idx &lt; _stack_length; }
 667 
 668   void copy_from_stack(void* from, void* to, int size);
 669   void copy_to_stack(void* from, void* to, int size);
 670 
 671   objArrayOop refStack(int size);
 672   objArrayOop refStack() { return _ref_stack; }
 673   int refSP() { return _ref_sp; }
 674   void set_refSP(int refSP) { log_develop_trace(jvmcont)(&quot;set_refSP: %d&quot;, refSP); _ref_sp = refSP; }
 675 
 676   inline int stack_index(void* p) const;
 677   inline intptr_t* stack_address(int i) const;
 678 
 679   static inline void relativize(intptr_t* const fp, intptr_t* const hfp, int offset);
 680   static inline void derelativize(intptr_t* const fp, int offset);
 681 
 682   bool is_in_stack(void* p) const;
 683   bool is_in_ref_stack(void* p) const;
 684   bool is_empty0();
 685   bool is_empty();
 686 
 687   static inline bool is_stack_chunk(oop obj);
 688   static inline bool is_empty_chunk(oop chunk);
 689   static bool is_in_chunk(oop chunk, void* p);
 690   static bool is_usable_in_chunk(oop chunk, void* p);
 691   static inline void reset_chunk_counters(oop chunk);
 692   oop find_chunk(void* p) const;
 693 
 694   template&lt;op_mode mode&gt; const hframe last_frame();
 695   template&lt;op_mode mode&gt; void set_last_frame(const hframe&amp; f);
 696   inline void set_last_frame_pd(const hframe&amp; f);
 697   inline void set_empty();
 698 
 699   hframe from_frame(const frame&amp; f);
 700 
 701   template &lt;typename ConfigT&gt;
 702   inline int add_oop(oop obj, int index);
 703 
 704   inline oop obj_at(int i);
 705   int num_oops();
 706   void null_ref_stack(int start, int num);
 707   void zero_ref_stack_prefix();
 708 
 709   inline size_t max_size() { return _max_size; }
 710   inline void add_size(size_t s) { log_develop_trace(jvmcont)(&quot;add max_size: &quot; SIZE_FORMAT &quot; s: &quot; SIZE_FORMAT, _max_size + s, s);
 711                                    _max_size += s; }
 712   inline void sub_size(size_t s) { log_develop_trace(jvmcont)(&quot;sub max_size: &quot; SIZE_FORMAT &quot; s: &quot; SIZE_FORMAT, _max_size - s, s);
 713                                    assert(s &lt;= _max_size, &quot;s: &quot; SIZE_FORMAT &quot; max_size: &quot; SIZE_FORMAT, s, _max_size);
 714                                    _max_size -= s; }
 715   inline short num_interpreted_frames() { return _num_interpreted_frames; }
 716   inline void inc_num_interpreted_frames() { _num_interpreted_frames++; _e_num_interpreted_frames++; }
 717   inline void dec_num_interpreted_frames() { _num_interpreted_frames--; _e_num_interpreted_frames++; }
 718 
 719   inline short num_frames() { return _num_frames; }
 720   inline void add_num_frames(int n) { _num_frames += n; _e_num_frames += n; }
 721   inline void sub_num_frames(int n) { _num_frames -= n; _e_num_frames -= n; }
 722   inline void inc_num_frames() { _num_frames++; _e_num_frames++; }
 723   inline void dec_num_frames() { _num_frames--; _e_num_frames++; }
 724 
 725   void print_hframes(outputStream* st = tty);
 726 
 727   inline void e_add_refs(int num) { _e_num_refs += num; }
 728   template&lt;typename Event&gt; void post_jfr_event(Event *e, JavaThread* jt);
 729 
 730 #ifdef ASSERT
 731   bool chunk_invariant() {
 732     // only the topmost chunk can be empty
 733     if (_tail == (oop)NULL)
 734       return true;
 735     assert (is_stack_chunk(_tail), &quot;&quot;);
 736     int i = 1;
 737     for (oop chunk = jdk_internal_misc_StackChunk::parent(_tail); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
 738       if (is_empty_chunk(chunk)) {
 739         assert (chunk != _tail, &quot;&quot;);
 740         tty-&gt;print_cr(&quot;i: %d&quot;, i);
 741         print_chunk(chunk, _cont, true);
 742         return false;
 743       }
 744       i++;
 745     }
 746     return true;
 747   }
 748 #endif
 749 };
 750 
 751 template&lt;typename SelfPD&gt;
 752 inline bool HFrameBase&lt;SelfPD&gt;::operator==(const HFrameBase&amp; other) const {
 753   return  _sp == other._sp &amp;&amp; _pc == other._pc;
 754 }
 755 
 756 template&lt;typename SelfPD&gt;
 757 address HFrameBase&lt;SelfPD&gt;::deopt_original_pc(const ContMirror&amp; cont, address pc, CodeBlob* cb, int sp) {
 758   // TODO DEOPT: unnecessary in the long term solution of unroll on freeze
 759 
 760   assert (cb != NULL &amp;&amp; cb-&gt;is_compiled(), &quot;&quot;);
 761   CompiledMethod* cm = cb-&gt;as_compiled_method();
 762   if (cm-&gt;is_deopt_pc(pc)) {
 763     log_develop_trace(jvmcont)(&quot;hframe::deopt_original_pc deoptimized frame&quot;);
 764     pc = *(address*)((address)cont.stack_address(sp) + cm-&gt;orig_pc_offset());
 765     assert(pc != NULL, &quot;&quot;);
 766     assert(cm-&gt;insts_contains_inclusive(pc), &quot;original PC must be in the main code section of the the compiled method (or must be immediately following it)&quot;);
 767     assert(!cm-&gt;is_deopt_pc(pc), &quot;&quot;);
 768     // _deopt_state = is_deoptimized;
 769   }
 770 
 771   return pc;
 772 }
 773 
 774 template&lt;typename SelfPD&gt;
 775 address HFrameBase&lt;SelfPD&gt;::real_pc(const ContMirror&amp; cont) const {
 776   address* pc_addr = cont.stack_address(self().pc_index());
 777   return *pc_addr;
 778 }
 779 
 780 template&lt;typename SelfPD&gt;
 781 template&lt;typename FKind&gt;
 782 inline void HFrameBase&lt;SelfPD&gt;::patch_return_pc(address value) {
 783   *(self().template return_pc_address&lt;FKind&gt;()) = value;
 784 }
 785 
 786 template&lt;typename SelfPD&gt;
 787 void HFrameBase&lt;SelfPD&gt;::patch_pc(address pc, const ContMirror&amp; cont) const {
 788   address* pc_addr = (address*)cont.stack_address(self().pc_index());
 789   // tty-&gt;print_cr(&quot;&gt;&gt;&gt;&gt; patching %p with %p&quot;, pc_addr, pc);
 790   *pc_addr = pc;
 791 }
 792 
 793 template&lt;typename SelfPD&gt;
 794 template&lt;typename FKind&gt;
 795 bool HFrameBase&lt;SelfPD&gt;::is_bottom(const ContMirror&amp; cont) const {
 796   return frame_bottom_index&lt;FKind&gt;()
 797     + ((FKind::interpreted || FKind::stub) ? 0 : cb()-&gt;as_compiled_method()-&gt;method()-&gt;num_stack_arg_slots() * VMRegImpl::stack_slot_size / elementSizeInBytes)
 798     &gt;= cont.stack_length();
 799 }
 800 
 801 template&lt;typename SelfPD&gt;
 802 int HFrameBase&lt;SelfPD&gt;::interpreted_frame_num_oops(const InterpreterOopMap&amp; mask) const {
 803   assert (_is_interpreted, &quot;&quot;);
 804   // we calculate on relativized metadata; all monitors must be NULL on hstack, but as f.oops_do walks them, we count them
 805   return   mask.num_oops()
 806          + 1 // for the mirror
 807          + interpreted_frame_num_monitors();
 808 }
 809 
 810 template&lt;typename SelfPD&gt;
 811 int HFrameBase&lt;SelfPD&gt;::interpreted_frame_size() const {
 812   assert (_is_interpreted, &quot;&quot;);
 813   return (frame_bottom_index&lt;Interpreted&gt;() - frame_top_index&lt;Interpreted&gt;()) * elementSizeInBytes;
 814 }
 815 
 816 template&lt;typename SelfPD&gt;
 817 inline int HFrameBase&lt;SelfPD&gt;::compiled_frame_num_oops() const {
 818   assert (!_is_interpreted, &quot;&quot;);
 819   return oop_map()-&gt;num_oops();
 820 }
 821 
 822 template&lt;typename SelfPD&gt;
 823 int HFrameBase&lt;SelfPD&gt;::compiled_frame_size() const {
 824   return NonInterpretedUnknown::size(*this);
 825 }
 826 
 827 template&lt;typename SelfPD&gt;
 828 int HFrameBase&lt;SelfPD&gt;::compiled_frame_stack_argsize() const {
 829   return NonInterpretedUnknown::stack_argsize(*this);
 830 }
 831 
 832 template&lt;typename SelfPD&gt;
 833 template &lt;typename FKind&gt;
 834 int HFrameBase&lt;SelfPD&gt;::frame_top_index() const {
 835   assert (!FKind::interpreted || interpreted_frame_top_index() &gt;= _sp, &quot;&quot;);
 836   assert (FKind::is_instance(*(hframe*)this), &quot;&quot;);
 837 
 838   return _sp;
 839 }
 840 
 841 #ifdef CONT_DOUBLE_NOP
 842 // TODO R remove after PD separation
 843 template&lt;op_mode mode&gt;
 844 static inline CachedCompiledMetadata cached_metadata(const hframe&amp; hf);
 845 #endif
 846 
 847 template&lt;typename SelfPD&gt;
 848 template&lt;typename FKind, op_mode mode&gt;
 849 SelfPD HFrameBase&lt;SelfPD&gt;::sender(const ContMirror&amp; cont, const InterpreterOopMap* mask, int extra_oops) const {
 850   assert (mode != mode_fast || !FKind::interpreted, &quot;&quot;);
 851   int num_oops;
 852 #ifdef CONT_DOUBLE_NOP
 853   CachedCompiledMetadata md;
 854 #endif
 855   if (FKind::interpreted) {
 856     num_oops = interpreted_frame_num_oops(*mask);
 857   } else
 858 #ifdef CONT_DOUBLE_NOP
 859   if (mode == mode_fast &amp;&amp; LIKELY(!(md = cached_metadata&lt;mode&gt;(self())).empty()))
 860     num_oops = md.num_oops();
 861   else {
 862     get_cb();
 863 #endif
 864     num_oops = compiled_frame_num_oops();
 865 #ifdef CONT_DOUBLE_NOP
 866   }
 867 #endif
 868 
 869   return sender&lt;FKind, mode&gt;(cont, extra_oops + num_oops);
 870 }
 871 
 872 template&lt;typename SelfPD&gt;
 873 template&lt;op_mode mode&gt;
 874 SelfPD HFrameBase&lt;SelfPD&gt;::sender(const ContMirror&amp; cont) const {
 875   if (_is_interpreted) {
 876     InterpreterOopMap mask;
 877     interpreted_frame_oop_map(&amp;mask);
 878     return sender&lt;Interpreted, mode&gt;(cont, &amp;mask);
 879   } else {
 880     return sender&lt;NonInterpretedUnknown, mode&gt;(cont, (InterpreterOopMap*)NULL);
 881   }
 882 }
 883 
 884 template&lt;&gt;
 885 template&lt;&gt; Method* HFrameBase&lt;hframe&gt;::method&lt;Interpreted&gt;() const; // pd
 886 
 887 template&lt;typename SelfPD&gt;
 888 template&lt;typename FKind&gt;
 889 Method* HFrameBase&lt;SelfPD&gt;::method() const {
 890   assert (!is_interpreted_frame(), &quot;&quot;);
 891   assert (!FKind::interpreted, &quot;&quot;);
 892 
 893   return ((CompiledMethod*)cb())-&gt;method();
 894 }
 895 
 896 template&lt;typename SelfPD&gt;
 897 inline frame HFrameBase&lt;SelfPD&gt;::to_frame(ContMirror&amp; cont) const {
 898   bool deopt = false;
 899   address pc = _pc;
 900   if (!is_interpreted_frame()) {
 901     CompiledMethod* cm = cb()-&gt;as_compiled_method_or_null();
 902     if (cm != NULL &amp;&amp; cm-&gt;is_deopt_pc(pc)) {
 903       intptr_t* hsp = cont.stack_address(sp());
 904       address orig_pc = *(address*) ((address)hsp + cm-&gt;orig_pc_offset());
 905       assert (orig_pc != pc, &quot;&quot;);
 906       assert (orig_pc != NULL, &quot;&quot;);
 907 
 908       pc = orig_pc;
 909       deopt = true;
 910     }
 911   }
 912 
 913   // tty-&gt;print_cr(&quot;-- to_frame:&quot;);
 914   // print_on(cont, tty);
 915   return self().to_frame(cont, pc, deopt);
 916 }
 917 
 918 #ifndef PRODUCT
 919 bool ContMirror::sp_unread() { return _sp == -10; }
 920 #endif
 921 
 922 ContMirror::ContMirror(JavaThread* thread, oop cont)
 923  : _thread(thread), _entry(thread-&gt;cont_entry()), _cont(cont), 
 924 #ifndef PRODUCT
 925   _tail(NULL),
 926   _sp(-10), _fp(0), _pc(0), // -10 is a special value showing _sp has not been read
 927   _stack(NULL), _hstack(NULL), _ref_stack(NULL),
 928 #endif
 929   _e_size(0) {
 930   assert(_cont != NULL &amp;&amp; oopDesc::is_oop_or_null(_cont), &quot;Invalid cont: &quot; INTPTR_FORMAT, p2i((void*)_cont));
 931   assert (_cont == _entry-&gt;cont_raw(), &quot;mirror: &quot; INTPTR_FORMAT &quot; entry: &quot; INTPTR_FORMAT &quot; entry_sp: &quot; INTPTR_FORMAT, p2i((oopDesc*)_cont), p2i((oopDesc*)_entry-&gt;cont_raw()), p2i(entrySP()));
 932 }
 933 
 934 ContMirror::ContMirror(oop cont)
 935  : _thread(NULL), _entry(NULL), _cont(cont), 
 936 #ifndef PRODUCT
 937   _tail(NULL),
 938   _sp(-10), _fp(0), _pc(0), // -10 is a special value showing _sp has not been read
 939   _stack(NULL), _hstack(NULL), _ref_stack(NULL),
 940 #endif
 941   _e_size(0) {
 942   assert(_cont != NULL &amp;&amp; oopDesc::is_oop_or_null(_cont), &quot;Invalid cont: &quot; INTPTR_FORMAT, p2i((void*)_cont));
 943 
 944   read();
 945 }
 946 
 947 ContMirror::ContMirror(const RegisterMap* map)
 948  : _thread(map-&gt;thread()), _entry(Continuation::get_continuation_entry_for_continuation(_thread, map-&gt;cont())), _cont(map-&gt;cont()),
 949 #ifndef PRODUCT
 950   _tail(NULL),
 951   _sp(-10), _fp(0), _pc(0), // -10 is a special value showing _sp has not been read
 952   _stack(NULL), _hstack(NULL), _ref_stack(NULL),
 953 #endif
 954   _e_size(0) {
 955   assert(_cont != NULL &amp;&amp; oopDesc::is_oop_or_null(_cont), &quot;Invalid cont: &quot; INTPTR_FORMAT, p2i((void*)_cont));
 956 
 957   assert (_entry == NULL || _cont == _entry-&gt;cont_raw(), &quot;mirror: &quot; INTPTR_FORMAT &quot; entry: &quot; INTPTR_FORMAT &quot; entry_sp: &quot; INTPTR_FORMAT, p2i((oopDesc*)_cont), p2i((oopDesc*)_entry-&gt;cont_raw()), p2i(entrySP()));
 958   read();
 959 }
 960 
 961 void ContMirror::read() {
 962   read_minimal();
 963   read_rest();
 964 }
 965 
 966 ALWAYSINLINE void ContMirror::read_minimal() {
 967   _tail  = java_lang_Continuation::tail(_cont);
 968   _pc    = java_lang_Continuation::pc(_cont); // for is_empty0
 969   _flags = java_lang_Continuation::flags(_cont);
 970   _max_size = java_lang_Continuation::maxSize(_cont);
 971 
 972   if (log_develop_is_enabled(Trace, jvmcont)) {
 973     log_develop_trace(jvmcont)(&quot;Reading continuation object: &quot; INTPTR_FORMAT, p2i((oopDesc*)_cont));
 974     log_develop_trace(jvmcont)(&quot;\ttail: &quot; INTPTR_FORMAT, p2i((oopDesc*)_tail));
 975     log_develop_trace(jvmcont)(&quot;\tpc: &quot; INTPTR_FORMAT, p2i(_pc));
 976     log_develop_trace(jvmcont)(&quot;\tmax_size: %lu&quot;, _max_size);
 977     log_develop_trace(jvmcont)(&quot;\tflags: %d&quot;, _flags);
 978   }
 979 }
 980 
 981 void ContMirror::read_rest() {
 982   _sp = java_lang_Continuation::sp(_cont);
 983   _fp = (intptr_t)java_lang_Continuation::fp(_cont);
 984 
 985   _stack = java_lang_Continuation::stack(_cont);
 986   if (_stack != NULL) {
 987     _stack_length = _stack-&gt;length();
 988     _hstack = (ElemType*)_stack-&gt;base(basicElementType);
 989   } else {
 990     _stack_length = 0;
 991     _hstack = NULL;
 992   }
 993 
 994   _ref_stack = java_lang_Continuation::refStack(_cont);
 995   _ref_sp = java_lang_Continuation::refSP(_cont);
 996 
 997   _num_frames = java_lang_Continuation::numFrames(_cont);
 998   _num_interpreted_frames = java_lang_Continuation::numInterpretedFrames(_cont);
 999 
1000   _e_num_interpreted_frames = 0; 
1001   _e_num_frames = 0;
1002   _e_num_refs = 0;
1003 
1004   if (log_develop_is_enabled(Trace, jvmcont)) {
1005     log_develop_trace(jvmcont)(&quot;Reading continuation object:&quot;);
1006     log_develop_trace(jvmcont)(&quot;\tsp: %d fp: %ld 0x%lx pc: &quot; INTPTR_FORMAT, _sp, _fp, _fp, p2i(_pc));
1007     log_develop_trace(jvmcont)(&quot;\tstack: &quot; INTPTR_FORMAT &quot; hstack: &quot; INTPTR_FORMAT &quot;, stack_length: %d max_size: &quot; SIZE_FORMAT, p2i((oopDesc*)_stack), p2i(_hstack), _stack_length, _max_size);
1008     log_develop_trace(jvmcont)(&quot;\tref_stack: &quot; INTPTR_FORMAT &quot; ref_sp: %d&quot;, p2i((oopDesc*)_ref_stack), _ref_sp);
1009     log_develop_trace(jvmcont)(&quot;\tnum_frames: %d&quot;, _num_frames);
1010     log_develop_trace(jvmcont)(&quot;\tnum_interpreted_frames: %d&quot;, _num_interpreted_frames);
1011   }
1012 }
1013 
1014 inline void ContMirror::write_minimal() {
1015   if (log_develop_is_enabled(Trace, jvmcont)) {
1016     log_develop_trace(jvmcont)(&quot;Writing continuation object:&quot;);
1017     log_develop_trace(jvmcont)(&quot;\tmax_size: &quot; SIZE_FORMAT, _max_size);
1018     log_develop_trace(jvmcont)(&quot;\tflags: %d&quot;, _flags);
1019   }
1020 
1021   java_lang_Continuation::set_maxSize(_cont, (jint)_max_size);
1022   java_lang_Continuation::set_flags(_cont, _flags);
1023 }
1024 
1025 void ContMirror::write() {
1026   if (log_develop_is_enabled(Trace, jvmcont)) {
1027     log_develop_trace(jvmcont)(&quot;Writing continuation object:&quot;);
1028     log_develop_trace(jvmcont)(&quot;\tsp: %d fp: %ld 0x%lx pc: &quot; INTPTR_FORMAT, _sp, _fp, _fp, p2i(_pc));
1029     log_develop_trace(jvmcont)(&quot;\ttail: &quot; INTPTR_FORMAT, p2i((oopDesc*)_tail));
1030     log_develop_trace(jvmcont)(&quot;\tmax_size: &quot; SIZE_FORMAT, _max_size);
1031     log_develop_trace(jvmcont)(&quot;\tref_sp: %d&quot;, _ref_sp);
1032     log_develop_trace(jvmcont)(&quot;\tflags: %d&quot;, _flags);
1033     log_develop_trace(jvmcont)(&quot;\tnum_frames: %d&quot;, _num_frames);
1034     log_develop_trace(jvmcont)(&quot;\tnum_interpreted_frames: %d&quot;, _num_interpreted_frames);
1035     log_develop_trace(jvmcont)(&quot;\tend write&quot;);
1036   }
1037 
1038   java_lang_Continuation::set_sp(_cont, _sp);
1039   java_lang_Continuation::set_fp(_cont, _fp);
1040   java_lang_Continuation::set_pc(_cont, _pc);
1041   java_lang_Continuation::set_refSP(_cont, _ref_sp);
1042 
1043   java_lang_Continuation::set_tail(_cont, _tail);
1044 
1045   java_lang_Continuation::set_maxSize(_cont, (jint)_max_size);
1046   java_lang_Continuation::set_flags(_cont, _flags);
1047 
1048   java_lang_Continuation::set_numFrames(_cont, _num_frames);
1049   java_lang_Continuation::set_numInterpretedFrames(_cont, _num_interpreted_frames);
1050 }
1051 
1052 void ContMirror::null_ref_stack(int start, int num) {
1053   if (java_lang_Continuation::is_reset(_cont)) return;
1054 
1055   //log_develop_info(jvmcont)(&quot;clearing %d at %d&quot;, num, start);
1056   for (int i = 0; i &lt; num; i++)
1057     _ref_stack-&gt;obj_at_put(start + i, NULL);
1058 }
1059 
1060 bool ContMirror::is_empty0() {
1061   assert (sp_unread() || (_pc == NULL) == (_sp &lt; 0 || _sp &gt;= _stack-&gt;length()), &quot;&quot;);
1062   return _pc == NULL;
1063 }
1064 bool ContMirror::is_empty() {
1065   if (_tail != (oop)NULL) {
1066     if (!is_empty_chunk(_tail))
1067       return false;
1068     // if (!jdk_internal_misc_StackChunk::is_parent_null&lt;HeapWord&gt;(_tail)) {
1069     if (jdk_internal_misc_StackChunk::parent(_tail) != (oop)NULL) {
1070       assert (!is_empty_chunk(jdk_internal_misc_StackChunk::parent(_tail)), &quot;&quot;); // only topmost chunk can be empty
1071       return false;
1072     }
1073   }
1074   return is_empty0();
1075 }
1076 
1077 template&lt;op_mode mode&gt;
1078 inline void ContMirror::set_last_frame(const hframe&amp; f) {
1079   assert (mode != mode_fast || !Interpreter::contains(f.pc()), &quot;&quot;);
1080   assert (mode == mode_fast || f.is_interpreted_frame() == Interpreter::contains(f.pc()), &quot;&quot;);
1081   assert (f.ref_sp() &gt;= -1, &quot;f.ref_sp(): %d&quot;, f.ref_sp());
1082   set_pc(f.pc(), mode == mode_fast ? false : f.is_interpreted_frame());
1083   set_sp(f.sp());
1084   set_last_frame_pd(f);
1085   set_refSP(f.ref_sp());
1086 
1087   assert (!is_empty(), &quot;&quot;); // if (is_empty()) set_empty();
1088 
1089   if (log_develop_is_enabled(Trace, jvmcont)) {
1090     log_develop_trace(jvmcont)(&quot;set_last_frame cont sp: %d fp: 0x%lx pc: &quot; INTPTR_FORMAT &quot; interpreted: %d flag: %d&quot;, sp(), fp(), p2i(pc()), f.is_interpreted_frame(), is_flag(FLAG_LAST_FRAME_INTERPRETED));
1091     f.print_on(tty);
1092   }
1093 }
1094 
1095 inline void ContMirror::set_empty() {
1096   if (_stack_length &gt; 0) {
1097     set_sp(_stack_length);
1098     set_refSP(_ref_stack-&gt;length());
1099   }
1100   set_fp(0);
1101   clear_pc();
1102 }
1103 
1104 bool ContMirror::is_in_stack(void* p) const {
1105   return p &gt;= (stack() + _sp) &amp;&amp; p &lt; (stack() + stack_length());
1106 }
1107 
1108 bool ContMirror::is_in_ref_stack(void* p) const {
1109   void* base = _ref_stack-&gt;base();
1110   int length = _ref_stack-&gt;length();
1111 
1112   return p &gt;= (UseCompressedOops ? (address)&amp;((narrowOop*)base)[_ref_sp]
1113                                  : (address)&amp;(      (oop*)base)[_ref_sp]) &amp;&amp;
1114          p &lt;= (UseCompressedOops ? (address)&amp;((narrowOop*)base)[length-1]
1115                                  : (address)&amp;(      (oop*)base)[length-1]);
1116 
1117    // _ref_stack-&gt;obj_at_addr&lt;narrowOop&gt;(_ref_sp) : (address)_ref_stack-&gt;obj_at_addr&lt;oop&gt;(_ref_sp));
1118 }
1119 
1120 inline int ContMirror::stack_index(void* p) const {
1121   int i = to_index(stack(), p);
1122   assert (i &gt;= 0 &amp;&amp; i &lt; stack_length(), &quot;i: %d length: %d&quot;, i, stack_length());
1123   return i;
1124 }
1125 
1126 inline intptr_t* ContMirror::stack_address(int i) const {
1127   assert (i &gt;= 0 &amp;&amp; i &lt; stack_length(), &quot;i: %d length: %d&quot;, i, stack_length());
1128   return (intptr_t*)&amp;stack()[i];
1129 }
1130 
1131 inline void ContMirror::relativize(intptr_t* const fp, intptr_t* const hfp, int offset) {
1132   intptr_t* addr = (hfp + offset);
1133   intptr_t value = to_index((address)*(hfp + offset) - (address)fp);
1134   *addr = value;
1135 }
1136 
1137 inline void ContMirror::derelativize(intptr_t* const fp, int offset) {
1138   *(fp + offset) = (intptr_t)((address)fp + to_bytes(*(intptr_t*)(fp + offset)));
1139 }
1140 
1141 void ContMirror::copy_from_stack(void* from, void* to, int size) {
1142   log_develop_trace(jvmcont)(&quot;Copying from v: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d bytes)&quot;, p2i(from), p2i((address)from + size), size);
1143   log_develop_trace(jvmcont)(&quot;Copying to h: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d - %d)&quot;, p2i(to), p2i((address)to + size), to_index(_hstack, to), to_index(_hstack, (address)to + size));
1144 
1145   assert (size &gt; 0, &quot;size: %d&quot;, size);
1146   assert (stack_index(to) &gt;= 0, &quot;&quot;);
1147   assert (to_index(_hstack, (address)to + size) &lt;= _stack_length, &quot;to: %d size: %d length: %d _sp: %d&quot;, stack_index(to), size, _stack_length, _sp);
1148   // assert (to_index(_hstack, (address)to + size) &lt;= _sp, &quot;to: %d size: %d _sp: %d&quot;, stack_index(to), size, _sp); -- not true when copying a bottom frame with argsize &gt; 0
1149 
1150   // TODO PERF non-temporal store
1151   PERFTEST_ONLY(if (PERFTEST_LEVEL &gt;= 25))
1152     memcpy(to, from, size); //Copy::conjoint_memory_atomic(from, to, size); // Copy::disjoint_words((HeapWord*)from, (HeapWord*)to, size/wordSize); //
1153 
1154   _e_size += size;
1155 }
1156 
1157 void ContMirror::copy_to_stack(void* from, void* to, int size) {
1158   log_develop_trace(jvmcont)(&quot;Copying from h: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d - %d)&quot;, p2i(from), p2i((address)from + size), to_index(stack(), from), to_index(stack(), (address)from + size));
1159   log_develop_trace(jvmcont)(&quot;Copying to v: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d bytes)&quot;, p2i(to), p2i((address)to + size), size);
1160 
1161   assert (size &gt; 0, &quot;size: %d&quot;, size);
1162   assert (stack_index(from) &gt;= 0, &quot;&quot;);
1163   assert (to_index(stack(), (address)from + size) &lt;= stack_length(), &quot;index: %d length: %d&quot;, to_index(stack(), (address)from + size), stack_length());
1164 
1165   // TODO PERF non-temporal load
1166   PERFTEST_ONLY(if (PERFTEST_LEVEL &gt;= 125))
1167     memcpy(to, from, size); //Copy::conjoint_memory_atomic(from, to, size);
1168 
1169   _e_size += size;
1170 }
1171 
1172 template &lt;typename OopWriterT&gt;
1173 inline int ContMirror::add_oop(oop obj, int index) {
1174   // assert (_ref_stack != NULL, &quot;&quot;);
1175   // assert (index &gt;= 0 &amp;&amp; index &lt; _ref_stack-&gt;length(), &quot;index: %d length: %d&quot;, index, _ref_stack-&gt;length());
1176   assert (index &lt; _ref_sp, &quot;&quot;);
1177 
1178   log_develop_trace(jvmcont)(&quot;i: %d&quot;, index);
1179   OopWriterT::obj_at_put(_ref_stack, index, obj);
1180   return index;
1181 }
1182 
1183 inline oop ContMirror::obj_at(int i) {
1184   assert (_ref_stack != NULL, &quot;&quot;);
1185   assert (0 &lt;= i &amp;&amp; i &lt; _ref_stack-&gt;length(), &quot;i: %d length: %d&quot;, i, _ref_stack-&gt;length());
1186   // assert (_ref_sp &lt;= i, &quot;i: %d _ref_sp: %d length: %d&quot;, i, _ref_sp, _ref_stack-&gt;length()); -- in Thaw, we set_last_frame before reading the objects during the recursion return trip
1187 
1188   return _ref_stack-&gt;obj_at(i);
1189 }
1190 
1191 int ContMirror::num_oops() {
1192   return _ref_stack == NULL ? 0 : _ref_stack-&gt;length() - _ref_sp;
1193 }
1194 
1195 inline bool ContMirror::is_stack_chunk(oop obj) {
1196   assert (obj != (oop)NULL, &quot;&quot;);
1197   Klass* k = obj-&gt;klass();
1198   assert (k != NULL, &quot;&quot;);
1199   return k-&gt;is_instance_klass() &amp;&amp; InstanceKlass::cast(k)-&gt;is_stack_chunk_instance_klass();
1200 }
1201 
1202 inline bool ContMirror::is_empty_chunk(oop chunk) {
1203   assert (is_stack_chunk(chunk), &quot;&quot;);
1204   assert ((jdk_internal_misc_StackChunk::sp(chunk) &lt; jdk_internal_misc_StackChunk::end(chunk)) || (jdk_internal_misc_StackChunk::sp(chunk) &gt;= jdk_internal_misc_StackChunk::size(chunk)), &quot;&quot;);
1205   return jdk_internal_misc_StackChunk::sp(chunk) &gt;= jdk_internal_misc_StackChunk::size(chunk);
1206 }
1207 
1208 bool ContMirror::is_in_chunk(oop chunk, void* p) {
1209   assert (is_stack_chunk(chunk), &quot;&quot;);
1210   HeapWord* start = InstanceStackChunkKlass::start_of_stack(chunk);
1211   HeapWord* end = InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::size(chunk);
1212   return (HeapWord*)p &gt;= start &amp;&amp; (HeapWord*)p &lt; end;
1213 }
1214 
1215 bool ContMirror::is_usable_in_chunk(oop chunk, void* p) {
1216   assert (is_stack_chunk(chunk), &quot;&quot;);
1217   HeapWord* start = InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::sp(chunk);
1218   HeapWord* end = InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::size(chunk);
1219   return (HeapWord*)p &gt;= start &amp;&amp; (HeapWord*)p &lt; end;
1220 }
1221 
1222 inline void ContMirror::reset_chunk_counters(oop chunk) {
1223   jdk_internal_misc_StackChunk::set_numFrames(chunk, -1);
1224   jdk_internal_misc_StackChunk::set_numOops(chunk, -1);
1225 }
1226 
1227 oop ContMirror::find_chunk(void* p) const {
1228   for (oop chunk = tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
1229     if (is_in_chunk(chunk, p)) {
1230       assert (is_usable_in_chunk(chunk, p), &quot;&quot;);
1231       return chunk;
1232     }
1233   }
1234   return (oop)NULL;
1235 }
1236 
1237 template&lt;typename Event&gt; void ContMirror::post_jfr_event(Event* e, JavaThread* jt) {
1238   if (e-&gt;should_commit()) {
1239     log_develop_trace(jvmcont)(&quot;JFR event: frames: %d iframes: %d size: %d refs: %d&quot;, _e_num_frames, _e_num_interpreted_frames, _e_size, _e_num_refs);
1240     e-&gt;set_carrierThread(JFR_VM_THREAD_ID(jt));
1241     e-&gt;set_contClass(_cont-&gt;klass());
1242     e-&gt;set_numFrames(_e_num_frames);
1243     e-&gt;set_numIFrames(_e_num_interpreted_frames);
1244     e-&gt;set_size(_e_size);
1245     e-&gt;set_numRefs(_e_num_refs);
1246     e-&gt;commit();
1247   }
1248 }
1249 
1250 //////////////////////////// frame functions ///////////////
1251 
1252 class CachedCompiledMetadata; // defined in PD
1253 struct FpOopInfo;
1254 
1255 
1256 
1257 class ContinuationHelper {
1258 public:
1259 #ifdef CONT_DOUBLE_NOP
1260   static inline CachedCompiledMetadata cached_metadata(address pc);
1261   template&lt;op_mode mode, typename FrameT&gt; static inline CachedCompiledMetadata cached_metadata(const FrameT&amp; f);
1262   template&lt;typename FrameT&gt; static void patch_freeze_stub(const FrameT&amp; f, address freeze_stub);
1263 #endif
1264 
1265   template&lt;op_mode mode, typename FrameT&gt; static FreezeFnT freeze_stub(const FrameT&amp; f);
1266   template&lt;op_mode mode, typename FrameT&gt; static ThawFnT thaw_stub(const FrameT&amp; f);
1267 
1268   template&lt;typename FKind, typename RegisterMapT&gt; static inline void update_register_map(RegisterMapT* map, const frame&amp; f);
1269   template&lt;typename RegisterMapT&gt; static inline void update_register_map_with_callee(RegisterMapT* map, const frame&amp; f);
1270   template&lt;typename RegisterMapT&gt; static inline void update_register_map(RegisterMapT* map, hframe::callee_info callee_info);
1271   static void update_register_map(RegisterMap* map, const hframe&amp; sender, const ContMirror&amp; cont);
1272   static void update_register_map_for_entry_frame(const ContMirror&amp; cont, RegisterMap* map);
1273 
1274   static inline frame frame_with(frame&amp; f, intptr_t* sp, address pc, intptr_t* fp);
1275   static inline frame last_frame(JavaThread* thread);
1276   static inline void push_pd(const frame&amp; f);
1277 };
1278 
1279 #ifdef ASSERT
1280   static char* method_name(Method* m);
1281   static inline Method* top_java_frame_method(const frame&amp; f);
1282   static inline Method* bottom_java_frame_method(const frame&amp; f);
1283   static char* top_java_frame_name(const frame&amp; f);
1284   static char* bottom_java_frame_name(const frame&amp; f);
1285   static bool assert_top_java_frame_name(const frame&amp; f, const char* name);
1286   static bool assert_bottom_java_frame_name(const frame&amp; f, const char* name);
1287   static inline bool is_deopt_return(address pc, const frame&amp; sender);
1288 
1289   template &lt;typename FrameT&gt; static CodeBlob* slow_get_cb(const FrameT&amp; f);
1290   template &lt;typename FrameT&gt; static const ImmutableOopMap* slow_get_oopmap(const FrameT&amp; f);
1291   template &lt;typename FrameT&gt; static int slow_size(const FrameT&amp; f);
1292   template &lt;typename FrameT&gt; static address slow_return_pc(const FrameT&amp; f);
1293   template &lt;typename FrameT&gt; static int slow_stack_argsize(const FrameT&amp; f);
1294   template &lt;typename FrameT&gt; static int slow_num_oops(const FrameT&amp; f);
1295 #endif
1296 
1297 
1298 inline Method* Frame::frame_method(const frame&amp; f) {
1299   Method* m = NULL;
1300   if (f.is_interpreted_frame()) {
1301     m = f.interpreter_frame_method();
1302   } else if (f.is_compiled_frame()) {
1303     m = ((CompiledMethod*)f.cb())-&gt;method();
1304   } else if (f.is_native_frame()) {
1305     m = ((CompiledMethod*)f.cb())-&gt;method();
1306   }
1307 
1308   return m;
1309 }
1310 
1311 address Frame::return_pc(const frame&amp; f) {
1312   return *return_pc_address(f);
1313 }
1314 
1315 // static void patch_interpreted_bci(frame&amp; f, int bci) {
1316 //   f.interpreter_frame_set_bcp(f.interpreter_frame_method()-&gt;bcp_from(bci));
1317 // }
1318 
1319 address Interpreted::return_pc(const frame&amp; f) {
1320   return *return_pc_address(f);
1321 }
1322 
1323 void Interpreted::patch_return_pc(frame&amp; f, address pc) {
1324   *return_pc_address(f) = pc;
1325 }
1326 
1327 void Interpreted::oop_map(const frame&amp; f, InterpreterOopMap* mask) {
1328   assert (mask != NULL, &quot;&quot;);
1329   Method* m = f.interpreter_frame_method();
1330   int   bci = f.interpreter_frame_bci();
1331   m-&gt;mask_for(bci, mask); // OopMapCache::compute_one_oop_map(m, bci, mask);
1332 }
1333 
1334 int Interpreted::num_oops(const frame&amp;f, InterpreterOopMap* mask) {
1335   return   mask-&gt;num_oops()
1336          + 1 // for the mirror oop
1337          + ((intptr_t*)f.interpreter_frame_monitor_begin() - (intptr_t*)f.interpreter_frame_monitor_end())/BasicObjectLock::size(); // all locks must be NULL when freezing, but f.oops_do walks them, so we count them
1338 }
1339 
1340 int Interpreted::size(const frame&amp;f, InterpreterOopMap* mask) {
1341   return (Interpreted::frame_bottom(f) - Interpreted::frame_top(f, mask)) * wordSize;
1342 }
1343 
1344 inline int Interpreted::expression_stack_size(const frame &amp;f, InterpreterOopMap* mask) {
1345   int size = mask-&gt;expression_stack_size();
1346   assert (size &lt;= f.interpreter_frame_expression_stack_size(), &quot;size1: %d size2: %d&quot;, size, f.interpreter_frame_expression_stack_size());
1347   return size;
1348 }
1349 
1350 bool Interpreted::is_owning_locks(const frame&amp; f) {
1351   assert (f.interpreter_frame_monitor_end() &lt;= f.interpreter_frame_monitor_begin(), &quot;must be&quot;);
1352   if (f.interpreter_frame_monitor_end() == f.interpreter_frame_monitor_begin())
1353     return false;
1354 
1355   for (BasicObjectLock* current = f.previous_monitor_in_interpreter_frame(f.interpreter_frame_monitor_begin());
1356         current &gt;= f.interpreter_frame_monitor_end();
1357         current = f.previous_monitor_in_interpreter_frame(current)) {
1358 
1359       oop obj = current-&gt;obj();
1360       if (obj != NULL) {
1361         return true;
1362       }
1363   }
1364   return false;
1365 }
1366 
1367 template&lt;typename Self&gt;
1368 inline intptr_t* NonInterpreted&lt;Self&gt;::frame_top(const frame&amp; f) { // inclusive; this will be copied with the frame
1369   return f.unextended_sp();
1370 }
1371 
1372 template&lt;typename Self&gt;
1373 inline intptr_t* NonInterpreted&lt;Self&gt;::frame_bottom(const frame&amp; f) { // exclusive; this will not be copied with the frame
1374   return f.unextended_sp() + f.cb()-&gt;frame_size();
1375 }
1376 
1377 #ifdef ASSERT
1378   intptr_t* Frame::frame_top(const frame &amp;f) {
1379     if (f.is_interpreted_frame()) {
1380       InterpreterOopMap mask;
1381       Interpreted::oop_map(f, &amp;mask);
1382       return Interpreted::frame_top(f, &amp;mask);
1383     } else {
1384       return Compiled::frame_top(f);
1385     }
1386   }
1387 #endif
1388 
1389 template&lt;typename Self&gt;
1390 template&lt;typename FrameT&gt;
1391 inline int NonInterpreted&lt;Self&gt;::size(const FrameT&amp; f) {
1392   assert (!f.is_interpreted_frame() &amp;&amp; Self::is_instance(f), &quot;&quot;);
1393   return f.cb()-&gt;frame_size() * wordSize;
1394 }
1395 
1396 template&lt;typename Self&gt;
1397 template&lt;typename FrameT&gt;
1398 inline int NonInterpreted&lt;Self&gt;::stack_argsize(const FrameT&amp; f) {  assert (f.cb()-&gt;is_compiled(), &quot;&quot;);
1399   return f.cb()-&gt;as_compiled_method()-&gt;method()-&gt;num_stack_arg_slots() * VMRegImpl::stack_slot_size;
1400 }
1401 
1402 template&lt;typename Self&gt;
1403 inline int NonInterpreted&lt;Self&gt;::num_oops(const frame&amp; f) {
1404   assert (!f.is_interpreted_frame() &amp;&amp; Self::is_instance(f), &quot;&quot;);
1405   assert (f.oop_map() != NULL, &quot;&quot;);
1406   return f.oop_map()-&gt;num_oops() + Self::extra_oops;
1407 }
1408 
1409 template&lt;typename Self&gt;
1410 template&lt;typename RegisterMapT&gt;
1411 bool NonInterpreted&lt;Self&gt;::is_owning_locks(JavaThread* thread, RegisterMapT* map, const frame&amp; f) {
1412   // if (!DetectLocksInCompiledFrames) return false;
1413   assert (!f.is_interpreted_frame() &amp;&amp; Self::is_instance(f), &quot;&quot;);
1414 
1415   CompiledMethod* cm = f.cb()-&gt;as_compiled_method();
1416   assert (!cm-&gt;is_compiled() || !cm-&gt;as_compiled_method()-&gt;is_native_method(), &quot;&quot;); // See compiledVFrame::compiledVFrame(...) in vframe_hp.cpp
1417 
1418   if (!cm-&gt;has_monitors()) return false;
1419 
1420   ContinuationHelper::update_register_map_with_callee(map, f); // the monitor object could be stored in the link register
1421   ResourceMark rm;
1422   for (ScopeDesc* scope = cm-&gt;scope_desc_at(f.pc()); scope != NULL; scope = scope-&gt;sender()) {
1423     GrowableArray&lt;MonitorValue*&gt;* mons = scope-&gt;monitors();
1424     if (mons == NULL || mons-&gt;is_empty())
1425       continue;
1426 
1427     for (int index = (mons-&gt;length()-1); index &gt;= 0; index--) { // see compiledVFrame::monitors()
1428       MonitorValue* mon = mons-&gt;at(index);
1429       if (mon-&gt;eliminated())
1430         continue; // we ignore scalar-replaced monitors
1431       ScopeValue* ov = mon-&gt;owner();
1432       StackValue* owner_sv = StackValue::create_stack_value(&amp;f, map, ov); // it is an oop
1433       oop owner = owner_sv-&gt;get_obj()();
1434       if (owner != NULL) {
1435         //assert(cm-&gt;has_monitors(), &quot;&quot;);
1436         return true;
1437       }
1438     }
1439   }
1440   return false;
1441 }
1442 
1443 ////////////////////////////////////
1444 
1445 void Continuation::set_cont_fastpath_thread_state(JavaThread* thread) {
1446   bool fast = 
1447        !thread-&gt;is_interp_only_mode()
1448     &amp;&amp; !JvmtiExport::should_post_continuation_run()
1449     &amp;&amp; !JvmtiExport::should_post_continuation_yield();
1450   thread-&gt;set_cont_fastpath_thread_state(fast);
1451 }
1452 
1453 static inline bool is_entry_frame(const ContMirror&amp; cont, const frame&amp; f) {
1454   return f.sp() == cont.entrySP();
1455 }
1456 
1457 static int num_java_frames(CompiledMethod* cm, address pc) {
1458   int count = 0;
1459   for (ScopeDesc* scope = cm-&gt;scope_desc_at(pc); scope != NULL; scope = scope-&gt;sender())
1460     count++;
1461   return count;
1462 }
1463 
1464 static int num_java_frames(const hframe&amp; f) {
1465   return f.is_interpreted_frame() ? 1 : num_java_frames(f.cb()-&gt;as_compiled_method(), f.pc());
1466 }
1467 
1468 static int num_java_frames(oop chunk) {
1469   int count = 0;
1470   CodeBlob* cb = NULL;
1471   intptr_t* start = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk);
1472   intptr_t* end = start + jdk_internal_misc_StackChunk::size(chunk) - jdk_internal_misc_StackChunk::argsize(chunk);
1473   for (intptr_t* sp = start + jdk_internal_misc_StackChunk::sp(chunk); sp &lt; end; sp += cb-&gt;frame_size()) {
1474     address pc = *(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET);
1475     cb = ContinuationCodeBlobLookup::find_blob(pc);
1476     CompiledMethod* cm = cb-&gt;as_compiled_method();
1477     count += num_java_frames(cm, pc);
1478   }
1479   return count;
1480 }
1481 
1482 static int num_java_frames(ContMirror&amp; cont) {
1483   ResourceMark rm; // used for scope traversal in num_java_frames(CompiledMethod*, address)
1484   int count = 0;
1485 
1486   for (oop chunk = cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
1487     count += num_java_frames(chunk);
1488   }
1489 
1490   hframe hf = cont.last_frame&lt;mode_slow&gt;();
1491   if (cont.is_flag(FLAG_SAFEPOINT_YIELD) &amp;&amp; is_stub(hf.cb())) {
1492     hf = hf.sender&lt;mode_slow&gt;(cont);
1493   }
1494   for (; !hf.is_empty(); hf = hf.sender&lt;mode_slow&gt;(cont)) {
1495     count += num_java_frames(hf);
1496   }
1497 
1498   return count;
1499 }
1500 
1501 static inline void clear_anchor(JavaThread* thread) {
1502   thread-&gt;frame_anchor()-&gt;clear();
1503 }
1504 
1505 static oop get_continuation(JavaThread* thread) {
1506   assert (thread != NULL, &quot;&quot;);
1507   assert (thread-&gt;threadObj() != NULL, &quot;&quot;);
1508   return java_lang_Thread::continuation(thread-&gt;threadObj());
1509 }
1510 
1511 template&lt;typename RegisterMapT&gt;
1512 class ContOopBase : public OopClosure, public DerivedOopClosure {
1513 protected:
1514   ContMirror* const _cont;
1515   const frame* _fr;
1516   void* const _vsp;
1517   int _count;
1518 #ifdef ASSERT
1519   RegisterMapT* _map;
1520 #endif
1521 
1522 public:
1523   int count() { return _count; }
1524 
1525 protected:
1526   ContOopBase(ContMirror* cont, const frame* fr, RegisterMapT* map, void* vsp)
1527    : _cont(cont), _fr(fr), _vsp(vsp) {
1528      _count = 0;
1529   #ifdef ASSERT
1530     _map = map;
1531   #endif
1532   }
1533 
1534   inline int verify(void* p) {
1535     int offset = (address)p - (address)_vsp; // in thaw_oops we set the saved link to a local, so if offset is negative, it can be big
1536 
1537 #ifdef ASSERT // this section adds substantial overhead
1538     VMReg reg;
1539     // The following is not true for the sender of the safepoint stub
1540     // assert(offset &gt;= 0 || p == Frame::map_link_address(_map),
1541     //   &quot;offset: %d reg: %s&quot;, offset, (reg = _map-&gt;find_register_spilled_here(p), reg != NULL ? reg-&gt;name() : &quot;NONE&quot;)); // calle-saved register can only be rbp
1542     reg = _map-&gt;find_register_spilled_here(p); // expensive operation
1543     if (reg != NULL) log_develop_trace(jvmcont)(&quot;reg: %s&quot;, reg-&gt;name());
1544     log_develop_trace(jvmcont)(&quot;p: &quot; INTPTR_FORMAT &quot; offset: %d %s&quot;, p2i(p), offset, p == Frame::map_link_address(_map) ? &quot;(link)&quot; : &quot;&quot;);
1545 #endif
1546 
1547     return offset;
1548   }
1549 
1550   inline void process(void* p) {
1551     DEBUG_ONLY(verify(p);)
1552     _count++;
1553   }
1554 };
1555 
1556 static MemcpyFnT cont_freeze_chunk_memcpy = NULL;
1557 static MemcpyFnT cont_thaw_chunk_memcpy = NULL;
1558 
1559 static void default_memcpy(void* from, void* to, size_t size) {
1560   memcpy(to, from, size &lt;&lt; LogBytesPerWord);
1561 }
1562 
1563 ///////////// FREEZE ///////
1564 
1565 enum freeze_result {
1566   freeze_ok = 0,
1567   freeze_pinned_cs = 1,
1568   freeze_pinned_native = 2,
1569   freeze_pinned_monitor = 3,
1570   freeze_exception = 4,
1571   freeze_retry_slow = 5,
1572 };
1573 
1574 typedef int (*FreezeContFnT)(JavaThread*, intptr_t*, bool);
1575 
1576 static void freeze_compiled_frame_bp() {}
1577 static void thaw_compiled_frame_bp() {}
1578 
1579 static FreezeContFnT cont_freeze_fast = NULL;
1580 static FreezeContFnT cont_freeze_slow = NULL;
1581 
1582 static ThawFnT cont_thaw_oops_slow = NULL;
1583 
1584 static FreezeFnT cont_freeze_oops_slow = NULL;
1585 static FreezeFnT cont_freeze_oops_generate = NULL;
1586 
1587 class OopStubs {
1588 public:
1589   static FreezeFnT freeze_oops_slow() { return (FreezeFnT) cont_freeze_oops_slow; }
1590   static ThawFnT thaw_oops_slow() { return (ThawFnT) cont_thaw_oops_slow; }
1591   static FreezeFnT generate_stub() { return (FreezeFnT) cont_freeze_oops_generate; }
1592 };
1593 
1594 template&lt;op_mode mode&gt;
1595 static int cont_freeze(JavaThread* thread, intptr_t* sp, bool preempt) {
1596   switch (mode) {
1597     case mode_fast:    return cont_freeze_fast(thread, sp, preempt);
1598     case mode_slow:    return cont_freeze_slow(thread, sp, preempt);
1599     default:
1600       guarantee(false, &quot;unreachable&quot;);
1601       return -1;
1602   }
1603 }
1604 
1605 class CountOops : public OopClosure {
1606 private:
1607   int _nr_oops;
1608 public:
1609   CountOops() : _nr_oops(0) {}
1610   int nr_oops() const { return _nr_oops; }
1611 
1612 
1613   virtual void do_oop(oop* o) { _nr_oops++; }
1614   virtual void do_oop(narrowOop* o) { _nr_oops++; }
1615 };
1616 
1617 struct FpOopInfo {
1618   bool _has_fp_oop; // is fp used to store a derived pointer
1619   int _fp_index;    // see FreezeOopFn::do_derived_oop
1620 
1621   FpOopInfo() {}
1622   void init() { _has_fp_oop = false; _fp_index = 0; }
1623 
1624   static int flag_offset() { return in_bytes(byte_offset_of(FpOopInfo, _has_fp_oop)); }
1625   static int index_offset() { return in_bytes(byte_offset_of(FpOopInfo, _fp_index)); }
1626 
1627   void set_oop_fp_index(int index) {
1628     assert(_has_fp_oop == false, &quot;can only have one&quot;);
1629     _has_fp_oop = true;
1630     _fp_index = index;
1631   }
1632 };
1633 
1634 template &lt;typename OopT&gt;
1635 class PersistOops : public OopClosure {
1636 private:
1637   int _limit;
1638   int _current;
1639   objArrayOop _array;
1640 public:
1641   PersistOops(int limit, objArrayOop array) : _limit(limit), _current(0), _array(array) {}
1642 
1643   virtual void do_oop(oop* o) { write_oop(o); }
1644   virtual void do_oop(narrowOop* o) { write_oop(o); }
1645 
1646 private:
1647   template &lt;typename T&gt;
1648   void write_oop(T* p) {
1649     assert(_current &lt; _limit, &quot;&quot;);
1650     oop obj = NativeAccess&lt;&gt;::oop_load(p);
1651     OopT* addr = _array-&gt;obj_at_address&lt;OopT&gt;(_current++); // depends on UseCompressedOops
1652     NativeAccess&lt;IS_DEST_UNINITIALIZED&gt;::oop_store(addr, obj);
1653   }
1654 };
1655 
1656 template &lt;typename RegisterMapT&gt;
1657 class ThawOopFn : public ContOopBase&lt;RegisterMapT&gt; {
1658 private:
1659   int _i;
1660 
1661 protected:
1662   template &lt;class T&gt; inline void do_oop_work(T* p) {
1663     this-&gt;process(p);
1664     oop obj = this-&gt;_cont-&gt;obj_at(_i); // does a HeapAccess&lt;IN_HEAP_ARRAY&gt; load barrier
1665     ZGC_ONLY(assert (!UseZGC || ZAddress::is_good_or_null(cast_from_oop&lt;uintptr_t&gt;(obj)), &quot;&quot;);)
1666 
1667     assert (oopDesc::is_oop_or_null(obj), &quot;invalid oop&quot;);
1668     log_develop_trace(jvmcont)(&quot;i: %d&quot;, _i); print_oop(p, obj);
1669 
1670     NativeAccess&lt;IS_DEST_UNINITIALIZED&gt;::oop_store(p, obj);
1671     _i++;
1672   }
1673 public:
1674   ThawOopFn(ContMirror* cont, frame* fr, int index, void* vsp, RegisterMapT* map)
1675     : ContOopBase&lt;RegisterMapT&gt;(cont, fr, map, vsp) { _i = index; }
1676   void do_oop(oop* p)       { do_oop_work(p); }
1677   void do_oop(narrowOop* p) { do_oop_work(p); }
1678 
1679   void do_derived_oop(oop *base_loc, oop *derived_loc) {
1680     oop base = NativeAccess&lt;&gt;::oop_load(base_loc);
1681     assert(Universe::heap()-&gt;is_in_or_null(base), &quot;not an oop: &quot; INTPTR_FORMAT &quot; (at &quot; INTPTR_FORMAT &quot;)&quot;, p2i((oopDesc*)base), p2i(base_loc));
1682     assert(derived_loc != base_loc, &quot;Base and derived in same location&quot;);
1683     DEBUG_ONLY(this-&gt;verify(base_loc);)
1684     DEBUG_ONLY(this-&gt;verify(derived_loc);)
1685     assert (oopDesc::is_oop_or_null(base), &quot;invalid oop&quot;);
1686     ZGC_ONLY(assert (!UseZGC || ZAddress::is_good_or_null(cast_from_oop&lt;uintptr_t&gt;(base)), &quot;&quot;);)
1687 
1688     intptr_t offset = *(intptr_t*)derived_loc;
1689 
1690     log_develop_trace(jvmcont)(
1691         &quot;Continuation thaw derived pointer@&quot; INTPTR_FORMAT &quot; - Derived: &quot; INTPTR_FORMAT &quot; Base: &quot; INTPTR_FORMAT &quot; (@&quot; INTPTR_FORMAT &quot;) (Offset: &quot; INTX_FORMAT &quot;)&quot;,
1692         p2i(derived_loc), p2i(*derived_loc), p2i(base), p2i(base_loc), offset);
1693 
1694     oop obj = cast_to_oop(cast_from_oop&lt;intptr_t&gt;(base) + offset);
1695     *derived_loc = obj;
1696 
1697     assert(Universe::heap()-&gt;is_in_or_null(obj), &quot;&quot;);
1698   }
1699 };
1700 
1701 template &lt;typename RegisterMapT, typename OopWriterT&gt;
1702 class FreezeOopFn : public ContOopBase&lt;RegisterMapT&gt; {
1703 private:
1704   FpOopInfo* _fp_info;
1705   void* const _hsp;
1706   int _starting_index;
1707 
1708   const address _stub_vsp;
1709 #ifndef PRODUCT
1710   const address _stub_hsp;
1711 #endif
1712 
1713   int add_oop(oop obj, int index) {
1714     //log_develop_info(jvmcont)(&quot;writing oop at %d&quot;, index);
1715     return this-&gt;_cont-&gt;template add_oop&lt;OopWriterT&gt;(obj, index);
1716   }
1717 
1718 protected:
1719   template &lt;class T&gt; inline void do_oop_work(T* p) {
1720     this-&gt;process(p);
1721     oop obj = RawAccess&lt;&gt;::oop_load(p); // we are reading off our own stack, Raw should be fine
1722     int index = add_oop(obj, _starting_index + this-&gt;_count - 1);
1723 
1724 #ifdef ASSERT
1725     print_oop(p, obj);
1726     assert (oopDesc::is_oop_or_null(obj), &quot;invalid oop&quot;);
1727     log_develop_trace(jvmcont)(&quot;narrow: %d&quot;, sizeof(T) &lt; wordSize);
1728 
1729     int offset = this-&gt;verify(p);
1730     assert(offset &lt; 32768, &quot;&quot;);
1731     if (_stub_vsp == NULL &amp;&amp; offset &lt; 0) { // rbp could be stored in the callee frame.
1732       assert (p == (T*)Frame::map_link_address(this-&gt;_map), &quot;&quot;);
1733       _fp_info-&gt;set_oop_fp_index(0xbaba); // assumed to be unnecessary at this time; used only in ASSERT for now
1734     } else {
1735       address hloc = (address)_hsp + offset; // address of oop in the (raw) h-stack
1736       assert (this-&gt;_cont-&gt;in_hstack(hloc), &quot;&quot;);
1737       assert (*(T*)hloc == *p, &quot;*hloc: &quot; INTPTR_FORMAT &quot; *p: &quot; INTPTR_FORMAT, *(intptr_t*)hloc, *(intptr_t*)p);
1738 
1739       log_develop_trace(jvmcont)(&quot;Marking oop at &quot; INTPTR_FORMAT &quot; (offset: %d)&quot;, p2i(hloc), offset);
1740       memset(hloc, 0xba, sizeof(T)); // we must take care not to write a full word to a narrow oop
1741       if (_stub_vsp != NULL &amp;&amp; offset &lt; 0) { // slow path
1742         int offset0 = (address)p - _stub_vsp;
1743         assert (offset0 &gt;= 0, &quot;stub vsp: &quot; INTPTR_FORMAT &quot; p: &quot; INTPTR_FORMAT &quot; offset: %d&quot;, p2i(_stub_vsp), p2i(p), offset0);
1744         assert (hloc == _stub_hsp + offset0, &quot;&quot;);
1745       }
1746     }
1747 #endif
1748   }
1749 
1750 public:
1751   FreezeOopFn(ContMirror* cont, FpOopInfo* fp_info, const frame* fr, void* vsp, void* hsp, RegisterMapT* map, int starting_index, intptr_t* stub_vsp = NULL, intptr_t* stub_hsp = NULL)
1752     : ContOopBase&lt;RegisterMapT&gt;(cont, fr, map, vsp), _fp_info(fp_info), _hsp(hsp), _starting_index(starting_index),
1753     _stub_vsp((address)stub_vsp)
1754 #ifndef PRODUCT
1755       , _stub_hsp((address)stub_hsp)
1756 #endif
1757       {
1758         assert (cont-&gt;in_hstack(hsp), &quot;&quot;);
1759       }
1760 
1761   void do_oop(oop* p)       { do_oop_work(p); }
1762   void do_oop(narrowOop* p) { do_oop_work(p); }
1763 
1764   void do_derived_oop(oop *base_loc, oop *derived_loc) {
1765     assert(Universe::heap()-&gt;is_in_or_null(*base_loc), &quot;not an oop&quot;);
1766     assert(derived_loc != base_loc, &quot;Base and derived in same location&quot;);
1767     DEBUG_ONLY(this-&gt;verify(base_loc);)
1768     DEBUG_ONLY(this-&gt;verify(derived_loc);)
1769 
1770     intptr_t offset = cast_from_oop&lt;intptr_t&gt;(*derived_loc) - cast_from_oop&lt;intptr_t&gt;(*base_loc);
1771 
1772     log_develop_trace(jvmcont)(
1773         &quot;Continuation freeze derived pointer@&quot; INTPTR_FORMAT &quot; - Derived: &quot; INTPTR_FORMAT &quot; Base: &quot; INTPTR_FORMAT &quot; (@&quot; INTPTR_FORMAT &quot;) (Offset: &quot; INTX_FORMAT &quot;)&quot;,
1774         p2i(derived_loc), p2i(*derived_loc), p2i(*base_loc), p2i(base_loc), offset);
1775 
1776     int hloc_offset = (address)derived_loc - (address)this-&gt;_vsp;
1777     if (hloc_offset &lt; 0 &amp;&amp; _stub_vsp == NULL) {
1778       assert ((intptr_t**)derived_loc == Frame::map_link_address(this-&gt;_map), &quot;&quot;);
1779       _fp_info-&gt;set_oop_fp_index(offset);
1780 
1781       log_develop_trace(jvmcont)(&quot;Writing derived pointer offset in fp (offset: %ld, 0x%lx)&quot;, offset, offset);
1782     } else {
1783       intptr_t* hloc = (intptr_t*)((address)_hsp + hloc_offset);
1784       *hloc = offset;
1785 
1786       log_develop_trace(jvmcont)(&quot;Writing derived pointer offset at &quot; INTPTR_FORMAT &quot; (offset: &quot; INTX_FORMAT &quot;, &quot; INTPTR_FORMAT &quot;)&quot;, p2i(hloc), offset, offset);
1787 
1788 #ifdef ASSERT
1789       if (_stub_vsp != NULL &amp;&amp; hloc_offset &lt; 0) {
1790         int hloc_offset0 = (address)derived_loc - _stub_vsp;
1791         assert (hloc_offset0 &gt;= 0, &quot;hloc_offset: %d&quot;, hloc_offset0);
1792         assert(hloc == (intptr_t*)(_stub_hsp + hloc_offset0), &quot;&quot;);
1793       }
1794 #endif
1795     }
1796   }
1797 };
1798 
1799 template &lt;typename OopWriterT&gt;
1800 class FreezeCompiledOops {
1801 
1802 public:
1803   class Extra {
1804     const frame *_f;
1805     ContMirror *_cont;
1806     address *_map;
1807     intptr_t *_stub_vsp;
1808 #ifndef PRODUCT
1809     intptr_t *_stub_hsp;
1810     bool _set_stub_vsp;
1811     bool _set_stub_hsp;
1812 #endif
1813 
1814   public:
1815     Extra(const frame* f, ContMirror* cont, address* map) : _f(f), _cont(cont), _map(map) {
1816 #ifndef PRODUCT
1817       _set_stub_vsp = false;
1818       _set_stub_hsp = false;
1819 #endif
1820     }
1821 
1822     const frame* get_frame() { return _f; }
1823     ContMirror* cont() { return _cont; }
1824     address* map() { return _map; }
1825 
1826     void set_stub_vsp(intptr_t* vsp) {
1827 #ifndef PRODUCT
1828       _set_stub_vsp = true;
1829 #endif
1830       _stub_vsp = vsp;
1831     }
1832     intptr_t* stub_vsp() const { assert(_set_stub_vsp, &quot;accessed before set&quot;); return _stub_vsp; }
1833 
1834 #ifndef PRODUCT
1835     void set_stub_hsp(intptr_t* hsp) {
1836       _set_stub_hsp = true;
1837       _stub_hsp = hsp;
1838     }
1839     intptr_t* stub_hsp() const { assert(_set_stub_hsp, &quot;accessed before set&quot;); return _stub_hsp; }
1840 #endif
1841   };
1842 
1843   static int generate_stub(intptr_t* vsp, address* oops, address* link, intptr_t *hsp, int idx, FpOopInfo* fp_oop_info, Extra *extra) {
1844     extra-&gt;get_frame()-&gt;oop_map()-&gt;generate_stub(extra-&gt;get_frame()-&gt;cb());
1845     FreezeFnT stub = (FreezeFnT)extra-&gt;get_frame()-&gt;oop_map()-&gt;freeze_stub();
1846   #ifdef CONT_DOUBLE_NOP
1847     ContinuationHelper::patch_freeze_stub(*extra-&gt;get_frame(), (address) stub);
1848   #endif
1849     return stub((address) vsp, (address) oops, (address) link, (address) hsp, idx, fp_oop_info, (void *) extra);
1850   }
1851 
1852   template &lt;typename RegisterMapT, bool is_preempt&gt;
1853   static int freeze_slow(address vsp, address oops, address addr, address hsp, int idx, FpOopInfo* fp_oop_info, Extra *extra) {
1854 #ifdef CONT_DOUBLE_NOP
1855     extra-&gt;get_frame()-&gt;get_cb();
1856 #endif
1857 
1858     const ImmutableOopMap* oopmap = extra-&gt;get_frame()-&gt;oop_map();
1859     assert(oopmap, &quot;must have&quot;);
1860     int starting_index = extra-&gt;cont()-&gt;refStack()-&gt;length() - idx;
1861 
1862     log_develop_trace(jvmcont)(&quot;freeze_slow starting_index: %d oops: %d&quot;, starting_index, oopmap-&gt;num_oops());
1863 
1864     RegisterMapT *map = (RegisterMapT*) extra-&gt;map();
1865     ContinuationHelper::update_register_map_with_callee(map, *(extra-&gt;get_frame())); // restore saved link
1866 
1867     intptr_t* stub_vsp = NULL;
1868     intptr_t* stub_hsp = NULL;
1869     if (is_preempt) {
1870       stub_vsp = extra-&gt;stub_vsp();
1871 #ifndef PRODUCT
1872       stub_hsp = extra-&gt;stub_hsp();
1873 #endif
1874     }
1875 
1876     FreezeOopFn&lt;RegisterMapT, OopWriterT&gt; oopFn(extra-&gt;cont(), fp_oop_info, extra-&gt;get_frame(), vsp, hsp, map, starting_index, stub_vsp, stub_hsp);
1877     OopMapDo&lt;FreezeOopFn&lt;RegisterMapT, OopWriterT&gt;, FreezeOopFn&lt;RegisterMapT, OopWriterT&gt;, IncludeAllValues&gt; visitor(&amp;oopFn, &amp;oopFn);
1878     visitor.oops_do(extra-&gt;get_frame(), map, oopmap);
1879     assert (!map-&gt;include_argument_oops(), &quot;&quot;);
1880     assert (oopFn.count() == oopmap-&gt;num_oops(), &quot;&quot;);
1881     return oopFn.count();
1882   }
1883 
1884   // need to keep this lower down waiting for definition of SmallRegisterMap
1885   static int slow_path(address vsp, address oops, address addr, address hsp, int idx, FpOopInfo* fp_oop_info, Extra *extra);
1886   static int slow_path_preempt(address vsp, address oops, address addr, address hsp, int idx, FpOopInfo* fp_oop_info, Extra *extra, bool is_preempt);
1887 };
1888 
1889 class ThawCompiledOops {
1890 public:
1891   class Extra {
1892   private:
1893     frame* _f;
1894     ContMirror* _cont;
1895     address* _map;
1896     int _starting_index;
1897   public:
1898     Extra(frame* f, ContMirror* cont, address* map, int starting_index) : _f(f), _cont(cont), _map(map), _starting_index(starting_index) {}
1899 
1900     frame* get_frame() { return _f; }
1901     ContMirror* cont() { return _cont; }
1902     address* map() { return _map; }
1903     int starting_index() { return _starting_index; }
1904   };
1905 
1906 
1907   template &lt;typename RegisterMapT&gt;
1908   static int thaw_slow(address vsp, address oops, address link_addr, Extra* extra) {
1909     frame* f = extra-&gt;get_frame();
1910     RegisterMapT* map = (RegisterMapT*) extra-&gt;map();
1911 
1912     DEBUG_ONLY(intptr_t* tmp_fp = f-&gt;fp();) // TODO PD
1913 
1914     log_develop_trace(jvmcont)(&quot;thaw_slow starting_index: %d oops: %d&quot;, extra-&gt;starting_index(), f-&gt;oop_map()-&gt;num_oops());
1915     // Thawing oops overwrite the link in the callee if rbp contained an oop (only possible if we&#39;re compiled).
1916     // This only matters when we&#39;re the top frame, as that&#39;s the value that will be restored into rbp when we jump to continue.
1917     ContinuationHelper::update_register_map(map, (intptr_t **) link_addr);
1918 
1919     ThawOopFn&lt;RegisterMapT&gt; oopFn(extra-&gt;cont(), f, extra-&gt;starting_index(), vsp, map);
1920     OopMapDo&lt;ThawOopFn&lt;RegisterMapT&gt;, ThawOopFn&lt;RegisterMapT&gt;, IncludeAllValues&gt; visitor(&amp;oopFn, &amp;oopFn);
1921     visitor.oops_do(f, map, f-&gt;oop_map());
1922 
1923     DEBUG_ONLY(if (tmp_fp != f-&gt;fp()) log_develop_trace(jvmcont)(&quot;WHOA link has changed (thaw) f.fp: &quot; INTPTR_FORMAT &quot; link: &quot; INTPTR_FORMAT, p2i(f-&gt;fp()), p2i(tmp_fp));) // TODO PD
1924 
1925     int cnt = oopFn.count();
1926     return cnt;
1927   }
1928 
1929   static int slow_path(address vsp, address oops, address link_addr, Extra* extra);
1930   static int slow_path_preempt(address vsp, address oops, address link_addr, Extra* extra);
1931 };
1932 
1933 /*
1934  * This class is mainly responsible for the work that is required to make sure that nmethods that
1935  * are referenced from a Continuation stack are kept alive.
1936  *
1937  * While freezing, for each nmethod a keepalive array is allocated. It contains elements for all the
1938  * oops that are either immediates or in the oop section in the nmethod (basically all that would be
1939  * published to the closure while running nm-&gt;oops_do().).
1940  *
1941  * The keepalive array is than strongly linked from the oop array in the Continuation, a weak reference
1942  * is kept in the nmethod -&gt; the keepalive array.
1943  *
1944  * Some GCs (currently only G1) have code that considers the weak reference to the keepalive array a
1945  * strong reference while this nmethod is on the stack. This is true while we are freezing, it helps
1946  * performance because we don&#39;t need to allocate and keep oops to this objects in a Handle for such GCs.
1947  * As soon as they are linked into the nmethod we know the object will stay alive.
1948  */
1949 template &lt;typename ConfigT&gt;
1950 class CompiledMethodKeepalive {
1951 private:
1952   typedef typename ConfigT::OopT OopT;
1953   typedef CompiledMethodKeepalive&lt;ConfigT&gt; SelfT;
1954   typedef typename ConfigT::KeepaliveObjectT KeepaliveObjectT;
1955 
1956   typename KeepaliveObjectT::TypeT _keepalive;
1957   CompiledMethod* _method;
1958   SelfT* _parent;
1959   JavaThread* _thread;
1960   int _nr_oops;
1961   bool _required;
1962 
1963   void store_keepalive(Thread* thread, oop* keepalive) { _keepalive = KeepaliveObjectT::make_keepalive(thread, keepalive); }
1964   oop read_keepalive() { return KeepaliveObjectT::read_keepalive(_keepalive); }
1965 
1966 public:
1967   CompiledMethodKeepalive(CompiledMethod* cm, SelfT* parent, JavaThread* thread) : _method(cm), _parent(NULL), _thread(thread), _nr_oops(0), _required(false) {
1968     oop* keepalive = cm-&gt;get_keepalive();
1969     if (keepalive != NULL) {
1970    //   log_info(jvmcont)(&quot;keepalive is %p (%p) for nm %p&quot;, keepalive, (void *) *keepalive, cm);
1971       WeakHandle wh = WeakHandle::from_raw(keepalive);
1972       oop resolved = wh.resolve();
1973       if (resolved != NULL) {
1974         //log_info(jvmcont)(&quot;found keepalive %p (%p)&quot;, keepalive, (void *) resolved);
1975         store_keepalive(thread, keepalive);
1976         return;
1977       }
1978 
1979       //log_info(jvmcont)(&quot;trying to clear stale keepalive for %p&quot;, _method);
1980       if (cm-&gt;clear_keepalive(keepalive)) {
1981         //log_info(jvmcont)(&quot;keepalive cleared for %p&quot;, _method);
1982         thread-&gt;keepalive_cleanup()-&gt;append(wh);
1983         // put on a list for cleanup in a safepoint
1984       }
1985     }
1986   //  log_info(jvmcont)(&quot;keepalive is %p for nm %p&quot;, keepalive, cm);
1987 
1988     nmethod* nm = cm-&gt;as_nmethod_or_null();
1989     if (nm != NULL) {
1990       _nr_oops = nm-&gt;nr_oops();
1991       //log_info(jvmcont)(&quot;need keepalive for %d oops&quot;, _nr_oops);
1992       _required = true;
1993       _parent = parent;
1994     }
1995   }
1996 
1997   void write_at(ContMirror&amp; mirror, int index) {
1998     //assert(_keepalive != NULL, &quot;&quot;);
1999     //log_develop_info(jvmcont)(&quot;writing mirror at %d\n&quot;, index);
2000     mirror.add_oop&lt;typename ConfigT::OopWriterT&gt;(read_keepalive(), index);
2001     //*(hsp + index)
2002   }
2003 
2004   void persist_oops() {
2005     if (!_required) {
2006       // Even though our first one might have said require, someone else might have written a new entry before we wrote our own.
2007       return;
2008     }
2009 
2010     nmethod* nm = _method-&gt;as_nmethod_or_null();
2011     if (nm != NULL) {
2012       //assert(_keepalive != NULL &amp;&amp; read_keepalive() != NULL, &quot;&quot;);
2013       PersistOops&lt;OopT&gt; persist(_nr_oops, (objArrayOop) read_keepalive());
2014       nm-&gt;oops_do(&amp;persist);
2015       //log_info(jvmcont)(&quot;oops persisted&quot;);
2016     }
2017   }
2018 
2019   void set_handle(Handle keepalive) {
2020     WeakHandle wh = WeakHandle(Universe::vm_weak(), keepalive);
2021     oop* result = _method-&gt;set_keepalive(wh.raw());
2022 
2023     if (result != NULL) {
2024       store_keepalive(_thread, result);
2025       // someone else managed to do it before us, destroy the weak
2026       _required = false;
2027       wh.release(Universe::vm_weak());
2028     } else {
2029       store_keepalive(_thread, wh.raw());
2030       //log_info(jvmcont)(&quot;Winning cas for %p (%p -&gt; %p (%p))&quot;, _method, result, wh.raw(), (void *) wh.resolve());
2031     }
2032   }
2033 
2034   SelfT* parent() { return _parent; }
2035   bool required() const { return _required; }
2036   int nr_oops() const { return _nr_oops; }
2037 
2038 };
2039 
2040 template &lt;typename FKind&gt;
2041 class FreezeFrame {
2042 };
2043 
2044 template &lt;&gt;
2045 class FreezeFrame&lt;Interpreted&gt; {
2046   public:
2047   template &lt;bool top, bool bottom, bool IsKeepalive, typename FreezeT&gt;
2048   static hframe dispatch(FreezeT&amp; self, const frame&amp; f, const hframe&amp; caller, int fsize, int argsize, int oops, InterpreterOopMap* mask, typename FreezeT::CompiledMethodKeepaliveT* ignore) {
2049     return self.template freeze_interpreted_frame&lt;top, bottom&gt;(f, caller, fsize, oops, mask);
2050   }
2051 };
2052 
2053 template &lt;&gt;
2054 class FreezeFrame&lt;Compiled&gt; {
2055   public:
2056   template &lt;bool top, bool bottom, bool IsKeepalive, typename FreezeT&gt;
2057   static hframe dispatch(FreezeT&amp; self, const frame&amp; f, const hframe&amp; caller, int fsize, int argsize, int oops, FreezeFnT freeze_stub, typename FreezeT::CompiledMethodKeepaliveT* kd) {
2058     return self.template freeze_compiled_frame&lt;Compiled, top, bottom, IsKeepalive&gt;(f, caller, fsize, argsize, oops, freeze_stub, kd);
2059   }
2060 };
2061 
2062 class FreezeOopVerify {
2063 public:
2064   template &lt;typename T&gt;
2065     static void verify(T* t);
2066 };
2067 
2068 template &lt;&gt;
2069 void FreezeOopVerify::verify&lt;narrowOop&gt;(narrowOop* addr) {
2070   oop obj = NativeAccess&lt;&gt;::oop_load(addr);
2071   assert(oopDesc::is_oop_or_null(obj), &quot;&quot;);
2072 }
2073 
2074 template&lt;&gt;
2075 void FreezeOopVerify::verify&lt;oop&gt;(oop* addr) {
2076   oop obj = NativeAccess&lt;&gt;::oop_load(addr);
2077   assert(oopDesc::is_oop_or_null(obj), &quot;&quot;);
2078 }
2079 
2080 static void verify_cookie(intptr_t *addr) {
2081   ContinuationEntry* entry = (ContinuationEntry*)addr;
2082   assert(entry-&gt;cookie == 0x1234, &quot;&quot;);
2083 }
2084 
2085 template &lt;typename ConfigT, op_mode mode&gt;
2086 class Freeze {
2087   typedef typename Conditional&lt;mode == mode_slow, RegisterMap, SmallRegisterMap&gt;::type RegisterMapT; // we need a full map to store the register dump for a safepoint stub during preemtion
2088   typedef Freeze&lt;ConfigT, mode&gt; SelfT;
2089   typedef CompiledMethodKeepalive&lt;ConfigT&gt; CompiledMethodKeepaliveT;
2090 
2091 private:
2092   JavaThread* _thread; // NULL when squashing chunks
2093   ContMirror&amp; _cont;
2094   intptr_t *_bottom_address;
2095 
2096   RegisterMapT _map;
2097 
2098   FpOopInfo _fp_oop_info;
2099   CompiledMethodKeepaliveT* _keepalive;
2100 
2101   bool _preempt;
2102   int _oops;
2103   int _size; // total size of all frames plus metadata. keeps track of offset where a frame should be written and how many bytes we need to allocate.
2104   int _frames;
2105   int _cgrind_interpreted_frames;
2106 
2107   bool  _safepoint_stub_caller;
2108   frame _safepoint_stub;
2109   hframe _safepoint_stub_h;
2110 
2111 #ifndef PRODUCT
2112   intptr_t* _safepoint_stub_hsp;
2113 #endif
2114 
2115   template&lt;typename FKind&gt; static inline frame sender(const frame&amp; f);
2116   template &lt;typename FKind, bool top, bool bottom&gt; inline void patch_pd(const frame&amp; f, hframe&amp; callee, const hframe&amp; caller);
2117   template &lt;bool bottom&gt; inline void align(const hframe&amp; caller);
2118   inline void relativize_interpreted_frame_metadata(const frame&amp; f, intptr_t* vsp, const hframe&amp; hf);
2119   template&lt;bool cont_empty&gt; hframe new_bottom_hframe(int sp, int ref_sp, address pc, bool interpreted);
2120   template&lt;typename FKind&gt; hframe new_hframe(const frame&amp; f, intptr_t* vsp, const hframe&amp; caller, int fsize, int num_oops);
2121   static frame chunk_start_frame_pd(oop chunk, intptr_t* sp);
2122   inline intptr_t* align_bottom(intptr_t* vsp, int argsize);
2123 
2124 public:
2125 
2126   Freeze(JavaThread* thread, ContMirror&amp; mirror) :
2127     _thread(thread), _cont(mirror),
2128     _map(thread, false, false, false), _fp_oop_info(),
2129     _preempt(false),
2130     _safepoint_stub(false), _safepoint_stub_h(false) { // don&#39;t intitialize
2131 
2132     // _cont.read_entry(); // even when retrying, because deopt can change entryPC; see Continuation::get_continuation_entry_pc_for_sender
2133     _cont.read_minimal();
2134 
2135     assert (thread-&gt;cont_entry()-&gt;entry_sp() == _cont.entrySP(), &quot;metadata sp: %p cont.entrySP(): %p &amp;metadata: %p&quot;, thread-&gt;cont_entry()-&gt;entry_sp(), _cont.entrySP(), thread-&gt;cont_entry()); // 111111
2136 
2137     int argsize = bottom_argsize();
2138     _bottom_address = _cont.entrySP() - argsize;
2139     verify_cookie(_cont.entrySP());
2140 
2141     assert (!Interpreter::contains(_cont.entryPC()), &quot;&quot;);
2142     assert (mode != mode_fast || !Interpreter::contains(_cont.entryPC()), &quot;&quot;);
2143     // if (mode != mode_fast &amp;&amp; Interpreter::contains(_cont.entryPC())) {
2144     //   _bottom_address -= argsize; // we subtract again; see Thaw::align
2145     // }
2146   #ifdef _LP64
2147     if (((intptr_t)_bottom_address &amp; 0xf) != 0) {
2148       _bottom_address--;
2149     }
2150     assert((intptr_t)_bottom_address % 16 == 0, &quot;&quot;);
2151   #endif
2152 
2153     log_develop_trace(jvmcont)(&quot;bottom_address: &quot; INTPTR_FORMAT &quot; entrySP: &quot; INTPTR_FORMAT &quot; argsize: %ld&quot;, p2i(_bottom_address), p2i(_cont.entrySP()), (_cont.entrySP() - _bottom_address) &lt;&lt; LogBytesPerWord);
2154     assert (_bottom_address != NULL &amp;&amp; _bottom_address &lt;= _cont.entrySP(), &quot;&quot;);
2155 
2156     _map.set_include_argument_oops(false);
2157   }
2158 
2159   Freeze(oop chunk, ContMirror&amp; mirror) :
2160     _thread(NULL), _cont(mirror),
2161     _map(NULL, false, false, false), _fp_oop_info(),
2162     _preempt(false),
2163     _safepoint_stub(false), _safepoint_stub_h(false) { // don&#39;t intitialize
2164 
2165     assert (mode == mode_fast, &quot;&quot;);
2166 
2167     // this is only uaed to terminate the frame loop.
2168     _bottom_address = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::size(chunk);
2169     int argsize = jdk_internal_misc_StackChunk::argsize(chunk);
2170     _bottom_address += LIKELY(argsize == 0) ? frame::sender_sp_offset // We add 2 because the chunk does not include the bottommost 2 words (return pc and link)
2171                                             : -argsize;
2172 
2173     log_develop_trace(jvmcont)(&quot;freeze chunk bottom_address: &quot; INTPTR_FORMAT &quot; argsize: %d&quot;, p2i(_bottom_address), argsize);
2174     if (log_develop_is_enabled(Trace, jvmcont)) print_chunk(chunk, _cont.mirror(), true);
2175   }
2176 
2177   void init_rest() { // we want to postpone some initialization after chunk handling
2178     _fp_oop_info.init();
2179     _keepalive = NULL;
2180     _oops = 0;
2181     _size = 0;
2182     _frames = 0;
2183     _cgrind_interpreted_frames = 0;
2184   }
2185 
2186   int nr_oops() const   { return _oops; }
2187   int nr_bytes() const  { return _size; }
2188   int nr_frames() const { return _frames; }
2189 
2190   Thread* cur_thread() {
2191     assert (_preempt || _thread == Thread::current(), &quot;&quot;); // could be VM thread in force preempt
2192     return mode == mode_fast ? _thread : Thread::current();
2193   }
2194 
2195   void verify() {
2196     if (_cont.refStack() == NULL) {
2197       return;
2198     }
2199     int len = _cont.refStack()-&gt;length();
2200     for (int i = 0; i &lt; len; ++i) {
2201       typename ConfigT::OopT* addr = _cont.refStack()-&gt;template obj_at_address&lt;typename ConfigT::OopT&gt;(i);
2202       FreezeOopVerify::verify(addr);
2203     }
2204   }
2205 
2206   freeze_result freeze(intptr_t* sp, bool chunk_available) {
2207     assert (!chunk_available || (USE_CHUNKS &amp;&amp; mode == mode_fast), &quot;&quot;);
2208     if (mode == mode_fast &amp;&amp; USE_CHUNKS) {
2209       if (freeze_chunk(sp, chunk_available)) {
2210         return freeze_ok;
2211       }
2212       if (!_thread-&gt;cont_fastpath()) {
2213         return freeze_retry_slow;  // things have deoptimized
2214       }
2215       EventContinuationFreezeOld e;
2216       if (e.should_commit()) {
2217         e.set_id(cast_from_oop&lt;u8&gt;(_cont.mirror()));
2218         e.commit();
2219       }
2220     }
2221     return freeze_no_chunk();
2222   }
2223 
2224   freeze_result freeze_preempt() {
2225     _preempt = true;
2226     // if (!is_safe_to_preempt(_thread)) {
2227     //   return freeze_pinned_native;
2228     // }
2229     return freeze_no_chunk();
2230   }
2231 
2232   freeze_result freeze_no_chunk() {
2233     log_develop_trace(jvmcont)(&quot;no young freeze mode: %d #&quot; INTPTR_FORMAT, mode, _cont.hash());
2234 
2235     assert (mode == mode_slow || !_preempt, &quot;&quot;);
2236     assert (_thread-&gt;thread_state() == _thread_in_vm || _thread-&gt;thread_state() == _thread_blocked, &quot;&quot;);
2237 
2238     init_rest();
2239     _cont.read_rest();
2240 
2241     HandleMark hm(cur_thread());
2242 
2243     // tty-&gt;print_cr(&quot;&gt;&gt;&gt; freeze mode: %d&quot;, mode);
2244 
2245     // assert (map.update_map(), &quot;RegisterMap not set to update&quot;);
2246     assert (!_map.include_argument_oops(), &quot;should be&quot;);
2247     frame f = freeze_start_frame();
2248     hframe caller;
2249     freeze_result res = freeze&lt;true&gt;(f, caller, 0);
2250 
2251     if (res == freeze_ok) {
2252       _cont.set_flag(FLAG_SAFEPOINT_YIELD, _preempt);
2253       _cont.write(); // commit the freeze
2254       DEBUG_ONLY(verify();)
2255     }
2256     return res;
2257   }
2258 
2259   inline int bottom_argsize() {
2260     int argsize = _cont.argsize(); // in words
2261     log_develop_trace(jvmcont)(&quot;bottom_argsize: %d&quot;, argsize);
2262     assert (argsize &gt;= 0, &quot;argsize: %d&quot;, argsize);
2263     return argsize;
2264   }
2265 
2266   bool is_chunk_available(intptr_t* top_sp) {
2267     if (mode != mode_fast || !USE_CHUNKS) return false;
2268 
2269     oop chunk = _cont.tail();
2270     // TODO The second conjunct means we don&#39;t squash old chunks, but let them be (Rickard&#39;s idea)
2271     if (chunk == (oop)NULL || requires_barriers(chunk))
2272       return false;
2273 
2274     assert (StubRoutines::cont_doYield_stub()-&gt;frame_size() == frame_metadata, &quot;&quot;);
2275     intptr_t* const top = top_sp + frame_metadata;
2276     const int argsize = bottom_argsize();
2277     intptr_t* const bottom = align_bottom(_cont.entrySP(), argsize);
2278     int size = bottom - top; // in words
2279     if (UNLIKELY(argsize != 0)) {
2280       size += frame::sender_sp_offset;
2281     }
2282     int sp = jdk_internal_misc_StackChunk::sp(chunk);
2283     if (sp &lt; jdk_internal_misc_StackChunk::size(chunk)) {
2284       size -= argsize;
2285     }
2286     assert (size &gt; 0, &quot;&quot;);
2287 
2288     bool available = jdk_internal_misc_StackChunk::sp(chunk) - frame::sender_sp_offset &gt;= size;
2289     log_develop_trace(jvmcont)(&quot;is_chunk_available available: %d size: %d argsize: %d top: &quot; INTPTR_FORMAT &quot; bottom: &quot; INTPTR_FORMAT, available, argsize, size, p2i(top), p2i(bottom));
2290     return available;
2291   }
2292 
2293   bool freeze_chunk(intptr_t* top_sp, bool chunk_available) {
2294   #ifdef CALLGRIND_START_INSTRUMENTATION
2295     if (_frames &gt; 0 &amp;&amp; callgrind_counter == 1) {
2296       callgrind_counter = 2;
2297       tty-&gt;print_cr(&quot;Starting callgrind instrumentation&quot;);
2298       CALLGRIND_START_INSTRUMENTATION;
2299     }
2300   #endif
2301 
2302     assert (USE_CHUNKS, &quot;&quot;);
2303     assert (_thread != NULL, &quot;&quot;);
2304     assert(_cont.chunk_invariant(), &quot;&quot;);
2305 
2306     // assert(verify_continuation&lt;111&gt;(_cont.mirror()), &quot;&quot;);
2307 
2308     log_develop_trace(jvmcont)(&quot;freeze_chunk&quot;);
2309     assert (mode == mode_fast, &quot;&quot;);
2310     assert (!Interpreter::contains(_cont.entryPC()), &quot;&quot;);
2311 
2312     oop chunk = _cont.tail();
2313 
2314     assert (StubRoutines::cont_doYield_stub()-&gt;frame_size() == frame_metadata, &quot;&quot;);
2315     intptr_t* const top = top_sp + frame_metadata;
2316 
2317     const int argsize = bottom_argsize();
2318     intptr_t* const bottom = align_bottom(_cont.entrySP(), argsize);
2319     int size = bottom - top; // in words
2320     log_develop_trace(jvmcont)(&quot;freeze_chunk size: %d argsize: %d top: &quot; INTPTR_FORMAT &quot; bottom: &quot; INTPTR_FORMAT, size, argsize, p2i(top), p2i(bottom));
2321     assert (size &gt; 0, &quot;&quot;);
2322 
2323     int sp;
2324     bool allocated;
2325     if (LIKELY(chunk_available)) {
2326       assert (chunk == _cont.tail() &amp;&amp; is_chunk_available(top_sp), &quot;&quot;);
2327       allocated = false;
2328       sp = jdk_internal_misc_StackChunk::sp(chunk);
2329       // TODO The the following is commented means we don&#39;t squash old chunks, but let them be (Rickard&#39;s idea)
2330       // if (requires_barriers(chunk)) {
2331       //   log_develop_trace(jvmcont)(&quot;Freeze chunk: found old chunk&quot;);
2332       //   assert(_cont.chunk_invariant(), &quot;&quot;);
2333       //   return false;
2334       // }
2335 
2336       if (sp &lt; jdk_internal_misc_StackChunk::size(chunk)) {
2337         sp += argsize;
2338         assert (sp &lt;= jdk_internal_misc_StackChunk::size(chunk), &quot;&quot;);
2339       }
2340       // ContMirror::reset_chunk_counters(chunk);
2341     } else {
2342       assert (!is_chunk_available(top_sp), &quot;&quot;);
2343       assert (_thread-&gt;thread_state() == _thread_in_vm, &quot;&quot;);
2344       assert (_thread-&gt;cont_fastpath(), &quot;&quot;);
2345 
2346       int size1 = size;
2347       if (UNLIKELY(argsize != 0)) {
2348         size1 += frame::sender_sp_offset; // b/c when argsize &gt; 0, we don&#39;t reach the caller&#39;s metadata
2349       }
2350       chunk = allocate_chunk(size1);
2351       if (!_thread-&gt;cont_fastpath()) {
2352         return false;
2353       }
2354 
2355       allocated = true;
2356       sp = jdk_internal_misc_StackChunk::sp(chunk);
2357 
2358       assert (jdk_internal_misc_StackChunk::parent(chunk) == (oop)NULL || ContMirror::is_stack_chunk(jdk_internal_misc_StackChunk::parent(chunk)), &quot;&quot;);
2359       // in a fresh chunk, we freeze *with* the bottom-most frame&#39;s stack arguments.
2360       // They&#39;ll then be stored twice: in the chunk and in the parent
2361 
2362       if (requires_barriers(chunk)) {
2363         log_develop_trace(jvmcont)(&quot;Young chunk: allocated old! size: %d&quot;, size1);
2364         tty-&gt;print_cr(&quot;Young chunk: allocated old! size: %d&quot;, size1);
2365         assert(_cont.chunk_invariant(), &quot;&quot;);
2366         assert (false, &quot;ewhkjefdhksadf&quot;);
2367         return false;
2368       }
2369       _cont.set_tail(chunk);
2370       java_lang_Continuation::set_tail(_cont.mirror(), chunk);
2371     }
2372 
2373     assert (chunk != NULL, &quot;&quot;);
2374 
2375     NoSafepointVerifier nsv;
2376     assert (ContMirror::is_stack_chunk(chunk), &quot;&quot;);
2377     assert (!requires_barriers(chunk), &quot;&quot;);
2378     assert (chunk == _cont.tail(), &quot;&quot;);
2379     assert (chunk == java_lang_Continuation::tail(_cont.mirror()), &quot;&quot;);
2380     assert (!jdk_internal_misc_StackChunk::gc_mode(chunk), &quot;&quot;);
2381     assert (sp &lt;= jdk_internal_misc_StackChunk::size(chunk) + frame::sender_sp_offset, &quot;sp: %d chunk size: %d size: %d argsize: %d allocated: %d&quot;, sp, jdk_internal_misc_StackChunk::size(chunk), size, argsize, allocated);
2382 
2383     _cont.add_size((size - argsize) &lt;&lt; LogBytesPerWord);
2384     assert (_cont.is_flag(FLAG_LAST_FRAME_INTERPRETED) == Interpreter::contains(_cont.pc()), &quot;&quot;);
2385 
2386     if (jdk_internal_misc_StackChunk::is_parent_null&lt;typename ConfigT::OopT&gt;(chunk) &amp;&amp; _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED)) {
2387       _cont.add_size(SP_WIGGLE &lt;&lt; LogBytesPerWord);
2388     }
2389 
2390     _cont.set_flag(FLAG_SAFEPOINT_YIELD, false);
2391     _cont.write_minimal();
2392 
2393     intptr_t* const bottom_sp = bottom - argsize;
2394     const address bottom_ret_pc = *(address*)(bottom_sp - SENDER_SP_RET_ADDRESS_OFFSET); // save before patching
2395 
2396     if (UNLIKELY(argsize != 0)) { // patch pc + fp
2397       // we&#39;re patching the thread stack, not the chunk, as it&#39;s hopefully still hot in the cache
2398       log_develop_trace(jvmcont)(&quot;patching bottom sp: &quot; INTPTR_FORMAT, p2i(bottom_sp));
2399       assert (bottom_sp == _bottom_address, &quot;&quot;);
2400 
2401       size += frame::sender_sp_offset; // b/c when argsize &gt; 0, we don&#39;t reach the caller&#39;s metadata
2402     }
2403 
2404     // copy; no need to patch because of how we handle return address and link
2405     log_develop_trace(jvmcont)(&quot;freeze_chunk start: chunk &quot; INTPTR_FORMAT &quot; size: %d orig sp: %d argsize: %d&quot;, p2i((oopDesc*)chunk), jdk_internal_misc_StackChunk::size(chunk), sp, argsize);
2406     assert (sp &gt;= size, &quot;&quot;);
2407     sp -= size;
2408     assert (size == (jdk_internal_misc_StackChunk::sp(chunk) - sp), &quot;size: %d used chunk size: %d&quot;, size, (jdk_internal_misc_StackChunk::sp(chunk) - sp));
2409     jdk_internal_misc_StackChunk::set_sp(chunk, sp);
2410     jdk_internal_misc_StackChunk::set_argsize(chunk, argsize);
2411     jdk_internal_misc_StackChunk::set_pc(chunk, *(address*)(top - SENDER_SP_RET_ADDRESS_OFFSET));
2412 
2413     // we copy the top frame&#39;s return address and link, but not the bottom&#39;s
2414     intptr_t* chunk_top = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + sp;
2415     log_develop_trace(jvmcont)(&quot;freeze_chunk start: &quot; INTPTR_FORMAT &quot; sp: %d chunk_top: &quot; INTPTR_FORMAT, p2i(InstanceStackChunkKlass::start_of_stack(chunk)), sp, p2i(chunk_top));
2416     intptr_t* from = top       - frame::sender_sp_offset;
2417     intptr_t* to   = chunk_top - frame::sender_sp_offset;
2418     copy_to_chunk(from, to, size, chunk);
2419 
2420     log_develop_trace(jvmcont)(&quot;Young chunk success&quot;);
2421     if (log_develop_is_enabled(Debug, jvmcont)) print_chunk(chunk, _cont.mirror(), true);
2422 
2423     log_develop_trace(jvmcont)(&quot;FREEZE CHUNK #&quot; INTPTR_FORMAT, _cont.hash());
2424     assert (!jdk_internal_misc_StackChunk::gc_mode(chunk), &quot;&quot;);
2425     assert (_cont.chunk_invariant(), &quot;&quot;);
2426     assert (Continuation::debug_verify_stack_chunk(chunk), &quot;&quot;);
2427 
2428   #if CONT_JFR
2429     EventContinuationFreezeYoung e;
2430     if (e.should_commit()) {
2431       e.set_id(cast_from_oop&lt;u8&gt;(chunk));
2432       e.set_allocate(allocated);
2433       e.set_size(size &lt;&lt; LogBytesPerWord);
2434       e.commit();
2435     }
2436   #endif
2437 
2438     // assert(verify_continuation&lt;222&gt;(_cont.mirror()), &quot;&quot;);
2439 
2440     return true;
2441   }
2442 
2443   oop allocate_chunk(int size) {
2444     log_develop_trace(jvmcont)(&quot;allocate_chunk allocating new chunk&quot;);
2445     oop chunk = _cont.allocate_stack_chunk(size);
2446     if (chunk == NULL) { // OOM
2447       guarantee(false, &quot;Unhandled OOM&quot;);
2448     }
2449     assert (jdk_internal_misc_StackChunk::size(chunk) == size, &quot;&quot;);
2450     assert (chunk-&gt;size() &gt;= size, &quot;chunk-&gt;size(): %d size: %d&quot;, chunk-&gt;size(), size);
2451     assert ((intptr_t)InstanceStackChunkKlass::start_of_stack(chunk) % 8 == 0, &quot;&quot;);
2452 
2453     oop chunk0 = _cont.tail();
2454     if (chunk0 != (oop)NULL &amp;&amp; ContMirror::is_empty_chunk(chunk0)) {
2455       // chunk0 = jdk_internal_misc_StackChunk::is_parent_null&lt;typename ConfigT::OopT&gt;(chunk0) ? (oop)NULL : jdk_internal_misc_StackChunk::parent(chunk0);
2456       chunk0 = jdk_internal_misc_StackChunk::parent(chunk0);
2457       assert (chunk0 == (oop)NULL || !ContMirror::is_empty_chunk(chunk0), &quot;&quot;);
2458     }
2459 
2460     int sp = size + frame::sender_sp_offset;
2461     jdk_internal_misc_StackChunk::set_sp(chunk, sp);
2462     jdk_internal_misc_StackChunk::set_pc(chunk, NULL);
2463     jdk_internal_misc_StackChunk::set_argsize(chunk, 0); // TODO PERF unnecessary?
2464     jdk_internal_misc_StackChunk::set_gc_mode(chunk, false);
2465     jdk_internal_misc_StackChunk::set_parent_raw&lt;typename ConfigT::OopT&gt;(chunk, chunk0); // field is uninitialized
2466     jdk_internal_misc_StackChunk::set_cont_raw&lt;typename ConfigT::OopT&gt;(chunk, _cont.mirror());
2467     ContMirror::reset_chunk_counters(chunk);
2468 
2469     // TODO Erik says: promote young chunks quickly
2470     chunk-&gt;set_mark(chunk-&gt;mark().set_age(15));
2471     return chunk;
2472   }
2473 
2474   void copy_to_chunk(intptr_t* from, intptr_t* to, int size, oop chunk) {
2475     log_develop_trace(jvmcont)(&quot;Copying from v: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d bytes)&quot;, p2i(from), p2i(from + size), size &lt;&lt; LogBytesPerWord);
2476     log_develop_trace(jvmcont)(&quot;Copying to h: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d bytes)&quot;, p2i(to), p2i(to + size), size &lt;&lt; LogBytesPerWord);
2477 
2478     copy_from_stack(from, to, size);
2479 
2480     assert (to &gt;= (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk), &quot;to: &quot; INTPTR_FORMAT &quot; start: &quot; INTPTR_FORMAT, p2i(to), p2i(InstanceStackChunkKlass::start_of_stack(chunk)));
2481     assert (to + size &lt;= (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::size(chunk),
2482       &quot;to + size: &quot; INTPTR_FORMAT &quot; end: &quot; INTPTR_FORMAT, p2i(to + size), p2i((intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::size(chunk)));
2483   }
2484 
2485 #ifdef ASSERT
2486   int top_stack_argsize() {
2487     assert(_cont.chunk_invariant(), &quot;&quot;);
2488     address pc = NULL;
2489     for (oop chunk = _cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
2490       if (ContMirror::is_empty_chunk(chunk)) continue;
2491       intptr_t* sp = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::sp(chunk);
2492       pc = *(address*)(sp - 1);
2493       break;
2494     }
2495     if (pc == NULL) {
2496       pc = java_lang_Continuation::pc(_cont.mirror()); // may not have been read yet
2497       // assert ((pc == NULL) == _cont.is_empty(), &quot;(pc == NULL): %d _cont.is_empty(): %d&quot;, (pc == NULL), _cont.is_empty());
2498       assert (Interpreter::contains(pc) == _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED), &quot;&quot;);
2499       if (pc == NULL || _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED))
2500         return 0;
2501     }
2502     CodeBlob* cb = ContinuationCodeBlobLookup::find_blob(pc);
2503     int size_in_bytes = cb-&gt;as_compiled_method()-&gt;method()-&gt;num_stack_arg_slots() * VMRegImpl::stack_slot_size;
2504     return size_in_bytes &gt;&gt; LogBytesPerWord;
2505   }
2506 #endif
2507 
2508   void add_chunks_size() {
2509     int num_chunks = 0;
2510     int orig_frames = _frames;
2511 
2512     for (oop chunk = _cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
2513       num_chunks++;
2514       // if (jdk_internal_misc_StackChunk::numFrames(chunk) &lt; 0) {
2515       //   assert (!requires_barriers(chunk) &amp;&amp; !jdk_internal_misc_StackChunk::gc_mode(chunk), &quot;&quot;);
2516         Continuation::stack_chunk_iterate_stack&lt;BasicOopIterateClosure, false&gt;(chunk, (BasicOopIterateClosure*)NULL); // , false /* do_metadata */); // &amp;do_nothing_cl
2517       // }
2518 
2519       static const int metadata = 2;
2520       int size = (jdk_internal_misc_StackChunk::size(chunk) - jdk_internal_misc_StackChunk::sp(chunk) + metadata) &lt;&lt; LogBytesPerWord;
2521       int frames = jdk_internal_misc_StackChunk::numFrames(chunk);
2522       int oops = jdk_internal_misc_StackChunk::numOops(chunk);
2523 
2524       assert (size &gt;= 0, &quot;&quot;);
2525       assert (frames &gt;= 0, &quot;&quot;);
2526       assert (oops &gt;= 0, &quot;&quot;);
2527       assert ((size == 0) == ContMirror::is_empty_chunk(chunk), &quot;&quot;);
2528 
2529       _frames += frames;
2530       _oops   += oops + (frames * Compiled::extra_oops);
2531       _size   += size;
2532       _cont.sub_size(size); // it will be re-added later
2533       _cont.sub_num_frames(frames);
2534       log_develop_trace(jvmcont)(&quot;add_chunks_size frames: %d size: %d oops: %d&quot;, frames, size, oops);
2535       if (log_develop_is_enabled(Trace, jvmcont)) print_chunk(chunk, _cont.mirror(), false);
2536     }
2537     assert (USE_CHUNKS || num_chunks == 0, &quot;&quot;);
2538     EventContinuationSquash e;
2539     if (e.should_commit() &amp;&amp; USE_CHUNKS) {
2540       e.set_id(cast_from_oop&lt;u8&gt;(_cont.mirror()));
2541       e.set_numChunks(num_chunks);
2542       e.set_numFrames(_frames - orig_frames);
2543       e.commit();
2544     }
2545   }
2546 
2547   void squash_chunks() {
2548     NoSafepointVerifier nsv;
2549 
2550     oop chunk = _cont.tail();
2551     if (chunk == (oop)NULL)
2552       return;
2553 
2554     log_develop_trace(jvmcont)(&quot;PRE SQUASH: sp: %d ref_sp: %d&quot;, _cont.sp(), _cont.refSP());
2555     squash_chunks(chunk);
2556     log_develop_trace(jvmcont)(&quot;POST SQUASH: sp: %d ref_sp: %d&quot;, _cont.sp(), _cont.refSP());
2557   }
2558 
2559   void squash_chunks(oop chunk) {
2560     if (chunk == (oop)NULL) return;
2561 
2562     assert (USE_CHUNKS, &quot;&quot;);
2563     squash_chunks(jdk_internal_misc_StackChunk::parent(chunk)); // recursion
2564     jdk_internal_misc_StackChunk::set_parent(chunk, (oop)NULL);
2565     _cont.set_tail(NULL);
2566     if (ContMirror::is_empty_chunk(chunk))
2567       return;
2568 
2569     DEBUG_ONLY(int orig_sp = _cont.sp();)
2570     Freeze&lt;ConfigT, mode_fast&gt; frz(chunk, _cont);
2571     frz.squash_chunk(chunk);
2572     assert (((orig_sp - _cont.sp()) &lt;&lt; LogBytesPerElement) == frz.nr_bytes(), &quot;sp diff size: %d frz.nr_bytes(): %d&quot;, ((orig_sp - _cont.sp()) &lt;&lt; LogBytesPerElement), frz.nr_bytes());
2573     // undo add_chunks_size
2574     // _frames -= frz.nr_frames();
2575     _size -= frz.nr_bytes();
2576     // _oops -= frz.nr_oops();
2577   }
2578 
2579   void squash_chunk(oop chunk) {
2580     assert (USE_CHUNKS, &quot;&quot;);
2581     assert (!ContMirror::is_empty_chunk(chunk), &quot;&quot;);
2582 
2583     frame f = chunk_start_frame(chunk);
2584     f.get_cb();
2585     hframe caller;
2586     freeze_result res = freeze&lt;true&gt;(f, caller, 0);
2587     assert (res == freeze_ok, &quot;&quot;);
2588 
2589     jdk_internal_misc_StackChunk::set_sp(chunk, jdk_internal_misc_StackChunk::size(chunk) + frame::sender_sp_offset);
2590     ContMirror::reset_chunk_counters(chunk);
2591   }
2592 
2593   int remaining_in_chunk(oop chunk) {
2594     return jdk_internal_misc_StackChunk::size(chunk) - jdk_internal_misc_StackChunk::sp(chunk);
2595   }
2596 
2597   frame freeze_start_frame() {
2598     if (mode == mode_fast) {
2599        // Note: if the doYield stub does not have its own frame, we may need to consider deopt here, especially if yield is inlinable
2600       return freeze_start_frame_yield_stub(ContinuationHelper::last_frame(_thread)); // thread-&gt;last_frame();
2601     }
2602 
2603     frame f = _thread-&gt;last_frame();
2604     if (LIKELY(!_preempt)) {
2605       assert (StubRoutines::cont_doYield_stub()-&gt;contains(f.pc()), &quot;&quot;);
2606       return freeze_start_frame_yield_stub(f);
2607     } else {
2608       return freeze_start_frame_safepoint_stub(f);
2609     }
2610   }
2611 
2612   frame freeze_start_frame_yield_stub(frame f) {
2613     // log_develop_trace(jvmcont)(&quot;%s nop at freeze yield&quot;, nativePostCallNop_at(_fi-&gt;pc) != NULL ? &quot;has&quot; : &quot;no&quot;);
2614     assert(StubRoutines::cont_doYield_stub()-&gt;contains(f.pc()), &quot;must be&quot;);
2615     // assert (f.sp() + slow_get_cb(f)-&gt;frame_size() == _fi-&gt;sp, &quot;f.sp: %p size: %d fi-&gt;sp: %p&quot;, f.sp(), slow_get_cb(f)-&gt;frame_size(), _fi-&gt;sp);
2616   #ifdef ASSERT
2617     hframe::callee_info my_info = slow_link_address&lt;StubF&gt;(f);
2618   #endif
2619     f = sender&lt;StubF&gt;(f);
2620     assert (Frame::callee_link_address(f) == my_info, &quot;&quot;);
2621     // ContinuationHelper::update_register_map_with_callee(&amp;_map, f);
2622 
2623     // The following doesn&#39;t work because fi-&gt;fp can contain an oop, that a GC doesn&#39;t know about when walking.
2624     // frame::update_map_with_saved_link(&amp;map, (intptr_t **)&amp;fi-&gt;fp);
2625     // frame f = ContinuationHelper::to_frame(fi); // the yield frame
2626 
2627     // assert (f.pc() == _fi-&gt;pc, &quot;&quot;);
2628 
2629     // Log(jvmcont) logv; LogStream st(logv.debug()); f.print_on(st);
2630     if (log_develop_is_enabled(Debug, jvmcont)) f.print_on(tty);
2631 
2632     return f;
2633   }
2634 
2635   frame freeze_start_frame_safepoint_stub(frame f) {
2636     assert (mode == mode_slow, &quot;&quot;);
2637 
2638     f.set_fp(f.real_fp()); // Instead of this, maybe in ContMirror::set_last_frame always use the real_fp? // TODO PD
2639     if (Interpreter::contains(f.pc())) {
2640       ContinuationHelper::update_register_map&lt;Interpreted&gt;(&amp;_map, f);
2641       // f.set_sp(f.sp() - 1); // state pushed to the stack
2642     } else {
2643   #ifdef ASSERT
2644       if (!is_stub(f.cb())) { f.print_value_on(tty, JavaThread::current()); }
2645   #endif
2646       assert (is_stub(f.cb()), &quot;must be&quot;);
2647       assert (f.oop_map() != NULL, &quot;must be&quot;);
2648 
2649       if (Interpreter::contains(StubF::return_pc(f))) {
2650         log_develop_trace(jvmcont)(&quot;Safepoint stub in interpreter&quot;);
2651         f = sender&lt;StubF&gt;(f);
2652       } else {
2653         ContinuationHelper::update_register_map&lt;StubF&gt;(&amp;_map, f);
2654         f.oop_map()-&gt;update_register_map(&amp;f, (RegisterMap*)&amp;_map); // we have callee-save registers in this case
2655       }
2656     }
2657 
2658     // Log(jvmcont) logv; LogStream st(logv.debug()); f.print_on(st);
2659     if (log_develop_is_enabled(Debug, jvmcont)) f.print_on(tty);
2660 
2661     return f;
2662   }
2663 
2664   static frame chunk_start_frame(oop chunk) {
2665     intptr_t* sp = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::sp(chunk);
2666     return chunk_start_frame_pd(chunk, sp);
2667   }
2668 
2669   template&lt;bool top&gt;
2670   NOINLINE freeze_result freeze(const frame&amp; f, hframe&amp; caller, int callee_argsize) {
2671     assert (f.unextended_sp() &lt; _bottom_address - SP_WIGGLE, &quot;&quot;); // see recurse_freeze_java_frame
2672     assert (f.is_interpreted_frame() || ((top &amp;&amp; _preempt) == is_stub(f.cb())), &quot;&quot;);
2673     assert (mode != mode_fast || (!f.is_interpreted_frame() &amp;&amp; slow_get_cb(f)-&gt;is_compiled()), &quot;&quot;);
2674     assert (mode != mode_fast || !f.is_deoptimized_frame(), &quot;&quot;);
2675 
2676     // Dynamically branch on frame type
2677     if (mode == mode_fast || f.is_compiled_frame()) {
2678       if (UNLIKELY(mode != mode_fast &amp;&amp; f.oop_map() == NULL)) return freeze_pinned_native; // special native frame
2679 
2680       #ifdef CONT_DOUBLE_NOP
2681         if (UNLIKELY(!(mode == mode_fast &amp;&amp; !ContinuationHelper::cached_metadata&lt;mode&gt;(f).empty()) &amp;&amp;
2682              Compiled::is_owning_locks(_cont.thread(), &amp;_map, f))) return freeze_pinned_monitor;
2683       #else
2684         if (UNLIKELY(mode != mode_fast &amp;&amp; Compiled::is_owning_locks(_cont.thread(), &amp;_map, f))) return freeze_pinned_monitor;
2685       #endif
2686       assert (mode != mode_fast || !Compiled::is_owning_locks(_cont.thread(), &amp;_map, f), &quot;&quot;);
2687 
2688       // Keepalive info here...
2689       CompiledMethodKeepaliveT kd(f.cb()-&gt;as_compiled_method(), _keepalive, _thread);
2690       if (kd.required()) {
2691         _keepalive = &amp;kd;
2692         return recurse_freeze_compiled_frame&lt;top, true&gt;(f, caller, &amp;kd);
2693       }
2694 
2695       return recurse_freeze_compiled_frame&lt;top, false&gt;(f, caller, &amp;kd);
2696     } else if (f.is_interpreted_frame()) {
2697       assert ((mode == mode_slow &amp;&amp; _preempt &amp;&amp; top) || !f.interpreter_frame_method()-&gt;is_native(), &quot;&quot;);
2698       if (Interpreted::is_owning_locks(f)) return freeze_pinned_monitor;
2699       if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; top &amp;&amp; f.interpreter_frame_method()-&gt;is_native()) return freeze_pinned_native; // interpreter native entry
2700 
2701       return recurse_freeze_interpreted_frame&lt;top&gt;(f, caller, callee_argsize);
2702     } else if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; top &amp;&amp; is_stub(f.cb())) {
2703       return recurse_freeze_stub_frame(f, caller);
2704     } else {
2705       return freeze_pinned_native;
2706     }
2707   }
2708 
2709   template&lt;typename FKind, bool top, bool IsKeepalive&gt;
2710   inline freeze_result recurse_freeze_java_frame(const frame&amp; f, hframe&amp; caller, int fsize, int argsize, int oops, typename FKind::ExtraT extra, CompiledMethodKeepaliveT* kd) {
2711     assert (FKind::is_instance(f), &quot;&quot;);
2712     log_develop_trace(jvmcont)(&quot;recurse_freeze_java_frame fsize: %d oops: %d&quot;, fsize, oops);
2713 
2714     intptr_t* frame_bottom = FKind::interpreted ? FKind::frame_bottom(f) : (intptr_t*)((address)f.unextended_sp() + fsize);
2715     // sometimes an interpreted caller&#39;s sp extends a bit below entrySP, plus another word for possible alignment of compiled callee
2716     if (frame_bottom &gt;= _bottom_address - SP_WIGGLE) { // dynamic branch
2717       // senderf is the entry frame
2718       freeze_result result = finalize&lt;FKind&gt;(f, caller, &amp;argsize); // recursion end
2719       if (UNLIKELY(result != freeze_ok)) {
2720         return result;
2721       }
2722 
2723       freeze_java_frame&lt;FKind, top, true, IsKeepalive&gt;(f, caller, fsize, argsize, oops, extra, kd);
2724 
2725       if (log_develop_is_enabled(Trace, jvmcont)) {
2726         log_develop_trace(jvmcont)(&quot;bottom h-frame:&quot;);
2727         caller.print_on(tty); // caller is now the current hframe
2728       }
2729     } else {
2730       bool safepoint_stub_caller; // the use of _safepoint_stub_caller is not nice, but given preemption being performance non-critical, we don&#39;t want to add either a template or a regular parameter
2731       if (mode == mode_slow &amp;&amp; _preempt) {
2732         safepoint_stub_caller = _safepoint_stub_caller;
2733         _safepoint_stub_caller = false;
2734       }
2735 
2736       frame senderf = sender&lt;FKind&gt;(f); // f.sender_for_compiled_frame&lt;ContinuationCodeBlobLookup&gt;(&amp;map);
2737       assert (FKind::interpreted || senderf.sp() == senderf.unextended_sp(), &quot;&quot;);
2738       // assert (frame_bottom == senderf.unextended_sp(), &quot;frame_bottom: &quot; INTPTR_FORMAT &quot; senderf.unextended_sp(): &quot; INTPTR_FORMAT, p2i(frame_bottom), p2i(senderf.unextended_sp()));
2739       assert (Frame::callee_link_address(senderf) == slow_link_address&lt;FKind&gt;(f), &quot;&quot;);
2740       freeze_result result = freeze&lt;false&gt;(senderf, caller, argsize); // recursive call
2741       if (UNLIKELY(result != freeze_ok))
2742         return result;
2743 
2744       if (mode == mode_slow &amp;&amp; _preempt) _safepoint_stub_caller = safepoint_stub_caller; // restore _stub_caller
2745 
2746       freeze_java_frame&lt;FKind, top, false, IsKeepalive&gt;(f, caller, fsize, argsize, oops, extra, kd);
2747     }
2748 
2749     if (top) {
2750       finish(f, caller);
2751     }
2752     return freeze_ok;
2753   }
2754 
2755   void allocate_keepalive() {
2756     if (_keepalive == NULL) {
2757       return;
2758     }
2759 
2760     CompiledMethodKeepaliveT* current = _keepalive;
2761     while (current != NULL) {
2762       _cont.make_keepalive&lt;ConfigT&gt;(cur_thread(), current);
2763       current = current-&gt;parent();
2764     }
2765   }
2766 
2767   template&lt;typename FKind&gt; // the callee&#39;s type
2768   freeze_result finalize(const frame&amp; callee, hframe&amp; caller, int* argsize_out) {
2769   #ifdef CALLGRIND_START_INSTRUMENTATION
2770     if (_frames &gt; 0 &amp;&amp; _cgrind_interpreted_frames == 0 &amp;&amp; callgrind_counter == 1) {
2771       callgrind_counter = 2;
2772       tty-&gt;print_cr(&quot;Starting callgrind instrumentation&quot;);
2773       CALLGRIND_START_INSTRUMENTATION;
2774     }
2775   #endif
2776 
2777     assert (mode != mode_fast || !callee.is_interpreted_frame(), &quot;&quot;);
2778     assert (mode != mode_fast || _thread-&gt;cont_fastpath(), &quot;&quot;);
2779 
2780     log_develop_trace(jvmcont)(&quot;bottom: &quot; INTPTR_FORMAT &quot; count %d size: %d, num_oops: %d&quot;, p2i(_bottom_address), nr_frames(), nr_bytes(), nr_oops());
2781 
2782     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 15) return freeze_ok;)
2783 
2784     // DEBUG_ONLY(size_t orig_max_size = _cont.max_size());
2785     assert ((_cont.max_size() == 0) == _cont.is_empty(), &quot;&quot;);
2786 
2787     DEBUG_ONLY(int pre_chunk_size = _size; int pre_chunk_oops = _oops;)
2788 
2789     if (_thread != NULL) {
2790       add_chunks_size();
2791     }
2792 
2793     DEBUG_ONLY(int post_chunk_size = _size; int post_chunk_oops = _oops;)
2794     assert (_thread != NULL || post_chunk_size == pre_chunk_size, &quot;_post_chunk_size: %d _pre_chunk_size: %d&quot;, post_chunk_size, pre_chunk_size);
2795 
2796     assert (!_cont.is_empty() || _cont.max_size() == 0, &quot;_cont.max_size(): %lu _cont.is_empty(): %d&quot;, _cont.max_size(), _cont.is_empty());
2797 
2798     // java_lang_Continuation::set_tail(_cont.mirror(), _cont.tail()); -- doesn&#39;t seem to help
2799 
2800     bool allocated = _cont.allocate_stacks&lt;ConfigT&gt;(_size, _oops, _frames);
2801     assert (_thread != NULL || !allocated, &quot;&quot;); // chunk squashing does not allocate here
2802     if (_thread != NULL &amp;&amp; _thread-&gt;has_pending_exception()) {
2803       return freeze_exception; // TODO: restore max size ???
2804     }
2805 
2806 #ifndef NO_KEEPALIVE
2807     allocate_keepalive();
2808 #endif
2809 
2810     if (mode == mode_fast &amp;&amp; !_thread-&gt;cont_fastpath()) {
2811       if (ConfigT::_post_barrier) {
2812         _cont.zero_ref_stack_prefix();
2813       }
2814       return freeze_retry_slow; // things have deoptimized
2815     }
2816 
2817     DEBUG_ONLY(int pre_chunk_sp = 0; int post_chunk_sp = 0);
2818     if (_thread != NULL) {
2819       DEBUG_ONLY(pre_chunk_sp = _cont.sp();)
2820       squash_chunks();
2821       DEBUG_ONLY(post_chunk_sp = _cont.sp();)
2822     }
2823     assert ((_cont.sp() &lt;&lt; LogBytesPerElement) &gt;= pre_chunk_size, &quot;sp: %d in bytes: %d _pre_chunk_size: %d reported chunk size: %d actual size: %d&quot;,
2824       _cont.sp(), (_cont.sp() &lt;&lt; LogBytesPerElement), pre_chunk_size, post_chunk_size - pre_chunk_size, (pre_chunk_sp - post_chunk_sp) &lt;&lt; LogBytesPerElement);
2825     // assert ((int)_cont.max_size() == orig_max_size, &quot;_cont.max_size(): %d orig_max_size: %d&quot;, (int)_cont.max_size(), orig_max_size);
2826 
2827   #ifdef ASSERT
2828     hframe orig_top_frame = _cont.last_frame&lt;mode_slow&gt;();
2829     bool empty = _cont.is_empty();
2830     log_develop_trace(jvmcont)(&quot;top_hframe before (freeze):&quot;);
2831     if (log_develop_is_enabled(Trace, jvmcont)) orig_top_frame.print_on(_cont, tty);
2832 
2833     log_develop_trace(jvmcont)(&quot;empty: %d&quot;, empty);
2834     assert (!FULL_STACK || empty, &quot;&quot;);
2835     assert (!empty || _cont.sp() &gt;= _cont.stack_length() || _cont.sp() &lt; 0, &quot;sp: %d stack_length: %d&quot;, _cont.sp(), _cont.stack_length());
2836     assert (orig_top_frame.is_empty() == empty, &quot;empty: %d f.sp: %d tail: %d&quot;, empty, orig_top_frame.sp(), (_cont.tail() != (oop)NULL));
2837     assert (!empty || assert_bottom_java_frame_name(callee, ENTER_SIG), &quot;&quot;);
2838   #endif
2839 
2840     int argsize = 0;
2841     if (_cont.is_empty0()) {
2842       assert (_cont.is_empty(), &quot;&quot;);
2843       caller = new_bottom_hframe&lt;true&gt;(_cont.sp(), _cont.refSP(), NULL, false);
2844     } else {
2845       assert (_cont.is_flag(FLAG_LAST_FRAME_INTERPRETED) == Interpreter::contains(_cont.pc()), &quot;&quot;);
2846       int sp = _cont.sp();
2847 
2848       if (!FKind::interpreted) {
2849     #ifdef CONT_DOUBLE_NOP
2850         CachedCompiledMetadata md = ContinuationHelper::cached_metadata&lt;mode&gt;(callee);
2851         if (LIKELY(!md.empty())) {
2852           argsize = md.stack_argsize();
2853           assert(argsize == slow_stack_argsize(callee), &quot;argsize: %d slow_stack_argsize: %d&quot;, argsize, slow_stack_argsize(callee));
2854         } else
2855     #endif
2856           argsize = Compiled::stack_argsize(callee);
2857 
2858         if (_cont.is_flag(FLAG_LAST_FRAME_INTERPRETED)) {
2859           log_develop_trace(jvmcont)(&quot;finalize _size: %d add argsize: %d&quot;, _size, argsize);
2860           _size += argsize;
2861         } else {
2862           // the arguments of the bottom-most frame are part of the topmost compiled frame on the hstack; we overwrite that part
2863           // tty-&gt;print_cr(&quot;&gt;&gt;&gt; BEFORE: sp: %d&quot;, sp);
2864           // sp += argsize &gt;&gt; LogBytesPerElement;
2865           // tty-&gt;print_cr(&quot;&gt;&gt;&gt; AFTER: sp: %d&quot;, sp);
2866         }
2867       }
2868       caller = new_bottom_hframe&lt;false&gt;(sp, _cont.refSP(), _cont.pc(), _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED));
2869       // tty-&gt;print_cr(&quot;&gt;&gt;&gt; new_bottom_hframe&quot;); caller.print_on(tty);
2870     }
2871 
2872     DEBUG_ONLY(log_develop_trace(jvmcont)(&quot;finalize bottom frame:&quot;); if (log_develop_is_enabled(Trace, jvmcont)) caller.print_on(_cont, tty);)
2873 
2874     _cont.add_num_frames(_frames);
2875     _cont.add_size(_size);
2876     _cont.e_add_refs(_oops);
2877 
2878     *argsize_out = argsize;
2879 
2880     if (_thread != NULL) {
2881       frame entry = sender&lt;FKind&gt;(callee); // f.sender_for_compiled_frame&lt;ContinuationCodeBlobLookup&gt;(&amp;map);
2882 
2883       log_develop_trace(jvmcont)(&quot;Found entry:&quot;);
2884       if (log_develop_is_enabled(Trace, jvmcont)) entry.print_on(tty);
2885 
2886       assert (FKind::interpreted || entry.sp() == entry.unextended_sp(), &quot;&quot;);
2887       assert (Frame::callee_link_address(entry) == slow_link_address&lt;FKind&gt;(callee), &quot;&quot;);
2888 
2889       // assert (callee.is_interpreted_frame() || _cont.entrySP() == _bottom_address + (Compiled::stack_argsize(callee) &gt; 0 ? (Compiled::stack_argsize(callee) &gt;&gt; LogBytesPerWord) + 1 : 0),
2890       //   &quot;argsize: %d _bottom_address: &quot; INTPTR_FORMAT &quot; entrySP: &quot; INTPTR_FORMAT, Compiled::stack_argsize(callee), p2i(_bottom_address), p2i(_cont.entrySP()));
2891       assert (mode != mode_fast || !Interpreter::contains(_cont.entryPC()), &quot;&quot;); // we do not allow entry to be interpreted in fast mode
2892       assert (mode != mode_fast || Interpreter::contains(_cont.entryPC()) || entry.sp() == _bottom_address, &quot;f.sp: &quot; INTPTR_FORMAT &quot; _bottom_address: &quot; INTPTR_FORMAT &quot; entrySP: &quot; INTPTR_FORMAT, p2i(entry.sp()), p2i(_bottom_address), p2i(_cont.entrySP()));
2893       // assert (mode != mode_fast || !Interpreter::contains(_cont.entryPC()) || entry.sp() == _cont.entrySP() - 2, &quot;f.sp: %p entrySP: %p&quot;, entry.sp(), _cont.entrySP());
2894     }
2895 
2896     return freeze_ok;
2897   }
2898 
2899   template &lt;typename T&gt;
2900   friend class FreezeFrame;
2901 
2902   template&lt;typename FKind, bool top, bool bottom, bool IsKeepalive&gt;
2903   void freeze_java_frame(const frame&amp; f, hframe&amp; caller, int fsize, int argsize, int oops, typename FKind::ExtraT extra, CompiledMethodKeepaliveT* kd) {
2904     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 15) return;)
2905 
2906     log_develop_trace(jvmcont)(&quot;============================= FREEZING FRAME interpreted: %d top: %d bottom: %d&quot;, FKind::interpreted, top, bottom);
2907     log_develop_trace(jvmcont)(&quot;fsize: %d argsize: %d oops: %d&quot;, fsize, argsize, oops);
2908     if (log_develop_is_enabled(Trace, jvmcont)) f.print_on(tty);
2909     assert ((mode == mode_fast &amp;&amp; !bottom) || caller.is_interpreted_frame() == Interpreter::contains(caller.pc()), &quot;&quot;);
2910 
2911     caller.copy_partial&lt;mode&gt;(FreezeFrame&lt;FKind&gt;::template dispatch&lt;top, bottom, IsKeepalive, SelfT&gt;(*this, f, caller, fsize, argsize, oops, extra, kd));
2912   }
2913 
2914   template &lt;typename FKind&gt;
2915   void freeze_oops(const frame&amp; f, intptr_t* vsp, intptr_t *hsp, int index, int num_oops, void* extra) {
2916     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt; 30) return;)
2917     //log_develop_info(jvmcont)(&quot;writing %d oops from %d (%c)&quot;, num_oops, index, FKind::type);
2918 
2919     log_develop_trace(jvmcont)(&quot;Walking oops (freeze)&quot;);
2920 
2921     assert (!_map.include_argument_oops(), &quot;&quot;);
2922 
2923     _fp_oop_info._has_fp_oop = false;
2924 
2925     int frozen;
2926     if (!FKind::interpreted) {
2927       FreezeFnT freeze_stub = (FreezeFnT)extra;
2928       frozen = freeze_compiled_oops(f, vsp, hsp, index, freeze_stub); //freeze_compiled_oops_stub(freeze_stub, f, vsp, hsp, index);
2929     } else {
2930       if (num_oops == 0)
2931         return;
2932       ContinuationHelper::update_register_map_with_callee(&amp;_map, f); // restore saved link
2933       frozen = freeze_intepreted_oops(f, vsp, hsp, index, *(InterpreterOopMap*) extra);
2934     }
2935     assert(frozen == num_oops, &quot;frozen: %d num_oops: %d&quot;, frozen, num_oops);
2936   }
2937 
2938   template &lt;typename FKind, bool top, bool bottom&gt;
2939   void patch(const frame&amp; f, hframe&amp; hf, const hframe&amp; caller) {
2940     assert (FKind::is_instance(f), &quot;&quot;);
2941     assert (bottom || !caller.is_empty(), &quot;&quot;);
2942     // in fast mode, partial copy does not copy _is_interpreted for the caller
2943     assert (bottom || mode == mode_fast || Interpreter::contains(FKind::interpreted ? hf.return_pc&lt;FKind&gt;() : caller.real_pc(_cont)) == caller.is_interpreted_frame(),
2944       &quot;FKind: %s contains: %d is_interpreted: %d&quot;, FKind::name, Interpreter::contains(FKind::interpreted ? hf.return_pc&lt;FKind&gt;() : caller.real_pc(_cont)), caller.is_interpreted_frame()); // fails for perftest &lt; 25, but that&#39;s ok
2945     assert (!bottom || !_cont.is_empty() || (_cont.fp() == 0 &amp;&amp; _cont.pc() == NULL), &quot;&quot;);
2946     assert (!bottom || _cont.is_empty() || (!FKind::interpreted &amp;&amp; caller.is_interpreted_frame() &amp;&amp; hf.compiled_frame_stack_argsize() &gt; 0) || caller == _cont.last_frame&lt;mode_slow&gt;(), &quot;&quot;);
2947     assert (!bottom || _cont.is_empty() || _thread == NULL || Continuation::is_cont_barrier_frame(f), &quot;&quot;);
2948     assert (!bottom || _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED) == Interpreter::contains(_cont.pc()), &quot;&quot;);
2949     assert (!FKind::interpreted || hf.interpreted_link_address() == _cont.stack_address(hf.fp()), &quot;&quot;);
2950 
2951     if (bottom) {
2952       log_develop_trace(jvmcont)(&quot;Fixing return address on bottom frame: &quot; INTPTR_FORMAT, p2i(_cont.pc()));
2953       FKind::interpreted ? hf.patch_return_pc&lt;FKind&gt;(_cont.pc())
2954                          : caller.patch_pc(_cont.pc(), _cont); // TODO PERF non-temporal store
2955     }
2956 
2957     patch_pd&lt;FKind, top, bottom&gt;(f, hf, caller);
2958 
2959 #ifdef ASSERT
2960     // TODO DEOPT: long term solution: unroll on freeze and patch pc
2961     if (mode != mode_fast &amp;&amp; !FKind::interpreted &amp;&amp; !FKind::stub) {
2962       assert (hf.cb()-&gt;is_compiled(), &quot;&quot;);
2963       if (f.is_deoptimized_frame()) {
2964         log_develop_trace(jvmcont)(&quot;Freezing deoptimized frame&quot;);
2965         assert (f.cb()-&gt;as_compiled_method()-&gt;is_deopt_pc(f.raw_pc()), &quot;&quot;);
2966         assert (f.cb()-&gt;as_compiled_method()-&gt;is_deopt_pc(Frame::real_pc(f)), &quot;&quot;);
2967       }
2968     }
2969 #endif
2970   }
2971 
2972   template&lt;bool top&gt;
2973   NOINLINE freeze_result recurse_freeze_interpreted_frame(const frame&amp; f, hframe&amp; caller, int callee_argsize) {
2974     // ResourceMark rm(_thread);
2975     InterpreterOopMap mask;
2976     Interpreted::oop_map(f, &amp;mask);
2977     int fsize = Interpreted::size(f, &amp;mask);
2978     int oops  = Interpreted::num_oops(f, &amp;mask);
2979 
2980     log_develop_trace(jvmcont)(&quot;recurse_interpreted_frame _size: %d add fsize: %d callee_argsize: %d -- %d&quot;, _size, fsize, callee_argsize, fsize + callee_argsize);
2981     _size += fsize + callee_argsize;
2982     _oops += oops;
2983     _frames++;
2984     _cgrind_interpreted_frames++;
2985 
2986     return recurse_freeze_java_frame&lt;Interpreted, top, false&gt;(f, caller, fsize, 0, oops, &amp;mask, NULL);
2987   }
2988 
2989   template &lt;bool top, bool bottom&gt;
2990   hframe freeze_interpreted_frame(const frame&amp; f, const hframe&amp; caller, int fsize, int oops, InterpreterOopMap* mask) {
2991     intptr_t* vsp = Interpreted::frame_top(f, mask);
2992     assert ((Interpreted::frame_bottom(f) - vsp) * sizeof(intptr_t) == (size_t)fsize, &quot;&quot;);
2993 
2994     hframe hf = new_hframe&lt;Interpreted&gt;(f, vsp, caller, fsize, oops);
2995     intptr_t* hsp = _cont.stack_address(hf.sp());
2996 
2997     freeze_raw_frame(vsp, hsp, fsize);
2998 
2999     relativize_interpreted_frame_metadata(f, vsp, hf);
3000 
3001     freeze_oops&lt;Interpreted&gt;(f, vsp, hsp, hf.ref_sp(), oops, mask);
3002 
3003     patch&lt;Interpreted, top, bottom&gt;(f, hf, caller);
3004 
3005     _cont.inc_num_interpreted_frames();
3006 
3007     return hf;
3008   }
3009 
3010   int freeze_intepreted_oops(const frame&amp; f, intptr_t* vsp, intptr_t* hsp, int starting_index, const InterpreterOopMap&amp; mask) {
3011     FreezeOopFn&lt;RegisterMapT, typename ConfigT::OopWriterT&gt; oopFn(&amp;_cont, &amp;_fp_oop_info, &amp;f, vsp, hsp, &amp;_map, starting_index);
3012     const_cast&lt;frame&amp;&gt;(f).oops_interpreted_do(&amp;oopFn, NULL, mask);
3013     return oopFn.count();
3014   }
3015 
3016   template&lt;bool top, bool IsKeepalive&gt;
3017   freeze_result recurse_freeze_compiled_frame(const frame&amp; f, hframe&amp; caller, CompiledMethodKeepaliveT* kd) {
3018     int fsize, oops, argsize;
3019 #ifdef CONT_DOUBLE_NOP
3020     CachedCompiledMetadata md = ContinuationHelper::cached_metadata&lt;mode&gt;(f); // MUST BE SAFE FOR STUB CALLER; we&#39;re not at a call instruction
3021     fsize = md.size();
3022     if (LIKELY(fsize != 0)) {
3023       oops = md.num_oops();
3024       argsize = md.stack_argsize();
3025 
3026       assert(fsize == slow_size(f), &quot;fsize: %d slow_size: %d&quot;, fsize, slow_size(f));
3027       assert(oops  == slow_num_oops(f), &quot;oops: %d slow_num_oops: %d&quot;, oops, slow_num_oops(f));
3028       assert(argsize == slow_stack_argsize(f), &quot;argsize: %d slow_stack_argsize: %d&quot;, argsize, slow_stack_argsize(f));
3029     } else
3030 #endif
3031     {
3032       fsize = Compiled::size(f);
3033       oops  = Compiled::num_oops(f);
3034       argsize = mode == mode_fast ? 0 : Compiled::stack_argsize(f);
3035     }
3036     FreezeFnT freeze_stub = get_oopmap_stub(f); // try to do this early, so we wouldn&#39;t need to look at the oopMap again.
3037 
3038     log_develop_trace(jvmcont)(&quot;recurse_freeze_compiled_frame _size: %d add fsize: %d&quot;, _size, fsize);
3039     _size += fsize;
3040     _oops += oops;
3041     _frames++;
3042 
3043     // TODO PERF: consider recalculating fsize, argsize and oops in freeze_compiled_frame instead of passing them, as we now do in thaw
3044     return recurse_freeze_java_frame&lt;Compiled, top, IsKeepalive&gt;(f, caller, fsize, argsize, oops, freeze_stub, kd);
3045   }
3046 
3047   template &lt;typename FKind, bool top, bool bottom, bool IsKeepalive&gt;
3048   hframe freeze_compiled_frame(const frame&amp; f, const hframe&amp; caller, int fsize, int argsize, int oops, FreezeFnT freeze_stub, CompiledMethodKeepaliveT* kd) {
3049     freeze_compiled_frame_bp();
3050 
3051     intptr_t* vsp = FKind::frame_top(f);
3052 
3053     // The following assertion appears also in patch_pd and align.
3054     // Even in fast mode, we allow the caller of the bottom frame (i.e. last frame still on the hstack) to be interpreted.
3055     // We can have a different tradeoff, and only set mode_fast if this is not the case by uncommenting _fastpath = false in Thaw::finalize where we&#39;re setting the last frame
3056     // Doing so can save us the test for caller.is_interpreted_frame() when we&#39;re in mode_fast and bottom, but at the cost of not switching to fast mode even if only a frozen frame is interpreted.
3057     assert (mode != mode_fast || bottom || !Interpreter::contains(caller.pc()), &quot;&quot;);
3058 
3059     // in mode_fast we must not look at caller.is_interpreted_frame() because it may be wrong (hframe::partial_copy)
3060 
3061     if (bottom || (mode != mode_fast &amp;&amp; caller.is_interpreted_frame())) {
3062       if (!bottom) { // if we&#39;re bottom, argsize has been computed in finalize
3063         argsize = Compiled::stack_argsize(f);
3064       }
3065       log_develop_trace(jvmcont)(&quot;freeze_compiled_frame add argsize: fsize: %d argsize: %d fsize: %d&quot;, fsize, argsize, fsize + argsize);
3066       align&lt;bottom&gt;(caller); // TODO PERF
3067 
3068       if (caller.is_interpreted_frame()) {
3069         const_cast&lt;hframe&amp;&gt;(caller).set_sp(caller.sp() - (argsize &gt;&gt; LogBytesPerElement));
3070       }
3071     }
3072 
3073     hframe hf = new_hframe&lt;FKind&gt;(f, vsp, caller, fsize, oops);
3074     intptr_t* hsp = _cont.stack_address(hf.sp());
3075 
3076     fsize += argsize;
3077     freeze_raw_frame(vsp, hsp, fsize);
3078 
3079     if (!FKind::stub) {
3080       if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; _safepoint_stub_caller) {
3081         _safepoint_stub_h = freeze_safepoint_stub(hf);
3082       }
3083 
3084       // The keepalive must always be written. If IsKeepalive is false it means that the
3085       // keepalive object already existed, it must still be written.
3086       // kd is always !null unless we are in preemption mode which is handled above.
3087       const int keepalive_index = hf.ref_sp() + oops - 1;
3088       _cont.add_oop&lt;typename ConfigT::OopWriterT&gt;(NULL, keepalive_index);
3089 #ifndef NO_KEEPALIVE
3090       if (mode == mode_slow &amp;&amp; _preempt) {
3091         if (kd != NULL) {
3092           kd-&gt;write_at(_cont, keepalive_index);
3093         }
3094       } else {
3095         assert(kd != NULL, &quot;must exist&quot;);
3096         // ref_sp: 3, oops 4  -&gt; [ 3: oop, 4: oop, 5: oop, 6: nmethod ]
3097         kd-&gt;write_at(_cont, keepalive_index);
3098       }
3099 #endif
3100 
3101       freeze_oops&lt;Compiled&gt;(f, vsp, hsp, hf.ref_sp(), oops - 1, (void*)freeze_stub);
3102 
3103       if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; _safepoint_stub_caller) {
3104         assert (!_fp_oop_info._has_fp_oop, &quot;must be&quot;);
3105         _safepoint_stub = frame();
3106       }
3107 
3108 #ifndef NO_KEEPALIVE
3109       if (IsKeepalive) {
3110         kd-&gt;persist_oops();
3111       }
3112 #endif
3113     } else { // stub frame has no oops
3114       _fp_oop_info._has_fp_oop = false;
3115     }
3116 
3117     patch&lt;FKind, top, bottom&gt;(f, hf, caller);
3118 
3119     // log_develop_trace(jvmcont)(&quot;freeze_compiled_frame real_pc: &quot; INTPTR_FORMAT &quot; address: &quot; INTPTR_FORMAT &quot; sp: &quot; INTPTR_FORMAT, p2i(Frame::real_pc(f)), p2i(&amp;(((address*) f.sp())[-1])), p2i(f.sp()));
3120     assert(bottom || mode == mode_fast || Interpreter::contains(caller.real_pc(_cont)) == caller.is_interpreted_frame(), &quot;&quot;);
3121 
3122     return hf;
3123   }
3124 
3125   int freeze_compiled_oops(const frame&amp; f, intptr_t* vsp, intptr_t* hsp, int starting_index, FreezeFnT stub) {
3126     typename FreezeCompiledOops &lt;typename ConfigT::OopWriterT&gt;::Extra extra(&amp;f, &amp;_cont, (address*) &amp;_map);
3127 
3128     if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; _safepoint_stub_caller) {
3129       assert (!_safepoint_stub.is_empty(), &quot;&quot;);
3130       extra.set_stub_vsp(StubF::frame_top(_safepoint_stub));
3131 #ifndef PRODUCT
3132       assert (_safepoint_stub_hsp != NULL, &quot;&quot;);
3133       extra.set_stub_hsp(_safepoint_stub_hsp);
3134 #endif
3135     }
3136     typename ConfigT::OopT* addr = _cont.refStack()-&gt;template obj_at_address&lt;typename ConfigT::OopT&gt;(starting_index);
3137 
3138     int idx = _cont.refStack()-&gt;length() - starting_index; // RB: I think this is unused in the fast path...
3139     intptr_t** link_addr = Frame::callee_link_address(f); // Frame::map_link_address(map);
3140 
3141     if (mode == mode_slow &amp;&amp; _preempt) {
3142       return FreezeCompiledOops&lt;typename ConfigT::OopWriterT&gt;::slow_path_preempt((address) vsp, (address) addr, (address) link_addr, (address) hsp, idx, &amp;_fp_oop_info, &amp;extra, _safepoint_stub_caller);
3143     } else {
3144       return stub((address) vsp, (address) addr, (address) link_addr, (address) hsp, idx, &amp;_fp_oop_info, (address) &amp;extra);
3145     }
3146   }
3147 
3148   NOINLINE void finish(const frame&amp; f, const hframe&amp; top) {
3149     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 15) return;)
3150 
3151     ConfigT::OopWriterT::finish(_cont, nr_oops(), top.ref_sp());
3152 
3153     assert (top.sp() &lt;= _cont.sp(), &quot;top.sp(): %d sp: %d&quot;, top.sp(), _cont.sp());
3154 
3155     _cont.set_last_frame&lt;mode&gt;(top);
3156 
3157     if (log_develop_is_enabled(Trace, jvmcont)) {
3158       log_develop_trace(jvmcont)(&quot;top_hframe after (freeze):&quot;);
3159       _cont.last_frame&lt;mode_slow&gt;().print_on(_cont, tty);
3160     }
3161 
3162     assert (_cont.is_flag(FLAG_LAST_FRAME_INTERPRETED) == _cont.last_frame&lt;mode&gt;().is_interpreted_frame(), &quot;&quot;);
3163     assert(_cont.chunk_invariant(), &quot;&quot;);
3164   }
3165 
3166   NOINLINE freeze_result recurse_freeze_stub_frame(const frame&amp; f, hframe&amp; caller) {
3167     int fsize = StubF::size(f);
3168 
3169     log_develop_trace(jvmcont)(&quot;recurse_stub_frame _size: %d add fsize: %d&quot;, _size, fsize);
3170     _size += fsize;
3171     _frames++;
3172 
3173     assert (mode == mode_slow &amp;&amp; _preempt, &quot;&quot;);
3174     _safepoint_stub = f;
3175 
3176   #ifdef ASSERT
3177     hframe::callee_info my_info = slow_link_address&lt;StubF&gt;(f);
3178   #endif
3179     frame senderf = sender&lt;StubF&gt;(f); // f.sender_for_compiled_frame&lt;ContinuationCodeBlobLookup&gt;(&amp;map);
3180 
3181     assert (Frame::callee_link_address(senderf) == my_info, &quot;&quot;);
3182     assert (senderf.unextended_sp() &lt; _bottom_address - SP_WIGGLE, &quot;&quot;);
3183     assert (senderf.is_compiled_frame(), &quot;&quot;);
3184     assert (senderf.oop_map() != NULL, &quot;&quot;);
3185 
3186     // we can have stub_caller as a value template argument, but that&#39;s unnecessary
3187     _safepoint_stub_caller = true;
3188     freeze_result result = recurse_freeze_compiled_frame&lt;false, false&gt;(senderf, caller, NULL);
3189     if (result == freeze_ok) {
3190       finish(f, _safepoint_stub_h);
3191     }
3192     return result;
3193   }
3194 
3195   NOINLINE hframe freeze_safepoint_stub(hframe&amp; caller) {
3196     log_develop_trace(jvmcont)(&quot;== FREEZING STUB FRAME:&quot;);
3197 
3198     assert(mode == mode_slow &amp;&amp; _preempt, &quot;&quot;);
3199     assert(!_safepoint_stub.is_empty(), &quot;&quot;);
3200 
3201     int fsize = StubF::size(_safepoint_stub);
3202 
3203     hframe hf = freeze_compiled_frame&lt;StubF, true, false, false&gt;(_safepoint_stub, caller, fsize, 0, 0, NULL, NULL);
3204 
3205 #ifndef PRODUCT
3206     _safepoint_stub_hsp = _cont.stack_address(hf.sp());
3207 #endif
3208 
3209     log_develop_trace(jvmcont)(&quot;== DONE FREEZING STUB FRAME&quot;);
3210     return hf;
3211   }
3212 
3213   inline FreezeFnT get_oopmap_stub(const frame&amp; f) {
3214     if (!USE_STUBS) {
3215       return OopStubs::freeze_oops_slow();
3216     }
3217     return ContinuationHelper::freeze_stub&lt;mode&gt;(f);
3218   }
3219 
3220   inline void freeze_raw_frame(intptr_t* vsp, intptr_t* hsp, int fsize) {
3221     log_develop_trace(jvmcont)(&quot;freeze_raw_frame: sp: %d&quot;, _cont.stack_index(hsp));
3222     _cont.copy_from_stack(vsp, hsp, fsize);
3223   }
3224 
3225 };
3226 
3227 template &lt;typename ConfigT&gt;
3228 class NormalOopWriter {
3229 public:
3230   typedef typename ConfigT::OopT OopT;
3231 
3232   static void obj_at_put(objArrayOop array, int index, oop obj) { array-&gt;obj_at_put_access&lt;IS_DEST_UNINITIALIZED&gt;(index, obj); }
3233   static void finish(ContMirror&amp; mirror, int count, int low_array_index) { }
3234 };
3235 
3236 template &lt;typename ConfigT&gt;
3237 class RawOopWriter {
3238 public:
3239   typedef typename ConfigT::OopT OopT;
3240 
3241   static void obj_at_put(objArrayOop array, int index, oop obj) {
3242     OopT* addr = array-&gt;obj_at_addr&lt;OopT&gt;(index); // depends on UseCompressedOops
3243     //assert(*addr == (OopT) NULL, &quot;&quot;);
3244     RawAccess&lt;IS_DEST_UNINITIALIZED&gt;::oop_store(addr, obj);
3245   }
3246 
3247   static void finish(ContMirror&amp; mirror, int count, int low_array_index) {
3248     if (count &gt; 0) {
3249       BarrierSet* bs = BarrierSet::barrier_set();
3250       ModRefBarrierSet* mbs = barrier_set_cast&lt;ModRefBarrierSet&gt;(bs);
3251       HeapWord* start = (HeapWord*) mirror.refStack()-&gt;obj_at_addr&lt;OopT&gt;(low_array_index);
3252       mbs-&gt;write_ref_array(start, count);
3253     }
3254   }
3255 };
3256 
3257 int early_return(int res, JavaThread* thread) {
3258   thread-&gt;set_cont_yield(false);
3259   log_develop_trace(jvmcont)(&quot;=== end of freeze (fail %d)&quot;, res);
3260   return res;
3261 }
3262 
3263 static void invlidate_JVMTI_stack(JavaThread* thread) {
3264   if (thread-&gt;is_interp_only_mode()) {
3265     JvmtiThreadState *jvmti_state = thread-&gt;jvmti_thread_state();
3266     if (jvmti_state != NULL)
3267       jvmti_state-&gt;invalidate_cur_stack_depth();
3268   }
3269 }
3270 
3271 static void post_JVMTI_yield(JavaThread* thread, ContMirror&amp; cont) {
3272   if (JvmtiExport::should_post_continuation_yield() || JvmtiExport::can_post_frame_pop()) {
3273     set_anchor_to_entry(thread, cont.entry()); // ensure frozen frames are invisible
3274 
3275     cont.read_rest();
3276     int num_frames = num_java_frames(cont);
3277 
3278     // The call to JVMTI can safepoint, so we need to restore oops.
3279     Handle conth(Thread::current(), cont.mirror());
3280     JvmtiExport::post_continuation_yield(JavaThread::current(), num_frames);
3281     cont.post_safepoint(conth);
3282   }
3283 
3284   invlidate_JVMTI_stack(thread);
3285 }
3286 
3287 static inline int freeze_epilog(JavaThread* thread, ContMirror&amp; cont) {
3288   PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 15) return freeze_ok;)
3289 
3290   assert (verify_continuation&lt;2&gt;(cont.mirror()), &quot;&quot;);
3291 
3292   assert (!cont.is_empty(), &quot;&quot;);
3293 
3294   thread-&gt;set_cont_yield(false);
3295 
3296   log_develop_debug(jvmcont)(&quot;=== End of freeze cont ### #&quot; INTPTR_FORMAT, cont.hash());
3297 
3298   return 0;
3299 }
3300 
3301 static int freeze_epilog(JavaThread* thread, ContMirror&amp; cont, freeze_result res) {
3302   if (UNLIKELY(res != freeze_ok)) {
3303     assert (verify_continuation&lt;11&gt;(cont.mirror()), &quot;&quot;);
3304     return early_return(res, thread);
3305   }
3306 #if CONT_JFR
3307   cont.post_jfr_event(&amp;event, thread);
3308 #endif
3309   post_JVMTI_yield(thread, cont); // can safepoint
3310   return freeze_epilog(thread, cont);
3311 }
3312 
3313 // returns the continuation yielding (based on context), or NULL for failure (due to pinning)
3314 // it freezes multiple continuations, depending on contex
3315 // it must set Continuation.stackSize
3316 // sets Continuation.fp/sp to relative indices
3317 template&lt;typename ConfigT, op_mode mode&gt;
3318 int freeze0(JavaThread* thread, intptr_t* const sp, bool preempt) {
3319   //callgrind();
3320   PERFTEST_ONLY(PERFTEST_LEVEL = ContPerfTest;)
3321 
3322   PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 10) return early_return(freeze_ok, thread);)
3323   PERFTEST_ONLY(if (PERFTEST_LEVEL &lt; 1000) thread-&gt;set_cont_yield(false);)
3324 
3325 #ifdef ASSERT
3326   log_develop_trace(jvmcont)(&quot;~~~~~~~~~ freeze mode: %d sp: &quot; INTPTR_FORMAT &quot; fp: &quot; INTPTR_FORMAT &quot; pc: &quot; INTPTR_FORMAT, 
3327     mode, p2i(thread-&gt;cont_entry()-&gt;entry_sp()), p2i(thread-&gt;cont_entry()-&gt;entry_fp()), p2i(thread-&gt;cont_entry()-&gt;entry_pc()));
3328 
3329   /* set_anchor(thread, fi); */ print_frames(thread);
3330 #endif
3331   // if (mode != mode_fast) tty-&gt;print_cr(&quot;&gt;&gt;&gt; freeze0 mode: %d&quot;, mode);
3332 
3333   assert (!thread-&gt;cont_yield(), &quot;&quot;);
3334   assert (!thread-&gt;has_pending_exception(), &quot;&quot;); // if (thread-&gt;has_pending_exception()) return early_return(freeze_exception, thread, fi);
3335 
3336 #if CONT_JFR
3337   EventContinuationFreeze event;
3338 #endif
3339 
3340   thread-&gt;set_cont_yield(true);
3341 
3342   oop oopCont = get_continuation(thread);
3343   assert (oopCont == thread-&gt;cont_entry()-&gt;continuation(), &quot;&quot;);
3344 
3345   assert (verify_continuation&lt;1&gt;(oopCont), &quot;&quot;);
3346   ContMirror cont(thread, oopCont);
3347   log_develop_debug(jvmcont)(&quot;FREEZE #&quot; INTPTR_FORMAT &quot; &quot; INTPTR_FORMAT, cont.hash(), p2i((oopDesc*)oopCont));
3348 
3349   if (java_lang_Continuation::critical_section(oopCont) &gt; 0) {
3350     log_develop_debug(jvmcont)(&quot;PINNED due to critical section&quot;);
3351     assert (verify_continuation&lt;10&gt;(cont.mirror()), &quot;&quot;);
3352     return early_return(freeze_pinned_cs, thread);
3353   }
3354 
3355   Freeze&lt;ConfigT, mode&gt; fr(thread, cont);
3356   if (mode == mode_fast &amp;&amp; fr.is_chunk_available(sp)) {
3357     // no transition
3358     log_develop_trace(jvmcont)(&quot;chunk available; no transition&quot;);
3359     freeze_result res = fr.freeze(sp, true);
3360     assert (res == freeze_ok, &quot;&quot;);
3361   #if CONT_JFR
3362     cont.post_jfr_event(&amp;event, thread);
3363   #endif
3364     return freeze_epilog(thread, cont);
3365   } else if (mode != mode_fast &amp;&amp; UNLIKELY(preempt)) {
3366     assert (thread-&gt;thread_state() == _thread_in_vm || thread-&gt;thread_state() == _thread_blocked, &quot;&quot;);
3367     freeze_result res = fr.freeze_preempt();
3368     return freeze_epilog(thread, cont, res);
3369   } else {
3370     // manual transtion. see JRT_ENTRY in interfaceSupport.inline.hpp
3371     log_develop_trace(jvmcont)(&quot;chunk unavailable; transitioning to VM&quot;);
3372     ThreadInVMfromJava __tiv(thread);
3373     HandleMarkCleaner __hm(thread);
3374     debug_only(VMEntryWrapper __vew;)
3375     assert (thread-&gt;thread_state() == _thread_in_vm, &quot;&quot;);
3376     
3377     freeze_result res = fr.freeze(sp, false);
3378     if (UNLIKELY(res == freeze_retry_slow)) {
3379       log_develop_trace(jvmcont)(&quot;-- RETRYING SLOW --&quot;);
3380       res = Freeze&lt;ConfigT, mode_slow&gt;(thread, cont).freeze(sp, false);
3381     } else if (LIKELY(res == freeze_ok)) {
3382       set_anchor_to_entry(thread, cont.entry()); // ensure frozen frames are invisible to stack walks, as they might be patched and broken
3383     }
3384     return freeze_epilog(thread, cont, res);
3385   }
3386 }
3387 
3388 static freeze_result is_pinned(const frame&amp; f, RegisterMap* map) {
3389   if (f.is_interpreted_frame()) {
3390     if (Interpreted::is_owning_locks(f))           return freeze_pinned_monitor;
3391     if (f.interpreter_frame_method()-&gt;is_native()) return freeze_pinned_native; // interpreter native entry
3392   } else if (f.is_compiled_frame()) {
3393     if (Compiled::is_owning_locks(map-&gt;thread(), map, f)) return freeze_pinned_monitor;
3394   } else {
3395     return freeze_pinned_native;
3396   }
3397   return freeze_ok;
3398 }
3399 
3400 #ifdef ASSERT
3401 static bool monitors_on_stack(JavaThread* thread) {
3402   ContinuationEntry* cont = thread-&gt;cont_entry();
3403   RegisterMap map(thread, false, false, false); // should first argument be true?
3404   map.set_include_argument_oops(false);
3405   frame f = thread-&gt;last_frame();
3406   while (true) {
3407     if (!Continuation::is_frame_in_continuation(cont, f)) break;
3408     if (is_pinned(f, &amp;map) == freeze_pinned_monitor) return true;
3409     f = f.sender(&amp;map);
3410   }
3411   return false;
3412 }
3413 #endif
3414 
3415 // Entry point to freeze. Transitions are handled manually
3416 int Continuation::freeze(JavaThread* thread, intptr_t* sp) {
3417   TRACE_CALL(int, Continuation::freeze(JavaThread* thread, intptr_t* sp))
3418   os::verify_stack_alignment();
3419   assert (sp == thread-&gt;frame_anchor()-&gt;last_Java_sp(), &quot;&quot;);
3420 
3421   // There are no interpreted frames if we&#39;re not called from the interpreter and we haven&#39;t ancountered an i2c adapter or called Deoptimization::unpack_frames
3422   // Calls from native frames also go through the interpreter (see JavaCalls::call_helper)
3423   // We also clear thread-&gt;cont_fastpath in Deoptimize::deoptimize_single_frame and when we thaw interpreted frames
3424   bool fast = UseContinuationFastPath &amp;&amp; thread-&gt;cont_fastpath();
3425   assert (!fast || monitors_on_stack(thread) == (thread-&gt;held_monitor_count() &gt; 0), &quot;monitors_on_stack: %d held_monitor_count: %d&quot;, monitors_on_stack(thread), thread-&gt;held_monitor_count());
3426   fast = fast &amp;&amp; thread-&gt;held_monitor_count() == 0;
3427   // if (!fast) tty-&gt;print_cr(&quot;&gt;&gt;&gt; freeze fast: %d thread.cont_fastpath: %d held_monitor_count: %d&quot;, fast, thread-&gt;cont_fastpath(), thread-&gt;held_monitor_count());
3428 
3429   return fast ? cont_freeze&lt;mode_fast&gt;(thread, sp, false)
3430               : cont_freeze&lt;mode_slow&gt;(thread, sp, false);
3431 }
3432 
3433 static freeze_result is_pinned0(JavaThread* thread, oop cont_scope, bool safepoint) {
3434   ContinuationEntry* cont = thread-&gt;cont_entry();
3435   if (cont == NULL) {
3436     return freeze_ok;
3437   }
3438   if (java_lang_Continuation::critical_section(cont-&gt;continuation()) &gt; 0)
3439     return freeze_pinned_cs;
3440 
3441   RegisterMap map(thread, false, false, false); // should first argument be true?
3442   map.set_include_argument_oops(false);
3443   frame f = thread-&gt;last_frame();
3444 
3445   if (!safepoint) {
3446     f = f.frame_sender&lt;ContinuationCodeBlobLookup&gt;(&amp;map); // LOOKUP // this is the yield frame
3447   } else { // safepoint yield
3448     f.set_fp(f.real_fp()); // Instead of this, maybe in ContMirror::set_last_frame always use the real_fp?
3449     if (!Interpreter::contains(f.pc())) {
3450       assert (is_stub(f.cb()), &quot;must be&quot;);
3451       assert (f.oop_map() != NULL, &quot;must be&quot;);
3452       f.oop_map()-&gt;update_register_map(&amp;f, &amp;map); // we have callee-save registers in this case
3453     }
3454   }
3455 
3456   while (true) {
3457     freeze_result res = is_pinned(f, &amp;map);
3458     if (res != freeze_ok)
3459       return res;
3460 
3461     f = f.frame_sender&lt;ContinuationCodeBlobLookup&gt;(&amp;map);
3462     if (!Continuation::is_frame_in_continuation(cont, f)) {
3463       oop scope = java_lang_Continuation::scope(cont-&gt;continuation());
3464       if (scope == cont_scope)
3465         break;
3466       cont = cont-&gt;parent();
3467       if (cont == NULL)
3468         break;
3469       if (java_lang_Continuation::critical_section(cont-&gt;continuation()) &gt; 0)
3470         return freeze_pinned_cs;
3471     }
3472   }
3473   return freeze_ok;
3474 }
3475 
3476 typedef int (*DoYieldStub)(int scopes);
3477 
3478 static bool is_safe_to_preempt(JavaThread* thread) {
3479   // if (Thread::current()-&gt;is_VM_thread() &amp;&amp; thread-&gt;thread_state() == _thread_blocked) {
3480   //   log_develop_trace(jvmcont)(&quot;is_safe_to_preempt: thread blocked&quot;);
3481   //   return false;
3482   // }
3483 
3484   if (!thread-&gt;has_last_Java_frame()) {
3485     log_develop_trace(jvmcont)(&quot;is_safe_to_preempt: no last Java frame&quot;);
3486     return false;
3487   }
3488 
3489   frame f = thread-&gt;last_frame();
3490   if (log_develop_is_enabled(Trace, jvmcont)) {
3491     log_develop_trace(jvmcont)(&quot;is_safe_to_preempt %sSAFEPOINT&quot;, Interpreter::contains(f.pc()) ? &quot;INTERPRETER &quot; : &quot;&quot;);
3492     f.cb()-&gt;print_on(tty); 
3493     f.print_on(tty);
3494   }
3495 
3496   if (Interpreter::contains(f.pc())) {
3497     InterpreterCodelet* desc = Interpreter::codelet_containing(f.pc());
3498     if (desc != NULL) {
3499       if (log_develop_is_enabled(Trace, jvmcont)) desc-&gt;print_on(tty);
3500       // We allow preemption only when no bytecode (safepoint codelet) or a return byteocde
3501       if (desc-&gt;bytecode() &gt;= 0 &amp;&amp; !Bytecodes::is_return(desc-&gt;bytecode())) {
3502         log_develop_trace(jvmcont)(&quot;is_safe_to_preempt: unsafe bytecode: %s&quot;, Bytecodes::name(desc-&gt;bytecode()));
3503         return false;
3504       } else {
3505         log_develop_trace(jvmcont)(&quot;is_safe_to_preempt: %s (safe)&quot;, desc-&gt;description());
3506         // assert (Bytecodes::is_return(desc-&gt;bytecode()) || desc-&gt;description() != NULL &amp;&amp; strncmp(&quot;safepoint&quot;, desc-&gt;description(), 9) == 0, &quot;desc: %s&quot;, desc-&gt;description());
3507       }
3508     } else {
3509       log_develop_trace(jvmcont)(&quot;is_safe_to_preempt: no codelet (safe?)&quot;);
3510     }
3511   } else {
3512     // if (f.is_compiled_frame()) {
3513     //   RelocIterator iter(f.cb()-&gt;as_compiled_method(), f.pc(), f.pc()+1);
3514     //   while (iter.next()) {
3515     //     iter.print_current();
3516     //   }
3517     // }
3518     if (!f.cb()-&gt;is_safepoint_stub()) {
3519       log_develop_trace(jvmcont)(&quot;is_safe_to_preempt: not safepoint stub&quot;);
3520       return false;
3521     }
3522   }
3523   return true;
3524 }
3525 
3526 // called in a safepoint
3527 int Continuation::try_force_yield(JavaThread* thread, const oop cont) {
3528   log_develop_trace(jvmcont)(&quot;try_force_yield: thread state: %s VM thread: %d&quot;, thread-&gt;thread_state_name(), Thread::current()-&gt;is_VM_thread());
3529 
3530   ContinuationEntry* ce = thread-&gt;cont_entry();
3531   oop innermost = ce-&gt;continuation();
3532   while (ce != NULL &amp;&amp; ce-&gt;continuation() != cont) {
3533     ce = ce-&gt;parent();
3534   }
3535   if (ce == NULL) {
3536     return -1; // no continuation
3537   }
3538   if (thread-&gt;_cont_yield) {
3539     return -2; // during yield
3540   }
3541   if (!is_safe_to_preempt(thread)) {
3542     return freeze_pinned_native;
3543   }
3544 
3545   const oop scope = java_lang_Continuation::scope(cont);
3546   if (innermost != cont) { // we have nested continuations
3547     // make sure none of the continuations in the hierarchy are pinned
3548     freeze_result res_pinned = is_pinned0(thread, scope, true);
3549     if (res_pinned != freeze_ok)
3550       return res_pinned;
3551 
3552     java_lang_Continuation::set_yieldInfo(cont, scope);
3553   }
3554 
3555 // #ifdef ASSERT
3556 //   tty-&gt;print_cr(&quot;FREEZING:&quot;);
3557 //   frame lf = thread-&gt;last_frame();
3558 //   lf.print_on(tty);
3559 //   tty-&gt;print_cr(&quot;&quot;);
3560 //   const ImmutableOopMap* oopmap = lf.oop_map();
3561 //   if (oopmap != NULL) {
3562 //     oopmap-&gt;print();
3563 //     tty-&gt;print_cr(&quot;&quot;);
3564 //   } else {
3565 //     tty-&gt;print_cr(&quot;oopmap: NULL&quot;);
3566 //   }
3567 //   tty-&gt;print_cr(&quot;*&amp;^*&amp;#^$*&amp;&amp;@(#*&amp;@(#&amp;*(*@#&amp;*(&amp;@#$^*(&amp;#$(*&amp;#@$(*&amp;#($*&amp;@#($*&amp;$(#*$&quot;);
3568 // #endif
3569   // TODO: save return value
3570 
3571   int res = cont_freeze&lt;mode_slow&gt;(thread, NULL /*thread-&gt;last_Java_sp()*/, true); // CAST_TO_FN_PTR(DoYieldStub, StubRoutines::cont_doYield_C())(-1);
3572   if (res == 0) { // success
3573     thread-&gt;set_cont_preempt(true);
3574 
3575     frame last = thread-&gt;last_frame();
3576     Frame::patch_pc(last, StubRoutines::cont_jump_from_sp()); // reinstates rbpc and rlocals for the sake of the interpreter
3577     log_develop_trace(jvmcont)(&quot;try_force_yield installed cont_jump_from_sp stub on&quot;); if (log_develop_is_enabled(Trace, jvmcont)) last.print_on(tty);
3578 
3579     // this return barrier is used for compiled frames; for interpreted frames we use the call to StubRoutines::cont_jump_from_sp_C in JavaThread::handle_special_runtime_exit_condition
3580   }
3581   return res;
3582 }
3583 /////////////// THAW ////
3584 
3585 enum thaw_kind {
3586   thaw_top = 0,
3587   thaw_return_barrier = 1,
3588   thaw_exception = 2,
3589 };
3590 
3591 typedef intptr_t* (*ThawContFnT)(JavaThread*, ContMirror&amp;, thaw_kind);
3592 
3593 static ThawContFnT cont_thaw_fast = NULL;
3594 static ThawContFnT cont_thaw_slow = NULL;
3595 
3596 template&lt;op_mode mode&gt;
3597 static intptr_t* cont_thaw(JavaThread* thread, ContMirror&amp; cont, thaw_kind kind) {
3598   switch (mode) {
3599     case mode_fast:    return cont_thaw_fast(thread, cont, kind);
3600     case mode_slow:    return cont_thaw_slow(thread, cont, kind);
3601     default:
3602       guarantee(false, &quot;unreachable&quot;);
3603       return NULL;
3604   }
3605 }
3606 
3607 static bool stack_overflow_check(JavaThread* thread, int size, address sp) {
3608   const int page_size = os::vm_page_size();
3609   if (size &gt; page_size) {
3610     if (sp - size &lt; thread-&gt;stack_overflow_limit()) {
3611       return false;
3612     }
3613   }
3614   return true;
3615 }
3616 
3617 JRT_LEAF(int, Continuation::prepare_thaw(JavaThread* thread, bool return_barrier))
3618   PERFTEST_ONLY(PERFTEST_LEVEL = ContPerfTest;)
3619 
3620   PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 110) return 0;)
3621 
3622   log_develop_trace(jvmcont)(&quot;~~~~~~~~~ prepare_thaw return_barrier: %d&quot;, return_barrier);
3623   log_develop_trace(jvmcont)(&quot;prepare_thaw&quot;);
3624 
3625   assert (thread == JavaThread::current(), &quot;&quot;);
3626   oop cont = thread-&gt;cont_entry()-&gt;cont_raw(); // get_continuation(thread);
3627   assert (cont == get_continuation(thread), &quot;cont: %p entry cont: %p&quot;, (oopDesc*)cont, (oopDesc*)get_continuation(thread));
3628   assert (verify_continuation&lt;1&gt;(cont), &quot;&quot;);
3629 
3630   // if the entry frame is interpreted, it may leave a parameter on the stack, which would be left there if the return barrier is hit
3631   // assert ((address)java_lang_Continuation::entrySP(cont) - bottom &lt;= 8, &quot;bottom: &quot; INTPTR_FORMAT &quot;, entrySP: &quot; INTPTR_FORMAT, bottom, java_lang_Continuation::entrySP(cont));
3632   int size = java_lang_Continuation::maxSize(cont); // TODO: we could use the top non-empty chunk size, but getting it might be too costly; consider.
3633   guarantee (size &gt; 0, &quot;&quot;);
3634 
3635   size += (2*frame_metadata) &lt;&lt; LogBytesPerWord; // twice, because we might want to add a frame for StubRoutines::cont_interpreter_forced_preempt_return()
3636   size += SP_WIGGLE &lt;&lt; LogBytesPerWord; // just in case we have an interpreted entry after which we need to align
3637 
3638   const address bottom = (address)thread-&gt;cont_entry()-&gt;entry_sp(); // os::current_stack_pointer(); points to the entry frame
3639   if (!stack_overflow_check(thread, size + 300, bottom)) {
3640     return 0;
3641   }
3642 
3643   log_develop_trace(jvmcont)(&quot;prepare_thaw bottom: &quot; INTPTR_FORMAT &quot; top: &quot; INTPTR_FORMAT &quot; size: %d&quot;, p2i(bottom), p2i(bottom - size), size);
3644 
3645   PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 120) return 0;)
3646   assert (verify_continuation&lt;2&gt;(cont), &quot;&quot;);
3647 
3648   return size;
3649 JRT_END
3650 
3651 template &lt;typename ConfigT, op_mode mode&gt;
3652 class Thaw {
3653   typedef typename Conditional&lt;mode == mode_slow, RegisterMap, SmallRegisterMap&gt;::type RegisterMapT; // we need a full map to store the register dump for a safepoint stub during preemtion
3654 
3655 private:
3656   JavaThread* _thread;
3657   ContMirror&amp; _cont;
3658 
3659   bool _fastpath; // if true, a subsequent freeze can be in mode_fast
3660 
3661   RegisterMapT _map; // map is only passed to thaw_compiled_frame for use in deoptimize, which uses it only for biased locks; we may not need deoptimize there at all -- investigate
3662 
3663   bool _preempt;
3664   const hframe* _safepoint_stub;
3665   bool _safepoint_stub_caller;
3666   frame _safepoint_stub_f;
3667 
3668   DEBUG_ONLY(int _frames;)
3669 
3670   inline frame new_entry_frame();
3671   template&lt;typename FKind&gt; frame new_frame(const hframe&amp; hf, intptr_t* vsp);
3672   template&lt;typename FKind, bool top, bool bottom&gt; inline void patch_pd(frame&amp; f, const frame&amp; sender);
3673   void derelativize_interpreted_frame_metadata(const hframe&amp; hf, const frame&amp; f);
3674   inline hframe::callee_info frame_callee_info_address(frame&amp; f);
3675   template&lt;typename FKind, bool top, bool bottom&gt; inline intptr_t* align(const hframe&amp; hf, intptr_t* vsp, frame&amp; caller);
3676   void deoptimize_frames_in_chunk(oop chunk);
3677   void deoptimize_frame_in_chunk(intptr_t* sp, address pc, CodeBlob* cb);
3678   void patch_chunk_pd(intptr_t* sp);
3679   inline intptr_t* align_chunk(intptr_t* vsp);
3680   inline void prefetch_chunk_pd(void* start, int size_words);
3681   intptr_t* push_interpreter_return_frame(intptr_t* sp);
3682 
3683   bool should_deoptimize() {
3684     // frame::deoptimize (or Deoptimize::deoptimize) is called either &quot;inline&quot; with execution (current method or caller), -- not relevant for frozen frames
3685     // on all frames in current thread on VM-&gt;Java transition with -XX:+DeoptimizeALot,
3686     // or on all frames in all threads for debugging. We only care about the last.
3687     return true; /* mode != mode_fast &amp;&amp; _thread-&gt;is_interp_only_mode(); */
3688   } // TODO PERF
3689 
3690 public:
3691 
3692   Thaw(JavaThread* thread, ContMirror&amp; mirror) :
3693     _thread(thread), _cont(mirror),
3694     _fastpath(true),
3695     _map(thread, false, false, false),
3696     _preempt(false),
3697     _safepoint_stub_f(false) { // don&#39;t intitialize
3698 
3699     _map.set_include_argument_oops(false);
3700   }
3701 
3702   void init_rest() { // we want to postpone some initialization after chunk handling
3703     _safepoint_stub = NULL;
3704     _safepoint_stub_caller = false;
3705   }
3706 
3707   intptr_t* thaw(thaw_kind kind) {
3708     assert (!Interpreter::contains(_cont.entryPC()), &quot;&quot;);
3709     // if (Interpreter::contains(_cont.entryPC())) _fastpath = false; // set _fastpath to false if entry is interpreted
3710 
3711     assert (verify_continuation&lt;1&gt;(_cont.mirror()), &quot;&quot;);
3712     assert (!java_lang_Continuation::done(_cont.mirror()), &quot;&quot;);
3713     assert (!_cont.is_empty(), &quot;&quot;);
3714 
3715     DEBUG_ONLY(int orig_num_frames = java_lang_Continuation::numFrames(_cont.mirror());/*_cont.num_frames();*/)
3716     DEBUG_ONLY(_frames = 0;)
3717 
3718     bool last_interpreted = false;
3719     if (mode == mode_slow &amp;&amp; _cont.is_flag(FLAG_SAFEPOINT_YIELD)) {
3720       _preempt = true;
3721       last_interpreted = _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED);
3722       assert (_cont.tail() == (oop)NULL, &quot;&quot;);
3723     }
3724 
3725     intptr_t* sp = thaw&lt;true&gt;(kind != thaw_top);
3726 
3727     assert (java_lang_Continuation::numFrames(_cont.mirror()) == orig_num_frames - _frames, &quot;num_frames: %d orig_num_frames: %d frame_count: %d&quot;, java_lang_Continuation::numFrames(_cont.mirror()), orig_num_frames, _frames);
3728     // assert (mode != mode_fast || _fastpath, &quot;&quot;);
3729     assert(_cont.chunk_invariant(), &quot;&quot;);
3730     _thread-&gt;set_cont_fastpath(_fastpath);
3731 
3732   #ifdef ASSERT
3733     log_develop_debug(jvmcont)(&quot;Jumping to frame (thaw): [%ld]&quot;, java_tid(_thread));
3734     frame f = sp_to_frame(sp);
3735     if (log_develop_is_enabled(Debug, jvmcont)) f.print_on(tty);
3736   #endif
3737 
3738     if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; last_interpreted) {
3739       assert (last_interpreted == Interpreter::contains(*(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET)), &quot;last_interpreted: %d interpreted: %d&quot;, last_interpreted, Interpreter::contains(*(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET)));
3740       sp = push_interpreter_return_frame(sp);
3741     }
3742     
3743     return sp;
3744   }
3745 
3746   template&lt;bool top&gt;
3747   intptr_t* thaw(bool return_barrier) {
3748     assert (mode == mode_slow || !_preempt, &quot;&quot;);
3749 
3750     oop chunk = _cont.tail();
3751     if (chunk == (oop)NULL) {
3752       EventContinuationThawOld e;
3753       if (e.should_commit()) {
3754         e.set_id(cast_from_oop&lt;u8&gt;(_cont.mirror()));
3755         e.commit();
3756       }
3757 
3758       if (!_cont.is_empty0()) {
3759         assert (!_map.include_argument_oops(), &quot;should be&quot;);
3760         init_rest();
3761         _cont.read_rest();
3762 
3763         hframe hf = _cont.last_frame&lt;mode&gt;();
3764         log_develop_trace(jvmcont)(&quot;top_hframe before (thaw):&quot;); if (log_develop_is_enabled(Trace, jvmcont)) hf.print_on(_cont, tty);
3765         frame caller;
3766 
3767         int num_frames = FULL_STACK ? 1000 // TODO
3768                                     : (return_barrier ? 1 : 2);
3769         thaw&lt;top&gt;(hf, caller, num_frames);
3770         intptr_t* sp = caller.sp();
3771         if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; caller.is_compiled_frame()) {
3772           sp = _safepoint_stub_f.sp();
3773         }
3774 
3775         _cont.write();
3776         assert(_cont.chunk_invariant(), &quot;&quot;);
3777 
3778         return sp;
3779       } else {
3780         assert (assert_entry_frame_laid_out(_cont.entry()), &quot;&quot;);
3781         return _cont.entrySP();
3782       }
3783     } else {
3784       assert (USE_CHUNKS, &quot;&quot;);
3785       return thaw_chunk(chunk, top);
3786     }
3787   }
3788 
3789   NOINLINE intptr_t* thaw_chunk(oop chunk, const bool top) {
3790     assert (USE_CHUNKS, &quot;&quot;);
3791     assert (top || FULL_STACK, &quot;&quot;);
3792     assert (chunk != (oop) NULL, &quot;&quot;);
3793     assert (chunk == _cont.tail(), &quot;&quot;);
3794 
3795     static const int threshold = 500; // words
3796 
3797     log_develop_trace(jvmcont)(&quot;thaw_chunk&quot;);
3798     if (log_develop_is_enabled(Debug, jvmcont)) print_chunk(chunk, _cont.mirror(), true);
3799 
3800     int sp = jdk_internal_misc_StackChunk::sp(chunk);
3801     int size = jdk_internal_misc_StackChunk::size(chunk) + frame::sender_sp_offset - sp;
3802     int argsize = jdk_internal_misc_StackChunk::argsize(chunk);
3803 
3804     // this initial size could be reduced if it&#39;s a partial thaw
3805 
3806     if (size &lt;= 0) {
3807       _cont.set_tail(jdk_internal_misc_StackChunk::parent(chunk));
3808       java_lang_Continuation::set_tail(_cont.mirror(), _cont.tail());
3809       return thaw&lt;true&gt;(true); // no harm if we&#39;re wrong about return_barrier
3810     }
3811 
3812     assert (verify_stack_chunk&lt;1&gt;(chunk), &quot;&quot;);
3813     // assert (verify_continuation&lt;99&gt;(_cont.mirror()), &quot;&quot;);
3814 
3815     // const bool barriers = requires_barriers(chunk); // TODO PERF
3816 
3817     // if (!barriers) { // TODO ????
3818     //   ContMirror::reset_chunk_counters(chunk);
3819     // }
3820 
3821     intptr_t* const hsp = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + sp;
3822     intptr_t* vsp;
3823     if (FULL_STACK) {
3824       _cont.set_tail(jdk_internal_misc_StackChunk::parent(chunk));
3825       java_lang_Continuation::set_tail(_cont.mirror(), _cont.tail());
3826       vsp = thaw&lt;false&gt;(false);
3827     } else {
3828       vsp = _cont.entrySP();
3829     }
3830 
3831     bool partial, empty;
3832     if (!TEST_THAW_ONE_CHUNK_FRAME &amp;&amp; LIKELY(FULL_STACK || (size &lt; threshold /*&amp;&amp; !barriers*/))) {
3833       // prefetch with anticipation of memcpy starting at highest address
3834       prefetch_chunk_pd(InstanceStackChunkKlass::start_of_stack(chunk), size);
3835 
3836       partial = false;
3837       empty = true;
3838       size -= argsize;
3839 
3840       if (mode != mode_fast &amp;&amp; should_deoptimize()) {
3841         deoptimize_frames_in_chunk(chunk);
3842       }
3843 
3844       jdk_internal_misc_StackChunk::set_sp(chunk, jdk_internal_misc_StackChunk::size(chunk) + frame::sender_sp_offset);
3845       jdk_internal_misc_StackChunk::set_argsize(chunk, 0);
3846       // jdk_internal_misc_StackChunk::set_pc(chunk, NULL);
3847       // if (barriers) {
3848       //   jdk_internal_misc_StackChunk::set_numFrames(chunk, 0);
3849       //   jdk_internal_misc_StackChunk::set_numOops(chunk, 0);
3850       // }
3851     } else { // thaw a single frame
3852       partial = true;
3853       empty = thaw_one_frame_from_chunk(chunk, hsp, &amp;size, &amp;argsize);
3854       if (UNLIKELY(argsize != 0)) {
3855         size += frame::sender_sp_offset;
3856       }
3857       // if (empty &amp;&amp; barriers) { // this is done in allocate_chunk; no need to do it here
3858       //   _cont.set_tail(jdk_internal_misc_StackChunk::parent(chunk));
3859       //   // java_lang_Continuation::set_tail(_cont.mirror(), _cont.tail());
3860       // }
3861     }
3862 
3863     const bool is_last_in_chunks = empty &amp;&amp; jdk_internal_misc_StackChunk::is_parent_null&lt;typename ConfigT::OopT&gt;(chunk);
3864     const bool is_last = is_last_in_chunks &amp;&amp; _cont.is_empty0();
3865 
3866     _cont.sub_size((size - (UNLIKELY(argsize != 0) ? frame::sender_sp_offset : 0)) &lt;&lt; LogBytesPerWord);
3867     assert (_cont.is_flag(FLAG_LAST_FRAME_INTERPRETED) == Interpreter::contains(_cont.pc()), &quot;&quot;);
3868     if (is_last_in_chunks &amp;&amp; _cont.is_flag(FLAG_LAST_FRAME_INTERPRETED)) {
3869       _cont.sub_size(SP_WIGGLE &lt;&lt; LogBytesPerWord);
3870     }
3871 
3872     log_develop_trace(jvmcont)(&quot;thaw_chunk partial: %d full: %d top: %d is_last: %d empty: %d size: %d argsize: %d&quot;, partial, FULL_STACK, top, is_last, empty, size, argsize);
3873 
3874     // if we&#39;re not in a full thaw, we&#39;re both top and bottom
3875     if (!FULL_STACK) {
3876       vsp -= argsize; // if not bottom, we&#39;re going to overwrite the args portion of the sender
3877       assert (argsize != 0 || vsp == align_chunk(vsp), &quot;&quot;);
3878       vsp = align_chunk(vsp);
3879     }
3880 
3881     intptr_t* bottom_sp = vsp;
3882 
3883     vsp -= size;
3884     if (UNLIKELY(argsize != 0)) {
3885       vsp += frame::sender_sp_offset;
3886     }
3887     assert (vsp == align_chunk(vsp), &quot;&quot;);
3888 
3889     size += argsize;
3890     intptr_t* from = hsp - frame::sender_sp_offset;
3891     intptr_t* to   = vsp - frame::sender_sp_offset;
3892     copy_from_chunk(from, to, size); // TODO: maybe use a memcpy that cares about ordering because we&#39;re racing with the GC
3893 
3894     // if (barriers) {
3895     //   memset(from, 0, size &lt;&lt; LogBytesPerWord);
3896     // }
3897 
3898     if (!FULL_STACK || is_last) {
3899       assert (!is_last || argsize == 0, &quot;&quot;);
3900       _cont.set_argsize(argsize);
3901       patch_chunk(bottom_sp, is_last);
3902 
3903       address pc = *(address*)(bottom_sp - SENDER_SP_RET_ADDRESS_OFFSET);
3904       assert (is_last ? CodeCache::find_blob(pc)-&gt;as_compiled_method()-&gt;method()-&gt;is_continuation_enter_intrinsic() : pc == StubRoutines::cont_returnBarrier(), &quot;is_last: %d&quot;, is_last);
3905     }
3906 
3907     if (!FULL_STACK || top) {
3908       // _cont.write_minimal(); // must be done after patch; really only need to write max_size
3909       java_lang_Continuation::set_maxSize(_cont.mirror(), (jint)_cont.max_size());
3910       assert (java_lang_Continuation::flags(_cont.mirror()) == _cont.flags(), &quot;&quot;);
3911     }
3912 
3913     OrderAccess::loadload(); // we must test the gc mode *after* the copy
3914     if (UNLIKELY(should_fix(chunk))) {
3915       intptr_t* end = vsp + size - argsize;
3916       if (argsize &gt; 0) {
3917         end -= frame_metadata;
3918       }
3919       fix_stack_chunk(chunk, vsp, end);
3920       if (empty) {
3921         // we&#39;ve set 
3922         jdk_internal_misc_StackChunk::set_gc_mode(chunk, false);
3923       }
3924     }
3925     
3926     assert (is_last == _cont.is_empty(), &quot;is_last: %d _cont.is_empty(): %d&quot;, is_last, _cont.is_empty());
3927     assert(_cont.chunk_invariant(), &quot;&quot;);
3928 
3929   #if CONT_JFR
3930     EventContinuationThawYoung e;
3931     if (e.should_commit()) {
3932       e.set_id(cast_from_oop&lt;u8&gt;(chunk));
3933       e.set_size(size &lt;&lt; LogBytesPerWord);
3934       e.set_full(!partial);
3935       e.commit();
3936     }
3937   #endif
3938 
3939     // assert (verify_continuation&lt;100&gt;(_cont.mirror()), &quot;&quot;);
3940     return vsp;
3941   }
3942 
3943   inline bool should_fix(oop chunk) {
3944     if (UNLIKELY(jdk_internal_misc_StackChunk::gc_mode(chunk))) return true;
3945     if (/*UseZGC*/ !ConfigT::_compressed_oops) { // TODO: This assumes ZGC doesn&#39;t have compressed oops; replace with ConfigT::_concurrenct_gc
3946       return !oop_fixed(chunk, jdk_internal_misc_StackChunk::cont_offset()); // this is the last oop traversed in this object -- see InstanceStackChunkKlass::oop_oop_iterate in instanceStackChunkKlass.inline.hpp
3947     }
3948     return false;
3949   }
3950 
3951   inline bool oop_fixed(oop obj, int offset) {
3952     typedef typename ConfigT::OopT OopT;
3953     OopT* loc = obj-&gt;obj_field_addr_raw&lt;OopT&gt;(offset);
3954     intptr_t before = *(intptr_t*)loc;
3955     intptr_t after = cast_from_oop&lt;intptr_t&gt;(HeapAccess&lt;&gt;::oop_load(loc));
3956     // tty-&gt;print_cr(&quot;!oop_fixed %d&quot;, before != after);
3957     return before == after;
3958   }
3959 
3960   NOINLINE bool thaw_one_frame_from_chunk(oop chunk, intptr_t* hsp, int* out_size, int* out_argsize) {
3961     assert (hsp == (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::sp(chunk), &quot;&quot;);
3962 
3963     address pc = *(address*)(hsp - SENDER_SP_RET_ADDRESS_OFFSET);
3964     int slot;
3965     CodeBlob* cb = ContinuationCodeBlobLookup::find_blob_and_oopmap(pc, slot);
3966     int size = cb-&gt;frame_size(); // in words
3967     int argsize = (cb-&gt;as_compiled_method()-&gt;method()-&gt;num_stack_arg_slots() * VMRegImpl::stack_slot_size) &gt;&gt; LogBytesPerWord; // in words
3968 
3969     // tty-&gt;print_cr(&quot;&gt;&gt;&gt;&gt; thaw_one_frame_from_chunk thawing: &quot;); cb-&gt;print_value_on(tty);
3970 
3971     if (should_deoptimize()
3972         &amp;&amp; (cb-&gt;as_compiled_method()-&gt;is_marked_for_deoptimization() || (mode != mode_fast &amp;&amp; _thread-&gt;is_interp_only_mode()))) {
3973       deoptimize_frame_in_chunk(hsp, pc, cb);
3974     }
3975 
3976     int empty = jdk_internal_misc_StackChunk::sp(chunk) + size &gt;= (jdk_internal_misc_StackChunk::size(chunk) - jdk_internal_misc_StackChunk::argsize(chunk));
3977     assert (!empty || argsize == jdk_internal_misc_StackChunk::argsize(chunk), &quot;&quot;);
3978 
3979     if (empty) {
3980       assert (jdk_internal_misc_StackChunk::size(chunk) + frame::sender_sp_offset == (argsize == 0 ? jdk_internal_misc_StackChunk::sp(chunk) + size
3981                                                                                                    : jdk_internal_misc_StackChunk::sp(chunk) + size + argsize + frame::sender_sp_offset), &quot;&quot;);
3982       jdk_internal_misc_StackChunk::set_sp(chunk, jdk_internal_misc_StackChunk::size(chunk) + frame::sender_sp_offset);
3983       jdk_internal_misc_StackChunk::set_argsize(chunk, 0);
3984       // jdk_internal_misc_StackChunk::set_pc(chunk, NULL);
3985       // if (barriers) {
3986         // assert (0 == jdk_internal_misc_StackChunk::numFrames(chunk) - 1, &quot;&quot;);
3987         // jdk_internal_misc_StackChunk::set_numFrames(chunk, jdk_internal_misc_StackChunk::numFrames(chunk) - 1);
3988 
3989         // assert (0 == jdk_internal_misc_StackChunk::numOops(chunk) - cb-&gt;oop_map_for_slot(slot, pc)-&gt;num_oops(), &quot;&quot;);
3990         // jdk_internal_misc_StackChunk::set_numOops(chunk, 0);
3991       // }
3992     } else {
3993       jdk_internal_misc_StackChunk::set_sp(chunk, jdk_internal_misc_StackChunk::sp(chunk) + size);
3994       address top_pc = *(address*)(hsp + size - SENDER_SP_RET_ADDRESS_OFFSET);
3995       jdk_internal_misc_StackChunk::set_pc(chunk, top_pc);
3996 
3997       // if (barriers) {
3998       //   assert (jdk_internal_misc_StackChunk::numFrames(chunk) &gt; 0, &quot;&quot;);
3999       //   jdk_internal_misc_StackChunk::set_numFrames(chunk, jdk_internal_misc_StackChunk::numFrames(chunk) - 1);
4000 
4001       //   const ImmutableOopMap* oopmap = cb-&gt;oop_map_for_slot(slot, pc);
4002       //   assert (jdk_internal_misc_StackChunk::numOops(chunk) &gt;= oopmap-&gt;num_oops(), &quot;jdk_internal_misc_StackChunk::numOops(chunk): %d oopmap-&gt;num_oops() : %d&quot;, jdk_internal_misc_StackChunk::numOops(chunk), oopmap-&gt;num_oops());
4003       //   jdk_internal_misc_StackChunk::set_numOops(chunk, jdk_internal_misc_StackChunk::numOops(chunk) - oopmap-&gt;num_oops());
4004       // }
4005     }
4006 
4007     *out_size = size;
4008     *out_argsize = argsize;
4009     return empty;
4010   }
4011 
4012   void copy_from_chunk(intptr_t* from, intptr_t* to, int size) {
4013     log_develop_trace(jvmcont)(&quot;Copying from h: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d bytes)&quot;, p2i(from), p2i(from + size), size &lt;&lt; LogBytesPerWord);
4014     log_develop_trace(jvmcont)(&quot;Copying to v: &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; (%d bytes)&quot;, p2i(to), p2i(to + size), size &lt;&lt; LogBytesPerWord);
4015 
4016     copy_to_stack(from, to, size);
4017   }
4018 
4019   void patch_chunk(intptr_t* sp, bool is_last) {
4020     log_develop_trace(jvmcont)(&quot;thaw_chunk patching -- sp: &quot; INTPTR_FORMAT, p2i(sp));
4021 
4022     address pc = !is_last ? StubRoutines::cont_returnBarrier() : _cont.entryPC();
4023     *(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET) = pc;
4024     log_develop_trace(jvmcont)(&quot;thaw_chunk is_last: %d sp: &quot; INTPTR_FORMAT &quot; patching pc at &quot; INTPTR_FORMAT &quot; to &quot; INTPTR_FORMAT, is_last, p2i(sp), p2i(sp - SENDER_SP_RET_ADDRESS_OFFSET), p2i(pc));
4025 
4026     // patch_chunk_pd(sp);
4027   }
4028 
4029   template&lt;bool top&gt;
4030   void thaw(const hframe&amp; hf, frame&amp; caller, int num_frames) {
4031     log_develop_debug(jvmcont)(&quot;thaw num_frames: %d&quot;, num_frames);
4032     assert(!_cont.is_empty(), &quot;no more frames&quot;);
4033     assert (_cont.tail() == (oop)NULL, &quot;&quot;);
4034     assert (num_frames &gt; 0 &amp;&amp; !hf.is_empty(), &quot;&quot;);
4035 
4036     // Dynamically branch on frame type
4037     if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; top &amp;&amp; !hf.is_interpreted_frame()) {
4038       assert (is_stub(hf.cb()), &quot;&quot;);
4039       recurse_stub_frame(hf, caller, num_frames);
4040     } else if (mode == mode_fast || !hf.is_interpreted_frame()) {
4041       recurse_compiled_frame&lt;top&gt;(hf, caller, num_frames);
4042     } else {
4043       assert (mode != mode_fast, &quot;&quot;);
4044       recurse_interpreted_frame&lt;top&gt;(hf, caller, num_frames);
4045     }
4046   }
4047 
4048   template&lt;typename FKind, bool top&gt;
4049   void recurse_thaw_java_frame(const hframe&amp; hf, frame&amp; caller, int num_frames, void* extra) {
4050     assert (num_frames &gt; 0, &quot;&quot;);
4051 
4052     //hframe hsender = hf.sender&lt;FKind, mode(_cont,
4053     //return sender&lt;FKind, mode&gt;(cont, FKind::interpreted ? interpreted_frame_num_oops(*mask) : compiled_frame_num_oops());
4054     hframe hsender = hf.sender&lt;FKind, mode&gt;(_cont, FKind::interpreted ? (InterpreterOopMap*)extra : NULL, FKind::extra_oops); // TODO PERF maybe we can reuse fsize?
4055 
4056     bool is_empty = hsender.is_empty();
4057     if (num_frames == 1 || is_empty) {
4058       log_develop_trace(jvmcont)(&quot;is_empty: %d&quot;, is_empty);
4059       finalize&lt;FKind&gt;(hsender, hf, is_empty, caller);
4060       thaw_java_frame&lt;FKind, top, true&gt;(hf, caller, extra);
4061     } else {
4062       bool safepoint_stub_caller; // the use of _safepoint_stub_caller is not nice, but given preemption being performance non-critical, we don&#39;t want to add either a template or a regular parameter
4063       if (mode == mode_slow &amp;&amp; _preempt) {
4064         safepoint_stub_caller = _safepoint_stub_caller;
4065         _safepoint_stub_caller = false;
4066       }
4067 
4068       thaw&lt;false&gt;(hsender, caller, num_frames - 1); // recurse
4069 
4070       if (mode == mode_slow &amp;&amp; _preempt) _safepoint_stub_caller = safepoint_stub_caller; // restore _stub_caller
4071 
4072       thaw_java_frame&lt;FKind, top, false&gt;(hf, caller, extra);
4073     }
4074 
4075     DEBUG_ONLY(_frames++;)
4076     
4077     if (top) {
4078       finish(caller); // caller is now the current frame
4079     }
4080   }
4081 
4082   template&lt;typename FKind&gt;
4083   void finalize(const hframe&amp; hf, const hframe&amp; callee, bool is_empty, frame&amp; entry) {
4084     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 115) return;)
4085 
4086     entry = new_entry_frame();
4087 
4088     assert (entry.sp() == _cont.entrySP(), &quot;entry.sp: %p entrySP: %p&quot;, entry.sp(), _cont.entrySP());
4089     // assert (Interpreter::contains(_cont.entryPC())
4090     //   || entry.sp() == _cont.entrySP(), &quot;entry.sp: %p entrySP: %p&quot;, entry.sp(), _cont.entrySP());
4091     // assert (!Interpreter::contains(_cont.entryPC())
4092     //   || entry.sp() == _cont.entrySP() - 2, &quot;entry.sp: %p entrySP: %p&quot;, entry.sp(), _cont.entrySP());
4093 
4094   #ifdef ASSERT
4095     log_develop_trace(jvmcont)(&quot;Found entry:&quot;);
4096     print_vframe(entry);
4097     assert_bottom_java_frame_name(entry, ENTER_SPECIAL_SIG);
4098   #endif
4099 
4100     _cont.set_argsize(0);
4101     if (is_empty) {
4102       _cont.set_empty();
4103     } else {
4104       _cont.set_last_frame&lt;mode&gt;(hf); // _last_frame = hf;
4105       if (!FKind::interpreted &amp;&amp; !hf.is_interpreted_frame()) {
4106         int argsize;
4107     #ifdef CONT_DOUBLE_NOP
4108         CachedCompiledMetadata md = ContinuationHelper::cached_metadata&lt;mode&gt;(callee);
4109         if (LIKELY(!md.empty())) {
4110           argsize = md.stack_argsize();
4111           assert(argsize == slow_stack_argsize(callee), &quot;argsize: %d slow_stack_argsize: %d&quot;, argsize, slow_stack_argsize(callee));
4112         } else
4113     #endif
4114           argsize = callee.compiled_frame_stack_argsize();
4115         // we&#39;ll be subtracting the argsize in thaw_compiled_frame, but if the caller is compiled, we shouldn&#39;t
4116         _cont.add_size(argsize);
4117       }
4118       // else {
4119       //   _fastpath = false; // see discussion in Freeze::freeze_compiled_frame
4120       // }
4121     }
4122 
4123     assert (is_entry_frame(_cont, entry), &quot;&quot;);
4124     assert (_frames == 0, &quot;&quot;);
4125     assert (is_empty == _cont.is_empty() /* _last_frame.is_empty()*/, &quot;hf.is_empty(cont): %d last_frame.is_empty(): %d &quot;, is_empty, _cont.is_empty()/*_last_frame.is_empty()*/);
4126   }
4127 
4128   template&lt;typename FKind, bool top, bool bottom&gt;
4129   void thaw_java_frame(const hframe&amp; hf, frame&amp; caller, void* extra) {
4130     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 115) return;)
4131 
4132     log_develop_trace(jvmcont)(&quot;============================= THAWING FRAME:&quot;);
4133 
4134     assert (FKind::is_instance(hf), &quot;&quot;);
4135     assert (bottom == is_entry_frame(_cont, caller), &quot;&quot;);
4136 
4137     if (log_develop_is_enabled(Trace, jvmcont)) hf.print(_cont);
4138 
4139     log_develop_trace(jvmcont)(&quot;stack_length: %d&quot;, _cont.stack_length());
4140 
4141     // TODO PERF see partial_copy in Freeze
4142     caller = FKind::interpreted ? thaw_interpreted_frame    &lt;top, bottom&gt;(hf, caller, (InterpreterOopMap*)extra)
4143                                 : thaw_compiled_frame&lt;FKind, top, bottom&gt;(hf, caller, (ThawFnT)extra);
4144 
4145     log_develop_trace(jvmcont)(&quot;thawed frame:&quot;);
4146     DEBUG_ONLY(print_vframe(caller, &amp;dmap);)
4147   }
4148 
4149   template &lt;typename FKind&gt;
4150   void thaw_oops(frame&amp; f, intptr_t* vsp, int oop_index, void* extra) {
4151     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt; 130) return;)
4152 
4153     log_develop_trace(jvmcont)(&quot;Walking oops (thaw)&quot;);
4154 
4155     assert (!_map.include_argument_oops(), &quot;&quot;);
4156 
4157     int thawed;
4158     assert(extra != NULL, &quot;&quot;);
4159     if (!FKind::interpreted) {
4160       thawed = thaw_compiled_oops(f, vsp, oop_index, (ThawFnT) extra);
4161       //log_develop_info(jvmcont)(&quot;thawing %d oops from %d (stub)&quot;, thawed, oop_index);
4162     } else {
4163       int num_oops = FKind::interpreted ? Interpreted::num_oops(f, (InterpreterOopMap*)extra) : NonInterpreted&lt;FKind&gt;::num_oops(f);
4164       num_oops -= FKind::extra_oops;
4165       //log_develop_info(jvmcont)(&quot;thawing %d oops from %d&quot;, num_oops, oop_index);
4166       if (num_oops == 0) {
4167         if (FKind::extra_oops &gt; 0) {
4168           _cont.null_ref_stack(oop_index, FKind::extra_oops);
4169         }
4170         return;
4171       }
4172 
4173       thawed = thaw_interpreted_oops(f, vsp, oop_index, (InterpreterOopMap*)extra);
4174     }
4175 
4176     log_develop_trace(jvmcont)(&quot;count: %d&quot;, thawed);
4177 #ifdef ASSERT
4178     int num_oops = FKind::interpreted ? Interpreted::num_oops(f, (InterpreterOopMap*)extra) : NonInterpreted&lt;FKind&gt;::num_oops(f);
4179     assert(thawed == num_oops - FKind::extra_oops, &quot;closure oop count different.&quot;);
4180 #endif
4181 
4182     _cont.null_ref_stack(oop_index, thawed + FKind::extra_oops);
4183     _cont.e_add_refs(thawed);
4184 
4185     log_develop_trace(jvmcont)(&quot;Done walking oops&quot;);
4186   }
4187 
4188   template&lt;typename FKind, bool top, bool bottom&gt;
4189   inline void patch(frame&amp; f, const frame&amp; caller) {
4190     assert (_cont.is_empty0() == _cont.is_empty(), &quot;is_empty0: %d is_empty: %d&quot;, _cont.is_empty0(), _cont.is_empty());
4191     if (bottom &amp;&amp; !_cont.is_empty()) {
4192       log_develop_trace(jvmcont)(&quot;Setting return address to return barrier: &quot; INTPTR_FORMAT, p2i(StubRoutines::cont_returnBarrier()));
4193       FKind::interpreted ? Interpreted::patch_return_pc(f, StubRoutines::cont_returnBarrier())
4194                          : FKind::patch_pc(caller, StubRoutines::cont_returnBarrier());
4195     } else if (bottom || should_deoptimize()) { // TODO PERF: is raw_pc necessary below (esp. when not bottom?)
4196       FKind::interpreted ? Interpreted::patch_return_pc(f, caller.raw_pc())
4197                          : FKind::patch_pc(caller, caller.raw_pc()); // this patches the return address to the deopt handler if necessary
4198     }
4199     patch_pd&lt;FKind, top, bottom&gt;(f, caller);
4200 
4201     if (FKind::interpreted) {
4202       Interpreted::patch_sender_sp(f, caller.unextended_sp()); // ContMirror::derelativize(vfp, frame::interpreter_frame_sender_sp_offset);
4203     }
4204 
4205     assert (!bottom || !_cont.is_empty() || assert_bottom_java_frame_name(f, ENTER_SIG), &quot;&quot;);
4206     assert (!bottom || (_cont.is_empty() != Continuation::is_cont_barrier_frame(f)), &quot;cont.is_empty(): %d is_cont_barrier_frame(f): %d &quot;, _cont.is_empty(), Continuation::is_cont_barrier_frame(f));
4207   }
4208 
4209   template&lt;bool top&gt;
4210   NOINLINE void recurse_interpreted_frame(const hframe&amp; hf, frame&amp; caller, int num_frames) {
4211     assert (hf.is_interpreted_frame(), &quot;&quot;);
4212     // ResourceMark rm(_thread);
4213     InterpreterOopMap mask;
4214     hf.interpreted_frame_oop_map(&amp;mask);
4215     int fsize = hf.interpreted_frame_size();
4216     int oops  = hf.interpreted_frame_num_oops(mask);
4217 
4218     recurse_thaw_java_frame&lt;Interpreted, top&gt;(hf, caller, num_frames, (void*)&amp;mask);
4219   }
4220 
4221   template&lt;bool top, bool bottom&gt;
4222   frame thaw_interpreted_frame(const hframe&amp; hf, const frame&amp; caller, InterpreterOopMap* mask) {
4223     int fsize = hf.interpreted_frame_size();
4224     log_develop_trace(jvmcont)(&quot;fsize: %d&quot;, fsize);
4225     intptr_t* vsp = (intptr_t*)((address)caller.unextended_sp() - fsize);
4226     intptr_t* hsp = _cont.stack_address(hf.sp());
4227 
4228     frame f = new_frame&lt;Interpreted&gt;(hf, vsp);
4229 
4230     // if the caller is compiled we should really extend its sp to be our fp + 2 (1 for the return address, plus 1), but we don&#39;t bother as we don&#39;t use it
4231 
4232     thaw_raw_frame(hsp, vsp, fsize);
4233 
4234     derelativize_interpreted_frame_metadata(hf, f);
4235 
4236     thaw_oops&lt;Interpreted&gt;(f, f.sp(), hf.ref_sp(), mask);
4237 
4238     patch&lt;Interpreted, top, bottom&gt;(f, caller);
4239 
4240     assert(f.is_interpreted_frame_valid(_cont.thread()), &quot;invalid thawed frame&quot;);
4241     assert(Interpreted::frame_bottom(f) &lt;= Frame::frame_top(caller), &quot;&quot;);
4242 
4243     _cont.sub_size(fsize);
4244     _cont.dec_num_frames();
4245     _cont.dec_num_interpreted_frames();
4246 
4247     _fastpath = false;
4248 
4249     return f;
4250   }
4251 
4252   int thaw_interpreted_oops(frame&amp; f, intptr_t* vsp, int starting_index, InterpreterOopMap* mask) {
4253     assert (mask != NULL, &quot;&quot;);
4254 
4255     ThawOopFn&lt;RegisterMapT&gt; oopFn(&amp;_cont, &amp;f, starting_index, vsp, &amp;_map);
4256     f.oops_interpreted_do(&amp;oopFn, NULL, mask); // f.oops_do(&amp;oopFn, NULL, &amp;oopFn, &amp;_map);
4257     return oopFn.count();
4258   }
4259 
4260   template&lt;bool top&gt;
4261   void recurse_compiled_frame(const hframe&amp; hf, frame&amp; caller, int num_frames) {
4262     assert (!hf.is_interpreted_frame(), &quot;&quot;);
4263     ThawFnT thaw_stub = get_oopmap_stub(hf); // try to do this early, so we wouldn&#39;t need to look at the oopMap again.
4264 
4265     return recurse_thaw_java_frame&lt;Compiled, top&gt;(hf, caller, num_frames, (void*)thaw_stub);
4266   }
4267 
4268   template&lt;typename FKind, bool top, bool bottom&gt;
4269   frame thaw_compiled_frame(const hframe&amp; hf, const frame&amp; caller, ThawFnT thaw_stub) {
4270     thaw_compiled_frame_bp();
4271     assert(FKind::stub == is_stub(hf.cb()), &quot;&quot;);
4272     assert (caller.sp() == caller.unextended_sp(), &quot;&quot;);
4273 
4274     int fsize;
4275 #ifdef CONT_DOUBLE_NOP
4276     CachedCompiledMetadata md;
4277     if (mode != mode_preempt) {
4278       md = ContinuationHelper::cached_metadata(hf.pc());
4279       fsize = md.size();
4280     }
4281     if (mode == mode_preempt || UNLIKELY(fsize == 0))
4282 #endif
4283       fsize = hf.compiled_frame_size();
4284     assert(fsize == slow_size(hf), &quot;fsize: %d slow_size: %d&quot;, fsize, slow_size(hf));
4285     log_develop_trace(jvmcont)(&quot;fsize: %d&quot;, fsize);
4286 
4287     intptr_t* vsp = (intptr_t*)((address)caller.unextended_sp() - fsize);
4288     log_develop_trace(jvmcont)(&quot;vsp: &quot; INTPTR_FORMAT, p2i(vsp));
4289 
4290     if (bottom || (mode != mode_fast &amp;&amp; caller.is_interpreted_frame())) {
4291       log_develop_trace(jvmcont)(&quot;thaw_compiled_frame add argsize: fsize: %d argsize: %d fsize: %d&quot;, fsize, hf.compiled_frame_stack_argsize(), fsize + hf.compiled_frame_stack_argsize());
4292       int argsize;
4293   #ifdef CONT_DOUBLE_NOP
4294       if (mode != mode_preempt &amp;&amp; LIKELY(!md.empty())) {
4295         argsize = md.stack_argsize();
4296         assert(argsize == slow_stack_argsize(hf), &quot;argsize: %d slow_stack_argsize: %d&quot;, argsize, slow_stack_argsize(hf));
4297       } else
4298   #endif
4299         argsize = hf.compiled_frame_stack_argsize();
4300 
4301       fsize += argsize;
4302       vsp   -= argsize &gt;&gt; LogBytesPerWord;
4303 
4304       const_cast&lt;frame&amp;&gt;(caller).set_sp((intptr_t*)((address)caller.sp() - argsize));
4305       assert (caller.sp() == (intptr_t*)((address)vsp + (fsize-argsize)), &quot;&quot;);
4306 
4307       vsp = align&lt;FKind, top, bottom&gt;(hf, vsp, const_cast&lt;frame&amp;&gt;(caller));
4308 
4309       if (bottom) {
4310         assert (_cont.argsize() == 0, &quot;entry argsize: %d: &quot;, _cont.argsize());
4311         _cont.set_argsize(argsize &gt;&gt; LogBytesPerWord);
4312         log_develop_trace(jvmcont)(&quot;setting entry argsize: %d&quot;, _cont.argsize());
4313       }
4314     }
4315 
4316     _cont.sub_size(fsize); // we&#39;re subtracting argsize here, but also in align.
4317 
4318     intptr_t* hsp = _cont.stack_address(hf.sp());
4319 
4320     log_develop_trace(jvmcont)(&quot;hsp: %d &quot;, _cont.stack_index(hsp));
4321 
4322     frame f = new_frame&lt;FKind&gt;(hf, vsp);
4323 
4324     thaw_raw_frame(hsp, vsp, fsize);
4325 
4326     if (!FKind::stub) {
4327       if (mode == mode_slow &amp;&amp; _preempt &amp;&amp; _safepoint_stub_caller) {
4328         _safepoint_stub_f = thaw_safepoint_stub(f);
4329       }
4330 
4331       thaw_oops&lt;FKind&gt;(f, f.sp(), hf.ref_sp(), (void*)thaw_stub);
4332     }
4333 
4334     patch&lt;FKind, top, bottom&gt;(f, caller);
4335 
4336     _cont.dec_num_frames();
4337 
4338     if (!FKind::stub) {
4339       hf.cb()-&gt;as_compiled_method()-&gt;run_nmethod_entry_barrier();
4340 
4341       if (f.is_deoptimized_frame()) { // TODO PERF
4342         _fastpath = false;
4343       } else if (should_deoptimize()
4344           &amp;&amp; (hf.cb()-&gt;as_compiled_method()-&gt;is_marked_for_deoptimization() || (mode != mode_fast &amp;&amp; _thread-&gt;is_interp_only_mode()))) {
4345         log_develop_trace(jvmcont)(&quot;Deoptimizing thawed frame&quot;);
4346         DEBUG_ONLY(Frame::patch_pc(f, NULL));
4347 
4348         f.deoptimize(_thread); // we&#39;re assuming there are no monitors; this doesn&#39;t revoke biased locks
4349         // set_anchor(_thread, f); // deoptimization may need this
4350         // Deoptimization::deoptimize(_thread, f, &amp;_map); // gets passed frame by value
4351         // clear_anchor(_thread);
4352 
4353         assert (f.is_deoptimized_frame() &amp;&amp; is_deopt_return(f.raw_pc(), f),
4354           &quot;f.is_deoptimized_frame(): %d is_deopt_return(f.raw_pc()): %d is_deopt_return(f.pc()): %d&quot;,
4355           f.is_deoptimized_frame(), is_deopt_return(f.raw_pc(), f), is_deopt_return(f.pc(), f));
4356         _fastpath = false;
4357       }
4358     }
4359 
4360     return f;
4361   }
4362 
4363   int thaw_compiled_oops(frame&amp; f, intptr_t* vsp, int starting_index, ThawFnT stub) {
4364     assert (starting_index &gt;= 0 &amp;&amp; starting_index &lt; _cont.refStack()-&gt;length(), &quot;starting_index: %d refStack.length: %d&quot;, starting_index, _cont.refStack()-&gt;length());
4365 
4366     ThawCompiledOops::Extra extra(&amp;f, &amp;_cont, (address*) &amp;_map, starting_index);
4367     typename ConfigT::OopT* addr = _cont.refStack()-&gt;template obj_at_address&lt;typename ConfigT::OopT&gt;(starting_index);
4368     if (mode == mode_slow &amp;&amp; _preempt) {
4369       return ThawCompiledOops::slow_path_preempt((address) vsp, (address) addr, (address)frame_callee_info_address(f), &amp;extra);
4370     } else {
4371       return stub((address) vsp, (address) addr, (address)frame_callee_info_address(f), (address) &amp;extra);
4372     }
4373   }
4374 
4375   void finish(frame&amp; f) {
4376     PERFTEST_ONLY(if (PERFTEST_LEVEL &lt;= 115) return;)
4377 
4378     push_return_frame(f);
4379 
4380     // _cont.set_last_frame(_last_frame);
4381 
4382     assert (!FULL_STACK || _cont.is_empty(), &quot;&quot;);
4383     assert (_cont.is_empty() == _cont.last_frame&lt;mode_slow&gt;().is_empty(), &quot;cont.is_empty: %d cont.last_frame().is_empty(): %d&quot;, _cont.is_empty(), _cont.last_frame&lt;mode_slow&gt;().is_empty());
4384     assert (_cont.is_empty() == (_cont.max_size() == 0), &quot;cont.is_empty: %d cont.max_size: &quot; SIZE_FORMAT &quot; #&quot; INTPTR_FORMAT, _cont.is_empty(), _cont.max_size(), _cont.hash());
4385     assert (_cont.is_empty() == (_cont.num_frames() == 0), &quot;cont.is_empty: %d num_frames: %d&quot;, _cont.is_empty(), _cont.num_frames());
4386     assert (_cont.is_empty() &lt;= (_cont.num_interpreted_frames() == 0), &quot;cont.is_empty: %d num_interpreted_frames: %d&quot;, _cont.is_empty(), _cont.num_interpreted_frames());
4387 
4388     log_develop_trace(jvmcont)(&quot;thawed %d frames&quot;, _frames);
4389 
4390     log_develop_trace(jvmcont)(&quot;top_hframe after (thaw):&quot;);
4391     if (log_develop_is_enabled(Trace, jvmcont)) _cont.last_frame&lt;mode_slow&gt;().print_on(_cont, tty);
4392   }
4393 
4394   void push_return_frame(frame&amp; f) { // see generate_cont_thaw
4395     assert (!f.is_compiled_frame() || f.is_deoptimized_frame() == f.cb()-&gt;as_compiled_method()-&gt;is_deopt_pc(f.raw_pc()), &quot;&quot;);
4396     assert (!f.is_compiled_frame() || f.is_deoptimized_frame() == (f.pc() != f.raw_pc()), &quot;&quot;);
4397 
4398     intptr_t* sp = f.sp();
4399     address pc = f.raw_pc();
4400     *(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET) = pc;
4401     Frame::patch_pc(f, pc); // in case we want to deopt the frame in a full transition, this is checked.
4402     ContinuationHelper::push_pd(f);
4403 
4404     assert(assert_frame_laid_out(f), &quot;&quot;);
4405 
4406     assert ((mode == mode_slow &amp;&amp;_preempt) || !FULL_STACK || assert_top_java_frame_name(f, YIELD0_SIG), &quot;&quot;);
4407   }
4408 
4409   void recurse_stub_frame(const hframe&amp; hf, frame&amp; caller, int num_frames) {
4410     log_develop_trace(jvmcont)(&quot;Found safepoint stub&quot;);
4411 
4412     assert (num_frames &gt; 1, &quot;&quot;);
4413     assert (mode == mode_slow &amp;&amp;_preempt, &quot;&quot;);
4414     assert(!hf.is_bottom&lt;StubF&gt;(_cont), &quot;&quot;);
4415 
4416     assert (hf.compiled_frame_num_oops() == 0, &quot;&quot;);
4417 
4418     _safepoint_stub = &amp;hf;
4419     _safepoint_stub_caller = true;
4420 
4421     hframe hsender = hf.sender&lt;StubF, mode&gt;(_cont, 0);
4422     assert (!hsender.is_interpreted_frame(), &quot;&quot;);
4423     recurse_compiled_frame&lt;false&gt;(hsender, caller, num_frames - 1);
4424 
4425     _safepoint_stub_caller = false;
4426 
4427     // In the case of a safepoint stub, the above line, called on the stub&#39;s sender, actually returns the safepoint stub after thawing it.
4428     finish(_safepoint_stub_f);
4429 
4430     DEBUG_ONLY(_frames++;)
4431   }
4432 
4433   NOINLINE frame thaw_safepoint_stub(frame&amp; caller) {
4434     // A safepoint stub is the only case we encounter callee-saved registers (aside from rbp). We therefore thaw that frame
4435     // before thawing the oops in its sender, as the oops will need to be written to that stub frame.
4436     log_develop_trace(jvmcont)(&quot;THAWING SAFEPOINT STUB&quot;);
4437 
4438     assert(mode == mode_slow &amp;&amp;_preempt, &quot;&quot;);
4439     assert (_safepoint_stub != NULL, &quot;&quot;);
4440 
4441     hframe stubf = *_safepoint_stub;
4442     _safepoint_stub_caller = false;
4443     _safepoint_stub = NULL;
4444 
4445     frame f = thaw_compiled_frame&lt;StubF, true, false&gt;(stubf, caller, NULL);
4446 
4447     f.oop_map()-&gt;update_register_map(&amp;f, _map.as_RegisterMap());
4448     log_develop_trace(jvmcont)(&quot;THAWING OOPS FOR SENDER OF SAFEPOINT STUB&quot;);
4449     return f;
4450   }
4451 
4452   inline ThawFnT get_oopmap_stub(const hframe&amp; f) {
4453     if (!USE_STUBS) {
4454       return OopStubs::thaw_oops_slow();
4455     }
4456     return ContinuationHelper::thaw_stub&lt;mode&gt;(f);
4457   }
4458 
4459   inline void thaw_raw_frame(intptr_t* hsp, intptr_t* vsp, int fsize) {
4460     log_develop_trace(jvmcont)(&quot;thaw_raw_frame: sp: %d&quot;, _cont.stack_index(hsp));
4461     _cont.copy_to_stack(hsp, vsp, fsize);
4462   }
4463 
4464 };
4465 
4466 static int maybe_count_Java_frames(ContMirror&amp; cont, bool return_barrier) {
4467   if (!return_barrier &amp;&amp; JvmtiExport::should_post_continuation_run()) {
4468     cont.read_rest();
4469     return num_java_frames(cont);
4470   }
4471   return -1;
4472 }
4473 
4474 static void post_JVMTI_continue(JavaThread* thread, ContMirror&amp; cont, intptr_t* sp, int java_frame_count) {
4475   if (JvmtiExport::should_post_continuation_run()) {
4476     set_anchor(thread, sp); // ensure thawed frames are visible
4477 
4478     // The call to JVMTI can safepoint, so we need to restore oops.
4479     Handle conth(thread, cont.mirror());
4480     JvmtiExport::post_continuation_run(JavaThread::current(), java_frame_count);
4481     cont.post_safepoint(conth);
4482 
4483     clear_anchor(thread);
4484   }
4485 
4486   invlidate_JVMTI_stack(thread);
4487 }
4488 
4489 static inline bool can_thaw_fast(ContMirror&amp; cont) {
4490   if (!cont.thread()-&gt;cont_fastpath_thread_state())
4491     return false;
4492 
4493   assert (!cont.thread()-&gt;is_interp_only_mode(), &quot;&quot;);
4494   assert (!JvmtiExport::should_post_continuation_run(), &quot;&quot;);
4495 
4496   if (LIKELY(!CONT_FULL_STACK)) {
4497     for (oop chunk = cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
4498       if (!ContMirror::is_empty_chunk(chunk))
4499         return true;
4500     }
4501     // return !cont.is_flag(FLAG_LAST_FRAME_INTERPRETED);
4502   }
4503 
4504   return java_lang_Continuation::numInterpretedFrames(cont.mirror()) == 0;
4505 }
4506 
4507 // returns new top sp; right below it are the pc and fp; see generate_cont_thaw
4508 // called after preparations (stack overflow check and making room)
4509 static inline intptr_t* thaw0(JavaThread* thread, const thaw_kind kind) {
4510   //callgrind();
4511   PERFTEST_ONLY(PERFTEST_LEVEL = ContPerfTest;)
4512   // NoSafepointVerifier nsv;
4513 #if CONT_JFR
4514   EventContinuationThaw event;
4515 #endif
4516 
4517   if (kind != thaw_top) {
4518     log_develop_trace(jvmcont)(&quot;== RETURN BARRIER&quot;);
4519   }
4520 
4521   log_develop_trace(jvmcont)(&quot;~~~~~~~~~ thaw kind: %d&quot;, kind);
4522   log_develop_trace(jvmcont)(&quot;sp: &quot; INTPTR_FORMAT &quot; fp: &quot; INTPTR_FORMAT &quot; pc: &quot; INTPTR_FORMAT, 
4523     p2i(thread-&gt;cont_entry()-&gt;entry_sp()), p2i(thread-&gt;cont_entry()-&gt;entry_fp()), p2i(thread-&gt;cont_entry()-&gt;entry_pc()));
4524   
4525   assert (thread == JavaThread::current(), &quot;&quot;);
4526 
4527   oop oopCont = thread-&gt;cont_entry()-&gt;cont_raw();
4528 
4529   assert (!java_lang_Continuation::done(oopCont), &quot;&quot;);
4530 
4531   assert (oopCont == get_continuation(thread), &quot;&quot;);
4532 
4533   assert (verify_continuation&lt;1&gt;(oopCont), &quot;&quot;);
4534   ContMirror cont(thread, oopCont);
4535   log_develop_debug(jvmcont)(&quot;THAW #&quot; INTPTR_FORMAT &quot; &quot; INTPTR_FORMAT, cont.hash(), p2i((oopDesc*)oopCont));
4536 
4537   cont.read_minimal();
4538 
4539 #ifdef ASSERT
4540   set_anchor_to_entry(thread, cont.entry());
4541   print_frames(thread);
4542 #endif
4543 
4544   intptr_t* sp;
4545   if (UNLIKELY(cont.is_flag(FLAG_SAFEPOINT_YIELD))) {
4546     int java_frame_count = maybe_count_Java_frames(cont, kind);
4547     sp = cont_thaw&lt;mode_slow&gt;(thread, cont, kind);
4548     if (kind == thaw_top) post_JVMTI_continue(thread, cont, sp, java_frame_count);
4549   } else if (LIKELY(can_thaw_fast(cont))) {
4550     sp = cont_thaw&lt;mode_fast&gt;(thread, cont, kind);
4551   } else {
4552     int java_frame_count = maybe_count_Java_frames(cont, kind);
4553     sp = cont_thaw&lt;mode_slow&gt;(thread, cont, kind);
4554     if (kind == thaw_top) post_JVMTI_continue(thread, cont, sp, java_frame_count);
4555   }
4556 
4557   thread-&gt;reset_held_monitor_count();
4558 
4559   assert (verify_continuation&lt;2&gt;(cont.mirror()), &quot;&quot;);
4560 
4561 #ifndef PRODUCT
4562   intptr_t* sp0 = sp;
4563   address pc0 = *(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET);
4564   if (pc0 == StubRoutines::cont_interpreter_forced_preempt_return()) {
4565     sp0 += frame_metadata; // see push_interpreter_return_frame
4566   }
4567   set_anchor(thread, sp0);
4568   print_frames(thread, tty); // must be done after write(), as frame walking reads fields off the Java objects.
4569   if (LoomVerifyAfterThaw) {
4570     intptr_t *v = 0;
4571     do_verify_after_thaw(thread, sp0, &amp;v);
4572   }
4573   clear_anchor(thread);
4574 #endif
4575 
4576   if (log_develop_is_enabled(Trace, jvmcont)) {
4577     log_develop_trace(jvmcont)(&quot;Jumping to frame (thaw):&quot;);
4578     frame f = sp_to_frame(sp);
4579     print_vframe(f, NULL);
4580   }
4581 
4582 #if CONT_JFR
4583   cont.post_jfr_event(&amp;event, thread);
4584 #endif
4585 
4586   assert (thread-&gt;cont_entry()-&gt;argsize() == 0 || Continuation::is_return_barrier_entry(*(address*)(thread-&gt;cont_entry()-&gt;bottom_sender_sp() - SENDER_SP_RET_ADDRESS_OFFSET)), &quot;&quot;);
4587   assert (verify_continuation&lt;3&gt;(cont.mirror()), &quot;&quot;);
4588   log_develop_debug(jvmcont)(&quot;=== End of thaw #&quot; INTPTR_FORMAT, cont.hash());
4589 
4590   return sp;
4591 }
4592 
4593 class ThawVerifyOopsClosure: public OopClosure {
4594   intptr_t* _sp;
4595   bool     _ok;
4596 public:
4597   ThawVerifyOopsClosure(intptr_t* sp) : _sp(sp), _ok(true) { }
4598   bool ok() { return _ok; }
4599 
4600   void reset() { _ok = true; }
4601 
4602   virtual void do_oop(oop* p) {
4603     if ((intptr_t*) p &lt; _sp) {
4604       return;
4605     }
4606 
4607     if (oopDesc::is_oop_or_null(*p)) return;
4608     // Print diagnostic information before calling print_nmethod().
4609     // Assertions therein might prevent call from returning.
4610     tty-&gt;print_cr(&quot;*** non-oop &quot; PTR_FORMAT &quot; found at &quot; PTR_FORMAT,
4611                   p2i(*p), p2i(p));
4612     if (_ok) {
4613       _ok = false;
4614     }
4615     assert(false, &quot;&quot;);
4616   }
4617   virtual void do_oop(narrowOop* p) {
4618     if ((intptr_t*) p &lt; _sp) {
4619       return;
4620     }
4621 
4622     oop obj = NativeAccess&lt;&gt;::oop_load(p);
4623     if (oopDesc::is_oop_or_null(obj)) return;
4624 
4625     tty-&gt;print_cr(&quot;*** (narrow) non-oop %x found at &quot; PTR_FORMAT,
4626                   *p, p2i(p));
4627     if (_ok) {
4628       _ok = false;
4629     }
4630     assert(false, &quot;&quot;);
4631   }
4632 };
4633 
4634 void do_verify_after_thaw(JavaThread* thread, intptr_t* sp, intptr_t **rbp_pos) {
4635   ThawVerifyOopsClosure cl(sp);
4636   // Traverse the execution stack
4637   int i = 0;
4638   StackFrameStream fst(thread);
4639   frame::update_map_with_saved_link(fst.register_map(), rbp_pos);
4640 
4641   for (; !fst.is_done(); fst.next()) {
4642     fst.current()-&gt;oops_do(&amp;cl, NULL, fst.register_map());
4643     if (!cl.ok()) {
4644       frame fr = fst.current();
4645       tty-&gt;print_cr(&quot;Failed for frame %d, pc: %p, sp: %p, fp: %p&quot;, i, fr.pc(), fr.unextended_sp(), fr.fp());
4646       cl.reset();
4647     }
4648   }
4649 }
4650 
4651 JRT_LEAF(intptr_t*, Continuation::thaw_leaf(JavaThread* thread, int kind))
4652   // TODO: JRT_LEAF and NoHandleMark is problematic for JFR events.
4653   // vFrameStreamCommon allocates Handles in RegisterMap for continuations.
4654   // JRT_ENTRY instead?
4655   ResetNoHandleMark rnhm;
4656 
4657   intptr_t* sp = thaw0(thread, (thaw_kind)kind);
4658   // clear_anchor(thread);
4659   return sp;
4660 JRT_END
4661 
4662 JRT_ENTRY(intptr_t*, Continuation::thaw(JavaThread* thread, int kind))
4663   intptr_t* sp = thaw0(thread, (thaw_kind)kind);
4664   set_anchor(thread, sp); // we&#39;re in a full transition that expects last_java_frame
4665   return sp;
4666 JRT_END
4667 
4668 bool Continuation::is_continuation_enterSpecial(const frame&amp; f, const RegisterMap* map) {
4669   Method* m = (map-&gt;in_cont() &amp;&amp; f.is_interpreted_frame()) ? Continuation::interpreter_frame_method(f, map)
4670                                                            : Frame::frame_method(f);
4671   if (m != NULL) {
4672     return m-&gt;is_continuation_enter_intrinsic();
4673   }
4674 
4675   return false;
4676 }
4677 
4678 bool Continuation::is_continuation_entry_frame(const frame&amp; f, const RegisterMap* map) {
4679   Method* m = (map-&gt;in_cont() &amp;&amp; f.is_interpreted_frame()) ? Continuation::interpreter_frame_method(f, map)
4680                                                            : Frame::frame_method(f);
4681   if (m == NULL)
4682     return false;
4683 
4684   // we can do this because the entry frame is never inlined
4685   return m-&gt;intrinsic_id() == vmIntrinsics::_Continuation_enter;
4686 }
4687 
4688 // bool Continuation::is_cont_post_barrier_entry_frame(const frame&amp; f) {
4689 //   return is_return_barrier_entry(Frame::real_pc(f));
4690 // }
4691 
4692 // When walking the virtual stack, this method returns true
4693 // iff the frame is a thawed continuation frame whose
4694 // caller is still frozen on the h-stack.
4695 // The continuation object can be extracted from the thread.
4696 bool Continuation::is_cont_barrier_frame(const frame&amp; f) {
4697 #ifdef CONT_DOUBLE_NOP
4698   #ifdef ASSERT
4699     if (!f.is_interpreted_frame()) return is_return_barrier_entry(slow_return_pc(f));
4700   #endif
4701 #endif
4702   assert (f.is_interpreted_frame() || f.cb() != NULL, &quot;&quot;);
4703   return is_return_barrier_entry(f.is_interpreted_frame() ? Interpreted::return_pc(f) : Compiled::return_pc(f));
4704 }
4705 
4706 bool Continuation::is_return_barrier_entry(const address pc) {
4707   return pc == StubRoutines::cont_returnBarrier();
4708 }
4709 
4710 static inline bool is_sp_in_continuation(ContinuationEntry* cont, intptr_t* const sp) {
4711   // tty-&gt;print_cr(&quot;&gt;&gt;&gt;&gt; is_sp_in_continuation cont: %p sp: %p entry: %p in: %d&quot;, (oopDesc*)cont, sp, java_lang_Continuation::entrySP(cont), java_lang_Continuation::entrySP(cont) &gt; sp);
4712   return cont-&gt;entry_sp() &gt; sp;
4713 }
4714 
4715 bool Continuation::is_frame_in_continuation(ContinuationEntry* cont, const frame&amp; f) {
4716   return is_sp_in_continuation(cont, f.unextended_sp());
4717 }
4718 
4719 static ContinuationEntry* get_continuation_entry_for_frame(JavaThread* thread, intptr_t* const sp) {
4720   ContinuationEntry* cont = thread-&gt;cont_entry();
4721   while (cont != NULL &amp;&amp; !is_sp_in_continuation(cont, sp)) {
4722     cont = cont-&gt;parent();
4723   }
4724   // if (cont != NULL) tty-&gt;print_cr(&quot;&gt;&gt;&gt; get_continuation_entry_for_frame: %p entry.sp: %p oop: %p&quot;, sp, cont-&gt;entry_sp(), (oopDesc*)cont-&gt;continuation());
4725   return cont;
4726 }
4727 
4728 static oop get_continuation_for_frame(JavaThread* thread, intptr_t* const sp) {
4729   ContinuationEntry* cont = get_continuation_entry_for_frame(thread, sp);
4730   return cont != NULL ? cont-&gt;continuation() : (oop)NULL;
4731 }
4732 
4733 oop Continuation::get_continutation_for_frame(JavaThread* thread, const frame&amp; f) {
4734   return get_continuation_for_frame(thread, f.unextended_sp());
4735 }
4736 
4737 ContinuationEntry* Continuation::get_continuation_entry_for_continuation(JavaThread* thread, oop cont) {
4738   if (thread == NULL || cont == (oop)NULL) return NULL;
4739   
4740   for (ContinuationEntry* entry = thread-&gt;cont_entry(); entry != NULL; entry = entry-&gt;parent()) {
4741     if (cont == entry-&gt;continuation()) return entry;
4742   }
4743   return NULL;
4744 }
4745 
4746 bool Continuation::is_frame_in_continuation(JavaThread* thread, const frame&amp; f) {
4747   return get_continuation_entry_for_frame(thread, f.unextended_sp()) != NULL;
4748 }
4749 
4750 bool Continuation::is_mounted(JavaThread* thread, oop cont_scope) {
4751   guarantee (thread-&gt;has_last_Java_frame(), &quot;&quot;);
4752   for (ContinuationEntry* entry = thread-&gt;cont_entry(); entry != NULL; entry = entry-&gt;parent()) {
4753     if (cont_scope == java_lang_Continuation::scope(entry-&gt;continuation()))
4754       return true;
4755   }
4756   return false;
4757 }
4758 
4759 bool Continuation::fix_continuation_bottom_sender(JavaThread* thread, const frame&amp; callee, address* sender_pc, intptr_t** sender_sp) {
4760   if (thread != NULL &amp;&amp; is_return_barrier_entry(*sender_pc)) {
4761     ContinuationEntry* cont = get_continuation_entry_for_frame(thread, callee.is_interpreted_frame() ? callee.interpreter_frame_last_sp() : callee.unextended_sp());
4762     assert (cont != NULL, &quot;callee.unextended_sp(): &quot; INTPTR_FORMAT, p2i(callee.unextended_sp()));
4763 
4764     log_develop_debug(jvmcont)(&quot;fix_continuation_bottom_sender: [%ld] [%ld]&quot;, java_tid(thread), (long) thread-&gt;osthread()-&gt;thread_id());
4765     log_develop_trace(jvmcont)(&quot;fix_continuation_bottom_sender: sender_pc: &quot; INTPTR_FORMAT &quot; -&gt; &quot; INTPTR_FORMAT, p2i(*sender_pc), p2i(cont-&gt;entry_pc()));
4766     log_develop_trace(jvmcont)(&quot;fix_continuation_bottom_sender: sender_sp: &quot; INTPTR_FORMAT &quot; -&gt; &quot; INTPTR_FORMAT, p2i(*sender_sp), p2i(cont-&gt;entry_sp()));
4767     // log_develop_trace(jvmcont)(&quot;fix_continuation_bottom_sender callee:&quot;); if (log_develop_is_enabled(Debug, jvmcont)) callee.print_value_on(tty, thread);
4768 
4769     *sender_pc = cont-&gt;entry_pc();
4770     *sender_sp = cont-&gt;entry_sp();
4771 
4772     // We DO NOT want to fix FP. It could contain an oop that has changed on the stack, and its location should be OK anyway
4773     // log_develop_trace(jvmcont)(&quot;fix_continuation_bottom_sender: sender_fp: &quot; INTPTR_FORMAT &quot; -&gt; &quot; INTPTR_FORMAT, p2i(*sender_fp), p2i(cont-&gt;entry_fp()));
4774     // *sender_fp = cont-&gt;entry_fp();
4775 
4776     return true;
4777   }
4778   return false;
4779 }
4780 
4781 address Continuation::get_top_return_pc_post_barrier(JavaThread* thread, address pc) {
4782   ContinuationEntry* cont;
4783   if (thread != NULL &amp;&amp; is_return_barrier_entry(pc) &amp;&amp; (cont = thread-&gt;cont_entry()) != NULL) {
4784     pc = cont-&gt;entry_pc();
4785   }
4786   return pc;
4787 }
4788 
4789 bool Continuation::is_scope_bottom(oop cont_scope, const frame&amp; f, const RegisterMap* map) {
4790   if (cont_scope == NULL || !is_continuation_entry_frame(f, map))
4791     return false;
4792 
4793   assert (!map-&gt;in_cont(), &quot;&quot;);
4794   // if (map-&gt;in_cont()) return false;
4795 
4796   oop cont = get_continuation_for_frame(map-&gt;thread(), f.sp());
4797   if (cont == NULL)
4798     return false;
4799 
4800   oop sc = continuation_scope(cont);
4801   assert(sc != NULL, &quot;&quot;);
4802   return sc == cont_scope;
4803 }
4804 
4805 static frame chunk_top_frame_pd(oop chunk, intptr_t* sp);
4806 
4807 static frame chunk_top_frame(oop chunk) {
4808   intptr_t* sp = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk) + jdk_internal_misc_StackChunk::sp(chunk);
4809   // tty-&gt;print_cr(&quot;&gt;&gt;&gt; chunk_top_frame usp: %p&quot;, sp);
4810   return chunk_top_frame_pd(chunk, sp);
4811 }
4812 
4813 static frame continuation_body_top_frame(ContMirror&amp; cont, RegisterMap* map) {
4814   hframe hf = cont.last_frame&lt;mode_slow&gt;(); // here mode_slow merely makes the fewest assumptions
4815   assert (!hf.is_empty(), &quot;&quot;);
4816 
4817   // tty-&gt;print_cr(&quot;&gt;&gt;&gt;&gt; continuation_top_frame&quot;);
4818   // hf.print_on(cont, tty);
4819 
4820   if (map-&gt;update_map() &amp;&amp; !hf.is_interpreted_frame()) { // TODO : what about forced preemption? see `if (callee_safepoint_stub != NULL)` in thaw_java_frame
4821     frame::update_map_with_saved_link(map, reinterpret_cast&lt;intptr_t**&gt;(-1));
4822   }
4823 
4824   return hf.to_frame(cont);
4825 }
4826 
4827 static frame continuation_top_frame(oop contOop, RegisterMap* map) {
4828   // if (!oopDesc::equals(map-&gt;cont(), contOop))
4829   map-&gt;set_cont(contOop);
4830   ContMirror cont(map); // cont(NULL, contOop);
4831 
4832   for (oop chunk = cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
4833     if (!ContMirror::is_empty_chunk(chunk)) {
4834       map-&gt;set_in_cont(true, true);
4835       return chunk_top_frame(chunk);
4836     }
4837   }
4838 
4839   map-&gt;set_in_cont(true, false);
4840   return continuation_body_top_frame(cont, map);
4841 }
4842 
4843 static frame continuation_parent_frame(ContMirror&amp; cont, RegisterMap* map) {
4844   assert (map-&gt;thread() != NULL || !cont.is_mounted(), &quot;map-&gt;thread() == NULL: %d cont.is_mounted(): %d&quot;, map-&gt;thread() == NULL, cont.is_mounted());
4845 
4846   // if (map-&gt;thread() == NULL) { // When a continuation is mounted, its entry frame is always on the v-stack
4847   //   oop parentOop = java_lang_Continuation::parent(cont.mirror());
4848   //   if (parentOop != NULL) {
4849   //     // tty-&gt;print_cr(&quot;continuation_parent_frame: parent&quot;);
4850   //     return continuation_top_frame(parentOop, map);
4851   //   }
4852   // }
4853 
4854   oop parent = java_lang_Continuation::parent(cont.mirror());
4855   map-&gt;set_cont(parent);
4856   map-&gt;set_in_cont(false, false); // TODO parent != (oop)NULL; consider getting rid of set_in_cont altogether
4857 
4858   if (!cont.is_mounted()) { // When we&#39;re walking an unmounted continuation and reached the end
4859     // tty-&gt;print_cr(&quot;continuation_parent_frame: no more&quot;);
4860     return frame();
4861   }
4862 
4863   frame sender(cont.entrySP(), cont.entryFP(), cont.entryPC());
4864 
4865   // tty-&gt;print_cr(&quot;continuation_parent_frame&quot;);
4866   // print_vframe(sender, map, NULL);
4867 
4868   return sender;
4869 }
4870 
4871 frame Continuation::last_frame(Handle continuation, RegisterMap *map) {
4872   assert(map != NULL, &quot;a map must be given&quot;);
4873   map-&gt;set_cont(continuation); // set handle
4874   return continuation_top_frame(continuation(), map);
4875 }
4876 
4877 bool Continuation::has_last_Java_frame(Handle continuation) {
4878   return java_lang_Continuation::pc(continuation()) != NULL;
4879 }
4880 
4881 javaVFrame* Continuation::last_java_vframe(Handle continuation, RegisterMap *map) {
4882   assert(map != NULL, &quot;a map must be given&quot;);
4883   frame f = last_frame(continuation, map);
4884   for (vframe* vf = vframe::new_vframe(&amp;f, map, NULL); vf; vf = vf-&gt;sender()) {
4885     if (vf-&gt;is_java_frame()) return javaVFrame::cast(vf);
4886   }
4887   return NULL;
4888 }
4889 
4890 frame Continuation::top_frame(const frame&amp; callee, RegisterMap* map) {
4891   oop cont = get_continuation_for_frame(map-&gt;thread(), callee.sp());
4892   return continuation_top_frame(cont, map);
4893 }
4894 
4895 static frame sender_for_frame(const frame&amp; f, RegisterMap* map) {
4896   ContMirror cont(map);
4897 
4898   oop chunk = cont.find_chunk(f.unextended_sp());
4899   if (chunk != (oop)NULL) {
4900     if (ContMirror::is_in_chunk(chunk, f.unextended_sp() + f.cb()-&gt;frame_size())) {
4901       map-&gt;set_in_cont(false, false); // to prevent infinite recursion
4902       frame sender = f.sender(map);
4903       map-&gt;set_in_cont(true, true);
4904       // tty-&gt;print_cr(&quot;&gt;&gt;&gt; sender_for_frame usp: %p&quot;, sender.unextended_sp());
4905       assert (ContMirror::is_usable_in_chunk(chunk, sender.unextended_sp()), &quot;&quot;);
4906       return sender;
4907     }
4908     chunk = jdk_internal_misc_StackChunk::parent(chunk);
4909     if (chunk != (oop)NULL) {
4910       assert (!ContMirror::is_empty_chunk(chunk), &quot;&quot;);
4911       return chunk_top_frame(chunk);
4912     }
4913     assert (map-&gt;in_cont(), &quot;&quot;);
4914     if (map-&gt;in_chunk()) map-&gt;set_in_cont(true, false);
4915     assert (!map-&gt;in_chunk(), &quot;&quot;);
4916     return continuation_body_top_frame(cont, map);
4917   }
4918 
4919   hframe hf = cont.from_frame(f);
4920   hframe sender = hf.sender&lt;mode_slow&gt;(cont);
4921 
4922   // tty-&gt;print_cr(&quot;&gt;&gt;&gt;&gt; sender_for_frame&quot;);
4923   // sender.print_on(cont, tty);
4924 
4925   if (map-&gt;update_map()) {
4926     if (sender.is_empty()) {
4927       ContinuationHelper::update_register_map_for_entry_frame(cont, map);
4928     } else { // if (!sender.is_interpreted_frame())
4929       if (is_stub(f.cb())) {
4930         f.oop_map()-&gt;update_register_map(&amp;f, map); // we have callee-save registers in this case
4931       }
4932       ContinuationHelper::update_register_map(map, sender, cont);
4933     }
4934   }
4935 
4936   if (!sender.is_empty()) {
4937     return sender.to_frame(cont);
4938   } else {
4939     log_develop_trace(jvmcont)(&quot;sender_for_frame: continuation_parent_frame&quot;);
4940     return continuation_parent_frame(cont, map);
4941   }
4942 }
4943 
4944 frame Continuation::sender_for_interpreter_frame(const frame&amp; callee, RegisterMap* map) {
4945   return sender_for_frame(callee, map);
4946 }
4947 
4948 frame Continuation::sender_for_compiled_frame(const frame&amp; callee, RegisterMap* map) {
4949   return sender_for_frame(callee, map);
4950 }
4951 
4952 int Continuation::frame_size(const frame&amp; f, const RegisterMap* map) {
4953   if (map-&gt;in_cont()) {
4954     ContMirror cont(map);
4955     hframe hf = cont.from_frame(f);
4956     return (hf.is_interpreted_frame() ? hf.interpreted_frame_size() : hf.compiled_frame_size()) &gt;&gt; LogBytesPerWord;
4957   } else {
4958     assert (Continuation::is_cont_barrier_frame(f), &quot;&quot;);
4959     return (f.is_interpreted_frame() ? ((address)Interpreted::frame_bottom(f) - (address)f.sp()) : NonInterpretedUnknown::size(f)) &gt;&gt; LogBytesPerWord;
4960   }
4961 }
4962 
4963 class OopIndexClosure : public OopMapClosure {
4964 private:
4965   int _i;
4966   int _index;
4967 
4968   int _offset;
4969   VMReg _reg;
4970 
4971 public:
4972   OopIndexClosure(int offset) : _i(0), _index(-1), _offset(offset), _reg(VMRegImpl::Bad()) {}
4973   OopIndexClosure(VMReg reg)  : _i(0), _index(-1), _offset(-1), _reg(reg) {}
4974 
4975   int index() { return _index; }
4976   int is_oop() { return _index &gt;= 0; }
4977 
4978   bool handle_type(OopMapValue::oop_types type) {
4979     return type == OopMapValue::oop_value || type == OopMapValue::narrowoop_value;
4980   }
4981   void do_value(VMReg reg, OopMapValue::oop_types type) {
4982     assert (type == OopMapValue::oop_value || type == OopMapValue::narrowoop_value, &quot;&quot;);
4983     if (reg-&gt;is_reg()) {
4984         if (_reg == reg) _index = _i;
4985     } else {
4986       int sp_offset_in_bytes = reg-&gt;reg2stack() * VMRegImpl::stack_slot_size;
4987       if (sp_offset_in_bytes == _offset) _index = _i;
4988     }
4989     _i++;
4990   }
4991 };
4992 
4993 class InterpreterOopIndexClosure : public OffsetClosure {
4994 private:
4995   int _i;
4996   int _index;
4997 
4998   int _offset;
4999 
5000 public:
5001   InterpreterOopIndexClosure(int offset) : _i(0), _index(-1), _offset(offset) {}
5002 
5003   int index() { return _index; }
5004   int is_oop() { return _index &gt;= 0; }
5005 
5006   void offset_do(int offset) {
5007     if (offset == _offset) _index = _i;
5008     _i++;
5009   }
5010 };
5011 
5012 // *grossly* inefficient
5013 static int find_oop_in_compiled_frame(const frame&amp; fr, const RegisterMap* map, const int usp_offset_in_bytes) {
5014   assert (fr.is_compiled_frame(), &quot;&quot;);
5015   const ImmutableOopMap* oop_map = fr.oop_map();
5016   assert (oop_map != NULL, &quot;&quot;);
5017   OopIndexClosure ioc(usp_offset_in_bytes);
5018   oop_map-&gt;all_type_do(&amp;fr, &amp;ioc);
5019   return ioc.index();
5020 }
5021 
5022 static int find_oop_in_compiled_frame(const frame&amp; fr, const RegisterMap* map, VMReg reg) {
5023   assert (fr.is_compiled_frame(), &quot;&quot;);
5024   const ImmutableOopMap* oop_map = fr.oop_map();
5025   assert (oop_map != NULL, &quot;&quot;);
5026   OopIndexClosure ioc(reg);
5027   oop_map-&gt;all_type_do(&amp;fr, &amp;ioc);
5028   return ioc.index();
5029 }
5030 
5031 static int find_oop_in_interpreted_frame(const hframe&amp; hf, int offset, const InterpreterOopMap&amp; mask, const ContMirror&amp; cont) {
5032   // see void frame::oops_interpreted_do
5033   InterpreterOopIndexClosure ioc(offset);
5034   mask.iterate_oop(&amp;ioc);
5035   int res = ioc.index() + 1 + hf.interpreted_frame_num_monitors(); // index 0 is mirror; next are InterpreterOopMap::iterate_oop
5036   return res; // index 0 is mirror
5037 }
5038 
5039 address Continuation::oop_address(objArrayOop ref_stack, int ref_sp, int index) {
5040   assert (index &gt;= ref_sp &amp;&amp; index &lt; ref_stack-&gt;length(), &quot;i: %d ref_sp: %d length: %d&quot;, index, ref_sp, ref_stack-&gt;length());
5041   oop obj = ref_stack-&gt;obj_at(index); // invoke barriers
5042   address p = UseCompressedOops ? (address)ref_stack-&gt;obj_at_addr&lt;narrowOop&gt;(index)
5043                                 : (address)ref_stack-&gt;obj_at_addr&lt;oop&gt;(index);
5044 
5045   log_develop_trace(jvmcont)(&quot;oop_address: index: %d&quot;, index);
5046   // print_oop(p, obj);
5047   assert (oopDesc::is_oop_or_null(obj), &quot;invalid oop&quot;);
5048   return p;
5049 }
5050 
5051 bool Continuation::is_in_usable_stack(address addr, const RegisterMap* map) {
5052   ContMirror cont(map);
5053   oop chunk = cont.find_chunk(addr);
5054   return (chunk != (oop)NULL) ? ContMirror::is_usable_in_chunk(chunk, addr)
5055                               : (cont.is_in_stack(addr) || cont.is_in_ref_stack(addr));
5056 }
5057 
5058 // address Continuation::usp_offset_to_location(const frame&amp; fr, const RegisterMap* map, const int usp_offset_in_bytes) {
5059 //   return usp_offset_to_location(fr, map, usp_offset_in_bytes, find_oop_in_compiled_frame(fr, map, usp_offset_in_bytes) &gt;= 0);
5060 // }
5061 
5062 // if oop, it is narrow iff UseCompressedOops
5063 address Continuation::usp_offset_to_location(const frame&amp; fr, const RegisterMap* map, const int usp_offset_in_bytes, bool is_oop) {
5064   assert (fr.is_compiled_frame(), &quot;&quot;);
5065   ContMirror cont(map);
5066 
5067   assert(map-&gt;in_cont(), &quot;&quot;);
5068 
5069   if (cont.find_chunk(fr.unextended_sp()) != (oop)NULL) {
5070     assert(map-&gt;in_chunk(), &quot;&quot;);
5071     return (address)fr.unextended_sp() + usp_offset_in_bytes;
5072   }
5073 
5074   assert(!map-&gt;in_chunk(), &quot;fr.unextended_sp: &quot; INTPTR_FORMAT, p2i(fr.unextended_sp()));
5075 
5076   hframe hf = cont.from_frame(fr);
5077 
5078   intptr_t* hsp = cont.stack_address(hf.sp());
5079   address loc = (address)hsp + usp_offset_in_bytes;
5080 
5081   log_develop_trace(jvmcont)(&quot;usp_offset_to_location oop_address: stack index: %d length: %d&quot;, cont.stack_index(loc), cont.stack_length());
5082 
5083   int oop_offset = find_oop_in_compiled_frame(fr, map, usp_offset_in_bytes);
5084   assert (is_oop == (oop_offset &gt;= 0), &quot;must be&quot;);
5085   address res = is_oop ? oop_address(cont.refStack(), cont.refSP(), hf.ref_sp() + oop_offset) : loc;
5086   return res;
5087 }
5088 
5089 int Continuation::usp_offset_to_index(const frame&amp; fr, const RegisterMap* map, const int usp_offset_in_bytes) {
5090   assert (fr.is_compiled_frame() || is_stub(fr.cb()), &quot;&quot;);
5091   ContMirror cont(map);
5092 
5093   if (cont.find_chunk(fr.unextended_sp()) != (oop)NULL) {
5094     return usp_offset_in_bytes;
5095   }
5096 
5097   hframe hf = cont.from_frame(fr);
5098 
5099   intptr_t* hsp;
5100   if (usp_offset_in_bytes &gt;= 0) {
5101      hsp = cont.stack_address(hf.sp());
5102   } else {
5103     hframe stub = cont.last_frame&lt;mode_slow&gt;();
5104 
5105     assert (cont.is_flag(FLAG_SAFEPOINT_YIELD), &quot;must be&quot;);
5106     assert (is_stub(stub.cb()), &quot;must be&quot;);
5107     assert (stub.sender&lt;mode_slow&gt;(cont) == hf, &quot;must be&quot;);
5108 
5109     hsp = cont.stack_address(stub.sp()) + stub.cb()-&gt;frame_size();
5110   }
5111   address loc = (address)hsp + usp_offset_in_bytes;
5112   int index = cont.stack_index(loc);
5113 
5114   log_develop_trace(jvmcont)(&quot;usp_offset_to_location oop_address: stack index: %d length: %d&quot;, index, cont.stack_length());
5115   return index;
5116 }
5117 
5118 // address Continuation::reg_to_location(const frame&amp; fr, const RegisterMap* map, VMReg reg) {
5119 //   return reg_to_location(fr, map, reg, find_oop_in_compiled_frame(fr, map, reg) &gt;= 0);
5120 // }
5121 
5122 // address Continuation::reg_to_location(const frame&amp; fr, const RegisterMap* map, VMReg reg, bool is_oop) {
5123 //   assert (map != NULL, &quot;&quot;);
5124 //   oop cont;
5125 //   assert (map-&gt;in_cont(), &quot;&quot;);
5126 //   if (map-&gt;in_cont()) {
5127 //     cont = map-&gt;cont();
5128 //   } else {
5129 //     cont = get_continutation_for_frame(map-&gt;thread(), fr);
5130 //   }
5131 //   return reg_to_location(cont, fr, map, reg, is_oop);
5132 // }
5133 
5134 address Continuation::reg_to_location(const frame&amp; fr, const RegisterMap* map, VMReg reg, bool is_oop) {
5135   assert (map != NULL, &quot;&quot;);
5136   assert (fr.is_compiled_frame(), &quot;&quot;);
5137   assert (map-&gt;in_cont(), &quot;&quot;);
5138 
5139   // assert (!is_continuation_entry_frame(fr, map), &quot;&quot;);
5140   // if (is_continuation_entry_frame(fr, map)) {
5141   //   log_develop_trace(jvmcont)(&quot;reg_to_location continuation entry link address: &quot; INTPTR_FORMAT, p2i(map-&gt;location(reg)));
5142   //   return map-&gt;location(reg); // see sender_for_frame, `if (sender.is_empty())`
5143   // }
5144 
5145   ContMirror cont(map);
5146   if (cont.find_chunk(fr.unextended_sp()) != (oop)NULL) {
5147     return map-&gt;location(reg);
5148   }
5149 
5150   hframe hf = cont.from_frame(fr);
5151 
5152   int oop_index = find_oop_in_compiled_frame(fr, map, reg);
5153   assert (is_oop == oop_index &gt;= 0, &quot;must be&quot;);
5154 
5155   address res = NULL;
5156   if (oop_index &gt;= 0) {
5157     res = oop_address(cont.refStack(), cont.refSP(), hf.ref_sp() + find_oop_in_compiled_frame(fr, map, reg));
5158   } else {
5159   // assert ((void*)Frame::map_link_address(map) == (void*)map-&gt;location(reg), &quot;must be the link register (rbp): %s&quot;, reg-&gt;name());
5160     int index = (int)reinterpret_cast&lt;uintptr_t&gt;(map-&gt;location(reg)); // the RegisterMap should contain the link index. See sender_for_frame
5161     assert (index &gt;= 0, &quot;non-oop in fp of the topmost frame is not supported&quot;);
5162     if (index &gt;= 0) { // see frame::update_map_with_saved_link in continuation_top_frame
5163       address loc = (address)cont.stack_address(index);
5164       log_develop_trace(jvmcont)(&quot;reg_to_location oop_address: stack index: %d length: %d&quot;, index, cont.stack_length());
5165       if (oop_index &lt; 0)
5166         res = loc;
5167     }
5168   }
5169   return res;
5170 }
5171 
5172 address Continuation::interpreter_frame_expression_stack_at(const frame&amp; fr, const RegisterMap* map, const InterpreterOopMap&amp; oop_mask, int index) {
5173   assert (fr.is_interpreted_frame(), &quot;&quot;);
5174   ContMirror cont(map);
5175   hframe hf = cont.from_frame(fr);
5176 
5177   int max_locals = hf.method&lt;Interpreted&gt;()-&gt;max_locals();
5178   address loc = (address)hf.interpreter_frame_expression_stack_at(index);
5179   if (loc == NULL)
5180     return NULL;
5181 
5182   int index1 = max_locals + index; // see stack_expressions in vframe.cpp
5183   log_develop_trace(jvmcont)(&quot;interpreter_frame_expression_stack_at oop_address: stack index: %d, length: %d exp: %d index1: %d&quot;, cont.stack_index(loc), cont.stack_length(), index, index1);
5184 
5185   address res = oop_mask.is_oop(index1)
5186     ? oop_address(cont.refStack(), cont.refSP(), hf.ref_sp() + find_oop_in_interpreted_frame(hf, index1, oop_mask, cont))
5187     : loc;
5188   return res;
5189 }
5190 
5191 address Continuation::interpreter_frame_local_at(const frame&amp; fr, const RegisterMap* map, const InterpreterOopMap&amp; oop_mask, int index) {
5192   assert (fr.is_interpreted_frame(), &quot;&quot;);
5193   ContMirror cont(map);
5194   hframe hf = cont.from_frame(fr);
5195 
5196   address loc = (address)hf.interpreter_frame_local_at(index);
5197 
5198   log_develop_trace(jvmcont)(&quot;interpreter_frame_local_at oop_address: stack index: %d length: %d local: %d&quot;, cont.stack_index(loc), cont.stack_length(), index);
5199   address res = oop_mask.is_oop(index)
5200     ? oop_address(cont.refStack(), cont.refSP(), hf.ref_sp() + find_oop_in_interpreted_frame(hf, index, oop_mask, cont))
5201     : loc;
5202   return res;
5203 }
5204 
5205 Method* Continuation::interpreter_frame_method(const frame&amp; fr, const RegisterMap* map) {
5206   assert (fr.is_interpreted_frame(), &quot;&quot;);
5207   hframe hf = ContMirror(map).from_frame(fr);
5208   return hf.method&lt;Interpreted&gt;();
5209 }
5210 
5211 address Continuation::interpreter_frame_bcp(const frame&amp; fr, const RegisterMap* map) {
5212   assert (fr.is_interpreted_frame(), &quot;&quot;);
5213   hframe hf = ContMirror(map).from_frame(fr);
5214   return hf.interpreter_frame_bcp();
5215 }
5216 
5217 oop Continuation::continuation_scope(oop cont) {
5218   return cont != NULL ? java_lang_Continuation::scope(cont) : (oop)NULL;
5219 }
5220 
5221 ///// Allocation
5222 
5223 template &lt;typename ConfigT&gt;
5224 void ContMirror::make_keepalive(Thread* thread, CompiledMethodKeepalive&lt;ConfigT&gt;* keepalive) {
5225   Handle conth(thread, _cont);
5226   int oops = keepalive-&gt;nr_oops();
5227   if (oops == 0) {
5228     oops = 1;
5229   }
5230   oop keepalive_obj = allocate_keepalive_array(oops);
5231 
5232   uint64_t counter = SafepointSynchronize::safepoint_counter();
5233   // check gc cycle
5234   Handle keepaliveHandle = Handle(thread, keepalive_obj);
5235   keepalive-&gt;set_handle(keepaliveHandle);
5236   // check gc cycle and maybe reload
5237   //if (!SafepointSynchronize::is_same_safepoint(counter)) {
5238     post_safepoint(conth);
5239   //}
5240 }
5241 
5242 template &lt;typename ConfigT&gt;
5243 inline bool ContMirror::allocate_stacks(int size, int oops, int frames) {
5244   bool needs_stack_allocation    = (_stack == NULL || to_index(size) &gt; (_sp &gt;= 0 ? _sp : _stack_length));
5245   bool needs_refStack_allocation = (_ref_stack == NULL || oops &gt; _ref_sp);
5246 
5247   log_develop_trace(jvmcont)(&quot;stack size: %d (int): %d sp: %d stack_length: %d needs alloc: %d&quot;, size, to_index(size), _sp, _stack_length, needs_stack_allocation);
5248   log_develop_trace(jvmcont)(&quot;num_oops: %d ref_sp: %d needs alloc: %d&quot;, oops, _ref_sp, needs_stack_allocation);
5249 
5250   assert(_sp == java_lang_Continuation::sp(_cont) &amp;&amp; _fp == java_lang_Continuation::fp(_cont) &amp;&amp; _pc == java_lang_Continuation::pc(_cont), &quot;&quot;);
5251 
5252   if (!(needs_stack_allocation | needs_refStack_allocation))
5253     return false;
5254 
5255 #ifdef PERFTEST
5256   if (PERFTEST_LEVEL &lt; 100) {
5257     tty-&gt;print_cr(&quot;stack size: %d (int): %d sp: %d stack_length: %d needs alloc: %d&quot;, size, to_index(size), _sp, _stack_length, needs_stack_allocation);
5258     tty-&gt;print_cr(&quot;num_oops: %d ref_sp: %d needs alloc: %d&quot;, oops, _ref_sp, needs_stack_allocation);
5259   }
5260   guarantee(PERFTEST_LEVEL &gt;= 100, &quot;&quot;);
5261 #endif
5262 
5263   if (!allocate_stacks_in_native&lt;ConfigT&gt;(size, oops, needs_stack_allocation, needs_refStack_allocation)) {
5264     guarantee(false, &quot;&quot;);
5265     allocate_stacks_in_java(size, oops, frames);
5266     if (!thread()-&gt;has_pending_exception()) return true;
5267   }
5268 
5269   // This first assertion isn&#39;t important, as we&#39;ll overwrite the Java-computed ones, but it&#39;s just to test that the Java computation is OK.
5270   assert(_sp == java_lang_Continuation::sp(_cont) &amp;&amp; _fp == java_lang_Continuation::fp(_cont) &amp;&amp; _pc == java_lang_Continuation::pc(_cont), &quot;&quot;);
5271   assert (_stack == java_lang_Continuation::stack(_cont), &quot;&quot;);
5272   assert (_stack-&gt;base(basicElementType) == _hstack, &quot;&quot;);
5273   assert (to_bytes(_stack_length) &gt;= size &amp;&amp; to_bytes(_sp) &gt;= size, &quot;stack_length: %d sp: %d size: %d&quot;, to_bytes(_stack_length), _sp, size);
5274   assert (to_bytes(_ref_sp) &gt;= oops, &quot;oops: %d ref_sp: %d refStack length: %d&quot;, oops, _ref_sp, _ref_stack-&gt;length());
5275   return true;
5276 }
5277 
5278 template &lt;typename ConfigT&gt;
5279 NOINLINE bool ContMirror::allocate_stacks_in_native(int size, int oops, bool needs_stack, bool needs_refstack) {
5280   if (needs_stack) {
5281     if (_stack == NULL) {
5282       if (!allocate_stack(size)) {
5283         return false;
5284       }
5285     } else {
5286       if (!grow_stack(size)) {
5287         return false;
5288       }
5289     }
5290 
5291     java_lang_Continuation::set_stack(_cont, _stack);
5292     // maybe we&#39;ll need to retry freeze
5293     java_lang_Continuation::set_sp(_cont, _sp);
5294     java_lang_Continuation::set_fp(_cont, _fp);
5295   }
5296 
5297   if (needs_refstack) {
5298     if (_ref_stack == NULL) {
5299       if (!allocate_ref_stack&lt;ConfigT::_post_barrier&gt;(oops)) {
5300         return false;
5301       }
5302     } else {
5303       if (!grow_ref_stack&lt;ConfigT&gt;(oops)) {
5304         return false;
5305       }
5306     }
5307     java_lang_Continuation::set_refStack(_cont, _ref_stack);
5308     // maybe we&#39;ll need to retry freeze:
5309     java_lang_Continuation::set_refSP(_cont, _ref_sp);
5310   }
5311 
5312   return true;
5313 }
5314 
5315 bool ContMirror::allocate_stack(int size) {
5316   int elements = size &gt;&gt; LogBytesPerElement;
5317   oop result = allocate_stack_array(elements);
5318   if (result == NULL) {
5319     return false;
5320   }
5321 
5322   _stack = typeArrayOop(result);
5323   _sp = elements;
5324   _stack_length = elements;
5325   _hstack = (ElemType*)_stack-&gt;base(basicElementType);
5326 
5327   return true;
5328 }
5329 
5330 bool ContMirror::grow_stack(int new_size) {
5331   new_size = new_size &gt;&gt; LogBytesPerElement;
5332 
5333   int old_length = _stack_length;
5334   int offset = _sp &gt;= 0 ? _sp : old_length;
5335   int min_length = (old_length - offset) + new_size;
5336   assert (min_length &gt; old_length, &quot;&quot;);
5337 
5338   int new_length = ensure_capacity(old_length, min_length);
5339   if (new_length == -1) {
5340     return false;
5341   }
5342 
5343   typeArrayOop new_stack = allocate_stack_array(new_length);
5344   if (new_stack == NULL) {
5345     return false;
5346   }
5347 
5348   log_develop_trace(jvmcont)(&quot;grow_stack old_length: %d new_length: %d&quot;, old_length, new_length);
5349   ElemType* new_hstack = (ElemType*)new_stack-&gt;base(basicElementType);
5350   int n = old_length - offset;
5351   assert(new_length &gt; n, &quot;&quot;);
5352   if (n &gt; 0) {
5353     copy_primitive_array(_stack, offset, new_stack, new_length - n, n);
5354   }
5355   _stack = new_stack;
5356   _stack_length = new_length;
5357   _hstack = new_hstack;
5358 
5359   log_develop_trace(jvmcont)(&quot;grow_stack old sp: %d fp: %ld&quot;, _sp, _fp);
5360   _sp = fix_decreasing_index(_sp, old_length, new_length);
5361   if (is_flag(FLAG_LAST_FRAME_INTERPRETED)) { // if (Interpreter::contains(_pc)) {// only interpreter frames use relative (index) fp
5362     _fp = fix_decreasing_index(_fp, old_length, new_length);
5363   }
5364   log_develop_trace(jvmcont)(&quot;grow_stack new sp: %d fp: %ld&quot;, _sp, _fp);
5365 
5366   return true;
5367 }
5368 
5369 template &lt;bool post_barrier&gt;
5370 bool ContMirror::allocate_ref_stack(int nr_oops) {
5371   // we don&#39;t zero the array because we allocate an array that exactly holds all the oops we&#39;ll fill in as we freeze
5372   oop result = allocate_refstack_array&lt;post_barrier&gt;(nr_oops);
5373   if (result == NULL) {
5374     return false;
5375   }
5376   _ref_stack = objArrayOop(result);
5377   _ref_sp = nr_oops;
5378 
5379   assert (_ref_stack-&gt;length() == nr_oops, &quot;&quot;);
5380 
5381   return true;
5382 }
5383 
5384 template &lt;typename ConfigT&gt;
5385 bool ContMirror::grow_ref_stack(int nr_oops) {
5386   int old_length = _ref_stack-&gt;length();
5387   int offset = _ref_sp &gt;= 0 ? _ref_sp : old_length;
5388   int old_oops = old_length - offset;
5389   int min_length = old_oops + nr_oops;
5390   assert (min_length &gt; old_length, &quot;&quot;);
5391 
5392   int new_length = ensure_capacity(old_length, min_length);
5393   if (new_length == -1) {
5394     guarantee(false, &quot;&quot;); // TODO handle somehow
5395     return false;
5396   }
5397 
5398   objArrayOop new_ref_stack = allocate_refstack_array&lt;ConfigT::_post_barrier&gt;(new_length);
5399   if (new_ref_stack == NULL) {
5400     return false;
5401   }
5402   assert (new_ref_stack-&gt;length() == new_length, &quot;&quot;);
5403   log_develop_trace(jvmcont)(&quot;grow_ref_stack old_length: %d new_length: %d&quot;, old_length, new_length);
5404 
5405   zero_ref_array&lt;ConfigT::_post_barrier&gt;(new_ref_stack, new_length, min_length);
5406   if (old_oops &gt; 0) {
5407     assert(!FULL_STACK, &quot;&quot;);
5408     copy_ref_array&lt;ConfigT&gt;(_ref_stack, offset, new_ref_stack, fix_decreasing_index(offset, old_length, new_length), old_oops);
5409   }
5410 
5411   _ref_stack = new_ref_stack;
5412 
5413   log_develop_trace(jvmcont)(&quot;grow_ref_stack old ref_sp: %d&quot;, _ref_sp);
5414   _ref_sp = fix_decreasing_index(_ref_sp, old_length, new_length);
5415   log_develop_trace(jvmcont)(&quot;grow_ref_stack new ref_sp: %d&quot;, _ref_sp);
5416   return true;
5417 }
5418 
5419 int ContMirror::ensure_capacity(int old, int min) {
5420   int newsize = old + (old &gt;&gt; 1);
5421   if (newsize - min &lt;= 0) {
5422     if (min &lt; 0) { // overflow
5423       return -1;
5424     }
5425     return min;
5426   }
5427   return newsize;
5428 }
5429 
5430 int ContMirror::fix_decreasing_index(int index, int old_length, int new_length) {
5431   return new_length - (old_length - index);
5432 }
5433 
5434 inline void ContMirror::post_safepoint(Handle conth) {
5435   _cont = conth(); // reload oop
5436   _tail = java_lang_Continuation::tail(_cont);
5437   _ref_stack = java_lang_Continuation::refStack(_cont);
5438   _stack = java_lang_Continuation::stack(_cont);
5439   _hstack = (ElemType*)_stack-&gt;base(basicElementType);
5440 }
5441 
5442 inline void ContMirror::post_safepoint_minimal(Handle conth) {
5443   _cont = conth(); // reload oop
5444   _tail = java_lang_Continuation::tail(_cont);
5445   assert(_ref_stack == (oop)NULL, &quot;&quot;);
5446   assert(_stack == (oop)NULL, &quot;&quot;);
5447   assert(_hstack == NULL, &quot;&quot;);
5448 }
5449 
5450 typeArrayOop ContMirror::allocate_stack_array(size_t elements) {
5451   assert(elements &gt; 0, &quot;&quot;);
5452   log_develop_trace(jvmcont)(&quot;allocate_stack_array elements: %lu&quot;, elements);
5453 
5454   TypeArrayKlass* klass = TypeArrayKlass::cast(Universe::intArrayKlassObj());
5455   size_t size_in_words = typeArrayOopDesc::object_size(klass, (int)elements);
5456   return typeArrayOop(raw_allocate(klass, size_in_words, elements, false));
5457 }
5458 
5459 void ContMirror::copy_primitive_array(typeArrayOop old_array, int old_start, typeArrayOop new_array, int new_start, int count) {
5460   ElemType* from = (ElemType*)old_array-&gt;base(basicElementType) + old_start;
5461   ElemType* to   = (ElemType*)new_array-&gt;base(basicElementType) + new_start;
5462   size_t size = to_bytes(count);
5463   memcpy(to, from, size);
5464 
5465   //Copy::conjoint_memory_atomic(from, to, size); // Copy::disjoint_words((HeapWord*)from, (HeapWord*)to, size/wordSize); //
5466   // ArrayAccess&lt;ARRAYCOPY_DISJOINT&gt;::oop_arraycopy(_stack, offset * elementSizeInBytes, new_stack, (new_length - n) * elementSizeInBytes, n);
5467 }
5468 
5469 template &lt;bool post_barrier&gt;
5470 objArrayOop ContMirror::allocate_refstack_array(size_t nr_oops) {
5471   assert(nr_oops &gt; 0, &quot;&quot;);
5472   bool zero = !post_barrier; // !BarrierSet::barrier_set()-&gt;is_a(BarrierSet::ModRef);
5473   log_develop_trace(jvmcont)(&quot;allocate_refstack_array nr_oops: %lu zero: %d&quot;, nr_oops, zero);
5474 
5475   ArrayKlass* klass = ArrayKlass::cast(Universe::objectArrayKlassObj());
5476   size_t size_in_words = objArrayOopDesc::object_size((int)nr_oops);
5477   return objArrayOop(raw_allocate(klass, size_in_words, nr_oops, zero));
5478 }
5479 
5480 objArrayOop ContMirror::allocate_keepalive_array(size_t nr_oops) {
5481   //assert(nr_oops &gt; 0, &quot;&quot;);
5482   bool zero = true; // !BarrierSet::barrier_set()-&gt;is_a(BarrierSet::ModRef);
5483   log_develop_trace(jvmcont)(&quot;allocate_keepalive_array nr_oops: %lu zero: %d&quot;, nr_oops, zero);
5484 
5485   ArrayKlass* klass = ArrayKlass::cast(Universe::objectArrayKlassObj());
5486   size_t size_in_words = objArrayOopDesc::object_size((int)nr_oops);
5487   return objArrayOop(raw_allocate(klass, size_in_words, nr_oops, zero));
5488 }
5489 
5490 
5491 template &lt;bool post_barrier&gt;
5492 void ContMirror::zero_ref_array(objArrayOop new_array, int new_length, int min_length) {
5493   assert (new_length == new_array-&gt;length(), &quot;&quot;);
5494   int extra_oops = new_length - min_length;
5495 
5496   if (post_barrier) {
5497     // zero the bottom part of the array that won&#39;t be filled in the freeze
5498     HeapWord* new_base = new_array-&gt;base();
5499     const uint OopsPerHeapWord = HeapWordSize/heapOopSize; // TODO PERF:  heapOopSize and OopsPerHeapWord can be constants in Config
5500     assert(OopsPerHeapWord &gt;= 1 &amp;&amp; (HeapWordSize % heapOopSize == 0), &quot;&quot;);
5501     uint word_size = ((uint)extra_oops + OopsPerHeapWord - 1)/OopsPerHeapWord;
5502     Copy::fill_to_aligned_words(new_base, word_size, 0); // fill_to_words (we could be filling more than the elements if narrow, but we do this before copying)
5503   }
5504 
5505   DEBUG_ONLY(for (int i=0; i&lt;extra_oops; i++) assert(new_array-&gt;obj_at(i) == (oop)NULL, &quot;&quot;);)
5506 }
5507 
5508 void ContMirror::zero_ref_stack_prefix() {
5509   if (_ref_stack != NULL &amp;&amp; _ref_sp &gt; 0) {
5510     Copy::fill_to_bytes(_ref_stack-&gt;base(), _ref_sp * heapOopSize, 0);
5511   }
5512 }
5513 
5514 
5515 template &lt;typename ConfigT&gt;
5516 void ContMirror::copy_ref_array(objArrayOop old_array, int old_start, objArrayOop new_array, int new_start, int count) {
5517   assert (new_start + count == new_array-&gt;length(), &quot;&quot;);
5518 
5519   typedef typename ConfigT::OopT OopT;
5520   if (ConfigT::_post_barrier) {
5521     OopT* from = (OopT*)old_array-&gt;base() + old_start;
5522     OopT* to   = (OopT*)new_array-&gt;base() + new_start;
5523     memcpy((void*)to, (void*)from, count * sizeof(OopT));
5524     barrier_set_cast&lt;ModRefBarrierSet&gt;(BarrierSet::barrier_set())-&gt;write_ref_array((HeapWord*)to, count);
5525   } else {
5526     // Requires the array is zeroed (see G1BarrierSet::write_ref_array_pre_work)
5527     DEBUG_ONLY(for (int i=0; i&lt;count; i++) assert(new_array-&gt;obj_at(new_start + i) == (oop)NULL, &quot;&quot;);)
5528     size_t src_offset = (size_t) objArrayOopDesc::obj_at_offset&lt;OopT&gt;(old_start);
5529     size_t dst_offset = (size_t) objArrayOopDesc::obj_at_offset&lt;OopT&gt;(new_start);
5530     ArrayAccess&lt;ARRAYCOPY_DISJOINT&gt;::oop_arraycopy(old_array, src_offset, new_array, dst_offset, count);
5531 
5532     // for (int i=0, old_i = old_start, new_i = new_start; i &lt; count; i++, old_i++, new_i++) new_array-&gt;obj_at_put(new_i, old_array-&gt;obj_at(old_i));
5533   }
5534 }
5535 
5536 /* try to allocate an array from the tlab, if it doesn&#39;t work allocate one using the allocate
5537  * method. In the later case we might have done a safepoint and need to reload our oops */
5538 oop ContMirror::raw_allocate(Klass* klass, size_t size_in_words, size_t elements, bool zero) {
5539   ObjArrayAllocator allocator(klass, size_in_words, (int)elements, zero, _thread);
5540   HeapWord* start = _thread-&gt;tlab().allocate(size_in_words);
5541   if (start != NULL) {
5542     return allocator.initialize(start);
5543   } else {
5544     //HandleMark hm(_thread);
5545     Handle conth(Thread::current(), _cont);
5546     uint64_t counter = SafepointSynchronize::safepoint_counter();
5547     oop result = allocator.allocate();
5548     //if (!SafepointSynchronize::is_same_safepoint(counter)) {
5549       post_safepoint(conth);
5550     //}
5551     return result;
5552   }
5553 }
5554 
5555 oop ContMirror::allocate_stack_chunk(int stack_size) {
5556   InstanceStackChunkKlass* klass = InstanceStackChunkKlass::cast(SystemDictionary::StackChunk_klass());
5557   int size_in_words = klass-&gt;instance_size(stack_size);
5558   StackChunkAllocator allocator(klass, size_in_words, stack_size, _thread);
5559   HeapWord* start = _thread-&gt;tlab().allocate(size_in_words);
5560   if (start != NULL) {
5561     return allocator.initialize(start);
5562   } else {
5563     assert (_thread == Thread::current(), &quot;&quot;);
5564     //HandleMark hm(_thread);
5565     Handle conth(_thread, _cont);
5566     // uint64_t counter = SafepointSynchronize::safepoint_counter();
5567     oop result = allocator.allocate();
5568     //if (!SafepointSynchronize::is_same_safepoint(counter)) {
5569       post_safepoint_minimal(conth);
5570     //}
5571     return result;
5572   }
5573 }
5574 
5575 NOINLINE void ContMirror::allocate_stacks_in_java(int size, int oops, int frames) {
5576   guarantee (false, &quot;unreachable&quot;);
5577   int old_stack_length = _stack_length;
5578 
5579   //HandleMark hm(_thread);
5580   Handle conth(_thread, _cont);
5581   JavaCallArguments args;
5582   args.push_oop(conth);
5583   args.push_int(size);
5584   args.push_int(oops);
5585   args.push_int(frames);
5586   JavaValue result(T_VOID);
5587   JavaCalls::call_virtual(&amp;result, SystemDictionary::Continuation_klass(), vmSymbols::getStacks_name(), vmSymbols::continuationGetStacks_signature(), &amp;args, _thread);
5588   post_safepoint(conth); // reload oop after java call
5589 
5590   _sp     = java_lang_Continuation::sp(_cont);
5591   _fp     = java_lang_Continuation::fp(_cont);
5592   _ref_sp = java_lang_Continuation::refSP(_cont);
5593   _stack_length = _stack-&gt;length();
5594   /* We probably should handle OOM? */
5595 }
5596 
5597 void Continuation::emit_chunk_iterate_event(oop chunk, int num_frames, int num_oops) {
5598   EventContinuationIterateOops e;
5599   if (e.should_commit()) {
5600     e.set_id(cast_from_oop&lt;u8&gt;(chunk));
5601     e.set_safepoint(SafepointSynchronize::is_at_safepoint());
5602     e.set_numFrames((u2)num_frames);
5603     e.set_numOops((u2)num_oops);
5604     e.commit();
5605   }
5606 }
5607 
5608 JVM_ENTRY(jint, CONT_isPinned0(JNIEnv* env, jobject cont_scope)) {
5609   JavaThread* thread = JavaThread::thread_from_jni_environment(env);
5610   return is_pinned0(thread, JNIHandles::resolve(cont_scope), false);
5611 }
5612 JVM_END
5613 
5614 JVM_ENTRY(jint, CONT_TryForceYield0(JNIEnv* env, jobject jcont, jobject jthread)) {
5615   JavaThread* thread = JavaThread::thread_from_jni_environment(env);
5616 
5617   class ForceYieldClosure : public HandshakeClosure {
5618     jobject _jcont;
5619     jint _result;
5620 
5621     void do_thread(Thread* th) {
5622       // assert (th == Thread::current(), &quot;&quot;); -- the handshake can be carried out by a VM thread (see HandshakeState::process_by_vmthread)
5623       assert (th-&gt;is_Java_thread(), &quot;&quot;);
5624       JavaThread* thread = (JavaThread*)th;
5625 
5626       // tty-&gt;print_cr(&quot;&gt;&gt;&gt; ForceYieldClosure thread&quot;);
5627       // thread-&gt;print_on(tty);
5628       // if (thread != Thread::current()) {
5629       //   tty-&gt;print_cr(&quot;&gt;&gt;&gt; current thread&quot;);
5630       //   Thread::current()-&gt;print_on(tty);
5631       // }
5632 
5633       oop oopCont = JNIHandles::resolve_non_null(_jcont);
5634       _result = Continuation::try_force_yield(thread, oopCont);
5635     }
5636 
5637   public:
5638     ForceYieldClosure(jobject jcont) : HandshakeClosure(&quot;ContinuationForceYieldClosure&quot;), _jcont(jcont), _result(-1) {}
5639     jint result() const { return _result; }
5640   };
5641   ForceYieldClosure fyc(jcont);
5642 
5643   // tty-&gt;print_cr(&quot;TRY_FORCE_YIELD0&quot;);
5644   // thread-&gt;print();
5645   // tty-&gt;print_cr(&quot;&quot;);
5646 
5647   oop thread_oop = JNIHandles::resolve(jthread);
5648   if (thread_oop != NULL) {
5649     JavaThread* target = java_lang_Thread::thread(thread_oop);
5650     Handshake::execute_direct(&amp;fyc, target);
5651   }
5652 
5653   return fyc.result();
5654 }
5655 JVM_END
5656 
5657 #define CC (char*)  /*cast a literal from (const char*)*/
5658 #define FN_PTR(f) CAST_FROM_FN_PTR(void*, &amp;f)
5659 
5660 static JNINativeMethod CONT_methods[] = {
5661     {CC&quot;tryForceYield0&quot;,   CC&quot;(Ljava/lang/Thread;)I&quot;,            FN_PTR(CONT_TryForceYield0)},
5662     {CC&quot;isPinned0&quot;,        CC&quot;(Ljava/lang/ContinuationScope;)I&quot;, FN_PTR(CONT_isPinned0)},
5663 };
5664 
5665 void CONT_RegisterNativeMethods(JNIEnv *env, jclass cls) {
5666     Thread* thread = Thread::current();
5667     assert(thread-&gt;is_Java_thread(), &quot;&quot;);
5668     ThreadToNativeFromVM trans((JavaThread*)thread);
5669     int status = env-&gt;RegisterNatives(cls, CONT_methods, sizeof(CONT_methods)/sizeof(JNINativeMethod));
5670     guarantee(status == JNI_OK &amp;&amp; !env-&gt;ExceptionOccurred(), &quot;register java.lang.Continuation natives&quot;);
5671 }
5672 
5673 #include CPU_HEADER_INLINE(continuation)
5674 
5675 #ifdef CONT_DOUBLE_NOP
5676 template&lt;op_mode mode&gt;
5677 static inline CachedCompiledMetadata cached_metadata(const hframe&amp; hf) {
5678   return ContinuationHelper::cached_metadata&lt;mode&gt;(hf);
5679 }
5680 #endif
5681 
5682 template &lt;typename OopWriterT&gt;
5683 int FreezeCompiledOops&lt;OopWriterT&gt;::slow_path_preempt(address vsp, address oops, address addr, address hsp, int idx, FpOopInfo* fp_oop_info, Extra *extra, bool is_preempt) {
5684   if (is_preempt) {
5685     return freeze_slow&lt;RegisterMap, true&gt;(vsp, oops, addr, hsp, idx, fp_oop_info, extra);
5686   }
5687   return freeze_slow&lt;RegisterMap, false&gt;(vsp, oops, addr, hsp, idx, fp_oop_info, extra);
5688 }
5689 
5690 template &lt;typename OopWriterT&gt;
5691 int FreezeCompiledOops&lt;OopWriterT&gt;::slow_path(address vsp, address oops, address addr, address hsp, int idx, FpOopInfo* fp_oop_info, Extra *extra) {
5692   return freeze_slow&lt;SmallRegisterMap, false&gt;(vsp, oops, addr, hsp, idx, fp_oop_info, extra);
5693 }
5694 
5695 int ThawCompiledOops::slow_path_preempt(address vsp, address oops, address link_addr, Extra* extra) {
5696   return thaw_slow&lt;RegisterMap&gt;(vsp, oops, link_addr, extra);
5697 }
5698 
5699 int ThawCompiledOops::slow_path(address vsp, address oops, address link_addr, Extra* extra) {
5700   return thaw_slow&lt;SmallRegisterMap&gt;(vsp, oops, link_addr, extra);
5701 }
5702 
5703 /* This is hopefully only temporary, currently only G1 has support for making the weak
5704  * keepalive OOPs strong while their nmethods are on the stack. */
5705 class HandleKeepalive {
5706 public:
5707   typedef Handle TypeT;
5708 
5709   static Handle make_keepalive(Thread* thread, oop* keepalive) {
5710     return Handle(thread, WeakHandle::from_raw(keepalive).resolve());
5711   }
5712 
5713   static oop read_keepalive(Handle obj) {
5714     return obj();
5715   }
5716 };
5717 
5718 class NoKeepalive {
5719 public:
5720   typedef oop* TypeT;
5721 
5722   static oop* make_keepalive(Thread* thread, oop* keepalive) {
5723     return keepalive;
5724   }
5725 
5726   static oop read_keepalive(oop* keepalive) {
5727     return WeakHandle::from_raw(keepalive).resolve();
5728   }
5729 };
5730 
5731 template &lt;bool compressed_oops, bool post_barrier, bool g1gc, bool slow_flags&gt;
5732 class Config {
5733 public:
5734   typedef Config&lt;compressed_oops, post_barrier, g1gc, slow_flags&gt; SelfT;
5735   typedef typename Conditional&lt;compressed_oops, narrowOop, oop&gt;::type OopT;
5736   typedef typename Conditional&lt;post_barrier, RawOopWriter&lt;SelfT&gt;, NormalOopWriter&lt;SelfT&gt; &gt;::type OopWriterT;
5737   typedef typename Conditional&lt;g1gc, NoKeepalive, HandleKeepalive&gt;::type KeepaliveObjectT;
5738 
5739   static const bool _compressed_oops = compressed_oops;
5740   static const bool _post_barrier = post_barrier;
5741   static const bool _slow_flags = slow_flags;
5742   // static const bool allow_stubs = gen_stubs &amp;&amp; post_barrier &amp;&amp; compressed_oops;
5743   // static const bool has_young = use_chunks;
5744   // static const bool full_stack = full;
5745 
5746   template&lt;op_mode mode&gt;
5747   static int freeze(JavaThread* thread, intptr_t* sp, bool preempt) {
5748     return freeze0&lt;SelfT, mode&gt;(thread, sp, preempt);
5749   }
5750 
5751   template&lt;op_mode mode&gt;
5752   static intptr_t* thaw(JavaThread* thread, ContMirror&amp; cont, thaw_kind kind) {
5753     return Thaw&lt;SelfT, mode&gt;(thread, cont).thaw(kind);
5754   }
5755 
5756   static void print() {
5757     tty-&gt;print_cr(&quot;&gt;&gt;&gt; Config compressed_oops: %d post_barrier: %d allow_stubs: %d use_chunks: %d slow_flags: %d&quot;, _compressed_oops, _post_barrier, LoomGenCode &amp;&amp; _compressed_oops &amp;&amp; _post_barrier, UseContinuationChunks, _slow_flags);
5758     tty-&gt;print_cr(&quot;&gt;&gt;&gt; Config UseAVX: %ld UseUnalignedLoadStores: %d Enhanced REP MOVSB: %d Fast Short REP MOVSB: %d rdtscp: %d rdpid: %d&quot;, UseAVX, UseUnalignedLoadStores, VM_Version::supports_erms(), VM_Version::supports_fsrm(), VM_Version::supports_rdtscp(), VM_Version::supports_rdpid());
5759     tty-&gt;print_cr(&quot;&gt;&gt;&gt; Config avx512bw (not legacy bw): %d avx512dq (not legacy dq): %d avx512vl (not legacy vl): %d avx512vlbw (not legacy vlbw): %d&quot;, VM_Version::supports_avx512bw(), VM_Version::supports_avx512dq(), VM_Version::supports_avx512vl(), VM_Version::supports_avx512vlbw());
5760   }
5761 };
5762 
5763 class ConfigResolve {
5764 public:
5765   static void resolve() { resolve_compressed(); }
5766 
5767   static void resolve_compressed() {
5768     UseCompressedOops ? resolve_modref&lt;true&gt;()
5769                       : resolve_modref&lt;false&gt;();
5770   }
5771 
5772   template &lt;bool use_compressed&gt;
5773   static void resolve_modref() {
5774     BarrierSet::barrier_set()-&gt;is_a(BarrierSet::ModRef)
5775       ? resolve_g1&lt;use_compressed, true&gt;()
5776       : resolve_g1&lt;use_compressed, false&gt;();
5777   }
5778 
5779   template &lt;bool use_compressed, bool is_modref&gt;
5780   static void resolve_g1() {
5781     UseG1GC &amp;&amp; UseContinuationStrong
5782       ? resolve_slow_flags&lt;use_compressed, is_modref, true&gt;()
5783       : resolve_slow_flags&lt;use_compressed, is_modref, false&gt;();
5784   }
5785 
5786   template &lt;bool use_compressed, bool is_modref, bool g1gc&gt;
5787   static void resolve_slow_flags() {
5788     (!LoomGenCode &amp;&amp; use_compressed &amp;&amp; is_modref)
5789     || !UseContinuationLazyCopy
5790       ? resolve&lt;use_compressed, is_modref, g1gc, true&gt;()
5791       : resolve&lt;use_compressed, is_modref, g1gc, false&gt;();
5792   }
5793   // template &lt;bool use_compressed, bool is_modref, bool g1gc&gt;
5794   // static void resolve_gencode() {
5795   //   LoomGenCode
5796   //     ? resolve_use_chunks&lt;use_compressed, is_modref, g1gc, true&gt;()
5797   //     : resolve_use_chunks&lt;use_compressed, is_modref, g1gc, false&gt;();
5798   // }
5799 
5800   // template &lt;bool use_compressed, bool is_modref, bool g1gc, bool gencode&gt;
5801   // static void resolve_use_chunks() {
5802   //   UseContinuationChunks
5803   //     ? resolve_full_stack&lt;use_compressed, is_modref, g1gc, gencode, true&gt;()
5804   //     : resolve_full_stack&lt;use_compressed, is_modref, g1gc, gencode, false&gt;();
5805   // }
5806 
5807   // template &lt;bool use_compressed, bool is_modref, bool g1gc, bool gencode, bool use_chunks&gt;
5808   // static void resolve_full_stack() {
5809   //   (!UseContinuationLazyCopy)
5810   //     ? resolve&lt;use_compressed, is_modref, gencode, use_chunks, g1gc, true&gt;()
5811   //     : resolve&lt;use_compressed, is_modref, gencode, use_chunks, g1gc, false&gt;();
5812   // }
5813 
5814   template &lt;bool use_compressed, bool is_modref, bool g1gc, bool slow_flags&gt;
5815   static void resolve() {
5816     typedef Config&lt;use_compressed, is_modref, g1gc, slow_flags&gt; SelectedConfigT;
5817     // SelectedConfigT::print();
5818 
5819     cont_freeze_fast    = SelectedConfigT::template freeze&lt;mode_fast&gt;;
5820     cont_freeze_slow    = SelectedConfigT::template freeze&lt;mode_slow&gt;;
5821     cont_freeze_chunk_memcpy = resolve_freeze_chunk_memcpy();
5822 
5823     cont_thaw_fast    = SelectedConfigT::template thaw&lt;mode_fast&gt;;
5824     cont_thaw_slow    = SelectedConfigT::template thaw&lt;mode_slow&gt;;
5825     cont_thaw_chunk_memcpy = resolve_thaw_chunk_memcpy();
5826 
5827     cont_freeze_oops_slow = (FreezeFnT) FreezeCompiledOops&lt;typename SelectedConfigT::OopWriterT&gt;::slow_path;
5828     if (LoomGenCode &amp;&amp; SelectedConfigT::_compressed_oops &amp;&amp; SelectedConfigT::_post_barrier) {
5829       cont_freeze_oops_generate = (FreezeFnT) FreezeCompiledOops&lt;typename SelectedConfigT::OopWriterT&gt;::generate_stub;
5830     } else {
5831       cont_freeze_oops_generate = cont_freeze_oops_slow;
5832     }
5833 
5834     cont_thaw_oops_slow = (ThawFnT) ThawCompiledOops::slow_path;
5835   }
5836 };
5837 
5838 void Continuations::init() {
5839   ConfigResolve::resolve();
5840   OopMapStubGenerator::init();
5841   Continuation::init();
5842 }
5843 
5844 address Continuations::default_thaw_oops_stub() {
5845   return (address) OopStubs::thaw_oops_slow();
5846 }
5847 
5848 address Continuations::default_freeze_oops_stub() {
5849   return (address) OopStubs::generate_stub();
5850 }
5851 
5852 address Continuations::freeze_oops_slow() {
5853   return (address) OopStubs::freeze_oops_slow();
5854 }
5855 
5856 address Continuations::thaw_oops_slow() {
5857   return (address) OopStubs::thaw_oops_slow();
5858 }
5859 
5860 void Continuation::init() {
5861 }
5862 
5863 class KeepaliveCleanupClosure : public ThreadClosure {
5864 private:
5865   int _count;
5866 public:
5867   KeepaliveCleanupClosure() : _count(0) {}
5868 
5869   int count() const { return _count; }
5870 
5871   virtual void do_thread(Thread* thread) {
5872     JavaThread* jthread = (JavaThread*) thread;
5873     GrowableArray&lt;WeakHandle&gt;* cleanup_list = jthread-&gt;keepalive_cleanup();
5874     int len = cleanup_list-&gt;length();
5875     _count += len;
5876     for (int i = 0; i &lt; len; ++i) {
5877       WeakHandle ref = cleanup_list-&gt;at(i);
5878       ref.release(Universe::vm_weak());
5879     }
5880 
5881     cleanup_list-&gt;clear();
5882     assert(cleanup_list-&gt;length() == 0, &quot;should be clean&quot;);
5883   }
5884 };
5885 
5886 void Continuations::cleanup_keepalives() {
5887   KeepaliveCleanupClosure closure;
5888   Threads::java_threads_do(&amp;closure);
5889   //log_info(jvmcont)(&quot;cleanup %d refs&quot;, closure.count());
5890 }
5891 
5892 volatile intptr_t Continuations::_exploded_miss = 0;
5893 volatile intptr_t Continuations::_exploded_hit = 0;
5894 volatile intptr_t Continuations::_nmethod_miss = 0;
5895 volatile intptr_t Continuations::_nmethod_hit = 0;
5896 
5897 void Continuations::exploded_miss() {
5898   //Atomic::inc(&amp;_exploded_miss);
5899 }
5900 
5901 void Continuations::exploded_hit() {
5902   //Atomic::inc(&amp;_exploded_hit);
5903 }
5904 
5905 void Continuations::nmethod_miss() {
5906   //Atomic::inc(&amp;_nmethod_miss);
5907 }
5908 
5909 void Continuations::nmethod_hit() {
5910   //Atomic::inc(&amp;_nmethod_hit);
5911 }
5912 
5913 void Continuations::print_statistics() {
5914   //tty-&gt;print_cr(&quot;Continuations hit/miss %ld / %ld&quot;, _exploded_hit, _exploded_miss);
5915   //tty-&gt;print_cr(&quot;Continuations nmethod hit/miss %ld / %ld&quot;, _nmethod_hit, _nmethod_miss);
5916 }
5917 
5918 ///// DEBUGGING
5919 
5920 #ifndef PRODUCT
5921 void Continuation::describe(FrameValues &amp;values) {
5922   JavaThread* thread = JavaThread::active();
5923   if (thread != NULL) {
5924     for (ContinuationEntry* cont = thread-&gt;cont_entry(); cont != NULL; cont = cont-&gt;parent()) {
5925       intptr_t* bottom = cont-&gt;entry_sp();
5926       if (bottom != NULL)
5927         values.describe(-1, bottom, &quot;continuation entry&quot;);
5928     }
5929   }
5930 }
5931 #endif
5932 
5933 void Continuation::nmethod_patched(nmethod* nm) {
5934   //log_info(jvmcont)(&quot;nmethod patched %p&quot;, nm);
5935   oop* keepalive = nm-&gt;get_keepalive();
5936   if (keepalive == NULL) {
5937     return;
5938   }
5939   WeakHandle wh = WeakHandle::from_raw(keepalive);
5940   oop resolved = wh.resolve();
5941 #ifdef DEBUG
5942   Universe::heap()-&gt;is_in_or_null(resolved);
5943 #endif
5944 
5945 #ifndef PRODUCT
5946   CountOops count;
5947   nm-&gt;oops_do(&amp;count, false, true);
5948   assert(nm-&gt;nr_oops() &gt;= count.nr_oops(), &quot;should be&quot;);
5949 #endif
5950 
5951   if (resolved == NULL) {
5952     return;
5953   }
5954 
5955   if (UseCompressedOops) {
5956     PersistOops&lt;narrowOop&gt; persist(nm-&gt;nr_oops(), (objArrayOop) resolved);
5957     nm-&gt;oops_do(&amp;persist);
5958   } else {
5959     PersistOops&lt;oop&gt; persist(nm-&gt;nr_oops(), (objArrayOop) resolved);;
5960     nm-&gt;oops_do(&amp;persist);
5961   }
5962 }
5963 
5964 static void print_oop(void *p, oop obj, outputStream* st) {
5965   if (!log_develop_is_enabled(Trace, jvmcont) &amp;&amp; st != NULL) return;
5966 
5967   if (st == NULL) st = tty;
5968 
5969   st-&gt;print_cr(INTPTR_FORMAT &quot;: &quot;, p2i(p));
5970   if (obj == NULL) {
5971     st-&gt;print_cr(&quot;*NULL*&quot;);
5972   } else {
5973     if (oopDesc::is_oop_or_null(obj)) {
5974       if (obj-&gt;is_objArray()) {
5975         st-&gt;print_cr(&quot;valid objArray: &quot; INTPTR_FORMAT, p2i(obj));
5976       } else {
5977         obj-&gt;print_value_on(st);
5978         // obj-&gt;print();
5979       }
5980     } else {
5981       st-&gt;print_cr(&quot;invalid oop: &quot; INTPTR_FORMAT, p2i(obj));
5982     }
5983     st-&gt;cr();
5984   }
5985 }
5986 
5987 void ContMirror::print_hframes(outputStream* st) {
5988   if (st != NULL &amp;&amp; !log_develop_is_enabled(Trace, jvmcont)) return;
5989   if (st == NULL) st = tty;
5990 
5991   st-&gt;print_cr(&quot;------- hframes ---------&quot;);
5992   st-&gt;print_cr(&quot;sp: %d length: %d&quot;, _sp, _stack_length);
5993   int i = 0;
5994   for (hframe f = last_frame&lt;mode_slow&gt;(); !f.is_empty(); f = f.sender&lt;mode_slow&gt;(*this)) {
5995     st-&gt;print_cr(&quot;frame: %d&quot;, i);
5996     f.print_on(*this, st);
5997     i++;
5998   }
5999   st-&gt;print_cr(&quot;======= end hframes =========&quot;);
6000 }
6001 
6002 #ifdef ASSERT
6003 
6004 bool Continuation::debug_is_stack_chunk(Klass* k) {
6005   return k-&gt;is_instance_klass() &amp;&amp; InstanceKlass::cast(k)-&gt;is_stack_chunk_instance_klass();
6006 }
6007 
6008 bool Continuation::debug_is_stack_chunk(oop obj) {
6009   return obj != (oop)NULL &amp;&amp; debug_is_stack_chunk(obj-&gt;klass());
6010 }
6011 
6012 void Continuation::debug_print_stack_chunk(oop chunk) {
6013   assert (debug_is_stack_chunk(chunk), &quot;&quot;);
6014   print_chunk(chunk, NULL, false);
6015 }
6016 
6017 bool Continuation::debug_is_continuation(Klass* klass) {
6018   return klass-&gt;is_subtype_of(SystemDictionary::Continuation_klass());
6019 }
6020 
6021 bool Continuation::debug_is_continuation(oop obj) {
6022   return obj-&gt;is_a(SystemDictionary::Continuation_klass());
6023 }
6024 
6025 bool Continuation::debug_is_continuation_run_frame(const frame&amp; f) {
6026   bool is_continuation_run = false;
6027   if (f.is_compiled_frame()) {
6028     HandleMark hm;
6029     ResourceMark rm;
6030     Method* m = f.cb()-&gt;as_compiled_method()-&gt;scope_desc_at(f.pc())-&gt;method();
6031     if (m != NULL) {
6032       char buf[50];
6033       if (0 == strcmp(ENTER_SPECIAL_SIG, m-&gt;name_and_sig_as_C_string(buf, 50))) {
6034         is_continuation_run = true;
6035       }
6036     }
6037   }
6038   return is_continuation_run;
6039 }
6040 
6041 
6042 NOINLINE bool Continuation::debug_verify_continuation(oop contOop) {
6043   assert (contOop != (oop)NULL, &quot;&quot;);
6044   assert (oopDesc::is_oop(contOop), &quot;&quot;);
6045   ContMirror cont(contOop);
6046   cont.read();
6047 
6048   size_t max_size = 0;
6049   bool nonempty_chunk = false;
6050   assert (oopDesc::is_oop_or_null(cont.tail()), &quot;&quot;);
6051   assert (cont.chunk_invariant(), &quot;&quot;);
6052   for (oop chunk = cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
6053     debug_verify_stack_chunk(chunk, contOop, &amp;max_size);
6054     if (!ContMirror::is_empty_chunk(chunk))
6055       nonempty_chunk = true;
6056   }
6057 
6058   // assert (cont.max_size() &gt;= 0, &quot;&quot;); // size_t can&#39;t be negative...
6059   const bool is_empty = cont.is_empty();
6060   assert (!nonempty_chunk || !is_empty, &quot;&quot;);
6061   assert (is_empty == (cont.max_size() == 0), &quot;cont.is_empty(): %d cont.max_size(): %lu cont: 0x%lx&quot;, is_empty, cont.max_size(), cont.mirror()-&gt;identity_hash());
6062   assert (is_empty == (!nonempty_chunk &amp;&amp; cont.last_frame&lt;mode_slow&gt;().is_empty()), &quot;&quot;);
6063 
6064   int frames = 0;
6065   int interpreted_frames = 0;
6066   int oops = 0;
6067   int callee_argsize = 0;
6068   bool callee_compiled = nonempty_chunk;
6069   for (hframe hf = cont.last_frame&lt;mode_slow&gt;(); !hf.is_empty(); hf = hf.sender&lt;mode_slow&gt;(cont)) {
6070     assert (frames &gt; 0 || (hf.is_interpreted_frame() == cont.is_flag(FLAG_LAST_FRAME_INTERPRETED)), &quot;frames: %d interpreted: %d FLAG_LAST_FRAME_INTERPRETED: %d&quot;, frames, hf.is_interpreted_frame(), cont.is_flag(FLAG_LAST_FRAME_INTERPRETED));
6071     assert (frames &gt; 0 || ((!hf.is_interpreted_frame() &amp;&amp; is_stub(hf.cb())) &lt;= cont.is_flag(FLAG_SAFEPOINT_YIELD)), &quot;frames: %d interpreted: %d stub: %d FLAG_SAFEPOINT_YIELD: %d&quot;, frames, hf.is_interpreted_frame(), !hf.is_interpreted_frame() &amp;&amp; is_stub(hf.cb()), cont.is_flag(FLAG_SAFEPOINT_YIELD));
6072     if (hf.is_interpreted_frame()) {
6073       interpreted_frames++;
6074 
6075       if (callee_compiled) { max_size += SP_WIGGLE &lt;&lt; LogBytesPerWord; } // TODO PD
6076       max_size += callee_argsize;
6077       max_size += hf.interpreted_frame_size();
6078       callee_argsize = 0;
6079       callee_compiled = false;
6080 
6081       // InterpreterOopMap mask;
6082       // hf.interpreted_frame_oop_map(&amp;mask);
6083       // oops += hf.interpreted_frame_num_oops(mask);
6084     } else {
6085       max_size += hf.compiled_frame_size();
6086       callee_compiled = true;
6087       assert ((frames == 0 &amp;&amp; cont.is_flag(FLAG_SAFEPOINT_YIELD)) || !is_stub(hf.cb()), &quot;&quot;);
6088       // FLAG_SAFEPOINT_YIELD is kept on after thawing safepoint stub, so is_stub may not be true if we verify in thaw
6089       if (frames == 0 &amp;&amp; cont.is_flag(FLAG_SAFEPOINT_YIELD) &amp;&amp; is_stub(hf.cb())) {
6090         callee_argsize = 0;
6091       } else {
6092         assert (hf.cb() != NULL &amp;&amp; hf.cb()-&gt;is_compiled(), &quot;&quot;);
6093         callee_argsize = hf.compiled_frame_stack_argsize();
6094       }
6095 
6096       // oops += hf.compiled_frame_num_oops();
6097     }
6098     frames++;
6099   }
6100   assert (frames == cont.num_frames(), &quot;&quot;);
6101   assert (interpreted_frames == cont.num_interpreted_frames(), &quot;&quot;);
6102   /*if (max_size != cont.max_size()) {
6103     debug_print_continuation(cont.mirror());
6104     tty-&gt;print_cr(&quot;isize: %d, csize: %d&quot;, i_size, c_size);
6105   }*/
6106   assert (max_size == cont.max_size(), &quot;max_size: %lu cont.max_size(): %lu&quot;, max_size, cont.max_size());
6107   // assert (oops == cont.num_oops(), &quot;&quot;);
6108   return true;
6109 }
6110 
6111 void Continuation::debug_print_continuation(oop contOop, outputStream* st) {
6112   if (st == NULL) st = tty;
6113 
6114   ContMirror cont(contOop);
6115 
6116   st-&gt;print_cr(&quot;CONTINUATION: 0x%lx done: %d max_size: %lu&quot;, contOop-&gt;identity_hash(), java_lang_Continuation::done(contOop), cont.max_size());
6117   st-&gt;print_cr(&quot;  flags FLAG_LAST_FRAME_INTERPRETED: %d FLAG_SAFEPOINT_YIELD: %d&quot;, cont.is_flag(FLAG_LAST_FRAME_INTERPRETED), cont.is_flag(FLAG_SAFEPOINT_YIELD));
6118   st-&gt;print_cr(&quot;  hstack length: %d ref_stack: %d&quot;, cont.stack_length(), cont.refStack() != NULL ? cont.refStack()-&gt;length() : 0);
6119 
6120   st-&gt;print_cr(&quot;CHUNKS:&quot;);
6121   for (oop chunk = cont.tail(); chunk != (oop)NULL; chunk = jdk_internal_misc_StackChunk::parent(chunk)) {
6122     st-&gt;print(&quot;* &quot;);
6123     print_chunk(chunk, contOop, true);
6124   }
6125 
6126   st-&gt;print_cr(&quot;frames: %d interpreted frames: %d oops: %d&quot;, cont.num_frames(), cont.num_interpreted_frames(), cont.num_oops());
6127   int frames = 0;
6128   for (hframe hf = cont.last_frame&lt;mode_slow&gt;(); !hf.is_empty(); hf = hf.sender&lt;mode_slow&gt;(cont)) {
6129     if (hf.is_interpreted_frame()) {
6130       st-&gt;print_cr(&quot;* size: %d&quot;, hf.interpreted_frame_size());
6131     } else {
6132       st-&gt;print_cr(&quot;* size: %d argsize: %d&quot;, hf.compiled_frame_size(), hf.compiled_frame_stack_argsize());
6133     }
6134 
6135     hf.print_on(cont, st);
6136 
6137     // if (hf.is_interpreted_frame()) {
6138     //   InterpreterOopMap mask;
6139     //   hf.interpreted_frame_oop_map(&amp;mask);
6140     //   oops += hf.interpreted_frame_num_oops(mask);
6141     // } else {
6142     //   oops += hf.compiled_frame_num_oops();
6143     // }
6144 
6145     frames++;
6146   }
6147 }
6148 static jlong java_tid(JavaThread* thread) {
6149   return java_lang_Thread::thread_id(thread-&gt;threadObj());
6150 }
6151 
6152 // template&lt;int x&gt;
6153 // NOINLINE static void walk_frames(JavaThread* thread) {
6154 //   RegisterMap map(thread, false, false, false);
6155 //   for (frame f = thread-&gt;last_frame(); !f.is_first_frame(); f = f.sender(&amp;map));
6156 // }
6157 
6158 static void print_frames(JavaThread* thread, outputStream* st) {
6159   if (st != NULL &amp;&amp; !log_develop_is_enabled(Trace, jvmcont)) return;
6160   if (st == NULL) st = tty;
6161 
6162   if (!thread-&gt;has_last_Java_frame()) st-&gt;print_cr(&quot;NO ANCHOR!&quot;);
6163 
6164   st-&gt;print_cr(&quot;------- frames ---------&quot;);
6165   RegisterMap map(thread, true, false);
6166 #ifndef PRODUCT
6167   map.set_skip_missing(true);
6168   ResetNoHandleMark rnhm;
6169   ResourceMark rm;
6170   HandleMark hm;
6171   FrameValues values;
6172 #endif
6173 
6174   int i = 0;
6175   for (frame f = thread-&gt;last_frame(); !f.is_entry_frame(); f = f.sender(&amp;map)) {
6176 #ifndef PRODUCT
6177     // print_vframe(f, &amp;map, st);
6178     f.describe(values, i, &amp;map);
6179 #else
6180     // f.print_on(st);
6181     // tty-&gt;print_cr(&quot;===&quot;);
6182     print_vframe(f, &amp;map, st);
6183 #endif
6184     i++;
6185   }
6186 #ifndef PRODUCT
6187   values.print(thread);
6188 #endif
6189   st-&gt;print_cr(&quot;======= end frames =========&quot;);
6190 }
6191 
6192 // static inline bool is_not_entrant(const frame&amp; f) {
6193 //   return  f.is_compiled_frame() ? f.cb()-&gt;as_nmethod()-&gt;is_not_entrant() : false;
6194 // }
6195 
6196 static char* method_name(Method* m) {
6197   return m != NULL ? m-&gt;name_and_sig_as_C_string() : NULL;
6198 }
6199 
6200 static inline Method* top_java_frame_method(const frame&amp; f) {
6201   Method* m = NULL;
6202   if (f.is_interpreted_frame()) {
6203     m = f.interpreter_frame_method();
6204   } else if (f.is_compiled_frame()) {
6205     CompiledMethod* cm = f.cb()-&gt;as_compiled_method();
6206     ScopeDesc* scope = cm-&gt;scope_desc_at(f.pc());
6207     m = scope-&gt;method();
6208   } else if (f.is_native_frame()) {
6209     return f.cb()-&gt;as_nmethod()-&gt;method();
6210   }
6211   // m = ((CompiledMethod*)f.cb())-&gt;method();
6212   return m;
6213 }
6214 
6215 void print_chunk(oop chunk, oop cont, bool verbose) {
6216   if (chunk == (oop)NULL) {
6217     tty-&gt;print_cr(&quot;CHUNK NULL&quot;);
6218     return;
6219   }
6220   // tty-&gt;print_cr(&quot;CHUNK &quot; INTPTR_FORMAT &quot; ::&quot;, p2i((oopDesc*)chunk));
6221   assert(ContMirror::is_stack_chunk(chunk), &quot;&quot;);
6222   HeapRegion* hr = G1CollectedHeap::heap()-&gt;heap_region_containing(chunk);
6223   tty-&gt;print_cr(&quot;CHUNK &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; :: %s 0x%lx&quot;, p2i((oopDesc*)chunk), p2i((HeapWord*)(chunk + chunk-&gt;size())), hr-&gt;get_type_str(), chunk-&gt;identity_hash());
6224   tty-&gt;print(&quot;CHUNK &quot; INTPTR_FORMAT &quot; young: %d size: %d sp: %d num_frames: %d num_oops: %d parent: &quot; INTPTR_FORMAT,
6225     p2i((oopDesc*)chunk), !requires_barriers(chunk),
6226     jdk_internal_misc_StackChunk::size(chunk), jdk_internal_misc_StackChunk::sp(chunk),
6227     jdk_internal_misc_StackChunk::numFrames(chunk), jdk_internal_misc_StackChunk::numOops(chunk),
6228     p2i((oopDesc*)jdk_internal_misc_StackChunk::parent(chunk)));
6229   if (cont != (oop)NULL) {
6230     int i=0;
6231     for (oop c = java_lang_Continuation::tail(cont); c != (oop)NULL &amp;&amp; c != chunk; c = jdk_internal_misc_StackChunk::parent(c), i++);
6232     tty-&gt;print(&quot; #%d parent: &quot;, i);
6233     if (jdk_internal_misc_StackChunk::parent(chunk) == (oop)NULL)
6234       tty-&gt;print(&quot;NULL&quot;);
6235     else
6236       tty-&gt;print(&quot;%d&quot;, i+1);
6237   }
6238 
6239   intptr_t* start = (intptr_t*)InstanceStackChunkKlass::start_of_stack(chunk);
6240   intptr_t* end   = start + jdk_internal_misc_StackChunk::size(chunk);
6241 
6242   if (verbose) {
6243     intptr_t* sp = start + jdk_internal_misc_StackChunk::sp(chunk);
6244     tty-&gt;cr();
6245     tty-&gt;print_cr(&quot;------ chunk frames end: &quot; INTPTR_FORMAT, p2i(end));
6246     if (sp &lt; end) {
6247       RegisterMap map(NULL, false);
6248       frame f = create_frame(sp);
6249       tty-&gt;print_cr(&quot;-- frame size: %d argsize: %d&quot;, Compiled::size(f), Compiled::stack_argsize(f));
6250       f.print_on(tty);
6251       while (f.sp() + ((Compiled::size(f) + Compiled::stack_argsize(f)) &gt;&gt; LogBytesPerWord) &lt; end) {
6252         f = f.sender(&amp;map);
6253         tty-&gt;print_cr(&quot;-- frame size: %d argsize: %d&quot;, Compiled::size(f), Compiled::stack_argsize(f));
6254         f.print_on(tty);
6255       }
6256     }
6257     tty-&gt;print_cr(&quot;------&quot;);
6258   } else {
6259     int frames = 0;
6260     CodeBlob* cb = NULL;
6261     for (intptr_t* sp = start + jdk_internal_misc_StackChunk::sp(chunk); sp &lt; end; sp += cb-&gt;frame_size()) {
6262       address pc = *(address*)(sp - SENDER_SP_RET_ADDRESS_OFFSET);
6263       cb = ContinuationCodeBlobLookup::find_blob(pc);
6264       frames++;
6265     }
6266     tty-&gt;print_cr(&quot; frames: %d&quot;, frames);
6267   }
6268 }
6269 
6270 static inline Method* bottom_java_frame_method(const frame&amp; f) {
6271   return Frame::frame_method(f);
6272 }
6273 
6274 static char* top_java_frame_name(const frame&amp; f) {
6275   return method_name(top_java_frame_method(f));
6276 }
6277 
6278 static char* bottom_java_frame_name(const frame&amp; f) {
6279   return method_name(bottom_java_frame_method(f));
6280 }
6281 
6282 static bool assert_top_java_frame_name(const frame&amp; f, const char* name) {
6283   ResourceMark rm;
6284   bool res = (strcmp(top_java_frame_name(f), name) == 0);
6285   assert (res, &quot;name: %s&quot;, top_java_frame_name(f));
6286   return res;
6287 }
6288 
6289 static bool assert_bottom_java_frame_name(const frame&amp; f, const char* name) {
6290   ResourceMark rm;
6291   bool res = (strcmp(bottom_java_frame_name(f), name) == 0);
6292   assert (res, &quot;name: %s&quot;, bottom_java_frame_name(f));
6293   return res;
6294 }
6295 
6296 static inline bool is_deopt_return(address pc, const frame&amp; sender) {
6297   if (sender.is_interpreted_frame()) return false;
6298 
6299   CompiledMethod* cm = sender.cb()-&gt;as_compiled_method();
6300   return cm-&gt;is_deopt_pc(pc);
6301 }
6302 
6303 #ifdef ASSERT
6304 //static bool is_deopt_pc(const frame&amp; f, address pc) {
6305 //  return f.is_compiled_frame() &amp;&amp; f.cb()-&gt;as_compiled_method()-&gt;is_deopt_pc(pc);
6306 //}
6307 //static bool is_deopt_pc(address pc) {
6308 //  CodeBlob* cb = CodeCache::find_blob(pc);
6309 //  return cb != NULL &amp;&amp; cb-&gt;is_compiled() &amp;&amp; cb-&gt;as_compiled_method()-&gt;is_deopt_pc(pc);
6310 //}
6311 #endif
6312 
6313 template &lt;typename FrameT&gt;
6314 static CodeBlob* slow_get_cb(const FrameT&amp; f) {
6315   assert (!f.is_interpreted_frame(), &quot;&quot;);
6316   CodeBlob* cb = f.cb();
6317   if (cb == NULL) {
6318     cb = CodeCache::find_blob(f.pc());
6319   }
6320   assert (cb != NULL, &quot;&quot;);
6321   return cb;
6322 }
6323 
6324 template &lt;typename FrameT&gt;
6325 static const ImmutableOopMap* slow_get_oopmap(const FrameT&amp; f) {
6326   const ImmutableOopMap* oopmap = f.oop_map();
6327   if (oopmap == NULL) {
6328     oopmap = OopMapSet::find_map(slow_get_cb(f), f.pc());
6329   }
6330   assert (oopmap != NULL, &quot;&quot;);
6331   return oopmap;
6332 }
6333 
6334 template &lt;typename FrameT&gt;
6335 static int slow_size(const FrameT&amp; f) {
6336   return slow_get_cb(f)-&gt;frame_size() * wordSize;
6337 }
6338 
6339 template &lt;typename FrameT&gt;
6340 static address slow_return_pc(const FrameT&amp; f) {
6341   return *slow_return_pc_address&lt;NonInterpretedUnknown&gt;(f);
6342 }
6343 
6344 template &lt;typename FrameT&gt;
6345 static int slow_stack_argsize(const FrameT&amp; f) {
6346   CodeBlob* cb = slow_get_cb(f);
6347   assert (cb-&gt;is_compiled(), &quot;&quot;);
6348   return cb-&gt;as_compiled_method()-&gt;method()-&gt;num_stack_arg_slots() * VMRegImpl::stack_slot_size;
6349 }
6350 
6351 template &lt;typename FrameT&gt;
6352 static int slow_num_oops(const FrameT&amp; f) {
6353   return slow_get_oopmap(f)-&gt;num_oops();
6354 }
6355 
6356 //static void print_blob(outputStream* st, address addr) {
6357 //  CodeBlob* b = CodeCache::find_blob_unsafe(addr);
6358 //  st-&gt;print(&quot;address: &quot; INTPTR_FORMAT &quot; blob: &quot;, p2i(addr));
6359 //  if (b != NULL) {
6360 //    b-&gt;dump_for_addr(addr, st, false);
6361 //  } else {
6362 //    st-&gt;print_cr(&quot;NULL&quot;);
6363 //  }
6364 //}
6365 
6366 // void static stop() {
6367 //     print_frames(JavaThread::current(), NULL);
6368 //     assert (false, &quot;&quot;);
6369 // }
6370 
6371 // void static stop(const frame&amp; f) {
6372 //     f.print_on(tty);
6373 //     stop();
6374 // }
6375 #endif
6376 
6377 // #ifdef ASSERT
6378 // #define JAVA_THREAD_OFFSET(field) tty-&gt;print_cr(&quot;JavaThread.&quot; #field &quot; 0x%x&quot;, in_bytes(JavaThread:: cat2(field,_offset()) ))
6379 // #define cat2(a,b)         cat2_hidden(a,b)
6380 // #define cat2_hidden(a,b)  a ## b
6381 // #define cat3(a,b,c)       cat3_hidden(a,b,c)
6382 // #define cat3_hidden(a,b,c)  a ## b ## c
6383 
6384 // static void print_JavaThread_offsets() {
6385 //   JAVA_THREAD_OFFSET(threadObj);
6386 //   JAVA_THREAD_OFFSET(jni_environment);
6387 //   JAVA_THREAD_OFFSET(pending_jni_exception_check_fn);
6388 //   JAVA_THREAD_OFFSET(last_Java_sp);
6389 //   JAVA_THREAD_OFFSET(last_Java_pc);
6390 //   JAVA_THREAD_OFFSET(frame_anchor);
6391 //   JAVA_THREAD_OFFSET(callee_target);
6392 //   JAVA_THREAD_OFFSET(vm_result);
6393 //   JAVA_THREAD_OFFSET(vm_result_2);
6394 //   JAVA_THREAD_OFFSET(thread_state);
6395 //   JAVA_THREAD_OFFSET(saved_exception_pc);
6396 //   JAVA_THREAD_OFFSET(osthread);
6397 //   JAVA_THREAD_OFFSET(continuation);
6398 //   JAVA_THREAD_OFFSET(exception_oop);
6399 //   JAVA_THREAD_OFFSET(exception_pc);
6400 //   JAVA_THREAD_OFFSET(exception_handler_pc);
6401 //   JAVA_THREAD_OFFSET(stack_overflow_limit);
6402 //   JAVA_THREAD_OFFSET(is_method_handle_return);
6403 //   JAVA_THREAD_OFFSET(stack_guard_state);
6404 //   JAVA_THREAD_OFFSET(reserved_stack_activation);
6405 //   JAVA_THREAD_OFFSET(suspend_flags);
6406 //   JAVA_THREAD_OFFSET(do_not_unlock_if_synchronized);
6407 //   JAVA_THREAD_OFFSET(should_post_on_exceptions_flag);
6408 // // #ifndef PRODUCT
6409 // //   static ByteSize jmp_ring_index_offset()        { return byte_offset_of(JavaThread, _jmp_ring_index); }
6410 // //   static ByteSize jmp_ring_offset()              { return byte_offset_of(JavaThread, _jmp_ring); }
6411 // // #endif // PRODUCT
6412 // // #if INCLUDE_JVMCI
6413 // //   static ByteSize pending_deoptimization_offset() { return byte_offset_of(JavaThread, _pending_deoptimization); }
6414 // //   static ByteSize pending_monitorenter_offset()  { return byte_offset_of(JavaThread, _pending_monitorenter); }
6415 // //   static ByteSize pending_failed_speculation_offset() { return byte_offset_of(JavaThread, _pending_failed_speculation); }
6416 // //   static ByteSize jvmci_alternate_call_target_offset() { return byte_offset_of(JavaThread, _jvmci._alternate_call_target); }
6417 // //   static ByteSize jvmci_implicit_exception_pc_offset() { return byte_offset_of(JavaThread, _jvmci._implicit_exception_pc); }
6418 // //   static ByteSize jvmci_counters_offset()        { return byte_offset_of(JavaThread, _jvmci_counters); }
6419 // // #endif // INCLUDE_JVMCI
6420 // }
6421 //
6422 // #endif
    </pre>
  </body>
</html>