<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/gc/z/zMark.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2015, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
 26 #include &quot;gc/z/zBarrier.inline.hpp&quot;
 27 #include &quot;gc/z/zMark.inline.hpp&quot;
 28 #include &quot;gc/z/zMarkCache.inline.hpp&quot;
 29 #include &quot;gc/z/zMarkStack.inline.hpp&quot;
 30 #include &quot;gc/z/zMarkTerminate.inline.hpp&quot;
 31 #include &quot;gc/z/zOopClosures.inline.hpp&quot;
 32 #include &quot;gc/z/zPage.hpp&quot;
 33 #include &quot;gc/z/zPageTable.inline.hpp&quot;
 34 #include &quot;gc/z/zRootsIterator.hpp&quot;
 35 #include &quot;gc/z/zStat.hpp&quot;
 36 #include &quot;gc/z/zTask.hpp&quot;
 37 #include &quot;gc/z/zThread.inline.hpp&quot;
 38 #include &quot;gc/z/zThreadLocalAllocBuffer.hpp&quot;
 39 #include &quot;gc/z/zUtils.inline.hpp&quot;
 40 #include &quot;gc/z/zWorkers.inline.hpp&quot;
 41 #include &quot;logging/log.hpp&quot;
 42 #include &quot;memory/iterator.inline.hpp&quot;
 43 #include &quot;oops/objArrayOop.inline.hpp&quot;
 44 #include &quot;oops/oop.inline.hpp&quot;
 45 #include &quot;runtime/atomic.hpp&quot;
 46 #include &quot;runtime/handshake.hpp&quot;
 47 #include &quot;runtime/prefetch.inline.hpp&quot;
 48 #include &quot;runtime/safepointMechanism.hpp&quot;
 49 #include &quot;runtime/thread.hpp&quot;
 50 #include &quot;utilities/align.hpp&quot;
 51 #include &quot;utilities/globalDefinitions.hpp&quot;
 52 #include &quot;utilities/powerOfTwo.hpp&quot;
 53 #include &quot;utilities/ticks.hpp&quot;
 54 
 55 static const ZStatSubPhase ZSubPhaseConcurrentMark(&quot;Concurrent Mark&quot;);
 56 static const ZStatSubPhase ZSubPhaseConcurrentMarkTryFlush(&quot;Concurrent Mark Try Flush&quot;);
 57 static const ZStatSubPhase ZSubPhaseConcurrentMarkIdle(&quot;Concurrent Mark Idle&quot;);
 58 static const ZStatSubPhase ZSubPhaseConcurrentMarkTryTerminate(&quot;Concurrent Mark Try Terminate&quot;);
 59 static const ZStatSubPhase ZSubPhaseMarkTryComplete(&quot;Pause Mark Try Complete&quot;);
 60 
 61 ZMark::ZMark(ZWorkers* workers, ZPageTable* page_table) :
 62     _workers(workers),
 63     _page_table(page_table),
 64     _allocator(),
 65     _stripes(),
 66     _terminate(),
 67     _work_terminateflush(true),
 68     _work_nproactiveflush(0),
 69     _work_nterminateflush(0),
 70     _nproactiveflush(0),
 71     _nterminateflush(0),
 72     _ntrycomplete(0),
 73     _ncontinue(0),
 74     _nworkers(0) {}
 75 
 76 bool ZMark::is_initialized() const {
 77   return _allocator.is_initialized();
 78 }
 79 
 80 size_t ZMark::calculate_nstripes(uint nworkers) const {
 81   // Calculate the number of stripes from the number of workers we use,
 82   // where the number of stripes must be a power of two and we want to
 83   // have at least one worker per stripe.
 84   const size_t nstripes = round_down_power_of_2(nworkers);
 85   return MIN2(nstripes, ZMarkStripesMax);
 86 }
 87 
 88 void ZMark::prepare_mark() {
 89   // Increment global sequence number to invalidate
 90   // marking information for all pages.
 91   ZGlobalSeqNum++;
 92 
 93   // Reset flush/continue counters
 94   _nproactiveflush = 0;
 95   _nterminateflush = 0;
 96   _ntrycomplete = 0;
 97   _ncontinue = 0;
 98 
 99   // Set number of workers to use
100   _nworkers = _workers-&gt;nconcurrent();
101 
102   // Set number of mark stripes to use, based on number
103   // of workers we will use in the concurrent mark phase.
104   const size_t nstripes = calculate_nstripes(_nworkers);
105   _stripes.set_nstripes(nstripes);
106 
107   // Update statistics
108   ZStatMark::set_at_mark_start(nstripes);
109 
110   // Print worker/stripe distribution
111   LogTarget(Debug, gc, marking) log;
112   if (log.is_enabled()) {
113     log.print(&quot;Mark Worker/Stripe Distribution&quot;);
114     for (uint worker_id = 0; worker_id &lt; _nworkers; worker_id++) {
115       const ZMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, worker_id);
116       const size_t stripe_id = _stripes.stripe_id(stripe);
117       log.print(&quot;  Worker %u(%u) -&gt; Stripe &quot; SIZE_FORMAT &quot;(&quot; SIZE_FORMAT &quot;)&quot;,
118                 worker_id, _nworkers, stripe_id, nstripes);
119     }
120   }
121 }
122 
123 class ZMarkRootsIteratorClosure : public ZRootsIteratorClosure {
124 public:
125   ZMarkRootsIteratorClosure() {
126     ZThreadLocalAllocBuffer::reset_statistics();
127   }
128 
129   ~ZMarkRootsIteratorClosure() {
130     ZThreadLocalAllocBuffer::publish_statistics();
131   }
132 
133   virtual void do_thread(Thread* thread) {
134     // Update thread local address bad mask
135     ZThreadLocalData::set_address_bad_mask(thread, ZAddressBadMask);
136 
137     // Mark invisible root
138     ZThreadLocalData::do_invisible_root(thread, ZBarrier::mark_barrier_on_invisible_root_oop_field);
139 
140     // Retire TLAB
141     ZThreadLocalAllocBuffer::retire(thread);
142   }
143 
144   virtual bool should_disarm_nmethods() const {
145     return true;
146   }
147 
148   virtual void do_oop(oop* p) {
149     ZBarrier::mark_barrier_on_root_oop_field(p);
150   }
151 
152   virtual void do_oop(narrowOop* p) {
153     ShouldNotReachHere();
154   }
155 };
156 
157 class ZMarkRootsTask : public ZTask {
158 private:
159   ZMark* const              _mark;
160   ZRootsIterator            _roots;
161   ZMarkRootsIteratorClosure _cl;
162 
163 public:
164   ZMarkRootsTask(ZMark* mark) :
165       ZTask(&quot;ZMarkRootsTask&quot;),
166       _mark(mark),
167       _roots(false /* visit_jvmti_weak_export */) {}
168 
169   virtual void work() {
170     _roots.oops_do(&amp;_cl);
171 
172     // Flush and free worker stacks. Needed here since
173     // the set of workers executing during root scanning
174     // can be different from the set of workers executing
175     // during mark.
176     _mark-&gt;flush_and_free();
177   }
178 };
179 
180 void ZMark::start() {
181   // Verification
182   if (ZVerifyMarking) {
183     verify_all_stacks_empty();
184   }
185 
186   // Prepare for concurrent mark
187   prepare_mark();
188 
189   // Mark roots
190   ZMarkRootsTask task(this);
191   _workers-&gt;run_parallel(&amp;task);
192 }
193 
194 void ZMark::prepare_work() {
195   assert(_nworkers == _workers-&gt;nconcurrent(), &quot;Invalid number of workers&quot;);
196 
197   // Set number of active workers
198   _terminate.reset(_nworkers);
199 
200   // Reset flush counters
201   _work_nproactiveflush = _work_nterminateflush = 0;
202   _work_terminateflush = true;
203 }
204 
205 void ZMark::finish_work() {
206   // Accumulate proactive/terminate flush counters
207   _nproactiveflush += _work_nproactiveflush;
208   _nterminateflush += _work_nterminateflush;
209 }
210 
211 bool ZMark::is_array(uintptr_t addr) const {
212   return ZOop::from_address(addr)-&gt;is_objArray();
213 }
214 
215 void ZMark::push_partial_array(uintptr_t addr, size_t size, bool finalizable) {
216   assert(is_aligned(addr, ZMarkPartialArrayMinSize), &quot;Address misaligned&quot;);
217   ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(Thread::current());
218   ZMarkStripe* const stripe = _stripes.stripe_for_addr(addr);
219   const uintptr_t offset = ZAddress::offset(addr) &gt;&gt; ZMarkPartialArrayMinSizeShift;
220   const uintptr_t length = size / oopSize;
221   const ZMarkStackEntry entry(offset, length, finalizable);
222 
223   log_develop_trace(gc, marking)(&quot;Array push partial: &quot; PTR_FORMAT &quot; (&quot; SIZE_FORMAT &quot;), stripe: &quot; SIZE_FORMAT,
224                                  addr, size, _stripes.stripe_id(stripe));
225 
226   stacks-&gt;push(&amp;_allocator, &amp;_stripes, stripe, entry, false /* publish */);
227 }
228 
229 void ZMark::follow_small_array(uintptr_t addr, size_t size, bool finalizable) {
230   assert(size &lt;= ZMarkPartialArrayMinSize, &quot;Too large, should be split&quot;);
231   const size_t length = size / oopSize;
232 
233   log_develop_trace(gc, marking)(&quot;Array follow small: &quot; PTR_FORMAT &quot; (&quot; SIZE_FORMAT &quot;)&quot;, addr, size);
234 
235   ZBarrier::mark_barrier_on_oop_array((oop*)addr, length, finalizable);
236 }
237 
238 void ZMark::follow_large_array(uintptr_t addr, size_t size, bool finalizable) {
239   assert(size &lt;= (size_t)arrayOopDesc::max_array_length(T_OBJECT) * oopSize, &quot;Too large&quot;);
240   assert(size &gt; ZMarkPartialArrayMinSize, &quot;Too small, should not be split&quot;);
241   const uintptr_t start = addr;
242   const uintptr_t end = start + size;
243 
244   // Calculate the aligned middle start/end/size, where the middle start
245   // should always be greater than the start (hence the +1 below) to make
246   // sure we always do some follow work, not just split the array into pieces.
247   const uintptr_t middle_start = align_up(start + 1, ZMarkPartialArrayMinSize);
248   const size_t    middle_size = align_down(end - middle_start, ZMarkPartialArrayMinSize);
249   const uintptr_t middle_end = middle_start + middle_size;
250 
251   log_develop_trace(gc, marking)(&quot;Array follow large: &quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT&quot; (&quot; SIZE_FORMAT &quot;), &quot;
252                                  &quot;middle: &quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot; (&quot; SIZE_FORMAT &quot;)&quot;,
253                                  start, end, size, middle_start, middle_end, middle_size);
254 
255   // Push unaligned trailing part
256   if (end &gt; middle_end) {
257     const uintptr_t trailing_addr = middle_end;
258     const size_t trailing_size = end - middle_end;
259     push_partial_array(trailing_addr, trailing_size, finalizable);
260   }
261 
262   // Push aligned middle part(s)
263   uintptr_t partial_addr = middle_end;
264   while (partial_addr &gt; middle_start) {
265     const size_t parts = 2;
266     const size_t partial_size = align_up((partial_addr - middle_start) / parts, ZMarkPartialArrayMinSize);
267     partial_addr -= partial_size;
268     push_partial_array(partial_addr, partial_size, finalizable);
269   }
270 
271   // Follow leading part
272   assert(start &lt; middle_start, &quot;Miscalculated middle start&quot;);
273   const uintptr_t leading_addr = start;
274   const size_t leading_size = middle_start - start;
275   follow_small_array(leading_addr, leading_size, finalizable);
276 }
277 
278 void ZMark::follow_array(uintptr_t addr, size_t size, bool finalizable) {
279   if (size &lt;= ZMarkPartialArrayMinSize) {
280     follow_small_array(addr, size, finalizable);
281   } else {
282     follow_large_array(addr, size, finalizable);
283   }
284 }
285 
286 void ZMark::follow_partial_array(ZMarkStackEntry entry, bool finalizable) {
287   const uintptr_t addr = ZAddress::good(entry.partial_array_offset() &lt;&lt; ZMarkPartialArrayMinSizeShift);
288   const size_t size = entry.partial_array_length() * oopSize;
289 
290   follow_array(addr, size, finalizable);
291 }
292 
293 void ZMark::follow_array_object(objArrayOop obj, bool finalizable) {
294   if (finalizable) {
295     ZMarkBarrierOopClosure&lt;true /* finalizable */&gt; cl;
296     cl.do_klass(obj-&gt;klass());
297   } else {
298     ZMarkBarrierOopClosure&lt;false /* finalizable */&gt; cl;
299     cl.do_klass(obj-&gt;klass());
300   }
301 
302   const uintptr_t addr = (uintptr_t)obj-&gt;base();
303   const size_t size = (size_t)obj-&gt;length() * oopSize;
304 
305   follow_array(addr, size, finalizable);
306 }
307 
308 void ZMark::follow_object(oop obj, bool finalizable) {
309   if (finalizable) {
310     ZMarkBarrierOopClosure&lt;true /* finalizable */&gt; cl;
311     obj-&gt;oop_iterate(&amp;cl);
312   } else {
313     ZMarkBarrierOopClosure&lt;false /* finalizable */&gt; cl;
314     obj-&gt;oop_iterate(&amp;cl);
315   }
316 }
317 
318 bool ZMark::try_mark_object(ZMarkCache* cache, uintptr_t addr, bool finalizable) {
319   ZPage* const page = _page_table-&gt;get(addr);
320   if (page-&gt;is_allocating()) {
321     // Newly allocated objects are implicitly marked
322     return false;
323   }
324 
325   // Try mark object
326   bool inc_live = false;
327   const bool success = page-&gt;mark_object(addr, finalizable, inc_live);
328   if (inc_live) {
329     // Update live objects/bytes for page. We use the aligned object
330     // size since that is the actual number of bytes used on the page
331     // and alignment paddings can never be reclaimed.
332     const size_t size = ZUtils::object_size(addr);
333     const size_t aligned_size = align_up(size, page-&gt;object_alignment());
334     cache-&gt;inc_live(page, aligned_size);
335   }
336 
337   return success;
338 }
339 
340 void ZMark::mark_and_follow(ZMarkCache* cache, ZMarkStackEntry entry) {
341   // Decode flags
342   const bool finalizable = entry.finalizable();
343   const bool partial_array = entry.partial_array();
344 
345   if (partial_array) {
346     follow_partial_array(entry, finalizable);
347     return;
348   }
349 
350   // Decode object address and follow flag
351   const uintptr_t addr = entry.object_address();
352 
353   if (!try_mark_object(cache, addr, finalizable)) {
354     // Already marked
355     return;
356   }
357 
358   if (is_array(addr)) {
359     // Decode follow flag
360     const bool follow = entry.follow();
361 
362     // The follow flag is currently only relevant for object arrays
363     if (follow) {
364       follow_array_object(objArrayOop(ZOop::from_address(addr)), finalizable);
365     }
366   } else {
367     follow_object(ZOop::from_address(addr), finalizable);
368   }
369 }
370 
371 template &lt;typename T&gt;
372 bool ZMark::drain(ZMarkStripe* stripe, ZMarkThreadLocalStacks* stacks, ZMarkCache* cache, T* timeout) {
373   ZMarkStackEntry entry;
374 
375   // Drain stripe stacks
376   while (stacks-&gt;pop(&amp;_allocator, &amp;_stripes, stripe, entry)) {
377     mark_and_follow(cache, entry);
378 
379     // Check timeout
380     if (timeout-&gt;has_expired()) {
381       // Timeout
382       return false;
383     }
384   }
385 
386   // Success
387   return true;
388 }
389 
390 template &lt;typename T&gt;
391 bool ZMark::drain_and_flush(ZMarkStripe* stripe, ZMarkThreadLocalStacks* stacks, ZMarkCache* cache, T* timeout) {
392   const bool success = drain(stripe, stacks, cache, timeout);
393 
394   // Flush and publish worker stacks
395   stacks-&gt;flush(&amp;_allocator, &amp;_stripes);
396 
397   return success;
398 }
399 
400 bool ZMark::try_steal(ZMarkStripe* stripe, ZMarkThreadLocalStacks* stacks) {
401   // Try to steal a stack from another stripe
402   for (ZMarkStripe* victim_stripe = _stripes.stripe_next(stripe);
403        victim_stripe != stripe;
404        victim_stripe = _stripes.stripe_next(victim_stripe)) {
405     ZMarkStack* const stack = victim_stripe-&gt;steal_stack();
406     if (stack != NULL) {
407       // Success, install the stolen stack
408       stacks-&gt;install(&amp;_stripes, stripe, stack);
409       return true;
410     }
411   }
412 
413   // Nothing to steal
414   return false;
415 }
416 
417 void ZMark::idle() const {
418   ZStatTimer timer(ZSubPhaseConcurrentMarkIdle);
419   os::naked_short_sleep(1);
420 }
421 
422 class ZMarkFlushAndFreeStacksClosure : public HandshakeClosure {
423 private:
424   ZMark* const _mark;
425   bool         _flushed;
426 
427 public:
428   ZMarkFlushAndFreeStacksClosure(ZMark* mark) :
429       HandshakeClosure(&quot;ZMarkFlushAndFreeStacks&quot;),
430       _mark(mark),
431       _flushed(false) {}
432 
433   void do_thread(Thread* thread) {
434     if (_mark-&gt;flush_and_free(thread)) {
435       _flushed = true;
436     }
437   }
438 
439   bool flushed() const {
440     return _flushed;
441   }
442 };
443 
444 bool ZMark::flush(bool at_safepoint) {
445   ZMarkFlushAndFreeStacksClosure cl(this);
446   if (at_safepoint) {
447     Threads::threads_do(&amp;cl);
448   } else {
449     Handshake::execute(&amp;cl);
450   }
451 
452   // Returns true if more work is available
453   return cl.flushed() || !_stripes.is_empty();
454 }
455 
456 bool ZMark::try_flush(volatile size_t* nflush) {
457   Atomic::inc(nflush);
458 
459   ZStatTimer timer(ZSubPhaseConcurrentMarkTryFlush);
460   return flush(false /* at_safepoint */);
461 }
462 
463 bool ZMark::try_proactive_flush() {
464   // Only do proactive flushes from worker 0
465   if (ZThread::worker_id() != 0) {
466     return false;
467   }
468 
469   if (Atomic::load(&amp;_work_nproactiveflush) == ZMarkProactiveFlushMax ||
470       Atomic::load(&amp;_work_nterminateflush) != 0) {
471     // Limit reached or we&#39;re trying to terminate
472     return false;
473   }
474 
475   return try_flush(&amp;_work_nproactiveflush);
476 }
477 
478 bool ZMark::try_terminate() {
479   ZStatTimer timer(ZSubPhaseConcurrentMarkTryTerminate);
480 
481   if (_terminate.enter_stage0()) {
482     // Last thread entered stage 0, flush
483     if (Atomic::load(&amp;_work_terminateflush) &amp;&amp;
484         Atomic::load(&amp;_work_nterminateflush) != ZMarkTerminateFlushMax) {
485       // Exit stage 0 to allow other threads to continue marking
486       _terminate.exit_stage0();
487 
488       // Flush before termination
489       if (!try_flush(&amp;_work_nterminateflush)) {
490         // No more work available, skip further flush attempts
491         Atomic::store(&amp;_work_terminateflush, false);
492       }
493 
494       // Don&#39;t terminate, regardless of whether we successfully
495       // flushed out more work or not. We&#39;ve already exited
496       // termination stage 0, to allow other threads to continue
497       // marking, so this thread has to return false and also
498       // make another round of attempted marking.
499       return false;
500     }
501   }
502 
503   for (;;) {
504     if (_terminate.enter_stage1()) {
505       // Last thread entered stage 1, terminate
506       return true;
507     }
508 
509     // Idle to give the other threads
510     // a chance to enter termination.
511     idle();
512 
513     if (!_terminate.try_exit_stage1()) {
514       // All workers in stage 1, terminate
515       return true;
516     }
517 
518     if (_terminate.try_exit_stage0()) {
519       // More work available, don&#39;t terminate
520       return false;
521     }
522   }
523 }
524 
525 class ZMarkNoTimeout : public StackObj {
526 public:
527   bool has_expired() {
528     return false;
529   }
530 };
531 
532 void ZMark::work_without_timeout(ZMarkCache* cache, ZMarkStripe* stripe, ZMarkThreadLocalStacks* stacks) {
533   ZStatTimer timer(ZSubPhaseConcurrentMark);
534   ZMarkNoTimeout no_timeout;
535 
536   for (;;) {
537     drain_and_flush(stripe, stacks, cache, &amp;no_timeout);
538 
539     if (try_steal(stripe, stacks)) {
540       // Stole work
541       continue;
542     }
543 
544     if (try_proactive_flush()) {
545       // Work available
546       continue;
547     }
548 
549     if (try_terminate()) {
550       // Terminate
551       break;
552     }
553   }
554 }
555 
556 class ZMarkTimeout : public StackObj {
557 private:
558   const Ticks    _start;
559   const uint64_t _timeout;
560   const uint64_t _check_interval;
561   uint64_t       _check_at;
562   uint64_t       _check_count;
563   bool           _expired;
564 
565 public:
566   ZMarkTimeout(uint64_t timeout_in_millis) :
567       _start(Ticks::now()),
568       _timeout(_start.value() + TimeHelper::millis_to_counter(timeout_in_millis)),
569       _check_interval(200),
570       _check_at(_check_interval),
571       _check_count(0),
572       _expired(false) {}
573 
574   ~ZMarkTimeout() {
575     const Tickspan duration = Ticks::now() - _start;
576     log_debug(gc, marking)(&quot;Mark With Timeout (%s): %s, &quot; UINT64_FORMAT &quot; oops, %.3fms&quot;,
577                            ZThread::name(), _expired ? &quot;Expired&quot; : &quot;Completed&quot;,
578                            _check_count, TimeHelper::counter_to_millis(duration.value()));
579   }
580 
581   bool has_expired() {
582     if (++_check_count == _check_at) {
583       _check_at += _check_interval;
584       if ((uint64_t)Ticks::now().value() &gt;= _timeout) {
585         // Timeout
586         _expired = true;
587       }
588     }
589 
590     return _expired;
591   }
592 };
593 
594 void ZMark::work_with_timeout(ZMarkCache* cache, ZMarkStripe* stripe, ZMarkThreadLocalStacks* stacks, uint64_t timeout_in_millis) {
595   ZStatTimer timer(ZSubPhaseMarkTryComplete);
596   ZMarkTimeout timeout(timeout_in_millis);
597 
598   for (;;) {
599     if (!drain_and_flush(stripe, stacks, cache, &amp;timeout)) {
600       // Timed out
601       break;
602     }
603 
604     if (try_steal(stripe, stacks)) {
605       // Stole work
606       continue;
607     }
608 
609     // Terminate
610     break;
611   }
612 }
613 
614 void ZMark::work(uint64_t timeout_in_millis) {
615   ZMarkCache cache(_stripes.nstripes());
616   ZMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, ZThread::worker_id());
617   ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(Thread::current());
618 
619   if (timeout_in_millis == 0) {
620     work_without_timeout(&amp;cache, stripe, stacks);
621   } else {
622     work_with_timeout(&amp;cache, stripe, stacks, timeout_in_millis);
623   }
624 
625   // Make sure stacks have been flushed
626   assert(stacks-&gt;is_empty(&amp;_stripes), &quot;Should be empty&quot;);
627 
628   // Free remaining stacks
629   stacks-&gt;free(&amp;_allocator);
630 }
631 
632 class ZMarkConcurrentRootsIteratorClosure : public ZRootsIteratorClosure {
633 public:
634   virtual void do_oop(oop* p) {
635     ZBarrier::mark_barrier_on_oop_field(p, false /* finalizable */);
636   }
637 
638   virtual void do_oop(narrowOop* p) {
639     ShouldNotReachHere();
640   }
641 };
642 
643 
644 class ZMarkConcurrentRootsTask : public ZTask {
645 private:
646   SuspendibleThreadSetJoiner          _sts_joiner;
647   ZConcurrentRootsIteratorClaimStrong _roots;
648   ZMarkConcurrentRootsIteratorClosure _cl;
649 
650 public:
651   ZMarkConcurrentRootsTask(ZMark* mark) :
652       ZTask(&quot;ZMarkConcurrentRootsTask&quot;),
653       _sts_joiner(),
654       _roots(),
655       _cl() {
656     ClassLoaderDataGraph_lock-&gt;lock();
657   }
658 
659   ~ZMarkConcurrentRootsTask() {
660     ClassLoaderDataGraph_lock-&gt;unlock();
661   }
662 
663   virtual void work() {
664     _roots.oops_do(&amp;_cl);
665   }
666 };
667 
668 class ZMarkTask : public ZTask {
669 private:
670   ZMark* const   _mark;
671   const uint64_t _timeout_in_millis;
672 
673 public:
674   ZMarkTask(ZMark* mark, uint64_t timeout_in_millis = 0) :
675       ZTask(&quot;ZMarkTask&quot;),
676       _mark(mark),
677       _timeout_in_millis(timeout_in_millis) {
678     _mark-&gt;prepare_work();
679   }
680 
681   ~ZMarkTask() {
682     _mark-&gt;finish_work();
683   }
684 
685   virtual void work() {
686     _mark-&gt;work(_timeout_in_millis);
687   }
688 };
689 
690 void ZMark::mark(bool initial) {
691   if (initial) {
692     ZMarkConcurrentRootsTask task(this);
693     _workers-&gt;run_concurrent(&amp;task);
694   }
695 
696   ZMarkTask task(this);
697   _workers-&gt;run_concurrent(&amp;task);
698 }
699 
700 bool ZMark::try_complete() {
701   _ntrycomplete++;
702 
703   // Use nconcurrent number of worker threads to maintain the
704   // worker/stripe distribution used during concurrent mark.
705   ZMarkTask task(this, ZMarkCompleteTimeout);
706   _workers-&gt;run_concurrent(&amp;task);
707 
708   // Successful if all stripes are empty
709   return _stripes.is_empty();
710 }
711 
712 bool ZMark::try_end() {
713   // Flush all mark stacks
714   if (!flush(true /* at_safepoint */)) {
715     // Mark completed
716     return true;
717   }
718 
719   // Try complete marking by doing a limited
720   // amount of mark work in this phase.
721   return try_complete();
722 }
723 
724 bool ZMark::end() {
725   // Try end marking
726   if (!try_end()) {
727     // Mark not completed
728     _ncontinue++;
729     return false;
730   }
731 
732   // Verification
733   if (ZVerifyMarking) {
734     verify_all_stacks_empty();
735   }
736 
737   // Update statistics
738   ZStatMark::set_at_mark_end(_nproactiveflush, _nterminateflush, _ntrycomplete, _ncontinue);
739 
740   // Mark completed
741   return true;
742 }
743 
744 void ZMark::flush_and_free() {
745   Thread* const thread = Thread::current();
746   flush_and_free(thread);
747 }
748 
749 bool ZMark::flush_and_free(Thread* thread) {
750   ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(thread);
751   const bool flushed = stacks-&gt;flush(&amp;_allocator, &amp;_stripes);
752   stacks-&gt;free(&amp;_allocator);
753   return flushed;
754 }
755 
756 class ZVerifyMarkStacksEmptyClosure : public ThreadClosure {
757 private:
758   const ZMarkStripeSet* const _stripes;
759 
760 public:
761   ZVerifyMarkStacksEmptyClosure(const ZMarkStripeSet* stripes) :
762       _stripes(stripes) {}
763 
764   void do_thread(Thread* thread) {
765     ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(thread);
766     guarantee(stacks-&gt;is_empty(_stripes), &quot;Should be empty&quot;);
767   }
768 };
769 
770 void ZMark::verify_all_stacks_empty() const {
771   // Verify thread stacks
772   ZVerifyMarkStacksEmptyClosure cl(&amp;_stripes);
773   Threads::threads_do(&amp;cl);
774 
775   // Verify stripe stacks
776   guarantee(_stripes.is_empty(), &quot;Should be empty&quot;);
777 }
    </pre>
  </body>
</html>