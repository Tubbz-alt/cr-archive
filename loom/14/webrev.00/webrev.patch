diff a/src/hotspot/cpu/x86/continuationChunk_x86.inline.hpp b/src/hotspot/cpu/x86/continuationChunk_x86.inline.hpp
--- a/src/hotspot/cpu/x86/continuationChunk_x86.inline.hpp
+++ b/src/hotspot/cpu/x86/continuationChunk_x86.inline.hpp
@@ -232,13 +232,13 @@
     assert (oopmap != NULL, "");
     log_develop_trace(jvmcont)("stack_chunk_iterate_stack slot: %d codeblob:", slot);
     if (log_develop_is_enabled(Trace, jvmcont)) cb->print_value_on(tty);
 
     if (Devirtualizer::do_metadata(closure) && cb->is_nmethod()) {
-      nmethod* nm = cb->as_nmethod();
-      nm->mark_as_maybe_on_continuation();
-      nm->oops_do(closure);
+      // The nmethod entry barrier takes care of having the right synchronization
+      // when keeping the nmethod alive during concurrent execution.
+      cb->as_nmethod_or_null()->run_nmethod_entry_barrier();
     }
 
     num_frames++;
     num_oops += oopmap->num_oops();
     if (closure == NULL) {
@@ -316,6 +316,6 @@
         omv.type() == OopMapValue::narrowoop_value ? Devirtualizer::do_oop(closure, (narrowOop*)p) : Devirtualizer::do_oop(closure, p);
     }
   }
 }
 
-#endif // CPU_X86_CONTINUATION_CHUNK_X86_INLINE_HPP
+#endif // CPU_X86_CONTINUATION_CHUNK_X86_INLINE_HPP
diff a/src/hotspot/cpu/x86/frame_x86.inline.hpp b/src/hotspot/cpu/x86/frame_x86.inline.hpp
--- a/src/hotspot/cpu/x86/frame_x86.inline.hpp
+++ b/src/hotspot/cpu/x86/frame_x86.inline.hpp
@@ -351,12 +351,16 @@
 // Compiled frames
 
 inline oop frame::saved_oop_result(RegisterMap* map) const {
   oop* result_adr = (oop *)map->location(rax->as_VMReg());
   guarantee(result_adr != NULL, "bad register save location");
+  oop result = *result_adr;
 
-  return (*result_adr);
+  // TODO: Erik: remove after integration with concurrent stack scanning
+  result = NativeAccess<>::oop_load(&result);
+
+  return result;
 }
 
 inline void frame::set_saved_oop_result(RegisterMap* map, oop obj) {
   oop* result_adr = (oop *)map->location(rax->as_VMReg());
   guarantee(result_adr != NULL, "bad register save location");
diff a/src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp b/src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp
--- a/src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp
+++ b/src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp
@@ -174,10 +174,19 @@
 
   NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);
   cmp->set_immediate(disarmed_value());
 }
 
+void BarrierSetNMethod::arm(nmethod* nm, int arm_value) {
+  if (!supports_entry_barrier(nm)) {
+    return;
+  }
+
+  NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);
+  cmp->set_immediate(arm_value);
+}
+
 bool BarrierSetNMethod::is_armed(nmethod* nm) {
   if (!supports_entry_barrier(nm)) {
     return false;
   }
 
diff a/src/hotspot/share/code/nmethod.cpp b/src/hotspot/share/code/nmethod.cpp
--- a/src/hotspot/share/code/nmethod.cpp
+++ b/src/hotspot/share/code/nmethod.cpp
@@ -1145,14 +1145,10 @@
 }
 
 void nmethod::mark_as_maybe_on_continuation() {
   assert(is_alive(), "Must be an alive method");
   _marking_cycle = CodeCache::marking_cycle();
-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();
-  if (bs_nm != NULL) {
-    bs_nm->disarm(this);
-  }
 }
 
 bool nmethod::is_not_on_continuation_stack() {
   // Odd marking cycles are found during concurrent marking. Even numbers are found
   // in nmethods that are marked when GC is inactive (e.g. nmethod entry barriers during
diff a/src/hotspot/share/gc/g1/g1CodeBlobClosure.cpp b/src/hotspot/share/gc/g1/g1CodeBlobClosure.cpp
--- a/src/hotspot/share/gc/g1/g1CodeBlobClosure.cpp
+++ b/src/hotspot/share/gc/g1/g1CodeBlobClosure.cpp
@@ -27,10 +27,11 @@
 #include "gc/g1/g1CodeBlobClosure.hpp"
 #include "gc/g1/g1CollectedHeap.inline.hpp"
 #include "gc/g1/g1ConcurrentMark.inline.hpp"
 #include "gc/g1/heapRegion.hpp"
 #include "gc/g1/heapRegionRemSet.hpp"
+#include "gc/shared/barrierSetNMethod.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/compressedOops.inline.hpp"
 #include "oops/oop.inline.hpp"
 
 template <typename T>
@@ -73,13 +74,19 @@
   do_oop_work(o);
 }
 
 void G1CodeBlobClosure::do_evacuation_and_fixup(nmethod* nm) {
   _oc.set_nm(nm);
-  nm->mark_as_maybe_on_continuation();
+  if (_keepalive_is_strong) {
+    nm->mark_as_maybe_on_continuation();
+  }
   nm->oops_do_keepalive(&_oc, _keepalive_is_strong);
   nm->fix_oop_relocations();
+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();
+  if (bs_nm != NULL) {
+    bs_nm->disarm(nm);
+  }
 }
 
 void G1CodeBlobClosure::do_marking(nmethod* nm) {
   nm->oops_do(&_marking_oc);
 }
diff a/src/hotspot/share/gc/shared/barrierSetNMethod.cpp b/src/hotspot/share/gc/shared/barrierSetNMethod.cpp
--- a/src/hotspot/share/gc/shared/barrierSetNMethod.cpp
+++ b/src/hotspot/share/gc/shared/barrierSetNMethod.cpp
@@ -59,10 +59,11 @@
 
   return true;
 }
 
 bool BarrierSetNMethod::nmethod_entry_barrier(nmethod* nm) {
+  nm->mark_as_maybe_on_continuation();
   LoadPhantomOopClosure cl;
   nm->oops_do(&cl);
   disarm(nm);
 
   return true;
diff a/src/hotspot/share/gc/shared/barrierSetNMethod.hpp b/src/hotspot/share/gc/shared/barrierSetNMethod.hpp
--- a/src/hotspot/share/gc/shared/barrierSetNMethod.hpp
+++ b/src/hotspot/share/gc/shared/barrierSetNMethod.hpp
@@ -48,10 +48,11 @@
 
   static int nmethod_stub_entry_barrier(address* return_address_ptr);
   bool nmethod_osr_entry_barrier(nmethod* nm);
   bool is_armed(nmethod* nm);
   void disarm(nmethod* nm);
+  void arm(nmethod* nm, int arm_value);
 
   void arm_all_nmethods();
 };
 
 
diff a/src/hotspot/share/gc/shared/collectedHeap.cpp b/src/hotspot/share/gc/shared/collectedHeap.cpp
--- a/src/hotspot/share/gc/shared/collectedHeap.cpp
+++ b/src/hotspot/share/gc/shared/collectedHeap.cpp
@@ -233,10 +233,11 @@
   assert(Heap_lock->is_locked(), "Precondition#2");
   GCCauseSetter gcs(this, cause);
   switch (cause) {
     case GCCause::_heap_inspection:
     case GCCause::_heap_dump:
+    case GCCause::_codecache_GC_threshold:
     case GCCause::_metadata_GC_threshold : {
       HandleMark hm;
       do_full_collection(false);        // don't clear all soft refs
       break;
     }
@@ -362,11 +363,11 @@
     }
 
     // Generate a VM operation
     VM_CollectForCodeCacheAllocation op(gc_count,
                                         full_gc_count,
-                                        GCCause::_metadata_GC_threshold);
+                                        GCCause::_codecache_GC_threshold);
     VMThread::execute(&op);
 
     // If GC was locked out, try again. Check before checking success because the
     // prologue could have succeeded and the GC still have been locked out.
     if (op.gc_locked()) {
diff a/src/hotspot/share/gc/shared/gcCause.cpp b/src/hotspot/share/gc/shared/gcCause.cpp
--- a/src/hotspot/share/gc/shared/gcCause.cpp
+++ b/src/hotspot/share/gc/shared/gcCause.cpp
@@ -76,10 +76,13 @@
       return "Tenured Generation Full";
 
     case _metadata_GC_threshold:
       return "Metadata GC Threshold";
 
+    case _codecache_GC_threshold:
+      return "CodeCache GC Threshold";
+
     case _metadata_GC_clear_soft_refs:
       return "Metadata GC Clear Soft References";
 
     case _old_generation_expanded_on_last_scavenge:
       return "Old Generation Expanded On Last Scavenge";
diff a/src/hotspot/share/gc/shared/gcCause.hpp b/src/hotspot/share/gc/shared/gcCause.hpp
--- a/src/hotspot/share/gc/shared/gcCause.hpp
+++ b/src/hotspot/share/gc/shared/gcCause.hpp
@@ -61,10 +61,11 @@
     _allocation_failure,
 
     /* implementation specific */
 
     _tenured_generation_full,
+    _codecache_GC_threshold,
     _metadata_GC_threshold,
     _metadata_GC_clear_soft_refs,
 
     _old_generation_expanded_on_last_scavenge,
     _old_generation_too_full_to_scavenge,
diff a/src/hotspot/share/gc/shared/gcVMOperations.cpp b/src/hotspot/share/gc/shared/gcVMOperations.cpp
--- a/src/hotspot/share/gc/shared/gcVMOperations.cpp
+++ b/src/hotspot/share/gc/shared/gcVMOperations.cpp
@@ -283,11 +283,11 @@
   GCCauseSetter gccs(heap, _gc_cause);
 
   log_debug(gc)("Full GC for CodeCache");
 
   // Don't clear the soft refs yet.
-  heap->collect_as_vm_thread(GCCause::_metadata_GC_threshold);
+  heap->collect_as_vm_thread(GCCause::_codecache_GC_threshold);
 
   log_debug(gc)("After GC for CodeCache");
 
   if (GCLocker::is_active_and_needs_gc()) {
     set_gc_locked();
diff a/src/hotspot/share/gc/z/zBarrierSetNMethod.cpp b/src/hotspot/share/gc/z/zBarrierSetNMethod.cpp
--- a/src/hotspot/share/gc/z/zBarrierSetNMethod.cpp
+++ b/src/hotspot/share/gc/z/zBarrierSetNMethod.cpp
@@ -53,11 +53,12 @@
     return false;
   }
 
   // Heal oops and disarm
   ZNMethodOopClosure cl;
-  ZNMethod::nmethod_oops_do(nm, &cl);
+  ZNMethod::nmethod_oops_do(nm, &cl, true /* keepalive_is_strong */);
+  nm->mark_as_maybe_on_continuation();
   disarm(nm);
 
   return true;
 }
 
diff a/src/hotspot/share/gc/z/zCollectedHeap.cpp b/src/hotspot/share/gc/z/zCollectedHeap.cpp
--- a/src/hotspot/share/gc/z/zCollectedHeap.cpp
+++ b/src/hotspot/share/gc/z/zCollectedHeap.cpp
@@ -189,10 +189,15 @@
 
 void ZCollectedHeap::collect(GCCause::Cause cause) {
   _driver->collect(cause);
 }
 
+void ZCollectedHeap::collect_for_codecache() {
+  // Start synchronous GC
+  collect(GCCause::_codecache_GC_threshold);
+}
+
 void ZCollectedHeap::collect_as_vm_thread(GCCause::Cause cause) {
   // These collection requests are ignored since ZGC can't run a synchronous
   // GC cycle from within the VM thread. This is considered benign, since the
   // only GC causes coming in here should be heap dumper and heap inspector.
   // However, neither the heap dumper nor the heap inspector really need a GC
diff a/src/hotspot/share/gc/z/zCollectedHeap.hpp b/src/hotspot/share/gc/z/zCollectedHeap.hpp
--- a/src/hotspot/share/gc/z/zCollectedHeap.hpp
+++ b/src/hotspot/share/gc/z/zCollectedHeap.hpp
@@ -72,11 +72,11 @@
   virtual bool is_maximal_no_gc() const;
   virtual bool is_in(const void* p) const;
   virtual bool requires_barriers(oop obj) const;
 
   // Continuation support
-  virtual void collect_for_codecache() {}
+  virtual void collect_for_codecache();
 
   virtual uint32_t hash_oop(oop obj) const;
 
   virtual oop array_allocate(Klass* klass, int size, int length, bool do_zero, TRAPS);
   virtual HeapWord* mem_allocate(size_t size, bool* gc_overhead_limit_was_exceeded);
diff a/src/hotspot/share/gc/z/zDriver.cpp b/src/hotspot/share/gc/z/zDriver.cpp
--- a/src/hotspot/share/gc/z/zDriver.cpp
+++ b/src/hotspot/share/gc/z/zDriver.cpp
@@ -239,10 +239,11 @@
   case GCCause::_java_lang_system_gc:
   case GCCause::_full_gc_alot:
   case GCCause::_scavenge_alot:
   case GCCause::_jvmti_force_gc:
   case GCCause::_metadata_GC_clear_soft_refs:
+  case GCCause::_codecache_GC_threshold:
     // Start synchronous GC
     _gc_cycle_port.send_sync(cause);
     break;
 
   case GCCause::_z_timer:
diff a/src/hotspot/share/gc/z/zMark.cpp b/src/hotspot/share/gc/z/zMark.cpp
--- a/src/hotspot/share/gc/z/zMark.cpp
+++ b/src/hotspot/share/gc/z/zMark.cpp
@@ -88,10 +88,12 @@
 void ZMark::prepare_mark() {
   // Increment global sequence number to invalidate
   // marking information for all pages.
   ZGlobalSeqNum++;
 
+  CodeCache::increment_marking_cycle();
+
   // Reset flush/continue counters
   _nproactiveflush = 0;
   _nterminateflush = 0;
   _ntrycomplete = 0;
   _ncontinue = 0;
@@ -735,10 +737,12 @@
   }
 
   // Update statistics
   ZStatMark::set_at_mark_end(_nproactiveflush, _nterminateflush, _ntrycomplete, _ncontinue);
 
+  CodeCache::increment_marking_cycle();
+
   // Mark completed
   return true;
 }
 
 void ZMark::flush_and_free() {
diff a/src/hotspot/share/gc/z/zNMethod.cpp b/src/hotspot/share/gc/z/zNMethod.cpp
--- a/src/hotspot/share/gc/z/zNMethod.cpp
+++ b/src/hotspot/share/gc/z/zNMethod.cpp
@@ -20,10 +20,11 @@
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  */
 
 #include "precompiled.hpp"
+#include "code/compiledMethod.inline.hpp"
 #include "code/relocInfo.hpp"
 #include "code/nmethod.hpp"
 #include "code/icBuffer.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "gc/shared/barrierSetNMethod.hpp"
@@ -210,11 +211,18 @@
   if (bs != NULL) {
     bs->disarm(nm);
   }
 }
 
-void ZNMethod::nmethod_oops_do(nmethod* nm, OopClosure* cl) {
+void ZNMethod::arm(nmethod* nm, int arm_value) {
+  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();
+  if (bs != NULL) {
+    bs->arm(nm, arm_value);
+  }
+}
+
+void ZNMethod::nmethod_oops_do(nmethod* nm, OopClosure* cl, bool keepalive_is_strong) {
   // Process oops table
   {
     oop* const begin = nm->oops_begin();
     oop* const end = nm->oops_end();
     for (oop* p = begin; p < end; p++) {
@@ -239,22 +247,31 @@
 
   // Process non-immediate oops
   if (oops->has_non_immediates()) {
     nm->fix_oop_relocations();
   }
+
+  if (keepalive_is_strong) {
+    oop* obj_ptr = nm->get_keepalive();
+    if (obj_ptr != NULL) {
+      cl->do_oop(obj_ptr);
+    }
+  }
 }
 
 class ZNMethodToOopsDoClosure : public NMethodClosure {
 private:
   OopClosure* _cl;
+  bool        _keepalive_is_strong;
 
 public:
-  ZNMethodToOopsDoClosure(OopClosure* cl) :
-      _cl(cl) {}
+  ZNMethodToOopsDoClosure(OopClosure* cl, bool keepalive_is_strong) :
+      _cl(cl),
+      _keepalive_is_strong(keepalive_is_strong) {}
 
   virtual void do_nmethod(nmethod* nm) {
-    ZNMethod::nmethod_oops_do(nm, _cl);
+    ZNMethod::nmethod_oops_do(nm, _cl, _keepalive_is_strong);
   }
 };
 
 void ZNMethod::oops_do_begin() {
   ZNMethodTable::nmethods_do_begin();
@@ -262,12 +279,12 @@
 
 void ZNMethod::oops_do_end() {
   ZNMethodTable::nmethods_do_end();
 }
 
-void ZNMethod::oops_do(OopClosure* cl) {
-  ZNMethodToOopsDoClosure nmethod_cl(cl);
+void ZNMethod::oops_do(OopClosure* cl, bool keepalive_is_strong) {
+  ZNMethodToOopsDoClosure nmethod_cl(cl, keepalive_is_strong);
   ZNMethodTable::nmethods_do(&nmethod_cl);
 }
 
 class ZNMethodUnlinkClosure : public NMethodClosure {
 private:
@@ -320,12 +337,12 @@
     ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));
 
     if (ZNMethod::is_armed(nm)) {
       // Heal oops and disarm
       ZNMethodOopClosure cl;
-      ZNMethod::nmethod_oops_do(nm, &cl);
-      ZNMethod::disarm(nm);
+      ZNMethod::nmethod_oops_do(nm, &cl, false /* keepalive_is_strong */);
+      ZNMethod::arm(nm, 0);
     }
 
     // Clear compiled ICs and exception caches
     if (!nm->unload_nmethod_caches(_unloading_occurred)) {
       set_failed();
diff a/src/hotspot/share/gc/z/zNMethod.hpp b/src/hotspot/share/gc/z/zNMethod.hpp
--- a/src/hotspot/share/gc/z/zNMethod.hpp
+++ b/src/hotspot/share/gc/z/zNMethod.hpp
@@ -45,16 +45,17 @@
 
   static bool supports_entry_barrier(nmethod* nm);
 
   static bool is_armed(nmethod* nm);
   static void disarm(nmethod* nm);
+  static void arm(nmethod* nm, int arm_value);
 
-  static void nmethod_oops_do(nmethod* nm, OopClosure* cl);
+  static void nmethod_oops_do(nmethod* nm, OopClosure* cl, bool keepalive_is_strong);
 
   static void oops_do_begin();
   static void oops_do_end();
-  static void oops_do(OopClosure* cl);
+  static void oops_do(OopClosure* cl, bool keepalive_is_strong);
 
   static ZReentrantLock* lock_for_nmethod(nmethod* nm);
 
   static void unlink(ZWorkers* workers, bool unloading_occurred);
   static void purge(ZWorkers* workers);
diff a/src/hotspot/share/gc/z/zRootsIterator.cpp b/src/hotspot/share/gc/z/zRootsIterator.cpp
--- a/src/hotspot/share/gc/z/zRootsIterator.cpp
+++ b/src/hotspot/share/gc/z/zRootsIterator.cpp
@@ -144,11 +144,12 @@
       _should_disarm_nmethods(cl->should_disarm_nmethods()) {}
 
   virtual void do_code_blob(CodeBlob* cb) {
     nmethod* const nm = cb->as_nmethod_or_null();
     if (nm != NULL && nm->oops_do_try_claim()) {
-      ZNMethod::nmethod_oops_do(nm, _cl);
+      ZNMethod::nmethod_oops_do(nm, _cl, _should_disarm_nmethods);
+      nm->mark_as_maybe_on_continuation();
       assert(!ZNMethod::supports_entry_barrier(nm) ||
              ZNMethod::is_armed(nm) == _should_disarm_nmethods, "Invalid state");
       if (_should_disarm_nmethods) {
         ZNMethod::disarm(nm);
       }
@@ -245,11 +246,11 @@
   _java_threads_iter.threads_do(&thread_cl);
 }
 
 void ZRootsIterator::do_code_cache(ZRootsIteratorClosure* cl) {
   ZStatTimer timer(ZSubPhasePauseRootsCodeCache);
-  ZNMethod::oops_do(cl);
+  ZNMethod::oops_do(cl, true /* keepalive_is_strong */);
 }
 
 void ZRootsIterator::oops_do(ZRootsIteratorClosure* cl) {
   ZStatTimer timer(ZSubPhasePauseRoots);
   _universe.oops_do(cl);
diff a/src/hotspot/share/gc/z/zUnload.cpp b/src/hotspot/share/gc/z/zUnload.cpp
--- a/src/hotspot/share/gc/z/zUnload.cpp
+++ b/src/hotspot/share/gc/z/zUnload.cpp
@@ -70,11 +70,11 @@
   virtual bool is_unloading(CompiledMethod* method) const {
     nmethod* const nm = method->as_nmethod();
     ZReentrantLock* const lock = ZNMethod::lock_for_nmethod(nm);
     ZLocker<ZReentrantLock> locker(lock);
     ZIsUnloadingOopClosure cl;
-    ZNMethod::nmethod_oops_do(nm, &cl);
+    ZNMethod::nmethod_oops_do(nm, &cl, false /* keepalive_is_strong */);
     return cl.is_unloading();
   }
 };
 
 class ZCompiledICProtectionBehaviour : public CompiledICProtectionBehaviour {
diff a/src/hotspot/share/memory/iterator.cpp b/src/hotspot/share/memory/iterator.cpp
--- a/src/hotspot/share/memory/iterator.cpp
+++ b/src/hotspot/share/memory/iterator.cpp
@@ -22,10 +22,11 @@
  *
  */
 
 #include "precompiled.hpp"
 #include "code/nmethod.hpp"
+#include "gc/shared/barrierSetNMethod.hpp"
 #include "memory/iterator.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "utilities/debug.hpp"
 #include "utilities/globalDefinitions.hpp"
 
@@ -59,8 +60,15 @@
 
 void MarkingCodeBlobClosure::do_code_blob(CodeBlob* cb) {
   nmethod* nm = cb->as_nmethod_or_null();
   if (nm != NULL && nm->oops_do_try_claim()) {
     nm->mark_as_maybe_on_continuation();
-    do_nmethod(nm);
+    nm->oops_do_keepalive(_cl, true /* keepalive_is_strong */);
+    if (_fix_relocations) {
+      nm->fix_oop_relocations();
+    }
+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();
+    if (bs_nm != NULL) {
+      bs_nm->disarm(nm);
+    }
   }
 }
diff a/src/hotspot/share/memory/iterator.hpp b/src/hotspot/share/memory/iterator.hpp
--- a/src/hotspot/share/memory/iterator.hpp
+++ b/src/hotspot/share/memory/iterator.hpp
@@ -240,13 +240,13 @@
 };
 
 // Applies an oop closure to all ref fields in code blobs
 // iterated over in an object iteration.
 class CodeBlobToOopClosure : public CodeBlobClosure {
+ protected:
   OopClosure* _cl;
   bool _fix_relocations;
- protected:
   void do_nmethod(nmethod* nm);
  public:
   // If fix_relocations(), then cl must copy objects to their new location immediately to avoid
   // patching nmethods with the old locations.
   CodeBlobToOopClosure(OopClosure* cl, bool fix_relocations) : _cl(cl), _fix_relocations(fix_relocations) {}
diff a/src/hotspot/share/runtime/continuation.cpp b/src/hotspot/share/runtime/continuation.cpp
--- a/src/hotspot/share/runtime/continuation.cpp
+++ b/src/hotspot/share/runtime/continuation.cpp
@@ -5942,11 +5942,11 @@
   Universe::heap()->is_in_or_null(resolved);
 #endif
 
 #ifndef PRODUCT
   CountOops count;
-  nm->oops_do(&count, false, true);
+  nm->oops_do(&count, false);
   assert(nm->nr_oops() >= count.nr_oops(), "should be");
 #endif
 
   if (resolved == NULL) {
     return;
diff a/src/hotspot/share/runtime/frame.cpp b/src/hotspot/share/runtime/frame.cpp
--- a/src/hotspot/share/runtime/frame.cpp
+++ b/src/hotspot/share/runtime/frame.cpp
@@ -887,10 +887,17 @@
   ArgumentSizeComputer asc(signature);
   int size = asc.size();
   return (oop *)interpreter_frame_tos_at(size);
 }
 
+oop frame::interpreter_callee_receiver(Symbol* signature) {
+  // TODO: Erik: remove after integration with concurrent stack scanning
+  oop r = *interpreter_callee_receiver_addr(signature);
+  r = NativeAccess<>::oop_load(&r);
+  return r;
+}
+
 void frame::oops_interpreted_do(OopClosure* f, const RegisterMap* map, bool query_oop_map_cache) {
   Thread *thread = Thread::current();
   methodHandle m (thread, interpreter_frame_method());
   jint bci = interpreter_frame_bci();
 
@@ -1097,10 +1104,12 @@
   if (oop_adr == NULL) {
     guarantee(oop_adr != NULL, "bad register save location");
     return NULL;
   }
   oop r = *oop_adr;
+  // TODO: Erik: remove after integration with concurrent stack scanning
+  r = NativeAccess<>::oop_load(&r);
   assert(Universe::heap()->is_in_or_null(r), "bad receiver: " INTPTR_FORMAT " (" INTX_FORMAT ")", p2i(r), p2i(r));
   return r;
 }
 
 
@@ -1118,10 +1127,12 @@
   assert(_cb != NULL && _cb->is_nmethod() && nm->method()->is_native(),
          "Should not call this unless it's a native nmethod");
   int byte_offset = in_bytes(nm->native_receiver_sp_offset());
   assert(byte_offset >= 0, "should not see invalid offset");
   oop owner = ((oop*) sp())[byte_offset / wordSize];
+  // TODO: Erik: remove after integration with concurrent stack scanning
+  owner = NativeAccess<>::oop_load(&owner);
   assert( Universe::heap()->is_in(owner), "bad receiver" );
   return owner;
 }
 
 void frame::oops_entry_do(OopClosure* f, const RegisterMap* map) {
diff a/src/hotspot/share/runtime/frame.hpp b/src/hotspot/share/runtime/frame.hpp
--- a/src/hotspot/share/runtime/frame.hpp
+++ b/src/hotspot/share/runtime/frame.hpp
@@ -284,11 +284,11 @@
   BasicLock* get_native_monitor();
   oop        get_native_receiver();
 
   // Find receiver for an invoke when arguments are just pushed on stack (i.e., callee stack-frame is
   // not setup)
-  oop interpreter_callee_receiver(Symbol* signature)     { return *interpreter_callee_receiver_addr(signature); }
+  oop interpreter_callee_receiver(Symbol* signature);
 
 
   oop* interpreter_callee_receiver_addr(Symbol* signature);
 
 
diff a/src/hotspot/share/runtime/sharedRuntime.cpp b/src/hotspot/share/runtime/sharedRuntime.cpp
--- a/src/hotspot/share/runtime/sharedRuntime.cpp
+++ b/src/hotspot/share/runtime/sharedRuntime.cpp
@@ -573,10 +573,12 @@
 oop SharedRuntime::retrieve_receiver( Symbol* sig, frame caller ) {
   assert(caller.is_interpreted_frame(), "");
   int args_size = ArgumentSizeComputer(sig).size() + 1;
   assert(args_size <= caller.interpreter_frame_expression_stack_size(), "receiver must be on interpreter stack");
   oop result = cast_to_oop(*caller.interpreter_frame_tos_at(args_size - 1));
+  // TODO: Erik: remove after integration with concurrent stack scanning
+  result = NativeAccess<>::oop_load(&result);
   assert(Universe::heap()->is_in(result) && oopDesc::is_oop(result), "receiver must be an oop");
   return result;
 }
 
 
diff a/src/hotspot/share/runtime/stackValue.cpp b/src/hotspot/share/runtime/stackValue.cpp
--- a/src/hotspot/share/runtime/stackValue.cpp
+++ b/src/hotspot/share/runtime/stackValue.cpp
@@ -96,32 +96,26 @@
         value.noop = *(narrowOop*) value_addr;
       }
       // Decode narrowoop
       oop val = CompressedOops::decode(value.noop);
       // Deoptimization must make sure all oops have passed load barriers
-#if INCLUDE_SHENANDOAHGC
-      if (UseShenandoahGC) {
-        val = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(val);
-      }
-#endif
+      // TODO: Erik: remove after integration with concurrent stack scanning
+      val = NativeAccess<>::oop_load(&val);
       Handle h(Thread::current(), val); // Wrap a handle around the oop
       return new StackValue(h);
     }
 #endif
     case Location::oop: {
       if (in_cont && UseCompressedOops) {
         narrowOop noop = *(narrowOop*) value_addr;
         oop val = CompressedOops::decode(noop);
-#if INCLUDE_SHENANDOAHGC
-        if (UseShenandoahGC) {
-          val = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(val);
-        }
-#endif
+        // TODO: Erik: remove after integration with concurrent stack scanning
+        val = NativeAccess<>::oop_load(&val);
         Handle h(Thread::current(), val);
         return new StackValue(h);
-      } 
-      
+      }
+
       oop val = *(oop *)value_addr;
 #ifdef _LP64
       if (CompressedOops::is_base(val)) {
          // Compiled code may produce decoded oop = narrow_oop_base
          // when a narrow oop implicit null check is used.
@@ -129,15 +123,12 @@
          // of the page below heap. Use NULL value for both cases.
          val = (oop)NULL;
       }
 #endif
       // Deoptimization must make sure all oops have passed load barriers
-#if INCLUDE_SHENANDOAHGC
-      if (UseShenandoahGC) {
-        val = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(val);
-      }
-#endif
+      // TODO: Erik: remove after integration with concurrent stack scanning
+      val = NativeAccess<>::oop_load(&val);
       assert(oopDesc::is_oop_or_null(val, false), "bad oop found");
       Handle h(Thread::current(), val); // Wrap a handle around the oop
       return new StackValue(h);
     }
     case Location::addr: {
