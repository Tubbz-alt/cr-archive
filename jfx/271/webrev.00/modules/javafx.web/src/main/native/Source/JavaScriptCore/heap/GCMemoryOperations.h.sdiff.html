<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/GCMemoryOperations.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="../dfg/DFGClobberize.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="../runtime/StructureInlines.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/GCMemoryOperations.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 36 // concurrent collector.
 37 
 38 constexpr size_t smallCutoff = 30 * 8;
 39 constexpr size_t mediumCutoff = 4 * 1024;
 40 
 41 // This is a forwards loop so gcSafeMemmove can rely on the direction.
 42 template &lt;typename T&gt;
 43 ALWAYS_INLINE void gcSafeMemcpy(T* dst, T* src, size_t bytes)
 44 {
 45     static_assert(sizeof(T) == sizeof(JSValue));
 46     RELEASE_ASSERT(bytes % 8 == 0);
 47 
 48 #if USE(JSVALUE64)
 49 
 50     auto slowPathForwardMemcpy = [&amp;] {
 51         size_t count = bytes / 8;
 52         for (unsigned i = 0; i &lt; count; ++i)
 53             bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = bitwise_cast&lt;volatile uint64_t*&gt;(src)[i];
 54     };
 55 
<span class="line-modified"> 56 #if COMPILER(GCC_COMPATIBLE) &amp;&amp; USE(JSVALUE64)</span>
 57     if (bytes &lt;= smallCutoff)
 58         slowPathForwardMemcpy();
 59     else if (isARM64() || bytes &lt;= mediumCutoff) {
 60 #if CPU(X86_64)
 61         size_t alignedBytes = (bytes / 64) * 64;
 62         size_t tmp;
 63         size_t offset = 0;
 64         asm volatile(
 65             &quot;.balign 32\t\n&quot;
 66             &quot;1:\t\n&quot;
 67             &quot;cmpq %q[offset], %q[alignedBytes]\t\n&quot;
 68             &quot;je 2f\t\n&quot;
 69             &quot;movups (%q[src], %q[offset], 1), %%xmm0\t\n&quot;
 70             &quot;movups 16(%q[src], %q[offset], 1), %%xmm1\t\n&quot;
 71             &quot;movups 32(%q[src], %q[offset], 1), %%xmm2\t\n&quot;
 72             &quot;movups 48(%q[src], %q[offset], 1), %%xmm3\t\n&quot;
 73             &quot;movups %%xmm0, (%q[dst], %q[offset], 1)\t\n&quot;
 74             &quot;movups %%xmm1, 16(%q[dst], %q[offset], 1)\t\n&quot;
 75             &quot;movups %%xmm2, 32(%q[dst], %q[offset], 1)\t\n&quot;
 76             &quot;movups %%xmm3, 48(%q[dst], %q[offset], 1)\t\n&quot;
</pre>
<hr />
<pre>
104             &quot;b.eq 2f\t\n&quot;
105             &quot;ldr q0, [%x[srcPtr], %x[offset]]\t\n&quot;
106             &quot;str q0, [%x[dstPtr], %x[offset]]\t\n&quot;
107             &quot;add %x[offset], %x[offset], #0x10\t\n&quot;
108             &quot;b 1b\t\n&quot;
109 
110             &quot;2:\t\n&quot;
111             &quot;cmp %x[offset], %x[bytes]\t\n&quot;
112             &quot;b.eq 3f\t\n&quot;
113             &quot;ldr d0, [%x[srcPtr], %x[offset]]\t\n&quot;
114             &quot;str d0, [%x[dstPtr], %x[offset]]\t\n&quot;
115             &quot;add %x[offset], %x[offset], #0x8\t\n&quot;
116             &quot;b 2b\t\n&quot;
117 
118             &quot;3:\t\n&quot;
119 
120             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [offset] &quot;+r&quot; (offset), [dstPtr] &quot;+r&quot; (dstPtr), [srcPtr] &quot;+r&quot; (srcPtr)
121             :
122             : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
123         );
<span class="line-removed">124 #else</span>
<span class="line-removed">125     slowPathForwardMemcpy();</span>
126 #endif // CPU(X86_64)
127     } else {
128         RELEASE_ASSERT(isX86_64());
129 #if CPU(X86_64)
130         size_t count = bytes / 8;
131         asm volatile(
132             &quot;.balign 16\t\n&quot;
133             &quot;cld\t\n&quot;
134             &quot;rep movsq\t\n&quot;
135             : &quot;+D&quot; (dst), &quot;+S&quot; (src), &quot;+c&quot; (count)
136             :
137             : &quot;memory&quot;);
138 #endif // CPU(X86_64)
139     }
140 #else
141     slowPathForwardMemcpy();
<span class="line-modified">142 #endif // COMPILER(GCC_COMPATIBLE)</span>
143 #else
144     memcpy(dst, src, bytes);
145 #endif // USE(JSVALUE64)
146 }
147 
148 template &lt;typename T&gt;
149 ALWAYS_INLINE void gcSafeMemmove(T* dst, T* src, size_t bytes)
150 {
151     static_assert(sizeof(T) == sizeof(JSValue));
152     RELEASE_ASSERT(bytes % 8 == 0);
153 #if USE(JSVALUE64)
154     if (bitwise_cast&lt;uintptr_t&gt;(src) &gt;= bitwise_cast&lt;uintptr_t&gt;(dst)) {
155         // This is written to do a forwards loop, so calling it is ok.
156         gcSafeMemcpy(dst, src, bytes);
157         return;
158     }
159 
160     if ((static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src)) + static_cast&lt;uint64_t&gt;(bytes)) &lt;= static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst))) {
161         gcSafeMemcpy(dst, src, bytes);
162         return;
</pre>
</td>
<td>
<hr />
<pre>
 36 // concurrent collector.
 37 
 38 constexpr size_t smallCutoff = 30 * 8;
 39 constexpr size_t mediumCutoff = 4 * 1024;
 40 
 41 // This is a forwards loop so gcSafeMemmove can rely on the direction.
 42 template &lt;typename T&gt;
 43 ALWAYS_INLINE void gcSafeMemcpy(T* dst, T* src, size_t bytes)
 44 {
 45     static_assert(sizeof(T) == sizeof(JSValue));
 46     RELEASE_ASSERT(bytes % 8 == 0);
 47 
 48 #if USE(JSVALUE64)
 49 
 50     auto slowPathForwardMemcpy = [&amp;] {
 51         size_t count = bytes / 8;
 52         for (unsigned i = 0; i &lt; count; ++i)
 53             bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = bitwise_cast&lt;volatile uint64_t*&gt;(src)[i];
 54     };
 55 
<span class="line-modified"> 56 #if COMPILER(GCC_COMPATIBLE) &amp;&amp; (CPU(X86_64) || CPU(ARM64))</span>
 57     if (bytes &lt;= smallCutoff)
 58         slowPathForwardMemcpy();
 59     else if (isARM64() || bytes &lt;= mediumCutoff) {
 60 #if CPU(X86_64)
 61         size_t alignedBytes = (bytes / 64) * 64;
 62         size_t tmp;
 63         size_t offset = 0;
 64         asm volatile(
 65             &quot;.balign 32\t\n&quot;
 66             &quot;1:\t\n&quot;
 67             &quot;cmpq %q[offset], %q[alignedBytes]\t\n&quot;
 68             &quot;je 2f\t\n&quot;
 69             &quot;movups (%q[src], %q[offset], 1), %%xmm0\t\n&quot;
 70             &quot;movups 16(%q[src], %q[offset], 1), %%xmm1\t\n&quot;
 71             &quot;movups 32(%q[src], %q[offset], 1), %%xmm2\t\n&quot;
 72             &quot;movups 48(%q[src], %q[offset], 1), %%xmm3\t\n&quot;
 73             &quot;movups %%xmm0, (%q[dst], %q[offset], 1)\t\n&quot;
 74             &quot;movups %%xmm1, 16(%q[dst], %q[offset], 1)\t\n&quot;
 75             &quot;movups %%xmm2, 32(%q[dst], %q[offset], 1)\t\n&quot;
 76             &quot;movups %%xmm3, 48(%q[dst], %q[offset], 1)\t\n&quot;
</pre>
<hr />
<pre>
104             &quot;b.eq 2f\t\n&quot;
105             &quot;ldr q0, [%x[srcPtr], %x[offset]]\t\n&quot;
106             &quot;str q0, [%x[dstPtr], %x[offset]]\t\n&quot;
107             &quot;add %x[offset], %x[offset], #0x10\t\n&quot;
108             &quot;b 1b\t\n&quot;
109 
110             &quot;2:\t\n&quot;
111             &quot;cmp %x[offset], %x[bytes]\t\n&quot;
112             &quot;b.eq 3f\t\n&quot;
113             &quot;ldr d0, [%x[srcPtr], %x[offset]]\t\n&quot;
114             &quot;str d0, [%x[dstPtr], %x[offset]]\t\n&quot;
115             &quot;add %x[offset], %x[offset], #0x8\t\n&quot;
116             &quot;b 2b\t\n&quot;
117 
118             &quot;3:\t\n&quot;
119 
120             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [offset] &quot;+r&quot; (offset), [dstPtr] &quot;+r&quot; (dstPtr), [srcPtr] &quot;+r&quot; (srcPtr)
121             :
122             : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
123         );


124 #endif // CPU(X86_64)
125     } else {
126         RELEASE_ASSERT(isX86_64());
127 #if CPU(X86_64)
128         size_t count = bytes / 8;
129         asm volatile(
130             &quot;.balign 16\t\n&quot;
131             &quot;cld\t\n&quot;
132             &quot;rep movsq\t\n&quot;
133             : &quot;+D&quot; (dst), &quot;+S&quot; (src), &quot;+c&quot; (count)
134             :
135             : &quot;memory&quot;);
136 #endif // CPU(X86_64)
137     }
138 #else
139     slowPathForwardMemcpy();
<span class="line-modified">140 #endif // COMPILER(GCC_COMPATIBLE) &amp;&amp; (CPU(X86_64) || CPU(ARM64))</span>
141 #else
142     memcpy(dst, src, bytes);
143 #endif // USE(JSVALUE64)
144 }
145 
146 template &lt;typename T&gt;
147 ALWAYS_INLINE void gcSafeMemmove(T* dst, T* src, size_t bytes)
148 {
149     static_assert(sizeof(T) == sizeof(JSValue));
150     RELEASE_ASSERT(bytes % 8 == 0);
151 #if USE(JSVALUE64)
152     if (bitwise_cast&lt;uintptr_t&gt;(src) &gt;= bitwise_cast&lt;uintptr_t&gt;(dst)) {
153         // This is written to do a forwards loop, so calling it is ok.
154         gcSafeMemcpy(dst, src, bytes);
155         return;
156     }
157 
158     if ((static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src)) + static_cast&lt;uint64_t&gt;(bytes)) &lt;= static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst))) {
159         gcSafeMemcpy(dst, src, bytes);
160         return;
</pre>
</td>
</tr>
</table>
<center><a href="../dfg/DFGClobberize.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="../runtime/StructureInlines.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>