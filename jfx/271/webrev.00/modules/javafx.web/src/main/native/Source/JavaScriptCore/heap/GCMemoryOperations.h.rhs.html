<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/GCMemoryOperations.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2016-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #include &quot;CPU.h&quot;
 29 #include &quot;JSCJSValue.h&quot;
 30 
 31 namespace JSC {
 32 
 33 // We use these memory operations when modifying memory that might be scanned by the concurrent collector.
 34 // We don&#39;t call the default operations because they&#39;re not guaranteed to store to memory in eight byte aligned
 35 // chunks. If we happened to fall into the system&#39;s normal byte copy loop, we may see a torn JSValue in the
 36 // concurrent collector.
 37 
 38 constexpr size_t smallCutoff = 30 * 8;
 39 constexpr size_t mediumCutoff = 4 * 1024;
 40 
 41 // This is a forwards loop so gcSafeMemmove can rely on the direction.
 42 template &lt;typename T&gt;
 43 ALWAYS_INLINE void gcSafeMemcpy(T* dst, T* src, size_t bytes)
 44 {
 45     static_assert(sizeof(T) == sizeof(JSValue));
 46     RELEASE_ASSERT(bytes % 8 == 0);
 47 
 48 #if USE(JSVALUE64)
 49 
 50     auto slowPathForwardMemcpy = [&amp;] {
 51         size_t count = bytes / 8;
 52         for (unsigned i = 0; i &lt; count; ++i)
 53             bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = bitwise_cast&lt;volatile uint64_t*&gt;(src)[i];
 54     };
 55 
<a name="1" id="anc1"></a><span class="line-modified"> 56 #if COMPILER(GCC_COMPATIBLE) &amp;&amp; (CPU(X86_64) || CPU(ARM64))</span>
 57     if (bytes &lt;= smallCutoff)
 58         slowPathForwardMemcpy();
 59     else if (isARM64() || bytes &lt;= mediumCutoff) {
 60 #if CPU(X86_64)
 61         size_t alignedBytes = (bytes / 64) * 64;
 62         size_t tmp;
 63         size_t offset = 0;
 64         asm volatile(
 65             &quot;.balign 32\t\n&quot;
 66             &quot;1:\t\n&quot;
 67             &quot;cmpq %q[offset], %q[alignedBytes]\t\n&quot;
 68             &quot;je 2f\t\n&quot;
 69             &quot;movups (%q[src], %q[offset], 1), %%xmm0\t\n&quot;
 70             &quot;movups 16(%q[src], %q[offset], 1), %%xmm1\t\n&quot;
 71             &quot;movups 32(%q[src], %q[offset], 1), %%xmm2\t\n&quot;
 72             &quot;movups 48(%q[src], %q[offset], 1), %%xmm3\t\n&quot;
 73             &quot;movups %%xmm0, (%q[dst], %q[offset], 1)\t\n&quot;
 74             &quot;movups %%xmm1, 16(%q[dst], %q[offset], 1)\t\n&quot;
 75             &quot;movups %%xmm2, 32(%q[dst], %q[offset], 1)\t\n&quot;
 76             &quot;movups %%xmm3, 48(%q[dst], %q[offset], 1)\t\n&quot;
 77             &quot;addq $64, %q[offset]\t\n&quot;
 78             &quot;jmp 1b\t\n&quot;
 79 
 80             &quot;2:\t\n&quot;
 81             &quot;cmpq %q[offset], %q[bytes]\t\n&quot;
 82             &quot;je 3f\t\n&quot;
 83             &quot;movq (%q[src], %q[offset], 1), %q[tmp]\t\n&quot;
 84             &quot;movq %q[tmp], (%q[dst], %q[offset], 1)\t\n&quot;
 85             &quot;addq $8, %q[offset]\t\n&quot;
 86             &quot;jmp 2b\t\n&quot;
 87 
 88             &quot;3:\t\n&quot;
 89 
 90             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [tmp] &quot;+r&quot; (tmp), [offset] &quot;+r&quot; (offset), [dst] &quot;+r&quot; (dst), [src] &quot;+r&quot; (src)
 91             :
 92             : &quot;xmm0&quot;, &quot;xmm1&quot;, &quot;xmm2&quot;, &quot;xmm3&quot;, &quot;memory&quot;, &quot;cc&quot;
 93         );
 94 #elif CPU(ARM64)
 95         uint64_t alignedBytes = (static_cast&lt;uint64_t&gt;(bytes) / 16) * 16;
 96         size_t offset = 0;
 97 
 98         uint64_t dstPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst));
 99         uint64_t srcPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src));
100 
101         asm volatile(
102             &quot;1:\t\n&quot;
103             &quot;cmp %x[offset], %x[alignedBytes]\t\n&quot;
104             &quot;b.eq 2f\t\n&quot;
105             &quot;ldr q0, [%x[srcPtr], %x[offset]]\t\n&quot;
106             &quot;str q0, [%x[dstPtr], %x[offset]]\t\n&quot;
107             &quot;add %x[offset], %x[offset], #0x10\t\n&quot;
108             &quot;b 1b\t\n&quot;
109 
110             &quot;2:\t\n&quot;
111             &quot;cmp %x[offset], %x[bytes]\t\n&quot;
112             &quot;b.eq 3f\t\n&quot;
113             &quot;ldr d0, [%x[srcPtr], %x[offset]]\t\n&quot;
114             &quot;str d0, [%x[dstPtr], %x[offset]]\t\n&quot;
115             &quot;add %x[offset], %x[offset], #0x8\t\n&quot;
116             &quot;b 2b\t\n&quot;
117 
118             &quot;3:\t\n&quot;
119 
120             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [offset] &quot;+r&quot; (offset), [dstPtr] &quot;+r&quot; (dstPtr), [srcPtr] &quot;+r&quot; (srcPtr)
121             :
122             : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
123         );
<a name="2" id="anc2"></a>

124 #endif // CPU(X86_64)
125     } else {
126         RELEASE_ASSERT(isX86_64());
127 #if CPU(X86_64)
128         size_t count = bytes / 8;
129         asm volatile(
130             &quot;.balign 16\t\n&quot;
131             &quot;cld\t\n&quot;
132             &quot;rep movsq\t\n&quot;
133             : &quot;+D&quot; (dst), &quot;+S&quot; (src), &quot;+c&quot; (count)
134             :
135             : &quot;memory&quot;);
136 #endif // CPU(X86_64)
137     }
138 #else
139     slowPathForwardMemcpy();
<a name="3" id="anc3"></a><span class="line-modified">140 #endif // COMPILER(GCC_COMPATIBLE) &amp;&amp; (CPU(X86_64) || CPU(ARM64))</span>
141 #else
142     memcpy(dst, src, bytes);
143 #endif // USE(JSVALUE64)
144 }
145 
146 template &lt;typename T&gt;
147 ALWAYS_INLINE void gcSafeMemmove(T* dst, T* src, size_t bytes)
148 {
149     static_assert(sizeof(T) == sizeof(JSValue));
150     RELEASE_ASSERT(bytes % 8 == 0);
151 #if USE(JSVALUE64)
152     if (bitwise_cast&lt;uintptr_t&gt;(src) &gt;= bitwise_cast&lt;uintptr_t&gt;(dst)) {
153         // This is written to do a forwards loop, so calling it is ok.
154         gcSafeMemcpy(dst, src, bytes);
155         return;
156     }
157 
158     if ((static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src)) + static_cast&lt;uint64_t&gt;(bytes)) &lt;= static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst))) {
159         gcSafeMemcpy(dst, src, bytes);
160         return;
161     }
162 
163     auto slowPathBackwardsMemmove = [&amp;] {
164         size_t count = bytes / 8;
165         for (size_t i = count; i--; )
166             bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = bitwise_cast&lt;volatile uint64_t*&gt;(src)[i];
167     };
168 
169 #if COMPILER(GCC_COMPATIBLE)
170     if (bytes &lt;= smallCutoff)
171         slowPathBackwardsMemmove();
172     else {
173 #if CPU(X86_64)
174         size_t alignedBytes = (bytes / 64) * 64;
175 
176         size_t tail = alignedBytes;
177         size_t tmp;
178         asm volatile(
179             &quot;2:\t\n&quot;
180             &quot;cmpq %q[tail], %q[bytes]\t\n&quot;
181             &quot;je 1f\t\n&quot;
182             &quot;addq $-8, %q[bytes]\t\n&quot;
183             &quot;movq (%q[src], %q[bytes], 1), %q[tmp]\t\n&quot;
184             &quot;movq %q[tmp], (%q[dst], %q[bytes], 1)\t\n&quot;
185             &quot;jmp 2b\t\n&quot;
186 
187             &quot;1:\t\n&quot;
188             &quot;test %q[alignedBytes], %q[alignedBytes]\t\n&quot;
189             &quot;jz 3f\t\n&quot;
190 
191             &quot;.balign 32\t\n&quot;
192             &quot;100:\t\n&quot;
193 
194             &quot;movups -64(%q[src], %q[alignedBytes], 1), %%xmm0\t\n&quot;
195             &quot;movups -48(%q[src], %q[alignedBytes], 1), %%xmm1\t\n&quot;
196             &quot;movups -32(%q[src], %q[alignedBytes], 1), %%xmm2\t\n&quot;
197             &quot;movups -16(%q[src], %q[alignedBytes], 1), %%xmm3\t\n&quot;
198             &quot;movups %%xmm0, -64(%q[dst], %q[alignedBytes], 1)\t\n&quot;
199             &quot;movups %%xmm1, -48(%q[dst], %q[alignedBytes], 1)\t\n&quot;
200             &quot;movups %%xmm2, -32(%q[dst], %q[alignedBytes], 1)\t\n&quot;
201             &quot;movups %%xmm3, -16(%q[dst], %q[alignedBytes], 1)\t\n&quot;
202             &quot;addq $-64, %q[alignedBytes]\t\n&quot;
203             &quot;jnz 100b\t\n&quot;
204 
205             &quot;3:\t\n&quot;
206 
207             : [alignedBytes] &quot;+r&quot; (alignedBytes), [tail] &quot;+r&quot; (tail), [bytes] &quot;+r&quot; (bytes), [tmp] &quot;+r&quot; (tmp), [dst] &quot;+r&quot; (dst), [src] &quot;+r&quot; (src)
208             :
209             : &quot;xmm0&quot;, &quot;xmm1&quot;, &quot;xmm2&quot;, &quot;xmm3&quot;, &quot;memory&quot;, &quot;cc&quot;
210         );
211 #elif CPU(ARM64)
212         uint64_t alignedBytes = (static_cast&lt;uint64_t&gt;(bytes) / 16) * 16;
213         uint64_t dstPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst));
214         uint64_t srcPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src));
215 
216         asm volatile(
217             &quot;1:\t\n&quot;
218             &quot;cmp %x[alignedBytes], %x[bytes]\t\n&quot;
219             &quot;b.eq 2f\t\n&quot;
220             &quot;sub %x[bytes], %x[bytes], #0x8\t\n&quot;
221             &quot;ldr d0, [%x[srcPtr], %x[bytes]]\t\n&quot;
222             &quot;str d0, [%x[dstPtr], %x[bytes]]\t\n&quot;
223             &quot;b 1b\t\n&quot;
224 
225             &quot;2:\t\n&quot;
226             &quot;cbz %x[alignedBytes], 3f\t\n&quot;
227             &quot;sub %x[alignedBytes], %x[alignedBytes], #0x10\t\n&quot;
228             &quot;ldr q0, [%x[srcPtr], %x[alignedBytes]]\t\n&quot;
229             &quot;str q0, [%x[dstPtr], %x[alignedBytes]]\t\n&quot;
230             &quot;b 2b\t\n&quot;
231 
232             &quot;3:\t\n&quot;
233 
234             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [dstPtr] &quot;+r&quot; (dstPtr), [srcPtr] &quot;+r&quot; (srcPtr)
235             :
236             : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
237         );
238 #else
239         slowPathBackwardsMemmove();
240 #endif // CPU(X86_64)
241     }
242 #else
243     slowPathBackwardsMemmove();
244 #endif // COMPILER(GCC_COMPATIBLE)
245 #else
246     memmove(dst, src, bytes);
247 #endif // USE(JSVALUE64)
248 }
249 
250 template &lt;typename T&gt;
251 ALWAYS_INLINE void gcSafeZeroMemory(T* dst, size_t bytes)
252 {
253     static_assert(sizeof(T) == sizeof(JSValue));
254     RELEASE_ASSERT(bytes % 8 == 0);
255 #if USE(JSVALUE64)
256 #if COMPILER(GCC_COMPATIBLE)
257 #if CPU(X86_64)
258     uint64_t zero = 0;
259     size_t count = bytes / 8;
260     asm volatile (
261         &quot;rep stosq\n\t&quot;
262         : &quot;+D&quot;(dst), &quot;+c&quot;(count)
263         : &quot;a&quot;(zero)
264         : &quot;memory&quot;
265     );
266 #elif CPU(ARM64)
267     uint64_t alignedBytes = (static_cast&lt;uint64_t&gt;(bytes) / 64) * 64;
268     uint64_t dstPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst));
269     uint64_t end = dstPtr + bytes;
270     uint64_t alignedEnd = dstPtr + alignedBytes;
271     asm volatile(
272         &quot;movi d0, #0\t\n&quot;
273         &quot;movi d1, #0\t\n&quot;
274 
275         &quot;.p2align 4\t\n&quot;
276         &quot;2:\t\n&quot;
277         &quot;cmp %x[dstPtr], %x[alignedEnd]\t\n&quot;
278         &quot;b.eq 4f\t\n&quot;
279         &quot;stnp q0, q0, [%x[dstPtr]]\t\n&quot;
280         &quot;stnp q0, q0, [%x[dstPtr], #0x20]\t\n&quot;
281         &quot;add %x[dstPtr], %x[dstPtr], #0x40\t\n&quot;
282         &quot;b 2b\t\n&quot;
283 
284         &quot;4:\t\n&quot;
285         &quot;cmp %x[dstPtr], %x[end]\t\n&quot;
286         &quot;b.eq 5f\t\n&quot;
287         &quot;str d0, [%x[dstPtr]], #0x8\t\n&quot;
288         &quot;b 4b\t\n&quot;
289 
290         &quot;5:\t\n&quot;
291 
292         : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [dstPtr] &quot;+r&quot; (dstPtr), [end] &quot;+r&quot; (end), [alignedEnd] &quot;+r&quot; (alignedEnd)
293         :
294         : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
295     );
296 #else
297     size_t count = bytes / 8;
298     for (size_t i = 0; i &lt; count; ++i)
299         bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = 0;
300 #endif // CPU(X86_64)
301 #else
302     size_t count = bytes / 8;
303     for (size_t i = 0; i &lt; count; ++i)
304         bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = 0;
305 #endif // COMPILER(GCC_COMPATIBLE)
306 #else
307     memset(reinterpret_cast&lt;char*&gt;(dst), 0, bytes);
308 #endif // USE(JSVALUE64)
309 }
310 
311 } // namespace JSC
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>