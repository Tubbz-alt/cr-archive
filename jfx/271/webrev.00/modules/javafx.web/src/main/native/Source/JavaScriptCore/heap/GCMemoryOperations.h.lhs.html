<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/GCMemoryOperations.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2016-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #include &quot;CPU.h&quot;
 29 #include &quot;JSCJSValue.h&quot;
 30 
 31 namespace JSC {
 32 
 33 // We use these memory operations when modifying memory that might be scanned by the concurrent collector.
 34 // We don&#39;t call the default operations because they&#39;re not guaranteed to store to memory in eight byte aligned
 35 // chunks. If we happened to fall into the system&#39;s normal byte copy loop, we may see a torn JSValue in the
 36 // concurrent collector.
 37 
 38 constexpr size_t smallCutoff = 30 * 8;
 39 constexpr size_t mediumCutoff = 4 * 1024;
 40 
 41 // This is a forwards loop so gcSafeMemmove can rely on the direction.
 42 template &lt;typename T&gt;
 43 ALWAYS_INLINE void gcSafeMemcpy(T* dst, T* src, size_t bytes)
 44 {
 45     static_assert(sizeof(T) == sizeof(JSValue));
 46     RELEASE_ASSERT(bytes % 8 == 0);
 47 
 48 #if USE(JSVALUE64)
 49 
 50     auto slowPathForwardMemcpy = [&amp;] {
 51         size_t count = bytes / 8;
 52         for (unsigned i = 0; i &lt; count; ++i)
 53             bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = bitwise_cast&lt;volatile uint64_t*&gt;(src)[i];
 54     };
 55 
<a name="1" id="anc1"></a><span class="line-modified"> 56 #if COMPILER(GCC_COMPATIBLE) &amp;&amp; USE(JSVALUE64)</span>
 57     if (bytes &lt;= smallCutoff)
 58         slowPathForwardMemcpy();
 59     else if (isARM64() || bytes &lt;= mediumCutoff) {
 60 #if CPU(X86_64)
 61         size_t alignedBytes = (bytes / 64) * 64;
 62         size_t tmp;
 63         size_t offset = 0;
 64         asm volatile(
 65             &quot;.balign 32\t\n&quot;
 66             &quot;1:\t\n&quot;
 67             &quot;cmpq %q[offset], %q[alignedBytes]\t\n&quot;
 68             &quot;je 2f\t\n&quot;
 69             &quot;movups (%q[src], %q[offset], 1), %%xmm0\t\n&quot;
 70             &quot;movups 16(%q[src], %q[offset], 1), %%xmm1\t\n&quot;
 71             &quot;movups 32(%q[src], %q[offset], 1), %%xmm2\t\n&quot;
 72             &quot;movups 48(%q[src], %q[offset], 1), %%xmm3\t\n&quot;
 73             &quot;movups %%xmm0, (%q[dst], %q[offset], 1)\t\n&quot;
 74             &quot;movups %%xmm1, 16(%q[dst], %q[offset], 1)\t\n&quot;
 75             &quot;movups %%xmm2, 32(%q[dst], %q[offset], 1)\t\n&quot;
 76             &quot;movups %%xmm3, 48(%q[dst], %q[offset], 1)\t\n&quot;
 77             &quot;addq $64, %q[offset]\t\n&quot;
 78             &quot;jmp 1b\t\n&quot;
 79 
 80             &quot;2:\t\n&quot;
 81             &quot;cmpq %q[offset], %q[bytes]\t\n&quot;
 82             &quot;je 3f\t\n&quot;
 83             &quot;movq (%q[src], %q[offset], 1), %q[tmp]\t\n&quot;
 84             &quot;movq %q[tmp], (%q[dst], %q[offset], 1)\t\n&quot;
 85             &quot;addq $8, %q[offset]\t\n&quot;
 86             &quot;jmp 2b\t\n&quot;
 87 
 88             &quot;3:\t\n&quot;
 89 
 90             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [tmp] &quot;+r&quot; (tmp), [offset] &quot;+r&quot; (offset), [dst] &quot;+r&quot; (dst), [src] &quot;+r&quot; (src)
 91             :
 92             : &quot;xmm0&quot;, &quot;xmm1&quot;, &quot;xmm2&quot;, &quot;xmm3&quot;, &quot;memory&quot;, &quot;cc&quot;
 93         );
 94 #elif CPU(ARM64)
 95         uint64_t alignedBytes = (static_cast&lt;uint64_t&gt;(bytes) / 16) * 16;
 96         size_t offset = 0;
 97 
 98         uint64_t dstPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst));
 99         uint64_t srcPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src));
100 
101         asm volatile(
102             &quot;1:\t\n&quot;
103             &quot;cmp %x[offset], %x[alignedBytes]\t\n&quot;
104             &quot;b.eq 2f\t\n&quot;
105             &quot;ldr q0, [%x[srcPtr], %x[offset]]\t\n&quot;
106             &quot;str q0, [%x[dstPtr], %x[offset]]\t\n&quot;
107             &quot;add %x[offset], %x[offset], #0x10\t\n&quot;
108             &quot;b 1b\t\n&quot;
109 
110             &quot;2:\t\n&quot;
111             &quot;cmp %x[offset], %x[bytes]\t\n&quot;
112             &quot;b.eq 3f\t\n&quot;
113             &quot;ldr d0, [%x[srcPtr], %x[offset]]\t\n&quot;
114             &quot;str d0, [%x[dstPtr], %x[offset]]\t\n&quot;
115             &quot;add %x[offset], %x[offset], #0x8\t\n&quot;
116             &quot;b 2b\t\n&quot;
117 
118             &quot;3:\t\n&quot;
119 
120             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [offset] &quot;+r&quot; (offset), [dstPtr] &quot;+r&quot; (dstPtr), [srcPtr] &quot;+r&quot; (srcPtr)
121             :
122             : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
123         );
<a name="2" id="anc2"></a><span class="line-removed">124 #else</span>
<span class="line-removed">125     slowPathForwardMemcpy();</span>
126 #endif // CPU(X86_64)
127     } else {
128         RELEASE_ASSERT(isX86_64());
129 #if CPU(X86_64)
130         size_t count = bytes / 8;
131         asm volatile(
132             &quot;.balign 16\t\n&quot;
133             &quot;cld\t\n&quot;
134             &quot;rep movsq\t\n&quot;
135             : &quot;+D&quot; (dst), &quot;+S&quot; (src), &quot;+c&quot; (count)
136             :
137             : &quot;memory&quot;);
138 #endif // CPU(X86_64)
139     }
140 #else
141     slowPathForwardMemcpy();
<a name="3" id="anc3"></a><span class="line-modified">142 #endif // COMPILER(GCC_COMPATIBLE)</span>
143 #else
144     memcpy(dst, src, bytes);
145 #endif // USE(JSVALUE64)
146 }
147 
148 template &lt;typename T&gt;
149 ALWAYS_INLINE void gcSafeMemmove(T* dst, T* src, size_t bytes)
150 {
151     static_assert(sizeof(T) == sizeof(JSValue));
152     RELEASE_ASSERT(bytes % 8 == 0);
153 #if USE(JSVALUE64)
154     if (bitwise_cast&lt;uintptr_t&gt;(src) &gt;= bitwise_cast&lt;uintptr_t&gt;(dst)) {
155         // This is written to do a forwards loop, so calling it is ok.
156         gcSafeMemcpy(dst, src, bytes);
157         return;
158     }
159 
160     if ((static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src)) + static_cast&lt;uint64_t&gt;(bytes)) &lt;= static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst))) {
161         gcSafeMemcpy(dst, src, bytes);
162         return;
163     }
164 
165     auto slowPathBackwardsMemmove = [&amp;] {
166         size_t count = bytes / 8;
167         for (size_t i = count; i--; )
168             bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = bitwise_cast&lt;volatile uint64_t*&gt;(src)[i];
169     };
170 
171 #if COMPILER(GCC_COMPATIBLE)
172     if (bytes &lt;= smallCutoff)
173         slowPathBackwardsMemmove();
174     else {
175 #if CPU(X86_64)
176         size_t alignedBytes = (bytes / 64) * 64;
177 
178         size_t tail = alignedBytes;
179         size_t tmp;
180         asm volatile(
181             &quot;2:\t\n&quot;
182             &quot;cmpq %q[tail], %q[bytes]\t\n&quot;
183             &quot;je 1f\t\n&quot;
184             &quot;addq $-8, %q[bytes]\t\n&quot;
185             &quot;movq (%q[src], %q[bytes], 1), %q[tmp]\t\n&quot;
186             &quot;movq %q[tmp], (%q[dst], %q[bytes], 1)\t\n&quot;
187             &quot;jmp 2b\t\n&quot;
188 
189             &quot;1:\t\n&quot;
190             &quot;test %q[alignedBytes], %q[alignedBytes]\t\n&quot;
191             &quot;jz 3f\t\n&quot;
192 
193             &quot;.balign 32\t\n&quot;
194             &quot;100:\t\n&quot;
195 
196             &quot;movups -64(%q[src], %q[alignedBytes], 1), %%xmm0\t\n&quot;
197             &quot;movups -48(%q[src], %q[alignedBytes], 1), %%xmm1\t\n&quot;
198             &quot;movups -32(%q[src], %q[alignedBytes], 1), %%xmm2\t\n&quot;
199             &quot;movups -16(%q[src], %q[alignedBytes], 1), %%xmm3\t\n&quot;
200             &quot;movups %%xmm0, -64(%q[dst], %q[alignedBytes], 1)\t\n&quot;
201             &quot;movups %%xmm1, -48(%q[dst], %q[alignedBytes], 1)\t\n&quot;
202             &quot;movups %%xmm2, -32(%q[dst], %q[alignedBytes], 1)\t\n&quot;
203             &quot;movups %%xmm3, -16(%q[dst], %q[alignedBytes], 1)\t\n&quot;
204             &quot;addq $-64, %q[alignedBytes]\t\n&quot;
205             &quot;jnz 100b\t\n&quot;
206 
207             &quot;3:\t\n&quot;
208 
209             : [alignedBytes] &quot;+r&quot; (alignedBytes), [tail] &quot;+r&quot; (tail), [bytes] &quot;+r&quot; (bytes), [tmp] &quot;+r&quot; (tmp), [dst] &quot;+r&quot; (dst), [src] &quot;+r&quot; (src)
210             :
211             : &quot;xmm0&quot;, &quot;xmm1&quot;, &quot;xmm2&quot;, &quot;xmm3&quot;, &quot;memory&quot;, &quot;cc&quot;
212         );
213 #elif CPU(ARM64)
214         uint64_t alignedBytes = (static_cast&lt;uint64_t&gt;(bytes) / 16) * 16;
215         uint64_t dstPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst));
216         uint64_t srcPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(src));
217 
218         asm volatile(
219             &quot;1:\t\n&quot;
220             &quot;cmp %x[alignedBytes], %x[bytes]\t\n&quot;
221             &quot;b.eq 2f\t\n&quot;
222             &quot;sub %x[bytes], %x[bytes], #0x8\t\n&quot;
223             &quot;ldr d0, [%x[srcPtr], %x[bytes]]\t\n&quot;
224             &quot;str d0, [%x[dstPtr], %x[bytes]]\t\n&quot;
225             &quot;b 1b\t\n&quot;
226 
227             &quot;2:\t\n&quot;
228             &quot;cbz %x[alignedBytes], 3f\t\n&quot;
229             &quot;sub %x[alignedBytes], %x[alignedBytes], #0x10\t\n&quot;
230             &quot;ldr q0, [%x[srcPtr], %x[alignedBytes]]\t\n&quot;
231             &quot;str q0, [%x[dstPtr], %x[alignedBytes]]\t\n&quot;
232             &quot;b 2b\t\n&quot;
233 
234             &quot;3:\t\n&quot;
235 
236             : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [dstPtr] &quot;+r&quot; (dstPtr), [srcPtr] &quot;+r&quot; (srcPtr)
237             :
238             : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
239         );
240 #else
241         slowPathBackwardsMemmove();
242 #endif // CPU(X86_64)
243     }
244 #else
245     slowPathBackwardsMemmove();
246 #endif // COMPILER(GCC_COMPATIBLE)
247 #else
248     memmove(dst, src, bytes);
249 #endif // USE(JSVALUE64)
250 }
251 
252 template &lt;typename T&gt;
253 ALWAYS_INLINE void gcSafeZeroMemory(T* dst, size_t bytes)
254 {
255     static_assert(sizeof(T) == sizeof(JSValue));
256     RELEASE_ASSERT(bytes % 8 == 0);
257 #if USE(JSVALUE64)
258 #if COMPILER(GCC_COMPATIBLE)
259 #if CPU(X86_64)
260     uint64_t zero = 0;
261     size_t count = bytes / 8;
262     asm volatile (
263         &quot;rep stosq\n\t&quot;
264         : &quot;+D&quot;(dst), &quot;+c&quot;(count)
265         : &quot;a&quot;(zero)
266         : &quot;memory&quot;
267     );
268 #elif CPU(ARM64)
269     uint64_t alignedBytes = (static_cast&lt;uint64_t&gt;(bytes) / 64) * 64;
270     uint64_t dstPtr = static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uintptr_t&gt;(dst));
271     uint64_t end = dstPtr + bytes;
272     uint64_t alignedEnd = dstPtr + alignedBytes;
273     asm volatile(
274         &quot;movi d0, #0\t\n&quot;
275         &quot;movi d1, #0\t\n&quot;
276 
277         &quot;.p2align 4\t\n&quot;
278         &quot;2:\t\n&quot;
279         &quot;cmp %x[dstPtr], %x[alignedEnd]\t\n&quot;
280         &quot;b.eq 4f\t\n&quot;
281         &quot;stnp q0, q0, [%x[dstPtr]]\t\n&quot;
282         &quot;stnp q0, q0, [%x[dstPtr], #0x20]\t\n&quot;
283         &quot;add %x[dstPtr], %x[dstPtr], #0x40\t\n&quot;
284         &quot;b 2b\t\n&quot;
285 
286         &quot;4:\t\n&quot;
287         &quot;cmp %x[dstPtr], %x[end]\t\n&quot;
288         &quot;b.eq 5f\t\n&quot;
289         &quot;str d0, [%x[dstPtr]], #0x8\t\n&quot;
290         &quot;b 4b\t\n&quot;
291 
292         &quot;5:\t\n&quot;
293 
294         : [alignedBytes] &quot;+r&quot; (alignedBytes), [bytes] &quot;+r&quot; (bytes), [dstPtr] &quot;+r&quot; (dstPtr), [end] &quot;+r&quot; (end), [alignedEnd] &quot;+r&quot; (alignedEnd)
295         :
296         : &quot;d0&quot;, &quot;d1&quot;, &quot;memory&quot;
297     );
298 #else
299     size_t count = bytes / 8;
300     for (size_t i = 0; i &lt; count; ++i)
301         bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = 0;
302 #endif // CPU(X86_64)
303 #else
304     size_t count = bytes / 8;
305     for (size_t i = 0; i &lt; count; ++i)
306         bitwise_cast&lt;volatile uint64_t*&gt;(dst)[i] = 0;
307 #endif // COMPILER(GCC_COMPATIBLE)
308 #else
309     memset(reinterpret_cast&lt;char*&gt;(dst), 0, bytes);
310 #endif // USE(JSVALUE64)
311 }
312 
313 } // namespace JSC
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>