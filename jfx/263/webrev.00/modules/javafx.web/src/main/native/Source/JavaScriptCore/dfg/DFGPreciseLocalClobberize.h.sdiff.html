<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGPreciseLocalClobberize.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGPlan.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGPredictionInjectionPhase.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGPreciseLocalClobberize.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 36 public:
 37     PreciseLocalClobberizeAdaptor(
 38         Graph&amp; graph, Node* node,
 39         const ReadFunctor&amp; read, const WriteFunctor&amp; write, const DefFunctor&amp; def)
 40         : m_graph(graph)
 41         , m_node(node)
 42         , m_read(read)
 43         , m_unconditionalWrite(write)
 44         , m_def(def)
 45     {
 46     }
 47 
 48     void read(AbstractHeap heap)
 49     {
 50         if (heap.kind() == Stack) {
 51             if (heap.payload().isTop()) {
 52                 readTop();
 53                 return;
 54             }
 55 
<span class="line-modified"> 56             callIfAppropriate(m_read, VirtualRegister(heap.payload().value32()));</span>
 57             return;
 58         }
 59 
 60         if (heap.overlaps(Stack)) {
 61             readTop();
 62             return;
 63         }
 64     }
 65 
 66     void write(AbstractHeap heap)
 67     {
 68         // We expect stack writes to already be precisely characterized by DFG::clobberize().
 69         if (heap.kind() == Stack) {
 70             RELEASE_ASSERT(!heap.payload().isTop());
<span class="line-modified"> 71             callIfAppropriate(m_unconditionalWrite, VirtualRegister(heap.payload().value32()));</span>
 72             return;
 73         }
 74 
 75         RELEASE_ASSERT(!heap.overlaps(Stack));
 76     }
 77 
 78     void def(PureValue)
 79     {
 80         // PureValue defs never have anything to do with locals, so ignore this.
 81     }
 82 
 83     void def(HeapLocation location, LazyNode node)
 84     {
 85         if (location.kind() != StackLoc)
 86             return;
 87 
 88         RELEASE_ASSERT(location.heap().kind() == Stack);
 89 
<span class="line-modified"> 90         m_def(VirtualRegister(location.heap().payload().value32()), node);</span>
 91     }
 92 
 93 private:
 94     template&lt;typename Functor&gt;
<span class="line-modified"> 95     void callIfAppropriate(const Functor&amp; functor, VirtualRegister operand)</span>
 96     {
 97         if (operand.isLocal() &amp;&amp; static_cast&lt;unsigned&gt;(operand.toLocal()) &gt;= m_graph.block(0)-&gt;variablesAtHead.numberOfLocals())
 98             return;
 99 
100         if (operand.isArgument() &amp;&amp; !operand.isHeader() &amp;&amp; static_cast&lt;unsigned&gt;(operand.toArgument()) &gt;= m_graph.block(0)-&gt;variablesAtHead.numberOfArguments())
101             return;
102 
103         functor(operand);
104     }
105 
106     void readTop()
107     {
108         auto readFrame = [&amp;] (InlineCallFrame* inlineCallFrame, unsigned numberOfArgumentsToSkip) {
109             if (!inlineCallFrame) {
110                 // Read the outermost arguments and argument count.
111                 for (unsigned i = numberOfArgumentsToSkip; i &lt; static_cast&lt;unsigned&gt;(m_graph.m_codeBlock-&gt;numParameters()); i++)
<span class="line-modified">112                     m_read(virtualRegisterForArgument(i));</span>
<span class="line-modified">113                 m_read(VirtualRegister(CallFrameSlot::argumentCount));</span>
114                 return;
115             }
116 
117             for (unsigned i = numberOfArgumentsToSkip; i &lt; inlineCallFrame-&gt;argumentsWithFixup.size(); i++)
<span class="line-modified">118                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgument(i).offset()));</span>
119             if (inlineCallFrame-&gt;isVarargs())
<span class="line-modified">120                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount));</span>
121         };
122 
123         auto readSpread = [&amp;] (Node* spread) {
124             ASSERT(spread-&gt;op() == Spread || spread-&gt;op() == PhantomSpread);
125             if (!spread-&gt;child1()-&gt;isPhantomAllocation())
126                 return;
127 
128             ASSERT(spread-&gt;child1()-&gt;op() == PhantomCreateRest || spread-&gt;child1()-&gt;op() == PhantomNewArrayBuffer);
129             if (spread-&gt;child1()-&gt;op() == PhantomNewArrayBuffer) {
130                 // This reads from a constant buffer.
131                 return;
132             }
133             InlineCallFrame* inlineCallFrame = spread-&gt;child1()-&gt;origin.semantic.inlineCallFrame();
134             unsigned numberOfArgumentsToSkip = spread-&gt;child1()-&gt;numberOfArgumentsToSkip();
135             readFrame(inlineCallFrame, numberOfArgumentsToSkip);
136         };
137 
138         auto readNewArrayWithSpreadNode = [&amp;] (Node* arrayWithSpread) {
139             ASSERT(arrayWithSpread-&gt;op() == NewArrayWithSpread || arrayWithSpread-&gt;op() == PhantomNewArrayWithSpread);
140             BitVector* bitVector = arrayWithSpread-&gt;bitVector();
141             for (unsigned i = 0; i &lt; arrayWithSpread-&gt;numChildren(); i++) {
142                 if (bitVector-&gt;get(i)) {
143                     Node* child = m_graph.varArgChild(arrayWithSpread, i).node();
144                     if (child-&gt;op() == PhantomSpread)
145                         readSpread(child);
146                 }
147             }
148         };
149 
150         switch (m_node-&gt;op()) {
151         case ForwardVarargs:
152         case CallForwardVarargs:
153         case ConstructForwardVarargs:
154         case TailCallForwardVarargs:
155         case TailCallForwardVarargsInlinedCaller:
156         case GetMyArgumentByVal:
157         case GetMyArgumentByValOutOfBounds:
158         case CreateDirectArguments:
159         case CreateScopedArguments:
160         case CreateClonedArguments:

161         case PhantomDirectArguments:
162         case PhantomClonedArguments:
163         case GetRestLength:
164         case CreateRest: {
165             bool isForwardingNode = false;
166             bool isPhantomNode = false;
167             switch (m_node-&gt;op()) {
168             case ForwardVarargs:
169             case CallForwardVarargs:
170             case ConstructForwardVarargs:
171             case TailCallForwardVarargs:
172             case TailCallForwardVarargsInlinedCaller:
173                 isForwardingNode = true;
174                 break;
175             case PhantomDirectArguments:
176             case PhantomClonedArguments:
177                 isPhantomNode = true;
178                 break;
179             default:
180                 break;
</pre>
<hr />
<pre>
207                 readFrame(inlineCallFrame, numberOfArgumentsToSkip);
208             }
209 
210             break;
211         }
212 
213         case Spread:
214             readSpread(m_node);
215             break;
216 
217         case NewArrayWithSpread: {
218             readNewArrayWithSpreadNode(m_node);
219             break;
220         }
221 
222         case GetArgument: {
223             InlineCallFrame* inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame();
224             unsigned indexIncludingThis = m_node-&gt;argumentIndex();
225             if (!inlineCallFrame) {
226                 if (indexIncludingThis &lt; static_cast&lt;unsigned&gt;(m_graph.m_codeBlock-&gt;numParameters()))
<span class="line-modified">227                     m_read(virtualRegisterForArgument(indexIncludingThis));</span>
<span class="line-modified">228                 m_read(VirtualRegister(CallFrameSlot::argumentCount));</span>
229                 break;
230             }
231 
232             ASSERT_WITH_MESSAGE(inlineCallFrame-&gt;isVarargs(), &quot;GetArgument is only used for InlineCallFrame if the call frame is varargs.&quot;);
233             if (indexIncludingThis &lt; inlineCallFrame-&gt;argumentsWithFixup.size())
<span class="line-modified">234                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgument(indexIncludingThis).offset()));</span>
<span class="line-modified">235             m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount));</span>
236             break;
237         }
238 
239         default: {
240             // All of the outermost arguments, except this, are read in sloppy mode.
241             if (!m_graph.m_codeBlock-&gt;isStrictMode()) {
242                 for (unsigned i = m_graph.m_codeBlock-&gt;numParameters(); i--;)
<span class="line-modified">243                     m_read(virtualRegisterForArgument(i));</span>
244             }
245 
246             // The stack header is read.
247             for (unsigned i = 0; i &lt; CallFrameSlot::thisArgument; ++i)
248                 m_read(VirtualRegister(i));
249 
250             // Read all of the inline arguments and call frame headers that we didn&#39;t already capture.
251             for (InlineCallFrame* inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;getCallerInlineFrameSkippingTailCalls()) {
252                 if (!inlineCallFrame-&gt;isStrictMode()) {
253                     for (unsigned i = inlineCallFrame-&gt;argumentsWithFixup.size(); i--;)
<span class="line-modified">254                         m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgument(i).offset()));</span>
255                 }
256                 if (inlineCallFrame-&gt;isClosureCall)
257                     m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee));
258                 if (inlineCallFrame-&gt;isVarargs())
<span class="line-modified">259                     m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount));</span>
260             }
261             break;
262         } }
263     }
264 
265     Graph&amp; m_graph;
266     Node* m_node;
267     const ReadFunctor&amp; m_read;
268     const WriteFunctor&amp; m_unconditionalWrite;
269     const DefFunctor&amp; m_def;
270 };
271 
272 template&lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor&gt;
273 void preciseLocalClobberize(
274     Graph&amp; graph, Node* node,
275     const ReadFunctor&amp; read, const WriteFunctor&amp; write, const DefFunctor&amp; def)
276 {
277     PreciseLocalClobberizeAdaptor&lt;ReadFunctor, WriteFunctor, DefFunctor&gt;
278         adaptor(graph, node, read, write, def);
279     clobberize(graph, node, adaptor);
</pre>
</td>
<td>
<hr />
<pre>
 36 public:
 37     PreciseLocalClobberizeAdaptor(
 38         Graph&amp; graph, Node* node,
 39         const ReadFunctor&amp; read, const WriteFunctor&amp; write, const DefFunctor&amp; def)
 40         : m_graph(graph)
 41         , m_node(node)
 42         , m_read(read)
 43         , m_unconditionalWrite(write)
 44         , m_def(def)
 45     {
 46     }
 47 
 48     void read(AbstractHeap heap)
 49     {
 50         if (heap.kind() == Stack) {
 51             if (heap.payload().isTop()) {
 52                 readTop();
 53                 return;
 54             }
 55 
<span class="line-modified"> 56             callIfAppropriate(m_read, heap.operand());</span>
 57             return;
 58         }
 59 
 60         if (heap.overlaps(Stack)) {
 61             readTop();
 62             return;
 63         }
 64     }
 65 
 66     void write(AbstractHeap heap)
 67     {
 68         // We expect stack writes to already be precisely characterized by DFG::clobberize().
 69         if (heap.kind() == Stack) {
 70             RELEASE_ASSERT(!heap.payload().isTop());
<span class="line-modified"> 71             callIfAppropriate(m_unconditionalWrite, heap.operand());</span>
 72             return;
 73         }
 74 
 75         RELEASE_ASSERT(!heap.overlaps(Stack));
 76     }
 77 
 78     void def(PureValue)
 79     {
 80         // PureValue defs never have anything to do with locals, so ignore this.
 81     }
 82 
 83     void def(HeapLocation location, LazyNode node)
 84     {
 85         if (location.kind() != StackLoc)
 86             return;
 87 
 88         RELEASE_ASSERT(location.heap().kind() == Stack);
 89 
<span class="line-modified"> 90         m_def(location.heap().operand(), node);</span>
 91     }
 92 
 93 private:
 94     template&lt;typename Functor&gt;
<span class="line-modified"> 95     void callIfAppropriate(const Functor&amp; functor, Operand operand)</span>
 96     {
 97         if (operand.isLocal() &amp;&amp; static_cast&lt;unsigned&gt;(operand.toLocal()) &gt;= m_graph.block(0)-&gt;variablesAtHead.numberOfLocals())
 98             return;
 99 
100         if (operand.isArgument() &amp;&amp; !operand.isHeader() &amp;&amp; static_cast&lt;unsigned&gt;(operand.toArgument()) &gt;= m_graph.block(0)-&gt;variablesAtHead.numberOfArguments())
101             return;
102 
103         functor(operand);
104     }
105 
106     void readTop()
107     {
108         auto readFrame = [&amp;] (InlineCallFrame* inlineCallFrame, unsigned numberOfArgumentsToSkip) {
109             if (!inlineCallFrame) {
110                 // Read the outermost arguments and argument count.
111                 for (unsigned i = numberOfArgumentsToSkip; i &lt; static_cast&lt;unsigned&gt;(m_graph.m_codeBlock-&gt;numParameters()); i++)
<span class="line-modified">112                     m_read(virtualRegisterForArgumentIncludingThis(i));</span>
<span class="line-modified">113                 m_read(VirtualRegister(CallFrameSlot::argumentCountIncludingThis));</span>
114                 return;
115             }
116 
117             for (unsigned i = numberOfArgumentsToSkip; i &lt; inlineCallFrame-&gt;argumentsWithFixup.size(); i++)
<span class="line-modified">118                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgumentIncludingThis(i).offset()));</span>
119             if (inlineCallFrame-&gt;isVarargs())
<span class="line-modified">120                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis));</span>
121         };
122 
123         auto readSpread = [&amp;] (Node* spread) {
124             ASSERT(spread-&gt;op() == Spread || spread-&gt;op() == PhantomSpread);
125             if (!spread-&gt;child1()-&gt;isPhantomAllocation())
126                 return;
127 
128             ASSERT(spread-&gt;child1()-&gt;op() == PhantomCreateRest || spread-&gt;child1()-&gt;op() == PhantomNewArrayBuffer);
129             if (spread-&gt;child1()-&gt;op() == PhantomNewArrayBuffer) {
130                 // This reads from a constant buffer.
131                 return;
132             }
133             InlineCallFrame* inlineCallFrame = spread-&gt;child1()-&gt;origin.semantic.inlineCallFrame();
134             unsigned numberOfArgumentsToSkip = spread-&gt;child1()-&gt;numberOfArgumentsToSkip();
135             readFrame(inlineCallFrame, numberOfArgumentsToSkip);
136         };
137 
138         auto readNewArrayWithSpreadNode = [&amp;] (Node* arrayWithSpread) {
139             ASSERT(arrayWithSpread-&gt;op() == NewArrayWithSpread || arrayWithSpread-&gt;op() == PhantomNewArrayWithSpread);
140             BitVector* bitVector = arrayWithSpread-&gt;bitVector();
141             for (unsigned i = 0; i &lt; arrayWithSpread-&gt;numChildren(); i++) {
142                 if (bitVector-&gt;get(i)) {
143                     Node* child = m_graph.varArgChild(arrayWithSpread, i).node();
144                     if (child-&gt;op() == PhantomSpread)
145                         readSpread(child);
146                 }
147             }
148         };
149 
150         switch (m_node-&gt;op()) {
151         case ForwardVarargs:
152         case CallForwardVarargs:
153         case ConstructForwardVarargs:
154         case TailCallForwardVarargs:
155         case TailCallForwardVarargsInlinedCaller:
156         case GetMyArgumentByVal:
157         case GetMyArgumentByValOutOfBounds:
158         case CreateDirectArguments:
159         case CreateScopedArguments:
160         case CreateClonedArguments:
<span class="line-added">161         case CreateArgumentsButterfly:</span>
162         case PhantomDirectArguments:
163         case PhantomClonedArguments:
164         case GetRestLength:
165         case CreateRest: {
166             bool isForwardingNode = false;
167             bool isPhantomNode = false;
168             switch (m_node-&gt;op()) {
169             case ForwardVarargs:
170             case CallForwardVarargs:
171             case ConstructForwardVarargs:
172             case TailCallForwardVarargs:
173             case TailCallForwardVarargsInlinedCaller:
174                 isForwardingNode = true;
175                 break;
176             case PhantomDirectArguments:
177             case PhantomClonedArguments:
178                 isPhantomNode = true;
179                 break;
180             default:
181                 break;
</pre>
<hr />
<pre>
208                 readFrame(inlineCallFrame, numberOfArgumentsToSkip);
209             }
210 
211             break;
212         }
213 
214         case Spread:
215             readSpread(m_node);
216             break;
217 
218         case NewArrayWithSpread: {
219             readNewArrayWithSpreadNode(m_node);
220             break;
221         }
222 
223         case GetArgument: {
224             InlineCallFrame* inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame();
225             unsigned indexIncludingThis = m_node-&gt;argumentIndex();
226             if (!inlineCallFrame) {
227                 if (indexIncludingThis &lt; static_cast&lt;unsigned&gt;(m_graph.m_codeBlock-&gt;numParameters()))
<span class="line-modified">228                     m_read(virtualRegisterForArgumentIncludingThis(indexIncludingThis));</span>
<span class="line-modified">229                 m_read(VirtualRegister(CallFrameSlot::argumentCountIncludingThis));</span>
230                 break;
231             }
232 
233             ASSERT_WITH_MESSAGE(inlineCallFrame-&gt;isVarargs(), &quot;GetArgument is only used for InlineCallFrame if the call frame is varargs.&quot;);
234             if (indexIncludingThis &lt; inlineCallFrame-&gt;argumentsWithFixup.size())
<span class="line-modified">235                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgumentIncludingThis(indexIncludingThis).offset()));</span>
<span class="line-modified">236             m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis));</span>
237             break;
238         }
239 
240         default: {
241             // All of the outermost arguments, except this, are read in sloppy mode.
242             if (!m_graph.m_codeBlock-&gt;isStrictMode()) {
243                 for (unsigned i = m_graph.m_codeBlock-&gt;numParameters(); i--;)
<span class="line-modified">244                     m_read(virtualRegisterForArgumentIncludingThis(i));</span>
245             }
246 
247             // The stack header is read.
248             for (unsigned i = 0; i &lt; CallFrameSlot::thisArgument; ++i)
249                 m_read(VirtualRegister(i));
250 
251             // Read all of the inline arguments and call frame headers that we didn&#39;t already capture.
252             for (InlineCallFrame* inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;getCallerInlineFrameSkippingTailCalls()) {
253                 if (!inlineCallFrame-&gt;isStrictMode()) {
254                     for (unsigned i = inlineCallFrame-&gt;argumentsWithFixup.size(); i--;)
<span class="line-modified">255                         m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgumentIncludingThis(i).offset()));</span>
256                 }
257                 if (inlineCallFrame-&gt;isClosureCall)
258                     m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee));
259                 if (inlineCallFrame-&gt;isVarargs())
<span class="line-modified">260                     m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis));</span>
261             }
262             break;
263         } }
264     }
265 
266     Graph&amp; m_graph;
267     Node* m_node;
268     const ReadFunctor&amp; m_read;
269     const WriteFunctor&amp; m_unconditionalWrite;
270     const DefFunctor&amp; m_def;
271 };
272 
273 template&lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor&gt;
274 void preciseLocalClobberize(
275     Graph&amp; graph, Node* node,
276     const ReadFunctor&amp; read, const WriteFunctor&amp; write, const DefFunctor&amp; def)
277 {
278     PreciseLocalClobberizeAdaptor&lt;ReadFunctor, WriteFunctor, DefFunctor&gt;
279         adaptor(graph, node, read, write, def);
280     clobberize(graph, node, adaptor);
</pre>
</td>
</tr>
</table>
<center><a href="DFGPlan.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGPredictionInjectionPhase.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>