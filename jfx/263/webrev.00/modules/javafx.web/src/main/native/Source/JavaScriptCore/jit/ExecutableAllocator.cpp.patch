diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp
@@ -90,59 +90,59 @@
 namespace JSC {
 
 using namespace WTF;
 
 #if defined(FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB) && FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB > 0
-static const size_t fixedExecutableMemoryPoolSize = FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB * 1024 * 1024;
+static constexpr size_t fixedExecutableMemoryPoolSize = FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB * 1024 * 1024;
 #elif CPU(ARM)
-static const size_t fixedExecutableMemoryPoolSize = 16 * 1024 * 1024;
+static constexpr size_t fixedExecutableMemoryPoolSize = 16 * 1024 * 1024;
 #elif CPU(ARM64)
-static const size_t fixedExecutableMemoryPoolSize = 128 * 1024 * 1024;
+static constexpr size_t fixedExecutableMemoryPoolSize = 128 * 1024 * 1024;
 #elif CPU(X86_64)
-static const size_t fixedExecutableMemoryPoolSize = 1024 * 1024 * 1024;
+static constexpr size_t fixedExecutableMemoryPoolSize = 1024 * 1024 * 1024;
 #else
-static const size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;
+static constexpr size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;
 #endif
 
 #if CPU(ARM)
-static const double executablePoolReservationFraction = 0.15;
+static constexpr double executablePoolReservationFraction = 0.15;
 #else
-static const double executablePoolReservationFraction = 0.25;
+static constexpr double executablePoolReservationFraction = 0.25;
 #endif
 
-#if ENABLE(SEPARATED_WX_HEAP)
-JS_EXPORT_PRIVATE bool useFastPermisionsJITCopy { false };
-JS_EXPORT_PRIVATE JITWriteSeparateHeapsFunction jitWriteSeparateHeapsFunction;
-#endif
-
-#if !USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION) && HAVE(REMAP_JIT)
-static uintptr_t startOfFixedWritableMemoryPool;
-#endif
-
-class FixedVMPoolExecutableAllocator;
-static FixedVMPoolExecutableAllocator* allocator = nullptr;
-
-static bool s_isJITEnabled = true;
 static bool isJITEnabled()
 {
+    bool jitEnabled = !g_jscConfig.jitDisabled;
 #if PLATFORM(IOS_FAMILY) && (CPU(ARM64) || CPU(ARM))
-    return processHasEntitlement("dynamic-codesigning") && s_isJITEnabled;
+    return processHasEntitlement("dynamic-codesigning") && jitEnabled;
 #else
-    return s_isJITEnabled;
+    return jitEnabled;
 #endif
 }
 
 void ExecutableAllocator::setJITEnabled(bool enabled)
 {
-    ASSERT(!allocator);
-    if (s_isJITEnabled == enabled)
+    bool jitEnabled = !g_jscConfig.jitDisabled;
+    ASSERT(!g_jscConfig.fixedVMPoolExecutableAllocator);
+    if (jitEnabled == enabled)
         return;
 
-    s_isJITEnabled = enabled;
+    g_jscConfig.jitDisabled = !enabled;
 
 #if PLATFORM(IOS_FAMILY) && (CPU(ARM64) || CPU(ARM))
     if (!enabled) {
+        // Because of an OS quirk, even after the JIT region has been unmapped,
+        // the OS thinks that region is reserved, and as such, can cause Gigacage
+        // allocation to fail. We work around this by initializing the Gigacage
+        // first.
+        // Note: when called, setJITEnabled() is always called extra early in the
+        // process bootstrap. Under normal operation (when setJITEnabled() isn't
+        // called at all), we will naturally initialize the Gigacage before we
+        // allocate the JIT region. Hence, this workaround is merely ensuring the
+        // same behavior of allocation ordering.
+        Gigacage::ensureGigacage();
+
         constexpr size_t size = 1;
         constexpr int protection = PROT_READ | PROT_WRITE | PROT_EXEC;
         constexpr int flags = MAP_PRIVATE | MAP_ANON | MAP_JIT;
         constexpr int fd = OSAllocator::JSJITCodePages;
         void* allocation = mmap(nullptr, size, protection, flags, fd, 0);
@@ -191,11 +191,11 @@
             os_thread_self_restrict_rwx_to_rx();
 
 #else // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
 #if ENABLE(FAST_JIT_PERMISSIONS)
             if (os_thread_self_restrict_rwx_is_supported()) {
-                useFastPermisionsJITCopy = true;
+                g_jscConfig.useFastPermisionsJITCopy = true;
                 os_thread_self_restrict_rwx_to_rx();
             } else
 #endif
             if (Options::useSeparatedWXHeap()) {
                 // First page of our JIT allocation is reserved.
@@ -210,19 +210,19 @@
 
             ASSERT(bytesReserved() == reservationSize); // Since our executable memory is fixed-sized, bytesReserved is never changed after initialization.
 
             void* reservationEnd = reinterpret_cast<uint8_t*>(reservationBase) + reservationSize;
 
-            m_memoryStart = MacroAssemblerCodePtr<ExecutableMemoryPtrTag>(tagCodePtr<ExecutableMemoryPtrTag>(reservationBase));
-            m_memoryEnd = MacroAssemblerCodePtr<ExecutableMemoryPtrTag>(tagCodePtr<ExecutableMemoryPtrTag>(reservationEnd));
+            g_jscConfig.startExecutableMemory = tagCodePtr<ExecutableMemoryPtrTag>(reservationBase);
+            g_jscConfig.endExecutableMemory = tagCodePtr<ExecutableMemoryPtrTag>(reservationEnd);
         }
     }
 
     virtual ~FixedVMPoolExecutableAllocator();
 
-    void* memoryStart() { return m_memoryStart.untaggedExecutableAddress(); }
-    void* memoryEnd() { return m_memoryEnd.untaggedExecutableAddress(); }
+    void* memoryStart() { return untagCodePtr<ExecutableMemoryPtrTag>(g_jscConfig.startExecutableMemory); }
+    void* memoryEnd() { return untagCodePtr<ExecutableMemoryPtrTag>(g_jscConfig.endExecutableMemory); }
     bool isJITPC(void* pc) { return memoryStart() <= pc && pc < memoryEnd(); }
 
 protected:
     FreeSpacePtr allocateNewSpace(size_t&) override
     {
@@ -300,11 +300,11 @@
 
         // Zero out writableAddr to avoid leaking the address of the writable mapping.
         memset_s(&writableAddr, sizeof(writableAddr), 0, sizeof(writableAddr));
 
 #if ENABLE(SEPARATED_WX_HEAP)
-        jitWriteSeparateHeapsFunction = reinterpret_cast<JITWriteSeparateHeapsFunction>(writeThunk.code().executableAddress());
+        g_jscConfig.jitWriteSeparateHeaps = reinterpret_cast<JITWriteSeparateHeapsFunction>(writeThunk.code().executableAddress());
 #endif
     }
 
 #if CPU(ARM64) && USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
     MacroAssemblerCodeRef<JITThunkPtrTag> jitWriteThunkGenerator(void* writableAddr, void* stubBase, size_t stubSize)
@@ -379,16 +379,16 @@
         return linkBuffer.finalizeCodeWithoutDisassembly<JITThunkPtrTag>();
     }
 #else // not CPU(ARM64) && USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
     static void genericWriteToJITRegion(off_t offset, const void* data, size_t dataSize)
     {
-        memcpy((void*)(startOfFixedWritableMemoryPool + offset), data, dataSize);
+        memcpy((void*)(g_jscConfig.startOfFixedWritableMemoryPool + offset), data, dataSize);
     }
 
     MacroAssemblerCodeRef<JITThunkPtrTag> jitWriteThunkGenerator(void* address, void*, size_t)
     {
-        startOfFixedWritableMemoryPool = reinterpret_cast<uintptr_t>(address);
+        g_jscConfig.startOfFixedWritableMemoryPool = reinterpret_cast<uintptr_t>(address);
         void* function = reinterpret_cast<void*>(&genericWriteToJITRegion);
 #if CPU(ARM_THUMB2)
         // Handle thumb offset
         uintptr_t functionAsInt = reinterpret_cast<uintptr_t>(function);
         functionAsInt -= 1;
@@ -405,42 +405,47 @@
     }
 #endif
 
 private:
     PageReservation m_reservation;
-    MacroAssemblerCodePtr<ExecutableMemoryPtrTag> m_memoryStart;
-    MacroAssemblerCodePtr<ExecutableMemoryPtrTag> m_memoryEnd;
 };
 
 FixedVMPoolExecutableAllocator::~FixedVMPoolExecutableAllocator()
 {
     m_reservation.deallocate();
 }
 
+// Keep this pointer in a mutable global variable to help Leaks find it.
+// But we do not use this pointer.
+static FixedVMPoolExecutableAllocator* globalFixedVMPoolExecutableAllocatorToWorkAroundLeaks = nullptr;
 void ExecutableAllocator::initializeUnderlyingAllocator()
 {
-    ASSERT(!allocator);
-    allocator = new FixedVMPoolExecutableAllocator();
-    CodeProfiling::notifyAllocator(allocator);
+    RELEASE_ASSERT(!g_jscConfig.fixedVMPoolExecutableAllocator);
+    g_jscConfig.fixedVMPoolExecutableAllocator = new FixedVMPoolExecutableAllocator();
+    globalFixedVMPoolExecutableAllocatorToWorkAroundLeaks = g_jscConfig.fixedVMPoolExecutableAllocator;
+    CodeProfiling::notifyAllocator(g_jscConfig.fixedVMPoolExecutableAllocator);
 }
 
 bool ExecutableAllocator::isValid() const
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::isValid();
     return !!allocator->bytesReserved();
 }
 
 bool ExecutableAllocator::underMemoryPressure()
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::underMemoryPressure();
     return allocator->bytesAllocated() > allocator->bytesReserved() / 2;
 }
 
 double ExecutableAllocator::memoryPressureMultiplier(size_t addedMemoryUsage)
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::memoryPressureMultiplier(addedMemoryUsage);
     ASSERT(allocator->bytesAllocated() <= allocator->bytesReserved());
     size_t bytesAllocated = allocator->bytesAllocated() + addedMemoryUsage;
     size_t bytesAvailable = static_cast<size_t>(
@@ -456,10 +461,11 @@
     return result;
 }
 
 RefPtr<ExecutableMemoryHandle> ExecutableAllocator::allocate(size_t sizeInBytes, void* ownerUID, JITCompilationEffort effort)
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::allocate(sizeInBytes, ownerUID, effort);
     if (Options::logExecutableAllocation()) {
         MetaAllocator::Statistics stats = allocator->currentStatistics();
         dataLog("Allocating ", sizeInBytes, " bytes of executable memory with ", stats.bytesAllocated, " bytes allocated, ", stats.bytesReserved, " bytes reserved, and ", stats.bytesCommitted, " committed.\n");
@@ -493,73 +499,78 @@
             CRASH();
         }
         return nullptr;
     }
 
-#if CPU(ARM64E)
     void* start = allocator->memoryStart();
     void* end = allocator->memoryEnd();
     void* resultStart = result->start().untaggedPtr();
     void* resultEnd = result->end().untaggedPtr();
     RELEASE_ASSERT(start <= resultStart && resultStart < end);
     RELEASE_ASSERT(start < resultEnd && resultEnd <= end);
-#endif
     return result;
 }
 
 bool ExecutableAllocator::isValidExecutableMemory(const AbstractLocker& locker, void* address)
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::isValidExecutableMemory(locker, address);
     return allocator->isInAllocatedMemory(locker, address);
 }
 
 Lock& ExecutableAllocator::getLock() const
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::getLock();
     return allocator->getLock();
 }
 
 size_t ExecutableAllocator::committedByteCount()
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return Base::committedByteCount();
     return allocator->bytesCommitted();
 }
 
 #if ENABLE(META_ALLOCATOR_PROFILE)
 void ExecutableAllocator::dumpProfile()
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return;
     allocator->dumpProfile();
 }
 #endif
 
 void* startOfFixedExecutableMemoryPoolImpl()
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return nullptr;
     return allocator->memoryStart();
 }
 
 void* endOfFixedExecutableMemoryPoolImpl()
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     if (!allocator)
         return nullptr;
     return allocator->memoryEnd();
 }
 
 bool isJITPC(void* pc)
 {
+    auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;
     return allocator && allocator->isJITPC(pc);
 }
 
 void dumpJITMemory(const void* dst, const void* src, size_t size)
 {
-    ASSERT(Options::dumpJITMemoryPath());
+    RELEASE_ASSERT(Options::dumpJITMemoryPath());
 
 #if OS(DARWIN)
     static int fd = -1;
     static uint8_t* buffer;
     static constexpr size_t bufferSize = fixedExecutableMemoryPoolSize;
@@ -633,19 +644,21 @@
 
 #endif // ENABLE(JIT)
 
 namespace JSC {
 
-static ExecutableAllocator* executableAllocator;
-
+// Keep this pointer in a mutable global variable to help Leaks find it.
+// But we do not use this pointer.
+static ExecutableAllocator* globalExecutableAllocatorToWorkAroundLeaks = nullptr;
 void ExecutableAllocator::initialize()
 {
-    executableAllocator = new ExecutableAllocator;
+    g_jscConfig.executableAllocator = new ExecutableAllocator;
+    globalExecutableAllocatorToWorkAroundLeaks = g_jscConfig.executableAllocator;
 }
 
 ExecutableAllocator& ExecutableAllocator::singleton()
 {
-    ASSERT(executableAllocator);
-    return *executableAllocator;
+    ASSERT(g_jscConfig.executableAllocator);
+    return *g_jscConfig.executableAllocator;
 }
 
 } // namespace JSC
