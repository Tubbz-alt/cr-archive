<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CodeBlockSetInlines.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CompleteSubspace.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;Subspace.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;AllocatorInlines.h&quot;
 31 #include &quot;BlockDirectoryInlines.h&quot;
 32 #include &quot;JSCInlines.h&quot;
 33 #include &quot;LocalAllocatorInlines.h&quot;
 34 #include &quot;MarkedBlockInlines.h&quot;

 35 #include &quot;PreventCollectionScope.h&quot;
 36 #include &quot;SubspaceInlines.h&quot;
 37 
 38 namespace JSC {
 39 
 40 CompleteSubspace::CompleteSubspace(CString name, Heap&amp; heap, HeapCellType* heapCellType, AlignedMemoryAllocator* alignedMemoryAllocator)
 41     : Subspace(name, heap)
 42 {
 43     initialize(heapCellType, alignedMemoryAllocator);
 44 }
 45 
 46 CompleteSubspace::~CompleteSubspace()
 47 {
 48 }
 49 
 50 Allocator CompleteSubspace::allocatorFor(size_t size, AllocatorForMode mode)
 51 {
 52     return allocatorForNonVirtual(size, mode);
 53 }
 54 
</pre>
<hr />
<pre>
 62     size_t index = MarkedSpace::sizeClassToIndex(size);
 63     size_t sizeClass = MarkedSpace::s_sizeClassForSizeStep[index];
 64     if (!sizeClass)
 65         return Allocator();
 66 
 67     // This is written in such a way that it&#39;s OK for the JIT threads to end up here if they want
 68     // to generate code that uses some allocator that hadn&#39;t been used yet. Note that a possibly-
 69     // just-as-good solution would be to return null if we&#39;re in the JIT since the JIT treats null
 70     // allocator as &quot;please always take the slow path&quot;. But, that could lead to performance
 71     // surprises and the algorithm here is pretty easy. Only this code has to hold the lock, to
 72     // prevent simultaneously BlockDirectory creations from multiple threads. This code ensures
 73     // that any &quot;forEachAllocator&quot; traversals will only see this allocator after it&#39;s initialized
 74     // enough: it will have
 75     auto locker = holdLock(m_space.directoryLock());
 76     if (Allocator allocator = m_allocatorForSizeStep[index])
 77         return allocator;
 78 
 79     if (false)
 80         dataLog(&quot;Creating BlockDirectory/LocalAllocator for &quot;, m_name, &quot;, &quot;, attributes(), &quot;, &quot;, sizeClass, &quot;.\n&quot;);
 81 
<span class="line-modified"> 82     std::unique_ptr&lt;BlockDirectory&gt; uniqueDirectory =</span>
<span class="line-removed"> 83         makeUnique&lt;BlockDirectory&gt;(m_space.heap(), sizeClass);</span>
 84     BlockDirectory* directory = uniqueDirectory.get();
 85     m_directories.append(WTFMove(uniqueDirectory));
 86 
 87     directory-&gt;setSubspace(this);
 88     m_space.addBlockDirectory(locker, directory);
 89 
 90     std::unique_ptr&lt;LocalAllocator&gt; uniqueLocalAllocator =
 91         makeUnique&lt;LocalAllocator&gt;(directory);
 92     LocalAllocator* localAllocator = uniqueLocalAllocator.get();
 93     m_localAllocators.append(WTFMove(uniqueLocalAllocator));
 94 
 95     Allocator allocator(localAllocator);
 96 
 97     index = MarkedSpace::sizeClassToIndex(sizeClass);
 98     for (;;) {
 99         if (MarkedSpace::s_sizeClassForSizeStep[index] != sizeClass)
100             break;
101 
102         m_allocatorForSizeStep[index] = allocator;
103 
104         if (!index--)
105             break;
106     }
107 
108     directory-&gt;setNextDirectoryInSubspace(m_firstDirectory);
<span class="line-modified">109     m_alignedMemoryAllocator-&gt;registerDirectory(directory);</span>
110     WTF::storeStoreFence();
111     m_firstDirectory = directory;
112     return allocator;
113 }
114 
115 void* CompleteSubspace::allocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
116 {
117     void* result = tryAllocateSlow(vm, size, deferralContext);
118     if (failureMode == AllocationFailureMode::Assert)
119         RELEASE_ASSERT(result);
120     return result;
121 }
122 
123 void* CompleteSubspace::tryAllocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext)
124 {
125     if (validateDFGDoesGC)
126         RELEASE_ASSERT(vm.heap.expectDoesGC());
127 
128     sanitizeStackForVM(vm);
129 
130     if (Allocator allocator = allocatorFor(size, AllocatorForMode::EnsureAllocator))
<span class="line-modified">131         return allocator.allocate(deferralContext, AllocationFailureMode::ReturnNull);</span>
132 
<span class="line-modified">133     if (size &lt;= Options::largeAllocationCutoff()</span>
134         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
135         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
136         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
137         RELEASE_ASSERT_NOT_REACHED();
138     }
139 
140     vm.heap.collectIfNecessaryOrDefer(deferralContext);
141 
142     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
<span class="line-modified">143     LargeAllocation* allocation = LargeAllocation::tryCreate(vm.heap, size, this, m_space.m_largeAllocations.size());</span>
144     if (!allocation)
145         return nullptr;
146 
<span class="line-modified">147     m_space.m_largeAllocations.append(allocation);</span>
<span class="line-modified">148     ASSERT(allocation-&gt;indexInSpace() == m_space.m_largeAllocations.size() - 1);</span>


149     vm.heap.didAllocate(size);
150     m_space.m_capacity += size;
151 
<span class="line-modified">152     m_largeAllocations.append(allocation);</span>
153 
154     return allocation-&gt;cell();
155 }
156 
<span class="line-modified">157 void* CompleteSubspace::reallocateLargeAllocationNonVirtual(VM&amp; vm, HeapCell* oldCell, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)</span>
158 {
159     if (validateDFGDoesGC)
160         RELEASE_ASSERT(vm.heap.expectDoesGC());
161 
162     // The following conditions are met in Butterfly for example.
<span class="line-modified">163     ASSERT(oldCell-&gt;isLargeAllocation());</span>
164 
<span class="line-modified">165     LargeAllocation* oldAllocation = &amp;oldCell-&gt;largeAllocation();</span>
166     ASSERT(oldAllocation-&gt;cellSize() &lt;= size);
167     ASSERT(oldAllocation-&gt;weakSet().isTriviallyDestructible());
168     ASSERT(oldAllocation-&gt;attributes().destruction == DoesNotNeedDestruction);
169     ASSERT(oldAllocation-&gt;attributes().cellKind == HeapCell::Auxiliary);
170     ASSERT(size &gt; MarkedSpace::largeCutoff);
171 
172     sanitizeStackForVM(vm);
173 
<span class="line-modified">174     if (size &lt;= Options::largeAllocationCutoff()</span>
175         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
176         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
177         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
178         RELEASE_ASSERT_NOT_REACHED();
179     }
180 
181     vm.heap.collectIfNecessaryOrDefer(deferralContext);
182 
183     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
184     size_t difference = size - oldAllocation-&gt;cellSize();
185     unsigned oldIndexInSpace = oldAllocation-&gt;indexInSpace();
186     if (oldAllocation-&gt;isOnList())
187         oldAllocation-&gt;remove();
188 
<span class="line-modified">189     LargeAllocation* allocation = oldAllocation-&gt;tryReallocate(size, this);</span>
190     if (!allocation) {
191         RELEASE_ASSERT(failureMode != AllocationFailureMode::Assert);
<span class="line-modified">192         m_largeAllocations.append(oldAllocation);</span>
193         return nullptr;
194     }
195     ASSERT(oldIndexInSpace == allocation-&gt;indexInSpace());
196 
<span class="line-modified">197     m_space.m_largeAllocations[oldIndexInSpace] = allocation;</span>








198     vm.heap.didAllocate(difference);
199     m_space.m_capacity += difference;
200 
<span class="line-modified">201     m_largeAllocations.append(allocation);</span>
202 
203     return allocation-&gt;cell();
204 }
205 
206 } // namespace JSC
207 
</pre>
</td>
<td>
<hr />
<pre>
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;Subspace.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;AllocatorInlines.h&quot;
 31 #include &quot;BlockDirectoryInlines.h&quot;
 32 #include &quot;JSCInlines.h&quot;
 33 #include &quot;LocalAllocatorInlines.h&quot;
 34 #include &quot;MarkedBlockInlines.h&quot;
<span class="line-added"> 35 #include &quot;MarkedSpaceInlines.h&quot;</span>
 36 #include &quot;PreventCollectionScope.h&quot;
 37 #include &quot;SubspaceInlines.h&quot;
 38 
 39 namespace JSC {
 40 
 41 CompleteSubspace::CompleteSubspace(CString name, Heap&amp; heap, HeapCellType* heapCellType, AlignedMemoryAllocator* alignedMemoryAllocator)
 42     : Subspace(name, heap)
 43 {
 44     initialize(heapCellType, alignedMemoryAllocator);
 45 }
 46 
 47 CompleteSubspace::~CompleteSubspace()
 48 {
 49 }
 50 
 51 Allocator CompleteSubspace::allocatorFor(size_t size, AllocatorForMode mode)
 52 {
 53     return allocatorForNonVirtual(size, mode);
 54 }
 55 
</pre>
<hr />
<pre>
 63     size_t index = MarkedSpace::sizeClassToIndex(size);
 64     size_t sizeClass = MarkedSpace::s_sizeClassForSizeStep[index];
 65     if (!sizeClass)
 66         return Allocator();
 67 
 68     // This is written in such a way that it&#39;s OK for the JIT threads to end up here if they want
 69     // to generate code that uses some allocator that hadn&#39;t been used yet. Note that a possibly-
 70     // just-as-good solution would be to return null if we&#39;re in the JIT since the JIT treats null
 71     // allocator as &quot;please always take the slow path&quot;. But, that could lead to performance
 72     // surprises and the algorithm here is pretty easy. Only this code has to hold the lock, to
 73     // prevent simultaneously BlockDirectory creations from multiple threads. This code ensures
 74     // that any &quot;forEachAllocator&quot; traversals will only see this allocator after it&#39;s initialized
 75     // enough: it will have
 76     auto locker = holdLock(m_space.directoryLock());
 77     if (Allocator allocator = m_allocatorForSizeStep[index])
 78         return allocator;
 79 
 80     if (false)
 81         dataLog(&quot;Creating BlockDirectory/LocalAllocator for &quot;, m_name, &quot;, &quot;, attributes(), &quot;, &quot;, sizeClass, &quot;.\n&quot;);
 82 
<span class="line-modified"> 83     std::unique_ptr&lt;BlockDirectory&gt; uniqueDirectory = makeUnique&lt;BlockDirectory&gt;(sizeClass);</span>

 84     BlockDirectory* directory = uniqueDirectory.get();
 85     m_directories.append(WTFMove(uniqueDirectory));
 86 
 87     directory-&gt;setSubspace(this);
 88     m_space.addBlockDirectory(locker, directory);
 89 
 90     std::unique_ptr&lt;LocalAllocator&gt; uniqueLocalAllocator =
 91         makeUnique&lt;LocalAllocator&gt;(directory);
 92     LocalAllocator* localAllocator = uniqueLocalAllocator.get();
 93     m_localAllocators.append(WTFMove(uniqueLocalAllocator));
 94 
 95     Allocator allocator(localAllocator);
 96 
 97     index = MarkedSpace::sizeClassToIndex(sizeClass);
 98     for (;;) {
 99         if (MarkedSpace::s_sizeClassForSizeStep[index] != sizeClass)
100             break;
101 
102         m_allocatorForSizeStep[index] = allocator;
103 
104         if (!index--)
105             break;
106     }
107 
108     directory-&gt;setNextDirectoryInSubspace(m_firstDirectory);
<span class="line-modified">109     m_alignedMemoryAllocator-&gt;registerDirectory(m_space.heap(), directory);</span>
110     WTF::storeStoreFence();
111     m_firstDirectory = directory;
112     return allocator;
113 }
114 
115 void* CompleteSubspace::allocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
116 {
117     void* result = tryAllocateSlow(vm, size, deferralContext);
118     if (failureMode == AllocationFailureMode::Assert)
119         RELEASE_ASSERT(result);
120     return result;
121 }
122 
123 void* CompleteSubspace::tryAllocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext)
124 {
125     if (validateDFGDoesGC)
126         RELEASE_ASSERT(vm.heap.expectDoesGC());
127 
128     sanitizeStackForVM(vm);
129 
130     if (Allocator allocator = allocatorFor(size, AllocatorForMode::EnsureAllocator))
<span class="line-modified">131         return allocator.allocate(vm.heap, deferralContext, AllocationFailureMode::ReturnNull);</span>
132 
<span class="line-modified">133     if (size &lt;= Options::preciseAllocationCutoff()</span>
134         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
135         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
136         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
137         RELEASE_ASSERT_NOT_REACHED();
138     }
139 
140     vm.heap.collectIfNecessaryOrDefer(deferralContext);
141 
142     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
<span class="line-modified">143     PreciseAllocation* allocation = PreciseAllocation::tryCreate(vm.heap, size, this, m_space.m_preciseAllocations.size());</span>
144     if (!allocation)
145         return nullptr;
146 
<span class="line-modified">147     m_space.m_preciseAllocations.append(allocation);</span>
<span class="line-modified">148     if (auto* set = m_space.preciseAllocationSet())</span>
<span class="line-added">149         set-&gt;add(allocation-&gt;cell());</span>
<span class="line-added">150     ASSERT(allocation-&gt;indexInSpace() == m_space.m_preciseAllocations.size() - 1);</span>
151     vm.heap.didAllocate(size);
152     m_space.m_capacity += size;
153 
<span class="line-modified">154     m_preciseAllocations.append(allocation);</span>
155 
156     return allocation-&gt;cell();
157 }
158 
<span class="line-modified">159 void* CompleteSubspace::reallocatePreciseAllocationNonVirtual(VM&amp; vm, HeapCell* oldCell, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)</span>
160 {
161     if (validateDFGDoesGC)
162         RELEASE_ASSERT(vm.heap.expectDoesGC());
163 
164     // The following conditions are met in Butterfly for example.
<span class="line-modified">165     ASSERT(oldCell-&gt;isPreciseAllocation());</span>
166 
<span class="line-modified">167     PreciseAllocation* oldAllocation = &amp;oldCell-&gt;preciseAllocation();</span>
168     ASSERT(oldAllocation-&gt;cellSize() &lt;= size);
169     ASSERT(oldAllocation-&gt;weakSet().isTriviallyDestructible());
170     ASSERT(oldAllocation-&gt;attributes().destruction == DoesNotNeedDestruction);
171     ASSERT(oldAllocation-&gt;attributes().cellKind == HeapCell::Auxiliary);
172     ASSERT(size &gt; MarkedSpace::largeCutoff);
173 
174     sanitizeStackForVM(vm);
175 
<span class="line-modified">176     if (size &lt;= Options::preciseAllocationCutoff()</span>
177         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
178         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
179         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
180         RELEASE_ASSERT_NOT_REACHED();
181     }
182 
183     vm.heap.collectIfNecessaryOrDefer(deferralContext);
184 
185     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
186     size_t difference = size - oldAllocation-&gt;cellSize();
187     unsigned oldIndexInSpace = oldAllocation-&gt;indexInSpace();
188     if (oldAllocation-&gt;isOnList())
189         oldAllocation-&gt;remove();
190 
<span class="line-modified">191     PreciseAllocation* allocation = oldAllocation-&gt;tryReallocate(size, this);</span>
192     if (!allocation) {
193         RELEASE_ASSERT(failureMode != AllocationFailureMode::Assert);
<span class="line-modified">194         m_preciseAllocations.append(oldAllocation);</span>
195         return nullptr;
196     }
197     ASSERT(oldIndexInSpace == allocation-&gt;indexInSpace());
198 
<span class="line-modified">199     // If reallocation changes the address, we should update HashSet.</span>
<span class="line-added">200     if (oldAllocation != allocation) {</span>
<span class="line-added">201         if (auto* set = m_space.preciseAllocationSet()) {</span>
<span class="line-added">202             set-&gt;remove(oldAllocation-&gt;cell());</span>
<span class="line-added">203             set-&gt;add(allocation-&gt;cell());</span>
<span class="line-added">204         }</span>
<span class="line-added">205     }</span>
<span class="line-added">206 </span>
<span class="line-added">207     m_space.m_preciseAllocations[oldIndexInSpace] = allocation;</span>
208     vm.heap.didAllocate(difference);
209     m_space.m_capacity += difference;
210 
<span class="line-modified">211     m_preciseAllocations.append(allocation);</span>
212 
213     return allocation-&gt;cell();
214 }
215 
216 } // namespace JSC
217 
</pre>
</td>
</tr>
</table>
<center><a href="CodeBlockSetInlines.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CompleteSubspace.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>