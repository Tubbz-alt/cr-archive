<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="../B3Width.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../index.html" target="_top">index</a> <a href="AirAllocateRegistersAndStackAndGenerateCode.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;AirArgInlines.h&quot;
 32 #include &quot;AirBlockInsertionSet.h&quot;
 33 #include &quot;AirCode.h&quot;
 34 #include &quot;AirHandleCalleeSaves.h&quot;
 35 #include &quot;AirLowerStackArgs.h&quot;
 36 #include &quot;AirStackAllocation.h&quot;
 37 #include &quot;AirTmpMap.h&quot;
 38 #include &quot;CCallHelpers.h&quot;
 39 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
 40 
 41 namespace JSC { namespace B3 { namespace Air {
 42 
 43 GenerateAndAllocateRegisters::GenerateAndAllocateRegisters(Code&amp; code)
 44     : m_code(code)
 45     , m_map(code)
 46 { }
 47 





























 48 void GenerateAndAllocateRegisters::buildLiveRanges(UnifiedTmpLiveness&amp; liveness)
 49 {
 50     m_liveRangeEnd = TmpMap&lt;size_t&gt;(m_code, 0);
 51 
 52     m_globalInstIndex = 0;
 53     for (BasicBlock* block : m_code) {
 54         for (Tmp tmp : liveness.liveAtHead(block)) {
 55             if (!tmp.isReg())
 56                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 57         }

 58         for (Inst&amp; inst : *block) {
 59             inst.forEachTmpFast([&amp;] (Tmp tmp) {
 60                 if (!tmp.isReg())
 61                     m_liveRangeEnd[tmp] = m_globalInstIndex;
 62             });
 63             ++m_globalInstIndex;
 64         }
 65         for (Tmp tmp : liveness.liveAtTail(block)) {
 66             if (!tmp.isReg())
 67                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 68         }

 69     }
 70 }
 71 
 72 void GenerateAndAllocateRegisters::insertBlocksForFlushAfterTerminalPatchpoints()
 73 {
 74     BlockInsertionSet blockInsertionSet(m_code);
 75     for (BasicBlock* block : m_code) {
 76         Inst&amp; inst = block-&gt;last();
 77         if (inst.kind.opcode != Patch)
 78             continue;
 79 
 80         HashMap&lt;Tmp, Arg*&gt; needToDef;
 81 
 82         inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
 83             if (!arg.isTmp())
 84                 return;
 85             Tmp tmp = arg.tmp();
 86             if (Arg::isAnyDef(role) &amp;&amp; !tmp.isReg())
 87                 needToDef.add(tmp, &amp;arg);
 88         });
</pre>
<hr />
<pre>
106     blockInsertionSet.execute();
107 }
108 
109 static ALWAYS_INLINE CCallHelpers::Address callFrameAddr(CCallHelpers&amp; jit, intptr_t offsetFromFP)
110 {
111     if (isX86()) {
112         ASSERT(Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP).isValidForm(Width64));
113         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
114     }
115 
116     ASSERT(pinnedExtendedOffsetAddrRegister());
117     auto addr = Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP);
118     if (addr.isValidForm(Width64))
119         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
120     GPRReg reg = *pinnedExtendedOffsetAddrRegister();
121     jit.move(CCallHelpers::TrustedImmPtr(offsetFromFP), reg);
122     jit.add64(GPRInfo::callFrameRegister, reg);
123     return CCallHelpers::Address(reg);
124 }
125 












126 ALWAYS_INLINE void GenerateAndAllocateRegisters::flush(Tmp tmp, Reg reg)
127 {
128     ASSERT(tmp);
129     intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
130     if (tmp.isGP())
131         m_jit-&gt;store64(reg.gpr(), callFrameAddr(*m_jit, offset));
132     else
133         m_jit-&gt;storeDouble(reg.fpr(), callFrameAddr(*m_jit, offset));
134 }
135 
136 ALWAYS_INLINE void GenerateAndAllocateRegisters::spill(Tmp tmp, Reg reg)
137 {
138     ASSERT(reg);
139     ASSERT(m_map[tmp].reg == reg);
<span class="line-removed">140     m_availableRegs[tmp.bank()].set(reg);</span>
<span class="line-removed">141     m_currentAllocation-&gt;at(reg) = Tmp();</span>
142     flush(tmp, reg);
<span class="line-modified">143     m_map[tmp].reg = Reg();</span>
144 }
145 
146 ALWAYS_INLINE void GenerateAndAllocateRegisters::alloc(Tmp tmp, Reg reg, bool isDef)
147 {
148     if (Tmp occupyingTmp = m_currentAllocation-&gt;at(reg))
149         spill(occupyingTmp, reg);
150     else {
151         ASSERT(!m_currentAllocation-&gt;at(reg));
152         ASSERT(m_availableRegs[tmp.bank()].get(reg));
153     }
154 
155     m_map[tmp].reg = reg;
156     m_availableRegs[tmp.bank()].clear(reg);
157     m_currentAllocation-&gt;at(reg) = tmp;
158 
159     if (!isDef) {
160         intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
161         if (tmp.bank() == GP)
162             m_jit-&gt;load64(callFrameAddr(*m_jit, offset), reg.gpr());
163         else
164             m_jit-&gt;loadDouble(callFrameAddr(*m_jit, offset), reg.fpr());
165     }
166 }
167 
168 ALWAYS_INLINE void GenerateAndAllocateRegisters::freeDeadTmpsIfNeeded()
169 {
170     if (m_didAlreadyFreeDeadSlots)
171         return;
172 
173     m_didAlreadyFreeDeadSlots = true;
174     for (size_t i = 0; i &lt; m_currentAllocation-&gt;size(); ++i) {
175         Tmp tmp = m_currentAllocation-&gt;at(i);
176         if (!tmp)
177             continue;
178         if (tmp.isReg())
179             continue;
180         if (m_liveRangeEnd[tmp] &gt;= m_globalInstIndex)
181             continue;
182 
<span class="line-modified">183         Reg reg = Reg::fromIndex(i);</span>
<span class="line-removed">184         m_map[tmp].reg = Reg();</span>
<span class="line-removed">185         m_availableRegs[tmp.bank()].set(reg);</span>
<span class="line-removed">186         m_currentAllocation-&gt;at(i) = Tmp();</span>
187     }
188 }
189 
190 ALWAYS_INLINE bool GenerateAndAllocateRegisters::assignTmp(Tmp&amp; tmp, Bank bank, bool isDef)
191 {
192     ASSERT(!tmp.isReg());
193     if (Reg reg = m_map[tmp].reg) {
194         ASSERT(!m_namedDefdRegs.contains(reg));
195         tmp = Tmp(reg);
196         m_namedUsedRegs.set(reg);
197         ASSERT(!m_availableRegs[bank].get(reg));
198         return true;
199     }
200 
201     if (!m_availableRegs[bank].numberOfSetRegisters())
202         freeDeadTmpsIfNeeded();
203 
204     if (m_availableRegs[bank].numberOfSetRegisters()) {
205         // We first take an available register.
206         for (Reg reg : m_registers[bank]) {
</pre>
<hr />
<pre>
227         alloc(tmp, reg, isDef);
228         tmp = Tmp(reg);
229         return true;
230     }
231 
232     // This can happen if we have a #WarmAnys &gt; #Available registers
233     return false;
234 }
235 
236 ALWAYS_INLINE bool GenerateAndAllocateRegisters::isDisallowedRegister(Reg reg)
237 {
238     return !m_allowedRegisters.get(reg);
239 }
240 
241 void GenerateAndAllocateRegisters::prepareForGeneration()
242 {
243     // We pessimistically assume we use all callee saves.
244     handleCalleeSaves(m_code, RegisterSet::calleeSaveRegisters());
245     allocateEscapedStackSlots(m_code);
246 
<span class="line-modified">247     // Each Tmp gets its own stack slot.</span>
<span class="line-removed">248     auto createStackSlot = [&amp;] (const Tmp&amp; tmp) {</span>
<span class="line-removed">249         TmpData data;</span>
<span class="line-removed">250         data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);</span>
<span class="line-removed">251         data.reg = Reg();</span>
<span class="line-removed">252         m_map[tmp] = data;</span>
<span class="line-removed">253 #if !ASSERT_DISABLED</span>
<span class="line-removed">254         m_allTmps[tmp.bank()].append(tmp);</span>
<span class="line-removed">255 #endif</span>
<span class="line-removed">256     };</span>
257 

258     m_code.forEachTmp([&amp;] (Tmp tmp) {
259         ASSERT(!tmp.isReg());
<span class="line-modified">260         createStackSlot(tmp);</span>
261     });





























































262 
263     m_allowedRegisters = RegisterSet();
264 
265     forEachBank([&amp;] (Bank bank) {
266         m_registers[bank] = m_code.regsInPriorityOrder(bank);
267 
268         for (Reg reg : m_registers[bank]) {
269             m_allowedRegisters.set(reg);
<span class="line-modified">270             createStackSlot(Tmp(reg));</span>


271         }
272     });
273 
274     {
275         unsigned nextIndex = 0;
276         for (StackSlot* slot : m_code.stackSlots()) {
277             if (slot-&gt;isLocked())
278                 continue;
279             intptr_t offset = -static_cast&lt;intptr_t&gt;(m_code.frameSize()) - static_cast&lt;intptr_t&gt;(nextIndex) * 8 - 8;
280             ++nextIndex;
281             slot-&gt;setOffsetFromFP(offset);
282         }
283     }
284 
285     updateFrameSizeBasedOnStackSlots(m_code);
286     m_code.setStackIsAllocated(true);
287 
288     lowerStackArgs(m_code);
289 

290     // Verify none of these passes add any tmps.
<span class="line-removed">291 #if !ASSERT_DISABLED</span>
292     forEachBank([&amp;] (Bank bank) {
<span class="line-modified">293         ASSERT(m_allTmps[bank].size() - m_registers[bank].size() == m_code.numTmps(bank));</span>
294     });


























295 #endif
296 }
297 
298 void GenerateAndAllocateRegisters::generate(CCallHelpers&amp; jit)
299 {
300     m_jit = &amp;jit;
301 
302     TimingScope timingScope(&quot;Air::generateAndAllocateRegisters&quot;);
303 
<span class="line-removed">304     insertBlocksForFlushAfterTerminalPatchpoints();</span>
<span class="line-removed">305 </span>
306     DisallowMacroScratchRegisterUsage disallowScratch(*m_jit);
307 
<span class="line-modified">308     UnifiedTmpLiveness liveness(m_code);</span>
<span class="line-removed">309     buildLiveRanges(liveness);</span>
310 
311     IndexMap&lt;BasicBlock*, IndexMap&lt;Reg, Tmp&gt;&gt; currentAllocationMap(m_code.size());
312     {
313         IndexMap&lt;Reg, Tmp&gt; defaultCurrentAllocation(Reg::maxIndex() + 1);
314         for (BasicBlock* block : m_code)
315             currentAllocationMap[block] = defaultCurrentAllocation;
316 
317         // The only things live that are in registers at the root blocks are
318         // the explicitly named registers that are live.
319 
320         for (unsigned i = m_code.numEntrypoints(); i--;) {
321             BasicBlock* entrypoint = m_code.entrypoint(i).block();
<span class="line-modified">322             for (Tmp tmp : liveness.liveAtHead(entrypoint)) {</span>
323                 if (tmp.isReg())
324                     currentAllocationMap[entrypoint][tmp.reg()] = tmp;
325             }
326         }
327     }
328 
329     // And now, we generate code.
330     GenerationContext context;
331     context.code = &amp;m_code;
332     context.blockLabels.resize(m_code.size());
333     for (BasicBlock* block : m_code)
334         context.blockLabels[block] = Box&lt;CCallHelpers::Label&gt;::create();
335     IndexMap&lt;BasicBlock*, CCallHelpers::JumpList&gt; blockJumps(m_code.size());
336 
337     auto link = [&amp;] (CCallHelpers::Jump jump, BasicBlock* target) {
338         if (context.blockLabels[target]-&gt;isSet()) {
339             jump.linkTo(*context.blockLabels[target], m_jit);
340             return;
341         }
342 
</pre>
<hr />
<pre>
364 
365             m_code.prologueGeneratorForEntrypoint(*entrypointIndex)-&gt;run(*m_jit, m_code);
366 
367             if (disassembler)
368                 disassembler-&gt;endEntrypoint(*m_jit);
369         } else
370             ASSERT(!m_code.isEntrypoint(block));
371 
372         auto startLabel = m_jit-&gt;labelIgnoringWatchpoints();
373 
374         {
375             auto iter = m_blocksAfterTerminalPatchForSpilling.find(block);
376             if (iter != m_blocksAfterTerminalPatchForSpilling.end()) {
377                 auto&amp; data = iter-&gt;value;
378                 data.jump = m_jit-&gt;jump();
379                 data.continueLabel = m_jit-&gt;label();
380             }
381         }
382 
383         forEachBank([&amp;] (Bank bank) {
<span class="line-modified">384 #if !ASSERT_DISABLED</span>
385             // By default, everything is spilled at block boundaries. We do this after we process each block
386             // so we don&#39;t have to walk all Tmps, since #Tmps &gt;&gt; #Available regs. Instead, we walk the register file at
387             // each block boundary and clear entries in this map.
388             for (Tmp tmp : m_allTmps[bank])
389                 ASSERT(m_map[tmp].reg == Reg());
390 #endif
391 
392             RegisterSet availableRegisters;
393             for (Reg reg : m_registers[bank])
394                 availableRegisters.set(reg);
395             m_availableRegs[bank] = WTFMove(availableRegisters);
396         });
397 
398         IndexMap&lt;Reg, Tmp&gt;&amp; currentAllocation = currentAllocationMap[block];
399         m_currentAllocation = &amp;currentAllocation;
400 
401         for (unsigned i = 0; i &lt; currentAllocation.size(); ++i) {
402             Tmp tmp = currentAllocation[i];
403             if (!tmp)
404                 continue;
405             Reg reg = Reg::fromIndex(i);
406             m_map[tmp].reg = reg;
407             m_availableRegs[tmp.bank()].clear(reg);
408         }
409 


410         bool isReplayingSameInst = false;
411         for (size_t instIndex = 0; instIndex &lt; block-&gt;size(); ++instIndex) {


412             if (instIndex &amp;&amp; !isReplayingSameInst)
413                 startLabel = m_jit-&gt;labelIgnoringWatchpoints();
414 
415             context.indexInBlock = instIndex;
416 
417             Inst&amp; inst = block-&gt;at(instIndex);
418 
419             m_didAlreadyFreeDeadSlots = false;
420 
421             m_namedUsedRegs = RegisterSet();
422             m_namedDefdRegs = RegisterSet();
423 








































424             inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
425                 if (!arg.isTmp())
426                     return;
427 
428                 Tmp tmp = arg.tmp();
429                 if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
430                     return;
431 
432                 if (tmp.isReg()) {
433                     if (Arg::isAnyUse(role))
434                         m_namedUsedRegs.set(tmp.reg());
435                     if (Arg::isAnyDef(role))
436                         m_namedDefdRegs.set(tmp.reg());
437                 }
438 
439                 // We convert any cold uses that are already in the stack to just point to
440                 // the canonical stack location.
441                 if (!Arg::isColdUse(role))
442                     return;
443 
</pre>
<hr />
<pre>
465 
466                     m_namedDefdRegs.merge(clobberedRegisters);
467                 }
468             }
469 
470             auto allocNamed = [&amp;] (const RegisterSet&amp; named, bool isDef) {
471                 for (Reg reg : named) {
472                     if (Tmp occupyingTmp = currentAllocation[reg]) {
473                         if (occupyingTmp == Tmp(reg))
474                             continue;
475                     }
476 
477                     freeDeadTmpsIfNeeded(); // We don&#39;t want to spill a dead tmp.
478                     alloc(Tmp(reg), reg, isDef);
479                 }
480             };
481 
482             allocNamed(m_namedUsedRegs, false); // Must come before the defd registers since we may use and def the same register.
483             allocNamed(m_namedDefdRegs, true);
484 
<span class="line-modified">485             {</span>
486                 auto tryAllocate = [&amp;] {
487                     Vector&lt;Tmp*, 8&gt; usesToAlloc;
488                     Vector&lt;Tmp*, 8&gt; defsToAlloc;
489 
490                     inst.forEachTmp([&amp;] (Tmp&amp; tmp, Arg::Role role, Bank, Width) {
491                         if (tmp.isReg())
492                             return;
493 
494                         // We treat Use+Def as a use.
495                         if (Arg::isAnyUse(role))
496                             usesToAlloc.append(&amp;tmp);
497                         else if (Arg::isAnyDef(role))
498                             defsToAlloc.append(&amp;tmp);
499                     });
500 
501                     auto tryAllocateTmps = [&amp;] (auto&amp; vector, bool isDef) {
502                         bool success = true;
503                         for (Tmp* tmp : vector)
504                             success &amp;= assignTmp(*tmp, tmp-&gt;bank(), isDef);
505                         return success;
</pre>
<hr />
<pre>
578                 }
579                 inst.reportUsedRegisters(registerSet);
580             }
581 
582             if (inst.isTerminal() &amp;&amp; block-&gt;numSuccessors()) {
583                 // By default, we spill everything between block boundaries. However, we have a small
584                 // heuristic to pass along register state. We should eventually make this better.
585                 // What we do now is if we have a successor with a single predecessor (us), and we
586                 // haven&#39;t yet generated code for it, we give it our register state. If all our successors
587                 // can take on our register state, we don&#39;t flush at the end of this block.
588 
589                 bool everySuccessorGetsOurRegisterState = true;
590                 for (unsigned i = 0; i &lt; block-&gt;numSuccessors(); ++i) {
591                     BasicBlock* successor = block-&gt;successorBlock(i);
592                     if (successor-&gt;numPredecessors() == 1 &amp;&amp; !context.blockLabels[successor]-&gt;isSet())
593                         currentAllocationMap[successor] = currentAllocation;
594                     else
595                         everySuccessorGetsOurRegisterState = false;
596                 }
597                 if (!everySuccessorGetsOurRegisterState) {
<span class="line-modified">598                     for (Tmp tmp : liveness.liveAtTail(block)) {</span>
599                         if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
600                             continue;
601                         if (Reg reg = m_map[tmp].reg)
602                             flush(tmp, reg);
603                     }
604                 }
605             }
606 
607             if (!inst.isTerminal()) {
<span class="line-modified">608                 CCallHelpers::Jump jump = inst.generate(*m_jit, context);</span>


609                 ASSERT_UNUSED(jump, !jump.isSet());
610 
611                 for (Reg reg : clobberedRegisters) {
612                     Tmp tmp(reg);
613                     ASSERT(currentAllocation[reg] == tmp);
614                     m_availableRegs[tmp.bank()].set(reg);
615                     m_currentAllocation-&gt;at(reg) = Tmp();
616                     m_map[tmp].reg = Reg();
617                 }
618             } else {
<span class="line-modified">619                 bool needsToGenerate = true;</span>
620                 if (inst.kind.opcode == Jump &amp;&amp; block-&gt;successorBlock(0) == m_code.findNextBlock(block))
621                     needsToGenerate = false;
622 
623                 if (isReturn(inst.kind.opcode)) {
624                     needsToGenerate = false;
625 
626                     // We currently don&#39;t represent the full epilogue in Air, so we need to
627                     // have this override.
628                     if (m_code.frameSize()) {
629                         m_jit-&gt;emitRestore(m_code.calleeSaveRegisterAtOffsetList());
630                         m_jit-&gt;emitFunctionEpilogue();
631                     } else
632                         m_jit-&gt;emitFunctionEpilogueWithEmptyFrame();
633                     m_jit-&gt;ret();
634                 }
635 
636                 if (needsToGenerate) {
637                     CCallHelpers::Jump jump = inst.generate(*m_jit, context);
638 
639                     // The jump won&#39;t be set for patchpoints. It won&#39;t be set for Oops because then it won&#39;t have
</pre>
<hr />
<pre>
655                     }
656                 }
657             }
658 
659             auto endLabel = m_jit-&gt;labelIgnoringWatchpoints();
660             if (disassembler)
661                 disassembler-&gt;addInst(&amp;inst, startLabel, endLabel);
662 
663             ++m_globalInstIndex;
664         }
665 
666         // Registers usually get spilled at block boundaries. We do it this way since we don&#39;t
667         // want to iterate the entire TmpMap, since usually #Tmps &gt;&gt; #Regs. We may not actually spill
668         // all registers, but at the top of this loop we handle that case by pre-populating register
669         // state. Here, we just clear this map. After this loop, this map should contain only
670         // null entries.
671         for (size_t i = 0; i &lt; currentAllocation.size(); ++i) {
672             if (Tmp tmp = currentAllocation[i])
673                 m_map[tmp].reg = Reg();
674         }


675     }
676 
677     for (auto&amp; entry : m_blocksAfterTerminalPatchForSpilling) {
678         entry.value.jump.linkTo(m_jit-&gt;label(), m_jit);
679         const HashMap&lt;Tmp, Arg*&gt;&amp; spills = entry.value.defdTmps;
680         for (auto&amp; entry : spills) {
681             Arg* arg = entry.value;
682             if (!arg-&gt;isTmp())
683                 continue;
684             Tmp originalTmp = entry.key;
685             Tmp currentTmp = arg-&gt;tmp();
686             ASSERT_WITH_MESSAGE(currentTmp.isReg(), &quot;We already did register allocation so we should have assigned this Tmp to a register.&quot;);
687             flush(originalTmp, currentTmp.reg());
688         }
689         m_jit-&gt;jump().linkTo(entry.value.continueLabel, m_jit);
690     }
691 
692     context.currentBlock = nullptr;
693     context.indexInBlock = UINT_MAX;
694 
</pre>
</td>
<td>
<hr />
<pre>
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;AirArgInlines.h&quot;
 32 #include &quot;AirBlockInsertionSet.h&quot;
 33 #include &quot;AirCode.h&quot;
 34 #include &quot;AirHandleCalleeSaves.h&quot;
 35 #include &quot;AirLowerStackArgs.h&quot;
 36 #include &quot;AirStackAllocation.h&quot;
 37 #include &quot;AirTmpMap.h&quot;
 38 #include &quot;CCallHelpers.h&quot;
 39 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
 40 
 41 namespace JSC { namespace B3 { namespace Air {
 42 
 43 GenerateAndAllocateRegisters::GenerateAndAllocateRegisters(Code&amp; code)
 44     : m_code(code)
 45     , m_map(code)
 46 { }
 47 
<span class="line-added"> 48 ALWAYS_INLINE void GenerateAndAllocateRegisters::checkConsistency()</span>
<span class="line-added"> 49 {</span>
<span class="line-added"> 50     // This isn&#39;t exactly the right option for this but adding a new one for just this seems silly.</span>
<span class="line-added"> 51     if (Options::validateGraph() || Options::validateGraphAtEachPhase()) {</span>
<span class="line-added"> 52         m_code.forEachTmp([&amp;] (Tmp tmp) {</span>
<span class="line-added"> 53             Reg reg = m_map[tmp].reg;</span>
<span class="line-added"> 54             if (!reg)</span>
<span class="line-added"> 55                 return;</span>
<span class="line-added"> 56 </span>
<span class="line-added"> 57             ASSERT(!m_availableRegs[tmp.bank()].contains(reg));</span>
<span class="line-added"> 58             ASSERT(m_currentAllocation-&gt;at(reg) == tmp);</span>
<span class="line-added"> 59         });</span>
<span class="line-added"> 60 </span>
<span class="line-added"> 61         for (Reg reg : RegisterSet::allRegisters()) {</span>
<span class="line-added"> 62             if (isDisallowedRegister(reg))</span>
<span class="line-added"> 63                 continue;</span>
<span class="line-added"> 64 </span>
<span class="line-added"> 65             Tmp tmp = m_currentAllocation-&gt;at(reg);</span>
<span class="line-added"> 66             if (!tmp) {</span>
<span class="line-added"> 67                 ASSERT(m_availableRegs[bankForReg(reg)].contains(reg));</span>
<span class="line-added"> 68                 continue;</span>
<span class="line-added"> 69             }</span>
<span class="line-added"> 70 </span>
<span class="line-added"> 71             ASSERT(!m_availableRegs[tmp.bank()].contains(reg));</span>
<span class="line-added"> 72             ASSERT(m_map[tmp].reg == reg);</span>
<span class="line-added"> 73         }</span>
<span class="line-added"> 74     }</span>
<span class="line-added"> 75 }</span>
<span class="line-added"> 76 </span>
 77 void GenerateAndAllocateRegisters::buildLiveRanges(UnifiedTmpLiveness&amp; liveness)
 78 {
 79     m_liveRangeEnd = TmpMap&lt;size_t&gt;(m_code, 0);
 80 
 81     m_globalInstIndex = 0;
 82     for (BasicBlock* block : m_code) {
 83         for (Tmp tmp : liveness.liveAtHead(block)) {
 84             if (!tmp.isReg())
 85                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 86         }
<span class="line-added"> 87         ++m_globalInstIndex;</span>
 88         for (Inst&amp; inst : *block) {
 89             inst.forEachTmpFast([&amp;] (Tmp tmp) {
 90                 if (!tmp.isReg())
 91                     m_liveRangeEnd[tmp] = m_globalInstIndex;
 92             });
 93             ++m_globalInstIndex;
 94         }
 95         for (Tmp tmp : liveness.liveAtTail(block)) {
 96             if (!tmp.isReg())
 97                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 98         }
<span class="line-added"> 99         ++m_globalInstIndex;</span>
100     }
101 }
102 
103 void GenerateAndAllocateRegisters::insertBlocksForFlushAfterTerminalPatchpoints()
104 {
105     BlockInsertionSet blockInsertionSet(m_code);
106     for (BasicBlock* block : m_code) {
107         Inst&amp; inst = block-&gt;last();
108         if (inst.kind.opcode != Patch)
109             continue;
110 
111         HashMap&lt;Tmp, Arg*&gt; needToDef;
112 
113         inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
114             if (!arg.isTmp())
115                 return;
116             Tmp tmp = arg.tmp();
117             if (Arg::isAnyDef(role) &amp;&amp; !tmp.isReg())
118                 needToDef.add(tmp, &amp;arg);
119         });
</pre>
<hr />
<pre>
137     blockInsertionSet.execute();
138 }
139 
140 static ALWAYS_INLINE CCallHelpers::Address callFrameAddr(CCallHelpers&amp; jit, intptr_t offsetFromFP)
141 {
142     if (isX86()) {
143         ASSERT(Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP).isValidForm(Width64));
144         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
145     }
146 
147     ASSERT(pinnedExtendedOffsetAddrRegister());
148     auto addr = Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP);
149     if (addr.isValidForm(Width64))
150         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
151     GPRReg reg = *pinnedExtendedOffsetAddrRegister();
152     jit.move(CCallHelpers::TrustedImmPtr(offsetFromFP), reg);
153     jit.add64(GPRInfo::callFrameRegister, reg);
154     return CCallHelpers::Address(reg);
155 }
156 
<span class="line-added">157 ALWAYS_INLINE void GenerateAndAllocateRegisters::release(Tmp tmp, Reg reg)</span>
<span class="line-added">158 {</span>
<span class="line-added">159     ASSERT(reg);</span>
<span class="line-added">160     ASSERT(m_currentAllocation-&gt;at(reg) == tmp);</span>
<span class="line-added">161     m_currentAllocation-&gt;at(reg) = Tmp();</span>
<span class="line-added">162     ASSERT(!m_availableRegs[tmp.bank()].contains(reg));</span>
<span class="line-added">163     m_availableRegs[tmp.bank()].set(reg);</span>
<span class="line-added">164     ASSERT(m_map[tmp].reg == reg);</span>
<span class="line-added">165     m_map[tmp].reg = Reg();</span>
<span class="line-added">166 }</span>
<span class="line-added">167 </span>
<span class="line-added">168 </span>
169 ALWAYS_INLINE void GenerateAndAllocateRegisters::flush(Tmp tmp, Reg reg)
170 {
171     ASSERT(tmp);
172     intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
173     if (tmp.isGP())
174         m_jit-&gt;store64(reg.gpr(), callFrameAddr(*m_jit, offset));
175     else
176         m_jit-&gt;storeDouble(reg.fpr(), callFrameAddr(*m_jit, offset));
177 }
178 
179 ALWAYS_INLINE void GenerateAndAllocateRegisters::spill(Tmp tmp, Reg reg)
180 {
181     ASSERT(reg);
182     ASSERT(m_map[tmp].reg == reg);


183     flush(tmp, reg);
<span class="line-modified">184     release(tmp, reg);</span>
185 }
186 
187 ALWAYS_INLINE void GenerateAndAllocateRegisters::alloc(Tmp tmp, Reg reg, bool isDef)
188 {
189     if (Tmp occupyingTmp = m_currentAllocation-&gt;at(reg))
190         spill(occupyingTmp, reg);
191     else {
192         ASSERT(!m_currentAllocation-&gt;at(reg));
193         ASSERT(m_availableRegs[tmp.bank()].get(reg));
194     }
195 
196     m_map[tmp].reg = reg;
197     m_availableRegs[tmp.bank()].clear(reg);
198     m_currentAllocation-&gt;at(reg) = tmp;
199 
200     if (!isDef) {
201         intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
202         if (tmp.bank() == GP)
203             m_jit-&gt;load64(callFrameAddr(*m_jit, offset), reg.gpr());
204         else
205             m_jit-&gt;loadDouble(callFrameAddr(*m_jit, offset), reg.fpr());
206     }
207 }
208 
209 ALWAYS_INLINE void GenerateAndAllocateRegisters::freeDeadTmpsIfNeeded()
210 {
211     if (m_didAlreadyFreeDeadSlots)
212         return;
213 
214     m_didAlreadyFreeDeadSlots = true;
215     for (size_t i = 0; i &lt; m_currentAllocation-&gt;size(); ++i) {
216         Tmp tmp = m_currentAllocation-&gt;at(i);
217         if (!tmp)
218             continue;
219         if (tmp.isReg())
220             continue;
221         if (m_liveRangeEnd[tmp] &gt;= m_globalInstIndex)
222             continue;
223 
<span class="line-modified">224         release(tmp, Reg::fromIndex(i));</span>



225     }
226 }
227 
228 ALWAYS_INLINE bool GenerateAndAllocateRegisters::assignTmp(Tmp&amp; tmp, Bank bank, bool isDef)
229 {
230     ASSERT(!tmp.isReg());
231     if (Reg reg = m_map[tmp].reg) {
232         ASSERT(!m_namedDefdRegs.contains(reg));
233         tmp = Tmp(reg);
234         m_namedUsedRegs.set(reg);
235         ASSERT(!m_availableRegs[bank].get(reg));
236         return true;
237     }
238 
239     if (!m_availableRegs[bank].numberOfSetRegisters())
240         freeDeadTmpsIfNeeded();
241 
242     if (m_availableRegs[bank].numberOfSetRegisters()) {
243         // We first take an available register.
244         for (Reg reg : m_registers[bank]) {
</pre>
<hr />
<pre>
265         alloc(tmp, reg, isDef);
266         tmp = Tmp(reg);
267         return true;
268     }
269 
270     // This can happen if we have a #WarmAnys &gt; #Available registers
271     return false;
272 }
273 
274 ALWAYS_INLINE bool GenerateAndAllocateRegisters::isDisallowedRegister(Reg reg)
275 {
276     return !m_allowedRegisters.get(reg);
277 }
278 
279 void GenerateAndAllocateRegisters::prepareForGeneration()
280 {
281     // We pessimistically assume we use all callee saves.
282     handleCalleeSaves(m_code, RegisterSet::calleeSaveRegisters());
283     allocateEscapedStackSlots(m_code);
284 
<span class="line-modified">285     insertBlocksForFlushAfterTerminalPatchpoints();</span>









286 
<span class="line-added">287 #if ASSERT_ENABLED</span>
288     m_code.forEachTmp([&amp;] (Tmp tmp) {
289         ASSERT(!tmp.isReg());
<span class="line-modified">290         m_allTmps[tmp.bank()].append(tmp);</span>
291     });
<span class="line-added">292 #endif</span>
<span class="line-added">293 </span>
<span class="line-added">294     m_liveness = makeUnique&lt;UnifiedTmpLiveness&gt;(m_code);</span>
<span class="line-added">295 </span>
<span class="line-added">296     {</span>
<span class="line-added">297         buildLiveRanges(*m_liveness);</span>
<span class="line-added">298 </span>
<span class="line-added">299         Vector&lt;StackSlot*, 16&gt; freeSlots;</span>
<span class="line-added">300         Vector&lt;StackSlot*, 4&gt; toFree;</span>
<span class="line-added">301         m_globalInstIndex = 0;</span>
<span class="line-added">302         for (BasicBlock* block : m_code) {</span>
<span class="line-added">303             auto assignStackSlotToTmp = [&amp;] (Tmp tmp) {</span>
<span class="line-added">304                 if (tmp.isReg())</span>
<span class="line-added">305                     return;</span>
<span class="line-added">306 </span>
<span class="line-added">307                 TmpData&amp; data = m_map[tmp];</span>
<span class="line-added">308                 if (data.spillSlot) {</span>
<span class="line-added">309                     if (m_liveRangeEnd[tmp] == m_globalInstIndex)</span>
<span class="line-added">310                         toFree.append(data.spillSlot);</span>
<span class="line-added">311                     return;</span>
<span class="line-added">312                 }</span>
<span class="line-added">313 </span>
<span class="line-added">314                 if (freeSlots.size())</span>
<span class="line-added">315                     data.spillSlot = freeSlots.takeLast();</span>
<span class="line-added">316                 else</span>
<span class="line-added">317                     data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);</span>
<span class="line-added">318                 data.reg = Reg();</span>
<span class="line-added">319             };</span>
<span class="line-added">320 </span>
<span class="line-added">321             auto flushToFreeList = [&amp;] {</span>
<span class="line-added">322                 for (auto* stackSlot : toFree)</span>
<span class="line-added">323                     freeSlots.append(stackSlot);</span>
<span class="line-added">324                 toFree.clear();</span>
<span class="line-added">325             };</span>
<span class="line-added">326 </span>
<span class="line-added">327             for (Tmp tmp : m_liveness-&gt;liveAtHead(block))</span>
<span class="line-added">328                 assignStackSlotToTmp(tmp);</span>
<span class="line-added">329             flushToFreeList();</span>
<span class="line-added">330 </span>
<span class="line-added">331             ++m_globalInstIndex;</span>
<span class="line-added">332 </span>
<span class="line-added">333             for (Inst&amp; inst : *block) {</span>
<span class="line-added">334                 Vector&lt;Tmp, 4&gt; seenTmps;</span>
<span class="line-added">335                 inst.forEachTmpFast([&amp;] (Tmp tmp) {</span>
<span class="line-added">336                     if (seenTmps.contains(tmp))</span>
<span class="line-added">337                         return;</span>
<span class="line-added">338                     seenTmps.append(tmp);</span>
<span class="line-added">339                     assignStackSlotToTmp(tmp);</span>
<span class="line-added">340                 });</span>
<span class="line-added">341 </span>
<span class="line-added">342                 flushToFreeList();</span>
<span class="line-added">343                 ++m_globalInstIndex;</span>
<span class="line-added">344             }</span>
<span class="line-added">345 </span>
<span class="line-added">346             for (Tmp tmp : m_liveness-&gt;liveAtTail(block))</span>
<span class="line-added">347                 assignStackSlotToTmp(tmp);</span>
<span class="line-added">348             flushToFreeList();</span>
<span class="line-added">349 </span>
<span class="line-added">350             ++m_globalInstIndex;</span>
<span class="line-added">351         }</span>
<span class="line-added">352     }</span>
353 
354     m_allowedRegisters = RegisterSet();
355 
356     forEachBank([&amp;] (Bank bank) {
357         m_registers[bank] = m_code.regsInPriorityOrder(bank);
358 
359         for (Reg reg : m_registers[bank]) {
360             m_allowedRegisters.set(reg);
<span class="line-modified">361             TmpData&amp; data = m_map[Tmp(reg)];</span>
<span class="line-added">362             data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);</span>
<span class="line-added">363             data.reg = Reg();</span>
364         }
365     });
366 
367     {
368         unsigned nextIndex = 0;
369         for (StackSlot* slot : m_code.stackSlots()) {
370             if (slot-&gt;isLocked())
371                 continue;
372             intptr_t offset = -static_cast&lt;intptr_t&gt;(m_code.frameSize()) - static_cast&lt;intptr_t&gt;(nextIndex) * 8 - 8;
373             ++nextIndex;
374             slot-&gt;setOffsetFromFP(offset);
375         }
376     }
377 
378     updateFrameSizeBasedOnStackSlots(m_code);
379     m_code.setStackIsAllocated(true);
380 
381     lowerStackArgs(m_code);
382 
<span class="line-added">383 #if ASSERT_ENABLED</span>
384     // Verify none of these passes add any tmps.

385     forEachBank([&amp;] (Bank bank) {
<span class="line-modified">386         ASSERT(m_allTmps[bank].size() == m_code.numTmps(bank));</span>
387     });
<span class="line-added">388 </span>
<span class="line-added">389     {</span>
<span class="line-added">390         // Verify that lowerStackArgs didn&#39;t change Tmp liveness at the boundaries for the Tmps and Registers we model.</span>
<span class="line-added">391         UnifiedTmpLiveness liveness(m_code);</span>
<span class="line-added">392         for (BasicBlock* block : m_code) {</span>
<span class="line-added">393             auto assertLivenessAreEqual = [&amp;] (auto a, auto b) {</span>
<span class="line-added">394                 HashSet&lt;Tmp&gt; livenessA;</span>
<span class="line-added">395                 HashSet&lt;Tmp&gt; livenessB;</span>
<span class="line-added">396                 for (Tmp tmp : a) {</span>
<span class="line-added">397                     if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))</span>
<span class="line-added">398                         continue;</span>
<span class="line-added">399                     livenessA.add(tmp);</span>
<span class="line-added">400                 }</span>
<span class="line-added">401                 for (Tmp tmp : b) {</span>
<span class="line-added">402                     if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))</span>
<span class="line-added">403                         continue;</span>
<span class="line-added">404                     livenessB.add(tmp);</span>
<span class="line-added">405                 }</span>
<span class="line-added">406 </span>
<span class="line-added">407                 ASSERT(livenessA == livenessB);</span>
<span class="line-added">408             };</span>
<span class="line-added">409 </span>
<span class="line-added">410             assertLivenessAreEqual(m_liveness-&gt;liveAtHead(block), liveness.liveAtHead(block));</span>
<span class="line-added">411             assertLivenessAreEqual(m_liveness-&gt;liveAtTail(block), liveness.liveAtTail(block));</span>
<span class="line-added">412         }</span>
<span class="line-added">413     }</span>
414 #endif
415 }
416 
417 void GenerateAndAllocateRegisters::generate(CCallHelpers&amp; jit)
418 {
419     m_jit = &amp;jit;
420 
421     TimingScope timingScope(&quot;Air::generateAndAllocateRegisters&quot;);
422 


423     DisallowMacroScratchRegisterUsage disallowScratch(*m_jit);
424 
<span class="line-modified">425     buildLiveRanges(*m_liveness);</span>

426 
427     IndexMap&lt;BasicBlock*, IndexMap&lt;Reg, Tmp&gt;&gt; currentAllocationMap(m_code.size());
428     {
429         IndexMap&lt;Reg, Tmp&gt; defaultCurrentAllocation(Reg::maxIndex() + 1);
430         for (BasicBlock* block : m_code)
431             currentAllocationMap[block] = defaultCurrentAllocation;
432 
433         // The only things live that are in registers at the root blocks are
434         // the explicitly named registers that are live.
435 
436         for (unsigned i = m_code.numEntrypoints(); i--;) {
437             BasicBlock* entrypoint = m_code.entrypoint(i).block();
<span class="line-modified">438             for (Tmp tmp : m_liveness-&gt;liveAtHead(entrypoint)) {</span>
439                 if (tmp.isReg())
440                     currentAllocationMap[entrypoint][tmp.reg()] = tmp;
441             }
442         }
443     }
444 
445     // And now, we generate code.
446     GenerationContext context;
447     context.code = &amp;m_code;
448     context.blockLabels.resize(m_code.size());
449     for (BasicBlock* block : m_code)
450         context.blockLabels[block] = Box&lt;CCallHelpers::Label&gt;::create();
451     IndexMap&lt;BasicBlock*, CCallHelpers::JumpList&gt; blockJumps(m_code.size());
452 
453     auto link = [&amp;] (CCallHelpers::Jump jump, BasicBlock* target) {
454         if (context.blockLabels[target]-&gt;isSet()) {
455             jump.linkTo(*context.blockLabels[target], m_jit);
456             return;
457         }
458 
</pre>
<hr />
<pre>
480 
481             m_code.prologueGeneratorForEntrypoint(*entrypointIndex)-&gt;run(*m_jit, m_code);
482 
483             if (disassembler)
484                 disassembler-&gt;endEntrypoint(*m_jit);
485         } else
486             ASSERT(!m_code.isEntrypoint(block));
487 
488         auto startLabel = m_jit-&gt;labelIgnoringWatchpoints();
489 
490         {
491             auto iter = m_blocksAfterTerminalPatchForSpilling.find(block);
492             if (iter != m_blocksAfterTerminalPatchForSpilling.end()) {
493                 auto&amp; data = iter-&gt;value;
494                 data.jump = m_jit-&gt;jump();
495                 data.continueLabel = m_jit-&gt;label();
496             }
497         }
498 
499         forEachBank([&amp;] (Bank bank) {
<span class="line-modified">500 #if ASSERT_ENABLED</span>
501             // By default, everything is spilled at block boundaries. We do this after we process each block
502             // so we don&#39;t have to walk all Tmps, since #Tmps &gt;&gt; #Available regs. Instead, we walk the register file at
503             // each block boundary and clear entries in this map.
504             for (Tmp tmp : m_allTmps[bank])
505                 ASSERT(m_map[tmp].reg == Reg());
506 #endif
507 
508             RegisterSet availableRegisters;
509             for (Reg reg : m_registers[bank])
510                 availableRegisters.set(reg);
511             m_availableRegs[bank] = WTFMove(availableRegisters);
512         });
513 
514         IndexMap&lt;Reg, Tmp&gt;&amp; currentAllocation = currentAllocationMap[block];
515         m_currentAllocation = &amp;currentAllocation;
516 
517         for (unsigned i = 0; i &lt; currentAllocation.size(); ++i) {
518             Tmp tmp = currentAllocation[i];
519             if (!tmp)
520                 continue;
521             Reg reg = Reg::fromIndex(i);
522             m_map[tmp].reg = reg;
523             m_availableRegs[tmp.bank()].clear(reg);
524         }
525 
<span class="line-added">526         ++m_globalInstIndex;</span>
<span class="line-added">527 </span>
528         bool isReplayingSameInst = false;
529         for (size_t instIndex = 0; instIndex &lt; block-&gt;size(); ++instIndex) {
<span class="line-added">530             checkConsistency();</span>
<span class="line-added">531 </span>
532             if (instIndex &amp;&amp; !isReplayingSameInst)
533                 startLabel = m_jit-&gt;labelIgnoringWatchpoints();
534 
535             context.indexInBlock = instIndex;
536 
537             Inst&amp; inst = block-&gt;at(instIndex);
538 
539             m_didAlreadyFreeDeadSlots = false;
540 
541             m_namedUsedRegs = RegisterSet();
542             m_namedDefdRegs = RegisterSet();
543 
<span class="line-added">544             bool needsToGenerate = ([&amp;] () -&gt; bool {</span>
<span class="line-added">545                 // FIXME: We should consider trying to figure out if we can also elide Mov32s</span>
<span class="line-added">546                 if (!(inst.kind.opcode == Move || inst.kind.opcode == MoveDouble))</span>
<span class="line-added">547                     return true;</span>
<span class="line-added">548 </span>
<span class="line-added">549                 ASSERT(inst.args.size() &gt;= 2);</span>
<span class="line-added">550                 Arg source = inst.args[0];</span>
<span class="line-added">551                 Arg dest = inst.args[1];</span>
<span class="line-added">552                 if (!source.isTmp() || !dest.isTmp())</span>
<span class="line-added">553                     return true;</span>
<span class="line-added">554 </span>
<span class="line-added">555                 // FIXME: We don&#39;t track where the last use of a reg is globally so we don&#39;t know where we can elide them.</span>
<span class="line-added">556                 ASSERT(source.isReg() || m_liveRangeEnd[source.tmp()] &gt;= m_globalInstIndex);</span>
<span class="line-added">557                 if (source.isReg() || m_liveRangeEnd[source.tmp()] != m_globalInstIndex)</span>
<span class="line-added">558                     return true;</span>
<span class="line-added">559 </span>
<span class="line-added">560                 // If we are doing a self move at the end of the temps liveness we can trivially elide the move.</span>
<span class="line-added">561                 if (source == dest)</span>
<span class="line-added">562                     return false;</span>
<span class="line-added">563 </span>
<span class="line-added">564                 Reg sourceReg = m_map[source.tmp()].reg;</span>
<span class="line-added">565                 // If the value is not already materialized into a register we may still move it into one so let the normal generation code run.</span>
<span class="line-added">566                 if (!sourceReg)</span>
<span class="line-added">567                     return true;</span>
<span class="line-added">568 </span>
<span class="line-added">569                 ASSERT(m_currentAllocation-&gt;at(sourceReg) == source.tmp());</span>
<span class="line-added">570 </span>
<span class="line-added">571                 if (dest.isReg() &amp;&amp; dest.reg() != sourceReg)</span>
<span class="line-added">572                     return true;</span>
<span class="line-added">573 </span>
<span class="line-added">574                 if (Reg oldReg = m_map[dest.tmp()].reg)</span>
<span class="line-added">575                     release(dest.tmp(), oldReg);</span>
<span class="line-added">576 </span>
<span class="line-added">577                 m_map[dest.tmp()].reg = sourceReg;</span>
<span class="line-added">578                 m_currentAllocation-&gt;at(sourceReg) = dest.tmp();</span>
<span class="line-added">579                 m_map[source.tmp()].reg = Reg();</span>
<span class="line-added">580                 return false;</span>
<span class="line-added">581             })();</span>
<span class="line-added">582             checkConsistency();</span>
<span class="line-added">583 </span>
584             inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
585                 if (!arg.isTmp())
586                     return;
587 
588                 Tmp tmp = arg.tmp();
589                 if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
590                     return;
591 
592                 if (tmp.isReg()) {
593                     if (Arg::isAnyUse(role))
594                         m_namedUsedRegs.set(tmp.reg());
595                     if (Arg::isAnyDef(role))
596                         m_namedDefdRegs.set(tmp.reg());
597                 }
598 
599                 // We convert any cold uses that are already in the stack to just point to
600                 // the canonical stack location.
601                 if (!Arg::isColdUse(role))
602                     return;
603 
</pre>
<hr />
<pre>
625 
626                     m_namedDefdRegs.merge(clobberedRegisters);
627                 }
628             }
629 
630             auto allocNamed = [&amp;] (const RegisterSet&amp; named, bool isDef) {
631                 for (Reg reg : named) {
632                     if (Tmp occupyingTmp = currentAllocation[reg]) {
633                         if (occupyingTmp == Tmp(reg))
634                             continue;
635                     }
636 
637                     freeDeadTmpsIfNeeded(); // We don&#39;t want to spill a dead tmp.
638                     alloc(Tmp(reg), reg, isDef);
639                 }
640             };
641 
642             allocNamed(m_namedUsedRegs, false); // Must come before the defd registers since we may use and def the same register.
643             allocNamed(m_namedDefdRegs, true);
644 
<span class="line-modified">645             if (needsToGenerate) {</span>
646                 auto tryAllocate = [&amp;] {
647                     Vector&lt;Tmp*, 8&gt; usesToAlloc;
648                     Vector&lt;Tmp*, 8&gt; defsToAlloc;
649 
650                     inst.forEachTmp([&amp;] (Tmp&amp; tmp, Arg::Role role, Bank, Width) {
651                         if (tmp.isReg())
652                             return;
653 
654                         // We treat Use+Def as a use.
655                         if (Arg::isAnyUse(role))
656                             usesToAlloc.append(&amp;tmp);
657                         else if (Arg::isAnyDef(role))
658                             defsToAlloc.append(&amp;tmp);
659                     });
660 
661                     auto tryAllocateTmps = [&amp;] (auto&amp; vector, bool isDef) {
662                         bool success = true;
663                         for (Tmp* tmp : vector)
664                             success &amp;= assignTmp(*tmp, tmp-&gt;bank(), isDef);
665                         return success;
</pre>
<hr />
<pre>
738                 }
739                 inst.reportUsedRegisters(registerSet);
740             }
741 
742             if (inst.isTerminal() &amp;&amp; block-&gt;numSuccessors()) {
743                 // By default, we spill everything between block boundaries. However, we have a small
744                 // heuristic to pass along register state. We should eventually make this better.
745                 // What we do now is if we have a successor with a single predecessor (us), and we
746                 // haven&#39;t yet generated code for it, we give it our register state. If all our successors
747                 // can take on our register state, we don&#39;t flush at the end of this block.
748 
749                 bool everySuccessorGetsOurRegisterState = true;
750                 for (unsigned i = 0; i &lt; block-&gt;numSuccessors(); ++i) {
751                     BasicBlock* successor = block-&gt;successorBlock(i);
752                     if (successor-&gt;numPredecessors() == 1 &amp;&amp; !context.blockLabels[successor]-&gt;isSet())
753                         currentAllocationMap[successor] = currentAllocation;
754                     else
755                         everySuccessorGetsOurRegisterState = false;
756                 }
757                 if (!everySuccessorGetsOurRegisterState) {
<span class="line-modified">758                     for (Tmp tmp : m_liveness-&gt;liveAtTail(block)) {</span>
759                         if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
760                             continue;
761                         if (Reg reg = m_map[tmp].reg)
762                             flush(tmp, reg);
763                     }
764                 }
765             }
766 
767             if (!inst.isTerminal()) {
<span class="line-modified">768                 CCallHelpers::Jump jump;</span>
<span class="line-added">769                 if (needsToGenerate)</span>
<span class="line-added">770                     jump = inst.generate(*m_jit, context);</span>
771                 ASSERT_UNUSED(jump, !jump.isSet());
772 
773                 for (Reg reg : clobberedRegisters) {
774                     Tmp tmp(reg);
775                     ASSERT(currentAllocation[reg] == tmp);
776                     m_availableRegs[tmp.bank()].set(reg);
777                     m_currentAllocation-&gt;at(reg) = Tmp();
778                     m_map[tmp].reg = Reg();
779                 }
780             } else {
<span class="line-modified">781                 ASSERT(needsToGenerate);</span>
782                 if (inst.kind.opcode == Jump &amp;&amp; block-&gt;successorBlock(0) == m_code.findNextBlock(block))
783                     needsToGenerate = false;
784 
785                 if (isReturn(inst.kind.opcode)) {
786                     needsToGenerate = false;
787 
788                     // We currently don&#39;t represent the full epilogue in Air, so we need to
789                     // have this override.
790                     if (m_code.frameSize()) {
791                         m_jit-&gt;emitRestore(m_code.calleeSaveRegisterAtOffsetList());
792                         m_jit-&gt;emitFunctionEpilogue();
793                     } else
794                         m_jit-&gt;emitFunctionEpilogueWithEmptyFrame();
795                     m_jit-&gt;ret();
796                 }
797 
798                 if (needsToGenerate) {
799                     CCallHelpers::Jump jump = inst.generate(*m_jit, context);
800 
801                     // The jump won&#39;t be set for patchpoints. It won&#39;t be set for Oops because then it won&#39;t have
</pre>
<hr />
<pre>
817                     }
818                 }
819             }
820 
821             auto endLabel = m_jit-&gt;labelIgnoringWatchpoints();
822             if (disassembler)
823                 disassembler-&gt;addInst(&amp;inst, startLabel, endLabel);
824 
825             ++m_globalInstIndex;
826         }
827 
828         // Registers usually get spilled at block boundaries. We do it this way since we don&#39;t
829         // want to iterate the entire TmpMap, since usually #Tmps &gt;&gt; #Regs. We may not actually spill
830         // all registers, but at the top of this loop we handle that case by pre-populating register
831         // state. Here, we just clear this map. After this loop, this map should contain only
832         // null entries.
833         for (size_t i = 0; i &lt; currentAllocation.size(); ++i) {
834             if (Tmp tmp = currentAllocation[i])
835                 m_map[tmp].reg = Reg();
836         }
<span class="line-added">837 </span>
<span class="line-added">838         ++m_globalInstIndex;</span>
839     }
840 
841     for (auto&amp; entry : m_blocksAfterTerminalPatchForSpilling) {
842         entry.value.jump.linkTo(m_jit-&gt;label(), m_jit);
843         const HashMap&lt;Tmp, Arg*&gt;&amp; spills = entry.value.defdTmps;
844         for (auto&amp; entry : spills) {
845             Arg* arg = entry.value;
846             if (!arg-&gt;isTmp())
847                 continue;
848             Tmp originalTmp = entry.key;
849             Tmp currentTmp = arg-&gt;tmp();
850             ASSERT_WITH_MESSAGE(currentTmp.isReg(), &quot;We already did register allocation so we should have assigned this Tmp to a register.&quot;);
851             flush(originalTmp, currentTmp.reg());
852         }
853         m_jit-&gt;jump().linkTo(entry.value.continueLabel, m_jit);
854     }
855 
856     context.currentBlock = nullptr;
857     context.indexInBlock = UINT_MAX;
858 
</pre>
</td>
</tr>
</table>
<center><a href="../B3Width.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../index.html" target="_top">index</a> <a href="AirAllocateRegistersAndStackAndGenerateCode.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>