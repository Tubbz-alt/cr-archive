<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2009-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2010 Patrick Gansterer &lt;paroga@paroga.com&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  * 1. Redistributions of source code must retain the above copyright
   9  *    notice, this list of conditions and the following disclaimer.
  10  * 2. Redistributions in binary form must reproduce the above copyright
  11  *    notice, this list of conditions and the following disclaimer in the
  12  *    documentation and/or other materials provided with the distribution.
  13  *
  14  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  15  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  16  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  17  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  18  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  19  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  20  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  21  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  22  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  23  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  24  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  25  */
  26 
  27 #include &quot;config.h&quot;
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;BasicBlockLocation.h&quot;
  32 #include &quot;BytecodeGenerator.h&quot;
  33 #include &quot;Exception.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSCast.h&quot;
  39 #include &quot;JSFunction.h&quot;
  40 #include &quot;JSPropertyNameEnumerator.h&quot;
  41 #include &quot;LinkBuffer.h&quot;
  42 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  43 #include &quot;OpcodeInlines.h&quot;
  44 #include &quot;SlowPathCall.h&quot;
  45 #include &quot;SuperSampler.h&quot;
  46 #include &quot;ThunkGenerators.h&quot;
  47 #include &quot;TypeLocation.h&quot;
  48 #include &quot;TypeProfilerLog.h&quot;
  49 #include &quot;VirtualRegister.h&quot;
  50 #include &quot;Watchdog.h&quot;
  51 
  52 namespace JSC {
  53 
  54 #if USE(JSVALUE64)
  55 
  56 void JIT::emit_op_mov(const Instruction* currentInstruction)
  57 {
  58     auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
  59     VirtualRegister dst = bytecode.m_dst;
  60     VirtualRegister src = bytecode.m_src;
  61 
  62     if (src.isConstant()) {
  63         JSValue value = m_codeBlock-&gt;getConstant(src);
  64         if (!value.isNumber())
  65             store64(TrustedImm64(JSValue::encode(value)), addressFor(dst));
  66         else
  67             store64(Imm64(JSValue::encode(value)), addressFor(dst));
  68         return;
  69     }
  70 
  71     load64(addressFor(src), regT0);
  72     store64(regT0, addressFor(dst));
  73 }
  74 
  75 
  76 void JIT::emit_op_end(const Instruction* currentInstruction)
  77 {
  78     auto bytecode = currentInstruction-&gt;as&lt;OpEnd&gt;();
  79     RELEASE_ASSERT(returnValueGPR != callFrameRegister);
  80     emitGetVirtualRegister(bytecode.m_value, returnValueGPR);
  81     emitRestoreCalleeSaves();
  82     emitFunctionEpilogue();
  83     ret();
  84 }
  85 
  86 void JIT::emit_op_jmp(const Instruction* currentInstruction)
  87 {
  88     auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
  89     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
  90     addJump(jump(), target);
  91 }
  92 
  93 void JIT::emit_op_new_object(const Instruction* currentInstruction)
  94 {
  95     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
  96     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  97     Structure* structure = metadata.m_objectAllocationProfile.structure();
  98     size_t allocationSize = JSFinalObject::allocationSize(structure-&gt;inlineCapacity());
  99     Allocator allocator = allocatorForNonVirtualConcurrently&lt;JSFinalObject&gt;(*m_vm, allocationSize, AllocatorForMode::AllocatorIfExists);
 100 
 101     RegisterID resultReg = regT0;
 102     RegisterID allocatorReg = regT1;
 103     RegisterID scratchReg = regT2;
 104 
 105     if (!allocator)
 106         addSlowCase(jump());
 107     else {
 108         JumpList slowCases;
 109         auto butterfly = TrustedImmPtr(nullptr);
 110         emitAllocateJSObject(resultReg, JITAllocator::constant(allocator), allocatorReg, TrustedImmPtr(structure), butterfly, scratchReg, slowCases);
 111         emitInitializeInlineStorage(resultReg, structure-&gt;inlineCapacity());
 112         addSlowCase(slowCases);
 113         emitPutVirtualRegister(bytecode.m_dst);
 114     }
 115 }
 116 
 117 void JIT::emitSlow_op_new_object(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 118 {
 119     linkAllSlowCases(iter);
 120 
 121     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
 122     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 123     VirtualRegister dst = bytecode.m_dst;
 124     Structure* structure = metadata.m_objectAllocationProfile.structure();
 125     callOperation(operationNewObject, TrustedImmPtr(&amp;vm()), structure);
 126     emitStoreCell(dst, returnValueGPR);
 127 }
 128 
 129 void JIT::emit_op_overrides_has_instance(const Instruction* currentInstruction)
 130 {
 131     auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
 132     VirtualRegister dst = bytecode.m_dst;
 133     VirtualRegister constructor = bytecode.m_constructor;
 134     VirtualRegister hasInstanceValue = bytecode.m_hasInstanceValue;
 135 
 136     emitGetVirtualRegister(hasInstanceValue, regT0);
 137 
 138     // We don&#39;t jump if we know what Symbol.hasInstance would do.
 139     Jump customhasInstanceValue = branchPtr(NotEqual, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()-&gt;functionProtoHasInstanceSymbolFunction()));
 140 
 141     emitGetVirtualRegister(constructor, regT0);
 142 
 143     // Check that constructor &#39;ImplementsDefaultHasInstance&#39; i.e. the object is not a C-API user nor a bound function.
 144     test8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(ImplementsDefaultHasInstance), regT0);
 145     boxBoolean(regT0, JSValueRegs { regT0 });
 146     Jump done = jump();
 147 
 148     customhasInstanceValue.link(this);
 149     move(TrustedImm32(JSValue::ValueTrue), regT0);
 150 
 151     done.link(this);
 152     emitPutVirtualRegister(dst);
 153 }
 154 
 155 void JIT::emit_op_instanceof(const Instruction* currentInstruction)
 156 {
 157     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
 158     VirtualRegister dst = bytecode.m_dst;
 159     VirtualRegister value = bytecode.m_value;
 160     VirtualRegister proto = bytecode.m_prototype;
 161 
 162     // Load the operands (baseVal, proto, and value respectively) into registers.
 163     // We use regT0 for baseVal since we will be done with this first, and we can then use it for the result.
 164     emitGetVirtualRegister(value, regT2);
 165     emitGetVirtualRegister(proto, regT1);
 166 
 167     // Check that proto are cells. baseVal must be a cell - this is checked by the get_by_id for Symbol.hasInstance.
 168     emitJumpSlowCaseIfNotJSCell(regT2, value);
 169     emitJumpSlowCaseIfNotJSCell(regT1, proto);
 170 
 171     JITInstanceOfGenerator gen(
 172         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex),
 173         RegisterSet::stubUnavailableRegisters(),
 174         regT0, // result
 175         regT2, // value
 176         regT1, // proto
 177         regT3, regT4); // scratch
 178     gen.generateFastPath(*this);
 179     m_instanceOfs.append(gen);
 180 
 181     emitPutVirtualRegister(dst);
 182 }
 183 
 184 void JIT::emitSlow_op_instanceof(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 185 {
 186     linkAllSlowCases(iter);
 187 
 188     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
 189     VirtualRegister resultVReg = bytecode.m_dst;
 190 
 191     JITInstanceOfGenerator&amp; gen = m_instanceOfs[m_instanceOfIndex++];
 192 
 193     Label coldPathBegin = label();
 194     Call call = callOperation(operationInstanceOfOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT2, regT1);
 195     gen.reportSlowPathCall(coldPathBegin, call);
 196 }
 197 
 198 void JIT::emit_op_instanceof_custom(const Instruction*)
 199 {
 200     // This always goes to slow path since we expect it to be rare.
 201     addSlowCase(jump());
 202 }
 203 
 204 void JIT::emit_op_is_empty(const Instruction* currentInstruction)
 205 {
 206     auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
 207     VirtualRegister dst = bytecode.m_dst;
 208     VirtualRegister value = bytecode.m_operand;
 209 
 210     emitGetVirtualRegister(value, regT0);
 211     compare64(Equal, regT0, TrustedImm32(JSValue::encode(JSValue())), regT0);
 212 
 213     boxBoolean(regT0, JSValueRegs { regT0 });
 214     emitPutVirtualRegister(dst);
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
 220     VirtualRegister dst = bytecode.m_dst;
 221     VirtualRegister value = bytecode.m_operand;
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
 226     compare64(Equal, regT0, TrustedImm32(JSValue::ValueUndefined), regT0);
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
 235     emitLoadStructure(vm(), regT0, regT1, regT2);
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
 249     VirtualRegister dst = bytecode.m_dst;
 250     VirtualRegister value = bytecode.m_operand;
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
 254     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 255     compare64(Equal, regT0, TrustedImm32(JSValue::ValueNull), regT0);
 256 
 257     boxBoolean(regT0, JSValueRegs { regT0 });
 258     emitPutVirtualRegister(dst);
 259 }
 260 
 261 void JIT::emit_op_is_boolean(const Instruction* currentInstruction)
 262 {
 263     auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
 264     VirtualRegister dst = bytecode.m_dst;
 265     VirtualRegister value = bytecode.m_operand;
 266 
 267     emitGetVirtualRegister(value, regT0);
 268     xor64(TrustedImm32(JSValue::ValueFalse), regT0);
 269     test64(Zero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1)), regT0);
 270     boxBoolean(regT0, JSValueRegs { regT0 });
 271     emitPutVirtualRegister(dst);
 272 }
 273 
 274 void JIT::emit_op_is_number(const Instruction* currentInstruction)
 275 {
 276     auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
 277     VirtualRegister dst = bytecode.m_dst;
 278     VirtualRegister value = bytecode.m_operand;
 279 
 280     emitGetVirtualRegister(value, regT0);
 281     test64(NonZero, regT0, numberTagRegister, regT0);
 282     boxBoolean(regT0, JSValueRegs { regT0 });
 283     emitPutVirtualRegister(dst);
 284 }
 285 
 286 void JIT::emit_op_is_cell_with_type(const Instruction* currentInstruction)
 287 {
 288     auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
 289     VirtualRegister dst = bytecode.m_dst;
 290     VirtualRegister value = bytecode.m_operand;
 291     int type = bytecode.m_type;
 292 
 293     emitGetVirtualRegister(value, regT0);
 294     Jump isNotCell = branchIfNotCell(regT0);
 295 
 296     compare8(Equal, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(type), regT0);
 297     boxBoolean(regT0, JSValueRegs { regT0 });
 298     Jump done = jump();
 299 
 300     isNotCell.link(this);
 301     move(TrustedImm32(JSValue::ValueFalse), regT0);
 302 
 303     done.link(this);
 304     emitPutVirtualRegister(dst);
 305 }
 306 
 307 void JIT::emit_op_is_object(const Instruction* currentInstruction)
 308 {
 309     auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
 310     VirtualRegister dst = bytecode.m_dst;
 311     VirtualRegister value = bytecode.m_operand;
 312 
 313     emitGetVirtualRegister(value, regT0);
 314     Jump isNotCell = branchIfNotCell(regT0);
 315 
 316     compare8(AboveOrEqual, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType), regT0);
 317     boxBoolean(regT0, JSValueRegs { regT0 });
 318     Jump done = jump();
 319 
 320     isNotCell.link(this);
 321     move(TrustedImm32(JSValue::ValueFalse), regT0);
 322 
 323     done.link(this);
 324     emitPutVirtualRegister(dst);
 325 }
 326 
 327 void JIT::emit_op_ret(const Instruction* currentInstruction)
 328 {
 329     ASSERT(callFrameRegister != regT1);
 330     ASSERT(regT1 != returnValueGPR);
 331     ASSERT(returnValueGPR != callFrameRegister);
 332 
 333     // Return the result in %eax.
 334     auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
 335     emitGetVirtualRegister(bytecode.m_value, returnValueGPR);
 336 
 337     checkStackPointerAlignment();
 338     emitRestoreCalleeSaves();
 339     emitFunctionEpilogue();
 340     ret();
 341 }
 342 
 343 void JIT::emit_op_to_primitive(const Instruction* currentInstruction)
 344 {
 345     auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
 346     VirtualRegister dst = bytecode.m_dst;
 347     VirtualRegister src = bytecode.m_src;
 348 
 349     emitGetVirtualRegister(src, regT0);
 350 
 351     Jump isImm = branchIfNotCell(regT0);
 352     addSlowCase(branchIfObject(regT0));
 353     isImm.link(this);
 354 
 355     if (dst != src)
 356         emitPutVirtualRegister(dst);
 357 
 358 }
 359 
 360 void JIT::emit_op_to_property_key(const Instruction* currentInstruction)
 361 {
 362     auto bytecode = currentInstruction-&gt;as&lt;OpToPropertyKey&gt;();
 363     VirtualRegister dst = bytecode.m_dst;
 364     VirtualRegister src = bytecode.m_src;
 365 
 366     emitGetVirtualRegister(src, regT0);
 367 
 368     addSlowCase(branchIfNotCell(regT0));
 369     Jump done = branchIfSymbol(regT0);
 370     addSlowCase(branchIfNotString(regT0));
 371 
 372     done.link(this);
 373     if (src != dst)
 374         emitPutVirtualRegister(dst);
 375 }
 376 
 377 void JIT::emit_op_set_function_name(const Instruction* currentInstruction)
 378 {
 379     auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
 380     emitGetVirtualRegister(bytecode.m_function, regT0);
 381     emitGetVirtualRegister(bytecode.m_name, regT1);
 382     callOperation(operationSetFunctionName, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
 383 }
 384 
 385 void JIT::emit_op_not(const Instruction* currentInstruction)
 386 {
 387     auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
 388     emitGetVirtualRegister(bytecode.m_operand, regT0);
 389 
 390     // Invert against JSValue(false); if the value was tagged as a boolean, then all bits will be
 391     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 392     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
 393     xor64(TrustedImm32(JSValue::ValueFalse), regT0);
 394     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
 395     xor64(TrustedImm32(JSValue::ValueTrue), regT0);
 396 
 397     emitPutVirtualRegister(bytecode.m_dst);
 398 }
 399 
 400 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 401 {
 402     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 403     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 404 
 405     GPRReg value = regT0;
 406     GPRReg scratch1 = regT1;
 407     GPRReg scratch2 = regT2;
 408     bool shouldCheckMasqueradesAsUndefined = true;
 409 
 410     emitGetVirtualRegister(bytecode.m_condition, value);
 411     addJump(branchIfFalsey(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 412 }
 413 
 414 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 415 {
 416     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
 417     VirtualRegister src = bytecode.m_value;
 418     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 419 
 420     emitGetVirtualRegister(src, regT0);
 421     Jump isImmediate = branchIfNotCell(regT0);
 422 
 423     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 424     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 425     emitLoadStructure(vm(), regT0, regT2, regT1);
 426     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 427     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 428     Jump masqueradesGlobalObjectIsForeign = jump();
 429 
 430     // Now handle the immediate cases - undefined &amp; null
 431     isImmediate.link(this);
 432     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 433     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 434 
 435     isNotMasqueradesAsUndefined.link(this);
 436     masqueradesGlobalObjectIsForeign.link(this);
 437 };
 438 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 439 {
 440     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
 441     VirtualRegister src = bytecode.m_value;
 442     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 443 
 444     emitGetVirtualRegister(src, regT0);
 445     Jump isImmediate = branchIfNotCell(regT0);
 446 
 447     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 448     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
 449     emitLoadStructure(vm(), regT0, regT2, regT1);
 450     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 451     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 452     Jump wasNotImmediate = jump();
 453 
 454     // Now handle the immediate cases - undefined &amp; null
 455     isImmediate.link(this);
 456     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 457     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 458 
 459     wasNotImmediate.link(this);
 460 }
 461 
 462 void JIT::emit_op_jundefined_or_null(const Instruction* currentInstruction)
 463 {
 464     auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();
 465     VirtualRegister value = bytecode.m_value;
 466     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 467 
 468     emitGetVirtualRegister(value, regT0);
 469 
 470     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 471     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 472 }
 473 
 474 void JIT::emit_op_jnundefined_or_null(const Instruction* currentInstruction)
 475 {
 476     auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();
 477     VirtualRegister value = bytecode.m_value;
 478     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 479 
 480     emitGetVirtualRegister(value, regT0);
 481 
 482     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 483     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 484 }
 485 
 486 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 487 {
 488     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 489     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 490     VirtualRegister src = bytecode.m_value;
 491     JSValue specialPointer = getConstantOperand(bytecode.m_specialPointer);
 492     ASSERT(specialPointer.isCell());
 493     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 494 
 495     emitGetVirtualRegister(src, regT0);
 496     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(specialPointer.asCell()));
 497     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 498     addJump(jump(), target);
 499     equal.link(this);
 500 }
 501 
 502 void JIT::emit_op_eq(const Instruction* currentInstruction)
 503 {
 504     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 505     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);
 506     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 507     compare32(Equal, regT1, regT0, regT0);
 508     boxBoolean(regT0, JSValueRegs { regT0 });
 509     emitPutVirtualRegister(bytecode.m_dst);
 510 }
 511 
 512 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 513 {
 514     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 515     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 516     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);
 517     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 518     addJump(branch32(Equal, regT0, regT1), target);
 519 }
 520 
 521 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 522 {
 523     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 524     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 525 
 526     GPRReg value = regT0;
 527     GPRReg scratch1 = regT1;
 528     GPRReg scratch2 = regT2;
 529     bool shouldCheckMasqueradesAsUndefined = true;
 530     emitGetVirtualRegister(bytecode.m_condition, value);
 531     addJump(branchIfTruthy(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 532 }
 533 
 534 void JIT::emit_op_neq(const Instruction* currentInstruction)
 535 {
 536     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 537     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);
 538     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 539     compare32(NotEqual, regT1, regT0, regT0);
 540     boxBoolean(regT0, JSValueRegs { regT0 });
 541 
 542     emitPutVirtualRegister(bytecode.m_dst);
 543 }
 544 
 545 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 546 {
 547     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 548     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 549     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);
 550     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 551     addJump(branch32(NotEqual, regT0, regT1), target);
 552 }
 553 
 554 void JIT::emit_op_throw(const Instruction* currentInstruction)
 555 {
 556     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 557     ASSERT(regT0 == returnValueGPR);
 558     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 559     emitGetVirtualRegister(bytecode.m_value, regT0);
 560     callOperationNoExceptionCheck(operationThrow, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 561     jumpToExceptionHandler(vm());
 562 }
 563 
 564 template&lt;typename Op&gt;
 565 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 566 {
 567     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 568     VirtualRegister dst = bytecode.m_dst;
 569     VirtualRegister src1 = bytecode.m_lhs;
 570     VirtualRegister src2 = bytecode.m_rhs;
 571 
 572     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 573 
 574     // Jump slow if both are cells (to cover strings).
 575     move(regT0, regT2);
 576     or64(regT1, regT2);
 577     addSlowCase(branchIfCell(regT2));
 578 
 579     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 580     // if it&#39;s a double.
 581     Jump leftOK = branchIfInt32(regT0);
 582     addSlowCase(branchIfNumber(regT0));
 583     leftOK.link(this);
 584     Jump rightOK = branchIfInt32(regT1);
 585     addSlowCase(branchIfNumber(regT1));
 586     rightOK.link(this);
 587 
 588     if (type == CompileOpStrictEqType::StrictEq)
 589         compare64(Equal, regT1, regT0, regT0);
 590     else
 591         compare64(NotEqual, regT1, regT0, regT0);
 592     boxBoolean(regT0, JSValueRegs { regT0 });
 593 
 594     emitPutVirtualRegister(dst);
 595 }
 596 
 597 void JIT::emit_op_stricteq(const Instruction* currentInstruction)
 598 {
 599     compileOpStrictEq&lt;OpStricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 600 }
 601 
 602 void JIT::emit_op_nstricteq(const Instruction* currentInstruction)
 603 {
 604     compileOpStrictEq&lt;OpNstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 605 }
 606 
 607 template&lt;typename Op&gt;
 608 void JIT::compileOpStrictEqJump(const Instruction* currentInstruction, CompileOpStrictEqType type)
 609 {
 610     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 611     int target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 612     VirtualRegister src1 = bytecode.m_lhs;
 613     VirtualRegister src2 = bytecode.m_rhs;
 614 
 615     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 616 
 617     // Jump slow if both are cells (to cover strings).
 618     move(regT0, regT2);
 619     or64(regT1, regT2);
 620     addSlowCase(branchIfCell(regT2));
 621 
 622     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 623     // if it&#39;s a double.
 624     Jump leftOK = branchIfInt32(regT0);
 625     addSlowCase(branchIfNumber(regT0));
 626     leftOK.link(this);
 627     Jump rightOK = branchIfInt32(regT1);
 628     addSlowCase(branchIfNumber(regT1));
 629     rightOK.link(this);
 630 
 631     if (type == CompileOpStrictEqType::StrictEq)
 632         addJump(branch64(Equal, regT1, regT0), target);
 633     else
 634         addJump(branch64(NotEqual, regT1, regT0), target);
 635 }
 636 
 637 void JIT::emit_op_jstricteq(const Instruction* currentInstruction)
 638 {
 639     compileOpStrictEqJump&lt;OpJstricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 640 }
 641 
 642 void JIT::emit_op_jnstricteq(const Instruction* currentInstruction)
 643 {
 644     compileOpStrictEqJump&lt;OpJnstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 645 }
 646 
 647 void JIT::emitSlow_op_jstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 648 {
 649     linkAllSlowCases(iter);
 650 
 651     auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
 652     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 653     callOperation(operationCompareStrictEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
 654     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 655 }
 656 
 657 void JIT::emitSlow_op_jnstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 658 {
 659     linkAllSlowCases(iter);
 660 
 661     auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
 662     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 663     callOperation(operationCompareStrictEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
 664     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 665 }
 666 
 667 void JIT::emit_op_to_number(const Instruction* currentInstruction)
 668 {
 669     auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
 670     VirtualRegister dstVReg = bytecode.m_dst;
 671     VirtualRegister srcVReg = bytecode.m_operand;
 672     emitGetVirtualRegister(srcVReg, regT0);
 673 
 674     addSlowCase(branchIfNotNumber(regT0));
 675 
 676     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 677     if (srcVReg != dstVReg)
 678         emitPutVirtualRegister(dstVReg);
 679 }
 680 
 681 void JIT::emit_op_to_numeric(const Instruction* currentInstruction)
 682 {
 683     auto bytecode = currentInstruction-&gt;as&lt;OpToNumeric&gt;();
 684     VirtualRegister dstVReg = bytecode.m_dst;
 685     VirtualRegister srcVReg = bytecode.m_operand;
 686     emitGetVirtualRegister(srcVReg, regT0);
 687 
 688     Jump isNotCell = branchIfNotCell(regT0);
 689     addSlowCase(branchIfNotBigInt(regT0));
 690     Jump isBigInt = jump();
 691 
 692     isNotCell.link(this);
 693     addSlowCase(branchIfNotNumber(regT0));
 694     isBigInt.link(this);
 695 
 696     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 697     if (srcVReg != dstVReg)
 698         emitPutVirtualRegister(dstVReg);
 699 }
 700 
 701 void JIT::emit_op_to_string(const Instruction* currentInstruction)
 702 {
 703     auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
 704     VirtualRegister srcVReg = bytecode.m_operand;
 705     emitGetVirtualRegister(srcVReg, regT0);
 706 
 707     addSlowCase(branchIfNotCell(regT0));
 708     addSlowCase(branchIfNotString(regT0));
 709 
 710     emitPutVirtualRegister(bytecode.m_dst);
 711 }
 712 
 713 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 714 {
 715     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
 716     VirtualRegister dstVReg = bytecode.m_dst;
 717     VirtualRegister srcVReg = bytecode.m_operand;
 718     emitGetVirtualRegister(srcVReg, regT0);
 719 
 720     addSlowCase(branchIfNotCell(regT0));
 721     addSlowCase(branchIfNotObject(regT0));
 722 
 723     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 724     if (srcVReg != dstVReg)
 725         emitPutVirtualRegister(dstVReg);
 726 }
 727 
 728 void JIT::emit_op_catch(const Instruction* currentInstruction)
 729 {
 730     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 731 
 732     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 733 
 734     move(TrustedImmPtr(m_vm), regT3);
 735     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 736     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 737 
 738     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 739 
 740     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler, TrustedImmPtr(&amp;vm()));
 741     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
 742     jumpToExceptionHandler(vm());
 743     isCatchableException.link(this);
 744 
 745     move(TrustedImmPtr(m_vm), regT3);
 746     load64(Address(regT3, VM::exceptionOffset()), regT0);
 747     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
 748     emitPutVirtualRegister(bytecode.m_exception);
 749 
 750     load64(Address(regT0, Exception::valueOffset()), regT0);
 751     emitPutVirtualRegister(bytecode.m_thrownValue);
 752 
 753 #if ENABLE(DFG_JIT)
 754     // FIXME: consider inline caching the process of doing OSR entry, including
 755     // argument type proofs, storing locals to the buffer, etc
 756     // https://bugs.webkit.org/show_bug.cgi?id=175598
 757 
 758     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 759     ValueProfileAndVirtualRegisterBuffer* buffer = metadata.m_buffer;
 760     if (buffer || !shouldEmitProfiling())
 761         callOperation(operationTryOSREnterAtCatch, &amp;vm(), m_bytecodeIndex.asBits());
 762     else
 763         callOperation(operationTryOSREnterAtCatchAndValueProfile, &amp;vm(), m_bytecodeIndex.asBits());
 764     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 765     emitRestoreCalleeSaves();
 766     farJump(returnValueGPR, ExceptionHandlerPtrTag);
 767     skipOSREntry.link(this);
 768     if (buffer &amp;&amp; shouldEmitProfiling()) {
 769         buffer-&gt;forEach([&amp;] (ValueProfileAndVirtualRegister&amp; profile) {
 770             JSValueRegs regs(regT0);
 771             emitGetVirtualRegister(profile.m_operand, regs);
 772             emitValueProfilingSite(static_cast&lt;ValueProfile&amp;&gt;(profile));
 773         });
 774     }
 775 #endif // ENABLE(DFG_JIT)
 776 }
 777 
 778 void JIT::emit_op_identity_with_profile(const Instruction*)
 779 {
 780     // We don&#39;t need to do anything here...
 781 }
 782 
 783 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 784 {
 785     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
 786     VirtualRegister currentScope = bytecode.m_scope;
 787     emitGetVirtualRegister(currentScope, regT0);
 788     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 789     emitStoreCell(bytecode.m_dst, regT0);
 790 }
 791 
 792 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 793 {
 794     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 795     size_t tableIndex = bytecode.m_tableIndex;
 796     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 797     VirtualRegister scrutinee = bytecode.m_scrutinee;
 798 
 799     // create jump table for switch destinations, track this switch statement.
 800     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 801     m_switches.append(SwitchRecord(jumpTable, m_bytecodeIndex, defaultOffset, SwitchRecord::Immediate));
 802     jumpTable-&gt;ensureCTITable();
 803 
 804     emitGetVirtualRegister(scrutinee, regT0);
 805     callOperation(operationSwitchImmWithUnknownKeyType, TrustedImmPtr(&amp;vm()), regT0, tableIndex);
 806     farJump(returnValueGPR, JSSwitchPtrTag);
 807 }
 808 
 809 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 810 {
 811     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 812     size_t tableIndex = bytecode.m_tableIndex;
 813     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 814     VirtualRegister scrutinee = bytecode.m_scrutinee;
 815 
 816     // create jump table for switch destinations, track this switch statement.
 817     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 818     m_switches.append(SwitchRecord(jumpTable, m_bytecodeIndex, defaultOffset, SwitchRecord::Character));
 819     jumpTable-&gt;ensureCTITable();
 820 
 821     emitGetVirtualRegister(scrutinee, regT0);
 822     callOperation(operationSwitchCharWithUnknownKeyType, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, tableIndex);
 823     farJump(returnValueGPR, JSSwitchPtrTag);
 824 }
 825 
 826 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 827 {
 828     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 829     size_t tableIndex = bytecode.m_tableIndex;
 830     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 831     VirtualRegister scrutinee = bytecode.m_scrutinee;
 832 
 833     // create jump table for switch destinations, track this switch statement.
 834     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
 835     m_switches.append(SwitchRecord(jumpTable, m_bytecodeIndex, defaultOffset));
 836 
 837     emitGetVirtualRegister(scrutinee, regT0);
 838     callOperation(operationSwitchStringWithUnknownKeyType, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, tableIndex);
 839     farJump(returnValueGPR, JSSwitchPtrTag);
 840 }
 841 
 842 void JIT::emit_op_debug(const Instruction* currentInstruction)
 843 {
 844     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 845     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 846     Jump noDebuggerRequests = branchTest32(Zero, regT0);
 847     callOperation(operationDebug, &amp;vm(), static_cast&lt;int&gt;(bytecode.m_debugHookType));
 848     noDebuggerRequests.link(this);
 849 }
 850 
 851 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 852 {
 853     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
 854     VirtualRegister dst = bytecode.m_dst;
 855     VirtualRegister src1 = bytecode.m_operand;
 856 
 857     emitGetVirtualRegister(src1, regT0);
 858     Jump isImmediate = branchIfNotCell(regT0);
 859 
 860     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 861     move(TrustedImm32(0), regT0);
 862     Jump wasNotMasqueradesAsUndefined = jump();
 863 
 864     isMasqueradesAsUndefined.link(this);
 865     emitLoadStructure(vm(), regT0, regT2, regT1);
 866     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 867     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 868     comparePtr(Equal, regT0, regT2, regT0);
 869     Jump wasNotImmediate = jump();
 870 
 871     isImmediate.link(this);
 872 
 873     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 874     compare64(Equal, regT0, TrustedImm32(JSValue::ValueNull), regT0);
 875 
 876     wasNotImmediate.link(this);
 877     wasNotMasqueradesAsUndefined.link(this);
 878 
 879     boxBoolean(regT0, JSValueRegs { regT0 });
 880     emitPutVirtualRegister(dst);
 881 
 882 }
 883 
 884 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 885 {
 886     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
 887     VirtualRegister dst = bytecode.m_dst;
 888     VirtualRegister src1 = bytecode.m_operand;
 889 
 890     emitGetVirtualRegister(src1, regT0);
 891     Jump isImmediate = branchIfNotCell(regT0);
 892 
 893     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 894     move(TrustedImm32(1), regT0);
 895     Jump wasNotMasqueradesAsUndefined = jump();
 896 
 897     isMasqueradesAsUndefined.link(this);
 898     emitLoadStructure(vm(), regT0, regT2, regT1);
 899     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 900     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 901     comparePtr(NotEqual, regT0, regT2, regT0);
 902     Jump wasNotImmediate = jump();
 903 
 904     isImmediate.link(this);
 905 
 906     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);
 907     compare64(NotEqual, regT0, TrustedImm32(JSValue::ValueNull), regT0);
 908 
 909     wasNotImmediate.link(this);
 910     wasNotMasqueradesAsUndefined.link(this);
 911 
 912     boxBoolean(regT0, JSValueRegs { regT0 });
 913     emitPutVirtualRegister(dst);
 914 }
 915 
 916 void JIT::emit_op_enter(const Instruction*)
 917 {
 918     // Even though CTI doesn&#39;t use them, we initialize our constant
 919     // registers to zap stale pointers, to avoid unnecessarily prolonging
 920     // object lifetime and increasing GC pressure.
 921     size_t count = m_codeBlock-&gt;numVars();
 922     for (size_t j = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); j &lt; count; ++j)
 923         emitInitRegister(virtualRegisterForLocal(j));
 924 
 925     emitWriteBarrier(m_codeBlock);
 926 
 927     emitEnterOptimizationCheck();
 928 }
 929 
 930 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 931 {
 932     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
 933     VirtualRegister dst = bytecode.m_dst;
 934     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 935     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 936     emitStoreCell(dst, regT0);
 937 }
 938 
 939 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 940 {
 941     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 942     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 943     StructureID* cachedStructureID = &amp;metadata.m_cachedStructureID;
 944     emitGetVirtualRegister(bytecode.m_srcDst, regT1);
 945 
 946     emitJumpSlowCaseIfNotJSCell(regT1);
 947 
 948     addSlowCase(branchIfNotType(regT1, FinalObjectType));
 949     load32(cachedStructureID, regT2);
 950     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 951 }
 952 
 953 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 954 {
 955     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 956     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 957     VirtualRegister callee = bytecode.m_callee;
 958     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 959     RegisterID calleeReg = regT0;
 960     RegisterID rareDataReg = regT4;
 961     RegisterID resultReg = regT0;
 962     RegisterID allocatorReg = regT1;
 963     RegisterID structureReg = regT2;
 964     RegisterID cachedFunctionReg = regT4;
 965     RegisterID scratchReg = regT3;
 966 
 967     emitGetVirtualRegister(callee, calleeReg);
 968     addSlowCase(branchIfNotFunction(calleeReg));
 969     loadPtr(Address(calleeReg, JSFunction::offsetOfExecutableOrRareData()), rareDataReg);
 970     addSlowCase(branchTestPtr(Zero, rareDataReg, TrustedImm32(JSFunction::rareDataTag)));
 971     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfAllocator() - JSFunction::rareDataTag), allocatorReg);
 972     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfStructure() - JSFunction::rareDataTag), structureReg);
 973 
 974     loadPtr(cachedFunction, cachedFunctionReg);
 975     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 976     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 977     hasSeenMultipleCallees.link(this);
 978 
 979     JumpList slowCases;
 980     auto butterfly = TrustedImmPtr(nullptr);
 981     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
 982     load8(Address(structureReg, Structure::inlineCapacityOffset()), scratchReg);
 983     emitInitializeInlineStorage(resultReg, scratchReg);
 984     addSlowCase(slowCases);
 985     emitPutVirtualRegister(bytecode.m_dst);
 986 }
 987 
 988 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 989 {
 990     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
 991     emitGetVirtualRegister(bytecode.m_targetVirtualRegister, regT0);
 992     addSlowCase(branchIfEmpty(regT0));
 993 }
 994 
 995 
 996 // Slow cases
 997 
 998 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 999 {
1000     linkAllSlowCases(iter);
1001 
1002     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
1003     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
1004     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1005     emitPutVirtualRegister(bytecode.m_dst, returnValueGPR);
1006 }
1007 
1008 void JIT::emitSlow_op_neq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1009 {
1010     linkAllSlowCases(iter);
1011 
1012     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
1013     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
1014     xor32(TrustedImm32(0x1), regT0);
1015     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1016     emitPutVirtualRegister(bytecode.m_dst, returnValueGPR);
1017 }
1018 
1019 void JIT::emitSlow_op_jeq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1020 {
1021     linkAllSlowCases(iter);
1022 
1023     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
1024     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
1025     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
1026     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
1027 }
1028 
1029 void JIT::emitSlow_op_jneq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1030 {
1031     linkAllSlowCases(iter);
1032 
1033     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
1034     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
1035     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
1036     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
1037 }
1038 
1039 void JIT::emitSlow_op_instanceof_custom(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1040 {
1041     linkAllSlowCases(iter);
1042 
1043     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
1044     VirtualRegister dst = bytecode.m_dst;
1045     VirtualRegister value = bytecode.m_value;
1046     VirtualRegister constructor = bytecode.m_constructor;
1047     VirtualRegister hasInstanceValue = bytecode.m_hasInstanceValue;
1048 
1049     emitGetVirtualRegister(value, regT0);
1050     emitGetVirtualRegister(constructor, regT1);
1051     emitGetVirtualRegister(hasInstanceValue, regT2);
1052     callOperation(operationInstanceOfCustom, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, regT2);
1053     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1054     emitPutVirtualRegister(dst, returnValueGPR);
1055 }
1056 
1057 #endif // USE(JSVALUE64)
1058 
1059 void JIT::emit_op_loop_hint(const Instruction*)
1060 {
1061     // Emit the JIT optimization check:
1062     if (canBeOptimized()) {
1063         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1064             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1065     }
1066 }
1067 
1068 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1069 {
1070 #if ENABLE(DFG_JIT)
1071     // Emit the slow path for the JIT optimization check:
1072     if (canBeOptimized()) {
1073         linkAllSlowCases(iter);
1074 
1075         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
1076 
1077         callOperation(operationOptimize, &amp;vm(), m_bytecodeIndex.asBits());
1078         Jump noOptimizedEntry = branchTestPtr(Zero, returnValueGPR);
1079         if (ASSERT_ENABLED) {
1080             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1081             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1082             ok.link(this);
1083         }
1084         farJump(returnValueGPR, GPRInfo::callFrameRegister);
1085         noOptimizedEntry.link(this);
1086 
1087         emitJumpSlowToHot(jump(), currentInstruction-&gt;size());
1088     }
1089 #else
1090     UNUSED_PARAM(currentInstruction);
1091     UNUSED_PARAM(iter);
1092 #endif
1093 }
1094 
1095 void JIT::emit_op_check_traps(const Instruction*)
1096 {
1097     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));
1098 }
1099 
1100 void JIT::emit_op_nop(const Instruction*)
1101 {
1102 }
1103 
1104 void JIT::emit_op_super_sampler_begin(const Instruction*)
1105 {
1106     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1107 }
1108 
1109 void JIT::emit_op_super_sampler_end(const Instruction*)
1110 {
1111     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1112 }
1113 
1114 void JIT::emitSlow_op_check_traps(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1115 {
1116     linkAllSlowCases(iter);
1117 
1118     callOperation(operationHandleTraps, TrustedImmPtr(m_codeBlock-&gt;globalObject()));
1119 }
1120 
1121 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1122 {
1123     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
1124     VirtualRegister dst = bytecode.m_dst;
1125     VirtualRegister regexp = bytecode.m_regexp;
1126     callOperation(operationNewRegexp, TrustedImmPtr(m_codeBlock-&gt;globalObject()), jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));
1127     emitStoreCell(dst, returnValueGPR);
1128 }
1129 
1130 template&lt;typename Op&gt;
1131 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1132 {
1133     Jump lazyJump;
1134     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1135     VirtualRegister dst = bytecode.m_dst;
1136 
1137 #if USE(JSVALUE64)
1138     emitGetVirtualRegister(bytecode.m_scope, regT0);
1139 #else
1140     emitLoadPayload(bytecode.m_scope, regT0);
1141 #endif
1142     FunctionExecutable* funcExec = m_codeBlock-&gt;functionDecl(bytecode.m_functionDecl);
1143 
1144     OpcodeID opcodeID = Op::opcodeID;
1145     if (opcodeID == op_new_func)
1146         callOperation(operationNewFunction, dst, &amp;vm(), regT0, funcExec);
1147     else if (opcodeID == op_new_generator_func)
1148         callOperation(operationNewGeneratorFunction, dst, &amp;vm(), regT0, funcExec);
1149     else if (opcodeID == op_new_async_func)
1150         callOperation(operationNewAsyncFunction, dst, &amp;vm(), regT0, funcExec);
1151     else {
1152         ASSERT(opcodeID == op_new_async_generator_func);
1153         callOperation(operationNewAsyncGeneratorFunction, dst, &amp;vm(), regT0, funcExec);
1154     }
1155 }
1156 
1157 void JIT::emit_op_new_func(const Instruction* currentInstruction)
1158 {
1159     emitNewFuncCommon&lt;OpNewFunc&gt;(currentInstruction);
1160 }
1161 
1162 void JIT::emit_op_new_generator_func(const Instruction* currentInstruction)
1163 {
1164     emitNewFuncCommon&lt;OpNewGeneratorFunc&gt;(currentInstruction);
1165 }
1166 
1167 void JIT::emit_op_new_async_generator_func(const Instruction* currentInstruction)
1168 {
1169     emitNewFuncCommon&lt;OpNewAsyncGeneratorFunc&gt;(currentInstruction);
1170 }
1171 
1172 void JIT::emit_op_new_async_func(const Instruction* currentInstruction)
1173 {
1174     emitNewFuncCommon&lt;OpNewAsyncFunc&gt;(currentInstruction);
1175 }
1176 
1177 template&lt;typename Op&gt;
1178 void JIT::emitNewFuncExprCommon(const Instruction* currentInstruction)
1179 {
1180     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1181     VirtualRegister dst = bytecode.m_dst;
1182 #if USE(JSVALUE64)
1183     emitGetVirtualRegister(bytecode.m_scope, regT0);
1184 #else
1185     emitLoadPayload(bytecode.m_scope, regT0);
1186 #endif
1187 
1188     FunctionExecutable* function = m_codeBlock-&gt;functionExpr(bytecode.m_functionDecl);
1189     OpcodeID opcodeID = Op::opcodeID;
1190 
1191     if (opcodeID == op_new_func_exp)
1192         callOperation(operationNewFunction, dst, &amp;vm(), regT0, function);
1193     else if (opcodeID == op_new_generator_func_exp)
1194         callOperation(operationNewGeneratorFunction, dst, &amp;vm(), regT0, function);
1195     else if (opcodeID == op_new_async_func_exp)
1196         callOperation(operationNewAsyncFunction, dst, &amp;vm(), regT0, function);
1197     else {
1198         ASSERT(opcodeID == op_new_async_generator_func_exp);
1199         callOperation(operationNewAsyncGeneratorFunction, dst, &amp;vm(), regT0, function);
1200     }
1201 }
1202 
1203 void JIT::emit_op_new_func_exp(const Instruction* currentInstruction)
1204 {
1205     emitNewFuncExprCommon&lt;OpNewFuncExp&gt;(currentInstruction);
1206 }
1207 
1208 void JIT::emit_op_new_generator_func_exp(const Instruction* currentInstruction)
1209 {
1210     emitNewFuncExprCommon&lt;OpNewGeneratorFuncExp&gt;(currentInstruction);
1211 }
1212 
1213 void JIT::emit_op_new_async_func_exp(const Instruction* currentInstruction)
1214 {
1215     emitNewFuncExprCommon&lt;OpNewAsyncFuncExp&gt;(currentInstruction);
1216 }
1217 
1218 void JIT::emit_op_new_async_generator_func_exp(const Instruction* currentInstruction)
1219 {
1220     emitNewFuncExprCommon&lt;OpNewAsyncGeneratorFuncExp&gt;(currentInstruction);
1221 }
1222 
1223 void JIT::emit_op_new_array(const Instruction* currentInstruction)
1224 {
1225     auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
1226     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1227     VirtualRegister dst = bytecode.m_dst;
1228     VirtualRegister valuesStart = bytecode.m_argv;
1229     int size = bytecode.m_argc;
1230     addPtr(TrustedImm32(valuesStart.offset() * sizeof(Register)), callFrameRegister, regT0);
1231     callOperation(operationNewArrayWithProfile, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()),
1232         &amp;metadata.m_arrayAllocationProfile, regT0, size);
1233 }
1234 
1235 void JIT::emit_op_new_array_with_size(const Instruction* currentInstruction)
1236 {
1237     auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
1238     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1239     VirtualRegister dst = bytecode.m_dst;
1240     VirtualRegister sizeIndex = bytecode.m_length;
1241 #if USE(JSVALUE64)
1242     emitGetVirtualRegister(sizeIndex, regT0);
1243     callOperation(operationNewArrayWithSizeAndProfile, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()),
1244         &amp;metadata.m_arrayAllocationProfile, regT0);
1245 #else
1246     emitLoad(sizeIndex, regT1, regT0);
1247     callOperation(operationNewArrayWithSizeAndProfile, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()),
1248         &amp;metadata.m_arrayAllocationProfile, JSValueRegs(regT1, regT0));
1249 #endif
1250 }
1251 
1252 #if USE(JSVALUE64)
1253 void JIT::emit_op_has_structure_property(const Instruction* currentInstruction)
1254 {
1255     auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
1256     VirtualRegister dst = bytecode.m_dst;
1257     VirtualRegister base = bytecode.m_base;
1258     VirtualRegister enumerator = bytecode.m_enumerator;
1259 
1260     emitGetVirtualRegister(base, regT0);
1261     emitGetVirtualRegister(enumerator, regT1);
1262     emitJumpSlowCaseIfNotJSCell(regT0, base);
1263 
1264     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1265     addSlowCase(branch32(NotEqual, regT0, Address(regT1, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1266 
1267     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1268     emitPutVirtualRegister(dst);
1269 }
1270 
1271 void JIT::privateCompileHasIndexedProperty(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1272 {
1273     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1274 
1275     PatchableJump badType;
1276 
1277     // FIXME: Add support for other types like TypedArrays and Arguments.
1278     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1279     JumpList slowCases = emitLoadForArrayMode(currentInstruction, arrayMode, badType);
1280     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1281     Jump done = jump();
1282 
1283     LinkBuffer patchBuffer(*this, m_codeBlock);
1284 
1285     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1286     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1287 
1288     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1289 
1290     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1291         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1292         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1293 
1294     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1295     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1296 }
1297 
1298 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1299 {
1300     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1301     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1302     VirtualRegister dst = bytecode.m_dst;
1303     VirtualRegister base = bytecode.m_base;
1304     VirtualRegister property = bytecode.m_property;
1305     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1306     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1307 
1308     emitGetVirtualRegisters(base, regT0, property, regT1);
1309 
1310     emitJumpSlowCaseIfNotInt(regT1);
1311 
1312     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1313     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1314     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1315     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1316     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1317     // extending since it makes it easier to re-tag the value in the slow case.
1318     zeroExtend32ToPtr(regT1, regT1);
1319 
1320     emitJumpSlowCaseIfNotJSCell(regT0, base);
1321     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1322     and32(TrustedImm32(IndexingShapeMask), regT2);
1323 
1324     JITArrayMode mode = chooseArrayMode(profile);
1325     PatchableJump badType;
1326 
1327     // FIXME: Add support for other types like TypedArrays and Arguments.
1328     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1329     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1330 
1331     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1332 
1333     addSlowCase(badType);
1334     addSlowCase(slowCases);
1335 
1336     Label done = label();
1337 
1338     emitPutVirtualRegister(dst);
1339 
1340     Label nextHotPath = label();
1341 
1342     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeIndex, PatchableJump(), badType, mode, profile, done, nextHotPath));
1343 }
1344 
1345 void JIT::emitSlow_op_has_indexed_property(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1346 {
1347     linkAllSlowCases(iter);
1348 
1349     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1350     VirtualRegister dst = bytecode.m_dst;
1351     VirtualRegister base = bytecode.m_base;
1352     VirtualRegister property = bytecode.m_property;
1353     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
1354 
1355     Label slowPath = label();
1356 
1357     emitGetVirtualRegister(base, regT0);
1358     emitGetVirtualRegister(property, regT1);
1359     Call call = callOperation(operationHasIndexedPropertyDefault, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, byValInfo);
1360 
1361     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
1362     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
1363     m_byValInstructionIndex++;
1364 }
1365 
1366 void JIT::emit_op_get_direct_pname(const Instruction* currentInstruction)
1367 {
1368     auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
1369     VirtualRegister dst = bytecode.m_dst;
1370     VirtualRegister base = bytecode.m_base;
1371     VirtualRegister index = bytecode.m_index;
1372     VirtualRegister enumerator = bytecode.m_enumerator;
1373 
1374     // Check that base is a cell
1375     emitGetVirtualRegister(base, regT0);
1376     emitJumpSlowCaseIfNotJSCell(regT0, base);
1377 
1378     // Check the structure
1379     emitGetVirtualRegister(enumerator, regT2);
1380     load32(Address(regT0, JSCell::structureIDOffset()), regT1);
1381     addSlowCase(branch32(NotEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1382 
1383     // Compute the offset
1384     emitGetVirtualRegister(index, regT1);
1385     // If index is less than the enumerator&#39;s cached inline storage, then it&#39;s an inline access
1386     Jump outOfLineAccess = branch32(AboveOrEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()));
1387     addPtr(TrustedImm32(JSObject::offsetOfInlineStorage()), regT0);
1388     signExtend32ToPtr(regT1, regT1);
1389     load64(BaseIndex(regT0, regT1, TimesEight), regT0);
1390 
1391     Jump done = jump();
1392 
1393     // Otherwise it&#39;s out of line
1394     outOfLineAccess.link(this);
1395     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
1396     sub32(Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()), regT1);
1397     neg32(regT1);
1398     signExtend32ToPtr(regT1, regT1);
1399     int32_t offsetOfFirstProperty = static_cast&lt;int32_t&gt;(offsetInButterfly(firstOutOfLineOffset)) * sizeof(EncodedJSValue);
1400     load64(BaseIndex(regT0, regT1, TimesEight, offsetOfFirstProperty), regT0);
1401 
1402     done.link(this);
1403     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1404     emitPutVirtualRegister(dst, regT0);
1405 }
1406 
1407 void JIT::emit_op_enumerator_structure_pname(const Instruction* currentInstruction)
1408 {
1409     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
1410     VirtualRegister dst = bytecode.m_dst;
1411     VirtualRegister enumerator = bytecode.m_enumerator;
1412     VirtualRegister index = bytecode.m_index;
1413 
1414     emitGetVirtualRegister(index, regT0);
1415     emitGetVirtualRegister(enumerator, regT1);
1416     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endStructurePropertyIndexOffset()));
1417 
1418     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1419 
1420     Jump done = jump();
1421     inBounds.link(this);
1422 
1423     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1424     signExtend32ToPtr(regT0, regT0);
1425     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1426 
1427     done.link(this);
1428     emitPutVirtualRegister(dst);
1429 }
1430 
1431 void JIT::emit_op_enumerator_generic_pname(const Instruction* currentInstruction)
1432 {
1433     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
1434     VirtualRegister dst = bytecode.m_dst;
1435     VirtualRegister enumerator = bytecode.m_enumerator;
1436     VirtualRegister index = bytecode.m_index;
1437 
1438     emitGetVirtualRegister(index, regT0);
1439     emitGetVirtualRegister(enumerator, regT1);
1440     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endGenericPropertyIndexOffset()));
1441 
1442     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1443 
1444     Jump done = jump();
1445     inBounds.link(this);
1446 
1447     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1448     signExtend32ToPtr(regT0, regT0);
1449     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1450 
1451     done.link(this);
1452     emitPutVirtualRegister(dst);
1453 }
1454 
1455 void JIT::emit_op_profile_type(const Instruction* currentInstruction)
1456 {
1457     auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
1458     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1459     TypeLocation* cachedTypeLocation = metadata.m_typeLocation;
1460     VirtualRegister valueToProfile = bytecode.m_targetVirtualRegister;
1461 
1462     emitGetVirtualRegister(valueToProfile, regT0);
1463 
1464     JumpList jumpToEnd;
1465 
1466     jumpToEnd.append(branchIfEmpty(regT0));
1467 
1468     // Compile in a predictive type check, if possible, to see if we can skip writing to the log.
1469     // These typechecks are inlined to match those of the 64-bit JSValue type checks.
1470     if (cachedTypeLocation-&gt;m_lastSeenType == TypeUndefined)
1471         jumpToEnd.append(branchIfUndefined(regT0));
1472     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNull)
1473         jumpToEnd.append(branchIfNull(regT0));
1474     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeBoolean)
1475         jumpToEnd.append(branchIfBoolean(regT0, regT1));
1476     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeAnyInt)
1477         jumpToEnd.append(branchIfInt32(regT0));
1478     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNumber)
1479         jumpToEnd.append(branchIfNumber(regT0));
1480     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeString) {
1481         Jump isNotCell = branchIfNotCell(regT0);
1482         jumpToEnd.append(branchIfString(regT0));
1483         isNotCell.link(this);
1484     }
1485 
1486     // Load the type profiling log into T2.
1487     TypeProfilerLog* cachedTypeProfilerLog = m_vm-&gt;typeProfilerLog();
1488     move(TrustedImmPtr(cachedTypeProfilerLog), regT2);
1489     // Load the next log entry into T1.
1490     loadPtr(Address(regT2, TypeProfilerLog::currentLogEntryOffset()), regT1);
1491 
1492     // Store the JSValue onto the log entry.
1493     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::valueOffset()));
1494 
1495     // Store the structureID of the cell if T0 is a cell, otherwise, store 0 on the log entry.
1496     Jump notCell = branchIfNotCell(regT0);
1497     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1498     store32(regT0, Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1499     Jump skipIsCell = jump();
1500     notCell.link(this);
1501     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1502     skipIsCell.link(this);
1503 
1504     // Store the typeLocation on the log entry.
1505     move(TrustedImmPtr(cachedTypeLocation), regT0);
1506     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1507 
1508     // Increment the current log entry.
1509     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1510     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1511     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1512     // Clear the log if we&#39;re at the end of the log.
1513     callOperation(operationProcessTypeProfilerLog, &amp;vm());
1514     skipClearLog.link(this);
1515 
1516     jumpToEnd.link(this);
1517 }
1518 
1519 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1520 {
1521     RELEASE_ASSERT(vm().shadowChicken());
1522     updateTopCallFrame();
1523     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1524     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1525     GPRReg shadowPacketReg = regT0;
1526     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1527     GPRReg scratch2Reg = regT2;
1528     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
1529     emitGetVirtualRegister(bytecode.m_scope, regT3);
1530     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1531 }
1532 
1533 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1534 {
1535     RELEASE_ASSERT(vm().shadowChicken());
1536     updateTopCallFrame();
1537     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1538     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1539     GPRReg shadowPacketReg = regT0;
1540     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1541     GPRReg scratch2Reg = regT2;
1542     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
1543     emitGetVirtualRegister(bytecode.m_thisValue, regT2);
1544     emitGetVirtualRegister(bytecode.m_scope, regT3);
1545     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeIndex));
1546 }
1547 
1548 #endif // USE(JSVALUE64)
1549 
1550 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1551 {
1552     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1553     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1554     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1555 #if USE(JSVALUE64)
1556     basicBlockLocation-&gt;emitExecuteCode(*this);
1557 #else
1558     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1559 #endif
1560 }
1561 
1562 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
1563 {
1564     auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
1565     VirtualRegister dst = bytecode.m_dst;
1566     load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT0);
1567     sub32(TrustedImm32(1), regT0);
1568     JSValueRegs result = JSValueRegs::withTwoAvailableRegs(regT0, regT1);
1569     boxInt32(regT0, result);
1570     emitPutVirtualRegister(dst, result);
1571 }
1572 
1573 void JIT::emit_op_get_rest_length(const Instruction* currentInstruction)
1574 {
1575     auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
1576     VirtualRegister dst = bytecode.m_dst;
1577     unsigned numParamsToSkip = bytecode.m_numParametersToSkip;
1578     load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT0);
1579     sub32(TrustedImm32(1), regT0);
1580     Jump zeroLength = branch32(LessThanOrEqual, regT0, Imm32(numParamsToSkip));
1581     sub32(Imm32(numParamsToSkip), regT0);
1582 #if USE(JSVALUE64)
1583     boxInt32(regT0, JSValueRegs(regT0));
1584 #endif
1585     Jump done = jump();
1586 
1587     zeroLength.link(this);
1588 #if USE(JSVALUE64)
1589     move(TrustedImm64(JSValue::encode(jsNumber(0))), regT0);
1590 #else
1591     move(TrustedImm32(0), regT0);
1592 #endif
1593 
1594     done.link(this);
1595 #if USE(JSVALUE64)
1596     emitPutVirtualRegister(dst, regT0);
1597 #else
1598     move(TrustedImm32(JSValue::Int32Tag), regT1);
1599     emitPutVirtualRegister(dst, JSValueRegs(regT1, regT0));
1600 #endif
1601 }
1602 
1603 void JIT::emit_op_get_argument(const Instruction* currentInstruction)
1604 {
1605     auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
1606     VirtualRegister dst = bytecode.m_dst;
1607     int index = bytecode.m_index;
1608 #if USE(JSVALUE64)
1609     JSValueRegs resultRegs(regT0);
1610 #else
1611     JSValueRegs resultRegs(regT1, regT0);
1612 #endif
1613 
1614     load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT2);
1615     Jump argumentOutOfBounds = branch32(LessThanOrEqual, regT2, TrustedImm32(index));
1616     loadValue(addressFor(VirtualRegister(CallFrameSlot::thisArgument + index)), resultRegs);
1617     Jump done = jump();
1618 
1619     argumentOutOfBounds.link(this);
1620     moveValue(jsUndefined(), resultRegs);
1621 
1622     done.link(this);
1623     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1624     emitPutVirtualRegister(dst, resultRegs);
1625 }
1626 
1627 } // namespace JSC
1628 
1629 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>