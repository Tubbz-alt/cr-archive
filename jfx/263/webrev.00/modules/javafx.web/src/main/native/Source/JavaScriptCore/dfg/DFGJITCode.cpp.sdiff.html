<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCode.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGInvalidationPointInjectionPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGJITCode.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 41     , osrEntryRetry(0)
 42     , abandonOSREntry(false)
 43 #endif // ENABLE(FTL_JIT)
 44 {
 45 }
 46 
 47 JITCode::~JITCode()
 48 {
 49 }
 50 
 51 CommonData* JITCode::dfgCommon()
 52 {
 53     return &amp;common;
 54 }
 55 
 56 JITCode* JITCode::dfg()
 57 {
 58     return this;
 59 }
 60 
<span class="line-modified"> 61 void JITCode::shrinkToFit()</span>
 62 {
 63     common.shrinkToFit();
 64     osrEntry.shrinkToFit();
 65     osrExit.shrinkToFit();
 66     speculationRecovery.shrinkToFit();
 67     minifiedDFG.prepareAndShrink();
 68     variableEventStream.shrinkToFit();
 69 }
 70 
 71 void JITCode::reconstruct(
 72     CodeBlock* codeBlock, CodeOrigin codeOrigin, unsigned streamIndex,
 73     Operands&lt;ValueRecovery&gt;&amp; result)
 74 {
 75     variableEventStream.reconstruct(
 76         codeBlock, codeOrigin, minifiedDFG, streamIndex, result);
 77 }
 78 
<span class="line-modified"> 79 void JITCode::reconstruct(</span>
<span class="line-removed"> 80     ExecState* exec, CodeBlock* codeBlock, CodeOrigin codeOrigin, unsigned streamIndex,</span>
<span class="line-removed"> 81     Operands&lt;Optional&lt;JSValue&gt;&gt;&amp; result)</span>
 82 {
 83     Operands&lt;ValueRecovery&gt; recoveries;
 84     reconstruct(codeBlock, codeOrigin, streamIndex, recoveries);
 85 
 86     result = Operands&lt;Optional&lt;JSValue&gt;&gt;(OperandsLike, recoveries);
 87     for (size_t i = result.size(); i--;)
<span class="line-modified"> 88         result[i] = recoveries[i].recover(exec);</span>
 89 }
 90 
 91 RegisterSet JITCode::liveRegistersToPreserveAtExceptionHandlingCallSite(CodeBlock* codeBlock, CallSiteIndex callSiteIndex)
 92 {
 93     for (OSRExit&amp; exit : osrExit) {
 94         if (exit.isExceptionHandler() &amp;&amp; exit.m_exceptionHandlerCallSiteIndex.bits() == callSiteIndex.bits()) {
 95             Operands&lt;ValueRecovery&gt; valueRecoveries;
 96             reconstruct(codeBlock, exit.m_codeOrigin, exit.m_streamIndex, valueRecoveries);
 97             RegisterSet liveAtOSRExit;
 98             for (size_t index = 0; index &lt; valueRecoveries.size(); ++index) {
 99                 const ValueRecovery&amp; recovery = valueRecoveries[index];
100                 if (recovery.isInRegisters()) {
101                     if (recovery.isInGPR())
102                         liveAtOSRExit.set(recovery.gpr());
103                     else if (recovery.isInFPR())
104                         liveAtOSRExit.set(recovery.fpr());
105 #if USE(JSVALUE32_64)
106                     else if (recovery.isInJSValueRegs()) {
107                         liveAtOSRExit.set(recovery.payloadGPR());
108                         liveAtOSRExit.set(recovery.tagGPR());
</pre>
<hr />
<pre>
113                 }
114             }
115 
116             return liveAtOSRExit;
117         }
118     }
119 
120     return { };
121 }
122 
123 #if ENABLE(FTL_JIT)
124 bool JITCode::checkIfOptimizationThresholdReached(CodeBlock* codeBlock)
125 {
126     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
127     return tierUpCounter.checkIfThresholdCrossedAndSet(codeBlock);
128 }
129 
130 void JITCode::optimizeNextInvocation(CodeBlock* codeBlock)
131 {
132     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">133     if (Options::verboseOSR())</span>
<span class="line-removed">134         dataLog(*codeBlock, &quot;: FTL-optimizing next invocation.\n&quot;);</span>
135     tierUpCounter.setNewThreshold(0, codeBlock);
136 }
137 
138 void JITCode::dontOptimizeAnytimeSoon(CodeBlock* codeBlock)
139 {
140     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">141     if (Options::verboseOSR())</span>
<span class="line-removed">142         dataLog(*codeBlock, &quot;: Not FTL-optimizing anytime soon.\n&quot;);</span>
143     tierUpCounter.deferIndefinitely();
144 }
145 
146 void JITCode::optimizeAfterWarmUp(CodeBlock* codeBlock)
147 {
148     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">149     if (Options::verboseOSR())</span>
<span class="line-removed">150         dataLog(*codeBlock, &quot;: FTL-optimizing after warm-up.\n&quot;);</span>
151     CodeBlock* baseline = codeBlock-&gt;baselineVersion();
152     tierUpCounter.setNewThreshold(
153         baseline-&gt;adjustedCounterValue(Options::thresholdForFTLOptimizeAfterWarmUp()),
154         baseline);
155 }
156 
157 void JITCode::optimizeSoon(CodeBlock* codeBlock)
158 {
159     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">160     if (Options::verboseOSR())</span>
<span class="line-removed">161         dataLog(*codeBlock, &quot;: FTL-optimizing soon.\n&quot;);</span>
162     CodeBlock* baseline = codeBlock-&gt;baselineVersion();
163     tierUpCounter.setNewThreshold(
164         baseline-&gt;adjustedCounterValue(Options::thresholdForFTLOptimizeSoon()),
165         codeBlock);
166 }
167 
168 void JITCode::forceOptimizationSlowPathConcurrently(CodeBlock* codeBlock)
169 {
170     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">171     if (Options::verboseOSR())</span>
<span class="line-removed">172         dataLog(*codeBlock, &quot;: Forcing slow path concurrently for FTL entry.\n&quot;);</span>
173     tierUpCounter.forceSlowPathConcurrently();
174 }
175 
176 void JITCode::setOptimizationThresholdBasedOnCompilationResult(
177     CodeBlock* codeBlock, CompilationResult result)
178 {
179     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
180     switch (result) {
181     case CompilationSuccessful:
182         optimizeNextInvocation(codeBlock);
183         codeBlock-&gt;baselineVersion()-&gt;m_hasBeenCompiledWithFTL = true;
184         return;
185     case CompilationFailed:
186         dontOptimizeAnytimeSoon(codeBlock);
187         codeBlock-&gt;baselineVersion()-&gt;m_didFailFTLCompilation = true;
188         return;
189     case CompilationDeferred:
190         optimizeAfterWarmUp(codeBlock);
191         return;
192     case CompilationInvalidated:
193         // This is weird - it will only happen in cases when the DFG code block (i.e.
194         // the code block that this JITCode belongs to) is also invalidated. So it
195         // doesn&#39;t really matter what we do. But, we do the right thing anyway. Note
196         // that us counting the reoptimization actually means that we might count it
197         // twice. But that&#39;s generally OK. It&#39;s better to overcount reoptimizations
198         // than it is to undercount them.
199         codeBlock-&gt;baselineVersion()-&gt;countReoptimization();
200         optimizeAfterWarmUp(codeBlock);
201         return;
202     }
203     RELEASE_ASSERT_NOT_REACHED();
204 }
205 
206 void JITCode::setOSREntryBlock(VM&amp; vm, const JSCell* owner, CodeBlock* osrEntryBlock)
207 {
208     if (Options::verboseOSR()) {
<span class="line-modified">209         dataLog(RawPointer(this), &quot;: Setting OSR entry block to &quot;, RawPointer(osrEntryBlock), &quot;\n&quot;);</span>
<span class="line-modified">210         dataLog(&quot;OSR entries will go to &quot;, osrEntryBlock-&gt;jitCode()-&gt;ftlForOSREntry()-&gt;addressForCall(ArityCheckNotRequired), &quot;\n&quot;);</span>
211     }
212     m_osrEntryBlock.set(vm, owner, osrEntryBlock);
213 }
214 
215 void JITCode::clearOSREntryBlockAndResetThresholds(CodeBlock *dfgCodeBlock)
216 {
217     ASSERT(m_osrEntryBlock);
218 
<span class="line-modified">219     unsigned osrEntryBytecode = m_osrEntryBlock-&gt;jitCode()-&gt;ftlForOSREntry()-&gt;bytecodeIndex();</span>
220     m_osrEntryBlock.clear();
221     osrEntryRetry = 0;
222     tierUpEntryTriggers.set(osrEntryBytecode, JITCode::TriggerReason::DontTrigger);
223     setOptimizationThresholdBasedOnCompilationResult(dfgCodeBlock, CompilationDeferred);
224 }
225 #endif // ENABLE(FTL_JIT)
226 
227 void JITCode::validateReferences(const TrackedReferences&amp; trackedReferences)
228 {
229     common.validateReferences(trackedReferences);
230 
231     for (OSREntryData&amp; entry : osrEntry) {
232         for (unsigned i = entry.m_expectedValues.size(); i--;)
233             entry.m_expectedValues[i].validateReferences(trackedReferences);
234     }
235 
236     minifiedDFG.validateReferences(trackedReferences);
237 }
238 
239 Optional&lt;CodeOrigin&gt; JITCode::findPC(CodeBlock*, void* pc)
240 {
241     for (OSRExit&amp; exit : osrExit) {
242         if (ExecutableMemoryHandle* handle = exit.m_code.executableMemory()) {
243             if (handle-&gt;start().untaggedPtr() &lt;= pc &amp;&amp; pc &lt; handle-&gt;end().untaggedPtr())
244                 return Optional&lt;CodeOrigin&gt;(exit.m_codeOriginForExitProfile);
245         }
246     }
247 
248     return WTF::nullopt;
249 }
250 
251 void JITCode::finalizeOSREntrypoints()
252 {
253     auto comparator = [] (const auto&amp; a, const auto&amp; b) {
254         return a.m_bytecodeIndex &lt; b.m_bytecodeIndex;
255     };
256     std::sort(osrEntry.begin(), osrEntry.end(), comparator);
257 
<span class="line-modified">258 #if !ASSERT_DISABLED</span>
259     auto verifyIsSorted = [&amp;] (auto&amp; osrVector) {
260         for (unsigned i = 0; i + 1 &lt; osrVector.size(); ++i)
261             ASSERT(osrVector[i].m_bytecodeIndex &lt;= osrVector[i + 1].m_bytecodeIndex);
262     };
263     verifyIsSorted(osrEntry);
264 #endif
265 }
266 
267 } } // namespace JSC::DFG
268 
269 #endif // ENABLE(DFG_JIT)
</pre>
</td>
<td>
<hr />
<pre>
 41     , osrEntryRetry(0)
 42     , abandonOSREntry(false)
 43 #endif // ENABLE(FTL_JIT)
 44 {
 45 }
 46 
 47 JITCode::~JITCode()
 48 {
 49 }
 50 
 51 CommonData* JITCode::dfgCommon()
 52 {
 53     return &amp;common;
 54 }
 55 
 56 JITCode* JITCode::dfg()
 57 {
 58     return this;
 59 }
 60 
<span class="line-modified"> 61 void JITCode::shrinkToFit(const ConcurrentJSLocker&amp;)</span>
 62 {
 63     common.shrinkToFit();
 64     osrEntry.shrinkToFit();
 65     osrExit.shrinkToFit();
 66     speculationRecovery.shrinkToFit();
 67     minifiedDFG.prepareAndShrink();
 68     variableEventStream.shrinkToFit();
 69 }
 70 
 71 void JITCode::reconstruct(
 72     CodeBlock* codeBlock, CodeOrigin codeOrigin, unsigned streamIndex,
 73     Operands&lt;ValueRecovery&gt;&amp; result)
 74 {
 75     variableEventStream.reconstruct(
 76         codeBlock, codeOrigin, minifiedDFG, streamIndex, result);
 77 }
 78 
<span class="line-modified"> 79 void JITCode::reconstruct(CallFrame* callFrame, CodeBlock* codeBlock, CodeOrigin codeOrigin, unsigned streamIndex, Operands&lt;Optional&lt;JSValue&gt;&gt;&amp; result)</span>


 80 {
 81     Operands&lt;ValueRecovery&gt; recoveries;
 82     reconstruct(codeBlock, codeOrigin, streamIndex, recoveries);
 83 
 84     result = Operands&lt;Optional&lt;JSValue&gt;&gt;(OperandsLike, recoveries);
 85     for (size_t i = result.size(); i--;)
<span class="line-modified"> 86         result[i] = recoveries[i].recover(callFrame);</span>
 87 }
 88 
 89 RegisterSet JITCode::liveRegistersToPreserveAtExceptionHandlingCallSite(CodeBlock* codeBlock, CallSiteIndex callSiteIndex)
 90 {
 91     for (OSRExit&amp; exit : osrExit) {
 92         if (exit.isExceptionHandler() &amp;&amp; exit.m_exceptionHandlerCallSiteIndex.bits() == callSiteIndex.bits()) {
 93             Operands&lt;ValueRecovery&gt; valueRecoveries;
 94             reconstruct(codeBlock, exit.m_codeOrigin, exit.m_streamIndex, valueRecoveries);
 95             RegisterSet liveAtOSRExit;
 96             for (size_t index = 0; index &lt; valueRecoveries.size(); ++index) {
 97                 const ValueRecovery&amp; recovery = valueRecoveries[index];
 98                 if (recovery.isInRegisters()) {
 99                     if (recovery.isInGPR())
100                         liveAtOSRExit.set(recovery.gpr());
101                     else if (recovery.isInFPR())
102                         liveAtOSRExit.set(recovery.fpr());
103 #if USE(JSVALUE32_64)
104                     else if (recovery.isInJSValueRegs()) {
105                         liveAtOSRExit.set(recovery.payloadGPR());
106                         liveAtOSRExit.set(recovery.tagGPR());
</pre>
<hr />
<pre>
111                 }
112             }
113 
114             return liveAtOSRExit;
115         }
116     }
117 
118     return { };
119 }
120 
121 #if ENABLE(FTL_JIT)
122 bool JITCode::checkIfOptimizationThresholdReached(CodeBlock* codeBlock)
123 {
124     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
125     return tierUpCounter.checkIfThresholdCrossedAndSet(codeBlock);
126 }
127 
128 void JITCode::optimizeNextInvocation(CodeBlock* codeBlock)
129 {
130     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">131     dataLogLnIf(Options::verboseOSR(), *codeBlock, &quot;: FTL-optimizing next invocation.&quot;);</span>

132     tierUpCounter.setNewThreshold(0, codeBlock);
133 }
134 
135 void JITCode::dontOptimizeAnytimeSoon(CodeBlock* codeBlock)
136 {
137     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">138     dataLogLnIf(Options::verboseOSR(), *codeBlock, &quot;: Not FTL-optimizing anytime soon.&quot;);</span>

139     tierUpCounter.deferIndefinitely();
140 }
141 
142 void JITCode::optimizeAfterWarmUp(CodeBlock* codeBlock)
143 {
144     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">145     dataLogLnIf(Options::verboseOSR(), *codeBlock, &quot;: FTL-optimizing after warm-up.&quot;);</span>

146     CodeBlock* baseline = codeBlock-&gt;baselineVersion();
147     tierUpCounter.setNewThreshold(
148         baseline-&gt;adjustedCounterValue(Options::thresholdForFTLOptimizeAfterWarmUp()),
149         baseline);
150 }
151 
152 void JITCode::optimizeSoon(CodeBlock* codeBlock)
153 {
154     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">155     dataLogLnIf(Options::verboseOSR(), *codeBlock, &quot;: FTL-optimizing soon.&quot;);</span>

156     CodeBlock* baseline = codeBlock-&gt;baselineVersion();
157     tierUpCounter.setNewThreshold(
158         baseline-&gt;adjustedCounterValue(Options::thresholdForFTLOptimizeSoon()),
159         codeBlock);
160 }
161 
162 void JITCode::forceOptimizationSlowPathConcurrently(CodeBlock* codeBlock)
163 {
164     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
<span class="line-modified">165     dataLogLnIf(Options::verboseOSR(), *codeBlock, &quot;: Forcing slow path concurrently for FTL entry.&quot;);</span>

166     tierUpCounter.forceSlowPathConcurrently();
167 }
168 
169 void JITCode::setOptimizationThresholdBasedOnCompilationResult(
170     CodeBlock* codeBlock, CompilationResult result)
171 {
172     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
173     switch (result) {
174     case CompilationSuccessful:
175         optimizeNextInvocation(codeBlock);
176         codeBlock-&gt;baselineVersion()-&gt;m_hasBeenCompiledWithFTL = true;
177         return;
178     case CompilationFailed:
179         dontOptimizeAnytimeSoon(codeBlock);
180         codeBlock-&gt;baselineVersion()-&gt;m_didFailFTLCompilation = true;
181         return;
182     case CompilationDeferred:
183         optimizeAfterWarmUp(codeBlock);
184         return;
185     case CompilationInvalidated:
186         // This is weird - it will only happen in cases when the DFG code block (i.e.
187         // the code block that this JITCode belongs to) is also invalidated. So it
188         // doesn&#39;t really matter what we do. But, we do the right thing anyway. Note
189         // that us counting the reoptimization actually means that we might count it
190         // twice. But that&#39;s generally OK. It&#39;s better to overcount reoptimizations
191         // than it is to undercount them.
192         codeBlock-&gt;baselineVersion()-&gt;countReoptimization();
193         optimizeAfterWarmUp(codeBlock);
194         return;
195     }
196     RELEASE_ASSERT_NOT_REACHED();
197 }
198 
199 void JITCode::setOSREntryBlock(VM&amp; vm, const JSCell* owner, CodeBlock* osrEntryBlock)
200 {
201     if (Options::verboseOSR()) {
<span class="line-modified">202         dataLogLn(RawPointer(this), &quot;: Setting OSR entry block to &quot;, RawPointer(osrEntryBlock));</span>
<span class="line-modified">203         dataLogLn(&quot;OSR entries will go to &quot;, osrEntryBlock-&gt;jitCode()-&gt;ftlForOSREntry()-&gt;addressForCall(ArityCheckNotRequired));</span>
204     }
205     m_osrEntryBlock.set(vm, owner, osrEntryBlock);
206 }
207 
208 void JITCode::clearOSREntryBlockAndResetThresholds(CodeBlock *dfgCodeBlock)
209 {
210     ASSERT(m_osrEntryBlock);
211 
<span class="line-modified">212     BytecodeIndex osrEntryBytecode = m_osrEntryBlock-&gt;jitCode()-&gt;ftlForOSREntry()-&gt;bytecodeIndex();</span>
213     m_osrEntryBlock.clear();
214     osrEntryRetry = 0;
215     tierUpEntryTriggers.set(osrEntryBytecode, JITCode::TriggerReason::DontTrigger);
216     setOptimizationThresholdBasedOnCompilationResult(dfgCodeBlock, CompilationDeferred);
217 }
218 #endif // ENABLE(FTL_JIT)
219 
220 void JITCode::validateReferences(const TrackedReferences&amp; trackedReferences)
221 {
222     common.validateReferences(trackedReferences);
223 
224     for (OSREntryData&amp; entry : osrEntry) {
225         for (unsigned i = entry.m_expectedValues.size(); i--;)
226             entry.m_expectedValues[i].validateReferences(trackedReferences);
227     }
228 
229     minifiedDFG.validateReferences(trackedReferences);
230 }
231 
232 Optional&lt;CodeOrigin&gt; JITCode::findPC(CodeBlock*, void* pc)
233 {
234     for (OSRExit&amp; exit : osrExit) {
235         if (ExecutableMemoryHandle* handle = exit.m_code.executableMemory()) {
236             if (handle-&gt;start().untaggedPtr() &lt;= pc &amp;&amp; pc &lt; handle-&gt;end().untaggedPtr())
237                 return Optional&lt;CodeOrigin&gt;(exit.m_codeOriginForExitProfile);
238         }
239     }
240 
241     return WTF::nullopt;
242 }
243 
244 void JITCode::finalizeOSREntrypoints()
245 {
246     auto comparator = [] (const auto&amp; a, const auto&amp; b) {
247         return a.m_bytecodeIndex &lt; b.m_bytecodeIndex;
248     };
249     std::sort(osrEntry.begin(), osrEntry.end(), comparator);
250 
<span class="line-modified">251 #if ASSERT_ENABLED</span>
252     auto verifyIsSorted = [&amp;] (auto&amp; osrVector) {
253         for (unsigned i = 0; i + 1 &lt; osrVector.size(); ++i)
254             ASSERT(osrVector[i].m_bytecodeIndex &lt;= osrVector[i + 1].m_bytecodeIndex);
255     };
256     verifyIsSorted(osrEntry);
257 #endif
258 }
259 
260 } } // namespace JSC::DFG
261 
262 #endif // ENABLE(DFG_JIT)
</pre>
</td>
</tr>
</table>
<center><a href="DFGInvalidationPointInjectionPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGJITCode.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>