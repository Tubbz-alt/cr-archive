diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2011-2018 Apple Inc. All rights reserved.
+ * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -46,11 +46,11 @@
 // optimization opportunities by virtue of being global.
 
 namespace {
 
 namespace DFGCSEPhaseInternal {
-static const bool verbose = false;
+static constexpr bool verbose = false;
 }
 
 class ImpureDataSlot {
     WTF_MAKE_NONCOPYABLE(ImpureDataSlot);
     WTF_MAKE_FAST_ALLOCATED;
@@ -145,12 +145,11 @@
         }
         case SideState:
             break;
         case Stack: {
             ASSERT(!heap.payload().isTop());
-            ASSERT(heap.payload().value() == heap.payload().value32());
-            m_abstractHeapStackMap.remove(heap.payload().value32());
+            m_abstractHeapStackMap.remove(heap.payload().value());
             if (clobberConservatively)
                 m_fallbackStackMap.clear();
             else
                 clobber(m_fallbackStackMap, heap);
             break;
@@ -170,11 +169,11 @@
                 break;
             case Stack: {
                 if (!clobberConservatively)
                     break;
                 if (pair.key.heap().kind() == Stack) {
-                    auto iterator = m_abstractHeapStackMap.find(pair.key.heap().payload().value32());
+                    auto iterator = m_abstractHeapStackMap.find(pair.key.heap().payload().value());
                     if (iterator != m_abstractHeapStackMap.end() && iterator->value->key == pair.key)
                         return false;
                     return true;
                 }
                 break;
@@ -224,12 +223,11 @@
             RELEASE_ASSERT_NOT_REACHED();
         case Stack: {
             AbstractHeap abstractHeap = location.heap();
             if (abstractHeap.payload().isTop())
                 return add(m_fallbackStackMap, location, node);
-            ASSERT(abstractHeap.payload().value() == abstractHeap.payload().value32());
-            auto addResult = m_abstractHeapStackMap.add(abstractHeap.payload().value32(), nullptr);
+            auto addResult = m_abstractHeapStackMap.add(abstractHeap.payload().value(), nullptr);
             if (addResult.isNewEntry) {
                 addResult.iterator->value.reset(new ImpureDataSlot {location, node, 0});
                 return nullptr;
             }
             if (addResult.iterator->value->key == location)
@@ -247,12 +245,11 @@
         switch (location.heap().kind()) {
         case World:
         case SideState:
             RELEASE_ASSERT_NOT_REACHED();
         case Stack: {
-            ASSERT(location.heap().payload().value() == location.heap().payload().value32());
-            auto iterator = m_abstractHeapStackMap.find(location.heap().payload().value32());
+            auto iterator = m_abstractHeapStackMap.find(location.heap().payload().value());
             if (iterator != m_abstractHeapStackMap.end()
                 && iterator->value->key == location)
                 return iterator->value->value;
             return get(m_fallbackStackMap, location);
         }
@@ -296,11 +293,11 @@
     //
     // One cannot assume a unique ImpureData is in m_abstractHeapStackMap. It may have been
     // a duplicate in the past and now only live in m_fallbackStackMap.
     //
     // Obviously, TOP always goes into m_fallbackStackMap since it does not have a unique value.
-    HashMap<int32_t, std::unique_ptr<ImpureDataSlot>, DefaultHash<int32_t>::Hash, WTF::SignedWithZeroKeyHashTraits<int32_t>> m_abstractHeapStackMap;
+    HashMap<int64_t, std::unique_ptr<ImpureDataSlot>, DefaultHash<int64_t>::Hash, WTF::SignedWithZeroKeyHashTraits<int64_t>> m_abstractHeapStackMap;
     Map m_fallbackStackMap;
 
     Map m_heapMap;
 
 #if !defined(NDEBUG)
@@ -351,11 +348,11 @@
         // Thus, a capacity limit of 100 probably means that somewhere around ~40 things may end up
         // in one of these "small" list-based maps. That number still seems largeish, except that
         // the overhead of HashMaps can be quite high currently: clearing them, or even removing
         // enough things from them, deletes (or resizes) their backing store eagerly. Hence
         // HashMaps induce a lot of malloc traffic.
-        static const unsigned capacity = 100;
+        static constexpr unsigned capacity = 100;
 
         SmallMaps()
             : m_pureLength(0)
             , m_impureLength(0)
         {
@@ -383,11 +380,11 @@
             for (unsigned i = m_pureLength; i--;) {
                 if (m_pureMap[i].key == value)
                     return m_pureMap[i].value;
             }
 
-            ASSERT(m_pureLength < capacity);
+            RELEASE_ASSERT(m_pureLength < capacity);
             m_pureMap[m_pureLength++] = WTF::KeyValuePair<PureValue, Node*>(value, node);
             return nullptr;
         }
 
         LazyNode findReplacement(HeapLocation location)
@@ -405,11 +402,11 @@
             // For now the only derived values we def() are constant-based.
             if (location.index() && !location.index().isNode())
                 return nullptr;
             if (LazyNode result = findReplacement(location))
                 return result;
-            ASSERT(m_impureLength < capacity);
+            RELEASE_ASSERT(m_impureLength < capacity);
             m_impureMap[m_impureLength++] = WTF::KeyValuePair<HeapLocation, LazyNode>(location, node);
             return nullptr;
         }
 
     private:
