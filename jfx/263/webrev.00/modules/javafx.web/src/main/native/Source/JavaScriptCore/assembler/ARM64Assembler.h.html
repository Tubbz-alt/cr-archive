<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/ARM64Assembler.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2012-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(ASSEMBLER) &amp;&amp; CPU(ARM64)
  29 
  30 #include &quot;ARM64Registers.h&quot;
  31 #include &quot;AssemblerBuffer.h&quot;
  32 #include &quot;AssemblerCommon.h&quot;
  33 #include &quot;CPU.h&quot;
  34 #include &quot;JSCPtrTag.h&quot;
  35 #include &lt;limits.h&gt;
  36 #include &lt;wtf/Assertions.h&gt;
  37 #include &lt;wtf/Vector.h&gt;
  38 #include &lt;stdint.h&gt;
  39 
  40 #if OS(FUCHSIA)
  41 #include &lt;zircon/syscalls.h&gt;
  42 #endif
  43 
  44 #define CHECK_DATASIZE_OF(datasize) ASSERT(datasize == 32 || datasize == 64)
  45 #define CHECK_MEMOPSIZE_OF(size) ASSERT(size == 8 || size == 16 || size == 32 || size == 64 || size == 128);
  46 #define DATASIZE_OF(datasize) ((datasize == 64) ? Datasize_64 : Datasize_32)
  47 #define MEMOPSIZE_OF(datasize) ((datasize == 8 || datasize == 128) ? MemOpSize_8_or_128 : (datasize == 16) ? MemOpSize_16 : (datasize == 32) ? MemOpSize_32 : MemOpSize_64)
  48 #define CHECK_DATASIZE() CHECK_DATASIZE_OF(datasize)
  49 #define CHECK_MEMOPSIZE() CHECK_MEMOPSIZE_OF(datasize)
  50 #define CHECK_VECTOR_DATASIZE() ASSERT(datasize == 64 || datasize == 128)
  51 #define DATASIZE DATASIZE_OF(datasize)
  52 #define MEMOPSIZE MEMOPSIZE_OF(datasize)
  53 #define CHECK_FP_MEMOP_DATASIZE() ASSERT(datasize == 8 || datasize == 16 || datasize == 32 || datasize == 64 || datasize == 128)
  54 #define MEMPAIROPSIZE_INT(datasize) ((datasize == 64) ? MemPairOp_64 : MemPairOp_32)
  55 #define MEMPAIROPSIZE_FP(datasize) ((datasize == 128) ? MemPairOp_V128 : (datasize == 64) ? MemPairOp_V64 : MemPairOp_32)
  56 
  57 namespace JSC {
  58 
  59 template&lt;size_t bits, typename Type&gt;
  60 ALWAYS_INLINE constexpr bool isInt(Type t)
  61 {
  62     constexpr size_t shift = sizeof(Type) * CHAR_BIT - bits;
  63     static_assert(sizeof(Type) * CHAR_BIT &gt; shift, &quot;shift is larger than the size of the value&quot;);
  64     return ((t &lt;&lt; shift) &gt;&gt; shift) == t;
  65 }
  66 
  67 static ALWAYS_INLINE bool is4ByteAligned(const void* ptr)
  68 {
  69     return !(reinterpret_cast&lt;intptr_t&gt;(ptr) &amp; 0x3);
  70 }
  71 
  72 ALWAYS_INLINE bool isUInt5(int32_t value)
  73 {
  74     return !(value &amp; ~0x1f);
  75 }
  76 
  77 class UInt5 {
  78 public:
  79     explicit UInt5(int value)
  80         : m_value(value)
  81     {
  82         ASSERT(isUInt5(value));
  83     }
  84 
  85     operator int() { return m_value; }
  86 
  87 private:
  88     int m_value;
  89 };
  90 
  91 class UInt12 {
  92 public:
  93     explicit UInt12(int value)
  94         : m_value(value)
  95     {
  96         ASSERT(isUInt12(value));
  97     }
  98 
  99     operator int() { return m_value; }
 100 
 101 private:
 102     int m_value;
 103 };
 104 
 105 class PostIndex {
 106 public:
 107     explicit PostIndex(int value)
 108         : m_value(value)
 109     {
 110         ASSERT(isInt9(value));
 111     }
 112 
 113     operator int() { return m_value; }
 114 
 115 private:
 116     int m_value;
 117 };
 118 
 119 class PreIndex {
 120 public:
 121     explicit PreIndex(int value)
 122         : m_value(value)
 123     {
 124         ASSERT(isInt9(value));
 125     }
 126 
 127     operator int() { return m_value; }
 128 
 129 private:
 130     int m_value;
 131 };
 132 
 133 class PairPostIndex {
 134 public:
 135     explicit PairPostIndex(int value)
 136         : m_value(value)
 137     {
 138         ASSERT(isInt&lt;11&gt;(value));
 139     }
 140 
 141     operator int() { return m_value; }
 142 
 143 private:
 144     int m_value;
 145 };
 146 
 147 class PairPreIndex {
 148 public:
 149     explicit PairPreIndex(int value)
 150         : m_value(value)
 151     {
 152         ASSERT(isInt&lt;11&gt;(value));
 153     }
 154 
 155     operator int() { return m_value; }
 156 
 157 private:
 158     int m_value;
 159 };
 160 
 161 typedef ARM64LogicalImmediate LogicalImmediate;
 162 
 163 inline uint16_t getHalfword(uint64_t value, int which)
 164 {
 165     return value &gt;&gt; (which &lt;&lt; 4);
 166 }
 167 
 168 namespace RegisterNames {
 169 
 170 typedef enum : int8_t {
 171 #define REGISTER_ID(id, name, r, cs) id,
 172     FOR_EACH_GP_REGISTER(REGISTER_ID)
 173 #undef REGISTER_ID
 174 
 175 #define REGISTER_ALIAS(id, name, alias) id = alias,
 176     FOR_EACH_REGISTER_ALIAS(REGISTER_ALIAS)
 177 #undef REGISTER_ALIAS
 178 
 179     InvalidGPRReg = -1,
 180 } RegisterID;
 181 
 182 typedef enum : int8_t {
 183 #define REGISTER_ID(id, name) id,
 184     FOR_EACH_SP_REGISTER(REGISTER_ID)
 185 #undef REGISTER_ID
 186 } SPRegisterID;
 187 
 188 // ARM64 always has 32 FPU registers 128-bits each. See http://llvm.org/devmtg/2012-11/Northover-AArch64.pdf
 189 // and Section 5.1.2 in http://infocenter.arm.com/help/topic/com.arm.doc.ihi0055b/IHI0055B_aapcs64.pdf.
 190 // However, we only use them for 64-bit doubles.
 191 typedef enum : int8_t {
 192 #define REGISTER_ID(id, name, r, cs) id,
 193     FOR_EACH_FP_REGISTER(REGISTER_ID)
 194 #undef REGISTER_ID
 195     InvalidFPRReg = -1,
 196 } FPRegisterID;
 197 
 198 static constexpr bool isSp(RegisterID reg) { return reg == sp; }
 199 static constexpr bool isZr(RegisterID reg) { return reg == zr; }
 200 
 201 } // namespace ARM64Registers
 202 
 203 class ARM64Assembler {
 204 public:
 205     static constexpr size_t instructionSize = sizeof(unsigned);
 206 
 207     typedef ARM64Registers::RegisterID RegisterID;
 208     typedef ARM64Registers::SPRegisterID SPRegisterID;
 209     typedef ARM64Registers::FPRegisterID FPRegisterID;
 210 
 211     static constexpr RegisterID firstRegister() { return ARM64Registers::x0; }
 212     static constexpr RegisterID lastRegister() { return ARM64Registers::sp; }
 213     static constexpr unsigned numberOfRegisters() { return lastRegister() - firstRegister() + 1; }
 214 
 215     static constexpr SPRegisterID firstSPRegister() { return ARM64Registers::pc; }
 216     static constexpr SPRegisterID lastSPRegister() { return ARM64Registers::fpsr; }
 217     static constexpr unsigned numberOfSPRegisters() { return lastSPRegister() - firstSPRegister() + 1; }
 218 
 219     static constexpr FPRegisterID firstFPRegister() { return ARM64Registers::q0; }
 220     static constexpr FPRegisterID lastFPRegister() { return ARM64Registers::q31; }
 221     static constexpr unsigned numberOfFPRegisters() { return lastFPRegister() - firstFPRegister() + 1; }
 222 
 223     static const char* gprName(RegisterID id)
 224     {
 225         ASSERT(id &gt;= firstRegister() &amp;&amp; id &lt;= lastRegister());
 226         static const char* const nameForRegister[numberOfRegisters()] = {
 227 #define REGISTER_NAME(id, name, r, cs) name,
 228         FOR_EACH_GP_REGISTER(REGISTER_NAME)
 229 #undef REGISTER_NAME
 230         };
 231         return nameForRegister[id];
 232     }
 233 
 234     static const char* sprName(SPRegisterID id)
 235     {
 236         ASSERT(id &gt;= firstSPRegister() &amp;&amp; id &lt;= lastSPRegister());
 237         static const char* const nameForRegister[numberOfSPRegisters()] = {
 238 #define REGISTER_NAME(id, name) name,
 239         FOR_EACH_SP_REGISTER(REGISTER_NAME)
 240 #undef REGISTER_NAME
 241         };
 242         return nameForRegister[id];
 243     }
 244 
 245     static const char* fprName(FPRegisterID id)
 246     {
 247         ASSERT(id &gt;= firstFPRegister() &amp;&amp; id &lt;= lastFPRegister());
 248         static const char* const nameForRegister[numberOfFPRegisters()] = {
 249 #define REGISTER_NAME(id, name, r, cs) name,
 250         FOR_EACH_FP_REGISTER(REGISTER_NAME)
 251 #undef REGISTER_NAME
 252         };
 253         return nameForRegister[id];
 254     }
 255 
 256 protected:
 257     static constexpr bool isSp(RegisterID reg) { return ARM64Registers::isSp(reg); }
 258     static constexpr bool isZr(RegisterID reg) { return ARM64Registers::isZr(reg); }
 259 
 260 public:
 261     ARM64Assembler()
 262         : m_indexOfLastWatchpoint(INT_MIN)
 263         , m_indexOfTailOfLastWatchpoint(INT_MIN)
 264     {
 265     }
 266 
 267     AssemblerBuffer&amp; buffer() { return m_buffer; }
 268 
 269     // (HS, LO, HI, LS) -&gt; (AE, B, A, BE)
 270     // (VS, VC) -&gt; (O, NO)
 271     typedef enum {
 272         ConditionEQ,
 273         ConditionNE,
 274         ConditionHS, ConditionCS = ConditionHS,
 275         ConditionLO, ConditionCC = ConditionLO,
 276         ConditionMI,
 277         ConditionPL,
 278         ConditionVS,
 279         ConditionVC,
 280         ConditionHI,
 281         ConditionLS,
 282         ConditionGE,
 283         ConditionLT,
 284         ConditionGT,
 285         ConditionLE,
 286         ConditionAL,
 287         ConditionInvalid
 288     } Condition;
 289 
 290     static Condition invert(Condition cond)
 291     {
 292         return static_cast&lt;Condition&gt;(cond ^ 1);
 293     }
 294 
 295     typedef enum {
 296         LSL,
 297         LSR,
 298         ASR,
 299         ROR
 300     } ShiftType;
 301 
 302     typedef enum {
 303         UXTB,
 304         UXTH,
 305         UXTW,
 306         UXTX,
 307         SXTB,
 308         SXTH,
 309         SXTW,
 310         SXTX
 311     } ExtendType;
 312 
 313     enum SetFlags {
 314         DontSetFlags,
 315         S
 316     };
 317 
 318 #define JUMP_ENUM_WITH_SIZE(index, value) (((value) &lt;&lt; 4) | (index))
 319 #define JUMP_ENUM_SIZE(jump) ((jump) &gt;&gt; 4)
 320     enum JumpType { JumpFixed = JUMP_ENUM_WITH_SIZE(0, 0),
 321         JumpNoCondition = JUMP_ENUM_WITH_SIZE(1, 1 * sizeof(uint32_t)),
 322         JumpCondition = JUMP_ENUM_WITH_SIZE(2, 2 * sizeof(uint32_t)),
 323         JumpCompareAndBranch = JUMP_ENUM_WITH_SIZE(3, 2 * sizeof(uint32_t)),
 324         JumpTestBit = JUMP_ENUM_WITH_SIZE(4, 2 * sizeof(uint32_t)),
 325         JumpNoConditionFixedSize = JUMP_ENUM_WITH_SIZE(5, 1 * sizeof(uint32_t)),
 326         JumpConditionFixedSize = JUMP_ENUM_WITH_SIZE(6, 2 * sizeof(uint32_t)),
 327         JumpCompareAndBranchFixedSize = JUMP_ENUM_WITH_SIZE(7, 2 * sizeof(uint32_t)),
 328         JumpTestBitFixedSize = JUMP_ENUM_WITH_SIZE(8, 2 * sizeof(uint32_t)),
 329     };
 330     enum JumpLinkType {
 331         LinkInvalid = JUMP_ENUM_WITH_SIZE(0, 0),
 332         LinkJumpNoCondition = JUMP_ENUM_WITH_SIZE(1, 1 * sizeof(uint32_t)),
 333         LinkJumpConditionDirect = JUMP_ENUM_WITH_SIZE(2, 1 * sizeof(uint32_t)),
 334         LinkJumpCondition = JUMP_ENUM_WITH_SIZE(3, 2 * sizeof(uint32_t)),
 335         LinkJumpCompareAndBranch = JUMP_ENUM_WITH_SIZE(4, 2 * sizeof(uint32_t)),
 336         LinkJumpCompareAndBranchDirect = JUMP_ENUM_WITH_SIZE(5, 1 * sizeof(uint32_t)),
 337         LinkJumpTestBit = JUMP_ENUM_WITH_SIZE(6, 2 * sizeof(uint32_t)),
 338         LinkJumpTestBitDirect = JUMP_ENUM_WITH_SIZE(7, 1 * sizeof(uint32_t)),
 339     };
 340 
 341     class LinkRecord {
 342     public:
 343         LinkRecord(intptr_t from, intptr_t to, JumpType type, Condition condition)
 344         {
 345             data.realTypes.m_from = from;
 346             data.realTypes.m_to = to;
 347             data.realTypes.m_type = type;
 348             data.realTypes.m_linkType = LinkInvalid;
 349             data.realTypes.m_condition = condition;
 350         }
 351         LinkRecord(intptr_t from, intptr_t to, JumpType type, Condition condition, bool is64Bit, RegisterID compareRegister)
 352         {
 353             data.realTypes.m_from = from;
 354             data.realTypes.m_to = to;
 355             data.realTypes.m_type = type;
 356             data.realTypes.m_linkType = LinkInvalid;
 357             data.realTypes.m_condition = condition;
 358             data.realTypes.m_is64Bit = is64Bit;
 359             data.realTypes.m_compareRegister = compareRegister;
 360         }
 361         LinkRecord(intptr_t from, intptr_t to, JumpType type, Condition condition, unsigned bitNumber, RegisterID compareRegister)
 362         {
 363             data.realTypes.m_from = from;
 364             data.realTypes.m_to = to;
 365             data.realTypes.m_type = type;
 366             data.realTypes.m_linkType = LinkInvalid;
 367             data.realTypes.m_condition = condition;
 368             data.realTypes.m_bitNumber = bitNumber;
 369             data.realTypes.m_compareRegister = compareRegister;
 370         }
 371         void operator=(const LinkRecord&amp; other)
 372         {
 373             data.copyTypes.content[0] = other.data.copyTypes.content[0];
 374             data.copyTypes.content[1] = other.data.copyTypes.content[1];
 375             data.copyTypes.content[2] = other.data.copyTypes.content[2];
 376         }
 377         intptr_t from() const { return data.realTypes.m_from; }
 378         void setFrom(intptr_t from) { data.realTypes.m_from = from; }
 379         intptr_t to() const { return data.realTypes.m_to; }
 380         JumpType type() const { return data.realTypes.m_type; }
 381         JumpLinkType linkType() const { return data.realTypes.m_linkType; }
 382         void setLinkType(JumpLinkType linkType) { ASSERT(data.realTypes.m_linkType == LinkInvalid); data.realTypes.m_linkType = linkType; }
 383         Condition condition() const { return data.realTypes.m_condition; }
 384         bool is64Bit() const { return data.realTypes.m_is64Bit; }
 385         unsigned bitNumber() const { return data.realTypes.m_bitNumber; }
 386         RegisterID compareRegister() const { return data.realTypes.m_compareRegister; }
 387 
 388     private:
 389         union {
 390             struct RealTypes {
 391                 int64_t m_from;
 392                 int64_t m_to;
 393                 RegisterID m_compareRegister;
 394                 JumpType m_type : 8;
 395                 JumpLinkType m_linkType : 8;
 396                 Condition m_condition : 4;
 397                 unsigned m_bitNumber : 6;
 398                 bool m_is64Bit : 1;
 399             } realTypes;
 400             struct CopyTypes {
 401                 uint64_t content[3];
 402             } copyTypes;
 403             COMPILE_ASSERT(sizeof(RealTypes) == sizeof(CopyTypes), LinkRecordCopyStructSizeEqualsRealStruct);
 404         } data;
 405     };
 406 
 407     // bits(N) VFPExpandImm(bits(8) imm8);
 408     //
 409     // Encoding of floating point immediates is a litte complicated. Here&#39;s a
 410     // high level description:
 411     //     +/-m*2-n where m and n are integers, 16 &lt;= m &lt;= 31, 0 &lt;= n &lt;= 7
 412     // and the algirithm for expanding to a single precision float:
 413     //     return imm8&lt;7&gt;:NOT(imm8&lt;6&gt;):Replicate(imm8&lt;6&gt;,5):imm8&lt;5:0&gt;:Zeros(19);
 414     //
 415     // The trickiest bit is how the exponent is handled. The following table
 416     // may help clarify things a little:
 417     //     654
 418     //     100 01111100 124 -3 1020 01111111100
 419     //     101 01111101 125 -2 1021 01111111101
 420     //     110 01111110 126 -1 1022 01111111110
 421     //     111 01111111 127  0 1023 01111111111
 422     //     000 10000000 128  1 1024 10000000000
 423     //     001 10000001 129  2 1025 10000000001
 424     //     010 10000010 130  3 1026 10000000010
 425     //     011 10000011 131  4 1027 10000000011
 426     // The first column shows the bit pattern stored in bits 6-4 of the arm
 427     // encoded immediate. The second column shows the 8-bit IEEE 754 single
 428     // -precision exponent in binary, the third column shows the raw decimal
 429     // value. IEEE 754 single-precision numbers are stored with a bias of 127
 430     // to the exponent, so the fourth column shows the resulting exponent.
 431     // From this was can see that the exponent can be in the range -3..4,
 432     // which agrees with the high level description given above. The fifth
 433     // and sixth columns shows the value stored in a IEEE 754 double-precision
 434     // number to represent these exponents in decimal and binary, given the
 435     // bias of 1023.
 436     //
 437     // Ultimately, detecting doubles that can be encoded as immediates on arm
 438     // and encoding doubles is actually not too bad. A floating point value can
 439     // be encoded by retaining the sign bit, the low three bits of the exponent
 440     // and the high 4 bits of the mantissa. To validly be able to encode an
 441     // immediate the remainder of the mantissa must be zero, and the high part
 442     // of the exponent must match the top bit retained, bar the highest bit
 443     // which must be its inverse.
 444     static bool canEncodeFPImm(double d)
 445     {
 446         // Discard the sign bit, the low two bits of the exponent &amp; the highest
 447         // four bits of the mantissa.
 448         uint64_t masked = bitwise_cast&lt;uint64_t&gt;(d) &amp; 0x7fc0ffffffffffffull;
 449         return (masked == 0x3fc0000000000000ull) || (masked == 0x4000000000000000ull);
 450     }
 451 
 452     template&lt;int datasize&gt;
 453     static bool canEncodePImmOffset(int32_t offset)
 454     {
 455         return isValidScaledUImm12&lt;datasize&gt;(offset);
 456     }
 457 
 458     static bool canEncodeSImmOffset(int32_t offset)
 459     {
 460         return isValidSignedImm9(offset);
 461     }
 462 
 463 protected:
 464     int encodeFPImm(double d)
 465     {
 466         ASSERT(canEncodeFPImm(d));
 467         uint64_t u64 = bitwise_cast&lt;uint64_t&gt;(d);
 468         return (static_cast&lt;int&gt;(u64 &gt;&gt; 56) &amp; 0x80) | (static_cast&lt;int&gt;(u64 &gt;&gt; 48) &amp; 0x7f);
 469     }
 470 
 471     template&lt;int datasize&gt;
 472     int encodeShiftAmount(int amount)
 473     {
 474         ASSERT(!amount || datasize == (8 &lt;&lt; amount));
 475         return amount;
 476     }
 477 
 478     template&lt;int datasize&gt;
 479     static int encodePositiveImmediate(unsigned pimm)
 480     {
 481         ASSERT(!(pimm &amp; ((datasize / 8) - 1)));
 482         return pimm / (datasize / 8);
 483     }
 484 
 485     enum Datasize {
 486         Datasize_32,
 487         Datasize_64,
 488         Datasize_64_top,
 489         Datasize_16
 490     };
 491 
 492     enum MemOpSize {
 493         MemOpSize_8_or_128,
 494         MemOpSize_16,
 495         MemOpSize_32,
 496         MemOpSize_64,
 497     };
 498 
 499     enum BranchType {
 500         BranchType_JMP,
 501         BranchType_CALL,
 502         BranchType_RET
 503     };
 504 
 505     enum AddOp {
 506         AddOp_ADD,
 507         AddOp_SUB
 508     };
 509 
 510     enum BitfieldOp {
 511         BitfieldOp_SBFM,
 512         BitfieldOp_BFM,
 513         BitfieldOp_UBFM
 514     };
 515 
 516     enum DataOp1Source {
 517         DataOp_RBIT,
 518         DataOp_REV16,
 519         DataOp_REV32,
 520         DataOp_REV64,
 521         DataOp_CLZ,
 522         DataOp_CLS
 523     };
 524 
 525     enum DataOp2Source {
 526         DataOp_UDIV = 2,
 527         DataOp_SDIV = 3,
 528         DataOp_LSLV = 8,
 529         DataOp_LSRV = 9,
 530         DataOp_ASRV = 10,
 531         DataOp_RORV = 11
 532     };
 533 
 534     enum DataOp3Source {
 535         DataOp_MADD = 0,
 536         DataOp_MSUB = 1,
 537         DataOp_SMADDL = 2,
 538         DataOp_SMSUBL = 3,
 539         DataOp_SMULH = 4,
 540         DataOp_UMADDL = 10,
 541         DataOp_UMSUBL = 11,
 542         DataOp_UMULH = 12
 543     };
 544 
 545     enum ExcepnOp {
 546         ExcepnOp_EXCEPTION = 0,
 547         ExcepnOp_BREAKPOINT = 1,
 548         ExcepnOp_HALT = 2,
 549         ExcepnOp_DCPS = 5
 550     };
 551 
 552     enum FPCmpOp {
 553         FPCmpOp_FCMP = 0x00,
 554         FPCmpOp_FCMP0 = 0x08,
 555         FPCmpOp_FCMPE = 0x10,
 556         FPCmpOp_FCMPE0 = 0x18
 557     };
 558 
 559     enum FPCondCmpOp {
 560         FPCondCmpOp_FCMP,
 561         FPCondCmpOp_FCMPE
 562     };
 563 
 564     enum FPDataOp1Source {
 565         FPDataOp_FMOV = 0,
 566         FPDataOp_FABS = 1,
 567         FPDataOp_FNEG = 2,
 568         FPDataOp_FSQRT = 3,
 569         FPDataOp_FCVT_toSingle = 4,
 570         FPDataOp_FCVT_toDouble = 5,
 571         FPDataOp_FCVT_toHalf = 7,
 572         FPDataOp_FRINTN = 8,
 573         FPDataOp_FRINTP = 9,
 574         FPDataOp_FRINTM = 10,
 575         FPDataOp_FRINTZ = 11,
 576         FPDataOp_FRINTA = 12,
 577         FPDataOp_FRINTX = 14,
 578         FPDataOp_FRINTI = 15
 579     };
 580 
 581     enum FPDataOp2Source {
 582         FPDataOp_FMUL,
 583         FPDataOp_FDIV,
 584         FPDataOp_FADD,
 585         FPDataOp_FSUB,
 586         FPDataOp_FMAX,
 587         FPDataOp_FMIN,
 588         FPDataOp_FMAXNM,
 589         FPDataOp_FMINNM,
 590         FPDataOp_FNMUL
 591     };
 592 
 593     enum SIMD3Same {
 594         SIMD_LogicalOp = 0x03
 595     };
 596 
 597     enum SIMD3SameLogical {
 598         // This includes both the U bit and the &quot;size&quot; / opc for convience.
 599         SIMD_LogicalOp_AND = 0x00,
 600         SIMD_LogicalOp_BIC = 0x01,
 601         SIMD_LogicalOp_ORR = 0x02,
 602         SIMD_LogicalOp_ORN = 0x03,
 603         SIMD_LogacalOp_EOR = 0x80,
 604         SIMD_LogicalOp_BSL = 0x81,
 605         SIMD_LogicalOp_BIT = 0x82,
 606         SIMD_LogicalOp_BIF = 0x83,
 607     };
 608 
 609     enum FPIntConvOp {
 610         FPIntConvOp_FCVTNS = 0x00,
 611         FPIntConvOp_FCVTNU = 0x01,
 612         FPIntConvOp_SCVTF = 0x02,
 613         FPIntConvOp_UCVTF = 0x03,
 614         FPIntConvOp_FCVTAS = 0x04,
 615         FPIntConvOp_FCVTAU = 0x05,
 616         FPIntConvOp_FMOV_QtoX = 0x06,
 617         FPIntConvOp_FMOV_XtoQ = 0x07,
 618         FPIntConvOp_FCVTPS = 0x08,
 619         FPIntConvOp_FCVTPU = 0x09,
 620         FPIntConvOp_FMOV_QtoX_top = 0x0e,
 621         FPIntConvOp_FMOV_XtoQ_top = 0x0f,
 622         FPIntConvOp_FCVTMS = 0x10,
 623         FPIntConvOp_FCVTMU = 0x11,
 624         FPIntConvOp_FCVTZS = 0x18,
 625         FPIntConvOp_FCVTZU = 0x19,
 626     };
 627 
 628     enum LogicalOp {
 629         LogicalOp_AND,
 630         LogicalOp_ORR,
 631         LogicalOp_EOR,
 632         LogicalOp_ANDS
 633     };
 634 
 635     enum MemOp {
 636         MemOp_STORE,
 637         MemOp_LOAD,
 638         MemOp_STORE_V128,
 639         MemOp_LOAD_V128,
 640         MemOp_PREFETCH = 2, // size must be 3
 641         MemOp_LOAD_signed64 = 2, // size may be 0, 1 or 2
 642         MemOp_LOAD_signed32 = 3 // size may be 0 or 1
 643     };
 644 
 645     enum MemPairOpSize {
 646         MemPairOp_32 = 0,
 647         MemPairOp_LoadSigned_32 = 1,
 648         MemPairOp_64 = 2,
 649 
 650         MemPairOp_V32 = MemPairOp_32,
 651         MemPairOp_V64 = 1,
 652         MemPairOp_V128 = 2
 653     };
 654 
 655     enum MoveWideOp {
 656         MoveWideOp_N = 0,
 657         MoveWideOp_Z = 2,
 658         MoveWideOp_K = 3
 659     };
 660 
 661     enum LdrLiteralOp {
 662         LdrLiteralOp_32BIT = 0,
 663         LdrLiteralOp_64BIT = 1,
 664         LdrLiteralOp_LDRSW = 2,
 665         LdrLiteralOp_128BIT = 2
 666     };
 667 
 668     enum ExoticLoadFence {
 669         ExoticLoadFence_None,
 670         ExoticLoadFence_Acquire
 671     };
 672 
 673     enum ExoticLoadAtomic {
 674         ExoticLoadAtomic_Link,
 675         ExoticLoadAtomic_None
 676     };
 677 
 678     enum ExoticStoreFence {
 679         ExoticStoreFence_None,
 680         ExoticStoreFence_Release,
 681     };
 682 
 683     static unsigned memPairOffsetShift(bool V, MemPairOpSize size)
 684     {
 685         // return the log2 of the size in bytes, e.g. 64 bit size returns 3
 686         if (V)
 687             return size + 2;
 688         return (size &gt;&gt; 1) + 2;
 689     }
 690 
 691 public:
 692     // Integer Instructions:
 693 
 694     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 695     ALWAYS_INLINE void adc(RegisterID rd, RegisterID rn, RegisterID rm)
 696     {
 697         CHECK_DATASIZE();
 698         insn(addSubtractWithCarry(DATASIZE, AddOp_ADD, setFlags, rm, rn, rd));
 699     }
 700 
 701     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 702     ALWAYS_INLINE void add(RegisterID rd, RegisterID rn, UInt12 imm12, int shift = 0)
 703     {
 704         CHECK_DATASIZE();
 705         ASSERT(!shift || shift == 12);
 706         insn(addSubtractImmediate(DATASIZE, AddOp_ADD, setFlags, shift == 12, imm12, rn, rd));
 707     }
 708 
 709     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 710     ALWAYS_INLINE void add(RegisterID rd, RegisterID rn, RegisterID rm)
 711     {
 712         add&lt;datasize, setFlags&gt;(rd, rn, rm, LSL, 0);
 713     }
 714 
 715     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 716     ALWAYS_INLINE void add(RegisterID rd, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
 717     {
 718         CHECK_DATASIZE();
 719         insn(addSubtractExtendedRegister(DATASIZE, AddOp_ADD, setFlags, rm, extend, amount, rn, rd));
 720     }
 721 
 722     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 723     ALWAYS_INLINE void add(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
 724     {
 725         CHECK_DATASIZE();
 726         if (isSp(rd) || isSp(rn)) {
 727             ASSERT(shift == LSL);
 728             ASSERT(!isSp(rm));
 729             add&lt;datasize, setFlags&gt;(rd, rn, rm, UXTX, amount);
 730         } else
 731             insn(addSubtractShiftedRegister(DATASIZE, AddOp_ADD, setFlags, shift, rm, amount, rn, rd));
 732     }
 733 
 734     ALWAYS_INLINE void adr(RegisterID rd, int offset)
 735     {
 736         insn(pcRelative(false, offset, rd));
 737     }
 738 
 739     ALWAYS_INLINE void adrp(RegisterID rd, int offset)
 740     {
 741         ASSERT(!(offset &amp; 0xfff));
 742         insn(pcRelative(true, offset &gt;&gt; 12, rd));
 743         nopCortexA53Fix843419();
 744     }
 745 
 746     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 747     ALWAYS_INLINE void and_(RegisterID rd, RegisterID rn, RegisterID rm)
 748     {
 749         and_&lt;datasize, setFlags&gt;(rd, rn, rm, LSL, 0);
 750     }
 751 
 752     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 753     ALWAYS_INLINE void and_(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
 754     {
 755         CHECK_DATASIZE();
 756         insn(logicalShiftedRegister(DATASIZE, setFlags ? LogicalOp_ANDS : LogicalOp_AND, shift, false, rm, amount, rn, rd));
 757     }
 758 
 759     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 760     ALWAYS_INLINE void and_(RegisterID rd, RegisterID rn, LogicalImmediate imm)
 761     {
 762         CHECK_DATASIZE();
 763         insn(logicalImmediate(DATASIZE, setFlags ? LogicalOp_ANDS : LogicalOp_AND, imm.value(), rn, rd));
 764     }
 765 
 766     template&lt;int datasize&gt;
 767     ALWAYS_INLINE void asr(RegisterID rd, RegisterID rn, int shift)
 768     {
 769         ASSERT(shift &lt; datasize);
 770         sbfm&lt;datasize&gt;(rd, rn, shift, datasize - 1);
 771     }
 772 
 773     template&lt;int datasize&gt;
 774     ALWAYS_INLINE void asr(RegisterID rd, RegisterID rn, RegisterID rm)
 775     {
 776         asrv&lt;datasize&gt;(rd, rn, rm);
 777     }
 778 
 779     template&lt;int datasize&gt;
 780     ALWAYS_INLINE void asrv(RegisterID rd, RegisterID rn, RegisterID rm)
 781     {
 782         CHECK_DATASIZE();
 783         insn(dataProcessing2Source(DATASIZE, rm, DataOp_ASRV, rn, rd));
 784     }
 785 
 786     ALWAYS_INLINE void b(int32_t offset = 0)
 787     {
 788         ASSERT(!(offset &amp; 3));
 789         offset &gt;&gt;= 2;
 790         ASSERT(offset == (offset &lt;&lt; 6) &gt;&gt; 6);
 791         insn(unconditionalBranchImmediate(false, offset));
 792     }
 793 
 794     ALWAYS_INLINE void b_cond(Condition cond, int32_t offset = 0)
 795     {
 796         ASSERT(!(offset &amp; 3));
 797         offset &gt;&gt;= 2;
 798         ASSERT(offset == (offset &lt;&lt; 13) &gt;&gt; 13);
 799         insn(conditionalBranchImmediate(offset, cond));
 800     }
 801 
 802     template&lt;int datasize&gt;
 803     ALWAYS_INLINE void bfi(RegisterID rd, RegisterID rn, int lsb, int width)
 804     {
 805         bfm&lt;datasize&gt;(rd, rn, (datasize - lsb) &amp; (datasize - 1), width - 1);
 806     }
 807 
 808     template&lt;int datasize&gt;
 809     ALWAYS_INLINE void bfm(RegisterID rd, RegisterID rn, int immr, int imms)
 810     {
 811         CHECK_DATASIZE();
 812         insn(bitfield(DATASIZE, BitfieldOp_BFM, immr, imms, rn, rd));
 813     }
 814 
 815     template&lt;int datasize&gt;
 816     ALWAYS_INLINE void bfxil(RegisterID rd, RegisterID rn, int lsb, int width)
 817     {
 818         bfm&lt;datasize&gt;(rd, rn, lsb, lsb + width - 1);
 819     }
 820 
 821     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 822     ALWAYS_INLINE void bic(RegisterID rd, RegisterID rn, RegisterID rm)
 823     {
 824         bic&lt;datasize, setFlags&gt;(rd, rn, rm, LSL, 0);
 825     }
 826 
 827     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
 828     ALWAYS_INLINE void bic(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
 829     {
 830         CHECK_DATASIZE();
 831         insn(logicalShiftedRegister(DATASIZE, setFlags ? LogicalOp_ANDS : LogicalOp_AND, shift, true, rm, amount, rn, rd));
 832     }
 833 
 834     ALWAYS_INLINE void bl(int32_t offset = 0)
 835     {
 836         ASSERT(!(offset &amp; 3));
 837         offset &gt;&gt;= 2;
 838         insn(unconditionalBranchImmediate(true, offset));
 839     }
 840 
 841     ALWAYS_INLINE void blr(RegisterID rn)
 842     {
 843         insn(unconditionalBranchRegister(BranchType_CALL, rn));
 844     }
 845 
 846     ALWAYS_INLINE void br(RegisterID rn)
 847     {
 848         insn(unconditionalBranchRegister(BranchType_JMP, rn));
 849     }
 850 
 851     ALWAYS_INLINE void brk(uint16_t imm)
 852     {
 853         insn(excepnGeneration(ExcepnOp_BREAKPOINT, imm, 0));
 854     }
 855 
 856     ALWAYS_INLINE static bool isBrk(void* address)
 857     {
 858         int expected = excepnGeneration(ExcepnOp_BREAKPOINT, 0, 0);
 859         int immediateMask = excepnGenerationImmMask();
 860         int candidateInstruction = *reinterpret_cast&lt;int*&gt;(address);
 861         return (candidateInstruction &amp; ~immediateMask) == expected;
 862     }
 863 
 864     template&lt;int datasize&gt;
 865     ALWAYS_INLINE void cbnz(RegisterID rt, int32_t offset = 0)
 866     {
 867         CHECK_DATASIZE();
 868         ASSERT(!(offset &amp; 3));
 869         offset &gt;&gt;= 2;
 870         insn(compareAndBranchImmediate(DATASIZE, true, offset, rt));
 871     }
 872 
 873     template&lt;int datasize&gt;
 874     ALWAYS_INLINE void cbz(RegisterID rt, int32_t offset = 0)
 875     {
 876         CHECK_DATASIZE();
 877         ASSERT(!(offset &amp; 3));
 878         offset &gt;&gt;= 2;
 879         insn(compareAndBranchImmediate(DATASIZE, false, offset, rt));
 880     }
 881 
 882     template&lt;int datasize&gt;
 883     ALWAYS_INLINE void ccmn(RegisterID rn, RegisterID rm, int nzcv, Condition cond)
 884     {
 885         CHECK_DATASIZE();
 886         insn(conditionalCompareRegister(DATASIZE, AddOp_ADD, rm, cond, rn, nzcv));
 887     }
 888 
 889     template&lt;int datasize&gt;
 890     ALWAYS_INLINE void ccmn(RegisterID rn, UInt5 imm, int nzcv, Condition cond)
 891     {
 892         CHECK_DATASIZE();
 893         insn(conditionalCompareImmediate(DATASIZE, AddOp_ADD, imm, cond, rn, nzcv));
 894     }
 895 
 896     template&lt;int datasize&gt;
 897     ALWAYS_INLINE void ccmp(RegisterID rn, RegisterID rm, int nzcv, Condition cond)
 898     {
 899         CHECK_DATASIZE();
 900         insn(conditionalCompareRegister(DATASIZE, AddOp_SUB, rm, cond, rn, nzcv));
 901     }
 902 
 903     template&lt;int datasize&gt;
 904     ALWAYS_INLINE void ccmp(RegisterID rn, UInt5 imm, int nzcv, Condition cond)
 905     {
 906         CHECK_DATASIZE();
 907         insn(conditionalCompareImmediate(DATASIZE, AddOp_SUB, imm, cond, rn, nzcv));
 908     }
 909 
 910     template&lt;int datasize&gt;
 911     ALWAYS_INLINE void cinc(RegisterID rd, RegisterID rn, Condition cond)
 912     {
 913         csinc&lt;datasize&gt;(rd, rn, rn, invert(cond));
 914     }
 915 
 916     template&lt;int datasize&gt;
 917     ALWAYS_INLINE void cinv(RegisterID rd, RegisterID rn, Condition cond)
 918     {
 919         csinv&lt;datasize&gt;(rd, rn, rn, invert(cond));
 920     }
 921 
 922     template&lt;int datasize&gt;
 923     ALWAYS_INLINE void cls(RegisterID rd, RegisterID rn)
 924     {
 925         CHECK_DATASIZE();
 926         insn(dataProcessing1Source(DATASIZE, DataOp_CLS, rn, rd));
 927     }
 928 
 929     template&lt;int datasize&gt;
 930     ALWAYS_INLINE void clz(RegisterID rd, RegisterID rn)
 931     {
 932         CHECK_DATASIZE();
 933         insn(dataProcessing1Source(DATASIZE, DataOp_CLZ, rn, rd));
 934     }
 935 
 936     template&lt;int datasize&gt;
 937     ALWAYS_INLINE void cmn(RegisterID rn, UInt12 imm12, int shift = 0)
 938     {
 939         add&lt;datasize, S&gt;(ARM64Registers::zr, rn, imm12, shift);
 940     }
 941 
 942     template&lt;int datasize&gt;
 943     ALWAYS_INLINE void cmn(RegisterID rn, RegisterID rm)
 944     {
 945         add&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm);
 946     }
 947 
 948     template&lt;int datasize&gt;
 949     ALWAYS_INLINE void cmn(RegisterID rn, RegisterID rm, ExtendType extend, int amount)
 950     {
 951         add&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm, extend, amount);
 952     }
 953 
 954     template&lt;int datasize&gt;
 955     ALWAYS_INLINE void cmn(RegisterID rn, RegisterID rm, ShiftType shift, int amount)
 956     {
 957         add&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm, shift, amount);
 958     }
 959 
 960     template&lt;int datasize&gt;
 961     ALWAYS_INLINE void cmp(RegisterID rn, UInt12 imm12, int shift = 0)
 962     {
 963         sub&lt;datasize, S&gt;(ARM64Registers::zr, rn, imm12, shift);
 964     }
 965 
 966     template&lt;int datasize&gt;
 967     ALWAYS_INLINE void cmp(RegisterID rn, RegisterID rm)
 968     {
 969         sub&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm);
 970     }
 971 
 972     template&lt;int datasize&gt;
 973     ALWAYS_INLINE void cmp(RegisterID rn, RegisterID rm, ExtendType extend, int amount)
 974     {
 975         sub&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm, extend, amount);
 976     }
 977 
 978     template&lt;int datasize&gt;
 979     ALWAYS_INLINE void cmp(RegisterID rn, RegisterID rm, ShiftType shift, int amount)
 980     {
 981         sub&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm, shift, amount);
 982     }
 983 
 984     template&lt;int datasize&gt;
 985     ALWAYS_INLINE void cneg(RegisterID rd, RegisterID rn, Condition cond)
 986     {
 987         csneg&lt;datasize&gt;(rd, rn, rn, invert(cond));
 988     }
 989 
 990     template&lt;int datasize&gt;
 991     ALWAYS_INLINE void csel(RegisterID rd, RegisterID rn, RegisterID rm, Condition cond)
 992     {
 993         CHECK_DATASIZE();
 994         insn(conditionalSelect(DATASIZE, false, rm, cond, false, rn, rd));
 995     }
 996 
 997     template&lt;int datasize&gt;
 998     ALWAYS_INLINE void cset(RegisterID rd, Condition cond)
 999     {
1000         csinc&lt;datasize&gt;(rd, ARM64Registers::zr, ARM64Registers::zr, invert(cond));
1001     }
1002 
1003     template&lt;int datasize&gt;
1004     ALWAYS_INLINE void csetm(RegisterID rd, Condition cond)
1005     {
1006         csinv&lt;datasize&gt;(rd, ARM64Registers::zr, ARM64Registers::zr, invert(cond));
1007     }
1008 
1009     template&lt;int datasize&gt;
1010     ALWAYS_INLINE void csinc(RegisterID rd, RegisterID rn, RegisterID rm, Condition cond)
1011     {
1012         CHECK_DATASIZE();
1013         insn(conditionalSelect(DATASIZE, false, rm, cond, true, rn, rd));
1014     }
1015 
1016     template&lt;int datasize&gt;
1017     ALWAYS_INLINE void csinv(RegisterID rd, RegisterID rn, RegisterID rm, Condition cond)
1018     {
1019         CHECK_DATASIZE();
1020         insn(conditionalSelect(DATASIZE, true, rm, cond, false, rn, rd));
1021     }
1022 
1023     template&lt;int datasize&gt;
1024     ALWAYS_INLINE void csneg(RegisterID rd, RegisterID rn, RegisterID rm, Condition cond)
1025     {
1026         CHECK_DATASIZE();
1027         insn(conditionalSelect(DATASIZE, true, rm, cond, true, rn, rd));
1028     }
1029 
1030     template&lt;int datasize&gt;
1031     ALWAYS_INLINE void eon(RegisterID rd, RegisterID rn, RegisterID rm)
1032     {
1033         eon&lt;datasize&gt;(rd, rn, rm, LSL, 0);
1034     }
1035 
1036     template&lt;int datasize&gt;
1037     ALWAYS_INLINE void eon(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
1038     {
1039         CHECK_DATASIZE();
1040         insn(logicalShiftedRegister(DATASIZE, LogicalOp_EOR, shift, true, rm, amount, rn, rd));
1041     }
1042 
1043     template&lt;int datasize&gt;
1044     ALWAYS_INLINE void eor(RegisterID rd, RegisterID rn, RegisterID rm)
1045     {
1046         eor&lt;datasize&gt;(rd, rn, rm, LSL, 0);
1047     }
1048 
1049     template&lt;int datasize&gt;
1050     ALWAYS_INLINE void eor(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
1051     {
1052         CHECK_DATASIZE();
1053         insn(logicalShiftedRegister(DATASIZE, LogicalOp_EOR, shift, false, rm, amount, rn, rd));
1054     }
1055 
1056     template&lt;int datasize&gt;
1057     ALWAYS_INLINE void eor(RegisterID rd, RegisterID rn, LogicalImmediate imm)
1058     {
1059         CHECK_DATASIZE();
1060         insn(logicalImmediate(DATASIZE, LogicalOp_EOR, imm.value(), rn, rd));
1061     }
1062 
1063     template&lt;int datasize&gt;
1064     ALWAYS_INLINE void extr(RegisterID rd, RegisterID rn, RegisterID rm, int lsb)
1065     {
1066         CHECK_DATASIZE();
1067         insn(extract(DATASIZE, rm, lsb, rn, rd));
1068     }
1069 
1070     ALWAYS_INLINE void hint(int imm)
1071     {
1072         insn(hintPseudo(imm));
1073     }
1074 
1075     ALWAYS_INLINE void hlt(uint16_t imm)
1076     {
1077         insn(excepnGeneration(ExcepnOp_HALT, imm, 0));
1078     }
1079 
1080     // Only used for testing purposes.
1081     void illegalInstruction()
1082     {
1083         insn(0x0);
1084     }
1085 
1086     template&lt;int datasize&gt;
1087     ALWAYS_INLINE void ldp(RegisterID rt, RegisterID rt2, RegisterID rn, PairPostIndex simm)
1088     {
1089         CHECK_DATASIZE();
1090         insn(loadStoreRegisterPairPostIndex(MEMPAIROPSIZE_INT(datasize), false, MemOp_LOAD, simm, rn, rt, rt2));
1091     }
1092 
1093     template&lt;int datasize&gt;
1094     ALWAYS_INLINE void ldp(RegisterID rt, RegisterID rt2, RegisterID rn, PairPreIndex simm)
1095     {
1096         CHECK_DATASIZE();
1097         insn(loadStoreRegisterPairPreIndex(MEMPAIROPSIZE_INT(datasize), false, MemOp_LOAD, simm, rn, rt, rt2));
1098     }
1099 
1100     template&lt;int datasize&gt;
1101     ALWAYS_INLINE void ldp(RegisterID rt, RegisterID rt2, RegisterID rn, unsigned pimm = 0)
1102     {
1103         CHECK_DATASIZE();
1104         insn(loadStoreRegisterPairOffset(MEMPAIROPSIZE_INT(datasize), false, MemOp_LOAD, pimm, rn, rt, rt2));
1105     }
1106 
1107     template&lt;int datasize&gt;
1108     ALWAYS_INLINE void ldnp(RegisterID rt, RegisterID rt2, RegisterID rn, unsigned pimm = 0)
1109     {
1110         CHECK_DATASIZE();
1111         insn(loadStoreRegisterPairNonTemporal(MEMPAIROPSIZE_INT(datasize), false, MemOp_LOAD, pimm, rn, rt, rt2));
1112     }
1113 
1114     template&lt;int datasize&gt;
1115     ALWAYS_INLINE void ldr(RegisterID rt, RegisterID rn, RegisterID rm)
1116     {
1117         ldr&lt;datasize&gt;(rt, rn, rm, UXTX, 0);
1118     }
1119 
1120     template&lt;int datasize&gt;
1121     ALWAYS_INLINE void ldr(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1122     {
1123         CHECK_DATASIZE();
1124         insn(loadStoreRegisterRegisterOffset(MEMOPSIZE, false, MemOp_LOAD, rm, extend, encodeShiftAmount&lt;datasize&gt;(amount), rn, rt));
1125     }
1126 
1127     template&lt;int datasize&gt;
1128     ALWAYS_INLINE void ldr(RegisterID rt, RegisterID rn, unsigned pimm)
1129     {
1130         CHECK_DATASIZE();
1131         insn(loadStoreRegisterUnsignedImmediate(MEMOPSIZE, false, MemOp_LOAD, encodePositiveImmediate&lt;datasize&gt;(pimm), rn, rt));
1132     }
1133 
1134     template&lt;int datasize&gt;
1135     ALWAYS_INLINE void ldr(RegisterID rt, RegisterID rn, PostIndex simm)
1136     {
1137         CHECK_DATASIZE();
1138         insn(loadStoreRegisterPostIndex(MEMOPSIZE, false, MemOp_LOAD, simm, rn, rt));
1139     }
1140 
1141     template&lt;int datasize&gt;
1142     ALWAYS_INLINE void ldr(RegisterID rt, RegisterID rn, PreIndex simm)
1143     {
1144         CHECK_DATASIZE();
1145         insn(loadStoreRegisterPreIndex(MEMOPSIZE, false, MemOp_LOAD, simm, rn, rt));
1146     }
1147 
1148     template&lt;int datasize&gt;
1149     ALWAYS_INLINE void ldr_literal(RegisterID rt, int offset = 0)
1150     {
1151         CHECK_DATASIZE();
1152         ASSERT(!(offset &amp; 3));
1153         insn(loadRegisterLiteral(datasize == 64 ? LdrLiteralOp_64BIT : LdrLiteralOp_32BIT, false, offset &gt;&gt; 2, rt));
1154     }
1155 
1156     ALWAYS_INLINE void ldrb(RegisterID rt, RegisterID rn, RegisterID rm)
1157     {
1158         // Not calling the 5 argument form of ldrb, since is amount is ommitted S is false.
1159         insn(loadStoreRegisterRegisterOffset(MemOpSize_8_or_128, false, MemOp_LOAD, rm, UXTX, false, rn, rt));
1160     }
1161 
1162     ALWAYS_INLINE void ldrb(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1163     {
1164         ASSERT_UNUSED(amount, !amount);
1165         insn(loadStoreRegisterRegisterOffset(MemOpSize_8_or_128, false, MemOp_LOAD, rm, extend, true, rn, rt));
1166     }
1167 
1168     ALWAYS_INLINE void ldrb(RegisterID rt, RegisterID rn, unsigned pimm)
1169     {
1170         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_8_or_128, false, MemOp_LOAD, encodePositiveImmediate&lt;8&gt;(pimm), rn, rt));
1171     }
1172 
1173     ALWAYS_INLINE void ldrb(RegisterID rt, RegisterID rn, PostIndex simm)
1174     {
1175         insn(loadStoreRegisterPostIndex(MemOpSize_8_or_128, false, MemOp_LOAD, simm, rn, rt));
1176     }
1177 
1178     ALWAYS_INLINE void ldrb(RegisterID rt, RegisterID rn, PreIndex simm)
1179     {
1180         insn(loadStoreRegisterPreIndex(MemOpSize_8_or_128, false, MemOp_LOAD, simm, rn, rt));
1181     }
1182 
1183     ALWAYS_INLINE void ldrh(RegisterID rt, RegisterID rn, RegisterID rm)
1184     {
1185         ldrh(rt, rn, rm, UXTX, 0);
1186     }
1187 
1188     ALWAYS_INLINE void ldrh(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1189     {
1190         ASSERT(!amount || amount == 1);
1191         insn(loadStoreRegisterRegisterOffset(MemOpSize_16, false, MemOp_LOAD, rm, extend, amount == 1, rn, rt));
1192     }
1193 
1194     ALWAYS_INLINE void ldrh(RegisterID rt, RegisterID rn, unsigned pimm)
1195     {
1196         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_16, false, MemOp_LOAD, encodePositiveImmediate&lt;16&gt;(pimm), rn, rt));
1197     }
1198 
1199     ALWAYS_INLINE void ldrh(RegisterID rt, RegisterID rn, PostIndex simm)
1200     {
1201         insn(loadStoreRegisterPostIndex(MemOpSize_16, false, MemOp_LOAD, simm, rn, rt));
1202     }
1203 
1204     ALWAYS_INLINE void ldrh(RegisterID rt, RegisterID rn, PreIndex simm)
1205     {
1206         insn(loadStoreRegisterPreIndex(MemOpSize_16, false, MemOp_LOAD, simm, rn, rt));
1207     }
1208 
1209     template&lt;int datasize&gt;
1210     ALWAYS_INLINE void ldrsb(RegisterID rt, RegisterID rn, RegisterID rm)
1211     {
1212         CHECK_DATASIZE();
1213         // Not calling the 5 argument form of ldrsb, since is amount is ommitted S is false.
1214         insn(loadStoreRegisterRegisterOffset(MemOpSize_8_or_128, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, rm, UXTX, false, rn, rt));
1215     }
1216 
1217     template&lt;int datasize&gt;
1218     ALWAYS_INLINE void ldrsb(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1219     {
1220         CHECK_DATASIZE();
1221         ASSERT_UNUSED(amount, !amount);
1222         insn(loadStoreRegisterRegisterOffset(MemOpSize_8_or_128, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, rm, extend, true, rn, rt));
1223     }
1224 
1225     template&lt;int datasize&gt;
1226     ALWAYS_INLINE void ldrsb(RegisterID rt, RegisterID rn, unsigned pimm)
1227     {
1228         CHECK_DATASIZE();
1229         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_8_or_128, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, encodePositiveImmediate&lt;8&gt;(pimm), rn, rt));
1230     }
1231 
1232     template&lt;int datasize&gt;
1233     ALWAYS_INLINE void ldrsb(RegisterID rt, RegisterID rn, PostIndex simm)
1234     {
1235         CHECK_DATASIZE();
1236         insn(loadStoreRegisterPostIndex(MemOpSize_8_or_128, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, simm, rn, rt));
1237     }
1238 
1239     template&lt;int datasize&gt;
1240     ALWAYS_INLINE void ldrsb(RegisterID rt, RegisterID rn, PreIndex simm)
1241     {
1242         CHECK_DATASIZE();
1243         insn(loadStoreRegisterPreIndex(MemOpSize_8_or_128, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, simm, rn, rt));
1244     }
1245 
1246     template&lt;int datasize&gt;
1247     ALWAYS_INLINE void ldrsh(RegisterID rt, RegisterID rn, RegisterID rm)
1248     {
1249         ldrsh&lt;datasize&gt;(rt, rn, rm, UXTX, 0);
1250     }
1251 
1252     template&lt;int datasize&gt;
1253     ALWAYS_INLINE void ldrsh(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1254     {
1255         CHECK_DATASIZE();
1256         ASSERT(!amount || amount == 1);
1257         insn(loadStoreRegisterRegisterOffset(MemOpSize_16, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, rm, extend, amount == 1, rn, rt));
1258     }
1259 
1260     template&lt;int datasize&gt;
1261     ALWAYS_INLINE void ldrsh(RegisterID rt, RegisterID rn, unsigned pimm)
1262     {
1263         CHECK_DATASIZE();
1264         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_16, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, encodePositiveImmediate&lt;16&gt;(pimm), rn, rt));
1265     }
1266 
1267     template&lt;int datasize&gt;
1268     ALWAYS_INLINE void ldrsh(RegisterID rt, RegisterID rn, PostIndex simm)
1269     {
1270         CHECK_DATASIZE();
1271         insn(loadStoreRegisterPostIndex(MemOpSize_16, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, simm, rn, rt));
1272     }
1273 
1274     template&lt;int datasize&gt;
1275     ALWAYS_INLINE void ldrsh(RegisterID rt, RegisterID rn, PreIndex simm)
1276     {
1277         CHECK_DATASIZE();
1278         insn(loadStoreRegisterPreIndex(MemOpSize_16, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, simm, rn, rt));
1279     }
1280 
1281     ALWAYS_INLINE void ldrsw(RegisterID rt, RegisterID rn, RegisterID rm)
1282     {
1283         ldrsw(rt, rn, rm, UXTX, 0);
1284     }
1285 
1286     ALWAYS_INLINE void ldrsw(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1287     {
1288         ASSERT(!amount || amount == 2);
1289         insn(loadStoreRegisterRegisterOffset(MemOpSize_32, false, MemOp_LOAD_signed64, rm, extend, amount == 2, rn, rt));
1290     }
1291 
1292     ALWAYS_INLINE void ldrsw(RegisterID rt, RegisterID rn, unsigned pimm)
1293     {
1294         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_32, false, MemOp_LOAD_signed64, encodePositiveImmediate&lt;32&gt;(pimm), rn, rt));
1295     }
1296 
1297     ALWAYS_INLINE void ldrsw(RegisterID rt, RegisterID rn, PostIndex simm)
1298     {
1299         insn(loadStoreRegisterPostIndex(MemOpSize_32, false, MemOp_LOAD_signed64, simm, rn, rt));
1300     }
1301 
1302     ALWAYS_INLINE void ldrsw(RegisterID rt, RegisterID rn, PreIndex simm)
1303     {
1304         insn(loadStoreRegisterPreIndex(MemOpSize_32, false, MemOp_LOAD_signed64, simm, rn, rt));
1305     }
1306 
1307     ALWAYS_INLINE void ldrsw_literal(RegisterID rt, int offset = 0)
1308     {
1309         ASSERT(!(offset &amp; 3));
1310         insn(loadRegisterLiteral(LdrLiteralOp_LDRSW, false, offset &gt;&gt; 2, rt));
1311     }
1312 
1313     template&lt;int datasize&gt;
1314     ALWAYS_INLINE void ldur(RegisterID rt, RegisterID rn, int simm)
1315     {
1316         CHECK_DATASIZE();
1317         insn(loadStoreRegisterUnscaledImmediate(MEMOPSIZE, false, MemOp_LOAD, simm, rn, rt));
1318     }
1319 
1320     ALWAYS_INLINE void ldurb(RegisterID rt, RegisterID rn, int simm)
1321     {
1322         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_8_or_128, false, MemOp_LOAD, simm, rn, rt));
1323     }
1324 
1325     ALWAYS_INLINE void ldurh(RegisterID rt, RegisterID rn, int simm)
1326     {
1327         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_16, false, MemOp_LOAD, simm, rn, rt));
1328     }
1329 
1330     template&lt;int datasize&gt;
1331     ALWAYS_INLINE void ldursb(RegisterID rt, RegisterID rn, int simm)
1332     {
1333         CHECK_DATASIZE();
1334         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_8_or_128, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, simm, rn, rt));
1335     }
1336 
1337     template&lt;int datasize&gt;
1338     ALWAYS_INLINE void ldursh(RegisterID rt, RegisterID rn, int simm)
1339     {
1340         CHECK_DATASIZE();
1341         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_16, false, (datasize == 64) ? MemOp_LOAD_signed64 : MemOp_LOAD_signed32, simm, rn, rt));
1342     }
1343 
1344     ALWAYS_INLINE void ldursw(RegisterID rt, RegisterID rn, int simm)
1345     {
1346         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_32, false, MemOp_LOAD_signed64, simm, rn, rt));
1347     }
1348 
1349     template&lt;int datasize&gt;
1350     ALWAYS_INLINE void lsl(RegisterID rd, RegisterID rn, int shift)
1351     {
1352         ASSERT(shift &lt; datasize);
1353         ubfm&lt;datasize&gt;(rd, rn, (datasize - shift) &amp; (datasize - 1), datasize - 1 - shift);
1354     }
1355 
1356     template&lt;int datasize&gt;
1357     ALWAYS_INLINE void lsl(RegisterID rd, RegisterID rn, RegisterID rm)
1358     {
1359         lslv&lt;datasize&gt;(rd, rn, rm);
1360     }
1361 
1362     template&lt;int datasize&gt;
1363     ALWAYS_INLINE void lslv(RegisterID rd, RegisterID rn, RegisterID rm)
1364     {
1365         CHECK_DATASIZE();
1366         insn(dataProcessing2Source(DATASIZE, rm, DataOp_LSLV, rn, rd));
1367     }
1368 
1369     template&lt;int datasize&gt;
1370     ALWAYS_INLINE void lsr(RegisterID rd, RegisterID rn, int shift)
1371     {
1372         ASSERT(shift &lt; datasize);
1373         ubfm&lt;datasize&gt;(rd, rn, shift, datasize - 1);
1374     }
1375 
1376     template&lt;int datasize&gt;
1377     ALWAYS_INLINE void lsr(RegisterID rd, RegisterID rn, RegisterID rm)
1378     {
1379         lsrv&lt;datasize&gt;(rd, rn, rm);
1380     }
1381 
1382     template&lt;int datasize&gt;
1383     ALWAYS_INLINE void lsrv(RegisterID rd, RegisterID rn, RegisterID rm)
1384     {
1385         CHECK_DATASIZE();
1386         insn(dataProcessing2Source(DATASIZE, rm, DataOp_LSRV, rn, rd));
1387     }
1388 
1389     template&lt;int datasize&gt;
1390     ALWAYS_INLINE void madd(RegisterID rd, RegisterID rn, RegisterID rm, RegisterID ra)
1391     {
1392         CHECK_DATASIZE();
1393         nopCortexA53Fix835769&lt;datasize&gt;();
1394         insn(dataProcessing3Source(DATASIZE, DataOp_MADD, rm, ra, rn, rd));
1395     }
1396 
1397     template&lt;int datasize&gt;
1398     ALWAYS_INLINE void mneg(RegisterID rd, RegisterID rn, RegisterID rm)
1399     {
1400         msub&lt;datasize&gt;(rd, rn, rm, ARM64Registers::zr);
1401     }
1402 
1403     template&lt;int datasize&gt;
1404     ALWAYS_INLINE void mov(RegisterID rd, RegisterID rm)
1405     {
1406         if (isSp(rd) || isSp(rm))
1407             add&lt;datasize&gt;(rd, rm, UInt12(0));
1408         else
1409             orr&lt;datasize&gt;(rd, ARM64Registers::zr, rm);
1410     }
1411 
1412     template&lt;int datasize&gt;
1413     ALWAYS_INLINE void movi(RegisterID rd, LogicalImmediate imm)
1414     {
1415         orr&lt;datasize&gt;(rd, ARM64Registers::zr, imm);
1416     }
1417 
1418     template&lt;int datasize&gt;
1419     ALWAYS_INLINE void movk(RegisterID rd, uint16_t value, int shift = 0)
1420     {
1421         CHECK_DATASIZE();
1422         ASSERT(!(shift &amp; 0xf));
1423         insn(moveWideImediate(DATASIZE, MoveWideOp_K, shift &gt;&gt; 4, value, rd));
1424     }
1425 
1426     template&lt;int datasize&gt;
1427     ALWAYS_INLINE void movn(RegisterID rd, uint16_t value, int shift = 0)
1428     {
1429         CHECK_DATASIZE();
1430         ASSERT(!(shift &amp; 0xf));
1431         insn(moveWideImediate(DATASIZE, MoveWideOp_N, shift &gt;&gt; 4, value, rd));
1432     }
1433 
1434     template&lt;int datasize&gt;
1435     ALWAYS_INLINE void movz(RegisterID rd, uint16_t value, int shift = 0)
1436     {
1437         CHECK_DATASIZE();
1438         ASSERT(!(shift &amp; 0xf));
1439         insn(moveWideImediate(DATASIZE, MoveWideOp_Z, shift &gt;&gt; 4, value, rd));
1440     }
1441 
1442     template&lt;int datasize&gt;
1443     ALWAYS_INLINE void msub(RegisterID rd, RegisterID rn, RegisterID rm, RegisterID ra)
1444     {
1445         CHECK_DATASIZE();
1446         nopCortexA53Fix835769&lt;datasize&gt;();
1447         insn(dataProcessing3Source(DATASIZE, DataOp_MSUB, rm, ra, rn, rd));
1448     }
1449 
1450     template&lt;int datasize&gt;
1451     ALWAYS_INLINE void mul(RegisterID rd, RegisterID rn, RegisterID rm)
1452     {
1453         madd&lt;datasize&gt;(rd, rn, rm, ARM64Registers::zr);
1454     }
1455 
1456     template&lt;int datasize&gt;
1457     ALWAYS_INLINE void mvn(RegisterID rd, RegisterID rm)
1458     {
1459         orn&lt;datasize&gt;(rd, ARM64Registers::zr, rm);
1460     }
1461 
1462     template&lt;int datasize&gt;
1463     ALWAYS_INLINE void mvn(RegisterID rd, RegisterID rm, ShiftType shift, int amount)
1464     {
1465         orn&lt;datasize&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1466     }
1467 
1468     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1469     ALWAYS_INLINE void neg(RegisterID rd, RegisterID rm)
1470     {
1471         sub&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm);
1472     }
1473 
1474     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1475     ALWAYS_INLINE void neg(RegisterID rd, RegisterID rm, ShiftType shift, int amount)
1476     {
1477         sub&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1478     }
1479 
1480     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1481     ALWAYS_INLINE void ngc(RegisterID rd, RegisterID rm)
1482     {
1483         sbc&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm);
1484     }
1485 
1486     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1487     ALWAYS_INLINE void ngc(RegisterID rd, RegisterID rm, ShiftType shift, int amount)
1488     {
1489         sbc&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1490     }
1491 
1492     ALWAYS_INLINE void nop()
1493     {
1494         insn(nopPseudo());
1495     }
1496 
1497     enum BranchTargetType { DirectBranch, IndirectBranch  };
1498     using CopyFunction = void*(&amp;)(void*, const void*, size_t);
1499 
1500     template &lt;CopyFunction copy&gt;
1501     static void fillNops(void* base, size_t size)
1502     {
1503         RELEASE_ASSERT(!(size % sizeof(int32_t)));
1504         size_t n = size / sizeof(int32_t);
1505         for (int32_t* ptr = static_cast&lt;int32_t*&gt;(base); n--;) {
1506             int insn = nopPseudo();
1507             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(ptr) == ptr);
1508             copy(ptr++, &amp;insn, sizeof(int));
1509         }
1510     }
1511 
1512     ALWAYS_INLINE void dmbISH()
1513     {
1514         insn(0xd5033bbf);
1515     }
1516 
1517     ALWAYS_INLINE void dmbISHST()
1518     {
1519         insn(0xd5033abf);
1520     }
1521 
1522     template&lt;int datasize&gt;
1523     void ldar(RegisterID dst, RegisterID src)
1524     {
1525         CHECK_MEMOPSIZE();
1526         insn(exoticLoad(MEMOPSIZE, ExoticLoadFence_Acquire, ExoticLoadAtomic_None, dst, src));
1527     }
1528 
1529     template&lt;int datasize&gt;
1530     void ldxr(RegisterID dst, RegisterID src)
1531     {
1532         CHECK_MEMOPSIZE();
1533         insn(exoticLoad(MEMOPSIZE, ExoticLoadFence_None, ExoticLoadAtomic_Link, dst, src));
1534     }
1535 
1536     template&lt;int datasize&gt;
1537     void ldaxr(RegisterID dst, RegisterID src)
1538     {
1539         CHECK_MEMOPSIZE();
1540         insn(exoticLoad(MEMOPSIZE, ExoticLoadFence_Acquire, ExoticLoadAtomic_Link, dst, src));
1541     }
1542 
1543     template&lt;int datasize&gt;
1544     void stxr(RegisterID result, RegisterID src, RegisterID dst)
1545     {
1546         CHECK_MEMOPSIZE();
1547         insn(exoticStore(MEMOPSIZE, ExoticStoreFence_None, result, src, dst));
1548     }
1549 
1550     template&lt;int datasize&gt;
1551     void stlr(RegisterID src, RegisterID dst)
1552     {
1553         CHECK_MEMOPSIZE();
1554         insn(storeRelease(MEMOPSIZE, src, dst));
1555     }
1556 
1557     template&lt;int datasize&gt;
1558     void stlxr(RegisterID result, RegisterID src, RegisterID dst)
1559     {
1560         CHECK_MEMOPSIZE();
1561         insn(exoticStore(MEMOPSIZE, ExoticStoreFence_Release, result, src, dst));
1562     }
1563 
1564 #if ENABLE(FAST_TLS_JIT)
1565     void mrs_TPIDRRO_EL0(RegisterID dst)
1566     {
1567         insn(0xd53bd060 | dst); // Thanks, otool -t!
1568     }
1569 #endif
1570 
1571     template&lt;int datasize&gt;
1572     ALWAYS_INLINE void orn(RegisterID rd, RegisterID rn, RegisterID rm)
1573     {
1574         orn&lt;datasize&gt;(rd, rn, rm, LSL, 0);
1575     }
1576 
1577     template&lt;int datasize&gt;
1578     ALWAYS_INLINE void orn(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
1579     {
1580         CHECK_DATASIZE();
1581         insn(logicalShiftedRegister(DATASIZE, LogicalOp_ORR, shift, true, rm, amount, rn, rd));
1582     }
1583 
1584     template&lt;int datasize&gt;
1585     ALWAYS_INLINE void orr(RegisterID rd, RegisterID rn, RegisterID rm)
1586     {
1587         orr&lt;datasize&gt;(rd, rn, rm, LSL, 0);
1588     }
1589 
1590     template&lt;int datasize&gt;
1591     ALWAYS_INLINE void orr(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
1592     {
1593         CHECK_DATASIZE();
1594         insn(logicalShiftedRegister(DATASIZE, LogicalOp_ORR, shift, false, rm, amount, rn, rd));
1595     }
1596 
1597     template&lt;int datasize&gt;
1598     ALWAYS_INLINE void orr(RegisterID rd, RegisterID rn, LogicalImmediate imm)
1599     {
1600         CHECK_DATASIZE();
1601         insn(logicalImmediate(DATASIZE, LogicalOp_ORR, imm.value(), rn, rd));
1602     }
1603 
1604     template&lt;int datasize&gt;
1605     ALWAYS_INLINE void rbit(RegisterID rd, RegisterID rn)
1606     {
1607         CHECK_DATASIZE();
1608         insn(dataProcessing1Source(DATASIZE, DataOp_RBIT, rn, rd));
1609     }
1610 
1611     ALWAYS_INLINE void ret(RegisterID rn = ARM64Registers::lr)
1612     {
1613         insn(unconditionalBranchRegister(BranchType_RET, rn));
1614     }
1615 
1616     template&lt;int datasize&gt;
1617     ALWAYS_INLINE void rev(RegisterID rd, RegisterID rn)
1618     {
1619         CHECK_DATASIZE();
1620         if (datasize == 32) // &#39;rev&#39; mnemonic means REV32 or REV64 depending on the operand width.
1621             insn(dataProcessing1Source(Datasize_32, DataOp_REV32, rn, rd));
1622         else
1623             insn(dataProcessing1Source(Datasize_64, DataOp_REV64, rn, rd));
1624     }
1625 
1626     template&lt;int datasize&gt;
1627     ALWAYS_INLINE void rev16(RegisterID rd, RegisterID rn)
1628     {
1629         CHECK_DATASIZE();
1630         insn(dataProcessing1Source(DATASIZE, DataOp_REV16, rn, rd));
1631     }
1632 
1633     template&lt;int datasize&gt;
1634     ALWAYS_INLINE void rev32(RegisterID rd, RegisterID rn)
1635     {
1636         ASSERT(datasize == 64); // &#39;rev32&#39; only valid with 64-bit operands.
1637         insn(dataProcessing1Source(Datasize_64, DataOp_REV32, rn, rd));
1638     }
1639 
1640     template&lt;int datasize&gt;
1641     ALWAYS_INLINE void ror(RegisterID rd, RegisterID rn, RegisterID rm)
1642     {
1643         rorv&lt;datasize&gt;(rd, rn, rm);
1644     }
1645 
1646     template&lt;int datasize&gt;
1647     ALWAYS_INLINE void ror(RegisterID rd, RegisterID rs, int shift)
1648     {
1649         extr&lt;datasize&gt;(rd, rs, rs, shift);
1650     }
1651 
1652     template&lt;int datasize&gt;
1653     ALWAYS_INLINE void rorv(RegisterID rd, RegisterID rn, RegisterID rm)
1654     {
1655         CHECK_DATASIZE();
1656         insn(dataProcessing2Source(DATASIZE, rm, DataOp_RORV, rn, rd));
1657     }
1658 
1659     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1660     ALWAYS_INLINE void sbc(RegisterID rd, RegisterID rn, RegisterID rm)
1661     {
1662         CHECK_DATASIZE();
1663         insn(addSubtractWithCarry(DATASIZE, AddOp_SUB, setFlags, rm, rn, rd));
1664     }
1665 
1666     template&lt;int datasize&gt;
1667     ALWAYS_INLINE void sbfiz(RegisterID rd, RegisterID rn, int lsb, int width)
1668     {
1669         sbfm&lt;datasize&gt;(rd, rn, (datasize - lsb) &amp; (datasize - 1), width - 1);
1670     }
1671 
1672     template&lt;int datasize&gt;
1673     ALWAYS_INLINE void sbfm(RegisterID rd, RegisterID rn, int immr, int imms)
1674     {
1675         CHECK_DATASIZE();
1676         insn(bitfield(DATASIZE, BitfieldOp_SBFM, immr, imms, rn, rd));
1677     }
1678 
1679     template&lt;int datasize&gt;
1680     ALWAYS_INLINE void sbfx(RegisterID rd, RegisterID rn, int lsb, int width)
1681     {
1682         sbfm&lt;datasize&gt;(rd, rn, lsb, lsb + width - 1);
1683     }
1684 
1685     template&lt;int datasize&gt;
1686     ALWAYS_INLINE void sdiv(RegisterID rd, RegisterID rn, RegisterID rm)
1687     {
1688         CHECK_DATASIZE();
1689         insn(dataProcessing2Source(DATASIZE, rm, DataOp_SDIV, rn, rd));
1690     }
1691 
1692     ALWAYS_INLINE void smaddl(RegisterID rd, RegisterID rn, RegisterID rm, RegisterID ra)
1693     {
1694         nopCortexA53Fix835769&lt;64&gt;();
1695         insn(dataProcessing3Source(Datasize_64, DataOp_SMADDL, rm, ra, rn, rd));
1696     }
1697 
1698     ALWAYS_INLINE void smnegl(RegisterID rd, RegisterID rn, RegisterID rm)
1699     {
1700         smsubl(rd, rn, rm, ARM64Registers::zr);
1701     }
1702 
1703     ALWAYS_INLINE void smsubl(RegisterID rd, RegisterID rn, RegisterID rm, RegisterID ra)
1704     {
1705         nopCortexA53Fix835769&lt;64&gt;();
1706         insn(dataProcessing3Source(Datasize_64, DataOp_SMSUBL, rm, ra, rn, rd));
1707     }
1708 
1709     ALWAYS_INLINE void smulh(RegisterID rd, RegisterID rn, RegisterID rm)
1710     {
1711         insn(dataProcessing3Source(Datasize_64, DataOp_SMULH, rm, ARM64Registers::zr, rn, rd));
1712     }
1713 
1714     ALWAYS_INLINE void smull(RegisterID rd, RegisterID rn, RegisterID rm)
1715     {
1716         smaddl(rd, rn, rm, ARM64Registers::zr);
1717     }
1718 
1719     template&lt;int datasize&gt;
1720     ALWAYS_INLINE void stp(RegisterID rt, RegisterID rt2, RegisterID rn, PairPostIndex simm)
1721     {
1722         CHECK_DATASIZE();
1723         insn(loadStoreRegisterPairPostIndex(MEMPAIROPSIZE_INT(datasize), false, MemOp_STORE, simm, rn, rt, rt2));
1724     }
1725 
1726     template&lt;int datasize&gt;
1727     ALWAYS_INLINE void stp(RegisterID rt, RegisterID rt2, RegisterID rn, PairPreIndex simm)
1728     {
1729         CHECK_DATASIZE();
1730         insn(loadStoreRegisterPairPreIndex(MEMPAIROPSIZE_INT(datasize), false, MemOp_STORE, simm, rn, rt, rt2));
1731     }
1732 
1733     template&lt;int datasize&gt;
1734     ALWAYS_INLINE void stp(RegisterID rt, RegisterID rt2, RegisterID rn, unsigned pimm = 0)
1735     {
1736         CHECK_DATASIZE();
1737         insn(loadStoreRegisterPairOffset(MEMPAIROPSIZE_INT(datasize), false, MemOp_STORE, pimm, rn, rt, rt2));
1738     }
1739 
1740     template&lt;int datasize&gt;
1741     ALWAYS_INLINE void stnp(RegisterID rt, RegisterID rt2, RegisterID rn, unsigned pimm = 0)
1742     {
1743         CHECK_DATASIZE();
1744         insn(loadStoreRegisterPairNonTemporal(MEMPAIROPSIZE_INT(datasize), false, MemOp_STORE, pimm, rn, rt, rt2));
1745     }
1746 
1747     template&lt;int datasize&gt;
1748     ALWAYS_INLINE void str(RegisterID rt, RegisterID rn, RegisterID rm)
1749     {
1750         str&lt;datasize&gt;(rt, rn, rm, UXTX, 0);
1751     }
1752 
1753     template&lt;int datasize&gt;
1754     ALWAYS_INLINE void str(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1755     {
1756         CHECK_DATASIZE();
1757         insn(loadStoreRegisterRegisterOffset(MEMOPSIZE, false, MemOp_STORE, rm, extend, encodeShiftAmount&lt;datasize&gt;(amount), rn, rt));
1758     }
1759 
1760     template&lt;int datasize&gt;
1761     ALWAYS_INLINE void str(RegisterID rt, RegisterID rn, unsigned pimm)
1762     {
1763         CHECK_DATASIZE();
1764         insn(loadStoreRegisterUnsignedImmediate(MEMOPSIZE, false, MemOp_STORE, encodePositiveImmediate&lt;datasize&gt;(pimm), rn, rt));
1765     }
1766 
1767     template&lt;int datasize&gt;
1768     ALWAYS_INLINE void str(RegisterID rt, RegisterID rn, PostIndex simm)
1769     {
1770         CHECK_DATASIZE();
1771         insn(loadStoreRegisterPostIndex(MEMOPSIZE, false, MemOp_STORE, simm, rn, rt));
1772     }
1773 
1774     template&lt;int datasize&gt;
1775     ALWAYS_INLINE void str(RegisterID rt, RegisterID rn, PreIndex simm)
1776     {
1777         CHECK_DATASIZE();
1778         insn(loadStoreRegisterPreIndex(MEMOPSIZE, false, MemOp_STORE, simm, rn, rt));
1779     }
1780 
1781     ALWAYS_INLINE void strb(RegisterID rt, RegisterID rn, RegisterID rm)
1782     {
1783         // Not calling the 5 argument form of strb, since is amount is ommitted S is false.
1784         insn(loadStoreRegisterRegisterOffset(MemOpSize_8_or_128, false, MemOp_STORE, rm, UXTX, false, rn, rt));
1785     }
1786 
1787     ALWAYS_INLINE void strb(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1788     {
1789         ASSERT_UNUSED(amount, !amount);
1790         insn(loadStoreRegisterRegisterOffset(MemOpSize_8_or_128, false, MemOp_STORE, rm, extend, true, rn, rt));
1791     }
1792 
1793     ALWAYS_INLINE void strb(RegisterID rt, RegisterID rn, unsigned pimm)
1794     {
1795         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_8_or_128, false, MemOp_STORE, encodePositiveImmediate&lt;8&gt;(pimm), rn, rt));
1796     }
1797 
1798     ALWAYS_INLINE void strb(RegisterID rt, RegisterID rn, PostIndex simm)
1799     {
1800         insn(loadStoreRegisterPostIndex(MemOpSize_8_or_128, false, MemOp_STORE, simm, rn, rt));
1801     }
1802 
1803     ALWAYS_INLINE void strb(RegisterID rt, RegisterID rn, PreIndex simm)
1804     {
1805         insn(loadStoreRegisterPreIndex(MemOpSize_8_or_128, false, MemOp_STORE, simm, rn, rt));
1806     }
1807 
1808     ALWAYS_INLINE void strh(RegisterID rt, RegisterID rn, RegisterID rm)
1809     {
1810         strh(rt, rn, rm, UXTX, 0);
1811     }
1812 
1813     ALWAYS_INLINE void strh(RegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1814     {
1815         ASSERT(!amount || amount == 1);
1816         insn(loadStoreRegisterRegisterOffset(MemOpSize_16, false, MemOp_STORE, rm, extend, amount == 1, rn, rt));
1817     }
1818 
1819     ALWAYS_INLINE void strh(RegisterID rt, RegisterID rn, unsigned pimm)
1820     {
1821         insn(loadStoreRegisterUnsignedImmediate(MemOpSize_16, false, MemOp_STORE, encodePositiveImmediate&lt;16&gt;(pimm), rn, rt));
1822     }
1823 
1824     ALWAYS_INLINE void strh(RegisterID rt, RegisterID rn, PostIndex simm)
1825     {
1826         insn(loadStoreRegisterPostIndex(MemOpSize_16, false, MemOp_STORE, simm, rn, rt));
1827     }
1828 
1829     ALWAYS_INLINE void strh(RegisterID rt, RegisterID rn, PreIndex simm)
1830     {
1831         insn(loadStoreRegisterPreIndex(MemOpSize_16, false, MemOp_STORE, simm, rn, rt));
1832     }
1833 
1834     template&lt;int datasize&gt;
1835     ALWAYS_INLINE void stur(RegisterID rt, RegisterID rn, int simm)
1836     {
1837         CHECK_DATASIZE();
1838         insn(loadStoreRegisterUnscaledImmediate(MEMOPSIZE, false, MemOp_STORE, simm, rn, rt));
1839     }
1840 
1841     ALWAYS_INLINE void sturb(RegisterID rt, RegisterID rn, int simm)
1842     {
1843         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_8_or_128, false, MemOp_STORE, simm, rn, rt));
1844     }
1845 
1846     ALWAYS_INLINE void sturh(RegisterID rt, RegisterID rn, int simm)
1847     {
1848         insn(loadStoreRegisterUnscaledImmediate(MemOpSize_16, false, MemOp_STORE, simm, rn, rt));
1849     }
1850 
1851     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1852     ALWAYS_INLINE void sub(RegisterID rd, RegisterID rn, UInt12 imm12, int shift = 0)
1853     {
1854         CHECK_DATASIZE();
1855         ASSERT(!shift || shift == 12);
1856         insn(addSubtractImmediate(DATASIZE, AddOp_SUB, setFlags, shift == 12, imm12, rn, rd));
1857     }
1858 
1859     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1860     ALWAYS_INLINE void sub(RegisterID rd, RegisterID rn, RegisterID rm)
1861     {
1862         ASSERT_WITH_MESSAGE(!isSp(rd) || setFlags == DontSetFlags, &quot;SUBS with shifted register does not support SP for Xd, it uses XZR for the register 31. SUBS with extended register support SP for Xd, but only if SetFlag is not used, otherwise register 31 is Xd.&quot;);
1863         ASSERT_WITH_MESSAGE(!isSp(rm), &quot;No encoding of SUBS supports SP for the third operand.&quot;);
1864 
1865         if (isSp(rd) || isSp(rn))
1866             sub&lt;datasize, setFlags&gt;(rd, rn, rm, UXTX, 0);
1867         else
1868             sub&lt;datasize, setFlags&gt;(rd, rn, rm, LSL, 0);
1869     }
1870 
1871     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1872     ALWAYS_INLINE void sub(RegisterID rd, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
1873     {
1874         CHECK_DATASIZE();
1875         insn(addSubtractExtendedRegister(DATASIZE, AddOp_SUB, setFlags, rm, extend, amount, rn, rd));
1876     }
1877 
1878     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1879     ALWAYS_INLINE void sub(RegisterID rd, RegisterID rn, RegisterID rm, ShiftType shift, int amount)
1880     {
1881         CHECK_DATASIZE();
1882         ASSERT(!isSp(rd) &amp;&amp; !isSp(rn) &amp;&amp; !isSp(rm));
1883         insn(addSubtractShiftedRegister(DATASIZE, AddOp_SUB, setFlags, shift, rm, amount, rn, rd));
1884     }
1885 
1886     template&lt;int datasize&gt;
1887     ALWAYS_INLINE void sxtb(RegisterID rd, RegisterID rn)
1888     {
1889         sbfm&lt;datasize&gt;(rd, rn, 0, 7);
1890     }
1891 
1892     template&lt;int datasize&gt;
1893     ALWAYS_INLINE void sxth(RegisterID rd, RegisterID rn)
1894     {
1895         sbfm&lt;datasize&gt;(rd, rn, 0, 15);
1896     }
1897 
1898     ALWAYS_INLINE void sxtw(RegisterID rd, RegisterID rn)
1899     {
1900         sbfm&lt;64&gt;(rd, rn, 0, 31);
1901     }
1902 
1903     ALWAYS_INLINE void tbz(RegisterID rt, int imm, int offset = 0)
1904     {
1905         ASSERT(!(offset &amp; 3));
1906         offset &gt;&gt;= 2;
1907         insn(testAndBranchImmediate(false, imm, offset, rt));
1908     }
1909 
1910     ALWAYS_INLINE void tbnz(RegisterID rt, int imm, int offset = 0)
1911     {
1912         ASSERT(!(offset &amp; 3));
1913         offset &gt;&gt;= 2;
1914         insn(testAndBranchImmediate(true, imm, offset, rt));
1915     }
1916 
1917     template&lt;int datasize&gt;
1918     ALWAYS_INLINE void tst(RegisterID rn, RegisterID rm)
1919     {
1920         and_&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm);
1921     }
1922 
1923     template&lt;int datasize&gt;
1924     ALWAYS_INLINE void tst(RegisterID rn, RegisterID rm, ShiftType shift, int amount)
1925     {
1926         and_&lt;datasize, S&gt;(ARM64Registers::zr, rn, rm, shift, amount);
1927     }
1928 
1929     template&lt;int datasize&gt;
1930     ALWAYS_INLINE void tst(RegisterID rn, LogicalImmediate imm)
1931     {
1932         and_&lt;datasize, S&gt;(ARM64Registers::zr, rn, imm);
1933     }
1934 
1935     template&lt;int datasize&gt;
1936     ALWAYS_INLINE void ubfiz(RegisterID rd, RegisterID rn, int lsb, int width)
1937     {
1938         ubfm&lt;datasize&gt;(rd, rn, (datasize - lsb) &amp; (datasize - 1), width - 1);
1939     }
1940 
1941     template&lt;int datasize&gt;
1942     ALWAYS_INLINE void ubfm(RegisterID rd, RegisterID rn, int immr, int imms)
1943     {
1944         CHECK_DATASIZE();
1945         insn(bitfield(DATASIZE, BitfieldOp_UBFM, immr, imms, rn, rd));
1946     }
1947 
1948     template&lt;int datasize&gt;
1949     ALWAYS_INLINE void ubfx(RegisterID rd, RegisterID rn, int lsb, int width)
1950     {
1951         ubfm&lt;datasize&gt;(rd, rn, lsb, lsb + width - 1);
1952     }
1953 
1954     template&lt;int datasize&gt;
1955     ALWAYS_INLINE void udiv(RegisterID rd, RegisterID rn, RegisterID rm)
1956     {
1957         CHECK_DATASIZE();
1958         insn(dataProcessing2Source(DATASIZE, rm, DataOp_UDIV, rn, rd));
1959     }
1960 
1961     ALWAYS_INLINE void umaddl(RegisterID rd, RegisterID rn, RegisterID rm, RegisterID ra)
1962     {
1963         nopCortexA53Fix835769&lt;64&gt;();
1964         insn(dataProcessing3Source(Datasize_64, DataOp_UMADDL, rm, ra, rn, rd));
1965     }
1966 
1967     ALWAYS_INLINE void umnegl(RegisterID rd, RegisterID rn, RegisterID rm)
1968     {
1969         umsubl(rd, rn, rm, ARM64Registers::zr);
1970     }
1971 
1972     ALWAYS_INLINE void umsubl(RegisterID rd, RegisterID rn, RegisterID rm, RegisterID ra)
1973     {
1974         nopCortexA53Fix835769&lt;64&gt;();
1975         insn(dataProcessing3Source(Datasize_64, DataOp_UMSUBL, rm, ra, rn, rd));
1976     }
1977 
1978     ALWAYS_INLINE void umulh(RegisterID rd, RegisterID rn, RegisterID rm)
1979     {
1980         insn(dataProcessing3Source(Datasize_64, DataOp_UMULH, rm, ARM64Registers::zr, rn, rd));
1981     }
1982 
1983     ALWAYS_INLINE void umull(RegisterID rd, RegisterID rn, RegisterID rm)
1984     {
1985         umaddl(rd, rn, rm, ARM64Registers::zr);
1986     }
1987 
1988     template&lt;int datasize&gt;
1989     ALWAYS_INLINE void uxtb(RegisterID rd, RegisterID rn)
1990     {
1991         ubfm&lt;datasize&gt;(rd, rn, 0, 7);
1992     }
1993 
1994     template&lt;int datasize&gt;
1995     ALWAYS_INLINE void uxth(RegisterID rd, RegisterID rn)
1996     {
1997         ubfm&lt;datasize&gt;(rd, rn, 0, 15);
1998     }
1999 
2000     ALWAYS_INLINE void uxtw(RegisterID rd, RegisterID rn)
2001     {
2002         ubfm&lt;64&gt;(rd, rn, 0, 31);
2003     }
2004 
2005     // Floating Point Instructions:
2006 
2007     template&lt;int datasize&gt;
2008     ALWAYS_INLINE void fabs(FPRegisterID vd, FPRegisterID vn)
2009     {
2010         CHECK_DATASIZE();
2011         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FABS, vn, vd));
2012     }
2013 
2014     template&lt;int datasize&gt;
2015     ALWAYS_INLINE void fadd(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2016     {
2017         CHECK_DATASIZE();
2018         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FADD, vn, vd));
2019     }
2020 
2021     template&lt;int datasize&gt;
2022     ALWAYS_INLINE void fccmp(FPRegisterID vn, FPRegisterID vm, int nzcv, Condition cond)
2023     {
2024         CHECK_DATASIZE();
2025         insn(floatingPointConditionalCompare(DATASIZE, vm, cond, vn, FPCondCmpOp_FCMP, nzcv));
2026     }
2027 
2028     template&lt;int datasize&gt;
2029     ALWAYS_INLINE void fccmpe(FPRegisterID vn, FPRegisterID vm, int nzcv, Condition cond)
2030     {
2031         CHECK_DATASIZE();
2032         insn(floatingPointConditionalCompare(DATASIZE, vm, cond, vn, FPCondCmpOp_FCMPE, nzcv));
2033     }
2034 
2035     template&lt;int datasize&gt;
2036     ALWAYS_INLINE void fcmp(FPRegisterID vn, FPRegisterID vm)
2037     {
2038         CHECK_DATASIZE();
2039         insn(floatingPointCompare(DATASIZE, vm, vn, FPCmpOp_FCMP));
2040     }
2041 
2042     template&lt;int datasize&gt;
2043     ALWAYS_INLINE void fcmp_0(FPRegisterID vn)
2044     {
2045         CHECK_DATASIZE();
2046         insn(floatingPointCompare(DATASIZE, static_cast&lt;FPRegisterID&gt;(0), vn, FPCmpOp_FCMP0));
2047     }
2048 
2049     template&lt;int datasize&gt;
2050     ALWAYS_INLINE void fcmpe(FPRegisterID vn, FPRegisterID vm)
2051     {
2052         CHECK_DATASIZE();
2053         insn(floatingPointCompare(DATASIZE, vm, vn, FPCmpOp_FCMPE));
2054     }
2055 
2056     template&lt;int datasize&gt;
2057     ALWAYS_INLINE void fcmpe_0(FPRegisterID vn)
2058     {
2059         CHECK_DATASIZE();
2060         insn(floatingPointCompare(DATASIZE, static_cast&lt;FPRegisterID&gt;(0), vn, FPCmpOp_FCMPE0));
2061     }
2062 
2063     template&lt;int datasize&gt;
2064     ALWAYS_INLINE void fcsel(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm, Condition cond)
2065     {
2066         CHECK_DATASIZE();
2067         insn(floatingPointConditionalSelect(DATASIZE, vm, cond, vn, vd));
2068     }
2069 
2070     template&lt;int dstsize, int srcsize&gt;
2071     ALWAYS_INLINE void fcvt(FPRegisterID vd, FPRegisterID vn)
2072     {
2073         ASSERT(dstsize == 16 || dstsize == 32 || dstsize == 64);
2074         ASSERT(srcsize == 16 || srcsize == 32 || srcsize == 64);
2075         ASSERT(dstsize != srcsize);
2076         Datasize type = (srcsize == 64) ? Datasize_64 : (srcsize == 32) ? Datasize_32 : Datasize_16;
2077         FPDataOp1Source opcode = (dstsize == 64) ? FPDataOp_FCVT_toDouble : (dstsize == 32) ? FPDataOp_FCVT_toSingle : FPDataOp_FCVT_toHalf;
2078         insn(floatingPointDataProcessing1Source(type, opcode, vn, vd));
2079     }
2080 
2081     template&lt;int dstsize, int srcsize&gt;
2082     ALWAYS_INLINE void fcvtas(RegisterID rd, FPRegisterID vn)
2083     {
2084         CHECK_DATASIZE_OF(dstsize);
2085         CHECK_DATASIZE_OF(srcsize);
2086         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTAS, vn, rd));
2087     }
2088 
2089     template&lt;int dstsize, int srcsize&gt;
2090     ALWAYS_INLINE void fcvtau(RegisterID rd, FPRegisterID vn)
2091     {
2092         CHECK_DATASIZE_OF(dstsize);
2093         CHECK_DATASIZE_OF(srcsize);
2094         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTAU, vn, rd));
2095     }
2096 
2097     template&lt;int dstsize, int srcsize&gt;
2098     ALWAYS_INLINE void fcvtms(RegisterID rd, FPRegisterID vn)
2099     {
2100         CHECK_DATASIZE_OF(dstsize);
2101         CHECK_DATASIZE_OF(srcsize);
2102         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTMS, vn, rd));
2103     }
2104 
2105     template&lt;int dstsize, int srcsize&gt;
2106     ALWAYS_INLINE void fcvtmu(RegisterID rd, FPRegisterID vn)
2107     {
2108         CHECK_DATASIZE_OF(dstsize);
2109         CHECK_DATASIZE_OF(srcsize);
2110         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTMU, vn, rd));
2111     }
2112 
2113     template&lt;int dstsize, int srcsize&gt;
2114     ALWAYS_INLINE void fcvtns(RegisterID rd, FPRegisterID vn)
2115     {
2116         CHECK_DATASIZE_OF(dstsize);
2117         CHECK_DATASIZE_OF(srcsize);
2118         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTNS, vn, rd));
2119     }
2120 
2121     template&lt;int dstsize, int srcsize&gt;
2122     ALWAYS_INLINE void fcvtnu(RegisterID rd, FPRegisterID vn)
2123     {
2124         CHECK_DATASIZE_OF(dstsize);
2125         CHECK_DATASIZE_OF(srcsize);
2126         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTNU, vn, rd));
2127     }
2128 
2129     template&lt;int dstsize, int srcsize&gt;
2130     ALWAYS_INLINE void fcvtps(RegisterID rd, FPRegisterID vn)
2131     {
2132         CHECK_DATASIZE_OF(dstsize);
2133         CHECK_DATASIZE_OF(srcsize);
2134         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTPS, vn, rd));
2135     }
2136 
2137     template&lt;int dstsize, int srcsize&gt;
2138     ALWAYS_INLINE void fcvtpu(RegisterID rd, FPRegisterID vn)
2139     {
2140         CHECK_DATASIZE_OF(dstsize);
2141         CHECK_DATASIZE_OF(srcsize);
2142         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTPU, vn, rd));
2143     }
2144 
2145     template&lt;int dstsize, int srcsize&gt;
2146     ALWAYS_INLINE void fcvtzs(RegisterID rd, FPRegisterID vn)
2147     {
2148         CHECK_DATASIZE_OF(dstsize);
2149         CHECK_DATASIZE_OF(srcsize);
2150         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTZS, vn, rd));
2151     }
2152 
2153     template&lt;int dstsize, int srcsize&gt;
2154     ALWAYS_INLINE void fcvtzu(RegisterID rd, FPRegisterID vn)
2155     {
2156         CHECK_DATASIZE_OF(dstsize);
2157         CHECK_DATASIZE_OF(srcsize);
2158         insn(floatingPointIntegerConversions(DATASIZE_OF(dstsize), DATASIZE_OF(srcsize), FPIntConvOp_FCVTZU, vn, rd));
2159     }
2160 
2161     template&lt;int datasize&gt;
2162     ALWAYS_INLINE void fdiv(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2163     {
2164         CHECK_DATASIZE();
2165         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FDIV, vn, vd));
2166     }
2167 
2168     template&lt;int datasize&gt;
2169     ALWAYS_INLINE void fmadd(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm, FPRegisterID va)
2170     {
2171         CHECK_DATASIZE();
2172         insn(floatingPointDataProcessing3Source(DATASIZE, false, vm, AddOp_ADD, va, vn, vd));
2173     }
2174 
2175     template&lt;int datasize&gt;
2176     ALWAYS_INLINE void fmax(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2177     {
2178         CHECK_DATASIZE();
2179         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FMAX, vn, vd));
2180     }
2181 
2182     template&lt;int datasize&gt;
2183     ALWAYS_INLINE void fmaxnm(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2184     {
2185         CHECK_DATASIZE();
2186         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FMAXNM, vn, vd));
2187     }
2188 
2189     template&lt;int datasize&gt;
2190     ALWAYS_INLINE void fmin(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2191     {
2192         CHECK_DATASIZE();
2193         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FMIN, vn, vd));
2194     }
2195 
2196     template&lt;int datasize&gt;
2197     ALWAYS_INLINE void fminnm(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2198     {
2199         CHECK_DATASIZE();
2200         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FMINNM, vn, vd));
2201     }
2202 
2203     template&lt;int datasize&gt;
2204     ALWAYS_INLINE void fmov(FPRegisterID vd, FPRegisterID vn)
2205     {
2206         CHECK_DATASIZE();
2207         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FMOV, vn, vd));
2208     }
2209 
2210     template&lt;int datasize&gt;
2211     ALWAYS_INLINE void fmov(FPRegisterID vd, RegisterID rn)
2212     {
2213         CHECK_DATASIZE();
2214         insn(floatingPointIntegerConversions(DATASIZE, DATASIZE, FPIntConvOp_FMOV_XtoQ, rn, vd));
2215     }
2216 
2217     template&lt;int datasize&gt;
2218     ALWAYS_INLINE void fmov(RegisterID rd, FPRegisterID vn)
2219     {
2220         CHECK_DATASIZE();
2221         insn(floatingPointIntegerConversions(DATASIZE, DATASIZE, FPIntConvOp_FMOV_QtoX, vn, rd));
2222     }
2223 
2224     template&lt;int datasize&gt;
2225     ALWAYS_INLINE void fmov(FPRegisterID vd, double imm)
2226     {
2227         CHECK_DATASIZE();
2228         insn(floatingPointImmediate(DATASIZE, encodeFPImm(imm), vd));
2229     }
2230 
2231     ALWAYS_INLINE void fmov_top(FPRegisterID vd, RegisterID rn)
2232     {
2233         insn(floatingPointIntegerConversions(Datasize_64, Datasize_64, FPIntConvOp_FMOV_XtoQ_top, rn, vd));
2234     }
2235 
2236     ALWAYS_INLINE void fmov_top(RegisterID rd, FPRegisterID vn)
2237     {
2238         insn(floatingPointIntegerConversions(Datasize_64, Datasize_64, FPIntConvOp_FMOV_QtoX_top, vn, rd));
2239     }
2240 
2241     template&lt;int datasize&gt;
2242     ALWAYS_INLINE void fmsub(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm, FPRegisterID va)
2243     {
2244         CHECK_DATASIZE();
2245         insn(floatingPointDataProcessing3Source(DATASIZE, false, vm, AddOp_SUB, va, vn, vd));
2246     }
2247 
2248     template&lt;int datasize&gt;
2249     ALWAYS_INLINE void fmul(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2250     {
2251         CHECK_DATASIZE();
2252         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FMUL, vn, vd));
2253     }
2254 
2255     template&lt;int datasize&gt;
2256     ALWAYS_INLINE void fneg(FPRegisterID vd, FPRegisterID vn)
2257     {
2258         CHECK_DATASIZE();
2259         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FNEG, vn, vd));
2260     }
2261 
2262     template&lt;int datasize&gt;
2263     ALWAYS_INLINE void fnmadd(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm, FPRegisterID va)
2264     {
2265         CHECK_DATASIZE();
2266         insn(floatingPointDataProcessing3Source(DATASIZE, true, vm, AddOp_ADD, va, vn, vd));
2267     }
2268 
2269     template&lt;int datasize&gt;
2270     ALWAYS_INLINE void fnmsub(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm, FPRegisterID va)
2271     {
2272         CHECK_DATASIZE();
2273         insn(floatingPointDataProcessing3Source(DATASIZE, true, vm, AddOp_SUB, va, vn, vd));
2274     }
2275 
2276     template&lt;int datasize&gt;
2277     ALWAYS_INLINE void fnmul(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2278     {
2279         CHECK_DATASIZE();
2280         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FNMUL, vn, vd));
2281     }
2282 
2283     template&lt;int datasize&gt;
2284     ALWAYS_INLINE void vand(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2285     {
2286         CHECK_VECTOR_DATASIZE();
2287         insn(vectorDataProcessingLogical(SIMD_LogicalOp_AND, vm, vn, vd));
2288     }
2289 
2290     template&lt;int datasize&gt;
2291     ALWAYS_INLINE void vorr(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2292     {
2293         CHECK_VECTOR_DATASIZE();
2294         insn(vectorDataProcessingLogical(SIMD_LogicalOp_ORR, vm, vn, vd));
2295     }
2296 
2297     template&lt;int datasize&gt;
2298     ALWAYS_INLINE void frinta(FPRegisterID vd, FPRegisterID vn)
2299     {
2300         CHECK_DATASIZE();
2301         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTA, vn, vd));
2302     }
2303 
2304     template&lt;int datasize&gt;
2305     ALWAYS_INLINE void frinti(FPRegisterID vd, FPRegisterID vn)
2306     {
2307         CHECK_DATASIZE();
2308         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTI, vn, vd));
2309     }
2310 
2311     template&lt;int datasize&gt;
2312     ALWAYS_INLINE void frintm(FPRegisterID vd, FPRegisterID vn)
2313     {
2314         CHECK_DATASIZE();
2315         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTM, vn, vd));
2316     }
2317 
2318     template&lt;int datasize&gt;
2319     ALWAYS_INLINE void frintn(FPRegisterID vd, FPRegisterID vn)
2320     {
2321         CHECK_DATASIZE();
2322         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTN, vn, vd));
2323     }
2324 
2325     template&lt;int datasize&gt;
2326     ALWAYS_INLINE void frintp(FPRegisterID vd, FPRegisterID vn)
2327     {
2328         CHECK_DATASIZE();
2329         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTP, vn, vd));
2330     }
2331 
2332     template&lt;int datasize&gt;
2333     ALWAYS_INLINE void frintx(FPRegisterID vd, FPRegisterID vn)
2334     {
2335         CHECK_DATASIZE();
2336         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTX, vn, vd));
2337     }
2338 
2339     template&lt;int datasize&gt;
2340     ALWAYS_INLINE void frintz(FPRegisterID vd, FPRegisterID vn)
2341     {
2342         CHECK_DATASIZE();
2343         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FRINTZ, vn, vd));
2344     }
2345 
2346     template&lt;int datasize&gt;
2347     ALWAYS_INLINE void fsqrt(FPRegisterID vd, FPRegisterID vn)
2348     {
2349         CHECK_DATASIZE();
2350         insn(floatingPointDataProcessing1Source(DATASIZE, FPDataOp_FSQRT, vn, vd));
2351     }
2352 
2353     template&lt;int datasize&gt;
2354     ALWAYS_INLINE void fsub(FPRegisterID vd, FPRegisterID vn, FPRegisterID vm)
2355     {
2356         CHECK_DATASIZE();
2357         insn(floatingPointDataProcessing2Source(DATASIZE, vm, FPDataOp_FSUB, vn, vd));
2358     }
2359 
2360     template&lt;int datasize&gt;
2361     ALWAYS_INLINE void ldr(FPRegisterID rt, RegisterID rn, RegisterID rm)
2362     {
2363         ldr&lt;datasize&gt;(rt, rn, rm, UXTX, 0);
2364     }
2365 
2366     template&lt;int datasize&gt;
2367     ALWAYS_INLINE void ldr(FPRegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
2368     {
2369         CHECK_FP_MEMOP_DATASIZE();
2370         insn(loadStoreRegisterRegisterOffset(MEMOPSIZE, true, datasize == 128 ? MemOp_LOAD_V128 : MemOp_LOAD, rm, extend, encodeShiftAmount&lt;datasize&gt;(amount), rn, rt));
2371     }
2372 
2373     template&lt;int datasize&gt;
2374     ALWAYS_INLINE void ldr(FPRegisterID rt, RegisterID rn, unsigned pimm)
2375     {
2376         CHECK_FP_MEMOP_DATASIZE();
2377         insn(loadStoreRegisterUnsignedImmediate(MEMOPSIZE, true, datasize == 128 ? MemOp_LOAD_V128 : MemOp_LOAD, encodePositiveImmediate&lt;datasize&gt;(pimm), rn, rt));
2378     }
2379 
2380     template&lt;int datasize&gt;
2381     ALWAYS_INLINE void ldr(FPRegisterID rt, RegisterID rn, PostIndex simm)
2382     {
2383         CHECK_FP_MEMOP_DATASIZE();
2384         insn(loadStoreRegisterPostIndex(MEMOPSIZE, true, datasize == 128 ? MemOp_LOAD_V128 : MemOp_LOAD, simm, rn, rt));
2385     }
2386 
2387     template&lt;int datasize&gt;
2388     ALWAYS_INLINE void ldr(FPRegisterID rt, RegisterID rn, PreIndex simm)
2389     {
2390         CHECK_FP_MEMOP_DATASIZE();
2391         insn(loadStoreRegisterPreIndex(MEMOPSIZE, true, datasize == 128 ? MemOp_LOAD_V128 : MemOp_LOAD, simm, rn, rt));
2392     }
2393 
2394     template&lt;int datasize&gt;
2395     ALWAYS_INLINE void ldr_literal(FPRegisterID rt, int offset = 0)
2396     {
2397         CHECK_FP_MEMOP_DATASIZE();
2398         ASSERT(datasize &gt;= 32);
2399         ASSERT(!(offset &amp; 3));
2400         insn(loadRegisterLiteral(datasize == 128 ? LdrLiteralOp_128BIT : datasize == 64 ? LdrLiteralOp_64BIT : LdrLiteralOp_32BIT, true, offset &gt;&gt; 2, rt));
2401     }
2402 
2403     template&lt;int datasize&gt;
2404     ALWAYS_INLINE void ldur(FPRegisterID rt, RegisterID rn, int simm)
2405     {
2406         CHECK_FP_MEMOP_DATASIZE();
2407         insn(loadStoreRegisterUnscaledImmediate(MEMOPSIZE, true, datasize == 128 ? MemOp_LOAD_V128 : MemOp_LOAD, simm, rn, rt));
2408     }
2409 
2410     template&lt;int dstsize, int srcsize&gt;
2411     ALWAYS_INLINE void scvtf(FPRegisterID vd, RegisterID rn)
2412     {
2413         CHECK_DATASIZE_OF(dstsize);
2414         CHECK_DATASIZE_OF(srcsize);
2415         insn(floatingPointIntegerConversions(DATASIZE_OF(srcsize), DATASIZE_OF(dstsize), FPIntConvOp_SCVTF, rn, vd));
2416     }
2417 
2418     template&lt;int datasize&gt;
2419     ALWAYS_INLINE void str(FPRegisterID rt, RegisterID rn, RegisterID rm)
2420     {
2421         str&lt;datasize&gt;(rt, rn, rm, UXTX, 0);
2422     }
2423 
2424     template&lt;int datasize&gt;
2425     ALWAYS_INLINE void str(FPRegisterID rt, RegisterID rn, RegisterID rm, ExtendType extend, int amount)
2426     {
2427         CHECK_FP_MEMOP_DATASIZE();
2428         insn(loadStoreRegisterRegisterOffset(MEMOPSIZE, true, datasize == 128 ? MemOp_STORE_V128 : MemOp_STORE, rm, extend, encodeShiftAmount&lt;datasize&gt;(amount), rn, rt));
2429     }
2430 
2431     template&lt;int datasize&gt;
2432     ALWAYS_INLINE void str(FPRegisterID rt, RegisterID rn, unsigned pimm)
2433     {
2434         CHECK_FP_MEMOP_DATASIZE();
2435         insn(loadStoreRegisterUnsignedImmediate(MEMOPSIZE, true, datasize == 128 ? MemOp_STORE_V128 : MemOp_STORE, encodePositiveImmediate&lt;datasize&gt;(pimm), rn, rt));
2436     }
2437 
2438     template&lt;int datasize&gt;
2439     ALWAYS_INLINE void str(FPRegisterID rt, RegisterID rn, PostIndex simm)
2440     {
2441         CHECK_FP_MEMOP_DATASIZE();
2442         insn(loadStoreRegisterPostIndex(MEMOPSIZE, true, datasize == 128 ? MemOp_STORE_V128 : MemOp_STORE, simm, rn, rt));
2443     }
2444 
2445     template&lt;int datasize&gt;
2446     ALWAYS_INLINE void str(FPRegisterID rt, RegisterID rn, PreIndex simm)
2447     {
2448         CHECK_FP_MEMOP_DATASIZE();
2449         insn(loadStoreRegisterPreIndex(MEMOPSIZE, true, datasize == 128 ? MemOp_STORE_V128 : MemOp_STORE, simm, rn, rt));
2450     }
2451 
2452     template&lt;int datasize&gt;
2453     ALWAYS_INLINE void stur(FPRegisterID rt, RegisterID rn, int simm)
2454     {
2455         CHECK_DATASIZE();
2456         insn(loadStoreRegisterUnscaledImmediate(MEMOPSIZE, true, datasize == 128 ? MemOp_STORE_V128 : MemOp_STORE, simm, rn, rt));
2457     }
2458 
2459     template&lt;int dstsize, int srcsize&gt;
2460     ALWAYS_INLINE void ucvtf(FPRegisterID vd, RegisterID rn)
2461     {
2462         CHECK_DATASIZE_OF(dstsize);
2463         CHECK_DATASIZE_OF(srcsize);
2464         insn(floatingPointIntegerConversions(DATASIZE_OF(srcsize), DATASIZE_OF(dstsize), FPIntConvOp_UCVTF, rn, vd));
2465     }
2466 
2467     ALWAYS_INLINE void fjcvtzs(RegisterID rd, FPRegisterID dn)
2468     {
2469         insn(fjcvtzsInsn(dn, rd));
2470     }
2471 
2472     // Admin methods:
2473 
2474     AssemblerLabel labelIgnoringWatchpoints()
2475     {
2476         return m_buffer.label();
2477     }
2478 
2479     AssemblerLabel labelForWatchpoint()
2480     {
2481         AssemblerLabel result = m_buffer.label();
2482         if (static_cast&lt;int&gt;(result.m_offset) != m_indexOfLastWatchpoint)
2483             result = label();
2484         m_indexOfLastWatchpoint = result.m_offset;
2485         m_indexOfTailOfLastWatchpoint = result.m_offset + maxJumpReplacementSize();
2486         return result;
2487     }
2488 
2489     AssemblerLabel label()
2490     {
2491         AssemblerLabel result = m_buffer.label();
2492         while (UNLIKELY(static_cast&lt;int&gt;(result.m_offset) &lt; m_indexOfTailOfLastWatchpoint)) {
2493             nop();
2494             result = m_buffer.label();
2495         }
2496         return result;
2497     }
2498 
2499     AssemblerLabel align(int alignment)
2500     {
2501         ASSERT(!(alignment &amp; 3));
2502         while (!m_buffer.isAligned(alignment))
2503             brk(0);
2504         return label();
2505     }
2506 
2507     static void* getRelocatedAddress(void* code, AssemblerLabel label)
2508     {
2509         ASSERT(label.isSet());
2510         return reinterpret_cast&lt;void*&gt;(reinterpret_cast&lt;ptrdiff_t&gt;(code) + label.m_offset);
2511     }
2512 
2513     static int getDifferenceBetweenLabels(AssemblerLabel a, AssemblerLabel b)
2514     {
2515         return b.m_offset - a.m_offset;
2516     }
2517 
2518     size_t codeSize() const { return m_buffer.codeSize(); }
2519 
2520     static unsigned getCallReturnOffset(AssemblerLabel call)
2521     {
2522         ASSERT(call.isSet());
2523         return call.m_offset;
2524     }
2525 
2526     // Linking &amp; patching:
2527     //
2528     // &#39;link&#39; and &#39;patch&#39; methods are for use on unprotected code - such as the code
2529     // within the AssemblerBuffer, and code being patched by the patch buffer. Once
2530     // code has been finalized it is (platform support permitting) within a non-
2531     // writable region of memory; to modify the code in an execute-only execuable
2532     // pool the &#39;repatch&#39; and &#39;relink&#39; methods should be used.
2533 
2534     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition)
2535     {
2536         ASSERT(to.isSet());
2537         ASSERT(from.isSet());
2538         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition));
2539     }
2540 
2541     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition, bool is64Bit, RegisterID compareRegister)
2542     {
2543         ASSERT(to.isSet());
2544         ASSERT(from.isSet());
2545         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition, is64Bit, compareRegister));
2546     }
2547 
2548     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition, unsigned bitNumber, RegisterID compareRegister)
2549     {
2550         ASSERT(to.isSet());
2551         ASSERT(from.isSet());
2552         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition, bitNumber, compareRegister));
2553     }
2554 
2555     static void linkJump(void* code, AssemblerLabel from, void* to)
2556     {
2557         ASSERT(from.isSet());
2558         relinkJumpOrCall&lt;BranchType_JMP&gt;(addressOf(code, from), addressOf(code, from), to);
2559     }
2560 
2561     static void linkCall(void* code, AssemblerLabel from, void* to)
2562     {
2563         ASSERT(from.isSet());
2564         linkJumpOrCall&lt;BranchType_CALL&gt;(addressOf(code, from) - 1, addressOf(code, from) - 1, to);
2565     }
2566 
2567     static void linkPointer(void* code, AssemblerLabel where, void* valuePtr)
2568     {
2569         linkPointer(addressOf(code, where), valuePtr);
2570     }
2571 
2572     static void replaceWithVMHalt(void* where)
2573     {
2574         // This should try to write to null which should always Segfault.
2575         int insn = dataCacheZeroVirtualAddress(ARM64Registers::zr);
2576         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2577         performJITMemcpy(where, &amp;insn, sizeof(int));
2578         cacheFlush(where, sizeof(int));
2579     }
2580 
2581     static void replaceWithJump(void* where, void* to)
2582     {
2583         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(where)) &gt;&gt; 2;
2584         ASSERT(static_cast&lt;int&gt;(offset) == offset);
2585         int insn = unconditionalBranchImmediate(false, static_cast&lt;int&gt;(offset));
2586         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2587         performJITMemcpy(where, &amp;insn, sizeof(int));
2588         cacheFlush(where, sizeof(int));
2589     }
2590 
2591     static ptrdiff_t maxJumpReplacementSize()
2592     {
2593         return 4;
2594     }
2595 
2596     static constexpr ptrdiff_t patchableJumpSize()
2597     {
2598         return 4;
2599     }
2600 
2601     static void replaceWithLoad(void* where)
2602     {
2603         Datasize sf;
2604         AddOp op;
2605         SetFlags S;
2606         int shift;
2607         int imm12;
2608         RegisterID rn;
2609         RegisterID rd;
2610         if (disassembleAddSubtractImmediate(where, sf, op, S, shift, imm12, rn, rd)) {
2611             ASSERT(sf == Datasize_64);
2612             ASSERT(op == AddOp_ADD);
2613             ASSERT(!S);
2614             ASSERT(!shift);
2615             ASSERT(!(imm12 &amp; ~0xff8));
2616             int insn = loadStoreRegisterUnsignedImmediate(MemOpSize_64, false, MemOp_LOAD, encodePositiveImmediate&lt;64&gt;(imm12), rn, rd);
2617             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2618             performJITMemcpy(where, &amp;insn, sizeof(int));
2619             cacheFlush(where, sizeof(int));
2620         }
2621 #if ASSERT_ENABLED
2622         else {
2623             MemOpSize size;
2624             bool V;
2625             MemOp opc;
2626             int imm12;
2627             RegisterID rn;
2628             RegisterID rt;
2629             ASSERT(disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt));
2630             ASSERT(size == MemOpSize_64);
2631             ASSERT(!V);
2632             ASSERT(opc == MemOp_LOAD);
2633             ASSERT(!(imm12 &amp; ~0x1ff));
2634         }
2635 #endif // ASSERT_ENABLED
2636     }
2637 
2638     static void replaceWithAddressComputation(void* where)
2639     {
2640         MemOpSize size;
2641         bool V;
2642         MemOp opc;
2643         int imm12;
2644         RegisterID rn;
2645         RegisterID rt;
2646         if (disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt)) {
2647             ASSERT(size == MemOpSize_64);
2648             ASSERT(!V);
2649             ASSERT(opc == MemOp_LOAD);
2650             ASSERT(!(imm12 &amp; ~0x1ff));
2651             int insn = addSubtractImmediate(Datasize_64, AddOp_ADD, DontSetFlags, 0, imm12 * sizeof(void*), rn, rt);
2652             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2653             performJITMemcpy(where, &amp;insn, sizeof(int));
2654             cacheFlush(where, sizeof(int));
2655         }
2656 #if ASSERT_ENABLED
2657         else {
2658             Datasize sf;
2659             AddOp op;
2660             SetFlags S;
2661             int shift;
2662             int imm12;
2663             RegisterID rn;
2664             RegisterID rd;
2665             ASSERT(disassembleAddSubtractImmediate(where, sf, op, S, shift, imm12, rn, rd));
2666             ASSERT(sf == Datasize_64);
2667             ASSERT(op == AddOp_ADD);
2668             ASSERT(!S);
2669             ASSERT(!shift);
2670             ASSERT(!(imm12 &amp; ~0xff8));
2671         }
2672 #endif // ASSERT_ENABLED
2673     }
2674 
2675     static void repatchPointer(void* where, void* valuePtr)
2676     {
2677         linkPointer(static_cast&lt;int*&gt;(where), valuePtr, true);
2678     }
2679 
2680     static void setPointer(int* address, void* valuePtr, RegisterID rd, bool flush)
2681     {
2682         uintptr_t value = reinterpret_cast&lt;uintptr_t&gt;(valuePtr);
2683         int buffer[3];
2684         buffer[0] = moveWideImediate(Datasize_64, MoveWideOp_Z, 0, getHalfword(value, 0), rd);
2685         buffer[1] = moveWideImediate(Datasize_64, MoveWideOp_K, 1, getHalfword(value, 1), rd);
2686         buffer[2] = moveWideImediate(Datasize_64, MoveWideOp_K, 2, getHalfword(value, 2), rd);
2687         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(address) == address);
2688         performJITMemcpy(address, buffer, sizeof(int) * 3);
2689 
2690         if (flush)
2691             cacheFlush(address, sizeof(int) * 3);
2692     }
2693 
2694     static void repatchInt32(void* where, int32_t value)
2695     {
2696         int* address = static_cast&lt;int*&gt;(where);
2697 
2698         Datasize sf;
2699         MoveWideOp opc;
2700         int hw;
2701         uint16_t imm16;
2702         RegisterID rd;
2703         bool expected = disassembleMoveWideImediate(address, sf, opc, hw, imm16, rd);
2704         ASSERT_UNUSED(expected, expected &amp;&amp; !sf &amp;&amp; (opc == MoveWideOp_Z || opc == MoveWideOp_N) &amp;&amp; !hw);
2705         ASSERT(checkMovk&lt;Datasize_32&gt;(address[1], 1, rd));
2706 
2707         int buffer[2];
2708         if (value &gt;= 0) {
2709             buffer[0] = moveWideImediate(Datasize_32, MoveWideOp_Z, 0, getHalfword(value, 0), rd);
2710             buffer[1] = moveWideImediate(Datasize_32, MoveWideOp_K, 1, getHalfword(value, 1), rd);
2711         } else {
2712             buffer[0] = moveWideImediate(Datasize_32, MoveWideOp_N, 0, ~getHalfword(value, 0), rd);
2713             buffer[1] = moveWideImediate(Datasize_32, MoveWideOp_K, 1, getHalfword(value, 1), rd);
2714         }
2715         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2716         performJITMemcpy(where, &amp;buffer, sizeof(int) * 2);
2717 
2718         cacheFlush(where, sizeof(int) * 2);
2719     }
2720 
2721     static void* readPointer(void* where)
2722     {
2723         int* address = static_cast&lt;int*&gt;(where);
2724 
2725         Datasize sf;
2726         MoveWideOp opc;
2727         int hw;
2728         uint16_t imm16;
2729         RegisterID rdFirst, rd;
2730 
2731         bool expected = disassembleMoveWideImediate(address, sf, opc, hw, imm16, rdFirst);
2732         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_Z &amp;&amp; !hw);
2733         uintptr_t result = imm16;
2734 
2735         expected = disassembleMoveWideImediate(address + 1, sf, opc, hw, imm16, rd);
2736         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_K &amp;&amp; hw == 1 &amp;&amp; rd == rdFirst);
2737         result |= static_cast&lt;uintptr_t&gt;(imm16) &lt;&lt; 16;
2738 
2739 #if CPU(ADDRESS64)
2740         expected = disassembleMoveWideImediate(address + 2, sf, opc, hw, imm16, rd);
2741         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_K &amp;&amp; hw == 2 &amp;&amp; rd == rdFirst);
2742         result |= static_cast&lt;uintptr_t&gt;(imm16) &lt;&lt; 32;
2743 #endif
2744 
2745         return reinterpret_cast&lt;void*&gt;(result);
2746     }
2747 
2748     static void* readCallTarget(void* from)
2749     {
2750         return readPointer(reinterpret_cast&lt;int*&gt;(from) - (isAddress64Bit() ? 4 : 3));
2751     }
2752 
2753     // The static relink, repatch, and replace methods can use can
2754     // use |from| for both the write and executable address for call
2755     // and jump patching as they&#39;re modifying existing (linked) code,
2756     // so the address being provided is correct for relative address
2757     // computation.
2758     static void relinkJump(void* from, void* to)
2759     {
2760         relinkJumpOrCall&lt;BranchType_JMP&gt;(reinterpret_cast&lt;int*&gt;(from), reinterpret_cast&lt;const int*&gt;(from), to);
2761         cacheFlush(from, sizeof(int));
2762     }
2763 
2764     static void relinkJumpToNop(void* from)
2765     {
2766         relinkJump(from, static_cast&lt;char*&gt;(from) + 4);
2767     }
2768 
2769     static void relinkCall(void* from, void* to)
2770     {
2771         relinkJumpOrCall&lt;BranchType_CALL&gt;(reinterpret_cast&lt;int*&gt;(from) - 1, reinterpret_cast&lt;const int*&gt;(from) - 1, to);
2772         cacheFlush(reinterpret_cast&lt;int*&gt;(from) - 1, sizeof(int));
2773     }
2774 
2775     static void repatchCompact(void* where, int32_t value)
2776     {
2777         ASSERT(!(value &amp; ~0x3ff8));
2778 
2779         MemOpSize size;
2780         bool V;
2781         MemOp opc;
2782         int imm12;
2783         RegisterID rn;
2784         RegisterID rt;
2785         bool expected = disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt);
2786         ASSERT_UNUSED(expected, expected &amp;&amp; size &gt;= MemOpSize_32 &amp;&amp; !V &amp;&amp; opc == MemOp_LOAD); // expect 32/64 bit load to GPR.
2787 
2788         if (size == MemOpSize_32)
2789             imm12 = encodePositiveImmediate&lt;32&gt;(value);
2790         else
2791             imm12 = encodePositiveImmediate&lt;64&gt;(value);
2792         int insn = loadStoreRegisterUnsignedImmediate(size, V, opc, imm12, rn, rt);
2793         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2794         performJITMemcpy(where, &amp;insn, sizeof(int));
2795 
2796         cacheFlush(where, sizeof(int));
2797     }
2798 
2799     unsigned debugOffset() { return m_buffer.debugOffset(); }
2800 
2801 #if OS(LINUX) &amp;&amp; COMPILER(GCC_COMPATIBLE)
2802     static inline void linuxPageFlush(uintptr_t begin, uintptr_t end)
2803     {
2804         __builtin___clear_cache(reinterpret_cast&lt;char*&gt;(begin), reinterpret_cast&lt;char*&gt;(end));
2805     }
2806 #endif
2807 
2808     static void cacheFlush(void* code, size_t size)
2809     {
2810 #if OS(DARWIN)
2811         sys_cache_control(kCacheFunctionPrepareForExecution, code, size);
2812 #elif OS(FUCHSIA)
2813         zx_cache_flush(code, size, ZX_CACHE_FLUSH_INSN);
2814 #elif OS(LINUX)
2815         size_t page = pageSize();
2816         uintptr_t current = reinterpret_cast&lt;uintptr_t&gt;(code);
2817         uintptr_t end = current + size;
2818         uintptr_t firstPageEnd = (current &amp; ~(page - 1)) + page;
2819 
2820         if (end &lt;= firstPageEnd) {
2821             linuxPageFlush(current, end);
2822             return;
2823         }
2824 
2825         linuxPageFlush(current, firstPageEnd);
2826 
2827         for (current = firstPageEnd; current + page &lt; end; current += page)
2828             linuxPageFlush(current, current + page);
2829 
2830         linuxPageFlush(current, end);
2831 #else
2832 #error &quot;The cacheFlush support is missing on this platform.&quot;
2833 #endif
2834     }
2835 
2836     // Assembler admin methods:
2837 
2838     static int jumpSizeDelta(JumpType jumpType, JumpLinkType jumpLinkType) { return JUMP_ENUM_SIZE(jumpType) - JUMP_ENUM_SIZE(jumpLinkType); }
2839 
2840     static ALWAYS_INLINE bool linkRecordSourceComparator(const LinkRecord&amp; a, const LinkRecord&amp; b)
2841     {
2842         return a.from() &lt; b.from();
2843     }
2844 
2845     static bool canCompact(JumpType jumpType)
2846     {
2847         // Fixed jumps cannot be compacted
2848         return (jumpType == JumpNoCondition) || (jumpType == JumpCondition) || (jumpType == JumpCompareAndBranch) || (jumpType == JumpTestBit);
2849     }
2850 
2851     static JumpLinkType computeJumpType(JumpType jumpType, const uint8_t* from, const uint8_t* to)
2852     {
2853         switch (jumpType) {
2854         case JumpFixed:
2855             return LinkInvalid;
2856         case JumpNoConditionFixedSize:
2857             return LinkJumpNoCondition;
2858         case JumpConditionFixedSize:
2859             return LinkJumpCondition;
2860         case JumpCompareAndBranchFixedSize:
2861             return LinkJumpCompareAndBranch;
2862         case JumpTestBitFixedSize:
2863             return LinkJumpTestBit;
2864         case JumpNoCondition:
2865             return LinkJumpNoCondition;
2866         case JumpCondition: {
2867             ASSERT(is4ByteAligned(from));
2868             ASSERT(is4ByteAligned(to));
2869             intptr_t relative = reinterpret_cast&lt;intptr_t&gt;(to) - (reinterpret_cast&lt;intptr_t&gt;(from));
2870 
2871             if (isInt&lt;21&gt;(relative))
2872                 return LinkJumpConditionDirect;
2873 
2874             return LinkJumpCondition;
2875             }
2876         case JumpCompareAndBranch:  {
2877             ASSERT(is4ByteAligned(from));
2878             ASSERT(is4ByteAligned(to));
2879             intptr_t relative = reinterpret_cast&lt;intptr_t&gt;(to) - (reinterpret_cast&lt;intptr_t&gt;(from));
2880 
2881             if (isInt&lt;21&gt;(relative))
2882                 return LinkJumpCompareAndBranchDirect;
2883 
2884             return LinkJumpCompareAndBranch;
2885         }
2886         case JumpTestBit:   {
2887             ASSERT(is4ByteAligned(from));
2888             ASSERT(is4ByteAligned(to));
2889             intptr_t relative = reinterpret_cast&lt;intptr_t&gt;(to) - (reinterpret_cast&lt;intptr_t&gt;(from));
2890 
2891             if (isInt&lt;14&gt;(relative))
2892                 return LinkJumpTestBitDirect;
2893 
2894             return LinkJumpTestBit;
2895         }
2896         default:
2897             ASSERT_NOT_REACHED();
2898         }
2899 
2900         return LinkJumpNoCondition;
2901     }
2902 
2903     static JumpLinkType computeJumpType(LinkRecord&amp; record, const uint8_t* from, const uint8_t* to)
2904     {
2905         JumpLinkType linkType = computeJumpType(record.type(), from, to);
2906         record.setLinkType(linkType);
2907         return linkType;
2908     }
2909 
2910     Vector&lt;LinkRecord, 0, UnsafeVectorOverflow&gt;&amp; jumpsToLink()
2911     {
2912         std::sort(m_jumpsToLink.begin(), m_jumpsToLink.end(), linkRecordSourceComparator);
2913         return m_jumpsToLink;
2914     }
2915 
2916     template&lt;CopyFunction copy&gt;
2917     static void ALWAYS_INLINE link(LinkRecord&amp; record, uint8_t* from, const uint8_t* fromInstruction8, uint8_t* to)
2918     {
2919         const int* fromInstruction = reinterpret_cast&lt;const int*&gt;(fromInstruction8);
2920         switch (record.linkType()) {
2921         case LinkJumpNoCondition:
2922             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);
2923             break;
2924         case LinkJumpConditionDirect:
2925             linkConditionalBranch&lt;DirectBranch, copy&gt;(record.condition(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);
2926             break;
2927         case LinkJumpCondition:
2928             linkConditionalBranch&lt;IndirectBranch, copy&gt;(record.condition(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to);
2929             break;
2930         case LinkJumpCompareAndBranchDirect:
2931             linkCompareAndBranch&lt;DirectBranch, copy&gt;(record.condition(), record.is64Bit(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);
2932             break;
2933         case LinkJumpCompareAndBranch:
2934             linkCompareAndBranch&lt;IndirectBranch, copy&gt;(record.condition(), record.is64Bit(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to);
2935             break;
2936         case LinkJumpTestBitDirect:
2937             linkTestAndBranch&lt;DirectBranch, copy&gt;(record.condition(), record.bitNumber(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);
2938             break;
2939         case LinkJumpTestBit:
2940             linkTestAndBranch&lt;IndirectBranch, copy&gt;(record.condition(), record.bitNumber(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to);
2941             break;
2942         default:
2943             ASSERT_NOT_REACHED();
2944             break;
2945         }
2946     }
2947 
2948 protected:
2949     template&lt;Datasize size&gt;
2950     static bool checkMovk(int insn, int _hw, RegisterID _rd)
2951     {
2952         Datasize sf;
2953         MoveWideOp opc;
2954         int hw;
2955         uint16_t imm16;
2956         RegisterID rd;
2957         bool expected = disassembleMoveWideImediate(&amp;insn, sf, opc, hw, imm16, rd);
2958 
2959         return expected
2960             &amp;&amp; sf == size
2961             &amp;&amp; opc == MoveWideOp_K
2962             &amp;&amp; hw == _hw
2963             &amp;&amp; rd == _rd;
2964     }
2965 
2966     static void linkPointer(int* address, void* valuePtr, bool flush = false)
2967     {
2968         Datasize sf;
2969         MoveWideOp opc;
2970         int hw;
2971         uint16_t imm16;
2972         RegisterID rd;
2973         bool expected = disassembleMoveWideImediate(address, sf, opc, hw, imm16, rd);
2974         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_Z &amp;&amp; !hw);
2975         ASSERT(checkMovk&lt;Datasize_64&gt;(address[1], 1, rd));
2976         ASSERT(checkMovk&lt;Datasize_64&gt;(address[2], 2, rd));
2977 
2978         setPointer(address, valuePtr, rd, flush);
2979     }
2980 
2981     template&lt;BranchType type, CopyFunction copy = performJITMemcpy&gt;
2982     static void linkJumpOrCall(int* from, const int* fromInstruction, void* to)
2983     {
2984         static_assert(type == BranchType_JMP || type == BranchType_CALL, &quot;&quot;);
2985 
2986         bool link;
2987         int imm26;
2988         bool isUnconditionalBranchImmediateOrNop = disassembleUnconditionalBranchImmediate(from, link, imm26) || disassembleNop(from);
2989 
2990         ASSERT_UNUSED(isUnconditionalBranchImmediateOrNop, isUnconditionalBranchImmediateOrNop);
2991         constexpr bool isCall = (type == BranchType_CALL);
2992         ASSERT_UNUSED(isCall, (link == isCall) || disassembleNop(from));
2993         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
2994         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
2995         assertIsNotTagged(to);
2996         assertIsNotTagged(fromInstruction);
2997         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
2998         ASSERT(static_cast&lt;int&gt;(offset) == offset);
2999 
3000         int insn = unconditionalBranchImmediate(isCall, static_cast&lt;int&gt;(offset));
3001         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3002         copy(from, &amp;insn, sizeof(int));
3003     }
3004 
3005     template&lt;BranchTargetType type, CopyFunction copy = performJITMemcpy&gt;
3006     static void linkCompareAndBranch(Condition condition, bool is64Bit, RegisterID rt, int* from, const int* fromInstruction, void* to)
3007     {
3008         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3009         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3010         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3011         ASSERT(isInt&lt;26&gt;(offset));
3012 
3013         bool useDirect = isInt&lt;19&gt;(offset);
3014         ASSERT(type == IndirectBranch || useDirect);
3015 
3016         if (useDirect || type == DirectBranch) {
3017             int insn = compareAndBranchImmediate(is64Bit ? Datasize_64 : Datasize_32, condition == ConditionNE, static_cast&lt;int&gt;(offset), rt);
3018             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3019             copy(from, &amp;insn, sizeof(int));
3020             if (type == IndirectBranch) {
3021                 insn = nopPseudo();
3022                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3023                 copy(from + 1, &amp;insn, sizeof(int));
3024             }
3025         } else {
3026             int insn = compareAndBranchImmediate(is64Bit ? Datasize_64 : Datasize_32, invert(condition) == ConditionNE, 2, rt);
3027             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3028             copy(from, &amp;insn, sizeof(int));
3029             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(from + 1, fromInstruction + 1, to);
3030         }
3031     }
3032 
3033     template&lt;BranchTargetType type, CopyFunction copy = performJITMemcpy&gt;
3034     static void linkConditionalBranch(Condition condition, int* from, const int* fromInstruction, void* to)
3035     {
3036         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3037         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3038         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3039         ASSERT(isInt&lt;26&gt;(offset));
3040 
3041         bool useDirect = isInt&lt;19&gt;(offset);
3042         ASSERT(type == IndirectBranch || useDirect);
3043 
3044         if (useDirect || type == DirectBranch) {
3045             int insn = conditionalBranchImmediate(static_cast&lt;int&gt;(offset), condition);
3046             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3047             copy(from, &amp;insn, sizeof(int));
3048             if (type == IndirectBranch) {
3049                 insn = nopPseudo();
3050                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3051                 copy(from + 1, &amp;insn, sizeof(int));
3052             }
3053         } else {
3054             int insn = conditionalBranchImmediate(2, invert(condition));
3055             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3056             copy(from, &amp;insn, sizeof(int));
3057             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(from + 1, fromInstruction + 1, to);
3058         }
3059     }
3060 
3061     template&lt;BranchTargetType type, CopyFunction copy = performJITMemcpy&gt;
3062     static void linkTestAndBranch(Condition condition, unsigned bitNumber, RegisterID rt, int* from, const int* fromInstruction, void* to)
3063     {
3064         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3065         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3066         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3067         ASSERT(static_cast&lt;int&gt;(offset) == offset);
3068         ASSERT(isInt&lt;26&gt;(offset));
3069 
3070         bool useDirect = isInt&lt;14&gt;(offset);
3071         ASSERT(type == IndirectBranch || useDirect);
3072 
3073         if (useDirect || type == DirectBranch) {
3074             int insn = testAndBranchImmediate(condition == ConditionNE, static_cast&lt;int&gt;(bitNumber), static_cast&lt;int&gt;(offset), rt);
3075             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3076             copy(from, &amp;insn, sizeof(int));
3077             if (type == IndirectBranch) {
3078                 insn = nopPseudo();
3079                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3080                 copy(from + 1, &amp;insn, sizeof(int));
3081             }
3082         } else {
3083             int insn = testAndBranchImmediate(invert(condition) == ConditionNE, static_cast&lt;int&gt;(bitNumber), 2, rt);
3084             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3085             copy(from, &amp;insn, sizeof(int));
3086             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(from + 1, fromInstruction + 1, to);
3087         }
3088     }
3089 
3090     template&lt;BranchType type&gt;
3091     static void relinkJumpOrCall(int* from, const int* fromInstruction, void* to)
3092     {
3093         static_assert(type == BranchType_JMP || type == BranchType_CALL, &quot;&quot;);
3094         if ((type == BranchType_JMP) &amp;&amp; disassembleNop(from)) {
3095             unsigned op01;
3096             int imm19;
3097             Condition condition;
3098             bool isConditionalBranchImmediate = disassembleConditionalBranchImmediate(from - 1, op01, imm19, condition);
3099 
3100             if (isConditionalBranchImmediate) {
3101                 ASSERT_UNUSED(op01, !op01);
3102                 ASSERT(type == BranchType_JMP);
3103 
3104                 if (imm19 == 8)
3105                     condition = invert(condition);
3106 
3107                 linkConditionalBranch&lt;IndirectBranch&gt;(condition, from - 1, fromInstruction - 1, to);
3108                 return;
3109             }
3110 
3111             Datasize opSize;
3112             bool op;
3113             RegisterID rt;
3114             bool isCompareAndBranchImmediate = disassembleCompareAndBranchImmediate(from - 1, opSize, op, imm19, rt);
3115 
3116             if (isCompareAndBranchImmediate) {
3117                 if (imm19 == 8)
3118                     op = !op;
3119 
3120                 linkCompareAndBranch&lt;IndirectBranch&gt;(op ? ConditionNE : ConditionEQ, opSize == Datasize_64, rt, from - 1, fromInstruction - 1, to);
3121                 return;
3122             }
3123 
3124             int imm14;
3125             unsigned bitNumber;
3126             bool isTestAndBranchImmediate = disassembleTestAndBranchImmediate(from - 1, op, bitNumber, imm14, rt);
3127 
3128             if (isTestAndBranchImmediate) {
3129                 if (imm14 == 8)
3130                     op = !op;
3131 
3132                 linkTestAndBranch&lt;IndirectBranch&gt;(op ? ConditionNE : ConditionEQ, bitNumber, rt, from - 1, fromInstruction - 1, to);
3133                 return;
3134             }
3135         }
3136 
3137         linkJumpOrCall&lt;type&gt;(from, fromInstruction, to);
3138     }
3139 
3140     static int* addressOf(void* code, AssemblerLabel label)
3141     {
3142         return reinterpret_cast&lt;int*&gt;(static_cast&lt;char*&gt;(code) + label.m_offset);
3143     }
3144 
3145     static RegisterID disassembleXOrSp(int reg) { return reg == 31 ? ARM64Registers::sp : static_cast&lt;RegisterID&gt;(reg); }
3146     static RegisterID disassembleXOrZr(int reg) { return reg == 31 ? ARM64Registers::zr : static_cast&lt;RegisterID&gt;(reg); }
3147     static RegisterID disassembleXOrZrOrSp(bool useZr, int reg) { return reg == 31 ? (useZr ? ARM64Registers::zr : ARM64Registers::sp) : static_cast&lt;RegisterID&gt;(reg); }
3148 
3149     static bool disassembleAddSubtractImmediate(void* address, Datasize&amp; sf, AddOp&amp; op, SetFlags&amp; S, int&amp; shift, int&amp; imm12, RegisterID&amp; rn, RegisterID&amp; rd)
3150     {
3151         int insn = *static_cast&lt;int*&gt;(address);
3152         sf = static_cast&lt;Datasize&gt;((insn &gt;&gt; 31) &amp; 1);
3153         op = static_cast&lt;AddOp&gt;((insn &gt;&gt; 30) &amp; 1);
3154         S = static_cast&lt;SetFlags&gt;((insn &gt;&gt; 29) &amp; 1);
3155         shift = (insn &gt;&gt; 22) &amp; 3;
3156         imm12 = (insn &gt;&gt; 10) &amp; 0x3ff;
3157         rn = disassembleXOrSp((insn &gt;&gt; 5) &amp; 0x1f);
3158         rd = disassembleXOrZrOrSp(S, insn &amp; 0x1f);
3159         return (insn &amp; 0x1f000000) == 0x11000000;
3160     }
3161 
3162     static bool disassembleLoadStoreRegisterUnsignedImmediate(void* address, MemOpSize&amp; size, bool&amp; V, MemOp&amp; opc, int&amp; imm12, RegisterID&amp; rn, RegisterID&amp; rt)
3163     {
3164         int insn = *static_cast&lt;int*&gt;(address);
3165         size = static_cast&lt;MemOpSize&gt;((insn &gt;&gt; 30) &amp; 3);
3166         V = (insn &gt;&gt; 26) &amp; 1;
3167         opc = static_cast&lt;MemOp&gt;((insn &gt;&gt; 22) &amp; 3);
3168         imm12 = (insn &gt;&gt; 10) &amp; 0xfff;
3169         rn = disassembleXOrSp((insn &gt;&gt; 5) &amp; 0x1f);
3170         rt = disassembleXOrZr(insn &amp; 0x1f);
3171         return (insn &amp; 0x3b000000) == 0x39000000;
3172     }
3173 
3174     static bool disassembleMoveWideImediate(void* address, Datasize&amp; sf, MoveWideOp&amp; opc, int&amp; hw, uint16_t&amp; imm16, RegisterID&amp; rd)
3175     {
3176         int insn = *static_cast&lt;int*&gt;(address);
3177         sf = static_cast&lt;Datasize&gt;((insn &gt;&gt; 31) &amp; 1);
3178         opc = static_cast&lt;MoveWideOp&gt;((insn &gt;&gt; 29) &amp; 3);
3179         hw = (insn &gt;&gt; 21) &amp; 3;
3180         imm16 = insn &gt;&gt; 5;
3181         rd = disassembleXOrZr(insn &amp; 0x1f);
3182         return (insn &amp; 0x1f800000) == 0x12800000;
3183     }
3184 
3185     static bool disassembleNop(void* address)
3186     {
3187         unsigned insn = *static_cast&lt;unsigned*&gt;(address);
3188         return insn == 0xd503201f;
3189     }
3190 
3191     static bool disassembleCompareAndBranchImmediate(void* address, Datasize&amp; sf, bool&amp; op, int&amp; imm19, RegisterID&amp; rt)
3192     {
3193         int insn = *static_cast&lt;int*&gt;(address);
3194         sf = static_cast&lt;Datasize&gt;((insn &gt;&gt; 31) &amp; 1);
3195         op = (insn &gt;&gt; 24) &amp; 0x1;
3196         imm19 = (insn &lt;&lt; 8) &gt;&gt; 13;
3197         rt = static_cast&lt;RegisterID&gt;(insn &amp; 0x1f);
3198         return (insn &amp; 0x7e000000) == 0x34000000;
3199 
3200     }
3201 
3202     static bool disassembleConditionalBranchImmediate(void* address, unsigned&amp; op01, int&amp; imm19, Condition &amp;condition)
3203     {
3204         int insn = *static_cast&lt;int*&gt;(address);
3205         op01 = ((insn &gt;&gt; 23) &amp; 0x2) | ((insn &gt;&gt; 4) &amp; 0x1);
3206         imm19 = (insn &lt;&lt; 8) &gt;&gt; 13;
3207         condition = static_cast&lt;Condition&gt;(insn &amp; 0xf);
3208         return (insn &amp; 0xfe000000) == 0x54000000;
3209     }
3210 
3211     static bool disassembleTestAndBranchImmediate(void* address, bool&amp; op, unsigned&amp; bitNumber, int&amp; imm14, RegisterID&amp; rt)
3212     {
3213         int insn = *static_cast&lt;int*&gt;(address);
3214         op = (insn &gt;&gt; 24) &amp; 0x1;
3215         imm14 = (insn &lt;&lt; 13) &gt;&gt; 18;
3216         bitNumber = static_cast&lt;unsigned&gt;((((insn &gt;&gt; 26) &amp; 0x20)) | ((insn &gt;&gt; 19) &amp; 0x1f));
3217         rt = static_cast&lt;RegisterID&gt;(insn &amp; 0x1f);
3218         return (insn &amp; 0x7e000000) == 0x36000000;
3219 
3220     }
3221 
3222     static bool disassembleUnconditionalBranchImmediate(void* address, bool&amp; op, int&amp; imm26)
3223     {
3224         int insn = *static_cast&lt;int*&gt;(address);
3225         op = (insn &gt;&gt; 31) &amp; 1;
3226         imm26 = (insn &lt;&lt; 6) &gt;&gt; 6;
3227         return (insn &amp; 0x7c000000) == 0x14000000;
3228     }
3229 
3230     static int xOrSp(RegisterID reg)
3231     {
3232         ASSERT(!isZr(reg));
3233         ASSERT(!isDarwin() || reg != ARM64Registers::x18);
3234         return reg;
3235     }
3236     static int xOrZr(RegisterID reg)
3237     {
3238         ASSERT(!isSp(reg));
3239         ASSERT(!isDarwin() || reg != ARM64Registers::x18);
3240         return reg &amp; 31;
3241     }
3242     static FPRegisterID xOrZrAsFPR(RegisterID reg) { return static_cast&lt;FPRegisterID&gt;(xOrZr(reg)); }
3243     static int xOrZrOrSp(bool useZr, RegisterID reg) { return useZr ? xOrZr(reg) : xOrSp(reg); }
3244 
3245     ALWAYS_INLINE void insn(int instruction)
3246     {
3247         m_buffer.putInt(instruction);
3248     }
3249 
3250     ALWAYS_INLINE static int addSubtractExtendedRegister(Datasize sf, AddOp op, SetFlags S, RegisterID rm, ExtendType option, int imm3, RegisterID rn, RegisterID rd)
3251     {
3252         ASSERT(imm3 &lt; 5);
3253         // The only allocated values for opt is 0.
3254         const int opt = 0;
3255         return (0x0b200000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | opt &lt;&lt; 22 | xOrZr(rm) &lt;&lt; 16 | option &lt;&lt; 13 | (imm3 &amp; 0x7) &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | xOrZrOrSp(S, rd));
3256     }
3257 
3258     ALWAYS_INLINE static int addSubtractImmediate(Datasize sf, AddOp op, SetFlags S, int shift, int imm12, RegisterID rn, RegisterID rd)
3259     {
3260         ASSERT(shift &lt; 2);
3261         ASSERT(isUInt12(imm12));
3262         return (0x11000000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | shift &lt;&lt; 22 | (imm12 &amp; 0xfff) &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | xOrZrOrSp(S, rd));
3263     }
3264 
3265     ALWAYS_INLINE static int addSubtractShiftedRegister(Datasize sf, AddOp op, SetFlags S, ShiftType shift, RegisterID rm, int imm6, RegisterID rn, RegisterID rd)
3266     {
3267         ASSERT(shift &lt; 3);
3268         ASSERT(!(imm6 &amp; (sf ? ~63 : ~31)));
3269         return (0x0b000000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | shift &lt;&lt; 22 | xOrZr(rm) &lt;&lt; 16 | (imm6 &amp; 0x3f) &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3270     }
3271 
3272     ALWAYS_INLINE static int addSubtractWithCarry(Datasize sf, AddOp op, SetFlags S, RegisterID rm, RegisterID rn, RegisterID rd)
3273     {
3274         const int opcode2 = 0;
3275         return (0x1a000000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | xOrZr(rm) &lt;&lt; 16 | opcode2 &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3276     }
3277 
3278     ALWAYS_INLINE static int bitfield(Datasize sf, BitfieldOp opc, int immr, int imms, RegisterID rn, RegisterID rd)
3279     {
3280         ASSERT(immr &lt; (sf ? 64 : 32));
3281         ASSERT(imms &lt; (sf ? 64 : 32));
3282         const int N = sf;
3283         return (0x13000000 | sf &lt;&lt; 31 | opc &lt;&lt; 29 | N &lt;&lt; 22 | immr &lt;&lt; 16 | imms &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3284     }
3285 
3286     // &#39;op&#39; means negate
3287     ALWAYS_INLINE static int compareAndBranchImmediate(Datasize sf, bool op, int32_t imm19, RegisterID rt)
3288     {
3289         ASSERT(imm19 == (imm19 &lt;&lt; 13) &gt;&gt; 13);
3290         return (0x34000000 | sf &lt;&lt; 31 | op &lt;&lt; 24 | (imm19 &amp; 0x7ffff) &lt;&lt; 5 | xOrZr(rt));
3291     }
3292 
3293     ALWAYS_INLINE static int conditionalBranchImmediate(int32_t imm19, Condition cond)
3294     {
3295         ASSERT(imm19 == (imm19 &lt;&lt; 13) &gt;&gt; 13);
3296         ASSERT(!(cond &amp; ~15));
3297         // The only allocated values for o1 &amp; o0 are 0.
3298         const int o1 = 0;
3299         const int o0 = 0;
3300         return (0x54000000 | o1 &lt;&lt; 24 | (imm19 &amp; 0x7ffff) &lt;&lt; 5 | o0 &lt;&lt; 4 | cond);
3301     }
3302 
3303     ALWAYS_INLINE static int conditionalCompareImmediate(Datasize sf, AddOp op, int imm5, Condition cond, RegisterID rn, int nzcv)
3304     {
3305         ASSERT(!(imm5 &amp; ~0x1f));
3306         ASSERT(nzcv &lt; 16);
3307         const int S = 1;
3308         const int o2 = 0;
3309         const int o3 = 0;
3310         return (0x1a400800 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | (imm5 &amp; 0x1f) &lt;&lt; 16 | cond &lt;&lt; 12 | o2 &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | o3 &lt;&lt; 4 | nzcv);
3311     }
3312 
3313     ALWAYS_INLINE static int conditionalCompareRegister(Datasize sf, AddOp op, RegisterID rm, Condition cond, RegisterID rn, int nzcv)
3314     {
3315         ASSERT(nzcv &lt; 16);
3316         const int S = 1;
3317         const int o2 = 0;
3318         const int o3 = 0;
3319         return (0x1a400000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | xOrZr(rm) &lt;&lt; 16 | cond &lt;&lt; 12 | o2 &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | o3 &lt;&lt; 4 | nzcv);
3320     }
3321 
3322     // &#39;op&#39; means negate
3323     // &#39;op2&#39; means increment
3324     ALWAYS_INLINE static int conditionalSelect(Datasize sf, bool op, RegisterID rm, Condition cond, bool op2, RegisterID rn, RegisterID rd)
3325     {
3326         const int S = 0;
3327         return (0x1a800000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | xOrZr(rm) &lt;&lt; 16 | cond &lt;&lt; 12 | op2 &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3328     }
3329 
3330     ALWAYS_INLINE static int dataProcessing1Source(Datasize sf, DataOp1Source opcode, RegisterID rn, RegisterID rd)
3331     {
3332         const int S = 0;
3333         const int opcode2 = 0;
3334         return (0x5ac00000 | sf &lt;&lt; 31 | S &lt;&lt; 29 | opcode2 &lt;&lt; 16 | opcode &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3335     }
3336 
3337     ALWAYS_INLINE static int dataProcessing2Source(Datasize sf, RegisterID rm, DataOp2Source opcode, RegisterID rn, RegisterID rd)
3338     {
3339         const int S = 0;
3340         return (0x1ac00000 | sf &lt;&lt; 31 | S &lt;&lt; 29 | xOrZr(rm) &lt;&lt; 16 | opcode &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3341     }
3342 
3343     ALWAYS_INLINE static int dataProcessing3Source(Datasize sf, DataOp3Source opcode, RegisterID rm, RegisterID ra, RegisterID rn, RegisterID rd)
3344     {
3345         int op54 = opcode &gt;&gt; 4;
3346         int op31 = (opcode &gt;&gt; 1) &amp; 7;
3347         int op0 = opcode &amp; 1;
3348         return (0x1b000000 | sf &lt;&lt; 31 | op54 &lt;&lt; 29 | op31 &lt;&lt; 21 | xOrZr(rm) &lt;&lt; 16 | op0 &lt;&lt; 15 | xOrZr(ra) &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3349     }
3350 
3351     ALWAYS_INLINE static int excepnGeneration(ExcepnOp opc, uint16_t imm16, int LL)
3352     {
3353         ASSERT((opc == ExcepnOp_BREAKPOINT || opc == ExcepnOp_HALT) ? !LL : (LL &amp;&amp; (LL &lt; 4)));
3354         const int op2 = 0;
3355         return (0xd4000000 | opc &lt;&lt; 21 | imm16 &lt;&lt; 5 | op2 &lt;&lt; 2 | LL);
3356     }
3357     ALWAYS_INLINE static int excepnGenerationImmMask()
3358     {
3359         uint16_t imm16 =  std::numeric_limits&lt;uint16_t&gt;::max();
3360         return (static_cast&lt;int&gt;(imm16) &lt;&lt; 5);
3361     }
3362 
3363     ALWAYS_INLINE static int extract(Datasize sf, RegisterID rm, int imms, RegisterID rn, RegisterID rd)
3364     {
3365         ASSERT(imms &lt; (sf ? 64 : 32));
3366         const int op21 = 0;
3367         const int N = sf;
3368         const int o0 = 0;
3369         return (0x13800000 | sf &lt;&lt; 31 | op21 &lt;&lt; 29 | N &lt;&lt; 22 | o0 &lt;&lt; 21 | xOrZr(rm) &lt;&lt; 16 | imms &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3370     }
3371 
3372     ALWAYS_INLINE static int floatingPointCompare(Datasize type, FPRegisterID rm, FPRegisterID rn, FPCmpOp opcode2)
3373     {
3374         const int M = 0;
3375         const int S = 0;
3376         const int op = 0;
3377         return (0x1e202000 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | rm &lt;&lt; 16 | op &lt;&lt; 14 | rn &lt;&lt; 5 | opcode2);
3378     }
3379 
3380     ALWAYS_INLINE static int floatingPointConditionalCompare(Datasize type, FPRegisterID rm, Condition cond, FPRegisterID rn, FPCondCmpOp op, int nzcv)
3381     {
3382         ASSERT(nzcv &lt; 16);
3383         const int M = 0;
3384         const int S = 0;
3385         return (0x1e200400 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | rm &lt;&lt; 16 | cond &lt;&lt; 12 | rn &lt;&lt; 5 | op &lt;&lt; 4 | nzcv);
3386     }
3387 
3388     ALWAYS_INLINE static int floatingPointConditionalSelect(Datasize type, FPRegisterID rm, Condition cond, FPRegisterID rn, FPRegisterID rd)
3389     {
3390         const int M = 0;
3391         const int S = 0;
3392         return (0x1e200c00 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | rm &lt;&lt; 16 | cond &lt;&lt; 12 | rn &lt;&lt; 5 | rd);
3393     }
3394 
3395     ALWAYS_INLINE static int floatingPointImmediate(Datasize type, int imm8, FPRegisterID rd)
3396     {
3397         const int M = 0;
3398         const int S = 0;
3399         const int imm5 = 0;
3400         return (0x1e201000 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | (imm8 &amp; 0xff) &lt;&lt; 13 | imm5 &lt;&lt; 5 | rd);
3401     }
3402 
3403     ALWAYS_INLINE static int floatingPointIntegerConversions(Datasize sf, Datasize type, FPIntConvOp rmodeOpcode, FPRegisterID rn, FPRegisterID rd)
3404     {
3405         const int S = 0;
3406         return (0x1e200000 | sf &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | rmodeOpcode &lt;&lt; 16 | rn &lt;&lt; 5 | rd);
3407     }
3408 
3409     ALWAYS_INLINE static int floatingPointIntegerConversions(Datasize sf, Datasize type, FPIntConvOp rmodeOpcode, FPRegisterID rn, RegisterID rd)
3410     {
3411         return floatingPointIntegerConversions(sf, type, rmodeOpcode, rn, xOrZrAsFPR(rd));
3412     }
3413 
3414     ALWAYS_INLINE static int floatingPointIntegerConversions(Datasize sf, Datasize type, FPIntConvOp rmodeOpcode, RegisterID rn, FPRegisterID rd)
3415     {
3416         return floatingPointIntegerConversions(sf, type, rmodeOpcode, xOrZrAsFPR(rn), rd);
3417     }
3418 
3419     ALWAYS_INLINE static int floatingPointDataProcessing1Source(Datasize type, FPDataOp1Source opcode, FPRegisterID rn, FPRegisterID rd)
3420     {
3421         const int M = 0;
3422         const int S = 0;
3423         return (0x1e204000 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | opcode &lt;&lt; 15 | rn &lt;&lt; 5 | rd);
3424     }
3425 
3426     ALWAYS_INLINE static int floatingPointDataProcessing2Source(Datasize type, FPRegisterID rm, FPDataOp2Source opcode, FPRegisterID rn, FPRegisterID rd)
3427     {
3428         const int M = 0;
3429         const int S = 0;
3430         return (0x1e200800 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | rm &lt;&lt; 16 | opcode &lt;&lt; 12 | rn &lt;&lt; 5 | rd);
3431     }
3432 
3433     ALWAYS_INLINE static int vectorDataProcessingLogical(SIMD3SameLogical uAndSize, FPRegisterID vm, FPRegisterID vn, FPRegisterID vd)
3434     {
3435         const int Q = 0;
3436         return (0xe200400 | Q &lt;&lt; 30 | uAndSize &lt;&lt; 22 | vm &lt;&lt; 16 | SIMD_LogicalOp &lt;&lt; 11 | vn &lt;&lt; 5 | vd);
3437     }
3438 
3439     // &#39;o1&#39; means negate
3440     ALWAYS_INLINE static int floatingPointDataProcessing3Source(Datasize type, bool o1, FPRegisterID rm, AddOp o2, FPRegisterID ra, FPRegisterID rn, FPRegisterID rd)
3441     {
3442         const int M = 0;
3443         const int S = 0;
3444         return (0x1f000000 | M &lt;&lt; 31 | S &lt;&lt; 29 | type &lt;&lt; 22 | o1 &lt;&lt; 21 | rm &lt;&lt; 16 | o2 &lt;&lt; 15 | ra &lt;&lt; 10 | rn &lt;&lt; 5 | rd);
3445     }
3446 
3447     // &#39;V&#39; means vector
3448     ALWAYS_INLINE static int loadRegisterLiteral(LdrLiteralOp opc, bool V, int imm19, FPRegisterID rt)
3449     {
3450         ASSERT(isInt&lt;19&gt;(imm19));
3451         return (0x18000000 | opc &lt;&lt; 30 | V &lt;&lt; 26 | (imm19 &amp; 0x7ffff) &lt;&lt; 5 | rt);
3452     }
3453 
3454     ALWAYS_INLINE static int loadRegisterLiteral(LdrLiteralOp opc, bool V, int imm19, RegisterID rt)
3455     {
3456         return loadRegisterLiteral(opc, V, imm19, xOrZrAsFPR(rt));
3457     }
3458 
3459     // &#39;V&#39; means vector
3460     ALWAYS_INLINE static int loadStoreRegisterPostIndex(MemOpSize size, bool V, MemOp opc, int imm9, RegisterID rn, FPRegisterID rt)
3461     {
3462         ASSERT(!(size &amp;&amp; V &amp;&amp; (opc &amp; 2))); // Maximum vector size is 128 bits.
3463         ASSERT(!((size &amp; 2) &amp;&amp; !V &amp;&amp; (opc == 3))); // signed 32-bit load must be extending from 8/16 bits.
3464         ASSERT(isInt9(imm9));
3465         return (0x38000400 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm9 &amp; 0x1ff) &lt;&lt; 12 | xOrSp(rn) &lt;&lt; 5 | rt);
3466     }
3467 
3468     ALWAYS_INLINE static int loadStoreRegisterPostIndex(MemOpSize size, bool V, MemOp opc, int imm9, RegisterID rn, RegisterID rt)
3469     {
3470         return loadStoreRegisterPostIndex(size, V, opc, imm9, rn, xOrZrAsFPR(rt));
3471     }
3472 
3473     // &#39;V&#39; means vector
3474     ALWAYS_INLINE static int loadStoreRegisterPairPostIndex(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, FPRegisterID rt, FPRegisterID rt2)
3475     {
3476         ASSERT(size &lt; 3);
3477         ASSERT(opc == (opc &amp; 1)); // Only load or store, load signed 64 is handled via size.
3478         ASSERT(V || (size != MemPairOp_LoadSigned_32) || (opc == MemOp_LOAD)); // There isn&#39;t an integer store signed.
3479         unsigned immedShiftAmount = memPairOffsetShift(V, size);
3480         int imm7 = immediate &gt;&gt; immedShiftAmount;
3481         ASSERT((imm7 &lt;&lt; immedShiftAmount) == immediate &amp;&amp; isInt&lt;7&gt;(imm7));
3482         return (0x28800000 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm7 &amp; 0x7f) &lt;&lt; 15 | rt2 &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | rt);
3483     }
3484 
3485     ALWAYS_INLINE static int loadStoreRegisterPairPostIndex(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, RegisterID rt, RegisterID rt2)
3486     {
3487         return loadStoreRegisterPairPostIndex(size, V, opc, immediate, rn, xOrZrAsFPR(rt), xOrZrAsFPR(rt2));
3488     }
3489 
3490     // &#39;V&#39; means vector
3491     ALWAYS_INLINE static int loadStoreRegisterPreIndex(MemOpSize size, bool V, MemOp opc, int imm9, RegisterID rn, FPRegisterID rt)
3492     {
3493         ASSERT(!(size &amp;&amp; V &amp;&amp; (opc &amp; 2))); // Maximum vector size is 128 bits.
3494         ASSERT(!((size &amp; 2) &amp;&amp; !V &amp;&amp; (opc == 3))); // signed 32-bit load must be extending from 8/16 bits.
3495         ASSERT(isInt9(imm9));
3496         return (0x38000c00 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm9 &amp; 0x1ff) &lt;&lt; 12 | xOrSp(rn) &lt;&lt; 5 | rt);
3497     }
3498 
3499     ALWAYS_INLINE static int loadStoreRegisterPreIndex(MemOpSize size, bool V, MemOp opc, int imm9, RegisterID rn, RegisterID rt)
3500     {
3501         return loadStoreRegisterPreIndex(size, V, opc, imm9, rn, xOrZrAsFPR(rt));
3502     }
3503 
3504     // &#39;V&#39; means vector
3505     ALWAYS_INLINE static int loadStoreRegisterPairPreIndex(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, FPRegisterID rt, FPRegisterID rt2)
3506     {
3507         ASSERT(size &lt; 3);
3508         ASSERT(opc == (opc &amp; 1)); // Only load or store, load signed 64 is handled via size.
3509         ASSERT(V || (size != MemPairOp_LoadSigned_32) || (opc == MemOp_LOAD)); // There isn&#39;t an integer store signed.
3510         unsigned immedShiftAmount = memPairOffsetShift(V, size);
3511         int imm7 = immediate &gt;&gt; immedShiftAmount;
3512         ASSERT((imm7 &lt;&lt; immedShiftAmount) == immediate &amp;&amp; isInt&lt;7&gt;(imm7));
3513         return (0x29800000 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm7 &amp; 0x7f) &lt;&lt; 15 | rt2 &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | rt);
3514     }
3515 
3516     ALWAYS_INLINE static int loadStoreRegisterPairPreIndex(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, RegisterID rt, RegisterID rt2)
3517     {
3518         return loadStoreRegisterPairPreIndex(size, V, opc, immediate, rn, xOrZrAsFPR(rt), xOrZrAsFPR(rt2));
3519     }
3520 
3521     // &#39;V&#39; means vector
3522     ALWAYS_INLINE static int loadStoreRegisterPairOffset(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, FPRegisterID rt, FPRegisterID rt2)
3523     {
3524         ASSERT(size &lt; 3);
3525         ASSERT(opc == (opc &amp; 1)); // Only load or store, load signed 64 is handled via size.
3526         ASSERT(V || (size != MemPairOp_LoadSigned_32) || (opc == MemOp_LOAD)); // There isn&#39;t an integer store signed.
3527         unsigned immedShiftAmount = memPairOffsetShift(V, size);
3528         int imm7 = immediate &gt;&gt; immedShiftAmount;
3529         ASSERT((imm7 &lt;&lt; immedShiftAmount) == immediate &amp;&amp; isInt&lt;7&gt;(imm7));
3530         return (0x29000000 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm7 &amp; 0x7f) &lt;&lt; 15 | rt2 &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | rt);
3531     }
3532 
3533     ALWAYS_INLINE static int loadStoreRegisterPairOffset(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, RegisterID rt, RegisterID rt2)
3534     {
3535         return loadStoreRegisterPairOffset(size, V, opc, immediate, rn, xOrZrAsFPR(rt), xOrZrAsFPR(rt2));
3536     }
3537 
3538     // &#39;V&#39; means vector
3539     ALWAYS_INLINE static int loadStoreRegisterPairNonTemporal(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, FPRegisterID rt, FPRegisterID rt2)
3540     {
3541         ASSERT(size &lt; 3);
3542         ASSERT(opc == (opc &amp; 1)); // Only load or store, load signed 64 is handled via size.
3543         ASSERT(V || (size != MemPairOp_LoadSigned_32) || (opc == MemOp_LOAD)); // There isn&#39;t an integer store signed.
3544         unsigned immedShiftAmount = memPairOffsetShift(V, size);
3545         int imm7 = immediate &gt;&gt; immedShiftAmount;
3546         ASSERT((imm7 &lt;&lt; immedShiftAmount) == immediate &amp;&amp; isInt&lt;7&gt;(imm7));
3547         return (0x28000000 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm7 &amp; 0x7f) &lt;&lt; 15 | rt2 &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | rt);
3548     }
3549 
3550     ALWAYS_INLINE static int loadStoreRegisterPairNonTemporal(MemPairOpSize size, bool V, MemOp opc, int immediate, RegisterID rn, RegisterID rt, RegisterID rt2)
3551     {
3552         return loadStoreRegisterPairNonTemporal(size, V, opc, immediate, rn, xOrZrAsFPR(rt), xOrZrAsFPR(rt2));
3553     }
3554 
3555     // &#39;V&#39; means vector
3556     // &#39;S&#39; means shift rm
3557     ALWAYS_INLINE static int loadStoreRegisterRegisterOffset(MemOpSize size, bool V, MemOp opc, RegisterID rm, ExtendType option, bool S, RegisterID rn, FPRegisterID rt)
3558     {
3559         ASSERT(!(size &amp;&amp; V &amp;&amp; (opc &amp; 2))); // Maximum vector size is 128 bits.
3560         ASSERT(!((size &amp; 2) &amp;&amp; !V &amp;&amp; (opc == 3))); // signed 32-bit load must be extending from 8/16 bits.
3561         ASSERT(option &amp; 2); // The ExtendType for the address must be 32/64 bit, signed or unsigned - not 8/16bit.
3562         return (0x38200800 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | xOrZr(rm) &lt;&lt; 16 | option &lt;&lt; 13 | S &lt;&lt; 12 | xOrSp(rn) &lt;&lt; 5 | rt);
3563     }
3564 
3565     ALWAYS_INLINE static int loadStoreRegisterRegisterOffset(MemOpSize size, bool V, MemOp opc, RegisterID rm, ExtendType option, bool S, RegisterID rn, RegisterID rt)
3566     {
3567         return loadStoreRegisterRegisterOffset(size, V, opc, rm, option, S, rn, xOrZrAsFPR(rt));
3568     }
3569 
3570     // &#39;V&#39; means vector
3571     ALWAYS_INLINE static int loadStoreRegisterUnscaledImmediate(MemOpSize size, bool V, MemOp opc, int imm9, RegisterID rn, FPRegisterID rt)
3572     {
3573         ASSERT(!(size &amp;&amp; V &amp;&amp; (opc &amp; 2))); // Maximum vector size is 128 bits.
3574         ASSERT(!((size &amp; 2) &amp;&amp; !V &amp;&amp; (opc == 3))); // signed 32-bit load must be extending from 8/16 bits.
3575         ASSERT(isInt9(imm9));
3576         return (0x38000000 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm9 &amp; 0x1ff) &lt;&lt; 12 | xOrSp(rn) &lt;&lt; 5 | rt);
3577     }
3578 
3579     ALWAYS_INLINE static int loadStoreRegisterUnscaledImmediate(MemOpSize size, bool V, MemOp opc, int imm9, RegisterID rn, RegisterID rt)
3580     {
3581         ASSERT(isInt9(imm9));
3582         return loadStoreRegisterUnscaledImmediate(size, V, opc, imm9, rn, xOrZrAsFPR(rt));
3583     }
3584 
3585     // &#39;V&#39; means vector
3586     ALWAYS_INLINE static int loadStoreRegisterUnsignedImmediate(MemOpSize size, bool V, MemOp opc, int imm12, RegisterID rn, FPRegisterID rt)
3587     {
3588         ASSERT(!(size &amp;&amp; V &amp;&amp; (opc &amp; 2))); // Maximum vector size is 128 bits.
3589         ASSERT(!((size &amp; 2) &amp;&amp; !V &amp;&amp; (opc == 3))); // signed 32-bit load must be extending from 8/16 bits.
3590         ASSERT(isUInt12(imm12));
3591         return (0x39000000 | size &lt;&lt; 30 | V &lt;&lt; 26 | opc &lt;&lt; 22 | (imm12 &amp; 0xfff) &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | rt);
3592     }
3593 
3594     ALWAYS_INLINE static int loadStoreRegisterUnsignedImmediate(MemOpSize size, bool V, MemOp opc, int imm12, RegisterID rn, RegisterID rt)
3595     {
3596         return loadStoreRegisterUnsignedImmediate(size, V, opc, imm12, rn, xOrZrAsFPR(rt));
3597     }
3598 
3599     ALWAYS_INLINE static int logicalImmediate(Datasize sf, LogicalOp opc, int N_immr_imms, RegisterID rn, RegisterID rd)
3600     {
3601         ASSERT(!(N_immr_imms &amp; (sf ? ~0x1fff : ~0xfff)));
3602         return (0x12000000 | sf &lt;&lt; 31 | opc &lt;&lt; 29 | N_immr_imms &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZrOrSp(opc == LogicalOp_ANDS, rd));
3603     }
3604 
3605     // &#39;N&#39; means negate rm
3606     ALWAYS_INLINE static int logicalShiftedRegister(Datasize sf, LogicalOp opc, ShiftType shift, bool N, RegisterID rm, int imm6, RegisterID rn, RegisterID rd)
3607     {
3608         ASSERT(!(imm6 &amp; (sf ? ~63 : ~31)));
3609         return (0x0a000000 | sf &lt;&lt; 31 | opc &lt;&lt; 29 | shift &lt;&lt; 22 | N &lt;&lt; 21 | xOrZr(rm) &lt;&lt; 16 | (imm6 &amp; 0x3f) &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | xOrZr(rd));
3610     }
3611 
3612     ALWAYS_INLINE static int moveWideImediate(Datasize sf, MoveWideOp opc, int hw, uint16_t imm16, RegisterID rd)
3613     {
3614         ASSERT(hw &lt; (sf ? 4 : 2));
3615         return (0x12800000 | sf &lt;&lt; 31 | opc &lt;&lt; 29 | hw &lt;&lt; 21 | (int)imm16 &lt;&lt; 5 | xOrZr(rd));
3616     }
3617 
3618     // &#39;op&#39; means link
3619     ALWAYS_INLINE static int unconditionalBranchImmediate(bool op, int32_t imm26)
3620     {
3621         ASSERT(imm26 == (imm26 &lt;&lt; 6) &gt;&gt; 6);
3622         return (0x14000000 | op &lt;&lt; 31 | (imm26 &amp; 0x3ffffff));
3623     }
3624 
3625     // &#39;op&#39; means page
3626     ALWAYS_INLINE static int pcRelative(bool op, int32_t imm21, RegisterID rd)
3627     {
3628         ASSERT(imm21 == (imm21 &lt;&lt; 11) &gt;&gt; 11);
3629         int32_t immlo = imm21 &amp; 3;
3630         int32_t immhi = (imm21 &gt;&gt; 2) &amp; 0x7ffff;
3631         return (0x10000000 | op &lt;&lt; 31 | immlo &lt;&lt; 29 | immhi &lt;&lt; 5 | xOrZr(rd));
3632     }
3633 
3634     ALWAYS_INLINE static int system(bool L, int op0, int op1, int crn, int crm, int op2, RegisterID rt)
3635     {
3636         return (0xd5000000 | L &lt;&lt; 21 | op0 &lt;&lt; 19 | op1 &lt;&lt; 16 | crn &lt;&lt; 12 | crm &lt;&lt; 8 | op2 &lt;&lt; 5 | xOrZr(rt));
3637     }
3638 
3639     ALWAYS_INLINE static int hintPseudo(int imm)
3640     {
3641         ASSERT(!(imm &amp; ~0x7f));
3642         return system(0, 0, 3, 2, (imm &gt;&gt; 3) &amp; 0xf, imm &amp; 0x7, ARM64Registers::zr);
3643     }
3644 
3645     ALWAYS_INLINE static int nopPseudo()
3646     {
3647         return hintPseudo(0);
3648     }
3649 
3650     ALWAYS_INLINE static int dataCacheZeroVirtualAddress(RegisterID rt)
3651     {
3652         return system(0, 1, 0x3, 0x7, 0x4, 0x1, rt);
3653     }
3654 
3655     // &#39;op&#39; means negate
3656     ALWAYS_INLINE static int testAndBranchImmediate(bool op, int b50, int imm14, RegisterID rt)
3657     {
3658         ASSERT(!(b50 &amp; ~0x3f));
3659         ASSERT(imm14 == (imm14 &lt;&lt; 18) &gt;&gt; 18);
3660         int b5 = b50 &gt;&gt; 5;
3661         int b40 = b50 &amp; 0x1f;
3662         return (0x36000000 | b5 &lt;&lt; 31 | op &lt;&lt; 24 | b40 &lt;&lt; 19 | (imm14 &amp; 0x3fff) &lt;&lt; 5 | xOrZr(rt));
3663     }
3664 
3665     ALWAYS_INLINE static int unconditionalBranchRegister(BranchType opc, RegisterID rn)
3666     {
3667         // The only allocated values for op2 is 0x1f, for op3 &amp; op4 are 0.
3668         const int op2 = 0x1f;
3669         const int op3 = 0;
3670         const int op4 = 0;
3671         return (0xd6000000 | opc &lt;&lt; 21 | op2 &lt;&lt; 16 | op3 &lt;&lt; 10 | xOrZr(rn) &lt;&lt; 5 | op4);
3672     }
3673 
3674     static int exoticLoad(MemOpSize size, ExoticLoadFence fence, ExoticLoadAtomic atomic, RegisterID dst, RegisterID src)
3675     {
3676         return 0x085f7c00 | size &lt;&lt; 30 | fence &lt;&lt; 15 | atomic &lt;&lt; 23 | src &lt;&lt; 5 | dst;
3677     }
3678 
3679     static int storeRelease(MemOpSize size, RegisterID src, RegisterID dst)
3680     {
3681         return 0x089ffc00 | size &lt;&lt; 30 | dst &lt;&lt; 5 | src;
3682     }
3683 
3684     static int exoticStore(MemOpSize size, ExoticStoreFence fence, RegisterID result, RegisterID src, RegisterID dst)
3685     {
3686         return 0x08007c00 | size &lt;&lt; 30 | result &lt;&lt; 16 | fence &lt;&lt; 15 | dst &lt;&lt; 5 | src;
3687     }
3688 
3689     static int fjcvtzsInsn(FPRegisterID dn, RegisterID rd)
3690     {
3691         return 0x1e7e0000 | (dn &lt;&lt; 5) | rd;
3692     }
3693 
3694     // Workaround for Cortex-A53 erratum (835769). Emit an extra nop if the
3695     // last instruction in the buffer is a load, store or prefetch. Needed
3696     // before 64-bit multiply-accumulate instructions.
3697     template&lt;int datasize&gt;
3698     ALWAYS_INLINE void nopCortexA53Fix835769()
3699     {
3700 #if CPU(ARM64_CORTEXA53)
3701         CHECK_DATASIZE();
3702         if (datasize == 64) {
3703             if (LIKELY(m_buffer.codeSize() &gt;= sizeof(int32_t))) {
3704                 // From ARMv8 Reference Manual, Section C4.1: the encoding of the
3705                 // instructions in the Loads and stores instruction group is:
3706                 // ---- 1-0- ---- ---- ---- ---- ---- ----
3707                 if (UNLIKELY((*reinterpret_cast_ptr&lt;int32_t*&gt;(reinterpret_cast_ptr&lt;char*&gt;(m_buffer.data()) + m_buffer.codeSize() - sizeof(int32_t)) &amp; 0x0a000000) == 0x08000000))
3708                     nop();
3709             }
3710         }
3711 #endif
3712     }
3713 
3714     // Workaround for Cortex-A53 erratum (843419). Emit extra nops to avoid
3715     // wrong address access after ADRP instruction.
3716     ALWAYS_INLINE void nopCortexA53Fix843419()
3717     {
3718 #if CPU(ARM64_CORTEXA53)
3719         nop();
3720         nop();
3721         nop();
3722 #endif
3723     }
3724 
3725     Vector&lt;LinkRecord, 0, UnsafeVectorOverflow&gt; m_jumpsToLink;
3726     int m_indexOfLastWatchpoint;
3727     int m_indexOfTailOfLastWatchpoint;
3728     AssemblerBuffer m_buffer;
3729 
3730 public:
3731     static constexpr ptrdiff_t MAX_POINTER_BITS = 48;
3732 };
3733 
3734 } // namespace JSC
3735 
3736 #undef CHECK_DATASIZE_OF
3737 #undef DATASIZE_OF
3738 #undef MEMOPSIZE_OF
3739 #undef CHECK_DATASIZE
3740 #undef DATASIZE
3741 #undef MEMOPSIZE
3742 #undef CHECK_FP_MEMOP_DATASIZE
3743 
3744 #endif // ENABLE(ASSEMBLER) &amp;&amp; CPU(ARM64)
    </pre>
  </body>
</html>