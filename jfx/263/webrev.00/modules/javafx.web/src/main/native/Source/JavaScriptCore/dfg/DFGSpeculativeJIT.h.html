<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGSpeculativeJIT.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(DFG_JIT)
  29 
  30 #include &quot;BlockDirectory.h&quot;
  31 #include &quot;DFGAbstractInterpreter.h&quot;
  32 #include &quot;DFGGenerationInfo.h&quot;
  33 #include &quot;DFGInPlaceAbstractState.h&quot;
  34 #include &quot;DFGJITCompiler.h&quot;
  35 #include &quot;DFGOSRExit.h&quot;
  36 #include &quot;DFGOSRExitJumpPlaceholder.h&quot;
  37 #include &quot;DFGRegisterBank.h&quot;
  38 #include &quot;DFGSilentRegisterSavePlan.h&quot;
  39 #include &quot;JITMathIC.h&quot;
  40 #include &quot;JITOperations.h&quot;
  41 #include &quot;PutKind.h&quot;
  42 #include &quot;SpillRegistersMode.h&quot;
  43 #include &quot;StructureStubInfo.h&quot;
  44 #include &quot;ValueRecovery.h&quot;
  45 #include &quot;VirtualRegister.h&quot;
  46 
  47 namespace JSC { namespace DFG {
  48 
  49 class GPRTemporary;
  50 class JSValueOperand;
  51 class SlowPathGenerator;
  52 class SpeculativeJIT;
  53 class SpeculateInt32Operand;
  54 class SpeculateStrictInt32Operand;
  55 class SpeculateDoubleOperand;
  56 class SpeculateCellOperand;
  57 class SpeculateBooleanOperand;
  58 
  59 enum GeneratedOperandType { GeneratedOperandTypeUnknown, GeneratedOperandInteger, GeneratedOperandJSValue};
  60 
  61 // === SpeculativeJIT ===
  62 //
  63 // The SpeculativeJIT is used to generate a fast, but potentially
  64 // incomplete code path for the dataflow. When code generating
  65 // we may make assumptions about operand types, dynamically check,
  66 // and bail-out to an alternate code path if these checks fail.
  67 // Importantly, the speculative code path cannot be reentered once
  68 // a speculative check has failed. This allows the SpeculativeJIT
  69 // to propagate type information (including information that has
  70 // only speculatively been asserted) through the dataflow.
  71 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(SpeculativeJIT);
  72 class SpeculativeJIT {
  73     WTF_MAKE_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(SpeculativeJIT);
  74     friend struct OSRExit;
  75 private:
  76     typedef JITCompiler::TrustedImm32 TrustedImm32;
  77     typedef JITCompiler::Imm32 Imm32;
  78     typedef JITCompiler::ImmPtr ImmPtr;
  79     typedef JITCompiler::TrustedImm64 TrustedImm64;
  80     typedef JITCompiler::Imm64 Imm64;
  81 
  82     // These constants are used to set priorities for spill order for
  83     // the register allocator.
  84 #if USE(JSVALUE64)
  85     enum SpillOrder {
  86         SpillOrderConstant = 1, // no spill, and cheap fill
  87         SpillOrderSpilled  = 2, // no spill
  88         SpillOrderJS       = 4, // needs spill
  89         SpillOrderCell     = 4, // needs spill
  90         SpillOrderStorage  = 4, // needs spill
  91         SpillOrderInteger  = 5, // needs spill and box
  92         SpillOrderBoolean  = 5, // needs spill and box
  93         SpillOrderDouble   = 6, // needs spill and convert
  94     };
  95 #elif USE(JSVALUE32_64)
  96     enum SpillOrder {
  97         SpillOrderConstant = 1, // no spill, and cheap fill
  98         SpillOrderSpilled  = 2, // no spill
  99         SpillOrderJS       = 4, // needs spill
 100         SpillOrderStorage  = 4, // needs spill
 101         SpillOrderDouble   = 4, // needs spill
 102         SpillOrderInteger  = 5, // needs spill and box
 103         SpillOrderCell     = 5, // needs spill and box
 104         SpillOrderBoolean  = 5, // needs spill and box
 105     };
 106 #endif
 107 
 108     enum UseChildrenMode { CallUseChildren, UseChildrenCalledExplicitly };
 109 
 110 public:
 111     SpeculativeJIT(JITCompiler&amp;);
 112     ~SpeculativeJIT();
 113 
 114     VM&amp; vm()
 115     {
 116         return m_jit.vm();
 117     }
 118 
 119     struct TrustedImmPtr {
 120         template &lt;typename T&gt;
 121         explicit TrustedImmPtr(T* value)
 122             : m_value(value)
 123         {
 124             static_assert(!std::is_base_of&lt;JSCell, T&gt;::value, &quot;To use a GC pointer, the graph must be aware of it. Use SpeculativeJIT::TrustedImmPtr::weakPointer instead.&quot;);
 125         }
 126 
 127         explicit TrustedImmPtr(RegisteredStructure structure)
 128             : m_value(structure.get())
 129         { }
 130 
 131         explicit TrustedImmPtr(std::nullptr_t)
 132             : m_value(nullptr)
 133         { }
 134 
 135         explicit TrustedImmPtr(FrozenValue* value)
 136             : m_value(value-&gt;cell())
 137         {
 138             RELEASE_ASSERT(value-&gt;value().isCell());
 139         }
 140 
 141         explicit TrustedImmPtr(size_t value)
 142             : m_value(bitwise_cast&lt;void*&gt;(value))
 143         {
 144         }
 145 
 146         static TrustedImmPtr weakPointer(Graph&amp; graph, JSCell* cell)
 147         {
 148             graph.m_plan.weakReferences().addLazily(cell);
 149             return TrustedImmPtr(bitwise_cast&lt;size_t&gt;(cell));
 150         }
 151 
 152         operator MacroAssembler::TrustedImmPtr() const { return m_value; }
 153         operator MacroAssembler::TrustedImm() const { return m_value; }
 154 
 155         intptr_t asIntptr()
 156         {
 157             return m_value.asIntptr();
 158         }
 159 
 160     private:
 161         MacroAssembler::TrustedImmPtr m_value;
 162     };
 163 
 164     bool compile();
 165 
 166     void createOSREntries();
 167     void linkOSREntries(LinkBuffer&amp;);
 168 
 169     BasicBlock* nextBlock()
 170     {
 171         for (BlockIndex resultIndex = m_block-&gt;index + 1; ; resultIndex++) {
 172             if (resultIndex &gt;= m_jit.graph().numBlocks())
 173                 return 0;
 174             if (BasicBlock* result = m_jit.graph().block(resultIndex))
 175                 return result;
 176         }
 177     }
 178 
 179 #if USE(JSVALUE64)
 180     GPRReg fillJSValue(Edge);
 181 #elif USE(JSVALUE32_64)
 182     bool fillJSValue(Edge, GPRReg&amp;, GPRReg&amp;, FPRReg&amp;);
 183 #endif
 184     GPRReg fillStorage(Edge);
 185 
 186     // lock and unlock GPR &amp; FPR registers.
 187     void lock(GPRReg reg)
 188     {
 189         m_gprs.lock(reg);
 190     }
 191     void lock(FPRReg reg)
 192     {
 193         m_fprs.lock(reg);
 194     }
 195     void unlock(GPRReg reg)
 196     {
 197         m_gprs.unlock(reg);
 198     }
 199     void unlock(FPRReg reg)
 200     {
 201         m_fprs.unlock(reg);
 202     }
 203 
 204     // Used to check whether a child node is on its last use,
 205     // and its machine registers may be reused.
 206     bool canReuse(Node* node)
 207     {
 208         return generationInfo(node).useCount() == 1;
 209     }
 210     bool canReuse(Node* nodeA, Node* nodeB)
 211     {
 212         return nodeA == nodeB &amp;&amp; generationInfo(nodeA).useCount() == 2;
 213     }
 214     bool canReuse(Edge nodeUse)
 215     {
 216         return canReuse(nodeUse.node());
 217     }
 218     GPRReg reuse(GPRReg reg)
 219     {
 220         m_gprs.lock(reg);
 221         return reg;
 222     }
 223     FPRReg reuse(FPRReg reg)
 224     {
 225         m_fprs.lock(reg);
 226         return reg;
 227     }
 228 
 229     // Allocate a gpr/fpr.
 230     GPRReg allocate()
 231     {
 232 #if ENABLE(DFG_REGISTER_ALLOCATION_VALIDATION)
 233         m_jit.addRegisterAllocationAtOffset(m_jit.debugOffset());
 234 #endif
 235         VirtualRegister spillMe;
 236         GPRReg gpr = m_gprs.allocate(spillMe);
 237         if (spillMe.isValid()) {
 238 #if USE(JSVALUE32_64)
 239             GenerationInfo&amp; info = generationInfoFromVirtualRegister(spillMe);
 240             if ((info.registerFormat() &amp; DataFormatJS))
 241                 m_gprs.release(info.tagGPR() == gpr ? info.payloadGPR() : info.tagGPR());
 242 #endif
 243             spill(spillMe);
 244         }
 245         return gpr;
 246     }
 247     GPRReg allocate(GPRReg specific)
 248     {
 249 #if ENABLE(DFG_REGISTER_ALLOCATION_VALIDATION)
 250         m_jit.addRegisterAllocationAtOffset(m_jit.debugOffset());
 251 #endif
 252         VirtualRegister spillMe = m_gprs.allocateSpecific(specific);
 253         if (spillMe.isValid()) {
 254 #if USE(JSVALUE32_64)
 255             GenerationInfo&amp; info = generationInfoFromVirtualRegister(spillMe);
 256             RELEASE_ASSERT(info.registerFormat() != DataFormatJSDouble);
 257             if ((info.registerFormat() &amp; DataFormatJS))
 258                 m_gprs.release(info.tagGPR() == specific ? info.payloadGPR() : info.tagGPR());
 259 #endif
 260             spill(spillMe);
 261         }
 262         return specific;
 263     }
 264     GPRReg tryAllocate()
 265     {
 266         return m_gprs.tryAllocate();
 267     }
 268     FPRReg fprAllocate()
 269     {
 270 #if ENABLE(DFG_REGISTER_ALLOCATION_VALIDATION)
 271         m_jit.addRegisterAllocationAtOffset(m_jit.debugOffset());
 272 #endif
 273         VirtualRegister spillMe;
 274         FPRReg fpr = m_fprs.allocate(spillMe);
 275         if (spillMe.isValid())
 276             spill(spillMe);
 277         return fpr;
 278     }
 279 
 280     // Check whether a VirtualRegsiter is currently in a machine register.
 281     // We use this when filling operands to fill those that are already in
 282     // machine registers first (by locking VirtualRegsiters that are already
 283     // in machine register before filling those that are not we attempt to
 284     // avoid spilling values we will need immediately).
 285     bool isFilled(Node* node)
 286     {
 287         return generationInfo(node).registerFormat() != DataFormatNone;
 288     }
 289     bool isFilledDouble(Node* node)
 290     {
 291         return generationInfo(node).registerFormat() == DataFormatDouble;
 292     }
 293 
 294     // Called on an operand once it has been consumed by a parent node.
 295     void use(Node* node)
 296     {
 297         if (!node-&gt;hasResult())
 298             return;
 299         GenerationInfo&amp; info = generationInfo(node);
 300 
 301         // use() returns true when the value becomes dead, and any
 302         // associated resources may be freed.
 303         if (!info.use(*m_stream))
 304             return;
 305 
 306         // Release the associated machine registers.
 307         DataFormat registerFormat = info.registerFormat();
 308 #if USE(JSVALUE64)
 309         if (registerFormat == DataFormatDouble)
 310             m_fprs.release(info.fpr());
 311         else if (registerFormat != DataFormatNone)
 312             m_gprs.release(info.gpr());
 313 #elif USE(JSVALUE32_64)
 314         if (registerFormat == DataFormatDouble)
 315             m_fprs.release(info.fpr());
 316         else if (registerFormat &amp; DataFormatJS) {
 317             m_gprs.release(info.tagGPR());
 318             m_gprs.release(info.payloadGPR());
 319         } else if (registerFormat != DataFormatNone)
 320             m_gprs.release(info.gpr());
 321 #endif
 322     }
 323     void use(Edge nodeUse)
 324     {
 325         use(nodeUse.node());
 326     }
 327 
 328     RegisterSet usedRegisters();
 329 
 330     bool masqueradesAsUndefinedWatchpointIsStillValid(const CodeOrigin&amp; codeOrigin)
 331     {
 332         return m_jit.graph().masqueradesAsUndefinedWatchpointIsStillValid(codeOrigin);
 333     }
 334     bool masqueradesAsUndefinedWatchpointIsStillValid()
 335     {
 336         return masqueradesAsUndefinedWatchpointIsStillValid(m_currentNode-&gt;origin.semantic);
 337     }
 338 
 339     void compileStoreBarrier(Node*);
 340 
 341     // Called by the speculative operand types, below, to fill operand to
 342     // machine registers, implicitly generating speculation checks as needed.
 343     GPRReg fillSpeculateInt32(Edge, DataFormat&amp; returnFormat);
 344     GPRReg fillSpeculateInt32Strict(Edge);
 345     GPRReg fillSpeculateInt52(Edge, DataFormat desiredFormat);
 346     FPRReg fillSpeculateDouble(Edge);
 347     GPRReg fillSpeculateCell(Edge);
 348     GPRReg fillSpeculateBoolean(Edge);
 349     GeneratedOperandType checkGeneratedTypeForToInt32(Node*);
 350 
 351     void addSlowPathGenerator(std::unique_ptr&lt;SlowPathGenerator&gt;);
 352     void addSlowPathGeneratorLambda(Function&lt;void()&gt;&amp;&amp;);
 353     void runSlowPathGenerators(PCToCodeOriginMapBuilder&amp;);
 354 
 355     void compile(Node*);
 356     void noticeOSRBirth(Node*);
 357     void bail(AbortReason);
 358     void compileCurrentBlock();
 359 
 360     void checkArgumentTypes();
 361 
 362     void clearGenerationInfo();
 363 
 364     // These methods are used when generating &#39;unexpected&#39;
 365     // calls out from JIT code to C++ helper routines -
 366     // they spill all live values to the appropriate
 367     // slots in the JSStack without changing any state
 368     // in the GenerationInfo.
 369     SilentRegisterSavePlan silentSavePlanForGPR(VirtualRegister spillMe, GPRReg source);
 370     SilentRegisterSavePlan silentSavePlanForFPR(VirtualRegister spillMe, FPRReg source);
 371     void silentSpill(const SilentRegisterSavePlan&amp;);
 372     void silentFill(const SilentRegisterSavePlan&amp;);
 373 
 374     template&lt;typename CollectionType&gt;
 375     void silentSpill(const CollectionType&amp; savePlans)
 376     {
 377         for (unsigned i = 0; i &lt; savePlans.size(); ++i)
 378             silentSpill(savePlans[i]);
 379     }
 380 
 381     template&lt;typename CollectionType&gt;
 382     void silentFill(const CollectionType&amp; savePlans)
 383     {
 384         for (unsigned i = savePlans.size(); i--;)
 385             silentFill(savePlans[i]);
 386     }
 387 
 388     template&lt;typename CollectionType&gt;
 389     void silentSpillAllRegistersImpl(bool doSpill, CollectionType&amp; plans, GPRReg exclude, GPRReg exclude2 = InvalidGPRReg, FPRReg fprExclude = InvalidFPRReg)
 390     {
 391         ASSERT(plans.isEmpty());
 392         for (gpr_iterator iter = m_gprs.begin(); iter != m_gprs.end(); ++iter) {
 393             GPRReg gpr = iter.regID();
 394             if (iter.name().isValid() &amp;&amp; gpr != exclude &amp;&amp; gpr != exclude2) {
 395                 SilentRegisterSavePlan plan = silentSavePlanForGPR(iter.name(), gpr);
 396                 if (doSpill)
 397                     silentSpill(plan);
 398                 plans.append(plan);
 399             }
 400         }
 401         for (fpr_iterator iter = m_fprs.begin(); iter != m_fprs.end(); ++iter) {
 402             if (iter.name().isValid() &amp;&amp; iter.regID() != fprExclude) {
 403                 SilentRegisterSavePlan plan = silentSavePlanForFPR(iter.name(), iter.regID());
 404                 if (doSpill)
 405                     silentSpill(plan);
 406                 plans.append(plan);
 407             }
 408         }
 409     }
 410     template&lt;typename CollectionType&gt;
 411     void silentSpillAllRegistersImpl(bool doSpill, CollectionType&amp; plans, NoResultTag)
 412     {
 413         silentSpillAllRegistersImpl(doSpill, plans, InvalidGPRReg, InvalidGPRReg, InvalidFPRReg);
 414     }
 415     template&lt;typename CollectionType&gt;
 416     void silentSpillAllRegistersImpl(bool doSpill, CollectionType&amp; plans, FPRReg exclude)
 417     {
 418         silentSpillAllRegistersImpl(doSpill, plans, InvalidGPRReg, InvalidGPRReg, exclude);
 419     }
 420     template&lt;typename CollectionType&gt;
 421     void silentSpillAllRegistersImpl(bool doSpill, CollectionType&amp; plans, JSValueRegs exclude)
 422     {
 423 #if USE(JSVALUE32_64)
 424         silentSpillAllRegistersImpl(doSpill, plans, exclude.tagGPR(), exclude.payloadGPR());
 425 #else
 426         silentSpillAllRegistersImpl(doSpill, plans, exclude.gpr());
 427 #endif
 428     }
 429 
 430     void silentSpillAllRegisters(GPRReg exclude, GPRReg exclude2 = InvalidGPRReg, FPRReg fprExclude = InvalidFPRReg)
 431     {
 432         silentSpillAllRegistersImpl(true, m_plans, exclude, exclude2, fprExclude);
 433     }
 434     void silentSpillAllRegisters(FPRReg exclude)
 435     {
 436         silentSpillAllRegisters(InvalidGPRReg, InvalidGPRReg, exclude);
 437     }
 438     void silentSpillAllRegisters(JSValueRegs exclude)
 439     {
 440 #if USE(JSVALUE64)
 441         silentSpillAllRegisters(exclude.payloadGPR());
 442 #else
 443         silentSpillAllRegisters(exclude.payloadGPR(), exclude.tagGPR());
 444 #endif
 445     }
 446 
 447     void silentFillAllRegisters()
 448     {
 449         while (!m_plans.isEmpty()) {
 450             SilentRegisterSavePlan&amp; plan = m_plans.last();
 451             silentFill(plan);
 452             m_plans.removeLast();
 453         }
 454     }
 455 
 456     // These methods convert between doubles, and doubles boxed and JSValues.
 457 #if USE(JSVALUE64)
 458     GPRReg boxDouble(FPRReg fpr, GPRReg gpr)
 459     {
 460         return m_jit.boxDouble(fpr, gpr);
 461     }
 462     FPRReg unboxDouble(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
 463     {
 464         return m_jit.unboxDouble(gpr, resultGPR, fpr);
 465     }
 466     GPRReg boxDouble(FPRReg fpr)
 467     {
 468         return boxDouble(fpr, allocate());
 469     }
 470 
 471     void boxInt52(GPRReg sourceGPR, GPRReg targetGPR, DataFormat);
 472 #elif USE(JSVALUE32_64)
 473     void boxDouble(FPRReg fpr, GPRReg tagGPR, GPRReg payloadGPR)
 474     {
 475         m_jit.boxDouble(fpr, tagGPR, payloadGPR);
 476     }
 477     void unboxDouble(GPRReg tagGPR, GPRReg payloadGPR, FPRReg fpr, FPRReg scratchFPR)
 478     {
 479         m_jit.unboxDouble(tagGPR, payloadGPR, fpr, scratchFPR);
 480     }
 481 #endif
 482     void boxDouble(FPRReg fpr, JSValueRegs regs)
 483     {
 484         m_jit.boxDouble(fpr, regs);
 485     }
 486 
 487     // Spill a VirtualRegister to the JSStack.
 488     void spill(VirtualRegister spillMe)
 489     {
 490         GenerationInfo&amp; info = generationInfoFromVirtualRegister(spillMe);
 491 
 492 #if USE(JSVALUE32_64)
 493         if (info.registerFormat() == DataFormatNone) // it has been spilled. JS values which have two GPRs can reach here
 494             return;
 495 #endif
 496         // Check the GenerationInfo to see if this value need writing
 497         // to the JSStack - if not, mark it as spilled &amp; return.
 498         if (!info.needsSpill()) {
 499             info.setSpilled(*m_stream, spillMe);
 500             return;
 501         }
 502 
 503         DataFormat spillFormat = info.registerFormat();
 504         switch (spillFormat) {
 505         case DataFormatStorage: {
 506             // This is special, since it&#39;s not a JS value - as in it&#39;s not visible to JS
 507             // code.
 508             m_jit.storePtr(info.gpr(), JITCompiler::addressFor(spillMe));
 509             info.spill(*m_stream, spillMe, DataFormatStorage);
 510             return;
 511         }
 512 
 513         case DataFormatInt32: {
 514             m_jit.store32(info.gpr(), JITCompiler::payloadFor(spillMe));
 515             info.spill(*m_stream, spillMe, DataFormatInt32);
 516             return;
 517         }
 518 
 519 #if USE(JSVALUE64)
 520         case DataFormatDouble: {
 521             m_jit.storeDouble(info.fpr(), JITCompiler::addressFor(spillMe));
 522             info.spill(*m_stream, spillMe, DataFormatDouble);
 523             return;
 524         }
 525 
 526         case DataFormatInt52:
 527         case DataFormatStrictInt52: {
 528             m_jit.store64(info.gpr(), JITCompiler::addressFor(spillMe));
 529             info.spill(*m_stream, spillMe, spillFormat);
 530             return;
 531         }
 532 
 533         default:
 534             // The following code handles JSValues, int32s, and cells.
 535             RELEASE_ASSERT(spillFormat == DataFormatCell || spillFormat &amp; DataFormatJS);
 536 
 537             GPRReg reg = info.gpr();
 538             // We need to box int32 and cell values ...
 539             // but on JSVALUE64 boxing a cell is a no-op!
 540             if (spillFormat == DataFormatInt32)
 541                 m_jit.or64(GPRInfo::numberTagRegister, reg);
 542 
 543             // Spill the value, and record it as spilled in its boxed form.
 544             m_jit.store64(reg, JITCompiler::addressFor(spillMe));
 545             info.spill(*m_stream, spillMe, (DataFormat)(spillFormat | DataFormatJS));
 546             return;
 547 #elif USE(JSVALUE32_64)
 548         case DataFormatCell:
 549         case DataFormatBoolean: {
 550             m_jit.store32(info.gpr(), JITCompiler::payloadFor(spillMe));
 551             info.spill(*m_stream, spillMe, spillFormat);
 552             return;
 553         }
 554 
 555         case DataFormatDouble: {
 556             // On JSVALUE32_64 boxing a double is a no-op.
 557             m_jit.storeDouble(info.fpr(), JITCompiler::addressFor(spillMe));
 558             info.spill(*m_stream, spillMe, DataFormatDouble);
 559             return;
 560         }
 561 
 562         default:
 563             // The following code handles JSValues.
 564             RELEASE_ASSERT(spillFormat &amp; DataFormatJS);
 565             m_jit.store32(info.tagGPR(), JITCompiler::tagFor(spillMe));
 566             m_jit.store32(info.payloadGPR(), JITCompiler::payloadFor(spillMe));
 567             info.spill(*m_stream, spillMe, spillFormat);
 568             return;
 569 #endif
 570         }
 571     }
 572 
 573     bool isKnownInteger(Node* node) { return m_state.forNode(node).isType(SpecInt32Only); }
 574     bool isKnownCell(Node* node) { return m_state.forNode(node).isType(SpecCell); }
 575 
 576     bool isKnownNotInteger(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecInt32Only); }
 577     bool isKnownNotNumber(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecFullNumber); }
 578     bool isKnownNotCell(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecCell); }
 579     bool isKnownNotOther(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecOther); }
 580 
 581     bool canBeRope(Edge&amp;);
 582 
 583     UniquedStringImpl* identifierUID(unsigned index)
 584     {
 585         return m_jit.graph().identifiers()[index];
 586     }
 587 
 588     // Spill all VirtualRegisters back to the JSStack.
 589     void flushRegisters()
 590     {
 591         for (gpr_iterator iter = m_gprs.begin(); iter != m_gprs.end(); ++iter) {
 592             if (iter.name().isValid()) {
 593                 spill(iter.name());
 594                 iter.release();
 595             }
 596         }
 597         for (fpr_iterator iter = m_fprs.begin(); iter != m_fprs.end(); ++iter) {
 598             if (iter.name().isValid()) {
 599                 spill(iter.name());
 600                 iter.release();
 601             }
 602         }
 603     }
 604 
 605     // Used to ASSERT flushRegisters() has been called prior to
 606     // calling out from JIT code to a C helper function.
 607     bool isFlushed()
 608     {
 609         for (gpr_iterator iter = m_gprs.begin(); iter != m_gprs.end(); ++iter) {
 610             if (iter.name().isValid())
 611                 return false;
 612         }
 613         for (fpr_iterator iter = m_fprs.begin(); iter != m_fprs.end(); ++iter) {
 614             if (iter.name().isValid())
 615                 return false;
 616         }
 617         return true;
 618     }
 619 
 620 #if USE(JSVALUE64)
 621     static MacroAssembler::Imm64 valueOfJSConstantAsImm64(Node* node)
 622     {
 623         return MacroAssembler::Imm64(JSValue::encode(node-&gt;asJSValue()));
 624     }
 625 #endif
 626 
 627     // Helper functions to enable code sharing in implementations of bit/shift ops.
 628     void bitOp(NodeType op, int32_t imm, GPRReg op1, GPRReg result)
 629     {
 630         switch (op) {
 631         case ArithBitAnd:
 632             m_jit.and32(Imm32(imm), op1, result);
 633             break;
 634         case ArithBitOr:
 635             m_jit.or32(Imm32(imm), op1, result);
 636             break;
 637         case ArithBitXor:
 638             m_jit.xor32(Imm32(imm), op1, result);
 639             break;
 640         default:
 641             RELEASE_ASSERT_NOT_REACHED();
 642         }
 643     }
 644     void bitOp(NodeType op, GPRReg op1, GPRReg op2, GPRReg result)
 645     {
 646         switch (op) {
 647         case ArithBitAnd:
 648             m_jit.and32(op1, op2, result);
 649             break;
 650         case ArithBitOr:
 651             m_jit.or32(op1, op2, result);
 652             break;
 653         case ArithBitXor:
 654             m_jit.xor32(op1, op2, result);
 655             break;
 656         default:
 657             RELEASE_ASSERT_NOT_REACHED();
 658         }
 659     }
 660     void shiftOp(NodeType op, GPRReg op1, int32_t shiftAmount, GPRReg result)
 661     {
 662         switch (op) {
 663         case ArithBitRShift:
 664             m_jit.rshift32(op1, Imm32(shiftAmount), result);
 665             break;
 666         case ArithBitLShift:
 667             m_jit.lshift32(op1, Imm32(shiftAmount), result);
 668             break;
 669         case BitURShift:
 670             m_jit.urshift32(op1, Imm32(shiftAmount), result);
 671             break;
 672         default:
 673             RELEASE_ASSERT_NOT_REACHED();
 674         }
 675     }
 676     void shiftOp(NodeType op, GPRReg op1, GPRReg shiftAmount, GPRReg result)
 677     {
 678         switch (op) {
 679         case ArithBitRShift:
 680             m_jit.rshift32(op1, shiftAmount, result);
 681             break;
 682         case ArithBitLShift:
 683             m_jit.lshift32(op1, shiftAmount, result);
 684             break;
 685         case BitURShift:
 686             m_jit.urshift32(op1, shiftAmount, result);
 687             break;
 688         default:
 689             RELEASE_ASSERT_NOT_REACHED();
 690         }
 691     }
 692 
 693     // Returns the index of the branch node if peephole is okay, UINT_MAX otherwise.
 694     unsigned detectPeepHoleBranch()
 695     {
 696         // Check that no intervening nodes will be generated.
 697         for (unsigned index = m_indexInBlock + 1; index &lt; m_block-&gt;size() - 1; ++index) {
 698             Node* node = m_block-&gt;at(index);
 699             if (!node-&gt;shouldGenerate())
 700                 continue;
 701             // Check if it&#39;s a Phantom that can be safely ignored.
 702             if (node-&gt;op() == Phantom &amp;&amp; !node-&gt;child1())
 703                 continue;
 704             return UINT_MAX;
 705         }
 706 
 707         // Check if the lastNode is a branch on this node.
 708         Node* lastNode = m_block-&gt;terminal();
 709         return lastNode-&gt;op() == Branch &amp;&amp; lastNode-&gt;child1() == m_currentNode ? m_block-&gt;size() - 1 : UINT_MAX;
 710     }
 711 
 712     void compileCheckTraps(Node*);
 713 
 714     void compileMovHint(Node*);
 715     void compileMovHintAndCheck(Node*);
 716 
 717     void compileCheckNeutered(Node*);
 718 
 719     void cachedGetById(CodeOrigin, JSValueRegs base, JSValueRegs result, unsigned identifierNumber, JITCompiler::Jump slowPathTarget, SpillRegistersMode, AccessType);
 720     void cachedPutById(CodeOrigin, GPRReg baseGPR, JSValueRegs valueRegs, GPRReg scratchGPR, unsigned identifierNumber, PutKind, JITCompiler::Jump slowPathTarget = JITCompiler::Jump(), SpillRegistersMode = NeedToSpill);
 721     void cachedGetByVal(CodeOrigin, JSValueRegs base, JSValueRegs property, JSValueRegs result, JITCompiler::Jump slowPathTarget);
 722 
 723 #if USE(JSVALUE64)
 724     void cachedGetById(CodeOrigin, GPRReg baseGPR, GPRReg resultGPR, unsigned identifierNumber, JITCompiler::Jump slowPathTarget, SpillRegistersMode, AccessType);
 725     void cachedGetByIdWithThis(CodeOrigin, GPRReg baseGPR, GPRReg thisGPR, GPRReg resultGPR, unsigned identifierNumber, const JITCompiler::JumpList&amp; slowPathTarget = JITCompiler::JumpList());
 726 #elif USE(JSVALUE32_64)
 727     void cachedGetById(CodeOrigin, GPRReg baseTagGPROrNone, GPRReg basePayloadGPR, GPRReg resultTagGPR, GPRReg resultPayloadGPR, unsigned identifierNumber, JITCompiler::Jump slowPathTarget, SpillRegistersMode, AccessType);
 728     void cachedGetByIdWithThis(CodeOrigin, GPRReg baseTagGPROrNone, GPRReg basePayloadGPR, GPRReg thisTagGPROrNone, GPRReg thisPayloadGPR, GPRReg resultTagGPR, GPRReg resultPayloadGPR, unsigned identifierNumber, const JITCompiler::JumpList&amp; slowPathTarget = JITCompiler::JumpList());
 729 #endif
 730 
 731     void compileDeleteById(Node*);
 732     void compileDeleteByVal(Node*);
 733     void compilePushWithScope(Node*);
 734     void compileGetById(Node*, AccessType);
 735     void compileGetByIdFlush(Node*, AccessType);
 736     void compileInById(Node*);
 737     void compileInByVal(Node*);
 738 
 739     void nonSpeculativeNonPeepholeCompareNullOrUndefined(Edge operand);
 740     void nonSpeculativePeepholeBranchNullOrUndefined(Edge operand, Node* branchNode);
 741 
 742     void nonSpeculativePeepholeBranch(Node*, Node* branchNode, MacroAssembler::RelationalCondition, S_JITOperation_GJJ helperFunction);
 743     void nonSpeculativeNonPeepholeCompare(Node*, MacroAssembler::RelationalCondition, S_JITOperation_GJJ helperFunction);
 744 
 745     void nonSpeculativePeepholeStrictEq(Node*, Node* branchNode, bool invert = false);
 746     void nonSpeculativeNonPeepholeStrictEq(Node*, bool invert = false);
 747     bool nonSpeculativeStrictEq(Node*, bool invert = false);
 748 
 749     void compileInstanceOfForCells(Node*, JSValueRegs valueGPR, JSValueRegs prototypeGPR, GPRReg resultGPT, GPRReg scratchGPR, GPRReg scratch2GPR, JITCompiler::Jump slowCase = JITCompiler::Jump());
 750     void compileInstanceOf(Node*);
 751     void compileInstanceOfCustom(Node*);
 752     void compileOverridesHasInstance(Node*);
 753 
 754     void compileIsCellWithType(Node*);
 755     void compileIsTypedArrayView(Node*);
 756 
 757     void emitCall(Node*);
 758 
 759     void emitAllocateButterfly(GPRReg storageGPR, GPRReg sizeGPR, GPRReg scratch1, GPRReg scratch2, GPRReg scratch3, MacroAssembler::JumpList&amp; slowCases);
 760     void emitInitializeButterfly(GPRReg storageGPR, GPRReg sizeGPR, JSValueRegs emptyValueRegs, GPRReg scratchGPR);
 761     void compileAllocateNewArrayWithSize(JSGlobalObject*, GPRReg resultGPR, GPRReg sizeGPR, IndexingType, bool shouldConvertLargeSizeToArrayStorage = true);
 762 
 763     // Called once a node has completed code generation but prior to setting
 764     // its result, to free up its children. (This must happen prior to setting
 765     // the nodes result, since the node may have the same VirtualRegister as
 766     // a child, and as such will use the same GeneratioInfo).
 767     void useChildren(Node*);
 768 
 769     // These method called to initialize the GenerationInfo
 770     // to describe the result of an operation.
 771     void int32Result(GPRReg reg, Node* node, DataFormat format = DataFormatInt32, UseChildrenMode mode = CallUseChildren)
 772     {
 773         if (mode == CallUseChildren)
 774             useChildren(node);
 775 
 776         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 777         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 778 
 779         if (format == DataFormatInt32) {
 780             m_jit.jitAssertIsInt32(reg);
 781             m_gprs.retain(reg, virtualRegister, SpillOrderInteger);
 782             info.initInt32(node, node-&gt;refCount(), reg);
 783         } else {
 784 #if USE(JSVALUE64)
 785             RELEASE_ASSERT(format == DataFormatJSInt32);
 786             m_jit.jitAssertIsJSInt32(reg);
 787             m_gprs.retain(reg, virtualRegister, SpillOrderJS);
 788             info.initJSValue(node, node-&gt;refCount(), reg, format);
 789 #elif USE(JSVALUE32_64)
 790             RELEASE_ASSERT_NOT_REACHED();
 791 #endif
 792         }
 793     }
 794     void int32Result(GPRReg reg, Node* node, UseChildrenMode mode)
 795     {
 796         int32Result(reg, node, DataFormatInt32, mode);
 797     }
 798     void int52Result(GPRReg reg, Node* node, DataFormat format, UseChildrenMode mode = CallUseChildren)
 799     {
 800         if (mode == CallUseChildren)
 801             useChildren(node);
 802 
 803         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 804         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 805 
 806         m_gprs.retain(reg, virtualRegister, SpillOrderJS);
 807         info.initInt52(node, node-&gt;refCount(), reg, format);
 808     }
 809     void int52Result(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 810     {
 811         int52Result(reg, node, DataFormatInt52, mode);
 812     }
 813     void strictInt52Result(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 814     {
 815         int52Result(reg, node, DataFormatStrictInt52, mode);
 816     }
 817     void noResult(Node* node, UseChildrenMode mode = CallUseChildren)
 818     {
 819         if (mode == UseChildrenCalledExplicitly)
 820             return;
 821         useChildren(node);
 822     }
 823     void cellResult(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 824     {
 825         if (mode == CallUseChildren)
 826             useChildren(node);
 827 
 828         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 829         m_gprs.retain(reg, virtualRegister, SpillOrderCell);
 830         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 831         info.initCell(node, node-&gt;refCount(), reg);
 832     }
 833     void blessedBooleanResult(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 834     {
 835 #if USE(JSVALUE64)
 836         jsValueResult(reg, node, DataFormatJSBoolean, mode);
 837 #else
 838         booleanResult(reg, node, mode);
 839 #endif
 840     }
 841     void unblessedBooleanResult(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 842     {
 843 #if USE(JSVALUE64)
 844         blessBoolean(reg);
 845 #endif
 846         blessedBooleanResult(reg, node, mode);
 847     }
 848 #if USE(JSVALUE64)
 849     void jsValueResult(GPRReg reg, Node* node, DataFormat format = DataFormatJS, UseChildrenMode mode = CallUseChildren)
 850     {
 851         if (format == DataFormatJSInt32)
 852             m_jit.jitAssertIsJSInt32(reg);
 853 
 854         if (mode == CallUseChildren)
 855             useChildren(node);
 856 
 857         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 858         m_gprs.retain(reg, virtualRegister, SpillOrderJS);
 859         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 860         info.initJSValue(node, node-&gt;refCount(), reg, format);
 861     }
 862     void jsValueResult(GPRReg reg, Node* node, UseChildrenMode mode)
 863     {
 864         jsValueResult(reg, node, DataFormatJS, mode);
 865     }
 866 #elif USE(JSVALUE32_64)
 867     void booleanResult(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 868     {
 869         if (mode == CallUseChildren)
 870             useChildren(node);
 871 
 872         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 873         m_gprs.retain(reg, virtualRegister, SpillOrderBoolean);
 874         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 875         info.initBoolean(node, node-&gt;refCount(), reg);
 876     }
 877     void jsValueResult(GPRReg tag, GPRReg payload, Node* node, DataFormat format = DataFormatJS, UseChildrenMode mode = CallUseChildren)
 878     {
 879         if (mode == CallUseChildren)
 880             useChildren(node);
 881 
 882         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 883         m_gprs.retain(tag, virtualRegister, SpillOrderJS);
 884         m_gprs.retain(payload, virtualRegister, SpillOrderJS);
 885         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 886         info.initJSValue(node, node-&gt;refCount(), tag, payload, format);
 887     }
 888     void jsValueResult(GPRReg tag, GPRReg payload, Node* node, UseChildrenMode mode)
 889     {
 890         jsValueResult(tag, payload, node, DataFormatJS, mode);
 891     }
 892 #endif
 893     void jsValueResult(JSValueRegs regs, Node* node, DataFormat format = DataFormatJS, UseChildrenMode mode = CallUseChildren)
 894     {
 895 #if USE(JSVALUE64)
 896         jsValueResult(regs.gpr(), node, format, mode);
 897 #else
 898         jsValueResult(regs.tagGPR(), regs.payloadGPR(), node, format, mode);
 899 #endif
 900     }
 901     void storageResult(GPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 902     {
 903         if (mode == CallUseChildren)
 904             useChildren(node);
 905 
 906         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 907         m_gprs.retain(reg, virtualRegister, SpillOrderStorage);
 908         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 909         info.initStorage(node, node-&gt;refCount(), reg);
 910     }
 911     void doubleResult(FPRReg reg, Node* node, UseChildrenMode mode = CallUseChildren)
 912     {
 913         if (mode == CallUseChildren)
 914             useChildren(node);
 915 
 916         VirtualRegister virtualRegister = node-&gt;virtualRegister();
 917         m_fprs.retain(reg, virtualRegister, SpillOrderDouble);
 918         GenerationInfo&amp; info = generationInfoFromVirtualRegister(virtualRegister);
 919         info.initDouble(node, node-&gt;refCount(), reg);
 920     }
 921     void initConstantInfo(Node* node)
 922     {
 923         ASSERT(node-&gt;hasConstant());
 924         generationInfo(node).initConstant(node, node-&gt;refCount());
 925     }
 926 
 927     template&lt;typename OperationType, typename ResultRegType, typename... Args&gt;
 928     std::enable_if_t&lt;
 929         FunctionTraits&lt;OperationType&gt;::hasResult,
 930     JITCompiler::Call&gt;
 931     callOperation(OperationType operation, ResultRegType result, Args... args)
 932     {
 933         m_jit.setupArguments&lt;OperationType&gt;(args...);
 934         return appendCallSetResult(operation, result);
 935     }
 936 
 937     template&lt;typename OperationType, typename... Args&gt;
 938     std::enable_if_t&lt;
 939         !FunctionTraits&lt;OperationType&gt;::hasResult,
 940     JITCompiler::Call&gt;
 941     callOperation(OperationType operation, Args... args)
 942     {
 943         m_jit.setupArguments&lt;OperationType&gt;(args...);
 944         return appendCall(operation);
 945     }
 946 
 947     JITCompiler::Call callOperationWithCallFrameRollbackOnException(V_JITOperation_Cb operation, CodeBlock* codeBlock)
 948     {
 949         // Do not register CodeBlock* as a weak-pointer.
 950         m_jit.setupArguments&lt;V_JITOperation_Cb&gt;(TrustedImmPtr(static_cast&lt;void*&gt;(codeBlock)));
 951         return appendCallWithCallFrameRollbackOnException(operation);
 952     }
 953 
 954     JITCompiler::Call callOperationWithCallFrameRollbackOnException(Z_JITOperation_G operation, GPRReg result, JSGlobalObject* globalObject)
 955     {
 956         m_jit.setupArguments&lt;Z_JITOperation_G&gt;(TrustedImmPtr::weakPointer(m_graph, globalObject));
 957         return appendCallWithCallFrameRollbackOnExceptionSetResult(operation, result);
 958     }
 959 
 960     void prepareForExternalCall()
 961     {
 962 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM_THUMB2) &amp;&amp; !CPU(MIPS)
 963         // We&#39;re about to call out to a &quot;native&quot; helper function. The helper
 964         // function is expected to set topCallFrame itself with the CallFrame
 965         // that is passed to it.
 966         //
 967         // We explicitly trash topCallFrame here so that we&#39;ll know if some of
 968         // the helper functions are not setting topCallFrame when they should
 969         // be doing so. Note: the previous value in topcallFrame was not valid
 970         // anyway since it was not being updated by JIT&#39;ed code by design.
 971 
 972         for (unsigned i = 0; i &lt; sizeof(void*) / 4; i++)
 973             m_jit.store32(TrustedImm32(0xbadbeef), reinterpret_cast&lt;char*&gt;(&amp;vm().topCallFrame) + i * 4);
 974 #endif
 975         m_jit.prepareCallOperation(vm());
 976     }
 977 
 978     // These methods add call instructions, optionally setting results, and optionally rolling back the call frame on an exception.
 979     JITCompiler::Call appendCall(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
 980     {
 981         prepareForExternalCall();
 982         m_jit.emitStoreCodeOrigin(m_currentNode-&gt;origin.semantic);
 983         return m_jit.appendCall(function);
 984     }
 985 
 986     JITCompiler::Call appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
 987     {
 988         JITCompiler::Call call = appendCall(function);
 989         m_jit.exceptionCheckWithCallFrameRollback();
 990         return call;
 991     }
 992 
 993     JITCompiler::Call appendCallWithCallFrameRollbackOnExceptionSetResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, GPRReg result)
 994     {
 995         JITCompiler::Call call = appendCallWithCallFrameRollbackOnException(function);
 996         if ((result != InvalidGPRReg) &amp;&amp; (result != GPRInfo::returnValueGPR))
 997             m_jit.move(GPRInfo::returnValueGPR, result);
 998         return call;
 999     }
1000 
1001     JITCompiler::Call appendCallSetResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, GPRReg result)
1002     {
1003         JITCompiler::Call call = appendCall(function);
1004         if (result != InvalidGPRReg)
1005             m_jit.move(GPRInfo::returnValueGPR, result);
1006         return call;
1007     }
1008 
1009     JITCompiler::Call appendCallSetResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, GPRReg result1, GPRReg result2)
1010     {
1011         JITCompiler::Call call = appendCall(function);
1012         m_jit.setupResults(result1, result2);
1013         return call;
1014     }
1015 
1016     JITCompiler::Call appendCallSetResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, JSValueRegs resultRegs)
1017     {
1018 #if USE(JSVALUE64)
1019         return appendCallSetResult(function, resultRegs.gpr());
1020 #else
1021         return appendCallSetResult(function, resultRegs.payloadGPR(), resultRegs.tagGPR());
1022 #endif
1023     }
1024 
1025 #if CPU(ARM_THUMB2) &amp;&amp; !CPU(ARM_HARDFP)
1026     JITCompiler::Call appendCallSetResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, FPRReg result)
1027     {
1028         JITCompiler::Call call = appendCall(function);
1029         if (result != InvalidFPRReg)
1030             m_jit.assembler().vmov(result, GPRInfo::returnValueGPR, GPRInfo::returnValueGPR2);
1031         return call;
1032     }
1033 #else // CPU(X86_64) || (CPU(ARM_THUMB2) &amp;&amp; CPU(ARM_HARDFP)) || CPU(ARM64) || CPU(MIPS)
1034     JITCompiler::Call appendCallSetResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, FPRReg result)
1035     {
1036         JITCompiler::Call call = appendCall(function);
1037         if (result != InvalidFPRReg)
1038             m_jit.moveDouble(FPRInfo::returnValueFPR, result);
1039         return call;
1040     }
1041 #endif
1042 
1043     void branchDouble(JITCompiler::DoubleCondition cond, FPRReg left, FPRReg right, BasicBlock* destination)
1044     {
1045         return addBranch(m_jit.branchDouble(cond, left, right), destination);
1046     }
1047 
1048     void branchDoubleNonZero(FPRReg value, FPRReg scratch, BasicBlock* destination)
1049     {
1050         return addBranch(m_jit.branchDoubleNonZero(value, scratch), destination);
1051     }
1052 
1053     template&lt;typename T, typename U&gt;
1054     void branch32(JITCompiler::RelationalCondition cond, T left, U right, BasicBlock* destination)
1055     {
1056         return addBranch(m_jit.branch32(cond, left, right), destination);
1057     }
1058 
1059     template&lt;typename T, typename U&gt;
1060     void branchTest32(JITCompiler::ResultCondition cond, T value, U mask, BasicBlock* destination)
1061     {
1062         return addBranch(m_jit.branchTest32(cond, value, mask), destination);
1063     }
1064 
1065     template&lt;typename T&gt;
1066     void branchTest32(JITCompiler::ResultCondition cond, T value, BasicBlock* destination)
1067     {
1068         return addBranch(m_jit.branchTest32(cond, value), destination);
1069     }
1070 
1071 #if USE(JSVALUE64)
1072     template&lt;typename T, typename U&gt;
1073     void branch64(JITCompiler::RelationalCondition cond, T left, U right, BasicBlock* destination)
1074     {
1075         return addBranch(m_jit.branch64(cond, left, right), destination);
1076     }
1077 #endif
1078 
1079     template&lt;typename T, typename U&gt;
1080     void branch8(JITCompiler::RelationalCondition cond, T left, U right, BasicBlock* destination)
1081     {
1082         return addBranch(m_jit.branch8(cond, left, right), destination);
1083     }
1084 
1085     template&lt;typename T, typename U&gt;
1086     void branchPtr(JITCompiler::RelationalCondition cond, T left, U right, BasicBlock* destination)
1087     {
1088         return addBranch(m_jit.branchPtr(cond, left, right), destination);
1089     }
1090 
1091     template&lt;typename T, typename U&gt;
1092     void branchTestPtr(JITCompiler::ResultCondition cond, T value, U mask, BasicBlock* destination)
1093     {
1094         return addBranch(m_jit.branchTestPtr(cond, value, mask), destination);
1095     }
1096 
1097     template&lt;typename T&gt;
1098     void branchTestPtr(JITCompiler::ResultCondition cond, T value, BasicBlock* destination)
1099     {
1100         return addBranch(m_jit.branchTestPtr(cond, value), destination);
1101     }
1102 
1103     template&lt;typename T, typename U&gt;
1104     void branchTest8(JITCompiler::ResultCondition cond, T value, U mask, BasicBlock* destination)
1105     {
1106         return addBranch(m_jit.branchTest8(cond, value, mask), destination);
1107     }
1108 
1109     template&lt;typename T&gt;
1110     void branchTest8(JITCompiler::ResultCondition cond, T value, BasicBlock* destination)
1111     {
1112         return addBranch(m_jit.branchTest8(cond, value), destination);
1113     }
1114 
1115     enum FallThroughMode {
1116         AtFallThroughPoint,
1117         ForceJump
1118     };
1119     void jump(BasicBlock* destination, FallThroughMode fallThroughMode = AtFallThroughPoint)
1120     {
1121         if (destination == nextBlock()
1122             &amp;&amp; fallThroughMode == AtFallThroughPoint)
1123             return;
1124         addBranch(m_jit.jump(), destination);
1125     }
1126 
1127     void addBranch(const MacroAssembler::Jump&amp; jump, BasicBlock* destination)
1128     {
1129         m_branches.append(BranchRecord(jump, destination));
1130     }
1131     void addBranch(const MacroAssembler::JumpList&amp; jump, BasicBlock* destination);
1132 
1133     void linkBranches();
1134 
1135     void dump(const char* label = 0);
1136 
1137     bool betterUseStrictInt52(Node* node)
1138     {
1139         return !generationInfo(node).isInt52();
1140     }
1141     bool betterUseStrictInt52(Edge edge)
1142     {
1143         return betterUseStrictInt52(edge.node());
1144     }
1145 
1146     bool compare(Node*, MacroAssembler::RelationalCondition, MacroAssembler::DoubleCondition, S_JITOperation_GJJ);
1147     void compileCompareUnsigned(Node*, MacroAssembler::RelationalCondition);
1148     bool compilePeepHoleBranch(Node*, MacroAssembler::RelationalCondition, MacroAssembler::DoubleCondition, S_JITOperation_GJJ);
1149     void compilePeepHoleInt32Branch(Node*, Node* branchNode, JITCompiler::RelationalCondition);
1150     void compilePeepHoleInt52Branch(Node*, Node* branchNode, JITCompiler::RelationalCondition);
1151     void compilePeepHoleBooleanBranch(Node*, Node* branchNode, JITCompiler::RelationalCondition);
1152     void compilePeepHoleDoubleBranch(Node*, Node* branchNode, JITCompiler::DoubleCondition);
1153     void compilePeepHoleObjectEquality(Node*, Node* branchNode);
1154     void compilePeepHoleObjectStrictEquality(Edge objectChild, Edge otherChild, Node* branchNode);
1155     void compilePeepHoleObjectToObjectOrOtherEquality(Edge leftChild, Edge rightChild, Node* branchNode);
1156     void compileObjectEquality(Node*);
1157     void compileObjectStrictEquality(Edge objectChild, Edge otherChild);
1158     void compileObjectToObjectOrOtherEquality(Edge leftChild, Edge rightChild);
1159     void compileObjectOrOtherLogicalNot(Edge value);
1160     void compileLogicalNot(Node*);
1161     void compileLogicalNotStringOrOther(Node*);
1162     void compileStringEquality(
1163         Node*, GPRReg leftGPR, GPRReg rightGPR, GPRReg lengthGPR,
1164         GPRReg leftTempGPR, GPRReg rightTempGPR, GPRReg leftTemp2GPR,
1165         GPRReg rightTemp2GPR, const JITCompiler::JumpList&amp; fastTrue,
1166         const JITCompiler::JumpList&amp; fastSlow);
1167     void compileStringEquality(Node*);
1168     void compileStringIdentEquality(Node*);
1169     void compileStringToUntypedEquality(Node*, Edge stringEdge, Edge untypedEdge);
1170     void compileStringIdentToNotStringVarEquality(Node*, Edge stringEdge, Edge notStringVarEdge);
1171     void compileStringZeroLength(Node*);
1172     void compileMiscStrictEq(Node*);
1173 
1174     void compileSymbolEquality(Node*);
1175     void compileBigIntEquality(Node*);
1176     void compilePeepHoleSymbolEquality(Node*, Node* branchNode);
1177     void compileSymbolUntypedEquality(Node*, Edge symbolEdge, Edge untypedEdge);
1178 
1179     void emitObjectOrOtherBranch(Edge value, BasicBlock* taken, BasicBlock* notTaken);
1180     void emitStringBranch(Edge value, BasicBlock* taken, BasicBlock* notTaken);
1181     void emitStringOrOtherBranch(Edge value, BasicBlock* taken, BasicBlock* notTaken);
1182     void emitBranch(Node*);
1183 
1184     struct StringSwitchCase {
1185         StringSwitchCase() { }
1186 
1187         StringSwitchCase(StringImpl* string, BasicBlock* target)
1188             : string(string)
1189             , target(target)
1190         {
1191         }
1192 
1193         bool operator&lt;(const StringSwitchCase&amp; other) const
1194         {
1195             return stringLessThan(*string, *other.string);
1196         }
1197 
1198         StringImpl* string;
1199         BasicBlock* target;
1200     };
1201 
1202     void emitSwitchIntJump(SwitchData*, GPRReg value, GPRReg scratch);
1203     void emitSwitchImm(Node*, SwitchData*);
1204     void emitSwitchCharStringJump(Node*, SwitchData*, GPRReg value, GPRReg scratch);
1205     void emitSwitchChar(Node*, SwitchData*);
1206     void emitBinarySwitchStringRecurse(
1207         SwitchData*, const Vector&lt;StringSwitchCase&gt;&amp;, unsigned numChecked,
1208         unsigned begin, unsigned end, GPRReg buffer, GPRReg length, GPRReg temp,
1209         unsigned alreadyCheckedLength, bool checkedExactLength);
1210     void emitSwitchStringOnString(Node*, SwitchData*, GPRReg string);
1211     void emitSwitchString(Node*, SwitchData*);
1212     void emitSwitch(Node*);
1213 
1214     void compileToStringOrCallStringConstructorOrStringValueOf(Node*);
1215     void compileNumberToStringWithRadix(Node*);
1216     void compileNumberToStringWithValidRadixConstant(Node*);
1217     void compileNumberToStringWithValidRadixConstant(Node*, int32_t radix);
1218     void compileNewStringObject(Node*);
1219     void compileNewSymbol(Node*);
1220 
1221     void compileNewTypedArrayWithSize(Node*);
1222 
1223     void compileInt32Compare(Node*, MacroAssembler::RelationalCondition);
1224     void compileInt52Compare(Node*, MacroAssembler::RelationalCondition);
1225     void compileBooleanCompare(Node*, MacroAssembler::RelationalCondition);
1226     void compileDoubleCompare(Node*, MacroAssembler::DoubleCondition);
1227     void compileStringCompare(Node*, MacroAssembler::RelationalCondition);
1228     void compileStringIdentCompare(Node*, MacroAssembler::RelationalCondition);
1229 
1230     bool compileStrictEq(Node*);
1231 
1232     void compileSameValue(Node*);
1233 
1234     void compileAllocatePropertyStorage(Node*);
1235     void compileReallocatePropertyStorage(Node*);
1236     void compileNukeStructureAndSetButterfly(Node*);
1237     void compileGetButterfly(Node*);
1238     void compileCallDOMGetter(Node*);
1239     void compileCallDOM(Node*);
1240     void compileCheckSubClass(Node*);
1241     void compileNormalizeMapKey(Node*);
1242     void compileGetMapBucketHead(Node*);
1243     void compileGetMapBucketNext(Node*);
1244     void compileSetAdd(Node*);
1245     void compileMapSet(Node*);
1246     void compileWeakMapGet(Node*);
1247     void compileWeakSetAdd(Node*);
1248     void compileWeakMapSet(Node*);
1249     void compileLoadKeyFromMapBucket(Node*);
1250     void compileLoadValueFromMapBucket(Node*);
1251     void compileExtractValueFromWeakMapGet(Node*);
1252     void compileGetPrototypeOf(Node*);
1253     void compileIdentity(Node*);
1254 
1255 #if USE(JSVALUE32_64)
1256     template&lt;typename BaseOperandType, typename PropertyOperandType, typename ValueOperandType, typename TagType&gt;
1257     void compileContiguousPutByVal(Node*, BaseOperandType&amp;, PropertyOperandType&amp;, ValueOperandType&amp;, GPRReg valuePayloadReg, TagType valueTag);
1258 #endif
1259     void compileDoublePutByVal(Node*, SpeculateCellOperand&amp; base, SpeculateStrictInt32Operand&amp; property);
1260     bool putByValWillNeedExtraRegister(ArrayMode arrayMode)
1261     {
1262         return arrayMode.mayStoreToHole();
1263     }
1264     GPRReg temporaryRegisterForPutByVal(GPRTemporary&amp;, ArrayMode);
1265     GPRReg temporaryRegisterForPutByVal(GPRTemporary&amp; temporary, Node* node)
1266     {
1267         return temporaryRegisterForPutByVal(temporary, node-&gt;arrayMode());
1268     }
1269 
1270     void compileGetCharCodeAt(Node*);
1271     void compileGetByValOnString(Node*);
1272     void compileFromCharCode(Node*);
1273 
1274     void compileGetByValOnDirectArguments(Node*);
1275     void compileGetByValOnScopedArguments(Node*);
1276 
1277     void compileGetScope(Node*);
1278     void compileSkipScope(Node*);
1279     void compileGetGlobalObject(Node*);
1280     void compileGetGlobalThis(Node*);
1281 
1282     void compileGetArrayLength(Node*);
1283 
1284     void compileCheckTypeInfoFlags(Node*);
1285     void compileCheckIdent(Node*);
1286 
1287     void compileParseInt(Node*);
1288 
1289     void compileValueRep(Node*);
1290     void compileDoubleRep(Node*);
1291 
1292     void compileValueToInt32(Node*);
1293     void compileUInt32ToNumber(Node*);
1294     void compileDoubleAsInt32(Node*);
1295 
1296     void compileValueBitNot(Node*);
1297     void compileBitwiseNot(Node*);
1298 
1299     template&lt;typename SnippetGenerator, J_JITOperation_GJJ slowPathFunction&gt;
1300     void emitUntypedBitOp(Node*);
1301     void compileBitwiseOp(Node*);
1302     void compileValueBitwiseOp(Node*);
1303 
1304     void emitUntypedRightShiftBitOp(Node*);
1305     void compileValueLShiftOp(Node*);
1306     void compileValueBitRShift(Node*);
1307     void compileShiftOp(Node*);
1308 
1309     template &lt;typename Generator, typename RepatchingFunction, typename NonRepatchingFunction&gt;
1310     void compileMathIC(Node*, JITBinaryMathIC&lt;Generator&gt;*, bool needsScratchGPRReg, bool needsScratchFPRReg, RepatchingFunction, NonRepatchingFunction);
1311     template &lt;typename Generator, typename RepatchingFunction, typename NonRepatchingFunction&gt;
1312     void compileMathIC(Node*, JITUnaryMathIC&lt;Generator&gt;*, bool needsScratchGPRReg, RepatchingFunction, NonRepatchingFunction);
1313 
1314     void compileArithDoubleUnaryOp(Node*, double (*doubleFunction)(double), double (*operation)(JSGlobalObject*, EncodedJSValue));
1315     void compileValueAdd(Node*);
1316     void compileValueSub(Node*);
1317     void compileArithAdd(Node*);
1318     void compileMakeRope(Node*);
1319     void compileArithAbs(Node*);
1320     void compileArithClz32(Node*);
1321     void compileArithSub(Node*);
1322     void compileIncOrDec(Node*);
1323     void compileValueNegate(Node*);
1324     void compileArithNegate(Node*);
1325     void compileValueMul(Node*);
1326     void compileArithMul(Node*);
1327     void compileValueDiv(Node*);
1328     void compileArithDiv(Node*);
1329     void compileArithFRound(Node*);
1330     void compileValueMod(Node*);
1331     void compileArithMod(Node*);
1332     void compileArithPow(Node*);
1333     void compileValuePow(Node*);
1334     void compileArithRounding(Node*);
1335     void compileArithRandom(Node*);
1336     void compileArithUnary(Node*);
1337     void compileArithSqrt(Node*);
1338     void compileArithMinMax(Node*);
1339     void compileConstantStoragePointer(Node*);
1340     void compileGetIndexedPropertyStorage(Node*);
1341     JITCompiler::Jump jumpForTypedArrayOutOfBounds(Node*, GPRReg baseGPR, GPRReg indexGPR);
1342     JITCompiler::Jump jumpForTypedArrayIsNeuteredIfOutOfBounds(Node*, GPRReg baseGPR, JITCompiler::Jump outOfBounds);
1343     void emitTypedArrayBoundsCheck(Node*, GPRReg baseGPR, GPRReg indexGPR);
1344     void compileGetTypedArrayByteOffset(Node*);
1345     void compileGetByValOnIntTypedArray(Node*, TypedArrayType);
1346     void compilePutByValForIntTypedArray(GPRReg base, GPRReg property, Node*, TypedArrayType);
1347     void compileGetByValOnFloatTypedArray(Node*, TypedArrayType);
1348     void compilePutByValForFloatTypedArray(GPRReg base, GPRReg property, Node*, TypedArrayType);
1349     void compileGetByValForObjectWithString(Node*);
1350     void compileGetByValForObjectWithSymbol(Node*);
1351     void compilePutByValForCellWithString(Node*, Edge&amp; child1, Edge&amp; child2, Edge&amp; child3);
1352     void compilePutByValForCellWithSymbol(Node*, Edge&amp; child1, Edge&amp; child2, Edge&amp; child3);
1353     void compileGetByValWithThis(Node*);
1354     void compileGetByOffset(Node*);
1355     void compilePutByOffset(Node*);
1356     void compileMatchStructure(Node*);
1357     // If this returns false it means that we terminated speculative execution.
1358     bool getIntTypedArrayStoreOperand(
1359         GPRTemporary&amp; value,
1360         GPRReg property,
1361 #if USE(JSVALUE32_64)
1362         GPRTemporary&amp; propertyTag,
1363         GPRTemporary&amp; valueTag,
1364 #endif
1365         Edge valueUse, JITCompiler::JumpList&amp; slowPathCases, bool isClamped = false);
1366     void loadFromIntTypedArray(GPRReg storageReg, GPRReg propertyReg, GPRReg resultReg, TypedArrayType);
1367     void setIntTypedArrayLoadResult(Node*, GPRReg resultReg, TypedArrayType, bool canSpeculate = false);
1368     template &lt;typename ClassType&gt; void compileNewFunctionCommon(GPRReg, RegisteredStructure, GPRReg, GPRReg, GPRReg, MacroAssembler::JumpList&amp;, size_t, FunctionExecutable*);
1369     void compileNewFunction(Node*);
1370     void compileSetFunctionName(Node*);
1371     void compileNewRegexp(Node*);
1372     void compileForwardVarargs(Node*);
1373     void compileVarargsLength(Node*);
1374     void compileLoadVarargs(Node*);
1375     void compileCreateActivation(Node*);
1376     void compileCreateDirectArguments(Node*);
1377     void compileGetFromArguments(Node*);
1378     void compilePutToArguments(Node*);
1379     void compileGetArgument(Node*);
1380     void compileCreateScopedArguments(Node*);
1381     void compileCreateClonedArguments(Node*);
1382     void compileCreateArgumentsButterfly(Node*);
1383     void compileCreateRest(Node*);
1384     void compileSpread(Node*);
1385     void compileNewArray(Node*);
1386     void compileNewArrayWithSpread(Node*);
1387     void compileGetRestLength(Node*);
1388     void compileArraySlice(Node*);
1389     void compileArrayIndexOf(Node*);
1390     void compileArrayPush(Node*);
1391     void compileNotifyWrite(Node*);
1392     void compileRegExpExec(Node*);
1393     void compileRegExpExecNonGlobalOrSticky(Node*);
1394     void compileRegExpMatchFast(Node*);
1395     void compileRegExpMatchFastGlobal(Node*);
1396     void compileRegExpTest(Node*);
1397     void compileStringReplace(Node*);
1398     void compileIsObject(Node*);
1399     void compileIsObjectOrNull(Node*);
1400     void compileIsFunction(Node*);
1401     void compileTypeOf(Node*);
1402     void compileCheckCell(Node*);
1403     void compileCheckNotEmpty(Node*);
1404     void compileCheckStructure(Node*);
1405     void emitStructureCheck(Node*, GPRReg cellGPR, GPRReg tempGPR);
1406     void compilePutAccessorById(Node*);
1407     void compilePutGetterSetterById(Node*);
1408     void compilePutAccessorByVal(Node*);
1409     void compileGetRegExpObjectLastIndex(Node*);
1410     void compileSetRegExpObjectLastIndex(Node*);
1411     void compileLazyJSConstant(Node*);
1412     void compileMaterializeNewObject(Node*);
1413     void compileRecordRegExpCachedResult(Node*);
1414     void compileToObjectOrCallObjectConstructor(Node*);
1415     void compileResolveScope(Node*);
1416     void compileResolveScopeForHoistingFuncDeclInEval(Node*);
1417     void compileGetGlobalVariable(Node*);
1418     void compilePutGlobalVariable(Node*);
1419     void compileGetDynamicVar(Node*);
1420     void compilePutDynamicVar(Node*);
1421     void compileGetClosureVar(Node*);
1422     void compilePutClosureVar(Node*);
1423     void compileGetInternalField(Node*);
1424     void compilePutInternalField(Node*);
1425     void compileCompareEqPtr(Node*);
1426     void compileDefineDataProperty(Node*);
1427     void compileDefineAccessorProperty(Node*);
1428     void compileStringSlice(Node*);
1429     void compileToLowerCase(Node*);
1430     void compileThrow(Node*);
1431     void compileThrowStaticError(Node*);
1432     void compileGetEnumerableLength(Node*);
1433     void compileHasGenericProperty(Node*);
1434     void compileToIndexString(Node*);
1435     void compilePutByIdFlush(Node*);
1436     void compilePutById(Node*);
1437     void compilePutByIdDirect(Node*);
1438     void compilePutByIdWithThis(Node*);
1439     void compileHasStructureProperty(Node*);
1440     void compileGetDirectPname(Node*);
1441     void compileGetPropertyEnumerator(Node*);
1442     void compileGetEnumeratorPname(Node*);
1443     void compileGetExecutable(Node*);
1444     void compileGetGetter(Node*);
1445     void compileGetSetter(Node*);
1446     void compileGetCallee(Node*);
1447     void compileSetCallee(Node*);
1448     void compileGetArgumentCountIncludingThis(Node*);
1449     void compileSetArgumentCountIncludingThis(Node*);
1450     void compileStrCat(Node*);
1451     void compileNewArrayBuffer(Node*);
1452     void compileNewArrayWithSize(Node*);
1453     void compileNewTypedArray(Node*);
1454     void compileToThis(Node*);
1455     void compileObjectKeys(Node*);
1456     void compileObjectCreate(Node*);
1457     void compileCreateThis(Node*);
1458     void compileCreatePromise(Node*);
1459     void compileCreateGenerator(Node*);
1460     void compileCreateAsyncGenerator(Node*);
1461     void compileNewObject(Node*);
1462     void compileNewPromise(Node*);
1463     void compileNewGenerator(Node*);
1464     void compileNewAsyncGenerator(Node*);
1465     void compileNewArrayIterator(Node*);
1466     void compileToPrimitive(Node*);
1467     void compileToPropertyKey(Node*);
1468     void compileToNumeric(Node*);
1469     void compileLogShadowChickenPrologue(Node*);
1470     void compileLogShadowChickenTail(Node*);
1471     void compileHasIndexedProperty(Node*);
1472     void compileExtractCatchLocal(Node*);
1473     void compileClearCatchLocals(Node*);
1474     void compileProfileType(Node*);
1475     void compileStringCodePointAt(Node*);
1476     void compileDateGet(Node*);
1477 
1478     template&lt;typename JSClass, typename Operation&gt;
1479     void compileCreateInternalFieldObject(Node*, Operation);
1480     template&lt;typename JSClass, typename Operation&gt;
1481     void compileNewInternalFieldObject(Node*, Operation);
1482 
1483     void moveTrueTo(GPRReg);
1484     void moveFalseTo(GPRReg);
1485     void blessBoolean(GPRReg);
1486 
1487     // Allocator for a cell of a specific size.
1488     template &lt;typename StructureType&gt; // StructureType can be GPR or ImmPtr.
1489     void emitAllocateJSCell(
1490         GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure,
1491         GPRReg scratchGPR, MacroAssembler::JumpList&amp; slowPath)
1492     {
1493         m_jit.emitAllocateJSCell(resultGPR, allocator, allocatorGPR, structure, scratchGPR, slowPath);
1494     }
1495 
1496     // Allocator for an object of a specific size.
1497     template &lt;typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1498     void emitAllocateJSObject(
1499         GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure,
1500         StorageType storage, GPRReg scratchGPR, MacroAssembler::JumpList&amp; slowPath)
1501     {
1502         m_jit.emitAllocateJSObject(
1503             resultGPR, allocator, allocatorGPR, structure, storage, scratchGPR, slowPath);
1504     }
1505 
1506     template &lt;typename ClassType, typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1507     void emitAllocateJSObjectWithKnownSize(
1508         GPRReg resultGPR, StructureType structure, StorageType storage, GPRReg scratchGPR1,
1509         GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath, size_t size)
1510     {
1511         m_jit.emitAllocateJSObjectWithKnownSize&lt;ClassType&gt;(vm(), resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath, size);
1512     }
1513 
1514     // Convenience allocator for a built-in object.
1515     template &lt;typename ClassType, typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1516     void emitAllocateJSObject(GPRReg resultGPR, StructureType structure, StorageType storage,
1517         GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1518     {
1519         m_jit.emitAllocateJSObject&lt;ClassType&gt;(vm(), resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath);
1520     }
1521 
1522     template &lt;typename ClassType, typename StructureType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1523     void emitAllocateVariableSizedJSObject(GPRReg resultGPR, StructureType structure, GPRReg allocationSize, GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1524     {
1525         m_jit.emitAllocateVariableSizedJSObject&lt;ClassType&gt;(vm(), resultGPR, structure, allocationSize, scratchGPR1, scratchGPR2, slowPath);
1526     }
1527 
1528     template&lt;typename ClassType&gt;
1529     void emitAllocateDestructibleObject(GPRReg resultGPR, RegisteredStructure structure,
1530         GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1531     {
1532         m_jit.emitAllocateDestructibleObject&lt;ClassType&gt;(vm(), resultGPR, structure.get(), scratchGPR1, scratchGPR2, slowPath);
1533     }
1534 
1535     void emitAllocateRawObject(GPRReg resultGPR, RegisteredStructure, GPRReg storageGPR, unsigned numElements, unsigned vectorLength);
1536 
1537     void emitGetLength(InlineCallFrame*, GPRReg lengthGPR, bool includeThis = false);
1538     void emitGetLength(CodeOrigin, GPRReg lengthGPR, bool includeThis = false);
1539     void emitGetCallee(CodeOrigin, GPRReg calleeGPR);
1540     void emitGetArgumentStart(CodeOrigin, GPRReg startGPR);
1541     void emitPopulateSliceIndex(Edge&amp;, Optional&lt;GPRReg&gt; indexGPR, GPRReg lengthGPR, GPRReg resultGPR);
1542 
1543     // Generate an OSR exit fuzz check. Returns Jump() if OSR exit fuzz is not enabled, or if
1544     // it&#39;s in training mode.
1545     MacroAssembler::Jump emitOSRExitFuzzCheck();
1546 
1547     // Add a speculation check.
1548     void speculationCheck(ExitKind, JSValueSource, Node*, MacroAssembler::Jump jumpToFail);
1549     void speculationCheck(ExitKind, JSValueSource, Node*, const MacroAssembler::JumpList&amp; jumpsToFail);
1550 
1551     // Add a speculation check without additional recovery, and with a promise to supply a jump later.
1552     OSRExitJumpPlaceholder speculationCheck(ExitKind, JSValueSource, Node*);
1553     OSRExitJumpPlaceholder speculationCheck(ExitKind, JSValueSource, Edge);
1554     void speculationCheck(ExitKind, JSValueSource, Edge, MacroAssembler::Jump jumpToFail);
1555     void speculationCheck(ExitKind, JSValueSource, Edge, const MacroAssembler::JumpList&amp; jumpsToFail);
1556     // Add a speculation check with additional recovery.
1557     void speculationCheck(ExitKind, JSValueSource, Node*, MacroAssembler::Jump jumpToFail, const SpeculationRecovery&amp;);
1558     void speculationCheck(ExitKind, JSValueSource, Edge, MacroAssembler::Jump jumpToFail, const SpeculationRecovery&amp;);
1559 
1560     void emitInvalidationPoint(Node*);
1561 
1562     void unreachable(Node*);
1563 
1564     // Called when we statically determine that a speculation will fail.
1565     void terminateSpeculativeExecution(ExitKind, JSValueRegs, Node*);
1566     void terminateSpeculativeExecution(ExitKind, JSValueRegs, Edge);
1567 
1568     // Helpers for performing type checks on an edge stored in the given registers.
1569     bool needsTypeCheck(Edge edge, SpeculatedType typesPassedThrough) { return m_interpreter.needsTypeCheck(edge, typesPassedThrough); }
1570     void typeCheck(JSValueSource, Edge, SpeculatedType typesPassedThrough, MacroAssembler::Jump jumpToFail, ExitKind = BadType);
1571 
1572     void speculateCellTypeWithoutTypeFiltering(Edge, GPRReg cellGPR, JSType);
1573     void speculateCellType(Edge, GPRReg cellGPR, SpeculatedType, JSType);
1574 
1575     void speculateInt32(Edge);
1576 #if USE(JSVALUE64)
1577     void convertAnyInt(Edge, GPRReg resultGPR);
1578     void speculateAnyInt(Edge);
1579     void speculateInt32(Edge, JSValueRegs);
1580     void speculateDoubleRepAnyInt(Edge);
1581 #endif // USE(JSVALUE64)
1582     void speculateNumber(Edge);
1583     void speculateRealNumber(Edge);
1584     void speculateDoubleRepReal(Edge);
1585     void speculateBoolean(Edge);
1586     void speculateCell(Edge);
1587     void speculateCellOrOther(Edge);
1588     void speculateObject(Edge, GPRReg cell);
1589     void speculateObject(Edge);
1590     void speculateArray(Edge, GPRReg cell);
1591     void speculateArray(Edge);
1592     void speculateFunction(Edge, GPRReg cell);
1593     void speculateFunction(Edge);
1594     void speculateFinalObject(Edge, GPRReg cell);
1595     void speculateFinalObject(Edge);
1596     void speculateRegExpObject(Edge, GPRReg cell);
1597     void speculateRegExpObject(Edge);
1598     void speculatePromiseObject(Edge);
1599     void speculatePromiseObject(Edge, GPRReg cell);
1600     void speculateProxyObject(Edge, GPRReg cell);
1601     void speculateProxyObject(Edge);
1602     void speculateDerivedArray(Edge, GPRReg cell);
1603     void speculateDerivedArray(Edge);
1604     void speculateDateObject(Edge);
1605     void speculateDateObject(Edge, GPRReg cell);
1606     void speculateMapObject(Edge);
1607     void speculateMapObject(Edge, GPRReg cell);
1608     void speculateSetObject(Edge);
1609     void speculateSetObject(Edge, GPRReg cell);
1610     void speculateWeakMapObject(Edge);
1611     void speculateWeakMapObject(Edge, GPRReg cell);
1612     void speculateWeakSetObject(Edge);
1613     void speculateWeakSetObject(Edge, GPRReg cell);
1614     void speculateDataViewObject(Edge);
1615     void speculateDataViewObject(Edge, GPRReg cell);
1616     void speculateObjectOrOther(Edge);
1617     void speculateString(Edge edge, GPRReg cell);
1618     void speculateStringIdentAndLoadStorage(Edge edge, GPRReg string, GPRReg storage);
1619     void speculateStringIdent(Edge edge, GPRReg string);
1620     void speculateStringIdent(Edge);
1621     void speculateString(Edge);
1622     void speculateStringOrOther(Edge, JSValueRegs, GPRReg scratch);
1623     void speculateStringOrOther(Edge);
1624     void speculateNotStringVar(Edge);
1625     void speculateNotSymbol(Edge);
1626     void speculateStringObject(Edge, GPRReg);
1627     void speculateStringObject(Edge);
1628     void speculateStringOrStringObject(Edge);
1629     void speculateSymbol(Edge, GPRReg cell);
1630     void speculateSymbol(Edge);
1631     void speculateBigInt(Edge, GPRReg cell);
1632     void speculateBigInt(Edge);
1633     void speculateNotCell(Edge, JSValueRegs);
1634     void speculateNotCell(Edge);
1635     void speculateOther(Edge, JSValueRegs, GPRReg temp);
1636     void speculateOther(Edge, JSValueRegs);
1637     void speculateOther(Edge);
1638     void speculateMisc(Edge, JSValueRegs);
1639     void speculateMisc(Edge);
1640     void speculate(Node*, Edge);
1641 
1642     JITCompiler::JumpList jumpSlowForUnwantedArrayMode(GPRReg tempWithIndexingTypeReg, ArrayMode);
1643     void checkArray(Node*);
1644     void arrayify(Node*, GPRReg baseReg, GPRReg propertyReg);
1645     void arrayify(Node*);
1646 
1647     template&lt;bool strict&gt;
1648     GPRReg fillSpeculateInt32Internal(Edge, DataFormat&amp; returnFormat);
1649 
1650     void cageTypedArrayStorage(GPRReg, GPRReg);
1651 
1652     void recordSetLocal(
1653         Operand bytecodeReg, VirtualRegister machineReg, DataFormat format)
1654     {
1655         ASSERT(!bytecodeReg.isArgument() || bytecodeReg.virtualRegister().toArgument() &gt;= 0);
1656         m_stream-&gt;appendAndLog(VariableEvent::setLocal(bytecodeReg, machineReg, format));
1657     }
1658 
1659     void recordSetLocal(DataFormat format)
1660     {
1661         VariableAccessData* variable = m_currentNode-&gt;variableAccessData();
1662         recordSetLocal(variable-&gt;operand(), variable-&gt;machineLocal(), format);
1663     }
1664 
1665     GenerationInfo&amp; generationInfoFromVirtualRegister(VirtualRegister virtualRegister)
1666     {
1667         return m_generationInfo[virtualRegister.toLocal()];
1668     }
1669 
1670     GenerationInfo&amp; generationInfo(Node* node)
1671     {
1672         return generationInfoFromVirtualRegister(node-&gt;virtualRegister());
1673     }
1674 
1675     GenerationInfo&amp; generationInfo(Edge edge)
1676     {
1677         return generationInfo(edge.node());
1678     }
1679 
1680     // The JIT, while also provides MacroAssembler functionality.
1681     JITCompiler&amp; m_jit;
1682     Graph&amp; m_graph;
1683 
1684     // The current node being generated.
1685     BasicBlock* m_block;
1686     Node* m_currentNode;
1687     NodeType m_lastGeneratedNode;
1688     unsigned m_indexInBlock;
1689 
1690     // Virtual and physical register maps.
1691     Vector&lt;GenerationInfo, 32&gt; m_generationInfo;
1692     RegisterBank&lt;GPRInfo&gt; m_gprs;
1693     RegisterBank&lt;FPRInfo&gt; m_fprs;
1694 
1695     // It is possible, during speculative generation, to reach a situation in which we
1696     // can statically determine a speculation will fail (for example, when two nodes
1697     // will make conflicting speculations about the same operand). In such cases this
1698     // flag is cleared, indicating no further code generation should take place.
1699     bool m_compileOkay;
1700 
1701     Vector&lt;MacroAssembler::Label&gt; m_osrEntryHeads;
1702 
1703     struct BranchRecord {
1704         BranchRecord(MacroAssembler::Jump jump, BasicBlock* destination)
1705             : jump(jump)
1706             , destination(destination)
1707         {
1708         }
1709 
1710         MacroAssembler::Jump jump;
1711         BasicBlock* destination;
1712     };
1713     Vector&lt;BranchRecord, 8&gt; m_branches;
1714 
1715     NodeOrigin m_origin;
1716 
1717     InPlaceAbstractState m_state;
1718     AbstractInterpreter&lt;InPlaceAbstractState&gt; m_interpreter;
1719 
1720     VariableEventStream* m_stream;
1721     MinifiedGraph* m_minifiedGraph;
1722 
1723     Vector&lt;std::unique_ptr&lt;SlowPathGenerator&gt;, 8&gt; m_slowPathGenerators;
1724     struct SlowPathLambda {
1725         Function&lt;void()&gt; generator;
1726         Node* currentNode;
1727         unsigned streamIndex;
1728     };
1729     Vector&lt;SlowPathLambda&gt; m_slowPathLambdas;
1730     Vector&lt;SilentRegisterSavePlan&gt; m_plans;
1731     Optional&lt;unsigned&gt; m_outOfLineStreamIndex;
1732 };
1733 
1734 
1735 // === Operand types ===
1736 //
1737 // These classes are used to lock the operands to a node into machine
1738 // registers. These classes implement of pattern of locking a value
1739 // into register at the point of construction only if it is already in
1740 // registers, and otherwise loading it lazily at the point it is first
1741 // used. We do so in order to attempt to avoid spilling one operand
1742 // in order to make space available for another.
1743 
1744 class JSValueOperand {
1745     WTF_MAKE_FAST_ALLOCATED;
1746 public:
1747     explicit JSValueOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
1748         : m_jit(jit)
1749         , m_edge(edge)
1750 #if USE(JSVALUE64)
1751         , m_gprOrInvalid(InvalidGPRReg)
1752 #elif USE(JSVALUE32_64)
1753         , m_isDouble(false)
1754 #endif
1755     {
1756         ASSERT(m_jit);
1757         if (!edge)
1758             return;
1759         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || edge.useKind() == UntypedUse);
1760 #if USE(JSVALUE64)
1761         if (jit-&gt;isFilled(node()))
1762             gpr();
1763 #elif USE(JSVALUE32_64)
1764         m_register.pair.tagGPR = InvalidGPRReg;
1765         m_register.pair.payloadGPR = InvalidGPRReg;
1766         if (jit-&gt;isFilled(node()))
1767             fill();
1768 #endif
1769     }
1770 
1771     explicit JSValueOperand(JSValueOperand&amp;&amp; other)
1772         : m_jit(other.m_jit)
1773         , m_edge(other.m_edge)
1774     {
1775 #if USE(JSVALUE64)
1776         m_gprOrInvalid = other.m_gprOrInvalid;
1777 #elif USE(JSVALUE32_64)
1778         m_register.pair.tagGPR = InvalidGPRReg;
1779         m_register.pair.payloadGPR = InvalidGPRReg;
1780         m_isDouble = other.m_isDouble;
1781 
1782         if (m_edge) {
1783             if (m_isDouble)
1784                 m_register.fpr = other.m_register.fpr;
1785             else
1786                 m_register.pair = other.m_register.pair;
1787         }
1788 #endif
1789         other.m_edge = Edge();
1790 #if USE(JSVALUE64)
1791         other.m_gprOrInvalid = InvalidGPRReg;
1792 #elif USE(JSVALUE32_64)
1793         other.m_isDouble = false;
1794 #endif
1795     }
1796 
1797     ~JSValueOperand()
1798     {
1799         if (!m_edge)
1800             return;
1801 #if USE(JSVALUE64)
1802         ASSERT(m_gprOrInvalid != InvalidGPRReg);
1803         m_jit-&gt;unlock(m_gprOrInvalid);
1804 #elif USE(JSVALUE32_64)
1805         if (m_isDouble) {
1806             ASSERT(m_register.fpr != InvalidFPRReg);
1807             m_jit-&gt;unlock(m_register.fpr);
1808         } else {
1809             ASSERT(m_register.pair.tagGPR != InvalidGPRReg &amp;&amp; m_register.pair.payloadGPR != InvalidGPRReg);
1810             m_jit-&gt;unlock(m_register.pair.tagGPR);
1811             m_jit-&gt;unlock(m_register.pair.payloadGPR);
1812         }
1813 #endif
1814     }
1815 
1816     Edge edge() const
1817     {
1818         return m_edge;
1819     }
1820 
1821     Node* node() const
1822     {
1823         return edge().node();
1824     }
1825 
1826 #if USE(JSVALUE64)
1827     GPRReg gpr()
1828     {
1829         if (m_gprOrInvalid == InvalidGPRReg)
1830             m_gprOrInvalid = m_jit-&gt;fillJSValue(m_edge);
1831         return m_gprOrInvalid;
1832     }
1833     JSValueRegs jsValueRegs()
1834     {
1835         return JSValueRegs(gpr());
1836     }
1837 #elif USE(JSVALUE32_64)
1838     bool isDouble() { return m_isDouble; }
1839 
1840     void fill()
1841     {
1842         if (m_register.pair.tagGPR == InvalidGPRReg &amp;&amp; m_register.pair.payloadGPR == InvalidGPRReg)
1843             m_isDouble = !m_jit-&gt;fillJSValue(m_edge, m_register.pair.tagGPR, m_register.pair.payloadGPR, m_register.fpr);
1844     }
1845 
1846     GPRReg tagGPR()
1847     {
1848         fill();
1849         ASSERT(!m_isDouble);
1850         return m_register.pair.tagGPR;
1851     }
1852 
1853     GPRReg payloadGPR()
1854     {
1855         fill();
1856         ASSERT(!m_isDouble);
1857         return m_register.pair.payloadGPR;
1858     }
1859 
1860     JSValueRegs jsValueRegs()
1861     {
1862         return JSValueRegs(tagGPR(), payloadGPR());
1863     }
1864 
1865     GPRReg gpr(WhichValueWord which)
1866     {
1867         return jsValueRegs().gpr(which);
1868     }
1869 
1870     FPRReg fpr()
1871     {
1872         fill();
1873         ASSERT(m_isDouble);
1874         return m_register.fpr;
1875     }
1876 #endif
1877 
1878     void use()
1879     {
1880         m_jit-&gt;use(node());
1881     }
1882 
1883 private:
1884     SpeculativeJIT* m_jit;
1885     Edge m_edge;
1886 #if USE(JSVALUE64)
1887     GPRReg m_gprOrInvalid;
1888 #elif USE(JSVALUE32_64)
1889     union {
1890         struct {
1891             GPRReg tagGPR;
1892             GPRReg payloadGPR;
1893         } pair;
1894         FPRReg fpr;
1895     } m_register;
1896     bool m_isDouble;
1897 #endif
1898 };
1899 
1900 class StorageOperand {
1901     WTF_MAKE_FAST_ALLOCATED;
1902 public:
1903     explicit StorageOperand(SpeculativeJIT* jit, Edge edge)
1904         : m_jit(jit)
1905         , m_edge(edge)
1906         , m_gprOrInvalid(InvalidGPRReg)
1907     {
1908         ASSERT(m_jit);
1909         ASSERT(edge.useKind() == UntypedUse || edge.useKind() == KnownCellUse);
1910         if (jit-&gt;isFilled(node()))
1911             gpr();
1912     }
1913 
1914     ~StorageOperand()
1915     {
1916         ASSERT(m_gprOrInvalid != InvalidGPRReg);
1917         m_jit-&gt;unlock(m_gprOrInvalid);
1918     }
1919 
1920     Edge edge() const
1921     {
1922         return m_edge;
1923     }
1924 
1925     Node* node() const
1926     {
1927         return edge().node();
1928     }
1929 
1930     GPRReg gpr()
1931     {
1932         if (m_gprOrInvalid == InvalidGPRReg)
1933             m_gprOrInvalid = m_jit-&gt;fillStorage(edge());
1934         return m_gprOrInvalid;
1935     }
1936 
1937     void use()
1938     {
1939         m_jit-&gt;use(node());
1940     }
1941 
1942 private:
1943     SpeculativeJIT* m_jit;
1944     Edge m_edge;
1945     GPRReg m_gprOrInvalid;
1946 };
1947 
1948 
1949 // === Temporaries ===
1950 //
1951 // These classes are used to allocate temporary registers.
1952 // A mechanism is provided to attempt to reuse the registers
1953 // currently allocated to child nodes whose value is consumed
1954 // by, and not live after, this operation.
1955 
1956 enum ReuseTag { Reuse };
1957 
1958 class GPRTemporary {
1959     WTF_MAKE_FAST_ALLOCATED;
1960 public:
1961     GPRTemporary();
1962     GPRTemporary(SpeculativeJIT*);
1963     GPRTemporary(SpeculativeJIT*, GPRReg specific);
1964     template&lt;typename T&gt;
1965     GPRTemporary(SpeculativeJIT* jit, ReuseTag, T&amp; operand)
1966         : m_jit(jit)
1967         , m_gpr(InvalidGPRReg)
1968     {
1969         if (m_jit-&gt;canReuse(operand.node()))
1970             m_gpr = m_jit-&gt;reuse(operand.gpr());
1971         else
1972             m_gpr = m_jit-&gt;allocate();
1973     }
1974     template&lt;typename T1, typename T2&gt;
1975     GPRTemporary(SpeculativeJIT* jit, ReuseTag, T1&amp; op1, T2&amp; op2)
1976         : m_jit(jit)
1977         , m_gpr(InvalidGPRReg)
1978     {
1979         if (m_jit-&gt;canReuse(op1.node()))
1980             m_gpr = m_jit-&gt;reuse(op1.gpr());
1981         else if (m_jit-&gt;canReuse(op2.node()))
1982             m_gpr = m_jit-&gt;reuse(op2.gpr());
1983         else if (m_jit-&gt;canReuse(op1.node(), op2.node()) &amp;&amp; op1.gpr() == op2.gpr())
1984             m_gpr = m_jit-&gt;reuse(op1.gpr());
1985         else
1986             m_gpr = m_jit-&gt;allocate();
1987     }
1988     GPRTemporary(SpeculativeJIT*, ReuseTag, JSValueOperand&amp;, WhichValueWord);
1989 
1990     GPRTemporary(GPRTemporary&amp; other) = delete;
1991 
1992     GPRTemporary(GPRTemporary&amp;&amp; other)
1993     {
1994         ASSERT(other.m_jit);
1995         ASSERT(other.m_gpr != InvalidGPRReg);
1996         m_jit = other.m_jit;
1997         m_gpr = other.m_gpr;
1998         other.m_jit = nullptr;
1999         other.m_gpr = InvalidGPRReg;
2000     }
2001 
2002     GPRTemporary&amp; operator=(GPRTemporary&amp;&amp; other)
2003     {
2004         ASSERT(!m_jit);
2005         ASSERT(m_gpr == InvalidGPRReg);
2006         std::swap(m_jit, other.m_jit);
2007         std::swap(m_gpr, other.m_gpr);
2008         return *this;
2009     }
2010 
2011     void adopt(GPRTemporary&amp;);
2012 
2013     ~GPRTemporary()
2014     {
2015         if (m_jit &amp;&amp; m_gpr != InvalidGPRReg)
2016             m_jit-&gt;unlock(gpr());
2017     }
2018 
2019     GPRReg gpr()
2020     {
2021         return m_gpr;
2022     }
2023 
2024 private:
2025     SpeculativeJIT* m_jit;
2026     GPRReg m_gpr;
2027 };
2028 
2029 class JSValueRegsTemporary {
2030     WTF_MAKE_FAST_ALLOCATED;
2031 public:
2032     JSValueRegsTemporary();
2033     JSValueRegsTemporary(SpeculativeJIT*);
2034     template&lt;typename T&gt;
2035     JSValueRegsTemporary(SpeculativeJIT*, ReuseTag, T&amp; operand, WhichValueWord resultRegWord = PayloadWord);
2036     JSValueRegsTemporary(SpeculativeJIT*, ReuseTag, JSValueOperand&amp;);
2037     ~JSValueRegsTemporary();
2038 
2039     JSValueRegs regs();
2040 
2041 private:
2042 #if USE(JSVALUE64)
2043     GPRTemporary m_gpr;
2044 #else
2045     GPRTemporary m_payloadGPR;
2046     GPRTemporary m_tagGPR;
2047 #endif
2048 };
2049 
2050 class FPRTemporary {
2051     WTF_MAKE_FAST_ALLOCATED;
2052 public:
2053     FPRTemporary(FPRTemporary&amp;&amp;);
2054     FPRTemporary(SpeculativeJIT*);
2055     FPRTemporary(SpeculativeJIT*, SpeculateDoubleOperand&amp;);
2056     FPRTemporary(SpeculativeJIT*, SpeculateDoubleOperand&amp;, SpeculateDoubleOperand&amp;);
2057 #if USE(JSVALUE32_64)
2058     FPRTemporary(SpeculativeJIT*, JSValueOperand&amp;);
2059 #endif
2060 
2061     ~FPRTemporary()
2062     {
2063         if (LIKELY(m_jit))
2064             m_jit-&gt;unlock(fpr());
2065     }
2066 
2067     FPRReg fpr() const
2068     {
2069         ASSERT(m_jit);
2070         ASSERT(m_fpr != InvalidFPRReg);
2071         return m_fpr;
2072     }
2073 
2074 protected:
2075     FPRTemporary(SpeculativeJIT* jit, FPRReg lockedFPR)
2076         : m_jit(jit)
2077         , m_fpr(lockedFPR)
2078     {
2079     }
2080 
2081 private:
2082     SpeculativeJIT* m_jit;
2083     FPRReg m_fpr;
2084 };
2085 
2086 
2087 // === Results ===
2088 //
2089 // These classes lock the result of a call to a C++ helper function.
2090 
2091 class GPRFlushedCallResult : public GPRTemporary {
2092 public:
2093     GPRFlushedCallResult(SpeculativeJIT* jit)
2094         : GPRTemporary(jit, GPRInfo::returnValueGPR)
2095     {
2096     }
2097 };
2098 
2099 #if USE(JSVALUE32_64)
2100 class GPRFlushedCallResult2 : public GPRTemporary {
2101 public:
2102     GPRFlushedCallResult2(SpeculativeJIT* jit)
2103         : GPRTemporary(jit, GPRInfo::returnValueGPR2)
2104     {
2105     }
2106 };
2107 #endif
2108 
2109 class FPRResult : public FPRTemporary {
2110 public:
2111     FPRResult(SpeculativeJIT* jit)
2112         : FPRTemporary(jit, lockedResult(jit))
2113     {
2114     }
2115 
2116 private:
2117     static FPRReg lockedResult(SpeculativeJIT* jit)
2118     {
2119         jit-&gt;lock(FPRInfo::returnValueFPR);
2120         return FPRInfo::returnValueFPR;
2121     }
2122 };
2123 
2124 class JSValueRegsFlushedCallResult {
2125     WTF_MAKE_FAST_ALLOCATED;
2126 public:
2127     JSValueRegsFlushedCallResult(SpeculativeJIT* jit)
2128 #if USE(JSVALUE64)
2129         : m_gpr(jit)
2130 #else
2131         : m_payloadGPR(jit)
2132         , m_tagGPR(jit)
2133 #endif
2134     {
2135     }
2136 
2137     JSValueRegs regs()
2138     {
2139 #if USE(JSVALUE64)
2140         return JSValueRegs { m_gpr.gpr() };
2141 #else
2142         return JSValueRegs { m_tagGPR.gpr(), m_payloadGPR.gpr() };
2143 #endif
2144     }
2145 
2146 private:
2147 #if USE(JSVALUE64)
2148     GPRFlushedCallResult m_gpr;
2149 #else
2150     GPRFlushedCallResult m_payloadGPR;
2151     GPRFlushedCallResult2 m_tagGPR;
2152 #endif
2153 };
2154 
2155 
2156 // === Speculative Operand types ===
2157 //
2158 // SpeculateInt32Operand, SpeculateStrictInt32Operand and SpeculateCellOperand.
2159 //
2160 // These are used to lock the operands to a node into machine registers within the
2161 // SpeculativeJIT. The classes operate like those above, however these will
2162 // perform a speculative check for a more restrictive type than we can statically
2163 // determine the operand to have. If the operand does not have the requested type,
2164 // a bail-out to the non-speculative path will be taken.
2165 
2166 class SpeculateInt32Operand {
2167     WTF_MAKE_FAST_ALLOCATED;
2168 public:
2169     explicit SpeculateInt32Operand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2170         : m_jit(jit)
2171         , m_edge(edge)
2172         , m_gprOrInvalid(InvalidGPRReg)
2173 #ifndef NDEBUG
2174         , m_format(DataFormatNone)
2175 #endif
2176     {
2177         ASSERT(m_jit);
2178         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || (edge.useKind() == Int32Use || edge.useKind() == KnownInt32Use));
2179         if (jit-&gt;isFilled(node()))
2180             gpr();
2181     }
2182 
2183     ~SpeculateInt32Operand()
2184     {
2185         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2186         m_jit-&gt;unlock(m_gprOrInvalid);
2187     }
2188 
2189     Edge edge() const
2190     {
2191         return m_edge;
2192     }
2193 
2194     Node* node() const
2195     {
2196         return edge().node();
2197     }
2198 
2199     DataFormat format()
2200     {
2201         gpr(); // m_format is set when m_gpr is locked.
2202         ASSERT(m_format == DataFormatInt32 || m_format == DataFormatJSInt32);
2203         return m_format;
2204     }
2205 
2206     GPRReg gpr()
2207     {
2208         if (m_gprOrInvalid == InvalidGPRReg)
2209             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt32(edge(), m_format);
2210         return m_gprOrInvalid;
2211     }
2212 
2213     void use()
2214     {
2215         m_jit-&gt;use(node());
2216     }
2217 
2218 private:
2219     SpeculativeJIT* m_jit;
2220     Edge m_edge;
2221     GPRReg m_gprOrInvalid;
2222     DataFormat m_format;
2223 };
2224 
2225 class SpeculateStrictInt32Operand {
2226     WTF_MAKE_FAST_ALLOCATED;
2227 public:
2228     explicit SpeculateStrictInt32Operand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2229         : m_jit(jit)
2230         , m_edge(edge)
2231         , m_gprOrInvalid(InvalidGPRReg)
2232     {
2233         ASSERT(m_jit);
2234         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || (edge.useKind() == Int32Use || edge.useKind() == KnownInt32Use));
2235         if (jit-&gt;isFilled(node()))
2236             gpr();
2237     }
2238 
2239     ~SpeculateStrictInt32Operand()
2240     {
2241         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2242         m_jit-&gt;unlock(m_gprOrInvalid);
2243     }
2244 
2245     Edge edge() const
2246     {
2247         return m_edge;
2248     }
2249 
2250     Node* node() const
2251     {
2252         return edge().node();
2253     }
2254 
2255     GPRReg gpr()
2256     {
2257         if (m_gprOrInvalid == InvalidGPRReg)
2258             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt32Strict(edge());
2259         return m_gprOrInvalid;
2260     }
2261 
2262     void use()
2263     {
2264         m_jit-&gt;use(node());
2265     }
2266 
2267 private:
2268     SpeculativeJIT* m_jit;
2269     Edge m_edge;
2270     GPRReg m_gprOrInvalid;
2271 };
2272 
2273 // Gives you a canonical Int52 (i.e. it&#39;s left-shifted by 16, low bits zero).
2274 class SpeculateInt52Operand {
2275     WTF_MAKE_FAST_ALLOCATED;
2276 public:
2277     explicit SpeculateInt52Operand(SpeculativeJIT* jit, Edge edge)
2278         : m_jit(jit)
2279         , m_edge(edge)
2280         , m_gprOrInvalid(InvalidGPRReg)
2281     {
2282         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2283         if (jit-&gt;isFilled(node()))
2284             gpr();
2285     }
2286 
2287     ~SpeculateInt52Operand()
2288     {
2289         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2290         m_jit-&gt;unlock(m_gprOrInvalid);
2291     }
2292 
2293     Edge edge() const
2294     {
2295         return m_edge;
2296     }
2297 
2298     Node* node() const
2299     {
2300         return edge().node();
2301     }
2302 
2303     GPRReg gpr()
2304     {
2305         if (m_gprOrInvalid == InvalidGPRReg)
2306             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(edge(), DataFormatInt52);
2307         return m_gprOrInvalid;
2308     }
2309 
2310     void use()
2311     {
2312         m_jit-&gt;use(node());
2313     }
2314 
2315 private:
2316     SpeculativeJIT* m_jit;
2317     Edge m_edge;
2318     GPRReg m_gprOrInvalid;
2319 };
2320 
2321 // Gives you a strict Int52 (i.e. the payload is in the low 48 bits, high 16 bits are sign-extended).
2322 class SpeculateStrictInt52Operand {
2323     WTF_MAKE_FAST_ALLOCATED;
2324 public:
2325     explicit SpeculateStrictInt52Operand(SpeculativeJIT* jit, Edge edge)
2326         : m_jit(jit)
2327         , m_edge(edge)
2328         , m_gprOrInvalid(InvalidGPRReg)
2329     {
2330         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2331         if (jit-&gt;isFilled(node()))
2332             gpr();
2333     }
2334 
2335     ~SpeculateStrictInt52Operand()
2336     {
2337         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2338         m_jit-&gt;unlock(m_gprOrInvalid);
2339     }
2340 
2341     Edge edge() const
2342     {
2343         return m_edge;
2344     }
2345 
2346     Node* node() const
2347     {
2348         return edge().node();
2349     }
2350 
2351     GPRReg gpr()
2352     {
2353         if (m_gprOrInvalid == InvalidGPRReg)
2354             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(edge(), DataFormatStrictInt52);
2355         return m_gprOrInvalid;
2356     }
2357 
2358     void use()
2359     {
2360         m_jit-&gt;use(node());
2361     }
2362 
2363 private:
2364     SpeculativeJIT* m_jit;
2365     Edge m_edge;
2366     GPRReg m_gprOrInvalid;
2367 };
2368 
2369 enum OppositeShiftTag { OppositeShift };
2370 
2371 class SpeculateWhicheverInt52Operand {
2372     WTF_MAKE_FAST_ALLOCATED;
2373 public:
2374     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge)
2375         : m_jit(jit)
2376         , m_edge(edge)
2377         , m_gprOrInvalid(InvalidGPRReg)
2378         , m_strict(jit-&gt;betterUseStrictInt52(edge))
2379     {
2380         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2381         if (jit-&gt;isFilled(node()))
2382             gpr();
2383     }
2384 
2385     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge, const SpeculateWhicheverInt52Operand&amp; other)
2386         : m_jit(jit)
2387         , m_edge(edge)
2388         , m_gprOrInvalid(InvalidGPRReg)
2389         , m_strict(other.m_strict)
2390     {
2391         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2392         if (jit-&gt;isFilled(node()))
2393             gpr();
2394     }
2395 
2396     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge, OppositeShiftTag, const SpeculateWhicheverInt52Operand&amp; other)
2397         : m_jit(jit)
2398         , m_edge(edge)
2399         , m_gprOrInvalid(InvalidGPRReg)
2400         , m_strict(!other.m_strict)
2401     {
2402         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2403         if (jit-&gt;isFilled(node()))
2404             gpr();
2405     }
2406 
2407     ~SpeculateWhicheverInt52Operand()
2408     {
2409         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2410         m_jit-&gt;unlock(m_gprOrInvalid);
2411     }
2412 
2413     Edge edge() const
2414     {
2415         return m_edge;
2416     }
2417 
2418     Node* node() const
2419     {
2420         return edge().node();
2421     }
2422 
2423     GPRReg gpr()
2424     {
2425         if (m_gprOrInvalid == InvalidGPRReg) {
2426             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(
2427                 edge(), m_strict ? DataFormatStrictInt52 : DataFormatInt52);
2428         }
2429         return m_gprOrInvalid;
2430     }
2431 
2432     void use()
2433     {
2434         m_jit-&gt;use(node());
2435     }
2436 
2437     DataFormat format() const
2438     {
2439         return m_strict ? DataFormatStrictInt52 : DataFormatInt52;
2440     }
2441 
2442 private:
2443     SpeculativeJIT* m_jit;
2444     Edge m_edge;
2445     GPRReg m_gprOrInvalid;
2446     bool m_strict;
2447 };
2448 
2449 class SpeculateDoubleOperand {
2450     WTF_MAKE_FAST_ALLOCATED;
2451 public:
2452     explicit SpeculateDoubleOperand(SpeculativeJIT* jit, Edge edge)
2453         : m_jit(jit)
2454         , m_edge(edge)
2455         , m_fprOrInvalid(InvalidFPRReg)
2456     {
2457         ASSERT(m_jit);
2458         RELEASE_ASSERT(isDouble(edge.useKind()));
2459         if (jit-&gt;isFilled(node()))
2460             fpr();
2461     }
2462 
2463     ~SpeculateDoubleOperand()
2464     {
2465         ASSERT(m_fprOrInvalid != InvalidFPRReg);
2466         m_jit-&gt;unlock(m_fprOrInvalid);
2467     }
2468 
2469     Edge edge() const
2470     {
2471         return m_edge;
2472     }
2473 
2474     Node* node() const
2475     {
2476         return edge().node();
2477     }
2478 
2479     FPRReg fpr()
2480     {
2481         if (m_fprOrInvalid == InvalidFPRReg)
2482             m_fprOrInvalid = m_jit-&gt;fillSpeculateDouble(edge());
2483         return m_fprOrInvalid;
2484     }
2485 
2486     void use()
2487     {
2488         m_jit-&gt;use(node());
2489     }
2490 
2491 private:
2492     SpeculativeJIT* m_jit;
2493     Edge m_edge;
2494     FPRReg m_fprOrInvalid;
2495 };
2496 
2497 class SpeculateCellOperand {
2498     WTF_MAKE_FAST_ALLOCATED;
2499 
2500 public:
2501     explicit SpeculateCellOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2502         : m_jit(jit)
2503         , m_edge(edge)
2504         , m_gprOrInvalid(InvalidGPRReg)
2505     {
2506         ASSERT(m_jit);
2507         if (!edge)
2508             return;
2509         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || isCell(edge.useKind()));
2510         if (jit-&gt;isFilled(node()))
2511             gpr();
2512     }
2513 
2514     explicit SpeculateCellOperand(SpeculateCellOperand&amp;&amp; other)
2515     {
2516         m_jit = other.m_jit;
2517         m_edge = other.m_edge;
2518         m_gprOrInvalid = other.m_gprOrInvalid;
2519 
2520         other.m_gprOrInvalid = InvalidGPRReg;
2521         other.m_edge = Edge();
2522     }
2523 
2524     ~SpeculateCellOperand()
2525     {
2526         if (!m_edge)
2527             return;
2528         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2529         m_jit-&gt;unlock(m_gprOrInvalid);
2530     }
2531 
2532     Edge edge() const
2533     {
2534         return m_edge;
2535     }
2536 
2537     Node* node() const
2538     {
2539         return edge().node();
2540     }
2541 
2542     GPRReg gpr()
2543     {
2544         ASSERT(m_edge);
2545         if (m_gprOrInvalid == InvalidGPRReg)
2546             m_gprOrInvalid = m_jit-&gt;fillSpeculateCell(edge());
2547         return m_gprOrInvalid;
2548     }
2549 
2550     void use()
2551     {
2552         ASSERT(m_edge);
2553         m_jit-&gt;use(node());
2554     }
2555 
2556 private:
2557     SpeculativeJIT* m_jit;
2558     Edge m_edge;
2559     GPRReg m_gprOrInvalid;
2560 };
2561 
2562 class SpeculateBooleanOperand {
2563     WTF_MAKE_FAST_ALLOCATED;
2564 public:
2565     explicit SpeculateBooleanOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2566         : m_jit(jit)
2567         , m_edge(edge)
2568         , m_gprOrInvalid(InvalidGPRReg)
2569     {
2570         ASSERT(m_jit);
2571         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || edge.useKind() == BooleanUse || edge.useKind() == KnownBooleanUse);
2572         if (jit-&gt;isFilled(node()))
2573             gpr();
2574     }
2575 
2576     ~SpeculateBooleanOperand()
2577     {
2578         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2579         m_jit-&gt;unlock(m_gprOrInvalid);
2580     }
2581 
2582     Edge edge() const
2583     {
2584         return m_edge;
2585     }
2586 
2587     Node* node() const
2588     {
2589         return edge().node();
2590     }
2591 
2592     GPRReg gpr()
2593     {
2594         if (m_gprOrInvalid == InvalidGPRReg)
2595             m_gprOrInvalid = m_jit-&gt;fillSpeculateBoolean(edge());
2596         return m_gprOrInvalid;
2597     }
2598 
2599     void use()
2600     {
2601         m_jit-&gt;use(node());
2602     }
2603 
2604 private:
2605     SpeculativeJIT* m_jit;
2606     Edge m_edge;
2607     GPRReg m_gprOrInvalid;
2608 };
2609 
2610 #define DFG_TYPE_CHECK_WITH_EXIT_KIND(exitKind, source, edge, typesPassedThrough, jumpToFail) do { \
2611         JSValueSource _dtc_source = (source);                           \
2612         Edge _dtc_edge = (edge);                                        \
2613         SpeculatedType _dtc_typesPassedThrough = typesPassedThrough;    \
2614         if (!needsTypeCheck(_dtc_edge, _dtc_typesPassedThrough))        \
2615             break;                                                      \
2616         typeCheck(_dtc_source, _dtc_edge, _dtc_typesPassedThrough, (jumpToFail), exitKind); \
2617     } while (0)
2618 
2619 #define DFG_TYPE_CHECK(source, edge, typesPassedThrough, jumpToFail) \
2620     DFG_TYPE_CHECK_WITH_EXIT_KIND(BadType, source, edge, typesPassedThrough, jumpToFail)
2621 
2622 } } // namespace JSC::DFG
2623 
2624 #endif
    </pre>
  </body>
</html>