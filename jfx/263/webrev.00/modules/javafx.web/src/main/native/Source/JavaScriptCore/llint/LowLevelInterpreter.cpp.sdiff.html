<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="LowLevelInterpreter.asm.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="LowLevelInterpreter.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
127 
128 //============================================================================
129 // CLoopRegister is the storage for an emulated CPU register.
130 // It defines the policy of how ints smaller than intptr_t are packed into the
131 // pseudo register, as well as hides endianness differences.
132 
133 class CLoopRegister {
134 public:
135     ALWAYS_INLINE intptr_t i() const { return m_value; };
136     ALWAYS_INLINE uintptr_t u() const { return m_value; }
137     ALWAYS_INLINE int32_t i32() const { return m_value; }
138     ALWAYS_INLINE uint32_t u32() const { return m_value; }
139     ALWAYS_INLINE int8_t i8() const { return m_value; }
140     ALWAYS_INLINE uint8_t u8() const { return m_value; }
141 
142     ALWAYS_INLINE intptr_t* ip() const { return bitwise_cast&lt;intptr_t*&gt;(m_value); }
143     ALWAYS_INLINE int8_t* i8p() const { return bitwise_cast&lt;int8_t*&gt;(m_value); }
144     ALWAYS_INLINE void* vp() const { return bitwise_cast&lt;void*&gt;(m_value); }
145     ALWAYS_INLINE const void* cvp() const { return bitwise_cast&lt;const void*&gt;(m_value); }
146     ALWAYS_INLINE CallFrame* callFrame() const { return bitwise_cast&lt;CallFrame*&gt;(m_value); }
<span class="line-removed">147     ALWAYS_INLINE ExecState* execState() const { return bitwise_cast&lt;ExecState*&gt;(m_value); }</span>
148     ALWAYS_INLINE const void* instruction() const { return bitwise_cast&lt;const void*&gt;(m_value); }
149     ALWAYS_INLINE VM* vm() const { return bitwise_cast&lt;VM*&gt;(m_value); }
150     ALWAYS_INLINE JSCell* cell() const { return bitwise_cast&lt;JSCell*&gt;(m_value); }
151     ALWAYS_INLINE ProtoCallFrame* protoCallFrame() const { return bitwise_cast&lt;ProtoCallFrame*&gt;(m_value); }
152     ALWAYS_INLINE NativeFunction nativeFunc() const { return bitwise_cast&lt;NativeFunction&gt;(m_value); }
153 #if USE(JSVALUE64)
154     ALWAYS_INLINE int64_t i64() const { return m_value; }
155     ALWAYS_INLINE uint64_t u64() const { return m_value; }
156     ALWAYS_INLINE EncodedJSValue encodedJSValue() const { return bitwise_cast&lt;EncodedJSValue&gt;(m_value); }
157 #endif
158     ALWAYS_INLINE Opcode opcode() const { return bitwise_cast&lt;Opcode&gt;(m_value); }
159 
<span class="line-modified">160     operator ExecState*() { return bitwise_cast&lt;ExecState*&gt;(m_value); }</span>
161     operator const Instruction*() { return bitwise_cast&lt;const Instruction*&gt;(m_value); }
162     operator JSCell*() { return bitwise_cast&lt;JSCell*&gt;(m_value); }
163     operator ProtoCallFrame*() { return bitwise_cast&lt;ProtoCallFrame*&gt;(m_value); }
164     operator Register*() { return bitwise_cast&lt;Register*&gt;(m_value); }
165     operator VM*() { return bitwise_cast&lt;VM*&gt;(m_value); }
166 
167     template&lt;typename T, typename = std::enable_if_t&lt;sizeof(T) == sizeof(uintptr_t)&gt;&gt;
168     ALWAYS_INLINE void operator=(T value) { m_value = bitwise_cast&lt;uintptr_t&gt;(value); }
169 #if USE(JSVALUE64)
170     ALWAYS_INLINE void operator=(int32_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
171     ALWAYS_INLINE void operator=(uint32_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
172 #endif
173     ALWAYS_INLINE void operator=(int16_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
174     ALWAYS_INLINE void operator=(uint16_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
175     ALWAYS_INLINE void operator=(int8_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
176     ALWAYS_INLINE void operator=(uint8_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
177     ALWAYS_INLINE void operator=(bool value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
178 
179 #if USE(JSVALUE64)
180     ALWAYS_INLINE double bitsAsDouble() const { return bitwise_cast&lt;double&gt;(m_value); }
</pre>
<hr />
<pre>
264         // FIXME: this mapping is unnecessarily expensive in the absence of COMPUTED_GOTO
265         //   narrow opcodes don&#39;t need any mapping and wide opcodes just need to add numOpcodeIDs
266         #define OPCODE_ENTRY(__opcode, length) \
267             opcodeMap[__opcode] = __opcode; \
268             opcodeMapWide16[__opcode] = static_cast&lt;OpcodeID&gt;(__opcode##_wide16); \
269             opcodeMapWide32[__opcode] = static_cast&lt;OpcodeID&gt;(__opcode##_wide32);
270 
271         #define LLINT_OPCODE_ENTRY(__opcode, length) \
272             opcodeMap[__opcode] = __opcode;
273 #endif
274         FOR_EACH_BYTECODE_ID(OPCODE_ENTRY)
275         FOR_EACH_CLOOP_BYTECODE_HELPER_ID(LLINT_OPCODE_ENTRY)
276         FOR_EACH_LLINT_NATIVE_HELPER(LLINT_OPCODE_ENTRY)
277         #undef OPCODE_ENTRY
278         #undef LLINT_OPCODE_ENTRY
279 
280         // Note: we can only set the exceptionInstructions after we have
281         // initialized the opcodeMap above. This is because getCodePtr()
282         // can depend on the opcodeMap.
283         uint8_t* exceptionInstructions = reinterpret_cast&lt;uint8_t*&gt;(LLInt::exceptionInstructions());
<span class="line-modified">284         for (int i = 0; i &lt; maxOpcodeLength + 1; ++i)</span>
285             exceptionInstructions[i] = llint_throw_from_slow_path_trampoline;
286 
287         return JSValue();
288     }
289 
290     // Define the pseudo registers used by the LLINT C Loop backend:
291     static_assert(sizeof(CLoopRegister) == sizeof(intptr_t));
292 
293     // The CLoop llint backend is initially based on the ARMv7 backend, and
294     // then further enhanced with a few instructions from the x86 backend to
295     // support building for X64 targets. Hence, the shape of the generated
296     // code and the usage convention of registers will look a lot like the
297     // ARMv7 backend&#39;s.
298     //
299     // For example, on a 32-bit build:
300     // 1. Outgoing args will be set up as follows:
301     //    arg1 in t0 (r0 on ARM)
302     //    arg2 in t1 (r1 on ARM)
303     // 2. 32 bit return values will be in t0 (r0 on ARM).
304     // 3. 64 bit return values (e.g. doubles) will be in t0,t1 (r0,r1 on ARM).
305     //
306     // But instead of naming these simulator registers based on their ARM
307     // counterparts, we&#39;ll name them based on their original llint asm names.
308     // This will make it easier to correlate the generated code with the
309     // original llint asm code.
310     //
311     // On a 64-bit build, it more like x64 in that the registers are 64 bit.
312     // Hence:
313     // 1. Outgoing args are still the same: arg1 in t0, arg2 in t1, etc.
314     // 2. 32 bit result values will be in the low 32-bit of t0.
315     // 3. 64 bit result values will be in t0.
316 
317     CLoopRegister t0, t1, t2, t3, t5, sp, cfr, lr, pc;
318 #if USE(JSVALUE64)
<span class="line-modified">319     CLoopRegister pcBase, tagTypeNumber, tagMask;</span>
320 #endif

321     CLoopRegister metadataTable;
322     CLoopDoubleRegister d0, d1;
323 
324     struct StackPointerScope {
325         StackPointerScope(CLoopStack&amp; stack)
326             : m_stack(stack)
327             , m_originalStackPointer(stack.currentStackPointer())
328         { }
329 
330         ~StackPointerScope()
331         {
332             m_stack.setCurrentStackPointer(m_originalStackPointer);
333         }
334 
335     private:
336         CLoopStack&amp; m_stack;
337         void* m_originalStackPointer;
338     };
339 
340     CLoopStack&amp; cloopStack = vm-&gt;interpreter-&gt;cloopStack();
341     StackPointerScope stackPointerScope(cloopStack);
342 
343     lr = getOpcode(llint_return_to_host);
344     sp = cloopStack.currentStackPointer();
345     cfr = vm-&gt;topCallFrame;
346 #ifndef NDEBUG
347     void* startSP = sp.vp();
348     CallFrame* startCFR = cfr.callFrame();
349 #endif
350 
351     // Initialize the incoming args for doVMEntryToJavaScript:
352     t0 = executableAddress;
353     t1 = vm;
354     t2 = protoCallFrame;
355 
356 #if USE(JSVALUE64)
357     // For the ASM llint, JITStubs takes care of this initialization. We do
358     // it explicitly here for the C loop:
<span class="line-modified">359     tagTypeNumber = 0xFFFF000000000000;</span>
<span class="line-modified">360     tagMask = 0xFFFF000000000002;</span>
361 #endif // USE(JSVALUE64)
362 
363     // Interpreter variables for value passing between opcodes and/or helpers:
364     NativeFunction nativeFunc = nullptr;
365     JSValue functionReturnValue;
366     Opcode opcode = getOpcode(entryOpcodeID);
367 
368 #define PUSH(cloopReg) \
369     do { \
370         sp = sp.ip() - 1; \
371         *sp.ip() = cloopReg.i(); \
372     } while (false)
373 
374 #define POP(cloopReg) \
375     do { \
376         cloopReg = *sp.ip(); \
377         sp = sp.ip() + 1; \
378     } while (false)
379 
380 #if ENABLE(OPCODE_STATS)
381 #define RECORD_OPCODE_STATS(__opcode) OpcodeStats::recordInstruction(__opcode)
382 #else
383 #define RECORD_OPCODE_STATS(__opcode)
384 #endif
385 
<span class="line-removed">386 #if USE(JSVALUE32_64)</span>
<span class="line-removed">387 #define FETCH_OPCODE() *pc.i8p</span>
<span class="line-removed">388 #else // USE(JSVALUE64)</span>
<span class="line-removed">389 #define FETCH_OPCODE() *bitwise_cast&lt;OpcodeID*&gt;(pcBase.i8p + pc.i)</span>
<span class="line-removed">390 #endif // USE(JSVALUE64)</span>
<span class="line-removed">391 </span>
<span class="line-removed">392 #define NEXT_INSTRUCTION() \</span>
<span class="line-removed">393     do {                         \</span>
<span class="line-removed">394         opcode = FETCH_OPCODE(); \</span>
<span class="line-removed">395         DISPATCH_OPCODE();       \</span>
<span class="line-removed">396     } while (false)</span>
<span class="line-removed">397 </span>
398 #if ENABLE(COMPUTED_GOTO_OPCODES)
399 
400     //========================================================================
401     // Loop dispatch mechanism using computed goto statements:
402 
403     #define DISPATCH_OPCODE() goto *opcode
404 
405     #define DEFINE_OPCODE(__opcode) \
406         __opcode: \
407             RECORD_OPCODE_STATS(__opcode);
408 
409     // Dispatch to the current PC&#39;s bytecode:
410     DISPATCH_OPCODE();
411 
412 #else // !ENABLE(COMPUTED_GOTO_OPCODES)
413     //========================================================================
414     // Loop dispatch mechanism using a C switch statement:
415 
416     #define DISPATCH_OPCODE() goto dispatchOpcode
417 
</pre>
<hr />
<pre>
468 #if !ENABLE(COMPUTED_GOTO_OPCODES)
469     default:
470         ASSERT(false);
471 #endif
472 
473     } // END bytecode handler cases.
474 
475 #if ENABLE(COMPUTED_GOTO_OPCODES)
476     // Keep the compiler happy so that it doesn&#39;t complain about unused
477     // labels for the LLInt trampoline glue. The labels are automatically
478     // emitted by label macros above, and some of them are referenced by
479     // the llint generated code. Since we can&#39;t tell ahead of time which
480     // will be referenced and which will be not, we&#39;ll just passify the
481     // compiler on all such labels:
482     #define LLINT_OPCODE_ENTRY(__opcode, length) \
483         UNUSED_LABEL(__opcode);
484         FOR_EACH_OPCODE_ID(LLINT_OPCODE_ENTRY);
485     #undef LLINT_OPCODE_ENTRY
486 #endif
487 
<span class="line-removed">488     #undef NEXT_INSTRUCTION</span>
489     #undef DEFINE_OPCODE
490     #undef CHECK_FOR_TIMEOUT
491     #undef CAST
492 
493     return JSValue(); // to suppress a compiler warning.
494 } // Interpreter::llintCLoopExecute()
495 
496 } // namespace JSC
497 
498 #elif !COMPILER(MSVC)
499 
500 //============================================================================
501 // Define the opcode dispatch mechanism when using an ASM loop:
502 //
503 
504 // These are for building an interpreter from generated assembly code:
505 #define OFFLINE_ASM_BEGIN   asm (
506 #define OFFLINE_ASM_END     );
507 
<span class="line-modified">508 #if USE(LLINT_EMBEDDED_OPCODE_ID)</span>
509 #define EMBED_OPCODE_ID_IF_NEEDED(__opcode) &quot;.int &quot; __opcode##_value_string &quot;\n&quot;
510 #else
511 #define EMBED_OPCODE_ID_IF_NEEDED(__opcode)
512 #endif
513 
514 #define OFFLINE_ASM_OPCODE_LABEL(__opcode) \
515     EMBED_OPCODE_ID_IF_NEEDED(__opcode) \
516     OFFLINE_ASM_OPCODE_DEBUG_LABEL(llint_##__opcode) \
517     OFFLINE_ASM_LOCAL_LABEL(llint_##__opcode)
518 
519 #define OFFLINE_ASM_GLUE_LABEL(__opcode)   OFFLINE_ASM_LOCAL_LABEL(__opcode)
520 
521 #if CPU(ARM_THUMB2)
522 #define OFFLINE_ASM_GLOBAL_LABEL(label)          \
523     &quot;.text\n&quot;                                    \
524     &quot;.align 4\n&quot;                                 \
525     &quot;.globl &quot; SYMBOL_STRING(label) &quot;\n&quot;          \
526     HIDE_SYMBOL(label) &quot;\n&quot;                      \
527     &quot;.thumb\n&quot;                                   \
528     &quot;.thumb_func &quot; THUMB_FUNC_PARAM(label) &quot;\n&quot;  \
</pre>
</td>
<td>
<hr />
<pre>
127 
128 //============================================================================
129 // CLoopRegister is the storage for an emulated CPU register.
130 // It defines the policy of how ints smaller than intptr_t are packed into the
131 // pseudo register, as well as hides endianness differences.
132 
133 class CLoopRegister {
134 public:
135     ALWAYS_INLINE intptr_t i() const { return m_value; };
136     ALWAYS_INLINE uintptr_t u() const { return m_value; }
137     ALWAYS_INLINE int32_t i32() const { return m_value; }
138     ALWAYS_INLINE uint32_t u32() const { return m_value; }
139     ALWAYS_INLINE int8_t i8() const { return m_value; }
140     ALWAYS_INLINE uint8_t u8() const { return m_value; }
141 
142     ALWAYS_INLINE intptr_t* ip() const { return bitwise_cast&lt;intptr_t*&gt;(m_value); }
143     ALWAYS_INLINE int8_t* i8p() const { return bitwise_cast&lt;int8_t*&gt;(m_value); }
144     ALWAYS_INLINE void* vp() const { return bitwise_cast&lt;void*&gt;(m_value); }
145     ALWAYS_INLINE const void* cvp() const { return bitwise_cast&lt;const void*&gt;(m_value); }
146     ALWAYS_INLINE CallFrame* callFrame() const { return bitwise_cast&lt;CallFrame*&gt;(m_value); }

147     ALWAYS_INLINE const void* instruction() const { return bitwise_cast&lt;const void*&gt;(m_value); }
148     ALWAYS_INLINE VM* vm() const { return bitwise_cast&lt;VM*&gt;(m_value); }
149     ALWAYS_INLINE JSCell* cell() const { return bitwise_cast&lt;JSCell*&gt;(m_value); }
150     ALWAYS_INLINE ProtoCallFrame* protoCallFrame() const { return bitwise_cast&lt;ProtoCallFrame*&gt;(m_value); }
151     ALWAYS_INLINE NativeFunction nativeFunc() const { return bitwise_cast&lt;NativeFunction&gt;(m_value); }
152 #if USE(JSVALUE64)
153     ALWAYS_INLINE int64_t i64() const { return m_value; }
154     ALWAYS_INLINE uint64_t u64() const { return m_value; }
155     ALWAYS_INLINE EncodedJSValue encodedJSValue() const { return bitwise_cast&lt;EncodedJSValue&gt;(m_value); }
156 #endif
157     ALWAYS_INLINE Opcode opcode() const { return bitwise_cast&lt;Opcode&gt;(m_value); }
158 
<span class="line-modified">159     operator CallFrame*() { return bitwise_cast&lt;CallFrame*&gt;(m_value); }</span>
160     operator const Instruction*() { return bitwise_cast&lt;const Instruction*&gt;(m_value); }
161     operator JSCell*() { return bitwise_cast&lt;JSCell*&gt;(m_value); }
162     operator ProtoCallFrame*() { return bitwise_cast&lt;ProtoCallFrame*&gt;(m_value); }
163     operator Register*() { return bitwise_cast&lt;Register*&gt;(m_value); }
164     operator VM*() { return bitwise_cast&lt;VM*&gt;(m_value); }
165 
166     template&lt;typename T, typename = std::enable_if_t&lt;sizeof(T) == sizeof(uintptr_t)&gt;&gt;
167     ALWAYS_INLINE void operator=(T value) { m_value = bitwise_cast&lt;uintptr_t&gt;(value); }
168 #if USE(JSVALUE64)
169     ALWAYS_INLINE void operator=(int32_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
170     ALWAYS_INLINE void operator=(uint32_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
171 #endif
172     ALWAYS_INLINE void operator=(int16_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
173     ALWAYS_INLINE void operator=(uint16_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
174     ALWAYS_INLINE void operator=(int8_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
175     ALWAYS_INLINE void operator=(uint8_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
176     ALWAYS_INLINE void operator=(bool value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
177 
178 #if USE(JSVALUE64)
179     ALWAYS_INLINE double bitsAsDouble() const { return bitwise_cast&lt;double&gt;(m_value); }
</pre>
<hr />
<pre>
263         // FIXME: this mapping is unnecessarily expensive in the absence of COMPUTED_GOTO
264         //   narrow opcodes don&#39;t need any mapping and wide opcodes just need to add numOpcodeIDs
265         #define OPCODE_ENTRY(__opcode, length) \
266             opcodeMap[__opcode] = __opcode; \
267             opcodeMapWide16[__opcode] = static_cast&lt;OpcodeID&gt;(__opcode##_wide16); \
268             opcodeMapWide32[__opcode] = static_cast&lt;OpcodeID&gt;(__opcode##_wide32);
269 
270         #define LLINT_OPCODE_ENTRY(__opcode, length) \
271             opcodeMap[__opcode] = __opcode;
272 #endif
273         FOR_EACH_BYTECODE_ID(OPCODE_ENTRY)
274         FOR_EACH_CLOOP_BYTECODE_HELPER_ID(LLINT_OPCODE_ENTRY)
275         FOR_EACH_LLINT_NATIVE_HELPER(LLINT_OPCODE_ENTRY)
276         #undef OPCODE_ENTRY
277         #undef LLINT_OPCODE_ENTRY
278 
279         // Note: we can only set the exceptionInstructions after we have
280         // initialized the opcodeMap above. This is because getCodePtr()
281         // can depend on the opcodeMap.
282         uint8_t* exceptionInstructions = reinterpret_cast&lt;uint8_t*&gt;(LLInt::exceptionInstructions());
<span class="line-modified">283         for (unsigned i = 0; i &lt; maxOpcodeLength + 1; ++i)</span>
284             exceptionInstructions[i] = llint_throw_from_slow_path_trampoline;
285 
286         return JSValue();
287     }
288 
289     // Define the pseudo registers used by the LLINT C Loop backend:
290     static_assert(sizeof(CLoopRegister) == sizeof(intptr_t));
291 
292     // The CLoop llint backend is initially based on the ARMv7 backend, and
293     // then further enhanced with a few instructions from the x86 backend to
294     // support building for X64 targets. Hence, the shape of the generated
295     // code and the usage convention of registers will look a lot like the
296     // ARMv7 backend&#39;s.
297     //
298     // For example, on a 32-bit build:
299     // 1. Outgoing args will be set up as follows:
300     //    arg1 in t0 (r0 on ARM)
301     //    arg2 in t1 (r1 on ARM)
302     // 2. 32 bit return values will be in t0 (r0 on ARM).
303     // 3. 64 bit return values (e.g. doubles) will be in t0,t1 (r0,r1 on ARM).
304     //
305     // But instead of naming these simulator registers based on their ARM
306     // counterparts, we&#39;ll name them based on their original llint asm names.
307     // This will make it easier to correlate the generated code with the
308     // original llint asm code.
309     //
310     // On a 64-bit build, it more like x64 in that the registers are 64 bit.
311     // Hence:
312     // 1. Outgoing args are still the same: arg1 in t0, arg2 in t1, etc.
313     // 2. 32 bit result values will be in the low 32-bit of t0.
314     // 3. 64 bit result values will be in t0.
315 
316     CLoopRegister t0, t1, t2, t3, t5, sp, cfr, lr, pc;
317 #if USE(JSVALUE64)
<span class="line-modified">318     CLoopRegister numberTag, notCellMask;</span>
319 #endif
<span class="line-added">320     CLoopRegister pcBase;</span>
321     CLoopRegister metadataTable;
322     CLoopDoubleRegister d0, d1;
323 
324     struct StackPointerScope {
325         StackPointerScope(CLoopStack&amp; stack)
326             : m_stack(stack)
327             , m_originalStackPointer(stack.currentStackPointer())
328         { }
329 
330         ~StackPointerScope()
331         {
332             m_stack.setCurrentStackPointer(m_originalStackPointer);
333         }
334 
335     private:
336         CLoopStack&amp; m_stack;
337         void* m_originalStackPointer;
338     };
339 
340     CLoopStack&amp; cloopStack = vm-&gt;interpreter-&gt;cloopStack();
341     StackPointerScope stackPointerScope(cloopStack);
342 
343     lr = getOpcode(llint_return_to_host);
344     sp = cloopStack.currentStackPointer();
345     cfr = vm-&gt;topCallFrame;
346 #ifndef NDEBUG
347     void* startSP = sp.vp();
348     CallFrame* startCFR = cfr.callFrame();
349 #endif
350 
351     // Initialize the incoming args for doVMEntryToJavaScript:
352     t0 = executableAddress;
353     t1 = vm;
354     t2 = protoCallFrame;
355 
356 #if USE(JSVALUE64)
357     // For the ASM llint, JITStubs takes care of this initialization. We do
358     // it explicitly here for the C loop:
<span class="line-modified">359     numberTag = JSValue::NumberTag;</span>
<span class="line-modified">360     notCellMask = JSValue::NotCellMask;</span>
361 #endif // USE(JSVALUE64)
362 
363     // Interpreter variables for value passing between opcodes and/or helpers:
364     NativeFunction nativeFunc = nullptr;
365     JSValue functionReturnValue;
366     Opcode opcode = getOpcode(entryOpcodeID);
367 
368 #define PUSH(cloopReg) \
369     do { \
370         sp = sp.ip() - 1; \
371         *sp.ip() = cloopReg.i(); \
372     } while (false)
373 
374 #define POP(cloopReg) \
375     do { \
376         cloopReg = *sp.ip(); \
377         sp = sp.ip() + 1; \
378     } while (false)
379 
380 #if ENABLE(OPCODE_STATS)
381 #define RECORD_OPCODE_STATS(__opcode) OpcodeStats::recordInstruction(__opcode)
382 #else
383 #define RECORD_OPCODE_STATS(__opcode)
384 #endif
385 












386 #if ENABLE(COMPUTED_GOTO_OPCODES)
387 
388     //========================================================================
389     // Loop dispatch mechanism using computed goto statements:
390 
391     #define DISPATCH_OPCODE() goto *opcode
392 
393     #define DEFINE_OPCODE(__opcode) \
394         __opcode: \
395             RECORD_OPCODE_STATS(__opcode);
396 
397     // Dispatch to the current PC&#39;s bytecode:
398     DISPATCH_OPCODE();
399 
400 #else // !ENABLE(COMPUTED_GOTO_OPCODES)
401     //========================================================================
402     // Loop dispatch mechanism using a C switch statement:
403 
404     #define DISPATCH_OPCODE() goto dispatchOpcode
405 
</pre>
<hr />
<pre>
456 #if !ENABLE(COMPUTED_GOTO_OPCODES)
457     default:
458         ASSERT(false);
459 #endif
460 
461     } // END bytecode handler cases.
462 
463 #if ENABLE(COMPUTED_GOTO_OPCODES)
464     // Keep the compiler happy so that it doesn&#39;t complain about unused
465     // labels for the LLInt trampoline glue. The labels are automatically
466     // emitted by label macros above, and some of them are referenced by
467     // the llint generated code. Since we can&#39;t tell ahead of time which
468     // will be referenced and which will be not, we&#39;ll just passify the
469     // compiler on all such labels:
470     #define LLINT_OPCODE_ENTRY(__opcode, length) \
471         UNUSED_LABEL(__opcode);
472         FOR_EACH_OPCODE_ID(LLINT_OPCODE_ENTRY);
473     #undef LLINT_OPCODE_ENTRY
474 #endif
475 

476     #undef DEFINE_OPCODE
477     #undef CHECK_FOR_TIMEOUT
478     #undef CAST
479 
480     return JSValue(); // to suppress a compiler warning.
481 } // Interpreter::llintCLoopExecute()
482 
483 } // namespace JSC
484 
485 #elif !COMPILER(MSVC)
486 
487 //============================================================================
488 // Define the opcode dispatch mechanism when using an ASM loop:
489 //
490 
491 // These are for building an interpreter from generated assembly code:
492 #define OFFLINE_ASM_BEGIN   asm (
493 #define OFFLINE_ASM_END     );
494 
<span class="line-modified">495 #if ENABLE(LLINT_EMBEDDED_OPCODE_ID)</span>
496 #define EMBED_OPCODE_ID_IF_NEEDED(__opcode) &quot;.int &quot; __opcode##_value_string &quot;\n&quot;
497 #else
498 #define EMBED_OPCODE_ID_IF_NEEDED(__opcode)
499 #endif
500 
501 #define OFFLINE_ASM_OPCODE_LABEL(__opcode) \
502     EMBED_OPCODE_ID_IF_NEEDED(__opcode) \
503     OFFLINE_ASM_OPCODE_DEBUG_LABEL(llint_##__opcode) \
504     OFFLINE_ASM_LOCAL_LABEL(llint_##__opcode)
505 
506 #define OFFLINE_ASM_GLUE_LABEL(__opcode)   OFFLINE_ASM_LOCAL_LABEL(__opcode)
507 
508 #if CPU(ARM_THUMB2)
509 #define OFFLINE_ASM_GLOBAL_LABEL(label)          \
510     &quot;.text\n&quot;                                    \
511     &quot;.align 4\n&quot;                                 \
512     &quot;.globl &quot; SYMBOL_STRING(label) &quot;\n&quot;          \
513     HIDE_SYMBOL(label) &quot;\n&quot;                      \
514     &quot;.thumb\n&quot;                                   \
515     &quot;.thumb_func &quot; THUMB_FUNC_PARAM(label) &quot;\n&quot;  \
</pre>
</td>
</tr>
</table>
<center><a href="LowLevelInterpreter.asm.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="LowLevelInterpreter.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>