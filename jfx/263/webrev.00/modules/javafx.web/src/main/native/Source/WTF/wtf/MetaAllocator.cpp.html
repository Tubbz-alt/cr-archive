<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/WTF/wtf/MetaAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  *
  8  * 1.  Redistributions of source code must retain the above copyright
  9  *     notice, this list of conditions and the following disclaimer.
 10  * 2.  Redistributions in binary form must reproduce the above copyright
 11  *     notice, this list of conditions and the following disclaimer in the
 12  *     documentation and/or other materials provided with the distribution.
 13  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
 14  *     its contributors may be used to endorse or promote products derived
 15  *     from this software without specific prior written permission.
 16  *
 17  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
 18  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 19  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 20  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
 21  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 22  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 23  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 24  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 25  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 26  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 27  */
 28 
 29 #include &quot;config.h&quot;
 30 #include &lt;wtf/MetaAllocator.h&gt;
 31 
 32 #include &lt;wtf/DataLog.h&gt;
 33 #include &lt;wtf/FastMalloc.h&gt;
 34 #include &lt;wtf/NeverDestroyed.h&gt;
 35 #include &lt;wtf/ProcessID.h&gt;
 36 
 37 namespace WTF {
 38 
 39 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MetaAllocatorHandle);
 40 
 41 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MetaAllocatorFreeSpace);
 42 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MetaAllocatorFreeSpace);
 43 
 44 MetaAllocator::~MetaAllocator()
 45 {
 46     for (FreeSpaceNode* node = m_freeSpaceSizeMap.first(); node;) {
 47         FreeSpaceNode* next = node-&gt;successor();
 48         m_freeSpaceSizeMap.remove(node);
 49         freeFreeSpaceNode(node);
 50         node = next;
 51     }
 52 #ifndef NDEBUG
 53     ASSERT(!m_mallocBalance);
 54 #endif
 55 }
 56 
 57 void MetaAllocatorTracker::notify(MetaAllocatorHandle* handle)
 58 {
 59     m_allocations.insert(handle);
 60 }
 61 
 62 void MetaAllocatorTracker::release(MetaAllocatorHandle* handle)
 63 {
 64     m_allocations.remove(handle);
 65 }
 66 
 67 ALWAYS_INLINE void MetaAllocator::release(MetaAllocatorHandle* handle)
 68 {
 69     LockHolder locker(&amp;m_lock);
 70     if (handle-&gt;sizeInBytes()) {
 71         void* start = handle-&gt;start().untaggedPtr();
 72         size_t sizeInBytes = handle-&gt;sizeInBytes();
 73         decrementPageOccupancy(start, sizeInBytes);
 74         addFreeSpaceFromReleasedHandle(FreeSpacePtr(start), sizeInBytes);
 75     }
 76 
 77     if (UNLIKELY(!!m_tracker))
 78         m_tracker-&gt;release(handle);
 79 }
 80 
 81 MetaAllocatorHandle::MetaAllocatorHandle(MetaAllocator* allocator, void* start, size_t sizeInBytes, void* ownerUID)
 82     : m_allocator(allocator)
 83     , m_start(start)
 84     , m_end(reinterpret_cast&lt;char*&gt;(start) + sizeInBytes)
 85     , m_ownerUID(ownerUID)
 86 {
 87     ASSERT(allocator);
 88     ASSERT(start);
 89     ASSERT(sizeInBytes);
 90 }
 91 
 92 MetaAllocatorHandle::~MetaAllocatorHandle()
 93 {
 94     ASSERT(m_allocator);
 95     m_allocator-&gt;release(this);
 96 }
 97 
 98 void MetaAllocatorHandle::shrink(size_t newSizeInBytes)
 99 {
100     size_t sizeInBytes = this-&gt;sizeInBytes();
101     ASSERT(newSizeInBytes &lt;= sizeInBytes);
102 
103     LockHolder locker(&amp;m_allocator-&gt;m_lock);
104 
105     newSizeInBytes = m_allocator-&gt;roundUp(newSizeInBytes);
106 
107     ASSERT(newSizeInBytes &lt;= sizeInBytes);
108 
109     if (newSizeInBytes == sizeInBytes)
110         return;
111 
112     uintptr_t freeStart = m_start.untaggedPtr&lt;uintptr_t&gt;() + newSizeInBytes;
113     size_t freeSize = sizeInBytes - newSizeInBytes;
114     uintptr_t freeEnd = freeStart + freeSize;
115 
116     uintptr_t firstCompletelyFreePage = (freeStart + m_allocator-&gt;m_pageSize - 1) &amp; ~(m_allocator-&gt;m_pageSize - 1);
117     if (firstCompletelyFreePage &lt; freeEnd)
118         m_allocator-&gt;decrementPageOccupancy(reinterpret_cast&lt;void*&gt;(firstCompletelyFreePage), freeSize - (firstCompletelyFreePage - freeStart));
119 
120     m_allocator-&gt;addFreeSpaceFromReleasedHandle(MetaAllocator::FreeSpacePtr(freeStart), freeSize);
121 
122     m_end = m_start + newSizeInBytes;
123 }
124 
125 void MetaAllocatorHandle::dump(PrintStream&amp; out) const
126 {
127     out.print(RawPointer(start().untaggedPtr()), &quot;...&quot;, RawPointer(end().untaggedPtr()));
128 }
129 
130 MetaAllocator::MetaAllocator(size_t allocationGranule, size_t pageSize)
131     : m_allocationGranule(allocationGranule)
132     , m_pageSize(pageSize)
133     , m_bytesAllocated(0)
134     , m_bytesReserved(0)
135     , m_bytesCommitted(0)
136     , m_tracker(0)
137 #ifndef NDEBUG
138     , m_mallocBalance(0)
139 #endif
140 #if ENABLE(META_ALLOCATOR_PROFILE)
141     , m_numAllocations(0)
142     , m_numFrees(0)
143 #endif
144 {
145     for (m_logPageSize = 0; m_logPageSize &lt; 32; ++m_logPageSize) {
146         if (static_cast&lt;size_t&gt;(1) &lt;&lt; m_logPageSize == m_pageSize)
147             break;
148     }
149 
150     ASSERT(static_cast&lt;size_t&gt;(1) &lt;&lt; m_logPageSize == m_pageSize);
151 
152     for (m_logAllocationGranule = 0; m_logAllocationGranule &lt; 32; ++m_logAllocationGranule) {
153         if (static_cast&lt;size_t&gt;(1) &lt;&lt; m_logAllocationGranule == m_allocationGranule)
154             break;
155     }
156 
157     ASSERT(static_cast&lt;size_t&gt;(1) &lt;&lt; m_logAllocationGranule == m_allocationGranule);
158 }
159 
160 RefPtr&lt;MetaAllocatorHandle&gt; MetaAllocator::allocate(size_t sizeInBytes, void* ownerUID)
161 {
162     LockHolder locker(&amp;m_lock);
163 
164     if (!sizeInBytes)
165         return nullptr;
166 
167     sizeInBytes = roundUp(sizeInBytes);
168 
169     FreeSpacePtr start = findAndRemoveFreeSpace(sizeInBytes);
170     if (!start) {
171         size_t requestedNumberOfPages = (sizeInBytes + m_pageSize - 1) &gt;&gt; m_logPageSize;
172         size_t numberOfPages = requestedNumberOfPages;
173 
174         start = allocateNewSpace(numberOfPages);
175         if (!start)
176             return nullptr;
177 
178         ASSERT(numberOfPages &gt;= requestedNumberOfPages);
179 
180         size_t roundedUpSize = numberOfPages &lt;&lt; m_logPageSize;
181 
182         ASSERT(roundedUpSize &gt;= sizeInBytes);
183 
184         m_bytesReserved += roundedUpSize;
185 
186         if (roundedUpSize &gt; sizeInBytes) {
187             FreeSpacePtr freeSpaceStart = start + sizeInBytes;
188             size_t freeSpaceSize = roundedUpSize - sizeInBytes;
189             addFreeSpace(freeSpaceStart, freeSpaceSize);
190         }
191     }
192     incrementPageOccupancy(start.untaggedPtr(), sizeInBytes);
193     m_bytesAllocated += sizeInBytes;
194 #if ENABLE(META_ALLOCATOR_PROFILE)
195     m_numAllocations++;
196 #endif
197 
198     auto handle = adoptRef(*new MetaAllocatorHandle(this, start.untaggedPtr(), sizeInBytes, ownerUID));
199 
200     if (UNLIKELY(!!m_tracker))
201         m_tracker-&gt;notify(handle.ptr());
202 
203     return handle;
204 }
205 
206 MetaAllocator::Statistics MetaAllocator::currentStatistics()
207 {
208     LockHolder locker(&amp;m_lock);
209     Statistics result;
210     result.bytesAllocated = m_bytesAllocated;
211     result.bytesReserved = m_bytesReserved;
212     result.bytesCommitted = m_bytesCommitted;
213     return result;
214 }
215 
216 MetaAllocator::FreeSpacePtr MetaAllocator::findAndRemoveFreeSpace(size_t sizeInBytes)
217 {
218     FreeSpaceNode* node = m_freeSpaceSizeMap.findLeastGreaterThanOrEqual(sizeInBytes);
219 
220     if (!node)
221         return 0;
222 
223     size_t nodeSizeInBytes = node-&gt;sizeInBytes();
224     ASSERT(nodeSizeInBytes &gt;= sizeInBytes);
225 
226     m_freeSpaceSizeMap.remove(node);
227 
228     FreeSpacePtr result;
229 
230     if (nodeSizeInBytes == sizeInBytes) {
231         // Easy case: perfect fit, so just remove the node entirely.
232         result = node-&gt;m_start;
233 
234         m_freeSpaceStartAddressMap.remove(node-&gt;m_start);
235         m_freeSpaceEndAddressMap.remove(node-&gt;m_end);
236         freeFreeSpaceNode(node);
237     } else {
238         // Try to be a good citizen and ensure that the returned chunk of memory
239         // straddles as few pages as possible, but only insofar as doing so will
240         // not increase fragmentation. The intuition is that minimizing
241         // fragmentation is a strictly higher priority than minimizing the number
242         // of committed pages, since in the long run, smaller fragmentation means
243         // fewer committed pages and fewer failures in general.
244 
245         uintptr_t nodeStartAsInt = node-&gt;m_start.untaggedPtr&lt;uintptr_t&gt;();
246         uintptr_t firstPage = nodeStartAsInt &gt;&gt; m_logPageSize;
247         uintptr_t lastPage = (nodeStartAsInt + nodeSizeInBytes - 1) &gt;&gt; m_logPageSize;
248 
249         uintptr_t lastPageForLeftAllocation = (nodeStartAsInt + sizeInBytes - 1) &gt;&gt; m_logPageSize;
250         uintptr_t firstPageForRightAllocation = (nodeStartAsInt + nodeSizeInBytes - sizeInBytes) &gt;&gt; m_logPageSize;
251 
252         if (lastPageForLeftAllocation - firstPage + 1 &lt;= lastPage - firstPageForRightAllocation + 1) {
253             // Allocate in the left side of the returned chunk, and slide the node to the right.
254             result = node-&gt;m_start;
255 
256             m_freeSpaceStartAddressMap.remove(node-&gt;m_start);
257 
258             node-&gt;m_start += sizeInBytes;
259 
260             m_freeSpaceSizeMap.insert(node);
261             m_freeSpaceStartAddressMap.add(node-&gt;m_start, node);
262         } else {
263             // Allocate in the right size of the returned chunk, and slide the node to the left;
264 
265             result = node-&gt;m_end - sizeInBytes;
266 
267             m_freeSpaceEndAddressMap.remove(node-&gt;m_end);
268 
269             node-&gt;m_end = result;
270 
271             m_freeSpaceSizeMap.insert(node);
272             m_freeSpaceEndAddressMap.add(result, node);
273         }
274     }
275 
276 #if ENABLE(META_ALLOCATOR_PROFILE)
277     dumpProfile();
278 #endif
279 
280     return result;
281 }
282 
283 void MetaAllocator::addFreeSpaceFromReleasedHandle(FreeSpacePtr start, size_t sizeInBytes)
284 {
285 #if ENABLE(META_ALLOCATOR_PROFILE)
286     m_numFrees++;
287 #endif
288     m_bytesAllocated -= sizeInBytes;
289     addFreeSpace(start, sizeInBytes);
290 }
291 
292 void MetaAllocator::addFreshFreeSpace(void* start, size_t sizeInBytes)
293 {
294     LockHolder locker(&amp;m_lock);
295     m_bytesReserved += sizeInBytes;
296     addFreeSpace(FreeSpacePtr(start), sizeInBytes);
297 }
298 
299 size_t MetaAllocator::debugFreeSpaceSize()
300 {
301 #ifndef NDEBUG
302     LockHolder locker(&amp;m_lock);
303     size_t result = 0;
304     for (FreeSpaceNode* node = m_freeSpaceSizeMap.first(); node; node = node-&gt;successor())
305         result += node-&gt;sizeInBytes();
306     return result;
307 #else
308     CRASH();
309     return 0;
310 #endif
311 }
312 
313 void MetaAllocator::addFreeSpace(FreeSpacePtr start, size_t sizeInBytes)
314 {
315     FreeSpacePtr end = start + sizeInBytes;
316 
317     HashMap&lt;FreeSpacePtr, FreeSpaceNode*&gt;::iterator leftNeighbor = m_freeSpaceEndAddressMap.find(start);
318     HashMap&lt;FreeSpacePtr, FreeSpaceNode*&gt;::iterator rightNeighbor = m_freeSpaceStartAddressMap.find(end);
319 
320     if (leftNeighbor != m_freeSpaceEndAddressMap.end()) {
321         // We have something we can coalesce with on the left. Remove it from the tree, and
322         // remove its end from the end address map.
323 
324         ASSERT(leftNeighbor-&gt;value-&gt;m_end == leftNeighbor-&gt;key);
325 
326         FreeSpaceNode* leftNode = leftNeighbor-&gt;value;
327 
328         FreeSpacePtr leftEnd = leftNode-&gt;m_end;
329 
330         ASSERT(leftEnd == start);
331 
332         m_freeSpaceSizeMap.remove(leftNode);
333         m_freeSpaceEndAddressMap.remove(leftEnd);
334 
335         // Now check if there is also something to coalesce with on the right.
336         if (rightNeighbor != m_freeSpaceStartAddressMap.end()) {
337             // Freeing something in the middle of free blocks. Coalesce both left and
338             // right, whilst removing the right neighbor from the maps.
339 
340             ASSERT(rightNeighbor-&gt;value-&gt;m_start == rightNeighbor-&gt;key);
341 
342             FreeSpaceNode* rightNode = rightNeighbor-&gt;value;
343             FreeSpacePtr rightStart = rightNeighbor-&gt;key;
344             size_t rightSize = rightNode-&gt;sizeInBytes();
345             FreeSpacePtr rightEnd = rightNode-&gt;m_end;
346 
347             ASSERT(rightStart == end);
348             ASSERT(leftNode-&gt;m_start + (leftNode-&gt;sizeInBytes() + sizeInBytes + rightSize) == rightEnd);
349 
350             m_freeSpaceSizeMap.remove(rightNode);
351             m_freeSpaceStartAddressMap.remove(rightStart);
352             m_freeSpaceEndAddressMap.remove(rightEnd);
353 
354             freeFreeSpaceNode(rightNode);
355 
356             leftNode-&gt;m_end += (sizeInBytes + rightSize);
357 
358             m_freeSpaceSizeMap.insert(leftNode);
359             m_freeSpaceEndAddressMap.add(rightEnd, leftNode);
360         } else {
361             leftNode-&gt;m_end += sizeInBytes;
362 
363             m_freeSpaceSizeMap.insert(leftNode);
364             m_freeSpaceEndAddressMap.add(end, leftNode);
365         }
366     } else {
367         // Cannot coalesce with left; try to see if we can coalesce with right.
368 
369         if (rightNeighbor != m_freeSpaceStartAddressMap.end()) {
370             FreeSpaceNode* rightNode = rightNeighbor-&gt;value;
371             FreeSpacePtr rightStart = rightNeighbor-&gt;key;
372 
373             ASSERT(rightStart == end);
374             ASSERT(start + (sizeInBytes + rightNode-&gt;sizeInBytes()) == rightNode-&gt;m_end);
375 
376             m_freeSpaceSizeMap.remove(rightNode);
377             m_freeSpaceStartAddressMap.remove(rightStart);
378 
379             rightNode-&gt;m_start = start;
380 
381             m_freeSpaceSizeMap.insert(rightNode);
382             m_freeSpaceStartAddressMap.add(start, rightNode);
383         } else {
384             // Nothing to coalesce with, so create a new free space node and add it.
385 
386             FreeSpaceNode* node = allocFreeSpaceNode();
387 
388             node-&gt;m_start = start;
389             node-&gt;m_end = start + sizeInBytes;
390 
391             m_freeSpaceSizeMap.insert(node);
392             m_freeSpaceStartAddressMap.add(start, node);
393             m_freeSpaceEndAddressMap.add(end, node);
394         }
395     }
396 
397 #if ENABLE(META_ALLOCATOR_PROFILE)
398     dumpProfile();
399 #endif
400 }
401 
402 void MetaAllocator::incrementPageOccupancy(void* address, size_t sizeInBytes)
403 {
404     uintptr_t firstPage = reinterpret_cast&lt;uintptr_t&gt;(address) &gt;&gt; m_logPageSize;
405     uintptr_t lastPage = (reinterpret_cast&lt;uintptr_t&gt;(address) + sizeInBytes - 1) &gt;&gt; m_logPageSize;
406 
407     uintptr_t currentPageStart = 0;
408     size_t count = 0;
409     auto flushNeedPages = [&amp;] {
410         if (!currentPageStart)
411             return;
412         notifyNeedPage(reinterpret_cast&lt;void*&gt;(currentPageStart &lt;&lt; m_logPageSize), count);
413         currentPageStart = 0;
414         count = 0;
415     };
416 
417     for (uintptr_t page = firstPage; page &lt;= lastPage; ++page) {
418         auto result = m_pageOccupancyMap.add(page, 1);
419         if (result.isNewEntry) {
420             m_bytesCommitted += m_pageSize;
421             if (!currentPageStart)
422                 currentPageStart = page;
423             ++count;
424         } else {
425             result.iterator-&gt;value++;
426             flushNeedPages();
427         }
428     }
429     flushNeedPages();
430 }
431 
432 void MetaAllocator::decrementPageOccupancy(void* address, size_t sizeInBytes)
433 {
434     uintptr_t firstPage = reinterpret_cast&lt;uintptr_t&gt;(address) &gt;&gt; m_logPageSize;
435     uintptr_t lastPage = (reinterpret_cast&lt;uintptr_t&gt;(address) + sizeInBytes - 1) &gt;&gt; m_logPageSize;
436 
437     uintptr_t currentPageStart = 0;
438     size_t count = 0;
439     auto flushFreePages = [&amp;] {
440         if (!currentPageStart)
441             return;
442         notifyPageIsFree(reinterpret_cast&lt;void*&gt;(currentPageStart &lt;&lt; m_logPageSize), count);
443         currentPageStart = 0;
444         count = 0;
445     };
446 
447     for (uintptr_t page = firstPage; page &lt;= lastPage; ++page) {
448         HashMap&lt;uintptr_t, size_t&gt;::iterator iter = m_pageOccupancyMap.find(page);
449         ASSERT(iter != m_pageOccupancyMap.end());
450         if (!--(iter-&gt;value)) {
451             m_pageOccupancyMap.remove(iter);
452             m_bytesCommitted -= m_pageSize;
453             if (!currentPageStart)
454                 currentPageStart = page;
455             ++count;
456         } else
457             flushFreePages();
458     }
459     flushFreePages();
460 }
461 
462 bool MetaAllocator::isInAllocatedMemory(const AbstractLocker&amp;, void* address)
463 {
464     ASSERT(m_lock.isLocked());
465     uintptr_t page = reinterpret_cast&lt;uintptr_t&gt;(address) &gt;&gt; m_logPageSize;
466     return m_pageOccupancyMap.contains(page);
467 }
468 
469 size_t MetaAllocator::roundUp(size_t sizeInBytes)
470 {
471     if (std::numeric_limits&lt;size_t&gt;::max() - m_allocationGranule &lt;= sizeInBytes)
472         CRASH();
473     return (sizeInBytes + m_allocationGranule - 1) &amp; ~(m_allocationGranule - 1);
474 }
475 
476 MetaAllocator::FreeSpaceNode* MetaAllocator::allocFreeSpaceNode()
477 {
478 #ifndef NDEBUG
479     m_mallocBalance++;
480 #endif
481     return new (NotNull, MetaAllocatorFreeSpaceMalloc::malloc(sizeof(FreeSpaceNode))) FreeSpaceNode();
482 }
483 
484 void MetaAllocator::freeFreeSpaceNode(FreeSpaceNode* node)
485 {
486 #ifndef NDEBUG
487     m_mallocBalance--;
488 #endif
489     MetaAllocatorFreeSpaceMalloc::free(node);
490 }
491 
492 #if ENABLE(META_ALLOCATOR_PROFILE)
493 void MetaAllocator::dumpProfile()
494 {
495     dataLogF(
496         &quot;%d: MetaAllocator(%p): num allocations = %u, num frees = %u, allocated = %lu, reserved = %lu, committed = %lu\n&quot;,
497         getCurrentProcessID(), this, m_numAllocations, m_numFrees, m_bytesAllocated, m_bytesReserved, m_bytesCommitted);
498 }
499 #endif
500 
501 } // namespace WTF
502 
503 
    </pre>
  </body>
</html>