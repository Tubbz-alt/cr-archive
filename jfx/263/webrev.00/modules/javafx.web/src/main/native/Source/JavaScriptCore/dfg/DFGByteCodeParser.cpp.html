<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGByteCodeParser.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2020 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGByteCodeParser.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;ArithProfile.h&quot;
  32 #include &quot;ArrayConstructor.h&quot;
  33 #include &quot;BasicBlockLocation.h&quot;
  34 #include &quot;BuiltinNames.h&quot;
  35 #include &quot;BytecodeGenerator.h&quot;
  36 #include &quot;BytecodeUseDef.h&quot;
  37 #include &quot;CacheableIdentifierInlines.h&quot;
  38 #include &quot;CallLinkStatus.h&quot;
  39 #include &quot;CodeBlock.h&quot;
  40 #include &quot;CodeBlockWithJITType.h&quot;
  41 #include &quot;CommonSlowPaths.h&quot;
  42 #include &quot;DFGAbstractHeap.h&quot;
  43 #include &quot;DFGArrayMode.h&quot;
  44 #include &quot;DFGCFG.h&quot;
  45 #include &quot;DFGCapabilities.h&quot;
  46 #include &quot;DFGClobberize.h&quot;
  47 #include &quot;DFGClobbersExitState.h&quot;
  48 #include &quot;DFGGraph.h&quot;
  49 #include &quot;DFGJITCode.h&quot;
  50 #include &quot;FunctionCodeBlock.h&quot;
  51 #include &quot;GetByStatus.h&quot;
  52 #include &quot;GetterSetter.h&quot;
  53 #include &quot;Heap.h&quot;
  54 #include &quot;InByIdStatus.h&quot;
  55 #include &quot;InstanceOfStatus.h&quot;
  56 #include &quot;JSArrayIterator.h&quot;
  57 #include &quot;JSCInlines.h&quot;
  58 #include &quot;JSImmutableButterfly.h&quot;
  59 #include &quot;JSInternalPromise.h&quot;
  60 #include &quot;JSInternalPromiseConstructor.h&quot;
  61 #include &quot;JSModuleEnvironment.h&quot;
  62 #include &quot;JSModuleNamespaceObject.h&quot;
  63 #include &quot;JSPromiseConstructor.h&quot;
  64 #include &quot;NumberConstructor.h&quot;
  65 #include &quot;ObjectConstructor.h&quot;
  66 #include &quot;OpcodeInlines.h&quot;
  67 #include &quot;PreciseJumpTargets.h&quot;
  68 #include &quot;PutByIdFlags.h&quot;
  69 #include &quot;PutByIdStatus.h&quot;
  70 #include &quot;RegExpPrototype.h&quot;
  71 #include &quot;StackAlignment.h&quot;
  72 #include &quot;StringConstructor.h&quot;
  73 #include &quot;StructureStubInfo.h&quot;
  74 #include &quot;SymbolConstructor.h&quot;
  75 #include &quot;Watchdog.h&quot;
  76 #include &lt;wtf/CommaPrinter.h&gt;
  77 #include &lt;wtf/HashMap.h&gt;
  78 #include &lt;wtf/MathExtras.h&gt;
  79 #include &lt;wtf/Scope.h&gt;
  80 #include &lt;wtf/SetForScope.h&gt;
  81 #include &lt;wtf/StdLibExtras.h&gt;
  82 
  83 namespace JSC { namespace DFG {
  84 
  85 namespace DFGByteCodeParserInternal {
  86 #ifdef NDEBUG
  87 static constexpr bool verbose = false;
  88 #else
  89 static constexpr bool verbose = true;
  90 #endif
  91 } // namespace DFGByteCodeParserInternal
  92 
  93 #define VERBOSE_LOG(...) do { \
  94 if (DFGByteCodeParserInternal::verbose &amp;&amp; Options::verboseDFGBytecodeParsing()) \
  95 dataLog(__VA_ARGS__); \
  96 } while (false)
  97 
  98 // === ByteCodeParser ===
  99 //
 100 // This class is used to compile the dataflow graph from a CodeBlock.
 101 class ByteCodeParser {
 102 public:
 103     ByteCodeParser(Graph&amp; graph)
 104         : m_vm(&amp;graph.m_vm)
 105         , m_codeBlock(graph.m_codeBlock)
 106         , m_profiledBlock(graph.m_profiledBlock)
 107         , m_graph(graph)
 108         , m_currentBlock(0)
 109         , m_currentIndex(0)
 110         , m_constantUndefined(graph.freeze(jsUndefined()))
 111         , m_constantNull(graph.freeze(jsNull()))
 112         , m_constantNaN(graph.freeze(jsNumber(PNaN)))
 113         , m_constantOne(graph.freeze(jsNumber(1)))
 114         , m_numArguments(m_codeBlock-&gt;numParameters())
 115         , m_numLocals(m_codeBlock-&gt;numCalleeLocals())
 116         , m_numTmps(m_codeBlock-&gt;numTmps())
 117         , m_parameterSlots(0)
 118         , m_numPassedVarArgs(0)
 119         , m_inlineStackTop(0)
 120         , m_currentInstruction(0)
 121         , m_hasDebuggerEnabled(graph.hasDebuggerEnabled())
 122     {
 123         ASSERT(m_profiledBlock);
 124     }
 125 
 126     // Parse a full CodeBlock of bytecode.
 127     void parse();
 128 
 129 private:
 130     struct InlineStackEntry;
 131 
 132     // Just parse from m_currentIndex to the end of the current CodeBlock.
 133     void parseCodeBlock();
 134 
 135     void ensureLocals(unsigned newNumLocals)
 136     {
 137         VERBOSE_LOG(&quot;   ensureLocals: trying to raise m_numLocals from &quot;, m_numLocals, &quot; to &quot;, newNumLocals, &quot;\n&quot;);
 138         if (newNumLocals &lt;= m_numLocals)
 139             return;
 140         m_numLocals = newNumLocals;
 141         for (size_t i = 0; i &lt; m_graph.numBlocks(); ++i)
 142             m_graph.block(i)-&gt;ensureLocals(newNumLocals);
 143     }
 144 
 145     void ensureTmps(unsigned newNumTmps)
 146     {
 147         VERBOSE_LOG(&quot;   ensureTmps: trying to raise m_numTmps from &quot;, m_numTmps, &quot; to &quot;, newNumTmps, &quot;\n&quot;);
 148         if (newNumTmps &lt;= m_numTmps)
 149             return;
 150         m_numTmps = newNumTmps;
 151         for (size_t i = 0; i &lt; m_graph.numBlocks(); ++i)
 152             m_graph.block(i)-&gt;ensureTmps(newNumTmps);
 153     }
 154 
 155 
 156     // Helper for min and max.
 157     template&lt;typename ChecksFunctor&gt;
 158     bool handleMinMax(VirtualRegister result, NodeType op, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; insertChecks);
 159 
 160     void refineStatically(CallLinkStatus&amp;, Node* callTarget);
 161     // Blocks can either be targetable (i.e. in the m_blockLinkingTargets of one InlineStackEntry) with a well-defined bytecodeBegin,
 162     // or they can be untargetable, with bytecodeBegin==UINT_MAX, to be managed manually and not by the linkBlock machinery.
 163     // This is used most notably when doing polyvariant inlining (it requires a fair bit of control-flow with no bytecode analog).
 164     // It is also used when doing an early return from an inlined callee: it is easier to fix the bytecode index later on if needed
 165     // than to move the right index all the way to the treatment of op_ret.
 166     BasicBlock* allocateTargetableBlock(BytecodeIndex);
 167     BasicBlock* allocateUntargetableBlock();
 168     // An untargetable block can be given a bytecodeIndex to be later managed by linkBlock, but only once, and it can never go in the other direction
 169     void makeBlockTargetable(BasicBlock*, BytecodeIndex);
 170     void addJumpTo(BasicBlock*);
 171     void addJumpTo(unsigned bytecodeIndex);
 172     // Handle calls. This resolves issues surrounding inlining and intrinsics.
 173     enum Terminality { Terminal, NonTerminal };
 174     Terminality handleCall(
 175         VirtualRegister result, NodeType op, InlineCallFrame::Kind, unsigned instructionSize,
 176         Node* callTarget, int argumentCountIncludingThis, int registerOffset, CallLinkStatus,
 177         SpeculatedType prediction);
 178     template&lt;typename CallOp&gt;
 179     Terminality handleCall(const Instruction* pc, NodeType op, CallMode);
 180     template&lt;typename CallOp&gt;
 181     Terminality handleVarargsCall(const Instruction* pc, NodeType op, CallMode);
 182     void emitFunctionChecks(CallVariant, Node* callTarget, VirtualRegister thisArgumnt);
 183     void emitArgumentPhantoms(int registerOffset, int argumentCountIncludingThis);
 184     Node* getArgumentCount();
 185     template&lt;typename ChecksFunctor&gt;
 186     bool handleRecursiveTailCall(Node* callTargetNode, CallVariant, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; emitFunctionCheckIfNeeded);
 187     unsigned inliningCost(CallVariant, int argumentCountIncludingThis, InlineCallFrame::Kind); // Return UINT_MAX if it&#39;s not an inlining candidate. By convention, intrinsics have a cost of 1.
 188     // Handle inlining. Return true if it succeeded, false if we need to plant a call.
 189     bool handleVarargsInlining(Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp;, int registerOffset, VirtualRegister thisArgument, VirtualRegister argumentsArgument, unsigned argumentsOffset, NodeType callOp, InlineCallFrame::Kind);
 190     unsigned getInliningBalance(const CallLinkStatus&amp;, CodeSpecializationKind);
 191     enum class CallOptimizationResult { OptimizedToJump, Inlined, DidNothing };
 192     CallOptimizationResult handleCallVariant(Node* callTargetNode, VirtualRegister result, CallVariant, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, BytecodeIndex nextIndex, InlineCallFrame::Kind, SpeculatedType prediction, unsigned&amp; inliningBalance, BasicBlock* continuationBlock, bool needsToCheckCallee);
 193     CallOptimizationResult handleInlining(Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp;, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, BytecodeIndex nextIndex, NodeType callOp, InlineCallFrame::Kind, SpeculatedType prediction);
 194     template&lt;typename ChecksFunctor&gt;
 195     void inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks);
 196     // Handle intrinsic functions. Return true if it succeeded, false if we need to plant a call.
 197     template&lt;typename ChecksFunctor&gt;
 198     bool handleIntrinsicCall(Node* callee, VirtualRegister result, Intrinsic, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks);
 199     template&lt;typename ChecksFunctor&gt;
 200     bool handleDOMJITCall(Node* callee, VirtualRegister result, const DOMJIT::Signature*, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks);
 201     template&lt;typename ChecksFunctor&gt;
 202     bool handleIntrinsicGetter(VirtualRegister result, SpeculatedType prediction, const GetByIdVariant&amp; intrinsicVariant, Node* thisNode, const ChecksFunctor&amp; insertChecks);
 203     template&lt;typename ChecksFunctor&gt;
 204     bool handleTypedArrayConstructor(VirtualRegister result, InternalFunction*, int registerOffset, int argumentCountIncludingThis, TypedArrayType, const ChecksFunctor&amp; insertChecks);
 205     template&lt;typename ChecksFunctor&gt;
 206     bool handleConstantInternalFunction(Node* callTargetNode, VirtualRegister result, InternalFunction*, int registerOffset, int argumentCountIncludingThis, CodeSpecializationKind, SpeculatedType, const ChecksFunctor&amp; insertChecks);
 207     Node* handlePutByOffset(Node* base, unsigned identifier, PropertyOffset, Node* value);
 208     Node* handleGetByOffset(SpeculatedType, Node* base, unsigned identifierNumber, PropertyOffset, NodeType = GetByOffset);
 209     bool handleDOMJITGetter(VirtualRegister result, const GetByIdVariant&amp;, Node* thisNode, unsigned identifierNumber, SpeculatedType prediction);
 210     bool handleModuleNamespaceLoad(VirtualRegister result, SpeculatedType, Node* base, GetByStatus);
 211 
 212     template&lt;typename Bytecode&gt;
 213     void handlePutByVal(Bytecode, unsigned instructionSize);
 214     template &lt;typename Bytecode&gt;
 215     void handlePutAccessorById(NodeType, Bytecode);
 216     template &lt;typename Bytecode&gt;
 217     void handlePutAccessorByVal(NodeType, Bytecode);
 218     template &lt;typename Bytecode&gt;
 219     void handleNewFunc(NodeType, Bytecode);
 220     template &lt;typename Bytecode&gt;
 221     void handleNewFuncExp(NodeType, Bytecode);
 222     template &lt;typename Bytecode&gt;
 223     void handleCreateInternalFieldObject(const ClassInfo*, NodeType createOp, NodeType newOp, Bytecode);
 224 
 225     // Create a presence ObjectPropertyCondition based on some known offset and structure set. Does not
 226     // check the validity of the condition, but it may return a null one if it encounters a contradiction.
 227     ObjectPropertyCondition presenceLike(
 228         JSObject* knownBase, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 229 
 230     // Attempt to watch the presence of a property. It will watch that the property is present in the same
 231     // way as in all of the structures in the set. It may emit code instead of just setting a watchpoint.
 232     // Returns true if this all works out.
 233     bool checkPresenceLike(JSObject* knownBase, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 234     void checkPresenceLike(Node* base, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 235 
 236     // Works with both GetByIdVariant and the setter form of PutByIdVariant.
 237     template&lt;typename VariantType&gt;
 238     Node* load(SpeculatedType, Node* base, unsigned identifierNumber, const VariantType&amp;);
 239 
 240     Node* store(Node* base, unsigned identifier, const PutByIdVariant&amp;, Node* value);
 241 
 242     template&lt;typename Op&gt;
 243     void parseGetById(const Instruction*);
 244     void handleGetById(
 245         VirtualRegister destination, SpeculatedType, Node* base, unsigned identifierNumber, GetByStatus, AccessType, unsigned instructionSize);
 246     void emitPutById(
 247         Node* base, unsigned identifierNumber, Node* value,  const PutByIdStatus&amp;, bool isDirect);
 248     void handlePutById(
 249         Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp;,
 250         bool isDirect, unsigned intructionSize);
 251 
 252     // Either register a watchpoint or emit a check for this condition. Returns false if the
 253     // condition no longer holds, and therefore no reasonable check can be emitted.
 254     bool check(const ObjectPropertyCondition&amp;);
 255 
 256     GetByOffsetMethod promoteToConstant(GetByOffsetMethod);
 257 
 258     // Either register a watchpoint or emit a check for this condition. It must be a Presence
 259     // condition. It will attempt to promote a Presence condition to an Equivalence condition.
 260     // Emits code for the loaded value that the condition guards, and returns a node containing
 261     // the loaded value. Returns null if the condition no longer holds.
 262     GetByOffsetMethod planLoad(const ObjectPropertyCondition&amp;);
 263     Node* load(SpeculatedType, unsigned identifierNumber, const GetByOffsetMethod&amp;, NodeType = GetByOffset);
 264     Node* load(SpeculatedType, const ObjectPropertyCondition&amp;, NodeType = GetByOffset);
 265 
 266     // Calls check() for each condition in the set: that is, it either emits checks or registers
 267     // watchpoints (or a combination of the two) to make the conditions hold. If any of those
 268     // conditions are no longer checkable, returns false.
 269     bool check(const ObjectPropertyConditionSet&amp;);
 270 
 271     // Calls check() for those conditions that aren&#39;t the slot base, and calls load() for the slot
 272     // base. Does a combination of watchpoint registration and check emission to guard the
 273     // conditions, and emits code to load the value from the slot base. Returns a node containing
 274     // the loaded value. Returns null if any of the conditions were no longer checkable.
 275     GetByOffsetMethod planLoad(const ObjectPropertyConditionSet&amp;);
 276     Node* load(SpeculatedType, const ObjectPropertyConditionSet&amp;, NodeType = GetByOffset);
 277 
 278     void prepareToParseBlock();
 279     void clearCaches();
 280 
 281     // Parse a single basic block of bytecode instructions.
 282     void parseBlock(unsigned limit);
 283     // Link block successors.
 284     void linkBlock(BasicBlock*, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 285     void linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 286 
 287     void progressToNextCheckpoint()
 288     {
 289         m_currentIndex = BytecodeIndex(m_currentIndex.offset(), m_currentIndex.checkpoint() + 1);
 290         // At this point, it&#39;s again OK to OSR exit.
 291         m_exitOK = true;
 292         addToGraph(ExitOK);
 293 
 294         processSetLocalQueue();
 295     }
 296 
 297     VariableAccessData* newVariableAccessData(Operand operand)
 298     {
 299         ASSERT(!operand.isConstant());
 300 
 301         m_graph.m_variableAccessData.append(operand);
 302         return &amp;m_graph.m_variableAccessData.last();
 303     }
 304 
 305     // Get/Set the operands/result of a bytecode instruction.
 306     Node* getDirect(Operand operand)
 307     {
 308         ASSERT(!operand.isConstant());
 309 
 310         if (operand.isArgument())
 311             return getArgument(operand.virtualRegister());
 312 
 313         return getLocalOrTmp(operand);
 314     }
 315 
 316     Node* get(VirtualRegister operand)
 317     {
 318         if (operand.isConstant()) {
 319             unsigned constantIndex = operand.toConstantIndex();
 320             unsigned oldSize = m_constants.size();
 321             if (constantIndex &gt;= oldSize || !m_constants[constantIndex]) {
 322                 const CodeBlock&amp; codeBlock = *m_inlineStackTop-&gt;m_codeBlock;
 323                 JSValue value = codeBlock.getConstant(operand);
 324                 SourceCodeRepresentation sourceCodeRepresentation = codeBlock.constantSourceCodeRepresentation(operand);
 325                 if (constantIndex &gt;= oldSize) {
 326                     m_constants.grow(constantIndex + 1);
 327                     for (unsigned i = oldSize; i &lt; m_constants.size(); ++i)
 328                         m_constants[i] = nullptr;
 329                 }
 330 
 331                 Node* constantNode = nullptr;
 332                 if (sourceCodeRepresentation == SourceCodeRepresentation::Double)
 333                     constantNode = addToGraph(DoubleConstant, OpInfo(m_graph.freezeStrong(jsDoubleNumber(value.asNumber()))));
 334                 else
 335                     constantNode = addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(value)));
 336                 m_constants[constantIndex] = constantNode;
 337             }
 338             ASSERT(m_constants[constantIndex]);
 339             return m_constants[constantIndex];
 340         }
 341 
 342         if (inlineCallFrame()) {
 343             if (!inlineCallFrame()-&gt;isClosureCall) {
 344                 JSFunction* callee = inlineCallFrame()-&gt;calleeConstant();
 345                 if (operand.offset() == CallFrameSlot::callee)
 346                     return weakJSConstant(callee);
 347             }
 348         } else if (operand.offset() == CallFrameSlot::callee) {
 349             // We have to do some constant-folding here because this enables CreateThis folding. Note
 350             // that we don&#39;t have such watchpoint-based folding for inlined uses of Callee, since in that
 351             // case if the function is a singleton then we already know it.
 352             if (FunctionExecutable* executable = jsDynamicCast&lt;FunctionExecutable*&gt;(*m_vm, m_codeBlock-&gt;ownerExecutable())) {
 353                 if (JSFunction* function = executable-&gt;singleton().inferredValue()) {
 354                     m_graph.watchpoints().addLazily(executable);
 355                     return weakJSConstant(function);
 356                 }
 357             }
 358             return addToGraph(GetCallee);
 359         }
 360 
 361         return getDirect(m_inlineStackTop-&gt;remapOperand(operand));
 362     }
 363 
 364     enum SetMode {
 365         // A normal set which follows a two-phase commit that spans code origins. During
 366         // the current code origin it issues a MovHint, and at the start of the next
 367         // code origin there will be a SetLocal. If the local needs flushing, the second
 368         // SetLocal will be preceded with a Flush.
 369         NormalSet,
 370 
 371         // A set where the SetLocal happens immediately and there is still a Flush. This
 372         // is relevant when assigning to a local in tricky situations for the delayed
 373         // SetLocal logic but where we know that we have not performed any side effects
 374         // within this code origin. This is a safe replacement for NormalSet anytime we
 375         // know that we have not yet performed side effects in this code origin.
 376         ImmediateSetWithFlush,
 377 
 378         // A set where the SetLocal happens immediately and we do not Flush it even if
 379         // this is a local that is marked as needing it. This is relevant when
 380         // initializing locals at the top of a function.
 381         ImmediateNakedSet
 382     };
 383 
 384     Node* setDirect(Operand operand, Node* value, SetMode setMode = NormalSet)
 385     {
 386         addToGraph(MovHint, OpInfo(operand), value);
 387 
 388         // We can&#39;t exit anymore because our OSR exit state has changed.
 389         m_exitOK = false;
 390 
 391         DelayedSetLocal delayed(currentCodeOrigin(), operand, value, setMode);
 392 
 393         if (setMode == NormalSet) {
 394             m_setLocalQueue.append(delayed);
 395             return nullptr;
 396         }
 397 
 398         return delayed.execute(this);
 399     }
 400 
 401     void processSetLocalQueue()
 402     {
 403         for (unsigned i = 0; i &lt; m_setLocalQueue.size(); ++i)
 404             m_setLocalQueue[i].execute(this);
 405         m_setLocalQueue.shrink(0);
 406     }
 407 
 408     Node* set(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 409     {
 410         return setDirect(m_inlineStackTop-&gt;remapOperand(operand), value, setMode);
 411     }
 412 
 413     Node* injectLazyOperandSpeculation(Node* node)
 414     {
 415         ASSERT(node-&gt;op() == GetLocal);
 416         ASSERT(node-&gt;origin.semantic.bytecodeIndex() == m_currentIndex);
 417         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 418         LazyOperandValueProfileKey key(m_currentIndex, node-&gt;operand());
 419         SpeculatedType prediction = m_inlineStackTop-&gt;m_lazyOperands.prediction(locker, key);
 420         node-&gt;variableAccessData()-&gt;predict(prediction);
 421         return node;
 422     }
 423 
 424     // Used in implementing get/set, above, where the operand is a local variable.
 425     Node* getLocalOrTmp(Operand operand)
 426     {
 427         ASSERT(operand.isTmp() || operand.isLocal());
 428         Node*&amp; node = m_currentBlock-&gt;variablesAtTail.operand(operand);
 429 
 430         // This has two goals: 1) link together variable access datas, and 2)
 431         // try to avoid creating redundant GetLocals. (1) is required for
 432         // correctness - no other phase will ensure that block-local variable
 433         // access data unification is done correctly. (2) is purely opportunistic
 434         // and is meant as an compile-time optimization only.
 435 
 436         VariableAccessData* variable;
 437 
 438         if (node) {
 439             variable = node-&gt;variableAccessData();
 440 
 441             switch (node-&gt;op()) {
 442             case GetLocal:
 443                 return node;
 444             case SetLocal:
 445                 return node-&gt;child1().node();
 446             default:
 447                 break;
 448             }
 449         } else
 450             variable = newVariableAccessData(operand);
 451 
 452         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 453         return node;
 454     }
 455     Node* setLocalOrTmp(const CodeOrigin&amp; semanticOrigin, Operand operand, Node* value, SetMode setMode = NormalSet)
 456     {
 457         ASSERT(operand.isTmp() || operand.isLocal());
 458         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 459 
 460         if (operand.isTmp() &amp;&amp; static_cast&lt;unsigned&gt;(operand.value()) &gt;= m_numTmps) {
 461             if (inlineCallFrame())
 462                 dataLogLn(*inlineCallFrame());
 463             dataLogLn(&quot;Bad operand: &quot;, operand, &quot; but current number of tmps is: &quot;, m_numTmps, &quot; code block has: &quot;, m_profiledBlock-&gt;numTmps(), &quot; tmps.&quot;);
 464             CRASH();
 465         }
 466 
 467         if (setMode != ImmediateNakedSet &amp;&amp; !operand.isTmp()) {
 468             VirtualRegister reg = operand.virtualRegister();
 469             ArgumentPosition* argumentPosition = findArgumentPositionForLocal(reg);
 470             if (argumentPosition)
 471                 flushDirect(operand, argumentPosition);
 472             else if (m_graph.needsScopeRegister() &amp;&amp; reg == m_codeBlock-&gt;scopeRegister())
 473                 flush(operand);
 474         }
 475 
 476         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 477         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
 478             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadCache));
 479         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
 480             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadIndexingType));
 481         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 482         m_currentBlock-&gt;variablesAtTail.operand(operand) = node;
 483         return node;
 484     }
 485 
 486     // Used in implementing get/set, above, where the operand is an argument.
 487     Node* getArgument(VirtualRegister operand)
 488     {
 489         unsigned argument = operand.toArgument();
 490         ASSERT(argument &lt; m_numArguments);
 491 
 492         Node* node = m_currentBlock-&gt;variablesAtTail.argument(argument);
 493 
 494         VariableAccessData* variable;
 495 
 496         if (node) {
 497             variable = node-&gt;variableAccessData();
 498 
 499             switch (node-&gt;op()) {
 500             case GetLocal:
 501                 return node;
 502             case SetLocal:
 503                 return node-&gt;child1().node();
 504             default:
 505                 break;
 506             }
 507         } else
 508             variable = newVariableAccessData(operand);
 509 
 510         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 511         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 512         return node;
 513     }
 514     Node* setArgument(const CodeOrigin&amp; semanticOrigin, Operand operand, Node* value, SetMode setMode = NormalSet)
 515     {
 516         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 517 
 518         VirtualRegister reg = operand.virtualRegister();
 519         unsigned argument = reg.toArgument();
 520         ASSERT(argument &lt; m_numArguments);
 521 
 522         VariableAccessData* variableAccessData = newVariableAccessData(reg);
 523 
 524         // Always flush arguments, except for &#39;this&#39;. If &#39;this&#39; is created by us,
 525         // then make sure that it&#39;s never unboxed.
 526         if (argument || m_graph.needsFlushedThis()) {
 527             if (setMode != ImmediateNakedSet)
 528                 flushDirect(reg);
 529         }
 530 
 531         if (!argument &amp;&amp; m_codeBlock-&gt;specializationKind() == CodeForConstruct)
 532             variableAccessData-&gt;mergeShouldNeverUnbox(true);
 533 
 534         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
 535             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadCache));
 536         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
 537             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadIndexingType));
 538         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 539         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 540         return node;
 541     }
 542 
 543     ArgumentPosition* findArgumentPositionForArgument(int argument)
 544     {
 545         InlineStackEntry* stack = m_inlineStackTop;
 546         while (stack-&gt;m_inlineCallFrame)
 547             stack = stack-&gt;m_caller;
 548         return stack-&gt;m_argumentPositions[argument];
 549     }
 550 
 551     ArgumentPosition* findArgumentPositionForLocal(VirtualRegister operand)
 552     {
 553         for (InlineStackEntry* stack = m_inlineStackTop; ; stack = stack-&gt;m_caller) {
 554             InlineCallFrame* inlineCallFrame = stack-&gt;m_inlineCallFrame;
 555             if (!inlineCallFrame)
 556                 break;
 557             if (operand.offset() &lt; static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::headerSizeInRegisters))
 558                 continue;
 559             if (operand.offset() &gt;= static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::thisArgumentOffset() + inlineCallFrame-&gt;argumentsWithFixup.size()))
 560                 continue;
 561             int argument = VirtualRegister(operand.offset() - inlineCallFrame-&gt;stackOffset).toArgument();
 562             return stack-&gt;m_argumentPositions[argument];
 563         }
 564         return nullptr;
 565     }
 566 
 567     ArgumentPosition* findArgumentPosition(Operand operand)
 568     {
 569         if (operand.isTmp())
 570             return nullptr;
 571         if (operand.isArgument())
 572             return findArgumentPositionForArgument(operand.toArgument());
 573         return findArgumentPositionForLocal(operand.virtualRegister());
 574     }
 575 
 576     template&lt;typename AddFlushDirectFunc&gt;
 577     void flushImpl(InlineCallFrame* inlineCallFrame, const AddFlushDirectFunc&amp; addFlushDirect)
 578     {
 579         int numArguments;
 580         if (inlineCallFrame) {
 581             ASSERT(!m_graph.hasDebuggerEnabled());
 582             numArguments = inlineCallFrame-&gt;argumentsWithFixup.size();
 583             if (inlineCallFrame-&gt;isClosureCall)
 584                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, CallFrameSlot::callee));
 585             if (inlineCallFrame-&gt;isVarargs())
 586                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, CallFrameSlot::argumentCountIncludingThis));
 587         } else
 588             numArguments = m_graph.baselineCodeBlockFor(inlineCallFrame)-&gt;numParameters();
 589 
 590         for (unsigned argument = numArguments; argument--;)
 591             addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForArgumentIncludingThis(argument)));
 592 
 593         if (m_graph.needsScopeRegister())
 594             addFlushDirect(nullptr, m_graph.m_codeBlock-&gt;scopeRegister());
 595     }
 596 
 597     template&lt;typename AddFlushDirectFunc, typename AddPhantomLocalDirectFunc&gt;
 598     void flushForTerminalImpl(CodeOrigin origin, const AddFlushDirectFunc&amp; addFlushDirect, const AddPhantomLocalDirectFunc&amp; addPhantomLocalDirect)
 599     {
 600         bool isCallerOrigin = false;
 601         origin.walkUpInlineStack(
 602             [&amp;] (CodeOrigin origin) {
 603                 BytecodeIndex bytecodeIndex = origin.bytecodeIndex();
 604                 InlineCallFrame* inlineCallFrame = origin.inlineCallFrame();
 605                 flushImpl(inlineCallFrame, addFlushDirect);
 606 
 607                 CodeBlock* codeBlock = m_graph.baselineCodeBlockFor(inlineCallFrame);
 608                 FullBytecodeLiveness&amp; fullLiveness = m_graph.livenessFor(codeBlock);
 609                 // Note: We don&#39;t need to handle tmps here because tmps are not required to be flushed to the stack.
 610                 const auto&amp; livenessAtBytecode = fullLiveness.getLiveness(bytecodeIndex, m_graph.appropriateLivenessCalculationPoint(origin, isCallerOrigin));
 611                 for (unsigned local = codeBlock-&gt;numCalleeLocals(); local--;) {
 612                     if (livenessAtBytecode[local])
 613                         addPhantomLocalDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForLocal(local)));
 614                 }
 615                 isCallerOrigin = true;
 616             });
 617     }
 618 
 619     void flush(Operand operand)
 620     {
 621         flushDirect(m_inlineStackTop-&gt;remapOperand(operand));
 622     }
 623 
 624     void flushDirect(Operand operand)
 625     {
 626         flushDirect(operand, findArgumentPosition(operand));
 627     }
 628 
 629     void flushDirect(Operand operand, ArgumentPosition* argumentPosition)
 630     {
 631         addFlushOrPhantomLocal&lt;Flush&gt;(operand, argumentPosition);
 632     }
 633 
 634     template&lt;NodeType nodeType&gt;
 635     void addFlushOrPhantomLocal(Operand operand, ArgumentPosition* argumentPosition)
 636     {
 637         ASSERT(!operand.isConstant());
 638 
 639         Node*&amp; node = m_currentBlock-&gt;variablesAtTail.operand(operand);
 640 
 641         VariableAccessData* variable;
 642 
 643         if (node)
 644             variable = node-&gt;variableAccessData();
 645         else
 646             variable = newVariableAccessData(operand);
 647 
 648         node = addToGraph(nodeType, OpInfo(variable));
 649         if (argumentPosition)
 650             argumentPosition-&gt;addVariable(variable);
 651     }
 652 
 653     void phantomLocalDirect(Operand operand)
 654     {
 655         addFlushOrPhantomLocal&lt;PhantomLocal&gt;(operand, findArgumentPosition(operand));
 656     }
 657 
 658     void flush(InlineStackEntry* inlineStackEntry)
 659     {
 660         auto addFlushDirect = [&amp;] (InlineCallFrame*, Operand operand) { flushDirect(operand); };
 661         flushImpl(inlineStackEntry-&gt;m_inlineCallFrame, addFlushDirect);
 662     }
 663 
 664     void flushForTerminal()
 665     {
 666         auto addFlushDirect = [&amp;] (InlineCallFrame*, Operand operand) { flushDirect(operand); };
 667         auto addPhantomLocalDirect = [&amp;] (InlineCallFrame*, Operand operand) { phantomLocalDirect(operand); };
 668         flushForTerminalImpl(currentCodeOrigin(), addFlushDirect, addPhantomLocalDirect);
 669     }
 670 
 671     void flushForReturn()
 672     {
 673         flush(m_inlineStackTop);
 674     }
 675 
 676     void flushIfTerminal(SwitchData&amp; data)
 677     {
 678         if (data.fallThrough.bytecodeIndex() &gt; m_currentIndex.offset())
 679             return;
 680 
 681         for (unsigned i = data.cases.size(); i--;) {
 682             if (data.cases[i].target.bytecodeIndex() &gt; m_currentIndex.offset())
 683                 return;
 684         }
 685 
 686         flushForTerminal();
 687     }
 688 
 689     // Assumes that the constant should be strongly marked.
 690     Node* jsConstant(JSValue constantValue)
 691     {
 692         return addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(constantValue)));
 693     }
 694 
 695     Node* weakJSConstant(JSValue constantValue)
 696     {
 697         return addToGraph(JSConstant, OpInfo(m_graph.freeze(constantValue)));
 698     }
 699 
 700     // Helper functions to get/set the this value.
 701     Node* getThis()
 702     {
 703         return get(m_inlineStackTop-&gt;m_codeBlock-&gt;thisRegister());
 704     }
 705 
 706     void setThis(Node* value)
 707     {
 708         set(m_inlineStackTop-&gt;m_codeBlock-&gt;thisRegister(), value);
 709     }
 710 
 711     InlineCallFrame* inlineCallFrame()
 712     {
 713         return m_inlineStackTop-&gt;m_inlineCallFrame;
 714     }
 715 
 716     bool allInlineFramesAreTailCalls()
 717     {
 718         return !inlineCallFrame() || !inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 719     }
 720 
 721     CodeOrigin currentCodeOrigin()
 722     {
 723         return CodeOrigin(m_currentIndex, inlineCallFrame());
 724     }
 725 
 726     NodeOrigin currentNodeOrigin()
 727     {
 728         CodeOrigin semantic;
 729         CodeOrigin forExit;
 730 
 731         if (m_currentSemanticOrigin.isSet())
 732             semantic = m_currentSemanticOrigin;
 733         else
 734             semantic = currentCodeOrigin();
 735 
 736         forExit = currentCodeOrigin();
 737 
 738         return NodeOrigin(semantic, forExit, m_exitOK);
 739     }
 740 
 741     BranchData* branchData(unsigned taken, unsigned notTaken)
 742     {
 743         // We assume that branches originating from bytecode always have a fall-through. We
 744         // use this assumption to avoid checking for the creation of terminal blocks.
 745         ASSERT((taken &gt; m_currentIndex.offset()) || (notTaken &gt; m_currentIndex.offset()));
 746         BranchData* data = m_graph.m_branchData.add();
 747         *data = BranchData::withBytecodeIndices(taken, notTaken);
 748         return data;
 749     }
 750 
 751     Node* addToGraph(Node* node)
 752     {
 753         VERBOSE_LOG(&quot;        appended &quot;, node, &quot; &quot;, Graph::opName(node-&gt;op()), &quot;\n&quot;);
 754 
 755         m_hasAnyForceOSRExits |= (node-&gt;op() == ForceOSRExit);
 756 
 757         m_currentBlock-&gt;append(node);
 758         if (clobbersExitState(m_graph, node))
 759             m_exitOK = false;
 760         return node;
 761     }
 762 
 763     Node* addToGraph(NodeType op, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 764     {
 765         Node* result = m_graph.addNode(
 766             op, currentNodeOrigin(), Edge(child1), Edge(child2),
 767             Edge(child3));
 768         return addToGraph(result);
 769     }
 770     Node* addToGraph(NodeType op, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 771     {
 772         Node* result = m_graph.addNode(
 773             op, currentNodeOrigin(), child1, child2, child3);
 774         return addToGraph(result);
 775     }
 776     Node* addToGraph(NodeType op, OpInfo info, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 777     {
 778         Node* result = m_graph.addNode(
 779             op, currentNodeOrigin(), info, Edge(child1), Edge(child2),
 780             Edge(child3));
 781         return addToGraph(result);
 782     }
 783     Node* addToGraph(NodeType op, OpInfo info, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 784     {
 785         Node* result = m_graph.addNode(op, currentNodeOrigin(), info, child1, child2, child3);
 786         return addToGraph(result);
 787     }
 788     Node* addToGraph(NodeType op, OpInfo info1, OpInfo info2, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 789     {
 790         Node* result = m_graph.addNode(
 791             op, currentNodeOrigin(), info1, info2,
 792             Edge(child1), Edge(child2), Edge(child3));
 793         return addToGraph(result);
 794     }
 795     Node* addToGraph(NodeType op, Operand operand, Node* child1)
 796     {
 797         ASSERT(op == MovHint);
 798         return addToGraph(op, OpInfo(operand.kind()), OpInfo(operand.value()), child1);
 799     }
 800     Node* addToGraph(NodeType op, OpInfo info1, OpInfo info2, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 801     {
 802         Node* result = m_graph.addNode(
 803             op, currentNodeOrigin(), info1, info2, child1, child2, child3);
 804         return addToGraph(result);
 805     }
 806 
 807     Node* addToGraph(Node::VarArgTag, NodeType op, OpInfo info1, OpInfo info2 = OpInfo())
 808     {
 809         Node* result = m_graph.addNode(
 810             Node::VarArg, op, currentNodeOrigin(), info1, info2,
 811             m_graph.m_varArgChildren.size() - m_numPassedVarArgs, m_numPassedVarArgs);
 812         addToGraph(result);
 813 
 814         m_numPassedVarArgs = 0;
 815 
 816         return result;
 817     }
 818 
 819     void addVarArgChild(Node* child)
 820     {
 821         m_graph.m_varArgChildren.append(Edge(child));
 822         m_numPassedVarArgs++;
 823     }
 824 
 825     void addVarArgChild(Edge child)
 826     {
 827         m_graph.m_varArgChildren.append(child);
 828         m_numPassedVarArgs++;
 829     }
 830 
 831     Node* addCallWithoutSettingResult(
 832         NodeType op, OpInfo opInfo, Node* callee, int argCount, int registerOffset,
 833         OpInfo prediction)
 834     {
 835         addVarArgChild(callee);
 836         size_t parameterSlots = Graph::parameterSlotsForArgCount(argCount);
 837 
 838         if (parameterSlots &gt; m_parameterSlots)
 839             m_parameterSlots = parameterSlots;
 840 
 841         for (int i = 0; i &lt; argCount; ++i)
 842             addVarArgChild(get(virtualRegisterForArgumentIncludingThis(i, registerOffset)));
 843 
 844         return addToGraph(Node::VarArg, op, opInfo, prediction);
 845     }
 846 
 847     Node* addCall(
 848         VirtualRegister result, NodeType op, const DOMJIT::Signature* signature, Node* callee, int argCount, int registerOffset,
 849         SpeculatedType prediction)
 850     {
 851         if (op == TailCall) {
 852             if (allInlineFramesAreTailCalls())
 853                 return addCallWithoutSettingResult(op, OpInfo(signature), callee, argCount, registerOffset, OpInfo());
 854             op = TailCallInlinedCaller;
 855         }
 856 
 857 
 858         Node* call = addCallWithoutSettingResult(
 859             op, OpInfo(signature), callee, argCount, registerOffset, OpInfo(prediction));
 860         if (result.isValid())
 861             set(result, call);
 862         return call;
 863     }
 864 
 865     Node* cellConstantWithStructureCheck(JSCell* object, Structure* structure)
 866     {
 867         // FIXME: This should route to emitPropertyCheck, not the other way around. But currently,
 868         // this gets no profit from using emitPropertyCheck() since we&#39;ll non-adaptively watch the
 869         // object&#39;s structure as soon as we make it a weakJSCosntant.
 870         Node* objectNode = weakJSConstant(object);
 871         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structure)), objectNode);
 872         return objectNode;
 873     }
 874 
 875     SpeculatedType getPredictionWithoutOSRExit(BytecodeIndex bytecodeIndex)
 876     {
 877         auto getValueProfilePredictionFromForCodeBlockAndBytecodeOffset = [&amp;] (CodeBlock* codeBlock, const CodeOrigin&amp; codeOrigin)
 878         {
 879             SpeculatedType prediction;
 880             {
 881                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
 882                 prediction = codeBlock-&gt;valueProfilePredictionForBytecodeIndex(locker, codeOrigin.bytecodeIndex());
 883             }
 884             auto* fuzzerAgent = m_vm-&gt;fuzzerAgent();
 885             if (UNLIKELY(fuzzerAgent))
 886                 return fuzzerAgent-&gt;getPrediction(codeBlock, codeOrigin, prediction) &amp; SpecBytecodeTop;
 887             return prediction;
 888         };
 889 
 890         SpeculatedType prediction = getValueProfilePredictionFromForCodeBlockAndBytecodeOffset(m_inlineStackTop-&gt;m_profiledBlock, CodeOrigin(bytecodeIndex, inlineCallFrame()));
 891         if (prediction != SpecNone)
 892             return prediction;
 893 
 894         // If we have no information about the values this
 895         // node generates, we check if by any chance it is
 896         // a tail call opcode. In that case, we walk up the
 897         // inline frames to find a call higher in the call
 898         // chain and use its prediction. If we only have
 899         // inlined tail call frames, we use SpecFullTop
 900         // to avoid a spurious OSR exit.
 901         auto instruction = m_inlineStackTop-&gt;m_profiledBlock-&gt;instructions().at(bytecodeIndex.offset());
 902         OpcodeID opcodeID = instruction-&gt;opcodeID();
 903 
 904         switch (opcodeID) {
 905         case op_tail_call:
 906         case op_tail_call_varargs:
 907         case op_tail_call_forward_arguments: {
 908             // Things should be more permissive to us returning BOTTOM instead of TOP here.
 909             // Currently, this will cause us to Force OSR exit. This is bad because returning
 910             // TOP will cause anything that transitively touches this speculated type to
 911             // also become TOP during prediction propagation.
 912             // https://bugs.webkit.org/show_bug.cgi?id=164337
 913             if (!inlineCallFrame())
 914                 return SpecFullTop;
 915 
 916             CodeOrigin* codeOrigin = inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 917             if (!codeOrigin)
 918                 return SpecFullTop;
 919 
 920             InlineStackEntry* stack = m_inlineStackTop;
 921             while (stack-&gt;m_inlineCallFrame != codeOrigin-&gt;inlineCallFrame())
 922                 stack = stack-&gt;m_caller;
 923 
 924             return getValueProfilePredictionFromForCodeBlockAndBytecodeOffset(stack-&gt;m_profiledBlock, *codeOrigin);
 925         }
 926 
 927         default:
 928             return SpecNone;
 929         }
 930 
 931         RELEASE_ASSERT_NOT_REACHED();
 932         return SpecNone;
 933     }
 934 
 935     SpeculatedType getPrediction(BytecodeIndex bytecodeIndex)
 936     {
 937         SpeculatedType prediction = getPredictionWithoutOSRExit(bytecodeIndex);
 938 
 939         if (prediction == SpecNone) {
 940             // We have no information about what values this node generates. Give up
 941             // on executing this code, since we&#39;re likely to do more damage than good.
 942             addToGraph(ForceOSRExit);
 943         }
 944 
 945         return prediction;
 946     }
 947 
 948     SpeculatedType getPredictionWithoutOSRExit()
 949     {
 950         return getPredictionWithoutOSRExit(m_currentIndex);
 951     }
 952 
 953     SpeculatedType getPrediction()
 954     {
 955         return getPrediction(m_currentIndex);
 956     }
 957 
 958     ArrayMode getArrayMode(Array::Action action)
 959     {
 960         CodeBlock* codeBlock = m_inlineStackTop-&gt;m_profiledBlock;
 961         ArrayProfile* profile = codeBlock-&gt;getArrayProfile(codeBlock-&gt;bytecodeIndex(m_currentInstruction));
 962         return getArrayMode(*profile, action);
 963     }
 964 
 965     ArrayMode getArrayMode(ArrayProfile&amp; profile, Array::Action action)
 966     {
 967         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 968         profile.computeUpdatedPrediction(locker, m_inlineStackTop-&gt;m_profiledBlock);
 969         bool makeSafe = profile.outOfBounds(locker);
 970         return ArrayMode::fromObserved(locker, &amp;profile, action, makeSafe);
 971     }
 972 
 973     Node* makeSafe(Node* node)
 974     {
 975         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 976             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
 977         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 978             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
 979 
 980         if (!isX86() &amp;&amp; (node-&gt;op() == ArithMod || node-&gt;op() == ValueMod))
 981             return node;
 982 
 983         switch (node-&gt;op()) {
 984         case ArithAdd:
 985         case ArithSub:
 986         case ValueAdd: {
 987             ObservedResults observed;
 988             if (BinaryArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;binaryArithProfileForBytecodeIndex(m_currentIndex))
 989                 observed = arithProfile-&gt;observedResults();
 990             else if (UnaryArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;unaryArithProfileForBytecodeIndex(m_currentIndex)) {
 991                 // Happens for OpInc/OpDec
 992                 observed = arithProfile-&gt;observedResults();
 993             } else
 994                 break;
 995 
 996             if (observed.didObserveDouble())
 997                 node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 998             if (observed.didObserveNonNumeric())
 999                 node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
1000             if (observed.didObserveBigInt())
1001                 node-&gt;mergeFlags(NodeMayHaveBigIntResult);
1002             break;
1003         }
1004         case ValueMul:
1005         case ArithMul: {
1006             BinaryArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;binaryArithProfileForBytecodeIndex(m_currentIndex);
1007             if (!arithProfile)
1008                 break;
1009             if (arithProfile-&gt;didObserveInt52Overflow())
1010                 node-&gt;mergeFlags(NodeMayOverflowInt52);
1011             if (arithProfile-&gt;didObserveInt32Overflow() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1012                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
1013             if (arithProfile-&gt;didObserveNegZeroDouble() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1014                 node-&gt;mergeFlags(NodeMayNegZeroInBaseline);
1015             if (arithProfile-&gt;didObserveDouble())
1016                 node-&gt;mergeFlags(NodeMayHaveDoubleResult);
1017             if (arithProfile-&gt;didObserveNonNumeric())
1018                 node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
1019             if (arithProfile-&gt;didObserveBigInt())
1020                 node-&gt;mergeFlags(NodeMayHaveBigIntResult);
1021             break;
1022         }
1023         case ValueNegate:
1024         case ArithNegate:
1025         case Inc:
1026         case Dec: {
1027             UnaryArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;unaryArithProfileForBytecodeIndex(m_currentIndex);
1028             if (!arithProfile)
1029                 break;
1030             if (arithProfile-&gt;argObservedType().sawNumber() || arithProfile-&gt;didObserveDouble())
1031                 node-&gt;mergeFlags(NodeMayHaveDoubleResult);
1032             if (arithProfile-&gt;didObserveNegZeroDouble() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1033                 node-&gt;mergeFlags(NodeMayNegZeroInBaseline);
1034             if (arithProfile-&gt;didObserveInt32Overflow() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1035                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
1036             if (arithProfile-&gt;didObserveNonNumeric())
1037                 node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
1038             if (arithProfile-&gt;didObserveBigInt())
1039                 node-&gt;mergeFlags(NodeMayHaveBigIntResult);
1040             break;
1041         }
1042 
1043         default:
1044             break;
1045         }
1046 
1047         if (m_inlineStackTop-&gt;m_profiledBlock-&gt;likelyToTakeSlowCase(m_currentIndex)) {
1048             switch (node-&gt;op()) {
1049             case UInt32ToNumber:
1050             case ArithAdd:
1051             case ArithSub:
1052             case ValueAdd:
1053             case ValueMod:
1054             case ArithMod: // for ArithMod &quot;MayOverflow&quot; means we tried to divide by zero, or we saw double.
1055                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
1056                 break;
1057 
1058             default:
1059                 break;
1060             }
1061         }
1062 
1063         return node;
1064     }
1065 
1066     Node* makeDivSafe(Node* node)
1067     {
1068         ASSERT(node-&gt;op() == ArithDiv || node-&gt;op() == ValueDiv);
1069 
1070         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1071             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
1072         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1073             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
1074 
1075         // The main slow case counter for op_div in the old JIT counts only when
1076         // the operands are not numbers. We don&#39;t care about that since we already
1077         // have speculations in place that take care of that separately. We only
1078         // care about when the outcome of the division is not an integer, which
1079         // is what the special fast case counter tells us.
1080 
1081         if (!m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSpecialArithFastCase(m_currentIndex))
1082             return node;
1083 
1084         // FIXME: It might be possible to make this more granular.
1085         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline | NodeMayNegZeroInBaseline);
1086 
1087         BinaryArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;binaryArithProfileForBytecodeIndex(m_currentIndex);
1088         if (arithProfile-&gt;didObserveBigInt())
1089             node-&gt;mergeFlags(NodeMayHaveBigIntResult);
1090 
1091         return node;
1092     }
1093 
1094     void noticeArgumentsUse()
1095     {
1096         // All of the arguments in this function need to be formatted as JSValues because we will
1097         // load from them in a random-access fashion and we don&#39;t want to have to switch on
1098         // format.
1099 
1100         for (ArgumentPosition* argument : m_inlineStackTop-&gt;m_argumentPositions)
1101             argument-&gt;mergeShouldNeverUnbox(true);
1102     }
1103 
1104     bool needsDynamicLookup(ResolveType, OpcodeID);
1105 
1106     VM* m_vm;
1107     CodeBlock* m_codeBlock;
1108     CodeBlock* m_profiledBlock;
1109     Graph&amp; m_graph;
1110 
1111     // The current block being generated.
1112     BasicBlock* m_currentBlock;
1113     // The bytecode index of the current instruction being generated.
1114     BytecodeIndex m_currentIndex;
1115     // The semantic origin of the current node if different from the current Index.
1116     CodeOrigin m_currentSemanticOrigin;
1117     // True if it&#39;s OK to OSR exit right now.
1118     bool m_exitOK { false };
1119 
1120     FrozenValue* m_constantUndefined;
1121     FrozenValue* m_constantNull;
1122     FrozenValue* m_constantNaN;
1123     FrozenValue* m_constantOne;
1124     Vector&lt;Node*, 16&gt; m_constants;
1125 
1126     HashMap&lt;InlineCallFrame*, Vector&lt;ArgumentPosition*&gt;, WTF::DefaultHash&lt;InlineCallFrame*&gt;::Hash, WTF::NullableHashTraits&lt;InlineCallFrame*&gt;&gt; m_inlineCallFrameToArgumentPositions;
1127 
1128     // The number of arguments passed to the function.
1129     unsigned m_numArguments;
1130     // The number of locals (vars + temporaries) used by the bytecode for the function.
1131     unsigned m_numLocals;
1132     // The max number of temps used for forwarding data to an OSR exit checkpoint.
1133     unsigned m_numTmps;
1134     // The number of slots (in units of sizeof(Register)) that we need to
1135     // preallocate for arguments to outgoing calls from this frame. This
1136     // number includes the CallFrame slots that we initialize for the callee
1137     // (but not the callee-initialized CallerFrame and ReturnPC slots).
1138     // This number is 0 if and only if this function is a leaf.
1139     unsigned m_parameterSlots;
1140     // The number of var args passed to the next var arg node.
1141     unsigned m_numPassedVarArgs;
1142 
1143     struct InlineStackEntry {
1144         ByteCodeParser* m_byteCodeParser;
1145 
1146         CodeBlock* m_codeBlock;
1147         CodeBlock* m_profiledBlock;
1148         InlineCallFrame* m_inlineCallFrame;
1149 
1150         ScriptExecutable* executable() { return m_codeBlock-&gt;ownerExecutable(); }
1151 
1152         QueryableExitProfile m_exitProfile;
1153 
1154         // Remapping of identifier and constant numbers from the code block being
1155         // inlined (inline callee) to the code block that we&#39;re inlining into
1156         // (the machine code block, which is the transitive, though not necessarily
1157         // direct, caller).
1158         Vector&lt;unsigned&gt; m_identifierRemap;
1159         Vector&lt;unsigned&gt; m_switchRemap;
1160 
1161         // These are blocks whose terminal is a Jump, Branch or Switch, and whose target has not yet been linked.
1162         // Their terminal instead refers to a bytecode index, and the right BB can be found in m_blockLinkingTargets.
1163         Vector&lt;BasicBlock*&gt; m_unlinkedBlocks;
1164 
1165         // Potential block linking targets. Must be sorted by bytecodeBegin, and
1166         // cannot have two blocks that have the same bytecodeBegin.
1167         Vector&lt;BasicBlock*&gt; m_blockLinkingTargets;
1168 
1169         // Optional: a continuation block for returns to jump to. It is set by early returns if it does not exist.
1170         BasicBlock* m_continuationBlock;
1171 
1172         VirtualRegister m_returnValue;
1173 
1174         // Speculations about variable types collected from the profiled code block,
1175         // which are based on OSR exit profiles that past DFG compilations of this
1176         // code block had gathered.
1177         LazyOperandValueProfileParser m_lazyOperands;
1178 
1179         ICStatusMap m_baselineMap;
1180         ICStatusContext m_optimizedContext;
1181 
1182         // Pointers to the argument position trackers for this slice of code.
1183         Vector&lt;ArgumentPosition*&gt; m_argumentPositions;
1184 
1185         InlineStackEntry* m_caller;
1186 
1187         InlineStackEntry(
1188             ByteCodeParser*,
1189             CodeBlock*,
1190             CodeBlock* profiledBlock,
1191             JSFunction* callee, // Null if this is a closure call.
1192             VirtualRegister returnValueVR,
1193             VirtualRegister inlineCallFrameStart,
1194             int argumentCountIncludingThis,
1195             InlineCallFrame::Kind,
1196             BasicBlock* continuationBlock);
1197 
1198         ~InlineStackEntry();
1199 
1200         Operand remapOperand(Operand operand) const
1201         {
1202             if (!m_inlineCallFrame)
1203                 return operand;
1204 
1205             if (operand.isTmp())
1206                 return Operand::tmp(operand.value() + m_inlineCallFrame-&gt;tmpOffset);
1207 
1208             ASSERT(!operand.virtualRegister().isConstant());
1209 
1210             return operand.virtualRegister() + m_inlineCallFrame-&gt;stackOffset;
1211         }
1212     };
1213 
1214     InlineStackEntry* m_inlineStackTop;
1215 
1216     ICStatusContextStack m_icContextStack;
1217 
1218     struct DelayedSetLocal {
1219         DelayedSetLocal() { }
1220         DelayedSetLocal(const CodeOrigin&amp; origin, Operand operand, Node* value, SetMode setMode)
1221             : m_origin(origin)
1222             , m_operand(operand)
1223             , m_value(value)
1224             , m_setMode(setMode)
1225         {
1226             RELEASE_ASSERT(operand.isValid());
1227         }
1228 
1229         Node* execute(ByteCodeParser* parser)
1230         {
1231             if (m_operand.isArgument())
1232                 return parser-&gt;setArgument(m_origin, m_operand, m_value, m_setMode);
1233             return parser-&gt;setLocalOrTmp(m_origin, m_operand, m_value, m_setMode);
1234         }
1235 
1236         CodeOrigin m_origin;
1237         Operand m_operand;
1238         Node* m_value { nullptr };
1239         SetMode m_setMode;
1240     };
1241 
1242     Vector&lt;DelayedSetLocal, 2&gt; m_setLocalQueue;
1243 
1244     const Instruction* m_currentInstruction;
1245     bool m_hasDebuggerEnabled;
1246     bool m_hasAnyForceOSRExits { false };
1247 };
1248 
1249 BasicBlock* ByteCodeParser::allocateTargetableBlock(BytecodeIndex bytecodeIndex)
1250 {
1251     ASSERT(bytecodeIndex);
1252     Ref&lt;BasicBlock&gt; block = adoptRef(*new BasicBlock(bytecodeIndex, m_numArguments, m_numLocals, m_numTmps, 1));
1253     BasicBlock* blockPtr = block.ptr();
1254     // m_blockLinkingTargets must always be sorted in increasing order of bytecodeBegin
1255     if (m_inlineStackTop-&gt;m_blockLinkingTargets.size())
1256         ASSERT(m_inlineStackTop-&gt;m_blockLinkingTargets.last()-&gt;bytecodeBegin.offset() &lt; bytecodeIndex.offset());
1257     m_inlineStackTop-&gt;m_blockLinkingTargets.append(blockPtr);
1258     m_graph.appendBlock(WTFMove(block));
1259     return blockPtr;
1260 }
1261 
1262 BasicBlock* ByteCodeParser::allocateUntargetableBlock()
1263 {
1264     Ref&lt;BasicBlock&gt; block = adoptRef(*new BasicBlock(BytecodeIndex(), m_numArguments, m_numLocals, m_numTmps, 1));
1265     BasicBlock* blockPtr = block.ptr();
1266     m_graph.appendBlock(WTFMove(block));
1267     return blockPtr;
1268 }
1269 
1270 void ByteCodeParser::makeBlockTargetable(BasicBlock* block, BytecodeIndex bytecodeIndex)
1271 {
1272     RELEASE_ASSERT(!block-&gt;bytecodeBegin);
1273     block-&gt;bytecodeBegin = bytecodeIndex;
1274     // m_blockLinkingTargets must always be sorted in increasing order of bytecodeBegin
1275     if (m_inlineStackTop-&gt;m_blockLinkingTargets.size())
1276         ASSERT(m_inlineStackTop-&gt;m_blockLinkingTargets.last()-&gt;bytecodeBegin.offset() &lt; bytecodeIndex.offset());
1277     m_inlineStackTop-&gt;m_blockLinkingTargets.append(block);
1278 }
1279 
1280 void ByteCodeParser::addJumpTo(BasicBlock* block)
1281 {
1282     ASSERT(!m_currentBlock-&gt;terminal());
1283     Node* jumpNode = addToGraph(Jump);
1284     jumpNode-&gt;targetBlock() = block;
1285     m_currentBlock-&gt;didLink();
1286 }
1287 
1288 void ByteCodeParser::addJumpTo(unsigned bytecodeIndex)
1289 {
1290     ASSERT(!m_currentBlock-&gt;terminal());
1291     addToGraph(Jump, OpInfo(bytecodeIndex));
1292     m_inlineStackTop-&gt;m_unlinkedBlocks.append(m_currentBlock);
1293 }
1294 
1295 template&lt;typename CallOp&gt;
1296 ByteCodeParser::Terminality ByteCodeParser::handleCall(const Instruction* pc, NodeType op, CallMode callMode)
1297 {
1298     auto bytecode = pc-&gt;as&lt;CallOp&gt;();
1299     Node* callTarget = get(bytecode.m_callee);
1300     int registerOffset = -static_cast&lt;int&gt;(bytecode.m_argv);
1301 
1302     CallLinkStatus callLinkStatus = CallLinkStatus::computeFor(
1303         m_inlineStackTop-&gt;m_profiledBlock, currentCodeOrigin(),
1304         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack);
1305 
1306     InlineCallFrame::Kind kind = InlineCallFrame::kindFor(callMode);
1307 
1308     return handleCall(bytecode.m_dst, op, kind, pc-&gt;size(), callTarget,
1309         bytecode.m_argc, registerOffset, callLinkStatus, getPrediction());
1310 }
1311 
1312 void ByteCodeParser::refineStatically(CallLinkStatus&amp; callLinkStatus, Node* callTarget)
1313 {
1314     if (callTarget-&gt;isCellConstant())
1315         callLinkStatus.setProvenConstantCallee(CallVariant(callTarget-&gt;asCell()));
1316 }
1317 
1318 ByteCodeParser::Terminality ByteCodeParser::handleCall(
1319     VirtualRegister result, NodeType op, InlineCallFrame::Kind kind, unsigned instructionSize,
1320     Node* callTarget, int argumentCountIncludingThis, int registerOffset,
1321     CallLinkStatus callLinkStatus, SpeculatedType prediction)
1322 {
1323     ASSERT(registerOffset &lt;= 0);
1324 
1325     refineStatically(callLinkStatus, callTarget);
1326 
1327     VERBOSE_LOG(&quot;    Handling call at &quot;, currentCodeOrigin(), &quot;: &quot;, callLinkStatus, &quot;\n&quot;);
1328 
1329     // If we have profiling information about this call, and it did not behave too polymorphically,
1330     // we may be able to inline it, or in the case of recursive tail calls turn it into a jump.
1331     if (callLinkStatus.canOptimize()) {
1332         addToGraph(FilterCallLinkStatus, OpInfo(m_graph.m_plan.recordedStatuses().addCallLinkStatus(currentCodeOrigin(), callLinkStatus)), callTarget);
1333 
1334         VirtualRegister thisArgument = virtualRegisterForArgumentIncludingThis(0, registerOffset);
1335         auto optimizationResult = handleInlining(callTarget, result, callLinkStatus, registerOffset, thisArgument,
1336             argumentCountIncludingThis, BytecodeIndex(m_currentIndex.offset() + instructionSize), op, kind, prediction);
1337         if (optimizationResult == CallOptimizationResult::OptimizedToJump)
1338             return Terminal;
1339         if (optimizationResult == CallOptimizationResult::Inlined) {
1340             if (UNLIKELY(m_graph.compilation()))
1341                 m_graph.compilation()-&gt;noticeInlinedCall();
1342             return NonTerminal;
1343         }
1344     }
1345 
1346     Node* callNode = addCall(result, op, nullptr, callTarget, argumentCountIncludingThis, registerOffset, prediction);
1347     ASSERT(callNode-&gt;op() != TailCallVarargs &amp;&amp; callNode-&gt;op() != TailCallForwardVarargs);
1348     return callNode-&gt;op() == TailCall ? Terminal : NonTerminal;
1349 }
1350 
1351 template&lt;typename CallOp&gt;
1352 ByteCodeParser::Terminality ByteCodeParser::handleVarargsCall(const Instruction* pc, NodeType op, CallMode callMode)
1353 {
1354     auto bytecode = pc-&gt;as&lt;CallOp&gt;();
1355     int firstFreeReg = bytecode.m_firstFree.offset();
1356     int firstVarArgOffset = bytecode.m_firstVarArg;
1357 
1358     SpeculatedType prediction = getPrediction();
1359 
1360     Node* callTarget = get(bytecode.m_callee);
1361 
1362     CallLinkStatus callLinkStatus = CallLinkStatus::computeFor(
1363         m_inlineStackTop-&gt;m_profiledBlock, currentCodeOrigin(),
1364         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack);
1365     refineStatically(callLinkStatus, callTarget);
1366 
1367     VERBOSE_LOG(&quot;    Varargs call link status at &quot;, currentCodeOrigin(), &quot;: &quot;, callLinkStatus, &quot;\n&quot;);
1368 
1369     if (callLinkStatus.canOptimize()) {
1370         addToGraph(FilterCallLinkStatus, OpInfo(m_graph.m_plan.recordedStatuses().addCallLinkStatus(currentCodeOrigin(), callLinkStatus)), callTarget);
1371 
1372         if (handleVarargsInlining(callTarget, bytecode.m_dst,
1373             callLinkStatus, firstFreeReg, bytecode.m_thisValue, bytecode.m_arguments,
1374             firstVarArgOffset, op,
1375             InlineCallFrame::varargsKindFor(callMode))) {
1376             if (UNLIKELY(m_graph.compilation()))
1377                 m_graph.compilation()-&gt;noticeInlinedCall();
1378             return NonTerminal;
1379         }
1380     }
1381 
1382     CallVarargsData* data = m_graph.m_callVarargsData.add();
1383     data-&gt;firstVarArgOffset = firstVarArgOffset;
1384 
1385     Node* thisChild = get(bytecode.m_thisValue);
1386     Node* argumentsChild = nullptr;
1387     if (op != TailCallForwardVarargs)
1388         argumentsChild = get(bytecode.m_arguments);
1389 
1390     if (op == TailCallVarargs || op == TailCallForwardVarargs) {
1391         if (allInlineFramesAreTailCalls()) {
1392             addToGraph(op, OpInfo(data), OpInfo(), callTarget, thisChild, argumentsChild);
1393             return Terminal;
1394         }
1395         op = op == TailCallVarargs ? TailCallVarargsInlinedCaller : TailCallForwardVarargsInlinedCaller;
1396     }
1397 
1398     Node* call = addToGraph(op, OpInfo(data), OpInfo(prediction), callTarget, thisChild, argumentsChild);
1399     if (bytecode.m_dst.isValid())
1400         set(bytecode.m_dst, call);
1401     return NonTerminal;
1402 }
1403 
1404 void ByteCodeParser::emitFunctionChecks(CallVariant callee, Node* callTarget, VirtualRegister thisArgumentReg)
1405 {
1406     Node* thisArgument;
1407     if (thisArgumentReg.isValid())
1408         thisArgument = get(thisArgumentReg);
1409     else
1410         thisArgument = nullptr;
1411 
1412     JSCell* calleeCell;
1413     Node* callTargetForCheck;
1414     if (callee.isClosureCall()) {
1415         calleeCell = callee.executable();
1416         callTargetForCheck = addToGraph(GetExecutable, callTarget);
1417     } else {
1418         calleeCell = callee.nonExecutableCallee();
1419         callTargetForCheck = callTarget;
1420     }
1421 
1422     ASSERT(calleeCell);
1423     addToGraph(CheckCell, OpInfo(m_graph.freeze(calleeCell)), callTargetForCheck);
1424     if (thisArgument)
1425         addToGraph(Phantom, thisArgument);
1426 }
1427 
1428 Node* ByteCodeParser::getArgumentCount()
1429 {
1430     Node* argumentCount;
1431     if (m_inlineStackTop-&gt;m_inlineCallFrame &amp;&amp; !m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isVarargs())
1432         argumentCount = jsConstant(m_graph.freeze(jsNumber(m_inlineStackTop-&gt;m_inlineCallFrame-&gt;argumentCountIncludingThis))-&gt;value());
1433     else
1434         argumentCount = addToGraph(GetArgumentCountIncludingThis, OpInfo(m_inlineStackTop-&gt;m_inlineCallFrame), OpInfo(SpecInt32Only));
1435     return argumentCount;
1436 }
1437 
1438 void ByteCodeParser::emitArgumentPhantoms(int registerOffset, int argumentCountIncludingThis)
1439 {
1440     for (int i = 0; i &lt; argumentCountIncludingThis; ++i)
1441         addToGraph(Phantom, get(virtualRegisterForArgumentIncludingThis(i, registerOffset)));
1442 }
1443 
1444 template&lt;typename ChecksFunctor&gt;
1445 bool ByteCodeParser::handleRecursiveTailCall(Node* callTargetNode, CallVariant callVariant, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; emitFunctionCheckIfNeeded)
1446 {
1447     if (UNLIKELY(!Options::optimizeRecursiveTailCalls()))
1448         return false;
1449 
1450     auto targetExecutable = callVariant.executable();
1451     InlineStackEntry* stackEntry = m_inlineStackTop;
1452     do {
1453         if (targetExecutable != stackEntry-&gt;executable())
1454             continue;
1455         VERBOSE_LOG(&quot;   We found a recursive tail call, trying to optimize it into a jump.\n&quot;);
1456 
1457         if (auto* callFrame = stackEntry-&gt;m_inlineCallFrame) {
1458             // FIXME: We only accept jump to CallFrame which has exact same argumentCountIncludingThis. But we can remove this by fixing up arguments.
1459             // And we can also allow jumping into CallFrame with Varargs if the passing number of arguments is greater than or equal to mandatoryMinimum of CallFrame.
1460             // https://bugs.webkit.org/show_bug.cgi?id=202317
1461 
1462             // Some code may statically use the argument count from the InlineCallFrame, so it would be invalid to loop back if it does not match.
1463             // We &quot;continue&quot; instead of returning false in case another stack entry further on the stack has the right number of arguments.
1464             if (argumentCountIncludingThis != static_cast&lt;int&gt;(callFrame-&gt;argumentCountIncludingThis))
1465                 continue;
1466             // If the target InlineCallFrame is Varargs, we do not know how many arguments are actually filled by LoadVarargs. Varargs InlineCallFrame&#39;s
1467             // argumentCountIncludingThis is maximum number of potentially filled arguments by xkLoadVarargs. We &quot;continue&quot; to the upper frame which may be
1468             // a good target to jump into.
1469             if (callFrame-&gt;isVarargs())
1470                 continue;
1471         } else {
1472             // We are in the machine code entry (i.e. the original caller).
1473             // If we have more arguments than the number of parameters to the function, it is not clear where we could put them on the stack.
1474             if (argumentCountIncludingThis &gt; m_codeBlock-&gt;numParameters())
1475                 return false;
1476         }
1477 
1478         // If an InlineCallFrame is not a closure, it was optimized using a constant callee.
1479         // Check if this is the same callee that we try to inline here.
1480         if (stackEntry-&gt;m_inlineCallFrame &amp;&amp; !stackEntry-&gt;m_inlineCallFrame-&gt;isClosureCall) {
1481             if (stackEntry-&gt;m_inlineCallFrame-&gt;calleeConstant() != callVariant.function())
1482                 continue;
1483         }
1484 
1485         // We must add some check that the profiling information was correct and the target of this call is what we thought.
1486         emitFunctionCheckIfNeeded();
1487         // We flush everything, as if we were in the backedge of a loop (see treatment of op_jmp in parseBlock).
1488         flushForTerminal();
1489 
1490         // We must set the callee to the right value
1491         if (stackEntry-&gt;m_inlineCallFrame) {
1492             if (stackEntry-&gt;m_inlineCallFrame-&gt;isClosureCall)
1493                 setDirect(remapOperand(stackEntry-&gt;m_inlineCallFrame, CallFrameSlot::callee), callTargetNode, NormalSet);
1494         } else
1495             addToGraph(SetCallee, callTargetNode);
1496 
1497         // We must set the arguments to the right values
1498         if (!stackEntry-&gt;m_inlineCallFrame)
1499             addToGraph(SetArgumentCountIncludingThis, OpInfo(argumentCountIncludingThis));
1500         int argIndex = 0;
1501         for (; argIndex &lt; argumentCountIncludingThis; ++argIndex) {
1502             Node* value = get(virtualRegisterForArgumentIncludingThis(argIndex, registerOffset));
1503             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgumentIncludingThis(argIndex)), value, NormalSet);
1504         }
1505         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1506         for (; argIndex &lt; stackEntry-&gt;m_codeBlock-&gt;numParameters(); ++argIndex)
1507             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgumentIncludingThis(argIndex)), undefined, NormalSet);
1508 
1509         // We must repeat the work of op_enter here as we will jump right after it.
1510         // We jump right after it and not before it, because of some invariant saying that a CFG root cannot have predecessors in the IR.
1511         for (int i = 0; i &lt; stackEntry-&gt;m_codeBlock-&gt;numVars(); ++i)
1512             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForLocal(i)), undefined, NormalSet);
1513 
1514         // We want to emit the SetLocals with an exit origin that points to the place we are jumping to.
1515         BytecodeIndex oldIndex = m_currentIndex;
1516         auto oldStackTop = m_inlineStackTop;
1517         m_inlineStackTop = stackEntry;
1518         m_currentIndex = BytecodeIndex(opcodeLengths[op_enter]);
1519         m_exitOK = true;
1520         processSetLocalQueue();
1521         m_currentIndex = oldIndex;
1522         m_inlineStackTop = oldStackTop;
1523         m_exitOK = false;
1524 
1525         BasicBlock** entryBlockPtr = tryBinarySearch&lt;BasicBlock*, BytecodeIndex&gt;(stackEntry-&gt;m_blockLinkingTargets, stackEntry-&gt;m_blockLinkingTargets.size(), BytecodeIndex(opcodeLengths[op_enter]), getBytecodeBeginForBlock);
1526         RELEASE_ASSERT(entryBlockPtr);
1527         addJumpTo(*entryBlockPtr);
1528         return true;
1529         // It would be unsound to jump over a non-tail call: the &quot;tail&quot; call is not really a tail call in that case.
1530     } while (stackEntry-&gt;m_inlineCallFrame &amp;&amp; stackEntry-&gt;m_inlineCallFrame-&gt;kind == InlineCallFrame::TailCall &amp;&amp; (stackEntry = stackEntry-&gt;m_caller));
1531 
1532     // The tail call was not recursive
1533     return false;
1534 }
1535 
1536 unsigned ByteCodeParser::inliningCost(CallVariant callee, int argumentCountIncludingThis, InlineCallFrame::Kind kind)
1537 {
1538     CallMode callMode = InlineCallFrame::callModeFor(kind);
1539     CodeSpecializationKind specializationKind = specializationKindFor(callMode);
1540     VERBOSE_LOG(&quot;Considering inlining &quot;, callee, &quot; into &quot;, currentCodeOrigin(), &quot;\n&quot;);
1541 
1542     if (m_hasDebuggerEnabled) {
1543         VERBOSE_LOG(&quot;    Failing because the debugger is in use.\n&quot;);
1544         return UINT_MAX;
1545     }
1546 
1547     FunctionExecutable* executable = callee.functionExecutable();
1548     if (!executable) {
1549         VERBOSE_LOG(&quot;    Failing because there is no function executable.\n&quot;);
1550         return UINT_MAX;
1551     }
1552 
1553     // Do we have a code block, and does the code block&#39;s size match the heuristics/requirements for
1554     // being an inline candidate? We might not have a code block (1) if code was thrown away,
1555     // (2) if we simply hadn&#39;t actually made this call yet or (3) code is a builtin function and
1556     // specialization kind is construct. In the former 2 cases, we could still theoretically attempt
1557     // to inline it if we had a static proof of what was being called; this might happen for example
1558     // if you call a global function, where watchpointing gives us static information. Overall,
1559     // it&#39;s a rare case because we expect that any hot callees would have already been compiled.
1560     CodeBlock* codeBlock = executable-&gt;baselineCodeBlockFor(specializationKind);
1561     if (!codeBlock) {
1562         VERBOSE_LOG(&quot;    Failing because no code block available.\n&quot;);
1563         return UINT_MAX;
1564     }
1565 
1566     if (!Options::useArityFixupInlining()) {
1567         if (codeBlock-&gt;numParameters() &gt; argumentCountIncludingThis) {
1568             VERBOSE_LOG(&quot;    Failing because of arity mismatch.\n&quot;);
1569             return UINT_MAX;
1570         }
1571     }
1572 
1573     CapabilityLevel capabilityLevel = inlineFunctionForCapabilityLevel(
1574         codeBlock, specializationKind, callee.isClosureCall());
1575     VERBOSE_LOG(&quot;    Call mode: &quot;, callMode, &quot;\n&quot;);
1576     VERBOSE_LOG(&quot;    Is closure call: &quot;, callee.isClosureCall(), &quot;\n&quot;);
1577     VERBOSE_LOG(&quot;    Capability level: &quot;, capabilityLevel, &quot;\n&quot;);
1578     VERBOSE_LOG(&quot;    Might inline function: &quot;, mightInlineFunctionFor(codeBlock, specializationKind), &quot;\n&quot;);
1579     VERBOSE_LOG(&quot;    Might compile function: &quot;, mightCompileFunctionFor(codeBlock, specializationKind), &quot;\n&quot;);
1580     VERBOSE_LOG(&quot;    Is supported for inlining: &quot;, isSupportedForInlining(codeBlock), &quot;\n&quot;);
1581     VERBOSE_LOG(&quot;    Is inlining candidate: &quot;, codeBlock-&gt;ownerExecutable()-&gt;isInliningCandidate(), &quot;\n&quot;);
1582     if (!canInline(capabilityLevel)) {
1583         VERBOSE_LOG(&quot;    Failing because the function is not inlineable.\n&quot;);
1584         return UINT_MAX;
1585     }
1586 
1587     // Check if the caller is already too large. We do this check here because that&#39;s just
1588     // where we happen to also have the callee&#39;s code block, and we want that for the
1589     // purpose of unsetting SABI.
1590     if (!isSmallEnoughToInlineCodeInto(m_codeBlock)) {
1591         codeBlock-&gt;m_shouldAlwaysBeInlined = false;
1592         VERBOSE_LOG(&quot;    Failing because the caller is too large.\n&quot;);
1593         return UINT_MAX;
1594     }
1595 
1596     // FIXME: this should be better at predicting how much bloat we will introduce by inlining
1597     // this function.
1598     // https://bugs.webkit.org/show_bug.cgi?id=127627
1599 
1600     // FIXME: We currently inline functions that have run in LLInt but not in Baseline. These
1601     // functions have very low fidelity profiling, and presumably they weren&#39;t very hot if they
1602     // haven&#39;t gotten to Baseline yet. Consider not inlining these functions.
1603     // https://bugs.webkit.org/show_bug.cgi?id=145503
1604 
1605     // Have we exceeded inline stack depth, or are we trying to inline a recursive call to
1606     // too many levels? If either of these are detected, then don&#39;t inline. We adjust our
1607     // heuristics if we are dealing with a function that cannot otherwise be compiled.
1608 
1609     unsigned depth = 0;
1610     unsigned recursion = 0;
1611 
1612     for (InlineStackEntry* entry = m_inlineStackTop; entry; entry = entry-&gt;m_caller) {
1613         ++depth;
1614         if (depth &gt;= Options::maximumInliningDepth()) {
1615             VERBOSE_LOG(&quot;    Failing because depth exceeded.\n&quot;);
1616             return UINT_MAX;
1617         }
1618 
1619         if (entry-&gt;executable() == executable) {
1620             ++recursion;
1621             if (recursion &gt;= Options::maximumInliningRecursion()) {
1622                 VERBOSE_LOG(&quot;    Failing because recursion detected.\n&quot;);
1623                 return UINT_MAX;
1624             }
1625         }
1626     }
1627 
1628     VERBOSE_LOG(&quot;    Inlining should be possible.\n&quot;);
1629 
1630     // It might be possible to inline.
1631     return codeBlock-&gt;bytecodeCost();
1632 }
1633 
1634 template&lt;typename ChecksFunctor&gt;
1635 void ByteCodeParser::inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks)
1636 {
1637     const Instruction* savedCurrentInstruction = m_currentInstruction;
1638     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1639 
1640     CodeBlock* codeBlock = callee.functionExecutable()-&gt;baselineCodeBlockFor(specializationKind);
1641     insertChecks(codeBlock);
1642 
1643     // FIXME: Don&#39;t flush constants!
1644 
1645     // arityFixupCount and numberOfStackPaddingSlots are different. While arityFixupCount does not consider about stack alignment,
1646     // numberOfStackPaddingSlots consider alignment. Consider the following case,
1647     //
1648     // before: [ ... ][arg0][header]
1649     // after:  [ ... ][ext ][arg1][arg0][header]
1650     //
1651     // In the above case, arityFixupCount is 1. But numberOfStackPaddingSlots is 2 because the stack needs to be aligned.
1652     // We insert extra slots to align stack.
1653     int arityFixupCount = std::max&lt;int&gt;(codeBlock-&gt;numParameters() - argumentCountIncludingThis, 0);
1654     int numberOfStackPaddingSlots = CommonSlowPaths::numberOfStackPaddingSlots(codeBlock, argumentCountIncludingThis);
1655     ASSERT(!(numberOfStackPaddingSlots % stackAlignmentRegisters()));
1656     int registerOffsetAfterFixup = registerOffset - numberOfStackPaddingSlots;
1657 
1658     Operand inlineCallFrameStart = VirtualRegister(m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffsetAfterFixup)).value() + CallFrame::headerSizeInRegisters);
1659 
1660     ensureLocals(
1661         inlineCallFrameStart.toLocal() + 1 +
1662         CallFrame::headerSizeInRegisters + codeBlock-&gt;numCalleeLocals());
1663 
1664     ensureTmps((m_inlineStackTop-&gt;m_inlineCallFrame ? m_inlineStackTop-&gt;m_inlineCallFrame-&gt;tmpOffset : 0) + m_inlineStackTop-&gt;m_codeBlock-&gt;numTmps() + codeBlock-&gt;numTmps());
1665 
1666     size_t argumentPositionStart = m_graph.m_argumentPositions.size();
1667 
1668     if (result.isValid())
1669         result = m_inlineStackTop-&gt;remapOperand(result).virtualRegister();
1670 
1671     VariableAccessData* calleeVariable = nullptr;
1672     if (callee.isClosureCall()) {
1673         Node* calleeSet = set(
1674             VirtualRegister(registerOffsetAfterFixup + CallFrameSlot::callee), callTargetNode, ImmediateNakedSet);
1675 
1676         calleeVariable = calleeSet-&gt;variableAccessData();
1677         calleeVariable-&gt;mergeShouldNeverUnbox(true);
1678     }
1679 
1680     InlineStackEntry* callerStackTop = m_inlineStackTop;
1681     InlineStackEntry inlineStackEntry(this, codeBlock, codeBlock, callee.function(), result,
1682         inlineCallFrameStart.virtualRegister(), argumentCountIncludingThis, kind, continuationBlock);
1683 
1684     // This is where the actual inlining really happens.
1685     BytecodeIndex oldIndex = m_currentIndex;
1686     m_currentIndex = BytecodeIndex(0);
1687 
1688     switch (kind) {
1689     case InlineCallFrame::GetterCall:
1690     case InlineCallFrame::SetterCall: {
1691         // When inlining getter and setter calls, we setup a stack frame which does not appear in the bytecode.
1692         // Because Inlining can switch on executable, we could have a graph like this.
1693         //
1694         // BB#0
1695         //     ...
1696         //     30: GetSetter
1697         //     31: MovHint(loc10)
1698         //     32: SetLocal(loc10)
1699         //     33: MovHint(loc9)
1700         //     34: SetLocal(loc9)
1701         //     ...
1702         //     37: GetExecutable(@30)
1703         //     ...
1704         //     41: Switch(@37)
1705         //
1706         // BB#2
1707         //     42: GetLocal(loc12, bc#7 of caller)
1708         //     ...
1709         //     --&gt; callee: loc9 and loc10 are arguments of callee.
1710         //       ...
1711         //       &lt;HERE, exit to callee, loc9 and loc10 are required in the bytecode&gt;
1712         //
1713         // When we prune OSR availability at the beginning of BB#2 (bc#7 in the caller), we prune loc9 and loc10&#39;s liveness because the caller does not actually have loc9 and loc10.
1714         // However, when we begin executing the callee, we need OSR exit to be aware of where it can recover the arguments to the setter, loc9 and loc10. The MovHints in the inlined
1715         // callee make it so that if we exit at &lt;HERE&gt;, we can recover loc9 and loc10.
1716         for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {
1717             Operand argumentToGet = callerStackTop-&gt;remapOperand(virtualRegisterForArgumentIncludingThis(index, registerOffset));
1718             Node* value = getDirect(argumentToGet);
1719             addToGraph(MovHint, OpInfo(argumentToGet), value);
1720             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToGet, value, ImmediateNakedSet });
1721         }
1722         break;
1723     }
1724     default:
1725         break;
1726     }
1727 
1728     if (arityFixupCount) {
1729         // Note: we do arity fixup in two phases:
1730         // 1. We get all the values we need and MovHint them to the expected locals.
1731         // 2. We SetLocal them after that. This way, if we exit, the callee&#39;s
1732         //    frame is already set up. If any SetLocal exits, we have a valid exit state.
1733         //    This is required because if we didn&#39;t do this in two phases, we may exit in
1734         //    the middle of arity fixup from the callee&#39;s CodeOrigin. This is unsound because exited
1735         //    code does not have arity fixup so that remaining necessary fixups are not executed.
1736         //    For example, consider if we need to pad two args:
1737         //    [arg3][arg2][arg1][arg0]
1738         //    [fix ][fix ][arg3][arg2][arg1][arg0]
1739         //    We memcpy starting from arg0 in the direction of arg3. If we were to exit at a type check
1740         //    for arg3&#39;s SetLocal in the callee&#39;s CodeOrigin, we&#39;d exit with a frame like so:
1741         //    [arg3][arg2][arg1][arg2][arg1][arg0]
1742         //    Since we do not perform arity fixup in the callee, this is the frame used by the callee.
1743         //    And the callee would then just end up thinking its argument are:
1744         //    [fix ][fix ][arg3][arg2][arg1][arg0]
1745         //    which is incorrect.
1746 
1747         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1748         // The stack needs to be aligned due to the JS calling convention. Thus, we have a hole if the count of arguments is not aligned.
1749         // We call this hole &quot;extra slot&quot;. Consider the following case, the number of arguments is 2. If this argument
1750         // count does not fulfill the stack alignment requirement, we already inserted extra slots.
1751         //
1752         // before: [ ... ][ext ][arg1][arg0][header]
1753         //
1754         // In the above case, one extra slot is inserted. If the code&#39;s parameter count is 3, we will fixup arguments.
1755         // At that time, we can simply use this extra slots. So the fixuped stack is the following.
1756         //
1757         // before: [ ... ][ext ][arg1][arg0][header]
1758         // after:  [ ... ][arg2][arg1][arg0][header]
1759         //
1760         // In such cases, we do not need to move frames.
1761         if (registerOffsetAfterFixup != registerOffset) {
1762             for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {
1763                 Operand argumentToGet = callerStackTop-&gt;remapOperand(virtualRegisterForArgumentIncludingThis(index, registerOffset));
1764                 Node* value = getDirect(argumentToGet);
1765                 Operand argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgumentIncludingThis(index));
1766                 addToGraph(MovHint, OpInfo(argumentToSet), value);
1767                 m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, value, ImmediateNakedSet });
1768             }
1769         }
1770         for (int index = 0; index &lt; arityFixupCount; ++index) {
1771             Operand argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgumentIncludingThis(argumentCountIncludingThis + index));
1772             addToGraph(MovHint, OpInfo(argumentToSet), undefined);
1773             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, undefined, ImmediateNakedSet });
1774         }
1775 
1776         // At this point, it&#39;s OK to OSR exit because we finished setting up
1777         // our callee&#39;s frame. We emit an ExitOK below.
1778     }
1779 
1780     // At this point, it&#39;s again OK to OSR exit.
1781     m_exitOK = true;
1782     addToGraph(ExitOK);
1783 
1784     processSetLocalQueue();
1785 
1786     InlineVariableData inlineVariableData;
1787     inlineVariableData.inlineCallFrame = m_inlineStackTop-&gt;m_inlineCallFrame;
1788     inlineVariableData.argumentPositionStart = argumentPositionStart;
1789     inlineVariableData.calleeVariable = 0;
1790 
1791     RELEASE_ASSERT(
1792         m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isClosureCall
1793         == callee.isClosureCall());
1794     if (callee.isClosureCall()) {
1795         RELEASE_ASSERT(calleeVariable);
1796         inlineVariableData.calleeVariable = calleeVariable;
1797     }
1798 
1799     m_graph.m_inlineVariableData.append(inlineVariableData);
1800 
1801     parseCodeBlock();
1802     clearCaches(); // Reset our state now that we&#39;re back to the outer code.
1803 
1804     m_currentIndex = oldIndex;
1805     m_exitOK = false;
1806 
1807     linkBlocks(inlineStackEntry.m_unlinkedBlocks, inlineStackEntry.m_blockLinkingTargets);
1808 
1809     // Most functions have at least one op_ret and thus set up the continuation block.
1810     // In some rare cases, a function ends in op_unreachable, forcing us to allocate a new continuationBlock here.
1811     if (inlineStackEntry.m_continuationBlock)
1812         m_currentBlock = inlineStackEntry.m_continuationBlock;
1813     else
1814         m_currentBlock = allocateUntargetableBlock();
1815     ASSERT(!m_currentBlock-&gt;terminal());
1816 
1817     prepareToParseBlock();
1818     m_currentInstruction = savedCurrentInstruction;
1819 }
1820 
1821 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleCallVariant(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, BytecodeIndex nextIndex, InlineCallFrame::Kind kind, SpeculatedType prediction, unsigned&amp; inliningBalance, BasicBlock* continuationBlock, bool needsToCheckCallee)
1822 {
1823     VERBOSE_LOG(&quot;    Considering callee &quot;, callee, &quot;\n&quot;);
1824 
1825     bool didInsertChecks = false;
1826     auto insertChecksWithAccounting = [&amp;] () {
1827         if (needsToCheckCallee)
1828             emitFunctionChecks(callee, callTargetNode, thisArgument);
1829         didInsertChecks = true;
1830     };
1831 
1832     if (kind == InlineCallFrame::TailCall &amp;&amp; ByteCodeParser::handleRecursiveTailCall(callTargetNode, callee, registerOffset, argumentCountIncludingThis, insertChecksWithAccounting)) {
1833         RELEASE_ASSERT(didInsertChecks);
1834         return CallOptimizationResult::OptimizedToJump;
1835     }
1836     RELEASE_ASSERT(!didInsertChecks);
1837 
1838     if (!inliningBalance)
1839         return CallOptimizationResult::DidNothing;
1840 
1841     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1842 
1843     auto endSpecialCase = [&amp;] () {
1844         RELEASE_ASSERT(didInsertChecks);
1845         addToGraph(Phantom, callTargetNode);
1846         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
1847         inliningBalance--;
1848         if (continuationBlock) {
1849             m_currentIndex = nextIndex;
1850             m_exitOK = true;
1851             processSetLocalQueue();
1852             addJumpTo(continuationBlock);
1853         }
1854     };
1855 
1856     if (InternalFunction* function = callee.internalFunction()) {
1857         if (handleConstantInternalFunction(callTargetNode, result, function, registerOffset, argumentCountIncludingThis, specializationKind, prediction, insertChecksWithAccounting)) {
1858             endSpecialCase();
1859             return CallOptimizationResult::Inlined;
1860         }
1861         RELEASE_ASSERT(!didInsertChecks);
1862         return CallOptimizationResult::DidNothing;
1863     }
1864 
1865     Intrinsic intrinsic = callee.intrinsicFor(specializationKind);
1866     if (intrinsic != NoIntrinsic) {
1867         if (handleIntrinsicCall(callTargetNode, result, intrinsic, registerOffset, argumentCountIncludingThis, prediction, insertChecksWithAccounting)) {
1868             endSpecialCase();
1869             return CallOptimizationResult::Inlined;
1870         }
1871         RELEASE_ASSERT(!didInsertChecks);
1872         // We might still try to inline the Intrinsic because it might be a builtin JS function.
1873     }
1874 
1875     if (Options::useDOMJIT()) {
1876         if (const DOMJIT::Signature* signature = callee.signatureFor(specializationKind)) {
1877             if (handleDOMJITCall(callTargetNode, result, signature, registerOffset, argumentCountIncludingThis, prediction, insertChecksWithAccounting)) {
1878                 endSpecialCase();
1879                 return CallOptimizationResult::Inlined;
1880             }
1881             RELEASE_ASSERT(!didInsertChecks);
1882         }
1883     }
1884 
1885     unsigned myInliningCost = inliningCost(callee, argumentCountIncludingThis, kind);
1886     if (myInliningCost &gt; inliningBalance)
1887         return CallOptimizationResult::DidNothing;
1888 
1889     auto insertCheck = [&amp;] (CodeBlock*) {
1890         if (needsToCheckCallee)
1891             emitFunctionChecks(callee, callTargetNode, thisArgument);
1892     };
1893     inlineCall(callTargetNode, result, callee, registerOffset, argumentCountIncludingThis, kind, continuationBlock, insertCheck);
1894     inliningBalance -= myInliningCost;
1895     return CallOptimizationResult::Inlined;
1896 }
1897 
1898 bool ByteCodeParser::handleVarargsInlining(Node* callTargetNode, VirtualRegister result,
1899     const CallLinkStatus&amp; callLinkStatus, int firstFreeReg, VirtualRegister thisArgument,
1900     VirtualRegister argumentsArgument, unsigned argumentsOffset,
1901     NodeType callOp, InlineCallFrame::Kind kind)
1902 {
1903     VERBOSE_LOG(&quot;Handling inlining (Varargs)...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1904     if (callLinkStatus.maxArgumentCountIncludingThis() &gt; Options::maximumVarargsForInlining()) {
1905         VERBOSE_LOG(&quot;Bailing inlining: too many arguments for varargs inlining.\n&quot;);
1906         return false;
1907     }
1908     if (callLinkStatus.couldTakeSlowPath() || callLinkStatus.size() != 1) {
1909         VERBOSE_LOG(&quot;Bailing inlining: polymorphic inlining is not yet supported for varargs.\n&quot;);
1910         return false;
1911     }
1912 
1913     CallVariant callVariant = callLinkStatus[0];
1914 
1915     unsigned mandatoryMinimum;
1916     if (FunctionExecutable* functionExecutable = callVariant.functionExecutable())
1917         mandatoryMinimum = functionExecutable-&gt;parameterCount();
1918     else
1919         mandatoryMinimum = 0;
1920 
1921     // includes &quot;this&quot;
1922     unsigned maxArgumentCountIncludingThis = std::max(callLinkStatus.maxArgumentCountIncludingThis(), mandatoryMinimum + 1);
1923 
1924     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1925     if (inliningCost(callVariant, maxArgumentCountIncludingThis, kind) &gt; getInliningBalance(callLinkStatus, specializationKind)) {
1926         VERBOSE_LOG(&quot;Bailing inlining: inlining cost too high.\n&quot;);
1927         return false;
1928     }
1929 
1930     int registerOffset = firstFreeReg;
1931     registerOffset -= maxArgumentCountIncludingThis;
1932     registerOffset -= CallFrame::headerSizeInRegisters;
1933     registerOffset = -WTF::roundUpToMultipleOf(stackAlignmentRegisters(), -registerOffset);
1934 
1935     auto insertChecks = [&amp;] (CodeBlock* codeBlock) {
1936         emitFunctionChecks(callVariant, callTargetNode, thisArgument);
1937 
1938         int remappedRegisterOffset =
1939         m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffset)).virtualRegister().offset();
1940 
1941         ensureLocals(VirtualRegister(remappedRegisterOffset).toLocal());
1942 
1943         int argumentStart = registerOffset + CallFrame::headerSizeInRegisters;
1944         int remappedArgumentStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(argumentStart)).virtualRegister().offset();
1945 
1946         LoadVarargsData* data = m_graph.m_loadVarargsData.add();
1947         data-&gt;start = VirtualRegister(remappedArgumentStart + 1);
1948         data-&gt;count = VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCountIncludingThis);
1949         data-&gt;offset = argumentsOffset;
1950         data-&gt;limit = maxArgumentCountIncludingThis;
1951         data-&gt;mandatoryMinimum = mandatoryMinimum;
1952 
1953         if (callOp == TailCallForwardVarargs) {
1954             Node* argumentCount;
1955             if (!inlineCallFrame())
1956                 argumentCount = addToGraph(GetArgumentCountIncludingThis);
1957             else if (inlineCallFrame()-&gt;isVarargs())
1958                 argumentCount = getDirect(remapOperand(inlineCallFrame(), CallFrameSlot::argumentCountIncludingThis));
1959             else
1960                 argumentCount = addToGraph(JSConstant, OpInfo(m_graph.freeze(jsNumber(inlineCallFrame()-&gt;argumentCountIncludingThis))));
1961             addToGraph(ForwardVarargs, OpInfo(data), argumentCount);
1962         } else {
1963             Node* arguments = get(argumentsArgument);
1964             auto argCountTmp = m_inlineStackTop-&gt;remapOperand(Operand::tmp(OpCallVarargs::argCountIncludingThis));
1965             setDirect(argCountTmp, addToGraph(VarargsLength, OpInfo(data), arguments));
1966             progressToNextCheckpoint();
1967 
1968             addToGraph(LoadVarargs, OpInfo(data), getLocalOrTmp(argCountTmp), arguments);
1969         }
1970 
1971         // LoadVarargs may OSR exit. Hence, we need to keep alive callTargetNode, thisArgument
1972         // and argumentsArgument for the baseline JIT. However, we only need a Phantom for
1973         // callTargetNode because the other 2 are still in use and alive at this point.
1974         addToGraph(Phantom, callTargetNode);
1975 
1976         // In DFG IR before SSA, we cannot insert control flow between after the
1977         // LoadVarargs and the last SetArgumentDefinitely. This isn&#39;t a problem once we get to DFG
1978         // SSA. Fortunately, we also have other reasons for not inserting control flow
1979         // before SSA.
1980 
1981         VariableAccessData* countVariable = newVariableAccessData(data-&gt;count);
1982         // This is pretty lame, but it will force the count to be flushed as an int. This doesn&#39;t
1983         // matter very much, since our use of a SetArgumentDefinitely and Flushes for this local slot is
1984         // mostly just a formality.
1985         countVariable-&gt;predict(SpecInt32Only);
1986         countVariable-&gt;mergeIsProfitableToUnbox(true);
1987         Node* setArgumentCount = addToGraph(SetArgumentDefinitely, OpInfo(countVariable));
1988         m_currentBlock-&gt;variablesAtTail.setOperand(countVariable-&gt;operand(), setArgumentCount);
1989 
1990         set(VirtualRegister(argumentStart), get(thisArgument), ImmediateNakedSet);
1991         unsigned numSetArguments = 0;
1992         for (unsigned argument = 1; argument &lt; maxArgumentCountIncludingThis; ++argument) {
1993             VariableAccessData* variable = newVariableAccessData(VirtualRegister(remappedArgumentStart + argument));
1994             variable-&gt;mergeShouldNeverUnbox(true); // We currently have nowhere to put the type check on the LoadVarargs. LoadVarargs is effectful, so after it finishes, we cannot exit.
1995 
1996             // For a while it had been my intention to do things like this inside the
1997             // prediction injection phase. But in this case it&#39;s really best to do it here,
1998             // because it&#39;s here that we have access to the variable access datas for the
1999             // inlining we&#39;re about to do.
2000             //
2001             // Something else that&#39;s interesting here is that we&#39;d really love to get
2002             // predictions from the arguments loaded at the callsite, rather than the
2003             // arguments received inside the callee. But that probably won&#39;t matter for most
2004             // calls.
2005             if (codeBlock &amp;&amp; argument &lt; static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters())) {
2006                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
2007                 ValueProfile&amp; profile = codeBlock-&gt;valueProfileForArgument(argument);
2008                 variable-&gt;predict(profile.computeUpdatedPrediction(locker));
2009             }
2010 
2011             Node* setArgument = addToGraph(numSetArguments &gt;= mandatoryMinimum ? SetArgumentMaybe : SetArgumentDefinitely, OpInfo(variable));
2012             m_currentBlock-&gt;variablesAtTail.setOperand(variable-&gt;operand(), setArgument);
2013             ++numSetArguments;
2014         }
2015     };
2016 
2017     // Intrinsics and internal functions can only be inlined if we&#39;re not doing varargs. This is because
2018     // we currently don&#39;t have any way of getting profiling information for arguments to non-JS varargs
2019     // calls. The prediction propagator won&#39;t be of any help because LoadVarargs obscures the data flow,
2020     // and there are no callsite value profiles and native function won&#39;t have callee value profiles for
2021     // those arguments. Even worse, if the intrinsic decides to exit, it won&#39;t really have anywhere to
2022     // exit to: LoadVarargs is effectful and it&#39;s part of the op_call_varargs, so we can&#39;t exit without
2023     // calling LoadVarargs twice.
2024     inlineCall(callTargetNode, result, callVariant, registerOffset, maxArgumentCountIncludingThis, kind, nullptr, insertChecks);
2025 
2026 
2027     VERBOSE_LOG(&quot;Successful inlining (varargs, monomorphic).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2028     return true;
2029 }
2030 
2031 unsigned ByteCodeParser::getInliningBalance(const CallLinkStatus&amp; callLinkStatus, CodeSpecializationKind specializationKind)
2032 {
2033     unsigned inliningBalance = Options::maximumFunctionForCallInlineCandidateBytecodeCost();
2034     if (specializationKind == CodeForConstruct)
2035         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForConstructInlineCandidateBytecoodeCost());
2036     if (callLinkStatus.isClosureCall())
2037         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForClosureCallInlineCandidateBytecodeCost());
2038     return inliningBalance;
2039 }
2040 
2041 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleInlining(
2042     Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp; callLinkStatus,
2043     int registerOffset, VirtualRegister thisArgument,
2044     int argumentCountIncludingThis,
2045     BytecodeIndex nextIndex, NodeType callOp, InlineCallFrame::Kind kind, SpeculatedType prediction)
2046 {
2047     VERBOSE_LOG(&quot;Handling inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2048 
2049     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
2050     unsigned inliningBalance = getInliningBalance(callLinkStatus, specializationKind);
2051 
2052     // First check if we can avoid creating control flow. Our inliner does some CFG
2053     // simplification on the fly and this helps reduce compile times, but we can only leverage
2054     // this in cases where we don&#39;t need control flow diamonds to check the callee.
2055     if (!callLinkStatus.couldTakeSlowPath() &amp;&amp; callLinkStatus.size() == 1) {
2056         return handleCallVariant(
2057             callTargetNode, result, callLinkStatus[0], registerOffset, thisArgument,
2058             argumentCountIncludingThis, nextIndex, kind, prediction, inliningBalance, nullptr, true);
2059     }
2060 
2061     // We need to create some kind of switch over callee. For now we only do this if we believe that
2062     // we&#39;re in the top tier. We have two reasons for this: first, it provides us an opportunity to
2063     // do more detailed polyvariant/polymorphic profiling; and second, it reduces compile times in
2064     // the DFG. And by polyvariant profiling we mean polyvariant profiling of *this* call. Note that
2065     // we could improve that aspect of this by doing polymorphic inlining but having the profiling
2066     // also.
2067     if (!m_graph.m_plan.isFTL() || !Options::usePolymorphicCallInlining()) {
2068         VERBOSE_LOG(&quot;Bailing inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2069         return CallOptimizationResult::DidNothing;
2070     }
2071 
2072     // If the claim is that this did not originate from a stub, then we don&#39;t want to emit a switch
2073     // statement. Whenever the non-stub profiling says that it could take slow path, it really means that
2074     // it has no idea.
2075     if (!Options::usePolymorphicCallInliningForNonStubStatus()
2076         &amp;&amp; !callLinkStatus.isBasedOnStub()) {
2077         VERBOSE_LOG(&quot;Bailing inlining (non-stub polymorphism).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2078         return CallOptimizationResult::DidNothing;
2079     }
2080 
2081     bool allAreClosureCalls = true;
2082     bool allAreDirectCalls = true;
2083     for (unsigned i = callLinkStatus.size(); i--;) {
2084         if (callLinkStatus[i].isClosureCall())
2085             allAreDirectCalls = false;
2086         else
2087             allAreClosureCalls = false;
2088     }
2089 
2090     Node* thingToSwitchOn;
2091     if (allAreDirectCalls)
2092         thingToSwitchOn = callTargetNode;
2093     else if (allAreClosureCalls)
2094         thingToSwitchOn = addToGraph(GetExecutable, callTargetNode);
2095     else {
2096         // FIXME: We should be able to handle this case, but it&#39;s tricky and we don&#39;t know of cases
2097         // where it would be beneficial. It might be best to handle these cases as if all calls were
2098         // closure calls.
2099         // https://bugs.webkit.org/show_bug.cgi?id=136020
2100         VERBOSE_LOG(&quot;Bailing inlining (mix).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2101         return CallOptimizationResult::DidNothing;
2102     }
2103 
2104     VERBOSE_LOG(&quot;Doing hard inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2105 
2106     // This makes me wish that we were in SSA all the time. We need to pick a variable into which to
2107     // store the callee so that it will be accessible to all of the blocks we&#39;re about to create. We
2108     // get away with doing an immediate-set here because we wouldn&#39;t have performed any side effects
2109     // yet.
2110     VERBOSE_LOG(&quot;Register offset: &quot;, registerOffset);
2111     VirtualRegister calleeReg(registerOffset + CallFrameSlot::callee);
2112     calleeReg = m_inlineStackTop-&gt;remapOperand(calleeReg).virtualRegister();
2113     VERBOSE_LOG(&quot;Callee is going to be &quot;, calleeReg, &quot;\n&quot;);
2114     setDirect(calleeReg, callTargetNode, ImmediateSetWithFlush);
2115 
2116     // It&#39;s OK to exit right now, even though we set some locals. That&#39;s because those locals are not
2117     // user-visible.
2118     m_exitOK = true;
2119     addToGraph(ExitOK);
2120 
2121     SwitchData&amp; data = *m_graph.m_switchData.add();
2122     data.kind = SwitchCell;
2123     addToGraph(Switch, OpInfo(&amp;data), thingToSwitchOn);
2124     m_currentBlock-&gt;didLink();
2125 
2126     BasicBlock* continuationBlock = allocateUntargetableBlock();
2127     VERBOSE_LOG(&quot;Adding untargetable block &quot;, RawPointer(continuationBlock), &quot; (continuation)\n&quot;);
2128 
2129     // We may force this true if we give up on inlining any of the edges.
2130     bool couldTakeSlowPath = callLinkStatus.couldTakeSlowPath();
2131 
2132     VERBOSE_LOG(&quot;About to loop over functions at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2133 
2134     BytecodeIndex oldIndex = m_currentIndex;
2135     for (unsigned i = 0; i &lt; callLinkStatus.size(); ++i) {
2136         m_currentIndex = oldIndex;
2137         BasicBlock* calleeEntryBlock = allocateUntargetableBlock();
2138         m_currentBlock = calleeEntryBlock;
2139         prepareToParseBlock();
2140 
2141         // At the top of each switch case, we can exit.
2142         m_exitOK = true;
2143 
2144         Node* myCallTargetNode = getDirect(calleeReg);
2145 
2146         auto inliningResult = handleCallVariant(
2147             myCallTargetNode, result, callLinkStatus[i], registerOffset,
2148             thisArgument, argumentCountIncludingThis, nextIndex, kind, prediction,
2149             inliningBalance, continuationBlock, false);
2150 
2151         if (inliningResult == CallOptimizationResult::DidNothing) {
2152             // That failed so we let the block die. Nothing interesting should have been added to
2153             // the block. We also give up on inlining any of the (less frequent) callees.
2154             ASSERT(m_graph.m_blocks.last() == m_currentBlock);
2155             m_graph.killBlockAndItsContents(m_currentBlock);
2156             m_graph.m_blocks.removeLast();
2157             VERBOSE_LOG(&quot;Inlining of a poly call failed, we will have to go through a slow path\n&quot;);
2158 
2159             // The fact that inlining failed means we need a slow path.
2160             couldTakeSlowPath = true;
2161             break;
2162         }
2163 
2164         JSCell* thingToCaseOn;
2165         if (allAreDirectCalls)
2166             thingToCaseOn = callLinkStatus[i].nonExecutableCallee();
2167         else {
2168             ASSERT(allAreClosureCalls);
2169             thingToCaseOn = callLinkStatus[i].executable();
2170         }
2171         data.cases.append(SwitchCase(m_graph.freeze(thingToCaseOn), calleeEntryBlock));
2172         VERBOSE_LOG(&quot;Finished optimizing &quot;, callLinkStatus[i], &quot; at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2173     }
2174 
2175     // Slow path block
2176     m_currentBlock = allocateUntargetableBlock();
2177     m_currentIndex = oldIndex;
2178     m_exitOK = true;
2179     data.fallThrough = BranchTarget(m_currentBlock);
2180     prepareToParseBlock();
2181     Node* myCallTargetNode = getDirect(calleeReg);
2182     if (couldTakeSlowPath) {
2183         addCall(
2184             result, callOp, nullptr, myCallTargetNode, argumentCountIncludingThis,
2185             registerOffset, prediction);
2186         VERBOSE_LOG(&quot;We added a call in the slow path\n&quot;);
2187     } else {
2188         addToGraph(CheckBadCell);
2189         addToGraph(Phantom, myCallTargetNode);
2190         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
2191 
2192         if (result.isValid())
2193             set(result, addToGraph(BottomValue));
2194         VERBOSE_LOG(&quot;couldTakeSlowPath was false\n&quot;);
2195     }
2196 
2197     m_currentIndex = nextIndex;
2198     m_exitOK = true; // Origin changed, so it&#39;s fine to exit again.
2199     processSetLocalQueue();
2200 
2201     if (Node* terminal = m_currentBlock-&gt;terminal())
2202         ASSERT_UNUSED(terminal, terminal-&gt;op() == TailCall || terminal-&gt;op() == TailCallVarargs || terminal-&gt;op() == TailCallForwardVarargs);
2203     else {
2204         addJumpTo(continuationBlock);
2205     }
2206 
2207     prepareToParseBlock();
2208 
2209     m_currentIndex = oldIndex;
2210     m_currentBlock = continuationBlock;
2211     m_exitOK = true;
2212 
2213     VERBOSE_LOG(&quot;Done inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2214     return CallOptimizationResult::Inlined;
2215 }
2216 
2217 template&lt;typename ChecksFunctor&gt;
2218 bool ByteCodeParser::handleMinMax(VirtualRegister result, NodeType op, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; insertChecks)
2219 {
2220     ASSERT(op == ArithMin || op == ArithMax);
2221 
2222     if (argumentCountIncludingThis == 1) {
2223         insertChecks();
2224         double limit = op == ArithMax ? -std::numeric_limits&lt;double&gt;::infinity() : +std::numeric_limits&lt;double&gt;::infinity();
2225         set(result, addToGraph(JSConstant, OpInfo(m_graph.freeze(jsDoubleNumber(limit)))));
2226         return true;
2227     }
2228 
2229     if (argumentCountIncludingThis == 2) {
2230         insertChecks();
2231         Node* resultNode = get(VirtualRegister(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
2232         addToGraph(Phantom, Edge(resultNode, NumberUse));
2233         set(result, resultNode);
2234         return true;
2235     }
2236 
2237     if (argumentCountIncludingThis == 3) {
2238         insertChecks();
2239         set(result, addToGraph(op, get(virtualRegisterForArgumentIncludingThis(1, registerOffset)), get(virtualRegisterForArgumentIncludingThis(2, registerOffset))));
2240         return true;
2241     }
2242 
2243     // Don&#39;t handle &gt;=3 arguments for now.
2244     return false;
2245 }
2246 
2247 template&lt;typename ChecksFunctor&gt;
2248 bool ByteCodeParser::handleIntrinsicCall(Node* callee, VirtualRegister result, Intrinsic intrinsic, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
2249 {
2250     VERBOSE_LOG(&quot;       The intrinsic is &quot;, intrinsic, &quot;\n&quot;);
2251 
2252     if (!isOpcodeShape&lt;OpCallShape&gt;(m_currentInstruction))
2253         return false;
2254 
2255     // It so happens that the code below doesn&#39;t handle the invalid result case. We could fix that, but
2256     // it would only benefit intrinsics called as setters, like if you do:
2257     //
2258     //     o.__defineSetter__(&quot;foo&quot;, Math.pow)
2259     //
2260     // Which is extremely amusing, but probably not worth optimizing.
2261     if (!result.isValid())
2262         return false;
2263 
2264     bool didSetResult = false;
2265     auto setResult = [&amp;] (Node* node) {
2266         RELEASE_ASSERT(!didSetResult);
2267         set(result, node);
2268         didSetResult = true;
2269     };
2270 
2271     auto inlineIntrinsic = [&amp;] {
2272         switch (intrinsic) {
2273 
2274         // Intrinsic Functions:
2275 
2276         case AbsIntrinsic: {
2277             if (argumentCountIncludingThis == 1) { // Math.abs()
2278                 insertChecks();
2279                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2280                 return true;
2281             }
2282 
2283             if (!MacroAssembler::supportsFloatingPointAbs())
2284                 return false;
2285 
2286             insertChecks();
2287             Node* node = addToGraph(ArithAbs, get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
2288             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
2289                 node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
2290             setResult(node);
2291             return true;
2292         }
2293 
2294         case MinIntrinsic:
2295         case MaxIntrinsic:
2296             if (handleMinMax(result, intrinsic == MinIntrinsic ? ArithMin : ArithMax, registerOffset, argumentCountIncludingThis, insertChecks)) {
2297                 didSetResult = true;
2298                 return true;
2299             }
2300             return false;
2301 
2302 #define DFG_ARITH_UNARY(capitalizedName, lowerName) \
2303         case capitalizedName##Intrinsic:
2304         FOR_EACH_DFG_ARITH_UNARY_OP(DFG_ARITH_UNARY)
2305 #undef DFG_ARITH_UNARY
2306         {
2307             if (argumentCountIncludingThis == 1) {
2308                 insertChecks();
2309                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2310                 return true;
2311             }
2312             Arith::UnaryType type = Arith::UnaryType::Sin;
2313             switch (intrinsic) {
2314 #define DFG_ARITH_UNARY(capitalizedName, lowerName) \
2315             case capitalizedName##Intrinsic: \
2316                 type = Arith::UnaryType::capitalizedName; \
2317                 break;
2318         FOR_EACH_DFG_ARITH_UNARY_OP(DFG_ARITH_UNARY)
2319 #undef DFG_ARITH_UNARY
2320             default:
2321                 RELEASE_ASSERT_NOT_REACHED();
2322             }
2323             insertChecks();
2324             setResult(addToGraph(ArithUnary, OpInfo(static_cast&lt;std::underlying_type&lt;Arith::UnaryType&gt;::type&gt;(type)), get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
2325             return true;
2326         }
2327 
2328         case FRoundIntrinsic:
2329         case SqrtIntrinsic: {
2330             if (argumentCountIncludingThis == 1) {
2331                 insertChecks();
2332                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2333                 return true;
2334             }
2335 
2336             NodeType nodeType = Unreachable;
2337             switch (intrinsic) {
2338             case FRoundIntrinsic:
2339                 nodeType = ArithFRound;
2340                 break;
2341             case SqrtIntrinsic:
2342                 nodeType = ArithSqrt;
2343                 break;
2344             default:
2345                 RELEASE_ASSERT_NOT_REACHED();
2346             }
2347             insertChecks();
2348             setResult(addToGraph(nodeType, get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
2349             return true;
2350         }
2351 
2352         case PowIntrinsic: {
2353             if (argumentCountIncludingThis &lt; 3) {
2354                 // Math.pow() and Math.pow(x) return NaN.
2355                 insertChecks();
2356                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2357                 return true;
2358             }
2359             insertChecks();
2360             VirtualRegister xOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
2361             VirtualRegister yOperand = virtualRegisterForArgumentIncludingThis(2, registerOffset);
2362             setResult(addToGraph(ArithPow, get(xOperand), get(yOperand)));
2363             return true;
2364         }
2365 
2366         case TypedArrayEntriesIntrinsic:
2367         case TypedArrayKeysIntrinsic:
2368         case TypedArrayValuesIntrinsic: {
2369             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType)
2370                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2371                 return false;
2372 
2373             ArrayMode mode = getArrayMode(Array::Read);
2374             if (!mode.isSomeTypedArrayView())
2375                 return false;
2376 
2377             addToGraph(CheckArray, OpInfo(mode.asWord()), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)));
2378             addToGraph(CheckNeutered, get(virtualRegisterForArgumentIncludingThis(0, registerOffset)));
2379             FALLTHROUGH;
2380         }
2381 
2382         case ArrayEntriesIntrinsic:
2383         case ArrayKeysIntrinsic:
2384         case ArrayValuesIntrinsic: {
2385             insertChecks();
2386 
2387             IterationKind kind;
2388             switch (intrinsic) {
2389             case ArrayValuesIntrinsic:
2390             case TypedArrayValuesIntrinsic:
2391                 kind = IterationKind::Values;
2392                 break;
2393             case ArrayKeysIntrinsic:
2394             case TypedArrayKeysIntrinsic:
2395                 kind = IterationKind::Keys;
2396                 break;
2397             case ArrayEntriesIntrinsic:
2398             case TypedArrayEntriesIntrinsic:
2399                 kind = IterationKind::Entries;
2400                 break;
2401             default:
2402                 RELEASE_ASSERT_NOT_REACHED();
2403                 break;
2404             }
2405 
2406             // Add the constant before exit becomes invalid because we may want to insert (redundant) checks on it in Fixup.
2407             Node* kindNode = jsConstant(jsNumber(static_cast&lt;uint32_t&gt;(kind)));
2408 
2409             // We don&#39;t have an existing error string.
2410             unsigned errorStringIndex = UINT32_MAX;
2411             Node* object = addToGraph(ToObject, OpInfo(errorStringIndex), OpInfo(SpecNone), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)));
2412 
2413             JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2414             Node* iterator = addToGraph(NewArrayIterator, OpInfo(m_graph.registerStructure(globalObject-&gt;arrayIteratorStructure())));
2415 
2416             addToGraph(PutInternalField, OpInfo(static_cast&lt;uint32_t&gt;(JSArrayIterator::Field::IteratedObject)), iterator, object);
2417             addToGraph(PutInternalField, OpInfo(static_cast&lt;uint32_t&gt;(JSArrayIterator::Field::Kind)), iterator, kindNode);
2418 
2419             setResult(iterator);
2420             return true;
2421         }
2422 
2423         case ArrayPushIntrinsic: {
2424             if (static_cast&lt;unsigned&gt;(argumentCountIncludingThis) &gt;= MIN_SPARSE_ARRAY_INDEX)
2425                 return false;
2426 
2427             ArrayMode arrayMode = getArrayMode(Array::Write);
2428             if (!arrayMode.isJSArray())
2429                 return false;
2430             switch (arrayMode.type()) {
2431             case Array::Int32:
2432             case Array::Double:
2433             case Array::Contiguous:
2434             case Array::ArrayStorage: {
2435                 insertChecks();
2436 
2437                 addVarArgChild(nullptr); // For storage.
2438                 for (int i = 0; i &lt; argumentCountIncludingThis; ++i)
2439                     addVarArgChild(get(virtualRegisterForArgumentIncludingThis(i, registerOffset)));
2440                 Node* arrayPush = addToGraph(Node::VarArg, ArrayPush, OpInfo(arrayMode.asWord()), OpInfo(prediction));
2441                 setResult(arrayPush);
2442                 return true;
2443             }
2444 
2445             default:
2446                 return false;
2447             }
2448         }
2449 
2450         case ArraySliceIntrinsic: {
2451             if (argumentCountIncludingThis &lt; 1)
2452                 return false;
2453 
2454             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadConstantCache)
2455                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache))
2456                 return false;
2457 
2458             ArrayMode arrayMode = getArrayMode(Array::Read);
2459             if (!arrayMode.isJSArray())
2460                 return false;
2461 
2462             if (!arrayMode.isJSArrayWithOriginalStructure())
2463                 return false;
2464 
2465             switch (arrayMode.type()) {
2466             case Array::Double:
2467             case Array::Int32:
2468             case Array::Contiguous: {
2469                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2470 
2471                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2472                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2473 
2474                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2475                 // https://bugs.webkit.org/show_bug.cgi?id=173171
2476                 if (globalObject-&gt;arraySpeciesWatchpointSet().state() == IsWatched
2477                     &amp;&amp; globalObject-&gt;havingABadTimeWatchpoint()-&gt;isStillValid()
2478                     &amp;&amp; arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2479                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2480                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2481 
2482                     m_graph.watchpoints().addLazily(globalObject-&gt;arraySpeciesWatchpointSet());
2483                     m_graph.watchpoints().addLazily(globalObject-&gt;havingABadTimeWatchpoint());
2484                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2485                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2486 
2487                     insertChecks();
2488 
2489                     Node* array = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
2490                     // We do a few things here to prove that we aren&#39;t skipping doing side-effects in an observable way:
2491                     // 1. We ensure that the &quot;constructor&quot; property hasn&#39;t been changed (because the observable
2492                     // effects of slice require that we perform a Get(array, &quot;constructor&quot;) and we can skip
2493                     // that if we&#39;re an original array structure. (We can relax this in the future by using
2494                     // TryGetById and CheckCell).
2495                     //
2496                     // 2. We check that the array we&#39;re calling slice on has the same global object as the lexical
2497                     // global object that this code is running in. This requirement is necessary because we setup the
2498                     // watchpoints above on the lexical global object. This means that code that calls slice on
2499                     // arrays produced by other global objects won&#39;t get this optimization. We could relax this
2500                     // requirement in the future by checking that the watchpoint hasn&#39;t fired at runtime in the code
2501                     // we generate instead of registering it as a watchpoint that would invalidate the compilation.
2502                     //
2503                     // 3. By proving we&#39;re an original array structure, we guarantee that the incoming array
2504                     // isn&#39;t a subclass of Array.
2505 
2506                     StructureSet structureSet;
2507                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithInt32));
2508                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithContiguous));
2509                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithDouble));
2510                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithInt32));
2511                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithContiguous));
2512                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithDouble));
2513                     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structureSet)), array);
2514 
2515                     addVarArgChild(array);
2516                     if (argumentCountIncludingThis &gt;= 2)
2517                         addVarArgChild(get(virtualRegisterForArgumentIncludingThis(1, registerOffset))); // Start index.
2518                     if (argumentCountIncludingThis &gt;= 3)
2519                         addVarArgChild(get(virtualRegisterForArgumentIncludingThis(2, registerOffset))); // End index.
2520                     addVarArgChild(addToGraph(GetButterfly, array));
2521 
2522                     Node* arraySlice = addToGraph(Node::VarArg, ArraySlice, OpInfo(), OpInfo());
2523                     setResult(arraySlice);
2524                     return true;
2525                 }
2526 
2527                 return false;
2528             }
2529             default:
2530                 return false;
2531             }
2532 
2533             RELEASE_ASSERT_NOT_REACHED();
2534             return false;
2535         }
2536 
2537         case ArrayIndexOfIntrinsic: {
2538             if (argumentCountIncludingThis &lt; 2)
2539                 return false;
2540 
2541             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType)
2542                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadConstantCache)
2543                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
2544                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2545                 return false;
2546 
2547             ArrayMode arrayMode = getArrayMode(Array::Read);
2548             if (!arrayMode.isJSArray())
2549                 return false;
2550 
2551             if (!arrayMode.isJSArrayWithOriginalStructure())
2552                 return false;
2553 
2554             // We do not want to convert arrays into one type just to perform indexOf.
2555             if (arrayMode.doesConversion())
2556                 return false;
2557 
2558             switch (arrayMode.type()) {
2559             case Array::Double:
2560             case Array::Int32:
2561             case Array::Contiguous: {
2562                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2563 
2564                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2565                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2566 
2567                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2568                 // https://bugs.webkit.org/show_bug.cgi?id=173171
2569                 if (arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2570                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2571                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2572 
2573                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2574                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2575 
2576                     insertChecks();
2577 
2578                     Node* array = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
2579                     addVarArgChild(array);
2580                     addVarArgChild(get(virtualRegisterForArgumentIncludingThis(1, registerOffset))); // Search element.
2581                     if (argumentCountIncludingThis &gt;= 3)
2582                         addVarArgChild(get(virtualRegisterForArgumentIncludingThis(2, registerOffset))); // Start index.
2583                     addVarArgChild(nullptr);
2584 
2585                     Node* node = addToGraph(Node::VarArg, ArrayIndexOf, OpInfo(arrayMode.asWord()), OpInfo());
2586                     setResult(node);
2587                     return true;
2588                 }
2589 
2590                 return false;
2591             }
2592             default:
2593                 return false;
2594             }
2595 
2596             RELEASE_ASSERT_NOT_REACHED();
2597             return false;
2598 
2599         }
2600 
2601         case ArrayPopIntrinsic: {
2602             ArrayMode arrayMode = getArrayMode(Array::Write);
2603             if (!arrayMode.isJSArray())
2604                 return false;
2605             switch (arrayMode.type()) {
2606             case Array::Int32:
2607             case Array::Double:
2608             case Array::Contiguous:
2609             case Array::ArrayStorage: {
2610                 insertChecks();
2611                 Node* arrayPop = addToGraph(ArrayPop, OpInfo(arrayMode.asWord()), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)));
2612                 setResult(arrayPop);
2613                 return true;
2614             }
2615 
2616             default:
2617                 return false;
2618             }
2619         }
2620 
2621         case AtomicsAddIntrinsic:
2622         case AtomicsAndIntrinsic:
2623         case AtomicsCompareExchangeIntrinsic:
2624         case AtomicsExchangeIntrinsic:
2625         case AtomicsIsLockFreeIntrinsic:
2626         case AtomicsLoadIntrinsic:
2627         case AtomicsOrIntrinsic:
2628         case AtomicsStoreIntrinsic:
2629         case AtomicsSubIntrinsic:
2630         case AtomicsXorIntrinsic: {
2631             if (!is64Bit())
2632                 return false;
2633 
2634             NodeType op = LastNodeType;
2635             Array::Action action = Array::Write;
2636             unsigned numArgs = 0; // Number of actual args; we add one for the backing store pointer.
2637             switch (intrinsic) {
2638             case AtomicsAddIntrinsic:
2639                 op = AtomicsAdd;
2640                 numArgs = 3;
2641                 break;
2642             case AtomicsAndIntrinsic:
2643                 op = AtomicsAnd;
2644                 numArgs = 3;
2645                 break;
2646             case AtomicsCompareExchangeIntrinsic:
2647                 op = AtomicsCompareExchange;
2648                 numArgs = 4;
2649                 break;
2650             case AtomicsExchangeIntrinsic:
2651                 op = AtomicsExchange;
2652                 numArgs = 3;
2653                 break;
2654             case AtomicsIsLockFreeIntrinsic:
2655                 // This gets no backing store, but we need no special logic for this since this also does
2656                 // not need varargs.
2657                 op = AtomicsIsLockFree;
2658                 numArgs = 1;
2659                 break;
2660             case AtomicsLoadIntrinsic:
2661                 op = AtomicsLoad;
2662                 numArgs = 2;
2663                 action = Array::Read;
2664                 break;
2665             case AtomicsOrIntrinsic:
2666                 op = AtomicsOr;
2667                 numArgs = 3;
2668                 break;
2669             case AtomicsStoreIntrinsic:
2670                 op = AtomicsStore;
2671                 numArgs = 3;
2672                 break;
2673             case AtomicsSubIntrinsic:
2674                 op = AtomicsSub;
2675                 numArgs = 3;
2676                 break;
2677             case AtomicsXorIntrinsic:
2678                 op = AtomicsXor;
2679                 numArgs = 3;
2680                 break;
2681             default:
2682                 RELEASE_ASSERT_NOT_REACHED();
2683                 break;
2684             }
2685 
2686             if (static_cast&lt;unsigned&gt;(argumentCountIncludingThis) &lt; 1 + numArgs)
2687                 return false;
2688 
2689             insertChecks();
2690 
2691             Vector&lt;Node*, 3&gt; args;
2692             for (unsigned i = 0; i &lt; numArgs; ++i)
2693                 args.append(get(virtualRegisterForArgumentIncludingThis(1 + i, registerOffset)));
2694 
2695             Node* resultNode;
2696             if (numArgs + 1 &lt;= 3) {
2697                 while (args.size() &lt; 3)
2698                     args.append(nullptr);
2699                 resultNode = addToGraph(op, OpInfo(ArrayMode(Array::SelectUsingPredictions, action).asWord()), OpInfo(prediction), args[0], args[1], args[2]);
2700             } else {
2701                 for (Node* node : args)
2702                     addVarArgChild(node);
2703                 addVarArgChild(nullptr);
2704                 resultNode = addToGraph(Node::VarArg, op, OpInfo(ArrayMode(Array::SelectUsingPredictions, action).asWord()), OpInfo(prediction));
2705             }
2706 
2707             setResult(resultNode);
2708             return true;
2709         }
2710 
2711         case ParseIntIntrinsic: {
2712             if (argumentCountIncludingThis &lt; 2)
2713                 return false;
2714 
2715             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell) || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2716                 return false;
2717 
2718             insertChecks();
2719             VirtualRegister valueOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
2720             Node* parseInt;
2721             if (argumentCountIncludingThis == 2)
2722                 parseInt = addToGraph(ParseInt, OpInfo(), OpInfo(prediction), get(valueOperand));
2723             else {
2724                 ASSERT(argumentCountIncludingThis &gt; 2);
2725                 VirtualRegister radixOperand = virtualRegisterForArgumentIncludingThis(2, registerOffset);
2726                 parseInt = addToGraph(ParseInt, OpInfo(), OpInfo(prediction), get(valueOperand), get(radixOperand));
2727             }
2728             setResult(parseInt);
2729             return true;
2730         }
2731 
2732         case CharCodeAtIntrinsic: {
2733             if (argumentCountIncludingThis &lt; 2)
2734                 return false;
2735 
2736             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Uncountable) || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2737                 return false;
2738 
2739             insertChecks();
2740             VirtualRegister thisOperand = virtualRegisterForArgumentIncludingThis(0, registerOffset);
2741             VirtualRegister indexOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
2742             Node* charCode = addToGraph(StringCharCodeAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2743 
2744             setResult(charCode);
2745             return true;
2746         }
2747 
2748         case StringPrototypeCodePointAtIntrinsic: {
2749             if (!is64Bit())
2750                 return false;
2751 
2752             if (argumentCountIncludingThis &lt; 2)
2753                 return false;
2754 
2755             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Uncountable) || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2756                 return false;
2757 
2758             insertChecks();
2759             VirtualRegister thisOperand = virtualRegisterForArgumentIncludingThis(0, registerOffset);
2760             VirtualRegister indexOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
2761             Node* result = addToGraph(StringCodePointAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2762 
2763             setResult(result);
2764             return true;
2765         }
2766 
2767         case CharAtIntrinsic: {
2768             if (argumentCountIncludingThis &lt; 2)
2769                 return false;
2770 
2771             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2772                 return false;
2773 
2774             // FIXME: String#charAt returns empty string when index is out-of-bounds, and this does not break the AI&#39;s claim.
2775             // Only FTL supports out-of-bounds version now. We should support out-of-bounds version even in DFG.
2776             // https://bugs.webkit.org/show_bug.cgi?id=201678
2777 
2778             insertChecks();
2779             VirtualRegister thisOperand = virtualRegisterForArgumentIncludingThis(0, registerOffset);
2780             VirtualRegister indexOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
2781             Node* charCode = addToGraph(StringCharAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2782 
2783             setResult(charCode);
2784             return true;
2785         }
2786         case Clz32Intrinsic: {
2787             insertChecks();
2788             if (argumentCountIncludingThis == 1)
2789                 setResult(addToGraph(JSConstant, OpInfo(m_graph.freeze(jsNumber(32)))));
2790             else {
2791                 Node* operand = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
2792                 setResult(addToGraph(ArithClz32, operand));
2793             }
2794             return true;
2795         }
2796         case FromCharCodeIntrinsic: {
2797             if (argumentCountIncludingThis != 2)
2798                 return false;
2799 
2800             insertChecks();
2801             VirtualRegister indexOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
2802             Node* charCode = addToGraph(StringFromCharCode, get(indexOperand));
2803 
2804             setResult(charCode);
2805 
2806             return true;
2807         }
2808 
2809         case RegExpExecIntrinsic: {
2810             if (argumentCountIncludingThis &lt; 2)
2811                 return false;
2812 
2813             insertChecks();
2814             Node* regExpExec = addToGraph(RegExpExec, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)), get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
2815             setResult(regExpExec);
2816 
2817             return true;
2818         }
2819 
2820         case RegExpTestIntrinsic:
2821         case RegExpTestFastIntrinsic: {
2822             if (argumentCountIncludingThis &lt; 2)
2823                 return false;
2824 
2825             if (intrinsic == RegExpTestIntrinsic) {
2826                 // Don&#39;t inline intrinsic if we exited due to one of the primordial RegExp checks failing.
2827                 if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
2828                     return false;
2829 
2830                 JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
2831                 Structure* regExpStructure = globalObject-&gt;regExpStructure();
2832                 m_graph.registerStructure(regExpStructure);
2833                 ASSERT(regExpStructure-&gt;storedPrototype().isObject());
2834                 ASSERT(regExpStructure-&gt;storedPrototype().asCell()-&gt;classInfo(*m_vm) == RegExpPrototype::info());
2835 
2836                 FrozenValue* regExpPrototypeObjectValue = m_graph.freeze(regExpStructure-&gt;storedPrototype());
2837                 Structure* regExpPrototypeStructure = regExpPrototypeObjectValue-&gt;structure();
2838 
2839                 auto isRegExpPropertySame = [&amp;] (JSValue primordialProperty, UniquedStringImpl* propertyUID) {
2840                     JSValue currentProperty;
2841                     if (!m_graph.getRegExpPrototypeProperty(regExpStructure-&gt;storedPrototypeObject(), regExpPrototypeStructure, propertyUID, currentProperty))
2842                         return false;
2843 
2844                     return currentProperty == primordialProperty;
2845                 };
2846 
2847                 // Check that RegExp.exec is still the primordial RegExp.prototype.exec
2848                 if (!isRegExpPropertySame(globalObject-&gt;regExpProtoExecFunction(), m_vm-&gt;propertyNames-&gt;exec.impl()))
2849                     return false;
2850 
2851                 // Check that regExpObject is actually a RegExp object.
2852                 Node* regExpObject = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
2853                 addToGraph(Check, Edge(regExpObject, RegExpObjectUse));
2854 
2855                 // Check that regExpObject&#39;s exec is actually the primodial RegExp.prototype.exec.
2856                 UniquedStringImpl* execPropertyID = m_vm-&gt;propertyNames-&gt;exec.impl();
2857                 unsigned execIndex = m_graph.identifiers().ensure(execPropertyID);
2858                 Node* actualProperty = addToGraph(TryGetById, OpInfo(execIndex), OpInfo(SpecFunction), Edge(regExpObject, CellUse));
2859                 FrozenValue* regExpPrototypeExec = m_graph.freeze(globalObject-&gt;regExpProtoExecFunction());
2860                 addToGraph(CheckCell, OpInfo(regExpPrototypeExec), Edge(actualProperty, CellUse));
2861             }
2862 
2863             insertChecks();
2864             Node* regExpObject = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
2865             Node* regExpExec = addToGraph(RegExpTest, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), regExpObject, get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
2866             setResult(regExpExec);
2867 
2868             return true;
2869         }
2870 
2871         case RegExpMatchFastIntrinsic: {
2872             RELEASE_ASSERT(argumentCountIncludingThis == 2);
2873 
2874             insertChecks();
2875             Node* regExpMatch = addToGraph(RegExpMatchFast, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)), get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
2876             setResult(regExpMatch);
2877             return true;
2878         }
2879 
2880         case ObjectCreateIntrinsic: {
2881             if (argumentCountIncludingThis != 2)
2882                 return false;
2883 
2884             insertChecks();
2885             setResult(addToGraph(ObjectCreate, get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
2886             return true;
2887         }
2888 
2889         case ObjectGetPrototypeOfIntrinsic: {
2890             if (argumentCountIncludingThis &lt; 2)
2891                 return false;
2892 
2893             insertChecks();
2894             setResult(addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
2895             return true;
2896         }
2897 
2898         case ObjectIsIntrinsic: {
2899             if (argumentCountIncludingThis &lt; 3)
2900                 return false;
2901 
2902             insertChecks();
2903             setResult(addToGraph(SameValue, get(virtualRegisterForArgumentIncludingThis(1, registerOffset)), get(virtualRegisterForArgumentIncludingThis(2, registerOffset))));
2904             return true;
2905         }
2906 
2907         case ObjectKeysIntrinsic: {
2908             if (argumentCountIncludingThis &lt; 2)
2909                 return false;
2910 
2911             insertChecks();
2912             setResult(addToGraph(ObjectKeys, get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
2913             return true;
2914         }
2915 
2916         case ReflectGetPrototypeOfIntrinsic: {
2917             if (argumentCountIncludingThis &lt; 2)
2918                 return false;
2919 
2920             insertChecks();
2921             setResult(addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), Edge(get(virtualRegisterForArgumentIncludingThis(1, registerOffset)), ObjectUse)));
2922             return true;
2923         }
2924 
2925         case IsTypedArrayViewIntrinsic: {
2926             ASSERT(argumentCountIncludingThis == 2);
2927 
2928             insertChecks();
2929             setResult(addToGraph(IsTypedArrayView, OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
2930             return true;
2931         }
2932 
2933         case StringPrototypeValueOfIntrinsic: {
2934             insertChecks();
2935             Node* value = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
2936             setResult(addToGraph(StringValueOf, value));
2937             return true;
2938         }
2939 
2940         case StringPrototypeReplaceIntrinsic: {
2941             if (argumentCountIncludingThis &lt; 3)
2942                 return false;
2943 
2944             // Don&#39;t inline intrinsic if we exited due to &quot;search&quot; not being a RegExp or String object.
2945             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2946                 return false;
2947 
2948             // Don&#39;t inline intrinsic if we exited due to one of the primordial RegExp checks failing.
2949             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
2950                 return false;
2951 
2952             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
2953             Structure* regExpStructure = globalObject-&gt;regExpStructure();
2954             m_graph.registerStructure(regExpStructure);
2955             ASSERT(regExpStructure-&gt;storedPrototype().isObject());
2956             ASSERT(regExpStructure-&gt;storedPrototype().asCell()-&gt;classInfo(*m_vm) == RegExpPrototype::info());
2957 
2958             FrozenValue* regExpPrototypeObjectValue = m_graph.freeze(regExpStructure-&gt;storedPrototype());
2959             Structure* regExpPrototypeStructure = regExpPrototypeObjectValue-&gt;structure();
2960 
2961             auto isRegExpPropertySame = [&amp;] (JSValue primordialProperty, UniquedStringImpl* propertyUID) {
2962                 JSValue currentProperty;
2963                 if (!m_graph.getRegExpPrototypeProperty(regExpStructure-&gt;storedPrototypeObject(), regExpPrototypeStructure, propertyUID, currentProperty))
2964                     return false;
2965 
2966                 return currentProperty == primordialProperty;
2967             };
2968 
2969             // Check that searchRegExp.exec is still the primordial RegExp.prototype.exec
2970             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoExecFunction(), m_vm-&gt;propertyNames-&gt;exec.impl()))
2971                 return false;
2972 
2973             // Check that searchRegExp.global is still the primordial RegExp.prototype.global
2974             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoGlobalGetter(), m_vm-&gt;propertyNames-&gt;global.impl()))
2975                 return false;
2976 
2977             // Check that searchRegExp.unicode is still the primordial RegExp.prototype.unicode
2978             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoUnicodeGetter(), m_vm-&gt;propertyNames-&gt;unicode.impl()))
2979                 return false;
2980 
2981             // Check that searchRegExp[Symbol.match] is still the primordial RegExp.prototype[Symbol.replace]
2982             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoSymbolReplaceFunction(), m_vm-&gt;propertyNames-&gt;replaceSymbol.impl()))
2983                 return false;
2984 
2985             insertChecks();
2986 
2987             Node* resultNode = addToGraph(StringReplace, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)), get(virtualRegisterForArgumentIncludingThis(1, registerOffset)), get(virtualRegisterForArgumentIncludingThis(2, registerOffset)));
2988             setResult(resultNode);
2989             return true;
2990         }
2991 
2992         case StringPrototypeReplaceRegExpIntrinsic: {
2993             if (argumentCountIncludingThis &lt; 3)
2994                 return false;
2995 
2996             insertChecks();
2997             Node* resultNode = addToGraph(StringReplaceRegExp, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)), get(virtualRegisterForArgumentIncludingThis(1, registerOffset)), get(virtualRegisterForArgumentIncludingThis(2, registerOffset)));
2998             setResult(resultNode);
2999             return true;
3000         }
3001 
3002         case RoundIntrinsic:
3003         case FloorIntrinsic:
3004         case CeilIntrinsic:
3005         case TruncIntrinsic: {
3006             if (argumentCountIncludingThis == 1) {
3007                 insertChecks();
3008                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
3009                 return true;
3010             }
3011             insertChecks();
3012             Node* operand = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3013             NodeType op;
3014             if (intrinsic == RoundIntrinsic)
3015                 op = ArithRound;
3016             else if (intrinsic == FloorIntrinsic)
3017                 op = ArithFloor;
3018             else if (intrinsic == CeilIntrinsic)
3019                 op = ArithCeil;
3020             else {
3021                 ASSERT(intrinsic == TruncIntrinsic);
3022                 op = ArithTrunc;
3023             }
3024             Node* roundNode = addToGraph(op, OpInfo(0), OpInfo(prediction), operand);
3025             setResult(roundNode);
3026             return true;
3027         }
3028         case IMulIntrinsic: {
3029             if (argumentCountIncludingThis &lt; 3)
3030                 return false;
3031             insertChecks();
3032             VirtualRegister leftOperand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
3033             VirtualRegister rightOperand = virtualRegisterForArgumentIncludingThis(2, registerOffset);
3034             Node* left = get(leftOperand);
3035             Node* right = get(rightOperand);
3036             setResult(addToGraph(ArithIMul, left, right));
3037             return true;
3038         }
3039 
3040         case RandomIntrinsic: {
3041             insertChecks();
3042             setResult(addToGraph(ArithRandom));
3043             return true;
3044         }
3045 
3046         case DFGTrueIntrinsic: {
3047             insertChecks();
3048             setResult(jsConstant(jsBoolean(true)));
3049             return true;
3050         }
3051 
3052         case FTLTrueIntrinsic: {
3053             insertChecks();
3054             setResult(jsConstant(jsBoolean(m_graph.m_plan.isFTL())));
3055             return true;
3056         }
3057 
3058         case OSRExitIntrinsic: {
3059             insertChecks();
3060             addToGraph(ForceOSRExit);
3061             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3062             return true;
3063         }
3064 
3065         case IsFinalTierIntrinsic: {
3066             insertChecks();
3067             setResult(jsConstant(jsBoolean(Options::useFTLJIT() ? m_graph.m_plan.isFTL() : true)));
3068             return true;
3069         }
3070 
3071         case SetInt32HeapPredictionIntrinsic: {
3072             insertChecks();
3073             for (int i = 1; i &lt; argumentCountIncludingThis; ++i) {
3074                 Node* node = get(virtualRegisterForArgumentIncludingThis(i, registerOffset));
3075                 if (node-&gt;hasHeapPrediction())
3076                     node-&gt;setHeapPrediction(SpecInt32Only);
3077             }
3078             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3079             return true;
3080         }
3081 
3082         case CheckInt32Intrinsic: {
3083             insertChecks();
3084             for (int i = 1; i &lt; argumentCountIncludingThis; ++i) {
3085                 Node* node = get(virtualRegisterForArgumentIncludingThis(i, registerOffset));
3086                 addToGraph(Phantom, Edge(node, Int32Use));
3087             }
3088             setResult(jsConstant(jsBoolean(true)));
3089             return true;
3090         }
3091 
3092         case FiatInt52Intrinsic: {
3093             if (argumentCountIncludingThis &lt; 2)
3094                 return false;
3095             insertChecks();
3096             VirtualRegister operand = virtualRegisterForArgumentIncludingThis(1, registerOffset);
3097             if (enableInt52())
3098                 setResult(addToGraph(FiatInt52, get(operand)));
3099             else
3100                 setResult(get(operand));
3101             return true;
3102         }
3103 
3104         case JSMapGetIntrinsic: {
3105             if (argumentCountIncludingThis &lt; 2)
3106                 return false;
3107 
3108             insertChecks();
3109             Node* map = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3110             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3111             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
3112             Node* hash = addToGraph(MapHash, normalizedKey);
3113             Node* bucket = addToGraph(GetMapBucket, Edge(map, MapObjectUse), Edge(normalizedKey), Edge(hash));
3114             Node* resultNode = addToGraph(LoadValueFromMapBucket, OpInfo(BucketOwnerType::Map), OpInfo(prediction), bucket);
3115             setResult(resultNode);
3116             return true;
3117         }
3118 
3119         case JSSetHasIntrinsic:
3120         case JSMapHasIntrinsic: {
3121             if (argumentCountIncludingThis &lt; 2)
3122                 return false;
3123 
3124             insertChecks();
3125             Node* mapOrSet = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3126             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3127             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
3128             Node* hash = addToGraph(MapHash, normalizedKey);
3129             UseKind useKind = intrinsic == JSSetHasIntrinsic ? SetObjectUse : MapObjectUse;
3130             Node* bucket = addToGraph(GetMapBucket, OpInfo(0), Edge(mapOrSet, useKind), Edge(normalizedKey), Edge(hash));
3131             JSCell* sentinel = nullptr;
3132             if (intrinsic == JSMapHasIntrinsic)
3133                 sentinel = m_vm-&gt;sentinelMapBucket();
3134             else
3135                 sentinel = m_vm-&gt;sentinelSetBucket();
3136 
3137             FrozenValue* frozenPointer = m_graph.freeze(sentinel);
3138             Node* invertedResult = addToGraph(CompareEqPtr, OpInfo(frozenPointer), bucket);
3139             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3140             setResult(resultNode);
3141             return true;
3142         }
3143 
3144         case JSSetAddIntrinsic: {
3145             if (argumentCountIncludingThis &lt; 2)
3146                 return false;
3147 
3148             insertChecks();
3149             Node* base = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3150             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3151             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
3152             Node* hash = addToGraph(MapHash, normalizedKey);
3153             addToGraph(SetAdd, base, normalizedKey, hash);
3154             setResult(base);
3155             return true;
3156         }
3157 
3158         case JSMapSetIntrinsic: {
3159             if (argumentCountIncludingThis &lt; 3)
3160                 return false;
3161 
3162             insertChecks();
3163             Node* base = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3164             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3165             Node* value = get(virtualRegisterForArgumentIncludingThis(2, registerOffset));
3166 
3167             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
3168             Node* hash = addToGraph(MapHash, normalizedKey);
3169 
3170             addVarArgChild(base);
3171             addVarArgChild(normalizedKey);
3172             addVarArgChild(value);
3173             addVarArgChild(hash);
3174             addToGraph(Node::VarArg, MapSet, OpInfo(0), OpInfo(0));
3175             setResult(base);
3176             return true;
3177         }
3178 
3179         case JSSetBucketHeadIntrinsic:
3180         case JSMapBucketHeadIntrinsic: {
3181             ASSERT(argumentCountIncludingThis == 2);
3182 
3183             insertChecks();
3184             Node* map = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3185             UseKind useKind = intrinsic == JSSetBucketHeadIntrinsic ? SetObjectUse : MapObjectUse;
3186             Node* resultNode = addToGraph(GetMapBucketHead, Edge(map, useKind));
3187             setResult(resultNode);
3188             return true;
3189         }
3190 
3191         case JSSetBucketNextIntrinsic:
3192         case JSMapBucketNextIntrinsic: {
3193             ASSERT(argumentCountIncludingThis == 2);
3194 
3195             insertChecks();
3196             Node* bucket = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3197             BucketOwnerType type = intrinsic == JSSetBucketNextIntrinsic ? BucketOwnerType::Set : BucketOwnerType::Map;
3198             Node* resultNode = addToGraph(GetMapBucketNext, OpInfo(type), bucket);
3199             setResult(resultNode);
3200             return true;
3201         }
3202 
3203         case JSSetBucketKeyIntrinsic:
3204         case JSMapBucketKeyIntrinsic: {
3205             ASSERT(argumentCountIncludingThis == 2);
3206 
3207             insertChecks();
3208             Node* bucket = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3209             BucketOwnerType type = intrinsic == JSSetBucketKeyIntrinsic ? BucketOwnerType::Set : BucketOwnerType::Map;
3210             Node* resultNode = addToGraph(LoadKeyFromMapBucket, OpInfo(type), OpInfo(prediction), bucket);
3211             setResult(resultNode);
3212             return true;
3213         }
3214 
3215         case JSMapBucketValueIntrinsic: {
3216             ASSERT(argumentCountIncludingThis == 2);
3217 
3218             insertChecks();
3219             Node* bucket = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3220             Node* resultNode = addToGraph(LoadValueFromMapBucket, OpInfo(BucketOwnerType::Map), OpInfo(prediction), bucket);
3221             setResult(resultNode);
3222             return true;
3223         }
3224 
3225         case JSWeakMapGetIntrinsic: {
3226             if (argumentCountIncludingThis &lt; 2)
3227                 return false;
3228 
3229             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3230                 return false;
3231 
3232             insertChecks();
3233             Node* map = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3234             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3235             addToGraph(Check, Edge(key, ObjectUse));
3236             Node* hash = addToGraph(MapHash, key);
3237             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakMapObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3238             Node* resultNode = addToGraph(ExtractValueFromWeakMapGet, OpInfo(), OpInfo(prediction), holder);
3239 
3240             setResult(resultNode);
3241             return true;
3242         }
3243 
3244         case JSWeakMapHasIntrinsic: {
3245             if (argumentCountIncludingThis &lt; 2)
3246                 return false;
3247 
3248             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3249                 return false;
3250 
3251             insertChecks();
3252             Node* map = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3253             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3254             addToGraph(Check, Edge(key, ObjectUse));
3255             Node* hash = addToGraph(MapHash, key);
3256             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakMapObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3257             Node* invertedResult = addToGraph(IsEmpty, holder);
3258             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3259 
3260             setResult(resultNode);
3261             return true;
3262         }
3263 
3264         case JSWeakSetHasIntrinsic: {
3265             if (argumentCountIncludingThis &lt; 2)
3266                 return false;
3267 
3268             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3269                 return false;
3270 
3271             insertChecks();
3272             Node* map = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3273             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3274             addToGraph(Check, Edge(key, ObjectUse));
3275             Node* hash = addToGraph(MapHash, key);
3276             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakSetObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3277             Node* invertedResult = addToGraph(IsEmpty, holder);
3278             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3279 
3280             setResult(resultNode);
3281             return true;
3282         }
3283 
3284         case JSWeakSetAddIntrinsic: {
3285             if (argumentCountIncludingThis &lt; 2)
3286                 return false;
3287 
3288             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3289                 return false;
3290 
3291             insertChecks();
3292             Node* base = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3293             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3294             addToGraph(Check, Edge(key, ObjectUse));
3295             Node* hash = addToGraph(MapHash, key);
3296             addToGraph(WeakSetAdd, Edge(base, WeakSetObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3297             setResult(base);
3298             return true;
3299         }
3300 
3301         case JSWeakMapSetIntrinsic: {
3302             if (argumentCountIncludingThis &lt; 3)
3303                 return false;
3304 
3305             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3306                 return false;
3307 
3308             insertChecks();
3309             Node* base = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3310             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3311             Node* value = get(virtualRegisterForArgumentIncludingThis(2, registerOffset));
3312 
3313             addToGraph(Check, Edge(key, ObjectUse));
3314             Node* hash = addToGraph(MapHash, key);
3315 
3316             addVarArgChild(Edge(base, WeakMapObjectUse));
3317             addVarArgChild(Edge(key, ObjectUse));
3318             addVarArgChild(Edge(value));
3319             addVarArgChild(Edge(hash, Int32Use));
3320             addToGraph(Node::VarArg, WeakMapSet, OpInfo(0), OpInfo(0));
3321             setResult(base);
3322             return true;
3323         }
3324 
3325         case DatePrototypeGetTimeIntrinsic: {
3326             if (!is64Bit())
3327                 return false;
3328             insertChecks();
3329             Node* base = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3330             setResult(addToGraph(DateGetTime, OpInfo(intrinsic), OpInfo(), base));
3331             return true;
3332         }
3333 
3334         case DatePrototypeGetFullYearIntrinsic:
3335         case DatePrototypeGetUTCFullYearIntrinsic:
3336         case DatePrototypeGetMonthIntrinsic:
3337         case DatePrototypeGetUTCMonthIntrinsic:
3338         case DatePrototypeGetDateIntrinsic:
3339         case DatePrototypeGetUTCDateIntrinsic:
3340         case DatePrototypeGetDayIntrinsic:
3341         case DatePrototypeGetUTCDayIntrinsic:
3342         case DatePrototypeGetHoursIntrinsic:
3343         case DatePrototypeGetUTCHoursIntrinsic:
3344         case DatePrototypeGetMinutesIntrinsic:
3345         case DatePrototypeGetUTCMinutesIntrinsic:
3346         case DatePrototypeGetSecondsIntrinsic:
3347         case DatePrototypeGetUTCSecondsIntrinsic:
3348         case DatePrototypeGetMillisecondsIntrinsic:
3349         case DatePrototypeGetUTCMillisecondsIntrinsic:
3350         case DatePrototypeGetTimezoneOffsetIntrinsic:
3351         case DatePrototypeGetYearIntrinsic: {
3352             if (!is64Bit())
3353                 return false;
3354             insertChecks();
3355             Node* base = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3356             setResult(addToGraph(DateGetInt32OrNaN, OpInfo(intrinsic), OpInfo(prediction), base));
3357             return true;
3358         }
3359 
3360         case DataViewGetInt8:
3361         case DataViewGetUint8:
3362         case DataViewGetInt16:
3363         case DataViewGetUint16:
3364         case DataViewGetInt32:
3365         case DataViewGetUint32:
3366         case DataViewGetFloat32:
3367         case DataViewGetFloat64: {
3368             if (!is64Bit())
3369                 return false;
3370 
3371             // To inline data view accesses, we assume the architecture we&#39;re running on:
3372             // - Is little endian.
3373             // - Allows unaligned loads/stores without crashing.
3374 
3375             if (argumentCountIncludingThis &lt; 2)
3376                 return false;
3377             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3378                 return false;
3379 
3380             insertChecks();
3381 
3382             uint8_t byteSize;
3383             NodeType op = DataViewGetInt;
3384             bool isSigned = false;
3385             switch (intrinsic) {
3386             case DataViewGetInt8:
3387                 isSigned = true;
3388                 FALLTHROUGH;
3389             case DataViewGetUint8:
3390                 byteSize = 1;
3391                 break;
3392 
3393             case DataViewGetInt16:
3394                 isSigned = true;
3395                 FALLTHROUGH;
3396             case DataViewGetUint16:
3397                 byteSize = 2;
3398                 break;
3399 
3400             case DataViewGetInt32:
3401                 isSigned = true;
3402                 FALLTHROUGH;
3403             case DataViewGetUint32:
3404                 byteSize = 4;
3405                 break;
3406 
3407             case DataViewGetFloat32:
3408                 byteSize = 4;
3409                 op = DataViewGetFloat;
3410                 break;
3411             case DataViewGetFloat64:
3412                 byteSize = 8;
3413                 op = DataViewGetFloat;
3414                 break;
3415             default:
3416                 RELEASE_ASSERT_NOT_REACHED();
3417             }
3418 
3419             TriState isLittleEndian = MixedTriState;
3420             Node* littleEndianChild = nullptr;
3421             if (byteSize &gt; 1) {
3422                 if (argumentCountIncludingThis &lt; 3)
3423                     isLittleEndian = FalseTriState;
3424                 else {
3425                     littleEndianChild = get(virtualRegisterForArgumentIncludingThis(2, registerOffset));
3426                     if (littleEndianChild-&gt;hasConstant()) {
3427                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
3428                         if (constant) {
3429                             isLittleEndian = constant.pureToBoolean();
3430                             if (isLittleEndian != MixedTriState)
3431                                 littleEndianChild = nullptr;
3432                         }
3433                     } else
3434                         isLittleEndian = MixedTriState;
3435                 }
3436             }
3437 
3438             DataViewData data { };
3439             data.isLittleEndian = isLittleEndian;
3440             data.isSigned = isSigned;
3441             data.byteSize = byteSize;
3442 
3443             setResult(
3444                 addToGraph(op, OpInfo(data.asQuadWord), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(0, registerOffset)), get(virtualRegisterForArgumentIncludingThis(1, registerOffset)), littleEndianChild));
3445             return true;
3446         }
3447 
3448         case DataViewSetInt8:
3449         case DataViewSetUint8:
3450         case DataViewSetInt16:
3451         case DataViewSetUint16:
3452         case DataViewSetInt32:
3453         case DataViewSetUint32:
3454         case DataViewSetFloat32:
3455         case DataViewSetFloat64: {
3456             if (!is64Bit())
3457                 return false;
3458 
3459             if (argumentCountIncludingThis &lt; 3)
3460                 return false;
3461 
3462             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3463                 return false;
3464 
3465             insertChecks();
3466 
3467             uint8_t byteSize;
3468             bool isFloatingPoint = false;
3469             bool isSigned = false;
3470             switch (intrinsic) {
3471             case DataViewSetInt8:
3472                 isSigned = true;
3473                 FALLTHROUGH;
3474             case DataViewSetUint8:
3475                 byteSize = 1;
3476                 break;
3477 
3478             case DataViewSetInt16:
3479                 isSigned = true;
3480                 FALLTHROUGH;
3481             case DataViewSetUint16:
3482                 byteSize = 2;
3483                 break;
3484 
3485             case DataViewSetInt32:
3486                 isSigned = true;
3487                 FALLTHROUGH;
3488             case DataViewSetUint32:
3489                 byteSize = 4;
3490                 break;
3491 
3492             case DataViewSetFloat32:
3493                 isFloatingPoint = true;
3494                 byteSize = 4;
3495                 break;
3496             case DataViewSetFloat64:
3497                 isFloatingPoint = true;
3498                 byteSize = 8;
3499                 break;
3500             default:
3501                 RELEASE_ASSERT_NOT_REACHED();
3502             }
3503 
3504             TriState isLittleEndian = MixedTriState;
3505             Node* littleEndianChild = nullptr;
3506             if (byteSize &gt; 1) {
3507                 if (argumentCountIncludingThis &lt; 4)
3508                     isLittleEndian = FalseTriState;
3509                 else {
3510                     littleEndianChild = get(virtualRegisterForArgumentIncludingThis(3, registerOffset));
3511                     if (littleEndianChild-&gt;hasConstant()) {
3512                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
3513                         if (constant) {
3514                             isLittleEndian = constant.pureToBoolean();
3515                             if (isLittleEndian != MixedTriState)
3516                                 littleEndianChild = nullptr;
3517                         }
3518                     } else
3519                         isLittleEndian = MixedTriState;
3520                 }
3521             }
3522 
3523             DataViewData data { };
3524             data.isLittleEndian = isLittleEndian;
3525             data.isSigned = isSigned;
3526             data.byteSize = byteSize;
3527             data.isFloatingPoint = isFloatingPoint;
3528 
3529             addVarArgChild(get(virtualRegisterForArgumentIncludingThis(0, registerOffset)));
3530             addVarArgChild(get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
3531             addVarArgChild(get(virtualRegisterForArgumentIncludingThis(2, registerOffset)));
3532             addVarArgChild(littleEndianChild);
3533 
3534             addToGraph(Node::VarArg, DataViewSet, OpInfo(data.asQuadWord), OpInfo());
3535             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3536             return true;
3537         }
3538 
3539         case HasOwnPropertyIntrinsic: {
3540             if (argumentCountIncludingThis &lt; 2)
3541                 return false;
3542 
3543             // This can be racy, that&#39;s fine. We know that once we observe that this is created,
3544             // that it will never be destroyed until the VM is destroyed. It&#39;s unlikely that
3545             // we&#39;d ever get to the point where we inline this as an intrinsic without the
3546             // cache being created, however, it&#39;s possible if we always throw exceptions inside
3547             // hasOwnProperty.
3548             if (!m_vm-&gt;hasOwnPropertyCache())
3549                 return false;
3550 
3551             insertChecks();
3552             Node* object = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3553             Node* key = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3554             Node* resultNode = addToGraph(HasOwnProperty, object, key);
3555             setResult(resultNode);
3556             return true;
3557         }
3558 
3559         case StringPrototypeSliceIntrinsic: {
3560             if (argumentCountIncludingThis &lt; 2)
3561                 return false;
3562 
3563             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3564                 return false;
3565 
3566             insertChecks();
3567             Node* thisString = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3568             Node* start = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3569             Node* end = nullptr;
3570             if (argumentCountIncludingThis &gt; 2)
3571                 end = get(virtualRegisterForArgumentIncludingThis(2, registerOffset));
3572             Node* resultNode = addToGraph(StringSlice, thisString, start, end);
3573             setResult(resultNode);
3574             return true;
3575         }
3576 
3577         case StringPrototypeToLowerCaseIntrinsic: {
3578             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3579                 return false;
3580 
3581             insertChecks();
3582             Node* thisString = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3583             Node* resultNode = addToGraph(ToLowerCase, thisString);
3584             setResult(resultNode);
3585             return true;
3586         }
3587 
3588         case NumberPrototypeToStringIntrinsic: {
3589             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3590                 return false;
3591 
3592             insertChecks();
3593             Node* thisNumber = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3594             if (argumentCountIncludingThis == 1) {
3595                 Node* resultNode = addToGraph(ToString, thisNumber);
3596                 setResult(resultNode);
3597             } else {
3598                 Node* radix = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3599                 Node* resultNode = addToGraph(NumberToStringWithRadix, thisNumber, radix);
3600                 setResult(resultNode);
3601             }
3602             return true;
3603         }
3604 
3605         case NumberIsIntegerIntrinsic: {
3606             if (argumentCountIncludingThis &lt; 2)
3607                 return false;
3608 
3609             insertChecks();
3610             Node* input = get(virtualRegisterForArgumentIncludingThis(1, registerOffset));
3611             Node* resultNode = addToGraph(NumberIsInteger, input);
3612             setResult(resultNode);
3613             return true;
3614         }
3615 
3616         case CPUMfenceIntrinsic:
3617         case CPURdtscIntrinsic:
3618         case CPUCpuidIntrinsic:
3619         case CPUPauseIntrinsic: {
3620 #if CPU(X86_64)
3621             if (!m_graph.m_plan.isFTL())
3622                 return false;
3623             insertChecks();
3624             setResult(addToGraph(CPUIntrinsic, OpInfo(intrinsic), OpInfo()));
3625             return true;
3626 #else
3627             return false;
3628 #endif
3629         }
3630 
3631         default:
3632             return false;
3633         }
3634     };
3635 
3636     if (inlineIntrinsic()) {
3637         RELEASE_ASSERT(didSetResult);
3638         return true;
3639     }
3640 
3641     return false;
3642 }
3643 
3644 template&lt;typename ChecksFunctor&gt;
3645 bool ByteCodeParser::handleDOMJITCall(Node* callTarget, VirtualRegister result, const DOMJIT::Signature* signature, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
3646 {
3647     if (argumentCountIncludingThis != static_cast&lt;int&gt;(1 + signature-&gt;argumentCount))
3648         return false;
3649     if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3650         return false;
3651 
3652     // FIXME: Currently, we only support functions which arguments are up to 2.
3653     // Eventually, we should extend this. But possibly, 2 or 3 can cover typical use cases.
3654     // https://bugs.webkit.org/show_bug.cgi?id=164346
3655     ASSERT_WITH_MESSAGE(argumentCountIncludingThis &lt;= JSC_DOMJIT_SIGNATURE_MAX_ARGUMENTS_INCLUDING_THIS, &quot;Currently CallDOM does not support an arbitrary length arguments.&quot;);
3656 
3657     insertChecks();
3658     addCall(result, Call, signature, callTarget, argumentCountIncludingThis, registerOffset, prediction);
3659     return true;
3660 }
3661 
3662 
3663 template&lt;typename ChecksFunctor&gt;
3664 bool ByteCodeParser::handleIntrinsicGetter(VirtualRegister result, SpeculatedType prediction, const GetByIdVariant&amp; variant, Node* thisNode, const ChecksFunctor&amp; insertChecks)
3665 {
3666     switch (variant.intrinsic()) {
3667     case TypedArrayByteLengthIntrinsic: {
3668         insertChecks();
3669 
3670         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3671         Array::Type arrayType = toArrayType(type);
3672         size_t logSize = logElementSize(type);
3673 
3674         variant.structureSet().forEach([&amp;] (Structure* structure) {
3675             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3676             ASSERT(logSize == logElementSize(curType));
3677             arrayType = refineTypedArrayType(arrayType, curType);
3678             ASSERT(arrayType != Array::Generic);
3679         });
3680 
3681         Node* lengthNode = addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode);
3682 
3683         if (!logSize) {
3684             set(result, lengthNode);
3685             return true;
3686         }
3687 
3688         // We can use a BitLShift here because typed arrays will never have a byteLength
3689         // that overflows int32.
3690         Node* shiftNode = jsConstant(jsNumber(logSize));
3691         set(result, addToGraph(ArithBitLShift, lengthNode, shiftNode));
3692 
3693         return true;
3694     }
3695 
3696     case TypedArrayLengthIntrinsic: {
3697         insertChecks();
3698 
3699         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3700         Array::Type arrayType = toArrayType(type);
3701 
3702         variant.structureSet().forEach([&amp;] (Structure* structure) {
3703             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3704             arrayType = refineTypedArrayType(arrayType, curType);
3705             ASSERT(arrayType != Array::Generic);
3706         });
3707 
3708         set(result, addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3709 
3710         return true;
3711 
3712     }
3713 
3714     case TypedArrayByteOffsetIntrinsic: {
3715         insertChecks();
3716 
3717         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3718         Array::Type arrayType = toArrayType(type);
3719 
3720         variant.structureSet().forEach([&amp;] (Structure* structure) {
3721             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3722             arrayType = refineTypedArrayType(arrayType, curType);
3723             ASSERT(arrayType != Array::Generic);
3724         });
3725 
3726         set(result, addToGraph(GetTypedArrayByteOffset, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3727 
3728         return true;
3729     }
3730 
3731     case UnderscoreProtoIntrinsic: {
3732         insertChecks();
3733 
3734         bool canFold = !variant.structureSet().isEmpty();
3735         JSValue prototype;
3736         variant.structureSet().forEach([&amp;] (Structure* structure) {
3737             auto getPrototypeMethod = structure-&gt;classInfo()-&gt;methodTable.getPrototype;
3738             MethodTable::GetPrototypeFunctionPtr defaultGetPrototype = JSObject::getPrototype;
3739             if (getPrototypeMethod != defaultGetPrototype) {
3740                 canFold = false;
3741                 return;
3742             }
3743 
3744             if (structure-&gt;hasPolyProto()) {
3745                 canFold = false;
3746                 return;
3747             }
3748             if (!prototype)
3749                 prototype = structure-&gt;storedPrototype();
3750             else if (prototype != structure-&gt;storedPrototype())
3751                 canFold = false;
3752         });
3753 
3754         // OK, only one prototype is found. We perform constant folding here.
3755         // This information is important for super&#39;s constructor call to get new.target constant.
3756         if (prototype &amp;&amp; canFold) {
3757             set(result, weakJSConstant(prototype));
3758             return true;
3759         }
3760 
3761         set(result, addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), thisNode));
3762         return true;
3763     }
3764 
3765     default:
3766         return false;
3767     }
3768     RELEASE_ASSERT_NOT_REACHED();
3769 }
3770 
3771 static void blessCallDOMGetter(Node* node)
3772 {
3773     DOMJIT::CallDOMGetterSnippet* snippet = node-&gt;callDOMGetterData()-&gt;snippet;
3774     if (snippet &amp;&amp; !snippet-&gt;effect.mustGenerate())
3775         node-&gt;clearFlags(NodeMustGenerate);
3776 }
3777 
3778 bool ByteCodeParser::handleDOMJITGetter(VirtualRegister result, const GetByIdVariant&amp; variant, Node* thisNode, unsigned identifierNumber, SpeculatedType prediction)
3779 {
3780     if (!variant.domAttribute())
3781         return false;
3782 
3783     auto* domAttribute = variant.domAttribute();
3784 
3785     // We do not need to actually look up CustomGetterSetter here. Checking Structures or registering watchpoints are enough,
3786     // since replacement of CustomGetterSetter always incurs Structure transition.
3787     if (!check(variant.conditionSet()))
3788         return false;
3789     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.structureSet())), thisNode);
3790 
3791     // We do not need to emit CheckCell thingy here. When the custom accessor is replaced to different one, Structure transition occurs.
3792     addToGraph(CheckSubClass, OpInfo(domAttribute-&gt;classInfo), thisNode);
3793 
3794     bool wasSeenInJIT = true;
3795     GetByStatus* status = m_graph.m_plan.recordedStatuses().addGetByStatus(currentCodeOrigin(), GetByStatus(GetByStatus::Custom, wasSeenInJIT));
3796     bool success = status-&gt;appendVariant(variant);
3797     RELEASE_ASSERT(success);
3798     addToGraph(FilterGetByStatus, OpInfo(status), thisNode);
3799 
3800     CallDOMGetterData* callDOMGetterData = m_graph.m_callDOMGetterData.add();
3801     callDOMGetterData-&gt;customAccessorGetter = variant.customAccessorGetter();
3802     ASSERT(callDOMGetterData-&gt;customAccessorGetter);
3803 
3804     if (const auto* domJIT = domAttribute-&gt;domJIT) {
3805         callDOMGetterData-&gt;domJIT = domJIT;
3806         Ref&lt;DOMJIT::CallDOMGetterSnippet&gt; snippet = domJIT-&gt;compiler()();
3807         callDOMGetterData-&gt;snippet = snippet.ptr();
3808         m_graph.m_domJITSnippets.append(WTFMove(snippet));
3809     }
3810     DOMJIT::CallDOMGetterSnippet* callDOMGetterSnippet = callDOMGetterData-&gt;snippet;
3811     callDOMGetterData-&gt;identifierNumber = identifierNumber;
3812 
3813     Node* callDOMGetterNode = nullptr;
3814     // GlobalObject of thisNode is always used to create a DOMWrapper.
3815     if (callDOMGetterSnippet &amp;&amp; callDOMGetterSnippet-&gt;requireGlobalObject) {
3816         Node* globalObject = addToGraph(GetGlobalObject, thisNode);
3817         callDOMGetterNode = addToGraph(CallDOMGetter, OpInfo(callDOMGetterData), OpInfo(prediction), thisNode, globalObject);
3818     } else
3819         callDOMGetterNode = addToGraph(CallDOMGetter, OpInfo(callDOMGetterData), OpInfo(prediction), thisNode);
3820     blessCallDOMGetter(callDOMGetterNode);
3821     set(result, callDOMGetterNode);
3822     return true;
3823 }
3824 
3825 bool ByteCodeParser::handleModuleNamespaceLoad(VirtualRegister result, SpeculatedType prediction, Node* base, GetByStatus getById)
3826 {
3827     if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
3828         return false;
3829     addToGraph(CheckCell, OpInfo(m_graph.freeze(getById.moduleNamespaceObject())), Edge(base, CellUse));
3830 
3831     addToGraph(FilterGetByStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByStatus(currentCodeOrigin(), getById)), base);
3832 
3833     // Ideally we wouldn&#39;t have to do this Phantom. But:
3834     //
3835     // For the constant case: we must do it because otherwise we would have no way of knowing
3836     // that the scope is live at OSR here.
3837     //
3838     // For the non-constant case: GetClosureVar could be DCE&#39;d, but baseline&#39;s implementation
3839     // won&#39;t be able to handle an Undefined scope.
3840     addToGraph(Phantom, base);
3841 
3842     // Constant folding in the bytecode parser is important for performance. This may not
3843     // have executed yet. If it hasn&#39;t, then we won&#39;t have a prediction. Lacking a
3844     // prediction, we&#39;d otherwise think that it has to exit. Then when it did execute, we
3845     // would recompile. But if we can fold it here, we avoid the exit.
3846     m_graph.freeze(getById.moduleEnvironment());
3847     if (JSValue value = m_graph.tryGetConstantClosureVar(getById.moduleEnvironment(), getById.scopeOffset())) {
3848         set(result, weakJSConstant(value));
3849         return true;
3850     }
3851     set(result, addToGraph(GetClosureVar, OpInfo(getById.scopeOffset().offset()), OpInfo(prediction), weakJSConstant(getById.moduleEnvironment())));
3852     return true;
3853 }
3854 
3855 template&lt;typename ChecksFunctor&gt;
3856 bool ByteCodeParser::handleTypedArrayConstructor(
3857     VirtualRegister result, InternalFunction* function, int registerOffset,
3858     int argumentCountIncludingThis, TypedArrayType type, const ChecksFunctor&amp; insertChecks)
3859 {
3860     if (!isTypedView(type))
3861         return false;
3862 
3863     if (function-&gt;classInfo(*m_vm) != constructorClassInfoForType(type))
3864         return false;
3865 
3866     if (function-&gt;globalObject() != m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject())
3867         return false;
3868 
3869     // We only have an intrinsic for the case where you say:
3870     //
3871     // new FooArray(blah);
3872     //
3873     // Of course, &#39;blah&#39; could be any of the following:
3874     //
3875     // - Integer, indicating that you want to allocate an array of that length.
3876     //   This is the thing we&#39;re hoping for, and what we can actually do meaningful
3877     //   optimizations for.
3878     //
3879     // - Array buffer, indicating that you want to create a view onto that _entire_
3880     //   buffer.
3881     //
3882     // - Non-buffer object, indicating that you want to create a copy of that
3883     //   object by pretending that it quacks like an array.
3884     //
3885     // - Anything else, indicating that you want to have an exception thrown at
3886     //   you.
3887     //
3888     // The intrinsic, NewTypedArray, will behave as if it could do any of these
3889     // things up until we do Fixup. Thereafter, if child1 (i.e. &#39;blah&#39;) is
3890     // predicted Int32, then we lock it in as a normal typed array allocation.
3891     // Otherwise, NewTypedArray turns into a totally opaque function call that
3892     // may clobber the world - by virtue of it accessing properties on what could
3893     // be an object.
3894     //
3895     // Note that although the generic form of NewTypedArray sounds sort of awful,
3896     // it is actually quite likely to be more efficient than a fully generic
3897     // Construct. So, we might want to think about making NewTypedArray variadic,
3898     // or else making Construct not super slow.
3899 
3900     if (argumentCountIncludingThis != 2)
3901         return false;
3902 
3903     if (!function-&gt;globalObject()-&gt;typedArrayStructureConcurrently(type))
3904         return false;
3905 
3906     insertChecks();
3907     set(result,
3908         addToGraph(NewTypedArray, OpInfo(type), get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
3909     return true;
3910 }
3911 
3912 template&lt;typename ChecksFunctor&gt;
3913 bool ByteCodeParser::handleConstantInternalFunction(
3914     Node* callTargetNode, VirtualRegister result, InternalFunction* function, int registerOffset,
3915     int argumentCountIncludingThis, CodeSpecializationKind kind, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
3916 {
3917     VERBOSE_LOG(&quot;    Handling constant internal function &quot;, JSValue(function), &quot;\n&quot;);
3918 
3919     // It so happens that the code below assumes that the result operand is valid. It&#39;s extremely
3920     // unlikely that the result operand would be invalid - you&#39;d have to call this via a setter call.
3921     if (!result.isValid())
3922         return false;
3923 
3924     if (kind == CodeForConstruct) {
3925         Node* newTargetNode = get(virtualRegisterForArgumentIncludingThis(0, registerOffset));
3926         // We cannot handle the case where new.target != callee (i.e. a construct from a super call) because we
3927         // don&#39;t know what the prototype of the constructed object will be.
3928         // FIXME: If we have inlined super calls up to the call site, however, we should be able to figure out the structure. https://bugs.webkit.org/show_bug.cgi?id=152700
3929         if (newTargetNode != callTargetNode)
3930             return false;
3931     }
3932 
3933     if (function-&gt;classInfo(*m_vm) == ArrayConstructor::info()) {
3934         if (function-&gt;globalObject() != m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject())
3935             return false;
3936 
3937         insertChecks();
3938         if (argumentCountIncludingThis == 2) {
3939             set(result,
3940                 addToGraph(NewArrayWithSize, OpInfo(ArrayWithUndecided), get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
3941             return true;
3942         }
3943 
3944         for (int i = 1; i &lt; argumentCountIncludingThis; ++i)
3945             addVarArgChild(get(virtualRegisterForArgumentIncludingThis(i, registerOffset)));
3946         set(result,
3947             addToGraph(Node::VarArg, NewArray, OpInfo(ArrayWithUndecided), OpInfo(argumentCountIncludingThis - 1)));
3948         return true;
3949     }
3950 
3951     if (function-&gt;classInfo(*m_vm) == NumberConstructor::info()) {
3952         if (kind == CodeForConstruct)
3953             return false;
3954 
3955         insertChecks();
3956         if (argumentCountIncludingThis &lt;= 1)
3957             set(result, jsConstant(jsNumber(0)));
3958         else
3959             set(result, addToGraph(ToNumber, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
3960 
3961         return true;
3962     }
3963 
3964     if (function-&gt;classInfo(*m_vm) == StringConstructor::info()) {
3965         insertChecks();
3966 
3967         Node* resultNode;
3968 
3969         if (argumentCountIncludingThis &lt;= 1)
3970             resultNode = jsConstant(m_vm-&gt;smallStrings.emptyString());
3971         else
3972             resultNode = addToGraph(CallStringConstructor, get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
3973 
3974         if (kind == CodeForConstruct)
3975             resultNode = addToGraph(NewStringObject, OpInfo(m_graph.registerStructure(function-&gt;globalObject()-&gt;stringObjectStructure())), resultNode);
3976 
3977         set(result, resultNode);
3978         return true;
3979     }
3980 
3981     if (function-&gt;classInfo(*m_vm) == SymbolConstructor::info() &amp;&amp; kind == CodeForCall) {
3982         insertChecks();
3983 
3984         Node* resultNode;
3985 
3986         if (argumentCountIncludingThis &lt;= 1)
3987             resultNode = addToGraph(NewSymbol);
3988         else
3989             resultNode = addToGraph(NewSymbol, addToGraph(ToString, get(virtualRegisterForArgumentIncludingThis(1, registerOffset))));
3990 
3991         set(result, resultNode);
3992         return true;
3993     }
3994 
3995     // FIXME: This should handle construction as well. https://bugs.webkit.org/show_bug.cgi?id=155591
3996     if (function-&gt;classInfo(*m_vm) == ObjectConstructor::info() &amp;&amp; kind == CodeForCall) {
3997         insertChecks();
3998 
3999         Node* resultNode;
4000         if (argumentCountIncludingThis &lt;= 1)
4001             resultNode = addToGraph(NewObject, OpInfo(m_graph.registerStructure(function-&gt;globalObject()-&gt;objectStructureForObjectConstructor())));
4002         else
4003             resultNode = addToGraph(CallObjectConstructor, OpInfo(m_graph.freeze(function-&gt;globalObject())), OpInfo(prediction), get(virtualRegisterForArgumentIncludingThis(1, registerOffset)));
4004         set(result, resultNode);
4005         return true;
4006     }
4007 
4008     for (unsigned typeIndex = 0; typeIndex &lt; NumberOfTypedArrayTypes; ++typeIndex) {
4009         bool handled = handleTypedArrayConstructor(
4010             result, function, registerOffset, argumentCountIncludingThis,
4011             indexToTypedArrayType(typeIndex), insertChecks);
4012         if (handled)
4013             return true;
4014     }
4015 
4016     return false;
4017 }
4018 
4019 Node* ByteCodeParser::handleGetByOffset(
4020     SpeculatedType prediction, Node* base, unsigned identifierNumber, PropertyOffset offset, NodeType op)
4021 {
4022     Node* propertyStorage;
4023     if (isInlineOffset(offset))
4024         propertyStorage = base;
4025     else
4026         propertyStorage = addToGraph(GetButterfly, base);
4027 
4028     StorageAccessData* data = m_graph.m_storageAccessData.add();
4029     data-&gt;offset = offset;
4030     data-&gt;identifierNumber = identifierNumber;
4031 
4032     Node* getByOffset = addToGraph(op, OpInfo(data), OpInfo(prediction), propertyStorage, base);
4033 
4034     return getByOffset;
4035 }
4036 
4037 Node* ByteCodeParser::handlePutByOffset(
4038     Node* base, unsigned identifier, PropertyOffset offset,
4039     Node* value)
4040 {
4041     Node* propertyStorage;
4042     if (isInlineOffset(offset))
4043         propertyStorage = base;
4044     else
4045         propertyStorage = addToGraph(GetButterfly, base);
4046 
4047     StorageAccessData* data = m_graph.m_storageAccessData.add();
4048     data-&gt;offset = offset;
4049     data-&gt;identifierNumber = identifier;
4050 
4051     Node* result = addToGraph(PutByOffset, OpInfo(data), propertyStorage, base, value);
4052 
4053     return result;
4054 }
4055 
4056 bool ByteCodeParser::check(const ObjectPropertyCondition&amp; condition)
4057 {
4058     if (!condition)
4059         return false;
4060 
4061     if (m_graph.watchCondition(condition))
4062         return true;
4063 
4064     Structure* structure = condition.object()-&gt;structure(*m_vm);
4065     if (!condition.structureEnsuresValidity(structure))
4066         return false;
4067 
4068     addToGraph(
4069         CheckStructure,
4070         OpInfo(m_graph.addStructureSet(structure)),
4071         weakJSConstant(condition.object()));
4072     return true;
4073 }
4074 
4075 GetByOffsetMethod ByteCodeParser::promoteToConstant(GetByOffsetMethod method)
4076 {
4077     if (method.kind() == GetByOffsetMethod::LoadFromPrototype
4078         &amp;&amp; method.prototype()-&gt;structure()-&gt;dfgShouldWatch()) {
4079         if (JSValue constant = m_graph.tryGetConstantProperty(method.prototype()-&gt;value(), method.prototype()-&gt;structure(), method.offset()))
4080             return GetByOffsetMethod::constant(m_graph.freeze(constant));
4081     }
4082 
4083     return method;
4084 }
4085 
4086 bool ByteCodeParser::needsDynamicLookup(ResolveType type, OpcodeID opcode)
4087 {
4088     ASSERT(opcode == op_resolve_scope || opcode == op_get_from_scope || opcode == op_put_to_scope);
4089 
4090     JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
4091     if (needsVarInjectionChecks(type) &amp;&amp; globalObject-&gt;varInjectionWatchpoint()-&gt;hasBeenInvalidated())
4092         return true;
4093 
4094     switch (type) {
4095     case GlobalProperty:
4096     case GlobalVar:
4097     case GlobalLexicalVar:
4098     case ClosureVar:
4099     case LocalClosureVar:
4100     case ModuleVar:
4101         return false;
4102 
4103     case UnresolvedProperty:
4104     case UnresolvedPropertyWithVarInjectionChecks: {
4105         // The heuristic for UnresolvedProperty scope accesses is we will ForceOSRExit if we
4106         // haven&#39;t exited from from this access before to let the baseline JIT try to better
4107         // cache the access. If we&#39;ve already exited from this operation, it&#39;s unlikely that
4108         // the baseline will come up with a better ResolveType and instead we will compile
4109         // this as a dynamic scope access.
4110 
4111         // We only track our heuristic through resolve_scope since resolve_scope will
4112         // dominate unresolved gets/puts on that scope.
4113         if (opcode != op_resolve_scope)
4114             return true;
4115 
4116         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, InadequateCoverage)) {
4117             // We&#39;ve already exited so give up on getting better ResolveType information.
4118             return true;
4119         }
4120 
4121         // We have not exited yet, so let&#39;s have the baseline get better ResolveType information for us.
4122         // This type of code is often seen when we tier up in a loop but haven&#39;t executed the part
4123         // of a function that comes after the loop.
4124         return false;
4125     }
4126 
4127     case Dynamic:
4128         return true;
4129 
4130     case GlobalPropertyWithVarInjectionChecks:
4131     case GlobalVarWithVarInjectionChecks:
4132     case GlobalLexicalVarWithVarInjectionChecks:
4133     case ClosureVarWithVarInjectionChecks:
4134         return false;
4135     }
4136 
4137     ASSERT_NOT_REACHED();
4138     return false;
4139 }
4140 
4141 GetByOffsetMethod ByteCodeParser::planLoad(const ObjectPropertyCondition&amp; condition)
4142 {
4143     VERBOSE_LOG(&quot;Planning a load: &quot;, condition, &quot;\n&quot;);
4144 
4145     // We might promote this to Equivalence, and a later DFG pass might also do such promotion
4146     // even if we fail, but for simplicity this cannot be asked to load an equivalence condition.
4147     // None of the clients of this method will request a load of an Equivalence condition anyway,
4148     // and supporting it would complicate the heuristics below.
4149     RELEASE_ASSERT(condition.kind() == PropertyCondition::Presence);
4150 
4151     // Here&#39;s the ranking of how to handle this, from most preferred to least preferred:
4152     //
4153     // 1) Watchpoint on an equivalence condition and return a constant node for the loaded value.
4154     //    No other code is emitted, and the structure of the base object is never registered.
4155     //    Hence this results in zero code and we won&#39;t jettison this compilation if the object
4156     //    transitions, even if the structure is watchable right now.
4157     //
4158     // 2) Need to emit a load, and the current structure of the base is going to be watched by the
4159     //    DFG anyway (i.e. dfgShouldWatch). Watch the structure and emit the load. Don&#39;t watch the
4160     //    condition, since the act of turning the base into a constant in IR will cause the DFG to
4161     //    watch the structure anyway and doing so would subsume watching the condition.
4162     //
4163     // 3) Need to emit a load, and the current structure of the base is watchable but not by the
4164     //    DFG (i.e. transitionWatchpointSetIsStillValid() and !dfgShouldWatchIfPossible()). Watch
4165     //    the condition, and emit a load.
4166     //
4167     // 4) Need to emit a load, and the current structure of the base is not watchable. Emit a
4168     //    structure check, and emit a load.
4169     //
4170     // 5) The condition does not hold. Give up and return null.
4171 
4172     // First, try to promote Presence to Equivalence. We do this before doing anything else
4173     // because it&#39;s the most profitable. Also, there are cases where the presence is watchable but
4174     // we don&#39;t want to watch it unless it became an equivalence (see the relationship between
4175     // (1), (2), and (3) above).
4176     ObjectPropertyCondition equivalenceCondition = condition.attemptToMakeEquivalenceWithoutBarrier(*m_vm);
4177     if (m_graph.watchCondition(equivalenceCondition))
4178         return GetByOffsetMethod::constant(m_graph.freeze(equivalenceCondition.requiredValue()));
4179 
4180     // At this point, we&#39;ll have to materialize the condition&#39;s base as a constant in DFG IR. Once
4181     // we do this, the frozen value will have its own idea of what the structure is. Use that from
4182     // now on just because it&#39;s less confusing.
4183     FrozenValue* base = m_graph.freeze(condition.object());
4184     Structure* structure = base-&gt;structure();
4185 
4186     // Check if the structure that we&#39;ve registered makes the condition hold. If not, just give
4187     // up. This is case (5) above.
4188     if (!condition.structureEnsuresValidity(structure))
4189         return GetByOffsetMethod();
4190 
4191     // If the structure is watched by the DFG already, then just use this fact to emit the load.
4192     // This is case (2) above.
4193     if (structure-&gt;dfgShouldWatch())
4194         return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
4195 
4196     // If we can watch the condition right now, then we can emit the load after watching it. This
4197     // is case (3) above.
4198     if (m_graph.watchCondition(condition))
4199         return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
4200 
4201     // We can&#39;t watch anything but we know that the current structure satisfies the condition. So,
4202     // check for that structure and then emit the load.
4203     addToGraph(
4204         CheckStructure,
4205         OpInfo(m_graph.addStructureSet(structure)),
4206         addToGraph(JSConstant, OpInfo(base)));
4207     return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
4208 }
4209 
4210 Node* ByteCodeParser::load(
4211     SpeculatedType prediction, unsigned identifierNumber, const GetByOffsetMethod&amp; method,
4212     NodeType op)
4213 {
4214     switch (method.kind()) {
4215     case GetByOffsetMethod::Invalid:
4216         return nullptr;
4217     case GetByOffsetMethod::Constant:
4218         return addToGraph(JSConstant, OpInfo(method.constant()));
4219     case GetByOffsetMethod::LoadFromPrototype: {
4220         Node* baseNode = addToGraph(JSConstant, OpInfo(method.prototype()));
4221         return handleGetByOffset(
4222             prediction, baseNode, identifierNumber, method.offset(), op);
4223     }
4224     case GetByOffsetMethod::Load:
4225         // Will never see this from planLoad().
4226         RELEASE_ASSERT_NOT_REACHED();
4227         return nullptr;
4228     }
4229 
4230     RELEASE_ASSERT_NOT_REACHED();
4231     return nullptr;
4232 }
4233 
4234 Node* ByteCodeParser::load(
4235     SpeculatedType prediction, const ObjectPropertyCondition&amp; condition, NodeType op)
4236 {
4237     GetByOffsetMethod method = planLoad(condition);
4238     return load(prediction, m_graph.identifiers().ensure(condition.uid()), method, op);
4239 }
4240 
4241 bool ByteCodeParser::check(const ObjectPropertyConditionSet&amp; conditionSet)
4242 {
4243     for (const ObjectPropertyCondition&amp; condition : conditionSet) {
4244         if (!check(condition))
4245             return false;
4246     }
4247     return true;
4248 }
4249 
4250 GetByOffsetMethod ByteCodeParser::planLoad(const ObjectPropertyConditionSet&amp; conditionSet)
4251 {
4252     VERBOSE_LOG(&quot;conditionSet = &quot;, conditionSet, &quot;\n&quot;);
4253 
4254     GetByOffsetMethod result;
4255     for (const ObjectPropertyCondition&amp; condition : conditionSet) {
4256         switch (condition.kind()) {
4257         case PropertyCondition::Presence:
4258             RELEASE_ASSERT(!result); // Should only see exactly one of these.
4259             result = planLoad(condition);
4260             if (!result)
4261                 return GetByOffsetMethod();
4262             break;
4263         default:
4264             if (!check(condition))
4265                 return GetByOffsetMethod();
4266             break;
4267         }
4268     }
4269     if (!result) {
4270         // We have a unset property.
4271         ASSERT(!conditionSet.numberOfConditionsWithKind(PropertyCondition::Presence));
4272         return GetByOffsetMethod::constant(m_constantUndefined);
4273     }
4274     return result;
4275 }
4276 
4277 Node* ByteCodeParser::load(
4278     SpeculatedType prediction, const ObjectPropertyConditionSet&amp; conditionSet, NodeType op)
4279 {
4280     GetByOffsetMethod method = planLoad(conditionSet);
4281     return load(
4282         prediction,
4283         m_graph.identifiers().ensure(conditionSet.slotBaseCondition().uid()),
4284         method, op);
4285 }
4286 
4287 ObjectPropertyCondition ByteCodeParser::presenceLike(
4288     JSObject* knownBase, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4289 {
4290     if (set.isEmpty())
4291         return ObjectPropertyCondition();
4292     unsigned attributes;
4293     PropertyOffset firstOffset = set[0]-&gt;getConcurrently(uid, attributes);
4294     if (firstOffset != offset)
4295         return ObjectPropertyCondition();
4296     for (unsigned i = 1; i &lt; set.size(); ++i) {
4297         unsigned otherAttributes;
4298         PropertyOffset otherOffset = set[i]-&gt;getConcurrently(uid, otherAttributes);
4299         if (otherOffset != offset || otherAttributes != attributes)
4300             return ObjectPropertyCondition();
4301     }
4302     return ObjectPropertyCondition::presenceWithoutBarrier(knownBase, uid, offset, attributes);
4303 }
4304 
4305 bool ByteCodeParser::checkPresenceLike(
4306     JSObject* knownBase, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4307 {
4308     return check(presenceLike(knownBase, uid, offset, set));
4309 }
4310 
4311 void ByteCodeParser::checkPresenceLike(
4312     Node* base, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4313 {
4314     if (JSObject* knownBase = base-&gt;dynamicCastConstant&lt;JSObject*&gt;(*m_vm)) {
4315         if (checkPresenceLike(knownBase, uid, offset, set))
4316             return;
4317     }
4318 
4319     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(set)), base);
4320 }
4321 
4322 template&lt;typename VariantType&gt;
4323 Node* ByteCodeParser::load(
4324     SpeculatedType prediction, Node* base, unsigned identifierNumber, const VariantType&amp; variant)
4325 {
4326     // Make sure backwards propagation knows that we&#39;ve used base.
4327     addToGraph(Phantom, base);
4328 
4329     bool needStructureCheck = true;
4330 
4331     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
4332 
4333     if (JSObject* knownBase = base-&gt;dynamicCastConstant&lt;JSObject*&gt;(*m_vm)) {
4334         // Try to optimize away the structure check. Note that it&#39;s not worth doing anything about this
4335         // if the base&#39;s structure is watched.
4336         Structure* structure = base-&gt;constant()-&gt;structure();
4337         if (!structure-&gt;dfgShouldWatch()) {
4338             if (!variant.conditionSet().isEmpty()) {
4339                 // This means that we&#39;re loading from a prototype or we have a property miss. We expect
4340                 // the base not to have the property. We can only use ObjectPropertyCondition if all of
4341                 // the structures in the variant.structureSet() agree on the prototype (it would be
4342                 // hilariously rare if they didn&#39;t). Note that we are relying on structureSet() having
4343                 // at least one element. That will always be true here because of how GetByStatus/PutByIdStatus work.
4344 
4345                 // FIXME: right now, if we have an OPCS, we have mono proto. However, this will
4346                 // need to be changed in the future once we have a hybrid data structure for
4347                 // poly proto:
4348                 // https://bugs.webkit.org/show_bug.cgi?id=177339
4349                 JSObject* prototype = variant.structureSet()[0]-&gt;storedPrototypeObject();
4350                 bool allAgree = true;
4351                 for (unsigned i = 1; i &lt; variant.structureSet().size(); ++i) {
4352                     if (variant.structureSet()[i]-&gt;storedPrototypeObject() != prototype) {
4353                         allAgree = false;
4354                         break;
4355                     }
4356                 }
4357                 if (allAgree) {
4358                     ObjectPropertyCondition condition = ObjectPropertyCondition::absenceWithoutBarrier(
4359                         knownBase, uid, prototype);
4360                     if (check(condition))
4361                         needStructureCheck = false;
4362                 }
4363             } else {
4364                 // This means we&#39;re loading directly from base. We can avoid all of the code that follows
4365                 // if we can prove that the property is a constant. Otherwise, we try to prove that the
4366                 // property is watchably present, in which case we get rid of the structure check.
4367 
4368                 ObjectPropertyCondition presenceCondition =
4369                     presenceLike(knownBase, uid, variant.offset(), variant.structureSet());
4370                 if (presenceCondition) {
4371                     ObjectPropertyCondition equivalenceCondition =
4372                         presenceCondition.attemptToMakeEquivalenceWithoutBarrier(*m_vm);
4373                     if (m_graph.watchCondition(equivalenceCondition))
4374                         return weakJSConstant(equivalenceCondition.requiredValue());
4375 
4376                     if (check(presenceCondition))
4377                         needStructureCheck = false;
4378                 }
4379             }
4380         }
4381     }
4382 
4383     if (needStructureCheck)
4384         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.structureSet())), base);
4385 
4386     if (variant.isPropertyUnset()) {
4387         if (m_graph.watchConditions(variant.conditionSet()))
4388             return jsConstant(jsUndefined());
4389         return nullptr;
4390     }
4391 
4392     SpeculatedType loadPrediction;
4393     NodeType loadOp;
4394     if (variant.callLinkStatus() || variant.intrinsic() != NoIntrinsic) {
4395         loadPrediction = SpecCellOther;
4396         loadOp = GetGetterSetterByOffset;
4397     } else {
4398         loadPrediction = prediction;
4399         loadOp = GetByOffset;
4400     }
4401 
4402     Node* loadedValue;
4403     if (!variant.conditionSet().isEmpty())
4404         loadedValue = load(loadPrediction, variant.conditionSet(), loadOp);
4405     else {
4406         if (needStructureCheck &amp;&amp; base-&gt;hasConstant()) {
4407             // We did emit a structure check. That means that we have an opportunity to do constant folding
4408             // here, since we didn&#39;t do it above.
4409             JSValue constant = m_graph.tryGetConstantProperty(
4410                 base-&gt;asJSValue(), *m_graph.addStructureSet(variant.structureSet()), variant.offset());
4411             if (constant)
4412                 return weakJSConstant(constant);
4413         }
4414 
4415         loadedValue = handleGetByOffset(
4416             loadPrediction, base, identifierNumber, variant.offset(), loadOp);
4417     }
4418 
4419     return loadedValue;
4420 }
4421 
4422 Node* ByteCodeParser::store(Node* base, unsigned identifier, const PutByIdVariant&amp; variant, Node* value)
4423 {
4424     RELEASE_ASSERT(variant.kind() == PutByIdVariant::Replace);
4425 
4426     checkPresenceLike(base, m_graph.identifiers()[identifier], variant.offset(), variant.structure());
4427     return handlePutByOffset(base, identifier, variant.offset(), value);
4428 }
4429 
4430 void ByteCodeParser::handleGetById(
4431     VirtualRegister destination, SpeculatedType prediction, Node* base, unsigned identifierNumber,
4432     GetByStatus getByStatus, AccessType type, unsigned instructionSize)
4433 {
4434     // Attempt to reduce the set of things in the GetByStatus.
4435     if (base-&gt;op() == NewObject) {
4436         bool ok = true;
4437         for (unsigned i = m_currentBlock-&gt;size(); i--;) {
4438             Node* node = m_currentBlock-&gt;at(i);
4439             if (node == base)
4440                 break;
4441             if (writesOverlap(m_graph, node, JSCell_structureID)) {
4442                 ok = false;
4443                 break;
4444             }
4445         }
4446         if (ok)
4447             getByStatus.filter(base-&gt;structure().get());
4448     }
4449 
4450     NodeType getById;
4451     if (type == AccessType::GetById)
4452         getById = getByStatus.makesCalls() ? GetByIdFlush : GetById;
4453     else if (type == AccessType::TryGetById)
4454         getById = TryGetById;
4455     else
4456         getById = getByStatus.makesCalls() ? GetByIdDirectFlush : GetByIdDirect;
4457 
4458     if (getById != TryGetById &amp;&amp; getByStatus.isModuleNamespace()) {
4459         if (handleModuleNamespaceLoad(destination, prediction, base, getByStatus)) {
4460             if (UNLIKELY(m_graph.compilation()))
4461                 m_graph.compilation()-&gt;noticeInlinedGetById();
4462             return;
4463         }
4464     }
4465 
4466     // Special path for custom accessors since custom&#39;s offset does not have any meanings.
4467     // So, this is completely different from Simple one. But we have a chance to optimize it when we use DOMJIT.
4468     if (Options::useDOMJIT() &amp;&amp; getByStatus.isCustom()) {
4469         ASSERT(getByStatus.numVariants() == 1);
4470         ASSERT(!getByStatus.makesCalls());
4471         GetByIdVariant variant = getByStatus[0];
4472         ASSERT(variant.domAttribute());
4473         if (handleDOMJITGetter(destination, variant, base, identifierNumber, prediction)) {
4474             if (UNLIKELY(m_graph.compilation()))
4475                 m_graph.compilation()-&gt;noticeInlinedGetById();
4476             return;
4477         }
4478     }
4479 
4480     ASSERT(type == AccessType::GetById || type == AccessType::GetByIdDirect ||  !getByStatus.makesCalls());
4481     if (!getByStatus.isSimple() || !getByStatus.numVariants() || !Options::useAccessInlining()) {
4482         set(destination,
4483             addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4484         return;
4485     }
4486 
4487     // FIXME: If we use the GetByStatus for anything then we should record it and insert a node
4488     // after everything else (like the GetByOffset or whatever) that will filter the recorded
4489     // GetByStatus. That means that the constant folder also needs to do the same!
4490 
4491     if (getByStatus.numVariants() &gt; 1) {
4492         if (getByStatus.makesCalls() || !m_graph.m_plan.isFTL()
4493             || !Options::usePolymorphicAccessInlining()
4494             || getByStatus.numVariants() &gt; Options::maxPolymorphicAccessInliningListSize()) {
4495             set(destination,
4496                 addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4497             return;
4498         }
4499 
4500         addToGraph(FilterGetByStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByStatus(currentCodeOrigin(), getByStatus)), base);
4501 
4502         Vector&lt;MultiGetByOffsetCase, 2&gt; cases;
4503 
4504         // 1) Emit prototype structure checks for all chains. This could sort of maybe not be
4505         //    optimal, if there is some rarely executed case in the chain that requires a lot
4506         //    of checks and those checks are not watchpointable.
4507         for (const GetByIdVariant&amp; variant : getByStatus.variants()) {
4508             if (variant.intrinsic() != NoIntrinsic) {
4509                 set(destination,
4510                     addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4511                 return;
4512             }
4513 
4514             if (variant.conditionSet().isEmpty()) {
4515                 cases.append(
4516                     MultiGetByOffsetCase(
4517                         *m_graph.addStructureSet(variant.structureSet()),
4518                         GetByOffsetMethod::load(variant.offset())));
4519                 continue;
4520             }
4521 
4522             GetByOffsetMethod method = planLoad(variant.conditionSet());
4523             if (!method) {
4524                 set(destination,
4525                     addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4526                 return;
4527             }
4528 
4529             cases.append(MultiGetByOffsetCase(*m_graph.addStructureSet(variant.structureSet()), method));
4530         }
4531 
4532         if (UNLIKELY(m_graph.compilation()))
4533             m_graph.compilation()-&gt;noticeInlinedGetById();
4534 
4535         // 2) Emit a MultiGetByOffset
4536         MultiGetByOffsetData* data = m_graph.m_multiGetByOffsetData.add();
4537         data-&gt;cases = cases;
4538         data-&gt;identifierNumber = identifierNumber;
4539         set(destination,
4540             addToGraph(MultiGetByOffset, OpInfo(data), OpInfo(prediction), base));
4541         return;
4542     }
4543 
4544     addToGraph(FilterGetByStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByStatus(currentCodeOrigin(), getByStatus)), base);
4545 
4546     ASSERT(getByStatus.numVariants() == 1);
4547     GetByIdVariant variant = getByStatus[0];
4548 
4549     Node* loadedValue = load(prediction, base, identifierNumber, variant);
4550     if (!loadedValue) {
4551         set(destination,
4552             addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4553         return;
4554     }
4555 
4556     if (UNLIKELY(m_graph.compilation()))
4557         m_graph.compilation()-&gt;noticeInlinedGetById();
4558 
4559     ASSERT(type == AccessType::GetById || type == AccessType::GetByIdDirect || !variant.callLinkStatus());
4560     if (!variant.callLinkStatus() &amp;&amp; variant.intrinsic() == NoIntrinsic) {
4561         set(destination, loadedValue);
4562         return;
4563     }
4564 
4565     Node* getter = addToGraph(GetGetter, loadedValue);
4566 
4567     if (handleIntrinsicGetter(destination, prediction, variant, base,
4568             [&amp;] () {
4569                 addToGraph(CheckCell, OpInfo(m_graph.freeze(variant.intrinsicFunction())), getter);
4570             })) {
4571         addToGraph(Phantom, base);
4572         return;
4573     }
4574 
4575     ASSERT(variant.intrinsic() == NoIntrinsic);
4576 
4577     // Make a call. We don&#39;t try to get fancy with using the smallest operand number because
4578     // the stack layout phase should compress the stack anyway.
4579 
4580     unsigned numberOfParameters = 0;
4581     numberOfParameters++; // The &#39;this&#39; argument.
4582     numberOfParameters++; // True return PC.
4583 
4584     // Start with a register offset that corresponds to the last in-use register.
4585     int registerOffset = virtualRegisterForLocal(
4586         m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4587     registerOffset -= numberOfParameters;
4588     registerOffset -= CallFrame::headerSizeInRegisters;
4589 
4590     // Get the alignment right.
4591     registerOffset = -WTF::roundUpToMultipleOf(
4592         stackAlignmentRegisters(),
4593         -registerOffset);
4594 
4595     ensureLocals(
4596         m_inlineStackTop-&gt;remapOperand(
4597             VirtualRegister(registerOffset)).toLocal());
4598 
4599     // Issue SetLocals. This has two effects:
4600     // 1) That&#39;s how handleCall() sees the arguments.
4601     // 2) If we inline then this ensures that the arguments are flushed so that if you use
4602     //    the dreaded arguments object on the getter, the right things happen. Well, sort of -
4603     //    since we only really care about &#39;this&#39; in this case. But we&#39;re not going to take that
4604     //    shortcut.
4605     set(virtualRegisterForArgumentIncludingThis(0, registerOffset), base, ImmediateNakedSet);
4606 
4607     // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4608     m_exitOK = true;
4609     addToGraph(ExitOK);
4610 
4611     handleCall(
4612         destination, Call, InlineCallFrame::GetterCall, instructionSize,
4613         getter, numberOfParameters - 1, registerOffset, *variant.callLinkStatus(), prediction);
4614 }
4615 
4616 void ByteCodeParser::emitPutById(
4617     Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp; putByIdStatus, bool isDirect)
4618 {
4619     if (isDirect)
4620         addToGraph(PutByIdDirect, OpInfo(identifierNumber), base, value);
4621     else
4622         addToGraph(putByIdStatus.makesCalls() ? PutByIdFlush : PutById, OpInfo(identifierNumber), base, value);
4623 }
4624 
4625 void ByteCodeParser::handlePutById(
4626     Node* base, unsigned identifierNumber, Node* value,
4627     const PutByIdStatus&amp; putByIdStatus, bool isDirect, unsigned instructionSize)
4628 {
4629     if (!putByIdStatus.isSimple() || !putByIdStatus.numVariants() || !Options::useAccessInlining()) {
4630         if (!putByIdStatus.isSet())
4631             addToGraph(ForceOSRExit);
4632         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4633         return;
4634     }
4635 
4636     if (putByIdStatus.numVariants() &gt; 1) {
4637         if (!m_graph.m_plan.isFTL() || putByIdStatus.makesCalls()
4638             || !Options::usePolymorphicAccessInlining()
4639             || putByIdStatus.numVariants() &gt; Options::maxPolymorphicAccessInliningListSize()) {
4640             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4641             return;
4642         }
4643 
4644         if (!isDirect) {
4645             for (unsigned variantIndex = putByIdStatus.numVariants(); variantIndex--;) {
4646                 if (putByIdStatus[variantIndex].kind() != PutByIdVariant::Transition)
4647                     continue;
4648                 if (!check(putByIdStatus[variantIndex].conditionSet())) {
4649                     emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4650                     return;
4651                 }
4652             }
4653         }
4654 
4655         if (UNLIKELY(m_graph.compilation()))
4656             m_graph.compilation()-&gt;noticeInlinedPutById();
4657 
4658         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4659 
4660         for (const PutByIdVariant&amp; variant : putByIdStatus.variants()) {
4661             for (Structure* structure : variant.oldStructure())
4662                 m_graph.registerStructure(structure);
4663             if (variant.kind() == PutByIdVariant::Transition)
4664                 m_graph.registerStructure(variant.newStructure());
4665         }
4666 
4667         MultiPutByOffsetData* data = m_graph.m_multiPutByOffsetData.add();
4668         data-&gt;variants = putByIdStatus.variants();
4669         data-&gt;identifierNumber = identifierNumber;
4670         addToGraph(MultiPutByOffset, OpInfo(data), base, value);
4671         return;
4672     }
4673 
4674     ASSERT(putByIdStatus.numVariants() == 1);
4675     const PutByIdVariant&amp; variant = putByIdStatus[0];
4676 
4677     switch (variant.kind()) {
4678     case PutByIdVariant::Replace: {
4679         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4680 
4681         store(base, identifierNumber, variant, value);
4682         if (UNLIKELY(m_graph.compilation()))
4683             m_graph.compilation()-&gt;noticeInlinedPutById();
4684         return;
4685     }
4686 
4687     case PutByIdVariant::Transition: {
4688         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4689 
4690         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.oldStructure())), base);
4691         if (!check(variant.conditionSet())) {
4692             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4693             return;
4694         }
4695 
4696         ASSERT(variant.oldStructureForTransition()-&gt;transitionWatchpointSetHasBeenInvalidated());
4697 
4698         Node* propertyStorage;
4699         Transition* transition = m_graph.m_transitions.add(
4700             m_graph.registerStructure(variant.oldStructureForTransition()), m_graph.registerStructure(variant.newStructure()));
4701 
4702         if (variant.reallocatesStorage()) {
4703 
4704             // If we&#39;re growing the property storage then it must be because we&#39;re
4705             // storing into the out-of-line storage.
4706             ASSERT(!isInlineOffset(variant.offset()));
4707 
4708             if (!variant.oldStructureForTransition()-&gt;outOfLineCapacity()) {
4709                 propertyStorage = addToGraph(
4710                     AllocatePropertyStorage, OpInfo(transition), base);
4711             } else {
4712                 propertyStorage = addToGraph(
4713                     ReallocatePropertyStorage, OpInfo(transition),
4714                     base, addToGraph(GetButterfly, base));
4715             }
4716         } else {
4717             if (isInlineOffset(variant.offset()))
4718                 propertyStorage = base;
4719             else
4720                 propertyStorage = addToGraph(GetButterfly, base);
4721         }
4722 
4723         StorageAccessData* data = m_graph.m_storageAccessData.add();
4724         data-&gt;offset = variant.offset();
4725         data-&gt;identifierNumber = identifierNumber;
4726 
4727         // NOTE: We could GC at this point because someone could insert an operation that GCs.
4728         // That&#39;s fine because:
4729         // - Things already in the structure will get scanned because we haven&#39;t messed with
4730         //   the object yet.
4731         // - The value we are fixing to put is going to be kept live by OSR exit handling. So
4732         //   if the GC does a conservative scan here it will see the new value.
4733 
4734         addToGraph(
4735             PutByOffset,
4736             OpInfo(data),
4737             propertyStorage,
4738             base,
4739             value);
4740 
4741         if (variant.reallocatesStorage())
4742             addToGraph(NukeStructureAndSetButterfly, base, propertyStorage);
4743 
4744         // FIXME: PutStructure goes last until we fix either
4745         // https://bugs.webkit.org/show_bug.cgi?id=142921 or
4746         // https://bugs.webkit.org/show_bug.cgi?id=142924.
4747         addToGraph(PutStructure, OpInfo(transition), base);
4748 
4749         if (UNLIKELY(m_graph.compilation()))
4750             m_graph.compilation()-&gt;noticeInlinedPutById();
4751         return;
4752     }
4753 
4754     case PutByIdVariant::Setter: {
4755         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4756 
4757         Node* loadedValue = load(SpecCellOther, base, identifierNumber, variant);
4758         if (!loadedValue) {
4759             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4760             return;
4761         }
4762 
4763         Node* setter = addToGraph(GetSetter, loadedValue);
4764 
4765         // Make a call. We don&#39;t try to get fancy with using the smallest operand number because
4766         // the stack layout phase should compress the stack anyway.
4767 
4768         unsigned numberOfParameters = 0;
4769         numberOfParameters++; // The &#39;this&#39; argument.
4770         numberOfParameters++; // The new value.
4771         numberOfParameters++; // True return PC.
4772 
4773         // Start with a register offset that corresponds to the last in-use register.
4774         int registerOffset = virtualRegisterForLocal(
4775             m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4776         registerOffset -= numberOfParameters;
4777         registerOffset -= CallFrame::headerSizeInRegisters;
4778 
4779         // Get the alignment right.
4780         registerOffset = -WTF::roundUpToMultipleOf(
4781             stackAlignmentRegisters(),
4782             -registerOffset);
4783 
4784         ensureLocals(
4785             m_inlineStackTop-&gt;remapOperand(
4786                 VirtualRegister(registerOffset)).toLocal());
4787 
4788         set(virtualRegisterForArgumentIncludingThis(0, registerOffset), base, ImmediateNakedSet);
4789         set(virtualRegisterForArgumentIncludingThis(1, registerOffset), value, ImmediateNakedSet);
4790 
4791         // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4792         m_exitOK = true;
4793         addToGraph(ExitOK);
4794 
4795         handleCall(
4796             VirtualRegister(), Call, InlineCallFrame::SetterCall,
4797             instructionSize, setter, numberOfParameters - 1, registerOffset,
4798             *variant.callLinkStatus(), SpecOther);
4799         return;
4800     }
4801 
4802     default: {
4803         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4804         return;
4805     } }
4806 }
4807 
4808 void ByteCodeParser::prepareToParseBlock()
4809 {
4810     clearCaches();
4811     ASSERT(m_setLocalQueue.isEmpty());
4812 }
4813 
4814 void ByteCodeParser::clearCaches()
4815 {
4816     m_constants.shrink(0);
4817 }
4818 
4819 template&lt;typename Op&gt;
4820 void ByteCodeParser::parseGetById(const Instruction* currentInstruction)
4821 {
4822     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
4823     SpeculatedType prediction = getPrediction();
4824 
4825     Node* base = get(bytecode.m_base);
4826     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
4827 
4828     AccessType type = AccessType::GetById;
4829     unsigned opcodeLength = currentInstruction-&gt;size();
4830     if (Op::opcodeID == op_try_get_by_id)
4831         type = AccessType::TryGetById;
4832     else if (Op::opcodeID == op_get_by_id_direct)
4833         type = AccessType::GetByIdDirect;
4834 
4835     GetByStatus getByStatus = GetByStatus::computeFor(
4836         m_inlineStackTop-&gt;m_profiledBlock,
4837         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
4838         currentCodeOrigin());
4839 
4840     handleGetById(
4841         bytecode.m_dst, prediction, base, identifierNumber, getByStatus, type, opcodeLength);
4842 }
4843 
4844 static uint64_t makeDynamicVarOpInfo(unsigned identifierNumber, unsigned getPutInfo)
4845 {
4846     static_assert(sizeof(identifierNumber) == 4,
4847         &quot;We cannot fit identifierNumber into the high bits of m_opInfo&quot;);
4848     return static_cast&lt;uint64_t&gt;(identifierNumber) | (static_cast&lt;uint64_t&gt;(getPutInfo) &lt;&lt; 32);
4849 }
4850 
4851 // The idiom:
4852 //     if (true) { ...; goto label; } else label: continue
4853 // Allows using NEXT_OPCODE as a statement, even in unbraced if+else, while containing a `continue`.
4854 // The more common idiom:
4855 //     do { ...; } while (false)
4856 // Doesn&#39;t allow using `continue`.
4857 #define NEXT_OPCODE(name) \
4858     if (true) { \
4859         m_currentIndex = BytecodeIndex(m_currentIndex.offset() + currentInstruction-&gt;size()); \
4860         goto WTF_CONCAT(NEXT_OPCODE_, __LINE__); /* Need a unique label: usable more than once per function. */ \
4861     } else \
4862         WTF_CONCAT(NEXT_OPCODE_, __LINE__): \
4863     continue
4864 
4865 #define LAST_OPCODE_LINKED(name) do { \
4866         m_currentIndex = BytecodeIndex(m_currentIndex.offset() + currentInstruction-&gt;size()); \
4867         m_exitOK = false; \
4868         return; \
4869     } while (false)
4870 
4871 #define LAST_OPCODE(name) \
4872     do { \
4873         if (m_currentBlock-&gt;terminal()) { \
4874             switch (m_currentBlock-&gt;terminal()-&gt;op()) { \
4875             case Jump: \
4876             case Branch: \
4877             case Switch: \
4878                 ASSERT(!m_currentBlock-&gt;isLinked); \
4879                 m_inlineStackTop-&gt;m_unlinkedBlocks.append(m_currentBlock); \
4880                 break;\
4881             default: break; \
4882             } \
4883         } \
4884         LAST_OPCODE_LINKED(name); \
4885     } while (false)
4886 
4887 void ByteCodeParser::parseBlock(unsigned limit)
4888 {
4889     auto&amp; instructions = m_inlineStackTop-&gt;m_codeBlock-&gt;instructions();
4890     BytecodeIndex blockBegin = m_currentIndex;
4891 
4892     // If we are the first basic block, introduce markers for arguments. This allows
4893     // us to track if a use of an argument may use the actual argument passed, as
4894     // opposed to using a value we set explicitly.
4895     if (m_currentBlock == m_graph.block(0) &amp;&amp; !inlineCallFrame()) {
4896         auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
4897         RELEASE_ASSERT(addResult.isNewEntry);
4898         ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
4899         entrypointArguments.resize(m_numArguments);
4900 
4901         // We will emit SetArgumentDefinitely nodes. They don&#39;t exit, but we&#39;re at the top of an op_enter so
4902         // exitOK = true.
4903         m_exitOK = true;
4904         for (unsigned argument = 0; argument &lt; m_numArguments; ++argument) {
4905             VariableAccessData* variable = newVariableAccessData(
4906                 virtualRegisterForArgumentIncludingThis(argument));
4907             variable-&gt;mergeStructureCheckHoistingFailed(
4908                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache));
4909             variable-&gt;mergeCheckArrayHoistingFailed(
4910                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType));
4911 
4912             Node* setArgument = addToGraph(SetArgumentDefinitely, OpInfo(variable));
4913             entrypointArguments[argument] = setArgument;
4914             m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
4915         }
4916     }
4917 
4918     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
4919 
4920     auto jumpTarget = [&amp;](int target) {
4921         if (target)
4922             return target;
4923         return codeBlock-&gt;outOfLineJumpOffset(m_currentInstruction);
4924     };
4925 
4926     while (true) {
4927         // We&#39;re staring at a new bytecode instruction. So we once again have a place that we can exit
4928         // to.
4929         m_exitOK = true;
4930 
4931         processSetLocalQueue();
4932 
4933         // Don&#39;t extend over jump destinations.
4934         if (m_currentIndex.offset() == limit) {
4935             // Ordinarily we want to plant a jump. But refuse to do this if the block is
4936             // empty. This is a special case for inlining, which might otherwise create
4937             // some empty blocks in some cases. When parseBlock() returns with an empty
4938             // block, it will get repurposed instead of creating a new one. Note that this
4939             // logic relies on every bytecode resulting in one or more nodes, which would
4940             // be true anyway except for op_loop_hint, which emits a Phantom to force this
4941             // to be true.
4942 
4943             if (!m_currentBlock-&gt;isEmpty())
4944                 addJumpTo(m_currentIndex.offset());
4945             return;
4946         }
4947 
4948         // Switch on the current bytecode opcode.
4949         const Instruction* currentInstruction = instructions.at(m_currentIndex).ptr();
4950         m_currentInstruction = currentInstruction; // Some methods want to use this, and we&#39;d rather not thread it through calls.
4951         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
4952 
4953         VERBOSE_LOG(&quot;    parsing &quot;, currentCodeOrigin(), &quot;: &quot;, opcodeID, &quot;\n&quot;);
4954 
4955         if (UNLIKELY(m_graph.compilation())) {
4956             addToGraph(CountExecution, OpInfo(m_graph.compilation()-&gt;executionCounterFor(
4957                 Profiler::OriginStack(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock, currentCodeOrigin()))));
4958         }
4959 
4960         switch (opcodeID) {
4961 
4962         // === Function entry opcodes ===
4963 
4964         case op_enter: {
4965             Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
4966             // Initialize all locals to undefined.
4967             for (int i = 0; i &lt; m_inlineStackTop-&gt;m_codeBlock-&gt;numVars(); ++i)
4968                 set(virtualRegisterForLocal(i), undefined, ImmediateNakedSet);
4969 
4970             NEXT_OPCODE(op_enter);
4971         }
4972 
4973         case op_to_this: {
4974             Node* op1 = getThis();
4975             auto&amp; metadata = currentInstruction-&gt;as&lt;OpToThis&gt;().metadata(codeBlock);
4976             StructureID cachedStructureID = metadata.m_cachedStructureID;
4977             Structure* cachedStructure = nullptr;
4978             if (cachedStructureID)
4979                 cachedStructure = m_vm-&gt;heap.structureIDTable().get(cachedStructureID);
4980             if (metadata.m_toThisStatus != ToThisOK
4981                 || !cachedStructure
4982                 || cachedStructure-&gt;classInfo()-&gt;methodTable.toThis != JSObject::info()-&gt;methodTable.toThis
4983                 || m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSlowCase(m_currentIndex)
4984                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
4985                 || (op1-&gt;op() == GetLocal &amp;&amp; op1-&gt;variableAccessData()-&gt;structureCheckHoistingFailed())) {
4986                 setThis(addToGraph(ToThis, OpInfo(), OpInfo(getPrediction()), op1));
4987             } else {
4988                 addToGraph(
4989                     CheckStructure,
4990                     OpInfo(m_graph.addStructureSet(cachedStructure)),
4991                     op1);
4992             }
4993             NEXT_OPCODE(op_to_this);
4994         }
4995 
4996         case op_create_this: {
4997             auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
4998             Node* callee = get(VirtualRegister(bytecode.m_callee));
4999 
5000             JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm);
5001             if (!function) {
5002                 JSCell* cachedFunction = bytecode.metadata(codeBlock).m_cachedCallee.unvalidatedGet();
5003                 if (cachedFunction
5004                     &amp;&amp; cachedFunction != JSCell::seenMultipleCalleeObjects()
5005                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5006                     ASSERT(cachedFunction-&gt;inherits&lt;JSFunction&gt;(*m_vm));
5007 
5008                     FrozenValue* frozen = m_graph.freeze(cachedFunction);
5009                     addToGraph(CheckCell, OpInfo(frozen), callee);
5010 
5011                     function = static_cast&lt;JSFunction*&gt;(cachedFunction);
5012                 }
5013             }
5014 
5015             bool alreadyEmitted = false;
5016             if (function) {
5017                 if (FunctionRareData* rareData = function-&gt;rareData()) {
5018                     if (rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
5019                         Structure* structure = rareData-&gt;objectAllocationStructure();
5020                         JSObject* prototype = rareData-&gt;objectAllocationPrototype();
5021                         if (structure
5022                             &amp;&amp; (structure-&gt;hasMonoProto() || prototype)
5023                             &amp;&amp; rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
5024 
5025                             m_graph.freeze(rareData);
5026                             m_graph.watchpoints().addLazily(rareData-&gt;allocationProfileWatchpointSet());
5027 
5028                             Node* object = addToGraph(NewObject, OpInfo(m_graph.registerStructure(structure)));
5029                             if (structure-&gt;hasPolyProto()) {
5030                                 StorageAccessData* data = m_graph.m_storageAccessData.add();
5031                                 data-&gt;offset = knownPolyProtoOffset;
5032                                 data-&gt;identifierNumber = m_graph.identifiers().ensure(m_graph.m_vm.propertyNames-&gt;builtinNames().polyProtoName().impl());
5033                                 ASSERT(isInlineOffset(knownPolyProtoOffset));
5034                                 addToGraph(PutByOffset, OpInfo(data), object, object, weakJSConstant(prototype));
5035                             }
5036                             set(VirtualRegister(bytecode.m_dst), object);
5037                             // The callee is still live up to this point.
5038                             addToGraph(Phantom, callee);
5039                             alreadyEmitted = true;
5040                         }
5041                     }
5042                 }
5043             }
5044             if (!alreadyEmitted) {
5045                 set(VirtualRegister(bytecode.m_dst),
5046                     addToGraph(CreateThis, OpInfo(bytecode.m_inlineCapacity), callee));
5047             }
5048             NEXT_OPCODE(op_create_this);
5049         }
5050 
5051         case op_create_promise: {
5052             JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
5053             auto bytecode = currentInstruction-&gt;as&lt;OpCreatePromise&gt;();
5054             Node* callee = get(VirtualRegister(bytecode.m_callee));
5055 
5056             bool alreadyEmitted = false;
5057 
5058             {
5059                 // Attempt to convert to NewPromise first in easy case.
5060                 JSPromiseConstructor* promiseConstructor = callee-&gt;dynamicCastConstant&lt;JSPromiseConstructor*&gt;(*m_vm);
5061                 if (promiseConstructor == (bytecode.m_isInternalPromise ? globalObject-&gt;internalPromiseConstructor() : globalObject-&gt;promiseConstructor())) {
5062                     JSCell* cachedFunction = bytecode.metadata(codeBlock).m_cachedCallee.unvalidatedGet();
5063                     if (cachedFunction
5064                         &amp;&amp; cachedFunction != JSCell::seenMultipleCalleeObjects()
5065                         &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)
5066                         &amp;&amp; cachedFunction == (bytecode.m_isInternalPromise ? globalObject-&gt;internalPromiseConstructor() : globalObject-&gt;promiseConstructor())) {
5067                         FrozenValue* frozen = m_graph.freeze(cachedFunction);
5068                         addToGraph(CheckCell, OpInfo(frozen), callee);
5069 
5070                         promiseConstructor = jsCast&lt;JSPromiseConstructor*&gt;(cachedFunction);
5071                     }
5072                 }
5073                 if (promiseConstructor) {
5074                     addToGraph(Phantom, callee);
5075                     set(VirtualRegister(bytecode.m_dst), addToGraph(NewPromise, OpInfo(m_graph.registerStructure(bytecode.m_isInternalPromise ? globalObject-&gt;internalPromiseStructure() : globalObject-&gt;promiseStructure())), OpInfo(bytecode.m_isInternalPromise)));
5076                     alreadyEmitted = true;
5077                 }
5078             }
5079 
5080             // Derived function case.
5081             if (!alreadyEmitted) {
5082                 JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm);
5083                 if (!function) {
5084                     JSCell* cachedFunction = bytecode.metadata(codeBlock).m_cachedCallee.unvalidatedGet();
5085                     if (cachedFunction
5086                         &amp;&amp; cachedFunction != JSCell::seenMultipleCalleeObjects()
5087                         &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5088                         ASSERT(cachedFunction-&gt;inherits&lt;JSFunction&gt;(*m_vm));
5089 
5090                         FrozenValue* frozen = m_graph.freeze(cachedFunction);
5091                         addToGraph(CheckCell, OpInfo(frozen), callee);
5092 
5093                         function = static_cast&lt;JSFunction*&gt;(cachedFunction);
5094                     }
5095                 }
5096 
5097                 if (function) {
5098                     if (FunctionRareData* rareData = function-&gt;rareData()) {
5099                         if (rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
5100                             Structure* structure = rareData-&gt;internalFunctionAllocationStructure();
5101                             if (structure
5102                                 &amp;&amp; structure-&gt;classInfo() == (bytecode.m_isInternalPromise ? JSInternalPromise::info() : JSPromise::info())
5103                                 &amp;&amp; structure-&gt;globalObject() == globalObject
5104                                 &amp;&amp; rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
5105                                 m_graph.freeze(rareData);
5106                                 m_graph.watchpoints().addLazily(rareData-&gt;allocationProfileWatchpointSet());
5107 
5108                                 set(VirtualRegister(bytecode.m_dst), addToGraph(NewPromise, OpInfo(m_graph.registerStructure(structure)), OpInfo(bytecode.m_isInternalPromise)));
5109                                 // The callee is still live up to this point.
5110                                 addToGraph(Phantom, callee);
5111                                 alreadyEmitted = true;
5112                             }
5113                         }
5114                     }
5115                 }
5116                 if (!alreadyEmitted)
5117                     set(VirtualRegister(bytecode.m_dst), addToGraph(CreatePromise, OpInfo(), OpInfo(bytecode.m_isInternalPromise), callee));
5118             }
5119             NEXT_OPCODE(op_create_promise);
5120         }
5121 
5122         case op_create_generator: {
5123             handleCreateInternalFieldObject(JSGenerator::info(), CreateGenerator, NewGenerator, currentInstruction-&gt;as&lt;OpCreateGenerator&gt;());
5124             NEXT_OPCODE(op_create_generator);
5125         }
5126 
5127         case op_create_async_generator: {
5128             handleCreateInternalFieldObject(JSAsyncGenerator::info(), CreateAsyncGenerator, NewAsyncGenerator, currentInstruction-&gt;as&lt;OpCreateAsyncGenerator&gt;());
5129             NEXT_OPCODE(op_create_async_generator);
5130         }
5131 
5132         case op_new_object: {
5133             auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
5134             set(bytecode.m_dst,
5135                 addToGraph(NewObject,
5136                     OpInfo(m_graph.registerStructure(bytecode.metadata(codeBlock).m_objectAllocationProfile.structure()))));
5137             NEXT_OPCODE(op_new_object);
5138         }
5139 
5140         case op_new_promise: {
5141             auto bytecode = currentInstruction-&gt;as&lt;OpNewPromise&gt;();
5142             JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
5143             set(bytecode.m_dst, addToGraph(NewPromise, OpInfo(m_graph.registerStructure(bytecode.m_isInternalPromise ? globalObject-&gt;internalPromiseStructure() : globalObject-&gt;promiseStructure())), OpInfo(bytecode.m_isInternalPromise)));
5144             NEXT_OPCODE(op_new_promise);
5145         }
5146 
5147         case op_new_generator: {
5148             auto bytecode = currentInstruction-&gt;as&lt;OpNewGenerator&gt;();
5149             JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
5150             set(bytecode.m_dst, addToGraph(NewGenerator, OpInfo(m_graph.registerStructure(globalObject-&gt;generatorStructure()))));
5151             NEXT_OPCODE(op_new_generator);
5152         }
5153 
5154         case op_new_array: {
5155             auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
5156             int startOperand = bytecode.m_argv.offset();
5157             int numOperands = bytecode.m_argc;
5158             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
5159             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
5160                 addVarArgChild(get(VirtualRegister(operandIdx)));
5161             unsigned vectorLengthHint = std::max&lt;unsigned&gt;(profile.vectorLengthHintConcurrently(), numOperands);
5162             set(bytecode.m_dst, addToGraph(Node::VarArg, NewArray, OpInfo(profile.selectIndexingTypeConcurrently()), OpInfo(vectorLengthHint)));
5163             NEXT_OPCODE(op_new_array);
5164         }
5165 
5166         case op_new_array_with_spread: {
5167             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSpread&gt;();
5168             int startOperand = bytecode.m_argv.offset();
5169             int numOperands = bytecode.m_argc;
5170             const BitVector&amp; bitVector = m_inlineStackTop-&gt;m_profiledBlock-&gt;unlinkedCodeBlock()-&gt;bitVector(bytecode.m_bitVector);
5171             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
5172                 addVarArgChild(get(VirtualRegister(operandIdx)));
5173 
5174             BitVector* copy = m_graph.m_bitVectors.add(bitVector);
5175             ASSERT(*copy == bitVector);
5176 
5177             set(bytecode.m_dst,
5178                 addToGraph(Node::VarArg, NewArrayWithSpread, OpInfo(copy)));
5179             NEXT_OPCODE(op_new_array_with_spread);
5180         }
5181 
5182         case op_spread: {
5183             auto bytecode = currentInstruction-&gt;as&lt;OpSpread&gt;();
5184             set(bytecode.m_dst,
5185                 addToGraph(Spread, get(bytecode.m_argument)));
5186             NEXT_OPCODE(op_spread);
5187         }
5188 
5189         case op_new_array_with_size: {
5190             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
5191             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
5192             set(bytecode.m_dst, addToGraph(NewArrayWithSize, OpInfo(profile.selectIndexingTypeConcurrently()), get(bytecode.m_length)));
5193             NEXT_OPCODE(op_new_array_with_size);
5194         }
5195 
5196         case op_new_array_buffer: {
5197             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayBuffer&gt;();
5198             // Unfortunately, we can&#39;t allocate a new JSImmutableButterfly if the profile tells us new information because we
5199             // cannot allocate from compilation threads.
5200             WTF::loadLoadFence();
5201             FrozenValue* frozen = get(VirtualRegister(bytecode.m_immutableButterfly))-&gt;constant();
5202             WTF::loadLoadFence();
5203             JSImmutableButterfly* immutableButterfly = frozen-&gt;cast&lt;JSImmutableButterfly*&gt;();
5204             NewArrayBufferData data { };
5205             data.indexingMode = immutableButterfly-&gt;indexingMode();
5206             data.vectorLengthHint = immutableButterfly-&gt;toButterfly()-&gt;vectorLength();
5207 
5208             set(VirtualRegister(bytecode.m_dst), addToGraph(NewArrayBuffer, OpInfo(frozen), OpInfo(data.asQuadWord)));
5209             NEXT_OPCODE(op_new_array_buffer);
5210         }
5211 
5212         case op_new_regexp: {
5213             auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
5214             ASSERT(bytecode.m_regexp.isConstant());
5215             FrozenValue* frozenRegExp = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_regexp));
5216             set(bytecode.m_dst, addToGraph(NewRegexp, OpInfo(frozenRegExp), jsConstant(jsNumber(0))));
5217             NEXT_OPCODE(op_new_regexp);
5218         }
5219 
5220         case op_get_rest_length: {
5221             auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
5222             InlineCallFrame* inlineCallFrame = this-&gt;inlineCallFrame();
5223             Node* length;
5224             if (inlineCallFrame &amp;&amp; !inlineCallFrame-&gt;isVarargs()) {
5225                 unsigned argumentsLength = inlineCallFrame-&gt;argumentCountIncludingThis - 1;
5226                 JSValue restLength;
5227                 if (argumentsLength &lt;= bytecode.m_numParametersToSkip)
5228                     restLength = jsNumber(0);
5229                 else
5230                     restLength = jsNumber(argumentsLength - bytecode.m_numParametersToSkip);
5231 
5232                 length = jsConstant(restLength);
5233             } else
5234                 length = addToGraph(GetRestLength, OpInfo(bytecode.m_numParametersToSkip));
5235             set(bytecode.m_dst, length);
5236             NEXT_OPCODE(op_get_rest_length);
5237         }
5238 
5239         case op_create_rest: {
5240             auto bytecode = currentInstruction-&gt;as&lt;OpCreateRest&gt;();
5241             noticeArgumentsUse();
5242             Node* arrayLength = get(bytecode.m_arraySize);
5243             set(bytecode.m_dst,
5244                 addToGraph(CreateRest, OpInfo(bytecode.m_numParametersToSkip), arrayLength));
5245             NEXT_OPCODE(op_create_rest);
5246         }
5247 
5248         // === Bitwise operations ===
5249 
5250         case op_bitnot: {
5251             auto bytecode = currentInstruction-&gt;as&lt;OpBitnot&gt;();
5252             SpeculatedType prediction = getPrediction();
5253             Node* op1 = get(bytecode.m_operand);
5254             if (op1-&gt;hasNumberOrAnyIntResult())
5255                 set(bytecode.m_dst, addToGraph(ArithBitNot, op1));
5256             else
5257                 set(bytecode.m_dst, addToGraph(ValueBitNot, OpInfo(), OpInfo(prediction), op1));
5258             NEXT_OPCODE(op_bitnot);
5259         }
5260 
5261         case op_bitand: {
5262             auto bytecode = currentInstruction-&gt;as&lt;OpBitand&gt;();
5263             SpeculatedType prediction = getPrediction();
5264             Node* op1 = get(bytecode.m_lhs);
5265             Node* op2 = get(bytecode.m_rhs);
5266             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5267                 set(bytecode.m_dst, addToGraph(ArithBitAnd, op1, op2));
5268             else
5269                 set(bytecode.m_dst, addToGraph(ValueBitAnd, OpInfo(), OpInfo(prediction), op1, op2));
5270             NEXT_OPCODE(op_bitand);
5271         }
5272 
5273         case op_bitor: {
5274             auto bytecode = currentInstruction-&gt;as&lt;OpBitor&gt;();
5275             SpeculatedType prediction = getPrediction();
5276             Node* op1 = get(bytecode.m_lhs);
5277             Node* op2 = get(bytecode.m_rhs);
5278             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5279                 set(bytecode.m_dst, addToGraph(ArithBitOr, op1, op2));
5280             else
5281                 set(bytecode.m_dst, addToGraph(ValueBitOr, OpInfo(), OpInfo(prediction), op1, op2));
5282             NEXT_OPCODE(op_bitor);
5283         }
5284 
5285         case op_bitxor: {
5286             auto bytecode = currentInstruction-&gt;as&lt;OpBitxor&gt;();
5287             SpeculatedType prediction = getPrediction();
5288             Node* op1 = get(bytecode.m_lhs);
5289             Node* op2 = get(bytecode.m_rhs);
5290             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5291                 set(bytecode.m_dst, addToGraph(ArithBitXor, op1, op2));
5292             else
5293                 set(bytecode.m_dst, addToGraph(ValueBitXor, OpInfo(), OpInfo(prediction), op1, op2));
5294             NEXT_OPCODE(op_bitxor);
5295         }
5296 
5297         case op_rshift: {
5298             auto bytecode = currentInstruction-&gt;as&lt;OpRshift&gt;();
5299             Node* op1 = get(bytecode.m_lhs);
5300             Node* op2 = get(bytecode.m_rhs);
5301             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5302                 set(bytecode.m_dst, addToGraph(ArithBitRShift, op1, op2));
5303             else {
5304                 SpeculatedType prediction = getPredictionWithoutOSRExit();
5305                 set(bytecode.m_dst, addToGraph(ValueBitRShift, OpInfo(), OpInfo(prediction), op1, op2));
5306             }
5307             NEXT_OPCODE(op_rshift);
5308         }
5309 
5310         case op_lshift: {
5311             auto bytecode = currentInstruction-&gt;as&lt;OpLshift&gt;();
5312             Node* op1 = get(bytecode.m_lhs);
5313             Node* op2 = get(bytecode.m_rhs);
5314             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5315                 set(bytecode.m_dst, addToGraph(ArithBitLShift, op1, op2));
5316             else {
5317                 SpeculatedType prediction = getPredictionWithoutOSRExit();
5318                 set(bytecode.m_dst, addToGraph(ValueBitLShift, OpInfo(), OpInfo(prediction), op1, op2));
5319             }
5320             NEXT_OPCODE(op_lshift);
5321         }
5322 
5323         case op_urshift: {
5324             auto bytecode = currentInstruction-&gt;as&lt;OpUrshift&gt;();
5325             Node* op1 = get(bytecode.m_lhs);
5326             Node* op2 = get(bytecode.m_rhs);
5327             set(bytecode.m_dst, addToGraph(BitURShift, op1, op2));
5328             NEXT_OPCODE(op_urshift);
5329         }
5330 
5331         case op_unsigned: {
5332             auto bytecode = currentInstruction-&gt;as&lt;OpUnsigned&gt;();
5333             set(bytecode.m_dst, makeSafe(addToGraph(UInt32ToNumber, get(bytecode.m_operand))));
5334             NEXT_OPCODE(op_unsigned);
5335         }
5336 
5337         // === Increment/Decrement opcodes ===
5338 
5339         case op_inc: {
5340             auto bytecode = currentInstruction-&gt;as&lt;OpInc&gt;();
5341             Node* op = get(bytecode.m_srcDst);
5342             // FIXME: we can replace the Inc by either ArithAdd with m_constantOne or ArithAdd with the equivalent BigInt in many cases.
5343             // For now we only do so in DFGFixupPhase.
5344             // We could probably do it earlier in some cases, but it is not clearly worth the trouble.
5345             set(bytecode.m_srcDst, makeSafe(addToGraph(Inc, op)));
5346             NEXT_OPCODE(op_inc);
5347         }
5348 
5349         case op_dec: {
5350             auto bytecode = currentInstruction-&gt;as&lt;OpDec&gt;();
5351             Node* op = get(bytecode.m_srcDst);
5352             // FIXME: we can replace the Inc by either ArithSub with m_constantOne or ArithSub with the equivalent BigInt in many cases.
5353             // For now we only do so in DFGFixupPhase.
5354             // We could probably do it earlier in some cases, but it is not clearly worth the trouble.
5355             set(bytecode.m_srcDst, makeSafe(addToGraph(Dec, op)));
5356             NEXT_OPCODE(op_dec);
5357         }
5358 
5359         // === Arithmetic operations ===
5360 
5361         case op_add: {
5362             auto bytecode = currentInstruction-&gt;as&lt;OpAdd&gt;();
5363             Node* op1 = get(bytecode.m_lhs);
5364             Node* op2 = get(bytecode.m_rhs);
5365             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5366                 set(bytecode.m_dst, makeSafe(addToGraph(ArithAdd, op1, op2)));
5367             else
5368                 set(bytecode.m_dst, makeSafe(addToGraph(ValueAdd, op1, op2)));
5369             NEXT_OPCODE(op_add);
5370         }
5371 
5372         case op_sub: {
5373             auto bytecode = currentInstruction-&gt;as&lt;OpSub&gt;();
5374             Node* op1 = get(bytecode.m_lhs);
5375             Node* op2 = get(bytecode.m_rhs);
5376             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5377                 set(bytecode.m_dst, makeSafe(addToGraph(ArithSub, op1, op2)));
5378             else
5379                 set(bytecode.m_dst, makeSafe(addToGraph(ValueSub, op1, op2)));
5380             NEXT_OPCODE(op_sub);
5381         }
5382 
5383         case op_negate: {
5384             auto bytecode = currentInstruction-&gt;as&lt;OpNegate&gt;();
5385             Node* op1 = get(bytecode.m_operand);
5386             if (op1-&gt;hasNumberResult())
5387                 set(bytecode.m_dst, makeSafe(addToGraph(ArithNegate, op1)));
5388             else
5389                 set(bytecode.m_dst, makeSafe(addToGraph(ValueNegate, op1)));
5390             NEXT_OPCODE(op_negate);
5391         }
5392 
5393         case op_mul: {
5394             // Multiply requires that the inputs are not truncated, unfortunately.
5395             auto bytecode = currentInstruction-&gt;as&lt;OpMul&gt;();
5396             Node* op1 = get(bytecode.m_lhs);
5397             Node* op2 = get(bytecode.m_rhs);
5398             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5399                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMul, op1, op2)));
5400             else
5401                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMul, op1, op2)));
5402             NEXT_OPCODE(op_mul);
5403         }
5404 
5405         case op_mod: {
5406             auto bytecode = currentInstruction-&gt;as&lt;OpMod&gt;();
5407             Node* op1 = get(bytecode.m_lhs);
5408             Node* op2 = get(bytecode.m_rhs);
5409             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5410                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMod, op1, op2)));
5411             else
5412                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMod, op1, op2)));
5413             NEXT_OPCODE(op_mod);
5414         }
5415 
5416         case op_pow: {
5417             auto bytecode = currentInstruction-&gt;as&lt;OpPow&gt;();
5418             Node* op1 = get(bytecode.m_lhs);
5419             Node* op2 = get(bytecode.m_rhs);
5420             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5421                 set(bytecode.m_dst, addToGraph(ArithPow, op1, op2));
5422             else
5423                 set(bytecode.m_dst, addToGraph(ValuePow, op1, op2));
5424             NEXT_OPCODE(op_pow);
5425         }
5426 
5427         case op_div: {
5428             auto bytecode = currentInstruction-&gt;as&lt;OpDiv&gt;();
5429             Node* op1 = get(bytecode.m_lhs);
5430             Node* op2 = get(bytecode.m_rhs);
5431             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5432                 set(bytecode.m_dst, makeDivSafe(addToGraph(ArithDiv, op1, op2)));
5433             else
5434                 set(bytecode.m_dst, makeDivSafe(addToGraph(ValueDiv, op1, op2)));
5435             NEXT_OPCODE(op_div);
5436         }
5437 
5438         // === Misc operations ===
5439 
5440         case op_debug: {
5441             // This is a nop in the DFG/FTL because when we set a breakpoint in the debugger,
5442             // we will jettison all optimized CodeBlocks that contains the breakpoint.
5443             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
5444             NEXT_OPCODE(op_debug);
5445         }
5446 
5447         case op_mov: {
5448             auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
5449             Node* op = get(bytecode.m_src);
5450             set(bytecode.m_dst, op);
5451             NEXT_OPCODE(op_mov);
5452         }
5453 
5454         case op_check_tdz: {
5455             auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
5456             addToGraph(CheckNotEmpty, get(bytecode.m_targetVirtualRegister));
5457             NEXT_OPCODE(op_check_tdz);
5458         }
5459 
5460         case op_overrides_has_instance: {
5461             auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
5462             JSFunction* defaultHasInstanceSymbolFunction = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObjectFor(currentCodeOrigin())-&gt;functionProtoHasInstanceSymbolFunction();
5463 
5464             Node* constructor = get(VirtualRegister(bytecode.m_constructor));
5465             Node* hasInstanceValue = get(VirtualRegister(bytecode.m_hasInstanceValue));
5466 
5467             set(VirtualRegister(bytecode.m_dst), addToGraph(OverridesHasInstance, OpInfo(m_graph.freeze(defaultHasInstanceSymbolFunction)), constructor, hasInstanceValue));
5468             NEXT_OPCODE(op_overrides_has_instance);
5469         }
5470 
5471         case op_identity_with_profile: {
5472             auto bytecode = currentInstruction-&gt;as&lt;OpIdentityWithProfile&gt;();
5473             Node* srcDst = get(bytecode.m_srcDst);
5474             SpeculatedType speculation = static_cast&lt;SpeculatedType&gt;(bytecode.m_topProfile) &lt;&lt; 32 | static_cast&lt;SpeculatedType&gt;(bytecode.m_bottomProfile);
5475             set(bytecode.m_srcDst, addToGraph(IdentityWithProfile, OpInfo(speculation), srcDst));
5476             NEXT_OPCODE(op_identity_with_profile);
5477         }
5478 
5479         case op_instanceof: {
5480             auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
5481 
5482             InstanceOfStatus status = InstanceOfStatus::computeFor(
5483                 m_inlineStackTop-&gt;m_profiledBlock, m_inlineStackTop-&gt;m_baselineMap,
5484                 m_currentIndex);
5485 
5486             Node* value = get(bytecode.m_value);
5487             Node* prototype = get(bytecode.m_prototype);
5488 
5489             // Only inline it if it&#39;s Simple with a commonPrototype; bottom/top or variable
5490             // prototypes both get handled by the IC. This makes sense for bottom (unprofiled)
5491             // instanceof ICs because the profit of this optimization is fairly low. So, in the
5492             // absence of any information, it&#39;s better to avoid making this be the cause of a
5493             // recompilation.
5494             if (JSObject* commonPrototype = status.commonPrototype()) {
5495                 addToGraph(CheckCell, OpInfo(m_graph.freeze(commonPrototype)), prototype);
5496 
5497                 bool allOK = true;
5498                 MatchStructureData* data = m_graph.m_matchStructureData.add();
5499                 for (const InstanceOfVariant&amp; variant : status.variants()) {
5500                     if (!check(variant.conditionSet())) {
5501                         allOK = false;
5502                         break;
5503                     }
5504                     for (Structure* structure : variant.structureSet()) {
5505                         MatchStructureVariant matchVariant;
5506                         matchVariant.structure = m_graph.registerStructure(structure);
5507                         matchVariant.result = variant.isHit();
5508 
5509                         data-&gt;variants.append(WTFMove(matchVariant));
5510                     }
5511                 }
5512 
5513                 if (allOK) {
5514                     Node* match = addToGraph(MatchStructure, OpInfo(data), value);
5515                     set(bytecode.m_dst, match);
5516                     NEXT_OPCODE(op_instanceof);
5517                 }
5518             }
5519 
5520             set(bytecode.m_dst, addToGraph(InstanceOf, value, prototype));
5521             NEXT_OPCODE(op_instanceof);
5522         }
5523 
5524         case op_instanceof_custom: {
5525             auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
5526             Node* value = get(bytecode.m_value);
5527             Node* constructor = get(bytecode.m_constructor);
5528             Node* hasInstanceValue = get(bytecode.m_hasInstanceValue);
5529             set(bytecode.m_dst, addToGraph(InstanceOfCustom, value, constructor, hasInstanceValue));
5530             NEXT_OPCODE(op_instanceof_custom);
5531         }
5532         case op_is_empty: {
5533             auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
5534             Node* value = get(bytecode.m_operand);
5535             set(bytecode.m_dst, addToGraph(IsEmpty, value));
5536             NEXT_OPCODE(op_is_empty);
5537         }
5538         case op_is_undefined: {
5539             auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
5540             Node* value = get(bytecode.m_operand);
5541             set(bytecode.m_dst, addToGraph(IsUndefined, value));
5542             NEXT_OPCODE(op_is_undefined);
5543         }
5544         case op_is_undefined_or_null: {
5545             auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
5546             Node* value = get(bytecode.m_operand);
5547             set(bytecode.m_dst, addToGraph(IsUndefinedOrNull, value));
5548             NEXT_OPCODE(op_is_undefined_or_null);
5549         }
5550 
5551         case op_is_boolean: {
5552             auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
5553             Node* value = get(bytecode.m_operand);
5554             set(bytecode.m_dst, addToGraph(IsBoolean, value));
5555             NEXT_OPCODE(op_is_boolean);
5556         }
5557 
5558         case op_is_number: {
5559             auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
5560             Node* value = get(bytecode.m_operand);
5561             set(bytecode.m_dst, addToGraph(IsNumber, value));
5562             NEXT_OPCODE(op_is_number);
5563         }
5564 
5565         case op_is_cell_with_type: {
5566             auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
5567             Node* value = get(bytecode.m_operand);
5568             set(bytecode.m_dst, addToGraph(IsCellWithType, OpInfo(bytecode.m_type), value));
5569             NEXT_OPCODE(op_is_cell_with_type);
5570         }
5571 
5572         case op_is_object: {
5573             auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
5574             Node* value = get(bytecode.m_operand);
5575             set(bytecode.m_dst, addToGraph(IsObject, value));
5576             NEXT_OPCODE(op_is_object);
5577         }
5578 
5579         case op_is_object_or_null: {
5580             auto bytecode = currentInstruction-&gt;as&lt;OpIsObjectOrNull&gt;();
5581             Node* value = get(bytecode.m_operand);
5582             set(bytecode.m_dst, addToGraph(IsObjectOrNull, value));
5583             NEXT_OPCODE(op_is_object_or_null);
5584         }
5585 
5586         case op_is_function: {
5587             auto bytecode = currentInstruction-&gt;as&lt;OpIsFunction&gt;();
5588             Node* value = get(bytecode.m_operand);
5589             set(bytecode.m_dst, addToGraph(IsFunction, value));
5590             NEXT_OPCODE(op_is_function);
5591         }
5592 
5593         case op_not: {
5594             auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
5595             Node* value = get(bytecode.m_operand);
5596             set(bytecode.m_dst, addToGraph(LogicalNot, value));
5597             NEXT_OPCODE(op_not);
5598         }
5599 
5600         case op_to_primitive: {
5601             auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
5602             Node* value = get(bytecode.m_src);
5603             set(bytecode.m_dst, addToGraph(ToPrimitive, value));
5604             NEXT_OPCODE(op_to_primitive);
5605         }
5606 
5607         case op_to_property_key: {
5608             auto bytecode = currentInstruction-&gt;as&lt;OpToPropertyKey&gt;();
5609             Node* value = get(bytecode.m_src);
5610             set(bytecode.m_dst, addToGraph(ToPropertyKey, value));
5611             NEXT_OPCODE(op_to_property_key);
5612         }
5613 
5614         case op_strcat: {
5615             auto bytecode = currentInstruction-&gt;as&lt;OpStrcat&gt;();
5616             int startOperand = bytecode.m_src.offset();
5617             int numOperands = bytecode.m_count;
5618             const unsigned maxArguments = 3;
5619             Node* operands[AdjacencyList::Size];
5620             unsigned indexInOperands = 0;
5621             for (unsigned i = 0; i &lt; AdjacencyList::Size; ++i)
5622                 operands[i] = 0;
5623             for (int operandIdx = 0; operandIdx &lt; numOperands; ++operandIdx) {
5624                 if (indexInOperands == maxArguments) {
5625                     operands[0] = addToGraph(StrCat, operands[0], operands[1], operands[2]);
5626                     for (unsigned i = 1; i &lt; AdjacencyList::Size; ++i)
5627                         operands[i] = 0;
5628                     indexInOperands = 1;
5629                 }
5630 
5631                 ASSERT(indexInOperands &lt; AdjacencyList::Size);
5632                 ASSERT(indexInOperands &lt; maxArguments);
5633                 operands[indexInOperands++] = get(VirtualRegister(startOperand - operandIdx));
5634             }
5635             set(bytecode.m_dst, addToGraph(StrCat, operands[0], operands[1], operands[2]));
5636             NEXT_OPCODE(op_strcat);
5637         }
5638 
5639         case op_less: {
5640             auto bytecode = currentInstruction-&gt;as&lt;OpLess&gt;();
5641             Node* op1 = get(bytecode.m_lhs);
5642             Node* op2 = get(bytecode.m_rhs);
5643             set(bytecode.m_dst, addToGraph(CompareLess, op1, op2));
5644             NEXT_OPCODE(op_less);
5645         }
5646 
5647         case op_lesseq: {
5648             auto bytecode = currentInstruction-&gt;as&lt;OpLesseq&gt;();
5649             Node* op1 = get(bytecode.m_lhs);
5650             Node* op2 = get(bytecode.m_rhs);
5651             set(bytecode.m_dst, addToGraph(CompareLessEq, op1, op2));
5652             NEXT_OPCODE(op_lesseq);
5653         }
5654 
5655         case op_greater: {
5656             auto bytecode = currentInstruction-&gt;as&lt;OpGreater&gt;();
5657             Node* op1 = get(bytecode.m_lhs);
5658             Node* op2 = get(bytecode.m_rhs);
5659             set(bytecode.m_dst, addToGraph(CompareGreater, op1, op2));
5660             NEXT_OPCODE(op_greater);
5661         }
5662 
5663         case op_greatereq: {
5664             auto bytecode = currentInstruction-&gt;as&lt;OpGreatereq&gt;();
5665             Node* op1 = get(bytecode.m_lhs);
5666             Node* op2 = get(bytecode.m_rhs);
5667             set(bytecode.m_dst, addToGraph(CompareGreaterEq, op1, op2));
5668             NEXT_OPCODE(op_greatereq);
5669         }
5670 
5671         case op_below: {
5672             auto bytecode = currentInstruction-&gt;as&lt;OpBelow&gt;();
5673             Node* op1 = get(bytecode.m_lhs);
5674             Node* op2 = get(bytecode.m_rhs);
5675             set(bytecode.m_dst, addToGraph(CompareBelow, op1, op2));
5676             NEXT_OPCODE(op_below);
5677         }
5678 
5679         case op_beloweq: {
5680             auto bytecode = currentInstruction-&gt;as&lt;OpBeloweq&gt;();
5681             Node* op1 = get(bytecode.m_lhs);
5682             Node* op2 = get(bytecode.m_rhs);
5683             set(bytecode.m_dst, addToGraph(CompareBelowEq, op1, op2));
5684             NEXT_OPCODE(op_beloweq);
5685         }
5686 
5687         case op_eq: {
5688             auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
5689             Node* op1 = get(bytecode.m_lhs);
5690             Node* op2 = get(bytecode.m_rhs);
5691             set(bytecode.m_dst, addToGraph(CompareEq, op1, op2));
5692             NEXT_OPCODE(op_eq);
5693         }
5694 
5695         case op_eq_null: {
5696             auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
5697             Node* value = get(bytecode.m_operand);
5698             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5699             set(bytecode.m_dst, addToGraph(CompareEq, value, nullConstant));
5700             NEXT_OPCODE(op_eq_null);
5701         }
5702 
5703         case op_stricteq: {
5704             auto bytecode = currentInstruction-&gt;as&lt;OpStricteq&gt;();
5705             Node* op1 = get(bytecode.m_lhs);
5706             Node* op2 = get(bytecode.m_rhs);
5707             set(bytecode.m_dst, addToGraph(CompareStrictEq, op1, op2));
5708             NEXT_OPCODE(op_stricteq);
5709         }
5710 
5711         case op_neq: {
5712             auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
5713             Node* op1 = get(bytecode.m_lhs);
5714             Node* op2 = get(bytecode.m_rhs);
5715             set(bytecode.m_dst, addToGraph(LogicalNot, addToGraph(CompareEq, op1, op2)));
5716             NEXT_OPCODE(op_neq);
5717         }
5718 
5719         case op_neq_null: {
5720             auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
5721             Node* value = get(bytecode.m_operand);
5722             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5723             set(bytecode.m_dst, addToGraph(LogicalNot, addToGraph(CompareEq, value, nullConstant)));
5724             NEXT_OPCODE(op_neq_null);
5725         }
5726 
5727         case op_nstricteq: {
5728             auto bytecode = currentInstruction-&gt;as&lt;OpNstricteq&gt;();
5729             Node* op1 = get(bytecode.m_lhs);
5730             Node* op2 = get(bytecode.m_rhs);
5731             Node* invertedResult;
5732             invertedResult = addToGraph(CompareStrictEq, op1, op2);
5733             set(bytecode.m_dst, addToGraph(LogicalNot, invertedResult));
5734             NEXT_OPCODE(op_nstricteq);
5735         }
5736 
5737         // === Property access operations ===
5738 
5739         case op_get_by_val: {
5740             auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
5741             SpeculatedType prediction = getPredictionWithoutOSRExit();
5742 
5743             Node* base = get(bytecode.m_base);
5744             Node* property = get(bytecode.m_property);
5745             bool shouldCompileAsGetById = false;
5746             GetByStatus getByStatus = GetByStatus::computeFor(m_inlineStackTop-&gt;m_profiledBlock, m_inlineStackTop-&gt;m_baselineMap, m_icContextStack, currentCodeOrigin());
5747 
5748             unsigned identifierNumber = 0;
5749 
5750             if (!m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
5751                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
5752                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5753 
5754                 // FIXME: In the future, we should be able to do something like MultiGetByOffset in a multi identifier mode.
5755                 // That way, we could both switch on multiple structures and multiple identifiers (or int 32 properties).
5756                 // https://bugs.webkit.org/show_bug.cgi?id=204216
5757                 if (CacheableIdentifier identifier = getByStatus.singleIdentifier()) {
5758                     UniquedStringImpl* uid = identifier.uid();
5759                     identifierNumber = m_graph.identifiers().ensure(identifier.uid());
5760                     if (identifier.isCell()) {
5761                         FrozenValue* frozen = m_graph.freezeStrong(identifier.cell());
5762                         if (identifier.isSymbolCell())
5763                             addToGraph(CheckCell, OpInfo(frozen), property);
5764                         else
5765                             addToGraph(CheckIdent, OpInfo(uid), property);
5766                     } else
5767                         addToGraph(CheckIdent, OpInfo(uid), property);
5768                     shouldCompileAsGetById = true;
5769                 }
5770             }
5771 
5772             if (shouldCompileAsGetById)
5773                 handleGetById(bytecode.m_dst, prediction, base, identifierNumber, getByStatus, AccessType::GetById, currentInstruction-&gt;size());
5774             else {
5775                 ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
5776                 // FIXME: We could consider making this not vararg, since it only uses three child
5777                 // slots.
5778                 // https://bugs.webkit.org/show_bug.cgi?id=184192
5779                 addVarArgChild(base);
5780                 addVarArgChild(property);
5781                 addVarArgChild(0); // Leave room for property storage.
5782                 Node* getByVal = addToGraph(Node::VarArg, GetByVal, OpInfo(arrayMode.asWord()), OpInfo(prediction));
5783                 m_exitOK = false; // GetByVal must be treated as if it clobbers exit state, since FixupPhase may make it generic.
5784                 set(bytecode.m_dst, getByVal);
5785                 if (getByStatus.observedStructureStubInfoSlowPath() || bytecode.metadata(codeBlock).m_seenIdentifiers.count() &gt; Options::getByValICMaxNumberOfIdentifiers())
5786                     m_graph.m_slowGetByVal.add(getByVal);
5787             }
5788 
5789             NEXT_OPCODE(op_get_by_val);
5790         }
5791 
5792         case op_get_by_val_with_this: {
5793             auto bytecode = currentInstruction-&gt;as&lt;OpGetByValWithThis&gt;();
5794             SpeculatedType prediction = getPrediction();
5795 
5796             Node* base = get(bytecode.m_base);
5797             Node* thisValue = get(bytecode.m_thisValue);
5798             Node* property = get(bytecode.m_property);
5799             Node* getByValWithThis = addToGraph(GetByValWithThis, OpInfo(), OpInfo(prediction), base, thisValue, property);
5800             set(bytecode.m_dst, getByValWithThis);
5801 
5802             NEXT_OPCODE(op_get_by_val_with_this);
5803         }
5804 
5805         case op_put_by_val_direct:
5806             handlePutByVal(currentInstruction-&gt;as&lt;OpPutByValDirect&gt;(), currentInstruction-&gt;size());
5807             NEXT_OPCODE(op_put_by_val_direct);
5808 
5809         case op_put_by_val: {
5810             handlePutByVal(currentInstruction-&gt;as&lt;OpPutByVal&gt;(), currentInstruction-&gt;size());
5811             NEXT_OPCODE(op_put_by_val);
5812         }
5813 
5814         case op_put_by_val_with_this: {
5815             auto bytecode = currentInstruction-&gt;as&lt;OpPutByValWithThis&gt;();
5816             Node* base = get(bytecode.m_base);
5817             Node* thisValue = get(bytecode.m_thisValue);
5818             Node* property = get(bytecode.m_property);
5819             Node* value = get(bytecode.m_value);
5820 
5821             addVarArgChild(base);
5822             addVarArgChild(thisValue);
5823             addVarArgChild(property);
5824             addVarArgChild(value);
5825             addToGraph(Node::VarArg, PutByValWithThis, OpInfo(0), OpInfo(0));
5826 
5827             NEXT_OPCODE(op_put_by_val_with_this);
5828         }
5829 
5830         case op_define_data_property: {
5831             auto bytecode = currentInstruction-&gt;as&lt;OpDefineDataProperty&gt;();
5832             Node* base = get(bytecode.m_base);
5833             Node* property = get(bytecode.m_property);
5834             Node* value = get(bytecode.m_value);
5835             Node* attributes = get(bytecode.m_attributes);
5836 
5837             addVarArgChild(base);
5838             addVarArgChild(property);
5839             addVarArgChild(value);
5840             addVarArgChild(attributes);
5841             addToGraph(Node::VarArg, DefineDataProperty, OpInfo(0), OpInfo(0));
5842 
5843             NEXT_OPCODE(op_define_data_property);
5844         }
5845 
5846         case op_define_accessor_property: {
5847             auto bytecode = currentInstruction-&gt;as&lt;OpDefineAccessorProperty&gt;();
5848             Node* base = get(bytecode.m_base);
5849             Node* property = get(bytecode.m_property);
5850             Node* getter = get(bytecode.m_getter);
5851             Node* setter = get(bytecode.m_setter);
5852             Node* attributes = get(bytecode.m_attributes);
5853 
5854             addVarArgChild(base);
5855             addVarArgChild(property);
5856             addVarArgChild(getter);
5857             addVarArgChild(setter);
5858             addVarArgChild(attributes);
5859             addToGraph(Node::VarArg, DefineAccessorProperty, OpInfo(0), OpInfo(0));
5860 
5861             NEXT_OPCODE(op_define_accessor_property);
5862         }
5863 
5864         case op_get_by_id_direct: {
5865             parseGetById&lt;OpGetByIdDirect&gt;(currentInstruction);
5866             NEXT_OPCODE(op_get_by_id_direct);
5867         }
5868         case op_try_get_by_id: {
5869             parseGetById&lt;OpTryGetById&gt;(currentInstruction);
5870             NEXT_OPCODE(op_try_get_by_id);
5871         }
5872         case op_get_by_id: {
5873             parseGetById&lt;OpGetById&gt;(currentInstruction);
5874             NEXT_OPCODE(op_get_by_id);
5875         }
5876         case op_get_by_id_with_this: {
5877             SpeculatedType prediction = getPrediction();
5878 
5879             auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
5880             Node* base = get(bytecode.m_base);
5881             Node* thisValue = get(bytecode.m_thisValue);
5882             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5883 
5884             set(bytecode.m_dst,
5885                 addToGraph(GetByIdWithThis, OpInfo(identifierNumber), OpInfo(prediction), base, thisValue));
5886 
5887             NEXT_OPCODE(op_get_by_id_with_this);
5888         }
5889         case op_put_by_id: {
5890             auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
5891             Node* value = get(bytecode.m_value);
5892             Node* base = get(bytecode.m_base);
5893             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5894             bool direct = !!(bytecode.m_flags &amp; PutByIdIsDirect);
5895 
5896             PutByIdStatus putByIdStatus = PutByIdStatus::computeFor(
5897                 m_inlineStackTop-&gt;m_profiledBlock,
5898                 m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
5899                 currentCodeOrigin(), m_graph.identifiers()[identifierNumber]);
5900 
5901             handlePutById(base, identifierNumber, value, putByIdStatus, direct, currentInstruction-&gt;size());
5902             NEXT_OPCODE(op_put_by_id);
5903         }
5904 
5905         case op_put_by_id_with_this: {
5906             auto bytecode = currentInstruction-&gt;as&lt;OpPutByIdWithThis&gt;();
5907             Node* base = get(bytecode.m_base);
5908             Node* thisValue = get(bytecode.m_thisValue);
5909             Node* value = get(bytecode.m_value);
5910             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5911 
5912             addToGraph(PutByIdWithThis, OpInfo(identifierNumber), base, thisValue, value);
5913             NEXT_OPCODE(op_put_by_id_with_this);
5914         }
5915 
5916         case op_put_getter_by_id:
5917             handlePutAccessorById(PutGetterById, currentInstruction-&gt;as&lt;OpPutGetterById&gt;());
5918             NEXT_OPCODE(op_put_getter_by_id);
5919         case op_put_setter_by_id: {
5920             handlePutAccessorById(PutSetterById, currentInstruction-&gt;as&lt;OpPutSetterById&gt;());
5921             NEXT_OPCODE(op_put_setter_by_id);
5922         }
5923 
5924         case op_put_getter_setter_by_id: {
5925             auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterSetterById&gt;();
5926             Node* base = get(bytecode.m_base);
5927             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5928             Node* getter = get(bytecode.m_getter);
5929             Node* setter = get(bytecode.m_setter);
5930             addToGraph(PutGetterSetterById, OpInfo(identifierNumber), OpInfo(bytecode.m_attributes), base, getter, setter);
5931             NEXT_OPCODE(op_put_getter_setter_by_id);
5932         }
5933 
5934         case op_put_getter_by_val:
5935             handlePutAccessorByVal(PutGetterByVal, currentInstruction-&gt;as&lt;OpPutGetterByVal&gt;());
5936             NEXT_OPCODE(op_put_getter_by_val);
5937         case op_put_setter_by_val: {
5938             handlePutAccessorByVal(PutSetterByVal, currentInstruction-&gt;as&lt;OpPutSetterByVal&gt;());
5939             NEXT_OPCODE(op_put_setter_by_val);
5940         }
5941 
5942         case op_del_by_id: {
5943             auto bytecode = currentInstruction-&gt;as&lt;OpDelById&gt;();
5944             Node* base = get(bytecode.m_base);
5945             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5946             set(bytecode.m_dst, addToGraph(DeleteById, OpInfo(identifierNumber), base));
5947             NEXT_OPCODE(op_del_by_id);
5948         }
5949 
5950         case op_del_by_val: {
5951             auto bytecode = currentInstruction-&gt;as&lt;OpDelByVal&gt;();
5952             Node* base = get(bytecode.m_base);
5953             Node* key = get(bytecode.m_property);
5954             set(bytecode.m_dst, addToGraph(DeleteByVal, base, key));
5955             NEXT_OPCODE(op_del_by_val);
5956         }
5957 
5958         case op_profile_type: {
5959             auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
5960             auto&amp; metadata = bytecode.metadata(codeBlock);
5961             Node* valueToProfile = get(bytecode.m_targetVirtualRegister);
5962             addToGraph(ProfileType, OpInfo(metadata.m_typeLocation), valueToProfile);
5963             NEXT_OPCODE(op_profile_type);
5964         }
5965 
5966         case op_profile_control_flow: {
5967             auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
5968             BasicBlockLocation* basicBlockLocation = bytecode.metadata(codeBlock).m_basicBlockLocation;
5969             addToGraph(ProfileControlFlow, OpInfo(basicBlockLocation));
5970             NEXT_OPCODE(op_profile_control_flow);
5971         }
5972 
5973         // === Block terminators. ===
5974 
5975         case op_jmp: {
5976             ASSERT(!m_currentBlock-&gt;terminal());
5977             auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
5978             int relativeOffset = jumpTarget(bytecode.m_targetLabel);
5979             addToGraph(Jump, OpInfo(m_currentIndex.offset() + relativeOffset));
5980             if (relativeOffset &lt;= 0)
5981                 flushForTerminal();
5982             LAST_OPCODE(op_jmp);
5983         }
5984 
5985         case op_jtrue: {
5986             auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
5987             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5988             Node* condition = get(bytecode.m_condition);
5989             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
5990             LAST_OPCODE(op_jtrue);
5991         }
5992 
5993         case op_jfalse: {
5994             auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
5995             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5996             Node* condition = get(bytecode.m_condition);
5997             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
5998             LAST_OPCODE(op_jfalse);
5999         }
6000 
6001         case op_jeq_null: {
6002             auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
6003             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6004             Node* value = get(bytecode.m_value);
6005             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
6006             Node* condition = addToGraph(CompareEq, value, nullConstant);
6007             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6008             LAST_OPCODE(op_jeq_null);
6009         }
6010 
6011         case op_jneq_null: {
6012             auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
6013             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6014             Node* value = get(bytecode.m_value);
6015             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
6016             Node* condition = addToGraph(CompareEq, value, nullConstant);
6017             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6018             LAST_OPCODE(op_jneq_null);
6019         }
6020 
6021         case op_jundefined_or_null: {
6022             auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();
6023             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6024             Node* value = get(bytecode.m_value);
6025             Node* condition = addToGraph(IsUndefinedOrNull, value);
6026             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6027             LAST_OPCODE(op_jundefined_or_null);
6028         }
6029 
6030         case op_jnundefined_or_null: {
6031             auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();
6032             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6033             Node* value = get(bytecode.m_value);
6034             Node* condition = addToGraph(IsUndefinedOrNull, value);
6035             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6036             LAST_OPCODE(op_jnundefined_or_null);
6037         }
6038 
6039         case op_jless: {
6040             auto bytecode = currentInstruction-&gt;as&lt;OpJless&gt;();
6041             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6042             Node* op1 = get(bytecode.m_lhs);
6043             Node* op2 = get(bytecode.m_rhs);
6044             Node* condition = addToGraph(CompareLess, op1, op2);
6045             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6046             LAST_OPCODE(op_jless);
6047         }
6048 
6049         case op_jlesseq: {
6050             auto bytecode = currentInstruction-&gt;as&lt;OpJlesseq&gt;();
6051             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6052             Node* op1 = get(bytecode.m_lhs);
6053             Node* op2 = get(bytecode.m_rhs);
6054             Node* condition = addToGraph(CompareLessEq, op1, op2);
6055             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6056             LAST_OPCODE(op_jlesseq);
6057         }
6058 
6059         case op_jgreater: {
6060             auto bytecode = currentInstruction-&gt;as&lt;OpJgreater&gt;();
6061             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6062             Node* op1 = get(bytecode.m_lhs);
6063             Node* op2 = get(bytecode.m_rhs);
6064             Node* condition = addToGraph(CompareGreater, op1, op2);
6065             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6066             LAST_OPCODE(op_jgreater);
6067         }
6068 
6069         case op_jgreatereq: {
6070             auto bytecode = currentInstruction-&gt;as&lt;OpJgreatereq&gt;();
6071             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6072             Node* op1 = get(bytecode.m_lhs);
6073             Node* op2 = get(bytecode.m_rhs);
6074             Node* condition = addToGraph(CompareGreaterEq, op1, op2);
6075             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6076             LAST_OPCODE(op_jgreatereq);
6077         }
6078 
6079         case op_jeq: {
6080             auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
6081             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6082             Node* op1 = get(bytecode.m_lhs);
6083             Node* op2 = get(bytecode.m_rhs);
6084             Node* condition = addToGraph(CompareEq, op1, op2);
6085             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6086             LAST_OPCODE(op_jeq);
6087         }
6088 
6089         case op_jstricteq: {
6090             auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
6091             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6092             Node* op1 = get(bytecode.m_lhs);
6093             Node* op2 = get(bytecode.m_rhs);
6094             Node* condition = addToGraph(CompareStrictEq, op1, op2);
6095             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6096             LAST_OPCODE(op_jstricteq);
6097         }
6098 
6099         case op_jnless: {
6100             auto bytecode = currentInstruction-&gt;as&lt;OpJnless&gt;();
6101             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6102             Node* op1 = get(bytecode.m_lhs);
6103             Node* op2 = get(bytecode.m_rhs);
6104             Node* condition = addToGraph(CompareLess, op1, op2);
6105             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6106             LAST_OPCODE(op_jnless);
6107         }
6108 
6109         case op_jnlesseq: {
6110             auto bytecode = currentInstruction-&gt;as&lt;OpJnlesseq&gt;();
6111             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6112             Node* op1 = get(bytecode.m_lhs);
6113             Node* op2 = get(bytecode.m_rhs);
6114             Node* condition = addToGraph(CompareLessEq, op1, op2);
6115             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6116             LAST_OPCODE(op_jnlesseq);
6117         }
6118 
6119         case op_jngreater: {
6120             auto bytecode = currentInstruction-&gt;as&lt;OpJngreater&gt;();
6121             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6122             Node* op1 = get(bytecode.m_lhs);
6123             Node* op2 = get(bytecode.m_rhs);
6124             Node* condition = addToGraph(CompareGreater, op1, op2);
6125             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6126             LAST_OPCODE(op_jngreater);
6127         }
6128 
6129         case op_jngreatereq: {
6130             auto bytecode = currentInstruction-&gt;as&lt;OpJngreatereq&gt;();
6131             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6132             Node* op1 = get(bytecode.m_lhs);
6133             Node* op2 = get(bytecode.m_rhs);
6134             Node* condition = addToGraph(CompareGreaterEq, op1, op2);
6135             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6136             LAST_OPCODE(op_jngreatereq);
6137         }
6138 
6139         case op_jneq: {
6140             auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
6141             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6142             Node* op1 = get(bytecode.m_lhs);
6143             Node* op2 = get(bytecode.m_rhs);
6144             Node* condition = addToGraph(CompareEq, op1, op2);
6145             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6146             LAST_OPCODE(op_jneq);
6147         }
6148 
6149         case op_jnstricteq: {
6150             auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
6151             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6152             Node* op1 = get(bytecode.m_lhs);
6153             Node* op2 = get(bytecode.m_rhs);
6154             Node* condition = addToGraph(CompareStrictEq, op1, op2);
6155             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6156             LAST_OPCODE(op_jnstricteq);
6157         }
6158 
6159         case op_jbelow: {
6160             auto bytecode = currentInstruction-&gt;as&lt;OpJbelow&gt;();
6161             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6162             Node* op1 = get(bytecode.m_lhs);
6163             Node* op2 = get(bytecode.m_rhs);
6164             Node* condition = addToGraph(CompareBelow, op1, op2);
6165             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6166             LAST_OPCODE(op_jbelow);
6167         }
6168 
6169         case op_jbeloweq: {
6170             auto bytecode = currentInstruction-&gt;as&lt;OpJbeloweq&gt;();
6171             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6172             Node* op1 = get(bytecode.m_lhs);
6173             Node* op2 = get(bytecode.m_rhs);
6174             Node* condition = addToGraph(CompareBelowEq, op1, op2);
6175             addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + relativeOffset, m_currentIndex.offset() + currentInstruction-&gt;size())), condition);
6176             LAST_OPCODE(op_jbeloweq);
6177         }
6178 
6179         case op_switch_imm: {
6180             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
6181             SwitchData&amp; data = *m_graph.m_switchData.add();
6182             data.kind = SwitchImm;
6183             data.switchTableIndex = m_inlineStackTop-&gt;m_switchRemap[bytecode.m_tableIndex];
6184             data.fallThrough.setBytecodeIndex(m_currentIndex.offset() + jumpTarget(bytecode.m_defaultOffset));
6185             SimpleJumpTable&amp; table = m_codeBlock-&gt;switchJumpTable(data.switchTableIndex);
6186             for (unsigned i = 0; i &lt; table.branchOffsets.size(); ++i) {
6187                 if (!table.branchOffsets[i])
6188                     continue;
6189                 unsigned target = m_currentIndex.offset() + table.branchOffsets[i];
6190                 if (target == data.fallThrough.bytecodeIndex())
6191                     continue;
6192                 data.cases.append(SwitchCase::withBytecodeIndex(m_graph.freeze(jsNumber(static_cast&lt;int32_t&gt;(table.min + i))), target));
6193             }
6194             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
6195             flushIfTerminal(data);
6196             LAST_OPCODE(op_switch_imm);
6197         }
6198 
6199         case op_switch_char: {
6200             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
6201             SwitchData&amp; data = *m_graph.m_switchData.add();
6202             data.kind = SwitchChar;
6203             data.switchTableIndex = m_inlineStackTop-&gt;m_switchRemap[bytecode.m_tableIndex];
6204             data.fallThrough.setBytecodeIndex(m_currentIndex.offset() + jumpTarget(bytecode.m_defaultOffset));
6205             SimpleJumpTable&amp; table = m_codeBlock-&gt;switchJumpTable(data.switchTableIndex);
6206             for (unsigned i = 0; i &lt; table.branchOffsets.size(); ++i) {
6207                 if (!table.branchOffsets[i])
6208                     continue;
6209                 unsigned target = m_currentIndex.offset() + table.branchOffsets[i];
6210                 if (target == data.fallThrough.bytecodeIndex())
6211                     continue;
6212                 data.cases.append(
6213                     SwitchCase::withBytecodeIndex(LazyJSValue::singleCharacterString(table.min + i), target));
6214             }
6215             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
6216             flushIfTerminal(data);
6217             LAST_OPCODE(op_switch_char);
6218         }
6219 
6220         case op_switch_string: {
6221             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
6222             SwitchData&amp; data = *m_graph.m_switchData.add();
6223             data.kind = SwitchString;
6224             data.switchTableIndex = bytecode.m_tableIndex;
6225             data.fallThrough.setBytecodeIndex(m_currentIndex.offset() + jumpTarget(bytecode.m_defaultOffset));
6226             StringJumpTable&amp; table = m_codeBlock-&gt;stringSwitchJumpTable(data.switchTableIndex);
6227             StringJumpTable::StringOffsetTable::iterator iter;
6228             StringJumpTable::StringOffsetTable::iterator end = table.offsetTable.end();
6229             for (iter = table.offsetTable.begin(); iter != end; ++iter) {
6230                 unsigned target = m_currentIndex.offset() + iter-&gt;value.branchOffset;
6231                 if (target == data.fallThrough.bytecodeIndex())
6232                     continue;
6233                 data.cases.append(
6234                     SwitchCase::withBytecodeIndex(LazyJSValue::knownStringImpl(iter-&gt;key.get()), target));
6235             }
6236             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
6237             flushIfTerminal(data);
6238             LAST_OPCODE(op_switch_string);
6239         }
6240 
6241         case op_ret: {
6242             auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
6243             ASSERT(!m_currentBlock-&gt;terminal());
6244             if (!inlineCallFrame()) {
6245                 // Simple case: we are just producing a return
6246                 addToGraph(Return, get(bytecode.m_value));
6247                 flushForReturn();
6248                 LAST_OPCODE(op_ret);
6249             }
6250 
6251             flushForReturn();
6252             if (m_inlineStackTop-&gt;m_returnValue.isValid())
6253                 setDirect(m_inlineStackTop-&gt;m_returnValue, get(bytecode.m_value), ImmediateSetWithFlush);
6254 
6255             if (!m_inlineStackTop-&gt;m_continuationBlock &amp;&amp; m_currentIndex.offset() + currentInstruction-&gt;size() != m_inlineStackTop-&gt;m_codeBlock-&gt;instructions().size()) {
6256                 // This is an early return from an inlined function and we do not have a continuation block, so we must allocate one.
6257                 // It is untargetable, because we do not know the appropriate index.
6258                 // If this block turns out to be a jump target, parseCodeBlock will fix its bytecodeIndex before putting it in m_blockLinkingTargets
6259                 m_inlineStackTop-&gt;m_continuationBlock = allocateUntargetableBlock();
6260             }
6261 
6262             if (m_inlineStackTop-&gt;m_continuationBlock)
6263                 addJumpTo(m_inlineStackTop-&gt;m_continuationBlock);
6264             else {
6265                 // We are returning from an inlined function, and do not need to jump anywhere, so we just keep the current block
6266                 m_inlineStackTop-&gt;m_continuationBlock = m_currentBlock;
6267             }
6268             LAST_OPCODE_LINKED(op_ret);
6269         }
6270         case op_end:
6271             ASSERT(!inlineCallFrame());
6272             addToGraph(Return, get(currentInstruction-&gt;as&lt;OpEnd&gt;().m_value));
6273             flushForReturn();
6274             LAST_OPCODE(op_end);
6275 
6276         case op_throw:
6277             addToGraph(Throw, get(currentInstruction-&gt;as&lt;OpThrow&gt;().m_value));
6278             flushForTerminal();
6279             LAST_OPCODE(op_throw);
6280 
6281         case op_throw_static_error: {
6282             auto bytecode = currentInstruction-&gt;as&lt;OpThrowStaticError&gt;();
6283             addToGraph(ThrowStaticError, OpInfo(bytecode.m_errorType), get(bytecode.m_message));
6284             flushForTerminal();
6285             LAST_OPCODE(op_throw_static_error);
6286         }
6287 
6288         case op_catch: {
6289             auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
6290             m_graph.m_hasExceptionHandlers = true;
6291 
6292             if (inlineCallFrame()) {
6293                 // We can&#39;t do OSR entry into an inlined frame.
6294                 NEXT_OPCODE(op_catch);
6295             }
6296 
6297             if (m_graph.m_plan.mode() == FTLForOSREntryMode) {
6298                 NEXT_OPCODE(op_catch);
6299             }
6300 
6301             RELEASE_ASSERT(!m_currentBlock-&gt;size() || (m_graph.compilation() &amp;&amp; m_currentBlock-&gt;size() == 1 &amp;&amp; m_currentBlock-&gt;at(0)-&gt;op() == CountExecution));
6302 
6303             ValueProfileAndVirtualRegisterBuffer* buffer = bytecode.metadata(codeBlock).m_buffer;
6304 
6305             if (!buffer) {
6306                 NEXT_OPCODE(op_catch); // This catch has yet to execute. Note: this load can be racy with the main thread.
6307             }
6308 
6309             // We&#39;re now committed to compiling this as an entrypoint.
6310             m_currentBlock-&gt;isCatchEntrypoint = true;
6311             m_graph.m_roots.append(m_currentBlock);
6312 
6313             Vector&lt;SpeculatedType&gt; argumentPredictions(m_numArguments);
6314             Vector&lt;SpeculatedType&gt; localPredictions;
6315             HashSet&lt;unsigned, WTF::IntHash&lt;unsigned&gt;, WTF::UnsignedWithZeroKeyHashTraits&lt;unsigned&gt;&gt; seenArguments;
6316 
6317             {
6318                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6319 
6320                 buffer-&gt;forEach([&amp;] (ValueProfileAndVirtualRegister&amp; profile) {
6321                     VirtualRegister operand(profile.m_operand);
6322                     SpeculatedType prediction = profile.computeUpdatedPrediction(locker);
6323                     if (operand.isLocal())
6324                         localPredictions.append(prediction);
6325                     else {
6326                         RELEASE_ASSERT(operand.isArgument());
6327                         RELEASE_ASSERT(static_cast&lt;uint32_t&gt;(operand.toArgument()) &lt; argumentPredictions.size());
6328                         if (validationEnabled())
6329                             seenArguments.add(operand.toArgument());
6330                         argumentPredictions[operand.toArgument()] = prediction;
6331                     }
6332                 });
6333 
6334                 if (validationEnabled()) {
6335                     for (unsigned argument = 0; argument &lt; m_numArguments; ++argument)
6336                         RELEASE_ASSERT(seenArguments.contains(argument));
6337                 }
6338             }
6339 
6340             Vector&lt;std::pair&lt;VirtualRegister, Node*&gt;&gt; localsToSet;
6341             localsToSet.reserveInitialCapacity(buffer-&gt;m_size); // Note: This will reserve more than the number of locals we see below because the buffer includes arguments.
6342 
6343             // We&#39;re not allowed to exit here since we would not properly recover values.
6344             // We first need to bootstrap the catch entrypoint state.
6345             m_exitOK = false;
6346 
6347             unsigned numberOfLocals = 0;
6348             buffer-&gt;forEach([&amp;] (ValueProfileAndVirtualRegister&amp; profile) {
6349                 VirtualRegister operand(profile.m_operand);
6350                 if (operand.isArgument())
6351                     return;
6352                 ASSERT(operand.isLocal());
6353                 Node* value = addToGraph(ExtractCatchLocal, OpInfo(numberOfLocals), OpInfo(localPredictions[numberOfLocals]));
6354                 ++numberOfLocals;
6355                 addToGraph(MovHint, OpInfo(operand), value);
6356                 localsToSet.uncheckedAppend(std::make_pair(operand, value));
6357             });
6358             if (numberOfLocals)
6359                 addToGraph(ClearCatchLocals);
6360 
6361             if (!m_graph.m_maxLocalsForCatchOSREntry)
6362                 m_graph.m_maxLocalsForCatchOSREntry = 0;
6363             m_graph.m_maxLocalsForCatchOSREntry = std::max(numberOfLocals, *m_graph.m_maxLocalsForCatchOSREntry);
6364 
6365             // We could not exit before this point in the program because we would not know how to do value
6366             // recovery for live locals. The above IR sets up the necessary state so we can recover values
6367             // during OSR exit.
6368             //
6369             // The nodes that follow here all exit to the following bytecode instruction, not
6370             // the op_catch. Exiting to op_catch is reserved for when an exception is thrown.
6371             // The SetArgumentDefinitely nodes that follow below may exit because we may hoist type checks
6372             // to them. The SetLocal nodes that follow below may exit because we may choose
6373             // a flush format that speculates on the type of the local.
6374             m_exitOK = true;
6375             addToGraph(ExitOK);
6376 
6377             {
6378                 auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
6379                 RELEASE_ASSERT(addResult.isNewEntry);
6380                 ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
6381                 entrypointArguments.resize(m_numArguments);
6382 
6383                 BytecodeIndex exitBytecodeIndex = BytecodeIndex(m_currentIndex.offset() + currentInstruction-&gt;size());
6384 
6385                 for (unsigned argument = 0; argument &lt; argumentPredictions.size(); ++argument) {
6386                     VariableAccessData* variable = newVariableAccessData(virtualRegisterForArgumentIncludingThis(argument));
6387                     variable-&gt;predict(argumentPredictions[argument]);
6388 
6389                     variable-&gt;mergeStructureCheckHoistingFailed(
6390                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadCache));
6391                     variable-&gt;mergeCheckArrayHoistingFailed(
6392                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadIndexingType));
6393 
6394                     Node* setArgument = addToGraph(SetArgumentDefinitely, OpInfo(variable));
6395                     setArgument-&gt;origin.forExit = CodeOrigin(exitBytecodeIndex, setArgument-&gt;origin.forExit.inlineCallFrame());
6396                     m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
6397                     entrypointArguments[argument] = setArgument;
6398                 }
6399             }
6400 
6401             for (const std::pair&lt;VirtualRegister, Node*&gt;&amp; pair : localsToSet) {
6402                 DelayedSetLocal delayed { currentCodeOrigin(), pair.first, pair.second, ImmediateNakedSet };
6403                 m_setLocalQueue.append(delayed);
6404             }
6405 
6406             NEXT_OPCODE(op_catch);
6407         }
6408 
6409         case op_call:
6410             handleCall&lt;OpCall&gt;(currentInstruction, Call, CallMode::Regular);
6411             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6412             NEXT_OPCODE(op_call);
6413 
6414         case op_tail_call: {
6415             flushForReturn();
6416             Terminality terminality = handleCall&lt;OpTailCall&gt;(currentInstruction, TailCall, CallMode::Tail);
6417             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6418             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6419             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6420             // things up.
6421             if (terminality == NonTerminal)
6422                 NEXT_OPCODE(op_tail_call);
6423             else
6424                 LAST_OPCODE_LINKED(op_tail_call);
6425             // We use LAST_OPCODE_LINKED instead of LAST_OPCODE because if the tail call was optimized, it may now be a jump to a bytecode index in a different InlineStackEntry.
6426         }
6427 
6428         case op_construct:
6429             handleCall&lt;OpConstruct&gt;(currentInstruction, Construct, CallMode::Construct);
6430             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6431             NEXT_OPCODE(op_construct);
6432 
6433         case op_call_varargs: {
6434             handleVarargsCall&lt;OpCallVarargs&gt;(currentInstruction, CallVarargs, CallMode::Regular);
6435             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6436             NEXT_OPCODE(op_call_varargs);
6437         }
6438 
6439         case op_tail_call_varargs: {
6440             flushForReturn();
6441             Terminality terminality = handleVarargsCall&lt;OpTailCallVarargs&gt;(currentInstruction, TailCallVarargs, CallMode::Tail);
6442             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6443             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6444             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6445             // things up.
6446             if (terminality == NonTerminal)
6447                 NEXT_OPCODE(op_tail_call_varargs);
6448             else
6449                 LAST_OPCODE(op_tail_call_varargs);
6450         }
6451 
6452         case op_tail_call_forward_arguments: {
6453             // We need to make sure that we don&#39;t unbox our arguments here since that won&#39;t be
6454             // done by the arguments object creation node as that node may not exist.
6455             noticeArgumentsUse();
6456             flushForReturn();
6457             Terminality terminality = handleVarargsCall&lt;OpTailCallForwardArguments&gt;(currentInstruction, TailCallForwardVarargs, CallMode::Tail);
6458             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6459             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6460             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6461             // things up.
6462             if (terminality == NonTerminal)
6463                 NEXT_OPCODE(op_tail_call_forward_arguments);
6464             else
6465                 LAST_OPCODE(op_tail_call_forward_arguments);
6466         }
6467 
6468         case op_construct_varargs: {
6469             handleVarargsCall&lt;OpConstructVarargs&gt;(currentInstruction, ConstructVarargs, CallMode::Construct);
6470             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6471             NEXT_OPCODE(op_construct_varargs);
6472         }
6473 
6474         case op_call_eval: {
6475             auto bytecode = currentInstruction-&gt;as&lt;OpCallEval&gt;();
6476             int registerOffset = -bytecode.m_argv;
6477             addCall(bytecode.m_dst, CallEval, nullptr, get(bytecode.m_callee), bytecode.m_argc, registerOffset, getPrediction());
6478             NEXT_OPCODE(op_call_eval);
6479         }
6480 
6481         case op_jneq_ptr: {
6482             auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
6483             FrozenValue* frozenPointer = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_specialPointer));
6484             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6485             Node* child = get(bytecode.m_value);
6486             if (bytecode.metadata(codeBlock).m_hasJumped) {
6487                 Node* condition = addToGraph(CompareEqPtr, OpInfo(frozenPointer), child);
6488                 addToGraph(Branch, OpInfo(branchData(m_currentIndex.offset() + currentInstruction-&gt;size(), m_currentIndex.offset() + relativeOffset)), condition);
6489                 LAST_OPCODE(op_jneq_ptr);
6490             }
6491             addToGraph(CheckCell, OpInfo(frozenPointer), child);
6492             NEXT_OPCODE(op_jneq_ptr);
6493         }
6494 
6495         case op_resolve_scope: {
6496             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScope&gt;();
6497             auto&amp; metadata = bytecode.metadata(codeBlock);
6498 
6499             ResolveType resolveType;
6500             unsigned depth;
6501             JSScope* constantScope = nullptr;
6502             JSCell* lexicalEnvironment = nullptr;
6503             SymbolTable* symbolTable = nullptr;
6504             {
6505                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6506                 resolveType = metadata.m_resolveType;
6507                 depth = metadata.m_localScopeDepth;
6508                 switch (resolveType) {
6509                 case GlobalProperty:
6510                 case GlobalVar:
6511                 case GlobalPropertyWithVarInjectionChecks:
6512                 case GlobalVarWithVarInjectionChecks:
6513                 case GlobalLexicalVar:
6514                 case GlobalLexicalVarWithVarInjectionChecks:
6515                     constantScope = metadata.m_constantScope.get();
6516                     break;
6517                 case ModuleVar:
6518                     lexicalEnvironment = metadata.m_lexicalEnvironment.get();
6519                     break;
6520                 case LocalClosureVar:
6521                 case ClosureVar:
6522                 case ClosureVarWithVarInjectionChecks:
6523                     symbolTable = metadata.m_symbolTable.get();
6524                     break;
6525                 default:
6526                     break;
6527                 }
6528             }
6529 
6530             if (needsDynamicLookup(resolveType, op_resolve_scope)) {
6531                 unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6532                 set(bytecode.m_dst, addToGraph(ResolveScope, OpInfo(identifierNumber), get(bytecode.m_scope)));
6533                 NEXT_OPCODE(op_resolve_scope);
6534             }
6535 
6536             // get_from_scope and put_to_scope depend on this watchpoint forcing OSR exit, so they don&#39;t add their own watchpoints.
6537             if (needsVarInjectionChecks(resolveType))
6538                 m_graph.watchpoints().addLazily(m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject()-&gt;varInjectionWatchpoint());
6539 
6540             // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6541             // https://bugs.webkit.org/show_bug.cgi?id=193347
6542             if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6543                 if (resolveType == GlobalProperty || resolveType == GlobalPropertyWithVarInjectionChecks) {
6544                     JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6545                     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6546                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6547                         addToGraph(ForceOSRExit);
6548                 }
6549             }
6550 
6551             switch (resolveType) {
6552             case GlobalProperty:
6553             case GlobalVar:
6554             case GlobalPropertyWithVarInjectionChecks:
6555             case GlobalVarWithVarInjectionChecks:
6556             case GlobalLexicalVar:
6557             case GlobalLexicalVarWithVarInjectionChecks: {
6558                 RELEASE_ASSERT(constantScope);
6559                 RELEASE_ASSERT(constantScope == JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6560                 set(bytecode.m_dst, weakJSConstant(constantScope));
6561                 addToGraph(Phantom, get(bytecode.m_scope));
6562                 break;
6563             }
6564             case ModuleVar: {
6565                 // Since the value of the &quot;scope&quot; virtual register is not used in LLInt / baseline op_resolve_scope with ModuleVar,
6566                 // we need not to keep it alive by the Phantom node.
6567                 // Module environment is already strongly referenced by the CodeBlock.
6568                 set(bytecode.m_dst, weakJSConstant(lexicalEnvironment));
6569                 break;
6570             }
6571             case LocalClosureVar:
6572             case ClosureVar:
6573             case ClosureVarWithVarInjectionChecks: {
6574                 Node* localBase = get(bytecode.m_scope);
6575                 addToGraph(Phantom, localBase); // OSR exit cannot handle resolve_scope on a DCE&#39;d scope.
6576 
6577                 // We have various forms of constant folding here. This is necessary to avoid
6578                 // spurious recompiles in dead-but-foldable code.
6579 
6580                 if (symbolTable) {
6581                     if (JSScope* scope = symbolTable-&gt;singleton().inferredValue()) {
6582                         m_graph.watchpoints().addLazily(symbolTable);
6583                         set(bytecode.m_dst, weakJSConstant(scope));
6584                         break;
6585                     }
6586                 }
6587                 if (JSScope* scope = localBase-&gt;dynamicCastConstant&lt;JSScope*&gt;(*m_vm)) {
6588                     for (unsigned n = depth; n--;)
6589                         scope = scope-&gt;next();
6590                     set(bytecode.m_dst, weakJSConstant(scope));
6591                     break;
6592                 }
6593                 for (unsigned n = depth; n--;)
6594                     localBase = addToGraph(SkipScope, localBase);
6595                 set(bytecode.m_dst, localBase);
6596                 break;
6597             }
6598             case UnresolvedProperty:
6599             case UnresolvedPropertyWithVarInjectionChecks: {
6600                 addToGraph(Phantom, get(bytecode.m_scope));
6601                 addToGraph(ForceOSRExit);
6602                 set(bytecode.m_dst, addToGraph(JSConstant, OpInfo(m_constantNull)));
6603                 break;
6604             }
6605             case Dynamic:
6606                 RELEASE_ASSERT_NOT_REACHED();
6607                 break;
6608             }
6609             NEXT_OPCODE(op_resolve_scope);
6610         }
6611         case op_resolve_scope_for_hoisting_func_decl_in_eval: {
6612             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScopeForHoistingFuncDeclInEval&gt;();
6613             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
6614             set(bytecode.m_dst, addToGraph(ResolveScopeForHoistingFuncDeclInEval, OpInfo(identifierNumber), get(bytecode.m_scope)));
6615 
6616             NEXT_OPCODE(op_resolve_scope_for_hoisting_func_decl_in_eval);
6617         }
6618 
6619         case op_get_from_scope: {
6620             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
6621             auto&amp; metadata = bytecode.metadata(codeBlock);
6622             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6623             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6624 
6625             ResolveType resolveType;
6626             GetPutInfo getPutInfo(0);
6627             Structure* structure = 0;
6628             WatchpointSet* watchpoints = 0;
6629             uintptr_t operand;
6630             {
6631                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6632                 getPutInfo = metadata.m_getPutInfo;
6633                 resolveType = getPutInfo.resolveType();
6634                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks || resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6635                     watchpoints = metadata.m_watchpointSet;
6636                 else if (resolveType != UnresolvedProperty &amp;&amp; resolveType != UnresolvedPropertyWithVarInjectionChecks)
6637                     structure = metadata.m_structure.get();
6638                 operand = metadata.m_operand;
6639             }
6640 
6641             if (needsDynamicLookup(resolveType, op_get_from_scope)) {
6642                 uint64_t opInfo1 = makeDynamicVarOpInfo(identifierNumber, getPutInfo.operand());
6643                 SpeculatedType prediction = getPrediction();
6644                 set(bytecode.m_dst,
6645                     addToGraph(GetDynamicVar, OpInfo(opInfo1), OpInfo(prediction), get(bytecode.m_scope)));
6646                 NEXT_OPCODE(op_get_from_scope);
6647             }
6648 
6649             UNUSED_PARAM(watchpoints); // We will use this in the future. For now we set it as a way of documenting the fact that that&#39;s what index 5 is in GlobalVar mode.
6650 
6651             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6652 
6653             switch (resolveType) {
6654             case GlobalProperty:
6655             case GlobalPropertyWithVarInjectionChecks: {
6656                 // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6657                 // https://bugs.webkit.org/show_bug.cgi?id=193347
6658                 if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6659                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6660                         addToGraph(ForceOSRExit);
6661                 }
6662 
6663                 SpeculatedType prediction = getPrediction();
6664 
6665                 GetByStatus status = GetByStatus::computeFor(structure, uid);
6666                 if (status.state() != GetByStatus::Simple
6667                     || status.numVariants() != 1
6668                     || status[0].structureSet().size() != 1) {
6669                     set(bytecode.m_dst, addToGraph(GetByIdFlush, OpInfo(identifierNumber), OpInfo(prediction), get(bytecode.m_scope)));
6670                     break;
6671                 }
6672 
6673                 Node* base = weakJSConstant(globalObject);
6674                 Node* result = load(prediction, base, identifierNumber, status[0]);
6675                 addToGraph(Phantom, get(bytecode.m_scope));
6676                 set(bytecode.m_dst, result);
6677                 break;
6678             }
6679             case GlobalVar:
6680             case GlobalVarWithVarInjectionChecks:
6681             case GlobalLexicalVar:
6682             case GlobalLexicalVarWithVarInjectionChecks: {
6683                 addToGraph(Phantom, get(bytecode.m_scope));
6684                 WatchpointSet* watchpointSet;
6685                 ScopeOffset offset;
6686                 JSSegmentedVariableObject* scopeObject = jsCast&lt;JSSegmentedVariableObject*&gt;(JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6687                 {
6688                     ConcurrentJSLocker locker(scopeObject-&gt;symbolTable()-&gt;m_lock);
6689                     SymbolTableEntry entry = scopeObject-&gt;symbolTable()-&gt;get(locker, uid);
6690                     watchpointSet = entry.watchpointSet();
6691                     offset = entry.scopeOffset();
6692                 }
6693                 if (watchpointSet &amp;&amp; watchpointSet-&gt;state() == IsWatched) {
6694                     // This has a fun concurrency story. There is the possibility of a race in two
6695                     // directions:
6696                     //
6697                     // We see that the set IsWatched, but in the meantime it gets invalidated: this is
6698                     // fine because if we saw that it IsWatched then we add a watchpoint. If it gets
6699                     // invalidated, then this compilation is invalidated. Note that in the meantime we
6700                     // may load an absurd value from the global object. It&#39;s fine to load an absurd
6701                     // value if the compilation is invalidated anyway.
6702                     //
6703                     // We see that the set IsWatched, but the value isn&#39;t yet initialized: this isn&#39;t
6704                     // possible because of the ordering of operations.
6705                     //
6706                     // Here&#39;s how we order operations:
6707                     //
6708                     // Main thread stores to the global object: always store a value first, and only
6709                     // after that do we touch the watchpoint set. There is a fence in the touch, that
6710                     // ensures that the store to the global object always happens before the touch on the
6711                     // set.
6712                     //
6713                     // Compilation thread: always first load the state of the watchpoint set, and then
6714                     // load the value. The WatchpointSet::state() method does fences for us to ensure
6715                     // that the load of the state happens before our load of the value.
6716                     //
6717                     // Finalizing compilation: this happens on the main thread and synchronously checks
6718                     // validity of all watchpoint sets.
6719                     //
6720                     // We will only perform optimizations if the load of the state yields IsWatched. That
6721                     // means that at least one store would have happened to initialize the original value
6722                     // of the variable (that is, the value we&#39;d like to constant fold to). There may be
6723                     // other stores that happen after that, but those stores will invalidate the
6724                     // watchpoint set and also the compilation.
6725 
6726                     // Note that we need to use the operand, which is a direct pointer at the global,
6727                     // rather than looking up the global by doing variableAt(offset). That&#39;s because the
6728                     // internal data structures of JSSegmentedVariableObject are not thread-safe even
6729                     // though accessing the global itself is. The segmentation involves a vector spine
6730                     // that resizes with malloc/free, so if new globals unrelated to the one we are
6731                     // reading are added, we might access freed memory if we do variableAt().
6732                     WriteBarrier&lt;Unknown&gt;* pointer = bitwise_cast&lt;WriteBarrier&lt;Unknown&gt;*&gt;(operand);
6733 
6734                     ASSERT(scopeObject-&gt;findVariableIndex(pointer) == offset);
6735 
6736                     JSValue value = pointer-&gt;get();
6737                     if (value) {
6738                         m_graph.watchpoints().addLazily(watchpointSet);
6739                         set(bytecode.m_dst, weakJSConstant(value));
6740                         break;
6741                     }
6742                 }
6743 
6744                 SpeculatedType prediction = getPrediction();
6745                 NodeType nodeType;
6746                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks)
6747                     nodeType = GetGlobalVar;
6748                 else
6749                     nodeType = GetGlobalLexicalVariable;
6750                 Node* value = addToGraph(nodeType, OpInfo(operand), OpInfo(prediction));
6751                 if (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6752                     addToGraph(CheckNotEmpty, value);
6753                 set(bytecode.m_dst, value);
6754                 break;
6755             }
6756             case LocalClosureVar:
6757             case ClosureVar:
6758             case ClosureVarWithVarInjectionChecks: {
6759                 Node* scopeNode = get(bytecode.m_scope);
6760 
6761                 // Ideally we wouldn&#39;t have to do this Phantom. But:
6762                 //
6763                 // For the constant case: we must do it because otherwise we would have no way of knowing
6764                 // that the scope is live at OSR here.
6765                 //
6766                 // For the non-constant case: GetClosureVar could be DCE&#39;d, but baseline&#39;s implementation
6767                 // won&#39;t be able to handle an Undefined scope.
6768                 addToGraph(Phantom, scopeNode);
6769 
6770                 // Constant folding in the bytecode parser is important for performance. This may not
6771                 // have executed yet. If it hasn&#39;t, then we won&#39;t have a prediction. Lacking a
6772                 // prediction, we&#39;d otherwise think that it has to exit. Then when it did execute, we
6773                 // would recompile. But if we can fold it here, we avoid the exit.
6774                 if (JSValue value = m_graph.tryGetConstantClosureVar(scopeNode, ScopeOffset(operand))) {
6775                     set(bytecode.m_dst, weakJSConstant(value));
6776                     break;
6777                 }
6778                 SpeculatedType prediction = getPrediction();
6779                 set(bytecode.m_dst,
6780                     addToGraph(GetClosureVar, OpInfo(operand), OpInfo(prediction), scopeNode));
6781                 break;
6782             }
6783             case UnresolvedProperty:
6784             case UnresolvedPropertyWithVarInjectionChecks:
6785             case ModuleVar:
6786             case Dynamic:
6787                 RELEASE_ASSERT_NOT_REACHED();
6788                 break;
6789             }
6790             NEXT_OPCODE(op_get_from_scope);
6791         }
6792 
6793         case op_put_to_scope: {
6794             auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
6795             auto&amp; metadata = bytecode.metadata(codeBlock);
6796             unsigned identifierNumber = bytecode.m_var;
6797             if (identifierNumber != UINT_MAX)
6798                 identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[identifierNumber];
6799             UniquedStringImpl* uid;
6800             if (identifierNumber != UINT_MAX)
6801                 uid = m_graph.identifiers()[identifierNumber];
6802             else
6803                 uid = nullptr;
6804 
6805             ResolveType resolveType;
6806             GetPutInfo getPutInfo(0);
6807             Structure* structure = nullptr;
6808             WatchpointSet* watchpoints = nullptr;
6809             uintptr_t operand;
6810             {
6811                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6812                 getPutInfo = metadata.m_getPutInfo;
6813                 resolveType = getPutInfo.resolveType();
6814                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks || resolveType == LocalClosureVar || resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6815                     watchpoints = metadata.m_watchpointSet;
6816                 else if (resolveType != UnresolvedProperty &amp;&amp; resolveType != UnresolvedPropertyWithVarInjectionChecks)
6817                     structure = metadata.m_structure.get();
6818                 operand = metadata.m_operand;
6819             }
6820 
6821             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6822 
6823             if (needsDynamicLookup(resolveType, op_put_to_scope)) {
6824                 ASSERT(identifierNumber != UINT_MAX);
6825                 uint64_t opInfo1 = makeDynamicVarOpInfo(identifierNumber, getPutInfo.operand());
6826                 addToGraph(PutDynamicVar, OpInfo(opInfo1), OpInfo(), get(bytecode.m_scope), get(bytecode.m_value));
6827                 NEXT_OPCODE(op_put_to_scope);
6828             }
6829 
6830             switch (resolveType) {
6831             case GlobalProperty:
6832             case GlobalPropertyWithVarInjectionChecks: {
6833                 // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6834                 // https://bugs.webkit.org/show_bug.cgi?id=193347
6835                 if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6836                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6837                         addToGraph(ForceOSRExit);
6838                 }
6839 
6840                 PutByIdStatus status;
6841                 if (uid)
6842                     status = PutByIdStatus::computeFor(globalObject, structure, uid, false);
6843                 else
6844                     status = PutByIdStatus(PutByIdStatus::TakesSlowPath);
6845                 if (status.numVariants() != 1
6846                     || status[0].kind() != PutByIdVariant::Replace
6847                     || status[0].structure().size() != 1) {
6848                     addToGraph(PutById, OpInfo(identifierNumber), get(bytecode.m_scope), get(bytecode.m_value));
6849                     break;
6850                 }
6851                 Node* base = weakJSConstant(globalObject);
6852                 store(base, identifierNumber, status[0], get(bytecode.m_value));
6853                 // Keep scope alive until after put.
6854                 addToGraph(Phantom, get(bytecode.m_scope));
6855                 break;
6856             }
6857             case GlobalLexicalVar:
6858             case GlobalLexicalVarWithVarInjectionChecks:
6859             case GlobalVar:
6860             case GlobalVarWithVarInjectionChecks: {
6861                 if (!isInitialization(getPutInfo.initializationMode()) &amp;&amp; (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)) {
6862                     SpeculatedType prediction = SpecEmpty;
6863                     Node* value = addToGraph(GetGlobalLexicalVariable, OpInfo(operand), OpInfo(prediction));
6864                     addToGraph(CheckNotEmpty, value);
6865                 }
6866 
6867                 JSSegmentedVariableObject* scopeObject = jsCast&lt;JSSegmentedVariableObject*&gt;(JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6868                 if (watchpoints) {
6869                     SymbolTableEntry entry = scopeObject-&gt;symbolTable()-&gt;get(uid);
6870                     ASSERT_UNUSED(entry, watchpoints == entry.watchpointSet());
6871                 }
6872                 Node* valueNode = get(bytecode.m_value);
6873                 addToGraph(PutGlobalVariable, OpInfo(operand), weakJSConstant(scopeObject), valueNode);
6874                 if (watchpoints &amp;&amp; watchpoints-&gt;state() != IsInvalidated) {
6875                     // Must happen after the store. See comment for GetGlobalVar.
6876                     addToGraph(NotifyWrite, OpInfo(watchpoints));
6877                 }
6878                 // Keep scope alive until after put.
6879                 addToGraph(Phantom, get(bytecode.m_scope));
6880                 break;
6881             }
6882             case LocalClosureVar:
6883             case ClosureVar:
6884             case ClosureVarWithVarInjectionChecks: {
6885                 Node* scopeNode = get(bytecode.m_scope);
6886                 Node* valueNode = get(bytecode.m_value);
6887 
6888                 addToGraph(PutClosureVar, OpInfo(operand), scopeNode, valueNode);
6889 
6890                 if (watchpoints &amp;&amp; watchpoints-&gt;state() != IsInvalidated) {
6891                     // Must happen after the store. See comment for GetGlobalVar.
6892                     addToGraph(NotifyWrite, OpInfo(watchpoints));
6893                 }
6894                 break;
6895             }
6896 
6897             case ModuleVar:
6898                 // Need not to keep &quot;scope&quot; and &quot;value&quot; register values here by Phantom because
6899                 // they are not used in LLInt / baseline op_put_to_scope with ModuleVar.
6900                 addToGraph(ForceOSRExit);
6901                 break;
6902 
6903             case Dynamic:
6904             case UnresolvedProperty:
6905             case UnresolvedPropertyWithVarInjectionChecks:
6906                 RELEASE_ASSERT_NOT_REACHED();
6907                 break;
6908             }
6909             NEXT_OPCODE(op_put_to_scope);
6910         }
6911 
6912         case op_loop_hint: {
6913             // Baseline-&gt;DFG OSR jumps between loop hints. The DFG assumes that Baseline-&gt;DFG
6914             // OSR can only happen at basic block boundaries. Assert that these two statements
6915             // are compatible.
6916             RELEASE_ASSERT(m_currentIndex == blockBegin);
6917 
6918             // We never do OSR into an inlined code block. That could not happen, since OSR
6919             // looks up the code block that is the replacement for the baseline JIT code
6920             // block. Hence, machine code block = true code block = not inline code block.
6921             if (!m_inlineStackTop-&gt;m_caller)
6922                 m_currentBlock-&gt;isOSRTarget = true;
6923 
6924             addToGraph(LoopHint);
6925             NEXT_OPCODE(op_loop_hint);
6926         }
6927 
6928         case op_check_traps: {
6929             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);
6930             NEXT_OPCODE(op_check_traps);
6931         }
6932 
6933         case op_nop: {
6934             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
6935             NEXT_OPCODE(op_nop);
6936         }
6937 
6938         case op_super_sampler_begin: {
6939             addToGraph(SuperSamplerBegin);
6940             NEXT_OPCODE(op_super_sampler_begin);
6941         }
6942 
6943         case op_super_sampler_end: {
6944             addToGraph(SuperSamplerEnd);
6945             NEXT_OPCODE(op_super_sampler_end);
6946         }
6947 
6948         case op_create_lexical_environment: {
6949             auto bytecode = currentInstruction-&gt;as&lt;OpCreateLexicalEnvironment&gt;();
6950             ASSERT(bytecode.m_symbolTable.isConstant() &amp;&amp; bytecode.m_initialValue.isConstant());
6951             FrozenValue* symbolTable = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_symbolTable));
6952             FrozenValue* initialValue = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_initialValue));
6953             Node* scope = get(bytecode.m_scope);
6954             Node* lexicalEnvironment = addToGraph(CreateActivation, OpInfo(symbolTable), OpInfo(initialValue), scope);
6955             set(bytecode.m_dst, lexicalEnvironment);
6956             NEXT_OPCODE(op_create_lexical_environment);
6957         }
6958 
6959         case op_push_with_scope: {
6960             auto bytecode = currentInstruction-&gt;as&lt;OpPushWithScope&gt;();
6961             Node* currentScope = get(bytecode.m_currentScope);
6962             Node* object = get(bytecode.m_newScope);
6963             set(bytecode.m_dst, addToGraph(PushWithScope, currentScope, object));
6964             NEXT_OPCODE(op_push_with_scope);
6965         }
6966 
6967         case op_get_parent_scope: {
6968             auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
6969             Node* currentScope = get(bytecode.m_scope);
6970             Node* newScope = addToGraph(SkipScope, currentScope);
6971             set(bytecode.m_dst, newScope);
6972             addToGraph(Phantom, currentScope);
6973             NEXT_OPCODE(op_get_parent_scope);
6974         }
6975 
6976         case op_get_scope: {
6977             // Help the later stages a bit by doing some small constant folding here. Note that this
6978             // only helps for the first basic block. It&#39;s extremely important not to constant fold
6979             // loads from the scope register later, as that would prevent the DFG from tracking the
6980             // bytecode-level liveness of the scope register.
6981             auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
6982             Node* callee = get(CallFrameSlot::callee);
6983             Node* result;
6984             if (JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm))
6985                 result = weakJSConstant(function-&gt;scope());
6986             else
6987                 result = addToGraph(GetScope, callee);
6988             set(bytecode.m_dst, result);
6989             NEXT_OPCODE(op_get_scope);
6990         }
6991 
6992         case op_argument_count: {
6993             auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
6994             Node* sub = addToGraph(ArithSub, OpInfo(Arith::Unchecked), OpInfo(SpecInt32Only), getArgumentCount(), addToGraph(JSConstant, OpInfo(m_constantOne)));
6995             set(bytecode.m_dst, sub);
6996             NEXT_OPCODE(op_argument_count);
6997         }
6998 
6999         case op_create_direct_arguments: {
7000             auto bytecode = currentInstruction-&gt;as&lt;OpCreateDirectArguments&gt;();
7001             noticeArgumentsUse();
7002             Node* createArguments = addToGraph(CreateDirectArguments);
7003             set(bytecode.m_dst, createArguments);
7004             NEXT_OPCODE(op_create_direct_arguments);
7005         }
7006 
7007         case op_create_scoped_arguments: {
7008             auto bytecode = currentInstruction-&gt;as&lt;OpCreateScopedArguments&gt;();
7009             noticeArgumentsUse();
7010             Node* createArguments = addToGraph(CreateScopedArguments, get(bytecode.m_scope));
7011             set(bytecode.m_dst, createArguments);
7012             NEXT_OPCODE(op_create_scoped_arguments);
7013         }
7014 
7015         case op_create_cloned_arguments: {
7016             auto bytecode = currentInstruction-&gt;as&lt;OpCreateClonedArguments&gt;();
7017             noticeArgumentsUse();
7018             Node* createArguments = addToGraph(CreateClonedArguments);
7019             set(bytecode.m_dst, createArguments);
7020             NEXT_OPCODE(op_create_cloned_arguments);
7021         }
7022 
7023         case op_create_arguments_butterfly: {
7024             auto bytecode = currentInstruction-&gt;as&lt;OpCreateArgumentsButterfly&gt;();
7025             noticeArgumentsUse();
7026             set(bytecode.m_dst, addToGraph(CreateArgumentsButterfly));
7027             NEXT_OPCODE(op_create_arguments_butterfly);
7028         }
7029 
7030         case op_get_from_arguments: {
7031             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromArguments&gt;();
7032             set(bytecode.m_dst,
7033                 addToGraph(
7034                     GetFromArguments,
7035                     OpInfo(bytecode.m_index),
7036                     OpInfo(getPrediction()),
7037                     get(bytecode.m_arguments)));
7038             NEXT_OPCODE(op_get_from_arguments);
7039         }
7040 
7041         case op_put_to_arguments: {
7042             auto bytecode = currentInstruction-&gt;as&lt;OpPutToArguments&gt;();
7043             addToGraph(
7044                 PutToArguments,
7045                 OpInfo(bytecode.m_index),
7046                 get(bytecode.m_arguments),
7047                 get(bytecode.m_value));
7048             NEXT_OPCODE(op_put_to_arguments);
7049         }
7050 
7051         case op_get_argument: {
7052             auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
7053             InlineCallFrame* inlineCallFrame = this-&gt;inlineCallFrame();
7054             Node* argument;
7055             int32_t argumentIndexIncludingThis = bytecode.m_index;
7056             if (inlineCallFrame &amp;&amp; !inlineCallFrame-&gt;isVarargs()) {
7057                 int32_t argumentCountIncludingThisWithFixup = inlineCallFrame-&gt;argumentsWithFixup.size();
7058                 if (argumentIndexIncludingThis &lt; argumentCountIncludingThisWithFixup)
7059                     argument = get(virtualRegisterForArgumentIncludingThis(argumentIndexIncludingThis));
7060                 else
7061                     argument = addToGraph(JSConstant, OpInfo(m_constantUndefined));
7062             } else
7063                 argument = addToGraph(GetArgument, OpInfo(argumentIndexIncludingThis), OpInfo(getPrediction()));
7064             set(bytecode.m_dst, argument);
7065             NEXT_OPCODE(op_get_argument);
7066         }
7067         case op_new_async_generator_func:
7068             handleNewFunc(NewAsyncGeneratorFunction, currentInstruction-&gt;as&lt;OpNewAsyncGeneratorFunc&gt;());
7069             NEXT_OPCODE(op_new_async_generator_func);
7070         case op_new_func:
7071             handleNewFunc(NewFunction, currentInstruction-&gt;as&lt;OpNewFunc&gt;());
7072             NEXT_OPCODE(op_new_func);
7073         case op_new_generator_func:
7074             handleNewFunc(NewGeneratorFunction, currentInstruction-&gt;as&lt;OpNewGeneratorFunc&gt;());
7075             NEXT_OPCODE(op_new_generator_func);
7076         case op_new_async_func:
7077             handleNewFunc(NewAsyncFunction, currentInstruction-&gt;as&lt;OpNewAsyncFunc&gt;());
7078             NEXT_OPCODE(op_new_async_func);
7079 
7080         case op_new_func_exp:
7081             handleNewFuncExp(NewFunction, currentInstruction-&gt;as&lt;OpNewFuncExp&gt;());
7082             NEXT_OPCODE(op_new_func_exp);
7083         case op_new_generator_func_exp:
7084             handleNewFuncExp(NewGeneratorFunction, currentInstruction-&gt;as&lt;OpNewGeneratorFuncExp&gt;());
7085             NEXT_OPCODE(op_new_generator_func_exp);
7086         case op_new_async_generator_func_exp:
7087             handleNewFuncExp(NewAsyncGeneratorFunction, currentInstruction-&gt;as&lt;OpNewAsyncGeneratorFuncExp&gt;());
7088             NEXT_OPCODE(op_new_async_generator_func_exp);
7089         case op_new_async_func_exp:
7090             handleNewFuncExp(NewAsyncFunction, currentInstruction-&gt;as&lt;OpNewAsyncFuncExp&gt;());
7091             NEXT_OPCODE(op_new_async_func_exp);
7092 
7093         case op_set_function_name: {
7094             auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
7095             Node* func = get(bytecode.m_function);
7096             Node* name = get(bytecode.m_name);
7097             addToGraph(SetFunctionName, func, name);
7098             NEXT_OPCODE(op_set_function_name);
7099         }
7100 
7101         case op_typeof: {
7102             auto bytecode = currentInstruction-&gt;as&lt;OpTypeof&gt;();
7103             set(bytecode.m_dst, addToGraph(TypeOf, get(bytecode.m_value)));
7104             NEXT_OPCODE(op_typeof);
7105         }
7106 
7107         case op_to_number: {
7108             auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
7109             SpeculatedType prediction = getPrediction();
7110             Node* value = get(bytecode.m_operand);
7111             set(bytecode.m_dst, addToGraph(ToNumber, OpInfo(0), OpInfo(prediction), value));
7112             NEXT_OPCODE(op_to_number);
7113         }
7114 
7115         case op_to_numeric: {
7116             auto bytecode = currentInstruction-&gt;as&lt;OpToNumeric&gt;();
7117             SpeculatedType prediction = getPrediction();
7118             Node* value = get(bytecode.m_operand);
7119             set(bytecode.m_dst, addToGraph(ToNumeric, OpInfo(0), OpInfo(prediction), value));
7120             NEXT_OPCODE(op_to_numeric);
7121         }
7122 
7123         case op_to_string: {
7124             auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
7125             Node* value = get(bytecode.m_operand);
7126             set(bytecode.m_dst, addToGraph(ToString, value));
7127             NEXT_OPCODE(op_to_string);
7128         }
7129 
7130         case op_to_object: {
7131             auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
7132             SpeculatedType prediction = getPrediction();
7133             Node* value = get(bytecode.m_operand);
7134             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_message];
7135             set(bytecode.m_dst, addToGraph(ToObject, OpInfo(identifierNumber), OpInfo(prediction), value));
7136             NEXT_OPCODE(op_to_object);
7137         }
7138 
7139         case op_in_by_val: {
7140             auto bytecode = currentInstruction-&gt;as&lt;OpInByVal&gt;();
7141             ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
7142             set(bytecode.m_dst, addToGraph(InByVal, OpInfo(arrayMode.asWord()), get(bytecode.m_base), get(bytecode.m_property)));
7143             NEXT_OPCODE(op_in_by_val);
7144         }
7145 
7146         case op_in_by_id: {
7147             auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
7148             Node* base = get(bytecode.m_base);
7149             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
7150             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
7151 
7152             InByIdStatus status = InByIdStatus::computeFor(
7153                 m_inlineStackTop-&gt;m_profiledBlock,
7154                 m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
7155                 currentCodeOrigin(), uid);
7156 
7157             if (status.isSimple()) {
7158                 bool allOK = true;
7159                 MatchStructureData* data = m_graph.m_matchStructureData.add();
7160                 for (const InByIdVariant&amp; variant : status.variants()) {
7161                     if (!check(variant.conditionSet())) {
7162                         allOK = false;
7163                         break;
7164                     }
7165                     for (Structure* structure : variant.structureSet()) {
7166                         MatchStructureVariant matchVariant;
7167                         matchVariant.structure = m_graph.registerStructure(structure);
7168                         matchVariant.result = variant.isHit();
7169 
7170                         data-&gt;variants.append(WTFMove(matchVariant));
7171                     }
7172                 }
7173 
7174                 if (allOK) {
7175                     addToGraph(FilterInByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addInByIdStatus(currentCodeOrigin(), status)), base);
7176 
7177                     Node* match = addToGraph(MatchStructure, OpInfo(data), base);
7178                     set(bytecode.m_dst, match);
7179                     NEXT_OPCODE(op_in_by_id);
7180                 }
7181             }
7182 
7183             set(bytecode.m_dst, addToGraph(InById, OpInfo(identifierNumber), base));
7184             NEXT_OPCODE(op_in_by_id);
7185         }
7186 
7187         case op_get_enumerable_length: {
7188             auto bytecode = currentInstruction-&gt;as&lt;OpGetEnumerableLength&gt;();
7189             set(bytecode.m_dst, addToGraph(GetEnumerableLength, get(bytecode.m_base)));
7190             NEXT_OPCODE(op_get_enumerable_length);
7191         }
7192 
7193         case op_has_generic_property: {
7194             auto bytecode = currentInstruction-&gt;as&lt;OpHasGenericProperty&gt;();
7195             set(bytecode.m_dst, addToGraph(HasGenericProperty, get(bytecode.m_base), get(bytecode.m_property)));
7196             NEXT_OPCODE(op_has_generic_property);
7197         }
7198 
7199         case op_has_structure_property: {
7200             auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
7201             set(bytecode.m_dst, addToGraph(HasStructureProperty,
7202                 get(bytecode.m_base),
7203                 get(bytecode.m_property),
7204                 get(bytecode.m_enumerator)));
7205             NEXT_OPCODE(op_has_structure_property);
7206         }
7207 
7208         case op_has_indexed_property: {
7209             auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
7210             Node* base = get(bytecode.m_base);
7211             ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
7212             Node* property = get(bytecode.m_property);
7213             addVarArgChild(base);
7214             addVarArgChild(property);
7215             addVarArgChild(nullptr);
7216             Node* hasIterableProperty = addToGraph(Node::VarArg, HasIndexedProperty, OpInfo(arrayMode.asWord()), OpInfo(static_cast&lt;uint32_t&gt;(PropertySlot::InternalMethodType::GetOwnProperty)));
7217             m_exitOK = false; // HasIndexedProperty must be treated as if it clobbers exit state, since FixupPhase may make it generic.
7218             set(bytecode.m_dst, hasIterableProperty);
7219             NEXT_OPCODE(op_has_indexed_property);
7220         }
7221 
7222         case op_get_direct_pname: {
7223             auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
7224             SpeculatedType prediction = getPredictionWithoutOSRExit();
7225 
7226             Node* base = get(bytecode.m_base);
7227             Node* property = get(bytecode.m_property);
7228             Node* index = get(bytecode.m_index);
7229             Node* enumerator = get(bytecode.m_enumerator);
7230 
7231             addVarArgChild(base);
7232             addVarArgChild(property);
7233             addVarArgChild(index);
7234             addVarArgChild(enumerator);
7235             set(bytecode.m_dst, addToGraph(Node::VarArg, GetDirectPname, OpInfo(0), OpInfo(prediction)));
7236 
7237             NEXT_OPCODE(op_get_direct_pname);
7238         }
7239 
7240         case op_get_property_enumerator: {
7241             auto bytecode = currentInstruction-&gt;as&lt;OpGetPropertyEnumerator&gt;();
7242             set(bytecode.m_dst, addToGraph(GetPropertyEnumerator, get(bytecode.m_base)));
7243             NEXT_OPCODE(op_get_property_enumerator);
7244         }
7245 
7246         case op_enumerator_structure_pname: {
7247             auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
7248             set(bytecode.m_dst, addToGraph(GetEnumeratorStructurePname,
7249                 get(bytecode.m_enumerator),
7250                 get(bytecode.m_index)));
7251             NEXT_OPCODE(op_enumerator_structure_pname);
7252         }
7253 
7254         case op_enumerator_generic_pname: {
7255             auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
7256             set(bytecode.m_dst, addToGraph(GetEnumeratorGenericPname,
7257                 get(bytecode.m_enumerator),
7258                 get(bytecode.m_index)));
7259             NEXT_OPCODE(op_enumerator_generic_pname);
7260         }
7261 
7262         case op_to_index_string: {
7263             auto bytecode = currentInstruction-&gt;as&lt;OpToIndexString&gt;();
7264             set(bytecode.m_dst, addToGraph(ToIndexString, get(bytecode.m_index)));
7265             NEXT_OPCODE(op_to_index_string);
7266         }
7267 
7268         case op_get_internal_field: {
7269             auto bytecode = currentInstruction-&gt;as&lt;OpGetInternalField&gt;();
7270             set(bytecode.m_dst, addToGraph(GetInternalField, OpInfo(bytecode.m_index), OpInfo(getPrediction()), get(bytecode.m_base)));
7271             NEXT_OPCODE(op_get_internal_field);
7272         }
7273 
7274         case op_put_internal_field: {
7275             auto bytecode = currentInstruction-&gt;as&lt;OpPutInternalField&gt;();
7276             addToGraph(PutInternalField, OpInfo(bytecode.m_index), get(bytecode.m_base), get(bytecode.m_value));
7277             NEXT_OPCODE(op_put_internal_field);
7278         }
7279 
7280         case op_log_shadow_chicken_prologue: {
7281             auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
7282             if (!m_inlineStackTop-&gt;m_inlineCallFrame)
7283                 addToGraph(LogShadowChickenPrologue, get(bytecode.m_scope));
7284             NEXT_OPCODE(op_log_shadow_chicken_prologue);
7285         }
7286 
7287         case op_log_shadow_chicken_tail: {
7288             auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
7289             if (!m_inlineStackTop-&gt;m_inlineCallFrame) {
7290                 // FIXME: The right solution for inlining is to elide these whenever the tail call
7291                 // ends up being inlined.
7292                 // https://bugs.webkit.org/show_bug.cgi?id=155686
7293                 addToGraph(LogShadowChickenTail, get(bytecode.m_thisValue), get(bytecode.m_scope));
7294             }
7295             NEXT_OPCODE(op_log_shadow_chicken_tail);
7296         }
7297 
7298         case op_unreachable: {
7299             flushForTerminal();
7300             addToGraph(Unreachable);
7301             LAST_OPCODE(op_unreachable);
7302         }
7303 
7304         default:
7305             // Parse failed! This should not happen because the capabilities checker
7306             // should have caught it.
7307             RELEASE_ASSERT_NOT_REACHED();
7308             return;
7309         }
7310     }
7311 }
7312 
7313 void ByteCodeParser::linkBlock(BasicBlock* block, Vector&lt;BasicBlock*&gt;&amp; possibleTargets)
7314 {
7315     ASSERT(!block-&gt;isLinked);
7316     ASSERT(!block-&gt;isEmpty());
7317     Node* node = block-&gt;terminal();
7318     ASSERT(node-&gt;isTerminal());
7319 
7320     switch (node-&gt;op()) {
7321     case Jump:
7322         node-&gt;targetBlock() = blockForBytecodeIndex(possibleTargets, BytecodeIndex(node-&gt;targetBytecodeOffsetDuringParsing()));
7323         break;
7324 
7325     case Branch: {
7326         BranchData* data = node-&gt;branchData();
7327         data-&gt;taken.block = blockForBytecodeIndex(possibleTargets, BytecodeIndex(data-&gt;takenBytecodeIndex()));
7328         data-&gt;notTaken.block = blockForBytecodeIndex(possibleTargets, BytecodeIndex(data-&gt;notTakenBytecodeIndex()));
7329         break;
7330     }
7331 
7332     case Switch: {
7333         SwitchData* data = node-&gt;switchData();
7334         for (unsigned i = node-&gt;switchData()-&gt;cases.size(); i--;)
7335             data-&gt;cases[i].target.block = blockForBytecodeIndex(possibleTargets, BytecodeIndex(data-&gt;cases[i].target.bytecodeIndex()));
7336         data-&gt;fallThrough.block = blockForBytecodeIndex(possibleTargets, BytecodeIndex(data-&gt;fallThrough.bytecodeIndex()));
7337         break;
7338     }
7339 
7340     default:
7341         RELEASE_ASSERT_NOT_REACHED();
7342     }
7343 
7344     VERBOSE_LOG(&quot;Marking &quot;, RawPointer(block), &quot; as linked (actually did linking)\n&quot;);
7345     block-&gt;didLink();
7346 }
7347 
7348 void ByteCodeParser::linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets)
7349 {
7350     for (size_t i = 0; i &lt; unlinkedBlocks.size(); ++i) {
7351         VERBOSE_LOG(&quot;Attempting to link &quot;, RawPointer(unlinkedBlocks[i]), &quot;\n&quot;);
7352         linkBlock(unlinkedBlocks[i], possibleTargets);
7353     }
7354 }
7355 
7356 ByteCodeParser::InlineStackEntry::InlineStackEntry(
7357     ByteCodeParser* byteCodeParser,
7358     CodeBlock* codeBlock,
7359     CodeBlock* profiledBlock,
7360     JSFunction* callee, // Null if this is a closure call.
7361     VirtualRegister returnValueVR,
7362     VirtualRegister inlineCallFrameStart,
7363     int argumentCountIncludingThis,
7364     InlineCallFrame::Kind kind,
7365     BasicBlock* continuationBlock)
7366     : m_byteCodeParser(byteCodeParser)
7367     , m_codeBlock(codeBlock)
7368     , m_profiledBlock(profiledBlock)
7369     , m_continuationBlock(continuationBlock)
7370     , m_returnValue(returnValueVR)
7371     , m_caller(byteCodeParser-&gt;m_inlineStackTop)
7372 {
7373     {
7374         m_exitProfile.initialize(m_profiledBlock-&gt;unlinkedCodeBlock());
7375 
7376         ConcurrentJSLocker locker(m_profiledBlock-&gt;m_lock);
7377         m_lazyOperands.initialize(locker, m_profiledBlock-&gt;lazyOperandValueProfiles(locker));
7378 
7379         // We do this while holding the lock because we want to encourage StructureStubInfo&#39;s
7380         // to be potentially added to operations and because the profiled block could be in the
7381         // middle of LLInt-&gt;JIT tier-up in which case we would be adding the info&#39;s right now.
7382         if (m_profiledBlock-&gt;hasBaselineJITProfiling())
7383             m_profiledBlock-&gt;getICStatusMap(locker, m_baselineMap);
7384     }
7385 
7386     CodeBlock* optimizedBlock = m_profiledBlock-&gt;replacement();
7387     m_optimizedContext.optimizedCodeBlock = optimizedBlock;
7388     if (Options::usePolyvariantDevirtualization() &amp;&amp; optimizedBlock) {
7389         ConcurrentJSLocker locker(optimizedBlock-&gt;m_lock);
7390         optimizedBlock-&gt;getICStatusMap(locker, m_optimizedContext.map);
7391     }
7392     byteCodeParser-&gt;m_icContextStack.append(&amp;m_optimizedContext);
7393 
7394     int argumentCountIncludingThisWithFixup = std::max&lt;int&gt;(argumentCountIncludingThis, codeBlock-&gt;numParameters());
7395 
7396     if (m_caller) {
7397         // Inline case.
7398         ASSERT(codeBlock != byteCodeParser-&gt;m_codeBlock);
7399         ASSERT(inlineCallFrameStart.isValid());
7400 
7401         m_inlineCallFrame = byteCodeParser-&gt;m_graph.m_plan.inlineCallFrames()-&gt;add();
7402         m_optimizedContext.inlineCallFrame = m_inlineCallFrame;
7403 
7404         // The owner is the machine code block, and we already have a barrier on that when the
7405         // plan finishes.
7406         m_inlineCallFrame-&gt;baselineCodeBlock.setWithoutWriteBarrier(codeBlock-&gt;baselineVersion());
7407         m_inlineCallFrame-&gt;setTmpOffset((m_caller-&gt;m_inlineCallFrame ? m_caller-&gt;m_inlineCallFrame-&gt;tmpOffset : 0) + m_caller-&gt;m_codeBlock-&gt;numTmps());
7408         m_inlineCallFrame-&gt;setStackOffset(inlineCallFrameStart.offset() - CallFrame::headerSizeInRegisters);
7409         m_inlineCallFrame-&gt;argumentCountIncludingThis = argumentCountIncludingThis;
7410         RELEASE_ASSERT(m_inlineCallFrame-&gt;argumentCountIncludingThis == argumentCountIncludingThis);
7411         if (callee) {
7412             m_inlineCallFrame-&gt;calleeRecovery = ValueRecovery::constant(callee);
7413             m_inlineCallFrame-&gt;isClosureCall = false;
7414         } else
7415             m_inlineCallFrame-&gt;isClosureCall = true;
7416         m_inlineCallFrame-&gt;directCaller = byteCodeParser-&gt;currentCodeOrigin();
7417         m_inlineCallFrame-&gt;argumentsWithFixup.resizeToFit(argumentCountIncludingThisWithFixup); // Set the number of arguments including this, but don&#39;t configure the value recoveries, yet.
7418         m_inlineCallFrame-&gt;kind = kind;
7419 
7420         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7421         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7422 
7423         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i) {
7424             UniquedStringImpl* rep = codeBlock-&gt;identifier(i).impl();
7425             unsigned index = byteCodeParser-&gt;m_graph.identifiers().ensure(rep);
7426             m_identifierRemap[i] = index;
7427         }
7428         for (unsigned i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i) {
7429             m_switchRemap[i] = byteCodeParser-&gt;m_codeBlock-&gt;numberOfSwitchJumpTables();
7430             byteCodeParser-&gt;m_codeBlock-&gt;addSwitchJumpTableFromProfiledCodeBlock(codeBlock-&gt;switchJumpTable(i));
7431         }
7432     } else {
7433         // Machine code block case.
7434         ASSERT(codeBlock == byteCodeParser-&gt;m_codeBlock);
7435         ASSERT(!callee);
7436         ASSERT(!returnValueVR.isValid());
7437         ASSERT(!inlineCallFrameStart.isValid());
7438 
7439         m_inlineCallFrame = 0;
7440 
7441         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7442         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7443         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i)
7444             m_identifierRemap[i] = i;
7445         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i)
7446             m_switchRemap[i] = i;
7447     }
7448 
7449     m_argumentPositions.resize(argumentCountIncludingThisWithFixup);
7450     for (int i = 0; i &lt; argumentCountIncludingThisWithFixup; ++i) {
7451         byteCodeParser-&gt;m_graph.m_argumentPositions.append(ArgumentPosition());
7452         ArgumentPosition* argumentPosition = &amp;byteCodeParser-&gt;m_graph.m_argumentPositions.last();
7453         m_argumentPositions[i] = argumentPosition;
7454     }
7455     byteCodeParser-&gt;m_inlineCallFrameToArgumentPositions.add(m_inlineCallFrame, m_argumentPositions);
7456 
7457     byteCodeParser-&gt;m_inlineStackTop = this;
7458 }
7459 
7460 ByteCodeParser::InlineStackEntry::~InlineStackEntry()
7461 {
7462     m_byteCodeParser-&gt;m_inlineStackTop = m_caller;
7463     RELEASE_ASSERT(m_byteCodeParser-&gt;m_icContextStack.last() == &amp;m_optimizedContext);
7464     m_byteCodeParser-&gt;m_icContextStack.removeLast();
7465 }
7466 
7467 void ByteCodeParser::parseCodeBlock()
7468 {
7469     clearCaches();
7470 
7471     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
7472 
7473     if (UNLIKELY(m_graph.compilation())) {
7474         m_graph.compilation()-&gt;addProfiledBytecodes(
7475             *m_vm-&gt;m_perBytecodeProfiler, m_inlineStackTop-&gt;m_profiledBlock);
7476     }
7477 
7478     if (UNLIKELY(Options::dumpSourceAtDFGTime())) {
7479         Vector&lt;DeferredSourceDump&gt;&amp; deferredSourceDump = m_graph.m_plan.callback()-&gt;ensureDeferredSourceDump();
7480         if (inlineCallFrame()) {
7481             DeferredSourceDump dump(codeBlock-&gt;baselineVersion(), m_codeBlock, JITType::DFGJIT, inlineCallFrame()-&gt;directCaller.bytecodeIndex());
7482             deferredSourceDump.append(dump);
7483         } else
7484             deferredSourceDump.append(DeferredSourceDump(codeBlock-&gt;baselineVersion()));
7485     }
7486 
7487     if (UNLIKELY(Options::dumpBytecodeAtDFGTime())) {
7488         dataLog(&quot;Parsing &quot;, *codeBlock);
7489         if (inlineCallFrame()) {
7490             dataLog(
7491                 &quot; for inlining at &quot;, CodeBlockWithJITType(m_codeBlock, JITType::DFGJIT),
7492                 &quot; &quot;, inlineCallFrame()-&gt;directCaller);
7493         }
7494         dataLog(
7495             &quot;, isStrictMode = &quot;, codeBlock-&gt;ownerExecutable()-&gt;isStrictMode(), &quot;\n&quot;);
7496         codeBlock-&gt;baselineVersion()-&gt;dumpBytecode();
7497     }
7498 
7499     Vector&lt;InstructionStream::Offset, 32&gt; jumpTargets;
7500     computePreciseJumpTargets(codeBlock, jumpTargets);
7501     if (UNLIKELY(Options::dumpBytecodeAtDFGTime())) {
7502         dataLog(&quot;Jump targets: &quot;);
7503         CommaPrinter comma;
7504         for (unsigned i = 0; i &lt; jumpTargets.size(); ++i)
7505             dataLog(comma, jumpTargets[i]);
7506         dataLog(&quot;\n&quot;);
7507     }
7508 
7509     for (unsigned jumpTargetIndex = 0; jumpTargetIndex &lt;= jumpTargets.size(); ++jumpTargetIndex) {
7510         // The maximum bytecode offset to go into the current basicblock is either the next jump target, or the end of the instructions.
7511         unsigned limit = jumpTargetIndex &lt; jumpTargets.size() ? jumpTargets[jumpTargetIndex] : codeBlock-&gt;instructions().size();
7512         ASSERT(m_currentIndex.offset() &lt; limit);
7513 
7514         // Loop until we reach the current limit (i.e. next jump target).
7515         do {
7516             // There may already be a currentBlock in two cases:
7517             // - we may have just entered the loop for the first time
7518             // - we may have just returned from an inlined callee that had some early returns and
7519             //   so allocated a continuation block, and the instruction after the call is a jump target.
7520             // In both cases, we want to keep using it.
7521             if (!m_currentBlock) {
7522                 m_currentBlock = allocateTargetableBlock(m_currentIndex);
7523 
7524                 // The first block is definitely an OSR target.
7525                 if (m_graph.numBlocks() == 1) {
7526                     m_currentBlock-&gt;isOSRTarget = true;
7527                     m_graph.m_roots.append(m_currentBlock);
7528                 }
7529                 prepareToParseBlock();
7530             }
7531 
7532             parseBlock(limit);
7533 
7534             // We should not have gone beyond the limit.
7535             ASSERT(m_currentIndex.offset() &lt;= limit);
7536 
7537             if (m_currentBlock-&gt;isEmpty()) {
7538                 // This case only happens if the last instruction was an inlined call with early returns
7539                 // or polymorphic (creating an empty continuation block),
7540                 // and then we hit the limit before putting anything in the continuation block.
7541                 ASSERT(m_currentIndex.offset() == limit);
7542                 makeBlockTargetable(m_currentBlock, m_currentIndex);
7543             } else {
7544                 ASSERT(m_currentBlock-&gt;terminal() || (m_currentIndex.offset() == codeBlock-&gt;instructions().size() &amp;&amp; inlineCallFrame()));
7545                 m_currentBlock = nullptr;
7546             }
7547         } while (m_currentIndex.offset() &lt; limit);
7548     }
7549 
7550     // Should have reached the end of the instructions.
7551     ASSERT(m_currentIndex.offset() == codeBlock-&gt;instructions().size());
7552 
7553     VERBOSE_LOG(&quot;Done parsing &quot;, *codeBlock, &quot; (fell off end)\n&quot;);
7554 }
7555 
7556 template &lt;typename Bytecode&gt;
7557 void ByteCodeParser::handlePutByVal(Bytecode bytecode, unsigned instructionSize)
7558 {
7559     Node* base = get(bytecode.m_base);
7560     Node* property = get(bytecode.m_property);
7561     Node* value = get(bytecode.m_value);
7562     bool isDirect = Bytecode::opcodeID == op_put_by_val_direct;
7563     bool compiledAsPutById = false;
7564     {
7565         unsigned identifierNumber = std::numeric_limits&lt;unsigned&gt;::max();
7566         PutByIdStatus putByIdStatus;
7567         {
7568             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
7569             ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex())).byValInfo;
7570             // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
7571             // At that time, there is no information.
7572             if (byValInfo
7573                 &amp;&amp; byValInfo-&gt;stubInfo
7574                 &amp;&amp; !byValInfo-&gt;tookSlowPath
7575                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
7576                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
7577                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
7578                 compiledAsPutById = true;
7579                 identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
7580                 UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
7581 
7582                 if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
7583                     FrozenValue* frozen = m_graph.freezeStrong(symbol);
7584                     addToGraph(CheckCell, OpInfo(frozen), property);
7585                 } else {
7586                     ASSERT(!uid-&gt;isSymbol());
7587                     addToGraph(CheckIdent, OpInfo(uid), property);
7588                 }
7589 
7590                 putByIdStatus = PutByIdStatus::computeForStubInfo(
7591                     locker, m_inlineStackTop-&gt;m_profiledBlock,
7592                     byValInfo-&gt;stubInfo, currentCodeOrigin(), uid);
7593 
7594             }
7595         }
7596 
7597         if (compiledAsPutById)
7598             handlePutById(base, identifierNumber, value, putByIdStatus, isDirect, instructionSize);
7599     }
7600 
7601     if (!compiledAsPutById) {
7602         ArrayMode arrayMode = getArrayMode(bytecode.metadata(m_inlineStackTop-&gt;m_codeBlock).m_arrayProfile, Array::Write);
7603 
7604         addVarArgChild(base);
7605         addVarArgChild(property);
7606         addVarArgChild(value);
7607         addVarArgChild(0); // Leave room for property storage.
7608         addVarArgChild(0); // Leave room for length.
7609         addToGraph(Node::VarArg, isDirect ? PutByValDirect : PutByVal, OpInfo(arrayMode.asWord()), OpInfo(0));
7610         m_exitOK = false; // PutByVal and PutByValDirect must be treated as if they clobber exit state, since FixupPhase may make them generic.
7611     }
7612 }
7613 
7614 template &lt;typename Bytecode&gt;
7615 void ByteCodeParser::handlePutAccessorById(NodeType op, Bytecode bytecode)
7616 {
7617     Node* base = get(bytecode.m_base);
7618     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
7619     Node* accessor = get(bytecode.m_accessor);
7620     addToGraph(op, OpInfo(identifierNumber), OpInfo(bytecode.m_attributes), base, accessor);
7621 }
7622 
7623 template &lt;typename Bytecode&gt;
7624 void ByteCodeParser::handlePutAccessorByVal(NodeType op, Bytecode bytecode)
7625 {
7626     Node* base = get(bytecode.m_base);
7627     Node* subscript = get(bytecode.m_property);
7628     Node* accessor = get(bytecode.m_accessor);
7629     addToGraph(op, OpInfo(bytecode.m_attributes), base, subscript, accessor);
7630 }
7631 
7632 template &lt;typename Bytecode&gt;
7633 void ByteCodeParser::handleNewFunc(NodeType op, Bytecode bytecode)
7634 {
7635     FunctionExecutable* decl = m_inlineStackTop-&gt;m_profiledBlock-&gt;functionDecl(bytecode.m_functionDecl);
7636     FrozenValue* frozen = m_graph.freezeStrong(decl);
7637     Node* scope = get(bytecode.m_scope);
7638     set(bytecode.m_dst, addToGraph(op, OpInfo(frozen), scope));
7639     // Ideally we wouldn&#39;t have to do this Phantom. But:
7640     //
7641     // For the constant case: we must do it because otherwise we would have no way of knowing
7642     // that the scope is live at OSR here.
7643     //
7644     // For the non-constant case: NewFunction could be DCE&#39;d, but baseline&#39;s implementation
7645     // won&#39;t be able to handle an Undefined scope.
7646     addToGraph(Phantom, scope);
7647 }
7648 
7649 template &lt;typename Bytecode&gt;
7650 void ByteCodeParser::handleNewFuncExp(NodeType op, Bytecode bytecode)
7651 {
7652     FunctionExecutable* expr = m_inlineStackTop-&gt;m_profiledBlock-&gt;functionExpr(bytecode.m_functionDecl);
7653     FrozenValue* frozen = m_graph.freezeStrong(expr);
7654     Node* scope = get(bytecode.m_scope);
7655     set(bytecode.m_dst, addToGraph(op, OpInfo(frozen), scope));
7656     // Ideally we wouldn&#39;t have to do this Phantom. But:
7657     //
7658     // For the constant case: we must do it because otherwise we would have no way of knowing
7659     // that the scope is live at OSR here.
7660     //
7661     // For the non-constant case: NewFunction could be DCE&#39;d, but baseline&#39;s implementation
7662     // won&#39;t be able to handle an Undefined scope.
7663     addToGraph(Phantom, scope);
7664 }
7665 
7666 template &lt;typename Bytecode&gt;
7667 void ByteCodeParser::handleCreateInternalFieldObject(const ClassInfo* classInfo, NodeType createOp, NodeType newOp, Bytecode bytecode)
7668 {
7669     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
7670     JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
7671     Node* callee = get(VirtualRegister(bytecode.m_callee));
7672 
7673     JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm);
7674     if (!function) {
7675         JSCell* cachedFunction = bytecode.metadata(codeBlock).m_cachedCallee.unvalidatedGet();
7676         if (cachedFunction
7677             &amp;&amp; cachedFunction != JSCell::seenMultipleCalleeObjects()
7678             &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
7679             ASSERT(cachedFunction-&gt;inherits&lt;JSFunction&gt;(*m_vm));
7680 
7681             FrozenValue* frozen = m_graph.freeze(cachedFunction);
7682             addToGraph(CheckCell, OpInfo(frozen), callee);
7683 
7684             function = static_cast&lt;JSFunction*&gt;(cachedFunction);
7685         }
7686     }
7687 
7688     if (function) {
7689         if (FunctionRareData* rareData = function-&gt;rareData()) {
7690             if (rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
7691                 Structure* structure = rareData-&gt;internalFunctionAllocationStructure();
7692                 if (structure
7693                     &amp;&amp; structure-&gt;classInfo() == classInfo
7694                     &amp;&amp; structure-&gt;globalObject() == globalObject
7695                     &amp;&amp; rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
7696                     m_graph.freeze(rareData);
7697                     m_graph.watchpoints().addLazily(rareData-&gt;allocationProfileWatchpointSet());
7698 
7699                     set(VirtualRegister(bytecode.m_dst), addToGraph(newOp, OpInfo(m_graph.registerStructure(structure))));
7700                     // The callee is still live up to this point.
7701                     addToGraph(Phantom, callee);
7702                     return;
7703                 }
7704             }
7705         }
7706     }
7707 
7708     set(VirtualRegister(bytecode.m_dst), addToGraph(createOp, callee));
7709 }
7710 
7711 void ByteCodeParser::parse()
7712 {
7713     // Set during construction.
7714     ASSERT(!m_currentIndex.offset());
7715 
7716     VERBOSE_LOG(&quot;Parsing &quot;, *m_codeBlock, &quot;\n&quot;);
7717 
7718     InlineStackEntry inlineStackEntry(
7719         this, m_codeBlock, m_profiledBlock, 0, VirtualRegister(), VirtualRegister(),
7720         m_codeBlock-&gt;numParameters(), InlineCallFrame::Call, nullptr);
7721 
7722     parseCodeBlock();
7723     linkBlocks(inlineStackEntry.m_unlinkedBlocks, inlineStackEntry.m_blockLinkingTargets);
7724 
7725     if (m_hasAnyForceOSRExits) {
7726         BlockSet blocksToIgnore;
7727         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7728             if (block-&gt;isOSRTarget &amp;&amp; block-&gt;bytecodeBegin == m_graph.m_plan.osrEntryBytecodeIndex()) {
7729                 blocksToIgnore.add(block);
7730                 break;
7731             }
7732         }
7733 
7734         {
7735             bool isSafeToValidate = false;
7736             auto postOrder = m_graph.blocksInPostOrder(isSafeToValidate); // This algorithm doesn&#39;t rely on the predecessors list, which is not yet built.
7737             bool changed;
7738             do {
7739                 changed = false;
7740                 for (BasicBlock* block : postOrder) {
7741                     for (BasicBlock* successor : block-&gt;successors()) {
7742                         if (blocksToIgnore.contains(successor)) {
7743                             changed |= blocksToIgnore.add(block);
7744                             break;
7745                         }
7746                     }
7747                 }
7748             } while (changed);
7749         }
7750 
7751         InsertionSet insertionSet(m_graph);
7752         Operands&lt;VariableAccessData*&gt; mapping(OperandsLike, m_graph.block(0)-&gt;variablesAtHead);
7753 
7754         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7755             if (blocksToIgnore.contains(block))
7756                 continue;
7757 
7758             mapping.fill(nullptr);
7759             if (validationEnabled()) {
7760                 // Verify that it&#39;s correct to fill mapping with nullptr.
7761                 for (unsigned i = 0; i &lt; block-&gt;variablesAtHead.size(); ++i) {
7762                     Node* node = block-&gt;variablesAtHead.at(i);
7763                     RELEASE_ASSERT(!node);
7764                 }
7765             }
7766 
7767             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
7768                 {
7769                     Node* node = block-&gt;at(nodeIndex);
7770 
7771                     if (node-&gt;hasVariableAccessData(m_graph))
7772                         mapping.operand(node-&gt;operand()) = node-&gt;variableAccessData();
7773 
7774                     if (node-&gt;op() != ForceOSRExit)
7775                         continue;
7776                 }
7777 
7778                 NodeOrigin origin = block-&gt;at(nodeIndex)-&gt;origin;
7779                 RELEASE_ASSERT(origin.exitOK);
7780 
7781                 ++nodeIndex;
7782 
7783                 {
7784                     if (validationEnabled()) {
7785                         // This verifies that we don&#39;t need to change any of the successors&#39;s predecessor
7786                         // list after planting the Unreachable below. At this point in the bytecode
7787                         // parser, we haven&#39;t linked up the predecessor lists yet.
7788                         for (BasicBlock* successor : block-&gt;successors())
7789                             RELEASE_ASSERT(successor-&gt;predecessors.isEmpty());
7790                     }
7791 
7792                     auto insertLivenessPreservingOp = [&amp;] (InlineCallFrame* inlineCallFrame, NodeType op, Operand operand) {
7793                         VariableAccessData* variable = mapping.operand(operand);
7794                         if (!variable) {
7795                             variable = newVariableAccessData(operand);
7796                             mapping.operand(operand) = variable;
7797                         }
7798 
7799                         Operand argument = unmapOperand(inlineCallFrame, operand);
7800                         if (argument.isArgument() &amp;&amp; !argument.isHeader()) {
7801                             const Vector&lt;ArgumentPosition*&gt;&amp; arguments = m_inlineCallFrameToArgumentPositions.get(inlineCallFrame);
7802                             arguments[argument.toArgument()]-&gt;addVariable(variable);
7803                         }
7804                         insertionSet.insertNode(nodeIndex, SpecNone, op, origin, OpInfo(variable));
7805                     };
7806                     auto addFlushDirect = [&amp;] (InlineCallFrame* inlineCallFrame, Operand operand) {
7807                         insertLivenessPreservingOp(inlineCallFrame, Flush, operand);
7808                     };
7809                     auto addPhantomLocalDirect = [&amp;] (InlineCallFrame* inlineCallFrame, Operand operand) {
7810                         insertLivenessPreservingOp(inlineCallFrame, PhantomLocal, operand);
7811                     };
7812                     flushForTerminalImpl(origin.semantic, addFlushDirect, addPhantomLocalDirect);
7813                 }
7814 
7815                 while (true) {
7816                     RELEASE_ASSERT(nodeIndex &lt; block-&gt;size());
7817 
7818                     Node* node = block-&gt;at(nodeIndex);
7819 
7820                     node-&gt;origin = origin;
7821                     m_graph.doToChildren(node, [&amp;] (Edge edge) {
7822                         // We only need to keep data flow edges to nodes defined prior to the ForceOSRExit. The reason
7823                         // for this is we rely on backwards propagation being able to see the &quot;full&quot; bytecode. To model
7824                         // this, we preserve uses of a node in a generic way so that backwards propagation can reason
7825                         // about them. Therefore, we can&#39;t remove uses of a node which is defined before the ForceOSRExit
7826                         // even when we&#39;re at a point in the program after the ForceOSRExit, because that would break backwards
7827                         // propagation&#39;s analysis over the uses of a node. However, we don&#39;t need this same preservation for
7828                         // nodes defined after ForceOSRExit, as we&#39;ve already exitted before those defs.
7829                         if (edge-&gt;hasResult())
7830                             insertionSet.insertNode(nodeIndex, SpecNone, Phantom, origin, Edge(edge.node(), UntypedUse));
7831                     });
7832 
7833                     bool isTerminal = node-&gt;isTerminal();
7834 
7835                     node-&gt;removeWithoutChecks();
7836 
7837                     if (isTerminal) {
7838                         insertionSet.insertNode(nodeIndex, SpecNone, Unreachable, origin);
7839                         break;
7840                     }
7841 
7842                     ++nodeIndex;
7843                 }
7844 
7845                 insertionSet.execute(block);
7846 
7847                 auto nodeAndIndex = block-&gt;findTerminal();
7848                 RELEASE_ASSERT(nodeAndIndex.node-&gt;op() == Unreachable);
7849                 block-&gt;resize(nodeAndIndex.index + 1);
7850                 break;
7851             }
7852         }
7853     } else if (validationEnabled()) {
7854         // Ensure our bookkeeping for ForceOSRExit nodes is working.
7855         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7856             for (Node* node : *block)
7857                 RELEASE_ASSERT(node-&gt;op() != ForceOSRExit);
7858         }
7859     }
7860 
7861     m_graph.determineReachability();
7862     m_graph.killUnreachableBlocks();
7863 
7864     for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
7865         BasicBlock* block = m_graph.block(blockIndex);
7866         if (!block)
7867             continue;
7868         ASSERT(block-&gt;variablesAtHead.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7869         ASSERT(block-&gt;variablesAtHead.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7870         ASSERT(block-&gt;variablesAtTail.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7871         ASSERT(block-&gt;variablesAtTail.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7872     }
7873 
7874     m_graph.m_tmps = m_numTmps;
7875     m_graph.m_localVars = m_numLocals;
7876     m_graph.m_parameterSlots = m_parameterSlots;
7877 }
7878 
7879 void parse(Graph&amp; graph)
7880 {
7881     ByteCodeParser(graph).parse();
7882 }
7883 
7884 } } // namespace JSC::DFG
7885 
7886 #endif
    </pre>
  </body>
</html>