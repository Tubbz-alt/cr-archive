diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h
@@ -24,14 +24,16 @@
 #include "CellAttributes.h"
 #include "DestructionMode.h"
 #include "HeapCell.h"
 #include "IterationStatus.h"
 #include "WeakSet.h"
+#include <algorithm>
 #include <wtf/Atomics.h>
 #include <wtf/Bitmap.h>
-#include <wtf/HashFunctions.h>
 #include <wtf/CountingLock.h>
+#include <wtf/HashFunctions.h>
+#include <wtf/PageBlock.h>
 #include <wtf/StdLibExtras.h>
 
 namespace JSC {
 
 class AlignedMemoryAllocator;
@@ -50,13 +52,15 @@
 // marked block, all cells have the same size. Objects smaller than the
 // cell size may be allocated in the marked block, in which case the
 // allocation suffers from internal fragmentation: wasted space whose
 // size is equal to the difference between the cell size and the object
 // size.
-
+DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MarkedBlock);
+DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MarkedBlockHandle);
 class MarkedBlock {
     WTF_MAKE_NONCOPYABLE(MarkedBlock);
+    WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(MarkedBlock);
     friend class LLIntOffsetsExtractor;
     friend struct VerifyMarked;
 
 public:
     class Footer;
@@ -66,20 +70,19 @@
     friend class Handle;
 public:
     static constexpr size_t atomSize = 16; // bytes
 
     // Block size must be at least as large as the system page size.
-#if CPU(PPC64) || CPU(PPC64LE) || CPU(PPC) || CPU(UNKNOWN)
-    static constexpr size_t blockSize = 64 * KB;
-#else
-    static constexpr size_t blockSize = 16 * KB;
-#endif
+    static constexpr size_t blockSize = std::max(16 * KB, CeilingOnPageSize);
 
     static constexpr size_t blockMask = ~(blockSize - 1); // blockSize must be a power of two.
 
     static constexpr size_t atomsPerBlock = blockSize / atomSize;
 
+    static constexpr size_t maxNumberOfLowerTierCells = 8;
+    static_assert(maxNumberOfLowerTierCells <= 256);
+
     static_assert(!(MarkedBlock::atomSize & (MarkedBlock::atomSize - 1)), "MarkedBlock::atomSize must be a power of two.");
     static_assert(!(MarkedBlock::blockSize & (MarkedBlock::blockSize - 1)), "MarkedBlock::blockSize must be a power of two.");
 
     struct VoidFunctor {
         typedef void ReturnType;
@@ -100,11 +103,11 @@
         mutable ReturnType m_count;
     };
 
     class Handle {
         WTF_MAKE_NONCOPYABLE(Handle);
-        WTF_MAKE_FAST_ALLOCATED;
+        WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(MarkedBlockHandle);
         friend class LLIntOffsetsExtractor;
         friend class MarkedBlock;
         friend struct VerifyMarked;
     public:
 
@@ -189,15 +192,15 @@
 
         void assertMarksNotStale();
 
         bool isFreeListed() const { return m_isFreeListed; }
 
-        size_t index() const { return m_index; }
+        unsigned index() const { return m_index; }
 
         void removeFromDirectory();
 
-        void didAddToDirectory(BlockDirectory*, size_t index);
+        void didAddToDirectory(BlockDirectory*, unsigned index);
         void didRemoveFromDirectory();
 
         void* start() const { return &m_block->atoms()[0]; }
         void* end() const { return &m_block->atoms()[m_endAtom]; }
         bool contains(void* p) const { return start() <= p && p < end(); }
@@ -222,22 +225,19 @@
         template<bool, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, typename DestroyFunc>
         void specializedSweep(FreeList*, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, const DestroyFunc&);
 
         void setIsFreeListed();
 
-        MarkedBlock::Handle* m_prev { nullptr };
-        MarkedBlock::Handle* m_next { nullptr };
-
-        size_t m_atomsPerCell { std::numeric_limits<size_t>::max() };
-        size_t m_endAtom { std::numeric_limits<size_t>::max() }; // This is a fuzzy end. Always test for < m_endAtom.
+        unsigned m_atomsPerCell { std::numeric_limits<unsigned>::max() };
+        unsigned m_endAtom { std::numeric_limits<unsigned>::max() }; // This is a fuzzy end. Always test for < m_endAtom.
 
         CellAttributes m_attributes;
         bool m_isFreeListed { false };
+        unsigned m_index { std::numeric_limits<unsigned>::max() };
 
         AlignedMemoryAllocator* m_alignedMemoryAllocator { nullptr };
         BlockDirectory* m_directory { nullptr };
-        size_t m_index { std::numeric_limits<size_t>::max() };
         WeakSet m_weakSet;
 
         MarkedBlock* m_block { nullptr };
     };
 
@@ -306,13 +306,10 @@
     static constexpr size_t endAtom = (blockSize - sizeof(Footer)) / atomSize;
     static constexpr size_t payloadSize = endAtom * atomSize;
     static constexpr size_t footerSize = blockSize - payloadSize;
 
     static_assert(payloadSize == ((blockSize - sizeof(MarkedBlock::Footer)) & ~(atomSize - 1)), "Payload size computed the alternate way should give the same result");
-    // Some of JSCell types assume that the last JSCell in a MarkedBlock has a subsequent memory region (Footer) that can still safely accessed.
-    // For example, JSRopeString assumes that it can safely access up to 2 bytes beyond the JSRopeString cell.
-    static_assert(sizeof(Footer) >= sizeof(uint16_t));
 
     static MarkedBlock::Handle* tryCreate(Heap&, AlignedMemoryAllocator*);
 
     Handle& handle();
     const Handle& handle() const;
@@ -321,11 +318,12 @@
     inline Heap* heap() const;
     inline MarkedSpace* space() const;
 
     static bool isAtomAligned(const void*);
     static MarkedBlock* blockFor(const void*);
-    size_t atomNumber(const void*);
+    unsigned atomNumber(const void*);
+    size_t candidateAtomNumber(const void*);
 
     size_t markCount();
 
     bool isMarked(const void*);
     bool isMarked(HeapVersion markingVersion, const void*);
@@ -350,27 +348,27 @@
     size_t cellSize();
     const CellAttributes& attributes() const;
 
     bool hasAnyMarked() const;
     void noteMarked();
-#if ASSERT_DISABLED
-    void assertValidCell(VM&, HeapCell*) const { }
-#else
+#if ASSERT_ENABLED
     void assertValidCell(VM&, HeapCell*) const;
+#else
+    void assertValidCell(VM&, HeapCell*) const { }
 #endif
 
     WeakSet& weakSet();
 
     JS_EXPORT_PRIVATE bool areMarksStale();
     bool areMarksStale(HeapVersion markingVersion);
 
     Dependency aboutToMark(HeapVersion markingVersion);
 
-#if ASSERT_DISABLED
-    void assertMarksNotStale() { }
-#else
+#if ASSERT_ENABLED
     JS_EXPORT_PRIVATE void assertMarksNotStale();
+#else
+    void assertMarksNotStale() { }
 #endif
 
     void resetMarks();
 
     bool isMarkedRaw(const void* p);
@@ -551,15 +549,24 @@
 inline size_t MarkedBlock::Handle::size()
 {
     return markCount() * cellSize();
 }
 
-inline size_t MarkedBlock::atomNumber(const void* p)
+inline size_t MarkedBlock::candidateAtomNumber(const void* p)
 {
+    // This function must return size_t instead of unsigned since pointer |p| is not guaranteed that this is within MarkedBlock.
+    // See MarkedBlock::isAtom which can accept out-of-bound pointers.
     return (reinterpret_cast<uintptr_t>(p) - reinterpret_cast<uintptr_t>(this)) / atomSize;
 }
 
+inline unsigned MarkedBlock::atomNumber(const void* p)
+{
+    size_t atomNumber = candidateAtomNumber(p);
+    ASSERT(atomNumber < handle().m_endAtom);
+    return atomNumber;
+}
+
 inline bool MarkedBlock::areMarksStale(HeapVersion markingVersion)
 {
     return markingVersion != footer().m_markingVersion;
 }
 
@@ -627,11 +634,11 @@
 }
 
 inline bool MarkedBlock::isAtom(const void* p)
 {
     ASSERT(MarkedBlock::isAtomAligned(p));
-    size_t atomNumber = this->atomNumber(p);
+    size_t atomNumber = candidateAtomNumber(p);
     if (atomNumber % handle().m_atomsPerCell) // Filters pointers into cell middles.
         return false;
     if (atomNumber >= handle().m_endAtom) // Filters pointers into invalid cells out of the range.
         return false;
     return true;
@@ -641,11 +648,11 @@
 inline IterationStatus MarkedBlock::Handle::forEachCell(const Functor& functor)
 {
     HeapCell::Kind kind = m_attributes.cellKind;
     for (size_t i = 0; i < m_endAtom; i += m_atomsPerCell) {
         HeapCell* cell = reinterpret_cast_ptr<HeapCell*>(&m_block->atoms()[i]);
-        if (functor(cell, kind) == IterationStatus::Done)
+        if (functor(i, cell, kind) == IterationStatus::Done)
             return IterationStatus::Done;
     }
     return IterationStatus::Continue;
 }
 
