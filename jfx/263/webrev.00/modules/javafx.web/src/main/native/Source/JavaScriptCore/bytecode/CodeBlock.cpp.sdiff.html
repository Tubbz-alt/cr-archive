<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CallVariant.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlock.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.</span>
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #include &quot;config.h&quot;
  31 #include &quot;CodeBlock.h&quot;
  32 
  33 #include &quot;ArithProfile.h&quot;
  34 #include &quot;BasicBlockLocation.h&quot;
  35 #include &quot;BytecodeDumper.h&quot;
  36 #include &quot;BytecodeGenerator.h&quot;
  37 #include &quot;BytecodeLivenessAnalysis.h&quot;
  38 #include &quot;BytecodeStructs.h&quot;
  39 #include &quot;BytecodeUseDef.h&quot;
  40 #include &quot;CallLinkStatus.h&quot;

  41 #include &quot;CodeBlockInlines.h&quot;
  42 #include &quot;CodeBlockSet.h&quot;
  43 #include &quot;DFGCapabilities.h&quot;
  44 #include &quot;DFGCommon.h&quot;
  45 #include &quot;DFGDriver.h&quot;
  46 #include &quot;DFGJITCode.h&quot;
  47 #include &quot;DFGWorklist.h&quot;
  48 #include &quot;Debugger.h&quot;
  49 #include &quot;EvalCodeBlock.h&quot;
  50 #include &quot;FullCodeOrigin.h&quot;
  51 #include &quot;FunctionCodeBlock.h&quot;
  52 #include &quot;FunctionExecutableDump.h&quot;
  53 #include &quot;GetPutInfo.h&quot;
  54 #include &quot;InlineCallFrame.h&quot;
  55 #include &quot;Instruction.h&quot;
  56 #include &quot;InstructionStream.h&quot;
  57 #include &quot;InterpreterInlines.h&quot;
  58 #include &quot;IsoCellSetInlines.h&quot;
  59 #include &quot;JIT.h&quot;
  60 #include &quot;JITMathIC.h&quot;
</pre>
<hr />
<pre>
  92 #include &lt;wtf/Forward.h&gt;
  93 #include &lt;wtf/SimpleStats.h&gt;
  94 #include &lt;wtf/StringPrintStream.h&gt;
  95 #include &lt;wtf/text/StringConcatenateNumbers.h&gt;
  96 #include &lt;wtf/text/UniquedStringImpl.h&gt;
  97 
  98 #if ENABLE(ASSEMBLER)
  99 #include &quot;RegisterAtOffsetList.h&quot;
 100 #endif
 101 
 102 #if ENABLE(DFG_JIT)
 103 #include &quot;DFGOperations.h&quot;
 104 #endif
 105 
 106 #if ENABLE(FTL_JIT)
 107 #include &quot;FTLJITCode.h&quot;
 108 #endif
 109 
 110 namespace JSC {
 111 


 112 const ClassInfo CodeBlock::s_info = {
 113     &quot;CodeBlock&quot;, nullptr, nullptr, nullptr,
 114     CREATE_METHOD_TABLE(CodeBlock)
 115 };
 116 
 117 CString CodeBlock::inferredName() const
 118 {
 119     switch (codeType()) {
 120     case GlobalCode:
 121         return &quot;&lt;global&gt;&quot;;
 122     case EvalCode:
 123         return &quot;&lt;eval&gt;&quot;;
 124     case FunctionCode:
 125         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;ecmaName().utf8();
 126     case ModuleCode:
 127         return &quot;&lt;module&gt;&quot;;
 128     default:
 129         CRASH();
 130         return CString(&quot;&quot;, 0);
 131     }
</pre>
<hr />
<pre>
 229         FunctionExecutable* functionExecutable = reinterpret_cast&lt;FunctionExecutable*&gt;(executable);
 230         StringView source = functionExecutable-&gt;source().provider()-&gt;getRange(
 231             functionExecutable-&gt;parametersStartOffset(),
 232             functionExecutable-&gt;typeProfilingEndOffset(vm()) + 1); // Type profiling end offset is the character before the &#39;}&#39;.
 233 
 234         out.print(&quot;function &quot;, inferredName(), source);
 235         return;
 236     }
 237     out.print(executable-&gt;source().view());
 238 }
 239 
 240 void CodeBlock::dumpBytecode()
 241 {
 242     dumpBytecode(WTF::dataFile());
 243 }
 244 
 245 void CodeBlock::dumpBytecode(PrintStream&amp; out)
 246 {
 247     ICStatusMap statusMap;
 248     getICStatusMap(statusMap);
<span class="line-modified"> 249     BytecodeDumper&lt;CodeBlock&gt;::dumpBlock(this, instructions(), out, statusMap);</span>
 250 }
 251 
 252 void CodeBlock::dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; statusMap)
 253 {
 254     BytecodeDumper&lt;CodeBlock&gt;::dumpBytecode(this, out, it, statusMap);
 255 }
 256 
 257 void CodeBlock::dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; statusMap)
 258 {
 259     const auto it = instructions().at(bytecodeOffset);
 260     dumpBytecode(out, it, statusMap);
 261 }
 262 
 263 namespace {
 264 
 265 class PutToScopeFireDetail : public FireDetail {
 266 public:
 267     PutToScopeFireDetail(CodeBlock* codeBlock, const Identifier&amp; ident)
 268         : m_codeBlock(codeBlock)
 269         , m_ident(ident)
</pre>
<hr />
<pre>
 275         out.print(&quot;Linking put_to_scope in &quot;, FunctionExecutableDump(jsCast&lt;FunctionExecutable*&gt;(m_codeBlock-&gt;ownerExecutable())), &quot; for &quot;, m_ident);
 276     }
 277 
 278 private:
 279     CodeBlock* m_codeBlock;
 280     const Identifier&amp; m_ident;
 281 };
 282 
 283 } // anonymous namespace
 284 
 285 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, CopyParsedBlockTag, CodeBlock&amp; other)
 286     : JSCell(vm, structure)
 287     , m_globalObject(other.m_globalObject)
 288     , m_shouldAlwaysBeInlined(true)
 289 #if ENABLE(JIT)
 290     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 291 #endif
 292     , m_didFailJITCompilation(false)
 293     , m_didFailFTLCompilation(false)
 294     , m_hasBeenCompiledWithFTL(false)


 295     , m_numCalleeLocals(other.m_numCalleeLocals)
 296     , m_numVars(other.m_numVars)
 297     , m_numberOfArgumentsToSkip(other.m_numberOfArgumentsToSkip)
 298     , m_hasDebuggerStatement(false)
 299     , m_steppingMode(SteppingModeDisabled)
 300     , m_numBreakpoints(0)
 301     , m_bytecodeCost(other.m_bytecodeCost)
 302     , m_scopeRegister(other.m_scopeRegister)
 303     , m_hash(other.m_hash)
 304     , m_unlinkedCode(other.vm(), this, other.m_unlinkedCode.get())
 305     , m_ownerExecutable(other.vm(), this, other.m_ownerExecutable.get())
 306     , m_vm(other.m_vm)
 307     , m_instructionsRawPointer(other.m_instructionsRawPointer)
 308     , m_constantRegisters(other.m_constantRegisters)
 309     , m_constantsSourceCodeRepresentation(other.m_constantsSourceCodeRepresentation)
 310     , m_functionDecls(other.m_functionDecls)
 311     , m_functionExprs(other.m_functionExprs)
 312     , m_osrExitCounter(0)
 313     , m_optimizationDelayCounter(0)
 314     , m_reoptimizationRetryCounter(0)
</pre>
<hr />
<pre>
 334 
 335     if (other.m_rareData) {
 336         createRareDataIfNecessary();
 337 
 338         m_rareData-&gt;m_exceptionHandlers = other.m_rareData-&gt;m_exceptionHandlers;
 339         m_rareData-&gt;m_switchJumpTables = other.m_rareData-&gt;m_switchJumpTables;
 340         m_rareData-&gt;m_stringSwitchJumpTables = other.m_rareData-&gt;m_stringSwitchJumpTables;
 341     }
 342 }
 343 
 344 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock, JSScope* scope)
 345     : JSCell(vm, structure)
 346     , m_globalObject(vm, this, scope-&gt;globalObject(vm))
 347     , m_shouldAlwaysBeInlined(true)
 348 #if ENABLE(JIT)
 349     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 350 #endif
 351     , m_didFailJITCompilation(false)
 352     , m_didFailFTLCompilation(false)
 353     , m_hasBeenCompiledWithFTL(false)


 354     , m_numCalleeLocals(unlinkedCodeBlock-&gt;numCalleeLocals())
 355     , m_numVars(unlinkedCodeBlock-&gt;numVars())
 356     , m_hasDebuggerStatement(false)
 357     , m_steppingMode(SteppingModeDisabled)
 358     , m_numBreakpoints(0)
 359     , m_scopeRegister(unlinkedCodeBlock-&gt;scopeRegister())
 360     , m_unlinkedCode(vm, this, unlinkedCodeBlock)
 361     , m_ownerExecutable(vm, this, ownerExecutable)
 362     , m_vm(&amp;vm)
 363     , m_instructionsRawPointer(unlinkedCodeBlock-&gt;instructions().rawPointer())
 364     , m_osrExitCounter(0)
 365     , m_optimizationDelayCounter(0)
 366     , m_reoptimizationRetryCounter(0)
 367     , m_metadata(unlinkedCodeBlock-&gt;metadata().link())
 368     , m_creationTime(MonotonicTime::now())
 369 {
 370     ASSERT(heap()-&gt;isDeferred());
 371     ASSERT(m_scopeRegister.isLocal());
 372 
 373     ASSERT(source().provider());
</pre>
<hr />
<pre>
 382 // outside of this CodeBlock&#39;s compilation unit. It also allows us to generate particular constants that
 383 // we can&#39;t generate during unlinked bytecode generation. This process is not allowed to generate control
 384 // flow or introduce new locals. The reason for this is we rely on liveness analysis to be the same for
 385 // all the CodeBlocks of an UnlinkedCodeBlock. We rely on this fact by caching the liveness analysis
 386 // inside UnlinkedCodeBlock.
 387 bool CodeBlock::finishCreation(VM&amp; vm, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock,
 388     JSScope* scope)
 389 {
 390     Base::finishCreation(vm);
 391     finishCreationCommon(vm);
 392 
 393     auto throwScope = DECLARE_THROW_SCOPE(vm);
 394 
 395     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())
 396         vm.functionHasExecutedCache()-&gt;removeUnexecutedRange(ownerExecutable-&gt;sourceID(), ownerExecutable-&gt;typeProfilingStartOffset(vm), ownerExecutable-&gt;typeProfilingEndOffset(vm));
 397 
 398     ScriptExecutable* topLevelExecutable = ownerExecutable-&gt;topLevelExecutable();
 399     setConstantRegisters(unlinkedCodeBlock-&gt;constantRegisters(), unlinkedCodeBlock-&gt;constantsSourceCodeRepresentation(), topLevelExecutable);
 400     RETURN_IF_EXCEPTION(throwScope, false);
 401 
<span class="line-removed"> 402     for (unsigned i = 0; i &lt; LinkTimeConstantCount; i++) {</span>
<span class="line-removed"> 403         LinkTimeConstant type = static_cast&lt;LinkTimeConstant&gt;(i);</span>
<span class="line-removed"> 404         if (unsigned registerIndex = unlinkedCodeBlock-&gt;registerIndexForLinkTimeConstant(type))</span>
<span class="line-removed"> 405             m_constantRegisters[registerIndex].set(vm, this, m_globalObject-&gt;jsCellForLinkTimeConstant(type));</span>
<span class="line-removed"> 406     }</span>
<span class="line-removed"> 407 </span>
 408     // We already have the cloned symbol table for the module environment since we need to instantiate
 409     // the module environments before linking the code block. We replace the stored symbol table with the already cloned one.
 410     if (UnlinkedModuleProgramCodeBlock* unlinkedModuleProgramCodeBlock = jsDynamicCast&lt;UnlinkedModuleProgramCodeBlock*&gt;(vm, unlinkedCodeBlock)) {
 411         SymbolTable* clonedSymbolTable = jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable)-&gt;moduleEnvironmentSymbolTable();
 412         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {
 413             ConcurrentJSLocker locker(clonedSymbolTable-&gt;m_lock);
 414             clonedSymbolTable-&gt;prepareForTypeProfiling(locker);
 415         }
<span class="line-modified"> 416         replaceConstant(unlinkedModuleProgramCodeBlock-&gt;moduleEnvironmentSymbolTableConstantRegisterOffset(), clonedSymbolTable);</span>
 417     }
 418 
 419     bool shouldUpdateFunctionHasExecutedCache = m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes();
 420     m_functionDecls = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionDecls());
 421     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionDecls(), i = 0; i &lt; count; ++i) {
 422         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionDecl(i);
 423         if (shouldUpdateFunctionHasExecutedCache)
 424             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
 425         m_functionDecls[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));
 426     }
 427 
 428     m_functionExprs = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionExprs());
 429     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionExprs(), i = 0; i &lt; count; ++i) {
 430         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionExpr(i);
 431         if (shouldUpdateFunctionHasExecutedCache)
 432             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
 433         m_functionExprs[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));
 434     }
 435 
 436     if (unlinkedCodeBlock-&gt;hasRareData()) {
 437         createRareDataIfNecessary();
 438 
 439         setConstantIdentifierSetRegisters(vm, unlinkedCodeBlock-&gt;constantIdentifierSets());
 440         RETURN_IF_EXCEPTION(throwScope, false);
 441 
 442         if (size_t count = unlinkedCodeBlock-&gt;numberOfExceptionHandlers()) {
 443             m_rareData-&gt;m_exceptionHandlers.resizeToFit(count);
 444             for (size_t i = 0; i &lt; count; i++) {
 445                 const UnlinkedHandlerInfo&amp; unlinkedHandler = unlinkedCodeBlock-&gt;exceptionHandler(i);
 446                 HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];
 447 #if ENABLE(JIT)
<span class="line-modified"> 448                 auto instruction = instructions().at(unlinkedHandler.target);</span>
<span class="line-modified"> 449                 MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr;</span>
<span class="line-removed"> 450                 if (instruction-&gt;isWide32())</span>
<span class="line-removed"> 451                     codePtr = LLInt::getWide32CodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
<span class="line-removed"> 452                 else if (instruction-&gt;isWide16())</span>
<span class="line-removed"> 453                     codePtr = LLInt::getWide16CodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
<span class="line-removed"> 454                 else</span>
<span class="line-removed"> 455                     codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
 456                 handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));
 457 #else
 458                 handler.initialize(unlinkedHandler);
 459 #endif
 460             }
 461         }
 462 
 463         if (size_t count = unlinkedCodeBlock-&gt;numberOfStringSwitchJumpTables()) {
 464             m_rareData-&gt;m_stringSwitchJumpTables.grow(count);
 465             for (size_t i = 0; i &lt; count; i++) {
 466                 UnlinkedStringJumpTable::StringOffsetTable::iterator ptr = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.begin();
 467                 UnlinkedStringJumpTable::StringOffsetTable::iterator end = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.end();
 468                 for (; ptr != end; ++ptr) {
 469                     OffsetLocation offset;
 470                     offset.branchOffset = ptr-&gt;value.branchOffset;
 471                     m_rareData-&gt;m_stringSwitchJumpTables[i].offsetTable.add(ptr-&gt;key, offset);
 472                 }
 473             }
 474         }
 475 
 476         if (size_t count = unlinkedCodeBlock-&gt;numberOfSwitchJumpTables()) {
 477             m_rareData-&gt;m_switchJumpTables.grow(count);
 478             for (size_t i = 0; i &lt; count; i++) {
 479                 UnlinkedSimpleJumpTable&amp; sourceTable = unlinkedCodeBlock-&gt;switchJumpTable(i);
 480                 SimpleJumpTable&amp; destTable = m_rareData-&gt;m_switchJumpTables[i];
<span class="line-modified"> 481                 destTable.branchOffsets = sourceTable.branchOffsets;</span>

 482                 destTable.min = sourceTable.min;
 483             }
 484         }
 485     }
 486 
 487     // Bookkeep the strongly referenced module environments.
 488     HashSet&lt;JSModuleEnvironment*&gt; stronglyReferencedModuleEnvironments;
 489 
 490     auto link_profile = [&amp;](const auto&amp; /*instruction*/, auto /*bytecode*/, auto&amp; /*metadata*/) {
 491         m_numberOfNonArgumentValueProfiles++;
 492     };
 493 
 494     auto link_objectAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 495         metadata.m_objectAllocationProfile.initializeProfile(vm, m_globalObject.get(), this, m_globalObject-&gt;objectPrototype(), bytecode.m_inlineCapacity);
 496     };
 497 
 498     auto link_arrayAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 499         metadata.m_arrayAllocationProfile.initializeIndexingMode(bytecode.m_recommendedIndexingType);
 500     };
 501 
</pre>
<hr />
<pre>
 521     const InstructionStream&amp; instructionStream = instructions();
 522     for (const auto&amp; instruction : instructionStream) {
 523         OpcodeID opcodeID = instruction-&gt;opcodeID();
 524         m_bytecodeCost += opcodeLengths[opcodeID];
 525         switch (opcodeID) {
 526         LINK(OpHasIndexedProperty)
 527 
 528         LINK(OpCallVarargs, profile)
 529         LINK(OpTailCallVarargs, profile)
 530         LINK(OpTailCallForwardArguments, profile)
 531         LINK(OpConstructVarargs, profile)
 532         LINK(OpGetByVal, profile)
 533 
 534         LINK(OpGetDirectPname, profile)
 535         LINK(OpGetByIdWithThis, profile)
 536         LINK(OpTryGetById, profile)
 537         LINK(OpGetByIdDirect, profile)
 538         LINK(OpGetByValWithThis, profile)
 539         LINK(OpGetFromArguments, profile)
 540         LINK(OpToNumber, profile)

 541         LINK(OpToObject, profile)
 542         LINK(OpGetArgument, profile)

 543         LINK(OpToThis, profile)
 544         LINK(OpBitand, profile)
 545         LINK(OpBitor, profile)
 546         LINK(OpBitnot, profile)
 547         LINK(OpBitxor, profile)
 548         LINK(OpLshift, profile)

 549 
 550         LINK(OpGetById, profile)
 551 
 552         LINK(OpCall, profile)
 553         LINK(OpTailCall, profile)
 554         LINK(OpCallEval, profile)
 555         LINK(OpConstruct, profile)
 556 
 557         LINK(OpInByVal)
 558         LINK(OpPutByVal)
 559         LINK(OpPutByValDirect)
 560 
 561         LINK(OpNewArray)
 562         LINK(OpNewArrayWithSize)
 563         LINK(OpNewArrayBuffer, arrayAllocationProfile)
 564 
 565         LINK(OpNewObject, objectAllocationProfile)
 566 
 567         LINK(OpPutById)
 568         LINK(OpCreateThis)


 569 
 570         LINK(OpAdd)
 571         LINK(OpMul)
 572         LINK(OpDiv)
 573         LINK(OpSub)
 574 
 575         LINK(OpNegate)


 576 
 577         LINK(OpJneqPtr)
 578 
 579         LINK(OpCatch)
 580         LINK(OpProfileControlFlow)
 581 
 582         case op_resolve_scope: {
 583             INITIALIZE_METADATA(OpResolveScope)
 584 
 585             const Identifier&amp; ident = identifier(bytecode.m_var);
 586             RELEASE_ASSERT(bytecode.m_resolveType != LocalClosureVar);
 587 
<span class="line-modified"> 588             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);</span>
 589             RETURN_IF_EXCEPTION(throwScope, false);
 590 
 591             metadata.m_resolveType = op.type;
 592             metadata.m_localScopeDepth = op.depth;
 593             if (op.lexicalEnvironment) {
 594                 if (op.type == ModuleVar) {
 595                     // Keep the linked module environment strongly referenced.
 596                     if (stronglyReferencedModuleEnvironments.add(jsCast&lt;JSModuleEnvironment*&gt;(op.lexicalEnvironment)).isNewEntry)
<span class="line-modified"> 597                         addConstant(op.lexicalEnvironment);</span>
 598                     metadata.m_lexicalEnvironment.set(vm, this, op.lexicalEnvironment);
 599                 } else
 600                     metadata.m_symbolTable.set(vm, this, op.lexicalEnvironment-&gt;symbolTable());
 601             } else if (JSScope* constantScope = JSScope::constantScopeForCodeBlock(op.type, this)) {
 602                 metadata.m_constantScope.set(vm, this, constantScope);
 603                 if (op.type == GlobalProperty || op.type == GlobalPropertyWithVarInjectionChecks)
 604                     metadata.m_globalLexicalBindingEpoch = m_globalObject-&gt;globalLexicalBindingEpoch();
 605             } else
 606                 metadata.m_globalObject.clear();
 607             break;
 608         }
 609 
 610         case op_get_from_scope: {
 611             INITIALIZE_METADATA(OpGetFromScope)
 612 
 613             link_profile(instruction, bytecode, metadata);
 614             metadata.m_watchpointSet = nullptr;
 615 
 616             ASSERT(!isInitialization(bytecode.m_getPutInfo.initializationMode()));
 617             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 618                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 619                 break;
 620             }
 621 
 622             const Identifier&amp; ident = identifier(bytecode.m_var);
<span class="line-modified"> 623             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_getPutInfo.resolveType(), InitializationMode::NotInitialization);</span>
 624             RETURN_IF_EXCEPTION(throwScope, false);
 625 
 626             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 627             if (op.type == ModuleVar)
 628                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 629             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 630                 metadata.m_watchpointSet = op.watchpointSet;
 631             else if (op.structure)
 632                 metadata.m_structure.set(vm, this, op.structure);
 633             metadata.m_operand = op.operand;
 634             break;
 635         }
 636 
 637         case op_put_to_scope: {
 638             INITIALIZE_METADATA(OpPutToScope)
 639 
 640             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 641                 // Only do watching if the property we&#39;re putting to is not anonymous.
 642                 if (bytecode.m_var != UINT_MAX) {
<span class="line-modified"> 643                     SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable().offset()));</span>
 644                     const Identifier&amp; ident = identifier(bytecode.m_var);
 645                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 646                     auto iter = symbolTable-&gt;find(locker, ident.impl());
 647                     ASSERT(iter != symbolTable-&gt;end(locker));
 648                     iter-&gt;value.prepareToWatch();
 649                     metadata.m_watchpointSet = iter-&gt;value.watchpointSet();
 650                 } else
 651                     metadata.m_watchpointSet = nullptr;
 652                 break;
 653             }
 654 
 655             const Identifier&amp; ident = identifier(bytecode.m_var);
 656             metadata.m_watchpointSet = nullptr;
<span class="line-modified"> 657             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_symbolTableOrScopeDepth.scopeDepth(), scope, ident, Put, bytecode.m_getPutInfo.resolveType(), bytecode.m_getPutInfo.initializationMode());</span>
 658             RETURN_IF_EXCEPTION(throwScope, false);
 659 
 660             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 661             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 662                 metadata.m_watchpointSet = op.watchpointSet;
 663             else if (op.type == ClosureVar || op.type == ClosureVarWithVarInjectionChecks) {
 664                 if (op.watchpointSet)
 665                     op.watchpointSet-&gt;invalidate(vm, PutToScopeFireDetail(this, ident));
 666             } else if (op.structure)
 667                 metadata.m_structure.set(vm, this, op.structure);
 668             metadata.m_operand = op.operand;
 669             break;
 670         }
 671 
 672         case op_profile_type: {
 673             RELEASE_ASSERT(m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes());
 674 
 675             INITIALIZE_METADATA(OpProfileType)
 676 
 677             size_t instructionOffset = instruction.offset() + instruction-&gt;size() - 1;
 678             unsigned divotStart, divotEnd;
 679             GlobalVariableID globalVariableID = 0;
 680             RefPtr&lt;TypeSet&gt; globalTypeSet;
 681             bool shouldAnalyze = m_unlinkedCode-&gt;typeProfilerExpressionInfoForBytecodeOffset(instructionOffset, divotStart, divotEnd);
 682             SymbolTable* symbolTable = nullptr;
 683 
 684             switch (bytecode.m_flag) {
 685             case ProfileTypeBytecodeClosureVar: {
 686                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 687                 unsigned localScopeDepth = bytecode.m_symbolTableOrScopeDepth.scopeDepth();
 688                 // Even though type profiling may be profiling either a Get or a Put, we can always claim a Get because
 689                 // we&#39;re abstractly &quot;read&quot;ing from a JSScope.
<span class="line-modified"> 690                 ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);</span>
 691                 RETURN_IF_EXCEPTION(throwScope, false);
 692 
 693                 if (op.type == ClosureVar || op.type == ModuleVar)
 694                     symbolTable = op.lexicalEnvironment-&gt;symbolTable();
 695                 else if (op.type == GlobalVar)
 696                     symbolTable = m_globalObject.get()-&gt;symbolTable();
 697 
 698                 UniquedStringImpl* impl = (op.type == ModuleVar) ? op.importedName.get() : ident.impl();
 699                 if (symbolTable) {
 700                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 701                     // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 702                     symbolTable-&gt;prepareForTypeProfiling(locker);
 703                     globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, impl, vm);
 704                     globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, impl, vm);
 705                 } else
 706                     globalVariableID = TypeProfilerNoGlobalIDExists;
 707 
 708                 break;
 709             }
 710             case ProfileTypeBytecodeLocallyResolved: {
<span class="line-modified"> 711                 int symbolTableIndex = bytecode.m_symbolTableOrScopeDepth.symbolTable().offset();</span>
<span class="line-removed"> 712                 SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(symbolTableIndex));</span>
 713                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 714                 ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 715                 // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 716                 globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, ident.impl(), vm);
 717                 globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, ident.impl(), vm);
 718 
 719                 break;
 720             }
 721             case ProfileTypeBytecodeDoesNotHaveGlobalID:
 722             case ProfileTypeBytecodeFunctionArgument: {
 723                 globalVariableID = TypeProfilerNoGlobalIDExists;
 724                 break;
 725             }
 726             case ProfileTypeBytecodeFunctionReturnStatement: {
 727                 RELEASE_ASSERT(ownerExecutable-&gt;isFunctionExecutable());
 728                 globalTypeSet = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable)-&gt;returnStatementTypeSet();
 729                 globalVariableID = TypeProfilerReturnStatement;
 730                 if (!shouldAnalyze) {
 731                     // Because a return statement can be added implicitly to return undefined at the end of a function,
 732                     // and these nodes don&#39;t emit expression ranges because they aren&#39;t in the actual source text of
</pre>
<hr />
<pre>
 739                 break;
 740             }
 741             }
 742 
 743             std::pair&lt;TypeLocation*, bool&gt; locationPair = vm.typeProfiler()-&gt;typeLocationCache()-&gt;getTypeLocation(globalVariableID,
 744                 ownerExecutable-&gt;sourceID(), divotStart, divotEnd, WTFMove(globalTypeSet), &amp;vm);
 745             TypeLocation* location = locationPair.first;
 746             bool isNewLocation = locationPair.second;
 747 
 748             if (bytecode.m_flag == ProfileTypeBytecodeFunctionReturnStatement)
 749                 location-&gt;m_divotForFunctionOffsetIfReturnStatement = ownerExecutable-&gt;typeProfilingStartOffset(vm);
 750 
 751             if (shouldAnalyze &amp;&amp; isNewLocation)
 752                 vm.typeProfiler()-&gt;insertNewLocation(location);
 753 
 754             metadata.m_typeLocation = location;
 755             break;
 756         }
 757 
 758         case op_debug: {
<span class="line-modified"> 759             if (instruction-&gt;as&lt;OpDebug&gt;().m_debugHookType == DidReachBreakpoint)</span>
 760                 m_hasDebuggerStatement = true;
 761             break;
 762         }
 763 
 764         case op_create_rest: {
 765             int numberOfArgumentsToSkip = instruction-&gt;as&lt;OpCreateRest&gt;().m_numParametersToSkip;
 766             ASSERT_UNUSED(numberOfArgumentsToSkip, numberOfArgumentsToSkip &gt;= 0);
 767             // This is used when rematerializing the rest parameter during OSR exit in the FTL JIT.&quot;);
 768             m_numberOfArgumentsToSkip = numberOfArgumentsToSkip;
 769             break;
 770         }
 771 
 772         default:
 773             break;
 774         }
 775     }
 776 
 777 #undef CASE
 778 #undef INITIALIZE_METADATA
 779 #undef LINK_FIELD
</pre>
<hr />
<pre>
 848     // is no guarantee about the order in which the CodeBlocks are destroyed.
 849     // So, if we don&#39;t remove incoming calls, and get destroyed before the
 850     // CodeBlock(s) that have calls into us, then the CallLinkInfo vector&#39;s
 851     // destructor will try to remove nodes from our (no longer valid) linked list.
 852     unlinkIncomingCalls();
 853 
 854     // Note that our outgoing calls will be removed from other CodeBlocks&#39;
 855     // m_incomingCalls linked lists through the execution of the ~CallLinkInfo
 856     // destructors.
 857 
 858 #if ENABLE(JIT)
 859     if (auto* jitData = m_jitData.get()) {
 860         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
 861             stubInfo-&gt;aboutToDie();
 862             stubInfo-&gt;deref();
 863         }
 864     }
 865 #endif // ENABLE(JIT)
 866 }
 867 
<span class="line-modified"> 868 void CodeBlock::setConstantIdentifierSetRegisters(VM&amp; vm, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants)</span>
 869 {
 870     auto scope = DECLARE_THROW_SCOPE(vm);
 871     JSGlobalObject* globalObject = m_globalObject.get();
<span class="line-removed"> 872     ExecState* exec = globalObject-&gt;globalExec();</span>
 873 
 874     for (const auto&amp; entry : constants) {
 875         const IdentifierSet&amp; set = entry.first;
 876 
 877         Structure* setStructure = globalObject-&gt;setStructure();
 878         RETURN_IF_EXCEPTION(scope, void());
<span class="line-modified"> 879         JSSet* jsSet = JSSet::create(exec, vm, setStructure, set.size());</span>
 880         RETURN_IF_EXCEPTION(scope, void());
 881 
<span class="line-modified"> 882         for (auto setEntry : set) {</span>
 883             JSString* jsString = jsOwnedString(vm, setEntry.get());
<span class="line-modified"> 884             jsSet-&gt;add(exec, jsString);</span>
 885             RETURN_IF_EXCEPTION(scope, void());
 886         }
 887         m_constantRegisters[entry.second].set(vm, this, jsSet);
 888     }
 889 }
 890 
<span class="line-modified"> 891 void CodeBlock::setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable)</span>
 892 {
 893     VM&amp; vm = *m_vm;
 894     auto scope = DECLARE_THROW_SCOPE(vm);
 895     JSGlobalObject* globalObject = m_globalObject.get();
<span class="line-removed"> 896     ExecState* exec = globalObject-&gt;globalExec();</span>
 897 
 898     ASSERT(constants.size() == constantsSourceCodeRepresentation.size());
 899     size_t count = constants.size();
<span class="line-modified"> 900     m_constantRegisters.resizeToFit(count);</span>




 901     for (size_t i = 0; i &lt; count; i++) {
 902         JSValue constant = constants[i].get();
<span class="line-modified"> 903 </span>
<span class="line-modified"> 904         if (!constant.isEmpty()) {</span>
<span class="line-modified"> 905             if (constant.isCell()) {</span>
<span class="line-modified"> 906                 JSCell* cell = constant.asCell();</span>
<span class="line-modified"> 907                 if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm, cell)) {</span>
<span class="line-modified"> 908                     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
<span class="line-modified"> 909                         ConcurrentJSLocker locker(symbolTable-&gt;m_lock);</span>
<span class="line-modified"> 910                         symbolTable-&gt;prepareForTypeProfiling(locker);</span>



















 911                     }
<span class="line-removed"> 912 </span>
<span class="line-removed"> 913                     SymbolTable* clone = symbolTable-&gt;cloneScopePart(vm);</span>
<span class="line-removed"> 914                     if (wasCompiledWithDebuggingOpcodes())</span>
<span class="line-removed"> 915                         clone-&gt;setRareDataCodeBlock(this);</span>
<span class="line-removed"> 916 </span>
<span class="line-removed"> 917                     constant = clone;</span>
<span class="line-removed"> 918                 } else if (auto* descriptor = jsDynamicCast&lt;JSTemplateObjectDescriptor*&gt;(vm, cell)) {</span>
<span class="line-removed"> 919                     auto* templateObject = topLevelExecutable-&gt;createTemplateObject(exec, descriptor);</span>
<span class="line-removed"> 920                     RETURN_IF_EXCEPTION(scope, void());</span>
<span class="line-removed"> 921                     constant = templateObject;</span>
 922                 }
 923             }

 924         }
<span class="line-removed"> 925 </span>
 926         m_constantRegisters[i].set(vm, this, constant);
 927     }
<span class="line-removed"> 928 </span>
<span class="line-removed"> 929     m_constantsSourceCodeRepresentation = constantsSourceCodeRepresentation;</span>
 930 }
 931 
 932 void CodeBlock::setAlternative(VM&amp; vm, CodeBlock* alternative)
 933 {
 934     RELEASE_ASSERT(alternative);
 935     RELEASE_ASSERT(alternative-&gt;jitCode());
 936     m_alternative.set(vm, this, alternative);
 937 }
 938 
 939 void CodeBlock::setNumParameters(int newValue)
 940 {
 941     m_numParameters = newValue;
 942 
 943     m_argumentValueProfiles = RefCountedArray&lt;ValueProfile&gt;(vm().canUseJIT() ? newValue : 0);
 944 }
 945 
 946 CodeBlock* CodeBlock::specialOSREntryBlockOrNull()
 947 {
 948 #if ENABLE(FTL_JIT)
 949     if (jitType() != JITType::DFGJIT)
</pre>
<hr />
<pre>
1060     if (UNLIKELY(Options::forceCodeBlockToJettisonDueToOldAge()))
1061         return true;
1062 
1063     if (timeSinceCreation() &lt; timeToLive(jitType()))
1064         return false;
1065 
1066     return true;
1067 }
1068 
1069 #if ENABLE(DFG_JIT)
1070 static bool shouldMarkTransition(VM&amp; vm, DFG::WeakReferenceTransition&amp; transition)
1071 {
1072     if (transition.m_codeOrigin &amp;&amp; !vm.heap.isMarked(transition.m_codeOrigin.get()))
1073         return false;
1074 
1075     if (!vm.heap.isMarked(transition.m_from.get()))
1076         return false;
1077 
1078     return true;
1079 }









1080 #endif // ENABLE(DFG_JIT)
1081 
1082 void CodeBlock::propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1083 {
<span class="line-removed">1084     UNUSED_PARAM(visitor);</span>
<span class="line-removed">1085 </span>
1086     VM&amp; vm = *m_vm;
1087 
1088     if (jitType() == JITType::InterpreterThunk) {
<span class="line-modified">1089         const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();</span>
<span class="line-modified">1090         const InstructionStream&amp; instructionStream = instructions();</span>
<span class="line-removed">1091         for (size_t i = 0; i &lt; propertyAccessInstructions.size(); ++i) {</span>
<span class="line-removed">1092             auto instruction = instructionStream.at(propertyAccessInstructions[i]);</span>
<span class="line-removed">1093             if (instruction-&gt;is&lt;OpPutById&gt;()) {</span>
<span class="line-removed">1094                 auto&amp; metadata = instruction-&gt;as&lt;OpPutById&gt;().metadata(this);</span>
1095                 StructureID oldStructureID = metadata.m_oldStructureID;
1096                 StructureID newStructureID = metadata.m_newStructureID;
1097                 if (!oldStructureID || !newStructureID)
<span class="line-modified">1098                     continue;</span>
1099                 Structure* oldStructure =
1100                     vm.heap.structureIDTable().get(oldStructureID);
1101                 Structure* newStructure =
1102                     vm.heap.structureIDTable().get(newStructureID);
1103                 if (vm.heap.isMarked(oldStructure))
1104                     visitor.appendUnbarriered(newStructure);
<span class="line-modified">1105                 continue;</span>
<span class="line-removed">1106             }</span>
1107         }
1108     }
1109 
1110 #if ENABLE(JIT)
1111     if (JITCode::isJIT(jitType())) {
1112         if (auto* jitData = m_jitData.get()) {
1113             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1114                 stubInfo-&gt;propagateTransitions(visitor);
1115         }
1116     }
1117 #endif // ENABLE(JIT)
1118 
1119 #if ENABLE(DFG_JIT)
1120     if (JITCode::isOptimizingJIT(jitType())) {
1121         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1122 
1123         dfgCommon-&gt;recordedStatuses.markIfCheap(visitor);
1124 
<span class="line-modified">1125         for (auto&amp; weakReference : dfgCommon-&gt;weakStructureReferences)</span>
<span class="line-modified">1126             weakReference-&gt;markIfCheap(visitor);</span>
1127 
1128         for (auto&amp; transition : dfgCommon-&gt;transitions) {
1129             if (shouldMarkTransition(vm, transition)) {
1130                 // If the following three things are live, then the target of the
1131                 // transition is also live:
1132                 //
1133                 // - This code block. We know it&#39;s live already because otherwise
1134                 //   we wouldn&#39;t be scanning ourselves.
1135                 //
1136                 // - The code origin of the transition. Transitions may arise from
1137                 //   code that was inlined. They are not relevant if the user&#39;s
1138                 //   object that is required for the inlinee to run is no longer
1139                 //   live.
1140                 //
1141                 // - The source of the transition. The transition checks if some
1142                 //   heap location holds the source, and if so, stores the target.
1143                 //   Hence the source must be live for the transition to be live.
1144                 //
1145                 // We also short-circuit the liveness if the structure is harmless
1146                 // to mark (i.e. its global object and prototype are both already
</pre>
<hr />
<pre>
1165     // In rare and weird cases, this could be called on a baseline CodeBlock. One that I found was
1166     // that we might decide that the CodeBlock should be jettisoned due to old age, so the
1167     // isMarked check doesn&#39;t protect us.
1168     if (!JITCode::isOptimizingJIT(jitType()))
1169         return;
1170 
1171     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1172     // Now check all of our weak references. If all of them are live, then we
1173     // have proved liveness and so we scan our strong references. If at end of
1174     // GC we still have not proved liveness, then this code block is toast.
1175     bool allAreLiveSoFar = true;
1176     for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1177         JSCell* reference = dfgCommon-&gt;weakReferences[i].get();
1178         ASSERT(!jsDynamicCast&lt;CodeBlock*&gt;(vm, reference));
1179         if (!vm.heap.isMarked(reference)) {
1180             allAreLiveSoFar = false;
1181             break;
1182         }
1183     }
1184     if (allAreLiveSoFar) {
<span class="line-modified">1185         for (unsigned i = 0; i &lt; dfgCommon-&gt;weakStructureReferences.size(); ++i) {</span>
<span class="line-modified">1186             if (!vm.heap.isMarked(dfgCommon-&gt;weakStructureReferences[i].get())) {</span>

1187                 allAreLiveSoFar = false;
1188                 break;
1189             }
1190         }
1191     }
1192 
1193     // If some weak references are dead, then this fixpoint iteration was
1194     // unsuccessful.
1195     if (!allAreLiveSoFar)
1196         return;
1197 
1198     // All weak references are live. Record this information so we don&#39;t
1199     // come back here again, and scan the strong references.
1200     visitor.appendUnbarriered(this);
1201 #endif // ENABLE(DFG_JIT)
1202 }
1203 
1204 void CodeBlock::finalizeLLIntInlineCaches()
1205 {
1206     VM&amp; vm = *m_vm;
<span class="line-removed">1207     const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();</span>
1208 
<span class="line-modified">1209     auto handleGetPutFromScope = [&amp;] (auto&amp; metadata) {</span>
<span class="line-modified">1210         GetPutInfo getPutInfo = metadata.m_getPutInfo;</span>
<span class="line-modified">1211         if (getPutInfo.resolveType() == GlobalVar || getPutInfo.resolveType() == GlobalVarWithVarInjectionChecks</span>
<span class="line-removed">1212             || getPutInfo.resolveType() == LocalClosureVar || getPutInfo.resolveType() == GlobalLexicalVar || getPutInfo.resolveType() == GlobalLexicalVarWithVarInjectionChecks)</span>
<span class="line-removed">1213             return;</span>
<span class="line-removed">1214         WriteBarrierBase&lt;Structure&gt;&amp; structure = metadata.m_structure;</span>
<span class="line-removed">1215         if (!structure || vm.heap.isMarked(structure.get()))</span>
<span class="line-removed">1216             return;</span>
<span class="line-removed">1217         if (Options::verboseOSR())</span>
<span class="line-removed">1218             dataLogF(&quot;Clearing scope access with structure %p.\n&quot;, structure.get());</span>
<span class="line-removed">1219         structure.clear();</span>
<span class="line-removed">1220     };</span>
1221 
<span class="line-modified">1222     const InstructionStream&amp; instructionStream = instructions();</span>
<span class="line-removed">1223     for (size_t size = propertyAccessInstructions.size(), i = 0; i &lt; size; ++i) {</span>
<span class="line-removed">1224         const auto curInstruction = instructionStream.at(propertyAccessInstructions[i]);</span>
<span class="line-removed">1225         switch (curInstruction-&gt;opcodeID()) {</span>
<span class="line-removed">1226         case op_get_by_id: {</span>
<span class="line-removed">1227             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetById&gt;().metadata(this);</span>
1228             if (metadata.m_modeMetadata.mode != GetByIdMode::Default)
<span class="line-modified">1229                 break;</span>
1230             StructureID oldStructureID = metadata.m_modeMetadata.defaultMode.structureID;
1231             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
<span class="line-modified">1232                 break;</span>
<span class="line-modified">1233             if (Options::verboseOSR())</span>
<span class="line-removed">1234                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);</span>
1235             LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(metadata);
<span class="line-modified">1236             break;</span>
<span class="line-modified">1237         }</span>
<span class="line-modified">1238         case op_get_by_id_direct: {</span>
<span class="line-removed">1239             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetByIdDirect&gt;().metadata(this);</span>
1240             StructureID oldStructureID = metadata.m_structureID;
1241             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
<span class="line-modified">1242                 break;</span>
<span class="line-modified">1243             if (Options::verboseOSR())</span>
<span class="line-removed">1244                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);</span>
1245             metadata.m_structureID = 0;
1246             metadata.m_offset = 0;
<span class="line-modified">1247             break;</span>
<span class="line-modified">1248         }</span>
<span class="line-modified">1249         case op_put_by_id: {</span>
<span class="line-removed">1250             auto&amp; metadata = curInstruction-&gt;as&lt;OpPutById&gt;().metadata(this);</span>
1251             StructureID oldStructureID = metadata.m_oldStructureID;
1252             StructureID newStructureID = metadata.m_newStructureID;
1253             StructureChain* chain = metadata.m_structureChain.get();
1254             if ((!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
1255                 &amp;&amp; (!newStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(newStructureID)))
1256                 &amp;&amp; (!chain || vm.heap.isMarked(chain)))
<span class="line-modified">1257                 break;</span>
<span class="line-modified">1258             if (Options::verboseOSR())</span>
<span class="line-removed">1259                 dataLogF(&quot;Clearing LLInt put transition.\n&quot;);</span>
1260             metadata.m_oldStructureID = 0;
1261             metadata.m_offset = 0;
1262             metadata.m_newStructureID = 0;
1263             metadata.m_structureChain.clear();
<span class="line-modified">1264             break;</span>
<span class="line-modified">1265         }</span>
<span class="line-modified">1266         // FIXME: https://bugs.webkit.org/show_bug.cgi?id=166418</span>
<span class="line-removed">1267         // We need to add optimizations for op_resolve_scope_for_hoisting_func_decl_in_eval to do link time scope resolution.</span>
<span class="line-removed">1268         case op_resolve_scope_for_hoisting_func_decl_in_eval:</span>
<span class="line-removed">1269             break;</span>
<span class="line-removed">1270         case op_to_this: {</span>
<span class="line-removed">1271             auto&amp; metadata = curInstruction-&gt;as&lt;OpToThis&gt;().metadata(this);</span>
1272             if (!metadata.m_cachedStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(metadata.m_cachedStructureID)))
<span class="line-modified">1273                 break;</span>
1274             if (Options::verboseOSR()) {
1275                 Structure* structure = vm.heap.structureIDTable().get(metadata.m_cachedStructureID);
1276                 dataLogF(&quot;Clearing LLInt to_this with structure %p.\n&quot;, structure);
1277             }
1278             metadata.m_cachedStructureID = 0;
1279             metadata.m_toThisStatus = merge(metadata.m_toThisStatus, ToThisClearedByGC);
<span class="line-modified">1280             break;</span>
<span class="line-modified">1281         }</span>
<span class="line-modified">1282         case op_create_this: {</span>
<span class="line-removed">1283             auto&amp; metadata = curInstruction-&gt;as&lt;OpCreateThis&gt;().metadata(this);</span>
1284             auto&amp; cacheWriteBarrier = metadata.m_cachedCallee;
1285             if (!cacheWriteBarrier || cacheWriteBarrier.unvalidatedGet() == JSCell::seenMultipleCalleeObjects())
<span class="line-modified">1286                 break;</span>
1287             JSCell* cachedFunction = cacheWriteBarrier.get();
1288             if (vm.heap.isMarked(cachedFunction))
<span class="line-modified">1289                 break;</span>
<span class="line-modified">1290             if (Options::verboseOSR())</span>
<span class="line-removed">1291                 dataLogF(&quot;Clearing LLInt create_this with cached callee %p.\n&quot;, cachedFunction);</span>
1292             cacheWriteBarrier.clear();
<span class="line-modified">1293             break;</span>
<span class="line-modified">1294         }</span>
<span class="line-modified">1295         case op_resolve_scope: {</span>













1296             // Right now this isn&#39;t strictly necessary. Any symbol tables that this will refer to
1297             // are for outer functions, and we refer to those functions strongly, and they refer
1298             // to the symbol table strongly. But it&#39;s nice to be on the safe side.
<span class="line-removed">1299             auto&amp; metadata = curInstruction-&gt;as&lt;OpResolveScope&gt;().metadata(this);</span>
1300             WriteBarrierBase&lt;SymbolTable&gt;&amp; symbolTable = metadata.m_symbolTable;
1301             if (!symbolTable || vm.heap.isMarked(symbolTable.get()))
<span class="line-modified">1302                 break;</span>
<span class="line-modified">1303             if (Options::verboseOSR())</span>
<span class="line-removed">1304                 dataLogF(&quot;Clearing dead symbolTable %p.\n&quot;, symbolTable.get());</span>
1305             symbolTable.clear();
<span class="line-modified">1306             break;</span>
<span class="line-modified">1307         }</span>
<span class="line-modified">1308         case op_get_from_scope:</span>
<span class="line-modified">1309             handleGetPutFromScope(curInstruction-&gt;as&lt;OpGetFromScope&gt;().metadata(this));</span>
<span class="line-modified">1310             break;</span>
<span class="line-modified">1311         case op_put_to_scope:</span>
<span class="line-modified">1312             handleGetPutFromScope(curInstruction-&gt;as&lt;OpPutToScope&gt;().metadata(this));</span>
<span class="line-modified">1313             break;</span>
<span class="line-modified">1314         default:</span>
<span class="line-modified">1315             OpcodeID opcodeID = curInstruction-&gt;opcodeID();</span>
<span class="line-modified">1316             ASSERT_WITH_MESSAGE_UNUSED(opcodeID, false, &quot;Unhandled opcode in CodeBlock::finalizeUnconditionally, %s(%d) at bc %u&quot;, opcodeNames[opcodeID], opcodeID, propertyAccessInstructions[i]);</span>
<span class="line-modified">1317         }</span>




1318     }
1319 
1320     // We can&#39;t just remove all the sets when we clear the caches since we might have created a watchpoint set
1321     // then cleared the cache without GCing in between.
1322     m_llintGetByIdWatchpointMap.removeIf([&amp;] (const StructureWatchpointMap::KeyValuePairType&amp; pair) -&gt; bool {
1323         auto clear = [&amp;] () {
1324             auto&amp; instruction = instructions().at(std::get&lt;1&gt;(pair.key));
1325             OpcodeID opcode = instruction-&gt;opcodeID();
1326             if (opcode == op_get_by_id) {
<span class="line-modified">1327                 if (Options::verboseOSR())</span>
<span class="line-removed">1328                     dataLogF(&quot;Clearing LLInt property access.\n&quot;);</span>
1329                 LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(instruction-&gt;as&lt;OpGetById&gt;().metadata(this));
1330             }
1331             return true;
1332         };
1333 
1334         if (!vm.heap.isMarked(vm.heap.structureIDTable().get(std::get&lt;0&gt;(pair.key))))
1335             return clear();
1336 
1337         for (const LLIntPrototypeLoadAdaptiveStructureWatchpoint&amp; watchpoint : pair.value) {
1338             if (!watchpoint.key().isStillLive(vm))
1339                 return clear();
1340         }
1341 
1342         return false;
1343     });
1344 
1345     forEachLLIntCallLinkInfo([&amp;](LLIntCallLinkInfo&amp; callLinkInfo) {
1346         if (callLinkInfo.isLinked() &amp;&amp; !vm.heap.isMarked(callLinkInfo.callee())) {
<span class="line-modified">1347             if (Options::verboseOSR())</span>
<span class="line-removed">1348                 dataLog(&quot;Clearing LLInt call from &quot;, *this, &quot;\n&quot;);</span>
1349             callLinkInfo.unlink();
1350         }
1351         if (callLinkInfo.lastSeenCallee() &amp;&amp; !vm.heap.isMarked(callLinkInfo.lastSeenCallee()))
1352             callLinkInfo.clearLastSeenCallee();
1353     });
1354 }
1355 
1356 #if ENABLE(JIT)
1357 CodeBlock::JITData&amp; CodeBlock::ensureJITDataSlow(const ConcurrentJSLocker&amp;)
1358 {
1359     ASSERT(!m_jitData);
<span class="line-modified">1360     m_jitData = makeUnique&lt;JITData&gt;();</span>




1361     return *m_jitData;
1362 }
1363 
1364 void CodeBlock::finalizeBaselineJITInlineCaches()
1365 {
1366     if (auto* jitData = m_jitData.get()) {
1367         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1368             callLinkInfo-&gt;visitWeak(vm());
1369 
1370         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1371             stubInfo-&gt;visitWeakReferences(this);
1372     }
1373 }
1374 #endif
1375 
1376 void CodeBlock::finalizeUnconditionally(VM&amp; vm)
1377 {
1378     UNUSED_PARAM(vm);
1379 
1380     updateAllPredictions();
1381 



































1382     if (JITCode::couldBeInterpreted(jitType()))
1383         finalizeLLIntInlineCaches();
1384 
1385 #if ENABLE(JIT)
1386     if (!!jitCode())
1387         finalizeBaselineJITInlineCaches();
1388 #endif
1389 
1390 #if ENABLE(DFG_JIT)
1391     if (JITCode::isOptimizingJIT(jitType())) {
1392         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1393         dfgCommon-&gt;recordedStatuses.finalize(vm);
1394     }
1395 #endif // ENABLE(DFG_JIT)
1396 
1397     auto updateActivity = [&amp;] {
1398         if (!VM::useUnlinkedCodeBlockJettisoning())
1399             return;
1400         JITCode* jitCode = m_jitCode.get();
1401         double count = 0;
</pre>
<hr />
<pre>
1464 #endif
1465     }
1466 #else
1467     UNUSED_PARAM(result);
1468 #endif
1469 }
1470 
1471 void CodeBlock::getICStatusMap(ICStatusMap&amp; result)
1472 {
1473     ConcurrentJSLocker locker(m_lock);
1474     getICStatusMap(locker, result);
1475 }
1476 
1477 #if ENABLE(JIT)
1478 StructureStubInfo* CodeBlock::addStubInfo(AccessType accessType)
1479 {
1480     ConcurrentJSLocker locker(m_lock);
1481     return ensureJITData(locker).m_stubInfos.add(accessType);
1482 }
1483 
<span class="line-modified">1484 JITAddIC* CodeBlock::addJITAddIC(ArithProfile* arithProfile)</span>
1485 {
1486     ConcurrentJSLocker locker(m_lock);
1487     return ensureJITData(locker).m_addICs.add(arithProfile);
1488 }
1489 
<span class="line-modified">1490 JITMulIC* CodeBlock::addJITMulIC(ArithProfile* arithProfile)</span>
1491 {
1492     ConcurrentJSLocker locker(m_lock);
1493     return ensureJITData(locker).m_mulICs.add(arithProfile);
1494 }
1495 
<span class="line-modified">1496 JITSubIC* CodeBlock::addJITSubIC(ArithProfile* arithProfile)</span>
1497 {
1498     ConcurrentJSLocker locker(m_lock);
1499     return ensureJITData(locker).m_subICs.add(arithProfile);
1500 }
1501 
<span class="line-modified">1502 JITNegIC* CodeBlock::addJITNegIC(ArithProfile* arithProfile)</span>
1503 {
1504     ConcurrentJSLocker locker(m_lock);
1505     return ensureJITData(locker).m_negICs.add(arithProfile);
1506 }
1507 
1508 StructureStubInfo* CodeBlock::findStubInfo(CodeOrigin codeOrigin)
1509 {
1510     ConcurrentJSLocker locker(m_lock);
1511     if (auto* jitData = m_jitData.get()) {
1512         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
1513             if (stubInfo-&gt;codeOrigin == codeOrigin)
1514                 return stubInfo;
1515         }
1516     }
1517     return nullptr;
1518 }
1519 
1520 ByValInfo* CodeBlock::addByValInfo()
1521 {
1522     ConcurrentJSLocker locker(m_lock);
1523     return ensureJITData(locker).m_byValInfos.add();
1524 }
1525 
1526 CallLinkInfo* CodeBlock::addCallLinkInfo()
1527 {
1528     ConcurrentJSLocker locker(m_lock);
1529     return ensureJITData(locker).m_callLinkInfos.add();
1530 }
1531 
<span class="line-modified">1532 CallLinkInfo* CodeBlock::getCallLinkInfoForBytecodeIndex(unsigned index)</span>
1533 {
1534     ConcurrentJSLocker locker(m_lock);
1535     if (auto* jitData = m_jitData.get()) {
1536         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos) {
1537             if (callLinkInfo-&gt;codeOrigin() == CodeOrigin(index))
1538                 return callLinkInfo;
1539         }
1540     }
1541     return nullptr;
1542 }
1543 
<span class="line-modified">1544 RareCaseProfile* CodeBlock::addRareCaseProfile(int bytecodeOffset)</span>
1545 {
1546     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1547     auto&amp; jitData = ensureJITData(locker);</span>
<span class="line-removed">1548     jitData.m_rareCaseProfiles.append(RareCaseProfile(bytecodeOffset));</span>
<span class="line-removed">1549     return &amp;jitData.m_rareCaseProfiles.last();</span>
1550 }
1551 
<span class="line-modified">1552 RareCaseProfile* CodeBlock::rareCaseProfileForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset)</span>
1553 {
1554     if (auto* jitData = m_jitData.get()) {
<span class="line-modified">1555         return tryBinarySearch&lt;RareCaseProfile, int&gt;(</span>
<span class="line-modified">1556             jitData-&gt;m_rareCaseProfiles, jitData-&gt;m_rareCaseProfiles.size(), bytecodeOffset,</span>
<span class="line-modified">1557             getRareCaseProfileBytecodeOffset);</span>
1558     }
1559     return nullptr;
1560 }
1561 
<span class="line-modified">1562 unsigned CodeBlock::rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp; locker, int bytecodeOffset)</span>
1563 {
<span class="line-modified">1564     RareCaseProfile* profile = rareCaseProfileForBytecodeOffset(locker, bytecodeOffset);</span>
1565     if (profile)
1566         return profile-&gt;m_counter;
1567     return 0;
1568 }
1569 
1570 void CodeBlock::setCalleeSaveRegisters(RegisterSet calleeSaveRegisters)
1571 {
1572     ConcurrentJSLocker locker(m_lock);
1573     ensureJITData(locker).m_calleeSaveRegisters = makeUnique&lt;RegisterAtOffsetList&gt;(calleeSaveRegisters);
1574 }
1575 
1576 void CodeBlock::setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt; registerAtOffsetList)
1577 {
1578     ConcurrentJSLocker locker(m_lock);
1579     ensureJITData(locker).m_calleeSaveRegisters = WTFMove(registerAtOffsetList);
1580 }
1581 
1582 void CodeBlock::resetJITData()
1583 {
1584     RELEASE_ASSERT(!JITCode::isJIT(jitType()));
1585     ConcurrentJSLocker locker(m_lock);
1586 
1587     if (auto* jitData = m_jitData.get()) {
1588         // We can clear these because no other thread will have references to any stub infos, call
1589         // link infos, or by val infos if we don&#39;t have JIT code. Attempts to query these data
1590         // structures using the concurrent API (getICStatusMap and friends) will return nothing if we
<span class="line-modified">1591         // don&#39;t have JIT code.</span>
<span class="line-modified">1592         jitData-&gt;m_stubInfos.clear();</span>
<span class="line-modified">1593         jitData-&gt;m_callLinkInfos.clear();</span>
<span class="line-modified">1594         jitData-&gt;m_byValInfos.clear();</span>










1595         // We can clear this because the DFG&#39;s queries to these data structures are guarded by whether
1596         // there is JIT code.
<span class="line-modified">1597         jitData-&gt;m_rareCaseProfiles.clear();</span>

1598     }
1599 }
1600 #endif
1601 
1602 void CodeBlock::visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1603 {
1604     // We strongly visit OSR exits targets because we don&#39;t want to deal with
1605     // the complexity of generating an exit target CodeBlock on demand and
1606     // guaranteeing that it matches the details of the CodeBlock we compiled
1607     // the OSR exit against.
1608 
1609     visitor.append(m_alternative);
1610 
1611 #if ENABLE(DFG_JIT)
1612     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1613     if (dfgCommon-&gt;inlineCallFrames) {
1614         for (auto* inlineCallFrame : *dfgCommon-&gt;inlineCallFrames) {
1615             ASSERT(inlineCallFrame-&gt;baselineCodeBlock);
1616             visitor.append(inlineCallFrame-&gt;baselineCodeBlock);
1617         }
</pre>
<hr />
<pre>
1624     UNUSED_PARAM(locker);
1625 
1626     visitor.append(m_globalObject);
1627     visitor.append(m_ownerExecutable); // This is extra important since it causes the ExecutableToCodeBlockEdge to be marked.
1628     visitor.append(m_unlinkedCode);
1629     if (m_rareData)
1630         m_rareData-&gt;m_directEvalCodeCache.visitAggregate(visitor);
1631     visitor.appendValues(m_constantRegisters.data(), m_constantRegisters.size());
1632     for (auto&amp; functionExpr : m_functionExprs)
1633         visitor.append(functionExpr);
1634     for (auto&amp; functionDecl : m_functionDecls)
1635         visitor.append(functionDecl);
1636     forEachObjectAllocationProfile([&amp;](ObjectAllocationProfile&amp; objectAllocationProfile) {
1637         objectAllocationProfile.visitAggregate(visitor);
1638     });
1639 
1640 #if ENABLE(JIT)
1641     if (auto* jitData = m_jitData.get()) {
1642         for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1643             visitor.append(byValInfo-&gt;cachedSymbol);


1644     }
1645 #endif
1646 
1647 #if ENABLE(DFG_JIT)
<span class="line-modified">1648     if (JITCode::isOptimizingJIT(jitType()))</span>


1649         visitOSRExitTargets(locker, visitor);

1650 #endif
1651 }
1652 
1653 void CodeBlock::stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1654 {
1655     UNUSED_PARAM(visitor);
1656 
1657 #if ENABLE(DFG_JIT)
1658     if (!JITCode::isOptimizingJIT(jitType()))
1659         return;
1660 
1661     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1662 
1663     for (auto&amp; transition : dfgCommon-&gt;transitions) {
1664         if (!!transition.m_codeOrigin)
1665             visitor.append(transition.m_codeOrigin); // Almost certainly not necessary, since the code origin should also be a weak reference. Better to be safe, though.
1666         visitor.append(transition.m_from);
1667         visitor.append(transition.m_to);
1668     }
1669 
1670     for (auto&amp; weakReference : dfgCommon-&gt;weakReferences)
1671         visitor.append(weakReference);
1672 
<span class="line-modified">1673     for (auto&amp; weakStructureReference : dfgCommon-&gt;weakStructureReferences)</span>
<span class="line-modified">1674         visitor.append(weakStructureReference);</span>
1675 
1676     dfgCommon-&gt;livenessHasBeenProved = true;
1677 #endif
1678 }
1679 
1680 CodeBlock* CodeBlock::baselineAlternative()
1681 {
1682 #if ENABLE(JIT)
1683     CodeBlock* result = this;
1684     while (result-&gt;alternative())
1685         result = result-&gt;alternative();
1686     RELEASE_ASSERT(result);
1687     RELEASE_ASSERT(JITCode::isBaselineCode(result-&gt;jitType()) || result-&gt;jitType() == JITType::None);
1688     return result;
1689 #else
1690     return this;
1691 #endif
1692 }
1693 
1694 CodeBlock* CodeBlock::baselineVersion()
</pre>
<hr />
<pre>
1704             // has been purged of its codeBlocks (see ExecutableBase::clearCode()). Regardless,
1705             // the current codeBlock is still live on the stack, and as an optimizing JIT
1706             // codeBlock, it will keep its baselineAlternative() alive for us to fetch below.
1707             result = this;
1708         } else {
1709             // This can happen if we&#39;re creating the original CodeBlock for an executable.
1710             // Assume that we&#39;re the baseline CodeBlock.
1711             RELEASE_ASSERT(selfJITType == JITType::None);
1712             return this;
1713         }
1714     }
1715     result = result-&gt;baselineAlternative();
1716     ASSERT(result);
1717     return result;
1718 #else
1719     return this;
1720 #endif
1721 }
1722 
1723 #if ENABLE(JIT)
<span class="line-modified">1724 bool CodeBlock::hasOptimizedReplacement(JITType typeToReplace)</span>
1725 {
1726     CodeBlock* replacement = this-&gt;replacement();
<span class="line-modified">1727     return replacement &amp;&amp; JITCode::isHigherTier(replacement-&gt;jitType(), typeToReplace);</span>














1728 }
1729 
1730 bool CodeBlock::hasOptimizedReplacement()
1731 {
1732     return hasOptimizedReplacement(jitType());
1733 }
1734 #endif
1735 
<span class="line-modified">1736 HandlerInfo* CodeBlock::handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler requiredHandler)</span>
1737 {
<span class="line-modified">1738     RELEASE_ASSERT(bytecodeOffset &lt; instructions().size());</span>
<span class="line-modified">1739     return handlerForIndex(bytecodeOffset, requiredHandler);</span>
1740 }
1741 
1742 HandlerInfo* CodeBlock::handlerForIndex(unsigned index, RequiredHandler requiredHandler)
1743 {
1744     if (!m_rareData)
1745         return 0;
<span class="line-modified">1746     return HandlerInfo::handlerForIndex(m_rareData-&gt;m_exceptionHandlers, index, requiredHandler);</span>
1747 }
1748 
1749 DisposableCallSiteIndex CodeBlock::newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite)
1750 {
1751 #if ENABLE(DFG_JIT)
1752     RELEASE_ASSERT(JITCode::isOptimizingJIT(jitType()));
1753     RELEASE_ASSERT(canGetCodeOrigin(originalCallSite));
1754     ASSERT(!!handlerForIndex(originalCallSite.bits()));
1755     CodeOrigin originalOrigin = codeOrigin(originalCallSite);
1756     return m_jitCode-&gt;dfgCommon()-&gt;addDisposableCallSiteIndex(originalOrigin);
1757 #else
1758     // We never create new on-the-fly exception handling
1759     // call sites outside the DFG/FTL inline caches.
1760     UNUSED_PARAM(originalCallSite);
1761     RELEASE_ASSERT_NOT_REACHED();
1762     return DisposableCallSiteIndex(0u);
1763 #endif
1764 }
1765 
1766 
1767 
<span class="line-modified">1768 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeOffset(InstructionStream::Offset bytecodeOffset)</span>
1769 {
<span class="line-modified">1770     auto&amp; instruction = instructions().at(bytecodeOffset);</span>
1771     OpCatch op = instruction-&gt;as&lt;OpCatch&gt;();
1772     auto&amp; metadata = op.metadata(this);
1773     if (!!metadata.m_buffer) {
<span class="line-modified">1774 #if !ASSERT_DISABLED</span>
1775         ConcurrentJSLocker locker(m_lock);
1776         bool found = false;
1777         auto* rareData = m_rareData.get();
1778         ASSERT(rareData);
1779         for (auto&amp; profile : rareData-&gt;m_catchProfiles) {
1780             if (profile.get() == metadata.m_buffer) {
1781                 found = true;
1782                 break;
1783             }
1784         }
1785         ASSERT(found);
<span class="line-modified">1786 #endif</span>
1787         return;
1788     }
1789 
<span class="line-modified">1790     ensureCatchLivenessIsComputedForBytecodeOffsetSlow(op, bytecodeOffset);</span>
1791 }
1792 
<span class="line-modified">1793 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp; op, InstructionStream::Offset bytecodeOffset)</span>
1794 {
1795     BytecodeLivenessAnalysis&amp; bytecodeLiveness = livenessAnalysis();
1796 
1797     // We get the live-out set of variables at op_catch, not the live-in. This
1798     // is because the variables that the op_catch defines might be dead, and
1799     // we can avoid profiling them and extracting them when doing OSR entry
1800     // into the DFG.
1801 
<span class="line-modified">1802     auto nextOffset = instructions().at(bytecodeOffset).next().offset();</span>
<span class="line-modified">1803     FastBitVector liveLocals = bytecodeLiveness.getLivenessInfoAtBytecodeOffset(this, nextOffset);</span>
1804     Vector&lt;VirtualRegister&gt; liveOperands;
1805     liveOperands.reserveInitialCapacity(liveLocals.bitCount());
1806     liveLocals.forEachSetBit([&amp;] (unsigned liveLocal) {
1807         liveOperands.append(virtualRegisterForLocal(liveLocal));
1808     });
1809 
1810     for (int i = 0; i &lt; numParameters(); ++i)
<span class="line-modified">1811         liveOperands.append(virtualRegisterForArgument(i));</span>
1812 
<span class="line-modified">1813     auto profiles = makeUnique&lt;ValueProfileAndOperandBuffer&gt;(liveOperands.size());</span>
1814     RELEASE_ASSERT(profiles-&gt;m_size == liveOperands.size());
1815     for (unsigned i = 0; i &lt; profiles-&gt;m_size; ++i)
<span class="line-modified">1816         profiles-&gt;m_buffer.get()[i].m_operand = liveOperands[i].offset();</span>
1817 
1818     createRareDataIfNecessary();
1819 
1820     // The compiler thread will read this pointer value and then proceed to dereference it
1821     // if it is not null. We need to make sure all above stores happen before this store so
1822     // the compiler thread reads fully initialized data.
1823     WTF::storeStoreFence();
1824 
1825     op.metadata(this).m_buffer = profiles.get();
1826     {
1827         ConcurrentJSLocker locker(m_lock);
1828         m_rareData-&gt;m_catchProfiles.append(WTFMove(profiles));
1829     }
1830 }
1831 
1832 void CodeBlock::removeExceptionHandlerForCallSite(DisposableCallSiteIndex callSiteIndex)
1833 {
1834     RELEASE_ASSERT(m_rareData);
1835     Vector&lt;HandlerInfo&gt;&amp; exceptionHandlers = m_rareData-&gt;m_exceptionHandlers;
1836     unsigned index = callSiteIndex.bits();
1837     for (size_t i = 0; i &lt; exceptionHandlers.size(); ++i) {
1838         HandlerInfo&amp; handler = exceptionHandlers[i];
1839         if (handler.start &lt;= index &amp;&amp; handler.end &gt; index) {
1840             exceptionHandlers.remove(i);
1841             return;
1842         }
1843     }
1844 
1845     RELEASE_ASSERT_NOT_REACHED();
1846 }
1847 
<span class="line-modified">1848 unsigned CodeBlock::lineNumberForBytecodeOffset(unsigned bytecodeOffset)</span>
1849 {
<span class="line-modified">1850     RELEASE_ASSERT(bytecodeOffset &lt; instructions().size());</span>
<span class="line-modified">1851     return ownerExecutable()-&gt;firstLine() + m_unlinkedCode-&gt;lineNumberForBytecodeOffset(bytecodeOffset);</span>
1852 }
1853 
<span class="line-modified">1854 unsigned CodeBlock::columnNumberForBytecodeOffset(unsigned bytecodeOffset)</span>
1855 {
1856     int divot;
1857     int startOffset;
1858     int endOffset;
1859     unsigned line;
1860     unsigned column;
<span class="line-modified">1861     expressionRangeForBytecodeOffset(bytecodeOffset, divot, startOffset, endOffset, line, column);</span>
1862     return column;
1863 }
1864 
<span class="line-modified">1865 void CodeBlock::expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot, int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const</span>
1866 {
<span class="line-modified">1867     m_unlinkedCode-&gt;expressionRangeForBytecodeOffset(bytecodeOffset, divot, startOffset, endOffset, line, column);</span>
1868     divot += sourceOffset();
1869     column += line ? 1 : firstLineColumnOffset();
1870     line += ownerExecutable()-&gt;firstLine();
1871 }
1872 
<span class="line-modified">1873 bool CodeBlock::hasOpDebugForLineAndColumn(unsigned line, unsigned column)</span>
1874 {
1875     const InstructionStream&amp; instructionStream = instructions();
1876     for (const auto&amp; it : instructionStream) {
1877         if (it-&gt;is&lt;OpDebug&gt;()) {
1878             int unused;
1879             unsigned opDebugLine;
1880             unsigned opDebugColumn;
<span class="line-modified">1881             expressionRangeForBytecodeOffset(it.offset(), unused, unused, unused, opDebugLine, opDebugColumn);</span>
<span class="line-modified">1882             if (line == opDebugLine &amp;&amp; (column == Breakpoint::unspecifiedColumn || column == opDebugColumn))</span>
1883                 return true;
1884         }
1885     }
1886     return false;
1887 }
1888 
<span class="line-modified">1889 void CodeBlock::shrinkToFit(ShrinkMode shrinkMode)</span>
1890 {
<span class="line-modified">1891     ConcurrentJSLocker locker(m_lock);</span>
<span class="line-modified">1892 </span>
<span class="line-modified">1893 #if ENABLE(JIT)</span>
<span class="line-removed">1894     if (auto* jitData = m_jitData.get())</span>
<span class="line-removed">1895         jitData-&gt;m_rareCaseProfiles.shrinkToFit();</span>
<span class="line-removed">1896 #endif</span>
<span class="line-removed">1897 </span>
<span class="line-removed">1898     if (shrinkMode == EarlyShrink) {</span>
1899         m_constantRegisters.shrinkToFit();
<span class="line-modified">1900         m_constantsSourceCodeRepresentation.shrinkToFit();</span>



1901 

1902         if (m_rareData) {
1903             m_rareData-&gt;m_switchJumpTables.shrinkToFit();
1904             m_rareData-&gt;m_stringSwitchJumpTables.shrinkToFit();
1905         }
1906     } // else don&#39;t shrink these, because we would have already pointed pointers into these tables.
1907 }
1908 
1909 #if ENABLE(JIT)
<span class="line-modified">1910 void CodeBlock::linkIncomingCall(ExecState* callerFrame, CallLinkInfo* incoming)</span>
1911 {
1912     noticeIncomingCall(callerFrame);
1913     ConcurrentJSLocker locker(m_lock);
1914     ensureJITData(locker).m_incomingCalls.push(incoming);
1915 }
1916 
<span class="line-modified">1917 void CodeBlock::linkIncomingPolymorphicCall(ExecState* callerFrame, PolymorphicCallNode* incoming)</span>
1918 {
1919     noticeIncomingCall(callerFrame);
1920     {
1921         ConcurrentJSLocker locker(m_lock);
1922         ensureJITData(locker).m_incomingPolymorphicCalls.push(incoming);
1923     }
1924 }
1925 #endif // ENABLE(JIT)
1926 
1927 void CodeBlock::unlinkIncomingCalls()
1928 {
1929     while (m_incomingLLIntCalls.begin() != m_incomingLLIntCalls.end())
1930         m_incomingLLIntCalls.begin()-&gt;unlink();
1931 #if ENABLE(JIT)
1932     JITData* jitData = nullptr;
1933     {
1934         ConcurrentJSLocker locker(m_lock);
1935         jitData = m_jitData.get();
1936     }
1937     if (jitData) {
1938         while (jitData-&gt;m_incomingCalls.begin() != jitData-&gt;m_incomingCalls.end())
1939             jitData-&gt;m_incomingCalls.begin()-&gt;unlink(vm());
1940         while (jitData-&gt;m_incomingPolymorphicCalls.begin() != jitData-&gt;m_incomingPolymorphicCalls.end())
1941             jitData-&gt;m_incomingPolymorphicCalls.begin()-&gt;unlink(vm());
1942     }
1943 #endif // ENABLE(JIT)
1944 }
1945 
<span class="line-modified">1946 void CodeBlock::linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo* incoming)</span>
1947 {
1948     noticeIncomingCall(callerFrame);
1949     m_incomingLLIntCalls.push(incoming);
1950 }
1951 
1952 CodeBlock* CodeBlock::newReplacement()
1953 {
1954     return ownerExecutable()-&gt;newReplacementCodeBlockFor(specializationKind());
1955 }
1956 
1957 #if ENABLE(JIT)
1958 CodeBlock* CodeBlock::replacement()
1959 {
1960     const ClassInfo* classInfo = this-&gt;classInfo(vm());
1961 
1962     if (classInfo == FunctionCodeBlock::info())
1963         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;codeBlockFor(isConstructor() ? CodeForConstruct : CodeForCall);
1964 
1965     if (classInfo == EvalCodeBlock::info())
1966         return jsCast&lt;EvalExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
</pre>
<hr />
<pre>
2151                 return StackVisitor::Done;
2152             }
2153 
2154             if (!m_depthToCheck--)
2155                 return StackVisitor::Done;
2156         }
2157 
2158         return StackVisitor::Continue;
2159     }
2160 
2161     bool didRecurse() const { return m_didRecurse; }
2162 
2163 private:
2164     CallFrame* m_startCallFrame;
2165     CodeBlock* m_codeBlock;
2166     mutable unsigned m_depthToCheck;
2167     mutable bool m_foundStartCallFrame;
2168     mutable bool m_didRecurse;
2169 };
2170 
<span class="line-modified">2171 void CodeBlock::noticeIncomingCall(ExecState* callerFrame)</span>
2172 {
2173     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
2174 
<span class="line-modified">2175     if (Options::verboseCallLink())</span>
<span class="line-removed">2176         dataLog(&quot;Noticing call link from &quot;, pointerDump(callerCodeBlock), &quot; to &quot;, *this, &quot;\n&quot;);</span>
2177 
2178 #if ENABLE(DFG_JIT)
2179     if (!m_shouldAlwaysBeInlined)
2180         return;
2181 
2182     if (!callerCodeBlock) {
2183         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2184         if (Options::verboseCallLink())</span>
<span class="line-removed">2185             dataLog(&quot;    Clearing SABI because caller is native.\n&quot;);</span>
2186         return;
2187     }
2188 
2189     if (!hasBaselineJITProfiling())
2190         return;
2191 
2192     if (!DFG::mightInlineFunction(this))
2193         return;
2194 
2195     if (!canInline(capabilityLevelState()))
2196         return;
2197 
2198     if (!DFG::isSmallEnoughToInlineCodeInto(callerCodeBlock)) {
2199         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2200         if (Options::verboseCallLink())</span>
<span class="line-removed">2201             dataLog(&quot;    Clearing SABI because caller is too large.\n&quot;);</span>
2202         return;
2203     }
2204 
2205     if (callerCodeBlock-&gt;jitType() == JITType::InterpreterThunk) {
2206         // If the caller is still in the interpreter, then we can&#39;t expect inlining to
2207         // happen anytime soon. Assume it&#39;s profitable to optimize it separately. This
2208         // ensures that a function is SABI only if it is called no more frequently than
2209         // any of its callers.
2210         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2211         if (Options::verboseCallLink())</span>
<span class="line-removed">2212             dataLog(&quot;    Clearing SABI because caller is in LLInt.\n&quot;);</span>
2213         return;
2214     }
2215 
2216     if (JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType())) {
2217         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2218         if (Options::verboseCallLink())</span>
<span class="line-removed">2219             dataLog(&quot;    Clearing SABI bcause caller was already optimized.\n&quot;);</span>
2220         return;
2221     }
2222 
2223     if (callerCodeBlock-&gt;codeType() != FunctionCode) {
2224         // If the caller is either eval or global code, assume that that won&#39;t be
2225         // optimized anytime soon. For eval code this is particularly true since we
2226         // delay eval optimization by a *lot*.
2227         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2228         if (Options::verboseCallLink())</span>
<span class="line-removed">2229             dataLog(&quot;    Clearing SABI because caller is not a function.\n&quot;);</span>
2230         return;
2231     }
2232 
2233     // Recursive calls won&#39;t be inlined.
2234     RecursionCheckFunctor functor(callerFrame, this, Options::maximumInliningDepth());
<span class="line-modified">2235     vm().topCallFrame-&gt;iterate(functor);</span>
2236 
2237     if (functor.didRecurse()) {
<span class="line-modified">2238         if (Options::verboseCallLink())</span>
<span class="line-removed">2239             dataLog(&quot;    Clearing SABI because recursion was detected.\n&quot;);</span>
2240         m_shouldAlwaysBeInlined = false;
2241         return;
2242     }
2243 
2244     if (callerCodeBlock-&gt;capabilityLevelState() == DFG::CapabilityLevelNotSet) {
2245         dataLog(&quot;In call from &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot; to &quot;, *this, &quot;: caller&#39;s DFG capability level is not set.\n&quot;);
2246         CRASH();
2247     }
2248 
2249     if (canCompile(callerCodeBlock-&gt;capabilityLevelState()))
2250         return;
2251 
<span class="line-modified">2252     if (Options::verboseCallLink())</span>
<span class="line-removed">2253         dataLog(&quot;    Clearing SABI because the caller is not a DFG candidate.\n&quot;);</span>
2254 
2255     m_shouldAlwaysBeInlined = false;
2256 #endif
2257 }
2258 
2259 unsigned CodeBlock::reoptimizationRetryCounter() const
2260 {
2261 #if ENABLE(JIT)
2262     ASSERT(m_reoptimizationRetryCounter &lt;= Options::reoptimizationRetryCounterMax());
2263     return m_reoptimizationRetryCounter;
2264 #else
2265     return 0;
2266 #endif // ENABLE(JIT)
2267 }
2268 
2269 #if !ENABLE(C_LOOP)
2270 const RegisterAtOffsetList* CodeBlock::calleeSaveRegisters() const
2271 {
2272 #if ENABLE(JIT)
2273     if (auto* jitData = m_jitData.get()) {
</pre>
<hr />
<pre>
2381     // that the fit didn&#39;t end up choosing a negative value of c (which would result in
2382     // the function turning over and going negative for large x) and I threw in a Sqrt
2383     // term because Sqrt represents my intution that the function should be more sensitive
2384     // to small changes in small values of x, but less sensitive when x gets large.
2385 
2386     // Note that the current fit essentially eliminates the linear portion of the
2387     // expression (c == 0.0).
2388     const double a = 0.061504;
2389     const double b = 1.02406;
2390     const double c = 0.0;
2391     const double d = 0.825914;
2392 
2393     double bytecodeCost = this-&gt;bytecodeCost();
2394 
2395     ASSERT(bytecodeCost); // Make sure this is called only after we have an instruction stream; otherwise it&#39;ll just return the value of d, which makes no sense.
2396 
2397     double result = d + a * sqrt(bytecodeCost + b) + c * bytecodeCost;
2398 
2399     result *= codeTypeThresholdMultiplier();
2400 
<span class="line-modified">2401     if (Options::verboseOSR()) {</span>
<span class="line-modified">2402         dataLog(</span>
<span class="line-modified">2403             *this, &quot;: bytecode cost is &quot;, bytecodeCost,</span>
<span class="line-removed">2404             &quot;, scaling execution counter by &quot;, result, &quot; * &quot;, codeTypeThresholdMultiplier(),</span>
<span class="line-removed">2405             &quot;\n&quot;);</span>
<span class="line-removed">2406     }</span>
2407     return result;
2408 }
2409 
2410 static int32_t clipThreshold(double threshold)
2411 {
2412     if (threshold &lt; 1.0)
2413         return 1;
2414 
2415     if (threshold &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::max()))
2416         return std::numeric_limits&lt;int32_t&gt;::max();
2417 
2418     return static_cast&lt;int32_t&gt;(threshold);
2419 }
2420 
2421 int32_t CodeBlock::adjustedCounterValue(int32_t desiredThreshold)
2422 {
2423     return clipThreshold(
2424         static_cast&lt;double&gt;(desiredThreshold) *
2425         optimizationThresholdScalingFactor() *
2426         (1 &lt;&lt; reoptimizationRetryCounter()));
</pre>
<hr />
<pre>
2475             didTryToEnterInLoop = true;
2476             break;
2477         }
2478     }
2479 
2480     uint32_t exitCountThreshold = didTryToEnterInLoop
2481         ? exitCountThresholdForReoptimizationFromLoop()
2482         : exitCountThresholdForReoptimization();
2483 
2484     if (m_osrExitCounter &gt; exitCountThreshold)
2485         return OptimizeAction::ReoptimizeNow;
2486 
2487     // Too few fails. Adjust the execution counter such that the target is to only optimize after a while.
2488     baselineCodeBlock-&gt;m_jitExecuteCounter.setNewThresholdForOSRExit(exitState.activeThreshold, exitState.memoryUsageAdjustedThreshold);
2489     return OptimizeAction::None;
2490 }
2491 #endif
2492 
2493 void CodeBlock::optimizeNextInvocation()
2494 {
<span class="line-modified">2495     if (Options::verboseOSR())</span>
<span class="line-removed">2496         dataLog(*this, &quot;: Optimizing next invocation.\n&quot;);</span>
2497     m_jitExecuteCounter.setNewThreshold(0, this);
2498 }
2499 
2500 void CodeBlock::dontOptimizeAnytimeSoon()
2501 {
<span class="line-modified">2502     if (Options::verboseOSR())</span>
<span class="line-removed">2503         dataLog(*this, &quot;: Not optimizing anytime soon.\n&quot;);</span>
2504     m_jitExecuteCounter.deferIndefinitely();
2505 }
2506 
2507 void CodeBlock::optimizeAfterWarmUp()
2508 {
<span class="line-modified">2509     if (Options::verboseOSR())</span>
<span class="line-removed">2510         dataLog(*this, &quot;: Optimizing after warm-up.\n&quot;);</span>
2511 #if ENABLE(DFG_JIT)
2512     m_jitExecuteCounter.setNewThreshold(
2513         adjustedCounterValue(Options::thresholdForOptimizeAfterWarmUp()), this);
2514 #endif
2515 }
2516 
2517 void CodeBlock::optimizeAfterLongWarmUp()
2518 {
<span class="line-modified">2519     if (Options::verboseOSR())</span>
<span class="line-removed">2520         dataLog(*this, &quot;: Optimizing after long warm-up.\n&quot;);</span>
2521 #if ENABLE(DFG_JIT)
2522     m_jitExecuteCounter.setNewThreshold(
2523         adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp()), this);
2524 #endif
2525 }
2526 
2527 void CodeBlock::optimizeSoon()
2528 {
<span class="line-modified">2529     if (Options::verboseOSR())</span>
<span class="line-removed">2530         dataLog(*this, &quot;: Optimizing soon.\n&quot;);</span>
2531 #if ENABLE(DFG_JIT)
2532     m_jitExecuteCounter.setNewThreshold(
2533         adjustedCounterValue(Options::thresholdForOptimizeSoon()), this);
2534 #endif
2535 }
2536 
2537 void CodeBlock::forceOptimizationSlowPathConcurrently()
2538 {
<span class="line-modified">2539     if (Options::verboseOSR())</span>
<span class="line-removed">2540         dataLog(*this, &quot;: Forcing slow path concurrently.\n&quot;);</span>
2541     m_jitExecuteCounter.forceSlowPathConcurrently();
2542 }
2543 
2544 #if ENABLE(DFG_JIT)
2545 void CodeBlock::setOptimizationThresholdBasedOnCompilationResult(CompilationResult result)
2546 {
2547     JITType type = jitType();
2548     if (type != JITType::BaselineJIT) {
<span class="line-modified">2549         dataLog(*this, &quot;: expected to have baseline code but have &quot;, type, &quot;\n&quot;);</span>
2550         CRASH_WITH_INFO(bitwise_cast&lt;uintptr_t&gt;(jitCode().get()), static_cast&lt;uint8_t&gt;(type));
2551     }
2552 
2553     CodeBlock* replacement = this-&gt;replacement();
2554     bool hasReplacement = (replacement &amp;&amp; replacement != this);
2555     if ((result == CompilationSuccessful) != hasReplacement) {
2556         dataLog(*this, &quot;: we have result = &quot;, result, &quot; but &quot;);
2557         if (replacement == this)
2558             dataLog(&quot;we are our own replacement.\n&quot;);
2559         else
2560             dataLog(&quot;our replacement is &quot;, pointerDump(replacement), &quot;\n&quot;);
2561         RELEASE_ASSERT_NOT_REACHED();
2562     }
2563 
2564     switch (result) {
2565     case CompilationSuccessful:
2566         RELEASE_ASSERT(replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType()));
2567         optimizeNextInvocation();
2568         return;
2569     case CompilationFailed:
</pre>
<hr />
<pre>
2609 {
2610     return adjustedExitCountThreshold(Options::osrExitCountForReoptimization() * codeTypeThresholdMultiplier());
2611 }
2612 
2613 uint32_t CodeBlock::exitCountThresholdForReoptimizationFromLoop()
2614 {
2615     return adjustedExitCountThreshold(Options::osrExitCountForReoptimizationFromLoop() * codeTypeThresholdMultiplier());
2616 }
2617 
2618 bool CodeBlock::shouldReoptimizeNow()
2619 {
2620     return osrExitCounter() &gt;= exitCountThresholdForReoptimization();
2621 }
2622 
2623 bool CodeBlock::shouldReoptimizeFromLoopNow()
2624 {
2625     return osrExitCounter() &gt;= exitCountThresholdForReoptimizationFromLoop();
2626 }
2627 #endif
2628 
<span class="line-modified">2629 ArrayProfile* CodeBlock::getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset)</span>
2630 {
<span class="line-modified">2631     auto instruction = instructions().at(bytecodeOffset);</span>
2632     switch (instruction-&gt;opcodeID()) {
2633 #define CASE1(Op) \
2634     case Op::opcodeID: \
2635         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_arrayProfile;
2636 
2637 #define CASE2(Op) \
2638     case Op::opcodeID: \
2639         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_callLinkInfo.m_arrayProfile;
2640 
2641     FOR_EACH_OPCODE_WITH_ARRAY_PROFILE(CASE1)
2642     FOR_EACH_OPCODE_WITH_LLINT_CALL_LINK_INFO(CASE2)
2643 
2644 #undef CASE1
2645 #undef CASE2
2646 
2647     case OpGetById::opcodeID: {
2648         auto bytecode = instruction-&gt;as&lt;OpGetById&gt;();
2649         auto&amp; metadata = bytecode.metadata(this);
2650         if (metadata.m_modeMetadata.mode == GetByIdMode::ArrayLength)
2651             return &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile;
2652         break;
2653     }
2654     default:
2655         break;
2656     }
2657 
2658     return nullptr;
2659 }
2660 
<span class="line-modified">2661 ArrayProfile* CodeBlock::getArrayProfile(unsigned bytecodeOffset)</span>
2662 {
2663     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">2664     return getArrayProfile(locker, bytecodeOffset);</span>
2665 }
2666 
2667 #if ENABLE(DFG_JIT)
2668 Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; CodeBlock::codeOrigins()
2669 {
2670     return m_jitCode-&gt;dfgCommon()-&gt;codeOrigins;
2671 }
2672 
2673 size_t CodeBlock::numberOfDFGIdentifiers() const
2674 {
2675     if (!JITCode::isOptimizingJIT(jitType()))
2676         return 0;
2677 
2678     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers.size();
2679 }
2680 
2681 const Identifier&amp; CodeBlock::identifier(int index) const
2682 {
2683     size_t unlinkedIdentifiers = m_unlinkedCode-&gt;numberOfIdentifiers();
2684     if (static_cast&lt;unsigned&gt;(index) &lt; unlinkedIdentifiers)
</pre>
<hr />
<pre>
2695     numberOfLiveNonArgumentValueProfiles = 0;
2696     numberOfSamplesInProfiles = 0; // If this divided by ValueProfile::numberOfBuckets equals numberOfValueProfiles() then value profiles are full.
2697 
2698     forEachValueProfile([&amp;](ValueProfile&amp; profile, bool isArgument) {
2699         unsigned numSamples = profile.totalNumberOfSamples();
2700         static_assert(ValueProfile::numberOfBuckets == 1);
2701         if (numSamples &gt; ValueProfile::numberOfBuckets)
2702             numSamples = ValueProfile::numberOfBuckets; // We don&#39;t want profiles that are extremely hot to be given more weight.
2703         numberOfSamplesInProfiles += numSamples;
2704         if (isArgument) {
2705             profile.computeUpdatedPrediction(locker);
2706             return;
2707         }
2708         if (profile.numberOfSamples() || profile.isSampledBefore())
2709             numberOfLiveNonArgumentValueProfiles++;
2710         profile.computeUpdatedPrediction(locker);
2711     });
2712 
2713     if (auto* rareData = m_rareData.get()) {
2714         for (auto&amp; profileBucket : rareData-&gt;m_catchProfiles) {
<span class="line-modified">2715             profileBucket-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {</span>
2716                 profile.computeUpdatedPrediction(locker);
2717             });
2718         }
2719     }
2720 
2721 #if ENABLE(DFG_JIT)
2722     lazyOperandValueProfiles(locker).computeUpdatedPredictions(locker);
2723 #endif
2724 }
2725 
2726 void CodeBlock::updateAllValueProfilePredictions()
2727 {
2728     unsigned ignoredValue1, ignoredValue2;
2729     updateAllValueProfilePredictionsAndCountLiveness(ignoredValue1, ignoredValue2);
2730 }
2731 
2732 void CodeBlock::updateAllArrayPredictions()
2733 {
2734     ConcurrentJSLocker locker(m_lock);
2735 
2736     forEachArrayProfile([&amp;](ArrayProfile&amp; profile) {
2737         profile.computeUpdatedPrediction(locker, this);
2738     });
2739 
2740     forEachArrayAllocationProfile([&amp;](ArrayAllocationProfile&amp; profile) {
2741         profile.updateProfile();
2742     });
2743 }
2744 
2745 void CodeBlock::updateAllPredictions()
2746 {
2747     updateAllValueProfilePredictions();
2748     updateAllArrayPredictions();
2749 }
2750 
2751 bool CodeBlock::shouldOptimizeNow()
2752 {
<span class="line-modified">2753     if (Options::verboseOSR())</span>
<span class="line-removed">2754         dataLog(&quot;Considering optimizing &quot;, *this, &quot;...\n&quot;);</span>
2755 
2756     if (m_optimizationDelayCounter &gt;= Options::maximumOptimizationDelay())
2757         return true;
2758 
2759     updateAllArrayPredictions();
2760 
2761     unsigned numberOfLiveNonArgumentValueProfiles;
2762     unsigned numberOfSamplesInProfiles;
2763     updateAllValueProfilePredictionsAndCountLiveness(numberOfLiveNonArgumentValueProfiles, numberOfSamplesInProfiles);
2764 
2765     if (Options::verboseOSR()) {
2766         dataLogF(
2767             &quot;Profile hotness: %lf (%u / %u), %lf (%u / %u)\n&quot;,
2768             (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles(),
2769             numberOfLiveNonArgumentValueProfiles, numberOfNonArgumentValueProfiles(),
2770             (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / numberOfNonArgumentValueProfiles(),
2771             numberOfSamplesInProfiles, ValueProfile::numberOfBuckets * numberOfNonArgumentValueProfiles());
2772     }
2773 
2774     if ((!numberOfNonArgumentValueProfiles() || (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles() &gt;= Options::desiredProfileLivenessRate())
2775         &amp;&amp; (!totalNumberOfValueProfiles() || (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / totalNumberOfValueProfiles() &gt;= Options::desiredProfileFullnessRate())
2776         &amp;&amp; static_cast&lt;unsigned&gt;(m_optimizationDelayCounter) + 1 &gt;= Options::minimumOptimizationDelay())
2777         return true;
2778 
2779     ASSERT(m_optimizationDelayCounter &lt; std::numeric_limits&lt;uint8_t&gt;::max());
2780     m_optimizationDelayCounter++;
2781     optimizeAfterWarmUp();
2782     return false;
2783 }
2784 
2785 #if ENABLE(DFG_JIT)
2786 void CodeBlock::tallyFrequentExitSites()
2787 {
2788     ASSERT(JITCode::isOptimizingJIT(jitType()));
<span class="line-modified">2789     ASSERT(alternative()-&gt;jitType() == JITType::BaselineJIT);</span>
2790 
2791     CodeBlock* profiledBlock = alternative();
2792 
2793     switch (jitType()) {
2794     case JITType::DFGJIT: {
2795         DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
2796         for (auto&amp; exit : jitCode-&gt;osrExit)
2797             exit.considerAddingAsFrequentExitSite(profiledBlock);
2798         break;
2799     }
2800 
2801 #if ENABLE(FTL_JIT)
2802     case JITType::FTLJIT: {
2803         // There is no easy way to avoid duplicating this code since the FTL::JITCode::osrExit
2804         // vector contains a totally different type, that just so happens to behave like
2805         // DFG::JITCode::osrExit.
2806         FTL::JITCode* jitCode = m_jitCode-&gt;ftl();
2807         for (unsigned i = 0; i &lt; jitCode-&gt;osrExit.size(); ++i) {
2808             FTL::OSRExit&amp; exit = jitCode-&gt;osrExit[i];
2809             exit.considerAddingAsFrequentExitSite(profiledBlock);
</pre>
<hr />
<pre>
2952         if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm(), constantRegister.get())) {
2953             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
2954             auto end = symbolTable-&gt;end(locker);
2955             for (auto ptr = symbolTable-&gt;begin(locker); ptr != end; ++ptr) {
2956                 if (ptr-&gt;value.varOffset() == VarOffset(virtualRegister)) {
2957                     // FIXME: This won&#39;t work from the compilation thread.
2958                     // https://bugs.webkit.org/show_bug.cgi?id=115300
2959                     return ptr-&gt;key.get();
2960                 }
2961             }
2962         }
2963     }
2964     if (virtualRegister == thisRegister())
2965         return &quot;this&quot;_s;
2966     if (virtualRegister.isArgument())
2967         return makeString(&quot;arguments[&quot;, pad(&#39; &#39;, 3, virtualRegister.toArgument()), &#39;]&#39;);
2968 
2969     return emptyString();
2970 }
2971 
<span class="line-modified">2972 ValueProfile* CodeBlock::tryGetValueProfileForBytecodeOffset(int bytecodeOffset)</span>
2973 {
<span class="line-modified">2974     auto instruction = instructions().at(bytecodeOffset);</span>
2975     switch (instruction-&gt;opcodeID()) {
2976 
2977 #define CASE(Op) \
2978     case Op::opcodeID: \
2979         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_profile;
2980 
2981         FOR_EACH_OPCODE_WITH_VALUE_PROFILE(CASE)
2982 
2983 #undef CASE
2984 
2985     default:
2986         return nullptr;
2987 
2988     }
2989 }
2990 
<span class="line-modified">2991 SpeculatedType CodeBlock::valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp; locker, int bytecodeOffset)</span>
2992 {
<span class="line-modified">2993     if (ValueProfile* valueProfile = tryGetValueProfileForBytecodeOffset(bytecodeOffset))</span>
2994         return valueProfile-&gt;computeUpdatedPrediction(locker);
2995     return SpecNone;
2996 }
2997 
<span class="line-modified">2998 ValueProfile&amp; CodeBlock::valueProfileForBytecodeOffset(int bytecodeOffset)</span>
2999 {
<span class="line-modified">3000     return *tryGetValueProfileForBytecodeOffset(bytecodeOffset);</span>
3001 }
3002 
3003 void CodeBlock::validate()
3004 {
3005     BytecodeLivenessAnalysis liveness(this); // Compute directly from scratch so it doesn&#39;t effect CodeBlock footprint.
3006 
<span class="line-modified">3007     FastBitVector liveAtHead = liveness.getLivenessInfoAtBytecodeOffset(this, 0);</span>
3008 
3009     if (liveAtHead.numBits() != static_cast&lt;size_t&gt;(m_numCalleeLocals)) {
3010         beginValidationDidFail();
3011         dataLog(&quot;    Wrong number of bits in result!\n&quot;);
3012         dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3013         dataLog(&quot;    Bit count: &quot;, liveAtHead.numBits(), &quot;\n&quot;);
3014         endValidationDidFail();
3015     }
3016 
3017     for (unsigned i = m_numCalleeLocals; i--;) {
3018         VirtualRegister reg = virtualRegisterForLocal(i);
3019 
3020         if (liveAtHead[i]) {
3021             beginValidationDidFail();
3022             dataLog(&quot;    Variable &quot;, reg, &quot; is expected to be dead.\n&quot;);
3023             dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3024             endValidationDidFail();
3025         }
3026     }
3027 
3028     const InstructionStream&amp; instructionStream = instructions();
3029     for (const auto&amp; instruction : instructionStream) {
3030         OpcodeID opcode = instruction-&gt;opcodeID();
<span class="line-modified">3031         if (!!baselineAlternative()-&gt;handlerForBytecodeOffset(instruction.offset())) {</span>
3032             if (opcode == op_catch || opcode == op_enter) {
3033                 // op_catch/op_enter logically represent an entrypoint. Entrypoints are not allowed to be
3034                 // inside of a try block because they are responsible for bootstrapping state. And they
3035                 // are never allowed throw an exception because of this. We rely on this when compiling
3036                 // in the DFG. Because an entrypoint never throws, the bytecode generator will never
3037                 // allow once inside a try block.
3038                 beginValidationDidFail();
3039                 dataLog(&quot;    entrypoint not allowed inside a try block.&quot;);
3040                 endValidationDidFail();
3041             }
3042         }
3043     }
3044 }
3045 
3046 void CodeBlock::beginValidationDidFail()
3047 {
3048     dataLog(&quot;Validation failure in &quot;, *this, &quot;:\n&quot;);
3049     dataLog(&quot;\n&quot;);
3050 }
3051 
</pre>
<hr />
<pre>
3069 void CodeBlock::setSteppingMode(CodeBlock::SteppingMode mode)
3070 {
3071     m_steppingMode = mode;
3072     if (mode == SteppingModeEnabled &amp;&amp; JITCode::isOptimizingJIT(jitType()))
3073         jettison(Profiler::JettisonDueToDebuggerStepping);
3074 }
3075 
3076 int CodeBlock::outOfLineJumpOffset(const Instruction* pc)
3077 {
3078     int offset = bytecodeOffset(pc);
3079     return m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3080 }
3081 
3082 const Instruction* CodeBlock::outOfLineJumpTarget(const Instruction* pc)
3083 {
3084     int offset = bytecodeOffset(pc);
3085     int target = m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3086     return instructions().at(offset + target).ptr();
3087 }
3088 
<span class="line-modified">3089 ArithProfile* CodeBlock::arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset)</span>





3090 {
<span class="line-modified">3091     return arithProfileForPC(instructions().at(bytecodeOffset).ptr());</span>
3092 }
3093 
<span class="line-modified">3094 ArithProfile* CodeBlock::arithProfileForPC(const Instruction* pc)</span>
3095 {
3096     switch (pc-&gt;opcodeID()) {
<span class="line-removed">3097     case op_negate:</span>
<span class="line-removed">3098         return &amp;pc-&gt;as&lt;OpNegate&gt;().metadata(this).m_arithProfile;</span>
3099     case op_add:
3100         return &amp;pc-&gt;as&lt;OpAdd&gt;().metadata(this).m_arithProfile;
3101     case op_mul:
3102         return &amp;pc-&gt;as&lt;OpMul&gt;().metadata(this).m_arithProfile;
3103     case op_sub:
3104         return &amp;pc-&gt;as&lt;OpSub&gt;().metadata(this).m_arithProfile;
3105     case op_div:
3106         return &amp;pc-&gt;as&lt;OpDiv&gt;().metadata(this).m_arithProfile;
3107     default:
3108         break;
3109     }
3110 
3111     return nullptr;
3112 }
3113 
<span class="line-modified">3114 bool CodeBlock::couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset)</span>
















3115 {
3116     if (!hasBaselineJITProfiling())
3117         return false;
<span class="line-modified">3118     ArithProfile* profile = arithProfileForBytecodeOffset(bytecodeOffset);</span>
3119     if (!profile)
3120         return false;
3121     return profile-&gt;tookSpecialFastPath();
3122 }
3123 
3124 #if ENABLE(JIT)
3125 DFG::CapabilityLevel CodeBlock::capabilityLevel()
3126 {
3127     DFG::CapabilityLevel result = computeCapabilityLevel();
3128     m_capabilityLevelState = result;
3129     return result;
3130 }
3131 #endif
3132 
3133 void CodeBlock::insertBasicBlockBoundariesForControlFlowProfiler()
3134 {
3135     if (!unlinkedCodeBlock()-&gt;hasOpProfileControlFlowBytecodeOffsets())
3136         return;
<span class="line-modified">3137     const Vector&lt;InstructionStream::Offset&gt;&amp; bytecodeOffsets = unlinkedCodeBlock()-&gt;opProfileControlFlowBytecodeOffsets();</span>
3138     for (size_t i = 0, offsetsLength = bytecodeOffsets.size(); i &lt; offsetsLength; i++) {
3139         // Because op_profile_control_flow is emitted at the beginning of every basic block, finding
3140         // the next op_profile_control_flow will give us the text range of a single basic block.
3141         size_t startIdx = bytecodeOffsets[i];
3142         auto instruction = instructions().at(startIdx);
3143         RELEASE_ASSERT(instruction-&gt;opcodeID() == op_profile_control_flow);
3144         auto bytecode = instruction-&gt;as&lt;OpProfileControlFlow&gt;();
3145         auto&amp; metadata = bytecode.metadata(this);
3146         int basicBlockStartOffset = bytecode.m_textOffset;
3147         int basicBlockEndOffset;
3148         if (i + 1 &lt; offsetsLength) {
3149             size_t endIdx = bytecodeOffsets[i + 1];
3150             auto endInstruction = instructions().at(endIdx);
3151             RELEASE_ASSERT(endInstruction-&gt;opcodeID() == op_profile_control_flow);
3152             basicBlockEndOffset = endInstruction-&gt;as&lt;OpProfileControlFlow&gt;().m_textOffset - 1;
3153         } else {
3154             basicBlockEndOffset = sourceOffset() + ownerExecutable()-&gt;source().length() - 1; // Offset before the closing brace.
3155             basicBlockStartOffset = std::min(basicBlockStartOffset, basicBlockEndOffset); // Some start offsets may be at the closing brace, ensure it is the offset before.
3156         }
3157 
</pre>
<hr />
<pre>
3217         if (auto* jitData = m_jitData.get()) {
3218             if (jitData-&gt;m_pcToCodeOriginMap) {
3219                 if (Optional&lt;CodeOrigin&gt; codeOrigin = jitData-&gt;m_pcToCodeOriginMap-&gt;findPC(pc))
3220                     return codeOrigin;
3221             }
3222 
3223             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
3224                 if (stubInfo-&gt;containsPC(pc))
3225                     return Optional&lt;CodeOrigin&gt;(stubInfo-&gt;codeOrigin);
3226             }
3227         }
3228     }
3229 
3230     if (Optional&lt;CodeOrigin&gt; codeOrigin = m_jitCode-&gt;findPC(this, pc))
3231         return codeOrigin;
3232 
3233     return WTF::nullopt;
3234 }
3235 #endif // ENABLE(JIT)
3236 
<span class="line-modified">3237 Optional&lt;unsigned&gt; CodeBlock::bytecodeOffsetFromCallSiteIndex(CallSiteIndex callSiteIndex)</span>
3238 {
<span class="line-modified">3239     Optional&lt;unsigned&gt; bytecodeOffset;</span>
3240     JITType jitType = this-&gt;jitType();
<span class="line-modified">3241     if (jitType == JITType::InterpreterThunk || jitType == JITType::BaselineJIT) {</span>
<span class="line-modified">3242 #if USE(JSVALUE64)</span>
<span class="line-modified">3243         bytecodeOffset = callSiteIndex.bits();</span>
<span class="line-removed">3244 #else</span>
<span class="line-removed">3245         Instruction* instruction = bitwise_cast&lt;Instruction*&gt;(callSiteIndex.bits());</span>
<span class="line-removed">3246         bytecodeOffset = this-&gt;bytecodeOffset(instruction);</span>
<span class="line-removed">3247 #endif</span>
<span class="line-removed">3248     } else if (jitType == JITType::DFGJIT || jitType == JITType::FTLJIT) {</span>
3249 #if ENABLE(DFG_JIT)
3250         RELEASE_ASSERT(canGetCodeOrigin(callSiteIndex));
3251         CodeOrigin origin = codeOrigin(callSiteIndex);
<span class="line-modified">3252         bytecodeOffset = origin.bytecodeIndex();</span>
3253 #else
3254         RELEASE_ASSERT_NOT_REACHED();
3255 #endif
3256     }
3257 
<span class="line-modified">3258     return bytecodeOffset;</span>
3259 }
3260 
3261 int32_t CodeBlock::thresholdForJIT(int32_t threshold)
3262 {
3263     switch (unlinkedCodeBlock()-&gt;didOptimize()) {
3264     case MixedTriState:
3265         return threshold;
3266     case FalseTriState:
3267         return threshold * 4;
3268     case TrueTriState:
3269         return threshold / 2;
3270     }
3271     ASSERT_NOT_REACHED();
3272     return threshold;
3273 }
3274 
3275 void CodeBlock::jitAfterWarmUp()
3276 {
3277     m_llintExecuteCounter.setNewThreshold(thresholdForJIT(Options::thresholdForJITAfterWarmUp()), this);
3278 }
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2008-2020 Apple Inc. All rights reserved.</span>
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #include &quot;config.h&quot;
  31 #include &quot;CodeBlock.h&quot;
  32 
  33 #include &quot;ArithProfile.h&quot;
  34 #include &quot;BasicBlockLocation.h&quot;
  35 #include &quot;BytecodeDumper.h&quot;
  36 #include &quot;BytecodeGenerator.h&quot;
  37 #include &quot;BytecodeLivenessAnalysis.h&quot;
  38 #include &quot;BytecodeStructs.h&quot;
  39 #include &quot;BytecodeUseDef.h&quot;
  40 #include &quot;CallLinkStatus.h&quot;
<span class="line-added">  41 #include &quot;CheckpointOSRExitSideState.h&quot;</span>
  42 #include &quot;CodeBlockInlines.h&quot;
  43 #include &quot;CodeBlockSet.h&quot;
  44 #include &quot;DFGCapabilities.h&quot;
  45 #include &quot;DFGCommon.h&quot;
  46 #include &quot;DFGDriver.h&quot;
  47 #include &quot;DFGJITCode.h&quot;
  48 #include &quot;DFGWorklist.h&quot;
  49 #include &quot;Debugger.h&quot;
  50 #include &quot;EvalCodeBlock.h&quot;
  51 #include &quot;FullCodeOrigin.h&quot;
  52 #include &quot;FunctionCodeBlock.h&quot;
  53 #include &quot;FunctionExecutableDump.h&quot;
  54 #include &quot;GetPutInfo.h&quot;
  55 #include &quot;InlineCallFrame.h&quot;
  56 #include &quot;Instruction.h&quot;
  57 #include &quot;InstructionStream.h&quot;
  58 #include &quot;InterpreterInlines.h&quot;
  59 #include &quot;IsoCellSetInlines.h&quot;
  60 #include &quot;JIT.h&quot;
  61 #include &quot;JITMathIC.h&quot;
</pre>
<hr />
<pre>
  93 #include &lt;wtf/Forward.h&gt;
  94 #include &lt;wtf/SimpleStats.h&gt;
  95 #include &lt;wtf/StringPrintStream.h&gt;
  96 #include &lt;wtf/text/StringConcatenateNumbers.h&gt;
  97 #include &lt;wtf/text/UniquedStringImpl.h&gt;
  98 
  99 #if ENABLE(ASSEMBLER)
 100 #include &quot;RegisterAtOffsetList.h&quot;
 101 #endif
 102 
 103 #if ENABLE(DFG_JIT)
 104 #include &quot;DFGOperations.h&quot;
 105 #endif
 106 
 107 #if ENABLE(FTL_JIT)
 108 #include &quot;FTLJITCode.h&quot;
 109 #endif
 110 
 111 namespace JSC {
 112 
<span class="line-added"> 113 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(CodeBlockRareData);</span>
<span class="line-added"> 114 </span>
 115 const ClassInfo CodeBlock::s_info = {
 116     &quot;CodeBlock&quot;, nullptr, nullptr, nullptr,
 117     CREATE_METHOD_TABLE(CodeBlock)
 118 };
 119 
 120 CString CodeBlock::inferredName() const
 121 {
 122     switch (codeType()) {
 123     case GlobalCode:
 124         return &quot;&lt;global&gt;&quot;;
 125     case EvalCode:
 126         return &quot;&lt;eval&gt;&quot;;
 127     case FunctionCode:
 128         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;ecmaName().utf8();
 129     case ModuleCode:
 130         return &quot;&lt;module&gt;&quot;;
 131     default:
 132         CRASH();
 133         return CString(&quot;&quot;, 0);
 134     }
</pre>
<hr />
<pre>
 232         FunctionExecutable* functionExecutable = reinterpret_cast&lt;FunctionExecutable*&gt;(executable);
 233         StringView source = functionExecutable-&gt;source().provider()-&gt;getRange(
 234             functionExecutable-&gt;parametersStartOffset(),
 235             functionExecutable-&gt;typeProfilingEndOffset(vm()) + 1); // Type profiling end offset is the character before the &#39;}&#39;.
 236 
 237         out.print(&quot;function &quot;, inferredName(), source);
 238         return;
 239     }
 240     out.print(executable-&gt;source().view());
 241 }
 242 
 243 void CodeBlock::dumpBytecode()
 244 {
 245     dumpBytecode(WTF::dataFile());
 246 }
 247 
 248 void CodeBlock::dumpBytecode(PrintStream&amp; out)
 249 {
 250     ICStatusMap statusMap;
 251     getICStatusMap(statusMap);
<span class="line-modified"> 252     CodeBlockBytecodeDumper&lt;CodeBlock&gt;::dumpBlock(this, instructions(), out, statusMap);</span>
 253 }
 254 
 255 void CodeBlock::dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; statusMap)
 256 {
 257     BytecodeDumper&lt;CodeBlock&gt;::dumpBytecode(this, out, it, statusMap);
 258 }
 259 
 260 void CodeBlock::dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; statusMap)
 261 {
 262     const auto it = instructions().at(bytecodeOffset);
 263     dumpBytecode(out, it, statusMap);
 264 }
 265 
 266 namespace {
 267 
 268 class PutToScopeFireDetail : public FireDetail {
 269 public:
 270     PutToScopeFireDetail(CodeBlock* codeBlock, const Identifier&amp; ident)
 271         : m_codeBlock(codeBlock)
 272         , m_ident(ident)
</pre>
<hr />
<pre>
 278         out.print(&quot;Linking put_to_scope in &quot;, FunctionExecutableDump(jsCast&lt;FunctionExecutable*&gt;(m_codeBlock-&gt;ownerExecutable())), &quot; for &quot;, m_ident);
 279     }
 280 
 281 private:
 282     CodeBlock* m_codeBlock;
 283     const Identifier&amp; m_ident;
 284 };
 285 
 286 } // anonymous namespace
 287 
 288 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, CopyParsedBlockTag, CodeBlock&amp; other)
 289     : JSCell(vm, structure)
 290     , m_globalObject(other.m_globalObject)
 291     , m_shouldAlwaysBeInlined(true)
 292 #if ENABLE(JIT)
 293     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 294 #endif
 295     , m_didFailJITCompilation(false)
 296     , m_didFailFTLCompilation(false)
 297     , m_hasBeenCompiledWithFTL(false)
<span class="line-added"> 298     , m_hasLinkedOSRExit(false)</span>
<span class="line-added"> 299     , m_isEligibleForLLIntDowngrade(false)</span>
 300     , m_numCalleeLocals(other.m_numCalleeLocals)
 301     , m_numVars(other.m_numVars)
 302     , m_numberOfArgumentsToSkip(other.m_numberOfArgumentsToSkip)
 303     , m_hasDebuggerStatement(false)
 304     , m_steppingMode(SteppingModeDisabled)
 305     , m_numBreakpoints(0)
 306     , m_bytecodeCost(other.m_bytecodeCost)
 307     , m_scopeRegister(other.m_scopeRegister)
 308     , m_hash(other.m_hash)
 309     , m_unlinkedCode(other.vm(), this, other.m_unlinkedCode.get())
 310     , m_ownerExecutable(other.vm(), this, other.m_ownerExecutable.get())
 311     , m_vm(other.m_vm)
 312     , m_instructionsRawPointer(other.m_instructionsRawPointer)
 313     , m_constantRegisters(other.m_constantRegisters)
 314     , m_constantsSourceCodeRepresentation(other.m_constantsSourceCodeRepresentation)
 315     , m_functionDecls(other.m_functionDecls)
 316     , m_functionExprs(other.m_functionExprs)
 317     , m_osrExitCounter(0)
 318     , m_optimizationDelayCounter(0)
 319     , m_reoptimizationRetryCounter(0)
</pre>
<hr />
<pre>
 339 
 340     if (other.m_rareData) {
 341         createRareDataIfNecessary();
 342 
 343         m_rareData-&gt;m_exceptionHandlers = other.m_rareData-&gt;m_exceptionHandlers;
 344         m_rareData-&gt;m_switchJumpTables = other.m_rareData-&gt;m_switchJumpTables;
 345         m_rareData-&gt;m_stringSwitchJumpTables = other.m_rareData-&gt;m_stringSwitchJumpTables;
 346     }
 347 }
 348 
 349 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock, JSScope* scope)
 350     : JSCell(vm, structure)
 351     , m_globalObject(vm, this, scope-&gt;globalObject(vm))
 352     , m_shouldAlwaysBeInlined(true)
 353 #if ENABLE(JIT)
 354     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 355 #endif
 356     , m_didFailJITCompilation(false)
 357     , m_didFailFTLCompilation(false)
 358     , m_hasBeenCompiledWithFTL(false)
<span class="line-added"> 359     , m_hasLinkedOSRExit(false)</span>
<span class="line-added"> 360     , m_isEligibleForLLIntDowngrade(false)</span>
 361     , m_numCalleeLocals(unlinkedCodeBlock-&gt;numCalleeLocals())
 362     , m_numVars(unlinkedCodeBlock-&gt;numVars())
 363     , m_hasDebuggerStatement(false)
 364     , m_steppingMode(SteppingModeDisabled)
 365     , m_numBreakpoints(0)
 366     , m_scopeRegister(unlinkedCodeBlock-&gt;scopeRegister())
 367     , m_unlinkedCode(vm, this, unlinkedCodeBlock)
 368     , m_ownerExecutable(vm, this, ownerExecutable)
 369     , m_vm(&amp;vm)
 370     , m_instructionsRawPointer(unlinkedCodeBlock-&gt;instructions().rawPointer())
 371     , m_osrExitCounter(0)
 372     , m_optimizationDelayCounter(0)
 373     , m_reoptimizationRetryCounter(0)
 374     , m_metadata(unlinkedCodeBlock-&gt;metadata().link())
 375     , m_creationTime(MonotonicTime::now())
 376 {
 377     ASSERT(heap()-&gt;isDeferred());
 378     ASSERT(m_scopeRegister.isLocal());
 379 
 380     ASSERT(source().provider());
</pre>
<hr />
<pre>
 389 // outside of this CodeBlock&#39;s compilation unit. It also allows us to generate particular constants that
 390 // we can&#39;t generate during unlinked bytecode generation. This process is not allowed to generate control
 391 // flow or introduce new locals. The reason for this is we rely on liveness analysis to be the same for
 392 // all the CodeBlocks of an UnlinkedCodeBlock. We rely on this fact by caching the liveness analysis
 393 // inside UnlinkedCodeBlock.
 394 bool CodeBlock::finishCreation(VM&amp; vm, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock,
 395     JSScope* scope)
 396 {
 397     Base::finishCreation(vm);
 398     finishCreationCommon(vm);
 399 
 400     auto throwScope = DECLARE_THROW_SCOPE(vm);
 401 
 402     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())
 403         vm.functionHasExecutedCache()-&gt;removeUnexecutedRange(ownerExecutable-&gt;sourceID(), ownerExecutable-&gt;typeProfilingStartOffset(vm), ownerExecutable-&gt;typeProfilingEndOffset(vm));
 404 
 405     ScriptExecutable* topLevelExecutable = ownerExecutable-&gt;topLevelExecutable();
 406     setConstantRegisters(unlinkedCodeBlock-&gt;constantRegisters(), unlinkedCodeBlock-&gt;constantsSourceCodeRepresentation(), topLevelExecutable);
 407     RETURN_IF_EXCEPTION(throwScope, false);
 408 






 409     // We already have the cloned symbol table for the module environment since we need to instantiate
 410     // the module environments before linking the code block. We replace the stored symbol table with the already cloned one.
 411     if (UnlinkedModuleProgramCodeBlock* unlinkedModuleProgramCodeBlock = jsDynamicCast&lt;UnlinkedModuleProgramCodeBlock*&gt;(vm, unlinkedCodeBlock)) {
 412         SymbolTable* clonedSymbolTable = jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable)-&gt;moduleEnvironmentSymbolTable();
 413         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {
 414             ConcurrentJSLocker locker(clonedSymbolTable-&gt;m_lock);
 415             clonedSymbolTable-&gt;prepareForTypeProfiling(locker);
 416         }
<span class="line-modified"> 417         replaceConstant(VirtualRegister(unlinkedModuleProgramCodeBlock-&gt;moduleEnvironmentSymbolTableConstantRegisterOffset()), clonedSymbolTable);</span>
 418     }
 419 
 420     bool shouldUpdateFunctionHasExecutedCache = m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes();
 421     m_functionDecls = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionDecls());
 422     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionDecls(), i = 0; i &lt; count; ++i) {
 423         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionDecl(i);
 424         if (shouldUpdateFunctionHasExecutedCache)
 425             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
 426         m_functionDecls[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));
 427     }
 428 
 429     m_functionExprs = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionExprs());
 430     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionExprs(), i = 0; i &lt; count; ++i) {
 431         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionExpr(i);
 432         if (shouldUpdateFunctionHasExecutedCache)
 433             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
 434         m_functionExprs[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));
 435     }
 436 
 437     if (unlinkedCodeBlock-&gt;hasRareData()) {
 438         createRareDataIfNecessary();
 439 
 440         setConstantIdentifierSetRegisters(vm, unlinkedCodeBlock-&gt;constantIdentifierSets());
 441         RETURN_IF_EXCEPTION(throwScope, false);
 442 
 443         if (size_t count = unlinkedCodeBlock-&gt;numberOfExceptionHandlers()) {
 444             m_rareData-&gt;m_exceptionHandlers.resizeToFit(count);
 445             for (size_t i = 0; i &lt; count; i++) {
 446                 const UnlinkedHandlerInfo&amp; unlinkedHandler = unlinkedCodeBlock-&gt;exceptionHandler(i);
 447                 HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];
 448 #if ENABLE(JIT)
<span class="line-modified"> 449                 auto&amp; instruction = *instructions().at(unlinkedHandler.target).ptr();</span>
<span class="line-modified"> 450                 MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(instruction);</span>






 451                 handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));
 452 #else
 453                 handler.initialize(unlinkedHandler);
 454 #endif
 455             }
 456         }
 457 
 458         if (size_t count = unlinkedCodeBlock-&gt;numberOfStringSwitchJumpTables()) {
 459             m_rareData-&gt;m_stringSwitchJumpTables.grow(count);
 460             for (size_t i = 0; i &lt; count; i++) {
 461                 UnlinkedStringJumpTable::StringOffsetTable::iterator ptr = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.begin();
 462                 UnlinkedStringJumpTable::StringOffsetTable::iterator end = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.end();
 463                 for (; ptr != end; ++ptr) {
 464                     OffsetLocation offset;
 465                     offset.branchOffset = ptr-&gt;value.branchOffset;
 466                     m_rareData-&gt;m_stringSwitchJumpTables[i].offsetTable.add(ptr-&gt;key, offset);
 467                 }
 468             }
 469         }
 470 
 471         if (size_t count = unlinkedCodeBlock-&gt;numberOfSwitchJumpTables()) {
 472             m_rareData-&gt;m_switchJumpTables.grow(count);
 473             for (size_t i = 0; i &lt; count; i++) {
 474                 UnlinkedSimpleJumpTable&amp; sourceTable = unlinkedCodeBlock-&gt;switchJumpTable(i);
 475                 SimpleJumpTable&amp; destTable = m_rareData-&gt;m_switchJumpTables[i];
<span class="line-modified"> 476                 destTable.branchOffsets.resizeToFit(sourceTable.branchOffsets.size());</span>
<span class="line-added"> 477                 std::copy(sourceTable.branchOffsets.begin(), sourceTable.branchOffsets.end(), destTable.branchOffsets.begin());</span>
 478                 destTable.min = sourceTable.min;
 479             }
 480         }
 481     }
 482 
 483     // Bookkeep the strongly referenced module environments.
 484     HashSet&lt;JSModuleEnvironment*&gt; stronglyReferencedModuleEnvironments;
 485 
 486     auto link_profile = [&amp;](const auto&amp; /*instruction*/, auto /*bytecode*/, auto&amp; /*metadata*/) {
 487         m_numberOfNonArgumentValueProfiles++;
 488     };
 489 
 490     auto link_objectAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 491         metadata.m_objectAllocationProfile.initializeProfile(vm, m_globalObject.get(), this, m_globalObject-&gt;objectPrototype(), bytecode.m_inlineCapacity);
 492     };
 493 
 494     auto link_arrayAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 495         metadata.m_arrayAllocationProfile.initializeIndexingMode(bytecode.m_recommendedIndexingType);
 496     };
 497 
</pre>
<hr />
<pre>
 517     const InstructionStream&amp; instructionStream = instructions();
 518     for (const auto&amp; instruction : instructionStream) {
 519         OpcodeID opcodeID = instruction-&gt;opcodeID();
 520         m_bytecodeCost += opcodeLengths[opcodeID];
 521         switch (opcodeID) {
 522         LINK(OpHasIndexedProperty)
 523 
 524         LINK(OpCallVarargs, profile)
 525         LINK(OpTailCallVarargs, profile)
 526         LINK(OpTailCallForwardArguments, profile)
 527         LINK(OpConstructVarargs, profile)
 528         LINK(OpGetByVal, profile)
 529 
 530         LINK(OpGetDirectPname, profile)
 531         LINK(OpGetByIdWithThis, profile)
 532         LINK(OpTryGetById, profile)
 533         LINK(OpGetByIdDirect, profile)
 534         LINK(OpGetByValWithThis, profile)
 535         LINK(OpGetFromArguments, profile)
 536         LINK(OpToNumber, profile)
<span class="line-added"> 537         LINK(OpToNumeric, profile)</span>
 538         LINK(OpToObject, profile)
 539         LINK(OpGetArgument, profile)
<span class="line-added"> 540         LINK(OpGetInternalField, profile)</span>
 541         LINK(OpToThis, profile)
 542         LINK(OpBitand, profile)
 543         LINK(OpBitor, profile)
 544         LINK(OpBitnot, profile)
 545         LINK(OpBitxor, profile)
 546         LINK(OpLshift, profile)
<span class="line-added"> 547         LINK(OpRshift, profile)</span>
 548 
 549         LINK(OpGetById, profile)
 550 
 551         LINK(OpCall, profile)
 552         LINK(OpTailCall, profile)
 553         LINK(OpCallEval, profile)
 554         LINK(OpConstruct, profile)
 555 
 556         LINK(OpInByVal)
 557         LINK(OpPutByVal)
 558         LINK(OpPutByValDirect)
 559 
 560         LINK(OpNewArray)
 561         LINK(OpNewArrayWithSize)
 562         LINK(OpNewArrayBuffer, arrayAllocationProfile)
 563 
 564         LINK(OpNewObject, objectAllocationProfile)
 565 
 566         LINK(OpPutById)
 567         LINK(OpCreateThis)
<span class="line-added"> 568         LINK(OpCreatePromise)</span>
<span class="line-added"> 569         LINK(OpCreateGenerator)</span>
 570 
 571         LINK(OpAdd)
 572         LINK(OpMul)
 573         LINK(OpDiv)
 574         LINK(OpSub)
 575 
 576         LINK(OpNegate)
<span class="line-added"> 577         LINK(OpInc)</span>
<span class="line-added"> 578         LINK(OpDec)</span>
 579 
 580         LINK(OpJneqPtr)
 581 
 582         LINK(OpCatch)
 583         LINK(OpProfileControlFlow)
 584 
 585         case op_resolve_scope: {
 586             INITIALIZE_METADATA(OpResolveScope)
 587 
 588             const Identifier&amp; ident = identifier(bytecode.m_var);
 589             RELEASE_ASSERT(bytecode.m_resolveType != LocalClosureVar);
 590 
<span class="line-modified"> 591             ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);</span>
 592             RETURN_IF_EXCEPTION(throwScope, false);
 593 
 594             metadata.m_resolveType = op.type;
 595             metadata.m_localScopeDepth = op.depth;
 596             if (op.lexicalEnvironment) {
 597                 if (op.type == ModuleVar) {
 598                     // Keep the linked module environment strongly referenced.
 599                     if (stronglyReferencedModuleEnvironments.add(jsCast&lt;JSModuleEnvironment*&gt;(op.lexicalEnvironment)).isNewEntry)
<span class="line-modified"> 600                         addConstant(ConcurrentJSLocker(m_lock), op.lexicalEnvironment);</span>
 601                     metadata.m_lexicalEnvironment.set(vm, this, op.lexicalEnvironment);
 602                 } else
 603                     metadata.m_symbolTable.set(vm, this, op.lexicalEnvironment-&gt;symbolTable());
 604             } else if (JSScope* constantScope = JSScope::constantScopeForCodeBlock(op.type, this)) {
 605                 metadata.m_constantScope.set(vm, this, constantScope);
 606                 if (op.type == GlobalProperty || op.type == GlobalPropertyWithVarInjectionChecks)
 607                     metadata.m_globalLexicalBindingEpoch = m_globalObject-&gt;globalLexicalBindingEpoch();
 608             } else
 609                 metadata.m_globalObject.clear();
 610             break;
 611         }
 612 
 613         case op_get_from_scope: {
 614             INITIALIZE_METADATA(OpGetFromScope)
 615 
 616             link_profile(instruction, bytecode, metadata);
 617             metadata.m_watchpointSet = nullptr;
 618 
 619             ASSERT(!isInitialization(bytecode.m_getPutInfo.initializationMode()));
 620             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 621                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 622                 break;
 623             }
 624 
 625             const Identifier&amp; ident = identifier(bytecode.m_var);
<span class="line-modified"> 626             ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_getPutInfo.resolveType(), InitializationMode::NotInitialization);</span>
 627             RETURN_IF_EXCEPTION(throwScope, false);
 628 
 629             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 630             if (op.type == ModuleVar)
 631                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 632             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 633                 metadata.m_watchpointSet = op.watchpointSet;
 634             else if (op.structure)
 635                 metadata.m_structure.set(vm, this, op.structure);
 636             metadata.m_operand = op.operand;
 637             break;
 638         }
 639 
 640         case op_put_to_scope: {
 641             INITIALIZE_METADATA(OpPutToScope)
 642 
 643             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 644                 // Only do watching if the property we&#39;re putting to is not anonymous.
 645                 if (bytecode.m_var != UINT_MAX) {
<span class="line-modified"> 646                     SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable()));</span>
 647                     const Identifier&amp; ident = identifier(bytecode.m_var);
 648                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 649                     auto iter = symbolTable-&gt;find(locker, ident.impl());
 650                     ASSERT(iter != symbolTable-&gt;end(locker));
 651                     iter-&gt;value.prepareToWatch();
 652                     metadata.m_watchpointSet = iter-&gt;value.watchpointSet();
 653                 } else
 654                     metadata.m_watchpointSet = nullptr;
 655                 break;
 656             }
 657 
 658             const Identifier&amp; ident = identifier(bytecode.m_var);
 659             metadata.m_watchpointSet = nullptr;
<span class="line-modified"> 660             ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), bytecode.m_symbolTableOrScopeDepth.scopeDepth(), scope, ident, Put, bytecode.m_getPutInfo.resolveType(), bytecode.m_getPutInfo.initializationMode());</span>
 661             RETURN_IF_EXCEPTION(throwScope, false);
 662 
 663             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 664             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 665                 metadata.m_watchpointSet = op.watchpointSet;
 666             else if (op.type == ClosureVar || op.type == ClosureVarWithVarInjectionChecks) {
 667                 if (op.watchpointSet)
 668                     op.watchpointSet-&gt;invalidate(vm, PutToScopeFireDetail(this, ident));
 669             } else if (op.structure)
 670                 metadata.m_structure.set(vm, this, op.structure);
 671             metadata.m_operand = op.operand;
 672             break;
 673         }
 674 
 675         case op_profile_type: {
 676             RELEASE_ASSERT(m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes());
 677 
 678             INITIALIZE_METADATA(OpProfileType)
 679 
 680             size_t instructionOffset = instruction.offset() + instruction-&gt;size() - 1;
 681             unsigned divotStart, divotEnd;
 682             GlobalVariableID globalVariableID = 0;
 683             RefPtr&lt;TypeSet&gt; globalTypeSet;
 684             bool shouldAnalyze = m_unlinkedCode-&gt;typeProfilerExpressionInfoForBytecodeOffset(instructionOffset, divotStart, divotEnd);
 685             SymbolTable* symbolTable = nullptr;
 686 
 687             switch (bytecode.m_flag) {
 688             case ProfileTypeBytecodeClosureVar: {
 689                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 690                 unsigned localScopeDepth = bytecode.m_symbolTableOrScopeDepth.scopeDepth();
 691                 // Even though type profiling may be profiling either a Get or a Put, we can always claim a Get because
 692                 // we&#39;re abstractly &quot;read&quot;ing from a JSScope.
<span class="line-modified"> 693                 ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);</span>
 694                 RETURN_IF_EXCEPTION(throwScope, false);
 695 
 696                 if (op.type == ClosureVar || op.type == ModuleVar)
 697                     symbolTable = op.lexicalEnvironment-&gt;symbolTable();
 698                 else if (op.type == GlobalVar)
 699                     symbolTable = m_globalObject.get()-&gt;symbolTable();
 700 
 701                 UniquedStringImpl* impl = (op.type == ModuleVar) ? op.importedName.get() : ident.impl();
 702                 if (symbolTable) {
 703                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 704                     // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 705                     symbolTable-&gt;prepareForTypeProfiling(locker);
 706                     globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, impl, vm);
 707                     globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, impl, vm);
 708                 } else
 709                     globalVariableID = TypeProfilerNoGlobalIDExists;
 710 
 711                 break;
 712             }
 713             case ProfileTypeBytecodeLocallyResolved: {
<span class="line-modified"> 714                 SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable()));</span>

 715                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 716                 ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 717                 // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 718                 globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, ident.impl(), vm);
 719                 globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, ident.impl(), vm);
 720 
 721                 break;
 722             }
 723             case ProfileTypeBytecodeDoesNotHaveGlobalID:
 724             case ProfileTypeBytecodeFunctionArgument: {
 725                 globalVariableID = TypeProfilerNoGlobalIDExists;
 726                 break;
 727             }
 728             case ProfileTypeBytecodeFunctionReturnStatement: {
 729                 RELEASE_ASSERT(ownerExecutable-&gt;isFunctionExecutable());
 730                 globalTypeSet = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable)-&gt;returnStatementTypeSet();
 731                 globalVariableID = TypeProfilerReturnStatement;
 732                 if (!shouldAnalyze) {
 733                     // Because a return statement can be added implicitly to return undefined at the end of a function,
 734                     // and these nodes don&#39;t emit expression ranges because they aren&#39;t in the actual source text of
</pre>
<hr />
<pre>
 741                 break;
 742             }
 743             }
 744 
 745             std::pair&lt;TypeLocation*, bool&gt; locationPair = vm.typeProfiler()-&gt;typeLocationCache()-&gt;getTypeLocation(globalVariableID,
 746                 ownerExecutable-&gt;sourceID(), divotStart, divotEnd, WTFMove(globalTypeSet), &amp;vm);
 747             TypeLocation* location = locationPair.first;
 748             bool isNewLocation = locationPair.second;
 749 
 750             if (bytecode.m_flag == ProfileTypeBytecodeFunctionReturnStatement)
 751                 location-&gt;m_divotForFunctionOffsetIfReturnStatement = ownerExecutable-&gt;typeProfilingStartOffset(vm);
 752 
 753             if (shouldAnalyze &amp;&amp; isNewLocation)
 754                 vm.typeProfiler()-&gt;insertNewLocation(location);
 755 
 756             metadata.m_typeLocation = location;
 757             break;
 758         }
 759 
 760         case op_debug: {
<span class="line-modified"> 761             if (instruction-&gt;as&lt;OpDebug&gt;().m_debugHookType == DidReachDebuggerStatement)</span>
 762                 m_hasDebuggerStatement = true;
 763             break;
 764         }
 765 
 766         case op_create_rest: {
 767             int numberOfArgumentsToSkip = instruction-&gt;as&lt;OpCreateRest&gt;().m_numParametersToSkip;
 768             ASSERT_UNUSED(numberOfArgumentsToSkip, numberOfArgumentsToSkip &gt;= 0);
 769             // This is used when rematerializing the rest parameter during OSR exit in the FTL JIT.&quot;);
 770             m_numberOfArgumentsToSkip = numberOfArgumentsToSkip;
 771             break;
 772         }
 773 
 774         default:
 775             break;
 776         }
 777     }
 778 
 779 #undef CASE
 780 #undef INITIALIZE_METADATA
 781 #undef LINK_FIELD
</pre>
<hr />
<pre>
 850     // is no guarantee about the order in which the CodeBlocks are destroyed.
 851     // So, if we don&#39;t remove incoming calls, and get destroyed before the
 852     // CodeBlock(s) that have calls into us, then the CallLinkInfo vector&#39;s
 853     // destructor will try to remove nodes from our (no longer valid) linked list.
 854     unlinkIncomingCalls();
 855 
 856     // Note that our outgoing calls will be removed from other CodeBlocks&#39;
 857     // m_incomingCalls linked lists through the execution of the ~CallLinkInfo
 858     // destructors.
 859 
 860 #if ENABLE(JIT)
 861     if (auto* jitData = m_jitData.get()) {
 862         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
 863             stubInfo-&gt;aboutToDie();
 864             stubInfo-&gt;deref();
 865         }
 866     }
 867 #endif // ENABLE(JIT)
 868 }
 869 
<span class="line-modified"> 870 void CodeBlock::setConstantIdentifierSetRegisters(VM&amp; vm, const RefCountedArray&lt;ConstantIdentifierSetEntry&gt;&amp; constants)</span>
 871 {
 872     auto scope = DECLARE_THROW_SCOPE(vm);
 873     JSGlobalObject* globalObject = m_globalObject.get();

 874 
 875     for (const auto&amp; entry : constants) {
 876         const IdentifierSet&amp; set = entry.first;
 877 
 878         Structure* setStructure = globalObject-&gt;setStructure();
 879         RETURN_IF_EXCEPTION(scope, void());
<span class="line-modified"> 880         JSSet* jsSet = JSSet::create(globalObject, vm, setStructure, set.size());</span>
 881         RETURN_IF_EXCEPTION(scope, void());
 882 
<span class="line-modified"> 883         for (const auto&amp; setEntry : set) {</span>
 884             JSString* jsString = jsOwnedString(vm, setEntry.get());
<span class="line-modified"> 885             jsSet-&gt;add(globalObject, jsString);</span>
 886             RETURN_IF_EXCEPTION(scope, void());
 887         }
 888         m_constantRegisters[entry.second].set(vm, this, jsSet);
 889     }
 890 }
 891 
<span class="line-modified"> 892 void CodeBlock::setConstantRegisters(const RefCountedArray&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const RefCountedArray&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable)</span>
 893 {
 894     VM&amp; vm = *m_vm;
 895     auto scope = DECLARE_THROW_SCOPE(vm);
 896     JSGlobalObject* globalObject = m_globalObject.get();

 897 
 898     ASSERT(constants.size() == constantsSourceCodeRepresentation.size());
 899     size_t count = constants.size();
<span class="line-modified"> 900     {</span>
<span class="line-added"> 901         ConcurrentJSLocker locker(m_lock);</span>
<span class="line-added"> 902         m_constantRegisters.resizeToFit(count);</span>
<span class="line-added"> 903         m_constantsSourceCodeRepresentation.resizeToFit(count);</span>
<span class="line-added"> 904     }</span>
 905     for (size_t i = 0; i &lt; count; i++) {
 906         JSValue constant = constants[i].get();
<span class="line-modified"> 907         SourceCodeRepresentation representation = constantsSourceCodeRepresentation[i];</span>
<span class="line-modified"> 908         m_constantsSourceCodeRepresentation[i] = representation;</span>
<span class="line-modified"> 909         switch (representation) {</span>
<span class="line-modified"> 910         case SourceCodeRepresentation::LinkTimeConstant:</span>
<span class="line-modified"> 911             constant = globalObject-&gt;linkTimeConstant(static_cast&lt;LinkTimeConstant&gt;(constant.asInt32AsAnyInt()));</span>
<span class="line-modified"> 912             break;</span>
<span class="line-modified"> 913         case SourceCodeRepresentation::Other:</span>
<span class="line-modified"> 914         case SourceCodeRepresentation::Integer:</span>
<span class="line-added"> 915         case SourceCodeRepresentation::Double:</span>
<span class="line-added"> 916             if (!constant.isEmpty()) {</span>
<span class="line-added"> 917                 if (constant.isCell()) {</span>
<span class="line-added"> 918                     JSCell* cell = constant.asCell();</span>
<span class="line-added"> 919                     if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm, cell)) {</span>
<span class="line-added"> 920                         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
<span class="line-added"> 921                             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);</span>
<span class="line-added"> 922                             symbolTable-&gt;prepareForTypeProfiling(locker);</span>
<span class="line-added"> 923                         }</span>
<span class="line-added"> 924 </span>
<span class="line-added"> 925                         SymbolTable* clone = symbolTable-&gt;cloneScopePart(vm);</span>
<span class="line-added"> 926                         if (wasCompiledWithDebuggingOpcodes())</span>
<span class="line-added"> 927                             clone-&gt;setRareDataCodeBlock(this);</span>
<span class="line-added"> 928 </span>
<span class="line-added"> 929                         constant = clone;</span>
<span class="line-added"> 930                     } else if (auto* descriptor = jsDynamicCast&lt;JSTemplateObjectDescriptor*&gt;(vm, cell)) {</span>
<span class="line-added"> 931                         auto* templateObject = topLevelExecutable-&gt;createTemplateObject(globalObject, descriptor);</span>
<span class="line-added"> 932                         RETURN_IF_EXCEPTION(scope, void());</span>
<span class="line-added"> 933                         constant = templateObject;</span>
 934                     }










 935                 }
 936             }
<span class="line-added"> 937             break;</span>
 938         }

 939         m_constantRegisters[i].set(vm, this, constant);
 940     }


 941 }
 942 
 943 void CodeBlock::setAlternative(VM&amp; vm, CodeBlock* alternative)
 944 {
 945     RELEASE_ASSERT(alternative);
 946     RELEASE_ASSERT(alternative-&gt;jitCode());
 947     m_alternative.set(vm, this, alternative);
 948 }
 949 
 950 void CodeBlock::setNumParameters(int newValue)
 951 {
 952     m_numParameters = newValue;
 953 
 954     m_argumentValueProfiles = RefCountedArray&lt;ValueProfile&gt;(vm().canUseJIT() ? newValue : 0);
 955 }
 956 
 957 CodeBlock* CodeBlock::specialOSREntryBlockOrNull()
 958 {
 959 #if ENABLE(FTL_JIT)
 960     if (jitType() != JITType::DFGJIT)
</pre>
<hr />
<pre>
1071     if (UNLIKELY(Options::forceCodeBlockToJettisonDueToOldAge()))
1072         return true;
1073 
1074     if (timeSinceCreation() &lt; timeToLive(jitType()))
1075         return false;
1076 
1077     return true;
1078 }
1079 
1080 #if ENABLE(DFG_JIT)
1081 static bool shouldMarkTransition(VM&amp; vm, DFG::WeakReferenceTransition&amp; transition)
1082 {
1083     if (transition.m_codeOrigin &amp;&amp; !vm.heap.isMarked(transition.m_codeOrigin.get()))
1084         return false;
1085 
1086     if (!vm.heap.isMarked(transition.m_from.get()))
1087         return false;
1088 
1089     return true;
1090 }
<span class="line-added">1091 </span>
<span class="line-added">1092 BytecodeIndex CodeBlock::bytecodeIndexForExit(BytecodeIndex exitIndex) const</span>
<span class="line-added">1093 {</span>
<span class="line-added">1094     if (exitIndex.checkpoint()) {</span>
<span class="line-added">1095         const auto&amp; instruction = instructions().at(exitIndex);</span>
<span class="line-added">1096         exitIndex = instruction.next().index();</span>
<span class="line-added">1097     }</span>
<span class="line-added">1098     return exitIndex;</span>
<span class="line-added">1099 }</span>
1100 #endif // ENABLE(DFG_JIT)
1101 
1102 void CodeBlock::propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1103 {


1104     VM&amp; vm = *m_vm;
1105 
1106     if (jitType() == JITType::InterpreterThunk) {
<span class="line-modified">1107         if (m_metadata) {</span>
<span class="line-modified">1108             m_metadata-&gt;forEach&lt;OpPutById&gt;([&amp;] (auto&amp; metadata) {</span>




1109                 StructureID oldStructureID = metadata.m_oldStructureID;
1110                 StructureID newStructureID = metadata.m_newStructureID;
1111                 if (!oldStructureID || !newStructureID)
<span class="line-modified">1112                     return;</span>
1113                 Structure* oldStructure =
1114                     vm.heap.structureIDTable().get(oldStructureID);
1115                 Structure* newStructure =
1116                     vm.heap.structureIDTable().get(newStructureID);
1117                 if (vm.heap.isMarked(oldStructure))
1118                     visitor.appendUnbarriered(newStructure);
<span class="line-modified">1119             });</span>

1120         }
1121     }
1122 
1123 #if ENABLE(JIT)
1124     if (JITCode::isJIT(jitType())) {
1125         if (auto* jitData = m_jitData.get()) {
1126             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1127                 stubInfo-&gt;propagateTransitions(visitor);
1128         }
1129     }
1130 #endif // ENABLE(JIT)
1131 
1132 #if ENABLE(DFG_JIT)
1133     if (JITCode::isOptimizingJIT(jitType())) {
1134         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1135 
1136         dfgCommon-&gt;recordedStatuses.markIfCheap(visitor);
1137 
<span class="line-modified">1138         for (StructureID structureID : dfgCommon-&gt;weakStructureReferences)</span>
<span class="line-modified">1139             vm.getStructure(structureID)-&gt;markIfCheap(visitor);</span>
1140 
1141         for (auto&amp; transition : dfgCommon-&gt;transitions) {
1142             if (shouldMarkTransition(vm, transition)) {
1143                 // If the following three things are live, then the target of the
1144                 // transition is also live:
1145                 //
1146                 // - This code block. We know it&#39;s live already because otherwise
1147                 //   we wouldn&#39;t be scanning ourselves.
1148                 //
1149                 // - The code origin of the transition. Transitions may arise from
1150                 //   code that was inlined. They are not relevant if the user&#39;s
1151                 //   object that is required for the inlinee to run is no longer
1152                 //   live.
1153                 //
1154                 // - The source of the transition. The transition checks if some
1155                 //   heap location holds the source, and if so, stores the target.
1156                 //   Hence the source must be live for the transition to be live.
1157                 //
1158                 // We also short-circuit the liveness if the structure is harmless
1159                 // to mark (i.e. its global object and prototype are both already
</pre>
<hr />
<pre>
1178     // In rare and weird cases, this could be called on a baseline CodeBlock. One that I found was
1179     // that we might decide that the CodeBlock should be jettisoned due to old age, so the
1180     // isMarked check doesn&#39;t protect us.
1181     if (!JITCode::isOptimizingJIT(jitType()))
1182         return;
1183 
1184     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1185     // Now check all of our weak references. If all of them are live, then we
1186     // have proved liveness and so we scan our strong references. If at end of
1187     // GC we still have not proved liveness, then this code block is toast.
1188     bool allAreLiveSoFar = true;
1189     for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1190         JSCell* reference = dfgCommon-&gt;weakReferences[i].get();
1191         ASSERT(!jsDynamicCast&lt;CodeBlock*&gt;(vm, reference));
1192         if (!vm.heap.isMarked(reference)) {
1193             allAreLiveSoFar = false;
1194             break;
1195         }
1196     }
1197     if (allAreLiveSoFar) {
<span class="line-modified">1198         for (StructureID structureID : dfgCommon-&gt;weakStructureReferences) {</span>
<span class="line-modified">1199             Structure* structure = vm.getStructure(structureID);</span>
<span class="line-added">1200             if (!vm.heap.isMarked(structure)) {</span>
1201                 allAreLiveSoFar = false;
1202                 break;
1203             }
1204         }
1205     }
1206 
1207     // If some weak references are dead, then this fixpoint iteration was
1208     // unsuccessful.
1209     if (!allAreLiveSoFar)
1210         return;
1211 
1212     // All weak references are live. Record this information so we don&#39;t
1213     // come back here again, and scan the strong references.
1214     visitor.appendUnbarriered(this);
1215 #endif // ENABLE(DFG_JIT)
1216 }
1217 
1218 void CodeBlock::finalizeLLIntInlineCaches()
1219 {
1220     VM&amp; vm = *m_vm;

1221 
<span class="line-modified">1222     if (m_metadata) {</span>
<span class="line-modified">1223         // FIXME: https://bugs.webkit.org/show_bug.cgi?id=166418</span>
<span class="line-modified">1224         // We need to add optimizations for op_resolve_scope_for_hoisting_func_decl_in_eval to do link time scope resolution.</span>









1225 
<span class="line-modified">1226         m_metadata-&gt;forEach&lt;OpGetById&gt;([&amp;] (auto&amp; metadata) {</span>





1227             if (metadata.m_modeMetadata.mode != GetByIdMode::Default)
<span class="line-modified">1228                 return;</span>
1229             StructureID oldStructureID = metadata.m_modeMetadata.defaultMode.structureID;
1230             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
<span class="line-modified">1231                 return;</span>
<span class="line-modified">1232             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt property access.&quot;);</span>

1233             LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(metadata);
<span class="line-modified">1234         });</span>
<span class="line-modified">1235 </span>
<span class="line-modified">1236         m_metadata-&gt;forEach&lt;OpGetByIdDirect&gt;([&amp;] (auto&amp; metadata) {</span>

1237             StructureID oldStructureID = metadata.m_structureID;
1238             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
<span class="line-modified">1239                 return;</span>
<span class="line-modified">1240             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt property access.&quot;);</span>

1241             metadata.m_structureID = 0;
1242             metadata.m_offset = 0;
<span class="line-modified">1243         });</span>
<span class="line-modified">1244 </span>
<span class="line-modified">1245         m_metadata-&gt;forEach&lt;OpPutById&gt;([&amp;] (auto&amp; metadata) {</span>

1246             StructureID oldStructureID = metadata.m_oldStructureID;
1247             StructureID newStructureID = metadata.m_newStructureID;
1248             StructureChain* chain = metadata.m_structureChain.get();
1249             if ((!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
1250                 &amp;&amp; (!newStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(newStructureID)))
1251                 &amp;&amp; (!chain || vm.heap.isMarked(chain)))
<span class="line-modified">1252                 return;</span>
<span class="line-modified">1253             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt put transition.&quot;);</span>

1254             metadata.m_oldStructureID = 0;
1255             metadata.m_offset = 0;
1256             metadata.m_newStructureID = 0;
1257             metadata.m_structureChain.clear();
<span class="line-modified">1258         });</span>
<span class="line-modified">1259 </span>
<span class="line-modified">1260         m_metadata-&gt;forEach&lt;OpToThis&gt;([&amp;] (auto&amp; metadata) {</span>





1261             if (!metadata.m_cachedStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(metadata.m_cachedStructureID)))
<span class="line-modified">1262                 return;</span>
1263             if (Options::verboseOSR()) {
1264                 Structure* structure = vm.heap.structureIDTable().get(metadata.m_cachedStructureID);
1265                 dataLogF(&quot;Clearing LLInt to_this with structure %p.\n&quot;, structure);
1266             }
1267             metadata.m_cachedStructureID = 0;
1268             metadata.m_toThisStatus = merge(metadata.m_toThisStatus, ToThisClearedByGC);
<span class="line-modified">1269         });</span>
<span class="line-modified">1270 </span>
<span class="line-modified">1271         auto handleCreateBytecode = [&amp;] (auto&amp; metadata, ASCIILiteral name) {</span>

1272             auto&amp; cacheWriteBarrier = metadata.m_cachedCallee;
1273             if (!cacheWriteBarrier || cacheWriteBarrier.unvalidatedGet() == JSCell::seenMultipleCalleeObjects())
<span class="line-modified">1274                 return;</span>
1275             JSCell* cachedFunction = cacheWriteBarrier.get();
1276             if (vm.heap.isMarked(cachedFunction))
<span class="line-modified">1277                 return;</span>
<span class="line-modified">1278             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt &quot;, name, &quot; with cached callee &quot;, RawPointer(cachedFunction), &quot;.&quot;);</span>

1279             cacheWriteBarrier.clear();
<span class="line-modified">1280         };</span>
<span class="line-modified">1281 </span>
<span class="line-modified">1282         m_metadata-&gt;forEach&lt;OpCreateThis&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1283             handleCreateBytecode(metadata, &quot;op_create_this&quot;_s);</span>
<span class="line-added">1284         });</span>
<span class="line-added">1285         m_metadata-&gt;forEach&lt;OpCreatePromise&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1286             handleCreateBytecode(metadata, &quot;op_create_promise&quot;_s);</span>
<span class="line-added">1287         });</span>
<span class="line-added">1288         m_metadata-&gt;forEach&lt;OpCreateGenerator&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1289             handleCreateBytecode(metadata, &quot;op_create_generator&quot;_s);</span>
<span class="line-added">1290         });</span>
<span class="line-added">1291         m_metadata-&gt;forEach&lt;OpCreateAsyncGenerator&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1292             handleCreateBytecode(metadata, &quot;op_create_async_generator&quot;_s);</span>
<span class="line-added">1293         });</span>
<span class="line-added">1294 </span>
<span class="line-added">1295         m_metadata-&gt;forEach&lt;OpResolveScope&gt;([&amp;] (auto&amp; metadata) {</span>
1296             // Right now this isn&#39;t strictly necessary. Any symbol tables that this will refer to
1297             // are for outer functions, and we refer to those functions strongly, and they refer
1298             // to the symbol table strongly. But it&#39;s nice to be on the safe side.

1299             WriteBarrierBase&lt;SymbolTable&gt;&amp; symbolTable = metadata.m_symbolTable;
1300             if (!symbolTable || vm.heap.isMarked(symbolTable.get()))
<span class="line-modified">1301                 return;</span>
<span class="line-modified">1302             dataLogLnIf(Options::verboseOSR(), &quot;Clearing dead symbolTable &quot;, RawPointer(symbolTable.get()));</span>

1303             symbolTable.clear();
<span class="line-modified">1304         });</span>
<span class="line-modified">1305 </span>
<span class="line-modified">1306         auto handleGetPutFromScope = [&amp;] (auto&amp; metadata) {</span>
<span class="line-modified">1307             GetPutInfo getPutInfo = metadata.m_getPutInfo;</span>
<span class="line-modified">1308             if (getPutInfo.resolveType() == GlobalVar || getPutInfo.resolveType() == GlobalVarWithVarInjectionChecks</span>
<span class="line-modified">1309                 || getPutInfo.resolveType() == LocalClosureVar || getPutInfo.resolveType() == GlobalLexicalVar || getPutInfo.resolveType() == GlobalLexicalVarWithVarInjectionChecks)</span>
<span class="line-modified">1310                 return;</span>
<span class="line-modified">1311             WriteBarrierBase&lt;Structure&gt;&amp; structure = metadata.m_structure;</span>
<span class="line-modified">1312             if (!structure || vm.heap.isMarked(structure.get()))</span>
<span class="line-modified">1313                 return;</span>
<span class="line-modified">1314             dataLogLnIf(Options::verboseOSR(), &quot;Clearing scope access with structure &quot;, RawPointer(structure.get()));</span>
<span class="line-modified">1315             structure.clear();</span>
<span class="line-added">1316         };</span>
<span class="line-added">1317 </span>
<span class="line-added">1318         m_metadata-&gt;forEach&lt;OpGetFromScope&gt;(handleGetPutFromScope);</span>
<span class="line-added">1319         m_metadata-&gt;forEach&lt;OpPutToScope&gt;(handleGetPutFromScope);</span>
1320     }
1321 
1322     // We can&#39;t just remove all the sets when we clear the caches since we might have created a watchpoint set
1323     // then cleared the cache without GCing in between.
1324     m_llintGetByIdWatchpointMap.removeIf([&amp;] (const StructureWatchpointMap::KeyValuePairType&amp; pair) -&gt; bool {
1325         auto clear = [&amp;] () {
1326             auto&amp; instruction = instructions().at(std::get&lt;1&gt;(pair.key));
1327             OpcodeID opcode = instruction-&gt;opcodeID();
1328             if (opcode == op_get_by_id) {
<span class="line-modified">1329                 dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt property access.&quot;);</span>

1330                 LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(instruction-&gt;as&lt;OpGetById&gt;().metadata(this));
1331             }
1332             return true;
1333         };
1334 
1335         if (!vm.heap.isMarked(vm.heap.structureIDTable().get(std::get&lt;0&gt;(pair.key))))
1336             return clear();
1337 
1338         for (const LLIntPrototypeLoadAdaptiveStructureWatchpoint&amp; watchpoint : pair.value) {
1339             if (!watchpoint.key().isStillLive(vm))
1340                 return clear();
1341         }
1342 
1343         return false;
1344     });
1345 
1346     forEachLLIntCallLinkInfo([&amp;](LLIntCallLinkInfo&amp; callLinkInfo) {
1347         if (callLinkInfo.isLinked() &amp;&amp; !vm.heap.isMarked(callLinkInfo.callee())) {
<span class="line-modified">1348             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt call from &quot;, *this);</span>

1349             callLinkInfo.unlink();
1350         }
1351         if (callLinkInfo.lastSeenCallee() &amp;&amp; !vm.heap.isMarked(callLinkInfo.lastSeenCallee()))
1352             callLinkInfo.clearLastSeenCallee();
1353     });
1354 }
1355 
1356 #if ENABLE(JIT)
1357 CodeBlock::JITData&amp; CodeBlock::ensureJITDataSlow(const ConcurrentJSLocker&amp;)
1358 {
1359     ASSERT(!m_jitData);
<span class="line-modified">1360     auto jitData = makeUnique&lt;JITData&gt;();</span>
<span class="line-added">1361     // calleeSaveRegisters() can access m_jitData without taking a lock from Baseline JIT. This is OK since JITData::m_calleeSaveRegisters is filled in DFG and FTL CodeBlocks.</span>
<span class="line-added">1362     // But we should not see garbage pointer in that case. We ensure JITData::m_calleeSaveRegisters is initialized as nullptr before exposing it to BaselineJIT by store-store-fence.</span>
<span class="line-added">1363     WTF::storeStoreFence();</span>
<span class="line-added">1364     m_jitData = WTFMove(jitData);</span>
1365     return *m_jitData;
1366 }
1367 
1368 void CodeBlock::finalizeBaselineJITInlineCaches()
1369 {
1370     if (auto* jitData = m_jitData.get()) {
1371         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1372             callLinkInfo-&gt;visitWeak(vm());
1373 
1374         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1375             stubInfo-&gt;visitWeakReferences(this);
1376     }
1377 }
1378 #endif
1379 
1380 void CodeBlock::finalizeUnconditionally(VM&amp; vm)
1381 {
1382     UNUSED_PARAM(vm);
1383 
1384     updateAllPredictions();
1385 
<span class="line-added">1386 #if ENABLE(JIT)</span>
<span class="line-added">1387     bool isEligibleForLLIntDowngrade = m_isEligibleForLLIntDowngrade;</span>
<span class="line-added">1388     m_isEligibleForLLIntDowngrade = false;</span>
<span class="line-added">1389     // If BaselineJIT code is not executing, and an optimized replacement exists, we attempt</span>
<span class="line-added">1390     // to discard baseline JIT code and reinstall LLInt code to save JIT memory.</span>
<span class="line-added">1391     if (Options::useLLInt() &amp;&amp; !m_hasLinkedOSRExit &amp;&amp; jitType() == JITType::BaselineJIT &amp;&amp; !m_vm-&gt;heap.codeBlockSet().isCurrentlyExecuting(this)) {</span>
<span class="line-added">1392         if (CodeBlock* optimizedCodeBlock = optimizedReplacement()) {</span>
<span class="line-added">1393             if (!optimizedCodeBlock-&gt;m_osrExitCounter) {</span>
<span class="line-added">1394                 if (isEligibleForLLIntDowngrade) {</span>
<span class="line-added">1395                     m_jitCode = nullptr;</span>
<span class="line-added">1396                     LLInt::setEntrypoint(this);</span>
<span class="line-added">1397                     RELEASE_ASSERT(jitType() == JITType::InterpreterThunk);</span>
<span class="line-added">1398 </span>
<span class="line-added">1399                     for (size_t i = 0; i &lt; m_unlinkedCode-&gt;numberOfExceptionHandlers(); i++) {</span>
<span class="line-added">1400                         const UnlinkedHandlerInfo&amp; unlinkedHandler = m_unlinkedCode-&gt;exceptionHandler(i);</span>
<span class="line-added">1401                         HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];</span>
<span class="line-added">1402                         auto&amp; instruction = *instructions().at(unlinkedHandler.target).ptr();</span>
<span class="line-added">1403                         MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(instruction);</span>
<span class="line-added">1404                         handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));</span>
<span class="line-added">1405                     }</span>
<span class="line-added">1406 </span>
<span class="line-added">1407                     unlinkIncomingCalls();</span>
<span class="line-added">1408 </span>
<span class="line-added">1409                     // It&#39;s safe to clear these out here because in finalizeUnconditionally all compiler threads</span>
<span class="line-added">1410                     // are safepointed, meaning they&#39;re running either before or after bytecode parser, and bytecode</span>
<span class="line-added">1411                     // parser is the only data structure pointing into the various *infos.</span>
<span class="line-added">1412                     resetJITData();</span>
<span class="line-added">1413                 } else</span>
<span class="line-added">1414                     m_isEligibleForLLIntDowngrade = true;</span>
<span class="line-added">1415             }</span>
<span class="line-added">1416         }</span>
<span class="line-added">1417     }</span>
<span class="line-added">1418 </span>
<span class="line-added">1419 #endif</span>
<span class="line-added">1420 </span>
1421     if (JITCode::couldBeInterpreted(jitType()))
1422         finalizeLLIntInlineCaches();
1423 
1424 #if ENABLE(JIT)
1425     if (!!jitCode())
1426         finalizeBaselineJITInlineCaches();
1427 #endif
1428 
1429 #if ENABLE(DFG_JIT)
1430     if (JITCode::isOptimizingJIT(jitType())) {
1431         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1432         dfgCommon-&gt;recordedStatuses.finalize(vm);
1433     }
1434 #endif // ENABLE(DFG_JIT)
1435 
1436     auto updateActivity = [&amp;] {
1437         if (!VM::useUnlinkedCodeBlockJettisoning())
1438             return;
1439         JITCode* jitCode = m_jitCode.get();
1440         double count = 0;
</pre>
<hr />
<pre>
1503 #endif
1504     }
1505 #else
1506     UNUSED_PARAM(result);
1507 #endif
1508 }
1509 
1510 void CodeBlock::getICStatusMap(ICStatusMap&amp; result)
1511 {
1512     ConcurrentJSLocker locker(m_lock);
1513     getICStatusMap(locker, result);
1514 }
1515 
1516 #if ENABLE(JIT)
1517 StructureStubInfo* CodeBlock::addStubInfo(AccessType accessType)
1518 {
1519     ConcurrentJSLocker locker(m_lock);
1520     return ensureJITData(locker).m_stubInfos.add(accessType);
1521 }
1522 
<span class="line-modified">1523 JITAddIC* CodeBlock::addJITAddIC(BinaryArithProfile* arithProfile)</span>
1524 {
1525     ConcurrentJSLocker locker(m_lock);
1526     return ensureJITData(locker).m_addICs.add(arithProfile);
1527 }
1528 
<span class="line-modified">1529 JITMulIC* CodeBlock::addJITMulIC(BinaryArithProfile* arithProfile)</span>
1530 {
1531     ConcurrentJSLocker locker(m_lock);
1532     return ensureJITData(locker).m_mulICs.add(arithProfile);
1533 }
1534 
<span class="line-modified">1535 JITSubIC* CodeBlock::addJITSubIC(BinaryArithProfile* arithProfile)</span>
1536 {
1537     ConcurrentJSLocker locker(m_lock);
1538     return ensureJITData(locker).m_subICs.add(arithProfile);
1539 }
1540 
<span class="line-modified">1541 JITNegIC* CodeBlock::addJITNegIC(UnaryArithProfile* arithProfile)</span>
1542 {
1543     ConcurrentJSLocker locker(m_lock);
1544     return ensureJITData(locker).m_negICs.add(arithProfile);
1545 }
1546 
1547 StructureStubInfo* CodeBlock::findStubInfo(CodeOrigin codeOrigin)
1548 {
1549     ConcurrentJSLocker locker(m_lock);
1550     if (auto* jitData = m_jitData.get()) {
1551         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
1552             if (stubInfo-&gt;codeOrigin == codeOrigin)
1553                 return stubInfo;
1554         }
1555     }
1556     return nullptr;
1557 }
1558 
1559 ByValInfo* CodeBlock::addByValInfo()
1560 {
1561     ConcurrentJSLocker locker(m_lock);
1562     return ensureJITData(locker).m_byValInfos.add();
1563 }
1564 
1565 CallLinkInfo* CodeBlock::addCallLinkInfo()
1566 {
1567     ConcurrentJSLocker locker(m_lock);
1568     return ensureJITData(locker).m_callLinkInfos.add();
1569 }
1570 
<span class="line-modified">1571 CallLinkInfo* CodeBlock::getCallLinkInfoForBytecodeIndex(BytecodeIndex index)</span>
1572 {
1573     ConcurrentJSLocker locker(m_lock);
1574     if (auto* jitData = m_jitData.get()) {
1575         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos) {
1576             if (callLinkInfo-&gt;codeOrigin() == CodeOrigin(index))
1577                 return callLinkInfo;
1578         }
1579     }
1580     return nullptr;
1581 }
1582 
<span class="line-modified">1583 void CodeBlock::setRareCaseProfiles(RefCountedArray&lt;RareCaseProfile&gt;&amp;&amp; rareCaseProfiles)</span>
1584 {
1585     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1586     ensureJITData(locker).m_rareCaseProfiles = WTFMove(rareCaseProfiles);</span>


1587 }
1588 
<span class="line-modified">1589 RareCaseProfile* CodeBlock::rareCaseProfileForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex bytecodeIndex)</span>
1590 {
1591     if (auto* jitData = m_jitData.get()) {
<span class="line-modified">1592         return tryBinarySearch&lt;RareCaseProfile, BytecodeIndex&gt;(</span>
<span class="line-modified">1593             jitData-&gt;m_rareCaseProfiles, jitData-&gt;m_rareCaseProfiles.size(), bytecodeIndex,</span>
<span class="line-modified">1594             getRareCaseProfileBytecodeIndex);</span>
1595     }
1596     return nullptr;
1597 }
1598 
<span class="line-modified">1599 unsigned CodeBlock::rareCaseProfileCountForBytecodeIndex(const ConcurrentJSLocker&amp; locker, BytecodeIndex bytecodeIndex)</span>
1600 {
<span class="line-modified">1601     RareCaseProfile* profile = rareCaseProfileForBytecodeIndex(locker, bytecodeIndex);</span>
1602     if (profile)
1603         return profile-&gt;m_counter;
1604     return 0;
1605 }
1606 
1607 void CodeBlock::setCalleeSaveRegisters(RegisterSet calleeSaveRegisters)
1608 {
1609     ConcurrentJSLocker locker(m_lock);
1610     ensureJITData(locker).m_calleeSaveRegisters = makeUnique&lt;RegisterAtOffsetList&gt;(calleeSaveRegisters);
1611 }
1612 
1613 void CodeBlock::setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt; registerAtOffsetList)
1614 {
1615     ConcurrentJSLocker locker(m_lock);
1616     ensureJITData(locker).m_calleeSaveRegisters = WTFMove(registerAtOffsetList);
1617 }
1618 
1619 void CodeBlock::resetJITData()
1620 {
1621     RELEASE_ASSERT(!JITCode::isJIT(jitType()));
1622     ConcurrentJSLocker locker(m_lock);
1623 
1624     if (auto* jitData = m_jitData.get()) {
1625         // We can clear these because no other thread will have references to any stub infos, call
1626         // link infos, or by val infos if we don&#39;t have JIT code. Attempts to query these data
1627         // structures using the concurrent API (getICStatusMap and friends) will return nothing if we
<span class="line-modified">1628         // don&#39;t have JIT code. So it&#39;s safe to call this if we fail a baseline JIT compile.</span>
<span class="line-modified">1629         //</span>
<span class="line-modified">1630         // We also call this from finalizeUnconditionally when we degrade from baseline JIT to LLInt</span>
<span class="line-modified">1631         // code. This is safe to do since all compiler threads are safepointed in finalizeUnconditionally,</span>
<span class="line-added">1632         // which means we&#39;ve made it past bytecode parsing. Only the bytecode parser will hold onto</span>
<span class="line-added">1633         // references to these various *infos via its use of ICStatusMap. Also, OSR exit might point to</span>
<span class="line-added">1634         // these *infos, but when we have an OSR exit linked to this CodeBlock, we won&#39;t downgrade</span>
<span class="line-added">1635         // to LLInt.</span>
<span class="line-added">1636 </span>
<span class="line-added">1637         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {</span>
<span class="line-added">1638             stubInfo-&gt;aboutToDie();</span>
<span class="line-added">1639             stubInfo-&gt;deref();</span>
<span class="line-added">1640         }</span>
<span class="line-added">1641 </span>
1642         // We can clear this because the DFG&#39;s queries to these data structures are guarded by whether
1643         // there is JIT code.
<span class="line-modified">1644 </span>
<span class="line-added">1645         m_jitData = nullptr;</span>
1646     }
1647 }
1648 #endif
1649 
1650 void CodeBlock::visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1651 {
1652     // We strongly visit OSR exits targets because we don&#39;t want to deal with
1653     // the complexity of generating an exit target CodeBlock on demand and
1654     // guaranteeing that it matches the details of the CodeBlock we compiled
1655     // the OSR exit against.
1656 
1657     visitor.append(m_alternative);
1658 
1659 #if ENABLE(DFG_JIT)
1660     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1661     if (dfgCommon-&gt;inlineCallFrames) {
1662         for (auto* inlineCallFrame : *dfgCommon-&gt;inlineCallFrames) {
1663             ASSERT(inlineCallFrame-&gt;baselineCodeBlock);
1664             visitor.append(inlineCallFrame-&gt;baselineCodeBlock);
1665         }
</pre>
<hr />
<pre>
1672     UNUSED_PARAM(locker);
1673 
1674     visitor.append(m_globalObject);
1675     visitor.append(m_ownerExecutable); // This is extra important since it causes the ExecutableToCodeBlockEdge to be marked.
1676     visitor.append(m_unlinkedCode);
1677     if (m_rareData)
1678         m_rareData-&gt;m_directEvalCodeCache.visitAggregate(visitor);
1679     visitor.appendValues(m_constantRegisters.data(), m_constantRegisters.size());
1680     for (auto&amp; functionExpr : m_functionExprs)
1681         visitor.append(functionExpr);
1682     for (auto&amp; functionDecl : m_functionDecls)
1683         visitor.append(functionDecl);
1684     forEachObjectAllocationProfile([&amp;](ObjectAllocationProfile&amp; objectAllocationProfile) {
1685         objectAllocationProfile.visitAggregate(visitor);
1686     });
1687 
1688 #if ENABLE(JIT)
1689     if (auto* jitData = m_jitData.get()) {
1690         for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1691             visitor.append(byValInfo-&gt;cachedSymbol);
<span class="line-added">1692         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)</span>
<span class="line-added">1693             stubInfo-&gt;visitAggregate(visitor);</span>
1694     }
1695 #endif
1696 
1697 #if ENABLE(DFG_JIT)
<span class="line-modified">1698     if (JITCode::isOptimizingJIT(jitType())) {</span>
<span class="line-added">1699         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();</span>
<span class="line-added">1700         dfgCommon-&gt;recordedStatuses.visitAggregate(visitor);</span>
1701         visitOSRExitTargets(locker, visitor);
<span class="line-added">1702     }</span>
1703 #endif
1704 }
1705 
1706 void CodeBlock::stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1707 {
1708     UNUSED_PARAM(visitor);
1709 
1710 #if ENABLE(DFG_JIT)
1711     if (!JITCode::isOptimizingJIT(jitType()))
1712         return;
1713 
1714     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1715 
1716     for (auto&amp; transition : dfgCommon-&gt;transitions) {
1717         if (!!transition.m_codeOrigin)
1718             visitor.append(transition.m_codeOrigin); // Almost certainly not necessary, since the code origin should also be a weak reference. Better to be safe, though.
1719         visitor.append(transition.m_from);
1720         visitor.append(transition.m_to);
1721     }
1722 
1723     for (auto&amp; weakReference : dfgCommon-&gt;weakReferences)
1724         visitor.append(weakReference);
1725 
<span class="line-modified">1726     for (StructureID structureID : dfgCommon-&gt;weakStructureReferences)</span>
<span class="line-modified">1727         visitor.appendUnbarriered(visitor.vm().getStructure(structureID));</span>
1728 
1729     dfgCommon-&gt;livenessHasBeenProved = true;
1730 #endif
1731 }
1732 
1733 CodeBlock* CodeBlock::baselineAlternative()
1734 {
1735 #if ENABLE(JIT)
1736     CodeBlock* result = this;
1737     while (result-&gt;alternative())
1738         result = result-&gt;alternative();
1739     RELEASE_ASSERT(result);
1740     RELEASE_ASSERT(JITCode::isBaselineCode(result-&gt;jitType()) || result-&gt;jitType() == JITType::None);
1741     return result;
1742 #else
1743     return this;
1744 #endif
1745 }
1746 
1747 CodeBlock* CodeBlock::baselineVersion()
</pre>
<hr />
<pre>
1757             // has been purged of its codeBlocks (see ExecutableBase::clearCode()). Regardless,
1758             // the current codeBlock is still live on the stack, and as an optimizing JIT
1759             // codeBlock, it will keep its baselineAlternative() alive for us to fetch below.
1760             result = this;
1761         } else {
1762             // This can happen if we&#39;re creating the original CodeBlock for an executable.
1763             // Assume that we&#39;re the baseline CodeBlock.
1764             RELEASE_ASSERT(selfJITType == JITType::None);
1765             return this;
1766         }
1767     }
1768     result = result-&gt;baselineAlternative();
1769     ASSERT(result);
1770     return result;
1771 #else
1772     return this;
1773 #endif
1774 }
1775 
1776 #if ENABLE(JIT)
<span class="line-modified">1777 CodeBlock* CodeBlock::optimizedReplacement(JITType typeToReplace)</span>
1778 {
1779     CodeBlock* replacement = this-&gt;replacement();
<span class="line-modified">1780     if (!replacement)</span>
<span class="line-added">1781         return nullptr;</span>
<span class="line-added">1782     if (JITCode::isHigherTier(replacement-&gt;jitType(), typeToReplace))</span>
<span class="line-added">1783         return replacement;</span>
<span class="line-added">1784     return nullptr;</span>
<span class="line-added">1785 }</span>
<span class="line-added">1786 </span>
<span class="line-added">1787 CodeBlock* CodeBlock::optimizedReplacement()</span>
<span class="line-added">1788 {</span>
<span class="line-added">1789     return optimizedReplacement(jitType());</span>
<span class="line-added">1790 }</span>
<span class="line-added">1791 </span>
<span class="line-added">1792 bool CodeBlock::hasOptimizedReplacement(JITType typeToReplace)</span>
<span class="line-added">1793 {</span>
<span class="line-added">1794     return !!optimizedReplacement(typeToReplace);</span>
1795 }
1796 
1797 bool CodeBlock::hasOptimizedReplacement()
1798 {
1799     return hasOptimizedReplacement(jitType());
1800 }
1801 #endif
1802 
<span class="line-modified">1803 HandlerInfo* CodeBlock::handlerForBytecodeIndex(BytecodeIndex bytecodeIndex, RequiredHandler requiredHandler)</span>
1804 {
<span class="line-modified">1805     RELEASE_ASSERT(bytecodeIndex.offset() &lt; instructions().size());</span>
<span class="line-modified">1806     return handlerForIndex(bytecodeIndex.offset(), requiredHandler);</span>
1807 }
1808 
1809 HandlerInfo* CodeBlock::handlerForIndex(unsigned index, RequiredHandler requiredHandler)
1810 {
1811     if (!m_rareData)
1812         return 0;
<span class="line-modified">1813     return HandlerInfo::handlerForIndex&lt;HandlerInfo&gt;(m_rareData-&gt;m_exceptionHandlers, index, requiredHandler);</span>
1814 }
1815 
1816 DisposableCallSiteIndex CodeBlock::newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite)
1817 {
1818 #if ENABLE(DFG_JIT)
1819     RELEASE_ASSERT(JITCode::isOptimizingJIT(jitType()));
1820     RELEASE_ASSERT(canGetCodeOrigin(originalCallSite));
1821     ASSERT(!!handlerForIndex(originalCallSite.bits()));
1822     CodeOrigin originalOrigin = codeOrigin(originalCallSite);
1823     return m_jitCode-&gt;dfgCommon()-&gt;addDisposableCallSiteIndex(originalOrigin);
1824 #else
1825     // We never create new on-the-fly exception handling
1826     // call sites outside the DFG/FTL inline caches.
1827     UNUSED_PARAM(originalCallSite);
1828     RELEASE_ASSERT_NOT_REACHED();
1829     return DisposableCallSiteIndex(0u);
1830 #endif
1831 }
1832 
1833 
1834 
<span class="line-modified">1835 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
1836 {
<span class="line-modified">1837     auto&amp; instruction = instructions().at(bytecodeIndex);</span>
1838     OpCatch op = instruction-&gt;as&lt;OpCatch&gt;();
1839     auto&amp; metadata = op.metadata(this);
1840     if (!!metadata.m_buffer) {
<span class="line-modified">1841 #if ASSERT_ENABLED</span>
1842         ConcurrentJSLocker locker(m_lock);
1843         bool found = false;
1844         auto* rareData = m_rareData.get();
1845         ASSERT(rareData);
1846         for (auto&amp; profile : rareData-&gt;m_catchProfiles) {
1847             if (profile.get() == metadata.m_buffer) {
1848                 found = true;
1849                 break;
1850             }
1851         }
1852         ASSERT(found);
<span class="line-modified">1853 #endif // ASSERT_ENABLED</span>
1854         return;
1855     }
1856 
<span class="line-modified">1857     ensureCatchLivenessIsComputedForBytecodeIndexSlow(op, bytecodeIndex);</span>
1858 }
1859 
<span class="line-modified">1860 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeIndexSlow(const OpCatch&amp; op, BytecodeIndex bytecodeIndex)</span>
1861 {
1862     BytecodeLivenessAnalysis&amp; bytecodeLiveness = livenessAnalysis();
1863 
1864     // We get the live-out set of variables at op_catch, not the live-in. This
1865     // is because the variables that the op_catch defines might be dead, and
1866     // we can avoid profiling them and extracting them when doing OSR entry
1867     // into the DFG.
1868 
<span class="line-modified">1869     auto nextOffset = instructions().at(bytecodeIndex).next().offset();</span>
<span class="line-modified">1870     FastBitVector liveLocals = bytecodeLiveness.getLivenessInfoAtBytecodeIndex(this, BytecodeIndex(nextOffset));</span>
1871     Vector&lt;VirtualRegister&gt; liveOperands;
1872     liveOperands.reserveInitialCapacity(liveLocals.bitCount());
1873     liveLocals.forEachSetBit([&amp;] (unsigned liveLocal) {
1874         liveOperands.append(virtualRegisterForLocal(liveLocal));
1875     });
1876 
1877     for (int i = 0; i &lt; numParameters(); ++i)
<span class="line-modified">1878         liveOperands.append(virtualRegisterForArgumentIncludingThis(i));</span>
1879 
<span class="line-modified">1880     auto profiles = makeUnique&lt;ValueProfileAndVirtualRegisterBuffer&gt;(liveOperands.size());</span>
1881     RELEASE_ASSERT(profiles-&gt;m_size == liveOperands.size());
1882     for (unsigned i = 0; i &lt; profiles-&gt;m_size; ++i)
<span class="line-modified">1883         profiles-&gt;m_buffer.get()[i].m_operand = liveOperands[i];</span>
1884 
1885     createRareDataIfNecessary();
1886 
1887     // The compiler thread will read this pointer value and then proceed to dereference it
1888     // if it is not null. We need to make sure all above stores happen before this store so
1889     // the compiler thread reads fully initialized data.
1890     WTF::storeStoreFence();
1891 
1892     op.metadata(this).m_buffer = profiles.get();
1893     {
1894         ConcurrentJSLocker locker(m_lock);
1895         m_rareData-&gt;m_catchProfiles.append(WTFMove(profiles));
1896     }
1897 }
1898 
1899 void CodeBlock::removeExceptionHandlerForCallSite(DisposableCallSiteIndex callSiteIndex)
1900 {
1901     RELEASE_ASSERT(m_rareData);
1902     Vector&lt;HandlerInfo&gt;&amp; exceptionHandlers = m_rareData-&gt;m_exceptionHandlers;
1903     unsigned index = callSiteIndex.bits();
1904     for (size_t i = 0; i &lt; exceptionHandlers.size(); ++i) {
1905         HandlerInfo&amp; handler = exceptionHandlers[i];
1906         if (handler.start &lt;= index &amp;&amp; handler.end &gt; index) {
1907             exceptionHandlers.remove(i);
1908             return;
1909         }
1910     }
1911 
1912     RELEASE_ASSERT_NOT_REACHED();
1913 }
1914 
<span class="line-modified">1915 unsigned CodeBlock::lineNumberForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
1916 {
<span class="line-modified">1917     RELEASE_ASSERT(bytecodeIndex.offset() &lt; instructions().size());</span>
<span class="line-modified">1918     return ownerExecutable()-&gt;firstLine() + m_unlinkedCode-&gt;lineNumberForBytecodeIndex(bytecodeIndex);</span>
1919 }
1920 
<span class="line-modified">1921 unsigned CodeBlock::columnNumberForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
1922 {
1923     int divot;
1924     int startOffset;
1925     int endOffset;
1926     unsigned line;
1927     unsigned column;
<span class="line-modified">1928     expressionRangeForBytecodeIndex(bytecodeIndex, divot, startOffset, endOffset, line, column);</span>
1929     return column;
1930 }
1931 
<span class="line-modified">1932 void CodeBlock::expressionRangeForBytecodeIndex(BytecodeIndex bytecodeIndex, int&amp; divot, int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const</span>
1933 {
<span class="line-modified">1934     m_unlinkedCode-&gt;expressionRangeForBytecodeIndex(bytecodeIndex, divot, startOffset, endOffset, line, column);</span>
1935     divot += sourceOffset();
1936     column += line ? 1 : firstLineColumnOffset();
1937     line += ownerExecutable()-&gt;firstLine();
1938 }
1939 
<span class="line-modified">1940 bool CodeBlock::hasOpDebugForLineAndColumn(unsigned line, Optional&lt;unsigned&gt; column)</span>
1941 {
1942     const InstructionStream&amp; instructionStream = instructions();
1943     for (const auto&amp; it : instructionStream) {
1944         if (it-&gt;is&lt;OpDebug&gt;()) {
1945             int unused;
1946             unsigned opDebugLine;
1947             unsigned opDebugColumn;
<span class="line-modified">1948             expressionRangeForBytecodeIndex(it.index(), unused, unused, unused, opDebugLine, opDebugColumn);</span>
<span class="line-modified">1949             if (line == opDebugLine &amp;&amp; (!column || column == opDebugColumn))</span>
1950                 return true;
1951         }
1952     }
1953     return false;
1954 }
1955 
<span class="line-modified">1956 void CodeBlock::shrinkToFit(const ConcurrentJSLocker&amp;, ShrinkMode shrinkMode)</span>
1957 {
<span class="line-modified">1958 #if USE(JSVALUE32_64)</span>
<span class="line-modified">1959     // Only 32bit Baseline JIT is touching m_constantRegisters address directly.</span>
<span class="line-modified">1960     if (shrinkMode == ShrinkMode::EarlyShrink)</span>





1961         m_constantRegisters.shrinkToFit();
<span class="line-modified">1962 #else</span>
<span class="line-added">1963     m_constantRegisters.shrinkToFit();</span>
<span class="line-added">1964 #endif</span>
<span class="line-added">1965     m_constantsSourceCodeRepresentation.shrinkToFit();</span>
1966 
<span class="line-added">1967     if (shrinkMode == ShrinkMode::EarlyShrink) {</span>
1968         if (m_rareData) {
1969             m_rareData-&gt;m_switchJumpTables.shrinkToFit();
1970             m_rareData-&gt;m_stringSwitchJumpTables.shrinkToFit();
1971         }
1972     } // else don&#39;t shrink these, because we would have already pointed pointers into these tables.
1973 }
1974 
1975 #if ENABLE(JIT)
<span class="line-modified">1976 void CodeBlock::linkIncomingCall(CallFrame* callerFrame, CallLinkInfo* incoming)</span>
1977 {
1978     noticeIncomingCall(callerFrame);
1979     ConcurrentJSLocker locker(m_lock);
1980     ensureJITData(locker).m_incomingCalls.push(incoming);
1981 }
1982 
<span class="line-modified">1983 void CodeBlock::linkIncomingPolymorphicCall(CallFrame* callerFrame, PolymorphicCallNode* incoming)</span>
1984 {
1985     noticeIncomingCall(callerFrame);
1986     {
1987         ConcurrentJSLocker locker(m_lock);
1988         ensureJITData(locker).m_incomingPolymorphicCalls.push(incoming);
1989     }
1990 }
1991 #endif // ENABLE(JIT)
1992 
1993 void CodeBlock::unlinkIncomingCalls()
1994 {
1995     while (m_incomingLLIntCalls.begin() != m_incomingLLIntCalls.end())
1996         m_incomingLLIntCalls.begin()-&gt;unlink();
1997 #if ENABLE(JIT)
1998     JITData* jitData = nullptr;
1999     {
2000         ConcurrentJSLocker locker(m_lock);
2001         jitData = m_jitData.get();
2002     }
2003     if (jitData) {
2004         while (jitData-&gt;m_incomingCalls.begin() != jitData-&gt;m_incomingCalls.end())
2005             jitData-&gt;m_incomingCalls.begin()-&gt;unlink(vm());
2006         while (jitData-&gt;m_incomingPolymorphicCalls.begin() != jitData-&gt;m_incomingPolymorphicCalls.end())
2007             jitData-&gt;m_incomingPolymorphicCalls.begin()-&gt;unlink(vm());
2008     }
2009 #endif // ENABLE(JIT)
2010 }
2011 
<span class="line-modified">2012 void CodeBlock::linkIncomingCall(CallFrame* callerFrame, LLIntCallLinkInfo* incoming)</span>
2013 {
2014     noticeIncomingCall(callerFrame);
2015     m_incomingLLIntCalls.push(incoming);
2016 }
2017 
2018 CodeBlock* CodeBlock::newReplacement()
2019 {
2020     return ownerExecutable()-&gt;newReplacementCodeBlockFor(specializationKind());
2021 }
2022 
2023 #if ENABLE(JIT)
2024 CodeBlock* CodeBlock::replacement()
2025 {
2026     const ClassInfo* classInfo = this-&gt;classInfo(vm());
2027 
2028     if (classInfo == FunctionCodeBlock::info())
2029         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;codeBlockFor(isConstructor() ? CodeForConstruct : CodeForCall);
2030 
2031     if (classInfo == EvalCodeBlock::info())
2032         return jsCast&lt;EvalExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
</pre>
<hr />
<pre>
2217                 return StackVisitor::Done;
2218             }
2219 
2220             if (!m_depthToCheck--)
2221                 return StackVisitor::Done;
2222         }
2223 
2224         return StackVisitor::Continue;
2225     }
2226 
2227     bool didRecurse() const { return m_didRecurse; }
2228 
2229 private:
2230     CallFrame* m_startCallFrame;
2231     CodeBlock* m_codeBlock;
2232     mutable unsigned m_depthToCheck;
2233     mutable bool m_foundStartCallFrame;
2234     mutable bool m_didRecurse;
2235 };
2236 
<span class="line-modified">2237 void CodeBlock::noticeIncomingCall(CallFrame* callerFrame)</span>
2238 {
2239     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
2240 
<span class="line-modified">2241     dataLogLnIf(Options::verboseCallLink(), &quot;Noticing call link from &quot;, pointerDump(callerCodeBlock), &quot; to &quot;, *this);</span>

2242 
2243 #if ENABLE(DFG_JIT)
2244     if (!m_shouldAlwaysBeInlined)
2245         return;
2246 
2247     if (!callerCodeBlock) {
2248         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2249         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is native.&quot;);</span>

2250         return;
2251     }
2252 
2253     if (!hasBaselineJITProfiling())
2254         return;
2255 
2256     if (!DFG::mightInlineFunction(this))
2257         return;
2258 
2259     if (!canInline(capabilityLevelState()))
2260         return;
2261 
2262     if (!DFG::isSmallEnoughToInlineCodeInto(callerCodeBlock)) {
2263         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2264         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is too large.&quot;);</span>

2265         return;
2266     }
2267 
2268     if (callerCodeBlock-&gt;jitType() == JITType::InterpreterThunk) {
2269         // If the caller is still in the interpreter, then we can&#39;t expect inlining to
2270         // happen anytime soon. Assume it&#39;s profitable to optimize it separately. This
2271         // ensures that a function is SABI only if it is called no more frequently than
2272         // any of its callers.
2273         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2274         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is in LLInt.&quot;);</span>

2275         return;
2276     }
2277 
2278     if (JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType())) {
2279         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2280         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI bcause caller was already optimized.&quot;);</span>

2281         return;
2282     }
2283 
2284     if (callerCodeBlock-&gt;codeType() != FunctionCode) {
2285         // If the caller is either eval or global code, assume that that won&#39;t be
2286         // optimized anytime soon. For eval code this is particularly true since we
2287         // delay eval optimization by a *lot*.
2288         m_shouldAlwaysBeInlined = false;
<span class="line-modified">2289         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is not a function.&quot;);</span>

2290         return;
2291     }
2292 
2293     // Recursive calls won&#39;t be inlined.
2294     RecursionCheckFunctor functor(callerFrame, this, Options::maximumInliningDepth());
<span class="line-modified">2295     vm().topCallFrame-&gt;iterate(vm(), functor);</span>
2296 
2297     if (functor.didRecurse()) {
<span class="line-modified">2298         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because recursion was detected.&quot;);</span>

2299         m_shouldAlwaysBeInlined = false;
2300         return;
2301     }
2302 
2303     if (callerCodeBlock-&gt;capabilityLevelState() == DFG::CapabilityLevelNotSet) {
2304         dataLog(&quot;In call from &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot; to &quot;, *this, &quot;: caller&#39;s DFG capability level is not set.\n&quot;);
2305         CRASH();
2306     }
2307 
2308     if (canCompile(callerCodeBlock-&gt;capabilityLevelState()))
2309         return;
2310 
<span class="line-modified">2311     dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because the caller is not a DFG candidate.&quot;);</span>

2312 
2313     m_shouldAlwaysBeInlined = false;
2314 #endif
2315 }
2316 
2317 unsigned CodeBlock::reoptimizationRetryCounter() const
2318 {
2319 #if ENABLE(JIT)
2320     ASSERT(m_reoptimizationRetryCounter &lt;= Options::reoptimizationRetryCounterMax());
2321     return m_reoptimizationRetryCounter;
2322 #else
2323     return 0;
2324 #endif // ENABLE(JIT)
2325 }
2326 
2327 #if !ENABLE(C_LOOP)
2328 const RegisterAtOffsetList* CodeBlock::calleeSaveRegisters() const
2329 {
2330 #if ENABLE(JIT)
2331     if (auto* jitData = m_jitData.get()) {
</pre>
<hr />
<pre>
2439     // that the fit didn&#39;t end up choosing a negative value of c (which would result in
2440     // the function turning over and going negative for large x) and I threw in a Sqrt
2441     // term because Sqrt represents my intution that the function should be more sensitive
2442     // to small changes in small values of x, but less sensitive when x gets large.
2443 
2444     // Note that the current fit essentially eliminates the linear portion of the
2445     // expression (c == 0.0).
2446     const double a = 0.061504;
2447     const double b = 1.02406;
2448     const double c = 0.0;
2449     const double d = 0.825914;
2450 
2451     double bytecodeCost = this-&gt;bytecodeCost();
2452 
2453     ASSERT(bytecodeCost); // Make sure this is called only after we have an instruction stream; otherwise it&#39;ll just return the value of d, which makes no sense.
2454 
2455     double result = d + a * sqrt(bytecodeCost + b) + c * bytecodeCost;
2456 
2457     result *= codeTypeThresholdMultiplier();
2458 
<span class="line-modified">2459     dataLogLnIf(Options::verboseOSR(),</span>
<span class="line-modified">2460         *this, &quot;: bytecode cost is &quot;, bytecodeCost,</span>
<span class="line-modified">2461         &quot;, scaling execution counter by &quot;, result, &quot; * &quot;, codeTypeThresholdMultiplier());</span>



2462     return result;
2463 }
2464 
2465 static int32_t clipThreshold(double threshold)
2466 {
2467     if (threshold &lt; 1.0)
2468         return 1;
2469 
2470     if (threshold &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::max()))
2471         return std::numeric_limits&lt;int32_t&gt;::max();
2472 
2473     return static_cast&lt;int32_t&gt;(threshold);
2474 }
2475 
2476 int32_t CodeBlock::adjustedCounterValue(int32_t desiredThreshold)
2477 {
2478     return clipThreshold(
2479         static_cast&lt;double&gt;(desiredThreshold) *
2480         optimizationThresholdScalingFactor() *
2481         (1 &lt;&lt; reoptimizationRetryCounter()));
</pre>
<hr />
<pre>
2530             didTryToEnterInLoop = true;
2531             break;
2532         }
2533     }
2534 
2535     uint32_t exitCountThreshold = didTryToEnterInLoop
2536         ? exitCountThresholdForReoptimizationFromLoop()
2537         : exitCountThresholdForReoptimization();
2538 
2539     if (m_osrExitCounter &gt; exitCountThreshold)
2540         return OptimizeAction::ReoptimizeNow;
2541 
2542     // Too few fails. Adjust the execution counter such that the target is to only optimize after a while.
2543     baselineCodeBlock-&gt;m_jitExecuteCounter.setNewThresholdForOSRExit(exitState.activeThreshold, exitState.memoryUsageAdjustedThreshold);
2544     return OptimizeAction::None;
2545 }
2546 #endif
2547 
2548 void CodeBlock::optimizeNextInvocation()
2549 {
<span class="line-modified">2550     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing next invocation.&quot;);</span>

2551     m_jitExecuteCounter.setNewThreshold(0, this);
2552 }
2553 
2554 void CodeBlock::dontOptimizeAnytimeSoon()
2555 {
<span class="line-modified">2556     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Not optimizing anytime soon.&quot;);</span>

2557     m_jitExecuteCounter.deferIndefinitely();
2558 }
2559 
2560 void CodeBlock::optimizeAfterWarmUp()
2561 {
<span class="line-modified">2562     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing after warm-up.&quot;);</span>

2563 #if ENABLE(DFG_JIT)
2564     m_jitExecuteCounter.setNewThreshold(
2565         adjustedCounterValue(Options::thresholdForOptimizeAfterWarmUp()), this);
2566 #endif
2567 }
2568 
2569 void CodeBlock::optimizeAfterLongWarmUp()
2570 {
<span class="line-modified">2571     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing after long warm-up.&quot;);</span>

2572 #if ENABLE(DFG_JIT)
2573     m_jitExecuteCounter.setNewThreshold(
2574         adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp()), this);
2575 #endif
2576 }
2577 
2578 void CodeBlock::optimizeSoon()
2579 {
<span class="line-modified">2580     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing soon.&quot;);</span>

2581 #if ENABLE(DFG_JIT)
2582     m_jitExecuteCounter.setNewThreshold(
2583         adjustedCounterValue(Options::thresholdForOptimizeSoon()), this);
2584 #endif
2585 }
2586 
2587 void CodeBlock::forceOptimizationSlowPathConcurrently()
2588 {
<span class="line-modified">2589     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Forcing slow path concurrently.&quot;);</span>

2590     m_jitExecuteCounter.forceSlowPathConcurrently();
2591 }
2592 
2593 #if ENABLE(DFG_JIT)
2594 void CodeBlock::setOptimizationThresholdBasedOnCompilationResult(CompilationResult result)
2595 {
2596     JITType type = jitType();
2597     if (type != JITType::BaselineJIT) {
<span class="line-modified">2598         dataLogLn(*this, &quot;: expected to have baseline code but have &quot;, type);</span>
2599         CRASH_WITH_INFO(bitwise_cast&lt;uintptr_t&gt;(jitCode().get()), static_cast&lt;uint8_t&gt;(type));
2600     }
2601 
2602     CodeBlock* replacement = this-&gt;replacement();
2603     bool hasReplacement = (replacement &amp;&amp; replacement != this);
2604     if ((result == CompilationSuccessful) != hasReplacement) {
2605         dataLog(*this, &quot;: we have result = &quot;, result, &quot; but &quot;);
2606         if (replacement == this)
2607             dataLog(&quot;we are our own replacement.\n&quot;);
2608         else
2609             dataLog(&quot;our replacement is &quot;, pointerDump(replacement), &quot;\n&quot;);
2610         RELEASE_ASSERT_NOT_REACHED();
2611     }
2612 
2613     switch (result) {
2614     case CompilationSuccessful:
2615         RELEASE_ASSERT(replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType()));
2616         optimizeNextInvocation();
2617         return;
2618     case CompilationFailed:
</pre>
<hr />
<pre>
2658 {
2659     return adjustedExitCountThreshold(Options::osrExitCountForReoptimization() * codeTypeThresholdMultiplier());
2660 }
2661 
2662 uint32_t CodeBlock::exitCountThresholdForReoptimizationFromLoop()
2663 {
2664     return adjustedExitCountThreshold(Options::osrExitCountForReoptimizationFromLoop() * codeTypeThresholdMultiplier());
2665 }
2666 
2667 bool CodeBlock::shouldReoptimizeNow()
2668 {
2669     return osrExitCounter() &gt;= exitCountThresholdForReoptimization();
2670 }
2671 
2672 bool CodeBlock::shouldReoptimizeFromLoopNow()
2673 {
2674     return osrExitCounter() &gt;= exitCountThresholdForReoptimizationFromLoop();
2675 }
2676 #endif
2677 
<span class="line-modified">2678 ArrayProfile* CodeBlock::getArrayProfile(const ConcurrentJSLocker&amp;, BytecodeIndex bytecodeIndex)</span>
2679 {
<span class="line-modified">2680     auto instruction = instructions().at(bytecodeIndex);</span>
2681     switch (instruction-&gt;opcodeID()) {
2682 #define CASE1(Op) \
2683     case Op::opcodeID: \
2684         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_arrayProfile;
2685 
2686 #define CASE2(Op) \
2687     case Op::opcodeID: \
2688         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_callLinkInfo.m_arrayProfile;
2689 
2690     FOR_EACH_OPCODE_WITH_ARRAY_PROFILE(CASE1)
2691     FOR_EACH_OPCODE_WITH_LLINT_CALL_LINK_INFO(CASE2)
2692 
2693 #undef CASE1
2694 #undef CASE2
2695 
2696     case OpGetById::opcodeID: {
2697         auto bytecode = instruction-&gt;as&lt;OpGetById&gt;();
2698         auto&amp; metadata = bytecode.metadata(this);
2699         if (metadata.m_modeMetadata.mode == GetByIdMode::ArrayLength)
2700             return &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile;
2701         break;
2702     }
2703     default:
2704         break;
2705     }
2706 
2707     return nullptr;
2708 }
2709 
<span class="line-modified">2710 ArrayProfile* CodeBlock::getArrayProfile(BytecodeIndex bytecodeIndex)</span>
2711 {
2712     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">2713     return getArrayProfile(locker, bytecodeIndex);</span>
2714 }
2715 
2716 #if ENABLE(DFG_JIT)
2717 Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; CodeBlock::codeOrigins()
2718 {
2719     return m_jitCode-&gt;dfgCommon()-&gt;codeOrigins;
2720 }
2721 
2722 size_t CodeBlock::numberOfDFGIdentifiers() const
2723 {
2724     if (!JITCode::isOptimizingJIT(jitType()))
2725         return 0;
2726 
2727     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers.size();
2728 }
2729 
2730 const Identifier&amp; CodeBlock::identifier(int index) const
2731 {
2732     size_t unlinkedIdentifiers = m_unlinkedCode-&gt;numberOfIdentifiers();
2733     if (static_cast&lt;unsigned&gt;(index) &lt; unlinkedIdentifiers)
</pre>
<hr />
<pre>
2744     numberOfLiveNonArgumentValueProfiles = 0;
2745     numberOfSamplesInProfiles = 0; // If this divided by ValueProfile::numberOfBuckets equals numberOfValueProfiles() then value profiles are full.
2746 
2747     forEachValueProfile([&amp;](ValueProfile&amp; profile, bool isArgument) {
2748         unsigned numSamples = profile.totalNumberOfSamples();
2749         static_assert(ValueProfile::numberOfBuckets == 1);
2750         if (numSamples &gt; ValueProfile::numberOfBuckets)
2751             numSamples = ValueProfile::numberOfBuckets; // We don&#39;t want profiles that are extremely hot to be given more weight.
2752         numberOfSamplesInProfiles += numSamples;
2753         if (isArgument) {
2754             profile.computeUpdatedPrediction(locker);
2755             return;
2756         }
2757         if (profile.numberOfSamples() || profile.isSampledBefore())
2758             numberOfLiveNonArgumentValueProfiles++;
2759         profile.computeUpdatedPrediction(locker);
2760     });
2761 
2762     if (auto* rareData = m_rareData.get()) {
2763         for (auto&amp; profileBucket : rareData-&gt;m_catchProfiles) {
<span class="line-modified">2764             profileBucket-&gt;forEach([&amp;] (ValueProfileAndVirtualRegister&amp; profile) {</span>
2765                 profile.computeUpdatedPrediction(locker);
2766             });
2767         }
2768     }
2769 
2770 #if ENABLE(DFG_JIT)
2771     lazyOperandValueProfiles(locker).computeUpdatedPredictions(locker);
2772 #endif
2773 }
2774 
2775 void CodeBlock::updateAllValueProfilePredictions()
2776 {
2777     unsigned ignoredValue1, ignoredValue2;
2778     updateAllValueProfilePredictionsAndCountLiveness(ignoredValue1, ignoredValue2);
2779 }
2780 
2781 void CodeBlock::updateAllArrayPredictions()
2782 {
2783     ConcurrentJSLocker locker(m_lock);
2784 
2785     forEachArrayProfile([&amp;](ArrayProfile&amp; profile) {
2786         profile.computeUpdatedPrediction(locker, this);
2787     });
2788 
2789     forEachArrayAllocationProfile([&amp;](ArrayAllocationProfile&amp; profile) {
2790         profile.updateProfile();
2791     });
2792 }
2793 
2794 void CodeBlock::updateAllPredictions()
2795 {
2796     updateAllValueProfilePredictions();
2797     updateAllArrayPredictions();
2798 }
2799 
2800 bool CodeBlock::shouldOptimizeNow()
2801 {
<span class="line-modified">2802     dataLogLnIf(Options::verboseOSR(), &quot;Considering optimizing &quot;, *this, &quot;...&quot;);</span>

2803 
2804     if (m_optimizationDelayCounter &gt;= Options::maximumOptimizationDelay())
2805         return true;
2806 
2807     updateAllArrayPredictions();
2808 
2809     unsigned numberOfLiveNonArgumentValueProfiles;
2810     unsigned numberOfSamplesInProfiles;
2811     updateAllValueProfilePredictionsAndCountLiveness(numberOfLiveNonArgumentValueProfiles, numberOfSamplesInProfiles);
2812 
2813     if (Options::verboseOSR()) {
2814         dataLogF(
2815             &quot;Profile hotness: %lf (%u / %u), %lf (%u / %u)\n&quot;,
2816             (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles(),
2817             numberOfLiveNonArgumentValueProfiles, numberOfNonArgumentValueProfiles(),
2818             (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / numberOfNonArgumentValueProfiles(),
2819             numberOfSamplesInProfiles, ValueProfile::numberOfBuckets * numberOfNonArgumentValueProfiles());
2820     }
2821 
2822     if ((!numberOfNonArgumentValueProfiles() || (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles() &gt;= Options::desiredProfileLivenessRate())
2823         &amp;&amp; (!totalNumberOfValueProfiles() || (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / totalNumberOfValueProfiles() &gt;= Options::desiredProfileFullnessRate())
2824         &amp;&amp; static_cast&lt;unsigned&gt;(m_optimizationDelayCounter) + 1 &gt;= Options::minimumOptimizationDelay())
2825         return true;
2826 
2827     ASSERT(m_optimizationDelayCounter &lt; std::numeric_limits&lt;uint8_t&gt;::max());
2828     m_optimizationDelayCounter++;
2829     optimizeAfterWarmUp();
2830     return false;
2831 }
2832 
2833 #if ENABLE(DFG_JIT)
2834 void CodeBlock::tallyFrequentExitSites()
2835 {
2836     ASSERT(JITCode::isOptimizingJIT(jitType()));
<span class="line-modified">2837     ASSERT(JITCode::isBaselineCode(alternative()-&gt;jitType()));</span>
2838 
2839     CodeBlock* profiledBlock = alternative();
2840 
2841     switch (jitType()) {
2842     case JITType::DFGJIT: {
2843         DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
2844         for (auto&amp; exit : jitCode-&gt;osrExit)
2845             exit.considerAddingAsFrequentExitSite(profiledBlock);
2846         break;
2847     }
2848 
2849 #if ENABLE(FTL_JIT)
2850     case JITType::FTLJIT: {
2851         // There is no easy way to avoid duplicating this code since the FTL::JITCode::osrExit
2852         // vector contains a totally different type, that just so happens to behave like
2853         // DFG::JITCode::osrExit.
2854         FTL::JITCode* jitCode = m_jitCode-&gt;ftl();
2855         for (unsigned i = 0; i &lt; jitCode-&gt;osrExit.size(); ++i) {
2856             FTL::OSRExit&amp; exit = jitCode-&gt;osrExit[i];
2857             exit.considerAddingAsFrequentExitSite(profiledBlock);
</pre>
<hr />
<pre>
3000         if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm(), constantRegister.get())) {
3001             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
3002             auto end = symbolTable-&gt;end(locker);
3003             for (auto ptr = symbolTable-&gt;begin(locker); ptr != end; ++ptr) {
3004                 if (ptr-&gt;value.varOffset() == VarOffset(virtualRegister)) {
3005                     // FIXME: This won&#39;t work from the compilation thread.
3006                     // https://bugs.webkit.org/show_bug.cgi?id=115300
3007                     return ptr-&gt;key.get();
3008                 }
3009             }
3010         }
3011     }
3012     if (virtualRegister == thisRegister())
3013         return &quot;this&quot;_s;
3014     if (virtualRegister.isArgument())
3015         return makeString(&quot;arguments[&quot;, pad(&#39; &#39;, 3, virtualRegister.toArgument()), &#39;]&#39;);
3016 
3017     return emptyString();
3018 }
3019 
<span class="line-modified">3020 ValueProfile* CodeBlock::tryGetValueProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
3021 {
<span class="line-modified">3022     auto instruction = instructions().at(bytecodeIndex);</span>
3023     switch (instruction-&gt;opcodeID()) {
3024 
3025 #define CASE(Op) \
3026     case Op::opcodeID: \
3027         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_profile;
3028 
3029         FOR_EACH_OPCODE_WITH_VALUE_PROFILE(CASE)
3030 
3031 #undef CASE
3032 
3033     default:
3034         return nullptr;
3035 
3036     }
3037 }
3038 
<span class="line-modified">3039 SpeculatedType CodeBlock::valueProfilePredictionForBytecodeIndex(const ConcurrentJSLocker&amp; locker, BytecodeIndex bytecodeIndex)</span>
3040 {
<span class="line-modified">3041     if (ValueProfile* valueProfile = tryGetValueProfileForBytecodeIndex(bytecodeIndex))</span>
3042         return valueProfile-&gt;computeUpdatedPrediction(locker);
3043     return SpecNone;
3044 }
3045 
<span class="line-modified">3046 ValueProfile&amp; CodeBlock::valueProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
3047 {
<span class="line-modified">3048     return *tryGetValueProfileForBytecodeIndex(bytecodeIndex);</span>
3049 }
3050 
3051 void CodeBlock::validate()
3052 {
3053     BytecodeLivenessAnalysis liveness(this); // Compute directly from scratch so it doesn&#39;t effect CodeBlock footprint.
3054 
<span class="line-modified">3055     FastBitVector liveAtHead = liveness.getLivenessInfoAtBytecodeIndex(this, BytecodeIndex(0));</span>
3056 
3057     if (liveAtHead.numBits() != static_cast&lt;size_t&gt;(m_numCalleeLocals)) {
3058         beginValidationDidFail();
3059         dataLog(&quot;    Wrong number of bits in result!\n&quot;);
3060         dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3061         dataLog(&quot;    Bit count: &quot;, liveAtHead.numBits(), &quot;\n&quot;);
3062         endValidationDidFail();
3063     }
3064 
3065     for (unsigned i = m_numCalleeLocals; i--;) {
3066         VirtualRegister reg = virtualRegisterForLocal(i);
3067 
3068         if (liveAtHead[i]) {
3069             beginValidationDidFail();
3070             dataLog(&quot;    Variable &quot;, reg, &quot; is expected to be dead.\n&quot;);
3071             dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3072             endValidationDidFail();
3073         }
3074     }
3075 
3076     const InstructionStream&amp; instructionStream = instructions();
3077     for (const auto&amp; instruction : instructionStream) {
3078         OpcodeID opcode = instruction-&gt;opcodeID();
<span class="line-modified">3079         if (!!baselineAlternative()-&gt;handlerForBytecodeIndex(BytecodeIndex(instruction.offset()))) {</span>
3080             if (opcode == op_catch || opcode == op_enter) {
3081                 // op_catch/op_enter logically represent an entrypoint. Entrypoints are not allowed to be
3082                 // inside of a try block because they are responsible for bootstrapping state. And they
3083                 // are never allowed throw an exception because of this. We rely on this when compiling
3084                 // in the DFG. Because an entrypoint never throws, the bytecode generator will never
3085                 // allow once inside a try block.
3086                 beginValidationDidFail();
3087                 dataLog(&quot;    entrypoint not allowed inside a try block.&quot;);
3088                 endValidationDidFail();
3089             }
3090         }
3091     }
3092 }
3093 
3094 void CodeBlock::beginValidationDidFail()
3095 {
3096     dataLog(&quot;Validation failure in &quot;, *this, &quot;:\n&quot;);
3097     dataLog(&quot;\n&quot;);
3098 }
3099 
</pre>
<hr />
<pre>
3117 void CodeBlock::setSteppingMode(CodeBlock::SteppingMode mode)
3118 {
3119     m_steppingMode = mode;
3120     if (mode == SteppingModeEnabled &amp;&amp; JITCode::isOptimizingJIT(jitType()))
3121         jettison(Profiler::JettisonDueToDebuggerStepping);
3122 }
3123 
3124 int CodeBlock::outOfLineJumpOffset(const Instruction* pc)
3125 {
3126     int offset = bytecodeOffset(pc);
3127     return m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3128 }
3129 
3130 const Instruction* CodeBlock::outOfLineJumpTarget(const Instruction* pc)
3131 {
3132     int offset = bytecodeOffset(pc);
3133     int target = m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3134     return instructions().at(offset + target).ptr();
3135 }
3136 
<span class="line-modified">3137 BinaryArithProfile* CodeBlock::binaryArithProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
<span class="line-added">3138 {</span>
<span class="line-added">3139     return binaryArithProfileForPC(instructions().at(bytecodeIndex.offset()).ptr());</span>
<span class="line-added">3140 }</span>
<span class="line-added">3141 </span>
<span class="line-added">3142 UnaryArithProfile* CodeBlock::unaryArithProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
3143 {
<span class="line-modified">3144     return unaryArithProfileForPC(instructions().at(bytecodeIndex.offset()).ptr());</span>
3145 }
3146 
<span class="line-modified">3147 BinaryArithProfile* CodeBlock::binaryArithProfileForPC(const Instruction* pc)</span>
3148 {
3149     switch (pc-&gt;opcodeID()) {


3150     case op_add:
3151         return &amp;pc-&gt;as&lt;OpAdd&gt;().metadata(this).m_arithProfile;
3152     case op_mul:
3153         return &amp;pc-&gt;as&lt;OpMul&gt;().metadata(this).m_arithProfile;
3154     case op_sub:
3155         return &amp;pc-&gt;as&lt;OpSub&gt;().metadata(this).m_arithProfile;
3156     case op_div:
3157         return &amp;pc-&gt;as&lt;OpDiv&gt;().metadata(this).m_arithProfile;
3158     default:
3159         break;
3160     }
3161 
3162     return nullptr;
3163 }
3164 
<span class="line-modified">3165 UnaryArithProfile* CodeBlock::unaryArithProfileForPC(const Instruction* pc)</span>
<span class="line-added">3166 {</span>
<span class="line-added">3167     switch (pc-&gt;opcodeID()) {</span>
<span class="line-added">3168     case op_negate:</span>
<span class="line-added">3169         return &amp;pc-&gt;as&lt;OpNegate&gt;().metadata(this).m_arithProfile;</span>
<span class="line-added">3170     case op_inc:</span>
<span class="line-added">3171         return &amp;pc-&gt;as&lt;OpInc&gt;().metadata(this).m_arithProfile;</span>
<span class="line-added">3172     case op_dec:</span>
<span class="line-added">3173         return &amp;pc-&gt;as&lt;OpDec&gt;().metadata(this).m_arithProfile;</span>
<span class="line-added">3174     default:</span>
<span class="line-added">3175         break;</span>
<span class="line-added">3176     }</span>
<span class="line-added">3177 </span>
<span class="line-added">3178     return nullptr;</span>
<span class="line-added">3179 }</span>
<span class="line-added">3180 </span>
<span class="line-added">3181 bool CodeBlock::couldTakeSpecialArithFastCase(BytecodeIndex bytecodeIndex)</span>
3182 {
3183     if (!hasBaselineJITProfiling())
3184         return false;
<span class="line-modified">3185     BinaryArithProfile* profile = binaryArithProfileForBytecodeIndex(bytecodeIndex);</span>
3186     if (!profile)
3187         return false;
3188     return profile-&gt;tookSpecialFastPath();
3189 }
3190 
3191 #if ENABLE(JIT)
3192 DFG::CapabilityLevel CodeBlock::capabilityLevel()
3193 {
3194     DFG::CapabilityLevel result = computeCapabilityLevel();
3195     m_capabilityLevelState = result;
3196     return result;
3197 }
3198 #endif
3199 
3200 void CodeBlock::insertBasicBlockBoundariesForControlFlowProfiler()
3201 {
3202     if (!unlinkedCodeBlock()-&gt;hasOpProfileControlFlowBytecodeOffsets())
3203         return;
<span class="line-modified">3204     const RefCountedArray&lt;InstructionStream::Offset&gt;&amp; bytecodeOffsets = unlinkedCodeBlock()-&gt;opProfileControlFlowBytecodeOffsets();</span>
3205     for (size_t i = 0, offsetsLength = bytecodeOffsets.size(); i &lt; offsetsLength; i++) {
3206         // Because op_profile_control_flow is emitted at the beginning of every basic block, finding
3207         // the next op_profile_control_flow will give us the text range of a single basic block.
3208         size_t startIdx = bytecodeOffsets[i];
3209         auto instruction = instructions().at(startIdx);
3210         RELEASE_ASSERT(instruction-&gt;opcodeID() == op_profile_control_flow);
3211         auto bytecode = instruction-&gt;as&lt;OpProfileControlFlow&gt;();
3212         auto&amp; metadata = bytecode.metadata(this);
3213         int basicBlockStartOffset = bytecode.m_textOffset;
3214         int basicBlockEndOffset;
3215         if (i + 1 &lt; offsetsLength) {
3216             size_t endIdx = bytecodeOffsets[i + 1];
3217             auto endInstruction = instructions().at(endIdx);
3218             RELEASE_ASSERT(endInstruction-&gt;opcodeID() == op_profile_control_flow);
3219             basicBlockEndOffset = endInstruction-&gt;as&lt;OpProfileControlFlow&gt;().m_textOffset - 1;
3220         } else {
3221             basicBlockEndOffset = sourceOffset() + ownerExecutable()-&gt;source().length() - 1; // Offset before the closing brace.
3222             basicBlockStartOffset = std::min(basicBlockStartOffset, basicBlockEndOffset); // Some start offsets may be at the closing brace, ensure it is the offset before.
3223         }
3224 
</pre>
<hr />
<pre>
3284         if (auto* jitData = m_jitData.get()) {
3285             if (jitData-&gt;m_pcToCodeOriginMap) {
3286                 if (Optional&lt;CodeOrigin&gt; codeOrigin = jitData-&gt;m_pcToCodeOriginMap-&gt;findPC(pc))
3287                     return codeOrigin;
3288             }
3289 
3290             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
3291                 if (stubInfo-&gt;containsPC(pc))
3292                     return Optional&lt;CodeOrigin&gt;(stubInfo-&gt;codeOrigin);
3293             }
3294         }
3295     }
3296 
3297     if (Optional&lt;CodeOrigin&gt; codeOrigin = m_jitCode-&gt;findPC(this, pc))
3298         return codeOrigin;
3299 
3300     return WTF::nullopt;
3301 }
3302 #endif // ENABLE(JIT)
3303 
<span class="line-modified">3304 Optional&lt;BytecodeIndex&gt; CodeBlock::bytecodeIndexFromCallSiteIndex(CallSiteIndex callSiteIndex)</span>
3305 {
<span class="line-modified">3306     Optional&lt;BytecodeIndex&gt; bytecodeIndex;</span>
3307     JITType jitType = this-&gt;jitType();
<span class="line-modified">3308     if (jitType == JITType::InterpreterThunk || jitType == JITType::BaselineJIT)</span>
<span class="line-modified">3309         bytecodeIndex = callSiteIndex.bytecodeIndex();</span>
<span class="line-modified">3310     else if (jitType == JITType::DFGJIT || jitType == JITType::FTLJIT) {</span>





3311 #if ENABLE(DFG_JIT)
3312         RELEASE_ASSERT(canGetCodeOrigin(callSiteIndex));
3313         CodeOrigin origin = codeOrigin(callSiteIndex);
<span class="line-modified">3314         bytecodeIndex = origin.bytecodeIndex();</span>
3315 #else
3316         RELEASE_ASSERT_NOT_REACHED();
3317 #endif
3318     }
3319 
<span class="line-modified">3320     return bytecodeIndex;</span>
3321 }
3322 
3323 int32_t CodeBlock::thresholdForJIT(int32_t threshold)
3324 {
3325     switch (unlinkedCodeBlock()-&gt;didOptimize()) {
3326     case MixedTriState:
3327         return threshold;
3328     case FalseTriState:
3329         return threshold * 4;
3330     case TrueTriState:
3331         return threshold / 2;
3332     }
3333     ASSERT_NOT_REACHED();
3334     return threshold;
3335 }
3336 
3337 void CodeBlock::jitAfterWarmUp()
3338 {
3339     m_llintExecuteCounter.setNewThreshold(thresholdForJIT(Options::thresholdForJITAfterWarmUp()), this);
3340 }
</pre>
</td>
</tr>
</table>
<center><a href="CallVariant.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlock.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>