<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExitCompilerCommon.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGOSRExitBase.h.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExitCompilerCommon.h.cdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExitCompilerCommon.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 26,20 ***</span>
  #include &quot;config.h&quot;
  #include &quot;DFGOSRExitCompilerCommon.h&quot;
  
  #if ENABLE(DFG_JIT)
  
  #include &quot;DFGJITCode.h&quot;
  #include &quot;DFGOperations.h&quot;
  #include &quot;JIT.h&quot;
  #include &quot;JSCJSValueInlines.h&quot;
  #include &quot;JSCInlines.h&quot;
  #include &quot;StructureStubInfo.h&quot;
  
  namespace JSC { namespace DFG {
  
<span class="line-modified">! void handleExitCounts(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)</span>
  {
      if (!exitKindMayJettison(exit.m_kind)) {
          // FIXME: We may want to notice that we&#39;re frequently exiting
          // at an op_catch that we didn&#39;t compile an entrypoint for, and
          // then trigger a reoptimization of this CodeBlock:
<span class="line-new-header">--- 26,25 ---</span>
  #include &quot;config.h&quot;
  #include &quot;DFGOSRExitCompilerCommon.h&quot;
  
  #if ENABLE(DFG_JIT)
  
<span class="line-added">+ #include &quot;Bytecodes.h&quot;</span>
<span class="line-added">+ #include &quot;CheckpointOSRExitSideState.h&quot;</span>
  #include &quot;DFGJITCode.h&quot;
  #include &quot;DFGOperations.h&quot;
  #include &quot;JIT.h&quot;
  #include &quot;JSCJSValueInlines.h&quot;
  #include &quot;JSCInlines.h&quot;
<span class="line-added">+ #include &quot;LLIntData.h&quot;</span>
<span class="line-added">+ #include &quot;LLIntThunks.h&quot;</span>
<span class="line-added">+ #include &quot;ProbeContext.h&quot;</span>
  #include &quot;StructureStubInfo.h&quot;
  
  namespace JSC { namespace DFG {
  
<span class="line-modified">! void handleExitCounts(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)</span>
  {
      if (!exitKindMayJettison(exit.m_kind)) {
          // FIXME: We may want to notice that we&#39;re frequently exiting
          // at an op_catch that we didn&#39;t compile an entrypoint for, and
          // then trigger a reoptimization of this CodeBlock:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 99,12 ***</span>
  
      tooFewFails = jit.branch32(AssemblyHelpers::BelowOrEqual, GPRInfo::regT2, GPRInfo::regT1);
  
      reoptimizeNow.link(&amp;jit);
  
<span class="line-modified">!     jit.setupArguments&lt;decltype(triggerReoptimizationNow)&gt;(GPRInfo::regT0, GPRInfo::regT3, AssemblyHelpers::TrustedImmPtr(&amp;exit));</span>
<span class="line-modified">!     jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(triggerReoptimizationNow)), GPRInfo::nonArgGPR0);</span>
      jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
      AssemblyHelpers::Jump doneAdjusting = jit.jump();
  
      tooFewFails.link(&amp;jit);
  
<span class="line-new-header">--- 104,13 ---</span>
  
      tooFewFails = jit.branch32(AssemblyHelpers::BelowOrEqual, GPRInfo::regT2, GPRInfo::regT1);
  
      reoptimizeNow.link(&amp;jit);
  
<span class="line-modified">!     jit.setupArguments&lt;decltype(operationTriggerReoptimizationNow)&gt;(GPRInfo::regT0, GPRInfo::regT3, AssemblyHelpers::TrustedImmPtr(&amp;exit));</span>
<span class="line-modified">!     jit.prepareCallOperation(vm);</span>
<span class="line-added">+     jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationTriggerReoptimizationNow)), GPRInfo::nonArgGPR0);</span>
      jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
      AssemblyHelpers::Jump doneAdjusting = jit.jump();
  
      tooFewFails.link(&amp;jit);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 134,26 ***</span>
      jit.store32(AssemblyHelpers::TrustedImm32(formattedTotalExecutionCount(clippedValue)), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionTotalCount()));
  
      doneAdjusting.link(&amp;jit);
  }
  
  void reifyInlinedCallFrames(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
  {
      // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
      // in presence of inlined tail calls.
      // https://bugs.webkit.org/show_bug.cgi?id=147511
<span class="line-modified">!     ASSERT(jit.baselineCodeBlock()-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-modified">!     jit.storePtr(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), AssemblyHelpers::addressFor((VirtualRegister)CallFrameSlot::codeBlock));</span>
  
      const CodeOrigin* codeOrigin;
      for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {
          InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();
          CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(*codeOrigin);
          InlineCallFrame::Kind trueCallerCallKind;
          CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
          GPRReg callerFrameGPR = GPRInfo::callFrameRegister;
  
          if (!trueCaller) {
              ASSERT(inlineCallFrame-&gt;isTail());
              jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
  #if CPU(ARM64E)
              jit.addPtr(AssemblyHelpers::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, GPRInfo::regT2);
<span class="line-new-header">--- 140,129 ---</span>
      jit.store32(AssemblyHelpers::TrustedImm32(formattedTotalExecutionCount(clippedValue)), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionTotalCount()));
  
      doneAdjusting.link(&amp;jit);
  }
  
<span class="line-added">+ MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; callerReturnPC(CodeBlock* baselineCodeBlockForCaller, BytecodeIndex callBytecodeIndex, InlineCallFrame::Kind trueCallerCallKind, bool&amp; callerIsLLInt)</span>
<span class="line-added">+ {</span>
<span class="line-added">+     callerIsLLInt = Options::forceOSRExitToLLInt() || baselineCodeBlockForCaller-&gt;jitType() == JITType::InterpreterThunk;</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (callBytecodeIndex.checkpoint()) {</span>
<span class="line-added">+         if (!callerIsLLInt)</span>
<span class="line-added">+             baselineCodeBlockForCaller-&gt;m_hasLinkedOSRExit = true;</span>
<span class="line-added">+         return LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_from_inlined_call_trampoline);</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jumpTarget;</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (callerIsLLInt) {</span>
<span class="line-added">+         const Instruction&amp; callInstruction = *baselineCodeBlockForCaller-&gt;instructions().at(callBytecodeIndex).ptr();</span>
<span class="line-added">+ #define LLINT_RETURN_LOCATION(name) (callInstruction.isWide16() ? LLInt::getWide16CodePtr&lt;JSEntryPtrTag&gt;(name##_return_location) : (callInstruction.isWide32() ? LLInt::getWide32CodePtr&lt;JSEntryPtrTag&gt;(name##_return_location) : LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(name##_return_location)))</span>
<span class="line-added">+ </span>
<span class="line-added">+         switch (trueCallerCallKind) {</span>
<span class="line-added">+         case InlineCallFrame::Call:</span>
<span class="line-added">+             jumpTarget = LLINT_RETURN_LOCATION(op_call);</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         case InlineCallFrame::Construct:</span>
<span class="line-added">+             jumpTarget = LLINT_RETURN_LOCATION(op_construct);</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         case InlineCallFrame::CallVarargs:</span>
<span class="line-added">+             jumpTarget = LLINT_RETURN_LOCATION(op_call_varargs_slow);</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         case InlineCallFrame::ConstructVarargs:</span>
<span class="line-added">+             jumpTarget = LLINT_RETURN_LOCATION(op_construct_varargs_slow);</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         case InlineCallFrame::GetterCall: {</span>
<span class="line-added">+             if (callInstruction.opcodeID() == op_get_by_id)</span>
<span class="line-added">+                 jumpTarget = LLINT_RETURN_LOCATION(op_get_by_id);</span>
<span class="line-added">+             else if (callInstruction.opcodeID() == op_get_by_val)</span>
<span class="line-added">+                 jumpTarget = LLINT_RETURN_LOCATION(op_get_by_val);</span>
<span class="line-added">+             else</span>
<span class="line-added">+                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         }</span>
<span class="line-added">+         case InlineCallFrame::SetterCall: {</span>
<span class="line-added">+             if (callInstruction.opcodeID() == op_put_by_id)</span>
<span class="line-added">+                 jumpTarget = LLINT_RETURN_LOCATION(op_put_by_id);</span>
<span class="line-added">+             else if (callInstruction.opcodeID() == op_put_by_val)</span>
<span class="line-added">+                 jumpTarget = LLINT_RETURN_LOCATION(op_put_by_val);</span>
<span class="line-added">+             else</span>
<span class="line-added">+                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         }</span>
<span class="line-added">+         default:</span>
<span class="line-added">+             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">+         }</span>
<span class="line-added">+ </span>
<span class="line-added">+ #undef LLINT_RETURN_LOCATION</span>
<span class="line-added">+ </span>
<span class="line-added">+     } else {</span>
<span class="line-added">+         baselineCodeBlockForCaller-&gt;m_hasLinkedOSRExit = true;</span>
<span class="line-added">+ </span>
<span class="line-added">+         switch (trueCallerCallKind) {</span>
<span class="line-added">+         case InlineCallFrame::Call:</span>
<span class="line-added">+         case InlineCallFrame::Construct:</span>
<span class="line-added">+         case InlineCallFrame::CallVarargs:</span>
<span class="line-added">+         case InlineCallFrame::ConstructVarargs: {</span>
<span class="line-added">+             CallLinkInfo* callLinkInfo =</span>
<span class="line-added">+                 baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);</span>
<span class="line-added">+             RELEASE_ASSERT(callLinkInfo);</span>
<span class="line-added">+ </span>
<span class="line-added">+             jumpTarget = callLinkInfo-&gt;callReturnLocation().retagged&lt;JSEntryPtrTag&gt;();</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         }</span>
<span class="line-added">+ </span>
<span class="line-added">+         case InlineCallFrame::GetterCall:</span>
<span class="line-added">+         case InlineCallFrame::SetterCall: {</span>
<span class="line-added">+             StructureStubInfo* stubInfo =</span>
<span class="line-added">+                 baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));</span>
<span class="line-added">+             RELEASE_ASSERT(stubInfo);</span>
<span class="line-added">+ </span>
<span class="line-added">+             jumpTarget = stubInfo-&gt;doneLocation.retagged&lt;JSEntryPtrTag&gt;();</span>
<span class="line-added">+             break;</span>
<span class="line-added">+         }</span>
<span class="line-added">+ </span>
<span class="line-added">+         default:</span>
<span class="line-added">+             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">+         }</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     return jumpTarget;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ CCallHelpers::Address calleeSaveSlot(InlineCallFrame* inlineCallFrame, CodeBlock* baselineCodeBlock, GPRReg calleeSave)</span>
<span class="line-added">+ {</span>
<span class="line-added">+     const RegisterAtOffsetList* calleeSaves = baselineCodeBlock-&gt;calleeSaveRegisters();</span>
<span class="line-added">+     for (unsigned i = 0; i &lt; calleeSaves-&gt;size(); i++) {</span>
<span class="line-added">+         RegisterAtOffset entry = calleeSaves-&gt;at(i);</span>
<span class="line-added">+         if (entry.reg() != calleeSave)</span>
<span class="line-added">+             continue;</span>
<span class="line-added">+         return CCallHelpers::Address(CCallHelpers::framePointerRegister, static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset).offsetInBytes() + entry.offset());</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">+     return CCallHelpers::Address(CCallHelpers::framePointerRegister);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  void reifyInlinedCallFrames(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
  {
      // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
      // in presence of inlined tail calls.
      // https://bugs.webkit.org/show_bug.cgi?id=147511
<span class="line-modified">!     ASSERT(JITCode::isBaselineCode(jit.baselineCodeBlock()-&gt;jitType()));</span>
<span class="line-modified">!     jit.storePtr(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), AssemblyHelpers::addressFor(CallFrameSlot::codeBlock));</span>
  
      const CodeOrigin* codeOrigin;
      for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {
          InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();
          CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(*codeOrigin);
          InlineCallFrame::Kind trueCallerCallKind;
          CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
          GPRReg callerFrameGPR = GPRInfo::callFrameRegister;
  
<span class="line-added">+         bool callerIsLLInt = false;</span>
<span class="line-added">+ </span>
          if (!trueCaller) {
              ASSERT(inlineCallFrame-&gt;isTail());
              jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
  #if CPU(ARM64E)
              jit.addPtr(AssemblyHelpers::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, GPRInfo::regT2);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 164,41 ***</span>
              jit.storePtr(GPRInfo::regT3, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
              jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), GPRInfo::regT3);
              callerFrameGPR = GPRInfo::regT3;
          } else {
              CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);
<span class="line-modified">!             unsigned callBytecodeIndex = trueCaller-&gt;bytecodeIndex();</span>
<span class="line-modified">!             void* jumpTarget = nullptr;</span>
<span class="line-removed">- </span>
<span class="line-removed">-             switch (trueCallerCallKind) {</span>
<span class="line-removed">-             case InlineCallFrame::Call:</span>
<span class="line-removed">-             case InlineCallFrame::Construct:</span>
<span class="line-removed">-             case InlineCallFrame::CallVarargs:</span>
<span class="line-removed">-             case InlineCallFrame::ConstructVarargs:</span>
<span class="line-removed">-             case InlineCallFrame::TailCall:</span>
<span class="line-removed">-             case InlineCallFrame::TailCallVarargs: {</span>
<span class="line-removed">-                 CallLinkInfo* callLinkInfo =</span>
<span class="line-removed">-                     baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);</span>
<span class="line-removed">-                 RELEASE_ASSERT(callLinkInfo);</span>
<span class="line-removed">- </span>
<span class="line-removed">-                 jumpTarget = callLinkInfo-&gt;callReturnLocation().untaggedExecutableAddress();</span>
<span class="line-removed">-                 break;</span>
<span class="line-removed">-             }</span>
<span class="line-removed">- </span>
<span class="line-removed">-             case InlineCallFrame::GetterCall:</span>
<span class="line-removed">-             case InlineCallFrame::SetterCall: {</span>
<span class="line-removed">-                 StructureStubInfo* stubInfo =</span>
<span class="line-removed">-                     baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));</span>
<span class="line-removed">-                 RELEASE_ASSERT(stubInfo);</span>
<span class="line-removed">- </span>
<span class="line-removed">-                 jumpTarget = stubInfo-&gt;doneLocation().untaggedExecutableAddress();</span>
<span class="line-removed">-                 break;</span>
<span class="line-removed">-             }</span>
<span class="line-removed">- </span>
<span class="line-removed">-             default:</span>
<span class="line-removed">-                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">-             }</span>
  
              if (trueCaller-&gt;inlineCallFrame()) {
                  jit.addPtr(
                      AssemblyHelpers::TrustedImm32(trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue)),
                      GPRInfo::callFrameRegister,
<span class="line-new-header">--- 273,12 ---</span>
              jit.storePtr(GPRInfo::regT3, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
              jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), GPRInfo::regT3);
              callerFrameGPR = GPRInfo::regT3;
          } else {
              CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);
<span class="line-modified">!             auto callBytecodeIndex = trueCaller-&gt;bytecodeIndex();</span>
<span class="line-modified">!             MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jumpTarget = callerReturnPC(baselineCodeBlockForCaller, callBytecodeIndex, trueCallerCallKind, callerIsLLInt);</span>
  
              if (trueCaller-&gt;inlineCallFrame()) {
                  jit.addPtr(
                      AssemblyHelpers::TrustedImm32(trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue)),
                      GPRInfo::callFrameRegister,
</pre>
<hr />
<pre>
<span class="line-old-header">*** 206,15 ***</span>
                  callerFrameGPR = GPRInfo::regT3;
              }
  
  #if CPU(ARM64E)
              jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
<span class="line-modified">!             jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget), GPRInfo::nonArgGPR0);</span>
              jit.tagPtr(GPRInfo::regT2, GPRInfo::nonArgGPR0);
              jit.storePtr(GPRInfo::nonArgGPR0, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
  #else
<span class="line-modified">!             jit.storePtr(AssemblyHelpers::TrustedImmPtr(jumpTarget), AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));</span>
  #endif
          }
  
          jit.storePtr(AssemblyHelpers::TrustedImmPtr(baselineCodeBlock), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock)));
  
<span class="line-new-header">--- 286,15 ---</span>
                  callerFrameGPR = GPRInfo::regT3;
              }
  
  #if CPU(ARM64E)
              jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
<span class="line-modified">!             jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget.untaggedExecutableAddress()), GPRInfo::nonArgGPR0);</span>
              jit.tagPtr(GPRInfo::regT2, GPRInfo::nonArgGPR0);
              jit.storePtr(GPRInfo::nonArgGPR0, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
  #else
<span class="line-modified">!             jit.storePtr(AssemblyHelpers::TrustedImmPtr(jumpTarget.untaggedExecutableAddress()), AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));</span>
  #endif
          }
  
          jit.storePtr(AssemblyHelpers::TrustedImmPtr(baselineCodeBlock), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock)));
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 225,69 ***</span>
              baselineCodeBlock,
              static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset),
              trueCaller ? AssemblyHelpers::UseExistingTagRegisterContents : AssemblyHelpers::CopyBaselineCalleeSavedRegistersFromBaseFrame,
              GPRInfo::regT2);
  
          if (!inlineCallFrame-&gt;isVarargs())
<span class="line-modified">!             jit.store32(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis), AssemblyHelpers::payloadFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount)));</span>
<span class="line-removed">- #if USE(JSVALUE64)</span>
          jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));
<span class="line-modified">!         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
<span class="line-modified">!         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount)));</span>
          if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">!             jit.store64(AssemblyHelpers::TrustedImm64(JSValue::encode(JSValue(inlineCallFrame-&gt;calleeConstant()))), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
  #else // USE(JSVALUE64) // so this is the 32-bit part
<span class="line-modified">!         jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));</span>
<span class="line-removed">-         const Instruction* instruction = baselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
<span class="line-removed">-         uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed">-         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount)));</span>
<span class="line-removed">-         jit.store32(AssemblyHelpers::TrustedImm32(JSValue::CellTag), AssemblyHelpers::tagFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
          if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">!             jit.storePtr(AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeConstant()), AssemblyHelpers::payloadFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
  #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
      }
  
      // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
      if (codeOrigin) {
<span class="line-modified">! #if USE(JSVALUE64)</span>
<span class="line-modified">!         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
<span class="line-removed">- #else</span>
<span class="line-removed">-         const Instruction* instruction = jit.baselineCodeBlock()-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
<span class="line-removed">-         uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">-         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor((VirtualRegister)(CallFrameSlot::argumentCount)));</span>
      }
  }
  
<span class="line-modified">! static void osrWriteBarrier(CCallHelpers&amp; jit, GPRReg owner, GPRReg scratch)</span>
  {
      AssemblyHelpers::Jump ownerIsRememberedOrInEden = jit.barrierBranchWithoutFence(owner);
  
<span class="line-modified">!     // We need these extra slots because setupArgumentsWithExecState will use poke on x86.</span>
<span class="line-modified">! #if CPU(X86)</span>
<span class="line-removed">-     jit.subPtr(MacroAssembler::TrustedImm32(sizeof(void*) * 4), MacroAssembler::stackPointerRegister);</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">- </span>
<span class="line-removed">-     jit.setupArguments&lt;decltype(operationOSRWriteBarrier)&gt;(owner);</span>
      jit.move(MacroAssembler::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationOSRWriteBarrier)), scratch);
      jit.call(scratch, OperationPtrTag);
  
<span class="line-removed">- #if CPU(X86)</span>
<span class="line-removed">-     jit.addPtr(MacroAssembler::TrustedImm32(sizeof(void*) * 4), MacroAssembler::stackPointerRegister);</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">- </span>
      ownerIsRememberedOrInEden.link(&amp;jit);
  }
  
  void adjustAndJumpToTarget(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
  {
      jit.memoryFence();
  
      jit.move(
          AssemblyHelpers::TrustedImmPtr(
              jit.codeBlock()-&gt;baselineAlternative()), GPRInfo::argumentGPR1);
<span class="line-modified">!     osrWriteBarrier(jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
  
      // We barrier all inlined frames -- and not just the current inline stack --
      // because we don&#39;t know which inlined function owns the value profile that
      // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
      // a and b are inlined, we might exit inside b due to a bad value loaded
<span class="line-new-header">--- 305,58 ---</span>
              baselineCodeBlock,
              static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset),
              trueCaller ? AssemblyHelpers::UseExistingTagRegisterContents : AssemblyHelpers::CopyBaselineCalleeSavedRegistersFromBaseFrame,
              GPRInfo::regT2);
  
<span class="line-added">+         if (callerIsLLInt) {</span>
<span class="line-added">+             CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);</span>
<span class="line-added">+             jit.storePtr(CCallHelpers::TrustedImmPtr(baselineCodeBlockForCaller-&gt;metadataTable()), calleeSaveSlot(inlineCallFrame, baselineCodeBlock, LLInt::Registers::metadataTableGPR));</span>
<span class="line-added">+             jit.storePtr(CCallHelpers::TrustedImmPtr(baselineCodeBlockForCaller-&gt;instructionsRawPointer()), calleeSaveSlot(inlineCallFrame, baselineCodeBlock, LLInt::Registers::pbGPR));</span>
<span class="line-added">+         }</span>
<span class="line-added">+ </span>
          if (!inlineCallFrame-&gt;isVarargs())
<span class="line-modified">!             jit.store32(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis), AssemblyHelpers::payloadFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis)));</span>
          jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));
<span class="line-modified">!         uint32_t locationBits = CallSiteIndex(baselineCodeBlock-&gt;bytecodeIndexForExit(codeOrigin-&gt;bytecodeIndex())).bits();</span>
<span class="line-modified">!         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis)));</span>
<span class="line-added">+ #if USE(JSVALUE64)</span>
          if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">!             jit.store64(AssemblyHelpers::TrustedImm64(JSValue::encode(JSValue(inlineCallFrame-&gt;calleeConstant()))), AssemblyHelpers::addressFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
  #else // USE(JSVALUE64) // so this is the 32-bit part
<span class="line-modified">!         jit.store32(AssemblyHelpers::TrustedImm32(JSValue::CellTag), AssemblyHelpers::tagFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
          if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">!             jit.storePtr(AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeConstant()), AssemblyHelpers::payloadFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
  #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
      }
  
      // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
      if (codeOrigin) {
<span class="line-modified">!         uint32_t locationBits = CallSiteIndex(BytecodeIndex(codeOrigin-&gt;bytecodeIndex().offset())).bits();</span>
<span class="line-modified">!         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));</span>
      }
  }
  
<span class="line-modified">! static void osrWriteBarrier(VM&amp; vm, CCallHelpers&amp; jit, GPRReg owner, GPRReg scratch)</span>
  {
      AssemblyHelpers::Jump ownerIsRememberedOrInEden = jit.barrierBranchWithoutFence(owner);
  
<span class="line-modified">!     jit.setupArguments&lt;decltype(operationOSRWriteBarrier)&gt;(&amp;vm, owner);</span>
<span class="line-modified">!     jit.prepareCallOperation(vm);</span>
      jit.move(MacroAssembler::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationOSRWriteBarrier)), scratch);
      jit.call(scratch, OperationPtrTag);
  
      ownerIsRememberedOrInEden.link(&amp;jit);
  }
  
  void adjustAndJumpToTarget(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
  {
      jit.memoryFence();
  
      jit.move(
          AssemblyHelpers::TrustedImmPtr(
              jit.codeBlock()-&gt;baselineAlternative()), GPRInfo::argumentGPR1);
<span class="line-modified">!     osrWriteBarrier(vm, jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
  
      // We barrier all inlined frames -- and not just the current inline stack --
      // because we don&#39;t know which inlined function owns the value profile that
      // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
      // a and b are inlined, we might exit inside b due to a bad value loaded
</pre>
<hr />
<pre>
<span class="line-old-header">*** 298,25 ***</span>
      if (inlineCallFrames) {
          for (InlineCallFrame* inlineCallFrame : *inlineCallFrames) {
              jit.move(
                  AssemblyHelpers::TrustedImmPtr(
                      inlineCallFrame-&gt;baselineCodeBlock.get()), GPRInfo::argumentGPR1);
<span class="line-modified">!             osrWriteBarrier(jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
          }
      }
  
      auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();
      if (exitInlineCallFrame)
          jit.addPtr(AssemblyHelpers::TrustedImm32(exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue)), GPRInfo::callFrameRegister);
  
      CodeBlock* codeBlockForExit = jit.baselineCodeBlockFor(exit.m_codeOrigin);
      ASSERT(codeBlockForExit == codeBlockForExit-&gt;baselineVersion());
<span class="line-modified">!     ASSERT(codeBlockForExit-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-modified">!     CodeLocationLabel&lt;JSEntryPtrTag&gt; codeLocation = codeBlockForExit-&gt;jitCodeMap().find(exit.m_codeOrigin.bytecodeIndex());</span>
<span class="line-modified">!     ASSERT(codeLocation);</span>
  
<span class="line-removed">-     void* jumpTarget = codeLocation.retagged&lt;OSRExitPtrTag&gt;().executableAddress();</span>
      jit.addPtr(AssemblyHelpers::TrustedImm32(JIT::stackPointerOffsetFor(codeBlockForExit) * sizeof(Register)), GPRInfo::callFrameRegister, AssemblyHelpers::stackPointerRegister);
      if (exit.isExceptionHandler()) {
          // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
          jit.storePtr(GPRInfo::callFrameRegister, vm.addressOfCallFrameForCatch());
      }
<span class="line-new-header">--- 367,59 ---</span>
      if (inlineCallFrames) {
          for (InlineCallFrame* inlineCallFrame : *inlineCallFrames) {
              jit.move(
                  AssemblyHelpers::TrustedImmPtr(
                      inlineCallFrame-&gt;baselineCodeBlock.get()), GPRInfo::argumentGPR1);
<span class="line-modified">!             osrWriteBarrier(vm, jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
          }
      }
  
      auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();
      if (exitInlineCallFrame)
          jit.addPtr(AssemblyHelpers::TrustedImm32(exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue)), GPRInfo::callFrameRegister);
  
      CodeBlock* codeBlockForExit = jit.baselineCodeBlockFor(exit.m_codeOrigin);
      ASSERT(codeBlockForExit == codeBlockForExit-&gt;baselineVersion());
<span class="line-modified">!     ASSERT(JITCode::isBaselineCode(codeBlockForExit-&gt;jitType()));</span>
<span class="line-modified">! </span>
<span class="line-modified">!     void* jumpTarget;</span>
<span class="line-added">+     bool exitToLLInt = Options::forceOSRExitToLLInt() || codeBlockForExit-&gt;jitType() == JITType::InterpreterThunk;</span>
<span class="line-added">+     if (exitToLLInt) {</span>
<span class="line-added">+         auto bytecodeIndex = exit.m_codeOrigin.bytecodeIndex();</span>
<span class="line-added">+         const Instruction&amp; currentInstruction = *codeBlockForExit-&gt;instructions().at(bytecodeIndex).ptr();</span>
<span class="line-added">+         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; destination;</span>
<span class="line-added">+         if (bytecodeIndex.checkpoint())</span>
<span class="line-added">+             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_trampoline);</span>
<span class="line-added">+         else</span>
<span class="line-added">+             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(currentInstruction);</span>
<span class="line-added">+ </span>
<span class="line-added">+         if (exit.isExceptionHandler()) {</span>
<span class="line-added">+             jit.move(CCallHelpers::TrustedImmPtr(&amp;currentInstruction), GPRInfo::regT2);</span>
<span class="line-added">+             jit.storePtr(GPRInfo::regT2, &amp;vm.targetInterpreterPCForThrow);</span>
<span class="line-added">+         }</span>
<span class="line-added">+ </span>
<span class="line-added">+         jit.move(CCallHelpers::TrustedImmPtr(codeBlockForExit-&gt;metadataTable()), LLInt::Registers::metadataTableGPR);</span>
<span class="line-added">+         jit.move(CCallHelpers::TrustedImmPtr(codeBlockForExit-&gt;instructionsRawPointer()), LLInt::Registers::pbGPR);</span>
<span class="line-added">+         jit.move(CCallHelpers::TrustedImm32(bytecodeIndex.offset()), LLInt::Registers::pcGPR);</span>
<span class="line-added">+         jumpTarget = destination.retagged&lt;OSRExitPtrTag&gt;().executableAddress();</span>
<span class="line-added">+     } else {</span>
<span class="line-added">+         codeBlockForExit-&gt;m_hasLinkedOSRExit = true;</span>
<span class="line-added">+ </span>
<span class="line-added">+         BytecodeIndex exitIndex = exit.m_codeOrigin.bytecodeIndex();</span>
<span class="line-added">+         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; destination;</span>
<span class="line-added">+         if (exitIndex.checkpoint())</span>
<span class="line-added">+             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_trampoline);</span>
<span class="line-added">+         else {</span>
<span class="line-added">+             ASSERT(codeBlockForExit-&gt;bytecodeIndexForExit(exitIndex) == exitIndex);</span>
<span class="line-added">+             destination = codeBlockForExit-&gt;jitCodeMap().find(exitIndex);</span>
<span class="line-added">+         }</span>
<span class="line-added">+ </span>
<span class="line-added">+         ASSERT(destination);</span>
<span class="line-added">+ </span>
<span class="line-added">+         jumpTarget = destination.retagged&lt;OSRExitPtrTag&gt;().executableAddress();</span>
<span class="line-added">+     }</span>
  
      jit.addPtr(AssemblyHelpers::TrustedImm32(JIT::stackPointerOffsetFor(codeBlockForExit) * sizeof(Register)), GPRInfo::callFrameRegister, AssemblyHelpers::stackPointerRegister);
      if (exit.isExceptionHandler()) {
          // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
          jit.storePtr(GPRInfo::callFrameRegister, vm.addressOfCallFrameForCatch());
      }
</pre>
<center><a href="DFGOSRExitBase.h.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExitCompilerCommon.h.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>