<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2015-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;AirAllocateRegistersByGraphColoring.h&quot;
  28 
  29 #if ENABLE(B3_JIT)
  30 
  31 #include &quot;AirCode.h&quot;
  32 #include &quot;AirFixSpillsAfterTerminals.h&quot;
  33 #include &quot;AirInsertionSet.h&quot;
  34 #include &quot;AirInstInlines.h&quot;
  35 #include &quot;AirLiveness.h&quot;
  36 #include &quot;AirPadInterference.h&quot;
  37 #include &quot;AirPhaseScope.h&quot;
  38 #include &quot;AirTmpInlines.h&quot;
  39 #include &quot;AirTmpWidth.h&quot;
  40 #include &quot;AirUseCounts.h&quot;
  41 #include &lt;wtf/ListDump.h&gt;
  42 
  43 namespace JSC { namespace B3 { namespace Air {
  44 
  45 namespace {
  46 
  47 bool debug = false;
  48 bool traceDebug = false;
  49 bool reportStats = false;
  50 
  51 // The AbstractColoringAllocator defines all the code that is independant
  52 // from the bank or register and can be shared when allocating registers.
  53 template&lt;typename IndexType, typename TmpMapper&gt;
  54 class AbstractColoringAllocator {
  55 public:
  56     AbstractColoringAllocator(Code&amp; code, const Vector&lt;Reg&gt;&amp; regsInPriorityOrder, IndexType lastPrecoloredRegisterIndex, unsigned tmpArraySize, const HashSet&lt;unsigned&gt;&amp; unspillableTmps, const UseCounts&lt;Tmp&gt;&amp; useCounts)
  57         : m_regsInPriorityOrder(regsInPriorityOrder)
  58         , m_lastPrecoloredRegisterIndex(lastPrecoloredRegisterIndex)
  59         , m_unspillableTmps(unspillableTmps)
  60         , m_useCounts(useCounts)
  61         , m_code(code)
  62     {
  63         initializeDegrees(tmpArraySize);
  64 
  65         m_adjacencyList.resize(tmpArraySize);
  66         m_moveList.resize(tmpArraySize);
  67         m_coalescedTmps.fill(0, tmpArraySize);
  68         m_isOnSelectStack.ensureSize(tmpArraySize);
  69         m_spillWorklist.ensureSize(tmpArraySize);
  70     }
  71 
  72 protected:
  73 
  74     unsigned registerCount() const { return m_regsInPriorityOrder.size(); }
  75 
  76     IndexType getAlias(IndexType tmpIndex) const
  77     {
  78         IndexType alias = tmpIndex;
  79         while (IndexType nextAlias = m_coalescedTmps[alias])
  80             alias = nextAlias;
  81         return alias;
  82     }
  83 
  84     void addEdge(IndexType a, IndexType b)
  85     {
  86         if (a == b)
  87             return;
  88         addEdgeDistinct(a, b);
  89     }
  90 
  91     void addToSpill(unsigned toSpill)
  92     {
  93         if (m_unspillableTmps.contains(toSpill))
  94             return;
  95 
  96         m_spillWorklist.add(toSpill);
  97     }
  98 
  99     bool isPrecolored(IndexType tmpIndex)
 100     {
 101         return tmpIndex &lt;= m_lastPrecoloredRegisterIndex;
 102     }
 103 
 104     void initializeDegrees(unsigned tmpArraySize)
 105     {
 106         m_degrees.resize(tmpArraySize);
 107 
 108         // All precolored registers have  an &quot;infinite&quot; degree.
 109         unsigned firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 110         for (unsigned i = 0; i &lt; firstNonRegIndex; ++i)
 111             m_degrees[i] = std::numeric_limits&lt;unsigned&gt;::max();
 112 
 113         memset(m_degrees.data() + firstNonRegIndex, 0, (tmpArraySize - firstNonRegIndex) * sizeof(unsigned));
 114     }
 115 
 116     void addEdgeDistinct(IndexType a, IndexType b)
 117     {
 118         ASSERT(a != b);
 119         bool isNewEdge = addInterferenceEdge(InterferenceEdge(a, b));
 120         if (isNewEdge) {
 121             if (!isPrecolored(a)) {
 122                 ASSERT(!m_adjacencyList[a].contains(b));
 123                 m_adjacencyList[a].append(b);
 124                 m_degrees[a]++;
 125             }
 126 
 127             if (!isPrecolored(b)) {
 128                 ASSERT(!m_adjacencyList[b].contains(a));
 129                 m_adjacencyList[b].append(a);
 130                 m_degrees[b]++;
 131             }
 132         }
 133     }
 134 
 135     bool addEdgeDistinctWithoutDegreeChange(IndexType a, IndexType b)
 136     {
 137         ASSERT(a != b);
 138         bool isNewEdge = addInterferenceEdge(InterferenceEdge(a, b));
 139         if (isNewEdge) {
 140             if (!isPrecolored(a)) {
 141                 ASSERT(!m_adjacencyList[a].contains(b));
 142                 m_adjacencyList[a].append(b);
 143             }
 144 
 145             if (!isPrecolored(b)) {
 146                 ASSERT(!m_adjacencyList[b].contains(a));
 147                 m_adjacencyList[b].append(a);
 148             }
 149             return true;
 150         }
 151         return false;
 152     }
 153 
 154     template&lt;typename Function&gt;
 155     void forEachAdjacent(IndexType tmpIndex, Function function)
 156     {
 157         for (IndexType adjacentTmpIndex : m_adjacencyList[tmpIndex]) {
 158             if (!hasBeenSimplified(adjacentTmpIndex))
 159                 function(adjacentTmpIndex);
 160         }
 161     }
 162 
 163     bool hasBeenSimplified(IndexType tmpIndex)
 164     {
 165         if (ASSERT_ENABLED) {
 166             if (!!m_coalescedTmps[tmpIndex])
 167                 ASSERT(getAlias(tmpIndex) != tmpIndex);
 168         }
 169 
 170         return m_isOnSelectStack.quickGet(tmpIndex) || !!m_coalescedTmps[tmpIndex];
 171     }
 172 
 173     bool canBeSafelyCoalesced(IndexType u, IndexType v)
 174     {
 175         ASSERT(!isPrecolored(v));
 176         if (isPrecolored(u))
 177             return precoloredCoalescingHeuristic(u, v);
 178         return conservativeHeuristic(u, v);
 179     }
 180 
 181     bool conservativeHeuristic(IndexType u, IndexType v)
 182     {
 183         // This is using the Briggs&#39; conservative coalescing rule:
 184         // If the number of combined adjacent node with a degree &gt;= K is less than K,
 185         // it is safe to combine the two nodes. The reason is that we know that if the graph
 186         // is colorable, we have fewer than K adjacents with high order and there is a color
 187         // for the current node.
 188         ASSERT(u != v);
 189         ASSERT(!isPrecolored(u));
 190         ASSERT(!isPrecolored(v));
 191 
 192         const auto&amp; adjacentsOfU = m_adjacencyList[u];
 193         const auto&amp; adjacentsOfV = m_adjacencyList[v];
 194 
 195         Vector&lt;IndexType, MacroAssembler::numGPRs + MacroAssembler::numFPRs&gt; highOrderAdjacents;
 196         RELEASE_ASSERT(registerCount() &lt;= MacroAssembler::numGPRs + MacroAssembler::numFPRs);
 197         unsigned numCandidates = adjacentsOfU.size() + adjacentsOfV.size();
 198         if (numCandidates &lt; registerCount()) {
 199             // Shortcut: if the total number of adjacents is less than the number of register, the condition is always met.
 200             return true;
 201         }
 202 
 203         for (IndexType adjacentTmpIndex : adjacentsOfU) {
 204             ASSERT(adjacentTmpIndex != v);
 205             ASSERT(adjacentTmpIndex != u);
 206             numCandidates--;
 207             if (!hasBeenSimplified(adjacentTmpIndex) &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()) {
 208                 ASSERT(std::find(highOrderAdjacents.begin(), highOrderAdjacents.end(), adjacentTmpIndex) == highOrderAdjacents.end());
 209                 highOrderAdjacents.uncheckedAppend(adjacentTmpIndex);
 210                 if (highOrderAdjacents.size() &gt;= registerCount())
 211                     return false;
 212             } else if (highOrderAdjacents.size() + numCandidates &lt; registerCount())
 213                 return true;
 214         }
 215         ASSERT(numCandidates == adjacentsOfV.size());
 216 
 217         auto iteratorEndHighOrderAdjacentsOfU = highOrderAdjacents.end();
 218         for (IndexType adjacentTmpIndex : adjacentsOfV) {
 219             ASSERT(adjacentTmpIndex != u);
 220             ASSERT(adjacentTmpIndex != v);
 221             numCandidates--;
 222             if (!hasBeenSimplified(adjacentTmpIndex)
 223                 &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()
 224                 &amp;&amp; std::find(highOrderAdjacents.begin(), iteratorEndHighOrderAdjacentsOfU, adjacentTmpIndex) == iteratorEndHighOrderAdjacentsOfU) {
 225                 ASSERT(std::find(iteratorEndHighOrderAdjacentsOfU, highOrderAdjacents.end(), adjacentTmpIndex) == highOrderAdjacents.end());
 226                 highOrderAdjacents.uncheckedAppend(adjacentTmpIndex);
 227                 if (highOrderAdjacents.size() &gt;= registerCount())
 228                     return false;
 229             } else if (highOrderAdjacents.size() + numCandidates &lt; registerCount())
 230                 return true;
 231         }
 232 
 233         ASSERT(!numCandidates);
 234         ASSERT(highOrderAdjacents.size() &lt; registerCount());
 235         return true;
 236     }
 237 
 238     bool precoloredCoalescingHeuristic(IndexType u, IndexType v)
 239     {
 240         if (traceDebug)
 241             dataLog(&quot;    Checking precoloredCoalescingHeuristic\n&quot;);
 242         ASSERT(isPrecolored(u));
 243         ASSERT(!isPrecolored(v));
 244 
 245         // If any adjacent of the non-colored node is not an adjacent of the colored node AND has a degree &gt;= K
 246         // there is a risk that this node needs to have the same color as our precolored node. If we coalesce such
 247         // move, we may create an uncolorable graph.
 248         const auto&amp; adjacentsOfV = m_adjacencyList[v];
 249         for (unsigned adjacentTmpIndex : adjacentsOfV) {
 250             if (!isPrecolored(adjacentTmpIndex)
 251                 &amp;&amp; !hasBeenSimplified(adjacentTmpIndex)
 252                 &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()
 253                 &amp;&amp; !hasInterferenceEdge(InterferenceEdge(u, adjacentTmpIndex)))
 254                 return false;
 255         }
 256         return true;
 257     }
 258 
 259     void addBias(IndexType u, IndexType v)
 260     {
 261         // We implement biased coloring as proposed by Briggs. See section
 262         // 5.3.3 of his thesis for more information: http://www.cs.utexas.edu/users/mckinley/380C/lecs/briggs-thesis-1992.pdf
 263         // The main idea of biased coloring is this:
 264         // We try to coalesce a move between u and v, but the conservative heuristic
 265         // fails. We don&#39;t want coalesce the move because we don&#39;t want to risk
 266         // creating an uncolorable graph. However, if the conservative heuristic fails,
 267         // it&#39;s not proof that the graph is uncolorable if the move were indeed coalesced.
 268         // So, when we go to color the tmps, we&#39;ll remember that we really want the
 269         // same register for u and v, and if legal, we will assign them to the same register.
 270         if (!isPrecolored(u))
 271             m_biases.add(u, IndexTypeSet()).iterator-&gt;value.add(v);
 272         if (!isPrecolored(v))
 273             m_biases.add(v, IndexTypeSet()).iterator-&gt;value.add(u);
 274     }
 275 
 276     IndexType selectSpill()
 277     {
 278         if (!m_hasSelectedSpill) {
 279             m_hasSelectedSpill = true;
 280 
 281             if (m_hasCoalescedNonTrivialMove)
 282                 m_coalescedTmpsAtSpill = m_coalescedTmps;
 283         }
 284 
 285         auto iterator = m_spillWorklist.begin();
 286 
 287         RELEASE_ASSERT_WITH_MESSAGE(iterator != m_spillWorklist.end(), &quot;selectSpill() called when there was no spill.&quot;);
 288         RELEASE_ASSERT_WITH_MESSAGE(!m_unspillableTmps.contains(*iterator), &quot;trying to spill unspillable tmp&quot;);
 289 
 290         // Higher score means more desirable to spill. Lower scores maximize the likelihood that a tmp
 291         // gets a register.
 292         auto score = [&amp;] (Tmp tmp) -&gt; double {
 293             // Air exposes the concept of &quot;fast tmps&quot;, and we interpret that to mean that the tmp
 294             // should always be in a register.
 295             if (m_code.isFastTmp(tmp))
 296                 return 0;
 297 
 298             // All else being equal, the score should be directly related to the degree.
 299             double degree = static_cast&lt;double&gt;(m_degrees[TmpMapper::absoluteIndex(tmp)]);
 300 
 301             // All else being equal, the score should be inversely related to the number of warm uses and
 302             // defs.
 303             const UseCounts&lt;Tmp&gt;::Counts* counts = m_useCounts[tmp];
 304             if (!counts)
 305                 return std::numeric_limits&lt;double&gt;::infinity();
 306 
 307             double uses = counts-&gt;numWarmUses + counts-&gt;numDefs;
 308 
 309             // If it&#39;s a constant, then it&#39;s not as bad to spill. We can rematerialize it in many
 310             // cases.
 311             if (counts-&gt;numConstDefs == 1 &amp;&amp; counts-&gt;numDefs == 1)
 312                 uses /= 2;
 313 
 314             return degree / uses;
 315         };
 316 
 317         auto victimIterator = iterator;
 318         double maxScore = score(TmpMapper::tmpFromAbsoluteIndex(*iterator));
 319 
 320         ++iterator;
 321         for (;iterator != m_spillWorklist.end(); ++iterator) {
 322             double tmpScore = score(TmpMapper::tmpFromAbsoluteIndex(*iterator));
 323             if (tmpScore &gt; maxScore) {
 324                 ASSERT(!m_unspillableTmps.contains(*iterator));
 325                 victimIterator = iterator;
 326                 maxScore = tmpScore;
 327             }
 328         }
 329 
 330         IndexType victimIndex = *victimIterator;
 331         if (traceDebug)
 332             dataLogLn(&quot;Selecting spill &quot;, victimIndex);
 333         ASSERT(!isPrecolored(victimIndex));
 334         return victimIndex;
 335     }
 336 
 337     void assignColors()
 338     {
 339         ASSERT(m_simplifyWorklist.isEmpty());
 340         ASSERT(m_spillWorklist.isEmpty());
 341 
 342         // Reclaim as much memory as possible.
 343         m_interferenceEdges.clear();
 344 
 345         m_degrees.clear();
 346         m_moveList.clear();
 347         m_simplifyWorklist.clear();
 348         m_spillWorklist.clearAll();
 349 
 350         // Try to color the Tmp on the stack.
 351         m_coloredTmp.resize(m_adjacencyList.size());
 352 
 353         {
 354             Vector&lt;IndexType, 4&gt; nowAliasedBiases;
 355             for (IndexType key : m_biases.keys()) {
 356                 if (key != getAlias(key))
 357                     nowAliasedBiases.append(key);
 358             }
 359             for (IndexType key : nowAliasedBiases) {
 360                 IndexTypeSet keysBiases(m_biases.take(key));
 361                 auto addResult = m_biases.add(getAlias(key), IndexTypeSet());
 362                 if (addResult.isNewEntry) {
 363                     ASSERT(!addResult.iterator-&gt;value.size());
 364                     addResult.iterator-&gt;value = WTFMove(keysBiases);
 365                 } else {
 366                     IndexTypeSet&amp; setToAddTo = addResult.iterator-&gt;value;
 367                     for (IndexType tmp : keysBiases)
 368                         setToAddTo.add(tmp);
 369                 }
 370             }
 371         }
 372 
 373         while (!m_selectStack.isEmpty()) {
 374             unsigned tmpIndex = m_selectStack.takeLast();
 375             ASSERT(!isPrecolored(tmpIndex));
 376             ASSERT(!m_coloredTmp[tmpIndex]);
 377             ASSERT(getAlias(tmpIndex) == tmpIndex);
 378 
 379             RegisterSet coloredRegisters;
 380             for (IndexType adjacentTmpIndex : m_adjacencyList[tmpIndex]) {
 381                 IndexType aliasTmpIndex = getAlias(adjacentTmpIndex);
 382                 Reg reg = m_coloredTmp[aliasTmpIndex];
 383 
 384                 ASSERT(!isPrecolored(aliasTmpIndex) || (isPrecolored(aliasTmpIndex) &amp;&amp; reg));
 385 
 386                 if (reg)
 387                     coloredRegisters.set(reg);
 388             }
 389 
 390             bool colorAssigned = false;
 391             auto iter = m_biases.find(tmpIndex);
 392             if (iter != m_biases.end()) {
 393                 for (IndexType desiredBias : iter-&gt;value) {
 394                     if (Reg desiredColor = m_coloredTmp[getAlias(desiredBias)]) {
 395                         if (!coloredRegisters.get(desiredColor)) {
 396                             m_coloredTmp[tmpIndex] = desiredColor;
 397                             colorAssigned = true;
 398                             break;
 399                         }
 400                     }
 401                 }
 402             }
 403             if (!colorAssigned) {
 404                 for (Reg reg : m_regsInPriorityOrder) {
 405                     if (!coloredRegisters.get(reg)) {
 406                         m_coloredTmp[tmpIndex] = reg;
 407                         colorAssigned = true;
 408                         break;
 409                     }
 410                 }
 411             }
 412 
 413             if (!colorAssigned)
 414                 m_spilledTmps.append(tmpIndex);
 415         }
 416 
 417         m_selectStack.clear();
 418 
 419         if (m_spilledTmps.isEmpty())
 420             m_coalescedTmpsAtSpill.clear();
 421         else
 422             m_coloredTmp.clear();
 423     }
 424 
 425     void dumpInterferenceGraphInDot(PrintStream&amp; out)
 426     {
 427         out.print(&quot;graph InterferenceGraph { \n&quot;);
 428 
 429         HashSet&lt;Tmp&gt; tmpsWithInterferences;
 430         for (const auto&amp; edge : m_interferenceEdges) {
 431             tmpsWithInterferences.add(TmpMapper::tmpFromAbsoluteIndex(edge.first()));
 432             tmpsWithInterferences.add(TmpMapper::tmpFromAbsoluteIndex(edge.second()));
 433         }
 434 
 435         for (const auto&amp; tmp : tmpsWithInterferences) {
 436             unsigned tmpIndex = TmpMapper::absoluteIndex(tmp);
 437             if (tmpIndex &lt; m_degrees.size())
 438                 out.print(&quot;    &quot;, tmp.internalValue(), &quot; [label=\&quot;&quot;, tmp, &quot; (&quot;, m_degrees[tmpIndex], &quot;)\&quot;];\n&quot;);
 439             else
 440                 out.print(&quot;    &quot;, tmp.internalValue(), &quot; [label=\&quot;&quot;, tmp, &quot;\&quot;];\n&quot;);
 441         }
 442 
 443         for (const auto&amp; edge : m_interferenceEdges)
 444             out.print(&quot;    &quot;, edge.first(), &quot; -- &quot;, edge.second(), &quot;;\n&quot;);
 445         out.print(&quot;}\n&quot;);
 446     }
 447 
 448     // Interference edges are not directed. An edge between any two Tmps is represented
 449     // by the concatenated values of the smallest Tmp followed by the bigger Tmp.
 450     class InterferenceEdge {
 451     public:
 452         InterferenceEdge()
 453         {
 454         }
 455 
 456         InterferenceEdge(IndexType a, IndexType b)
 457         {
 458             ASSERT(a);
 459             ASSERT(b);
 460             ASSERT_WITH_MESSAGE(a != b, &quot;A Tmp can never interfere with itself. Doing so would force it to be the superposition of two registers.&quot;);
 461 
 462             if (b &lt; a)
 463                 std::swap(a, b);
 464             m_value = static_cast&lt;uint64_t&gt;(a) &lt;&lt; 32 | b;
 465         }
 466 
 467         InterferenceEdge(WTF::HashTableDeletedValueType)
 468             : m_value(std::numeric_limits&lt;uint64_t&gt;::max())
 469         {
 470         }
 471 
 472         IndexType first() const
 473         {
 474             return m_value &gt;&gt; 32 &amp; 0xffffffff;
 475         }
 476 
 477         IndexType second() const
 478         {
 479             return m_value &amp; 0xffffffff;
 480         }
 481 
 482         bool operator==(const InterferenceEdge&amp; other) const
 483         {
 484             return m_value == other.m_value;
 485         }
 486 
 487         bool isHashTableDeletedValue() const
 488         {
 489             return *this == InterferenceEdge(WTF::HashTableDeletedValue);
 490         }
 491 
 492         unsigned hash() const
 493         {
 494             return WTF::IntHash&lt;uint64_t&gt;::hash(m_value);
 495         }
 496 
 497         void dump(PrintStream&amp; out) const
 498         {
 499             out.print(first(), &quot;&lt;=&gt;&quot;, second());
 500         }
 501 
 502     private:
 503         uint64_t m_value { 0 };
 504     };
 505 
 506     bool addInterferenceEdge(InterferenceEdge edge)
 507     {
 508         return m_interferenceEdges.add(edge).isNewEntry;
 509     }
 510 
 511     bool hasInterferenceEdge(InterferenceEdge edge)
 512     {
 513         return m_interferenceEdges.contains(edge);
 514     }
 515 
 516     struct InterferenceEdgeHash {
 517         static unsigned hash(const InterferenceEdge&amp; key) { return key.hash(); }
 518         static bool equal(const InterferenceEdge&amp; a, const InterferenceEdge&amp; b) { return a == b; }
 519         static constexpr bool safeToCompareToEmptyOrDeleted = true;
 520     };
 521     typedef SimpleClassHashTraits&lt;InterferenceEdge&gt; InterferenceEdgeHashTraits;
 522 
 523     Vector&lt;Reg&gt; m_regsInPriorityOrder;
 524     IndexType m_lastPrecoloredRegisterIndex { 0 };
 525 
 526     // The interference graph.
 527     HashSet&lt;InterferenceEdge, InterferenceEdgeHash, InterferenceEdgeHashTraits&gt; m_interferenceEdges;
 528 
 529     Vector&lt;Vector&lt;IndexType, 0, UnsafeVectorOverflow, 4&gt;, 0, UnsafeVectorOverflow&gt; m_adjacencyList;
 530     Vector&lt;IndexType, 0, UnsafeVectorOverflow&gt; m_degrees;
 531 
 532     using IndexTypeSet = HashSet&lt;IndexType, typename DefaultHash&lt;IndexType&gt;::Hash, WTF::UnsignedWithZeroKeyHashTraits&lt;IndexType&gt;&gt;;
 533 
 534     HashMap&lt;IndexType, IndexTypeSet, typename DefaultHash&lt;IndexType&gt;::Hash, WTF::UnsignedWithZeroKeyHashTraits&lt;IndexType&gt;&gt; m_biases;
 535 
 536     // Instead of keeping track of the move instructions, we just keep their operands around and use the index
 537     // in the vector as the &quot;identifier&quot; for the move.
 538     struct MoveOperands {
 539         IndexType srcIndex;
 540         IndexType dstIndex;
 541     };
 542     Vector&lt;MoveOperands, 0, UnsafeVectorOverflow&gt; m_coalescingCandidates;
 543 
 544     // List of every move instruction associated with a Tmp.
 545     Vector&lt;IndexTypeSet&gt; m_moveList;
 546 
 547     // Colors.
 548     Vector&lt;Reg, 0, UnsafeVectorOverflow&gt; m_coloredTmp;
 549     Vector&lt;IndexType&gt; m_spilledTmps;
 550 
 551     // Values that have been coalesced with an other value.
 552     Vector&lt;IndexType, 0, UnsafeVectorOverflow&gt; m_coalescedTmps;
 553 
 554     // The stack of Tmp removed from the graph and ready for coloring.
 555     BitVector m_isOnSelectStack;
 556     Vector&lt;IndexType&gt; m_selectStack;
 557 
 558     // Low-degree, non-Move related.
 559     Vector&lt;IndexType&gt; m_simplifyWorklist;
 560     // High-degree Tmp.
 561     BitVector m_spillWorklist;
 562 
 563     bool m_hasSelectedSpill { false };
 564     bool m_hasCoalescedNonTrivialMove { false };
 565 
 566     // The mapping of Tmp to their alias for Moves that are always coalescing regardless of spilling.
 567     Vector&lt;IndexType, 0, UnsafeVectorOverflow&gt; m_coalescedTmpsAtSpill;
 568 
 569     const HashSet&lt;unsigned&gt;&amp; m_unspillableTmps;
 570     const UseCounts&lt;Tmp&gt;&amp; m_useCounts;
 571     Code&amp; m_code;
 572 
 573     Vector&lt;Tmp, 4&gt; m_pinnedRegs;
 574 };
 575 
 576 template &lt;typename IndexType, typename TmpMapper&gt;
 577 class Briggs : public AbstractColoringAllocator&lt;IndexType, TmpMapper&gt; {
 578     using Base = AbstractColoringAllocator&lt;IndexType, TmpMapper&gt;;
 579 protected:
 580     using Base::m_isOnSelectStack;
 581     using Base::m_selectStack;
 582     using Base::m_simplifyWorklist;
 583     using Base::m_spillWorklist;
 584     using Base::m_hasSelectedSpill;
 585     using Base::m_hasCoalescedNonTrivialMove;
 586     using Base::m_degrees;
 587     using Base::registerCount;
 588     using Base::m_coalescedTmps;
 589     using Base::m_moveList;
 590     using Base::m_useCounts;
 591     using Base::m_coalescedTmpsAtSpill;
 592     using Base::m_spilledTmps;
 593     using MoveOperands = typename Base::MoveOperands;
 594     using Base::m_coalescingCandidates;
 595     using Base::m_lastPrecoloredRegisterIndex;
 596     using Base::m_coloredTmp;
 597     using Base::m_code;
 598     using InterferenceEdge = typename Base::InterferenceEdge;
 599     using Base::m_unspillableTmps;
 600     using Base::hasInterferenceEdge;
 601     using Base::getAlias;
 602     using Base::addEdge;
 603     using Base::isPrecolored;
 604     using Base::canBeSafelyCoalesced;
 605     using Base::addEdgeDistinctWithoutDegreeChange;
 606     using Base::forEachAdjacent;
 607     using Base::hasBeenSimplified;
 608     using Base::addToSpill;
 609     using Base::m_interferenceEdges;
 610     using Base::addBias;
 611     using Base::m_pinnedRegs;
 612     using Base::m_regsInPriorityOrder;
 613 
 614 public:
 615     Briggs(Code&amp; code, const Vector&lt;Reg&gt;&amp; regsInPriorityOrder, IndexType lastPrecoloredRegisterIndex, unsigned tmpArraySize, const HashSet&lt;unsigned&gt;&amp; unspillableTmps, const UseCounts&lt;Tmp&gt;&amp; useCounts)
 616         : Base(code, regsInPriorityOrder, lastPrecoloredRegisterIndex, tmpArraySize, unspillableTmps, useCounts)
 617     {
 618     }
 619 
 620     void allocate()
 621     {
 622         bool changed = false;
 623 
 624         auto coalesceMove = [&amp;] (unsigned&amp; move) {
 625             bool coalesced = coalesce(move);
 626             if (coalesced) {
 627                 changed = true;
 628                 // We won&#39;t need to handle this move in the future since it&#39;s already coalesced.
 629                 move = UINT_MAX;
 630             }
 631         };
 632 
 633         // We first coalesce until we can&#39;t coalesce any more.
 634         do {
 635             changed = false;
 636             m_worklistMoves.forEachMove(coalesceMove);
 637         } while (changed);
 638         do {
 639             changed = false;
 640             m_worklistMoves.forEachLowPriorityMove(coalesceMove);
 641         } while (changed);
 642 
 643         // Then we create our select stack. The invariant we start with here is that nodes in
 644         // the interference graph with degree &gt;= k are on the spill list. Nodes with degree &lt; k
 645         // are on the simplify worklist. A node can move from the spill list to the simplify
 646         // list (but not the other way around, note that this is different than IRC because IRC
 647         // runs this while coalescing, but we do all our coalescing before this). Once a node is
 648         // added to the select stack, it&#39;s not on either list, but only on the select stack.
 649         // Once on the select stack, logically, it&#39;s no longer in the interference graph.
 650         auto assertInvariants = [&amp;] () {
 651             if (!ASSERT_ENABLED)
 652                 return;
 653             if (!shouldValidateIRAtEachPhase())
 654                 return;
 655 
 656             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 657             unsigned registerCount = this-&gt;registerCount();
 658             for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
 659                 if (getAlias(i) != i)
 660                     continue;
 661                 if (m_isOnSelectStack.contains(i)) {
 662                     ASSERT(!m_simplifyWorklist.contains(i) &amp;&amp; !m_spillWorklist.contains(i));
 663                     continue;
 664                 }
 665                 unsigned degree = m_degrees[i];
 666                 if (degree &gt;= registerCount) {
 667                     ASSERT(m_unspillableTmps.contains(i) || m_spillWorklist.contains(i));
 668                     ASSERT(!m_simplifyWorklist.contains(i));
 669                     continue;
 670                 }
 671                 ASSERT(m_simplifyWorklist.contains(i));
 672             }
 673         };
 674 
 675         makeInitialWorklist();
 676         assertInvariants();
 677         do {
 678             changed = false;
 679 
 680             while (m_simplifyWorklist.size()) {
 681                 simplify();
 682                 assertInvariants();
 683             }
 684 
 685             if (!m_spillWorklist.isEmpty()) {
 686                 selectSpill();
 687                 changed = true;
 688                 ASSERT(m_simplifyWorklist.size() == 1);
 689             }
 690         } while (changed);
 691 
 692         if (ASSERT_ENABLED) {
 693             ASSERT(!m_simplifyWorklist.size());
 694             ASSERT(m_spillWorklist.isEmpty());
 695             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 696             for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i)
 697                 ASSERT(hasBeenSimplified(i));
 698         }
 699 
 700         assignColors();
 701     }
 702 
 703 protected:
 704 
 705     bool coalesce(unsigned&amp; moveIndex)
 706     {
 707         const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
 708         IndexType u = getAlias(moveOperands.srcIndex);
 709         IndexType v = getAlias(moveOperands.dstIndex);
 710 
 711         if (isPrecolored(v))
 712             std::swap(u, v);
 713 
 714         if (traceDebug)
 715             dataLog(&quot;Coalescing move at index &quot;, moveIndex, &quot; u = &quot;, TmpMapper::tmpFromAbsoluteIndex(u), &quot; v = &quot;, TmpMapper::tmpFromAbsoluteIndex(v), &quot;    &quot;);
 716 
 717         if (u == v) {
 718             if (traceDebug)
 719                 dataLog(&quot;Already Coalesced. They&#39;re equal.\n&quot;);
 720             return false;
 721         }
 722 
 723         if (isPrecolored(v)
 724             || hasInterferenceEdge(InterferenceEdge(u, v))) {
 725 
 726             // No need to ever consider this move again if it interferes.
 727             // No coalescing will remove the interference.
 728             moveIndex = UINT_MAX;
 729 
 730             if (ASSERT_ENABLED) {
 731                 if (isPrecolored(v))
 732                     ASSERT(isPrecolored(u));
 733             }
 734 
 735             if (traceDebug)
 736                 dataLog(&quot;Constrained\n&quot;);
 737 
 738             return false;
 739         }
 740 
 741         if (canBeSafelyCoalesced(u, v)) {
 742             combine(u, v);
 743             m_hasCoalescedNonTrivialMove = true;
 744 
 745             if (traceDebug)
 746                 dataLog(&quot;    Safe Coalescing\n&quot;);
 747             return true;
 748         }
 749 
 750         addBias(u, v);
 751 
 752         if (traceDebug)
 753             dataLog(&quot;    Failed coalescing.\n&quot;);
 754 
 755         return false;
 756     }
 757 
 758     void combine(IndexType u, IndexType v)
 759     {
 760         ASSERT(!m_coalescedTmps[v]);
 761         m_coalescedTmps[v] = u;
 762 
 763         auto&amp; vMoves = m_moveList[v];
 764         m_moveList[u].add(vMoves.begin(), vMoves.end());
 765 
 766         forEachAdjacent(v, [this, u] (IndexType adjacentTmpIndex) {
 767             if (addEdgeDistinctWithoutDegreeChange(adjacentTmpIndex, u)) {
 768                 // If we added a new edge between the adjacentTmp and u, it replaces the edge
 769                 // that existed with v.
 770                 // The degree of adjacentTmp remains the same since the edge just changed from u to v.
 771                 // All we need to do is update the degree of u.
 772                 if (!isPrecolored(u))
 773                     m_degrees[u]++;
 774             } else {
 775                 // If we already had an edge between the adjacentTmp and u, the degree of u
 776                 // is already correct. The degree of the adjacentTmp decreases since the edge
 777                 // with v is no longer relevant (we can think of it as merged with the edge with u).
 778                 decrementDegree(adjacentTmpIndex);
 779             }
 780         });
 781     }
 782 
 783 
 784     void makeInitialWorklist()
 785     {
 786         m_simplifyWorklist.clear();
 787         m_spillWorklist.clearAll();
 788 
 789         if (traceDebug)
 790             dataLogLn(&quot;------------------\nMaking initial worklist&quot;);
 791 
 792         IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 793         unsigned registerCount = this-&gt;registerCount();
 794         for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
 795             ASSERT(!isPrecolored(i));
 796             if (hasBeenSimplified(i))
 797                 continue;
 798             unsigned degree = m_degrees[i];
 799             if (degree &lt; registerCount) {
 800                 if (traceDebug)
 801                     dataLogLn(&quot;Adding &quot;, TmpMapper::tmpFromAbsoluteIndex(i), &quot; to simplify worklist&quot;);
 802                 m_simplifyWorklist.append(i);
 803             } else {
 804                 if (traceDebug)
 805                     dataLogLn(&quot;Adding &quot;, TmpMapper::tmpFromAbsoluteIndex(i), &quot; to spill worklist&quot;);
 806                 addToSpill(i);
 807             }
 808         }
 809     }
 810 
 811     // Low-degree vertex can always be colored: just pick any of the color taken by any
 812     // other adjacent verices.
 813     // The &quot;Simplify&quot; phase takes a low-degree out of the interference graph to simplify it.
 814     void simplify()
 815     {
 816         IndexType lastIndex = m_simplifyWorklist.takeLast();
 817 
 818         ASSERT(!m_selectStack.contains(lastIndex));
 819         ASSERT(!m_isOnSelectStack.get(lastIndex));
 820         ASSERT(!m_spillWorklist.contains(lastIndex));
 821 
 822         m_selectStack.append(lastIndex);
 823         m_isOnSelectStack.quickSet(lastIndex);
 824 
 825         if (traceDebug)
 826             dataLogLn(&quot;Simplifying &quot;, lastIndex, &quot; by adding it to select stack&quot;);
 827 
 828         forEachAdjacent(lastIndex, [this](IndexType adjacentTmpIndex) {
 829             decrementDegreeInSimplification(adjacentTmpIndex);
 830         });
 831     }
 832 
 833     void selectSpill()
 834     {
 835         IndexType victimIndex = Base::selectSpill();
 836         m_spillWorklist.quickClear(victimIndex);
 837         m_simplifyWorklist.append(victimIndex);
 838     }
 839 
 840     struct MoveSet {
 841         unsigned addMove()
 842         {
 843             ASSERT(m_lowPriorityMoveList.isEmpty());
 844 
 845             unsigned nextIndex = m_positionInMoveList++;
 846             m_moveList.append(nextIndex);
 847             return nextIndex;
 848         }
 849 
 850         void startAddingLowPriorityMoves()
 851         {
 852             ASSERT(m_lowPriorityMoveList.isEmpty());
 853         }
 854 
 855         unsigned addLowPriorityMove()
 856         {
 857             unsigned nextIndex = m_positionInMoveList++;
 858             m_lowPriorityMoveList.append(nextIndex);
 859 
 860             return nextIndex;
 861         }
 862 
 863         // We use references to moves here because the function
 864         // may do coalescing, indicating it doesn&#39;t need to see
 865         // the move again.
 866         template &lt;typename Function&gt;
 867         void forEachMove(Function function)
 868         {
 869             for (unsigned&amp; move : m_moveList) {
 870                 if (move != UINT_MAX)
 871                     function(move);
 872             }
 873         }
 874 
 875         template &lt;typename Function&gt;
 876         void forEachLowPriorityMove(Function function)
 877         {
 878             for (unsigned&amp; move : m_lowPriorityMoveList) {
 879                 if (move != UINT_MAX)
 880                     function(move);
 881             }
 882         }
 883 
 884         void clear()
 885         {
 886             m_positionInMoveList = 0;
 887             m_moveList.clear();
 888             m_lowPriorityMoveList.clear();
 889         }
 890 
 891     private:
 892         unsigned m_positionInMoveList;
 893         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_moveList;
 894         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_lowPriorityMoveList;
 895     };
 896 
 897     void decrementDegree(IndexType tmpIndex)
 898     {
 899         ASSERT(m_degrees[tmpIndex]);
 900         --m_degrees[tmpIndex];
 901     }
 902 
 903     void decrementDegreeInSimplification(IndexType tmpIndex)
 904     {
 905         ASSERT(m_degrees[tmpIndex]);
 906         unsigned oldDegree = m_degrees[tmpIndex]--;
 907 
 908         if (oldDegree == registerCount()) {
 909             ASSERT(m_degrees[tmpIndex] &lt; registerCount());
 910             if (traceDebug)
 911                 dataLogLn(&quot;Moving tmp &quot;, tmpIndex, &quot; from spill list to simplify list because it&#39;s degree is now less than k&quot;);
 912 
 913             if (ASSERT_ENABLED)
 914                 ASSERT(m_unspillableTmps.contains(tmpIndex) || m_spillWorklist.contains(tmpIndex));
 915             m_spillWorklist.quickClear(tmpIndex);
 916 
 917             ASSERT(!m_simplifyWorklist.contains(tmpIndex));
 918             m_simplifyWorklist.append(tmpIndex);
 919         }
 920     }
 921 
 922     void assignColors()
 923     {
 924         m_worklistMoves.clear();
 925         Base::assignColors();
 926     }
 927 
 928     // Set of &quot;move&quot; enabled for possible coalescing.
 929     MoveSet m_worklistMoves;
 930 };
 931 
 932 template &lt;typename IndexType, typename TmpMapper&gt;
 933 class IRC : public AbstractColoringAllocator&lt;IndexType, TmpMapper&gt; {
 934     using Base = AbstractColoringAllocator&lt;IndexType, TmpMapper&gt;;
 935 protected:
 936     using Base::m_isOnSelectStack;
 937     using Base::m_selectStack;
 938     using Base::m_simplifyWorklist;
 939     using Base::m_spillWorklist;
 940     using Base::m_hasSelectedSpill;
 941     using Base::m_hasCoalescedNonTrivialMove;
 942     using Base::m_degrees;
 943     using Base::registerCount;
 944     using Base::m_coalescedTmps;
 945     using Base::m_moveList;
 946     using Base::m_useCounts;
 947     using Base::m_coalescedTmpsAtSpill;
 948     using Base::m_spilledTmps;
 949     using MoveOperands = typename Base::MoveOperands;
 950     using Base::m_coalescingCandidates;
 951     using Base::m_lastPrecoloredRegisterIndex;
 952     using Base::m_coloredTmp;
 953     using Base::m_code;
 954     using InterferenceEdge = typename Base::InterferenceEdge;
 955     using Base::m_unspillableTmps;
 956     using Base::hasInterferenceEdge;
 957     using Base::getAlias;
 958     using Base::addEdge;
 959     using Base::isPrecolored;
 960     using Base::canBeSafelyCoalesced;
 961     using Base::addEdgeDistinctWithoutDegreeChange;
 962     using Base::forEachAdjacent;
 963     using Base::hasBeenSimplified;
 964     using Base::addToSpill;
 965     using Base::m_interferenceEdges;
 966     using Base::m_adjacencyList;
 967     using Base::dumpInterferenceGraphInDot;
 968     using Base::addBias;
 969     using Base::m_pinnedRegs;
 970     using Base::m_regsInPriorityOrder;
 971 
 972 public:
 973     IRC(Code&amp; code, const Vector&lt;Reg&gt;&amp; regsInPriorityOrder, IndexType lastPrecoloredRegisterIndex, unsigned tmpArraySize, const HashSet&lt;unsigned&gt;&amp; unspillableTmps, const UseCounts&lt;Tmp&gt;&amp; useCounts)
 974         : Base(code, regsInPriorityOrder, lastPrecoloredRegisterIndex, tmpArraySize, unspillableTmps, useCounts)
 975     {
 976     }
 977 
 978     void allocate()
 979     {
 980         m_activeMoves.ensureSize(m_worklistMoves.totalNumberOfMoves());
 981         ASSERT_WITH_MESSAGE(m_activeMoves.size() &gt;= m_coalescingCandidates.size(), &quot;The activeMove set should be big enough for the quick operations of BitVector.&quot;);
 982 
 983         makeWorkList();
 984 
 985         if (debug) {
 986             dumpInterferenceGraphInDot(WTF::dataFile());
 987             dataLog(&quot;Coalescing candidates:\n&quot;);
 988             for (MoveOperands&amp; moveOp : m_coalescingCandidates) {
 989                 dataLog(&quot;    &quot;, TmpMapper::tmpFromAbsoluteIndex(moveOp.srcIndex),
 990                     &quot; -&gt; &quot;, TmpMapper::tmpFromAbsoluteIndex(moveOp.dstIndex), &quot;\n&quot;);
 991             }
 992             dataLog(&quot;Initial work list\n&quot;);
 993             dumpWorkLists(WTF::dataFile());
 994         }
 995 
 996         do {
 997             if (traceDebug) {
 998                 dataLog(&quot;Before Graph simplification iteration\n&quot;);
 999                 dumpWorkLists(WTF::dataFile());
1000             }
1001 
1002             if (!m_simplifyWorklist.isEmpty())
1003                 simplify();
1004             else if (!m_worklistMoves.isEmpty())
1005                 coalesce();
1006             else if (!m_freezeWorklist.isEmpty())
1007                 freeze();
1008             else if (!m_spillWorklist.isEmpty())
1009                 selectSpill();
1010 
1011             if (traceDebug) {
1012                 dataLog(&quot;After Graph simplification iteration\n&quot;);
1013                 dumpWorkLists(WTF::dataFile());
1014             }
1015         } while (!m_simplifyWorklist.isEmpty() || !m_worklistMoves.isEmpty() || !m_freezeWorklist.isEmpty() || !m_spillWorklist.isEmpty());
1016 
1017         assignColors();
1018     }
1019 
1020 protected:
1021 
1022     void makeWorkList()
1023     {
1024         IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
1025         for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
1026             unsigned degree = m_degrees[i];
1027             if (degree &gt;= registerCount())
1028                 addToSpill(i);
1029             else if (!m_moveList[i].isEmpty())
1030                 m_freezeWorklist.add(i);
1031             else
1032                 m_simplifyWorklist.append(i);
1033         }
1034     }
1035 
1036     // Low-degree vertex can always be colored: just pick any of the color taken by any
1037     // other adjacent vertices.
1038     // The &quot;Simplify&quot; phase takes a low-degree out of the interference graph to simplify it.
1039     void simplify()
1040     {
1041         IndexType lastIndex = m_simplifyWorklist.takeLast();
1042 
1043         ASSERT(!m_selectStack.contains(lastIndex));
1044         ASSERT(!m_isOnSelectStack.get(lastIndex));
1045         m_selectStack.append(lastIndex);
1046         m_isOnSelectStack.quickSet(lastIndex);
1047 
1048         forEachAdjacent(lastIndex, [this](IndexType adjacentTmpIndex) {
1049             decrementDegree(adjacentTmpIndex);
1050         });
1051     }
1052 
1053     void coalesce()
1054     {
1055         unsigned moveIndex = m_worklistMoves.takeLastMove();
1056         const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
1057         IndexType u = getAlias(moveOperands.srcIndex);
1058         IndexType v = getAlias(moveOperands.dstIndex);
1059 
1060         if (isPrecolored(v))
1061             std::swap(u, v);
1062 
1063         if (traceDebug)
1064             dataLog(&quot;Coalescing move at index &quot;, moveIndex, &quot; u = &quot;, u, &quot; v = &quot;, v, &quot;\n&quot;);
1065 
1066         if (u == v) {
1067             addWorkList(u);
1068 
1069             if (traceDebug)
1070                 dataLog(&quot;    Coalesced\n&quot;);
1071         } else if (isPrecolored(v)
1072             || hasInterferenceEdge(InterferenceEdge(u, v))) {
1073             addWorkList(u);
1074             addWorkList(v);
1075 
1076             if (traceDebug)
1077                 dataLog(&quot;    Constrained\n&quot;);
1078         } else if (canBeSafelyCoalesced(u, v)) {
1079             combine(u, v);
1080             addWorkList(u);
1081             m_hasCoalescedNonTrivialMove = true;
1082 
1083             if (traceDebug)
1084                 dataLog(&quot;    Safe Coalescing\n&quot;);
1085         } else {
1086             m_activeMoves.quickSet(moveIndex);
1087             if (traceDebug)
1088                 dataLog(&quot;    Failed coalescing, added to active moves.\n&quot;);
1089 
1090             addBias(u, v);
1091         }
1092     }
1093 
1094     void addWorkList(IndexType tmpIndex)
1095     {
1096         if (!isPrecolored(tmpIndex) &amp;&amp; m_degrees[tmpIndex] &lt; registerCount() &amp;&amp; !isMoveRelated(tmpIndex)) {
1097             m_freezeWorklist.remove(tmpIndex);
1098             m_simplifyWorklist.append(tmpIndex);
1099         }
1100     }
1101 
1102     void combine(IndexType u, IndexType v)
1103     {
1104         if (!m_freezeWorklist.remove(v))
1105             m_spillWorklist.quickClear(v);
1106 
1107         ASSERT(!m_coalescedTmps[v]);
1108         m_coalescedTmps[v] = u;
1109 
1110         auto&amp; vMoves = m_moveList[v];
1111         m_moveList[u].add(vMoves.begin(), vMoves.end());
1112 
1113         forEachAdjacent(v, [this, u] (IndexType adjacentTmpIndex) {
1114             if (addEdgeDistinctWithoutDegreeChange(adjacentTmpIndex, u)) {
1115                 // If we added a new edge between the adjacentTmp and u, it replaces the edge
1116                 // that existed with v.
1117                 // The degree of adjacentTmp remains the same since the edge just changed from u to v.
1118                 // All we need to do is update the degree of u.
1119                 if (!isPrecolored(u))
1120                     m_degrees[u]++;
1121             } else {
1122                 // If we already had an edge between the adjacentTmp and u, the degree of u
1123                 // is already correct. The degree of the adjacentTmp decreases since the edge
1124                 // with v is no longer relevant (we can think of it as merged with the edge with u).
1125                 decrementDegree(adjacentTmpIndex);
1126             }
1127         });
1128 
1129         if (m_degrees[u] &gt;= registerCount() &amp;&amp; m_freezeWorklist.remove(u))
1130             addToSpill(u);
1131     }
1132 
1133     void freeze()
1134     {
1135         IndexType victimIndex = m_freezeWorklist.takeAny();
1136         ASSERT_WITH_MESSAGE(getAlias(victimIndex) == victimIndex, &quot;coalesce() should not leave aliased Tmp in the worklist.&quot;);
1137         m_simplifyWorklist.append(victimIndex);
1138         freezeMoves(victimIndex);
1139     }
1140 
1141     void freezeMoves(IndexType tmpIndex)
1142     {
1143         forEachNodeMoves(tmpIndex, [this, tmpIndex] (IndexType moveIndex) {
1144             if (!m_activeMoves.quickClear(moveIndex))
1145                 m_worklistMoves.takeMove(moveIndex);
1146 
1147             const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
1148             IndexType srcTmpIndex = moveOperands.srcIndex;
1149             IndexType dstTmpIndex = moveOperands.dstIndex;
1150 
1151             IndexType originalOtherTmp = srcTmpIndex != tmpIndex ? srcTmpIndex : dstTmpIndex;
1152             IndexType otherTmpIndex = getAlias(originalOtherTmp);
1153             if (m_degrees[otherTmpIndex] &lt; registerCount() &amp;&amp; !isMoveRelated(otherTmpIndex)) {
1154                 if (m_freezeWorklist.remove(otherTmpIndex))
1155                     m_simplifyWorklist.append(otherTmpIndex);
1156             }
1157         });
1158     }
1159 
1160     void decrementDegree(IndexType tmpIndex)
1161     {
1162         ASSERT(m_degrees[tmpIndex]);
1163 
1164         unsigned oldDegree = m_degrees[tmpIndex]--;
1165         if (oldDegree == registerCount()) {
1166             enableMovesOnValueAndAdjacents(tmpIndex);
1167             m_spillWorklist.quickClear(tmpIndex);
1168             if (isMoveRelated(tmpIndex))
1169                 m_freezeWorklist.add(tmpIndex);
1170             else
1171                 m_simplifyWorklist.append(tmpIndex);
1172         }
1173     }
1174 
1175     void selectSpill()
1176     {
1177         IndexType victimIndex = Base::selectSpill();
1178         m_spillWorklist.quickClear(victimIndex);
1179         m_simplifyWorklist.append(victimIndex);
1180         freezeMoves(victimIndex);
1181     }
1182 
1183     void assignColors()
1184     {
1185         ASSERT(m_freezeWorklist.isEmpty());
1186         m_worklistMoves.clear();
1187         Base::assignColors();
1188     }
1189 
1190 
1191     bool isMoveRelated(IndexType tmpIndex)
1192     {
1193         for (unsigned moveIndex : m_moveList[tmpIndex]) {
1194             if (m_activeMoves.quickGet(moveIndex) || m_worklistMoves.contains(moveIndex))
1195                 return true;
1196         }
1197         return false;
1198     }
1199 
1200     template&lt;typename Function&gt;
1201     void forEachNodeMoves(IndexType tmpIndex, Function function)
1202     {
1203         for (unsigned moveIndex : m_moveList[tmpIndex]) {
1204             if (m_activeMoves.quickGet(moveIndex) || m_worklistMoves.contains(moveIndex))
1205                 function(moveIndex);
1206         }
1207     }
1208 
1209     void enableMovesOnValue(IndexType tmpIndex)
1210     {
1211         for (unsigned moveIndex : m_moveList[tmpIndex]) {
1212             if (m_activeMoves.quickClear(moveIndex))
1213                 m_worklistMoves.returnMove(moveIndex);
1214         }
1215     }
1216 
1217     void enableMovesOnValueAndAdjacents(IndexType tmpIndex)
1218     {
1219         enableMovesOnValue(tmpIndex);
1220 
1221         forEachAdjacent(tmpIndex, [this] (IndexType adjacentTmpIndex) {
1222             enableMovesOnValue(adjacentTmpIndex);
1223         });
1224     }
1225 
1226     struct OrderedMoveSet {
1227         unsigned addMove()
1228         {
1229             ASSERT(m_lowPriorityMoveList.isEmpty());
1230             ASSERT(!m_firstLowPriorityMoveIndex);
1231 
1232             unsigned nextIndex = m_positionInMoveList.size();
1233             unsigned position = m_moveList.size();
1234             m_moveList.append(nextIndex);
1235             m_positionInMoveList.append(position);
1236             return nextIndex;
1237         }
1238 
1239         void startAddingLowPriorityMoves()
1240         {
1241             ASSERT(m_lowPriorityMoveList.isEmpty());
1242             m_firstLowPriorityMoveIndex = m_moveList.size();
1243         }
1244 
1245         unsigned addLowPriorityMove()
1246         {
1247             ASSERT(m_firstLowPriorityMoveIndex == m_moveList.size());
1248 
1249             unsigned nextIndex = m_positionInMoveList.size();
1250             unsigned position = m_lowPriorityMoveList.size();
1251             m_lowPriorityMoveList.append(nextIndex);
1252             m_positionInMoveList.append(position);
1253 
1254             ASSERT(nextIndex &gt;= m_firstLowPriorityMoveIndex);
1255 
1256             return nextIndex;
1257         }
1258 
1259         bool isEmpty() const
1260         {
1261             return m_moveList.isEmpty() &amp;&amp; m_lowPriorityMoveList.isEmpty();
1262         }
1263 
1264         bool contains(unsigned index)
1265         {
1266             return m_positionInMoveList[index] != std::numeric_limits&lt;unsigned&gt;::max();
1267         }
1268 
1269         void takeMove(unsigned moveIndex)
1270         {
1271             unsigned positionInMoveList = m_positionInMoveList[moveIndex];
1272             if (positionInMoveList == std::numeric_limits&lt;unsigned&gt;::max())
1273                 return;
1274 
1275             if (moveIndex &lt; m_firstLowPriorityMoveIndex) {
1276                 ASSERT(m_moveList[positionInMoveList] == moveIndex);
1277                 unsigned lastIndex = m_moveList.last();
1278                 m_positionInMoveList[lastIndex] = positionInMoveList;
1279                 m_moveList[positionInMoveList] = lastIndex;
1280                 m_moveList.removeLast();
1281             } else {
1282                 ASSERT(m_lowPriorityMoveList[positionInMoveList] == moveIndex);
1283                 unsigned lastIndex = m_lowPriorityMoveList.last();
1284                 m_positionInMoveList[lastIndex] = positionInMoveList;
1285                 m_lowPriorityMoveList[positionInMoveList] = lastIndex;
1286                 m_lowPriorityMoveList.removeLast();
1287             }
1288 
1289             m_positionInMoveList[moveIndex] = std::numeric_limits&lt;unsigned&gt;::max();
1290 
1291             ASSERT(!contains(moveIndex));
1292         }
1293 
1294         unsigned takeLastMove()
1295         {
1296             ASSERT(!isEmpty());
1297 
1298             unsigned lastIndex;
1299             if (!m_moveList.isEmpty()) {
1300                 lastIndex = m_moveList.takeLast();
1301                 ASSERT(m_positionInMoveList[lastIndex] == m_moveList.size());
1302             } else {
1303                 lastIndex = m_lowPriorityMoveList.takeLast();
1304                 ASSERT(m_positionInMoveList[lastIndex] == m_lowPriorityMoveList.size());
1305             }
1306             m_positionInMoveList[lastIndex] = std::numeric_limits&lt;unsigned&gt;::max();
1307 
1308             ASSERT(!contains(lastIndex));
1309             return lastIndex;
1310         }
1311 
1312         void returnMove(unsigned index)
1313         {
1314             // This assertion is a bit strict but that is how the move list should be used. The only kind of moves that can
1315             // return to the list are the ones that we previously failed to coalesce with the conservative heuristics.
1316             // Values should not be added back if they were never taken out when attempting coalescing.
1317             ASSERT(!contains(index));
1318 
1319             if (index &lt; m_firstLowPriorityMoveIndex) {
1320                 unsigned position = m_moveList.size();
1321                 m_moveList.append(index);
1322                 m_positionInMoveList[index] = position;
1323             } else {
1324                 unsigned position = m_lowPriorityMoveList.size();
1325                 m_lowPriorityMoveList.append(index);
1326                 m_positionInMoveList[index] = position;
1327             }
1328 
1329             ASSERT(contains(index));
1330         }
1331 
1332         void clear()
1333         {
1334             m_positionInMoveList.clear();
1335             m_moveList.clear();
1336             m_lowPriorityMoveList.clear();
1337         }
1338 
1339         unsigned totalNumberOfMoves()
1340         {
1341             return m_moveList.size() + m_lowPriorityMoveList.size();
1342         }
1343 
1344     private:
1345         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_positionInMoveList;
1346         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_moveList;
1347         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_lowPriorityMoveList;
1348         unsigned m_firstLowPriorityMoveIndex { 0 };
1349     };
1350 
1351     void dumpWorkLists(PrintStream&amp; out)
1352     {
1353         out.print(&quot;Simplify work list:\n&quot;);
1354         for (unsigned tmpIndex : m_simplifyWorklist)
1355             out.print(&quot;    &quot;, TmpMapper::tmpFromAbsoluteIndex(tmpIndex), &quot;\n&quot;);
1356         out.printf(&quot;Moves work list is empty? %d\n&quot;, m_worklistMoves.isEmpty());
1357         out.print(&quot;Freeze work list:\n&quot;);
1358         for (unsigned tmpIndex : m_freezeWorklist)
1359             out.print(&quot;    &quot;, TmpMapper::tmpFromAbsoluteIndex(tmpIndex), &quot;\n&quot;);
1360         out.print(&quot;Spill work list:\n&quot;);
1361         for (unsigned tmpIndex : m_spillWorklist)
1362             out.print(&quot;    &quot;, TmpMapper::tmpFromAbsoluteIndex(tmpIndex), &quot;\n&quot;);
1363     }
1364 
1365     // Work lists.
1366     // Low-degree, Move related.
1367     HashSet&lt;IndexType&gt; m_freezeWorklist;
1368     // Set of &quot;move&quot; enabled for possible coalescing.
1369     OrderedMoveSet m_worklistMoves;
1370     // Set of &quot;move&quot; not yet ready for coalescing.
1371     BitVector m_activeMoves;
1372 };
1373 
1374 // This perform all the tasks that are specific to certain register type.
1375 template&lt;Bank bank, template&lt;typename, typename&gt; class AllocatorType&gt;
1376 class ColoringAllocator : public AllocatorType&lt;unsigned, AbsoluteTmpMapper&lt;bank&gt;&gt; {
1377     using TmpMapper = AbsoluteTmpMapper&lt;bank&gt;;
1378     using Base = AllocatorType&lt;unsigned, TmpMapper&gt;;
1379     using Base::m_isOnSelectStack;
1380     using Base::m_selectStack;
1381     using Base::m_simplifyWorklist;
1382     using Base::m_spillWorklist;
1383     using Base::m_hasSelectedSpill;
1384     using Base::m_hasCoalescedNonTrivialMove;
1385     using Base::m_degrees;
1386     using Base::registerCount;
1387     using Base::m_coalescedTmps;
1388     using Base::m_moveList;
1389     using Base::m_useCounts;
1390     using Base::m_coalescedTmpsAtSpill;
1391     using Base::m_spilledTmps;
1392     using MoveOperands = typename Base::MoveOperands;
1393     using Base::m_coalescingCandidates;
1394     using Base::m_lastPrecoloredRegisterIndex;
1395     using Base::m_coloredTmp;
1396     using Base::m_code;
1397     using InterferenceEdge = typename Base::InterferenceEdge;
1398     using Base::m_worklistMoves;
1399     using Base::hasInterferenceEdge;
1400     using Base::getAlias;
1401     using Base::addEdge;
1402     using Base::m_interferenceEdges;
1403     using Base::m_pinnedRegs;
1404     using Base::m_regsInPriorityOrder;
1405 
1406 public:
1407 
1408     ColoringAllocator(Code&amp; code, TmpWidth&amp; tmpWidth, const UseCounts&lt;Tmp&gt;&amp; useCounts, const HashSet&lt;unsigned&gt;&amp; unspillableTmp)
1409         : Base(code, code.regsInPriorityOrder(bank), TmpMapper::lastMachineRegisterIndex(), tmpArraySize(code), unspillableTmp, useCounts)
1410         , m_tmpWidth(tmpWidth)
1411     {
1412         for (Reg reg : code.pinnedRegisters()) {
1413             if ((bank == GP &amp;&amp; reg.isGPR()) || (bank == FP &amp;&amp; reg.isFPR())) {
1414                 m_pinnedRegs.append(Tmp(reg));
1415                 ASSERT(!m_regsInPriorityOrder.contains(reg));
1416                 m_regsInPriorityOrder.append(reg);
1417             }
1418         }
1419 
1420         initializePrecoloredTmp();
1421         build();
1422     }
1423 
1424     Tmp getAlias(Tmp tmp) const
1425     {
1426         return TmpMapper::tmpFromAbsoluteIndex(getAlias(TmpMapper::absoluteIndex(tmp)));
1427     }
1428 
1429     // This tells you if a Move will be coalescable if the src and dst end up matching. This method
1430     // relies on an analysis that is invalidated by register allocation, so you it&#39;s only meaningful to
1431     // call this *before* replacing the Tmp&#39;s in this Inst with registers or spill slots.
1432     bool mayBeCoalescable(const Inst&amp; inst) const
1433     {
1434         return mayBeCoalescableImpl(inst, &amp;m_tmpWidth);
1435     }
1436 
1437     bool isUselessMove(const Inst&amp; inst) const
1438     {
1439         return mayBeCoalescableImpl(inst, nullptr) &amp;&amp; inst.args[0].tmp() == inst.args[1].tmp();
1440     }
1441 
1442     Tmp getAliasWhenSpilling(Tmp tmp) const
1443     {
1444         ASSERT_WITH_MESSAGE(!m_spilledTmps.isEmpty(), &quot;This function is only valid for coalescing during spilling.&quot;);
1445 
1446         if (m_coalescedTmpsAtSpill.isEmpty())
1447             return tmp;
1448 
1449         unsigned aliasIndex = TmpMapper::absoluteIndex(tmp);
1450         while (unsigned nextAliasIndex = m_coalescedTmpsAtSpill[aliasIndex])
1451             aliasIndex = nextAliasIndex;
1452 
1453         Tmp alias = TmpMapper::tmpFromAbsoluteIndex(aliasIndex);
1454 
1455         ASSERT_WITH_MESSAGE(!m_spilledTmps.contains(aliasIndex) || alias == tmp, &quot;The aliases at spill should always be colorable. Something went horribly wrong.&quot;);
1456 
1457         return alias;
1458     }
1459 
1460     template&lt;typename IndexIterator&gt;
1461     class IndexToTmpIteratorAdaptor {
1462     public:
1463         IndexToTmpIteratorAdaptor(IndexIterator&amp;&amp; indexIterator)
1464             : m_indexIterator(WTFMove(indexIterator))
1465         {
1466         }
1467 
1468         Tmp operator*() const { return TmpMapper::tmpFromAbsoluteIndex(*m_indexIterator); }
1469         IndexToTmpIteratorAdaptor&amp; operator++() { ++m_indexIterator; return *this; }
1470 
1471         bool operator==(const IndexToTmpIteratorAdaptor&amp; other) const
1472         {
1473             return m_indexIterator == other.m_indexIterator;
1474         }
1475 
1476         bool operator!=(const IndexToTmpIteratorAdaptor&amp; other) const
1477         {
1478             return !(*this == other);
1479         }
1480 
1481     private:
1482         IndexIterator m_indexIterator;
1483     };
1484 
1485     template&lt;typename Collection&gt;
1486     class IndexToTmpIterableAdaptor {
1487     public:
1488         IndexToTmpIterableAdaptor(const Collection&amp; collection)
1489             : m_collection(collection)
1490         {
1491         }
1492 
1493         IndexToTmpIteratorAdaptor&lt;typename Collection::const_iterator&gt; begin() const
1494         {
1495             return m_collection.begin();
1496         }
1497 
1498         IndexToTmpIteratorAdaptor&lt;typename Collection::const_iterator&gt; end() const
1499         {
1500             return m_collection.end();
1501         }
1502 
1503     private:
1504         const Collection&amp; m_collection;
1505     };
1506 
1507     IndexToTmpIterableAdaptor&lt;Vector&lt;unsigned&gt;&gt; spilledTmps() const { return m_spilledTmps; }
1508 
1509     bool requiresSpilling() const { return !m_spilledTmps.isEmpty(); }
1510 
1511     Reg allocatedReg(Tmp tmp) const
1512     {
1513         ASSERT(!tmp.isReg());
1514         ASSERT(m_coloredTmp.size());
1515         ASSERT(tmp.isGP() == (bank == GP));
1516 
1517         Reg reg = m_coloredTmp[TmpMapper::absoluteIndex(tmp)];
1518         if (!reg) {
1519             dataLog(&quot;FATAL: No color for &quot;, tmp, &quot;\n&quot;);
1520             dataLog(&quot;Code:\n&quot;);
1521             dataLog(m_code);
1522             RELEASE_ASSERT_NOT_REACHED();
1523         }
1524         return reg;
1525     }
1526 
1527 protected:
1528     static unsigned tmpArraySize(Code&amp; code)
1529     {
1530         unsigned numTmps = code.numTmps(bank);
1531         return TmpMapper::absoluteIndex(numTmps);
1532     }
1533 
1534     void initializePrecoloredTmp()
1535     {
1536         m_coloredTmp.resize(m_lastPrecoloredRegisterIndex + 1);
1537         for (unsigned i = 1; i &lt;= m_lastPrecoloredRegisterIndex; ++i) {
1538             Tmp tmp = TmpMapper::tmpFromAbsoluteIndex(i);
1539             ASSERT(tmp.isReg());
1540             m_coloredTmp[i] = tmp.reg();
1541         }
1542     }
1543 
1544     bool mayBeCoalesced(Arg left, Arg right)
1545     {
1546         if (!left.isTmp() || !right.isTmp())
1547             return false;
1548 
1549         Tmp leftTmp = left.tmp();
1550         Tmp rightTmp = right.tmp();
1551 
1552         if (leftTmp == rightTmp)
1553             return false;
1554 
1555         if (leftTmp.isGP() != (bank == GP) || rightTmp.isGP() != (bank == GP))
1556             return false;
1557 
1558         unsigned leftIndex = TmpMapper::absoluteIndex(leftTmp);
1559         unsigned rightIndex = TmpMapper::absoluteIndex(rightTmp);
1560 
1561         return !hasInterferenceEdge(InterferenceEdge(leftIndex, rightIndex));
1562     }
1563 
1564     void addToLowPriorityCoalescingCandidates(Arg left, Arg right)
1565     {
1566         ASSERT(mayBeCoalesced(left, right));
1567         Tmp leftTmp = left.tmp();
1568         Tmp rightTmp = right.tmp();
1569 
1570         unsigned leftIndex = TmpMapper::absoluteIndex(leftTmp);
1571         unsigned rightIndex = TmpMapper::absoluteIndex(rightTmp);
1572 
1573         unsigned nextMoveIndex = m_coalescingCandidates.size();
1574         m_coalescingCandidates.append({ leftIndex, rightIndex });
1575 
1576         unsigned newIndexInWorklist = m_worklistMoves.addLowPriorityMove();
1577         ASSERT_UNUSED(newIndexInWorklist, newIndexInWorklist == nextMoveIndex);
1578 
1579         m_moveList[leftIndex].add(nextMoveIndex);
1580         m_moveList[rightIndex].add(nextMoveIndex);
1581     }
1582 
1583     void build()
1584     {
1585         m_coalescingCandidates.clear();
1586         m_worklistMoves.clear();
1587 
1588         // FIXME: It seems like we don&#39;t need to recompute liveness. We just need to update its data
1589         // structures so that it knows that the newly introduced temporaries are not live at any basic
1590         // block boundary. This should trivially be true for now.
1591         TmpLiveness&lt;bank&gt; liveness(m_code);
1592         for (BasicBlock* block : m_code) {
1593             typename TmpLiveness&lt;bank&gt;::LocalCalc localCalc(liveness, block);
1594             for (unsigned instIndex = block-&gt;size(); instIndex--;) {
1595                 Inst&amp; inst = block-&gt;at(instIndex);
1596                 Inst* nextInst = block-&gt;get(instIndex + 1);
1597                 build(&amp;inst, nextInst, localCalc);
1598                 localCalc.execute(instIndex);
1599             }
1600             build(nullptr, &amp;block-&gt;at(0), localCalc);
1601         }
1602         buildLowPriorityMoveList();
1603     }
1604 
1605     void build(Inst* prevInst, Inst* nextInst, const typename TmpLiveness&lt;bank&gt;::LocalCalc&amp; localCalc)
1606     {
1607         if (traceDebug)
1608             dataLog(&quot;Building between &quot;, pointerDump(prevInst), &quot; and &quot;, pointerDump(nextInst), &quot;:\n&quot;);
1609 
1610         Inst::forEachDefWithExtraClobberedRegs&lt;Tmp&gt;(
1611             prevInst, nextInst,
1612             [&amp;] (const Tmp&amp; arg, Arg::Role, Bank argBank, Width) {
1613                 if (argBank != bank)
1614                     return;
1615 
1616                 // All the Def()s interfere with each other and with all the extra clobbered Tmps.
1617                 // We should not use forEachDefWithExtraClobberedRegs() here since colored Tmps
1618                 // do not need interference edges in our implementation.
1619                 Inst::forEachDef&lt;Tmp&gt;(
1620                     prevInst, nextInst,
1621                     [&amp;] (Tmp&amp; otherArg, Arg::Role, Bank argBank, Width) {
1622                         if (argBank != bank)
1623                             return;
1624 
1625                         if (traceDebug)
1626                             dataLog(&quot;    Adding def-def edge: &quot;, arg, &quot;, &quot;, otherArg, &quot;\n&quot;);
1627                         this-&gt;addEdge(arg, otherArg);
1628                     });
1629             });
1630 
1631         if (prevInst &amp;&amp; mayBeCoalescable(*prevInst)) {
1632             // We do not want the Use() of this move to interfere with the Def(), even if it is live
1633             // after the Move. If we were to add the interference edge, it would be impossible to
1634             // coalesce the Move even if the two Tmp never interfere anywhere.
1635             Tmp defTmp;
1636             Tmp useTmp;
1637             prevInst-&gt;forEachTmp([&amp;defTmp, &amp;useTmp] (Tmp&amp; argTmp, Arg::Role role, Bank, Width) {
1638                 if (Arg::isLateDef(role))
1639                     defTmp = argTmp;
1640                 else {
1641                     ASSERT(Arg::isEarlyUse(role));
1642                     useTmp = argTmp;
1643                 }
1644             });
1645             ASSERT(defTmp);
1646             ASSERT(useTmp);
1647 
1648             unsigned nextMoveIndex = m_coalescingCandidates.size();
1649             m_coalescingCandidates.append({ TmpMapper::absoluteIndex(useTmp), TmpMapper::absoluteIndex(defTmp) });
1650             if (traceDebug)
1651                 dataLogLn(&quot;Move at index &quot;, nextMoveIndex, &quot; is: &quot;, *prevInst);
1652 
1653             unsigned newIndexInWorklist = m_worklistMoves.addMove();
1654             ASSERT_UNUSED(newIndexInWorklist, newIndexInWorklist == nextMoveIndex);
1655 
1656             for (const Arg&amp; arg : prevInst-&gt;args) {
1657                 auto&amp; list = m_moveList[TmpMapper::absoluteIndex(arg.tmp())];
1658                 list.add(nextMoveIndex);
1659             }
1660 
1661             auto considerEdge = [&amp;] (const Tmp&amp; liveTmp) {
1662                 if (liveTmp != useTmp) {
1663                     if (traceDebug)
1664                         dataLog(&quot;    Adding def-live for coalescable: &quot;, defTmp, &quot;, &quot;, liveTmp, &quot;\n&quot;);
1665                     addEdge(defTmp, liveTmp);
1666                 }
1667             };
1668 
1669             for (Tmp liveTmp : localCalc.live())
1670                 considerEdge(liveTmp);
1671             for (const Tmp&amp; pinnedRegTmp : m_pinnedRegs)
1672                 considerEdge(pinnedRegTmp);
1673 
1674             // The next instruction could have early clobbers or early def&#39;s. We need to consider
1675             // those now.
1676             addEdges(nullptr, nextInst, localCalc.live());
1677         } else
1678             addEdges(prevInst, nextInst, localCalc.live());
1679     }
1680 
1681     void buildLowPriorityMoveList()
1682     {
1683         if (!isX86())
1684             return;
1685 
1686         m_worklistMoves.startAddingLowPriorityMoves();
1687         for (BasicBlock* block : m_code) {
1688             for (Inst&amp; inst : *block) {
1689                 if (Optional&lt;unsigned&gt; defArgIndex = inst.shouldTryAliasingDef()) {
1690                     Arg op1 = inst.args[*defArgIndex - 2];
1691                     Arg op2 = inst.args[*defArgIndex - 1];
1692                     Arg dest = inst.args[*defArgIndex];
1693 
1694                     if (op1 == dest || op2 == dest)
1695                         continue;
1696 
1697                     if (mayBeCoalesced(op1, dest))
1698                         addToLowPriorityCoalescingCandidates(op1, dest);
1699                     if (op1 != op2 &amp;&amp; mayBeCoalesced(op2, dest))
1700                         addToLowPriorityCoalescingCandidates(op2, dest);
1701                 }
1702             }
1703         }
1704     }
1705 
1706     void addEdges(Inst* prevInst, Inst* nextInst, typename TmpLiveness&lt;bank&gt;::LocalCalc::Iterable liveTmps)
1707     {
1708         // All the Def()s interfere with everthing live.
1709         Inst::forEachDefWithExtraClobberedRegs&lt;Tmp&gt;(
1710             prevInst, nextInst,
1711             [&amp;] (const Tmp&amp; arg, Arg::Role, Bank argBank, Width) {
1712                 if (argBank != bank)
1713                     return;
1714 
1715                 for (Tmp liveTmp : liveTmps) {
1716                     ASSERT(liveTmp.isGP() == (bank == GP));
1717 
1718                     if (traceDebug)
1719                         dataLog(&quot;    Adding def-live edge: &quot;, arg, &quot;, &quot;, liveTmp, &quot;\n&quot;);
1720 
1721                     addEdge(arg, liveTmp);
1722                 }
1723                 for (const Tmp&amp; pinnedRegTmp : m_pinnedRegs)
1724                     addEdge(arg, pinnedRegTmp);
1725             });
1726     }
1727 
1728     void addEdge(Tmp a, Tmp b)
1729     {
1730         ASSERT_WITH_MESSAGE(a.isGP() == b.isGP(), &quot;An interference between registers of different types does not make sense, it can lead to non-colorable graphs.&quot;);
1731 
1732         addEdge(TmpMapper::absoluteIndex(a), TmpMapper::absoluteIndex(b));
1733     }
1734 
1735     // Calling this without a tmpWidth will perform a more conservative coalescing analysis that assumes
1736     // that Move32&#39;s are not coalescable.
1737     static bool mayBeCoalescableImpl(const Inst&amp; inst, TmpWidth* tmpWidth)
1738     {
1739         switch (bank) {
1740         case GP:
1741             switch (inst.kind.opcode) {
1742             case Move:
1743             case Move32:
1744                 break;
1745             default:
1746                 return false;
1747             }
1748             break;
1749         case FP:
1750             switch (inst.kind.opcode) {
1751             case MoveFloat:
1752             case MoveDouble:
1753                 break;
1754             default:
1755                 return false;
1756             }
1757             break;
1758         }
1759 
1760         // Avoid the three-argument coalescable spill moves.
1761         if (inst.args.size() != 2)
1762             return false;
1763 
1764         if (!inst.args[0].isTmp() || !inst.args[1].isTmp())
1765             return false;
1766 
1767         ASSERT(inst.args[0].bank() == bank);
1768         ASSERT(inst.args[1].bank() == bank);
1769 
1770         // We can coalesce a Move32 so long as either of the following holds:
1771         // - The input is already zero-filled.
1772         // - The output only cares about the low 32 bits.
1773         //
1774         // Note that the input property requires an analysis over ZDef&#39;s, so it&#39;s only valid so long
1775         // as the input gets a register. We don&#39;t know if the input gets a register, but we do know
1776         // that if it doesn&#39;t get a register then we will still emit this Move32.
1777         if (inst.kind.opcode == Move32) {
1778             if (!tmpWidth)
1779                 return false;
1780 
1781             if (tmpWidth-&gt;defWidth(inst.args[0].tmp()) &gt; Width32
1782                 &amp;&amp; tmpWidth-&gt;useWidth(inst.args[1].tmp()) &gt; Width32)
1783                 return false;
1784         }
1785 
1786         return true;
1787     }
1788 
1789     TmpWidth&amp; m_tmpWidth;
1790 };
1791 
1792 class GraphColoringRegisterAllocation {
1793 public:
1794     GraphColoringRegisterAllocation(Code&amp; code, UseCounts&lt;Tmp&gt;&amp; useCounts)
1795         : m_code(code)
1796         , m_useCounts(useCounts)
1797     {
1798     }
1799 
1800     void run()
1801     {
1802         padInterference(m_code);
1803 
1804         allocateOnBank&lt;GP&gt;();
1805         allocateOnBank&lt;FP&gt;();
1806 
1807         fixSpillsAfterTerminals(m_code);
1808     }
1809 
1810 private:
1811     template&lt;Bank bank&gt;
1812     void allocateOnBank()
1813     {
1814         HashSet&lt;unsigned&gt; unspillableTmps = computeUnspillableTmps&lt;bank&gt;();
1815 
1816         // FIXME: If a Tmp is used only from a Scratch role and that argument is !admitsStack, then
1817         // we should add the Tmp to unspillableTmps. That will help avoid relooping only to turn the
1818         // Tmp into an unspillable Tmp.
1819         // https://bugs.webkit.org/show_bug.cgi?id=152699
1820 
1821         unsigned numIterations = 0;
1822         bool done = false;
1823 
1824         while (!done) {
1825             ++numIterations;
1826 
1827             if (traceDebug)
1828                 dataLog(&quot;Code at iteration &quot;, numIterations, &quot;:\n&quot;, m_code);
1829 
1830             // FIXME: One way to optimize this code is to remove the recomputation inside the fixpoint.
1831             // We need to recompute because spilling adds tmps, but we could just update tmpWidth when we
1832             // add those tmps. Note that one easy way to remove the recomputation is to make any newly
1833             // added Tmps get the same use/def widths that the original Tmp got. But, this may hurt the
1834             // spill code we emit. Since we currently recompute TmpWidth after spilling, the newly
1835             // created Tmps may get narrower use/def widths. On the other hand, the spiller already
1836             // selects which move instruction to use based on the original Tmp&#39;s widths, so it may not
1837             // matter than a subsequent iteration sees a conservative width for the new Tmps. Also, the
1838             // recomputation may not actually be a performance problem; it&#39;s likely that a better way to
1839             // improve performance of TmpWidth is to replace its HashMap with something else. It&#39;s
1840             // possible that most of the TmpWidth overhead is from queries of TmpWidth rather than the
1841             // recomputation, in which case speeding up the lookup would be a bigger win.
1842             // https://bugs.webkit.org/show_bug.cgi?id=152478
1843             m_tmpWidth.recompute(m_code);
1844 
1845             auto doAllocation = [&amp;] (auto&amp; allocator) -&gt; bool {
1846                 allocator.allocate();
1847                 if (!allocator.requiresSpilling()) {
1848                     this-&gt;assignRegistersToTmp&lt;bank&gt;(allocator);
1849                     if (traceDebug)
1850                         dataLog(&quot;Successfull allocation at iteration &quot;, numIterations, &quot;:\n&quot;, m_code);
1851 
1852                     return true;
1853                 }
1854 
1855                 this-&gt;addSpillAndFill&lt;bank&gt;(allocator, unspillableTmps);
1856                 return false;
1857             };
1858 
1859             if (useIRC()) {
1860                 ColoringAllocator&lt;bank, IRC&gt; allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
1861                 done = doAllocation(allocator);
1862             } else {
1863                 ColoringAllocator&lt;bank, Briggs&gt; allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
1864                 done = doAllocation(allocator);
1865             }
1866         }
1867         dataLogLnIf(reportStats, &quot;Num iterations = &quot;, numIterations, &quot; for bank: &quot;, bank);
1868     }
1869 
1870     template&lt;Bank bank&gt;
1871     HashSet&lt;unsigned&gt; computeUnspillableTmps()
1872     {
1873 
1874         HashSet&lt;unsigned&gt; unspillableTmps;
1875 
1876         struct Range {
1877             unsigned first { std::numeric_limits&lt;unsigned&gt;::max() };
1878             unsigned last { 0 };
1879             unsigned count { 0 };
1880             unsigned admitStackCount { 0 };
1881         };
1882 
1883         unsigned numTmps = m_code.numTmps(bank);
1884         unsigned arraySize = AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(numTmps);
1885 
1886         Vector&lt;Range, 0, UnsafeVectorOverflow&gt; ranges;
1887         ranges.fill(Range(), arraySize);
1888 
1889         unsigned globalIndex = 0;
1890         for (BasicBlock* block : m_code) {
1891             for (Inst&amp; inst : *block) {
1892                 inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role, Bank argBank, Width) {
1893                     if (arg.isTmp() &amp;&amp; inst.admitsStack(arg)) {
1894                         if (argBank != bank)
1895                             return;
1896 
1897                         Tmp tmp = arg.tmp();
1898                         Range&amp; range = ranges[AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(tmp)];
1899                         range.count++;
1900                         range.admitStackCount++;
1901                         if (globalIndex &lt; range.first) {
1902                             range.first = globalIndex;
1903                             range.last = globalIndex;
1904                         } else
1905                             range.last = globalIndex;
1906 
1907                         return;
1908                     }
1909 
1910                     arg.forEachTmpFast([&amp;] (Tmp&amp; tmp) {
1911                         if (tmp.isGP() != (bank == GP))
1912                             return;
1913 
1914                         Range&amp; range = ranges[AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(tmp)];
1915                         range.count++;
1916                         if (globalIndex &lt; range.first) {
1917                             range.first = globalIndex;
1918                             range.last = globalIndex;
1919                         } else
1920                             range.last = globalIndex;
1921                     });
1922                 });
1923 
1924                 ++globalIndex;
1925             }
1926             ++globalIndex;
1927         }
1928         for (unsigned i = AbsoluteTmpMapper&lt;bank&gt;::lastMachineRegisterIndex() + 1; i &lt; ranges.size(); ++i) {
1929             Range&amp; range = ranges[i];
1930             if (range.last - range.first &lt;= 1 &amp;&amp; range.count &gt; range.admitStackCount)
1931                 unspillableTmps.add(i);
1932         }
1933 
1934         return unspillableTmps;
1935     }
1936 
1937     template&lt;Bank bank, typename AllocatorType&gt;
1938     void assignRegistersToTmp(const AllocatorType&amp; allocator)
1939     {
1940         for (BasicBlock* block : m_code) {
1941             // Give Tmp a valid register.
1942             for (unsigned instIndex = 0; instIndex &lt; block-&gt;size(); ++instIndex) {
1943                 Inst&amp; inst = block-&gt;at(instIndex);
1944 
1945                 // The mayBeCoalescable() method will change its mind for some operations after we
1946                 // complete register allocation. So, we record this before starting.
1947                 bool mayBeCoalescable = allocator.mayBeCoalescable(inst);
1948 
1949                 // Move32 is cheaper if we know that it&#39;s equivalent to a Move. It&#39;s
1950                 // equivalent if the destination&#39;s high bits are not observable or if the source&#39;s high
1951                 // bits are all zero. Note that we don&#39;t have the opposite optimization for other
1952                 // architectures, which may prefer Move over Move32, because Move is canonical already.
1953                 if (bank == GP &amp;&amp; inst.kind.opcode == Move
1954                     &amp;&amp; inst.args[0].isTmp() &amp;&amp; inst.args[1].isTmp()) {
1955                     if (m_tmpWidth.useWidth(inst.args[1].tmp()) &lt;= Width32
1956                         || m_tmpWidth.defWidth(inst.args[0].tmp()) &lt;= Width32)
1957                         inst.kind.opcode = Move32;
1958                 }
1959 
1960                 inst.forEachTmpFast([&amp;] (Tmp&amp; tmp) {
1961                     if (tmp.isReg() || tmp.bank() != bank)
1962                         return;
1963 
1964                     Tmp aliasTmp = allocator.getAlias(tmp);
1965                     Tmp assignedTmp;
1966                     if (aliasTmp.isReg())
1967                         assignedTmp = Tmp(aliasTmp.reg());
1968                     else {
1969                         auto reg = allocator.allocatedReg(aliasTmp);
1970                         ASSERT(reg);
1971                         assignedTmp = Tmp(reg);
1972                     }
1973                     ASSERT(assignedTmp.isReg());
1974                     tmp = assignedTmp;
1975                 });
1976 
1977                 if (mayBeCoalescable &amp;&amp; inst.args[0].isTmp() &amp;&amp; inst.args[1].isTmp()
1978                     &amp;&amp; inst.args[0].tmp() == inst.args[1].tmp())
1979                     inst = Inst();
1980             }
1981 
1982             // Remove all the useless moves we created in this block.
1983             block-&gt;insts().removeAllMatching([&amp;] (const Inst&amp; inst) {
1984                 return !inst;
1985             });
1986         }
1987     }
1988 
1989     static unsigned stackSlotMinimumWidth(Width width)
1990     {
1991         return width &lt;= Width32 ? 4 : 8;
1992     }
1993 
1994     template&lt;Bank bank, typename AllocatorType&gt;
1995     void addSpillAndFill(const AllocatorType&amp; allocator, HashSet&lt;unsigned&gt;&amp; unspillableTmps)
1996     {
1997         HashMap&lt;Tmp, StackSlot*&gt; stackSlots;
1998         for (Tmp tmp : allocator.spilledTmps()) {
1999             // All the spilled values become unspillable.
2000             unspillableTmps.add(AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(tmp));
2001 
2002             // Allocate stack slot for each spilled value.
2003             StackSlot* stackSlot = m_code.addStackSlot(
2004                 stackSlotMinimumWidth(m_tmpWidth.requiredWidth(tmp)), StackSlotKind::Spill);
2005             bool isNewTmp = stackSlots.add(tmp, stackSlot).isNewEntry;
2006             ASSERT_UNUSED(isNewTmp, isNewTmp);
2007         }
2008 
2009         // Rewrite the program to get rid of the spilled Tmp.
2010         InsertionSet insertionSet(m_code);
2011         for (BasicBlock* block : m_code) {
2012             bool hasAliasedTmps = false;
2013 
2014             for (unsigned instIndex = 0; instIndex &lt; block-&gt;size(); ++instIndex) {
2015                 Inst&amp; inst = block-&gt;at(instIndex);
2016 
2017                 // The TmpWidth analysis will say that a Move only stores 32 bits into the destination,
2018                 // if the source only had 32 bits worth of non-zero bits. Same for the source: it will
2019                 // only claim to read 32 bits from the source if only 32 bits of the destination are
2020                 // read. Note that we only apply this logic if this turns into a load or store, since
2021                 // Move is the canonical way to move data between GPRs.
2022                 bool canUseMove32IfDidSpill = false;
2023                 bool didSpill = false;
2024                 bool needScratch = false;
2025                 if (bank == GP &amp;&amp; inst.kind.opcode == Move) {
2026                     if ((inst.args[0].isTmp() &amp;&amp; m_tmpWidth.width(inst.args[0].tmp()) &lt;= Width32)
2027                         || (inst.args[1].isTmp() &amp;&amp; m_tmpWidth.width(inst.args[1].tmp()) &lt;= Width32))
2028                         canUseMove32IfDidSpill = true;
2029                 }
2030 
2031                 // Try to replace the register use by memory use when possible.
2032                 inst.forEachArg(
2033                     [&amp;] (Arg&amp; arg, Arg::Role role, Bank argBank, Width width) {
2034                         if (!arg.isTmp())
2035                             return;
2036                         if (argBank != bank)
2037                             return;
2038                         if (arg.isReg())
2039                             return;
2040 
2041                         auto stackSlotEntry = stackSlots.find(arg.tmp());
2042                         if (stackSlotEntry == stackSlots.end())
2043                             return;
2044                         bool needScratchIfSpilledInPlace = false;
2045                         if (!inst.admitsStack(arg)) {
2046                             if (traceDebug)
2047                                 dataLog(&quot;Have an inst that won&#39;t admit stack: &quot;, inst, &quot;\n&quot;);
2048                             switch (inst.kind.opcode) {
2049                             case Move:
2050                             case MoveDouble:
2051                             case MoveFloat:
2052                             case Move32: {
2053                                 unsigned argIndex = &amp;arg - &amp;inst.args[0];
2054                                 unsigned otherArgIndex = argIndex ^ 1;
2055                                 Arg otherArg = inst.args[otherArgIndex];
2056                                 if (inst.args.size() == 2
2057                                     &amp;&amp; otherArg.isStack()
2058                                     &amp;&amp; otherArg.stackSlot()-&gt;isSpill()) {
2059                                     needScratchIfSpilledInPlace = true;
2060                                     break;
2061                                 }
2062                                 return;
2063                             }
2064                             default:
2065                                 return;
2066                             }
2067                         }
2068 
2069                         // If the Tmp holds a constant then we want to rematerialize its
2070                         // value rather than loading it from the stack. In order for that
2071                         // optimization to kick in, we need to avoid placing the Tmp&#39;s stack
2072                         // address into the instruction.
2073                         if (!Arg::isColdUse(role)) {
2074                             const UseCounts&lt;Tmp&gt;::Counts* counts = m_useCounts[arg.tmp()];
2075                             if (counts &amp;&amp; counts-&gt;numConstDefs == 1 &amp;&amp; counts-&gt;numDefs == 1)
2076                                 return;
2077                         }
2078 
2079                         Width spillWidth = m_tmpWidth.requiredWidth(arg.tmp());
2080                         if (Arg::isAnyDef(role) &amp;&amp; width &lt; spillWidth) {
2081                             // Either there are users of this tmp who will use more than width,
2082                             // or there are producers who will produce more than width non-zero
2083                             // bits.
2084                             // FIXME: It&#39;s not clear why we should have to return here. We have
2085                             // a ZDef fixup in allocateStack. And if this isn&#39;t a ZDef, then it
2086                             // doesn&#39;t seem like it matters what happens to the high bits. Note
2087                             // that this isn&#39;t the case where we&#39;re storing more than what the
2088                             // spill slot can hold - we already got that covered because we
2089                             // stretch the spill slot on demand. One possibility is that it&#39;s ZDefs of
2090                             // smaller width than 32-bit.
2091                             // https://bugs.webkit.org/show_bug.cgi?id=169823
2092                             return;
2093                         }
2094                         ASSERT(inst.kind.opcode == Move || !(Arg::isAnyUse(role) &amp;&amp; width &gt; spillWidth));
2095 
2096                         if (spillWidth != Width32)
2097                             canUseMove32IfDidSpill = false;
2098 
2099                         stackSlotEntry-&gt;value-&gt;ensureSize(
2100                             canUseMove32IfDidSpill ? 4 : bytes(width));
2101                         arg = Arg::stack(stackSlotEntry-&gt;value);
2102                         didSpill = true;
2103                         if (needScratchIfSpilledInPlace)
2104                             needScratch = true;
2105                     });
2106 
2107                 if (didSpill &amp;&amp; canUseMove32IfDidSpill)
2108                     inst.kind.opcode = Move32;
2109 
2110                 if (needScratch) {
2111                     Bank instBank;
2112                     switch (inst.kind.opcode) {
2113                     case Move:
2114                     case Move32:
2115                         instBank = GP;
2116                         break;
2117                     case MoveDouble:
2118                     case MoveFloat:
2119                         instBank = FP;
2120                         break;
2121                     default:
2122                         RELEASE_ASSERT_NOT_REACHED();
2123                         instBank = GP;
2124                         break;
2125                     }
2126 
2127                     RELEASE_ASSERT(instBank == bank);
2128 
2129                     Tmp tmp = m_code.newTmp(bank);
2130                     unspillableTmps.add(AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(tmp));
2131                     inst.args.append(tmp);
2132                     RELEASE_ASSERT(inst.args.size() == 3);
2133 
2134                     // Without this, a chain of spill moves would need two registers, not one, because
2135                     // the scratch registers of successive moves would appear to interfere with each
2136                     // other. As well, we need this if the previous instruction had any late effects,
2137                     // since otherwise the scratch would appear to interfere with those. On the other
2138                     // hand, the late use added at the end of this spill move (previously it was just a
2139                     // late def) doesn&#39;t change the padding situation.: the late def would have already
2140                     // caused it to report hasLateUseOrDef in Inst::needsPadding.
2141                     insertionSet.insert(instIndex, Nop, inst.origin);
2142                     continue;
2143                 }
2144 
2145                 // For every other case, add Load/Store as needed.
2146                 inst.forEachTmp([&amp;] (Tmp&amp; tmp, Arg::Role role, Bank argBank, Width) {
2147                     if (tmp.isReg() || argBank != bank)
2148                         return;
2149 
2150                     auto stackSlotEntry = stackSlots.find(tmp);
2151                     if (stackSlotEntry == stackSlots.end()) {
2152                         Tmp alias = allocator.getAliasWhenSpilling(tmp);
2153                         if (alias != tmp) {
2154                             tmp = alias;
2155                             hasAliasedTmps = true;
2156                         }
2157                         return;
2158                     }
2159 
2160                     Width spillWidth = m_tmpWidth.requiredWidth(tmp);
2161                     Opcode move = Oops;
2162                     switch (stackSlotMinimumWidth(spillWidth)) {
2163                     case 4:
2164                         move = bank == GP ? Move32 : MoveFloat;
2165                         break;
2166                     case 8:
2167                         move = bank == GP ? Move : MoveDouble;
2168                         break;
2169                     default:
2170                         RELEASE_ASSERT_NOT_REACHED();
2171                         break;
2172                     }
2173 
2174                     tmp = m_code.newTmp(bank);
2175                     unspillableTmps.add(AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(tmp));
2176 
2177                     if (role == Arg::Scratch)
2178                         return;
2179 
2180                     Arg arg = Arg::stack(stackSlotEntry-&gt;value);
2181                     if (Arg::isAnyUse(role))
2182                         insertionSet.insert(instIndex, move, inst.origin, arg, tmp);
2183                     if (Arg::isAnyDef(role))
2184                         insertionSet.insert(instIndex + 1, move, inst.origin, tmp, arg);
2185                 });
2186             }
2187             insertionSet.execute(block);
2188 
2189             if (hasAliasedTmps) {
2190                 block-&gt;insts().removeAllMatching([&amp;] (const Inst&amp; inst) {
2191                     return allocator.isUselessMove(inst);
2192                 });
2193             }
2194         }
2195     }
2196 
2197     Code&amp; m_code;
2198     TmpWidth m_tmpWidth;
2199     UseCounts&lt;Tmp&gt;&amp; m_useCounts;
2200 };
2201 
2202 } // anonymous namespace
2203 
2204 void allocateRegistersByGraphColoring(Code&amp; code)
2205 {
2206     PhaseScope phaseScope(code, &quot;allocateRegistersByGraphColoring&quot;);
2207 
2208     if (false)
2209         dataLog(&quot;Code before graph coloring:\n&quot;, code);
2210 
2211     UseCounts&lt;Tmp&gt; useCounts(code);
2212     GraphColoringRegisterAllocation graphColoringRegisterAllocation(code, useCounts);
2213     graphColoringRegisterAllocation.run();
2214 }
2215 
2216 } } } // namespace JSC::B3::Air
2217 
2218 #endif // ENABLE(B3_JIT)
    </pre>
  </body>
</html>