<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/StructureStubInfo.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2008-2020 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #include &quot;CacheableIdentifier.h&quot;
 29 #include &quot;CodeBlock.h&quot;
 30 #include &quot;CodeOrigin.h&quot;
 31 #include &quot;Instruction.h&quot;
 32 #include &quot;JITStubRoutine.h&quot;
 33 #include &quot;MacroAssembler.h&quot;
 34 #include &quot;Options.h&quot;
 35 #include &quot;RegisterSet.h&quot;
 36 #include &quot;Structure.h&quot;
 37 #include &quot;StructureSet.h&quot;
 38 #include &quot;StructureStubClearingWatchpoint.h&quot;
 39 #include &quot;StubInfoSummary.h&quot;
 40 #include &lt;wtf/Box.h&gt;
 41 
 42 namespace JSC {
 43 
 44 #if ENABLE(JIT)
 45 
 46 class AccessCase;
 47 class AccessGenerationResult;
 48 class PolymorphicAccess;
 49 
 50 enum class AccessType : int8_t {
 51     GetById,
 52     GetByIdWithThis,
 53     GetByIdDirect,
 54     TryGetById,
 55     GetByVal,
 56     Put,
 57     In,
 58     InstanceOf
 59 };
 60 
 61 enum class CacheType : int8_t {
 62     Unset,
 63     GetByIdSelf,
 64     PutByIdReplace,
 65     InByIdSelf,
 66     Stub,
 67     ArrayLength,
 68     StringLength
 69 };
 70 
 71 class StructureStubInfo {
 72     WTF_MAKE_NONCOPYABLE(StructureStubInfo);
 73     WTF_MAKE_FAST_ALLOCATED;
 74 public:
 75     StructureStubInfo(AccessType);
 76     ~StructureStubInfo();
 77 
 78     void initGetByIdSelf(CodeBlock*, Structure* baseObjectStructure, PropertyOffset, CacheableIdentifier);
 79     void initArrayLength();
 80     void initStringLength();
 81     void initPutByIdReplace(CodeBlock*, Structure* baseObjectStructure, PropertyOffset);
 82     void initInByIdSelf(CodeBlock*, Structure* baseObjectStructure, PropertyOffset);
 83 
 84     AccessGenerationResult addAccessCase(const GCSafeConcurrentJSLocker&amp;, CodeBlock*, CacheableIdentifier, std::unique_ptr&lt;AccessCase&gt;);
 85 
 86     void reset(CodeBlock*);
 87 
 88     void deref();
 89     void aboutToDie();
 90 
 91     void visitAggregate(SlotVisitor&amp;);
 92 
 93     // Check if the stub has weak references that are dead. If it does, then it resets itself,
 94     // either entirely or just enough to ensure that those dead pointers don&#39;t get used anymore.
 95     void visitWeakReferences(CodeBlock*);
 96 
 97     // This returns true if it has marked everything that it will ever mark.
 98     bool propagateTransitions(SlotVisitor&amp;);
 99 
100     ALWAYS_INLINE bool considerCaching(VM&amp; vm, CodeBlock* codeBlock, Structure* structure, UniquedStringImpl* impl = nullptr)
101     {
102         DisallowGC disallowGC;
103 
104         // We never cache non-cells.
105         if (!structure) {
106             sawNonCell = true;
107             return false;
108         }
109 
110         // This method is called from the Optimize variants of IC slow paths. The first part of this
111         // method tries to determine if the Optimize variant should really behave like the
112         // non-Optimize variant and leave the IC untouched.
113         //
114         // If we determine that we should do something to the IC then the next order of business is
115         // to determine if this Structure would impact the IC at all. We know that it won&#39;t, if we
116         // have already buffered something on its behalf. That&#39;s what the bufferedStructures set is
117         // for.
118 
119         everConsidered = true;
120         if (!countdown) {
121             // Check if we have been doing repatching too frequently. If so, then we should cool off
122             // for a while.
123             WTF::incrementWithSaturation(repatchCount);
124             if (repatchCount &gt; Options::repatchCountForCoolDown()) {
125                 // We&#39;ve been repatching too much, so don&#39;t do it now.
126                 repatchCount = 0;
127                 // The amount of time we require for cool-down depends on the number of times we&#39;ve
128                 // had to cool down in the past. The relationship is exponential. The max value we
129                 // allow here is 2^256 - 2, since the slow paths may increment the count to indicate
130                 // that they&#39;d like to temporarily skip patching just this once.
131                 countdown = WTF::leftShiftWithSaturation(
132                     static_cast&lt;uint8_t&gt;(Options::initialCoolDownCount()),
133                     numberOfCoolDowns,
134                     static_cast&lt;uint8_t&gt;(std::numeric_limits&lt;uint8_t&gt;::max() - 1));
135                 WTF::incrementWithSaturation(numberOfCoolDowns);
136 
137                 // We may still have had something buffered. Trigger generation now.
138                 bufferingCountdown = 0;
139                 return true;
140             }
141 
142             // We don&#39;t want to return false due to buffering indefinitely.
143             if (!bufferingCountdown) {
144                 // Note that when this returns true, it&#39;s possible that we will not even get an
145                 // AccessCase because this may cause Repatch.cpp to simply do an in-place
146                 // repatching.
147                 return true;
148             }
149 
150             bufferingCountdown--;
151 
152             // Now protect the IC buffering. We want to proceed only if this is a structure that
153             // we don&#39;t already have a case buffered for. Note that if this returns true but the
154             // bufferingCountdown is not zero then we will buffer the access case for later without
155             // immediately generating code for it.
156             //
157             // NOTE: This will behave oddly for InstanceOf if the user varies the prototype but not
158             // the base&#39;s structure. That seems unlikely for the canonical use of instanceof, where
159             // the prototype is fixed.
160             bool isNewlyAdded = bufferedStructures.add({ structure, impl }).isNewEntry;
161             if (isNewlyAdded)
162                 vm.heap.writeBarrier(codeBlock);
163             return isNewlyAdded;
164         }
165         countdown--;
166         return false;
167     }
168 
169     StubInfoSummary summary(VM&amp;) const;
170 
171     static StubInfoSummary summary(VM&amp;, const StructureStubInfo*);
172 
173     bool containsPC(void* pc) const;
174 
175     CodeOrigin codeOrigin;
176 private:
177     CacheableIdentifier m_getByIdSelfIdentifier;
178 public:
179 
180     union {
181         struct {
182             WriteBarrierBase&lt;Structure&gt; baseObjectStructure;
183             PropertyOffset offset;
184         } byIdSelf;
185         PolymorphicAccess* stub;
186     } u;
187 
188     CacheableIdentifier getByIdSelfIdentifier()
189     {
190         RELEASE_ASSERT(m_cacheType == CacheType::GetByIdSelf);
191         return m_getByIdSelfIdentifier;
192     }
193 
194 private:
195     // Represents those structures that already have buffered AccessCases in the PolymorphicAccess.
196     // Note that it&#39;s always safe to clear this. If we clear it prematurely, then if we see the same
197     // structure again during this buffering countdown, we will create an AccessCase object for it.
198     // That&#39;s not so bad - we&#39;ll get rid of the redundant ones once we regenerate.
199     HashSet&lt;std::pair&lt;Structure*, RefPtr&lt;UniquedStringImpl&gt;&gt;&gt; bufferedStructures;
200 public:
201 
202     CodeLocationLabel&lt;JITStubRoutinePtrTag&gt; start; // This is either the start of the inline IC for *byId caches. or the location of patchable jump for &#39;instanceof&#39; caches.
203     CodeLocationLabel&lt;JSInternalPtrTag&gt; doneLocation;
204     CodeLocationCall&lt;JSInternalPtrTag&gt; slowPathCallLocation;
205     CodeLocationLabel&lt;JITStubRoutinePtrTag&gt; slowPathStartLocation;
206 
207     RegisterSet usedRegisters;
208 
209     uint32_t inlineSize() const
210     {
211         int32_t inlineSize = MacroAssembler::differenceBetweenCodePtr(start, doneLocation);
212         ASSERT(inlineSize &gt;= 0);
213         return inlineSize;
214     }
215 
216     GPRReg baseGPR;
217     GPRReg valueGPR;
218     union {
219         GPRReg thisGPR;
220         GPRReg prototypeGPR;
221         GPRReg propertyGPR;
222     } regs;
223 #if USE(JSVALUE32_64)
224     GPRReg valueTagGPR;
225     // FIXME: [32-bits] Check if StructureStubInfo::baseTagGPR is used somewhere.
226     // https://bugs.webkit.org/show_bug.cgi?id=204726
227     GPRReg baseTagGPR;
228     union {
229         GPRReg thisTagGPR;
230         GPRReg propertyTagGPR;
231     } v;
232 #endif
233 
234     CodeLocationJump&lt;JSInternalPtrTag&gt; patchableJump()
235     {
236         ASSERT(accessType == AccessType::InstanceOf);
237         return start.jumpAtOffset&lt;JSInternalPtrTag&gt;(0);
238     }
239 
240     JSValueRegs valueRegs() const
241     {
242         return JSValueRegs(
243 #if USE(JSVALUE32_64)
244             valueTagGPR,
245 #endif
246             valueGPR);
247     }
248 
249     JSValueRegs propertyRegs() const
250     {
251         return JSValueRegs(
252 #if USE(JSVALUE32_64)
253             v.propertyTagGPR,
254 #endif
255             regs.propertyGPR);
256     }
257 
258     JSValueRegs baseRegs() const
259     {
260         return JSValueRegs(
261 #if USE(JSVALUE32_64)
262             baseTagGPR,
263 #endif
264             baseGPR);
265     }
266 
267     bool thisValueIsInThisGPR() const { return accessType == AccessType::GetByIdWithThis; }
268 
269 #if ASSERT_ENABLED
270     void checkConsistency();
271 #else
272     ALWAYS_INLINE void checkConsistency() { }
273 #endif
274 
275     AccessType accessType;
276 private:
277     CacheType m_cacheType;
278     void setCacheType(CacheType);
279 public:
280     CacheType cacheType() const { return m_cacheType; }
281     uint8_t countdown; // We repatch only when this is zero. If not zero, we decrement.
282     uint8_t repatchCount;
283     uint8_t numberOfCoolDowns;
284 
285     CallSiteIndex callSiteIndex;
286 
287     uint8_t bufferingCountdown;
288     bool resetByGC : 1;
289     bool tookSlowPath : 1;
290     bool everConsidered : 1;
291     bool prototypeIsKnownObject : 1; // Only relevant for InstanceOf.
292     bool sawNonCell : 1;
293     bool hasConstantIdentifier : 1;
294     bool propertyIsString : 1;
295     bool propertyIsInt32 : 1;
296     bool propertyIsSymbol : 1;
297 };
298 
299 inline CodeOrigin getStructureStubInfoCodeOrigin(StructureStubInfo&amp; structureStubInfo)
300 {
301     return structureStubInfo.codeOrigin;
302 }
303 
304 inline auto appropriateOptimizingGetByIdFunction(AccessType type) -&gt; decltype(&amp;operationGetByIdOptimize)
305 {
306     switch (type) {
307     case AccessType::GetById:
308         return operationGetByIdOptimize;
309     case AccessType::TryGetById:
310         return operationTryGetByIdOptimize;
311     case AccessType::GetByIdDirect:
312         return operationGetByIdDirectOptimize;
313     case AccessType::GetByIdWithThis:
314     default:
315         ASSERT_NOT_REACHED();
316         return nullptr;
317     }
318 }
319 
320 inline auto appropriateGenericGetByIdFunction(AccessType type) -&gt; decltype(&amp;operationGetByIdGeneric)
321 {
322     switch (type) {
323     case AccessType::GetById:
324         return operationGetByIdGeneric;
325     case AccessType::TryGetById:
326         return operationTryGetByIdGeneric;
327     case AccessType::GetByIdDirect:
328         return operationGetByIdDirectGeneric;
329     case AccessType::GetByIdWithThis:
330     default:
331         ASSERT_NOT_REACHED();
332         return nullptr;
333     }
334 }
335 
336 #else
337 
338 class StructureStubInfo;
339 
340 #endif // ENABLE(JIT)
341 
342 typedef HashMap&lt;CodeOrigin, StructureStubInfo*, CodeOriginApproximateHash&gt; StubInfoMap;
343 
344 } // namespace JSC
    </pre>
  </body>
</html>