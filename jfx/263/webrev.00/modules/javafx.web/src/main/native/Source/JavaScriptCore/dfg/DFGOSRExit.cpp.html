<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExit.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGOSRExit.h&quot;
 28 
 29 #if ENABLE(DFG_JIT)
 30 
 31 #include &quot;AssemblyHelpers.h&quot;
 32 #include &quot;BytecodeUseDef.h&quot;
 33 #include &quot;CheckpointOSRExitSideState.h&quot;
 34 #include &quot;ClonedArguments.h&quot;
 35 #include &quot;DFGGraph.h&quot;
 36 #include &quot;DFGMayExit.h&quot;
 37 #include &quot;DFGOSRExitCompilerCommon.h&quot;
 38 #include &quot;DFGOperations.h&quot;
 39 #include &quot;DFGSpeculativeJIT.h&quot;
 40 #include &quot;DirectArguments.h&quot;
 41 #include &quot;FrameTracers.h&quot;
 42 #include &quot;InlineCallFrame.h&quot;
 43 #include &quot;JSCInlines.h&quot;
 44 #include &quot;JSCJSValue.h&quot;
 45 #include &quot;OperandsInlines.h&quot;
 46 #include &quot;ProbeContext.h&quot;
 47 #include &quot;ProbeFrame.h&quot;
 48 
 49 namespace JSC { namespace DFG {
 50 
 51 OSRExit::OSRExit(ExitKind kind, JSValueSource jsValueSource, MethodOfGettingAValueProfile valueProfile, SpeculativeJIT* jit, unsigned streamIndex, unsigned recoveryIndex)
 52     : OSRExitBase(kind, jit-&gt;m_origin.forExit, jit-&gt;m_origin.semantic, jit-&gt;m_origin.wasHoisted)
 53     , m_jsValueSource(jsValueSource)
 54     , m_valueProfile(valueProfile)
 55     , m_recoveryIndex(recoveryIndex)
 56     , m_streamIndex(streamIndex)
 57 {
 58     bool canExit = jit-&gt;m_origin.exitOK;
 59     if (!canExit &amp;&amp; jit-&gt;m_currentNode) {
 60         ExitMode exitMode = mayExit(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode);
 61         canExit = exitMode == ExitMode::Exits || exitMode == ExitMode::ExitsForExceptions;
 62     }
 63     DFG_ASSERT(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode, canExit);
 64 }
 65 
 66 CodeLocationJump&lt;JSInternalPtrTag&gt; OSRExit::codeLocationForRepatch() const
 67 {
 68     return CodeLocationJump&lt;JSInternalPtrTag&gt;(m_patchableJumpLocation);
 69 }
 70 
 71 void OSRExit::emitRestoreArguments(CCallHelpers&amp; jit, VM&amp; vm, const Operands&lt;ValueRecovery&gt;&amp; operands)
 72 {
 73     HashMap&lt;MinifiedID, VirtualRegister&gt; alreadyAllocatedArguments; // Maps phantom arguments node ID to operand.
 74     for (size_t index = 0; index &lt; operands.size(); ++index) {
 75         const ValueRecovery&amp; recovery = operands[index];
 76 
 77         if (recovery.technique() != DirectArgumentsThatWereNotCreated
 78             &amp;&amp; recovery.technique() != ClonedArgumentsThatWereNotCreated)
 79             continue;
 80 
 81         Operand operand = operands.operandForIndex(index);
 82         if (operand.isTmp())
 83             continue;
 84 
 85         MinifiedID id = recovery.nodeID();
 86         auto iter = alreadyAllocatedArguments.find(id);
 87         if (iter != alreadyAllocatedArguments.end()) {
 88             JSValueRegs regs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT1);
 89             jit.loadValue(CCallHelpers::addressFor(iter-&gt;value), regs);
 90             jit.storeValue(regs, CCallHelpers::addressFor(operand));
 91             continue;
 92         }
 93 
 94         InlineCallFrame* inlineCallFrame =
 95             jit.codeBlock()-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG.at(id)-&gt;inlineCallFrame();
 96 
 97         int stackOffset;
 98         if (inlineCallFrame)
 99             stackOffset = inlineCallFrame-&gt;stackOffset;
100         else
101             stackOffset = 0;
102 
103         if (!inlineCallFrame || inlineCallFrame-&gt;isClosureCall) {
104             jit.loadPtr(
105                 AssemblyHelpers::addressFor(VirtualRegister(stackOffset + CallFrameSlot::callee)),
106                 GPRInfo::regT0);
107         } else {
108             jit.move(
109                 AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeRecovery.constant().asCell()),
110                 GPRInfo::regT0);
111         }
112 
113         if (!inlineCallFrame || inlineCallFrame-&gt;isVarargs()) {
114             jit.load32(
115                 AssemblyHelpers::payloadFor(VirtualRegister(stackOffset + CallFrameSlot::argumentCountIncludingThis)),
116                 GPRInfo::regT1);
117         } else {
118             jit.move(
119                 AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis),
120                 GPRInfo::regT1);
121         }
122 
123         static_assert(std::is_same&lt;decltype(operationCreateDirectArgumentsDuringExit), decltype(operationCreateClonedArgumentsDuringExit)&gt;::value, &quot;We assume these functions have the same signature below.&quot;);
124         jit.setupArguments&lt;decltype(operationCreateDirectArgumentsDuringExit)&gt;(
125             AssemblyHelpers::TrustedImmPtr(&amp;vm), AssemblyHelpers::TrustedImmPtr(inlineCallFrame), GPRInfo::regT0, GPRInfo::regT1);
126         jit.prepareCallOperation(vm);
127         switch (recovery.technique()) {
128         case DirectArgumentsThatWereNotCreated:
129             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateDirectArgumentsDuringExit)), GPRInfo::nonArgGPR0);
130             break;
131         case ClonedArgumentsThatWereNotCreated:
132             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateClonedArgumentsDuringExit)), GPRInfo::nonArgGPR0);
133             break;
134         default:
135             RELEASE_ASSERT_NOT_REACHED();
136             break;
137         }
138         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
139         jit.storeCell(GPRInfo::returnValueGPR, AssemblyHelpers::addressFor(operand));
140 
141         alreadyAllocatedArguments.add(id, operand.virtualRegister());
142     }
143 }
144 
145 void JIT_OPERATION operationCompileOSRExit(CallFrame* callFrame)
146 {
147     VM&amp; vm = callFrame-&gt;deprecatedVM();
148     auto scope = DECLARE_THROW_SCOPE(vm);
149 
150     if (validateDFGDoesGC) {
151         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
152         // code running that expects no GC.
153         vm.heap.setExpectDoesGC(true);
154     }
155 
156     if (vm.callFrameForCatch)
157         RELEASE_ASSERT(vm.callFrameForCatch == callFrame);
158 
159     CodeBlock* codeBlock = callFrame-&gt;codeBlock();
160     ASSERT(codeBlock);
161     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
162 
163     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
164     // really be profitable.
165     DeferGCForAWhile deferGC(vm.heap);
166 
167     uint32_t exitIndex = vm.osrExitIndex;
168     OSRExit&amp; exit = codeBlock-&gt;jitCode()-&gt;dfg()-&gt;osrExit[exitIndex];
169 
170     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);
171     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
172 
173     // Compute the value recoveries.
174     Operands&lt;ValueRecovery&gt; operands;
175     codeBlock-&gt;jitCode()-&gt;dfg()-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, codeBlock-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG, exit.m_streamIndex, operands);
176 
177     SpeculationRecovery* recovery = 0;
178     if (exit.m_recoveryIndex != UINT_MAX)
179         recovery = &amp;codeBlock-&gt;jitCode()-&gt;dfg()-&gt;speculationRecovery[exit.m_recoveryIndex];
180 
181     {
182         CCallHelpers jit(codeBlock);
183 
184         if (exit.m_kind == GenericUnwind) {
185             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
186             // So, we must restore our call frame and stack pointer.
187             jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
188             jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);
189         }
190         jit.addPtr(
191             CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
192             GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
193 
194         jit.jitAssertHasValidCallFrame();
195 
196         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {
197             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;
198             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
199 
200             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
201                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
202                 exit.m_kind, exit.m_kind == UncountableInvalidation);
203             jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
204         }
205 
206         OSRExit::compileExit(jit, vm, exit, operands, recovery);
207 
208         LinkBuffer patchBuffer(jit, codeBlock);
209         exit.m_code = FINALIZE_CODE_IF(
210             shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseDFGOSRExit(),
211             patchBuffer, OSRExitPtrTag,
212             &quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
213                 exitIndex, toCString(exit.m_codeOrigin).data(),
214                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
215                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
216     }
217 
218     MacroAssembler::repatchJump(exit.codeLocationForRepatch(), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
219 
220     vm.osrExitJumpDestination = exit.m_code.code().executableAddress();
221 }
222 
223 void OSRExit::compileExit(CCallHelpers&amp; jit, VM&amp; vm, const OSRExit&amp; exit, const Operands&lt;ValueRecovery&gt;&amp; operands, SpeculationRecovery* recovery)
224 {
225     jit.jitAssertTagsInPlace();
226 
227     // Pro-forma stuff.
228     if (UNLIKELY(Options::printEachOSRExit())) {
229         SpeculationFailureDebugInfo* debugInfo = new SpeculationFailureDebugInfo;
230         debugInfo-&gt;codeBlock = jit.codeBlock();
231         debugInfo-&gt;kind = exit.m_kind;
232         debugInfo-&gt;bytecodeIndex = exit.m_codeOrigin.bytecodeIndex();
233 
234         jit.debugCall(vm, operationDebugPrintSpeculationFailure, debugInfo);
235     }
236 
237     // Perform speculation recovery. This only comes into play when an operation
238     // starts mutating state before verifying the speculation it has already made.
239 
240     if (recovery) {
241         switch (recovery-&gt;type()) {
242         case SpeculativeAdd:
243             jit.sub32(recovery-&gt;src(), recovery-&gt;dest());
244 #if USE(JSVALUE64)
245             jit.or64(GPRInfo::numberTagRegister, recovery-&gt;dest());
246 #endif
247             break;
248 
249         case SpeculativeAddSelf:
250             // If A + A = A (int32_t) overflows, A can be recovered by ((static_cast&lt;int32_t&gt;(A) &gt;&gt; 1) ^ 0x8000000).
251             jit.rshift32(AssemblyHelpers::TrustedImm32(1), recovery-&gt;dest());
252             jit.xor32(AssemblyHelpers::TrustedImm32(0x80000000), recovery-&gt;dest());
253 #if USE(JSVALUE64)
254             jit.or64(GPRInfo::numberTagRegister, recovery-&gt;dest());
255 #endif
256             break;
257 
258         case SpeculativeAddImmediate:
259             jit.sub32(AssemblyHelpers::Imm32(recovery-&gt;immediate()), recovery-&gt;dest());
260 #if USE(JSVALUE64)
261             jit.or64(GPRInfo::numberTagRegister, recovery-&gt;dest());
262 #endif
263             break;
264 
265         case BooleanSpeculationCheck:
266 #if USE(JSVALUE64)
267             jit.xor64(AssemblyHelpers::TrustedImm32(JSValue::ValueFalse), recovery-&gt;dest());
268 #endif
269             break;
270 
271         default:
272             break;
273         }
274     }
275 
276     // Refine some array and/or value profile, if appropriate.
277 
278     if (!!exit.m_jsValueSource) {
279         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
280             // If the instruction that this originated from has an array profile, then
281             // refine it. If it doesn&#39;t, then do nothing. The latter could happen for
282             // hoisted checks, or checks emitted for operations that didn&#39;t have array
283             // profiling - either ops that aren&#39;t array accesses at all, or weren&#39;t
284             // known to be array acceses in the bytecode. The latter case is a FIXME
285             // while the former case is an outcome of a CheckStructure not knowing why
286             // it was emitted (could be either due to an inline cache of a property
287             // property access, or due to an array profile).
288 
289             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
290             CodeBlock* codeBlock = jit.baselineCodeBlockFor(codeOrigin);
291             if (ArrayProfile* arrayProfile = codeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex())) {
292                 const Instruction* instruction = codeBlock-&gt;instructions().at(codeOrigin.bytecodeIndex()).ptr();
293                 CCallHelpers::Jump skipProfile;
294                 if (instruction-&gt;is&lt;OpGetById&gt;()) {
295                     auto&amp; metadata = instruction-&gt;as&lt;OpGetById&gt;().metadata(codeBlock);
296                     skipProfile = jit.branch8(CCallHelpers::NotEqual, CCallHelpers::AbsoluteAddress(&amp;metadata.m_modeMetadata.mode), CCallHelpers::TrustedImm32(static_cast&lt;uint8_t&gt;(GetByIdMode::ArrayLength)));
297                 }
298 
299 #if USE(JSVALUE64)
300                 GPRReg usedRegister;
301                 if (exit.m_jsValueSource.isAddress())
302                     usedRegister = exit.m_jsValueSource.base();
303                 else
304                     usedRegister = exit.m_jsValueSource.gpr();
305 #else
306                 GPRReg usedRegister1;
307                 GPRReg usedRegister2;
308                 if (exit.m_jsValueSource.isAddress()) {
309                     usedRegister1 = exit.m_jsValueSource.base();
310                     usedRegister2 = InvalidGPRReg;
311                 } else {
312                     usedRegister1 = exit.m_jsValueSource.payloadGPR();
313                     if (exit.m_jsValueSource.hasKnownTag())
314                         usedRegister2 = InvalidGPRReg;
315                     else
316                         usedRegister2 = exit.m_jsValueSource.tagGPR();
317                 }
318 #endif
319 
320                 GPRReg scratch1;
321                 GPRReg scratch2;
322 #if USE(JSVALUE64)
323                 scratch1 = AssemblyHelpers::selectScratchGPR(usedRegister);
324                 scratch2 = AssemblyHelpers::selectScratchGPR(usedRegister, scratch1);
325 #else
326                 scratch1 = AssemblyHelpers::selectScratchGPR(usedRegister1, usedRegister2);
327                 scratch2 = AssemblyHelpers::selectScratchGPR(usedRegister1, usedRegister2, scratch1);
328 #endif
329 
330                 if (isARM64()) {
331                     jit.pushToSave(scratch1);
332                     jit.pushToSave(scratch2);
333                 } else {
334                     jit.push(scratch1);
335                     jit.push(scratch2);
336                 }
337 
338                 GPRReg value;
339                 if (exit.m_jsValueSource.isAddress()) {
340                     value = scratch1;
341                     jit.loadPtr(AssemblyHelpers::Address(exit.m_jsValueSource.asAddress()), value);
342                 } else
343                     value = exit.m_jsValueSource.payloadGPR();
344 
345                 jit.load32(AssemblyHelpers::Address(value, JSCell::structureIDOffset()), scratch1);
346                 jit.store32(scratch1, arrayProfile-&gt;addressOfLastSeenStructureID());
347 
348                 jit.load8(AssemblyHelpers::Address(value, JSCell::typeInfoTypeOffset()), scratch2);
349                 jit.sub32(AssemblyHelpers::TrustedImm32(FirstTypedArrayType), scratch2);
350                 auto notTypedArray = jit.branch32(MacroAssembler::AboveOrEqual, scratch2, AssemblyHelpers::TrustedImm32(NumberOfTypedArrayTypesExcludingDataView));
351                 jit.move(AssemblyHelpers::TrustedImmPtr(typedArrayModes), scratch1);
352                 jit.load32(AssemblyHelpers::BaseIndex(scratch1, scratch2, AssemblyHelpers::TimesFour), scratch2);
353                 auto storeArrayModes = jit.jump();
354 
355                 notTypedArray.link(&amp;jit);
356 #if USE(JSVALUE64)
357                 jit.load8(AssemblyHelpers::Address(value, JSCell::indexingTypeAndMiscOffset()), scratch1);
358 #else
359                 jit.load8(AssemblyHelpers::Address(scratch1, Structure::indexingModeIncludingHistoryOffset()), scratch1);
360 #endif
361                 jit.and32(AssemblyHelpers::TrustedImm32(IndexingModeMask), scratch1);
362                 jit.move(AssemblyHelpers::TrustedImm32(1), scratch2);
363                 jit.lshift32(scratch1, scratch2);
364                 storeArrayModes.link(&amp;jit);
365                 jit.or32(scratch2, AssemblyHelpers::AbsoluteAddress(arrayProfile-&gt;addressOfArrayModes()));
366 
367                 if (isARM64()) {
368                     jit.popToRestore(scratch2);
369                     jit.popToRestore(scratch1);
370                 } else {
371                     jit.pop(scratch2);
372                     jit.pop(scratch1);
373                 }
374 
375                 if (skipProfile.isSet())
376                     skipProfile.link(&amp;jit);
377             }
378         }
379 
380         if (MethodOfGettingAValueProfile profile = exit.m_valueProfile) {
381 #if USE(JSVALUE64)
382             if (exit.m_jsValueSource.isAddress()) {
383                 // We can&#39;t be sure that we have a spare register. So use the numberTagRegister,
384                 // since we know how to restore it.
385                 jit.load64(AssemblyHelpers::Address(exit.m_jsValueSource.asAddress()), GPRInfo::numberTagRegister);
386                 profile.emitReportValue(jit, JSValueRegs(GPRInfo::numberTagRegister));
387                 jit.move(AssemblyHelpers::TrustedImm64(JSValue::NumberTag), GPRInfo::numberTagRegister);
388             } else
389                 profile.emitReportValue(jit, JSValueRegs(exit.m_jsValueSource.gpr()));
390 #else // not USE(JSVALUE64)
391             if (exit.m_jsValueSource.isAddress()) {
392                 // Save a register so we can use it.
393                 GPRReg scratchPayload = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base());
394                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base(), scratchPayload);
395                 jit.pushToSave(scratchPayload);
396                 jit.pushToSave(scratchTag);
397 
398                 JSValueRegs scratch(scratchTag, scratchPayload);
399 
400                 jit.loadValue(exit.m_jsValueSource.asAddress(), scratch);
401                 profile.emitReportValue(jit, scratch);
402 
403                 jit.popToRestore(scratchTag);
404                 jit.popToRestore(scratchPayload);
405             } else if (exit.m_jsValueSource.hasKnownTag()) {
406                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.payloadGPR());
407                 jit.pushToSave(scratchTag);
408                 jit.move(AssemblyHelpers::TrustedImm32(exit.m_jsValueSource.tag()), scratchTag);
409                 JSValueRegs value(scratchTag, exit.m_jsValueSource.payloadGPR());
410                 profile.emitReportValue(jit, value);
411                 jit.popToRestore(scratchTag);
412             } else
413                 profile.emitReportValue(jit, exit.m_jsValueSource.regs());
414 #endif // USE(JSVALUE64)
415         }
416     }
417 
418     // What follows is an intentionally simple OSR exit implementation that generates
419     // fairly poor code but is very easy to hack. In particular, it dumps all state that
420     // needs conversion into a scratch buffer so that in step 6, where we actually do the
421     // conversions, we know that all temp registers are free to use and the variable is
422     // definitely in a well-known spot in the scratch buffer regardless of whether it had
423     // originally been in a register or spilled. This allows us to decouple &quot;where was
424     // the variable&quot; from &quot;how was it represented&quot;. Consider that the
425     // Int32DisplacedInJSStack recovery: it tells us that the value is in a
426     // particular place and that that place holds an unboxed int32. We have two different
427     // places that a value could be (displaced, register) and a bunch of different
428     // ways of representing a value. The number of recoveries is two * a bunch. The code
429     // below means that we have to have two + a bunch cases rather than two * a bunch.
430     // Once we have loaded the value from wherever it was, the reboxing is the same
431     // regardless of its location. Likewise, before we do the reboxing, the way we get to
432     // the value (i.e. where we load it from) is the same regardless of its type. Because
433     // the code below always dumps everything into a scratch buffer first, the two
434     // questions become orthogonal, which simplifies adding new types and adding new
435     // locations.
436     //
437     // This raises the question: does using such a suboptimal implementation of OSR exit,
438     // where we always emit code to dump all state into a scratch buffer only to then
439     // dump it right back into the stack, hurt us in any way? The asnwer is that OSR exits
440     // are rare. Our tiering strategy ensures this. This is because if an OSR exit is
441     // taken more than ~100 times, we jettison the DFG code block along with all of its
442     // exits. It is impossible for an OSR exit - i.e. the code we compile below - to
443     // execute frequently enough for the codegen to matter that much. It probably matters
444     // enough that we don&#39;t want to turn this into some super-slow function call, but so
445     // long as we&#39;re generating straight-line code, that code can be pretty bad. Also
446     // because we tend to exit only along one OSR exit from any DFG code block - that&#39;s an
447     // empirical result that we&#39;re extremely confident about - the code size of this
448     // doesn&#39;t matter much. Hence any attempt to optimize the codegen here is just purely
449     // harmful to the system: it probably won&#39;t reduce either net memory usage or net
450     // execution time. It will only prevent us from cleanly decoupling &quot;where was the
451     // variable&quot; from &quot;how was it represented&quot;, which will make it more difficult to add
452     // features in the future and it will make it harder to reason about bugs.
453 
454     // Save all state from GPRs into the scratch buffer.
455 
456     ScratchBuffer* scratchBuffer = vm.scratchBufferForSize(sizeof(EncodedJSValue) * operands.size());
457     EncodedJSValue* scratch = scratchBuffer ? static_cast&lt;EncodedJSValue*&gt;(scratchBuffer-&gt;dataBuffer()) : 0;
458 
459     for (size_t index = 0; index &lt; operands.size(); ++index) {
460         const ValueRecovery&amp; recovery = operands[index];
461 
462         switch (recovery.technique()) {
463         case UnboxedInt32InGPR:
464         case UnboxedCellInGPR:
465 #if USE(JSVALUE64)
466         case InGPR:
467         case UnboxedInt52InGPR:
468         case UnboxedStrictInt52InGPR:
469             jit.store64(recovery.gpr(), scratch + index);
470             break;
471 #else
472         case UnboxedBooleanInGPR:
473             jit.store32(
474                 recovery.gpr(),
475                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload);
476             break;
477 
478         case InPair:
479             jit.store32(
480                 recovery.tagGPR(),
481                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag);
482             jit.store32(
483                 recovery.payloadGPR(),
484                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload);
485             break;
486 #endif
487 
488         default:
489             break;
490         }
491     }
492 
493     // And voila, all GPRs are free to reuse.
494 
495     // Save all state from FPRs into the scratch buffer.
496 
497     for (size_t index = 0; index &lt; operands.size(); ++index) {
498         const ValueRecovery&amp; recovery = operands[index];
499 
500         switch (recovery.technique()) {
501         case UnboxedDoubleInFPR:
502         case InFPR:
503             jit.move(AssemblyHelpers::TrustedImmPtr(scratch + index), GPRInfo::regT0);
504             jit.storeDouble(recovery.fpr(), MacroAssembler::Address(GPRInfo::regT0));
505             break;
506 
507         default:
508             break;
509         }
510     }
511 
512     // Now, all FPRs are also free.
513 
514     // Save all state from the stack into the scratch buffer. For simplicity we
515     // do this even for state that&#39;s already in the right place on the stack.
516     // It makes things simpler later.
517 
518     for (size_t index = 0; index &lt; operands.size(); ++index) {
519         const ValueRecovery&amp; recovery = operands[index];
520 
521         switch (recovery.technique()) {
522         case DisplacedInJSStack:
523         case CellDisplacedInJSStack:
524         case BooleanDisplacedInJSStack:
525         case Int32DisplacedInJSStack:
526         case DoubleDisplacedInJSStack:
527 #if USE(JSVALUE64)
528         case Int52DisplacedInJSStack:
529         case StrictInt52DisplacedInJSStack:
530             jit.load64(AssemblyHelpers::addressFor(recovery.virtualRegister()), GPRInfo::regT0);
531             jit.store64(GPRInfo::regT0, scratch + index);
532             break;
533 #else
534             jit.load32(
535                 AssemblyHelpers::tagFor(recovery.virtualRegister()),
536                 GPRInfo::regT0);
537             jit.load32(
538                 AssemblyHelpers::payloadFor(recovery.virtualRegister()),
539                 GPRInfo::regT1);
540             jit.store32(
541                 GPRInfo::regT0,
542                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag);
543             jit.store32(
544                 GPRInfo::regT1,
545                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload);
546             break;
547 #endif
548 
549         default:
550             break;
551         }
552     }
553 
554     if (validateDFGDoesGC) {
555         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
556         // code running that expects no GC. We need to set this before arguments
557         // materialization below (see emitRestoreArguments()).
558 
559         // Even though we set Heap::m_expectDoesGC in compileOSRExit(), we also need
560         // to set it here because compileOSRExit() is only called on the first time
561         // we exit from this site, but all subsequent exits will take this compiled
562         // ramp without calling compileOSRExit() first.
563         jit.store8(CCallHelpers::TrustedImm32(true), vm.heap.addressOfExpectDoesGC());
564     }
565 
566     // Need to ensure that the stack pointer accounts for the worst-case stack usage at exit. This
567     // could toast some stack that the DFG used. We need to do it before storing to stack offsets
568     // used by baseline.
569     jit.addPtr(
570         CCallHelpers::TrustedImm32(
571             -jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit * sizeof(Register)),
572         CCallHelpers::framePointerRegister, CCallHelpers::stackPointerRegister);
573 
574     // Restore the DFG callee saves and then save the ones the baseline JIT uses.
575     jit.emitRestoreCalleeSaves();
576     jit.emitSaveCalleeSavesFor(jit.baselineCodeBlock());
577 
578     // The tag registers are needed to materialize recoveries below.
579     jit.emitMaterializeTagCheckRegisters();
580 
581     if (exit.isExceptionHandler())
582         jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
583 
584     if (exit.m_codeOrigin.inlineStackContainsActiveCheckpoint()) {
585         // FIXME: Maybe we shouldn&#39;t use a probe but filling all the side state objects is tricky otherwise...
586         Vector&lt;ValueRecovery&gt; values(operands.numberOfTmps());
587         for (size_t i = 0; i &lt; operands.numberOfTmps(); ++i)
588             values[i] = operands.tmp(i);
589 
590         VM* vmPtr = &amp;vm;
591         auto* tmpScratch = scratch + operands.tmpIndex(0);
592         jit.probe([=, values = WTFMove(values)] (Probe::Context&amp; context) {
593             auto addSideState = [&amp;] (CallFrame* frame, BytecodeIndex index, size_t tmpOffset) {
594                 std::unique_ptr&lt;CheckpointOSRExitSideState&gt; sideState = WTF::makeUnique&lt;CheckpointOSRExitSideState&gt;();
595 
596                 sideState-&gt;bytecodeIndex = index;
597                 for (size_t i = 0; i &lt; maxNumCheckpointTmps; ++i) {
598                     auto&amp; recovery = values[i + tmpOffset];
599                     // FIXME: We should do what the FTL does and materialize all the JSValues into the scratch buffer.
600                     switch (recovery.technique()) {
601                     case Constant:
602                         sideState-&gt;tmps[i] = recovery.constant();
603                         break;
604 
605                     case UnboxedInt32InGPR:
606                     case Int32DisplacedInJSStack: {
607                         sideState-&gt;tmps[i] = jsNumber(static_cast&lt;int32_t&gt;(tmpScratch[i + tmpOffset]));
608                         break;
609                     }
610 
611 #if USE(JSVALUE32_64)
612                     case InPair:
613 #endif
614                     case InGPR:
615                     case BooleanDisplacedInJSStack:
616                     case CellDisplacedInJSStack:
617                     case DisplacedInJSStack: {
618                         sideState-&gt;tmps[i] = reinterpret_cast&lt;JSValue*&gt;(tmpScratch)[i + tmpOffset];
619                         break;
620                     }
621 
622                     case UnboxedCellInGPR: {
623 #if USE(JSVALUE64)
624                         sideState-&gt;tmps[i] = reinterpret_cast&lt;JSValue*&gt;(tmpScratch)[i + tmpOffset];
625 #else
626                         EncodedValueDescriptor* valueDescriptor = bitwise_cast&lt;EncodedValueDescriptor*&gt;(tmpScratch + i + tmpOffset);
627                         sideState-&gt;tmps[i] = JSValue(JSValue::CellTag, valueDescriptor-&gt;asBits.payload);
628 #endif
629                         break;
630                     }
631 
632                     case UnboxedBooleanInGPR: {
633                         sideState-&gt;tmps[i] = jsBoolean(static_cast&lt;bool&gt;(tmpScratch[i + tmpOffset]));
634                         break;
635                     }
636 
637                     default:
638                         RELEASE_ASSERT_NOT_REACHED();
639                         break;
640                     }
641                 }
642 
643                 vmPtr-&gt;addCheckpointOSRSideState(frame, WTFMove(sideState));
644             };
645 
646             const CodeOrigin* codeOrigin;
647             CallFrame* callFrame = context.gpr&lt;CallFrame*&gt;(GPRInfo::callFrameRegister);
648             for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {
649                 BytecodeIndex callBytecodeIndex = codeOrigin-&gt;bytecodeIndex();
650                 if (!callBytecodeIndex.checkpoint())
651                     continue;
652 
653                 auto* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();
654                 addSideState(reinterpret_cast_ptr&lt;CallFrame*&gt;(reinterpret_cast&lt;char*&gt;(callFrame) + inlineCallFrame-&gt;returnPCOffset() - sizeof(CPURegister)), callBytecodeIndex, inlineCallFrame-&gt;tmpOffset);
655             }
656 
657             if (!codeOrigin)
658                 return;
659 
660             if (BytecodeIndex bytecodeIndex = codeOrigin-&gt;bytecodeIndex(); bytecodeIndex.checkpoint())
661                 addSideState(callFrame, bytecodeIndex, 0);
662         });
663     }
664 
665     // Do all data format conversions and store the results into the stack.
666 
667     for (size_t index = 0; index &lt; operands.size(); ++index) {
668         const ValueRecovery&amp; recovery = operands[index];
669         Operand operand = operands.operandForIndex(index);
670         if (operand.isTmp())
671             continue;
672 
673         if (operand.isLocal() &amp;&amp; operand.toLocal() &lt; static_cast&lt;int&gt;(jit.baselineCodeBlock()-&gt;calleeSaveSpaceAsVirtualRegisters()))
674             continue;
675 
676         switch (recovery.technique()) {
677         case DisplacedInJSStack:
678         case InFPR:
679 #if USE(JSVALUE64)
680         case InGPR:
681         case UnboxedCellInGPR:
682         case CellDisplacedInJSStack:
683         case BooleanDisplacedInJSStack:
684             jit.load64(scratch + index, GPRInfo::regT0);
685             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
686             break;
687 #else // not USE(JSVALUE64)
688         case InPair:
689             jit.load32(
690                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag,
691                 GPRInfo::regT0);
692             jit.load32(
693                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
694                 GPRInfo::regT1);
695             jit.store32(
696                 GPRInfo::regT0,
697                 AssemblyHelpers::tagFor(operand));
698             jit.store32(
699                 GPRInfo::regT1,
700                 AssemblyHelpers::payloadFor(operand));
701             break;
702 
703         case UnboxedCellInGPR:
704         case CellDisplacedInJSStack:
705             jit.load32(
706                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
707                 GPRInfo::regT0);
708             jit.store32(
709                 AssemblyHelpers::TrustedImm32(JSValue::CellTag),
710                 AssemblyHelpers::tagFor(operand));
711             jit.store32(
712                 GPRInfo::regT0,
713                 AssemblyHelpers::payloadFor(operand));
714             break;
715 
716         case UnboxedBooleanInGPR:
717         case BooleanDisplacedInJSStack:
718             jit.load32(
719                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
720                 GPRInfo::regT0);
721             jit.store32(
722                 AssemblyHelpers::TrustedImm32(JSValue::BooleanTag),
723                 AssemblyHelpers::tagFor(operand));
724             jit.store32(
725                 GPRInfo::regT0,
726                 AssemblyHelpers::payloadFor(operand));
727             break;
728 #endif // USE(JSVALUE64)
729 
730         case UnboxedInt32InGPR:
731         case Int32DisplacedInJSStack:
732 #if USE(JSVALUE64)
733             jit.load64(scratch + index, GPRInfo::regT0);
734             jit.zeroExtend32ToPtr(GPRInfo::regT0, GPRInfo::regT0);
735             jit.or64(GPRInfo::numberTagRegister, GPRInfo::regT0);
736             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
737 #else
738             jit.load32(
739                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
740                 GPRInfo::regT0);
741             jit.store32(
742                 AssemblyHelpers::TrustedImm32(JSValue::Int32Tag),
743                 AssemblyHelpers::tagFor(operand));
744             jit.store32(
745                 GPRInfo::regT0,
746                 AssemblyHelpers::payloadFor(operand));
747 #endif
748             break;
749 
750 #if USE(JSVALUE64)
751         case UnboxedInt52InGPR:
752         case Int52DisplacedInJSStack:
753             jit.load64(scratch + index, GPRInfo::regT0);
754             jit.rshift64(
755                 AssemblyHelpers::TrustedImm32(JSValue::int52ShiftAmount), GPRInfo::regT0);
756             jit.boxInt52(GPRInfo::regT0, GPRInfo::regT0, GPRInfo::regT1, FPRInfo::fpRegT0);
757             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
758             break;
759 
760         case UnboxedStrictInt52InGPR:
761         case StrictInt52DisplacedInJSStack:
762             jit.load64(scratch + index, GPRInfo::regT0);
763             jit.boxInt52(GPRInfo::regT0, GPRInfo::regT0, GPRInfo::regT1, FPRInfo::fpRegT0);
764             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
765             break;
766 #endif
767 
768         case UnboxedDoubleInFPR:
769         case DoubleDisplacedInJSStack:
770             jit.move(AssemblyHelpers::TrustedImmPtr(scratch + index), GPRInfo::regT0);
771             jit.loadDouble(MacroAssembler::Address(GPRInfo::regT0), FPRInfo::fpRegT0);
772             jit.purifyNaN(FPRInfo::fpRegT0);
773 #if USE(JSVALUE64)
774             jit.boxDouble(FPRInfo::fpRegT0, GPRInfo::regT0);
775             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
776 #else
777             jit.storeDouble(FPRInfo::fpRegT0, AssemblyHelpers::addressFor(operand));
778 #endif
779             break;
780 
781         case Constant:
782 #if USE(JSVALUE64)
783             jit.store64(
784                 AssemblyHelpers::TrustedImm64(JSValue::encode(recovery.constant())),
785                 AssemblyHelpers::addressFor(operand));
786 #else
787             jit.store32(
788                 AssemblyHelpers::TrustedImm32(recovery.constant().tag()),
789                 AssemblyHelpers::tagFor(operand));
790             jit.store32(
791                 AssemblyHelpers::TrustedImm32(recovery.constant().payload()),
792                 AssemblyHelpers::payloadFor(operand));
793 #endif
794             break;
795 
796         case DirectArgumentsThatWereNotCreated:
797         case ClonedArgumentsThatWereNotCreated:
798             // Don&#39;t do this, yet.
799             break;
800 
801         default:
802             RELEASE_ASSERT_NOT_REACHED();
803             break;
804         }
805     }
806 
807     // Now that things on the stack are recovered, do the arguments recovery. We assume that arguments
808     // recoveries don&#39;t recursively refer to each other. But, we don&#39;t try to assume that they only
809     // refer to certain ranges of locals. Hence why we need to do this here, once the stack is sensible.
810     // Note that we also roughly assume that the arguments might still be materialized outside of its
811     // inline call frame scope - but for now the DFG wouldn&#39;t do that.
812 
813     emitRestoreArguments(jit, vm, operands);
814 
815     // Adjust the old JIT&#39;s execute counter. Since we are exiting OSR, we know
816     // that all new calls into this code will go to the new JIT, so the execute
817     // counter only affects call frames that performed OSR exit and call frames
818     // that were still executing the old JIT at the time of another call frame&#39;s
819     // OSR exit. We want to ensure that the following is true:
820     //
821     // (a) Code the performs an OSR exit gets a chance to reenter optimized
822     //     code eventually, since optimized code is faster. But we don&#39;t
823     //     want to do such reentery too aggressively (see (c) below).
824     //
825     // (b) If there is code on the call stack that is still running the old
826     //     JIT&#39;s code and has never OSR&#39;d, then it should get a chance to
827     //     perform OSR entry despite the fact that we&#39;ve exited.
828     //
829     // (c) Code the performs an OSR exit should not immediately retry OSR
830     //     entry, since both forms of OSR are expensive. OSR entry is
831     //     particularly expensive.
832     //
833     // (d) Frequent OSR failures, even those that do not result in the code
834     //     running in a hot loop, result in recompilation getting triggered.
835     //
836     // To ensure (c), we&#39;d like to set the execute counter to
837     // counterValueForOptimizeAfterWarmUp(). This seems like it would endanger
838     // (a) and (b), since then every OSR exit would delay the opportunity for
839     // every call frame to perform OSR entry. Essentially, if OSR exit happens
840     // frequently and the function has few loops, then the counter will never
841     // become non-negative and OSR entry will never be triggered. OSR entry
842     // will only happen if a loop gets hot in the old JIT, which does a pretty
843     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
844     // since each speculation failure would reset the execute counter.
845     // So we check here if the number of speculation failures is significantly
846     // larger than the number of successes (we want 90% success rate), and if
847     // there have been a large enough number of failures. If so, we set the
848     // counter to 0; otherwise we set the counter to
849     // counterValueForOptimizeAfterWarmUp().
850 
851     handleExitCounts(vm, jit, exit);
852 
853     // Reify inlined call frames.
854 
855     reifyInlinedCallFrames(jit, exit);
856 
857     // And finish.
858     adjustAndJumpToTarget(vm, jit, exit);
859 }
860 
861 void JIT_OPERATION operationDebugPrintSpeculationFailure(CallFrame* callFrame, void* debugInfoRaw, void* scratch)
862 {
863     VM&amp; vm = callFrame-&gt;deprecatedVM();
864     NativeCallFrameTracer tracer(vm, callFrame);
865 
866     SpeculationFailureDebugInfo* debugInfo = static_cast&lt;SpeculationFailureDebugInfo*&gt;(debugInfoRaw);
867     CodeBlock* codeBlock = debugInfo-&gt;codeBlock;
868     CodeBlock* alternative = codeBlock-&gt;alternative();
869     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
870     dataLog(&quot; @ exit #&quot;, vm.osrExitIndex, &quot; (&quot;, debugInfo-&gt;bytecodeIndex, &quot;, &quot;, exitKindToString(debugInfo-&gt;kind), &quot;) with &quot;);
871     if (alternative) {
872         dataLog(
873             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
874             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
875             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
876     } else
877         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
878     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
879     dataLog(&quot;    GPRs at time of exit:&quot;);
880     char* scratchPointer = static_cast&lt;char*&gt;(scratch);
881     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
882         GPRReg gpr = GPRInfo::toRegister(i);
883         dataLog(&quot; &quot;, GPRInfo::debugName(gpr), &quot;:&quot;, RawPointer(*reinterpret_cast_ptr&lt;void**&gt;(scratchPointer)));
884         scratchPointer += sizeof(EncodedJSValue);
885     }
886     dataLog(&quot;\n&quot;);
887     dataLog(&quot;    FPRs at time of exit:&quot;);
888     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
889         FPRReg fpr = FPRInfo::toRegister(i);
890         dataLog(&quot; &quot;, FPRInfo::debugName(fpr), &quot;:&quot;);
891         uint64_t bits = *reinterpret_cast_ptr&lt;uint64_t*&gt;(scratchPointer);
892         double value = *reinterpret_cast_ptr&lt;double*&gt;(scratchPointer);
893         dataLogF(&quot;%llx:%lf&quot;, static_cast&lt;long long&gt;(bits), value);
894         scratchPointer += sizeof(EncodedJSValue);
895     }
896     dataLog(&quot;\n&quot;);
897 }
898 
899 } } // namespace JSC::DFG
900 
901 #endif // ENABLE(DFG_JIT)
    </pre>
  </body>
</html>