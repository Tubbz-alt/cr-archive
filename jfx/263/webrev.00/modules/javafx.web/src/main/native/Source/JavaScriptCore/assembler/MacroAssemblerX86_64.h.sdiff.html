<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/MacroAssemblerX86_64.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="MacroAssemblerX86Common.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ProbeFrame.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/MacroAssemblerX86_64.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2008-2018 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(ASSEMBLER) &amp;&amp; CPU(X86_64)
  29 
  30 #include &quot;MacroAssemblerX86Common.h&quot;
  31 
  32 #define REPATCH_OFFSET_CALL_R11 3
  33 
  34 inline bool CAN_SIGN_EXTEND_32_64(int64_t value) { return value == (int64_t)(int32_t)value; }
  35 
  36 namespace JSC {
  37 
  38 class MacroAssemblerX86_64 : public MacroAssemblerX86Common {
  39 public:
<span class="line-modified">  40     static const unsigned numGPRs = 16;</span>
<span class="line-modified">  41     static const unsigned numFPRs = 16;</span>
  42 
<span class="line-modified">  43     static const Scale ScalePtr = TimesEight;</span>
  44 
  45     using MacroAssemblerX86Common::add32;
  46     using MacroAssemblerX86Common::and32;
  47     using MacroAssemblerX86Common::branch32;
  48     using MacroAssemblerX86Common::branchAdd32;
  49     using MacroAssemblerX86Common::or32;

  50     using MacroAssemblerX86Common::sub32;
  51     using MacroAssemblerX86Common::load8;
  52     using MacroAssemblerX86Common::load32;
  53     using MacroAssemblerX86Common::store32;
  54     using MacroAssemblerX86Common::store8;
  55     using MacroAssemblerX86Common::call;
  56     using MacroAssemblerX86Common::jump;
  57     using MacroAssemblerX86Common::farJump;
  58     using MacroAssemblerX86Common::addDouble;
  59     using MacroAssemblerX86Common::loadDouble;
  60     using MacroAssemblerX86Common::convertInt32ToDouble;
  61 
  62     void add32(TrustedImm32 imm, AbsoluteAddress address)
  63     {
  64         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  65         add32(imm, Address(scratchRegister()));
  66     }
  67 
  68     void and32(TrustedImm32 imm, AbsoluteAddress address)
  69     {
</pre>
<hr />
<pre>
  72     }
  73 
  74     void add32(AbsoluteAddress address, RegisterID dest)
  75     {
  76         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  77         add32(Address(scratchRegister()), dest);
  78     }
  79 
  80     void or32(TrustedImm32 imm, AbsoluteAddress address)
  81     {
  82         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  83         or32(imm, Address(scratchRegister()));
  84     }
  85 
  86     void or32(RegisterID reg, AbsoluteAddress address)
  87     {
  88         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  89         or32(reg, Address(scratchRegister()));
  90     }
  91 






  92     void sub32(TrustedImm32 imm, AbsoluteAddress address)
  93     {
  94         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  95         sub32(imm, Address(scratchRegister()));
  96     }
  97 
  98     void load8(const void* address, RegisterID dest)
  99     {
 100         move(TrustedImmPtr(address), dest);
 101         load8(dest, dest);
 102     }
 103 
 104     void load16(ExtendedAddress address, RegisterID dest)
 105     {
 106         TrustedImmPtr addr(reinterpret_cast&lt;void*&gt;(address.offset));
 107         MacroAssemblerX86Common::move(addr, scratchRegister());
 108         MacroAssemblerX86Common::load16(BaseIndex(scratchRegister(), address.base, TimesTwo), dest);
 109     }
 110 
 111     void load16(BaseIndex address, RegisterID dest)
</pre>
<hr />
<pre>
 224         store64(scratchRegister(), Address(X86Registers::esp, -4 * static_cast&lt;int32_t&gt;(sizeof(int64_t))));
 225 
 226         // Copy argument 6
 227         load64(Address(X86Registers::esp, 5 * sizeof(int64_t)), scratchRegister());
 228         store64(scratchRegister(), Address(X86Registers::esp, -3 * static_cast&lt;int32_t&gt;(sizeof(int64_t))));
 229 
 230         // We also need to allocate the shadow space on the stack for the 4 parameter registers.
 231         // Also, we should allocate 16 bytes for the frame pointer, and return address (not populated).
 232         // In addition, we need to allocate 16 bytes for two more parameters, since the call can have up to 6 parameters.
 233         sub64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 234 #endif
 235         DataLabelPtr label = moveWithPatch(TrustedImmPtr(nullptr), scratchRegister());
 236         Call result = Call(m_assembler.call(scratchRegister()), Call::Linkable);
 237 #if OS(WINDOWS)
 238         add64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 239 #endif
 240         ASSERT_UNUSED(label, differenceBetween(label, result) == REPATCH_OFFSET_CALL_R11);
 241         return result;
 242     }
 243 






 244     ALWAYS_INLINE Call call(RegisterID callTag) { return UNUSED_PARAM(callTag), call(NoPtrTag); }
 245 
 246     // Address is a memory location containing the address to jump to
 247     void farJump(AbsoluteAddress address, PtrTag tag)
 248     {
 249         move(TrustedImmPtr(address.m_ptr), scratchRegister());
 250         farJump(Address(scratchRegister()), tag);
 251     }
 252 
 253     ALWAYS_INLINE void farJump(AbsoluteAddress address, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), farJump(address, NoPtrTag); }
 254 
 255     Call threadSafePatchableNearCall()
 256     {
 257         const size_t nearCallOpcodeSize = 1;
 258         const size_t nearCallRelativeLocationSize = sizeof(int32_t);
 259         // We want to make sure the 32-bit near call immediate is 32-bit aligned.
 260         size_t codeSize = m_assembler.codeSize();
 261         size_t alignedSize = WTF::roundUpToMultipleOf&lt;nearCallRelativeLocationSize&gt;(codeSize + nearCallOpcodeSize);
 262         emitNops(alignedSize - (codeSize + nearCallOpcodeSize));
 263         DataLabelPtr label = DataLabelPtr(this);
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(ASSEMBLER) &amp;&amp; CPU(X86_64)
  29 
  30 #include &quot;MacroAssemblerX86Common.h&quot;
  31 
  32 #define REPATCH_OFFSET_CALL_R11 3
  33 
  34 inline bool CAN_SIGN_EXTEND_32_64(int64_t value) { return value == (int64_t)(int32_t)value; }
  35 
  36 namespace JSC {
  37 
  38 class MacroAssemblerX86_64 : public MacroAssemblerX86Common {
  39 public:
<span class="line-modified">  40     static constexpr unsigned numGPRs = 16;</span>
<span class="line-modified">  41     static constexpr unsigned numFPRs = 16;</span>
  42 
<span class="line-modified">  43     static constexpr Scale ScalePtr = TimesEight;</span>
  44 
  45     using MacroAssemblerX86Common::add32;
  46     using MacroAssemblerX86Common::and32;
  47     using MacroAssemblerX86Common::branch32;
  48     using MacroAssemblerX86Common::branchAdd32;
  49     using MacroAssemblerX86Common::or32;
<span class="line-added">  50     using MacroAssemblerX86Common::or16;</span>
  51     using MacroAssemblerX86Common::sub32;
  52     using MacroAssemblerX86Common::load8;
  53     using MacroAssemblerX86Common::load32;
  54     using MacroAssemblerX86Common::store32;
  55     using MacroAssemblerX86Common::store8;
  56     using MacroAssemblerX86Common::call;
  57     using MacroAssemblerX86Common::jump;
  58     using MacroAssemblerX86Common::farJump;
  59     using MacroAssemblerX86Common::addDouble;
  60     using MacroAssemblerX86Common::loadDouble;
  61     using MacroAssemblerX86Common::convertInt32ToDouble;
  62 
  63     void add32(TrustedImm32 imm, AbsoluteAddress address)
  64     {
  65         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  66         add32(imm, Address(scratchRegister()));
  67     }
  68 
  69     void and32(TrustedImm32 imm, AbsoluteAddress address)
  70     {
</pre>
<hr />
<pre>
  73     }
  74 
  75     void add32(AbsoluteAddress address, RegisterID dest)
  76     {
  77         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  78         add32(Address(scratchRegister()), dest);
  79     }
  80 
  81     void or32(TrustedImm32 imm, AbsoluteAddress address)
  82     {
  83         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  84         or32(imm, Address(scratchRegister()));
  85     }
  86 
  87     void or32(RegisterID reg, AbsoluteAddress address)
  88     {
  89         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  90         or32(reg, Address(scratchRegister()));
  91     }
  92 
<span class="line-added">  93     void or16(TrustedImm32 imm, AbsoluteAddress address)</span>
<span class="line-added">  94     {</span>
<span class="line-added">  95         move(TrustedImmPtr(address.m_ptr), scratchRegister());</span>
<span class="line-added">  96         or16(imm, Address(scratchRegister()));</span>
<span class="line-added">  97     }</span>
<span class="line-added">  98 </span>
  99     void sub32(TrustedImm32 imm, AbsoluteAddress address)
 100     {
 101         move(TrustedImmPtr(address.m_ptr), scratchRegister());
 102         sub32(imm, Address(scratchRegister()));
 103     }
 104 
 105     void load8(const void* address, RegisterID dest)
 106     {
 107         move(TrustedImmPtr(address), dest);
 108         load8(dest, dest);
 109     }
 110 
 111     void load16(ExtendedAddress address, RegisterID dest)
 112     {
 113         TrustedImmPtr addr(reinterpret_cast&lt;void*&gt;(address.offset));
 114         MacroAssemblerX86Common::move(addr, scratchRegister());
 115         MacroAssemblerX86Common::load16(BaseIndex(scratchRegister(), address.base, TimesTwo), dest);
 116     }
 117 
 118     void load16(BaseIndex address, RegisterID dest)
</pre>
<hr />
<pre>
 231         store64(scratchRegister(), Address(X86Registers::esp, -4 * static_cast&lt;int32_t&gt;(sizeof(int64_t))));
 232 
 233         // Copy argument 6
 234         load64(Address(X86Registers::esp, 5 * sizeof(int64_t)), scratchRegister());
 235         store64(scratchRegister(), Address(X86Registers::esp, -3 * static_cast&lt;int32_t&gt;(sizeof(int64_t))));
 236 
 237         // We also need to allocate the shadow space on the stack for the 4 parameter registers.
 238         // Also, we should allocate 16 bytes for the frame pointer, and return address (not populated).
 239         // In addition, we need to allocate 16 bytes for two more parameters, since the call can have up to 6 parameters.
 240         sub64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 241 #endif
 242         DataLabelPtr label = moveWithPatch(TrustedImmPtr(nullptr), scratchRegister());
 243         Call result = Call(m_assembler.call(scratchRegister()), Call::Linkable);
 244 #if OS(WINDOWS)
 245         add64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 246 #endif
 247         ASSERT_UNUSED(label, differenceBetween(label, result) == REPATCH_OFFSET_CALL_R11);
 248         return result;
 249     }
 250 
<span class="line-added"> 251     void callOperation(const FunctionPtr&lt;OperationPtrTag&gt; operation)</span>
<span class="line-added"> 252     {</span>
<span class="line-added"> 253         move(TrustedImmPtr(operation.executableAddress()), scratchRegister());</span>
<span class="line-added"> 254         m_assembler.call(scratchRegister());</span>
<span class="line-added"> 255     }</span>
<span class="line-added"> 256 </span>
 257     ALWAYS_INLINE Call call(RegisterID callTag) { return UNUSED_PARAM(callTag), call(NoPtrTag); }
 258 
 259     // Address is a memory location containing the address to jump to
 260     void farJump(AbsoluteAddress address, PtrTag tag)
 261     {
 262         move(TrustedImmPtr(address.m_ptr), scratchRegister());
 263         farJump(Address(scratchRegister()), tag);
 264     }
 265 
 266     ALWAYS_INLINE void farJump(AbsoluteAddress address, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), farJump(address, NoPtrTag); }
 267 
 268     Call threadSafePatchableNearCall()
 269     {
 270         const size_t nearCallOpcodeSize = 1;
 271         const size_t nearCallRelativeLocationSize = sizeof(int32_t);
 272         // We want to make sure the 32-bit near call immediate is 32-bit aligned.
 273         size_t codeSize = m_assembler.codeSize();
 274         size_t alignedSize = WTF::roundUpToMultipleOf&lt;nearCallRelativeLocationSize&gt;(codeSize + nearCallOpcodeSize);
 275         emitNops(alignedSize - (codeSize + nearCallOpcodeSize));
 276         DataLabelPtr label = DataLabelPtr(this);
</pre>
</td>
</tr>
</table>
<center><a href="MacroAssemblerX86Common.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ProbeFrame.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>