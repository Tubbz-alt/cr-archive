<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExit.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGOSREntrypointCreationPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExit.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExit.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGOSRExit.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;AssemblyHelpers.h&quot;


  32 #include &quot;ClonedArguments.h&quot;
  33 #include &quot;DFGGraph.h&quot;
  34 #include &quot;DFGMayExit.h&quot;
  35 #include &quot;DFGOSRExitCompilerCommon.h&quot;
<span class="line-removed">  36 #include &quot;DFGOSRExitPreparation.h&quot;</span>
  37 #include &quot;DFGOperations.h&quot;
  38 #include &quot;DFGSpeculativeJIT.h&quot;
  39 #include &quot;DirectArguments.h&quot;
  40 #include &quot;FrameTracers.h&quot;
  41 #include &quot;InlineCallFrame.h&quot;
  42 #include &quot;JSCInlines.h&quot;
  43 #include &quot;JSCJSValue.h&quot;
  44 #include &quot;OperandsInlines.h&quot;
  45 #include &quot;ProbeContext.h&quot;
  46 #include &quot;ProbeFrame.h&quot;
  47 
  48 namespace JSC { namespace DFG {
  49 
<span class="line-removed">  50 // Probe based OSR Exit.</span>
<span class="line-removed">  51 </span>
<span class="line-removed">  52 using CPUState = Probe::CPUState;</span>
<span class="line-removed">  53 using Context = Probe::Context;</span>
<span class="line-removed">  54 using Frame = Probe::Frame;</span>
<span class="line-removed">  55 </span>
<span class="line-removed">  56 static void reifyInlinedCallFrames(Probe::Context&amp;, CodeBlock* baselineCodeBlock, const OSRExitBase&amp;);</span>
<span class="line-removed">  57 static void adjustAndJumpToTarget(Probe::Context&amp;, VM&amp;, CodeBlock*, CodeBlock* baselineCodeBlock, OSRExit&amp;);</span>
<span class="line-removed">  58 static void printOSRExit(Context&amp;, uint32_t osrExitIndex, const OSRExit&amp;);</span>
<span class="line-removed">  59 </span>
<span class="line-removed">  60 static JSValue jsValueFor(CPUState&amp; cpu, JSValueSource source)</span>
<span class="line-removed">  61 {</span>
<span class="line-removed">  62     if (source.isAddress()) {</span>
<span class="line-removed">  63         JSValue result;</span>
<span class="line-removed">  64         std::memcpy(&amp;result, cpu.gpr&lt;uint8_t*&gt;(source.base()) + source.offset(), sizeof(JSValue));</span>
<span class="line-removed">  65         return result;</span>
<span class="line-removed">  66     }</span>
<span class="line-removed">  67 #if USE(JSVALUE64)</span>
<span class="line-removed">  68     return JSValue::decode(cpu.gpr&lt;EncodedJSValue&gt;(source.gpr()));</span>
<span class="line-removed">  69 #else</span>
<span class="line-removed">  70     if (source.hasKnownTag())</span>
<span class="line-removed">  71         return JSValue(source.tag(), cpu.gpr&lt;int32_t&gt;(source.payloadGPR()));</span>
<span class="line-removed">  72     return JSValue(cpu.gpr&lt;int32_t&gt;(source.tagGPR()), cpu.gpr&lt;int32_t&gt;(source.payloadGPR()));</span>
<span class="line-removed">  73 #endif</span>
<span class="line-removed">  74 }</span>
<span class="line-removed">  75 </span>
<span class="line-removed">  76 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0</span>
<span class="line-removed">  77 </span>
<span class="line-removed">  78 // Based on AssemblyHelpers::emitRestoreCalleeSavesFor().</span>
<span class="line-removed">  79 static void restoreCalleeSavesFor(Context&amp; context, CodeBlock* codeBlock)</span>
<span class="line-removed">  80 {</span>
<span class="line-removed">  81     ASSERT(codeBlock);</span>
<span class="line-removed">  82 </span>
<span class="line-removed">  83     const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();</span>
<span class="line-removed">  84     RegisterSet dontRestoreRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());</span>
<span class="line-removed">  85     unsigned registerCount = calleeSaves-&gt;size();</span>
<span class="line-removed">  86 </span>
<span class="line-removed">  87     UCPURegister* physicalStackFrame = context.fp&lt;UCPURegister*&gt;();</span>
<span class="line-removed">  88     for (unsigned i = 0; i &lt; registerCount; i++) {</span>
<span class="line-removed">  89         RegisterAtOffset entry = calleeSaves-&gt;at(i);</span>
<span class="line-removed">  90         if (dontRestoreRegisters.get(entry.reg()))</span>
<span class="line-removed">  91             continue;</span>
<span class="line-removed">  92         // The callee saved values come from the original stack, not the recovered stack.</span>
<span class="line-removed">  93         // Hence, we read the values directly from the physical stack memory instead of</span>
<span class="line-removed">  94         // going through context.stack().</span>
<span class="line-removed">  95         ASSERT(!(entry.offset() % sizeof(UCPURegister)));</span>
<span class="line-removed">  96         context.gpr(entry.reg().gpr()) = physicalStackFrame[entry.offset() / sizeof(UCPURegister)];</span>
<span class="line-removed">  97     }</span>
<span class="line-removed">  98 }</span>
<span class="line-removed">  99 </span>
<span class="line-removed"> 100 // Based on AssemblyHelpers::emitSaveCalleeSavesFor().</span>
<span class="line-removed"> 101 static void saveCalleeSavesFor(Context&amp; context, CodeBlock* codeBlock)</span>
<span class="line-removed"> 102 {</span>
<span class="line-removed"> 103     auto&amp; stack = context.stack();</span>
<span class="line-removed"> 104     ASSERT(codeBlock);</span>
<span class="line-removed"> 105 </span>
<span class="line-removed"> 106     const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();</span>
<span class="line-removed"> 107     RegisterSet dontSaveRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());</span>
<span class="line-removed"> 108     unsigned registerCount = calleeSaves-&gt;size();</span>
<span class="line-removed"> 109 </span>
<span class="line-removed"> 110     for (unsigned i = 0; i &lt; registerCount; i++) {</span>
<span class="line-removed"> 111         RegisterAtOffset entry = calleeSaves-&gt;at(i);</span>
<span class="line-removed"> 112         if (dontSaveRegisters.get(entry.reg()))</span>
<span class="line-removed"> 113             continue;</span>
<span class="line-removed"> 114         stack.set(context.fp(), entry.offset(), context.gpr&lt;UCPURegister&gt;(entry.reg().gpr()));</span>
<span class="line-removed"> 115     }</span>
<span class="line-removed"> 116 }</span>
<span class="line-removed"> 117 </span>
<span class="line-removed"> 118 // Based on AssemblyHelpers::restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer().</span>
<span class="line-removed"> 119 static void restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(Context&amp; context)</span>
<span class="line-removed"> 120 {</span>
<span class="line-removed"> 121     VM&amp; vm = *context.arg&lt;VM*&gt;();</span>
<span class="line-removed"> 122 </span>
<span class="line-removed"> 123     RegisterAtOffsetList* allCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();</span>
<span class="line-removed"> 124     RegisterSet dontRestoreRegisters = RegisterSet::stackRegisters();</span>
<span class="line-removed"> 125     unsigned registerCount = allCalleeSaves-&gt;size();</span>
<span class="line-removed"> 126 </span>
<span class="line-removed"> 127     VMEntryRecord* entryRecord = vmEntryRecord(vm.topEntryFrame);</span>
<span class="line-removed"> 128     UCPURegister* calleeSaveBuffer = reinterpret_cast&lt;UCPURegister*&gt;(entryRecord-&gt;calleeSaveRegistersBuffer);</span>
<span class="line-removed"> 129 </span>
<span class="line-removed"> 130     // Restore all callee saves.</span>
<span class="line-removed"> 131     for (unsigned i = 0; i &lt; registerCount; i++) {</span>
<span class="line-removed"> 132         RegisterAtOffset entry = allCalleeSaves-&gt;at(i);</span>
<span class="line-removed"> 133         if (dontRestoreRegisters.get(entry.reg()))</span>
<span class="line-removed"> 134             continue;</span>
<span class="line-removed"> 135         size_t uintptrOffset = entry.offset() / sizeof(UCPURegister);</span>
<span class="line-removed"> 136         if (entry.reg().isGPR())</span>
<span class="line-removed"> 137             context.gpr(entry.reg().gpr()) = calleeSaveBuffer[uintptrOffset];</span>
<span class="line-removed"> 138         else {</span>
<span class="line-removed"> 139 #if USE(JSVALUE64)</span>
<span class="line-removed"> 140             context.fpr(entry.reg().fpr()) = bitwise_cast&lt;double&gt;(calleeSaveBuffer[uintptrOffset]);</span>
<span class="line-removed"> 141 #else</span>
<span class="line-removed"> 142             // FIXME: &lt;https://webkit.org/b/193275&gt; support callee-saved floating point registers on 32-bit architectures</span>
<span class="line-removed"> 143             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed"> 144 #endif</span>
<span class="line-removed"> 145         }</span>
<span class="line-removed"> 146     }</span>
<span class="line-removed"> 147 }</span>
<span class="line-removed"> 148 </span>
<span class="line-removed"> 149 // Based on AssemblyHelpers::copyCalleeSavesToVMEntryFrameCalleeSavesBuffer().</span>
<span class="line-removed"> 150 static void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(Context&amp; context)</span>
<span class="line-removed"> 151 {</span>
<span class="line-removed"> 152     VM&amp; vm = *context.arg&lt;VM*&gt;();</span>
<span class="line-removed"> 153     auto&amp; stack = context.stack();</span>
<span class="line-removed"> 154 </span>
<span class="line-removed"> 155     VMEntryRecord* entryRecord = vmEntryRecord(vm.topEntryFrame);</span>
<span class="line-removed"> 156     void* calleeSaveBuffer = entryRecord-&gt;calleeSaveRegistersBuffer;</span>
<span class="line-removed"> 157 </span>
<span class="line-removed"> 158     RegisterAtOffsetList* allCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();</span>
<span class="line-removed"> 159     RegisterSet dontCopyRegisters = RegisterSet::stackRegisters();</span>
<span class="line-removed"> 160     unsigned registerCount = allCalleeSaves-&gt;size();</span>
<span class="line-removed"> 161 </span>
<span class="line-removed"> 162     for (unsigned i = 0; i &lt; registerCount; i++) {</span>
<span class="line-removed"> 163         RegisterAtOffset entry = allCalleeSaves-&gt;at(i);</span>
<span class="line-removed"> 164         if (dontCopyRegisters.get(entry.reg()))</span>
<span class="line-removed"> 165             continue;</span>
<span class="line-removed"> 166         if (entry.reg().isGPR())</span>
<span class="line-removed"> 167             stack.set(calleeSaveBuffer, entry.offset(), context.gpr&lt;UCPURegister&gt;(entry.reg().gpr()));</span>
<span class="line-removed"> 168         else {</span>
<span class="line-removed"> 169 #if USE(JSVALUE64)</span>
<span class="line-removed"> 170             stack.set(calleeSaveBuffer, entry.offset(), context.fpr&lt;UCPURegister&gt;(entry.reg().fpr()));</span>
<span class="line-removed"> 171 #else</span>
<span class="line-removed"> 172             // FIXME: &lt;https://webkit.org/b/193275&gt; support callee-saved floating point registers on 32-bit architectures</span>
<span class="line-removed"> 173             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed"> 174 #endif</span>
<span class="line-removed"> 175         }</span>
<span class="line-removed"> 176     }</span>
<span class="line-removed"> 177 }</span>
<span class="line-removed"> 178 </span>
<span class="line-removed"> 179 // Based on AssemblyHelpers::emitSaveOrCopyCalleeSavesFor().</span>
<span class="line-removed"> 180 static void saveOrCopyCalleeSavesFor(Context&amp; context, CodeBlock* codeBlock, VirtualRegister offsetVirtualRegister, bool wasCalledViaTailCall)</span>
<span class="line-removed"> 181 {</span>
<span class="line-removed"> 182     Frame frame(context.fp(), context.stack());</span>
<span class="line-removed"> 183     ASSERT(codeBlock);</span>
<span class="line-removed"> 184 </span>
<span class="line-removed"> 185     const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();</span>
<span class="line-removed"> 186     RegisterSet dontSaveRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());</span>
<span class="line-removed"> 187     unsigned registerCount = calleeSaves-&gt;size();</span>
<span class="line-removed"> 188 </span>
<span class="line-removed"> 189     RegisterSet baselineCalleeSaves = RegisterSet::llintBaselineCalleeSaveRegisters();</span>
<span class="line-removed"> 190 </span>
<span class="line-removed"> 191     for (unsigned i = 0; i &lt; registerCount; i++) {</span>
<span class="line-removed"> 192         RegisterAtOffset entry = calleeSaves-&gt;at(i);</span>
<span class="line-removed"> 193         if (dontSaveRegisters.get(entry.reg()))</span>
<span class="line-removed"> 194             continue;</span>
<span class="line-removed"> 195 </span>
<span class="line-removed"> 196         uintptr_t savedRegisterValue;</span>
<span class="line-removed"> 197 </span>
<span class="line-removed"> 198         if (wasCalledViaTailCall &amp;&amp; baselineCalleeSaves.get(entry.reg()))</span>
<span class="line-removed"> 199             savedRegisterValue = frame.get&lt;uintptr_t&gt;(entry.offset());</span>
<span class="line-removed"> 200         else</span>
<span class="line-removed"> 201             savedRegisterValue = context.gpr(entry.reg().gpr());</span>
<span class="line-removed"> 202 </span>
<span class="line-removed"> 203         frame.set(offsetVirtualRegister.offsetInBytes() + entry.offset(), savedRegisterValue);</span>
<span class="line-removed"> 204     }</span>
<span class="line-removed"> 205 }</span>
<span class="line-removed"> 206 #else // not NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0</span>
<span class="line-removed"> 207 </span>
<span class="line-removed"> 208 static void restoreCalleeSavesFor(Context&amp;, CodeBlock*) { }</span>
<span class="line-removed"> 209 static void saveCalleeSavesFor(Context&amp;, CodeBlock*) { }</span>
<span class="line-removed"> 210 static void restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(Context&amp;) { }</span>
<span class="line-removed"> 211 static void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(Context&amp;) { }</span>
<span class="line-removed"> 212 static void saveOrCopyCalleeSavesFor(Context&amp;, CodeBlock*, VirtualRegister, bool) { }</span>
<span class="line-removed"> 213 </span>
<span class="line-removed"> 214 #endif // NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0</span>
<span class="line-removed"> 215 </span>
<span class="line-removed"> 216 static JSCell* createDirectArgumentsDuringExit(Context&amp; context, CodeBlock* codeBlock, InlineCallFrame* inlineCallFrame, JSFunction* callee, int32_t argumentCount)</span>
<span class="line-removed"> 217 {</span>
<span class="line-removed"> 218     VM&amp; vm = *context.arg&lt;VM*&gt;();</span>
<span class="line-removed"> 219 </span>
<span class="line-removed"> 220     ASSERT(vm.heap.isDeferred());</span>
<span class="line-removed"> 221 </span>
<span class="line-removed"> 222     if (inlineCallFrame)</span>
<span class="line-removed"> 223         codeBlock = baselineCodeBlockForInlineCallFrame(inlineCallFrame);</span>
<span class="line-removed"> 224 </span>
<span class="line-removed"> 225     unsigned length = argumentCount - 1;</span>
<span class="line-removed"> 226     unsigned capacity = std::max(length, static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters() - 1));</span>
<span class="line-removed"> 227     DirectArguments* result = DirectArguments::create(</span>
<span class="line-removed"> 228         vm, codeBlock-&gt;globalObject()-&gt;directArgumentsStructure(), length, capacity);</span>
<span class="line-removed"> 229 </span>
<span class="line-removed"> 230     result-&gt;setCallee(vm, callee);</span>
<span class="line-removed"> 231 </span>
<span class="line-removed"> 232     void* frameBase = context.fp&lt;Register*&gt;() + (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);</span>
<span class="line-removed"> 233     Frame frame(frameBase, context.stack());</span>
<span class="line-removed"> 234     for (unsigned i = length; i--;)</span>
<span class="line-removed"> 235         result-&gt;setIndexQuickly(vm, i, frame.argument(i));</span>
<span class="line-removed"> 236 </span>
<span class="line-removed"> 237     return result;</span>
<span class="line-removed"> 238 }</span>
<span class="line-removed"> 239 </span>
<span class="line-removed"> 240 static JSCell* createClonedArgumentsDuringExit(Context&amp; context, CodeBlock* codeBlock, InlineCallFrame* inlineCallFrame, JSFunction* callee, int32_t argumentCount)</span>
<span class="line-removed"> 241 {</span>
<span class="line-removed"> 242     VM&amp; vm = *context.arg&lt;VM*&gt;();</span>
<span class="line-removed"> 243     ExecState* exec = context.fp&lt;ExecState*&gt;();</span>
<span class="line-removed"> 244 </span>
<span class="line-removed"> 245     ASSERT(vm.heap.isDeferred());</span>
<span class="line-removed"> 246 </span>
<span class="line-removed"> 247     if (inlineCallFrame)</span>
<span class="line-removed"> 248         codeBlock = baselineCodeBlockForInlineCallFrame(inlineCallFrame);</span>
<span class="line-removed"> 249 </span>
<span class="line-removed"> 250     unsigned length = argumentCount - 1;</span>
<span class="line-removed"> 251     ClonedArguments* result = ClonedArguments::createEmpty(</span>
<span class="line-removed"> 252         vm, codeBlock-&gt;globalObject()-&gt;clonedArgumentsStructure(), callee, length);</span>
<span class="line-removed"> 253 </span>
<span class="line-removed"> 254     void* frameBase = context.fp&lt;Register*&gt;() + (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);</span>
<span class="line-removed"> 255     Frame frame(frameBase, context.stack());</span>
<span class="line-removed"> 256     for (unsigned i = length; i--;)</span>
<span class="line-removed"> 257         result-&gt;putDirectIndex(exec, i, frame.argument(i));</span>
<span class="line-removed"> 258     return result;</span>
<span class="line-removed"> 259 }</span>
<span class="line-removed"> 260 </span>
<span class="line-removed"> 261 static void emitRestoreArguments(Context&amp; context, CodeBlock* codeBlock, DFG::JITCode* dfgJITCode, const Operands&lt;ValueRecovery&gt;&amp; operands)</span>
<span class="line-removed"> 262 {</span>
<span class="line-removed"> 263     Frame frame(context.fp(), context.stack());</span>
<span class="line-removed"> 264 </span>
<span class="line-removed"> 265     HashMap&lt;MinifiedID, int&gt; alreadyAllocatedArguments; // Maps phantom arguments node ID to operand.</span>
<span class="line-removed"> 266     for (size_t index = 0; index &lt; operands.size(); ++index) {</span>
<span class="line-removed"> 267         const ValueRecovery&amp; recovery = operands[index];</span>
<span class="line-removed"> 268         int operand = operands.operandForIndex(index);</span>
<span class="line-removed"> 269 </span>
<span class="line-removed"> 270         if (recovery.technique() != DirectArgumentsThatWereNotCreated</span>
<span class="line-removed"> 271             &amp;&amp; recovery.technique() != ClonedArgumentsThatWereNotCreated)</span>
<span class="line-removed"> 272             continue;</span>
<span class="line-removed"> 273 </span>
<span class="line-removed"> 274         MinifiedID id = recovery.nodeID();</span>
<span class="line-removed"> 275         auto iter = alreadyAllocatedArguments.find(id);</span>
<span class="line-removed"> 276         if (iter != alreadyAllocatedArguments.end()) {</span>
<span class="line-removed"> 277             frame.setOperand(operand, frame.operand(iter-&gt;value));</span>
<span class="line-removed"> 278             continue;</span>
<span class="line-removed"> 279         }</span>
<span class="line-removed"> 280 </span>
<span class="line-removed"> 281         InlineCallFrame* inlineCallFrame =</span>
<span class="line-removed"> 282             dfgJITCode-&gt;minifiedDFG.at(id)-&gt;inlineCallFrame();</span>
<span class="line-removed"> 283 </span>
<span class="line-removed"> 284         int stackOffset;</span>
<span class="line-removed"> 285         if (inlineCallFrame)</span>
<span class="line-removed"> 286             stackOffset = inlineCallFrame-&gt;stackOffset;</span>
<span class="line-removed"> 287         else</span>
<span class="line-removed"> 288             stackOffset = 0;</span>
<span class="line-removed"> 289 </span>
<span class="line-removed"> 290         JSFunction* callee;</span>
<span class="line-removed"> 291         if (!inlineCallFrame || inlineCallFrame-&gt;isClosureCall)</span>
<span class="line-removed"> 292             callee = jsCast&lt;JSFunction*&gt;(frame.operand(stackOffset + CallFrameSlot::callee).asCell());</span>
<span class="line-removed"> 293         else</span>
<span class="line-removed"> 294             callee = jsCast&lt;JSFunction*&gt;(inlineCallFrame-&gt;calleeRecovery.constant().asCell());</span>
<span class="line-removed"> 295 </span>
<span class="line-removed"> 296         int32_t argumentCount;</span>
<span class="line-removed"> 297         if (!inlineCallFrame || inlineCallFrame-&gt;isVarargs())</span>
<span class="line-removed"> 298             argumentCount = frame.operand&lt;int32_t&gt;(stackOffset + CallFrameSlot::argumentCount, PayloadOffset);</span>
<span class="line-removed"> 299         else</span>
<span class="line-removed"> 300             argumentCount = inlineCallFrame-&gt;argumentCountIncludingThis;</span>
<span class="line-removed"> 301 </span>
<span class="line-removed"> 302         JSCell* argumentsObject;</span>
<span class="line-removed"> 303         switch (recovery.technique()) {</span>
<span class="line-removed"> 304         case DirectArgumentsThatWereNotCreated:</span>
<span class="line-removed"> 305             argumentsObject = createDirectArgumentsDuringExit(context, codeBlock, inlineCallFrame, callee, argumentCount);</span>
<span class="line-removed"> 306             break;</span>
<span class="line-removed"> 307         case ClonedArgumentsThatWereNotCreated:</span>
<span class="line-removed"> 308             argumentsObject = createClonedArgumentsDuringExit(context, codeBlock, inlineCallFrame, callee, argumentCount);</span>
<span class="line-removed"> 309             break;</span>
<span class="line-removed"> 310         default:</span>
<span class="line-removed"> 311             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed"> 312             break;</span>
<span class="line-removed"> 313         }</span>
<span class="line-removed"> 314         frame.setOperand(operand, JSValue(argumentsObject));</span>
<span class="line-removed"> 315 </span>
<span class="line-removed"> 316         alreadyAllocatedArguments.add(id, operand);</span>
<span class="line-removed"> 317     }</span>
<span class="line-removed"> 318 }</span>
<span class="line-removed"> 319 </span>
<span class="line-removed"> 320 // The following is a list of extra initializations that need to be done in order</span>
<span class="line-removed"> 321 // of most likely needed (lower enum value) to least likely needed (higher enum value).</span>
<span class="line-removed"> 322 // Each level initialization includes the previous lower enum value (see use of the</span>
<span class="line-removed"> 323 // extraInitializationLevel value below).</span>
<span class="line-removed"> 324 enum class ExtraInitializationLevel {</span>
<span class="line-removed"> 325     None,</span>
<span class="line-removed"> 326     SpeculationRecovery,</span>
<span class="line-removed"> 327     ValueProfileUpdate,</span>
<span class="line-removed"> 328     ArrayProfileUpdate,</span>
<span class="line-removed"> 329     Other</span>
<span class="line-removed"> 330 };</span>
<span class="line-removed"> 331 </span>
<span class="line-removed"> 332 void OSRExit::executeOSRExit(Context&amp; context)</span>
<span class="line-removed"> 333 {</span>
<span class="line-removed"> 334     VM&amp; vm = *context.arg&lt;VM*&gt;();</span>
<span class="line-removed"> 335     auto scope = DECLARE_THROW_SCOPE(vm);</span>
<span class="line-removed"> 336 </span>
<span class="line-removed"> 337     ExecState* exec = context.fp&lt;ExecState*&gt;();</span>
<span class="line-removed"> 338     ASSERT(&amp;exec-&gt;vm() == &amp;vm);</span>
<span class="line-removed"> 339     auto&amp; cpu = context.cpu;</span>
<span class="line-removed"> 340 </span>
<span class="line-removed"> 341     if (validateDFGDoesGC) {</span>
<span class="line-removed"> 342         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized</span>
<span class="line-removed"> 343         // code running that expects no GC.</span>
<span class="line-removed"> 344         vm.heap.setExpectDoesGC(true);</span>
<span class="line-removed"> 345     }</span>
<span class="line-removed"> 346 </span>
<span class="line-removed"> 347     if (vm.callFrameForCatch) {</span>
<span class="line-removed"> 348         exec = vm.callFrameForCatch;</span>
<span class="line-removed"> 349         context.fp() = exec;</span>
<span class="line-removed"> 350     }</span>
<span class="line-removed"> 351 </span>
<span class="line-removed"> 352     CodeBlock* codeBlock = exec-&gt;codeBlock();</span>
<span class="line-removed"> 353     ASSERT(codeBlock);</span>
<span class="line-removed"> 354     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);</span>
<span class="line-removed"> 355 </span>
<span class="line-removed"> 356     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t</span>
<span class="line-removed"> 357     // really be profitable.</span>
<span class="line-removed"> 358     DeferGCForAWhile deferGC(vm.heap);</span>
<span class="line-removed"> 359 </span>
<span class="line-removed"> 360     uint32_t exitIndex = vm.osrExitIndex;</span>
<span class="line-removed"> 361     DFG::JITCode* dfgJITCode = codeBlock-&gt;jitCode()-&gt;dfg();</span>
<span class="line-removed"> 362     OSRExit&amp; exit = dfgJITCode-&gt;osrExit[exitIndex];</span>
<span class="line-removed"> 363 </span>
<span class="line-removed"> 364     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);</span>
<span class="line-removed"> 365     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());</span>
<span class="line-removed"> 366 </span>
<span class="line-removed"> 367     if (UNLIKELY(!exit.exitState)) {</span>
<span class="line-removed"> 368         ExtraInitializationLevel extraInitializationLevel = ExtraInitializationLevel::None;</span>
<span class="line-removed"> 369 </span>
<span class="line-removed"> 370         // We only need to execute this block once for each OSRExit record. The computed</span>
<span class="line-removed"> 371         // results will be cached in the OSRExitState record for use of the rest of the</span>
<span class="line-removed"> 372         // exit ramp code.</span>
<span class="line-removed"> 373 </span>
<span class="line-removed"> 374         // Ensure we have baseline codeBlocks to OSR exit to.</span>
<span class="line-removed"> 375         prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);</span>
<span class="line-removed"> 376 </span>
<span class="line-removed"> 377         CodeBlock* baselineCodeBlock = codeBlock-&gt;baselineAlternative();</span>
<span class="line-removed"> 378         ASSERT(baselineCodeBlock-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-removed"> 379 </span>
<span class="line-removed"> 380         SpeculationRecovery* recovery = nullptr;</span>
<span class="line-removed"> 381         if (exit.m_recoveryIndex != UINT_MAX) {</span>
<span class="line-removed"> 382             recovery = &amp;dfgJITCode-&gt;speculationRecovery[exit.m_recoveryIndex];</span>
<span class="line-removed"> 383             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::SpeculationRecovery);</span>
<span class="line-removed"> 384         }</span>
<span class="line-removed"> 385 </span>
<span class="line-removed"> 386         if (UNLIKELY(exit.m_kind == GenericUnwind))</span>
<span class="line-removed"> 387             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);</span>
<span class="line-removed"> 388 </span>
<span class="line-removed"> 389         ArrayProfile* arrayProfile = nullptr;</span>
<span class="line-removed"> 390         if (!!exit.m_jsValueSource) {</span>
<span class="line-removed"> 391             if (exit.m_valueProfile)</span>
<span class="line-removed"> 392                 extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ValueProfileUpdate);</span>
<span class="line-removed"> 393             if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {</span>
<span class="line-removed"> 394                 CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;</span>
<span class="line-removed"> 395                 CodeBlock* profiledCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock);</span>
<span class="line-removed"> 396                 arrayProfile = profiledCodeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex());</span>
<span class="line-removed"> 397                 if (arrayProfile)</span>
<span class="line-removed"> 398                     extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ArrayProfileUpdate);</span>
<span class="line-removed"> 399             }</span>
<span class="line-removed"> 400         }</span>
<span class="line-removed"> 401 </span>
<span class="line-removed"> 402         int32_t activeThreshold = baselineCodeBlock-&gt;adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp());</span>
<span class="line-removed"> 403         double adjustedThreshold = applyMemoryUsageHeuristicsAndConvertToInt(activeThreshold, baselineCodeBlock);</span>
<span class="line-removed"> 404         ASSERT(adjustedThreshold &gt; 0);</span>
<span class="line-removed"> 405         adjustedThreshold = BaselineExecutionCounter::clippedThreshold(codeBlock-&gt;globalObject(), adjustedThreshold);</span>
<span class="line-removed"> 406 </span>
<span class="line-removed"> 407         CodeBlock* codeBlockForExit = baselineCodeBlockForOriginAndBaselineCodeBlock(exit.m_codeOrigin, baselineCodeBlock);</span>
<span class="line-removed"> 408         const JITCodeMap&amp; codeMap = codeBlockForExit-&gt;jitCodeMap();</span>
<span class="line-removed"> 409         CodeLocationLabel&lt;JSEntryPtrTag&gt; codeLocation = codeMap.find(exit.m_codeOrigin.bytecodeIndex());</span>
<span class="line-removed"> 410         ASSERT(codeLocation);</span>
<span class="line-removed"> 411 </span>
<span class="line-removed"> 412         void* jumpTarget = codeLocation.executableAddress();</span>
<span class="line-removed"> 413 </span>
<span class="line-removed"> 414         // Compute the value recoveries.</span>
<span class="line-removed"> 415         Operands&lt;ValueRecovery&gt; operands;</span>
<span class="line-removed"> 416         Vector&lt;UndefinedOperandSpan&gt; undefinedOperandSpans;</span>
<span class="line-removed"> 417         dfgJITCode-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, dfgJITCode-&gt;minifiedDFG, exit.m_streamIndex, operands, &amp;undefinedOperandSpans);</span>
<span class="line-removed"> 418         ptrdiff_t stackPointerOffset = -static_cast&lt;ptrdiff_t&gt;(codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit) * sizeof(Register);</span>
<span class="line-removed"> 419 </span>
<span class="line-removed"> 420         exit.exitState = adoptRef(new OSRExitState(exit, codeBlock, baselineCodeBlock, operands, WTFMove(undefinedOperandSpans), recovery, stackPointerOffset, activeThreshold, adjustedThreshold, jumpTarget, arrayProfile));</span>
<span class="line-removed"> 421 </span>
<span class="line-removed"> 422         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {</span>
<span class="line-removed"> 423             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;</span>
<span class="line-removed"> 424             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();</span>
<span class="line-removed"> 425 </span>
<span class="line-removed"> 426             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(</span>
<span class="line-removed"> 427                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),</span>
<span class="line-removed"> 428                 exit.m_kind, exit.m_kind == UncountableInvalidation);</span>
<span class="line-removed"> 429             exit.exitState-&gt;profilerExit = profilerExit;</span>
<span class="line-removed"> 430             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);</span>
<span class="line-removed"> 431         }</span>
<span class="line-removed"> 432 </span>
<span class="line-removed"> 433         if (UNLIKELY(Options::printEachOSRExit()))</span>
<span class="line-removed"> 434             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);</span>
<span class="line-removed"> 435 </span>
<span class="line-removed"> 436         exit.exitState-&gt;extraInitializationLevel = extraInitializationLevel;</span>
<span class="line-removed"> 437 </span>
<span class="line-removed"> 438         if (UNLIKELY(Options::verboseOSR() || Options::verboseDFGOSRExit())) {</span>
<span class="line-removed"> 439             dataLogF(&quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s\n&quot;,</span>
<span class="line-removed"> 440                 exitIndex, toCString(exit.m_codeOrigin).data(),</span>
<span class="line-removed"> 441                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),</span>
<span class="line-removed"> 442                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());</span>
<span class="line-removed"> 443         }</span>
<span class="line-removed"> 444     }</span>
<span class="line-removed"> 445 </span>
<span class="line-removed"> 446     OSRExitState&amp; exitState = *exit.exitState.get();</span>
<span class="line-removed"> 447     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;</span>
<span class="line-removed"> 448     ASSERT(baselineCodeBlock-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-removed"> 449 </span>
<span class="line-removed"> 450     Operands&lt;ValueRecovery&gt;&amp; operands = exitState.operands;</span>
<span class="line-removed"> 451     Vector&lt;UndefinedOperandSpan&gt;&amp; undefinedOperandSpans = exitState.undefinedOperandSpans;</span>
<span class="line-removed"> 452 </span>
<span class="line-removed"> 453     context.sp() = context.fp&lt;uint8_t*&gt;() + exitState.stackPointerOffset;</span>
<span class="line-removed"> 454 </span>
<span class="line-removed"> 455     // The only reason for using this do while loop is so we can break out midway when appropriate.</span>
<span class="line-removed"> 456     do {</span>
<span class="line-removed"> 457         auto extraInitializationLevel = static_cast&lt;ExtraInitializationLevel&gt;(exitState.extraInitializationLevel);</span>
<span class="line-removed"> 458 </span>
<span class="line-removed"> 459         if (extraInitializationLevel == ExtraInitializationLevel::None)</span>
<span class="line-removed"> 460             break;</span>
<span class="line-removed"> 461 </span>
<span class="line-removed"> 462         // Begin extra initilization level: SpeculationRecovery</span>
<span class="line-removed"> 463 </span>
<span class="line-removed"> 464         // We need to do speculation recovery first because array profiling and value profiling</span>
<span class="line-removed"> 465         // may rely on a value that it recovers. However, that doesn&#39;t mean that it is likely</span>
<span class="line-removed"> 466         // to have a recovery value. So, we&#39;ll decorate it as UNLIKELY.</span>
<span class="line-removed"> 467         SpeculationRecovery* recovery = exitState.recovery;</span>
<span class="line-removed"> 468         if (UNLIKELY(recovery)) {</span>
<span class="line-removed"> 469             switch (recovery-&gt;type()) {</span>
<span class="line-removed"> 470             case SpeculativeAdd:</span>
<span class="line-removed"> 471                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - cpu.gpr&lt;uint32_t&gt;(recovery-&gt;src());</span>
<span class="line-removed"> 472 #if USE(JSVALUE64)</span>
<span class="line-removed"> 473                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));</span>
<span class="line-removed"> 474                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;</span>
<span class="line-removed"> 475 #endif</span>
<span class="line-removed"> 476                 break;</span>
<span class="line-removed"> 477 </span>
<span class="line-removed"> 478             case SpeculativeAddSelf:</span>
<span class="line-removed"> 479                 cpu.gpr(recovery-&gt;dest()) = static_cast&lt;uint32_t&gt;(cpu.gpr&lt;int32_t&gt;(recovery-&gt;dest()) &gt;&gt; 1) ^ 0x80000000U;</span>
<span class="line-removed"> 480 #if USE(JSVALUE64)</span>
<span class="line-removed"> 481                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));</span>
<span class="line-removed"> 482                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;</span>
<span class="line-removed"> 483 #endif</span>
<span class="line-removed"> 484                 break;</span>
<span class="line-removed"> 485 </span>
<span class="line-removed"> 486             case SpeculativeAddImmediate:</span>
<span class="line-removed"> 487                 cpu.gpr(recovery-&gt;dest()) = (cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - recovery-&gt;immediate());</span>
<span class="line-removed"> 488 #if USE(JSVALUE64)</span>
<span class="line-removed"> 489                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));</span>
<span class="line-removed"> 490                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;</span>
<span class="line-removed"> 491 #endif</span>
<span class="line-removed"> 492                 break;</span>
<span class="line-removed"> 493 </span>
<span class="line-removed"> 494             case BooleanSpeculationCheck:</span>
<span class="line-removed"> 495 #if USE(JSVALUE64)</span>
<span class="line-removed"> 496                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr(recovery-&gt;dest()) ^ ValueFalse;</span>
<span class="line-removed"> 497 #endif</span>
<span class="line-removed"> 498                 break;</span>
<span class="line-removed"> 499 </span>
<span class="line-removed"> 500             default:</span>
<span class="line-removed"> 501                 break;</span>
<span class="line-removed"> 502             }</span>
<span class="line-removed"> 503         }</span>
<span class="line-removed"> 504         if (extraInitializationLevel &lt;= ExtraInitializationLevel::SpeculationRecovery)</span>
<span class="line-removed"> 505             break;</span>
<span class="line-removed"> 506 </span>
<span class="line-removed"> 507         // Begin extra initilization level: ValueProfileUpdate</span>
<span class="line-removed"> 508         JSValue profiledValue;</span>
<span class="line-removed"> 509         if (!!exit.m_jsValueSource) {</span>
<span class="line-removed"> 510             profiledValue = jsValueFor(cpu, exit.m_jsValueSource);</span>
<span class="line-removed"> 511             if (MethodOfGettingAValueProfile profile = exit.m_valueProfile)</span>
<span class="line-removed"> 512                 profile.reportValue(profiledValue);</span>
<span class="line-removed"> 513         }</span>
<span class="line-removed"> 514         if (extraInitializationLevel &lt;= ExtraInitializationLevel::ValueProfileUpdate)</span>
<span class="line-removed"> 515             break;</span>
<span class="line-removed"> 516 </span>
<span class="line-removed"> 517         // Begin extra initilization level: ArrayProfileUpdate</span>
<span class="line-removed"> 518         ArrayProfile* arrayProfile = exitState.arrayProfile;</span>
<span class="line-removed"> 519         if (arrayProfile) {</span>
<span class="line-removed"> 520             ASSERT(!!exit.m_jsValueSource);</span>
<span class="line-removed"> 521             ASSERT(exit.m_kind == BadCache || exit.m_kind == BadIndexingType);</span>
<span class="line-removed"> 522             Structure* structure = profiledValue.asCell()-&gt;structure(vm);</span>
<span class="line-removed"> 523             arrayProfile-&gt;observeStructure(structure);</span>
<span class="line-removed"> 524             arrayProfile-&gt;observeArrayMode(arrayModesFromStructure(structure));</span>
<span class="line-removed"> 525         }</span>
<span class="line-removed"> 526         if (extraInitializationLevel &lt;= ExtraInitializationLevel::ArrayProfileUpdate)</span>
<span class="line-removed"> 527             break;</span>
<span class="line-removed"> 528 </span>
<span class="line-removed"> 529         // Begin Extra initilization level: Other</span>
<span class="line-removed"> 530         if (UNLIKELY(exit.m_kind == GenericUnwind)) {</span>
<span class="line-removed"> 531             // We are acting as a defacto op_catch because we arrive here from genericUnwind().</span>
<span class="line-removed"> 532             // So, we must restore our call frame and stack pointer.</span>
<span class="line-removed"> 533             restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(context);</span>
<span class="line-removed"> 534             ASSERT(context.fp() == vm.callFrameForCatch);</span>
<span class="line-removed"> 535         }</span>
<span class="line-removed"> 536 </span>
<span class="line-removed"> 537         if (exitState.profilerExit)</span>
<span class="line-removed"> 538             exitState.profilerExit-&gt;incCount();</span>
<span class="line-removed"> 539 </span>
<span class="line-removed"> 540         if (UNLIKELY(Options::printEachOSRExit()))</span>
<span class="line-removed"> 541             printOSRExit(context, vm.osrExitIndex, exit);</span>
<span class="line-removed"> 542 </span>
<span class="line-removed"> 543     } while (false); // End extra initialization.</span>
<span class="line-removed"> 544 </span>
<span class="line-removed"> 545     Frame frame(cpu.fp(), context.stack());</span>
<span class="line-removed"> 546     ASSERT(!(context.fp&lt;uintptr_t&gt;() &amp; 0x7));</span>
<span class="line-removed"> 547 </span>
<span class="line-removed"> 548 #if USE(JSVALUE64)</span>
<span class="line-removed"> 549     ASSERT(cpu.gpr(GPRInfo::tagTypeNumberRegister) == TagTypeNumber);</span>
<span class="line-removed"> 550     ASSERT(cpu.gpr(GPRInfo::tagMaskRegister) == TagMask);</span>
<span class="line-removed"> 551 #endif</span>
<span class="line-removed"> 552 </span>
<span class="line-removed"> 553     // Do all data format conversions and store the results into the stack.</span>
<span class="line-removed"> 554     // Note: we need to recover values before restoring callee save registers below</span>
<span class="line-removed"> 555     // because the recovery may rely on values in some of callee save registers.</span>
<span class="line-removed"> 556 </span>
<span class="line-removed"> 557     int calleeSaveSpaceAsVirtualRegisters = static_cast&lt;int&gt;(baselineCodeBlock-&gt;calleeSaveSpaceAsVirtualRegisters());</span>
<span class="line-removed"> 558     size_t numberOfOperands = operands.size();</span>
<span class="line-removed"> 559     size_t numUndefinedOperandSpans = undefinedOperandSpans.size();</span>
<span class="line-removed"> 560 </span>
<span class="line-removed"> 561     size_t nextUndefinedSpanIndex = 0;</span>
<span class="line-removed"> 562     size_t nextUndefinedOperandIndex = numberOfOperands;</span>
<span class="line-removed"> 563     if (numUndefinedOperandSpans)</span>
<span class="line-removed"> 564         nextUndefinedOperandIndex = undefinedOperandSpans[nextUndefinedSpanIndex].firstIndex;</span>
<span class="line-removed"> 565 </span>
<span class="line-removed"> 566     JSValue undefined = jsUndefined();</span>
<span class="line-removed"> 567     for (size_t spanIndex = 0; spanIndex &lt; numUndefinedOperandSpans; ++spanIndex) {</span>
<span class="line-removed"> 568         auto&amp; span = undefinedOperandSpans[spanIndex];</span>
<span class="line-removed"> 569         int firstOffset = span.minOffset;</span>
<span class="line-removed"> 570         int lastOffset = firstOffset + span.numberOfRegisters;</span>
<span class="line-removed"> 571 </span>
<span class="line-removed"> 572         for (int offset = firstOffset; offset &lt; lastOffset; ++offset)</span>
<span class="line-removed"> 573             frame.setOperand(offset, undefined);</span>
<span class="line-removed"> 574     }</span>
<span class="line-removed"> 575 </span>
<span class="line-removed"> 576     for (size_t index = 0; index &lt; numberOfOperands; ++index) {</span>
<span class="line-removed"> 577         const ValueRecovery&amp; recovery = operands[index];</span>
<span class="line-removed"> 578         VirtualRegister reg = operands.virtualRegisterForIndex(index);</span>
<span class="line-removed"> 579 </span>
<span class="line-removed"> 580         if (UNLIKELY(index == nextUndefinedOperandIndex)) {</span>
<span class="line-removed"> 581             index += undefinedOperandSpans[nextUndefinedSpanIndex++].numberOfRegisters - 1;</span>
<span class="line-removed"> 582             if (nextUndefinedSpanIndex &lt; numUndefinedOperandSpans)</span>
<span class="line-removed"> 583                 nextUndefinedOperandIndex = undefinedOperandSpans[nextUndefinedSpanIndex].firstIndex;</span>
<span class="line-removed"> 584             else</span>
<span class="line-removed"> 585                 nextUndefinedOperandIndex = numberOfOperands;</span>
<span class="line-removed"> 586             continue;</span>
<span class="line-removed"> 587         }</span>
<span class="line-removed"> 588 </span>
<span class="line-removed"> 589         if (reg.isLocal() &amp;&amp; reg.toLocal() &lt; calleeSaveSpaceAsVirtualRegisters)</span>
<span class="line-removed"> 590             continue;</span>
<span class="line-removed"> 591 </span>
<span class="line-removed"> 592         int operand = reg.offset();</span>
<span class="line-removed"> 593 </span>
<span class="line-removed"> 594         switch (recovery.technique()) {</span>
<span class="line-removed"> 595         case DisplacedInJSStack:</span>
<span class="line-removed"> 596             frame.setOperand(operand, exec-&gt;r(recovery.virtualRegister()).asanUnsafeJSValue());</span>
<span class="line-removed"> 597             break;</span>
<span class="line-removed"> 598 </span>
<span class="line-removed"> 599         case InFPR:</span>
<span class="line-removed"> 600             frame.setOperand(operand, cpu.fpr&lt;JSValue&gt;(recovery.fpr()));</span>
<span class="line-removed"> 601             break;</span>
<span class="line-removed"> 602 </span>
<span class="line-removed"> 603 #if USE(JSVALUE64)</span>
<span class="line-removed"> 604         case InGPR:</span>
<span class="line-removed"> 605             frame.setOperand(operand, cpu.gpr&lt;JSValue&gt;(recovery.gpr()));</span>
<span class="line-removed"> 606             break;</span>
<span class="line-removed"> 607 #else</span>
<span class="line-removed"> 608         case InPair:</span>
<span class="line-removed"> 609             frame.setOperand(operand, JSValue(cpu.gpr&lt;int32_t&gt;(recovery.tagGPR()), cpu.gpr&lt;int32_t&gt;(recovery.payloadGPR())));</span>
<span class="line-removed"> 610             break;</span>
<span class="line-removed"> 611 #endif</span>
<span class="line-removed"> 612 </span>
<span class="line-removed"> 613         case UnboxedCellInGPR:</span>
<span class="line-removed"> 614             frame.setOperand(operand, JSValue(cpu.gpr&lt;JSCell*&gt;(recovery.gpr())));</span>
<span class="line-removed"> 615             break;</span>
<span class="line-removed"> 616 </span>
<span class="line-removed"> 617         case CellDisplacedInJSStack:</span>
<span class="line-removed"> 618             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedCell()));</span>
<span class="line-removed"> 619             break;</span>
<span class="line-removed"> 620 </span>
<span class="line-removed"> 621 #if USE(JSVALUE32_64)</span>
<span class="line-removed"> 622         case UnboxedBooleanInGPR:</span>
<span class="line-removed"> 623             frame.setOperand(operand, jsBoolean(cpu.gpr&lt;bool&gt;(recovery.gpr())));</span>
<span class="line-removed"> 624             break;</span>
<span class="line-removed"> 625 #endif</span>
<span class="line-removed"> 626 </span>
<span class="line-removed"> 627         case BooleanDisplacedInJSStack:</span>
<span class="line-removed"> 628 #if USE(JSVALUE64)</span>
<span class="line-removed"> 629             frame.setOperand(operand, exec-&gt;r(recovery.virtualRegister()).asanUnsafeJSValue());</span>
<span class="line-removed"> 630 #else</span>
<span class="line-removed"> 631             frame.setOperand(operand, jsBoolean(exec-&gt;r(recovery.virtualRegister()).asanUnsafeJSValue().payload()));</span>
<span class="line-removed"> 632 #endif</span>
<span class="line-removed"> 633             break;</span>
<span class="line-removed"> 634 </span>
<span class="line-removed"> 635         case UnboxedInt32InGPR:</span>
<span class="line-removed"> 636             frame.setOperand(operand, JSValue(cpu.gpr&lt;int32_t&gt;(recovery.gpr())));</span>
<span class="line-removed"> 637             break;</span>
<span class="line-removed"> 638 </span>
<span class="line-removed"> 639         case Int32DisplacedInJSStack:</span>
<span class="line-removed"> 640             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedInt32()));</span>
<span class="line-removed"> 641             break;</span>
<span class="line-removed"> 642 </span>
<span class="line-removed"> 643 #if USE(JSVALUE64)</span>
<span class="line-removed"> 644         case UnboxedInt52InGPR:</span>
<span class="line-removed"> 645             frame.setOperand(operand, JSValue(cpu.gpr&lt;int64_t&gt;(recovery.gpr()) &gt;&gt; JSValue::int52ShiftAmount));</span>
<span class="line-removed"> 646             break;</span>
<span class="line-removed"> 647 </span>
<span class="line-removed"> 648         case Int52DisplacedInJSStack:</span>
<span class="line-removed"> 649             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedInt52()));</span>
<span class="line-removed"> 650             break;</span>
<span class="line-removed"> 651 </span>
<span class="line-removed"> 652         case UnboxedStrictInt52InGPR:</span>
<span class="line-removed"> 653             frame.setOperand(operand, JSValue(cpu.gpr&lt;int64_t&gt;(recovery.gpr())));</span>
<span class="line-removed"> 654             break;</span>
<span class="line-removed"> 655 </span>
<span class="line-removed"> 656         case StrictInt52DisplacedInJSStack:</span>
<span class="line-removed"> 657             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedStrictInt52()));</span>
<span class="line-removed"> 658             break;</span>
<span class="line-removed"> 659 #endif</span>
<span class="line-removed"> 660 </span>
<span class="line-removed"> 661         case UnboxedDoubleInFPR:</span>
<span class="line-removed"> 662             frame.setOperand(operand, JSValue(JSValue::EncodeAsDouble, purifyNaN(cpu.fpr(recovery.fpr()))));</span>
<span class="line-removed"> 663             break;</span>
<span class="line-removed"> 664 </span>
<span class="line-removed"> 665         case DoubleDisplacedInJSStack:</span>
<span class="line-removed"> 666             frame.setOperand(operand, JSValue(JSValue::EncodeAsDouble, purifyNaN(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedDouble())));</span>
<span class="line-removed"> 667             break;</span>
<span class="line-removed"> 668 </span>
<span class="line-removed"> 669         case Constant:</span>
<span class="line-removed"> 670             frame.setOperand(operand, recovery.constant());</span>
<span class="line-removed"> 671             break;</span>
<span class="line-removed"> 672 </span>
<span class="line-removed"> 673         case DirectArgumentsThatWereNotCreated:</span>
<span class="line-removed"> 674         case ClonedArgumentsThatWereNotCreated:</span>
<span class="line-removed"> 675             // Don&#39;t do this, yet.</span>
<span class="line-removed"> 676             break;</span>
<span class="line-removed"> 677 </span>
<span class="line-removed"> 678         default:</span>
<span class="line-removed"> 679             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed"> 680             break;</span>
<span class="line-removed"> 681         }</span>
<span class="line-removed"> 682     }</span>
<span class="line-removed"> 683 </span>
<span class="line-removed"> 684     // Restore the DFG callee saves and then save the ones the baseline JIT uses.</span>
<span class="line-removed"> 685     restoreCalleeSavesFor(context, codeBlock);</span>
<span class="line-removed"> 686     saveCalleeSavesFor(context, baselineCodeBlock);</span>
<span class="line-removed"> 687 </span>
<span class="line-removed"> 688 #if USE(JSVALUE64)</span>
<span class="line-removed"> 689     cpu.gpr(GPRInfo::tagTypeNumberRegister) = static_cast&lt;uintptr_t&gt;(TagTypeNumber);</span>
<span class="line-removed"> 690     cpu.gpr(GPRInfo::tagMaskRegister) = static_cast&lt;uintptr_t&gt;(TagTypeNumber | TagBitTypeOther);</span>
<span class="line-removed"> 691 #endif</span>
<span class="line-removed"> 692 </span>
<span class="line-removed"> 693     if (exit.isExceptionHandler())</span>
<span class="line-removed"> 694         copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(context);</span>
<span class="line-removed"> 695 </span>
<span class="line-removed"> 696     // Now that things on the stack are recovered, do the arguments recovery. We assume that arguments</span>
<span class="line-removed"> 697     // recoveries don&#39;t recursively refer to each other. But, we don&#39;t try to assume that they only</span>
<span class="line-removed"> 698     // refer to certain ranges of locals. Hence why we need to do this here, once the stack is sensible.</span>
<span class="line-removed"> 699     // Note that we also roughly assume that the arguments might still be materialized outside of its</span>
<span class="line-removed"> 700     // inline call frame scope - but for now the DFG wouldn&#39;t do that.</span>
<span class="line-removed"> 701 </span>
<span class="line-removed"> 702     DFG::emitRestoreArguments(context, codeBlock, dfgJITCode, operands);</span>
<span class="line-removed"> 703 </span>
<span class="line-removed"> 704     // Adjust the old JIT&#39;s execute counter. Since we are exiting OSR, we know</span>
<span class="line-removed"> 705     // that all new calls into this code will go to the new JIT, so the execute</span>
<span class="line-removed"> 706     // counter only affects call frames that performed OSR exit and call frames</span>
<span class="line-removed"> 707     // that were still executing the old JIT at the time of another call frame&#39;s</span>
<span class="line-removed"> 708     // OSR exit. We want to ensure that the following is true:</span>
<span class="line-removed"> 709     //</span>
<span class="line-removed"> 710     // (a) Code the performs an OSR exit gets a chance to reenter optimized</span>
<span class="line-removed"> 711     //     code eventually, since optimized code is faster. But we don&#39;t</span>
<span class="line-removed"> 712     //     want to do such reentery too aggressively (see (c) below).</span>
<span class="line-removed"> 713     //</span>
<span class="line-removed"> 714     // (b) If there is code on the call stack that is still running the old</span>
<span class="line-removed"> 715     //     JIT&#39;s code and has never OSR&#39;d, then it should get a chance to</span>
<span class="line-removed"> 716     //     perform OSR entry despite the fact that we&#39;ve exited.</span>
<span class="line-removed"> 717     //</span>
<span class="line-removed"> 718     // (c) Code the performs an OSR exit should not immediately retry OSR</span>
<span class="line-removed"> 719     //     entry, since both forms of OSR are expensive. OSR entry is</span>
<span class="line-removed"> 720     //     particularly expensive.</span>
<span class="line-removed"> 721     //</span>
<span class="line-removed"> 722     // (d) Frequent OSR failures, even those that do not result in the code</span>
<span class="line-removed"> 723     //     running in a hot loop, result in recompilation getting triggered.</span>
<span class="line-removed"> 724     //</span>
<span class="line-removed"> 725     // To ensure (c), we&#39;d like to set the execute counter to</span>
<span class="line-removed"> 726     // counterValueForOptimizeAfterWarmUp(). This seems like it would endanger</span>
<span class="line-removed"> 727     // (a) and (b), since then every OSR exit would delay the opportunity for</span>
<span class="line-removed"> 728     // every call frame to perform OSR entry. Essentially, if OSR exit happens</span>
<span class="line-removed"> 729     // frequently and the function has few loops, then the counter will never</span>
<span class="line-removed"> 730     // become non-negative and OSR entry will never be triggered. OSR entry</span>
<span class="line-removed"> 731     // will only happen if a loop gets hot in the old JIT, which does a pretty</span>
<span class="line-removed"> 732     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),</span>
<span class="line-removed"> 733     // since each speculation failure would reset the execute counter.</span>
<span class="line-removed"> 734     // So we check here if the number of speculation failures is significantly</span>
<span class="line-removed"> 735     // larger than the number of successes (we want 90% success rate), and if</span>
<span class="line-removed"> 736     // there have been a large enough number of failures. If so, we set the</span>
<span class="line-removed"> 737     // counter to 0; otherwise we set the counter to</span>
<span class="line-removed"> 738     // counterValueForOptimizeAfterWarmUp().</span>
<span class="line-removed"> 739 </span>
<span class="line-removed"> 740     if (UNLIKELY(codeBlock-&gt;updateOSRExitCounterAndCheckIfNeedToReoptimize(exitState) == CodeBlock::OptimizeAction::ReoptimizeNow))</span>
<span class="line-removed"> 741         triggerReoptimizationNow(baselineCodeBlock, codeBlock, &amp;exit);</span>
<span class="line-removed"> 742 </span>
<span class="line-removed"> 743     reifyInlinedCallFrames(context, baselineCodeBlock, exit);</span>
<span class="line-removed"> 744     adjustAndJumpToTarget(context, vm, codeBlock, baselineCodeBlock, exit);</span>
<span class="line-removed"> 745 }</span>
<span class="line-removed"> 746 </span>
<span class="line-removed"> 747 static void reifyInlinedCallFrames(Context&amp; context, CodeBlock* outermostBaselineCodeBlock, const OSRExitBase&amp; exit)</span>
<span class="line-removed"> 748 {</span>
<span class="line-removed"> 749     auto&amp; cpu = context.cpu;</span>
<span class="line-removed"> 750     Frame frame(cpu.fp(), context.stack());</span>
<span class="line-removed"> 751 </span>
<span class="line-removed"> 752     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit</span>
<span class="line-removed"> 753     // in presence of inlined tail calls.</span>
<span class="line-removed"> 754     // https://bugs.webkit.org/show_bug.cgi?id=147511</span>
<span class="line-removed"> 755     ASSERT(outermostBaselineCodeBlock-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-removed"> 756     frame.setOperand&lt;CodeBlock*&gt;(CallFrameSlot::codeBlock, outermostBaselineCodeBlock);</span>
<span class="line-removed"> 757 </span>
<span class="line-removed"> 758     const CodeOrigin* codeOrigin;</span>
<span class="line-removed"> 759     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {</span>
<span class="line-removed"> 760         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();</span>
<span class="line-removed"> 761         CodeBlock* baselineCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(*codeOrigin, outermostBaselineCodeBlock);</span>
<span class="line-removed"> 762         InlineCallFrame::Kind trueCallerCallKind;</span>
<span class="line-removed"> 763         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);</span>
<span class="line-removed"> 764         void* callerFrame = cpu.fp();</span>
<span class="line-removed"> 765 </span>
<span class="line-removed"> 766         if (!trueCaller) {</span>
<span class="line-removed"> 767             ASSERT(inlineCallFrame-&gt;isTail());</span>
<span class="line-removed"> 768             void* returnPC = frame.get&lt;void*&gt;(CallFrame::returnPCOffset());</span>
<span class="line-removed"> 769 #if CPU(ARM64E)</span>
<span class="line-removed"> 770             void* oldEntrySP = cpu.fp&lt;uint8_t*&gt;() + sizeof(CallerFrameAndPC);</span>
<span class="line-removed"> 771             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);</span>
<span class="line-removed"> 772             returnPC = retagCodePtr(returnPC, bitwise_cast&lt;PtrTag&gt;(oldEntrySP), bitwise_cast&lt;PtrTag&gt;(newEntrySP));</span>
<span class="line-removed"> 773 #endif</span>
<span class="line-removed"> 774             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), returnPC);</span>
<span class="line-removed"> 775             callerFrame = frame.get&lt;void*&gt;(CallFrame::callerFrameOffset());</span>
<span class="line-removed"> 776         } else {</span>
<span class="line-removed"> 777             CodeBlock* baselineCodeBlockForCaller = baselineCodeBlockForOriginAndBaselineCodeBlock(*trueCaller, outermostBaselineCodeBlock);</span>
<span class="line-removed"> 778             unsigned callBytecodeIndex = trueCaller-&gt;bytecodeIndex();</span>
<span class="line-removed"> 779             MacroAssemblerCodePtr&lt;JSInternalPtrTag&gt; jumpTarget;</span>
<span class="line-removed"> 780 </span>
<span class="line-removed"> 781             switch (trueCallerCallKind) {</span>
<span class="line-removed"> 782             case InlineCallFrame::Call:</span>
<span class="line-removed"> 783             case InlineCallFrame::Construct:</span>
<span class="line-removed"> 784             case InlineCallFrame::CallVarargs:</span>
<span class="line-removed"> 785             case InlineCallFrame::ConstructVarargs:</span>
<span class="line-removed"> 786             case InlineCallFrame::TailCall:</span>
<span class="line-removed"> 787             case InlineCallFrame::TailCallVarargs: {</span>
<span class="line-removed"> 788                 CallLinkInfo* callLinkInfo =</span>
<span class="line-removed"> 789                     baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);</span>
<span class="line-removed"> 790                 RELEASE_ASSERT(callLinkInfo);</span>
<span class="line-removed"> 791 </span>
<span class="line-removed"> 792                 jumpTarget = callLinkInfo-&gt;callReturnLocation();</span>
<span class="line-removed"> 793                 break;</span>
<span class="line-removed"> 794             }</span>
<span class="line-removed"> 795 </span>
<span class="line-removed"> 796             case InlineCallFrame::GetterCall:</span>
<span class="line-removed"> 797             case InlineCallFrame::SetterCall: {</span>
<span class="line-removed"> 798                 StructureStubInfo* stubInfo =</span>
<span class="line-removed"> 799                     baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));</span>
<span class="line-removed"> 800                 RELEASE_ASSERT(stubInfo);</span>
<span class="line-removed"> 801 </span>
<span class="line-removed"> 802                 jumpTarget = stubInfo-&gt;doneLocation();</span>
<span class="line-removed"> 803                 break;</span>
<span class="line-removed"> 804             }</span>
<span class="line-removed"> 805 </span>
<span class="line-removed"> 806             default:</span>
<span class="line-removed"> 807                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed"> 808             }</span>
<span class="line-removed"> 809 </span>
<span class="line-removed"> 810             if (trueCaller-&gt;inlineCallFrame())</span>
<span class="line-removed"> 811                 callerFrame = cpu.fp&lt;uint8_t*&gt;() + trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue);</span>
<span class="line-removed"> 812 </span>
<span class="line-removed"> 813             void* targetAddress = jumpTarget.executableAddress();</span>
<span class="line-removed"> 814 #if CPU(ARM64E)</span>
<span class="line-removed"> 815             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);</span>
<span class="line-removed"> 816             targetAddress = retagCodePtr(targetAddress, JSInternalPtrTag, bitwise_cast&lt;PtrTag&gt;(newEntrySP));</span>
<span class="line-removed"> 817 #endif</span>
<span class="line-removed"> 818             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), targetAddress);</span>
<span class="line-removed"> 819         }</span>
<span class="line-removed"> 820 </span>
<span class="line-removed"> 821         frame.setOperand&lt;void*&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock, baselineCodeBlock);</span>
<span class="line-removed"> 822 </span>
<span class="line-removed"> 823         // Restore the inline call frame&#39;s callee save registers.</span>
<span class="line-removed"> 824         // If this inlined frame is a tail call that will return back to the original caller, we need to</span>
<span class="line-removed"> 825         // copy the prior contents of the tag registers already saved for the outer frame to this frame.</span>
<span class="line-removed"> 826         saveOrCopyCalleeSavesFor(context, baselineCodeBlock, VirtualRegister(inlineCallFrame-&gt;stackOffset), !trueCaller);</span>
<span class="line-removed"> 827 </span>
<span class="line-removed"> 828         if (!inlineCallFrame-&gt;isVarargs())</span>
<span class="line-removed"> 829             frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, PayloadOffset, inlineCallFrame-&gt;argumentCountIncludingThis);</span>
<span class="line-removed"> 830         ASSERT(callerFrame);</span>
<span class="line-removed"> 831         frame.set&lt;void*&gt;(inlineCallFrame-&gt;callerFrameOffset(), callerFrame);</span>
<span class="line-removed"> 832 #if USE(JSVALUE64)</span>
<span class="line-removed"> 833         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
<span class="line-removed"> 834         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);</span>
<span class="line-removed"> 835         if (!inlineCallFrame-&gt;isClosureCall)</span>
<span class="line-removed"> 836             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, JSValue(inlineCallFrame-&gt;calleeConstant()));</span>
<span class="line-removed"> 837 #else // USE(JSVALUE64) // so this is the 32-bit part</span>
<span class="line-removed"> 838         const Instruction* instruction = baselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
<span class="line-removed"> 839         uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed"> 840         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);</span>
<span class="line-removed"> 841         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, TagOffset, static_cast&lt;uint32_t&gt;(JSValue::CellTag));</span>
<span class="line-removed"> 842         if (!inlineCallFrame-&gt;isClosureCall)</span>
<span class="line-removed"> 843             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, PayloadOffset, inlineCallFrame-&gt;calleeConstant());</span>
<span class="line-removed"> 844 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part</span>
<span class="line-removed"> 845     }</span>
<span class="line-removed"> 846 </span>
<span class="line-removed"> 847     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls</span>
<span class="line-removed"> 848     if (codeOrigin) {</span>
<span class="line-removed"> 849 #if USE(JSVALUE64)</span>
<span class="line-removed"> 850         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
<span class="line-removed"> 851 #else</span>
<span class="line-removed"> 852         const Instruction* instruction = outermostBaselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
<span class="line-removed"> 853         uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed"> 854 #endif</span>
<span class="line-removed"> 855         frame.setOperand&lt;uint32_t&gt;(CallFrameSlot::argumentCount, TagOffset, locationBits);</span>
<span class="line-removed"> 856     }</span>
<span class="line-removed"> 857 }</span>
<span class="line-removed"> 858 </span>
<span class="line-removed"> 859 static void adjustAndJumpToTarget(Context&amp; context, VM&amp; vm, CodeBlock* codeBlock, CodeBlock* baselineCodeBlock, OSRExit&amp; exit)</span>
<span class="line-removed"> 860 {</span>
<span class="line-removed"> 861     OSRExitState* exitState = exit.exitState.get();</span>
<span class="line-removed"> 862 </span>
<span class="line-removed"> 863     WTF::storeLoadFence(); // The optimizing compiler expects that the OSR exit mechanism will execute this fence.</span>
<span class="line-removed"> 864     vm.heap.writeBarrier(baselineCodeBlock);</span>
<span class="line-removed"> 865 </span>
<span class="line-removed"> 866     // We barrier all inlined frames -- and not just the current inline stack --</span>
<span class="line-removed"> 867     // because we don&#39;t know which inlined function owns the value profile that</span>
<span class="line-removed"> 868     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both</span>
<span class="line-removed"> 869     // a and b are inlined, we might exit inside b due to a bad value loaded</span>
<span class="line-removed"> 870     // from a.</span>
<span class="line-removed"> 871     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns</span>
<span class="line-removed"> 872     // the value profile.</span>
<span class="line-removed"> 873     InlineCallFrameSet* inlineCallFrames = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();</span>
<span class="line-removed"> 874     if (inlineCallFrames) {</span>
<span class="line-removed"> 875         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames)</span>
<span class="line-removed"> 876             vm.heap.writeBarrier(inlineCallFrame-&gt;baselineCodeBlock.get());</span>
<span class="line-removed"> 877     }</span>
<span class="line-removed"> 878 </span>
<span class="line-removed"> 879     auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();</span>
<span class="line-removed"> 880     if (exitInlineCallFrame)</span>
<span class="line-removed"> 881         context.fp() = context.fp&lt;uint8_t*&gt;() + exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue);</span>
<span class="line-removed"> 882 </span>
<span class="line-removed"> 883     void* jumpTarget = exitState-&gt;jumpTarget;</span>
<span class="line-removed"> 884     ASSERT(jumpTarget);</span>
<span class="line-removed"> 885 </span>
<span class="line-removed"> 886     if (exit.isExceptionHandler()) {</span>
<span class="line-removed"> 887         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.</span>
<span class="line-removed"> 888         vm.callFrameForCatch = context.fp&lt;ExecState*&gt;();</span>
<span class="line-removed"> 889     }</span>
<span class="line-removed"> 890 </span>
<span class="line-removed"> 891     vm.topCallFrame = context.fp&lt;ExecState*&gt;();</span>
<span class="line-removed"> 892     context.pc() = untagCodePtr&lt;JSEntryPtrTag&gt;(jumpTarget);</span>
<span class="line-removed"> 893 }</span>
<span class="line-removed"> 894 </span>
<span class="line-removed"> 895 static void printOSRExit(Context&amp; context, uint32_t osrExitIndex, const OSRExit&amp; exit)</span>
<span class="line-removed"> 896 {</span>
<span class="line-removed"> 897     ExecState* exec = context.fp&lt;ExecState*&gt;();</span>
<span class="line-removed"> 898     CodeBlock* codeBlock = exec-&gt;codeBlock();</span>
<span class="line-removed"> 899     CodeBlock* alternative = codeBlock-&gt;alternative();</span>
<span class="line-removed"> 900     ExitKind kind = exit.m_kind;</span>
<span class="line-removed"> 901     unsigned bytecodeOffset = exit.m_codeOrigin.bytecodeIndex();</span>
<span class="line-removed"> 902 </span>
<span class="line-removed"> 903     dataLog(&quot;Speculation failure in &quot;, *codeBlock);</span>
<span class="line-removed"> 904     dataLog(&quot; @ exit #&quot;, osrExitIndex, &quot; (bc#&quot;, bytecodeOffset, &quot;, &quot;, exitKindToString(kind), &quot;) with &quot;);</span>
<span class="line-removed"> 905     if (alternative) {</span>
<span class="line-removed"> 906         dataLog(</span>
<span class="line-removed"> 907             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),</span>
<span class="line-removed"> 908             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),</span>
<span class="line-removed"> 909             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());</span>
<span class="line-removed"> 910     } else</span>
<span class="line-removed"> 911         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);</span>
<span class="line-removed"> 912     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);</span>
<span class="line-removed"> 913     dataLog(&quot;    GPRs at time of exit:&quot;);</span>
<span class="line-removed"> 914     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {</span>
<span class="line-removed"> 915         GPRReg gpr = GPRInfo::toRegister(i);</span>
<span class="line-removed"> 916         dataLog(&quot; &quot;, context.gprName(gpr), &quot;:&quot;, RawPointer(context.gpr&lt;void*&gt;(gpr)));</span>
<span class="line-removed"> 917     }</span>
<span class="line-removed"> 918     dataLog(&quot;\n&quot;);</span>
<span class="line-removed"> 919     dataLog(&quot;    FPRs at time of exit:&quot;);</span>
<span class="line-removed"> 920     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {</span>
<span class="line-removed"> 921         FPRReg fpr = FPRInfo::toRegister(i);</span>
<span class="line-removed"> 922         dataLog(&quot; &quot;, context.fprName(fpr), &quot;:&quot;);</span>
<span class="line-removed"> 923         uint64_t bits = context.fpr&lt;uint64_t&gt;(fpr);</span>
<span class="line-removed"> 924         double value = context.fpr(fpr);</span>
<span class="line-removed"> 925         dataLogF(&quot;%llx:%lf&quot;, static_cast&lt;long long&gt;(bits), value);</span>
<span class="line-removed"> 926     }</span>
<span class="line-removed"> 927     dataLog(&quot;\n&quot;);</span>
<span class="line-removed"> 928 }</span>
<span class="line-removed"> 929 </span>
<span class="line-removed"> 930 // JIT based OSR Exit.</span>
<span class="line-removed"> 931 </span>
 932 OSRExit::OSRExit(ExitKind kind, JSValueSource jsValueSource, MethodOfGettingAValueProfile valueProfile, SpeculativeJIT* jit, unsigned streamIndex, unsigned recoveryIndex)
 933     : OSRExitBase(kind, jit-&gt;m_origin.forExit, jit-&gt;m_origin.semantic, jit-&gt;m_origin.wasHoisted)
 934     , m_jsValueSource(jsValueSource)
 935     , m_valueProfile(valueProfile)
 936     , m_recoveryIndex(recoveryIndex)
 937     , m_streamIndex(streamIndex)
 938 {
 939     bool canExit = jit-&gt;m_origin.exitOK;
 940     if (!canExit &amp;&amp; jit-&gt;m_currentNode) {
 941         ExitMode exitMode = mayExit(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode);
 942         canExit = exitMode == ExitMode::Exits || exitMode == ExitMode::ExitsForExceptions;
 943     }
 944     DFG_ASSERT(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode, canExit);
 945 }
 946 
 947 CodeLocationJump&lt;JSInternalPtrTag&gt; OSRExit::codeLocationForRepatch() const
 948 {
 949     return CodeLocationJump&lt;JSInternalPtrTag&gt;(m_patchableJumpLocation);
 950 }
 951 
<span class="line-modified"> 952 void OSRExit::emitRestoreArguments(CCallHelpers&amp; jit, const Operands&lt;ValueRecovery&gt;&amp; operands)</span>
 953 {
<span class="line-modified"> 954     HashMap&lt;MinifiedID, int&gt; alreadyAllocatedArguments; // Maps phantom arguments node ID to operand.</span>
 955     for (size_t index = 0; index &lt; operands.size(); ++index) {
 956         const ValueRecovery&amp; recovery = operands[index];
<span class="line-removed"> 957         int operand = operands.operandForIndex(index);</span>
 958 
 959         if (recovery.technique() != DirectArgumentsThatWereNotCreated
 960             &amp;&amp; recovery.technique() != ClonedArgumentsThatWereNotCreated)
 961             continue;
 962 




 963         MinifiedID id = recovery.nodeID();
 964         auto iter = alreadyAllocatedArguments.find(id);
 965         if (iter != alreadyAllocatedArguments.end()) {
 966             JSValueRegs regs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT1);
 967             jit.loadValue(CCallHelpers::addressFor(iter-&gt;value), regs);
 968             jit.storeValue(regs, CCallHelpers::addressFor(operand));
 969             continue;
 970         }
 971 
 972         InlineCallFrame* inlineCallFrame =
 973             jit.codeBlock()-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG.at(id)-&gt;inlineCallFrame();
 974 
 975         int stackOffset;
 976         if (inlineCallFrame)
 977             stackOffset = inlineCallFrame-&gt;stackOffset;
 978         else
 979             stackOffset = 0;
 980 
 981         if (!inlineCallFrame || inlineCallFrame-&gt;isClosureCall) {
 982             jit.loadPtr(
<span class="line-modified"> 983                 AssemblyHelpers::addressFor(stackOffset + CallFrameSlot::callee),</span>
 984                 GPRInfo::regT0);
 985         } else {
 986             jit.move(
 987                 AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeRecovery.constant().asCell()),
 988                 GPRInfo::regT0);
 989         }
 990 
 991         if (!inlineCallFrame || inlineCallFrame-&gt;isVarargs()) {
 992             jit.load32(
<span class="line-modified"> 993                 AssemblyHelpers::payloadFor(stackOffset + CallFrameSlot::argumentCount),</span>
 994                 GPRInfo::regT1);
 995         } else {
 996             jit.move(
 997                 AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis),
 998                 GPRInfo::regT1);
 999         }
1000 
1001         static_assert(std::is_same&lt;decltype(operationCreateDirectArgumentsDuringExit), decltype(operationCreateClonedArgumentsDuringExit)&gt;::value, &quot;We assume these functions have the same signature below.&quot;);
1002         jit.setupArguments&lt;decltype(operationCreateDirectArgumentsDuringExit)&gt;(
<span class="line-modified">1003             AssemblyHelpers::TrustedImmPtr(inlineCallFrame), GPRInfo::regT0, GPRInfo::regT1);</span>

1004         switch (recovery.technique()) {
1005         case DirectArgumentsThatWereNotCreated:
1006             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateDirectArgumentsDuringExit)), GPRInfo::nonArgGPR0);
1007             break;
1008         case ClonedArgumentsThatWereNotCreated:
1009             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateClonedArgumentsDuringExit)), GPRInfo::nonArgGPR0);
1010             break;
1011         default:
1012             RELEASE_ASSERT_NOT_REACHED();
1013             break;
1014         }
1015         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
1016         jit.storeCell(GPRInfo::returnValueGPR, AssemblyHelpers::addressFor(operand));
1017 
<span class="line-modified">1018         alreadyAllocatedArguments.add(id, operand);</span>
1019     }
1020 }
1021 
<span class="line-modified">1022 void JIT_OPERATION OSRExit::compileOSRExit(ExecState* exec)</span>
1023 {
<span class="line-modified">1024     VM&amp; vm = exec-&gt;vm();</span>
1025     auto scope = DECLARE_THROW_SCOPE(vm);
1026 
1027     if (validateDFGDoesGC) {
1028         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
1029         // code running that expects no GC.
1030         vm.heap.setExpectDoesGC(true);
1031     }
1032 
1033     if (vm.callFrameForCatch)
<span class="line-modified">1034         RELEASE_ASSERT(vm.callFrameForCatch == exec);</span>
1035 
<span class="line-modified">1036     CodeBlock* codeBlock = exec-&gt;codeBlock();</span>
1037     ASSERT(codeBlock);
1038     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
1039 
1040     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
1041     // really be profitable.
1042     DeferGCForAWhile deferGC(vm.heap);
1043 
1044     uint32_t exitIndex = vm.osrExitIndex;
1045     OSRExit&amp; exit = codeBlock-&gt;jitCode()-&gt;dfg()-&gt;osrExit[exitIndex];
1046 
1047     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);
1048     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
1049 
<span class="line-removed">1050     prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);</span>
<span class="line-removed">1051 </span>
1052     // Compute the value recoveries.
1053     Operands&lt;ValueRecovery&gt; operands;
1054     codeBlock-&gt;jitCode()-&gt;dfg()-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, codeBlock-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG, exit.m_streamIndex, operands);
1055 
1056     SpeculationRecovery* recovery = 0;
1057     if (exit.m_recoveryIndex != UINT_MAX)
1058         recovery = &amp;codeBlock-&gt;jitCode()-&gt;dfg()-&gt;speculationRecovery[exit.m_recoveryIndex];
1059 
1060     {
1061         CCallHelpers jit(codeBlock);
1062 
1063         if (exit.m_kind == GenericUnwind) {
1064             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
1065             // So, we must restore our call frame and stack pointer.
1066             jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
1067             jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);
1068         }
1069         jit.addPtr(
1070             CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
1071             GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
1072 
1073         jit.jitAssertHasValidCallFrame();
1074 
1075         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {
1076             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;
1077             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
1078 
1079             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
1080                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
1081                 exit.m_kind, exit.m_kind == UncountableInvalidation);
1082             jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
1083         }
1084 
<span class="line-modified">1085         compileExit(jit, vm, exit, operands, recovery);</span>
1086 
1087         LinkBuffer patchBuffer(jit, codeBlock);
1088         exit.m_code = FINALIZE_CODE_IF(
1089             shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseDFGOSRExit(),
1090             patchBuffer, OSRExitPtrTag,
1091             &quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
1092                 exitIndex, toCString(exit.m_codeOrigin).data(),
1093                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
1094                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
1095     }
1096 
1097     MacroAssembler::repatchJump(exit.codeLocationForRepatch(), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
1098 
1099     vm.osrExitJumpDestination = exit.m_code.code().executableAddress();
1100 }
1101 
1102 void OSRExit::compileExit(CCallHelpers&amp; jit, VM&amp; vm, const OSRExit&amp; exit, const Operands&lt;ValueRecovery&gt;&amp; operands, SpeculationRecovery* recovery)
1103 {
1104     jit.jitAssertTagsInPlace();
1105 
1106     // Pro-forma stuff.
<span class="line-modified">1107     if (Options::printEachOSRExit()) {</span>
1108         SpeculationFailureDebugInfo* debugInfo = new SpeculationFailureDebugInfo;
1109         debugInfo-&gt;codeBlock = jit.codeBlock();
1110         debugInfo-&gt;kind = exit.m_kind;
<span class="line-modified">1111         debugInfo-&gt;bytecodeOffset = exit.m_codeOrigin.bytecodeIndex();</span>
1112 
<span class="line-modified">1113         jit.debugCall(vm, debugOperationPrintSpeculationFailure, debugInfo);</span>
1114     }
1115 
1116     // Perform speculation recovery. This only comes into play when an operation
1117     // starts mutating state before verifying the speculation it has already made.
1118 
1119     if (recovery) {
1120         switch (recovery-&gt;type()) {
1121         case SpeculativeAdd:
1122             jit.sub32(recovery-&gt;src(), recovery-&gt;dest());
1123 #if USE(JSVALUE64)
<span class="line-modified">1124             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());</span>
1125 #endif
1126             break;
1127 
1128         case SpeculativeAddSelf:
1129             // If A + A = A (int32_t) overflows, A can be recovered by ((static_cast&lt;int32_t&gt;(A) &gt;&gt; 1) ^ 0x8000000).
1130             jit.rshift32(AssemblyHelpers::TrustedImm32(1), recovery-&gt;dest());
1131             jit.xor32(AssemblyHelpers::TrustedImm32(0x80000000), recovery-&gt;dest());
1132 #if USE(JSVALUE64)
<span class="line-modified">1133             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());</span>
1134 #endif
1135             break;
1136 
1137         case SpeculativeAddImmediate:
1138             jit.sub32(AssemblyHelpers::Imm32(recovery-&gt;immediate()), recovery-&gt;dest());
1139 #if USE(JSVALUE64)
<span class="line-modified">1140             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());</span>
1141 #endif
1142             break;
1143 
1144         case BooleanSpeculationCheck:
1145 #if USE(JSVALUE64)
<span class="line-modified">1146             jit.xor64(AssemblyHelpers::TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), recovery-&gt;dest());</span>
1147 #endif
1148             break;
1149 
1150         default:
1151             break;
1152         }
1153     }
1154 
1155     // Refine some array and/or value profile, if appropriate.
1156 
1157     if (!!exit.m_jsValueSource) {
1158         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
1159             // If the instruction that this originated from has an array profile, then
1160             // refine it. If it doesn&#39;t, then do nothing. The latter could happen for
1161             // hoisted checks, or checks emitted for operations that didn&#39;t have array
1162             // profiling - either ops that aren&#39;t array accesses at all, or weren&#39;t
1163             // known to be array acceses in the bytecode. The latter case is a FIXME
1164             // while the former case is an outcome of a CheckStructure not knowing why
1165             // it was emitted (could be either due to an inline cache of a property
1166             // property access, or due to an array profile).
1167 
1168             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<span class="line-modified">1169             if (ArrayProfile* arrayProfile = jit.baselineCodeBlockFor(codeOrigin)-&gt;getArrayProfile(codeOrigin.bytecodeIndex())) {</span>








1170 #if USE(JSVALUE64)
1171                 GPRReg usedRegister;
1172                 if (exit.m_jsValueSource.isAddress())
1173                     usedRegister = exit.m_jsValueSource.base();
1174                 else
1175                     usedRegister = exit.m_jsValueSource.gpr();
1176 #else
1177                 GPRReg usedRegister1;
1178                 GPRReg usedRegister2;
1179                 if (exit.m_jsValueSource.isAddress()) {
1180                     usedRegister1 = exit.m_jsValueSource.base();
1181                     usedRegister2 = InvalidGPRReg;
1182                 } else {
1183                     usedRegister1 = exit.m_jsValueSource.payloadGPR();
1184                     if (exit.m_jsValueSource.hasKnownTag())
1185                         usedRegister2 = InvalidGPRReg;
1186                     else
1187                         usedRegister2 = exit.m_jsValueSource.tagGPR();
1188                 }
1189 #endif
</pre>
<hr />
<pre>
1225 
1226                 notTypedArray.link(&amp;jit);
1227 #if USE(JSVALUE64)
1228                 jit.load8(AssemblyHelpers::Address(value, JSCell::indexingTypeAndMiscOffset()), scratch1);
1229 #else
1230                 jit.load8(AssemblyHelpers::Address(scratch1, Structure::indexingModeIncludingHistoryOffset()), scratch1);
1231 #endif
1232                 jit.and32(AssemblyHelpers::TrustedImm32(IndexingModeMask), scratch1);
1233                 jit.move(AssemblyHelpers::TrustedImm32(1), scratch2);
1234                 jit.lshift32(scratch1, scratch2);
1235                 storeArrayModes.link(&amp;jit);
1236                 jit.or32(scratch2, AssemblyHelpers::AbsoluteAddress(arrayProfile-&gt;addressOfArrayModes()));
1237 
1238                 if (isARM64()) {
1239                     jit.popToRestore(scratch2);
1240                     jit.popToRestore(scratch1);
1241                 } else {
1242                     jit.pop(scratch2);
1243                     jit.pop(scratch1);
1244                 }



1245             }
1246         }
1247 
1248         if (MethodOfGettingAValueProfile profile = exit.m_valueProfile) {
1249 #if USE(JSVALUE64)
1250             if (exit.m_jsValueSource.isAddress()) {
<span class="line-modified">1251                 // We can&#39;t be sure that we have a spare register. So use the tagTypeNumberRegister,</span>
1252                 // since we know how to restore it.
<span class="line-modified">1253                 jit.load64(AssemblyHelpers::Address(exit.m_jsValueSource.asAddress()), GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified">1254                 profile.emitReportValue(jit, JSValueRegs(GPRInfo::tagTypeNumberRegister));</span>
<span class="line-modified">1255                 jit.move(AssemblyHelpers::TrustedImm64(TagTypeNumber), GPRInfo::tagTypeNumberRegister);</span>
1256             } else
1257                 profile.emitReportValue(jit, JSValueRegs(exit.m_jsValueSource.gpr()));
1258 #else // not USE(JSVALUE64)
1259             if (exit.m_jsValueSource.isAddress()) {
1260                 // Save a register so we can use it.
1261                 GPRReg scratchPayload = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base());
1262                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base(), scratchPayload);
1263                 jit.pushToSave(scratchPayload);
1264                 jit.pushToSave(scratchTag);
1265 
1266                 JSValueRegs scratch(scratchTag, scratchPayload);
1267 
1268                 jit.loadValue(exit.m_jsValueSource.asAddress(), scratch);
1269                 profile.emitReportValue(jit, scratch);
1270 
1271                 jit.popToRestore(scratchTag);
1272                 jit.popToRestore(scratchPayload);
1273             } else if (exit.m_jsValueSource.hasKnownTag()) {
1274                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.payloadGPR());
1275                 jit.pushToSave(scratchTag);
</pre>
<hr />
<pre>
1432     }
1433 
1434     // Need to ensure that the stack pointer accounts for the worst-case stack usage at exit. This
1435     // could toast some stack that the DFG used. We need to do it before storing to stack offsets
1436     // used by baseline.
1437     jit.addPtr(
1438         CCallHelpers::TrustedImm32(
1439             -jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit * sizeof(Register)),
1440         CCallHelpers::framePointerRegister, CCallHelpers::stackPointerRegister);
1441 
1442     // Restore the DFG callee saves and then save the ones the baseline JIT uses.
1443     jit.emitRestoreCalleeSaves();
1444     jit.emitSaveCalleeSavesFor(jit.baselineCodeBlock());
1445 
1446     // The tag registers are needed to materialize recoveries below.
1447     jit.emitMaterializeTagCheckRegisters();
1448 
1449     if (exit.isExceptionHandler())
1450         jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
1451 

















































































1452     // Do all data format conversions and store the results into the stack.
1453 
1454     for (size_t index = 0; index &lt; operands.size(); ++index) {
1455         const ValueRecovery&amp; recovery = operands[index];
<span class="line-modified">1456         VirtualRegister reg = operands.virtualRegisterForIndex(index);</span>
<span class="line-modified">1457 </span>
<span class="line-removed">1458         if (reg.isLocal() &amp;&amp; reg.toLocal() &lt; static_cast&lt;int&gt;(jit.baselineCodeBlock()-&gt;calleeSaveSpaceAsVirtualRegisters()))</span>
1459             continue;
1460 
<span class="line-modified">1461         int operand = reg.offset();</span>

1462 
1463         switch (recovery.technique()) {
1464         case DisplacedInJSStack:
1465         case InFPR:
1466 #if USE(JSVALUE64)
1467         case InGPR:
1468         case UnboxedCellInGPR:
1469         case CellDisplacedInJSStack:
1470         case BooleanDisplacedInJSStack:
1471             jit.load64(scratch + index, GPRInfo::regT0);
1472             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1473             break;
1474 #else // not USE(JSVALUE64)
1475         case InPair:
1476             jit.load32(
1477                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag,
1478                 GPRInfo::regT0);
1479             jit.load32(
1480                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1481                 GPRInfo::regT1);
</pre>
<hr />
<pre>
1502 
1503         case UnboxedBooleanInGPR:
1504         case BooleanDisplacedInJSStack:
1505             jit.load32(
1506                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1507                 GPRInfo::regT0);
1508             jit.store32(
1509                 AssemblyHelpers::TrustedImm32(JSValue::BooleanTag),
1510                 AssemblyHelpers::tagFor(operand));
1511             jit.store32(
1512                 GPRInfo::regT0,
1513                 AssemblyHelpers::payloadFor(operand));
1514             break;
1515 #endif // USE(JSVALUE64)
1516 
1517         case UnboxedInt32InGPR:
1518         case Int32DisplacedInJSStack:
1519 #if USE(JSVALUE64)
1520             jit.load64(scratch + index, GPRInfo::regT0);
1521             jit.zeroExtend32ToPtr(GPRInfo::regT0, GPRInfo::regT0);
<span class="line-modified">1522             jit.or64(GPRInfo::tagTypeNumberRegister, GPRInfo::regT0);</span>
1523             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1524 #else
1525             jit.load32(
1526                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1527                 GPRInfo::regT0);
1528             jit.store32(
1529                 AssemblyHelpers::TrustedImm32(JSValue::Int32Tag),
1530                 AssemblyHelpers::tagFor(operand));
1531             jit.store32(
1532                 GPRInfo::regT0,
1533                 AssemblyHelpers::payloadFor(operand));
1534 #endif
1535             break;
1536 
1537 #if USE(JSVALUE64)
1538         case UnboxedInt52InGPR:
1539         case Int52DisplacedInJSStack:
1540             jit.load64(scratch + index, GPRInfo::regT0);
1541             jit.rshift64(
1542                 AssemblyHelpers::TrustedImm32(JSValue::int52ShiftAmount), GPRInfo::regT0);
</pre>
<hr />
<pre>
1580 #endif
1581             break;
1582 
1583         case DirectArgumentsThatWereNotCreated:
1584         case ClonedArgumentsThatWereNotCreated:
1585             // Don&#39;t do this, yet.
1586             break;
1587 
1588         default:
1589             RELEASE_ASSERT_NOT_REACHED();
1590             break;
1591         }
1592     }
1593 
1594     // Now that things on the stack are recovered, do the arguments recovery. We assume that arguments
1595     // recoveries don&#39;t recursively refer to each other. But, we don&#39;t try to assume that they only
1596     // refer to certain ranges of locals. Hence why we need to do this here, once the stack is sensible.
1597     // Note that we also roughly assume that the arguments might still be materialized outside of its
1598     // inline call frame scope - but for now the DFG wouldn&#39;t do that.
1599 
<span class="line-modified">1600     emitRestoreArguments(jit, operands);</span>
1601 
1602     // Adjust the old JIT&#39;s execute counter. Since we are exiting OSR, we know
1603     // that all new calls into this code will go to the new JIT, so the execute
1604     // counter only affects call frames that performed OSR exit and call frames
1605     // that were still executing the old JIT at the time of another call frame&#39;s
1606     // OSR exit. We want to ensure that the following is true:
1607     //
1608     // (a) Code the performs an OSR exit gets a chance to reenter optimized
1609     //     code eventually, since optimized code is faster. But we don&#39;t
1610     //     want to do such reentery too aggressively (see (c) below).
1611     //
1612     // (b) If there is code on the call stack that is still running the old
1613     //     JIT&#39;s code and has never OSR&#39;d, then it should get a chance to
1614     //     perform OSR entry despite the fact that we&#39;ve exited.
1615     //
1616     // (c) Code the performs an OSR exit should not immediately retry OSR
1617     //     entry, since both forms of OSR are expensive. OSR entry is
1618     //     particularly expensive.
1619     //
1620     // (d) Frequent OSR failures, even those that do not result in the code
1621     //     running in a hot loop, result in recompilation getting triggered.
1622     //
1623     // To ensure (c), we&#39;d like to set the execute counter to
1624     // counterValueForOptimizeAfterWarmUp(). This seems like it would endanger
1625     // (a) and (b), since then every OSR exit would delay the opportunity for
1626     // every call frame to perform OSR entry. Essentially, if OSR exit happens
1627     // frequently and the function has few loops, then the counter will never
1628     // become non-negative and OSR entry will never be triggered. OSR entry
1629     // will only happen if a loop gets hot in the old JIT, which does a pretty
1630     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
1631     // since each speculation failure would reset the execute counter.
1632     // So we check here if the number of speculation failures is significantly
1633     // larger than the number of successes (we want 90% success rate), and if
1634     // there have been a large enough number of failures. If so, we set the
1635     // counter to 0; otherwise we set the counter to
1636     // counterValueForOptimizeAfterWarmUp().
1637 
<span class="line-modified">1638     handleExitCounts(jit, exit);</span>
1639 
1640     // Reify inlined call frames.
1641 
1642     reifyInlinedCallFrames(jit, exit);
1643 
1644     // And finish.
1645     adjustAndJumpToTarget(vm, jit, exit);
1646 }
1647 
<span class="line-modified">1648 void JIT_OPERATION OSRExit::debugOperationPrintSpeculationFailure(ExecState* exec, void* debugInfoRaw, void* scratch)</span>
1649 {
<span class="line-modified">1650     VM&amp; vm = exec-&gt;vm();</span>
<span class="line-modified">1651     NativeCallFrameTracer tracer(vm, exec);</span>
1652 
1653     SpeculationFailureDebugInfo* debugInfo = static_cast&lt;SpeculationFailureDebugInfo*&gt;(debugInfoRaw);
1654     CodeBlock* codeBlock = debugInfo-&gt;codeBlock;
1655     CodeBlock* alternative = codeBlock-&gt;alternative();
1656     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
<span class="line-modified">1657     dataLog(&quot; @ exit #&quot;, vm.osrExitIndex, &quot; (bc#&quot;, debugInfo-&gt;bytecodeOffset, &quot;, &quot;, exitKindToString(debugInfo-&gt;kind), &quot;) with &quot;);</span>
1658     if (alternative) {
1659         dataLog(
1660             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
1661             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
1662             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
1663     } else
1664         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
1665     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
1666     dataLog(&quot;    GPRs at time of exit:&quot;);
1667     char* scratchPointer = static_cast&lt;char*&gt;(scratch);
1668     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
1669         GPRReg gpr = GPRInfo::toRegister(i);
1670         dataLog(&quot; &quot;, GPRInfo::debugName(gpr), &quot;:&quot;, RawPointer(*reinterpret_cast_ptr&lt;void**&gt;(scratchPointer)));
1671         scratchPointer += sizeof(EncodedJSValue);
1672     }
1673     dataLog(&quot;\n&quot;);
1674     dataLog(&quot;    FPRs at time of exit:&quot;);
1675     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
1676         FPRReg fpr = FPRInfo::toRegister(i);
1677         dataLog(&quot; &quot;, FPRInfo::debugName(fpr), &quot;:&quot;);
</pre>
</td>
<td>
<hr />
<pre>
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGOSRExit.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;AssemblyHelpers.h&quot;
<span class="line-added">  32 #include &quot;BytecodeUseDef.h&quot;</span>
<span class="line-added">  33 #include &quot;CheckpointOSRExitSideState.h&quot;</span>
  34 #include &quot;ClonedArguments.h&quot;
  35 #include &quot;DFGGraph.h&quot;
  36 #include &quot;DFGMayExit.h&quot;
  37 #include &quot;DFGOSRExitCompilerCommon.h&quot;

  38 #include &quot;DFGOperations.h&quot;
  39 #include &quot;DFGSpeculativeJIT.h&quot;
  40 #include &quot;DirectArguments.h&quot;
  41 #include &quot;FrameTracers.h&quot;
  42 #include &quot;InlineCallFrame.h&quot;
  43 #include &quot;JSCInlines.h&quot;
  44 #include &quot;JSCJSValue.h&quot;
  45 #include &quot;OperandsInlines.h&quot;
  46 #include &quot;ProbeContext.h&quot;
  47 #include &quot;ProbeFrame.h&quot;
  48 
  49 namespace JSC { namespace DFG {
  50 


















































































































































































































































































































































































































































































































































































































































































































































































































































































































  51 OSRExit::OSRExit(ExitKind kind, JSValueSource jsValueSource, MethodOfGettingAValueProfile valueProfile, SpeculativeJIT* jit, unsigned streamIndex, unsigned recoveryIndex)
  52     : OSRExitBase(kind, jit-&gt;m_origin.forExit, jit-&gt;m_origin.semantic, jit-&gt;m_origin.wasHoisted)
  53     , m_jsValueSource(jsValueSource)
  54     , m_valueProfile(valueProfile)
  55     , m_recoveryIndex(recoveryIndex)
  56     , m_streamIndex(streamIndex)
  57 {
  58     bool canExit = jit-&gt;m_origin.exitOK;
  59     if (!canExit &amp;&amp; jit-&gt;m_currentNode) {
  60         ExitMode exitMode = mayExit(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode);
  61         canExit = exitMode == ExitMode::Exits || exitMode == ExitMode::ExitsForExceptions;
  62     }
  63     DFG_ASSERT(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode, canExit);
  64 }
  65 
  66 CodeLocationJump&lt;JSInternalPtrTag&gt; OSRExit::codeLocationForRepatch() const
  67 {
  68     return CodeLocationJump&lt;JSInternalPtrTag&gt;(m_patchableJumpLocation);
  69 }
  70 
<span class="line-modified">  71 void OSRExit::emitRestoreArguments(CCallHelpers&amp; jit, VM&amp; vm, const Operands&lt;ValueRecovery&gt;&amp; operands)</span>
  72 {
<span class="line-modified">  73     HashMap&lt;MinifiedID, VirtualRegister&gt; alreadyAllocatedArguments; // Maps phantom arguments node ID to operand.</span>
  74     for (size_t index = 0; index &lt; operands.size(); ++index) {
  75         const ValueRecovery&amp; recovery = operands[index];

  76 
  77         if (recovery.technique() != DirectArgumentsThatWereNotCreated
  78             &amp;&amp; recovery.technique() != ClonedArgumentsThatWereNotCreated)
  79             continue;
  80 
<span class="line-added">  81         Operand operand = operands.operandForIndex(index);</span>
<span class="line-added">  82         if (operand.isTmp())</span>
<span class="line-added">  83             continue;</span>
<span class="line-added">  84 </span>
  85         MinifiedID id = recovery.nodeID();
  86         auto iter = alreadyAllocatedArguments.find(id);
  87         if (iter != alreadyAllocatedArguments.end()) {
  88             JSValueRegs regs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT1);
  89             jit.loadValue(CCallHelpers::addressFor(iter-&gt;value), regs);
  90             jit.storeValue(regs, CCallHelpers::addressFor(operand));
  91             continue;
  92         }
  93 
  94         InlineCallFrame* inlineCallFrame =
  95             jit.codeBlock()-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG.at(id)-&gt;inlineCallFrame();
  96 
  97         int stackOffset;
  98         if (inlineCallFrame)
  99             stackOffset = inlineCallFrame-&gt;stackOffset;
 100         else
 101             stackOffset = 0;
 102 
 103         if (!inlineCallFrame || inlineCallFrame-&gt;isClosureCall) {
 104             jit.loadPtr(
<span class="line-modified"> 105                 AssemblyHelpers::addressFor(VirtualRegister(stackOffset + CallFrameSlot::callee)),</span>
 106                 GPRInfo::regT0);
 107         } else {
 108             jit.move(
 109                 AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeRecovery.constant().asCell()),
 110                 GPRInfo::regT0);
 111         }
 112 
 113         if (!inlineCallFrame || inlineCallFrame-&gt;isVarargs()) {
 114             jit.load32(
<span class="line-modified"> 115                 AssemblyHelpers::payloadFor(VirtualRegister(stackOffset + CallFrameSlot::argumentCountIncludingThis)),</span>
 116                 GPRInfo::regT1);
 117         } else {
 118             jit.move(
 119                 AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis),
 120                 GPRInfo::regT1);
 121         }
 122 
 123         static_assert(std::is_same&lt;decltype(operationCreateDirectArgumentsDuringExit), decltype(operationCreateClonedArgumentsDuringExit)&gt;::value, &quot;We assume these functions have the same signature below.&quot;);
 124         jit.setupArguments&lt;decltype(operationCreateDirectArgumentsDuringExit)&gt;(
<span class="line-modified"> 125             AssemblyHelpers::TrustedImmPtr(&amp;vm), AssemblyHelpers::TrustedImmPtr(inlineCallFrame), GPRInfo::regT0, GPRInfo::regT1);</span>
<span class="line-added"> 126         jit.prepareCallOperation(vm);</span>
 127         switch (recovery.technique()) {
 128         case DirectArgumentsThatWereNotCreated:
 129             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateDirectArgumentsDuringExit)), GPRInfo::nonArgGPR0);
 130             break;
 131         case ClonedArgumentsThatWereNotCreated:
 132             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateClonedArgumentsDuringExit)), GPRInfo::nonArgGPR0);
 133             break;
 134         default:
 135             RELEASE_ASSERT_NOT_REACHED();
 136             break;
 137         }
 138         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 139         jit.storeCell(GPRInfo::returnValueGPR, AssemblyHelpers::addressFor(operand));
 140 
<span class="line-modified"> 141         alreadyAllocatedArguments.add(id, operand.virtualRegister());</span>
 142     }
 143 }
 144 
<span class="line-modified"> 145 void JIT_OPERATION operationCompileOSRExit(CallFrame* callFrame)</span>
 146 {
<span class="line-modified"> 147     VM&amp; vm = callFrame-&gt;deprecatedVM();</span>
 148     auto scope = DECLARE_THROW_SCOPE(vm);
 149 
 150     if (validateDFGDoesGC) {
 151         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
 152         // code running that expects no GC.
 153         vm.heap.setExpectDoesGC(true);
 154     }
 155 
 156     if (vm.callFrameForCatch)
<span class="line-modified"> 157         RELEASE_ASSERT(vm.callFrameForCatch == callFrame);</span>
 158 
<span class="line-modified"> 159     CodeBlock* codeBlock = callFrame-&gt;codeBlock();</span>
 160     ASSERT(codeBlock);
 161     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);
 162 
 163     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
 164     // really be profitable.
 165     DeferGCForAWhile deferGC(vm.heap);
 166 
 167     uint32_t exitIndex = vm.osrExitIndex;
 168     OSRExit&amp; exit = codeBlock-&gt;jitCode()-&gt;dfg()-&gt;osrExit[exitIndex];
 169 
 170     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);
 171     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
 172 


 173     // Compute the value recoveries.
 174     Operands&lt;ValueRecovery&gt; operands;
 175     codeBlock-&gt;jitCode()-&gt;dfg()-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, codeBlock-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG, exit.m_streamIndex, operands);
 176 
 177     SpeculationRecovery* recovery = 0;
 178     if (exit.m_recoveryIndex != UINT_MAX)
 179         recovery = &amp;codeBlock-&gt;jitCode()-&gt;dfg()-&gt;speculationRecovery[exit.m_recoveryIndex];
 180 
 181     {
 182         CCallHelpers jit(codeBlock);
 183 
 184         if (exit.m_kind == GenericUnwind) {
 185             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
 186             // So, we must restore our call frame and stack pointer.
 187             jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
 188             jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);
 189         }
 190         jit.addPtr(
 191             CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
 192             GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
 193 
 194         jit.jitAssertHasValidCallFrame();
 195 
 196         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {
 197             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;
 198             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
 199 
 200             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
 201                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
 202                 exit.m_kind, exit.m_kind == UncountableInvalidation);
 203             jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
 204         }
 205 
<span class="line-modified"> 206         OSRExit::compileExit(jit, vm, exit, operands, recovery);</span>
 207 
 208         LinkBuffer patchBuffer(jit, codeBlock);
 209         exit.m_code = FINALIZE_CODE_IF(
 210             shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseDFGOSRExit(),
 211             patchBuffer, OSRExitPtrTag,
 212             &quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
 213                 exitIndex, toCString(exit.m_codeOrigin).data(),
 214                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
 215                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
 216     }
 217 
 218     MacroAssembler::repatchJump(exit.codeLocationForRepatch(), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
 219 
 220     vm.osrExitJumpDestination = exit.m_code.code().executableAddress();
 221 }
 222 
 223 void OSRExit::compileExit(CCallHelpers&amp; jit, VM&amp; vm, const OSRExit&amp; exit, const Operands&lt;ValueRecovery&gt;&amp; operands, SpeculationRecovery* recovery)
 224 {
 225     jit.jitAssertTagsInPlace();
 226 
 227     // Pro-forma stuff.
<span class="line-modified"> 228     if (UNLIKELY(Options::printEachOSRExit())) {</span>
 229         SpeculationFailureDebugInfo* debugInfo = new SpeculationFailureDebugInfo;
 230         debugInfo-&gt;codeBlock = jit.codeBlock();
 231         debugInfo-&gt;kind = exit.m_kind;
<span class="line-modified"> 232         debugInfo-&gt;bytecodeIndex = exit.m_codeOrigin.bytecodeIndex();</span>
 233 
<span class="line-modified"> 234         jit.debugCall(vm, operationDebugPrintSpeculationFailure, debugInfo);</span>
 235     }
 236 
 237     // Perform speculation recovery. This only comes into play when an operation
 238     // starts mutating state before verifying the speculation it has already made.
 239 
 240     if (recovery) {
 241         switch (recovery-&gt;type()) {
 242         case SpeculativeAdd:
 243             jit.sub32(recovery-&gt;src(), recovery-&gt;dest());
 244 #if USE(JSVALUE64)
<span class="line-modified"> 245             jit.or64(GPRInfo::numberTagRegister, recovery-&gt;dest());</span>
 246 #endif
 247             break;
 248 
 249         case SpeculativeAddSelf:
 250             // If A + A = A (int32_t) overflows, A can be recovered by ((static_cast&lt;int32_t&gt;(A) &gt;&gt; 1) ^ 0x8000000).
 251             jit.rshift32(AssemblyHelpers::TrustedImm32(1), recovery-&gt;dest());
 252             jit.xor32(AssemblyHelpers::TrustedImm32(0x80000000), recovery-&gt;dest());
 253 #if USE(JSVALUE64)
<span class="line-modified"> 254             jit.or64(GPRInfo::numberTagRegister, recovery-&gt;dest());</span>
 255 #endif
 256             break;
 257 
 258         case SpeculativeAddImmediate:
 259             jit.sub32(AssemblyHelpers::Imm32(recovery-&gt;immediate()), recovery-&gt;dest());
 260 #if USE(JSVALUE64)
<span class="line-modified"> 261             jit.or64(GPRInfo::numberTagRegister, recovery-&gt;dest());</span>
 262 #endif
 263             break;
 264 
 265         case BooleanSpeculationCheck:
 266 #if USE(JSVALUE64)
<span class="line-modified"> 267             jit.xor64(AssemblyHelpers::TrustedImm32(JSValue::ValueFalse), recovery-&gt;dest());</span>
 268 #endif
 269             break;
 270 
 271         default:
 272             break;
 273         }
 274     }
 275 
 276     // Refine some array and/or value profile, if appropriate.
 277 
 278     if (!!exit.m_jsValueSource) {
 279         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
 280             // If the instruction that this originated from has an array profile, then
 281             // refine it. If it doesn&#39;t, then do nothing. The latter could happen for
 282             // hoisted checks, or checks emitted for operations that didn&#39;t have array
 283             // profiling - either ops that aren&#39;t array accesses at all, or weren&#39;t
 284             // known to be array acceses in the bytecode. The latter case is a FIXME
 285             // while the former case is an outcome of a CheckStructure not knowing why
 286             // it was emitted (could be either due to an inline cache of a property
 287             // property access, or due to an array profile).
 288 
 289             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<span class="line-modified"> 290             CodeBlock* codeBlock = jit.baselineCodeBlockFor(codeOrigin);</span>
<span class="line-added"> 291             if (ArrayProfile* arrayProfile = codeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex())) {</span>
<span class="line-added"> 292                 const Instruction* instruction = codeBlock-&gt;instructions().at(codeOrigin.bytecodeIndex()).ptr();</span>
<span class="line-added"> 293                 CCallHelpers::Jump skipProfile;</span>
<span class="line-added"> 294                 if (instruction-&gt;is&lt;OpGetById&gt;()) {</span>
<span class="line-added"> 295                     auto&amp; metadata = instruction-&gt;as&lt;OpGetById&gt;().metadata(codeBlock);</span>
<span class="line-added"> 296                     skipProfile = jit.branch8(CCallHelpers::NotEqual, CCallHelpers::AbsoluteAddress(&amp;metadata.m_modeMetadata.mode), CCallHelpers::TrustedImm32(static_cast&lt;uint8_t&gt;(GetByIdMode::ArrayLength)));</span>
<span class="line-added"> 297                 }</span>
<span class="line-added"> 298 </span>
 299 #if USE(JSVALUE64)
 300                 GPRReg usedRegister;
 301                 if (exit.m_jsValueSource.isAddress())
 302                     usedRegister = exit.m_jsValueSource.base();
 303                 else
 304                     usedRegister = exit.m_jsValueSource.gpr();
 305 #else
 306                 GPRReg usedRegister1;
 307                 GPRReg usedRegister2;
 308                 if (exit.m_jsValueSource.isAddress()) {
 309                     usedRegister1 = exit.m_jsValueSource.base();
 310                     usedRegister2 = InvalidGPRReg;
 311                 } else {
 312                     usedRegister1 = exit.m_jsValueSource.payloadGPR();
 313                     if (exit.m_jsValueSource.hasKnownTag())
 314                         usedRegister2 = InvalidGPRReg;
 315                     else
 316                         usedRegister2 = exit.m_jsValueSource.tagGPR();
 317                 }
 318 #endif
</pre>
<hr />
<pre>
 354 
 355                 notTypedArray.link(&amp;jit);
 356 #if USE(JSVALUE64)
 357                 jit.load8(AssemblyHelpers::Address(value, JSCell::indexingTypeAndMiscOffset()), scratch1);
 358 #else
 359                 jit.load8(AssemblyHelpers::Address(scratch1, Structure::indexingModeIncludingHistoryOffset()), scratch1);
 360 #endif
 361                 jit.and32(AssemblyHelpers::TrustedImm32(IndexingModeMask), scratch1);
 362                 jit.move(AssemblyHelpers::TrustedImm32(1), scratch2);
 363                 jit.lshift32(scratch1, scratch2);
 364                 storeArrayModes.link(&amp;jit);
 365                 jit.or32(scratch2, AssemblyHelpers::AbsoluteAddress(arrayProfile-&gt;addressOfArrayModes()));
 366 
 367                 if (isARM64()) {
 368                     jit.popToRestore(scratch2);
 369                     jit.popToRestore(scratch1);
 370                 } else {
 371                     jit.pop(scratch2);
 372                     jit.pop(scratch1);
 373                 }
<span class="line-added"> 374 </span>
<span class="line-added"> 375                 if (skipProfile.isSet())</span>
<span class="line-added"> 376                     skipProfile.link(&amp;jit);</span>
 377             }
 378         }
 379 
 380         if (MethodOfGettingAValueProfile profile = exit.m_valueProfile) {
 381 #if USE(JSVALUE64)
 382             if (exit.m_jsValueSource.isAddress()) {
<span class="line-modified"> 383                 // We can&#39;t be sure that we have a spare register. So use the numberTagRegister,</span>
 384                 // since we know how to restore it.
<span class="line-modified"> 385                 jit.load64(AssemblyHelpers::Address(exit.m_jsValueSource.asAddress()), GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 386                 profile.emitReportValue(jit, JSValueRegs(GPRInfo::numberTagRegister));</span>
<span class="line-modified"> 387                 jit.move(AssemblyHelpers::TrustedImm64(JSValue::NumberTag), GPRInfo::numberTagRegister);</span>
 388             } else
 389                 profile.emitReportValue(jit, JSValueRegs(exit.m_jsValueSource.gpr()));
 390 #else // not USE(JSVALUE64)
 391             if (exit.m_jsValueSource.isAddress()) {
 392                 // Save a register so we can use it.
 393                 GPRReg scratchPayload = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base());
 394                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base(), scratchPayload);
 395                 jit.pushToSave(scratchPayload);
 396                 jit.pushToSave(scratchTag);
 397 
 398                 JSValueRegs scratch(scratchTag, scratchPayload);
 399 
 400                 jit.loadValue(exit.m_jsValueSource.asAddress(), scratch);
 401                 profile.emitReportValue(jit, scratch);
 402 
 403                 jit.popToRestore(scratchTag);
 404                 jit.popToRestore(scratchPayload);
 405             } else if (exit.m_jsValueSource.hasKnownTag()) {
 406                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.payloadGPR());
 407                 jit.pushToSave(scratchTag);
</pre>
<hr />
<pre>
 564     }
 565 
 566     // Need to ensure that the stack pointer accounts for the worst-case stack usage at exit. This
 567     // could toast some stack that the DFG used. We need to do it before storing to stack offsets
 568     // used by baseline.
 569     jit.addPtr(
 570         CCallHelpers::TrustedImm32(
 571             -jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit * sizeof(Register)),
 572         CCallHelpers::framePointerRegister, CCallHelpers::stackPointerRegister);
 573 
 574     // Restore the DFG callee saves and then save the ones the baseline JIT uses.
 575     jit.emitRestoreCalleeSaves();
 576     jit.emitSaveCalleeSavesFor(jit.baselineCodeBlock());
 577 
 578     // The tag registers are needed to materialize recoveries below.
 579     jit.emitMaterializeTagCheckRegisters();
 580 
 581     if (exit.isExceptionHandler())
 582         jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
 583 
<span class="line-added"> 584     if (exit.m_codeOrigin.inlineStackContainsActiveCheckpoint()) {</span>
<span class="line-added"> 585         // FIXME: Maybe we shouldn&#39;t use a probe but filling all the side state objects is tricky otherwise...</span>
<span class="line-added"> 586         Vector&lt;ValueRecovery&gt; values(operands.numberOfTmps());</span>
<span class="line-added"> 587         for (size_t i = 0; i &lt; operands.numberOfTmps(); ++i)</span>
<span class="line-added"> 588             values[i] = operands.tmp(i);</span>
<span class="line-added"> 589 </span>
<span class="line-added"> 590         VM* vmPtr = &amp;vm;</span>
<span class="line-added"> 591         auto* tmpScratch = scratch + operands.tmpIndex(0);</span>
<span class="line-added"> 592         jit.probe([=, values = WTFMove(values)] (Probe::Context&amp; context) {</span>
<span class="line-added"> 593             auto addSideState = [&amp;] (CallFrame* frame, BytecodeIndex index, size_t tmpOffset) {</span>
<span class="line-added"> 594                 std::unique_ptr&lt;CheckpointOSRExitSideState&gt; sideState = WTF::makeUnique&lt;CheckpointOSRExitSideState&gt;();</span>
<span class="line-added"> 595 </span>
<span class="line-added"> 596                 sideState-&gt;bytecodeIndex = index;</span>
<span class="line-added"> 597                 for (size_t i = 0; i &lt; maxNumCheckpointTmps; ++i) {</span>
<span class="line-added"> 598                     auto&amp; recovery = values[i + tmpOffset];</span>
<span class="line-added"> 599                     // FIXME: We should do what the FTL does and materialize all the JSValues into the scratch buffer.</span>
<span class="line-added"> 600                     switch (recovery.technique()) {</span>
<span class="line-added"> 601                     case Constant:</span>
<span class="line-added"> 602                         sideState-&gt;tmps[i] = recovery.constant();</span>
<span class="line-added"> 603                         break;</span>
<span class="line-added"> 604 </span>
<span class="line-added"> 605                     case UnboxedInt32InGPR:</span>
<span class="line-added"> 606                     case Int32DisplacedInJSStack: {</span>
<span class="line-added"> 607                         sideState-&gt;tmps[i] = jsNumber(static_cast&lt;int32_t&gt;(tmpScratch[i + tmpOffset]));</span>
<span class="line-added"> 608                         break;</span>
<span class="line-added"> 609                     }</span>
<span class="line-added"> 610 </span>
<span class="line-added"> 611 #if USE(JSVALUE32_64)</span>
<span class="line-added"> 612                     case InPair:</span>
<span class="line-added"> 613 #endif</span>
<span class="line-added"> 614                     case InGPR:</span>
<span class="line-added"> 615                     case BooleanDisplacedInJSStack:</span>
<span class="line-added"> 616                     case CellDisplacedInJSStack:</span>
<span class="line-added"> 617                     case DisplacedInJSStack: {</span>
<span class="line-added"> 618                         sideState-&gt;tmps[i] = reinterpret_cast&lt;JSValue*&gt;(tmpScratch)[i + tmpOffset];</span>
<span class="line-added"> 619                         break;</span>
<span class="line-added"> 620                     }</span>
<span class="line-added"> 621 </span>
<span class="line-added"> 622                     case UnboxedCellInGPR: {</span>
<span class="line-added"> 623 #if USE(JSVALUE64)</span>
<span class="line-added"> 624                         sideState-&gt;tmps[i] = reinterpret_cast&lt;JSValue*&gt;(tmpScratch)[i + tmpOffset];</span>
<span class="line-added"> 625 #else</span>
<span class="line-added"> 626                         EncodedValueDescriptor* valueDescriptor = bitwise_cast&lt;EncodedValueDescriptor*&gt;(tmpScratch + i + tmpOffset);</span>
<span class="line-added"> 627                         sideState-&gt;tmps[i] = JSValue(JSValue::CellTag, valueDescriptor-&gt;asBits.payload);</span>
<span class="line-added"> 628 #endif</span>
<span class="line-added"> 629                         break;</span>
<span class="line-added"> 630                     }</span>
<span class="line-added"> 631 </span>
<span class="line-added"> 632                     case UnboxedBooleanInGPR: {</span>
<span class="line-added"> 633                         sideState-&gt;tmps[i] = jsBoolean(static_cast&lt;bool&gt;(tmpScratch[i + tmpOffset]));</span>
<span class="line-added"> 634                         break;</span>
<span class="line-added"> 635                     }</span>
<span class="line-added"> 636 </span>
<span class="line-added"> 637                     default:</span>
<span class="line-added"> 638                         RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added"> 639                         break;</span>
<span class="line-added"> 640                     }</span>
<span class="line-added"> 641                 }</span>
<span class="line-added"> 642 </span>
<span class="line-added"> 643                 vmPtr-&gt;addCheckpointOSRSideState(frame, WTFMove(sideState));</span>
<span class="line-added"> 644             };</span>
<span class="line-added"> 645 </span>
<span class="line-added"> 646             const CodeOrigin* codeOrigin;</span>
<span class="line-added"> 647             CallFrame* callFrame = context.gpr&lt;CallFrame*&gt;(GPRInfo::callFrameRegister);</span>
<span class="line-added"> 648             for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {</span>
<span class="line-added"> 649                 BytecodeIndex callBytecodeIndex = codeOrigin-&gt;bytecodeIndex();</span>
<span class="line-added"> 650                 if (!callBytecodeIndex.checkpoint())</span>
<span class="line-added"> 651                     continue;</span>
<span class="line-added"> 652 </span>
<span class="line-added"> 653                 auto* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();</span>
<span class="line-added"> 654                 addSideState(reinterpret_cast_ptr&lt;CallFrame*&gt;(reinterpret_cast&lt;char*&gt;(callFrame) + inlineCallFrame-&gt;returnPCOffset() - sizeof(CPURegister)), callBytecodeIndex, inlineCallFrame-&gt;tmpOffset);</span>
<span class="line-added"> 655             }</span>
<span class="line-added"> 656 </span>
<span class="line-added"> 657             if (!codeOrigin)</span>
<span class="line-added"> 658                 return;</span>
<span class="line-added"> 659 </span>
<span class="line-added"> 660             if (BytecodeIndex bytecodeIndex = codeOrigin-&gt;bytecodeIndex(); bytecodeIndex.checkpoint())</span>
<span class="line-added"> 661                 addSideState(callFrame, bytecodeIndex, 0);</span>
<span class="line-added"> 662         });</span>
<span class="line-added"> 663     }</span>
<span class="line-added"> 664 </span>
 665     // Do all data format conversions and store the results into the stack.
 666 
 667     for (size_t index = 0; index &lt; operands.size(); ++index) {
 668         const ValueRecovery&amp; recovery = operands[index];
<span class="line-modified"> 669         Operand operand = operands.operandForIndex(index);</span>
<span class="line-modified"> 670         if (operand.isTmp())</span>

 671             continue;
 672 
<span class="line-modified"> 673         if (operand.isLocal() &amp;&amp; operand.toLocal() &lt; static_cast&lt;int&gt;(jit.baselineCodeBlock()-&gt;calleeSaveSpaceAsVirtualRegisters()))</span>
<span class="line-added"> 674             continue;</span>
 675 
 676         switch (recovery.technique()) {
 677         case DisplacedInJSStack:
 678         case InFPR:
 679 #if USE(JSVALUE64)
 680         case InGPR:
 681         case UnboxedCellInGPR:
 682         case CellDisplacedInJSStack:
 683         case BooleanDisplacedInJSStack:
 684             jit.load64(scratch + index, GPRInfo::regT0);
 685             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
 686             break;
 687 #else // not USE(JSVALUE64)
 688         case InPair:
 689             jit.load32(
 690                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag,
 691                 GPRInfo::regT0);
 692             jit.load32(
 693                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
 694                 GPRInfo::regT1);
</pre>
<hr />
<pre>
 715 
 716         case UnboxedBooleanInGPR:
 717         case BooleanDisplacedInJSStack:
 718             jit.load32(
 719                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
 720                 GPRInfo::regT0);
 721             jit.store32(
 722                 AssemblyHelpers::TrustedImm32(JSValue::BooleanTag),
 723                 AssemblyHelpers::tagFor(operand));
 724             jit.store32(
 725                 GPRInfo::regT0,
 726                 AssemblyHelpers::payloadFor(operand));
 727             break;
 728 #endif // USE(JSVALUE64)
 729 
 730         case UnboxedInt32InGPR:
 731         case Int32DisplacedInJSStack:
 732 #if USE(JSVALUE64)
 733             jit.load64(scratch + index, GPRInfo::regT0);
 734             jit.zeroExtend32ToPtr(GPRInfo::regT0, GPRInfo::regT0);
<span class="line-modified"> 735             jit.or64(GPRInfo::numberTagRegister, GPRInfo::regT0);</span>
 736             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
 737 #else
 738             jit.load32(
 739                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
 740                 GPRInfo::regT0);
 741             jit.store32(
 742                 AssemblyHelpers::TrustedImm32(JSValue::Int32Tag),
 743                 AssemblyHelpers::tagFor(operand));
 744             jit.store32(
 745                 GPRInfo::regT0,
 746                 AssemblyHelpers::payloadFor(operand));
 747 #endif
 748             break;
 749 
 750 #if USE(JSVALUE64)
 751         case UnboxedInt52InGPR:
 752         case Int52DisplacedInJSStack:
 753             jit.load64(scratch + index, GPRInfo::regT0);
 754             jit.rshift64(
 755                 AssemblyHelpers::TrustedImm32(JSValue::int52ShiftAmount), GPRInfo::regT0);
</pre>
<hr />
<pre>
 793 #endif
 794             break;
 795 
 796         case DirectArgumentsThatWereNotCreated:
 797         case ClonedArgumentsThatWereNotCreated:
 798             // Don&#39;t do this, yet.
 799             break;
 800 
 801         default:
 802             RELEASE_ASSERT_NOT_REACHED();
 803             break;
 804         }
 805     }
 806 
 807     // Now that things on the stack are recovered, do the arguments recovery. We assume that arguments
 808     // recoveries don&#39;t recursively refer to each other. But, we don&#39;t try to assume that they only
 809     // refer to certain ranges of locals. Hence why we need to do this here, once the stack is sensible.
 810     // Note that we also roughly assume that the arguments might still be materialized outside of its
 811     // inline call frame scope - but for now the DFG wouldn&#39;t do that.
 812 
<span class="line-modified"> 813     emitRestoreArguments(jit, vm, operands);</span>
 814 
 815     // Adjust the old JIT&#39;s execute counter. Since we are exiting OSR, we know
 816     // that all new calls into this code will go to the new JIT, so the execute
 817     // counter only affects call frames that performed OSR exit and call frames
 818     // that were still executing the old JIT at the time of another call frame&#39;s
 819     // OSR exit. We want to ensure that the following is true:
 820     //
 821     // (a) Code the performs an OSR exit gets a chance to reenter optimized
 822     //     code eventually, since optimized code is faster. But we don&#39;t
 823     //     want to do such reentery too aggressively (see (c) below).
 824     //
 825     // (b) If there is code on the call stack that is still running the old
 826     //     JIT&#39;s code and has never OSR&#39;d, then it should get a chance to
 827     //     perform OSR entry despite the fact that we&#39;ve exited.
 828     //
 829     // (c) Code the performs an OSR exit should not immediately retry OSR
 830     //     entry, since both forms of OSR are expensive. OSR entry is
 831     //     particularly expensive.
 832     //
 833     // (d) Frequent OSR failures, even those that do not result in the code
 834     //     running in a hot loop, result in recompilation getting triggered.
 835     //
 836     // To ensure (c), we&#39;d like to set the execute counter to
 837     // counterValueForOptimizeAfterWarmUp(). This seems like it would endanger
 838     // (a) and (b), since then every OSR exit would delay the opportunity for
 839     // every call frame to perform OSR entry. Essentially, if OSR exit happens
 840     // frequently and the function has few loops, then the counter will never
 841     // become non-negative and OSR entry will never be triggered. OSR entry
 842     // will only happen if a loop gets hot in the old JIT, which does a pretty
 843     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
 844     // since each speculation failure would reset the execute counter.
 845     // So we check here if the number of speculation failures is significantly
 846     // larger than the number of successes (we want 90% success rate), and if
 847     // there have been a large enough number of failures. If so, we set the
 848     // counter to 0; otherwise we set the counter to
 849     // counterValueForOptimizeAfterWarmUp().
 850 
<span class="line-modified"> 851     handleExitCounts(vm, jit, exit);</span>
 852 
 853     // Reify inlined call frames.
 854 
 855     reifyInlinedCallFrames(jit, exit);
 856 
 857     // And finish.
 858     adjustAndJumpToTarget(vm, jit, exit);
 859 }
 860 
<span class="line-modified"> 861 void JIT_OPERATION operationDebugPrintSpeculationFailure(CallFrame* callFrame, void* debugInfoRaw, void* scratch)</span>
 862 {
<span class="line-modified"> 863     VM&amp; vm = callFrame-&gt;deprecatedVM();</span>
<span class="line-modified"> 864     NativeCallFrameTracer tracer(vm, callFrame);</span>
 865 
 866     SpeculationFailureDebugInfo* debugInfo = static_cast&lt;SpeculationFailureDebugInfo*&gt;(debugInfoRaw);
 867     CodeBlock* codeBlock = debugInfo-&gt;codeBlock;
 868     CodeBlock* alternative = codeBlock-&gt;alternative();
 869     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
<span class="line-modified"> 870     dataLog(&quot; @ exit #&quot;, vm.osrExitIndex, &quot; (&quot;, debugInfo-&gt;bytecodeIndex, &quot;, &quot;, exitKindToString(debugInfo-&gt;kind), &quot;) with &quot;);</span>
 871     if (alternative) {
 872         dataLog(
 873             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
 874             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
 875             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
 876     } else
 877         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
 878     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
 879     dataLog(&quot;    GPRs at time of exit:&quot;);
 880     char* scratchPointer = static_cast&lt;char*&gt;(scratch);
 881     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
 882         GPRReg gpr = GPRInfo::toRegister(i);
 883         dataLog(&quot; &quot;, GPRInfo::debugName(gpr), &quot;:&quot;, RawPointer(*reinterpret_cast_ptr&lt;void**&gt;(scratchPointer)));
 884         scratchPointer += sizeof(EncodedJSValue);
 885     }
 886     dataLog(&quot;\n&quot;);
 887     dataLog(&quot;    FPRs at time of exit:&quot;);
 888     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
 889         FPRReg fpr = FPRInfo::toRegister(i);
 890         dataLog(&quot; &quot;, FPRInfo::debugName(fpr), &quot;:&quot;);
</pre>
</td>
</tr>
</table>
<center><a href="DFGOSREntrypointCreationPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExit.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>