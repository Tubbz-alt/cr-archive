<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #if ENABLE(JIT)
 29 
 30 // We&#39;ve run into some problems where changing the size of the class JIT leads to
 31 // performance fluctuations. Try forcing alignment in an attempt to stabilize this.
 32 #if COMPILER(GCC_COMPATIBLE)
 33 #define JIT_CLASS_ALIGNMENT alignas(32)
 34 #else
 35 #define JIT_CLASS_ALIGNMENT
 36 #endif
 37 
 38 #define ASSERT_JIT_OFFSET(actual, expected) ASSERT_WITH_MESSAGE(actual == expected, &quot;JIT Offset \&quot;%s\&quot; should be %d, not %d.\n&quot;, #expected, static_cast&lt;int&gt;(expected), static_cast&lt;int&gt;(actual));
 39 
 40 #include &quot;CodeBlock.h&quot;
 41 #include &quot;CommonSlowPaths.h&quot;
 42 #include &quot;JITDisassembler.h&quot;
 43 #include &quot;JITInlineCacheGenerator.h&quot;
 44 #include &quot;JITMathIC.h&quot;
 45 #include &quot;JITRightShiftGenerator.h&quot;
 46 #include &quot;JSInterfaceJIT.h&quot;
 47 #include &quot;PCToCodeOriginMap.h&quot;
 48 #include &quot;UnusedPointer.h&quot;
 49 
 50 namespace JSC {
 51 
 52     enum OpcodeID : unsigned;
 53 
 54     class ArrayAllocationProfile;
 55     class CallLinkInfo;
 56     class CodeBlock;
 57     class FunctionExecutable;
 58     class JIT;
 59     class Identifier;
 60     class Interpreter;
 61     class BlockDirectory;
 62     class Register;
 63     class StructureChain;
 64     class StructureStubInfo;
 65 
 66     struct Instruction;
 67     struct OperandTypes;
 68     struct SimpleJumpTable;
 69     struct StringJumpTable;
 70 
 71     struct OpPutByVal;
 72     struct OpPutByValDirect;
 73     struct OpPutToScope;
 74 
 75     struct CallRecord {
 76         MacroAssembler::Call from;
 77         BytecodeIndex bytecodeIndex;
 78         FunctionPtr&lt;OperationPtrTag&gt; callee;
 79 
 80         CallRecord()
 81         {
 82         }
 83 
 84         CallRecord(MacroAssembler::Call from, BytecodeIndex bytecodeIndex, FunctionPtr&lt;OperationPtrTag&gt; callee)
 85             : from(from)
 86             , bytecodeIndex(bytecodeIndex)
 87             , callee(callee)
 88         {
 89         }
 90     };
 91 
 92     struct JumpTable {
 93         MacroAssembler::Jump from;
 94         unsigned toBytecodeOffset;
 95 
 96         JumpTable(MacroAssembler::Jump f, unsigned t)
 97             : from(f)
 98             , toBytecodeOffset(t)
 99         {
100         }
101     };
102 
103     struct SlowCaseEntry {
104         MacroAssembler::Jump from;
105         BytecodeIndex to;
106 
107         SlowCaseEntry(MacroAssembler::Jump f, BytecodeIndex t)
108             : from(f)
109             , to(t)
110         {
111         }
112     };
113 
114     struct SwitchRecord {
115         enum Type {
116             Immediate,
117             Character,
118             String
119         };
120 
121         Type type;
122 
123         union {
124             SimpleJumpTable* simpleJumpTable;
125             StringJumpTable* stringJumpTable;
126         } jumpTable;
127 
128         BytecodeIndex bytecodeIndex;
129         unsigned defaultOffset;
130 
131         SwitchRecord(SimpleJumpTable* jumpTable, BytecodeIndex bytecodeIndex, unsigned defaultOffset, Type type)
132             : type(type)
133             , bytecodeIndex(bytecodeIndex)
134             , defaultOffset(defaultOffset)
135         {
136             this-&gt;jumpTable.simpleJumpTable = jumpTable;
137         }
138 
139         SwitchRecord(StringJumpTable* jumpTable, BytecodeIndex bytecodeIndex, unsigned defaultOffset)
140             : type(String)
141             , bytecodeIndex(bytecodeIndex)
142             , defaultOffset(defaultOffset)
143         {
144             this-&gt;jumpTable.stringJumpTable = jumpTable;
145         }
146     };
147 
148     struct ByValCompilationInfo {
149         ByValCompilationInfo() { }
150 
151         ByValCompilationInfo(ByValInfo* byValInfo, BytecodeIndex bytecodeIndex, MacroAssembler::PatchableJump notIndexJump, MacroAssembler::PatchableJump badTypeJump, JITArrayMode arrayMode, ArrayProfile* arrayProfile, MacroAssembler::Label doneTarget, MacroAssembler::Label nextHotPathTarget)
152             : byValInfo(byValInfo)
153             , bytecodeIndex(bytecodeIndex)
154             , notIndexJump(notIndexJump)
155             , badTypeJump(badTypeJump)
156             , arrayMode(arrayMode)
157             , arrayProfile(arrayProfile)
158             , doneTarget(doneTarget)
159             , nextHotPathTarget(nextHotPathTarget)
160         {
161         }
162 
163         ByValInfo* byValInfo;
164         BytecodeIndex bytecodeIndex;
165         MacroAssembler::PatchableJump notIndexJump;
166         MacroAssembler::PatchableJump badTypeJump;
167         JITArrayMode arrayMode;
168         ArrayProfile* arrayProfile;
169         MacroAssembler::Label doneTarget;
170         MacroAssembler::Label nextHotPathTarget;
171         MacroAssembler::Label slowPathTarget;
172         MacroAssembler::Call returnAddress;
173     };
174 
175     struct CallCompilationInfo {
176         MacroAssembler::DataLabelPtr hotPathBegin;
177         MacroAssembler::Call hotPathOther;
178         MacroAssembler::Call callReturnLocation;
179         CallLinkInfo* callLinkInfo;
180     };
181 
182     void ctiPatchCallByReturnAddress(ReturnAddressPtr, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction);
183 
184     class JIT_CLASS_ALIGNMENT JIT : private JSInterfaceJIT {
185         friend class JITSlowPathCall;
186         friend class JITStubCall;
187 
188         using MacroAssembler::Jump;
189         using MacroAssembler::JumpList;
190         using MacroAssembler::Label;
191 
192         static constexpr uintptr_t patchGetByIdDefaultStructure = unusedPointer;
193         static constexpr int patchGetByIdDefaultOffset = 0;
194         // Magic number - initial offset cannot be representable as a signed 8bit value, or the X86Assembler
195         // will compress the displacement, and we may not be able to fit a patched offset.
196         static constexpr int patchPutByIdDefaultOffset = 256;
197 
198     public:
199         JIT(VM&amp;, CodeBlock* = nullptr, BytecodeIndex loopOSREntryBytecodeOffset = BytecodeIndex(0));
200         ~JIT();
201 
202         VM&amp; vm() { return *JSInterfaceJIT::vm(); }
203 
204         void compileWithoutLinking(JITCompilationEffort);
205         CompilationResult link();
206 
207         void doMainThreadPreparationBeforeCompile();
208 
209         static CompilationResult compile(VM&amp; vm, CodeBlock* codeBlock, JITCompilationEffort effort, BytecodeIndex bytecodeOffset = BytecodeIndex(0))
210         {
211             return JIT(vm, codeBlock, bytecodeOffset).privateCompile(effort);
212         }
213 
214         static void compilePutByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
215         {
216             JIT jit(vm, codeBlock);
217             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;
218             jit.privateCompilePutByVal&lt;OpPutByVal&gt;(locker, byValInfo, returnAddress, arrayMode);
219         }
220 
221         static void compileDirectPutByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
222         {
223             JIT jit(vm, codeBlock);
224             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;
225             jit.privateCompilePutByVal&lt;OpPutByValDirect&gt;(locker, byValInfo, returnAddress, arrayMode);
226         }
227 
228         template&lt;typename Op&gt;
229         static void compilePutByValWithCachedId(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, PutKind putKind, const Identifier&amp; propertyName)
230         {
231             JIT jit(vm, codeBlock);
232             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;
233             jit.privateCompilePutByValWithCachedId&lt;Op&gt;(byValInfo, returnAddress, putKind, propertyName);
234         }
235 
236         static void compileHasIndexedProperty(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
237         {
238             JIT jit(vm, codeBlock);
239             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;
240             jit.privateCompileHasIndexedProperty(byValInfo, returnAddress, arrayMode);
241         }
242 
243         static unsigned frameRegisterCountFor(CodeBlock*);
244         static int stackPointerOffsetFor(CodeBlock*);
245 
246         JS_EXPORT_PRIVATE static HashMap&lt;CString, Seconds&gt; compileTimeStats();
247         JS_EXPORT_PRIVATE static Seconds totalCompileTime();
248 
249     private:
250         void privateCompileMainPass();
251         void privateCompileLinkPass();
252         void privateCompileSlowCases();
253         CompilationResult privateCompile(JITCompilationEffort);
254 
255         void privateCompileGetByVal(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
256         void privateCompileGetByValWithCachedId(ByValInfo*, ReturnAddressPtr, const Identifier&amp;);
257         template&lt;typename Op&gt;
258         void privateCompilePutByVal(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
259         template&lt;typename Op&gt;
260         void privateCompilePutByValWithCachedId(ByValInfo*, ReturnAddressPtr, PutKind, const Identifier&amp;);
261 
262         void privateCompileHasIndexedProperty(ByValInfo*, ReturnAddressPtr, JITArrayMode);
263 
264         void privateCompilePatchGetArrayLength(ReturnAddressPtr returnAddress);
265 
266         // Add a call out from JIT code, without an exception check.
267         Call appendCall(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
268         {
269             Call functionCall = call(OperationPtrTag);
270             m_calls.append(CallRecord(functionCall, m_bytecodeIndex, function.retagged&lt;OperationPtrTag&gt;()));
271             return functionCall;
272         }
273 
274 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
275         Call appendCallWithSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
276         {
277             Call functionCall = callWithSlowPathReturnType(OperationPtrTag);
278             m_calls.append(CallRecord(functionCall, m_bytecodeIndex, function.retagged&lt;OperationPtrTag&gt;()));
279             return functionCall;
280         }
281 #endif
282 
283         void exceptionCheck(Jump jumpToHandler)
284         {
285             m_exceptionChecks.append(jumpToHandler);
286         }
287 
288         void exceptionCheck()
289         {
290             m_exceptionChecks.append(emitExceptionCheck(vm()));
291         }
292 
293         void exceptionCheckWithCallFrameRollback()
294         {
295             m_exceptionChecksWithCallFrameRollback.append(emitExceptionCheck(vm()));
296         }
297 
298         void privateCompileExceptionHandlers();
299 
300         void addSlowCase(Jump);
301         void addSlowCase(const JumpList&amp;);
302         void addSlowCase();
303         void addJump(Jump, int);
304         void addJump(const JumpList&amp;, int);
305         void emitJumpSlowToHot(Jump, int);
306 
307         template&lt;typename Op&gt;
308         void compileOpCall(const Instruction*, unsigned callLinkInfoIndex);
309         template&lt;typename Op&gt;
310         void compileOpCallSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, unsigned callLinkInfoIndex);
311         template&lt;typename Op&gt;
312         std::enable_if_t&lt;
313             Op::opcodeID != op_call_varargs &amp;&amp; Op::opcodeID != op_construct_varargs
314             &amp;&amp; Op::opcodeID != op_tail_call_varargs &amp;&amp; Op::opcodeID != op_tail_call_forward_arguments
315         , void&gt; compileSetupFrame(const Op&amp;, CallLinkInfo*);
316 
317         template&lt;typename Op&gt;
318         std::enable_if_t&lt;
319             Op::opcodeID == op_call_varargs || Op::opcodeID == op_construct_varargs
320             || Op::opcodeID == op_tail_call_varargs || Op::opcodeID == op_tail_call_forward_arguments
321         , void&gt; compileSetupFrame(const Op&amp;, CallLinkInfo*);
322 
323         template&lt;typename Op&gt;
324         bool compileTailCall(const Op&amp;, CallLinkInfo*, unsigned callLinkInfoIndex);
325         template&lt;typename Op&gt;
326         bool compileCallEval(const Op&amp;);
327         void compileCallEvalSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
328         template&lt;typename Op&gt;
329         void emitPutCallResult(const Op&amp;);
330 
331         enum class CompileOpStrictEqType { StrictEq, NStrictEq };
332         template&lt;typename Op&gt;
333         void compileOpStrictEq(const Instruction*, CompileOpStrictEqType);
334         template&lt;typename Op&gt;
335         void compileOpStrictEqJump(const Instruction*, CompileOpStrictEqType);
336         enum class CompileOpEqType { Eq, NEq };
337         void compileOpEqJumpSlow(Vector&lt;SlowCaseEntry&gt;::iterator&amp;, CompileOpEqType, int jumpTarget);
338         bool isOperandConstantDouble(VirtualRegister);
339 
340         enum WriteBarrierMode { UnconditionalWriteBarrier, ShouldFilterBase, ShouldFilterValue, ShouldFilterBaseAndValue };
341         // value register in write barrier is used before any scratch registers
342         // so may safely be the same as either of the scratch registers.
343         void emitWriteBarrier(VirtualRegister owner, VirtualRegister value, WriteBarrierMode);
344         void emitWriteBarrier(JSCell* owner, VirtualRegister value, WriteBarrierMode);
345         void emitWriteBarrier(JSCell* owner);
346 
347         // This assumes that the value to profile is in regT0 and that regT3 is available for
348         // scratch.
349         void emitValueProfilingSite(ValueProfile&amp;);
350         template&lt;typename Metadata&gt; void emitValueProfilingSite(Metadata&amp;);
351         void emitValueProfilingSiteIfProfiledOpcode(...);
352         template&lt;typename Op&gt;
353         std::enable_if_t&lt;std::is_same&lt;decltype(Op::Metadata::m_profile), ValueProfile&gt;::value, void&gt;
354         emitValueProfilingSiteIfProfiledOpcode(Op bytecode);
355 
356         void emitArrayProfilingSiteWithCell(RegisterID cell, RegisterID indexingType, ArrayProfile*);
357         void emitArrayProfileStoreToHoleSpecialCase(ArrayProfile*);
358         void emitArrayProfileOutOfBoundsSpecialCase(ArrayProfile*);
359 
360         JITArrayMode chooseArrayMode(ArrayProfile*);
361 
362         // Property is in regT1, base is in regT0. regT2 contains indexing type.
363         // Property is int-checked and zero extended. Base is cell checked.
364         // Structure is already profiled. Returns the slow cases. Fall-through
365         // case contains result in regT0, and it is not yet profiled.
366         JumpList emitInt32Load(const Instruction* instruction, PatchableJump&amp; badType) { return emitContiguousLoad(instruction, badType, Int32Shape); }
367         JumpList emitDoubleLoad(const Instruction*, PatchableJump&amp; badType);
368         JumpList emitContiguousLoad(const Instruction*, PatchableJump&amp; badType, IndexingType expectedShape = ContiguousShape);
369         JumpList emitArrayStorageLoad(const Instruction*, PatchableJump&amp; badType);
370         JumpList emitLoadForArrayMode(const Instruction*, JITArrayMode, PatchableJump&amp; badType);
371 
372         // Property is in regT1, base is in regT0. regT2 contains indecing type.
373         // The value to store is not yet loaded. Property is int-checked and
374         // zero-extended. Base is cell checked. Structure is already profiled.
375         // returns the slow cases.
376         template&lt;typename Op&gt;
377         JumpList emitInt32PutByVal(Op bytecode, PatchableJump&amp; badType)
378         {
379             return emitGenericContiguousPutByVal(bytecode, badType, Int32Shape);
380         }
381         template&lt;typename Op&gt;
382         JumpList emitDoublePutByVal(Op bytecode, PatchableJump&amp; badType)
383         {
384             return emitGenericContiguousPutByVal(bytecode, badType, DoubleShape);
385         }
386         template&lt;typename Op&gt;
387         JumpList emitContiguousPutByVal(Op bytecode, PatchableJump&amp; badType)
388         {
389             return emitGenericContiguousPutByVal(bytecode, badType);
390         }
391         template&lt;typename Op&gt;
392         JumpList emitGenericContiguousPutByVal(Op, PatchableJump&amp; badType, IndexingType indexingShape = ContiguousShape);
393         template&lt;typename Op&gt;
394         JumpList emitArrayStoragePutByVal(Op, PatchableJump&amp; badType);
395         template&lt;typename Op&gt;
396         JumpList emitIntTypedArrayPutByVal(Op, PatchableJump&amp; badType, TypedArrayType);
397         template&lt;typename Op&gt;
398         JumpList emitFloatTypedArrayPutByVal(Op, PatchableJump&amp; badType, TypedArrayType);
399 
400         // Identifier check helper for GetByVal and PutByVal.
401         void emitByValIdentifierCheck(ByValInfo*, RegisterID cell, RegisterID scratch, const Identifier&amp;, JumpList&amp; slowCases);
402 
403         template&lt;typename Op&gt;
404         JITPutByIdGenerator emitPutByValWithCachedId(ByValInfo*, Op, PutKind, const Identifier&amp;, JumpList&amp; doneCases, JumpList&amp; slowCases);
405 
406         enum FinalObjectMode { MayBeFinal, KnownNotFinal };
407 
408         void emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst);
409         void emitPutVirtualRegister(VirtualRegister dst, JSValueRegs src);
410 
411         int32_t getOperandConstantInt(VirtualRegister src);
412         double getOperandConstantDouble(VirtualRegister src);
413 
414 #if USE(JSVALUE32_64)
415         bool getOperandConstantInt(VirtualRegister op1, VirtualRegister op2, VirtualRegister&amp; op, int32_t&amp; constant);
416 
417         void emitLoadDouble(VirtualRegister, FPRegisterID value);
418         void emitLoadTag(VirtualRegister, RegisterID tag);
419         void emitLoadPayload(VirtualRegister, RegisterID payload);
420 
421         void emitLoad(const JSValue&amp; v, RegisterID tag, RegisterID payload);
422         void emitLoad(VirtualRegister, RegisterID tag, RegisterID payload, RegisterID base = callFrameRegister);
423         void emitLoad2(VirtualRegister, RegisterID tag1, RegisterID payload1, VirtualRegister, RegisterID tag2, RegisterID payload2);
424 
425         void emitStore(VirtualRegister, RegisterID tag, RegisterID payload, RegisterID base = callFrameRegister);
426         void emitStore(VirtualRegister, const JSValue constant, RegisterID base = callFrameRegister);
427         void emitStoreInt32(VirtualRegister, RegisterID payload, bool indexIsInt32 = false);
428         void emitStoreInt32(VirtualRegister, TrustedImm32 payload, bool indexIsInt32 = false);
429         void emitStoreCell(VirtualRegister, RegisterID payload, bool indexIsCell = false);
430         void emitStoreBool(VirtualRegister, RegisterID payload, bool indexIsBool = false);
431         void emitStoreDouble(VirtualRegister, FPRegisterID value);
432 
433         void emitJumpSlowCaseIfNotJSCell(VirtualRegister);
434         void emitJumpSlowCaseIfNotJSCell(VirtualRegister, RegisterID tag);
435 
436         void compileGetByIdHotPath(const Identifier*);
437 
438         // Arithmetic opcode helpers
439         template &lt;typename Op&gt;
440         void emitBinaryDoubleOp(const Instruction *, OperandTypes, JumpList&amp; notInt32Op1, JumpList&amp; notInt32Op2, bool op1IsInRegisters = true, bool op2IsInRegisters = true);
441 
442 #else // USE(JSVALUE32_64)
443         void emitGetVirtualRegister(VirtualRegister src, RegisterID dst);
444         void emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2);
445         void emitPutVirtualRegister(VirtualRegister dst, RegisterID from = regT0);
446         void emitStoreCell(VirtualRegister dst, RegisterID payload, bool /* only used in JSValue32_64 */ = false)
447         {
448             emitPutVirtualRegister(dst, payload);
449         }
450 
451         Jump emitJumpIfBothJSCells(RegisterID, RegisterID, RegisterID);
452         void emitJumpSlowCaseIfJSCell(RegisterID);
453         void emitJumpSlowCaseIfNotJSCell(RegisterID);
454         void emitJumpSlowCaseIfNotJSCell(RegisterID, VirtualRegister);
455         Jump emitJumpIfNotInt(RegisterID, RegisterID, RegisterID scratch);
456         PatchableJump emitPatchableJumpIfNotInt(RegisterID);
457         void emitJumpSlowCaseIfNotInt(RegisterID);
458         void emitJumpSlowCaseIfNotNumber(RegisterID);
459         void emitJumpSlowCaseIfNotInt(RegisterID, RegisterID, RegisterID scratch);
460 
461         void compileGetByIdHotPath(VirtualRegister baseReg, const Identifier*);
462 
463 #endif // USE(JSVALUE32_64)
464 
465         template&lt;typename Op&gt;
466         void emit_compareAndJump(const Instruction*, RelationalCondition);
467         void emit_compareAndJumpImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, RelationalCondition);
468         template&lt;typename Op&gt;
469         void emit_compareUnsigned(const Instruction*, RelationalCondition);
470         void emit_compareUnsignedImpl(VirtualRegister dst, VirtualRegister op1, VirtualRegister op2, RelationalCondition);
471         template&lt;typename Op&gt;
472         void emit_compareUnsignedAndJump(const Instruction*, RelationalCondition);
473         void emit_compareUnsignedAndJumpImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, RelationalCondition);
474         template&lt;typename Op&gt;
475         void emit_compareAndJumpSlow(const Instruction*, DoubleCondition, size_t (JIT_OPERATION *operation)(JSGlobalObject*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
476         void emit_compareAndJumpSlowImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, size_t instructionSize, DoubleCondition, size_t (JIT_OPERATION *operation)(JSGlobalObject*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
477 
478         void assertStackPointerOffset();
479 
480         void emit_op_add(const Instruction*);
481         void emit_op_bitand(const Instruction*);
482         void emit_op_bitor(const Instruction*);
483         void emit_op_bitxor(const Instruction*);
484         void emit_op_bitnot(const Instruction*);
485         void emit_op_call(const Instruction*);
486         void emit_op_tail_call(const Instruction*);
487         void emit_op_call_eval(const Instruction*);
488         void emit_op_call_varargs(const Instruction*);
489         void emit_op_tail_call_varargs(const Instruction*);
490         void emit_op_tail_call_forward_arguments(const Instruction*);
491         void emit_op_construct_varargs(const Instruction*);
492         void emit_op_catch(const Instruction*);
493         void emit_op_construct(const Instruction*);
494         void emit_op_create_this(const Instruction*);
495         void emit_op_to_this(const Instruction*);
496         void emit_op_get_argument(const Instruction*);
497         void emit_op_argument_count(const Instruction*);
498         void emit_op_get_rest_length(const Instruction*);
499         void emit_op_check_tdz(const Instruction*);
500         void emit_op_identity_with_profile(const Instruction*);
501         void emit_op_debug(const Instruction*);
502         void emit_op_del_by_id(const Instruction*);
503         void emit_op_del_by_val(const Instruction*);
504         void emit_op_div(const Instruction*);
505         void emit_op_end(const Instruction*);
506         void emit_op_enter(const Instruction*);
507         void emit_op_get_scope(const Instruction*);
508         void emit_op_eq(const Instruction*);
509         void emit_op_eq_null(const Instruction*);
510         void emit_op_below(const Instruction*);
511         void emit_op_beloweq(const Instruction*);
512         void emit_op_try_get_by_id(const Instruction*);
513         void emit_op_get_by_id(const Instruction*);
514         void emit_op_get_by_id_with_this(const Instruction*);
515         void emit_op_get_by_id_direct(const Instruction*);
516         void emit_op_get_by_val(const Instruction*);
517         void emit_op_get_argument_by_val(const Instruction*);
518         void emit_op_in_by_id(const Instruction*);
519         void emit_op_init_lazy_reg(const Instruction*);
520         void emit_op_overrides_has_instance(const Instruction*);
521         void emit_op_instanceof(const Instruction*);
522         void emit_op_instanceof_custom(const Instruction*);
523         void emit_op_is_empty(const Instruction*);
524         void emit_op_is_undefined(const Instruction*);
525         void emit_op_is_undefined_or_null(const Instruction*);
526         void emit_op_is_boolean(const Instruction*);
527         void emit_op_is_number(const Instruction*);
528         void emit_op_is_object(const Instruction*);
529         void emit_op_is_cell_with_type(const Instruction*);
530         void emit_op_jeq_null(const Instruction*);
531         void emit_op_jfalse(const Instruction*);
532         void emit_op_jmp(const Instruction*);
533         void emit_op_jneq_null(const Instruction*);
534         void emit_op_jundefined_or_null(const Instruction*);
535         void emit_op_jnundefined_or_null(const Instruction*);
536         void emit_op_jneq_ptr(const Instruction*);
537         void emit_op_jless(const Instruction*);
538         void emit_op_jlesseq(const Instruction*);
539         void emit_op_jgreater(const Instruction*);
540         void emit_op_jgreatereq(const Instruction*);
541         void emit_op_jnless(const Instruction*);
542         void emit_op_jnlesseq(const Instruction*);
543         void emit_op_jngreater(const Instruction*);
544         void emit_op_jngreatereq(const Instruction*);
545         void emit_op_jeq(const Instruction*);
546         void emit_op_jneq(const Instruction*);
547         void emit_op_jstricteq(const Instruction*);
548         void emit_op_jnstricteq(const Instruction*);
549         void emit_op_jbelow(const Instruction*);
550         void emit_op_jbeloweq(const Instruction*);
551         void emit_op_jtrue(const Instruction*);
552         void emit_op_loop_hint(const Instruction*);
553         void emit_op_check_traps(const Instruction*);
554         void emit_op_nop(const Instruction*);
555         void emit_op_super_sampler_begin(const Instruction*);
556         void emit_op_super_sampler_end(const Instruction*);
557         void emit_op_lshift(const Instruction*);
558         void emit_op_mod(const Instruction*);
559         void emit_op_mov(const Instruction*);
560         void emit_op_mul(const Instruction*);
561         void emit_op_negate(const Instruction*);
562         void emit_op_neq(const Instruction*);
563         void emit_op_neq_null(const Instruction*);
564         void emit_op_new_array(const Instruction*);
565         void emit_op_new_array_with_size(const Instruction*);
566         void emit_op_new_func(const Instruction*);
567         void emit_op_new_func_exp(const Instruction*);
568         void emit_op_new_generator_func(const Instruction*);
569         void emit_op_new_generator_func_exp(const Instruction*);
570         void emit_op_new_async_func(const Instruction*);
571         void emit_op_new_async_func_exp(const Instruction*);
572         void emit_op_new_async_generator_func(const Instruction*);
573         void emit_op_new_async_generator_func_exp(const Instruction*);
574         void emit_op_new_object(const Instruction*);
575         void emit_op_new_regexp(const Instruction*);
576         void emit_op_not(const Instruction*);
577         void emit_op_nstricteq(const Instruction*);
578         void emit_op_dec(const Instruction*);
579         void emit_op_inc(const Instruction*);
580         void emit_op_profile_type(const Instruction*);
581         void emit_op_profile_control_flow(const Instruction*);
582         void emit_op_get_parent_scope(const Instruction*);
583         void emit_op_put_by_id(const Instruction*);
584         template&lt;typename Op = OpPutByVal&gt;
585         void emit_op_put_by_val(const Instruction*);
586         void emit_op_put_by_val_direct(const Instruction*);
587         void emit_op_put_getter_by_id(const Instruction*);
588         void emit_op_put_setter_by_id(const Instruction*);
589         void emit_op_put_getter_setter_by_id(const Instruction*);
590         void emit_op_put_getter_by_val(const Instruction*);
591         void emit_op_put_setter_by_val(const Instruction*);
592         void emit_op_ret(const Instruction*);
593         void emit_op_rshift(const Instruction*);
594         void emit_op_set_function_name(const Instruction*);
595         void emit_op_stricteq(const Instruction*);
596         void emit_op_sub(const Instruction*);
597         void emit_op_switch_char(const Instruction*);
598         void emit_op_switch_imm(const Instruction*);
599         void emit_op_switch_string(const Instruction*);
600         void emit_op_tear_off_arguments(const Instruction*);
601         void emit_op_throw(const Instruction*);
602         void emit_op_to_number(const Instruction*);
603         void emit_op_to_numeric(const Instruction*);
604         void emit_op_to_string(const Instruction*);
605         void emit_op_to_object(const Instruction*);
606         void emit_op_to_primitive(const Instruction*);
607         void emit_op_unexpected_load(const Instruction*);
608         void emit_op_unsigned(const Instruction*);
609         void emit_op_urshift(const Instruction*);
610         void emit_op_has_structure_property(const Instruction*);
611         void emit_op_has_indexed_property(const Instruction*);
612         void emit_op_get_direct_pname(const Instruction*);
613         void emit_op_enumerator_structure_pname(const Instruction*);
614         void emit_op_enumerator_generic_pname(const Instruction*);
615         void emit_op_get_internal_field(const Instruction*);
616         void emit_op_put_internal_field(const Instruction*);
617         void emit_op_log_shadow_chicken_prologue(const Instruction*);
618         void emit_op_log_shadow_chicken_tail(const Instruction*);
619         void emit_op_to_property_key(const Instruction*);
620 
621         void emitSlow_op_add(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
622         void emitSlow_op_call(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
623         void emitSlow_op_tail_call(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
624         void emitSlow_op_call_eval(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
625         void emitSlow_op_call_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
626         void emitSlow_op_tail_call_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
627         void emitSlow_op_tail_call_forward_arguments(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
628         void emitSlow_op_construct_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
629         void emitSlow_op_construct(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
630         void emitSlow_op_eq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
631         void emitSlow_op_get_callee(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
632         void emitSlow_op_try_get_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
633         void emitSlow_op_get_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
634         void emitSlow_op_get_by_id_with_this(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
635         void emitSlow_op_get_by_id_direct(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
636         void emitSlow_op_get_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
637         void emitSlow_op_get_argument_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
638         void emitSlow_op_in_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
639         void emitSlow_op_instanceof(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
640         void emitSlow_op_instanceof_custom(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
641         void emitSlow_op_jless(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
642         void emitSlow_op_jlesseq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
643         void emitSlow_op_jgreater(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
644         void emitSlow_op_jgreatereq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
645         void emitSlow_op_jnless(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
646         void emitSlow_op_jnlesseq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
647         void emitSlow_op_jngreater(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
648         void emitSlow_op_jngreatereq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
649         void emitSlow_op_jeq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
650         void emitSlow_op_jneq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
651         void emitSlow_op_jstricteq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
652         void emitSlow_op_jnstricteq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
653         void emitSlow_op_jtrue(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
654         void emitSlow_op_loop_hint(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
655         void emitSlow_op_check_traps(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
656         void emitSlow_op_mod(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
657         void emitSlow_op_mul(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
658         void emitSlow_op_negate(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
659         void emitSlow_op_neq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
660         void emitSlow_op_new_object(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
661         void emitSlow_op_put_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
662         void emitSlow_op_put_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
663         void emitSlow_op_sub(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
664         void emitSlow_op_has_indexed_property(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
665 
666         void emit_op_resolve_scope(const Instruction*);
667         void emit_op_get_from_scope(const Instruction*);
668         void emit_op_put_to_scope(const Instruction*);
669         void emit_op_get_from_arguments(const Instruction*);
670         void emit_op_put_to_arguments(const Instruction*);
671         void emitSlow_op_get_from_scope(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
672         void emitSlow_op_put_to_scope(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
673 
674         void emitSlowCaseCall(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, SlowPathFunction);
675 
676         void emitRightShift(const Instruction*, bool isUnsigned);
677         void emitRightShiftSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, bool isUnsigned);
678 
679         template&lt;typename Op&gt;
680         void emitNewFuncCommon(const Instruction*);
681         template&lt;typename Op&gt;
682         void emitNewFuncExprCommon(const Instruction*);
683         void emitVarInjectionCheck(bool needsVarInjectionChecks);
684         void emitResolveClosure(VirtualRegister dst, VirtualRegister scope, bool needsVarInjectionChecks, unsigned depth);
685         void emitLoadWithStructureCheck(VirtualRegister scope, Structure** structureSlot);
686 #if USE(JSVALUE64)
687         void emitGetVarFromPointer(JSValue* operand, GPRReg);
688         void emitGetVarFromIndirectPointer(JSValue** operand, GPRReg);
689 #else
690         void emitGetVarFromIndirectPointer(JSValue** operand, GPRReg tag, GPRReg payload);
691         void emitGetVarFromPointer(JSValue* operand, GPRReg tag, GPRReg payload);
692 #endif
693         void emitGetClosureVar(VirtualRegister scope, uintptr_t operand);
694         void emitNotifyWrite(WatchpointSet*);
695         void emitNotifyWrite(GPRReg pointerToSet);
696         void emitPutGlobalVariable(JSValue* operand, VirtualRegister value, WatchpointSet*);
697         void emitPutGlobalVariableIndirect(JSValue** addressOfOperand, VirtualRegister value, WatchpointSet**);
698         void emitPutClosureVar(VirtualRegister scope, uintptr_t operand, VirtualRegister value, WatchpointSet*);
699 
700         void emitInitRegister(VirtualRegister);
701 
702         void emitPutIntToCallFrameHeader(RegisterID from, VirtualRegister);
703 
704         JSValue getConstantOperand(VirtualRegister);
705         bool isOperandConstantInt(VirtualRegister);
706         bool isOperandConstantChar(VirtualRegister);
707 
708         template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
709         void emitMathICFast(JITUnaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledFunction, NonProfiledFunction);
710         template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
711         void emitMathICFast(JITBinaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledFunction, NonProfiledFunction);
712 
713         template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
714         void emitMathICSlow(JITBinaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledRepatchFunction, ProfiledFunction, RepatchFunction);
715         template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
716         void emitMathICSlow(JITUnaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledRepatchFunction, ProfiledFunction, RepatchFunction);
717 
718         Jump getSlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
719         {
720             return iter++-&gt;from;
721         }
722         void linkSlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
723         {
724             if (iter-&gt;from.isSet())
725                 iter-&gt;from.link(this);
726             ++iter;
727         }
728         void linkDummySlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
729         {
730             ASSERT(!iter-&gt;from.isSet());
731             ++iter;
732         }
733         void linkSlowCaseIfNotJSCell(Vector&lt;SlowCaseEntry&gt;::iterator&amp;, VirtualRegister);
734         void linkAllSlowCasesForBytecodeIndex(Vector&lt;SlowCaseEntry&gt;&amp; slowCases,
735             Vector&lt;SlowCaseEntry&gt;::iterator&amp;, BytecodeIndex bytecodeOffset);
736 
737         void linkAllSlowCases(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
738         {
739             linkAllSlowCasesForBytecodeIndex(m_slowCases, iter, m_bytecodeIndex);
740         }
741 
742         bool hasAnySlowCases(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, BytecodeIndex bytecodeOffset);
743         bool hasAnySlowCases(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
744         {
745             return hasAnySlowCases(m_slowCases, iter, m_bytecodeIndex);
746         }
747 
748         MacroAssembler::Call appendCallWithExceptionCheck(const FunctionPtr&lt;CFunctionPtrTag&gt;);
749 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
750         MacroAssembler::Call appendCallWithExceptionCheckAndSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt;);
751 #endif
752         MacroAssembler::Call appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt;);
753         MacroAssembler::Call appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr&lt;CFunctionPtrTag&gt;, VirtualRegister result);
754         template&lt;typename Metadata&gt;
755         MacroAssembler::Call appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata&amp;, const FunctionPtr&lt;CFunctionPtrTag&gt;, VirtualRegister result);
756 
757         template&lt;typename OperationType, typename... Args&gt;
758         std::enable_if_t&lt;FunctionTraits&lt;OperationType&gt;::hasResult, MacroAssembler::Call&gt;
759         callOperation(OperationType operation, VirtualRegister result, Args... args)
760         {
761             setupArguments&lt;OperationType&gt;(args...);
762             return appendCallWithExceptionCheckSetJSValueResult(operation, result);
763         }
764 
765 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
766         template&lt;typename OperationType, typename... Args&gt;
767         std::enable_if_t&lt;std::is_same&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType, SlowPathReturnType&gt;::value, MacroAssembler::Call&gt;
768         callOperation(OperationType operation, Args... args)
769         {
770             setupArguments&lt;OperationType&gt;(args...);
771             return appendCallWithExceptionCheckAndSlowPathReturnType(operation);
772         }
773 
774         template&lt;typename Type&gt;
775         struct is64BitType {
776             static constexpr bool value = sizeof(Type) &lt;= 8;
777         };
778 
779         template&lt;&gt;
780         struct is64BitType&lt;void&gt; {
781             static constexpr bool value = true;
782         };
783 
784         template&lt;typename OperationType, typename... Args&gt;
785         std::enable_if_t&lt;!std::is_same&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType, SlowPathReturnType&gt;::value, MacroAssembler::Call&gt;
786         callOperation(OperationType operation, Args... args)
787         {
788             static_assert(is64BitType&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType&gt;::value, &quot;Win64 cannot use standard call when return type is larger than 64 bits.&quot;);
789             setupArguments&lt;OperationType&gt;(args...);
790             return appendCallWithExceptionCheck(operation);
791         }
792 #else // OS(WINDOWS) &amp;&amp; CPU(X86_64)
793         template&lt;typename OperationType, typename... Args&gt;
794         MacroAssembler::Call callOperation(OperationType operation, Args... args)
795         {
796             setupArguments&lt;OperationType&gt;(args...);
797             return appendCallWithExceptionCheck(operation);
798         }
799 #endif // OS(WINDOWS) &amp;&amp; CPU(X86_64)
800 
801         template&lt;typename Metadata, typename OperationType, typename... Args&gt;
802         std::enable_if_t&lt;FunctionTraits&lt;OperationType&gt;::hasResult, MacroAssembler::Call&gt;
803         callOperationWithProfile(Metadata&amp; metadata, OperationType operation, VirtualRegister result, Args... args)
804         {
805             setupArguments&lt;OperationType&gt;(args...);
806             return appendCallWithExceptionCheckSetJSValueResultWithProfile(metadata, operation, result);
807         }
808 
809         template&lt;typename OperationType, typename... Args&gt;
810         MacroAssembler::Call callOperationWithResult(OperationType operation, JSValueRegs resultRegs, Args... args)
811         {
812             setupArguments&lt;OperationType&gt;(args...);
813             auto result = appendCallWithExceptionCheck(operation);
814             setupResults(resultRegs);
815             return result;
816         }
817 
818         template&lt;typename OperationType, typename... Args&gt;
819         MacroAssembler::Call callOperationNoExceptionCheck(OperationType operation, Args... args)
820         {
821             setupArguments&lt;OperationType&gt;(args...);
822             updateTopCallFrame();
823             return appendCall(operation);
824         }
825 
826         template&lt;typename OperationType, typename... Args&gt;
827         MacroAssembler::Call callOperationWithCallFrameRollbackOnException(OperationType operation, Args... args)
828         {
829             setupArguments&lt;OperationType&gt;(args...);
830             return appendCallWithCallFrameRollbackOnException(operation);
831         }
832 
833         enum class ProfilingPolicy {
834             ShouldEmitProfiling,
835             NoProfiling
836         };
837 
838         template&lt;typename Op, typename SnippetGenerator&gt;
839         void emitBitBinaryOpFastPath(const Instruction* currentInstruction, ProfilingPolicy shouldEmitProfiling = ProfilingPolicy::NoProfiling);
840 
841         void emitRightShiftFastPath(const Instruction* currentInstruction, OpcodeID);
842 
843         template&lt;typename Op&gt;
844         void emitRightShiftFastPath(const Instruction* currentInstruction, JITRightShiftGenerator::ShiftType);
845 
846         void updateTopCallFrame();
847 
848         Call emitNakedCall(CodePtr&lt;NoPtrTag&gt; function = CodePtr&lt;NoPtrTag&gt;());
849         Call emitNakedTailCall(CodePtr&lt;NoPtrTag&gt; function = CodePtr&lt;NoPtrTag&gt;());
850 
851         // Loads the character value of a single character string into dst.
852         void emitLoadCharacterString(RegisterID src, RegisterID dst, JumpList&amp; failures);
853 
854         int jumpTarget(const Instruction*, int target);
855 
856 #if ENABLE(DFG_JIT)
857         void emitEnterOptimizationCheck();
858 #else
859         void emitEnterOptimizationCheck() { }
860 #endif
861 
862 #ifndef NDEBUG
863         void printBytecodeOperandTypes(VirtualRegister src1, VirtualRegister src2);
864 #endif
865 
866 #if ENABLE(SAMPLING_FLAGS)
867         void setSamplingFlag(int32_t);
868         void clearSamplingFlag(int32_t);
869 #endif
870 
871 #if ENABLE(SAMPLING_COUNTERS)
872         void emitCount(AbstractSamplingCounter&amp;, int32_t = 1);
873 #endif
874 
875 #if ENABLE(OPCODE_SAMPLING)
876         void sampleInstruction(const Instruction*, bool = false);
877 #endif
878 
879 #if ENABLE(CODEBLOCK_SAMPLING)
880         void sampleCodeBlock(CodeBlock*);
881 #else
882         void sampleCodeBlock(CodeBlock*) {}
883 #endif
884 
885 #if ENABLE(DFG_JIT)
886         bool canBeOptimized() { return m_canBeOptimized; }
887         bool canBeOptimizedOrInlined() { return m_canBeOptimizedOrInlined; }
888         bool shouldEmitProfiling() { return m_shouldEmitProfiling; }
889 #else
890         bool canBeOptimized() { return false; }
891         bool canBeOptimizedOrInlined() { return false; }
892         // Enables use of value profiler with tiered compilation turned off,
893         // in which case all code gets profiled.
894         bool shouldEmitProfiling() { return false; }
895 #endif
896 
897         static bool reportCompileTimes();
898         static bool computeCompileTimes();
899 
900         // If you need to check a value from the metadata table and you need it to
901         // be consistent across the fast and slow path, then you want to use this.
902         // It will give the slow path the same value read by the fast path.
903         GetPutInfo copiedGetPutInfo(OpPutToScope);
904         template&lt;typename BinaryOp&gt;
905         BinaryArithProfile copiedArithProfile(BinaryOp);
906 
907         Interpreter* m_interpreter;
908 
909         Vector&lt;CallRecord&gt; m_calls;
910         Vector&lt;Label&gt; m_labels;
911         Vector&lt;JITGetByIdGenerator&gt; m_getByIds;
912         Vector&lt;JITGetByValGenerator&gt; m_getByVals;
913         Vector&lt;JITGetByIdWithThisGenerator&gt; m_getByIdsWithThis;
914         Vector&lt;JITPutByIdGenerator&gt; m_putByIds;
915         Vector&lt;JITInByIdGenerator&gt; m_inByIds;
916         Vector&lt;JITInstanceOfGenerator&gt; m_instanceOfs;
917         Vector&lt;ByValCompilationInfo&gt; m_byValCompilationInfo;
918         Vector&lt;CallCompilationInfo&gt; m_callCompilationInfo;
919         Vector&lt;JumpTable&gt; m_jmpTable;
920 
921         BytecodeIndex m_bytecodeIndex;
922         Vector&lt;SlowCaseEntry&gt; m_slowCases;
923         Vector&lt;SwitchRecord&gt; m_switches;
924 
925         HashMap&lt;unsigned, unsigned&gt; m_copiedGetPutInfos;
926         HashMap&lt;uint64_t, BinaryArithProfile&gt; m_copiedArithProfiles;
927 
928         JumpList m_exceptionChecks;
929         JumpList m_exceptionChecksWithCallFrameRollback;
930         Label m_exceptionHandler;
931 
932         unsigned m_getByIdIndex { UINT_MAX };
933         unsigned m_getByValIndex { UINT_MAX };
934         unsigned m_getByIdWithThisIndex { UINT_MAX };
935         unsigned m_putByIdIndex { UINT_MAX };
936         unsigned m_inByIdIndex { UINT_MAX };
937         unsigned m_instanceOfIndex { UINT_MAX };
938         unsigned m_byValInstructionIndex { UINT_MAX };
939         unsigned m_callLinkInfoIndex { UINT_MAX };
940         unsigned m_bytecodeCountHavingSlowCase { 0 };
941 
942         Label m_arityCheck;
943         std::unique_ptr&lt;LinkBuffer&gt; m_linkBuffer;
944 
945         std::unique_ptr&lt;JITDisassembler&gt; m_disassembler;
946         RefPtr&lt;Profiler::Compilation&gt; m_compilation;
947 
948         PCToCodeOriginMapBuilder m_pcToCodeOriginMapBuilder;
949 
950         HashMap&lt;const Instruction*, void*&gt; m_instructionToMathIC;
951         HashMap&lt;const Instruction*, MathICGenerationState&gt; m_instructionToMathICGenerationState;
952 
953         bool m_canBeOptimized;
954         bool m_canBeOptimizedOrInlined;
955         bool m_shouldEmitProfiling;
956         BytecodeIndex m_loopOSREntryBytecodeIndex;
957     };
958 
959 } // namespace JSC
960 
961 
962 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>