<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ThunkGenerators.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="TempRegisterSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ThunkGenerators.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ThunkGenerators.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  23  * THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;ThunkGenerators.h&quot;
  28 
  29 #include &quot;CodeBlock.h&quot;
  30 #include &quot;DFGSpeculativeJIT.h&quot;
  31 #include &quot;JITExceptions.h&quot;
  32 #include &quot;JITOperations.h&quot;
  33 #include &quot;JSArray.h&quot;
  34 #include &quot;JSBoundFunction.h&quot;
  35 #include &quot;JSCInlines.h&quot;
  36 #include &quot;MathCommon.h&quot;
  37 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;

  38 #include &quot;SpecializedThunkJIT.h&quot;
  39 #include &lt;wtf/InlineASM.h&gt;
  40 #include &lt;wtf/StringPrintStream.h&gt;
  41 #include &lt;wtf/text/StringImpl.h&gt;
  42 
  43 #if ENABLE(JIT)
  44 
  45 namespace JSC {
  46 
  47 template&lt;typename TagType&gt;
  48 inline void emitPointerValidation(CCallHelpers&amp; jit, GPRReg pointerGPR, TagType tag)
  49 {
<span class="line-modified">  50     if (ASSERT_DISABLED)</span>
  51         return;
  52     CCallHelpers::Jump isNonZero = jit.branchTestPtr(CCallHelpers::NonZero, pointerGPR);
  53     jit.abortWithReason(TGInvalidPointer);
  54     isNonZero.link(&amp;jit);
  55     jit.pushToSave(pointerGPR);
  56     jit.untagPtr(tag, pointerGPR);
  57     jit.load8(pointerGPR, pointerGPR);
  58     jit.popToRestore(pointerGPR);
  59 }
  60 
  61 // We will jump here if the JIT code tries to make a call, but the
  62 // linking helper (C++ code) decides to throw an exception instead.
  63 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; throwExceptionFromCallSlowPathGenerator(VM&amp; vm)
  64 {
  65     CCallHelpers jit;
  66 
  67     // The call pushed a return address, so we need to pop it back off to re-align the stack,
  68     // even though we won&#39;t use it.
  69     jit.preserveReturnAddressAfterCall(GPRInfo::nonPreservedNonReturnGPR);
  70 
  71     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
  72 
<span class="line-modified">  73     jit.setupArguments&lt;decltype(lookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm), GPRInfo::callFrameRegister);</span>
<span class="line-modified">  74     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)), GPRInfo::nonArgGPR0);</span>

  75     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  76     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
  77     jit.jumpToExceptionHandler(vm);
  78 
  79     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
  80     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Throw exception from call slow path thunk&quot;);
  81 }
  82 
<span class="line-modified">  83 static void slowPathFor(CCallHelpers&amp; jit, VM&amp; vm, Sprt_JITOperation_ECli slowPathFunction)</span>
  84 {
  85     jit.sanitizeStackInline(vm, GPRInfo::nonArgGPR0);
  86     jit.emitFunctionPrologue();
  87     jit.storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);
  88 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
  89     // Windows X86_64 needs some space pointed to by arg0 for return types larger than 64 bits.
  90     // Other argument values are shift by 1. Use space on the stack for our two return values.
  91     // Moving the stack down maxFrameExtentForSlowPathCall bytes gives us room for our 3 arguments
  92     // and space for the 16 byte return area.
  93     jit.addPtr(CCallHelpers::TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), CCallHelpers::stackPointerRegister);
<span class="line-modified">  94     jit.move(GPRInfo::regT2, GPRInfo::argumentGPR2);</span>




  95     jit.addPtr(CCallHelpers::TrustedImm32(32), CCallHelpers::stackPointerRegister, GPRInfo::argumentGPR0);
  96     jit.move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
  97     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
  98     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  99     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 100     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR, 8), GPRInfo::returnValueGPR2);
 101     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR), GPRInfo::returnValueGPR);
 102     jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 103 #else
 104     if (maxFrameExtentForSlowPathCall)
 105         jit.addPtr(CCallHelpers::TrustedImm32(-maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
<span class="line-modified"> 106     jit.setupArguments&lt;decltype(slowPathFunction)&gt;(GPRInfo::regT2);</span>
 107     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
 108     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
 109     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 110     if (maxFrameExtentForSlowPathCall)
 111         jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 112 #endif
 113 
 114     // This slow call will return the address of one of the following:
 115     // 1) Exception throwing thunk.
 116     // 2) Host call return value returner thingy.
 117     // 3) The function to call.
 118     // The second return value GPR will hold a non-zero value for tail calls.
 119 
 120     emitPointerValidation(jit, GPRInfo::returnValueGPR, JSEntryPtrTag);
 121     jit.emitFunctionEpilogue();
 122     jit.untagReturnAddress();
 123 
 124     RELEASE_ASSERT(reinterpret_cast&lt;void*&gt;(KeepTheFrame) == reinterpret_cast&lt;void*&gt;(0));
 125     CCallHelpers::Jump doNotTrash = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::returnValueGPR2);
 126 
</pre>
<hr />
<pre>
 167     // The callee is in regT0 (for JSVALUE32_64, the tag is in regT1).
 168     // The return address is on the stack, or in the link register. We will hence
 169     // jump to the callee, or save the return address to the call frame while we
 170     // make a C++ function call to the appropriate JIT operation.
 171 
 172     CCallHelpers jit;
 173 
 174     CCallHelpers::JumpList slowCase;
 175 
 176     // This is a slow path execution, and regT2 contains the CallLinkInfo. Count the
 177     // slow path execution for the profiler.
 178     jit.add32(
 179         CCallHelpers::TrustedImm32(1),
 180         CCallHelpers::Address(GPRInfo::regT2, CallLinkInfo::offsetOfSlowPathCount()));
 181 
 182     // FIXME: we should have a story for eliminating these checks. In many cases,
 183     // the DFG knows that the value is definitely a cell, or definitely a function.
 184 
 185 #if USE(JSVALUE64)
 186     if (callLinkInfo.isTailCall()) {
<span class="line-modified"> 187         // Tail calls could have clobbered the GPRInfo::tagMaskRegister because they</span>
 188         // restore callee saved registers before getthing here. So, let&#39;s materialize
<span class="line-modified"> 189         // the TagMask in a temp register and use the temp instead.</span>
 190         slowCase.append(jit.branchIfNotCell(GPRInfo::regT0, DoNotHaveTagRegisters));
 191     } else
 192         slowCase.append(jit.branchIfNotCell(GPRInfo::regT0));
 193 #else
 194     slowCase.append(jit.branchIfNotCell(GPRInfo::regT1));
 195 #endif
 196     auto notJSFunction = jit.branchIfNotFunction(GPRInfo::regT0);
 197 
 198     // Now we know we have a JSFunction.
 199 
<span class="line-modified"> 200     jit.loadPtr(</span>
<span class="line-modified"> 201         CCallHelpers::Address(GPRInfo::regT0, JSFunction::offsetOfExecutable()),</span>
<span class="line-modified"> 202         GPRInfo::regT4);</span>

 203     jit.loadPtr(
 204         CCallHelpers::Address(
 205             GPRInfo::regT4, ExecutableBase::offsetOfJITCodeWithArityCheckFor(
 206                 callLinkInfo.specializationKind())),
 207         GPRInfo::regT4);
 208     slowCase.append(jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT4));
 209 
 210     // Now we know that we have a CodeBlock, and we&#39;re committed to making a fast
 211     // call.
 212 
 213     // Make a tail call. This will return back to JIT code.
 214     JSInterfaceJIT::Label callCode(jit.label());
 215     emitPointerValidation(jit, GPRInfo::regT4, JSEntryPtrTag);
 216     if (callLinkInfo.isTailCall()) {
 217         jit.preserveReturnAddressAfterCall(GPRInfo::regT0);
 218         jit.prepareForTailCallSlow(GPRInfo::regT4);
 219     }
 220     jit.farJump(GPRInfo::regT4, JSEntryPtrTag);
 221 
 222     notJSFunction.link(&amp;jit);
</pre>
<hr />
<pre>
 240 enum ThunkEntryType { EnterViaCall, EnterViaJumpWithSavedTags, EnterViaJumpWithoutSavedTags };
 241 enum class ThunkFunctionType { JSFunction, InternalFunction };
 242 
 243 static MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeForGenerator(VM&amp; vm, ThunkFunctionType thunkFunctionType, CodeSpecializationKind kind, ThunkEntryType entryType = EnterViaCall)
 244 {
 245     // FIXME: This should be able to log ShadowChicken prologue packets.
 246     // https://bugs.webkit.org/show_bug.cgi?id=155689
 247 
 248     int executableOffsetToFunction = NativeExecutable::offsetOfNativeFunctionFor(kind);
 249 
 250     JSInterfaceJIT jit(&amp;vm);
 251 
 252     switch (entryType) {
 253     case EnterViaCall:
 254         jit.emitFunctionPrologue();
 255         break;
 256     case EnterViaJumpWithSavedTags:
 257 #if USE(JSVALUE64)
 258         // We&#39;re coming from a specialized thunk that has saved the prior tag registers&#39; contents.
 259         // Restore them now.
<span class="line-modified"> 260         jit.popPair(JSInterfaceJIT::tagTypeNumberRegister, JSInterfaceJIT::tagMaskRegister);</span>
 261 #endif
 262         break;
 263     case EnterViaJumpWithoutSavedTags:
 264         jit.move(JSInterfaceJIT::framePointerRegister, JSInterfaceJIT::stackPointerRegister);
 265         break;
 266     }
 267 
 268     jit.emitPutToCallFrameHeader(0, CallFrameSlot::codeBlock);
<span class="line-modified"> 269     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm.topCallFrame);</span>
<span class="line-removed"> 270 </span>
<span class="line-removed"> 271 #if CPU(X86)</span>
<span class="line-removed"> 272     // Calling convention:      f(ecx, edx, ...);</span>
<span class="line-removed"> 273     // Host function signature: f(ExecState*);</span>
<span class="line-removed"> 274     jit.move(JSInterfaceJIT::callFrameRegister, X86Registers::ecx);</span>
<span class="line-removed"> 275 </span>
<span class="line-removed"> 276     jit.subPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister); // Align stack after prologue.</span>
<span class="line-removed"> 277 </span>
<span class="line-removed"> 278     // call the function</span>
<span class="line-removed"> 279     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, JSInterfaceJIT::regT1);</span>
<span class="line-removed"> 280     if (thunkFunctionType == ThunkFunctionType::JSFunction) {</span>
<span class="line-removed"> 281         jit.loadPtr(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, JSFunction::offsetOfExecutable()), JSInterfaceJIT::regT1);</span>
<span class="line-removed"> 282         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, executableOffsetToFunction), JSEntryPtrTag);</span>
<span class="line-removed"> 283     } else</span>
<span class="line-removed"> 284         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);</span>
<span class="line-removed"> 285 </span>
<span class="line-removed"> 286     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister);</span>
<span class="line-removed"> 287 </span>
<span class="line-removed"> 288 #elif CPU(X86_64)</span>
<span class="line-removed"> 289 #if !OS(WINDOWS)</span>
<span class="line-removed"> 290     // Calling convention:      f(edi, esi, edx, ecx, ...);</span>
<span class="line-removed"> 291     // Host function signature: f(ExecState*);</span>
<span class="line-removed"> 292     jit.move(JSInterfaceJIT::callFrameRegister, X86Registers::edi);</span>
<span class="line-removed"> 293 </span>
<span class="line-removed"> 294     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, X86Registers::esi);</span>
<span class="line-removed"> 295     if (thunkFunctionType == ThunkFunctionType::JSFunction) {</span>
<span class="line-removed"> 296         jit.loadPtr(JSInterfaceJIT::Address(X86Registers::esi, JSFunction::offsetOfExecutable()), X86Registers::r9);</span>
<span class="line-removed"> 297         jit.loadPtr(JSInterfaceJIT::Address(X86Registers::r9, executableOffsetToFunction), X86Registers::r9);</span>
<span class="line-removed"> 298     } else</span>
<span class="line-removed"> 299         jit.loadPtr(JSInterfaceJIT::Address(X86Registers::esi, InternalFunction::offsetOfNativeFunctionFor(kind)), X86Registers::r9);</span>
<span class="line-removed"> 300     jit.call(X86Registers::r9, JSEntryPtrTag);</span>
<span class="line-removed"> 301 </span>
<span class="line-removed"> 302 #else</span>
<span class="line-removed"> 303     // Calling convention:      f(ecx, edx, r8, r9, ...);</span>
<span class="line-removed"> 304     // Host function signature: f(ExecState*);</span>
<span class="line-removed"> 305     jit.move(JSInterfaceJIT::callFrameRegister, X86Registers::ecx);</span>
 306 


 307     // Leave space for the callee parameter home addresses.
 308     // At this point the stack is aligned to 16 bytes, but if this changes at some point, we need to emit code to align it.
<span class="line-modified"> 309     jit.subPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);</span>
<span class="line-modified"> 310 </span>
<span class="line-removed"> 311     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, X86Registers::edx);</span>
<span class="line-removed"> 312     if (thunkFunctionType == ThunkFunctionType::JSFunction) {</span>
<span class="line-removed"> 313         jit.loadPtr(JSInterfaceJIT::Address(X86Registers::edx, JSFunction::offsetOfExecutable()), X86Registers::r9);</span>
<span class="line-removed"> 314         jit.call(JSInterfaceJIT::Address(X86Registers::r9, executableOffsetToFunction), JSEntryPtrTag);</span>
<span class="line-removed"> 315     } else</span>
<span class="line-removed"> 316         jit.call(JSInterfaceJIT::Address(X86Registers::edx, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);</span>
<span class="line-removed"> 317 </span>
<span class="line-removed"> 318     jit.addPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);</span>
<span class="line-removed"> 319 #endif</span>
<span class="line-removed"> 320 </span>
<span class="line-removed"> 321 #elif CPU(ARM64)</span>
<span class="line-removed"> 322     COMPILE_ASSERT(ARM64Registers::x0 != JSInterfaceJIT::regT3, T3_not_trampled_by_arg_0);</span>
<span class="line-removed"> 323     COMPILE_ASSERT(ARM64Registers::x1 != JSInterfaceJIT::regT3, T3_not_trampled_by_arg_1);</span>
<span class="line-removed"> 324     COMPILE_ASSERT(ARM64Registers::x2 != JSInterfaceJIT::regT3, T3_not_trampled_by_arg_2);</span>
<span class="line-removed"> 325 </span>
<span class="line-removed"> 326     // Host function signature: f(ExecState*);</span>
<span class="line-removed"> 327     jit.move(JSInterfaceJIT::callFrameRegister, ARM64Registers::x0);</span>
<span class="line-removed"> 328 </span>
<span class="line-removed"> 329     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, ARM64Registers::x1);</span>
<span class="line-removed"> 330     if (thunkFunctionType == ThunkFunctionType::JSFunction) {</span>
<span class="line-removed"> 331         jit.loadPtr(JSInterfaceJIT::Address(ARM64Registers::x1, JSFunction::offsetOfExecutable()), ARM64Registers::x2);</span>
<span class="line-removed"> 332         jit.loadPtr(JSInterfaceJIT::Address(ARM64Registers::x2, executableOffsetToFunction), ARM64Registers::x2);</span>
<span class="line-removed"> 333     } else</span>
<span class="line-removed"> 334         jit.loadPtr(JSInterfaceJIT::Address(ARM64Registers::x1, InternalFunction::offsetOfNativeFunctionFor(kind)), ARM64Registers::x2);</span>
<span class="line-removed"> 335     jit.call(ARM64Registers::x2, JSEntryPtrTag);</span>
<span class="line-removed"> 336 </span>
<span class="line-removed"> 337 #elif CPU(ARM_THUMB2) || CPU(MIPS)</span>
<span class="line-removed"> 338 #if CPU(MIPS)</span>
 339     // Allocate stack space for (unused) 16 bytes (8-byte aligned) for 4 arguments.
<span class="line-modified"> 340     jit.subPtr(JSInterfaceJIT::TrustedImm32(16), JSInterfaceJIT::stackPointerRegister);</span>
 341 #endif
 342 
<span class="line-modified"> 343     // Calling convention is f(argumentGPR0, argumentGPR1, ...).</span>
<span class="line-modified"> 344     // Host function signature is f(ExecState*).</span>
<span class="line-removed"> 345     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR0);</span>
 346 
<span class="line-removed"> 347     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, JSInterfaceJIT::argumentGPR1);</span>
 348     if (thunkFunctionType == ThunkFunctionType::JSFunction) {
<span class="line-modified"> 349         jit.loadPtr(JSInterfaceJIT::Address(JSInterfaceJIT::argumentGPR1, JSFunction::offsetOfExecutable()), JSInterfaceJIT::regT2);</span>
<span class="line-modified"> 350         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT2, executableOffsetToFunction), JSEntryPtrTag);</span>
<span class="line-modified"> 351     } else</span>
<span class="line-modified"> 352         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::argumentGPR1, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);</span>







 353 
<span class="line-modified"> 354 #if CPU(MIPS)</span>
<span class="line-modified"> 355     // Restore stack space</span>
<span class="line-modified"> 356     jit.addPtr(JSInterfaceJIT::TrustedImm32(16), JSInterfaceJIT::stackPointerRegister);</span>
<span class="line-modified"> 357 #endif</span>
<span class="line-removed"> 358 #else</span>
<span class="line-removed"> 359 #error &quot;JIT not supported on this platform.&quot;</span>
<span class="line-removed"> 360     UNUSED_PARAM(executableOffsetToFunction);</span>
<span class="line-removed"> 361     abortWithReason(TGNotSupported);</span>
 362 #endif
 363 
 364     // Check for an exception
 365 #if USE(JSVALUE64)
 366     jit.load64(vm.addressOfException(), JSInterfaceJIT::regT2);
 367     JSInterfaceJIT::Jump exceptionHandler = jit.branchTest64(JSInterfaceJIT::NonZero, JSInterfaceJIT::regT2);
 368 #else
 369     JSInterfaceJIT::Jump exceptionHandler = jit.branch32(
 370         JSInterfaceJIT::NotEqual,
 371         JSInterfaceJIT::AbsoluteAddress(vm.addressOfException()),
 372         JSInterfaceJIT::TrustedImm32(0));
 373 #endif
 374 
 375     jit.emitFunctionEpilogue();
 376     // Return.
 377     jit.ret();
 378 
 379     // Handle an exception
 380     exceptionHandler.link(&amp;jit);
 381 
 382     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
 383     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm.topCallFrame);
<span class="line-removed"> 384 </span>
<span class="line-removed"> 385 #if CPU(X86) &amp;&amp; USE(JSVALUE32_64)</span>
<span class="line-removed"> 386     jit.subPtr(JSInterfaceJIT::TrustedImm32(4), JSInterfaceJIT::stackPointerRegister);</span>
<span class="line-removed"> 387     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT0);</span>
<span class="line-removed"> 388     jit.push(JSInterfaceJIT::regT0);</span>
<span class="line-removed"> 389 #else</span>
 390 #if OS(WINDOWS)
 391     // Allocate space on stack for the 4 parameter registers.
 392     jit.subPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);



 393 #endif
<span class="line-modified"> 394     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR0);</span>
<span class="line-removed"> 395 #endif</span>
 396     jit.move(JSInterfaceJIT::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationVMHandleException)), JSInterfaceJIT::regT3);
 397     jit.call(JSInterfaceJIT::regT3, OperationPtrTag);
<span class="line-modified"> 398 #if CPU(X86) &amp;&amp; USE(JSVALUE32_64)</span>
<span class="line-removed"> 399     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister);</span>
<span class="line-removed"> 400 #elif OS(WINDOWS)</span>
 401     jit.addPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);


 402 #endif
 403 
 404     jit.jumpToExceptionHandler(vm);
 405 
 406     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 407     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;%s %s%s trampoline&quot;, thunkFunctionType == ThunkFunctionType::JSFunction ? &quot;native&quot; : &quot;internal&quot;, entryType == EnterViaJumpWithSavedTags ? &quot;Tail With Saved Tags &quot; : entryType == EnterViaJumpWithoutSavedTags ? &quot;Tail Without Saved Tags &quot; : &quot;&quot;, toCString(kind).data());
 408 }
 409 
 410 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeCallGenerator(VM&amp; vm)
 411 {
 412     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall);
 413 }
 414 
 415 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallGenerator(VM&amp; vm)
 416 {
 417     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall, EnterViaJumpWithSavedTags);
 418 }
 419 
 420 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallWithoutSavedTagsGenerator(VM&amp; vm)
 421 {
</pre>
<hr />
<pre>
 446 #if USE(JSVALUE64)
 447 #if OS(WINDOWS)
 448     const GPRReg extraTemp = JSInterfaceJIT::regT0;
 449 #else
 450     const GPRReg extraTemp = JSInterfaceJIT::regT5;
 451 #endif
 452 #  if CPU(X86_64)
 453     jit.pop(JSInterfaceJIT::regT4);
 454 #  endif
 455     jit.tagReturnAddress();
 456 #if CPU(ARM64E)
 457     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 458     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
 459     jit.untagPtr(extraTemp, GPRInfo::regT3);
 460     PtrTag tempReturnPCTag = static_cast&lt;PtrTag&gt;(random());
 461     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
 462     jit.tagPtr(extraTemp, GPRInfo::regT3);
 463     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 464 #endif
 465     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
<span class="line-modified"> 466     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCount), JSInterfaceJIT::argumentGPR2);</span>
 467     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 468 
 469     // Check to see if we have extra slots we can use
 470     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 471     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 472     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
<span class="line-modified"> 473     jit.move(JSInterfaceJIT::TrustedImm64(ValueUndefined), extraTemp);</span>
 474     JSInterfaceJIT::Label fillExtraSlots(jit.label());
 475     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight));
 476     jit.add32(JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2);
 477     jit.branchSub32(JSInterfaceJIT::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR1).linkTo(fillExtraSlots, &amp;jit);
 478     jit.and32(JSInterfaceJIT::TrustedImm32(-stackAlignmentRegisters()), JSInterfaceJIT::argumentGPR0);
 479     JSInterfaceJIT::Jump done = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR0);
 480     noExtraSlot.link(&amp;jit);
 481 
 482     jit.neg64(JSInterfaceJIT::argumentGPR0);
 483 
 484     // Adjust call frame register and stack pointer to account for missing args.
 485     // We need to change the stack pointer first before performing copy/fill loops.
 486     // This stack space below the stack pointer is considered unused by OS. Therefore,
 487     // OS may corrupt this space when constructing a signal stack.
 488     jit.move(JSInterfaceJIT::argumentGPR0, extraTemp);
 489     jit.lshift64(JSInterfaceJIT::TrustedImm32(3), extraTemp);
 490     jit.addPtr(extraTemp, JSInterfaceJIT::callFrameRegister);
 491     jit.untagReturnAddress();
 492     jit.addPtr(extraTemp, JSInterfaceJIT::stackPointerRegister);
 493     jit.tagReturnAddress();
 494 
 495     // Move current frame down argumentGPR0 number of slots
 496     JSInterfaceJIT::Label copyLoop(jit.label());
 497     jit.load64(JSInterfaceJIT::regT3, extraTemp);
 498     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 499     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 500     jit.branchSub32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(copyLoop, &amp;jit);
 501 
 502     // Fill in argumentGPR0 missing arg slots with undefined
 503     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR2);
<span class="line-modified"> 504     jit.move(JSInterfaceJIT::TrustedImm64(ValueUndefined), extraTemp);</span>
 505     JSInterfaceJIT::Label fillUndefinedLoop(jit.label());
 506     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 507     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 508     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 509 
 510     done.link(&amp;jit);
 511 
 512 #if CPU(ARM64E)
 513     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 514     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
 515     jit.untagPtr(extraTemp, GPRInfo::regT3);
 516     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
 517     jit.tagPtr(extraTemp, GPRInfo::regT3);
 518     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 519 #endif
 520 
 521 #  if CPU(X86_64)
 522     jit.push(JSInterfaceJIT::regT4);
 523 #  endif
 524     jit.ret();
 525 #else // USE(JSVALUE64) section above, USE(JSVALUE32_64) section below.
<span class="line-removed"> 526 #  if CPU(X86)</span>
<span class="line-removed"> 527     jit.pop(JSInterfaceJIT::regT4);</span>
<span class="line-removed"> 528 #  endif</span>
 529     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
<span class="line-modified"> 530     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCount), JSInterfaceJIT::argumentGPR2);</span>
 531     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 532 
 533     // Check to see if we have extra slots we can use
 534     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 535     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 536     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
 537     JSInterfaceJIT::Label fillExtraSlots(jit.label());
 538     jit.move(JSInterfaceJIT::TrustedImm32(0), JSInterfaceJIT::regT5);
 539     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight, PayloadOffset));
 540     jit.move(JSInterfaceJIT::TrustedImm32(JSValue::UndefinedTag), JSInterfaceJIT::regT5);
 541     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight, TagOffset));
 542     jit.add32(JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2);
 543     jit.branchSub32(JSInterfaceJIT::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR1).linkTo(fillExtraSlots, &amp;jit);
 544     jit.and32(JSInterfaceJIT::TrustedImm32(-stackAlignmentRegisters()), JSInterfaceJIT::argumentGPR0);
 545     JSInterfaceJIT::Jump done = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR0);
 546     noExtraSlot.link(&amp;jit);
 547 
 548     jit.neg32(JSInterfaceJIT::argumentGPR0);
 549 
 550     // Adjust call frame register and stack pointer to account for missing args.
</pre>
<hr />
<pre>
 563     jit.load32(MacroAssembler::Address(JSInterfaceJIT::regT3, PayloadOffset), JSInterfaceJIT::regT5);
 564     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, PayloadOffset));
 565     jit.load32(MacroAssembler::Address(JSInterfaceJIT::regT3, TagOffset), JSInterfaceJIT::regT5);
 566     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, TagOffset));
 567     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 568     jit.branchSub32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(copyLoop, &amp;jit);
 569 
 570     // Fill in argumentGPR0 missing arg slots with undefined
 571     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR2);
 572     JSInterfaceJIT::Label fillUndefinedLoop(jit.label());
 573     jit.move(JSInterfaceJIT::TrustedImm32(0), JSInterfaceJIT::regT5);
 574     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, PayloadOffset));
 575     jit.move(JSInterfaceJIT::TrustedImm32(JSValue::UndefinedTag), JSInterfaceJIT::regT5);
 576     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, TagOffset));
 577 
 578     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 579     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 580 
 581     done.link(&amp;jit);
 582 
<span class="line-removed"> 583 #  if CPU(X86)</span>
<span class="line-removed"> 584     jit.push(JSInterfaceJIT::regT4);</span>
<span class="line-removed"> 585 #  endif</span>
 586     jit.ret();
 587 #endif // End of USE(JSVALUE32_64) section.
 588 
 589     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 590     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;fixup arity&quot;);
 591 }
 592 
 593 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; unreachableGenerator(VM&amp; vm)
 594 {
 595     JSInterfaceJIT jit(&amp;vm);
 596 
 597     jit.breakpoint();
 598 
 599     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 600     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;unreachable thunk&quot;);
 601 }
 602 
 603 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; stringGetByValGenerator(VM&amp; vm)
 604 {
 605     // regT0 is JSString*, and regT1 (64bit) or regT2 (32bit) is int index.
</pre>
<hr />
<pre>
 610     GPRReg scratchGPR = GPRInfo::regT2;
 611 #else
 612     GPRReg stringGPR = GPRInfo::regT0;
 613     GPRReg indexGPR = GPRInfo::regT2;
 614     GPRReg scratchGPR = GPRInfo::regT1;
 615 #endif
 616 
 617     JSInterfaceJIT jit(&amp;vm);
 618     JSInterfaceJIT::JumpList failures;
 619     jit.tagReturnAddress();
 620 
 621     // Load string length to regT2, and start the process of loading the data pointer into regT0
 622     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, JSString::offsetOfValue()), stringGPR);
 623     failures.append(jit.branchIfRopeStringImpl(stringGPR));
 624     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::lengthMemoryOffset()), scratchGPR);
 625 
 626     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 627     failures.append(jit.branch32(JSInterfaceJIT::AboveOrEqual, indexGPR, scratchGPR));
 628 
 629     // Load the character
<span class="line-removed"> 630     JSInterfaceJIT::JumpList is16Bit;</span>
 631     JSInterfaceJIT::JumpList cont8Bit;
 632     // Load the string flags
 633     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::flagsOffset()), scratchGPR);
 634     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, StringImpl::dataOffset()), stringGPR);
<span class="line-modified"> 635     is16Bit.append(jit.branchTest32(JSInterfaceJIT::Zero, scratchGPR, JSInterfaceJIT::TrustedImm32(StringImpl::flagIs8Bit())));</span>
 636     jit.load8(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesOne, 0), stringGPR);
 637     cont8Bit.append(jit.jump());
 638     is16Bit.link(&amp;jit);
 639     jit.load16(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesTwo, 0), stringGPR);
 640     cont8Bit.link(&amp;jit);
 641 
 642     failures.append(jit.branch32(JSInterfaceJIT::Above, stringGPR, JSInterfaceJIT::TrustedImm32(maxSingleCharacterString)));
 643     jit.move(JSInterfaceJIT::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), indexGPR);
 644     jit.loadPtr(JSInterfaceJIT::BaseIndex(indexGPR, stringGPR, JSInterfaceJIT::ScalePtr, 0), stringGPR);
 645     jit.ret();
 646 
 647     failures.link(&amp;jit);
 648     jit.move(JSInterfaceJIT::TrustedImm32(0), stringGPR);
 649     jit.ret();
 650 
 651     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 652     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;String get_by_val stub&quot;);
 653 }
 654 
 655 static void stringCharLoad(SpecializedThunkJIT&amp; jit)
 656 {
 657     // load string
 658     jit.loadJSStringArgument(SpecializedThunkJIT::ThisArgument, SpecializedThunkJIT::regT0);
 659 
 660     // Load string length to regT2, and start the process of loading the data pointer into regT0
 661     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, JSString::offsetOfValue()), SpecializedThunkJIT::regT0);
 662     jit.appendFailure(jit.branchIfRopeStringImpl(SpecializedThunkJIT::regT0));
 663     jit.load32(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::lengthMemoryOffset()), SpecializedThunkJIT::regT2);
 664 
 665     // load index
 666     jit.loadInt32Argument(0, SpecializedThunkJIT::regT1); // regT1 contains the index
 667 
 668     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 669     jit.appendFailure(jit.branch32(MacroAssembler::AboveOrEqual, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2));
 670 
 671     // Load the character
 672     SpecializedThunkJIT::JumpList is16Bit;
 673     SpecializedThunkJIT::JumpList cont8Bit;
 674     // Load the string flags
<span class="line-modified"> 675     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::flagsOffset()), SpecializedThunkJIT::regT2);</span>
 676     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::dataOffset()), SpecializedThunkJIT::regT0);
 677     is16Bit.append(jit.branchTest32(MacroAssembler::Zero, SpecializedThunkJIT::regT2, MacroAssembler::TrustedImm32(StringImpl::flagIs8Bit())));
 678     jit.load8(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesOne, 0), SpecializedThunkJIT::regT0);
 679     cont8Bit.append(jit.jump());
 680     is16Bit.link(&amp;jit);
 681     jit.load16(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesTwo, 0), SpecializedThunkJIT::regT0);
 682     cont8Bit.link(&amp;jit);
 683 }
 684 
 685 static void charToString(SpecializedThunkJIT&amp; jit, VM&amp; vm, MacroAssembler::RegisterID src, MacroAssembler::RegisterID dst, MacroAssembler::RegisterID scratch)
 686 {
 687     jit.appendFailure(jit.branch32(MacroAssembler::Above, src, MacroAssembler::TrustedImm32(maxSingleCharacterString)));
 688     jit.move(MacroAssembler::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), scratch);
 689     jit.loadPtr(MacroAssembler::BaseIndex(scratch, src, MacroAssembler::ScalePtr, 0), dst);
 690     jit.appendFailure(jit.branchTestPtr(MacroAssembler::Zero, dst));
 691 }
 692 
 693 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charCodeAtThunkGenerator(VM&amp; vm)
 694 {
 695     SpecializedThunkJIT jit(vm, 1);
</pre>
<hr />
<pre>
 700 
 701 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charAtThunkGenerator(VM&amp; vm)
 702 {
 703     SpecializedThunkJIT jit(vm, 1);
 704     stringCharLoad(jit);
 705     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 706     jit.returnJSCell(SpecializedThunkJIT::regT0);
 707     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;charAt&quot;);
 708 }
 709 
 710 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; fromCharCodeThunkGenerator(VM&amp; vm)
 711 {
 712     SpecializedThunkJIT jit(vm, 1);
 713     // load char code
 714     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0);
 715     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 716     jit.returnJSCell(SpecializedThunkJIT::regT0);
 717     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;fromCharCode&quot;);
 718 }
 719 












































 720 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; clz32ThunkGenerator(VM&amp; vm)
 721 {
 722     SpecializedThunkJIT jit(vm, 1);
 723     MacroAssembler::Jump nonIntArgJump;
 724     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntArgJump);
 725 
 726     SpecializedThunkJIT::Label convertedArgumentReentry(&amp;jit);
 727     jit.countLeadingZeros32(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 728     jit.returnInt32(SpecializedThunkJIT::regT1);
 729 
 730     if (jit.supportsFloatingPointTruncate()) {
 731         nonIntArgJump.link(&amp;jit);
 732         jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 733         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(convertedArgumentReentry, &amp;jit);
 734         jit.appendFailure(jit.jump());
 735     } else
 736         jit.appendFailure(nonIntArgJump);
 737 
 738     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;clz32&quot;);
 739 }
</pre>
<hr />
<pre>
 800 #elif CPU(X86) &amp;&amp; COMPILER(GCC_COMPATIBLE) &amp;&amp; (OS(DARWIN) || OS(LINUX))
 801 #define defineUnaryDoubleOpWrapper(function) \
 802     asm( \
 803         &quot;.text\n&quot; \
 804         &quot;.globl &quot; SYMBOL_STRING(function##Thunk) &quot;\n&quot; \
 805         HIDE_SYMBOL(function##Thunk) &quot;\n&quot; \
 806         SYMBOL_STRING(function##Thunk) &quot;:&quot; &quot;\n&quot; \
 807         &quot;subl $20, %esp\n&quot; \
 808         &quot;movsd %xmm0, (%esp) \n&quot; \
 809         &quot;call &quot; GLOBAL_REFERENCE(function) &quot;\n&quot; \
 810         &quot;fstpl (%esp) \n&quot; \
 811         &quot;movsd (%esp), %xmm0 \n&quot; \
 812         &quot;addl $20, %esp\n&quot; \
 813         &quot;ret\n&quot; \
 814     );\
 815     extern &quot;C&quot; { \
 816         MathThunkCallingConvention function##Thunk(MathThunkCallingConvention); \
 817     } \
 818     static MathThunk UnaryDoubleOpWrapper(function) = &amp;function##Thunk;
 819 
<span class="line-modified"> 820 #elif CPU(ARM_THUMB2) &amp;&amp; COMPILER(GCC_COMPATIBLE) &amp;&amp; PLATFORM(IOS_FAMILY)</span>
 821 
 822 #define defineUnaryDoubleOpWrapper(function) \
 823     asm( \
 824         &quot;.text\n&quot; \
 825         &quot;.align 2\n&quot; \
 826         &quot;.globl &quot; SYMBOL_STRING(function##Thunk) &quot;\n&quot; \
 827         HIDE_SYMBOL(function##Thunk) &quot;\n&quot; \
 828         &quot;.thumb\n&quot; \
 829         &quot;.thumb_func &quot; THUMB_FUNC_PARAM(function##Thunk) &quot;\n&quot; \
 830         SYMBOL_STRING(function##Thunk) &quot;:&quot; &quot;\n&quot; \
 831         &quot;push {lr}\n&quot; \
 832         &quot;vmov r0, r1, d0\n&quot; \
 833         &quot;blx &quot; GLOBAL_REFERENCE(function) &quot;\n&quot; \
 834         &quot;vmov d0, r0, r1\n&quot; \
 835         &quot;pop {lr}\n&quot; \
 836         &quot;bx lr\n&quot; \
 837     ); \
 838     extern &quot;C&quot; { \
 839         MathThunkCallingConvention function##Thunk(MathThunkCallingConvention); \
 840     } \
</pre>
<hr />
<pre>
 879         __asm movsd xmm0, mmword ptr [esp] \
 880         __asm add esp, 20 \
 881         __asm ret \
 882         } \
 883     } \
 884     static MathThunk UnaryDoubleOpWrapper(function) = &amp;function##Thunk;
 885 
 886 #else
 887 
 888 #define defineUnaryDoubleOpWrapper(function) \
 889     static MathThunk UnaryDoubleOpWrapper(function) = 0
 890 #endif
 891 
 892 defineUnaryDoubleOpWrapper(jsRound);
 893 defineUnaryDoubleOpWrapper(exp);
 894 defineUnaryDoubleOpWrapper(log);
 895 defineUnaryDoubleOpWrapper(floor);
 896 defineUnaryDoubleOpWrapper(ceil);
 897 defineUnaryDoubleOpWrapper(trunc);
 898 
<span class="line-removed"> 899 static const double halfConstant = 0.5;</span>
<span class="line-removed"> 900 </span>
 901 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; floorThunkGenerator(VM&amp; vm)
 902 {
 903     SpecializedThunkJIT jit(vm, 1);
 904     MacroAssembler::Jump nonIntJump;
 905     if (!UnaryDoubleOpWrapper(floor) || !jit.supportsFloatingPoint())
 906         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
 907     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 908     jit.returnInt32(SpecializedThunkJIT::regT0);
 909     nonIntJump.link(&amp;jit);
 910     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 911 
 912     if (jit.supportsFloatingPointRounding()) {
 913         SpecializedThunkJIT::JumpList doubleResult;
 914         jit.floorDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 915         jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 916         jit.returnInt32(SpecializedThunkJIT::regT0);
 917         doubleResult.link(&amp;jit);
 918         jit.returnDouble(SpecializedThunkJIT::fpRegT0);
 919         return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;floor&quot;);
 920     }
</pre>
<hr />
<pre>
 980         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(trunc));
 981 
 982     SpecializedThunkJIT::JumpList doubleResult;
 983     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 984     jit.returnInt32(SpecializedThunkJIT::regT0);
 985     doubleResult.link(&amp;jit);
 986     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
 987     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;trunc&quot;);
 988 }
 989 
 990 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; roundThunkGenerator(VM&amp; vm)
 991 {
 992     SpecializedThunkJIT jit(vm, 1);
 993     if (!UnaryDoubleOpWrapper(jsRound) || !jit.supportsFloatingPoint())
 994         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
 995     MacroAssembler::Jump nonIntJump;
 996     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 997     jit.returnInt32(SpecializedThunkJIT::regT0);
 998     nonIntJump.link(&amp;jit);
 999     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
<span class="line-removed">1000     SpecializedThunkJIT::Jump intResult;</span>
1001     SpecializedThunkJIT::JumpList doubleResult;
<span class="line-modified">1002     if (jit.supportsFloatingPointTruncate()) {</span>
1003         jit.moveZeroToDouble(SpecializedThunkJIT::fpRegT1);
1004         doubleResult.append(jit.branchDouble(MacroAssembler::DoubleEqual, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
<span class="line-modified">1005         SpecializedThunkJIT::JumpList slowPath;</span>
<span class="line-modified">1006         // Handle the negative doubles in the slow path for now.</span>
<span class="line-modified">1007         slowPath.append(jit.branchDouble(MacroAssembler::DoubleLessThanOrUnordered, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));</span>
<span class="line-modified">1008         jit.loadDouble(MacroAssembler::TrustedImmPtr(&amp;halfConstant), SpecializedThunkJIT::fpRegT1);</span>
<span class="line-modified">1009         jit.addDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1);</span>
<span class="line-modified">1010         slowPath.append(jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT1, SpecializedThunkJIT::regT0));</span>
<span class="line-modified">1011         intResult = jit.jump();</span>
<span class="line-modified">1012         slowPath.link(&amp;jit);</span>
<span class="line-modified">1013     }</span>
<span class="line-modified">1014     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(jsRound));</span>








1015     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
<span class="line-removed">1016     if (jit.supportsFloatingPointTruncate())</span>
<span class="line-removed">1017         intResult.link(&amp;jit);</span>
1018     jit.returnInt32(SpecializedThunkJIT::regT0);
1019     doubleResult.link(&amp;jit);
1020     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1021     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;round&quot;);
1022 }
1023 
1024 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; expThunkGenerator(VM&amp; vm)
1025 {
1026     if (!UnaryDoubleOpWrapper(exp))
1027         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1028     SpecializedThunkJIT jit(vm, 1);
1029     if (!jit.supportsFloatingPoint())
1030         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1031     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1032     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(exp));
1033     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1034     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;exp&quot;);
1035 }
1036 
1037 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; logThunkGenerator(VM&amp; vm)
1038 {
1039     if (!UnaryDoubleOpWrapper(log))
1040         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1041     SpecializedThunkJIT jit(vm, 1);
1042     if (!jit.supportsFloatingPoint())
1043         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1044     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1045     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(log));
1046     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1047     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;log&quot;);
1048 }
1049 
1050 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; absThunkGenerator(VM&amp; vm)
1051 {
1052     SpecializedThunkJIT jit(vm, 1);
1053     if (!jit.supportsFloatingPointAbs())
1054         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1055 
1056 #if USE(JSVALUE64)
<span class="line-modified">1057     unsigned virtualRegisterIndex = CallFrame::argumentOffset(0);</span>
<span class="line-modified">1058     jit.load64(AssemblyHelpers::addressFor(virtualRegisterIndex), GPRInfo::regT0);</span>
1059     auto notInteger = jit.branchIfNotInt32(GPRInfo::regT0);
1060 
1061     // Abs Int32.
1062     jit.rshift32(GPRInfo::regT0, MacroAssembler::TrustedImm32(31), GPRInfo::regT1);
1063     jit.add32(GPRInfo::regT1, GPRInfo::regT0);
1064     jit.xor32(GPRInfo::regT1, GPRInfo::regT0);
1065 
1066     // IntMin cannot be inverted.
1067     MacroAssembler::Jump integerIsIntMin = jit.branchTest32(MacroAssembler::Signed, GPRInfo::regT0);
1068 
1069     // Box and finish.
<span class="line-modified">1070     jit.or64(GPRInfo::tagTypeNumberRegister, GPRInfo::regT0);</span>
1071     MacroAssembler::Jump doneWithIntegers = jit.jump();
1072 
1073     // Handle Doubles.
1074     notInteger.link(&amp;jit);
1075     jit.appendFailure(jit.branchIfNotNumber(GPRInfo::regT0));
1076     jit.unboxDoubleWithoutAssertions(GPRInfo::regT0, GPRInfo::regT0, FPRInfo::fpRegT0);
1077     MacroAssembler::Label absFPR0Label = jit.label();
1078     jit.absDouble(FPRInfo::fpRegT0, FPRInfo::fpRegT1);
1079     jit.boxDouble(FPRInfo::fpRegT1, GPRInfo::regT0);
1080 
1081     // Tail.
1082     doneWithIntegers.link(&amp;jit);
1083     jit.returnJSValue(GPRInfo::regT0);
1084 
1085     // We know the value of regT0 is IntMin. We could load that value from memory but
1086     // it is simpler to just convert it.
1087     integerIsIntMin.link(&amp;jit);
1088     jit.convertInt32ToDouble(GPRInfo::regT0, FPRInfo::fpRegT0);
1089     jit.jump().linkTo(absFPR0Label, &amp;jit);
1090 #else
</pre>
<hr />
<pre>
1134 
1135     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;imul&quot;);
1136 }
1137 
1138 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; randomThunkGenerator(VM&amp; vm)
1139 {
1140     SpecializedThunkJIT jit(vm, 0);
1141     if (!jit.supportsFloatingPoint())
1142         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1143 
1144 #if USE(JSVALUE64)
1145     jit.emitRandomThunk(vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2, SpecializedThunkJIT::regT3, SpecializedThunkJIT::fpRegT0);
1146     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1147 
1148     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;random&quot;);
1149 #else
1150     return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1151 #endif
1152 }
1153 
<span class="line-modified">1154 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; boundThisNoArgsFunctionCallGenerator(VM&amp; vm)</span>
1155 {
1156     CCallHelpers jit;
1157 
1158     jit.emitFunctionPrologue();
1159 
1160     // Set up our call frame.
1161     jit.storePtr(CCallHelpers::TrustedImmPtr(nullptr), CCallHelpers::addressFor(CallFrameSlot::codeBlock));
<span class="line-modified">1162     jit.store32(CCallHelpers::TrustedImm32(0), CCallHelpers::tagFor(CallFrameSlot::argumentCount));</span>
1163 
1164     unsigned extraStackNeeded = 0;
1165     if (unsigned stackMisalignment = sizeof(CallerFrameAndPC) % stackAlignmentBytes())
1166         extraStackNeeded = stackAlignmentBytes() - stackMisalignment;
1167 
1168     // We need to forward all of the arguments that we were passed. We aren&#39;t allowed to do a tail
1169     // call here as far as I can tell. At least not so long as the generic path doesn&#39;t do a tail
1170     // call, since that would be way too weird.
1171 
1172     // The formula for the number of stack bytes needed given some number of parameters (including
1173     // this) is:
1174     //
1175     //     stackAlign((numParams + CallFrameHeaderSize) * sizeof(Register) - sizeof(CallerFrameAndPC))
1176     //
1177     // Probably we want to write this as:
1178     //
1179     //     stackAlign((numParams + (CallFrameHeaderSize - CallerFrameAndPCSize)) * sizeof(Register))
1180     //
1181     // That&#39;s really all there is to this. We have all the registers we need to do it.
1182 
<span class="line-modified">1183     jit.load32(CCallHelpers::payloadFor(CallFrameSlot::argumentCount), GPRInfo::regT1);</span>








1184     jit.add32(CCallHelpers::TrustedImm32(CallFrame::headerSizeInRegisters - CallerFrameAndPC::sizeInRegisters), GPRInfo::regT1, GPRInfo::regT2);
1185     jit.lshift32(CCallHelpers::TrustedImm32(3), GPRInfo::regT2);
1186     jit.add32(CCallHelpers::TrustedImm32(stackAlignmentBytes() - 1), GPRInfo::regT2);
1187     jit.and32(CCallHelpers::TrustedImm32(-stackAlignmentBytes()), GPRInfo::regT2);
1188 
1189     if (extraStackNeeded)
1190         jit.add32(CCallHelpers::TrustedImm32(extraStackNeeded), GPRInfo::regT2);
1191 
<span class="line-modified">1192     // At this point regT1 has the actual argument count and regT2 has the amount of stack we will need.</span>
1193     // Check to see if we have enough stack space.
1194 
1195     jit.negPtr(GPRInfo::regT2);
1196     jit.addPtr(CCallHelpers::stackPointerRegister, GPRInfo::regT2);
1197     CCallHelpers::Jump haveStackSpace = jit.branchPtr(CCallHelpers::BelowOrEqual, CCallHelpers::AbsoluteAddress(vm.addressOfSoftStackLimit()), GPRInfo::regT2);
1198 
1199     // Throw Stack Overflow exception
1200     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
<span class="line-modified">1201     jit.setupArguments&lt;decltype(throwStackOverflowErrorFromThunk)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm), GPRInfo::callFrameRegister);</span>
<span class="line-modified">1202     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(throwStackOverflowErrorFromThunk)), GPRInfo::nonArgGPR0);</span>


1203     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
1204     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
1205     jit.jumpToExceptionHandler(vm);
1206 
1207     haveStackSpace.link(&amp;jit);
1208     jit.move(GPRInfo::regT2, CCallHelpers::stackPointerRegister);
1209 
1210     // Do basic callee frame setup, including &#39;this&#39;.
1211 
<span class="line-modified">1212     jit.loadCell(CCallHelpers::addressFor(CallFrameSlot::callee), GPRInfo::regT3);</span>
1213 
<span class="line-modified">1214     jit.store32(GPRInfo::regT1, CCallHelpers::calleeFramePayloadSlot(CallFrameSlot::argumentCount));</span>
<span class="line-modified">1215 </span>
<span class="line-removed">1216     JSValueRegs valueRegs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT2);</span>
<span class="line-removed">1217     jit.loadValue(CCallHelpers::Address(GPRInfo::regT3, JSBoundFunction::offsetOfBoundThis()), valueRegs);</span>
1218     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(0));
1219 
<span class="line-removed">1220     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT3, JSBoundFunction::offsetOfTargetFunction()), GPRInfo::regT3);</span>
<span class="line-removed">1221     jit.storeCell(GPRInfo::regT3, CCallHelpers::calleeFrameSlot(CallFrameSlot::callee));</span>
<span class="line-removed">1222 </span>
1223     // OK, now we can start copying. This is a simple matter of copying parameters from the caller&#39;s
<span class="line-modified">1224     // frame to the callee&#39;s frame. Note that we know that regT1 (the argument count) must be at</span>
1225     // least 1.

1226     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);
<span class="line-modified">1227     CCallHelpers::Jump done = jit.branchTest32(CCallHelpers::Zero, GPRInfo::regT1);</span>
1228 
1229     CCallHelpers::Label loop = jit.label();

1230     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);
<span class="line-modified">1231     jit.loadValue(CCallHelpers::addressFor(virtualRegisterForArgument(1)).indexedBy(GPRInfo::regT1, CCallHelpers::TimesEight), valueRegs);</span>
1232     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(1).indexedBy(GPRInfo::regT1, CCallHelpers::TimesEight));
<span class="line-modified">1233     jit.branchTest32(CCallHelpers::NonZero, GPRInfo::regT1).linkTo(loop, &amp;jit);</span>
1234 
1235     done.link(&amp;jit);



















1236 
<span class="line-removed">1237     jit.loadPtr(</span>
<span class="line-removed">1238         CCallHelpers::Address(GPRInfo::regT3, JSFunction::offsetOfExecutable()),</span>
<span class="line-removed">1239         GPRInfo::regT0);</span>
1240     jit.loadPtr(
1241         CCallHelpers::Address(
1242             GPRInfo::regT0, ExecutableBase::offsetOfJITCodeWithArityCheckFor(CodeForCall)),
1243         GPRInfo::regT0);
1244     CCallHelpers::Jump noCode = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT0);
1245 
1246     emitPointerValidation(jit, GPRInfo::regT0, JSEntryPtrTag);
1247     jit.call(GPRInfo::regT0, JSEntryPtrTag);
1248 
1249     jit.emitFunctionEpilogue();
1250     jit.ret();
1251 
1252     LinkBuffer linkBuffer(jit, GLOBAL_THUNK_ID);
1253     linkBuffer.link(noCode, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.jitStubs-&gt;ctiNativeTailCallWithoutSavedTags(vm)));
1254     return FINALIZE_CODE(
1255         linkBuffer, JITThunkPtrTag, &quot;Specialized thunk for bound function calls with no arguments&quot;);
1256 }
1257 
1258 } // namespace JSC
1259 
</pre>
</td>
<td>
<hr />
<pre>
  18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  23  * THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;ThunkGenerators.h&quot;
  28 
  29 #include &quot;CodeBlock.h&quot;
  30 #include &quot;DFGSpeculativeJIT.h&quot;
  31 #include &quot;JITExceptions.h&quot;
  32 #include &quot;JITOperations.h&quot;
  33 #include &quot;JSArray.h&quot;
  34 #include &quot;JSBoundFunction.h&quot;
  35 #include &quot;JSCInlines.h&quot;
  36 #include &quot;MathCommon.h&quot;
  37 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
<span class="line-added">  38 #include &quot;ProbeContext.h&quot;</span>
  39 #include &quot;SpecializedThunkJIT.h&quot;
  40 #include &lt;wtf/InlineASM.h&gt;
  41 #include &lt;wtf/StringPrintStream.h&gt;
  42 #include &lt;wtf/text/StringImpl.h&gt;
  43 
  44 #if ENABLE(JIT)
  45 
  46 namespace JSC {
  47 
  48 template&lt;typename TagType&gt;
  49 inline void emitPointerValidation(CCallHelpers&amp; jit, GPRReg pointerGPR, TagType tag)
  50 {
<span class="line-modified">  51     if (!ASSERT_ENABLED)</span>
  52         return;
  53     CCallHelpers::Jump isNonZero = jit.branchTestPtr(CCallHelpers::NonZero, pointerGPR);
  54     jit.abortWithReason(TGInvalidPointer);
  55     isNonZero.link(&amp;jit);
  56     jit.pushToSave(pointerGPR);
  57     jit.untagPtr(tag, pointerGPR);
  58     jit.load8(pointerGPR, pointerGPR);
  59     jit.popToRestore(pointerGPR);
  60 }
  61 
  62 // We will jump here if the JIT code tries to make a call, but the
  63 // linking helper (C++ code) decides to throw an exception instead.
  64 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; throwExceptionFromCallSlowPathGenerator(VM&amp; vm)
  65 {
  66     CCallHelpers jit;
  67 
  68     // The call pushed a return address, so we need to pop it back off to re-align the stack,
  69     // even though we won&#39;t use it.
  70     jit.preserveReturnAddressAfterCall(GPRInfo::nonPreservedNonReturnGPR);
  71 
  72     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
  73 
<span class="line-modified">  74     jit.setupArguments&lt;decltype(operationLookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm));</span>
<span class="line-modified">  75     jit.prepareCallOperation(vm);</span>
<span class="line-added">  76     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandler)), GPRInfo::nonArgGPR0);</span>
  77     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  78     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
  79     jit.jumpToExceptionHandler(vm);
  80 
  81     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
  82     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Throw exception from call slow path thunk&quot;);
  83 }
  84 
<span class="line-modified">  85 static void slowPathFor(CCallHelpers&amp; jit, VM&amp; vm, Sprt_JITOperation_EGCli slowPathFunction)</span>
  86 {
  87     jit.sanitizeStackInline(vm, GPRInfo::nonArgGPR0);
  88     jit.emitFunctionPrologue();
  89     jit.storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);
  90 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
  91     // Windows X86_64 needs some space pointed to by arg0 for return types larger than 64 bits.
  92     // Other argument values are shift by 1. Use space on the stack for our two return values.
  93     // Moving the stack down maxFrameExtentForSlowPathCall bytes gives us room for our 3 arguments
  94     // and space for the 16 byte return area.
  95     jit.addPtr(CCallHelpers::TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), CCallHelpers::stackPointerRegister);
<span class="line-modified">  96     static_assert(GPRInfo::regT2 != GPRInfo::argumentGPR0);</span>
<span class="line-added">  97     static_assert(GPRInfo::regT3 != GPRInfo::argumentGPR0);</span>
<span class="line-added">  98     jit.move(GPRInfo::regT2, GPRInfo::argumentGPR0);</span>
<span class="line-added">  99     jit.move(GPRInfo::regT3, GPRInfo::argumentGPR2);</span>
<span class="line-added"> 100     jit.move(GPRInfo::argumentGPR0, GPRInfo::argumentGPR3);</span>
 101     jit.addPtr(CCallHelpers::TrustedImm32(32), CCallHelpers::stackPointerRegister, GPRInfo::argumentGPR0);
 102     jit.move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 103     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
 104     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
 105     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 106     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR, 8), GPRInfo::returnValueGPR2);
 107     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR), GPRInfo::returnValueGPR);
 108     jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 109 #else
 110     if (maxFrameExtentForSlowPathCall)
 111         jit.addPtr(CCallHelpers::TrustedImm32(-maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
<span class="line-modified"> 112     jit.setupArguments&lt;decltype(slowPathFunction)&gt;(GPRInfo::regT3, GPRInfo::regT2);</span>
 113     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
 114     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
 115     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 116     if (maxFrameExtentForSlowPathCall)
 117         jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 118 #endif
 119 
 120     // This slow call will return the address of one of the following:
 121     // 1) Exception throwing thunk.
 122     // 2) Host call return value returner thingy.
 123     // 3) The function to call.
 124     // The second return value GPR will hold a non-zero value for tail calls.
 125 
 126     emitPointerValidation(jit, GPRInfo::returnValueGPR, JSEntryPtrTag);
 127     jit.emitFunctionEpilogue();
 128     jit.untagReturnAddress();
 129 
 130     RELEASE_ASSERT(reinterpret_cast&lt;void*&gt;(KeepTheFrame) == reinterpret_cast&lt;void*&gt;(0));
 131     CCallHelpers::Jump doNotTrash = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::returnValueGPR2);
 132 
</pre>
<hr />
<pre>
 173     // The callee is in regT0 (for JSVALUE32_64, the tag is in regT1).
 174     // The return address is on the stack, or in the link register. We will hence
 175     // jump to the callee, or save the return address to the call frame while we
 176     // make a C++ function call to the appropriate JIT operation.
 177 
 178     CCallHelpers jit;
 179 
 180     CCallHelpers::JumpList slowCase;
 181 
 182     // This is a slow path execution, and regT2 contains the CallLinkInfo. Count the
 183     // slow path execution for the profiler.
 184     jit.add32(
 185         CCallHelpers::TrustedImm32(1),
 186         CCallHelpers::Address(GPRInfo::regT2, CallLinkInfo::offsetOfSlowPathCount()));
 187 
 188     // FIXME: we should have a story for eliminating these checks. In many cases,
 189     // the DFG knows that the value is definitely a cell, or definitely a function.
 190 
 191 #if USE(JSVALUE64)
 192     if (callLinkInfo.isTailCall()) {
<span class="line-modified"> 193         // Tail calls could have clobbered the GPRInfo::notCellMaskRegister because they</span>
 194         // restore callee saved registers before getthing here. So, let&#39;s materialize
<span class="line-modified"> 195         // the NotCellMask in a temp register and use the temp instead.</span>
 196         slowCase.append(jit.branchIfNotCell(GPRInfo::regT0, DoNotHaveTagRegisters));
 197     } else
 198         slowCase.append(jit.branchIfNotCell(GPRInfo::regT0));
 199 #else
 200     slowCase.append(jit.branchIfNotCell(GPRInfo::regT1));
 201 #endif
 202     auto notJSFunction = jit.branchIfNotFunction(GPRInfo::regT0);
 203 
 204     // Now we know we have a JSFunction.
 205 
<span class="line-modified"> 206     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, JSFunction::offsetOfExecutableOrRareData()), GPRInfo::regT4);</span>
<span class="line-modified"> 207     auto hasExecutable = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT4, CCallHelpers::TrustedImm32(JSFunction::rareDataTag));</span>
<span class="line-modified"> 208     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT4, FunctionRareData::offsetOfExecutable() - JSFunction::rareDataTag), GPRInfo::regT4);</span>
<span class="line-added"> 209     hasExecutable.link(&amp;jit);</span>
 210     jit.loadPtr(
 211         CCallHelpers::Address(
 212             GPRInfo::regT4, ExecutableBase::offsetOfJITCodeWithArityCheckFor(
 213                 callLinkInfo.specializationKind())),
 214         GPRInfo::regT4);
 215     slowCase.append(jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT4));
 216 
 217     // Now we know that we have a CodeBlock, and we&#39;re committed to making a fast
 218     // call.
 219 
 220     // Make a tail call. This will return back to JIT code.
 221     JSInterfaceJIT::Label callCode(jit.label());
 222     emitPointerValidation(jit, GPRInfo::regT4, JSEntryPtrTag);
 223     if (callLinkInfo.isTailCall()) {
 224         jit.preserveReturnAddressAfterCall(GPRInfo::regT0);
 225         jit.prepareForTailCallSlow(GPRInfo::regT4);
 226     }
 227     jit.farJump(GPRInfo::regT4, JSEntryPtrTag);
 228 
 229     notJSFunction.link(&amp;jit);
</pre>
<hr />
<pre>
 247 enum ThunkEntryType { EnterViaCall, EnterViaJumpWithSavedTags, EnterViaJumpWithoutSavedTags };
 248 enum class ThunkFunctionType { JSFunction, InternalFunction };
 249 
 250 static MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeForGenerator(VM&amp; vm, ThunkFunctionType thunkFunctionType, CodeSpecializationKind kind, ThunkEntryType entryType = EnterViaCall)
 251 {
 252     // FIXME: This should be able to log ShadowChicken prologue packets.
 253     // https://bugs.webkit.org/show_bug.cgi?id=155689
 254 
 255     int executableOffsetToFunction = NativeExecutable::offsetOfNativeFunctionFor(kind);
 256 
 257     JSInterfaceJIT jit(&amp;vm);
 258 
 259     switch (entryType) {
 260     case EnterViaCall:
 261         jit.emitFunctionPrologue();
 262         break;
 263     case EnterViaJumpWithSavedTags:
 264 #if USE(JSVALUE64)
 265         // We&#39;re coming from a specialized thunk that has saved the prior tag registers&#39; contents.
 266         // Restore them now.
<span class="line-modified"> 267         jit.popPair(JSInterfaceJIT::numberTagRegister, JSInterfaceJIT::notCellMaskRegister);</span>
 268 #endif
 269         break;
 270     case EnterViaJumpWithoutSavedTags:
 271         jit.move(JSInterfaceJIT::framePointerRegister, JSInterfaceJIT::stackPointerRegister);
 272         break;
 273     }
 274 
 275     jit.emitPutToCallFrameHeader(0, CallFrameSlot::codeBlock);
<span class="line-modified"> 276     jit.storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);</span>




































 277 
<span class="line-added"> 278     // Host function signature: f(JSGlobalObject*, CallFrame*);</span>
<span class="line-added"> 279 #if CPU(X86_64) &amp;&amp; OS(WINDOWS)</span>
 280     // Leave space for the callee parameter home addresses.
 281     // At this point the stack is aligned to 16 bytes, but if this changes at some point, we need to emit code to align it.
<span class="line-modified"> 282     jit.subPtr(CCallHelpers::TrustedImm32(4 * sizeof(int64_t)), CCallHelpers::stackPointerRegister);</span>
<span class="line-modified"> 283 #elif CPU(MIPS)</span>




























 284     // Allocate stack space for (unused) 16 bytes (8-byte aligned) for 4 arguments.
<span class="line-modified"> 285     jit.subPtr(CCallHelpers::TrustedImm32(16), CCallHelpers::stackPointerRegister);</span>
 286 #endif
 287 
<span class="line-modified"> 288     jit.move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);</span>
<span class="line-modified"> 289     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, GPRInfo::argumentGPR2);</span>

 290 

 291     if (thunkFunctionType == ThunkFunctionType::JSFunction) {
<span class="line-modified"> 292         jit.loadPtr(CCallHelpers::Address(GPRInfo::argumentGPR2, JSFunction::offsetOfScopeChain()), GPRInfo::argumentGPR0);</span>
<span class="line-modified"> 293         jit.loadPtr(CCallHelpers::Address(GPRInfo::argumentGPR2, JSFunction::offsetOfExecutableOrRareData()), GPRInfo::argumentGPR2);</span>
<span class="line-modified"> 294         auto hasExecutable = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::argumentGPR2, CCallHelpers::TrustedImm32(JSFunction::rareDataTag));</span>
<span class="line-modified"> 295         jit.loadPtr(CCallHelpers::Address(GPRInfo::argumentGPR2, FunctionRareData::offsetOfExecutable() - JSFunction::rareDataTag), GPRInfo::argumentGPR2);</span>
<span class="line-added"> 296         hasExecutable.link(&amp;jit);</span>
<span class="line-added"> 297         jit.call(CCallHelpers::Address(GPRInfo::argumentGPR2, executableOffsetToFunction), JSEntryPtrTag);</span>
<span class="line-added"> 298     } else {</span>
<span class="line-added"> 299         ASSERT(thunkFunctionType == ThunkFunctionType::InternalFunction);</span>
<span class="line-added"> 300         jit.loadPtr(CCallHelpers::Address(GPRInfo::argumentGPR2, InternalFunction::offsetOfGlobalObject()), GPRInfo::argumentGPR0);</span>
<span class="line-added"> 301         jit.call(CCallHelpers::Address(GPRInfo::argumentGPR2, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);</span>
<span class="line-added"> 302     }</span>
 303 
<span class="line-modified"> 304 #if CPU(X86_64) &amp;&amp; OS(WINDOWS)</span>
<span class="line-modified"> 305     jit.addPtr(CCallHelpers::TrustedImm32(4 * sizeof(int64_t)), CCallHelpers::stackPointerRegister);</span>
<span class="line-modified"> 306 #elif CPU(MIPS)</span>
<span class="line-modified"> 307     jit.addPtr(CCallHelpers::TrustedImm32(16), CCallHelpers::stackPointerRegister);</span>




 308 #endif
 309 
 310     // Check for an exception
 311 #if USE(JSVALUE64)
 312     jit.load64(vm.addressOfException(), JSInterfaceJIT::regT2);
 313     JSInterfaceJIT::Jump exceptionHandler = jit.branchTest64(JSInterfaceJIT::NonZero, JSInterfaceJIT::regT2);
 314 #else
 315     JSInterfaceJIT::Jump exceptionHandler = jit.branch32(
 316         JSInterfaceJIT::NotEqual,
 317         JSInterfaceJIT::AbsoluteAddress(vm.addressOfException()),
 318         JSInterfaceJIT::TrustedImm32(0));
 319 #endif
 320 
 321     jit.emitFunctionEpilogue();
 322     // Return.
 323     jit.ret();
 324 
 325     // Handle an exception
 326     exceptionHandler.link(&amp;jit);
 327 
 328     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
 329     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm.topCallFrame);






 330 #if OS(WINDOWS)
 331     // Allocate space on stack for the 4 parameter registers.
 332     jit.subPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);
<span class="line-added"> 333 #elif CPU(MIPS)</span>
<span class="line-added"> 334     // Allocate stack space for (unused) 16 bytes (8-byte aligned) for 4 arguments.</span>
<span class="line-added"> 335     jit.subPtr(CCallHelpers::TrustedImm32(16), CCallHelpers::stackPointerRegister);</span>
 336 #endif
<span class="line-modified"> 337     jit.move(CCallHelpers::TrustedImmPtr(&amp;vm), JSInterfaceJIT::argumentGPR0);</span>

 338     jit.move(JSInterfaceJIT::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationVMHandleException)), JSInterfaceJIT::regT3);
 339     jit.call(JSInterfaceJIT::regT3, OperationPtrTag);
<span class="line-modified"> 340 #if OS(WINDOWS)</span>


 341     jit.addPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);
<span class="line-added"> 342 #elif CPU(MIPS)</span>
<span class="line-added"> 343     jit.addPtr(CCallHelpers::TrustedImm32(16), CCallHelpers::stackPointerRegister);</span>
 344 #endif
 345 
 346     jit.jumpToExceptionHandler(vm);
 347 
 348     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 349     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;%s %s%s trampoline&quot;, thunkFunctionType == ThunkFunctionType::JSFunction ? &quot;native&quot; : &quot;internal&quot;, entryType == EnterViaJumpWithSavedTags ? &quot;Tail With Saved Tags &quot; : entryType == EnterViaJumpWithoutSavedTags ? &quot;Tail Without Saved Tags &quot; : &quot;&quot;, toCString(kind).data());
 350 }
 351 
 352 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeCallGenerator(VM&amp; vm)
 353 {
 354     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall);
 355 }
 356 
 357 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallGenerator(VM&amp; vm)
 358 {
 359     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall, EnterViaJumpWithSavedTags);
 360 }
 361 
 362 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallWithoutSavedTagsGenerator(VM&amp; vm)
 363 {
</pre>
<hr />
<pre>
 388 #if USE(JSVALUE64)
 389 #if OS(WINDOWS)
 390     const GPRReg extraTemp = JSInterfaceJIT::regT0;
 391 #else
 392     const GPRReg extraTemp = JSInterfaceJIT::regT5;
 393 #endif
 394 #  if CPU(X86_64)
 395     jit.pop(JSInterfaceJIT::regT4);
 396 #  endif
 397     jit.tagReturnAddress();
 398 #if CPU(ARM64E)
 399     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 400     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
 401     jit.untagPtr(extraTemp, GPRInfo::regT3);
 402     PtrTag tempReturnPCTag = static_cast&lt;PtrTag&gt;(random());
 403     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
 404     jit.tagPtr(extraTemp, GPRInfo::regT3);
 405     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 406 #endif
 407     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
<span class="line-modified"> 408     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCountIncludingThis), JSInterfaceJIT::argumentGPR2);</span>
 409     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 410 
 411     // Check to see if we have extra slots we can use
 412     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 413     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 414     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
<span class="line-modified"> 415     jit.move(JSInterfaceJIT::TrustedImm64(JSValue::ValueUndefined), extraTemp);</span>
 416     JSInterfaceJIT::Label fillExtraSlots(jit.label());
 417     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight));
 418     jit.add32(JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2);
 419     jit.branchSub32(JSInterfaceJIT::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR1).linkTo(fillExtraSlots, &amp;jit);
 420     jit.and32(JSInterfaceJIT::TrustedImm32(-stackAlignmentRegisters()), JSInterfaceJIT::argumentGPR0);
 421     JSInterfaceJIT::Jump done = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR0);
 422     noExtraSlot.link(&amp;jit);
 423 
 424     jit.neg64(JSInterfaceJIT::argumentGPR0);
 425 
 426     // Adjust call frame register and stack pointer to account for missing args.
 427     // We need to change the stack pointer first before performing copy/fill loops.
 428     // This stack space below the stack pointer is considered unused by OS. Therefore,
 429     // OS may corrupt this space when constructing a signal stack.
 430     jit.move(JSInterfaceJIT::argumentGPR0, extraTemp);
 431     jit.lshift64(JSInterfaceJIT::TrustedImm32(3), extraTemp);
 432     jit.addPtr(extraTemp, JSInterfaceJIT::callFrameRegister);
 433     jit.untagReturnAddress();
 434     jit.addPtr(extraTemp, JSInterfaceJIT::stackPointerRegister);
 435     jit.tagReturnAddress();
 436 
 437     // Move current frame down argumentGPR0 number of slots
 438     JSInterfaceJIT::Label copyLoop(jit.label());
 439     jit.load64(JSInterfaceJIT::regT3, extraTemp);
 440     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 441     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 442     jit.branchSub32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(copyLoop, &amp;jit);
 443 
 444     // Fill in argumentGPR0 missing arg slots with undefined
 445     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR2);
<span class="line-modified"> 446     jit.move(JSInterfaceJIT::TrustedImm64(JSValue::ValueUndefined), extraTemp);</span>
 447     JSInterfaceJIT::Label fillUndefinedLoop(jit.label());
 448     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 449     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 450     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 451 
 452     done.link(&amp;jit);
 453 
 454 #if CPU(ARM64E)
 455     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 456     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
 457     jit.untagPtr(extraTemp, GPRInfo::regT3);
 458     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
 459     jit.tagPtr(extraTemp, GPRInfo::regT3);
 460     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 461 #endif
 462 
 463 #  if CPU(X86_64)
 464     jit.push(JSInterfaceJIT::regT4);
 465 #  endif
 466     jit.ret();
 467 #else // USE(JSVALUE64) section above, USE(JSVALUE32_64) section below.



 468     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
<span class="line-modified"> 469     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCountIncludingThis), JSInterfaceJIT::argumentGPR2);</span>
 470     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 471 
 472     // Check to see if we have extra slots we can use
 473     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 474     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 475     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
 476     JSInterfaceJIT::Label fillExtraSlots(jit.label());
 477     jit.move(JSInterfaceJIT::TrustedImm32(0), JSInterfaceJIT::regT5);
 478     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight, PayloadOffset));
 479     jit.move(JSInterfaceJIT::TrustedImm32(JSValue::UndefinedTag), JSInterfaceJIT::regT5);
 480     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight, TagOffset));
 481     jit.add32(JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2);
 482     jit.branchSub32(JSInterfaceJIT::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR1).linkTo(fillExtraSlots, &amp;jit);
 483     jit.and32(JSInterfaceJIT::TrustedImm32(-stackAlignmentRegisters()), JSInterfaceJIT::argumentGPR0);
 484     JSInterfaceJIT::Jump done = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR0);
 485     noExtraSlot.link(&amp;jit);
 486 
 487     jit.neg32(JSInterfaceJIT::argumentGPR0);
 488 
 489     // Adjust call frame register and stack pointer to account for missing args.
</pre>
<hr />
<pre>
 502     jit.load32(MacroAssembler::Address(JSInterfaceJIT::regT3, PayloadOffset), JSInterfaceJIT::regT5);
 503     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, PayloadOffset));
 504     jit.load32(MacroAssembler::Address(JSInterfaceJIT::regT3, TagOffset), JSInterfaceJIT::regT5);
 505     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, TagOffset));
 506     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 507     jit.branchSub32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(copyLoop, &amp;jit);
 508 
 509     // Fill in argumentGPR0 missing arg slots with undefined
 510     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR2);
 511     JSInterfaceJIT::Label fillUndefinedLoop(jit.label());
 512     jit.move(JSInterfaceJIT::TrustedImm32(0), JSInterfaceJIT::regT5);
 513     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, PayloadOffset));
 514     jit.move(JSInterfaceJIT::TrustedImm32(JSValue::UndefinedTag), JSInterfaceJIT::regT5);
 515     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, TagOffset));
 516 
 517     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 518     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 519 
 520     done.link(&amp;jit);
 521 



 522     jit.ret();
 523 #endif // End of USE(JSVALUE32_64) section.
 524 
 525     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 526     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;fixup arity&quot;);
 527 }
 528 
 529 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; unreachableGenerator(VM&amp; vm)
 530 {
 531     JSInterfaceJIT jit(&amp;vm);
 532 
 533     jit.breakpoint();
 534 
 535     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 536     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;unreachable thunk&quot;);
 537 }
 538 
 539 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; stringGetByValGenerator(VM&amp; vm)
 540 {
 541     // regT0 is JSString*, and regT1 (64bit) or regT2 (32bit) is int index.
</pre>
<hr />
<pre>
 546     GPRReg scratchGPR = GPRInfo::regT2;
 547 #else
 548     GPRReg stringGPR = GPRInfo::regT0;
 549     GPRReg indexGPR = GPRInfo::regT2;
 550     GPRReg scratchGPR = GPRInfo::regT1;
 551 #endif
 552 
 553     JSInterfaceJIT jit(&amp;vm);
 554     JSInterfaceJIT::JumpList failures;
 555     jit.tagReturnAddress();
 556 
 557     // Load string length to regT2, and start the process of loading the data pointer into regT0
 558     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, JSString::offsetOfValue()), stringGPR);
 559     failures.append(jit.branchIfRopeStringImpl(stringGPR));
 560     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::lengthMemoryOffset()), scratchGPR);
 561 
 562     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 563     failures.append(jit.branch32(JSInterfaceJIT::AboveOrEqual, indexGPR, scratchGPR));
 564 
 565     // Load the character

 566     JSInterfaceJIT::JumpList cont8Bit;
 567     // Load the string flags
 568     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::flagsOffset()), scratchGPR);
 569     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, StringImpl::dataOffset()), stringGPR);
<span class="line-modified"> 570     auto is16Bit = jit.branchTest32(JSInterfaceJIT::Zero, scratchGPR, JSInterfaceJIT::TrustedImm32(StringImpl::flagIs8Bit()));</span>
 571     jit.load8(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesOne, 0), stringGPR);
 572     cont8Bit.append(jit.jump());
 573     is16Bit.link(&amp;jit);
 574     jit.load16(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesTwo, 0), stringGPR);
 575     cont8Bit.link(&amp;jit);
 576 
 577     failures.append(jit.branch32(JSInterfaceJIT::Above, stringGPR, JSInterfaceJIT::TrustedImm32(maxSingleCharacterString)));
 578     jit.move(JSInterfaceJIT::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), indexGPR);
 579     jit.loadPtr(JSInterfaceJIT::BaseIndex(indexGPR, stringGPR, JSInterfaceJIT::ScalePtr, 0), stringGPR);
 580     jit.ret();
 581 
 582     failures.link(&amp;jit);
 583     jit.move(JSInterfaceJIT::TrustedImm32(0), stringGPR);
 584     jit.ret();
 585 
 586     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 587     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;String get_by_val stub&quot;);
 588 }
 589 
 590 static void stringCharLoad(SpecializedThunkJIT&amp; jit)
 591 {
 592     // load string
 593     jit.loadJSStringArgument(SpecializedThunkJIT::ThisArgument, SpecializedThunkJIT::regT0);
 594 
 595     // Load string length to regT2, and start the process of loading the data pointer into regT0
 596     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, JSString::offsetOfValue()), SpecializedThunkJIT::regT0);
 597     jit.appendFailure(jit.branchIfRopeStringImpl(SpecializedThunkJIT::regT0));
 598     jit.load32(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::lengthMemoryOffset()), SpecializedThunkJIT::regT2);
 599 
 600     // load index
 601     jit.loadInt32Argument(0, SpecializedThunkJIT::regT1); // regT1 contains the index
 602 
 603     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 604     jit.appendFailure(jit.branch32(MacroAssembler::AboveOrEqual, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2));
 605 
 606     // Load the character
 607     SpecializedThunkJIT::JumpList is16Bit;
 608     SpecializedThunkJIT::JumpList cont8Bit;
 609     // Load the string flags
<span class="line-modified"> 610     jit.load32(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::flagsOffset()), SpecializedThunkJIT::regT2);</span>
 611     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::dataOffset()), SpecializedThunkJIT::regT0);
 612     is16Bit.append(jit.branchTest32(MacroAssembler::Zero, SpecializedThunkJIT::regT2, MacroAssembler::TrustedImm32(StringImpl::flagIs8Bit())));
 613     jit.load8(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesOne, 0), SpecializedThunkJIT::regT0);
 614     cont8Bit.append(jit.jump());
 615     is16Bit.link(&amp;jit);
 616     jit.load16(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesTwo, 0), SpecializedThunkJIT::regT0);
 617     cont8Bit.link(&amp;jit);
 618 }
 619 
 620 static void charToString(SpecializedThunkJIT&amp; jit, VM&amp; vm, MacroAssembler::RegisterID src, MacroAssembler::RegisterID dst, MacroAssembler::RegisterID scratch)
 621 {
 622     jit.appendFailure(jit.branch32(MacroAssembler::Above, src, MacroAssembler::TrustedImm32(maxSingleCharacterString)));
 623     jit.move(MacroAssembler::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), scratch);
 624     jit.loadPtr(MacroAssembler::BaseIndex(scratch, src, MacroAssembler::ScalePtr, 0), dst);
 625     jit.appendFailure(jit.branchTestPtr(MacroAssembler::Zero, dst));
 626 }
 627 
 628 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charCodeAtThunkGenerator(VM&amp; vm)
 629 {
 630     SpecializedThunkJIT jit(vm, 1);
</pre>
<hr />
<pre>
 635 
 636 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charAtThunkGenerator(VM&amp; vm)
 637 {
 638     SpecializedThunkJIT jit(vm, 1);
 639     stringCharLoad(jit);
 640     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 641     jit.returnJSCell(SpecializedThunkJIT::regT0);
 642     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;charAt&quot;);
 643 }
 644 
 645 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; fromCharCodeThunkGenerator(VM&amp; vm)
 646 {
 647     SpecializedThunkJIT jit(vm, 1);
 648     // load char code
 649     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0);
 650     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 651     jit.returnJSCell(SpecializedThunkJIT::regT0);
 652     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;fromCharCode&quot;);
 653 }
 654 
<span class="line-added"> 655 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; stringPrototypeCodePointAtThunkGenerator(VM&amp; vm)</span>
<span class="line-added"> 656 {</span>
<span class="line-added"> 657     SpecializedThunkJIT jit(vm, 1);</span>
<span class="line-added"> 658 </span>
<span class="line-added"> 659     // load string</span>
<span class="line-added"> 660     jit.loadJSStringArgument(SpecializedThunkJIT::ThisArgument, GPRInfo::regT0);</span>
<span class="line-added"> 661 </span>
<span class="line-added"> 662     // Load string length to regT3, and start the process of loading the data pointer into regT2</span>
<span class="line-added"> 663     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, JSString::offsetOfValue()), GPRInfo::regT0);</span>
<span class="line-added"> 664     jit.appendFailure(jit.branchIfRopeStringImpl(GPRInfo::regT0));</span>
<span class="line-added"> 665     jit.load32(CCallHelpers::Address(GPRInfo::regT0, StringImpl::lengthMemoryOffset()), GPRInfo::regT3);</span>
<span class="line-added"> 666 </span>
<span class="line-added"> 667     // load index</span>
<span class="line-added"> 668     jit.loadInt32Argument(0, GPRInfo::regT1); // regT1 contains the index</span>
<span class="line-added"> 669 </span>
<span class="line-added"> 670     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large</span>
<span class="line-added"> 671     jit.appendFailure(jit.branch32(CCallHelpers::AboveOrEqual, GPRInfo::regT1, GPRInfo::regT3));</span>
<span class="line-added"> 672 </span>
<span class="line-added"> 673     // Load the character</span>
<span class="line-added"> 674     CCallHelpers::JumpList done;</span>
<span class="line-added"> 675     // Load the string flags</span>
<span class="line-added"> 676     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, StringImpl::dataOffset()), GPRInfo::regT2);</span>
<span class="line-added"> 677     auto is16Bit = jit.branchTest32(CCallHelpers::Zero, CCallHelpers::Address(GPRInfo::regT0, StringImpl::flagsOffset()), CCallHelpers::TrustedImm32(StringImpl::flagIs8Bit()));</span>
<span class="line-added"> 678     jit.load8(CCallHelpers::BaseIndex(GPRInfo::regT2, GPRInfo::regT1, CCallHelpers::TimesOne, 0), GPRInfo::regT0);</span>
<span class="line-added"> 679     done.append(jit.jump());</span>
<span class="line-added"> 680 </span>
<span class="line-added"> 681     is16Bit.link(&amp;jit);</span>
<span class="line-added"> 682     jit.load16(CCallHelpers::BaseIndex(GPRInfo::regT2, GPRInfo::regT1, CCallHelpers::TimesTwo, 0), GPRInfo::regT0);</span>
<span class="line-added"> 683     // Original index is int32_t, and here, we ensure that it is positive. If we interpret it as uint32_t, adding 1 never overflows.</span>
<span class="line-added"> 684     jit.add32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);</span>
<span class="line-added"> 685     done.append(jit.branch32(CCallHelpers::AboveOrEqual, GPRInfo::regT1, GPRInfo::regT3));</span>
<span class="line-added"> 686     jit.and32(CCallHelpers::TrustedImm32(0xfffffc00), GPRInfo::regT0, GPRInfo::regT3);</span>
<span class="line-added"> 687     done.append(jit.branch32(CCallHelpers::NotEqual, GPRInfo::regT3, CCallHelpers::TrustedImm32(0xd800)));</span>
<span class="line-added"> 688     jit.load16(CCallHelpers::BaseIndex(GPRInfo::regT2, GPRInfo::regT1, CCallHelpers::TimesTwo, 0), GPRInfo::regT2);</span>
<span class="line-added"> 689     jit.and32(CCallHelpers::TrustedImm32(0xfffffc00), GPRInfo::regT2, GPRInfo::regT3);</span>
<span class="line-added"> 690     done.append(jit.branch32(CCallHelpers::NotEqual, GPRInfo::regT3, CCallHelpers::TrustedImm32(0xdc00)));</span>
<span class="line-added"> 691     jit.lshift32(CCallHelpers::TrustedImm32(10), GPRInfo::regT0);</span>
<span class="line-added"> 692     jit.getEffectiveAddress(CCallHelpers::BaseIndex(GPRInfo::regT0, GPRInfo::regT2, CCallHelpers::TimesOne, -U16_SURROGATE_OFFSET), GPRInfo::regT0);</span>
<span class="line-added"> 693     done.link(&amp;jit);</span>
<span class="line-added"> 694 </span>
<span class="line-added"> 695     jit.returnInt32(GPRInfo::regT0);</span>
<span class="line-added"> 696     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;codePointAt&quot;);</span>
<span class="line-added"> 697 }</span>
<span class="line-added"> 698 </span>
 699 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; clz32ThunkGenerator(VM&amp; vm)
 700 {
 701     SpecializedThunkJIT jit(vm, 1);
 702     MacroAssembler::Jump nonIntArgJump;
 703     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntArgJump);
 704 
 705     SpecializedThunkJIT::Label convertedArgumentReentry(&amp;jit);
 706     jit.countLeadingZeros32(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 707     jit.returnInt32(SpecializedThunkJIT::regT1);
 708 
 709     if (jit.supportsFloatingPointTruncate()) {
 710         nonIntArgJump.link(&amp;jit);
 711         jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 712         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(convertedArgumentReentry, &amp;jit);
 713         jit.appendFailure(jit.jump());
 714     } else
 715         jit.appendFailure(nonIntArgJump);
 716 
 717     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;clz32&quot;);
 718 }
</pre>
<hr />
<pre>
 779 #elif CPU(X86) &amp;&amp; COMPILER(GCC_COMPATIBLE) &amp;&amp; (OS(DARWIN) || OS(LINUX))
 780 #define defineUnaryDoubleOpWrapper(function) \
 781     asm( \
 782         &quot;.text\n&quot; \
 783         &quot;.globl &quot; SYMBOL_STRING(function##Thunk) &quot;\n&quot; \
 784         HIDE_SYMBOL(function##Thunk) &quot;\n&quot; \
 785         SYMBOL_STRING(function##Thunk) &quot;:&quot; &quot;\n&quot; \
 786         &quot;subl $20, %esp\n&quot; \
 787         &quot;movsd %xmm0, (%esp) \n&quot; \
 788         &quot;call &quot; GLOBAL_REFERENCE(function) &quot;\n&quot; \
 789         &quot;fstpl (%esp) \n&quot; \
 790         &quot;movsd (%esp), %xmm0 \n&quot; \
 791         &quot;addl $20, %esp\n&quot; \
 792         &quot;ret\n&quot; \
 793     );\
 794     extern &quot;C&quot; { \
 795         MathThunkCallingConvention function##Thunk(MathThunkCallingConvention); \
 796     } \
 797     static MathThunk UnaryDoubleOpWrapper(function) = &amp;function##Thunk;
 798 
<span class="line-modified"> 799 #elif CPU(ARM_THUMB2) &amp;&amp; COMPILER(GCC_COMPATIBLE) &amp;&amp; OS(DARWIN)</span>
 800 
 801 #define defineUnaryDoubleOpWrapper(function) \
 802     asm( \
 803         &quot;.text\n&quot; \
 804         &quot;.align 2\n&quot; \
 805         &quot;.globl &quot; SYMBOL_STRING(function##Thunk) &quot;\n&quot; \
 806         HIDE_SYMBOL(function##Thunk) &quot;\n&quot; \
 807         &quot;.thumb\n&quot; \
 808         &quot;.thumb_func &quot; THUMB_FUNC_PARAM(function##Thunk) &quot;\n&quot; \
 809         SYMBOL_STRING(function##Thunk) &quot;:&quot; &quot;\n&quot; \
 810         &quot;push {lr}\n&quot; \
 811         &quot;vmov r0, r1, d0\n&quot; \
 812         &quot;blx &quot; GLOBAL_REFERENCE(function) &quot;\n&quot; \
 813         &quot;vmov d0, r0, r1\n&quot; \
 814         &quot;pop {lr}\n&quot; \
 815         &quot;bx lr\n&quot; \
 816     ); \
 817     extern &quot;C&quot; { \
 818         MathThunkCallingConvention function##Thunk(MathThunkCallingConvention); \
 819     } \
</pre>
<hr />
<pre>
 858         __asm movsd xmm0, mmword ptr [esp] \
 859         __asm add esp, 20 \
 860         __asm ret \
 861         } \
 862     } \
 863     static MathThunk UnaryDoubleOpWrapper(function) = &amp;function##Thunk;
 864 
 865 #else
 866 
 867 #define defineUnaryDoubleOpWrapper(function) \
 868     static MathThunk UnaryDoubleOpWrapper(function) = 0
 869 #endif
 870 
 871 defineUnaryDoubleOpWrapper(jsRound);
 872 defineUnaryDoubleOpWrapper(exp);
 873 defineUnaryDoubleOpWrapper(log);
 874 defineUnaryDoubleOpWrapper(floor);
 875 defineUnaryDoubleOpWrapper(ceil);
 876 defineUnaryDoubleOpWrapper(trunc);
 877 


 878 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; floorThunkGenerator(VM&amp; vm)
 879 {
 880     SpecializedThunkJIT jit(vm, 1);
 881     MacroAssembler::Jump nonIntJump;
 882     if (!UnaryDoubleOpWrapper(floor) || !jit.supportsFloatingPoint())
 883         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
 884     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 885     jit.returnInt32(SpecializedThunkJIT::regT0);
 886     nonIntJump.link(&amp;jit);
 887     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 888 
 889     if (jit.supportsFloatingPointRounding()) {
 890         SpecializedThunkJIT::JumpList doubleResult;
 891         jit.floorDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 892         jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 893         jit.returnInt32(SpecializedThunkJIT::regT0);
 894         doubleResult.link(&amp;jit);
 895         jit.returnDouble(SpecializedThunkJIT::fpRegT0);
 896         return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;floor&quot;);
 897     }
</pre>
<hr />
<pre>
 957         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(trunc));
 958 
 959     SpecializedThunkJIT::JumpList doubleResult;
 960     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 961     jit.returnInt32(SpecializedThunkJIT::regT0);
 962     doubleResult.link(&amp;jit);
 963     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
 964     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;trunc&quot;);
 965 }
 966 
 967 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; roundThunkGenerator(VM&amp; vm)
 968 {
 969     SpecializedThunkJIT jit(vm, 1);
 970     if (!UnaryDoubleOpWrapper(jsRound) || !jit.supportsFloatingPoint())
 971         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
 972     MacroAssembler::Jump nonIntJump;
 973     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 974     jit.returnInt32(SpecializedThunkJIT::regT0);
 975     nonIntJump.link(&amp;jit);
 976     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);

 977     SpecializedThunkJIT::JumpList doubleResult;
<span class="line-modified"> 978     if (jit.supportsFloatingPointRounding()) {</span>
 979         jit.moveZeroToDouble(SpecializedThunkJIT::fpRegT1);
 980         doubleResult.append(jit.branchDouble(MacroAssembler::DoubleEqual, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
<span class="line-modified"> 981 </span>
<span class="line-modified"> 982         jit.ceilDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1);</span>
<span class="line-modified"> 983         static constexpr double halfConstant = -0.5;</span>
<span class="line-modified"> 984         jit.loadDouble(MacroAssembler::TrustedImmPtr(&amp;halfConstant), SpecializedThunkJIT::fpRegT2);</span>
<span class="line-modified"> 985         jit.addDouble(SpecializedThunkJIT::fpRegT1, SpecializedThunkJIT::fpRegT2);</span>
<span class="line-modified"> 986         MacroAssembler::Jump shouldRoundDown = jit.branchDouble(MacroAssembler::DoubleGreaterThan, SpecializedThunkJIT::fpRegT2, SpecializedThunkJIT::fpRegT0);</span>
<span class="line-modified"> 987 </span>
<span class="line-modified"> 988         jit.moveDouble(SpecializedThunkJIT::fpRegT1, SpecializedThunkJIT::fpRegT0);</span>
<span class="line-modified"> 989         MacroAssembler::Jump continuation = jit.jump();</span>
<span class="line-modified"> 990 </span>
<span class="line-added"> 991         shouldRoundDown.link(&amp;jit);</span>
<span class="line-added"> 992         static constexpr double oneConstant = 1.0;</span>
<span class="line-added"> 993         jit.loadDouble(MacroAssembler::TrustedImmPtr(&amp;oneConstant), SpecializedThunkJIT::fpRegT2);</span>
<span class="line-added"> 994         jit.subDouble(SpecializedThunkJIT::fpRegT1, SpecializedThunkJIT::fpRegT2, SpecializedThunkJIT::fpRegT0);</span>
<span class="line-added"> 995 </span>
<span class="line-added"> 996         continuation.link(&amp;jit);</span>
<span class="line-added"> 997     } else</span>
<span class="line-added"> 998         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(jsRound));</span>
 999     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);


1000     jit.returnInt32(SpecializedThunkJIT::regT0);
1001     doubleResult.link(&amp;jit);
1002     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1003     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;round&quot;);
1004 }
1005 
1006 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; expThunkGenerator(VM&amp; vm)
1007 {
1008     if (!UnaryDoubleOpWrapper(exp))
1009         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1010     SpecializedThunkJIT jit(vm, 1);
1011     if (!jit.supportsFloatingPoint())
1012         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1013     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1014     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(exp));
1015     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1016     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;exp&quot;);
1017 }
1018 
1019 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; logThunkGenerator(VM&amp; vm)
1020 {
1021     if (!UnaryDoubleOpWrapper(log))
1022         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1023     SpecializedThunkJIT jit(vm, 1);
1024     if (!jit.supportsFloatingPoint())
1025         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1026     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1027     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(log));
1028     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1029     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;log&quot;);
1030 }
1031 
1032 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; absThunkGenerator(VM&amp; vm)
1033 {
1034     SpecializedThunkJIT jit(vm, 1);
1035     if (!jit.supportsFloatingPointAbs())
1036         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1037 
1038 #if USE(JSVALUE64)
<span class="line-modified">1039     VirtualRegister virtualRegister = CallFrameSlot::firstArgument;</span>
<span class="line-modified">1040     jit.load64(AssemblyHelpers::addressFor(virtualRegister), GPRInfo::regT0);</span>
1041     auto notInteger = jit.branchIfNotInt32(GPRInfo::regT0);
1042 
1043     // Abs Int32.
1044     jit.rshift32(GPRInfo::regT0, MacroAssembler::TrustedImm32(31), GPRInfo::regT1);
1045     jit.add32(GPRInfo::regT1, GPRInfo::regT0);
1046     jit.xor32(GPRInfo::regT1, GPRInfo::regT0);
1047 
1048     // IntMin cannot be inverted.
1049     MacroAssembler::Jump integerIsIntMin = jit.branchTest32(MacroAssembler::Signed, GPRInfo::regT0);
1050 
1051     // Box and finish.
<span class="line-modified">1052     jit.or64(GPRInfo::numberTagRegister, GPRInfo::regT0);</span>
1053     MacroAssembler::Jump doneWithIntegers = jit.jump();
1054 
1055     // Handle Doubles.
1056     notInteger.link(&amp;jit);
1057     jit.appendFailure(jit.branchIfNotNumber(GPRInfo::regT0));
1058     jit.unboxDoubleWithoutAssertions(GPRInfo::regT0, GPRInfo::regT0, FPRInfo::fpRegT0);
1059     MacroAssembler::Label absFPR0Label = jit.label();
1060     jit.absDouble(FPRInfo::fpRegT0, FPRInfo::fpRegT1);
1061     jit.boxDouble(FPRInfo::fpRegT1, GPRInfo::regT0);
1062 
1063     // Tail.
1064     doneWithIntegers.link(&amp;jit);
1065     jit.returnJSValue(GPRInfo::regT0);
1066 
1067     // We know the value of regT0 is IntMin. We could load that value from memory but
1068     // it is simpler to just convert it.
1069     integerIsIntMin.link(&amp;jit);
1070     jit.convertInt32ToDouble(GPRInfo::regT0, FPRInfo::fpRegT0);
1071     jit.jump().linkTo(absFPR0Label, &amp;jit);
1072 #else
</pre>
<hr />
<pre>
1116 
1117     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;imul&quot;);
1118 }
1119 
1120 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; randomThunkGenerator(VM&amp; vm)
1121 {
1122     SpecializedThunkJIT jit(vm, 0);
1123     if (!jit.supportsFloatingPoint())
1124         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1125 
1126 #if USE(JSVALUE64)
1127     jit.emitRandomThunk(vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2, SpecializedThunkJIT::regT3, SpecializedThunkJIT::fpRegT0);
1128     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1129 
1130     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;random&quot;);
1131 #else
1132     return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));
1133 #endif
1134 }
1135 
<span class="line-modified">1136 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; boundFunctionCallGenerator(VM&amp; vm)</span>
1137 {
1138     CCallHelpers jit;
1139 
1140     jit.emitFunctionPrologue();
1141 
1142     // Set up our call frame.
1143     jit.storePtr(CCallHelpers::TrustedImmPtr(nullptr), CCallHelpers::addressFor(CallFrameSlot::codeBlock));
<span class="line-modified">1144     jit.store32(CCallHelpers::TrustedImm32(0), CCallHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));</span>
1145 
1146     unsigned extraStackNeeded = 0;
1147     if (unsigned stackMisalignment = sizeof(CallerFrameAndPC) % stackAlignmentBytes())
1148         extraStackNeeded = stackAlignmentBytes() - stackMisalignment;
1149 
1150     // We need to forward all of the arguments that we were passed. We aren&#39;t allowed to do a tail
1151     // call here as far as I can tell. At least not so long as the generic path doesn&#39;t do a tail
1152     // call, since that would be way too weird.
1153 
1154     // The formula for the number of stack bytes needed given some number of parameters (including
1155     // this) is:
1156     //
1157     //     stackAlign((numParams + CallFrameHeaderSize) * sizeof(Register) - sizeof(CallerFrameAndPC))
1158     //
1159     // Probably we want to write this as:
1160     //
1161     //     stackAlign((numParams + (CallFrameHeaderSize - CallerFrameAndPCSize)) * sizeof(Register))
1162     //
1163     // That&#39;s really all there is to this. We have all the registers we need to do it.
1164 
<span class="line-modified">1165     jit.loadCell(CCallHelpers::addressFor(CallFrameSlot::callee), GPRInfo::regT0);</span>
<span class="line-added">1166     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, JSBoundFunction::offsetOfBoundArgs()), GPRInfo::regT2);</span>
<span class="line-added">1167     jit.load32(CCallHelpers::payloadFor(CallFrameSlot::argumentCountIncludingThis), GPRInfo::regT1);</span>
<span class="line-added">1168     jit.move(GPRInfo::regT1, GPRInfo::regT3);</span>
<span class="line-added">1169     auto noArgs = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT2);</span>
<span class="line-added">1170     jit.load32(CCallHelpers::Address(GPRInfo::regT2, JSImmutableButterfly::offsetOfPublicLength()), GPRInfo::regT2);</span>
<span class="line-added">1171     jit.add32(GPRInfo::regT2, GPRInfo::regT1);</span>
<span class="line-added">1172     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);</span>
<span class="line-added">1173     noArgs.link(&amp;jit);</span>
1174     jit.add32(CCallHelpers::TrustedImm32(CallFrame::headerSizeInRegisters - CallerFrameAndPC::sizeInRegisters), GPRInfo::regT1, GPRInfo::regT2);
1175     jit.lshift32(CCallHelpers::TrustedImm32(3), GPRInfo::regT2);
1176     jit.add32(CCallHelpers::TrustedImm32(stackAlignmentBytes() - 1), GPRInfo::regT2);
1177     jit.and32(CCallHelpers::TrustedImm32(-stackAlignmentBytes()), GPRInfo::regT2);
1178 
1179     if (extraStackNeeded)
1180         jit.add32(CCallHelpers::TrustedImm32(extraStackNeeded), GPRInfo::regT2);
1181 
<span class="line-modified">1182     // At this point regT1 has the actual argument count, regT2 has the amount of stack we will need, and regT3 has the passed argument count.</span>
1183     // Check to see if we have enough stack space.
1184 
1185     jit.negPtr(GPRInfo::regT2);
1186     jit.addPtr(CCallHelpers::stackPointerRegister, GPRInfo::regT2);
1187     CCallHelpers::Jump haveStackSpace = jit.branchPtr(CCallHelpers::BelowOrEqual, CCallHelpers::AbsoluteAddress(vm.addressOfSoftStackLimit()), GPRInfo::regT2);
1188 
1189     // Throw Stack Overflow exception
1190     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
<span class="line-modified">1191     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, JSBoundFunction::offsetOfScopeChain()), GPRInfo::regT3);</span>
<span class="line-modified">1192     jit.setupArguments&lt;decltype(operationThrowStackOverflowErrorFromThunk)&gt;(GPRInfo::regT3);</span>
<span class="line-added">1193     jit.prepareCallOperation(vm);</span>
<span class="line-added">1194     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationThrowStackOverflowErrorFromThunk)), GPRInfo::nonArgGPR0);</span>
1195     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
1196     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
1197     jit.jumpToExceptionHandler(vm);
1198 
1199     haveStackSpace.link(&amp;jit);
1200     jit.move(GPRInfo::regT2, CCallHelpers::stackPointerRegister);
1201 
1202     // Do basic callee frame setup, including &#39;this&#39;.
1203 
<span class="line-modified">1204     jit.store32(GPRInfo::regT1, CCallHelpers::calleeFramePayloadSlot(CallFrameSlot::argumentCountIncludingThis));</span>
1205 
<span class="line-modified">1206     JSValueRegs valueRegs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT4, GPRInfo::regT2);</span>
<span class="line-modified">1207     jit.loadValue(CCallHelpers::Address(GPRInfo::regT0, JSBoundFunction::offsetOfBoundThis()), valueRegs);</span>


1208     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(0));
1209 



1210     // OK, now we can start copying. This is a simple matter of copying parameters from the caller&#39;s
<span class="line-modified">1211     // frame to the callee&#39;s frame. Note that we know that regT3 (the argument count) must be at</span>
1212     // least 1.
<span class="line-added">1213     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT3);</span>
1214     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);
<span class="line-modified">1215     CCallHelpers::Jump done = jit.branchTest32(CCallHelpers::Zero, GPRInfo::regT3);</span>
1216 
1217     CCallHelpers::Label loop = jit.label();
<span class="line-added">1218     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT3);</span>
1219     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);
<span class="line-modified">1220     jit.loadValue(CCallHelpers::addressFor(virtualRegisterForArgumentIncludingThis(1)).indexedBy(GPRInfo::regT3, CCallHelpers::TimesEight), valueRegs);</span>
1221     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(1).indexedBy(GPRInfo::regT1, CCallHelpers::TimesEight));
<span class="line-modified">1222     jit.branchTest32(CCallHelpers::NonZero, GPRInfo::regT3).linkTo(loop, &amp;jit);</span>
1223 
1224     done.link(&amp;jit);
<span class="line-added">1225     auto noArgs2 = jit.branchTest32(CCallHelpers::Zero, GPRInfo::regT1);</span>
<span class="line-added">1226 </span>
<span class="line-added">1227     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, JSBoundFunction::offsetOfBoundArgs()), GPRInfo::regT3);</span>
<span class="line-added">1228 </span>
<span class="line-added">1229     CCallHelpers::Label loopBound = jit.label();</span>
<span class="line-added">1230     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);</span>
<span class="line-added">1231     jit.loadValue(CCallHelpers::BaseIndex(GPRInfo::regT3, GPRInfo::regT1, CCallHelpers::TimesEight, JSImmutableButterfly::offsetOfData() + sizeof(WriteBarrier&lt;Unknown&gt;)), valueRegs);</span>
<span class="line-added">1232     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(1).indexedBy(GPRInfo::regT1, CCallHelpers::TimesEight));</span>
<span class="line-added">1233     jit.branchTest32(CCallHelpers::NonZero, GPRInfo::regT1).linkTo(loopBound, &amp;jit);</span>
<span class="line-added">1234 </span>
<span class="line-added">1235     noArgs2.link(&amp;jit);</span>
<span class="line-added">1236 </span>
<span class="line-added">1237     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, JSBoundFunction::offsetOfTargetFunction()), GPRInfo::regT2);</span>
<span class="line-added">1238     jit.storeCell(GPRInfo::regT2, CCallHelpers::calleeFrameSlot(CallFrameSlot::callee));</span>
<span class="line-added">1239 </span>
<span class="line-added">1240     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT2, JSFunction::offsetOfExecutableOrRareData()), GPRInfo::regT0);</span>
<span class="line-added">1241     auto hasExecutable = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT0, CCallHelpers::TrustedImm32(JSFunction::rareDataTag));</span>
<span class="line-added">1242     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT0, FunctionRareData::offsetOfExecutable() - JSFunction::rareDataTag), GPRInfo::regT0);</span>
<span class="line-added">1243     hasExecutable.link(&amp;jit);</span>
1244 



1245     jit.loadPtr(
1246         CCallHelpers::Address(
1247             GPRInfo::regT0, ExecutableBase::offsetOfJITCodeWithArityCheckFor(CodeForCall)),
1248         GPRInfo::regT0);
1249     CCallHelpers::Jump noCode = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT0);
1250 
1251     emitPointerValidation(jit, GPRInfo::regT0, JSEntryPtrTag);
1252     jit.call(GPRInfo::regT0, JSEntryPtrTag);
1253 
1254     jit.emitFunctionEpilogue();
1255     jit.ret();
1256 
1257     LinkBuffer linkBuffer(jit, GLOBAL_THUNK_ID);
1258     linkBuffer.link(noCode, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.jitStubs-&gt;ctiNativeTailCallWithoutSavedTags(vm)));
1259     return FINALIZE_CODE(
1260         linkBuffer, JITThunkPtrTag, &quot;Specialized thunk for bound function calls with no arguments&quot;);
1261 }
1262 
1263 } // namespace JSC
1264 
</pre>
</td>
</tr>
</table>
<center><a href="TempRegisterSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ThunkGenerators.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>