diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp
@@ -25,10 +25,11 @@
 #include "FunctionCodeBlock.h"
 #include "IncrementalSweeper.h"
 #include "JSObject.h"
 #include "JSCInlines.h"
 #include "MarkedBlockInlines.h"
+#include "MarkedSpaceInlines.h"
 #include <wtf/ListDump.h>
 
 namespace JSC {
 
 std::array<size_t, MarkedSpace::numSizeClasses> MarkedSpace::s_sizeClassForSizeStep;
@@ -42,19 +43,18 @@
     std::call_once(
         once,
         [] {
             result = new Vector<size_t>();
 
-            if (Options::dumpSizeClasses()) {
+            if (UNLIKELY(Options::dumpSizeClasses())) {
                 dataLog("Block size: ", MarkedBlock::blockSize, "\n");
                 dataLog("Footer size: ", sizeof(MarkedBlock::Footer), "\n");
             }
 
             auto add = [&] (size_t sizeClass) {
                 sizeClass = WTF::roundUpToMultipleOf<MarkedBlock::atomSize>(sizeClass);
-                if (Options::dumpSizeClasses())
-                    dataLog("Adding JSC MarkedSpace size class: ", sizeClass, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "Adding JSC MarkedSpace size class: ", sizeClass);
                 // Perform some validation as we go.
                 RELEASE_ASSERT(!(sizeClass % MarkedSpace::sizeStep));
                 if (result->isEmpty())
                     RELEASE_ASSERT(sizeClass == MarkedSpace::sizeStep);
                 result->append(sizeClass);
@@ -70,89 +70,75 @@
 
             // We want to make sure that the remaining size classes minimize internal fragmentation (i.e.
             // the wasted space at the tail end of a MarkedBlock) while proceeding roughly in an exponential
             // way starting at just above the precise size classes to four cells per block.
 
-            if (Options::dumpSizeClasses())
-                dataLog("    Marked block payload size: ", static_cast<size_t>(MarkedSpace::blockPayload), "\n");
+            dataLogLnIf(Options::dumpSizeClasses(), "    Marked block payload size: ", static_cast<size_t>(MarkedSpace::blockPayload));
 
             for (unsigned i = 0; ; ++i) {
                 double approximateSize = MarkedSpace::preciseCutoff * pow(Options::sizeClassProgression(), i);
-
-                if (Options::dumpSizeClasses())
-                    dataLog("    Next size class as a double: ", approximateSize, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "    Next size class as a double: ", approximateSize);
 
                 size_t approximateSizeInBytes = static_cast<size_t>(approximateSize);
-
-                if (Options::dumpSizeClasses())
-                    dataLog("    Next size class as bytes: ", approximateSizeInBytes, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "    Next size class as bytes: ", approximateSizeInBytes);
 
                 // Make sure that the computer did the math correctly.
                 RELEASE_ASSERT(approximateSizeInBytes >= MarkedSpace::preciseCutoff);
 
                 if (approximateSizeInBytes > MarkedSpace::largeCutoff)
                     break;
 
                 size_t sizeClass =
                     WTF::roundUpToMultipleOf<MarkedSpace::sizeStep>(approximateSizeInBytes);
-
-                if (Options::dumpSizeClasses())
-                    dataLog("    Size class: ", sizeClass, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "    Size class: ", sizeClass);
 
                 // Optimize the size class so that there isn't any slop at the end of the block's
                 // payload.
                 unsigned cellsPerBlock = MarkedSpace::blockPayload / sizeClass;
                 size_t possiblyBetterSizeClass = (MarkedSpace::blockPayload / cellsPerBlock) & ~(MarkedSpace::sizeStep - 1);
-
-                if (Options::dumpSizeClasses())
-                    dataLog("    Possibly better size class: ", possiblyBetterSizeClass, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "    Possibly better size class: ", possiblyBetterSizeClass);
 
                 // The size class we just came up with is better than the other one if it reduces
                 // total wastage assuming we only allocate cells of that size.
                 size_t originalWastage = MarkedSpace::blockPayload - cellsPerBlock * sizeClass;
                 size_t newWastage = (possiblyBetterSizeClass - sizeClass) * cellsPerBlock;
-
-                if (Options::dumpSizeClasses())
-                    dataLog("    Original wastage: ", originalWastage, ", new wastage: ", newWastage, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "    Original wastage: ", originalWastage, ", new wastage: ", newWastage);
 
                 size_t betterSizeClass;
                 if (newWastage > originalWastage)
                     betterSizeClass = sizeClass;
                 else
                     betterSizeClass = possiblyBetterSizeClass;
 
-                if (Options::dumpSizeClasses())
-                    dataLog("    Choosing size class: ", betterSizeClass, "\n");
+                dataLogLnIf(Options::dumpSizeClasses(), "    Choosing size class: ", betterSizeClass);
 
                 if (betterSizeClass == result->last()) {
                     // Defense for when expStep is small.
                     continue;
                 }
 
                 // This is usually how we get out of the loop.
                 if (betterSizeClass > MarkedSpace::largeCutoff
-                    || betterSizeClass > Options::largeAllocationCutoff())
+                    || betterSizeClass > Options::preciseAllocationCutoff())
                     break;
 
                 add(betterSizeClass);
             }
 
             // Manually inject size classes for objects we know will be allocated in high volume.
             // FIXME: All of these things should have IsoSubspaces.
             // https://bugs.webkit.org/show_bug.cgi?id=179876
-            add(sizeof(UnlinkedFunctionCodeBlock));
-            add(sizeof(JSString));
+            add(256);
 
             {
                 // Sort and deduplicate.
                 std::sort(result->begin(), result->end());
                 auto it = std::unique(result->begin(), result->end());
                 result->shrinkCapacity(it - result->begin());
             }
 
-            if (Options::dumpSizeClasses())
-                dataLog("JSC Heap MarkedSpace size class dump: ", listDump(*result), "\n");
+            dataLogLnIf(Options::dumpSizeClasses(), "JSC Heap MarkedSpace size class dump: ", listDump(*result));
 
             // We have an optimiation in MarkedSpace::optimalSizeFor() that assumes things about
             // the size class table. This checks our results against that function's assumptions.
             for (size_t size = MarkedSpace::sizeStep, i = 0; size <= MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep, i++)
                 RELEASE_ASSERT(result->at(i) == size);
@@ -194,12 +180,12 @@
                 });
         });
 }
 
 MarkedSpace::MarkedSpace(Heap* heap)
-    : m_heap(heap)
 {
+    ASSERT_UNUSED(heap, heap == &this->heap());
     initializeSizeClassForStepSize();
 }
 
 MarkedSpace::~MarkedSpace()
 {
@@ -210,79 +196,98 @@
 {
     forEachBlock(
         [&] (MarkedBlock::Handle* block) {
             freeBlock(block);
         });
-    for (LargeAllocation* allocation : m_largeAllocations)
+    for (PreciseAllocation* allocation : m_preciseAllocations)
         allocation->destroy();
+    forEachSubspace([&](Subspace& subspace) {
+        if (subspace.isIsoSubspace())
+            static_cast<IsoSubspace&>(subspace).destroyLowerTierFreeList();
+        return IterationStatus::Continue;
+    });
 }
 
 void MarkedSpace::lastChanceToFinalize()
 {
     forEachDirectory(
         [&] (BlockDirectory& directory) -> IterationStatus {
             directory.lastChanceToFinalize();
             return IterationStatus::Continue;
         });
-    for (LargeAllocation* allocation : m_largeAllocations)
+    for (PreciseAllocation* allocation : m_preciseAllocations)
         allocation->lastChanceToFinalize();
+    // We do not call lastChanceToFinalize for lower-tier swept cells since we need nothing to do.
 }
 
-void MarkedSpace::sweep()
+void MarkedSpace::sweepBlocks()
 {
-    m_heap->sweeper().stopSweeping();
+    heap().sweeper().stopSweeping();
     forEachDirectory(
         [&] (BlockDirectory& directory) -> IterationStatus {
             directory.sweep();
             return IterationStatus::Continue;
         });
 }
 
-void MarkedSpace::sweepLargeAllocations()
+void MarkedSpace::sweepPreciseAllocations()
 {
-    RELEASE_ASSERT(m_largeAllocationsNurseryOffset == m_largeAllocations.size());
-    unsigned srcIndex = m_largeAllocationsNurseryOffsetForSweep;
+    RELEASE_ASSERT(m_preciseAllocationsNurseryOffset == m_preciseAllocations.size());
+    unsigned srcIndex = m_preciseAllocationsNurseryOffsetForSweep;
     unsigned dstIndex = srcIndex;
-    while (srcIndex < m_largeAllocations.size()) {
-        LargeAllocation* allocation = m_largeAllocations[srcIndex++];
+    while (srcIndex < m_preciseAllocations.size()) {
+        PreciseAllocation* allocation = m_preciseAllocations[srcIndex++];
         allocation->sweep();
         if (allocation->isEmpty()) {
-            m_capacity -= allocation->cellSize();
-            allocation->destroy();
+            if (auto* set = preciseAllocationSet())
+                set->remove(allocation->cell());
+            if (allocation->isLowerTier())
+                static_cast<IsoSubspace*>(allocation->subspace())->sweepLowerTierCell(allocation);
+            else {
+                m_capacity -= allocation->cellSize();
+                allocation->destroy();
+            }
             continue;
         }
         allocation->setIndexInSpace(dstIndex);
-        m_largeAllocations[dstIndex++] = allocation;
+        m_preciseAllocations[dstIndex++] = allocation;
     }
-    m_largeAllocations.shrink(dstIndex);
-    m_largeAllocationsNurseryOffset = m_largeAllocations.size();
+    m_preciseAllocations.shrink(dstIndex);
+    m_preciseAllocationsNurseryOffset = m_preciseAllocations.size();
 }
 
 void MarkedSpace::prepareForAllocation()
 {
-    ASSERT(!Thread::mayBeGCThread() || m_heap->worldIsStopped());
+    ASSERT(!Thread::mayBeGCThread() || heap().worldIsStopped());
     for (Subspace* subspace : m_subspaces)
         subspace->prepareForAllocation();
 
     m_activeWeakSets.takeFrom(m_newActiveWeakSets);
 
-    if (m_heap->collectionScope() == CollectionScope::Eden)
-        m_largeAllocationsNurseryOffsetForSweep = m_largeAllocationsNurseryOffset;
+    if (heap().collectionScope() == CollectionScope::Eden)
+        m_preciseAllocationsNurseryOffsetForSweep = m_preciseAllocationsNurseryOffset;
     else
-        m_largeAllocationsNurseryOffsetForSweep = 0;
-    m_largeAllocationsNurseryOffset = m_largeAllocations.size();
+        m_preciseAllocationsNurseryOffsetForSweep = 0;
+    m_preciseAllocationsNurseryOffset = m_preciseAllocations.size();
+}
+
+void MarkedSpace::enablePreciseAllocationTracking()
+{
+    m_preciseAllocationSet = makeUnique<HashSet<HeapCell*>>();
+    for (auto* allocation : m_preciseAllocations)
+        m_preciseAllocationSet->add(allocation->cell());
 }
 
 void MarkedSpace::visitWeakSets(SlotVisitor& visitor)
 {
     auto visit = [&] (WeakSet* weakSet) {
         weakSet->visit(visitor);
     };
 
     m_newActiveWeakSets.forEach(visit);
 
-    if (m_heap->collectionScope() == CollectionScope::Full)
+    if (heap().collectionScope() == CollectionScope::Full)
         m_activeWeakSets.forEach(visit);
 }
 
 void MarkedSpace::reapWeakSets()
 {
@@ -290,11 +295,11 @@
         weakSet->reap();
     };
 
     m_newActiveWeakSets.forEach(visit);
 
-    if (m_heap->collectionScope() == CollectionScope::Full)
+    if (heap().collectionScope() == CollectionScope::Full)
         m_activeWeakSets.forEach(visit);
 }
 
 void MarkedSpace::stopAllocating()
 {
@@ -316,44 +321,44 @@
         });
 }
 
 void MarkedSpace::prepareForConservativeScan()
 {
-    m_largeAllocationsForThisCollectionBegin = m_largeAllocations.begin() + m_largeAllocationsOffsetForThisCollection;
-    m_largeAllocationsForThisCollectionSize = m_largeAllocations.size() - m_largeAllocationsOffsetForThisCollection;
-    m_largeAllocationsForThisCollectionEnd = m_largeAllocations.end();
-    RELEASE_ASSERT(m_largeAllocationsForThisCollectionEnd == m_largeAllocationsForThisCollectionBegin + m_largeAllocationsForThisCollectionSize);
+    m_preciseAllocationsForThisCollectionBegin = m_preciseAllocations.begin() + m_preciseAllocationsOffsetForThisCollection;
+    m_preciseAllocationsForThisCollectionSize = m_preciseAllocations.size() - m_preciseAllocationsOffsetForThisCollection;
+    m_preciseAllocationsForThisCollectionEnd = m_preciseAllocations.end();
+    RELEASE_ASSERT(m_preciseAllocationsForThisCollectionEnd == m_preciseAllocationsForThisCollectionBegin + m_preciseAllocationsForThisCollectionSize);
 
     std::sort(
-        m_largeAllocationsForThisCollectionBegin, m_largeAllocationsForThisCollectionEnd,
-        [&] (LargeAllocation* a, LargeAllocation* b) {
+        m_preciseAllocationsForThisCollectionBegin, m_preciseAllocationsForThisCollectionEnd,
+        [&] (PreciseAllocation* a, PreciseAllocation* b) {
             return a < b;
         });
-    unsigned index = m_largeAllocationsOffsetForThisCollection;
-    for (auto* start = m_largeAllocationsForThisCollectionBegin; start != m_largeAllocationsForThisCollectionEnd; ++start, ++index) {
+    unsigned index = m_preciseAllocationsOffsetForThisCollection;
+    for (auto* start = m_preciseAllocationsForThisCollectionBegin; start != m_preciseAllocationsForThisCollectionEnd; ++start, ++index) {
         (*start)->setIndexInSpace(index);
-        ASSERT(m_largeAllocations[index] == *start);
-        ASSERT(m_largeAllocations[index]->indexInSpace() == index);
+        ASSERT(m_preciseAllocations[index] == *start);
+        ASSERT(m_preciseAllocations[index]->indexInSpace() == index);
     }
 }
 
 void MarkedSpace::prepareForMarking()
 {
-    if (m_heap->collectionScope() == CollectionScope::Eden)
-        m_largeAllocationsOffsetForThisCollection = m_largeAllocationsNurseryOffset;
+    if (heap().collectionScope() == CollectionScope::Eden)
+        m_preciseAllocationsOffsetForThisCollection = m_preciseAllocationsNurseryOffset;
     else
-        m_largeAllocationsOffsetForThisCollection = 0;
+        m_preciseAllocationsOffsetForThisCollection = 0;
 }
 
 void MarkedSpace::resumeAllocating()
 {
     forEachDirectory(
         [&] (BlockDirectory& directory) -> IterationStatus {
             directory.resumeAllocating();
             return IterationStatus::Continue;
         });
-    // Nothing to do for LargeAllocations.
+    // Nothing to do for PreciseAllocations.
 }
 
 bool MarkedSpace::isPagedOut(MonotonicTime deadline)
 {
     bool result = false;
@@ -363,11 +368,11 @@
                 result = true;
                 return IterationStatus::Done;
             }
             return IterationStatus::Continue;
         });
-    // FIXME: Consider taking LargeAllocations into account here.
+    // FIXME: Consider taking PreciseAllocations into account here.
     return result;
 }
 
 void MarkedSpace::freeBlock(MarkedBlock::Handle* block)
 {
@@ -396,11 +401,11 @@
         });
 }
 
 void MarkedSpace::beginMarking()
 {
-    if (m_heap->collectionScope() == CollectionScope::Full) {
+    if (heap().collectionScope() == CollectionScope::Full) {
         forEachDirectory(
             [&] (BlockDirectory& directory) -> IterationStatus {
                 directory.beginMarkingForFullCollection();
                 return IterationStatus::Continue;
             });
@@ -412,15 +417,15 @@
                 });
         }
 
         m_markingVersion = nextVersion(m_markingVersion);
 
-        for (LargeAllocation* allocation : m_largeAllocations)
+        for (PreciseAllocation* allocation : m_preciseAllocations)
             allocation->flip();
     }
 
-    if (!ASSERT_DISABLED) {
+    if (ASSERT_ENABLED) {
         forEachBlock(
             [&] (MarkedBlock::Handle* block) {
                 if (block->areMarksStale())
                     return;
                 ASSERT(!block->isFreeListed());
@@ -439,15 +444,15 @@
             });
     }
 
     m_newlyAllocatedVersion = nextVersion(m_newlyAllocatedVersion);
 
-    for (unsigned i = m_largeAllocationsOffsetForThisCollection; i < m_largeAllocations.size(); ++i)
-        m_largeAllocations[i]->clearNewlyAllocated();
+    for (unsigned i = m_preciseAllocationsOffsetForThisCollection; i < m_preciseAllocations.size(); ++i)
+        m_preciseAllocations[i]->clearNewlyAllocated();
 
-    if (!ASSERT_DISABLED) {
-        for (LargeAllocation* allocation : m_largeAllocations)
+    if (ASSERT_ENABLED) {
+        for (PreciseAllocation* allocation : m_preciseAllocations)
             ASSERT_UNUSED(allocation, !allocation->isNewlyAllocated());
     }
 
     forEachDirectory(
         [&] (BlockDirectory& directory) -> IterationStatus {
@@ -477,11 +482,11 @@
     size_t result = 0;
     forEachBlock(
         [&] (MarkedBlock::Handle* block) {
             result += block->markCount();
         });
-    for (LargeAllocation* allocation : m_largeAllocations) {
+    for (PreciseAllocation* allocation : m_preciseAllocations) {
         if (allocation->isMarked())
             result++;
     }
     return result;
 }
@@ -491,11 +496,11 @@
     size_t result = 0;
     forEachBlock(
         [&] (MarkedBlock::Handle* block) {
             result += block->markCount() * block->cellSize();
         });
-    for (LargeAllocation* allocation : m_largeAllocations) {
+    for (PreciseAllocation* allocation : m_preciseAllocations) {
         if (allocation->isMarked())
             result += allocation->cellSize();
     }
     return result;
 }
@@ -531,11 +536,11 @@
     }
 }
 
 void MarkedSpace::snapshotUnswept()
 {
-    if (m_heap->collectionScope() == CollectionScope::Eden) {
+    if (heap().collectionScope() == CollectionScope::Eden) {
         forEachDirectory(
             [&] (BlockDirectory& directory) -> IterationStatus {
                 directory.snapshotUnsweptForEdenCollection();
                 return IterationStatus::Continue;
             });
@@ -548,11 +553,11 @@
     }
 }
 
 void MarkedSpace::assertNoUnswept()
 {
-    if (ASSERT_DISABLED)
+    if (!ASSERT_ENABLED)
         return;
     forEachDirectory(
         [&] (BlockDirectory& directory) -> IterationStatus {
             directory.assertNoUnswept();
             return IterationStatus::Continue;
