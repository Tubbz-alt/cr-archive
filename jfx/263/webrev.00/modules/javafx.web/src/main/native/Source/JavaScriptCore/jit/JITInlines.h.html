<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #if ENABLE(JIT)
 29 #include &quot;CommonSlowPathsInlines.h&quot;
 30 #include &quot;JSCInlines.h&quot;
 31 
 32 namespace JSC {
 33 
 34 ALWAYS_INLINE MacroAssembler::JumpList JIT::emitLoadForArrayMode(const Instruction* currentInstruction, JITArrayMode arrayMode, PatchableJump&amp; badType)
 35 {
 36     switch (arrayMode) {
 37     case JITInt32:
 38         return emitInt32Load(currentInstruction, badType);
 39     case JITDouble:
 40         return emitDoubleLoad(currentInstruction, badType);
 41     case JITContiguous:
 42         return emitContiguousLoad(currentInstruction, badType);
 43     case JITArrayStorage:
 44         return emitArrayStorageLoad(currentInstruction, badType);
 45     default:
 46         break;
 47     }
 48     RELEASE_ASSERT_NOT_REACHED();
 49     return MacroAssembler::JumpList();
 50 }
 51 
 52 ALWAYS_INLINE bool JIT::isOperandConstantDouble(VirtualRegister src)
 53 {
 54     return src.isConstant() &amp;&amp; getConstantOperand(src).isDouble();
 55 }
 56 
 57 ALWAYS_INLINE JSValue JIT::getConstantOperand(VirtualRegister src)
 58 {
 59     ASSERT(src.isConstant());
 60     return m_codeBlock-&gt;getConstant(src);
 61 }
 62 
 63 ALWAYS_INLINE void JIT::emitPutIntToCallFrameHeader(RegisterID from, VirtualRegister entry)
 64 {
 65     ASSERT(entry.isHeader());
 66 #if USE(JSVALUE32_64)
 67     store32(TrustedImm32(JSValue::Int32Tag), tagFor(entry));
 68     store32(from, payloadFor(entry));
 69 #else
 70     store64(from, addressFor(entry));
 71 #endif
 72 }
 73 
 74 ALWAYS_INLINE void JIT::emitLoadCharacterString(RegisterID src, RegisterID dst, JumpList&amp; failures)
 75 {
 76     failures.append(branchIfNotString(src));
 77     loadPtr(MacroAssembler::Address(src, JSString::offsetOfValue()), dst);
 78     failures.append(branchIfRopeStringImpl(dst));
 79     failures.append(branch32(NotEqual, MacroAssembler::Address(dst, StringImpl::lengthMemoryOffset()), TrustedImm32(1)));
 80     loadPtr(MacroAssembler::Address(dst, StringImpl::dataOffset()), regT1);
 81 
 82     auto is16Bit = branchTest32(Zero, Address(dst, StringImpl::flagsOffset()), TrustedImm32(StringImpl::flagIs8Bit()));
 83     load8(MacroAssembler::Address(regT1, 0), dst);
 84     auto done = jump();
 85     is16Bit.link(this);
 86     load16(MacroAssembler::Address(regT1, 0), dst);
 87     done.link(this);
 88 }
 89 
 90 ALWAYS_INLINE JIT::Call JIT::emitNakedCall(CodePtr&lt;NoPtrTag&gt; target)
 91 {
 92     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 93     Call nakedCall = nearCall();
 94     m_calls.append(CallRecord(nakedCall, m_bytecodeIndex, FunctionPtr&lt;OperationPtrTag&gt;(target.retagged&lt;OperationPtrTag&gt;())));
 95     return nakedCall;
 96 }
 97 
 98 ALWAYS_INLINE JIT::Call JIT::emitNakedTailCall(CodePtr&lt;NoPtrTag&gt; target)
 99 {
100     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
101     Call nakedCall = nearTailCall();
102     m_calls.append(CallRecord(nakedCall, m_bytecodeIndex, FunctionPtr&lt;OperationPtrTag&gt;(target.retagged&lt;OperationPtrTag&gt;())));
103     return nakedCall;
104 }
105 
106 ALWAYS_INLINE void JIT::updateTopCallFrame()
107 {
108     uint32_t locationBits = CallSiteIndex(m_bytecodeIndex).bits();
109     store32(TrustedImm32(locationBits), tagFor(CallFrameSlot::argumentCountIncludingThis));
110 
111     // FIXME: It&#39;s not clear that this is needed. JITOperations tend to update the top call frame on
112     // the C++ side.
113     // https://bugs.webkit.org/show_bug.cgi?id=155693
114     storePtr(callFrameRegister, &amp;m_vm-&gt;topCallFrame);
115 }
116 
117 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheck(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
118 {
119     updateTopCallFrame();
120     MacroAssembler::Call call = appendCall(function);
121     exceptionCheck();
122     return call;
123 }
124 
125 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
126 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckAndSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
127 {
128     updateTopCallFrame();
129     MacroAssembler::Call call = appendCallWithSlowPathReturnType(function);
130     exceptionCheck();
131     return call;
132 }
133 #endif
134 
135 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
136 {
137     updateTopCallFrame(); // The callee is responsible for setting topCallFrame to their caller
138     MacroAssembler::Call call = appendCall(function);
139     exceptionCheckWithCallFrameRollback();
140     return call;
141 }
142 
143 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, VirtualRegister dst)
144 {
145     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
146 #if USE(JSVALUE64)
147     emitPutVirtualRegister(dst, returnValueGPR);
148 #else
149     emitStore(dst, returnValueGPR2, returnValueGPR);
150 #endif
151     return call;
152 }
153 
154 template&lt;typename Metadata&gt;
155 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata&amp; metadata, const FunctionPtr&lt;CFunctionPtrTag&gt; function, VirtualRegister dst)
156 {
157     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
158     emitValueProfilingSite(metadata);
159 #if USE(JSVALUE64)
160     emitPutVirtualRegister(dst, returnValueGPR);
161 #else
162     emitStore(dst, returnValueGPR2, returnValueGPR);
163 #endif
164     return call;
165 }
166 
167 ALWAYS_INLINE void JIT::linkSlowCaseIfNotJSCell(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, VirtualRegister reg)
168 {
169     if (!m_codeBlock-&gt;isKnownNotImmediate(reg))
170         linkSlowCase(iter);
171 }
172 
173 ALWAYS_INLINE void JIT::linkAllSlowCasesForBytecodeIndex(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, BytecodeIndex bytecodeIndex)
174 {
175     while (iter != slowCases.end() &amp;&amp; iter-&gt;to == bytecodeIndex)
176         linkSlowCase(iter);
177 }
178 
179 ALWAYS_INLINE bool JIT::hasAnySlowCases(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, BytecodeIndex bytecodeIndex)
180 {
181     if (iter != slowCases.end() &amp;&amp; iter-&gt;to == bytecodeIndex)
182         return true;
183     return false;
184 }
185 
186 ALWAYS_INLINE void JIT::addSlowCase(Jump jump)
187 {
188     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
189 
190     m_slowCases.append(SlowCaseEntry(jump, m_bytecodeIndex));
191 }
192 
193 ALWAYS_INLINE void JIT::addSlowCase(const JumpList&amp; jumpList)
194 {
195     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
196 
197     for (const Jump&amp; jump : jumpList.jumps())
198         m_slowCases.append(SlowCaseEntry(jump, m_bytecodeIndex));
199 }
200 
201 ALWAYS_INLINE void JIT::addSlowCase()
202 {
203     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
204 
205     Jump emptyJump; // Doing it this way to make Windows happy.
206     m_slowCases.append(SlowCaseEntry(emptyJump, m_bytecodeIndex));
207 }
208 
209 ALWAYS_INLINE void JIT::addJump(Jump jump, int relativeOffset)
210 {
211     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
212 
213     m_jmpTable.append(JumpTable(jump, m_bytecodeIndex.offset() + relativeOffset));
214 }
215 
216 ALWAYS_INLINE void JIT::addJump(const JumpList&amp; jumpList, int relativeOffset)
217 {
218     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
219 
220     for (auto&amp; jump : jumpList.jumps())
221         addJump(jump, relativeOffset);
222 }
223 
224 ALWAYS_INLINE void JIT::emitJumpSlowToHot(Jump jump, int relativeOffset)
225 {
226     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
227 
228     jump.linkTo(m_labels[m_bytecodeIndex.offset() + relativeOffset], this);
229 }
230 
231 #if ENABLE(SAMPLING_FLAGS)
232 ALWAYS_INLINE void JIT::setSamplingFlag(int32_t flag)
233 {
234     ASSERT(flag &gt;= 1);
235     ASSERT(flag &lt;= 32);
236     or32(TrustedImm32(1u &lt;&lt; (flag - 1)), AbsoluteAddress(SamplingFlags::addressOfFlags()));
237 }
238 
239 ALWAYS_INLINE void JIT::clearSamplingFlag(int32_t flag)
240 {
241     ASSERT(flag &gt;= 1);
242     ASSERT(flag &lt;= 32);
243     and32(TrustedImm32(~(1u &lt;&lt; (flag - 1))), AbsoluteAddress(SamplingFlags::addressOfFlags()));
244 }
245 #endif
246 
247 #if ENABLE(SAMPLING_COUNTERS)
248 ALWAYS_INLINE void JIT::emitCount(AbstractSamplingCounter&amp; counter, int32_t count)
249 {
250     add64(TrustedImm32(count), AbsoluteAddress(counter.addressOfCounter()));
251 }
252 #endif
253 
254 #if ENABLE(OPCODE_SAMPLING)
255 #if CPU(X86_64)
256 ALWAYS_INLINE void JIT::sampleInstruction(const Instruction* instruction, bool inHostFunction)
257 {
258     move(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;sampleSlot()), X86Registers::ecx);
259     storePtr(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;encodeSample(instruction, inHostFunction)), X86Registers::ecx);
260 }
261 #else
262 ALWAYS_INLINE void JIT::sampleInstruction(const Instruction* instruction, bool inHostFunction)
263 {
264     storePtr(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;encodeSample(instruction, inHostFunction)), m_interpreter-&gt;sampler()-&gt;sampleSlot());
265 }
266 #endif
267 #endif
268 
269 #if ENABLE(CODEBLOCK_SAMPLING)
270 #if CPU(X86_64)
271 ALWAYS_INLINE void JIT::sampleCodeBlock(CodeBlock* codeBlock)
272 {
273     move(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;codeBlockSlot()), X86Registers::ecx);
274     storePtr(TrustedImmPtr(codeBlock), X86Registers::ecx);
275 }
276 #else
277 ALWAYS_INLINE void JIT::sampleCodeBlock(CodeBlock* codeBlock)
278 {
279     storePtr(TrustedImmPtr(codeBlock), m_interpreter-&gt;sampler()-&gt;codeBlockSlot());
280 }
281 #endif
282 #endif
283 
284 ALWAYS_INLINE bool JIT::isOperandConstantChar(VirtualRegister src)
285 {
286     return src.isConstant() &amp;&amp; getConstantOperand(src).isString() &amp;&amp; asString(getConstantOperand(src).asCell())-&gt;length() == 1;
287 }
288 
289 inline void JIT::emitValueProfilingSite(ValueProfile&amp; valueProfile)
290 {
291     ASSERT(shouldEmitProfiling());
292 
293     const RegisterID value = regT0;
294 #if USE(JSVALUE32_64)
295     const RegisterID valueTag = regT1;
296 #endif
297 
298     // We&#39;re in a simple configuration: only one bucket, so we can just do a direct
299     // store.
300 #if USE(JSVALUE64)
301     store64(value, valueProfile.m_buckets);
302 #else
303     EncodedValueDescriptor* descriptor = bitwise_cast&lt;EncodedValueDescriptor*&gt;(valueProfile.m_buckets);
304     store32(value, &amp;descriptor-&gt;asBits.payload);
305     store32(valueTag, &amp;descriptor-&gt;asBits.tag);
306 #endif
307 }
308 
309 template&lt;typename Op&gt;
310 inline std::enable_if_t&lt;std::is_same&lt;decltype(Op::Metadata::m_profile), ValueProfile&gt;::value, void&gt; JIT::emitValueProfilingSiteIfProfiledOpcode(Op bytecode)
311 {
312     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
313 }
314 
315 inline void JIT::emitValueProfilingSiteIfProfiledOpcode(...) { }
316 
317 template&lt;typename Metadata&gt;
318 inline void JIT::emitValueProfilingSite(Metadata&amp; metadata)
319 {
320     if (!shouldEmitProfiling())
321         return;
322     emitValueProfilingSite(metadata.m_profile);
323 }
324 
325 inline void JIT::emitArrayProfilingSiteWithCell(RegisterID cell, RegisterID indexingType, ArrayProfile* arrayProfile)
326 {
327     if (shouldEmitProfiling()) {
328         load32(MacroAssembler::Address(cell, JSCell::structureIDOffset()), indexingType);
329         store32(indexingType, arrayProfile-&gt;addressOfLastSeenStructureID());
330     }
331 
332     load8(Address(cell, JSCell::indexingTypeAndMiscOffset()), indexingType);
333 }
334 
335 inline void JIT::emitArrayProfileStoreToHoleSpecialCase(ArrayProfile* arrayProfile)
336 {
337     store8(TrustedImm32(1), arrayProfile-&gt;addressOfMayStoreToHole());
338 }
339 
340 inline void JIT::emitArrayProfileOutOfBoundsSpecialCase(ArrayProfile* arrayProfile)
341 {
342     store8(TrustedImm32(1), arrayProfile-&gt;addressOfOutOfBounds());
343 }
344 
345 inline JITArrayMode JIT::chooseArrayMode(ArrayProfile* profile)
346 {
347     auto arrayProfileSaw = [] (ArrayModes arrayModes, IndexingType capability) {
348         return arrayModesIncludeIgnoringTypedArrays(arrayModes, capability);
349     };
350 
351     ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
352     profile-&gt;computeUpdatedPrediction(locker, m_codeBlock);
353     ArrayModes arrayModes = profile-&gt;observedArrayModes(locker);
354     if (arrayProfileSaw(arrayModes, DoubleShape))
355         return JITDouble;
356     if (arrayProfileSaw(arrayModes, Int32Shape))
357         return JITInt32;
358     if (arrayProfileSaw(arrayModes, ArrayStorageShape))
359         return JITArrayStorage;
360     return JITContiguous;
361 }
362 
363 ALWAYS_INLINE int32_t JIT::getOperandConstantInt(VirtualRegister src)
364 {
365     return getConstantOperand(src).asInt32();
366 }
367 
368 ALWAYS_INLINE double JIT::getOperandConstantDouble(VirtualRegister src)
369 {
370     return getConstantOperand(src).asDouble();
371 }
372 
373 ALWAYS_INLINE void JIT::emitInitRegister(VirtualRegister dst)
374 {
375     storeTrustedValue(jsUndefined(), addressFor(dst));
376 }
377 
378 #if USE(JSVALUE32_64)
379 
380 inline void JIT::emitLoadDouble(VirtualRegister reg, FPRegisterID value)
381 {
382     if (reg.isConstant()) {
383         WriteBarrier&lt;Unknown&gt;&amp; inConstantPool = m_codeBlock-&gt;constantRegister(reg);
384         loadDouble(TrustedImmPtr(&amp;inConstantPool), value);
385     } else
386         loadDouble(addressFor(reg), value);
387 }
388 
389 inline void JIT::emitLoadTag(VirtualRegister reg, RegisterID tag)
390 {
391     if (reg.isConstant()) {
392         move(Imm32(getConstantOperand(reg).tag()), tag);
393         return;
394     }
395 
396     load32(tagFor(reg), tag);
397 }
398 
399 inline void JIT::emitLoadPayload(VirtualRegister reg, RegisterID payload)
400 {
401     if (reg.isConstant()) {
402         move(Imm32(getConstantOperand(reg).payload()), payload);
403         return;
404     }
405 
406     load32(payloadFor(reg), payload);
407 }
408 
409 inline void JIT::emitLoad(const JSValue&amp; v, RegisterID tag, RegisterID payload)
410 {
411     move(Imm32(v.payload()), payload);
412     move(Imm32(v.tag()), tag);
413 }
414 
415 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst)
416 {
417     emitLoad(src, dst.tagGPR(), dst.payloadGPR());
418 }
419 
420 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, JSValueRegs from)
421 {
422     emitStore(dst, from.tagGPR(), from.payloadGPR());
423 }
424 
425 inline void JIT::emitLoad(VirtualRegister reg, RegisterID tag, RegisterID payload, RegisterID base)
426 {
427     RELEASE_ASSERT(tag != payload);
428 
429     if (base == callFrameRegister) {
430         RELEASE_ASSERT(payload != base);
431         emitLoadPayload(reg, payload);
432         emitLoadTag(reg, tag);
433         return;
434     }
435 
436     if (payload == base) { // avoid stomping base
437         load32(tagFor(reg, base), tag);
438         load32(payloadFor(reg, base), payload);
439         return;
440     }
441 
442     load32(payloadFor(reg, base), payload);
443     load32(tagFor(reg, base), tag);
444 }
445 
446 inline void JIT::emitLoad2(VirtualRegister reg1, RegisterID tag1, RegisterID payload1, VirtualRegister reg2, RegisterID tag2, RegisterID payload2)
447 {
448     emitLoad(reg2, tag2, payload2);
449     emitLoad(reg1, tag1, payload1);
450 }
451 
452 inline void JIT::emitStore(VirtualRegister reg, RegisterID tag, RegisterID payload, RegisterID base)
453 {
454     store32(payload, payloadFor(reg, base));
455     store32(tag, tagFor(reg, base));
456 }
457 
458 inline void JIT::emitStoreInt32(VirtualRegister reg, RegisterID payload, bool indexIsInt32)
459 {
460     store32(payload, payloadFor(reg));
461     if (!indexIsInt32)
462         store32(TrustedImm32(JSValue::Int32Tag), tagFor(reg));
463 }
464 
465 inline void JIT::emitStoreInt32(VirtualRegister reg, TrustedImm32 payload, bool indexIsInt32)
466 {
467     store32(payload, payloadFor(reg));
468     if (!indexIsInt32)
469         store32(TrustedImm32(JSValue::Int32Tag), tagFor(reg));
470 }
471 
472 inline void JIT::emitStoreCell(VirtualRegister reg, RegisterID payload, bool indexIsCell)
473 {
474     store32(payload, payloadFor(reg));
475     if (!indexIsCell)
476         store32(TrustedImm32(JSValue::CellTag), tagFor(reg));
477 }
478 
479 inline void JIT::emitStoreBool(VirtualRegister reg, RegisterID payload, bool indexIsBool)
480 {
481     store32(payload, payloadFor(reg));
482     if (!indexIsBool)
483         store32(TrustedImm32(JSValue::BooleanTag), tagFor(reg));
484 }
485 
486 inline void JIT::emitStoreDouble(VirtualRegister reg, FPRegisterID value)
487 {
488     storeDouble(value, addressFor(reg));
489 }
490 
491 inline void JIT::emitStore(VirtualRegister reg, const JSValue constant, RegisterID base)
492 {
493     store32(Imm32(constant.payload()), payloadFor(reg, base));
494     store32(Imm32(constant.tag()), tagFor(reg, base));
495 }
496 
497 inline void JIT::emitJumpSlowCaseIfNotJSCell(VirtualRegister reg)
498 {
499     if (!m_codeBlock-&gt;isKnownNotImmediate(reg)) {
500         if (reg.isConstant())
501             addSlowCase(jump());
502         else
503             addSlowCase(emitJumpIfNotJSCell(reg));
504     }
505 }
506 
507 inline void JIT::emitJumpSlowCaseIfNotJSCell(VirtualRegister reg, RegisterID tag)
508 {
509     if (!m_codeBlock-&gt;isKnownNotImmediate(reg)) {
510         if (reg.isConstant())
511             addSlowCase(jump());
512         else
513             addSlowCase(branchIfNotCell(tag));
514     }
515 }
516 
517 ALWAYS_INLINE bool JIT::isOperandConstantInt(VirtualRegister src)
518 {
519     return src.isConstant() &amp;&amp; getConstantOperand(src).isInt32();
520 }
521 
522 ALWAYS_INLINE bool JIT::getOperandConstantInt(VirtualRegister op1, VirtualRegister op2, VirtualRegister&amp; op, int32_t&amp; constant)
523 {
524     if (isOperandConstantInt(op1)) {
525         constant = getConstantOperand(op1).asInt32();
526         op = op2;
527         return true;
528     }
529 
530     if (isOperandConstantInt(op2)) {
531         constant = getConstantOperand(op2).asInt32();
532         op = op1;
533         return true;
534     }
535 
536     return false;
537 }
538 
539 #else // USE(JSVALUE32_64)
540 
541 // get arg puts an arg from the SF register array into a h/w register
542 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, RegisterID dst)
543 {
544     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
545 
546     if (src.isConstant()) {
547         JSValue value = m_codeBlock-&gt;getConstant(src);
548         if (!value.isNumber())
549             move(TrustedImm64(JSValue::encode(value)), dst);
550         else
551             move(Imm64(JSValue::encode(value)), dst);
552         return;
553     }
554 
555     load64(addressFor(src), dst);
556 }
557 
558 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst)
559 {
560     emitGetVirtualRegister(src, dst.payloadGPR());
561 }
562 
563 ALWAYS_INLINE void JIT::emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2)
564 {
565     emitGetVirtualRegister(src1, dst1);
566     emitGetVirtualRegister(src2, dst2);
567 }
568 
569 ALWAYS_INLINE bool JIT::isOperandConstantInt(VirtualRegister src)
570 {
571     return src.isConstant() &amp;&amp; getConstantOperand(src).isInt32();
572 }
573 
574 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, RegisterID from)
575 {
576     store64(from, addressFor(dst));
577 }
578 
579 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, JSValueRegs from)
580 {
581     emitPutVirtualRegister(dst, from.payloadGPR());
582 }
583 
584 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfBothJSCells(RegisterID reg1, RegisterID reg2, RegisterID scratch)
585 {
586     move(reg1, scratch);
587     or64(reg2, scratch);
588     return branchIfCell(scratch);
589 }
590 
591 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfJSCell(RegisterID reg)
592 {
593     addSlowCase(branchIfCell(reg));
594 }
595 
596 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg)
597 {
598     addSlowCase(branchIfNotCell(reg));
599 }
600 
601 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg, VirtualRegister vReg)
602 {
603     if (!m_codeBlock-&gt;isKnownNotImmediate(vReg))
604         emitJumpSlowCaseIfNotJSCell(reg);
605 }
606 
607 ALWAYS_INLINE JIT::PatchableJump JIT::emitPatchableJumpIfNotInt(RegisterID reg)
608 {
609     return patchableBranch64(Below, reg, numberTagRegister);
610 }
611 
612 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
613 {
614     move(reg1, scratch);
615     and64(reg2, scratch);
616     return branchIfNotInt32(scratch);
617 }
618 
619 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotInt(RegisterID reg)
620 {
621     addSlowCase(branchIfNotInt32(reg));
622 }
623 
624 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
625 {
626     addSlowCase(emitJumpIfNotInt(reg1, reg2, scratch));
627 }
628 
629 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotNumber(RegisterID reg)
630 {
631     addSlowCase(branchIfNotNumber(reg));
632 }
633 
634 #endif // USE(JSVALUE32_64)
635 
636 ALWAYS_INLINE int JIT::jumpTarget(const Instruction* instruction, int target)
637 {
638     if (target)
639         return target;
640     return m_codeBlock-&gt;outOfLineJumpOffset(instruction);
641 }
642 
643 ALWAYS_INLINE GetPutInfo JIT::copiedGetPutInfo(OpPutToScope bytecode)
644 {
645     unsigned key = bytecode.m_metadataID + 1; // HashMap doesn&#39;t like 0 as a key
646     auto iterator = m_copiedGetPutInfos.find(key);
647     if (iterator != m_copiedGetPutInfos.end())
648         return GetPutInfo(iterator-&gt;value);
649     GetPutInfo getPutInfo = bytecode.metadata(m_codeBlock).m_getPutInfo;
650     m_copiedGetPutInfos.add(key, getPutInfo.operand());
651     return getPutInfo;
652 }
653 
654 template&lt;typename BinaryOp&gt;
655 ALWAYS_INLINE BinaryArithProfile JIT::copiedArithProfile(BinaryOp bytecode)
656 {
657     uint64_t key = (static_cast&lt;uint64_t&gt;(BinaryOp::opcodeID) + 1) &lt;&lt; 32 | static_cast&lt;uint64_t&gt;(bytecode.m_metadataID);
658     auto iterator = m_copiedArithProfiles.find(key);
659     if (iterator != m_copiedArithProfiles.end())
660         return iterator-&gt;value;
661     BinaryArithProfile arithProfile = bytecode.metadata(m_codeBlock).m_arithProfile;
662     m_copiedArithProfiles.add(key, arithProfile);
663     return arithProfile;
664 }
665 
666 } // namespace JSC
667 
668 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>