diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2015-2017 Apple Inc. All rights reserved.
+ * Copyright (C) 2015-2019 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -160,11 +160,11 @@
         }
     }
 
     bool hasBeenSimplified(IndexType tmpIndex)
     {
-        if (!ASSERT_DISABLED) {
+        if (ASSERT_ENABLED) {
             if (!!m_coalescedTmps[tmpIndex])
                 ASSERT(getAlias(tmpIndex) != tmpIndex);
         }
 
         return m_isOnSelectStack.quickGet(tmpIndex) || !!m_coalescedTmps[tmpIndex];
@@ -190,36 +190,49 @@
         ASSERT(!isPrecolored(v));
 
         const auto& adjacentsOfU = m_adjacencyList[u];
         const auto& adjacentsOfV = m_adjacencyList[v];
 
-        if (adjacentsOfU.size() + adjacentsOfV.size() < registerCount()) {
+        Vector<IndexType, MacroAssembler::numGPRs + MacroAssembler::numFPRs> highOrderAdjacents;
+        RELEASE_ASSERT(registerCount() <= MacroAssembler::numGPRs + MacroAssembler::numFPRs);
+        unsigned numCandidates = adjacentsOfU.size() + adjacentsOfV.size();
+        if (numCandidates < registerCount()) {
             // Shortcut: if the total number of adjacents is less than the number of register, the condition is always met.
             return true;
         }
 
-        HashSet<IndexType> highOrderAdjacents;
-
         for (IndexType adjacentTmpIndex : adjacentsOfU) {
             ASSERT(adjacentTmpIndex != v);
             ASSERT(adjacentTmpIndex != u);
+            numCandidates--;
             if (!hasBeenSimplified(adjacentTmpIndex) && m_degrees[adjacentTmpIndex] >= registerCount()) {
-                auto addResult = highOrderAdjacents.add(adjacentTmpIndex);
-                if (addResult.isNewEntry && highOrderAdjacents.size() >= registerCount())
+                ASSERT(std::find(highOrderAdjacents.begin(), highOrderAdjacents.end(), adjacentTmpIndex) == highOrderAdjacents.end());
+                highOrderAdjacents.uncheckedAppend(adjacentTmpIndex);
+                if (highOrderAdjacents.size() >= registerCount())
                     return false;
-            }
+            } else if (highOrderAdjacents.size() + numCandidates < registerCount())
+                return true;
         }
+        ASSERT(numCandidates == adjacentsOfV.size());
+
+        auto iteratorEndHighOrderAdjacentsOfU = highOrderAdjacents.end();
         for (IndexType adjacentTmpIndex : adjacentsOfV) {
             ASSERT(adjacentTmpIndex != u);
             ASSERT(adjacentTmpIndex != v);
-            if (!hasBeenSimplified(adjacentTmpIndex) && m_degrees[adjacentTmpIndex] >= registerCount()) {
-                auto addResult = highOrderAdjacents.add(adjacentTmpIndex);
-                if (addResult.isNewEntry && highOrderAdjacents.size() >= registerCount())
+            numCandidates--;
+            if (!hasBeenSimplified(adjacentTmpIndex)
+                && m_degrees[adjacentTmpIndex] >= registerCount()
+                && std::find(highOrderAdjacents.begin(), iteratorEndHighOrderAdjacentsOfU, adjacentTmpIndex) == iteratorEndHighOrderAdjacentsOfU) {
+                ASSERT(std::find(iteratorEndHighOrderAdjacentsOfU, highOrderAdjacents.end(), adjacentTmpIndex) == highOrderAdjacents.end());
+                highOrderAdjacents.uncheckedAppend(adjacentTmpIndex);
+                if (highOrderAdjacents.size() >= registerCount())
                     return false;
-            }
+            } else if (highOrderAdjacents.size() + numCandidates < registerCount())
+                return true;
         }
 
+        ASSERT(!numCandidates);
         ASSERT(highOrderAdjacents.size() < registerCount());
         return true;
     }
 
     bool precoloredCoalescingHeuristic(IndexType u, IndexType v)
@@ -501,11 +514,11 @@
     }
 
     struct InterferenceEdgeHash {
         static unsigned hash(const InterferenceEdge& key) { return key.hash(); }
         static bool equal(const InterferenceEdge& a, const InterferenceEdge& b) { return a == b; }
-        static const bool safeToCompareToEmptyOrDeleted = true;
+        static constexpr bool safeToCompareToEmptyOrDeleted = true;
     };
     typedef SimpleClassHashTraits<InterferenceEdge> InterferenceEdgeHashTraits;
 
     Vector<Reg> m_regsInPriorityOrder;
     IndexType m_lastPrecoloredRegisterIndex { 0 };
@@ -633,11 +646,11 @@
         // list (but not the other way around, note that this is different than IRC because IRC
         // runs this while coalescing, but we do all our coalescing before this). Once a node is
         // added to the select stack, it's not on either list, but only on the select stack.
         // Once on the select stack, logically, it's no longer in the interference graph.
         auto assertInvariants = [&] () {
-            if (ASSERT_DISABLED)
+            if (!ASSERT_ENABLED)
                 return;
             if (!shouldValidateIRAtEachPhase())
                 return;
 
             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
@@ -674,11 +687,11 @@
                 changed = true;
                 ASSERT(m_simplifyWorklist.size() == 1);
             }
         } while (changed);
 
-        if (!ASSERT_DISABLED) {
+        if (ASSERT_ENABLED) {
             ASSERT(!m_simplifyWorklist.size());
             ASSERT(m_spillWorklist.isEmpty());
             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
             for (IndexType i = firstNonRegIndex; i < m_degrees.size(); ++i)
                 ASSERT(hasBeenSimplified(i));
@@ -712,11 +725,11 @@
 
             // No need to ever consider this move again if it interferes.
             // No coalescing will remove the interference.
             moveIndex = UINT_MAX;
 
-            if (!ASSERT_DISABLED) {
+            if (ASSERT_ENABLED) {
                 if (isPrecolored(v))
                     ASSERT(isPrecolored(u));
             }
 
             if (traceDebug)
@@ -895,11 +908,11 @@
         if (oldDegree == registerCount()) {
             ASSERT(m_degrees[tmpIndex] < registerCount());
             if (traceDebug)
                 dataLogLn("Moving tmp ", tmpIndex, " from spill list to simplify list because it's degree is now less than k");
 
-            if (!ASSERT_DISABLED)
+            if (ASSERT_ENABLED)
                 ASSERT(m_unspillableTmps.contains(tmpIndex) || m_spillWorklist.contains(tmpIndex));
             m_spillWorklist.quickClear(tmpIndex);
 
             ASSERT(!m_simplifyWorklist.contains(tmpIndex));
             m_simplifyWorklist.append(tmpIndex);
@@ -1019,11 +1032,11 @@
                 m_simplifyWorklist.append(i);
         }
     }
 
     // Low-degree vertex can always be colored: just pick any of the color taken by any
-    // other adjacent verices.
+    // other adjacent vertices.
     // The "Simplify" phase takes a low-degree out of the interference graph to simplify it.
     void simplify()
     {
         IndexType lastIndex = m_simplifyWorklist.takeLast();
 
@@ -1787,17 +1800,13 @@
     void run()
     {
         padInterference(m_code);
 
         allocateOnBank<GP>();
-        m_numIterations = 0;
         allocateOnBank<FP>();
 
         fixSpillsAfterTerminals(m_code);
-
-        if (reportStats)
-            dataLog("Num iterations = ", m_numIterations, "\n");
     }
 
 private:
     template<Bank bank>
     void allocateOnBank()
@@ -1807,24 +1816,27 @@
         // FIXME: If a Tmp is used only from a Scratch role and that argument is !admitsStack, then
         // we should add the Tmp to unspillableTmps. That will help avoid relooping only to turn the
         // Tmp into an unspillable Tmp.
         // https://bugs.webkit.org/show_bug.cgi?id=152699
 
-        while (true) {
-            ++m_numIterations;
+        unsigned numIterations = 0;
+        bool done = false;
+
+        while (!done) {
+            ++numIterations;
 
             if (traceDebug)
-                dataLog("Code at iteration ", m_numIterations, ":\n", m_code);
+                dataLog("Code at iteration ", numIterations, ":\n", m_code);
 
             // FIXME: One way to optimize this code is to remove the recomputation inside the fixpoint.
             // We need to recompute because spilling adds tmps, but we could just update tmpWidth when we
             // add those tmps. Note that one easy way to remove the recomputation is to make any newly
             // added Tmps get the same use/def widths that the original Tmp got. But, this may hurt the
             // spill code we emit. Since we currently recompute TmpWidth after spilling, the newly
             // created Tmps may get narrower use/def widths. On the other hand, the spiller already
             // selects which move instruction to use based on the original Tmp's widths, so it may not
-            // matter than a subsequent iteration sees a coservative width for the new Tmps. Also, the
+            // matter than a subsequent iteration sees a conservative width for the new Tmps. Also, the
             // recomputation may not actually be a performance problem; it's likely that a better way to
             // improve performance of TmpWidth is to replace its HashMap with something else. It's
             // possible that most of the TmpWidth overhead is from queries of TmpWidth rather than the
             // recomputation, in which case speeding up the lookup would be a bigger win.
             // https://bugs.webkit.org/show_bug.cgi?id=152478
@@ -1833,30 +1845,28 @@
             auto doAllocation = [&] (auto& allocator) -> bool {
                 allocator.allocate();
                 if (!allocator.requiresSpilling()) {
                     this->assignRegistersToTmp<bank>(allocator);
                     if (traceDebug)
-                        dataLog("Successfull allocation at iteration ", m_numIterations, ":\n", m_code);
+                        dataLog("Successfull allocation at iteration ", numIterations, ":\n", m_code);
 
                     return true;
                 }
 
                 this->addSpillAndFill<bank>(allocator, unspillableTmps);
                 return false;
             };
 
-            bool done;
             if (useIRC()) {
                 ColoringAllocator<bank, IRC> allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
                 done = doAllocation(allocator);
             } else {
                 ColoringAllocator<bank, Briggs> allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
                 done = doAllocation(allocator);
             }
-            if (done)
-                return;
         }
+        dataLogLnIf(reportStats, "Num iterations = ", numIterations, " for bank: ", bank);
     }
 
     template<Bank bank>
     HashSet<unsigned> computeUnspillableTmps()
     {
@@ -2185,11 +2195,10 @@
     }
 
     Code& m_code;
     TmpWidth m_tmpWidth;
     UseCounts<Tmp>& m_useCounts;
-    unsigned m_numIterations { 0 };
 };
 
 } // anonymous namespace
 
 void allocateRegistersByGraphColoring(Code& code)
