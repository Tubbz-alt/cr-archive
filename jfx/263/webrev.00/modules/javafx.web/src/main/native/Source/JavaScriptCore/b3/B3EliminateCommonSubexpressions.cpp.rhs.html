<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3EliminateCommonSubexpressions.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (C) 2016-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;B3EliminateCommonSubexpressions.h&quot;
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;B3BasicBlockInlines.h&quot;
 32 #include &quot;B3BlockWorklist.h&quot;
 33 #include &quot;B3Dominators.h&quot;
 34 #include &quot;B3HeapRange.h&quot;
 35 #include &quot;B3InsertionSetInlines.h&quot;
 36 #include &quot;B3MemoryValue.h&quot;
 37 #include &quot;B3PhaseScope.h&quot;
 38 #include &quot;B3ProcedureInlines.h&quot;
 39 #include &quot;B3PureCSE.h&quot;
 40 #include &quot;B3SlotBaseValue.h&quot;
 41 #include &quot;B3StackSlot.h&quot;
 42 #include &quot;B3ValueKey.h&quot;
 43 #include &quot;B3ValueInlines.h&quot;
 44 #include &quot;B3Variable.h&quot;
 45 #include &quot;B3VariableValue.h&quot;
<a name="2" id="anc2"></a>
 46 #include &lt;wtf/CommaPrinter.h&gt;
 47 #include &lt;wtf/HashMap.h&gt;
 48 #include &lt;wtf/ListDump.h&gt;
 49 #include &lt;wtf/RangeSet.h&gt;
 50 
 51 namespace JSC { namespace B3 {
 52 
 53 namespace {
 54 
 55 namespace B3EliminateCommonSubexpressionsInternal {
<a name="3" id="anc3"></a><span class="line-modified"> 56 static constexpr bool verbose = false;</span>
 57 }
 58 
 59 // FIXME: We could treat Patchpoints with a non-empty set of reads as a &quot;memory value&quot; and somehow
 60 // eliminate redundant ones. We would need some way of determining if two patchpoints are replacable.
 61 // It doesn&#39;t seem right to use the reads set for this. We could use the generator, but that feels
 62 // lame because the FTL will pretty much use a unique generator for each patchpoint even when two
 63 // patchpoints have the same semantics as far as CSE would be concerned. We could invent something
 64 // like a &quot;value ID&quot; for patchpoints. By default, each one gets a unique value ID, but FTL could force
 65 // some patchpoints to share the same one as a signal that they will return the same value if executed
 66 // in the same heap with the same inputs.
 67 
 68 typedef Vector&lt;MemoryValue*, 1&gt; MemoryMatches;
 69 
 70 class MemoryValueMap {
 71 public:
 72     MemoryValueMap() { }
 73 
 74     void add(MemoryValue* memory)
 75     {
 76         Matches&amp; matches = m_map.add(memory-&gt;lastChild(), Matches()).iterator-&gt;value;
 77         if (matches.contains(memory))
 78             return;
 79         matches.append(memory);
 80     }
 81 
 82     template&lt;typename Functor&gt;
 83     void removeIf(const Functor&amp; functor)
 84     {
 85         m_map.removeIf(
 86             [&amp;] (HashMap&lt;Value*, Matches&gt;::KeyValuePairType&amp; entry) -&gt; bool {
 87                 entry.value.removeAllMatching(
 88                     [&amp;] (Value* value) -&gt; bool {
 89                         if (MemoryValue* memory = value-&gt;as&lt;MemoryValue&gt;())
 90                             return functor(memory);
 91                         return true;
 92                     });
 93                 return entry.value.isEmpty();
 94             });
 95     }
 96 
 97     Matches* find(Value* ptr)
 98     {
 99         auto iter = m_map.find(ptr);
100         if (iter == m_map.end())
101             return nullptr;
102         return &amp;iter-&gt;value;
103     }
104 
105     template&lt;typename Functor&gt;
106     MemoryValue* find(Value* ptr, const Functor&amp; functor)
107     {
108         if (B3EliminateCommonSubexpressionsInternal::verbose)
109             dataLog(&quot;        Looking for &quot;, pointerDump(ptr), &quot; in &quot;, *this, &quot;\n&quot;);
110         if (Matches* matches = find(ptr)) {
111             if (B3EliminateCommonSubexpressionsInternal::verbose)
112                 dataLog(&quot;        Matches: &quot;, pointerListDump(*matches), &quot;\n&quot;);
113             for (Value* candidateValue : *matches) {
114                 if (B3EliminateCommonSubexpressionsInternal::verbose)
115                     dataLog(&quot;        Having candidate: &quot;, pointerDump(candidateValue), &quot;\n&quot;);
116                 if (MemoryValue* candidateMemory = candidateValue-&gt;as&lt;MemoryValue&gt;()) {
117                     if (functor(candidateMemory))
118                         return candidateMemory;
119                 }
120             }
121         }
122         return nullptr;
123     }
124 
125     void dump(PrintStream&amp; out) const
126     {
127         out.print(&quot;{&quot;);
128         CommaPrinter comma;
129         for (auto&amp; entry : m_map)
130             out.print(comma, pointerDump(entry.key), &quot;=&gt;&quot;, pointerListDump(entry.value));
131         out.print(&quot;}&quot;);
132     }
133 
134 private:
135     // This uses Matches for two reasons:
136     // - It cannot be a MemoryValue* because the key is imprecise. Many MemoryValues could have the
137     //   same key while being unaliased.
138     // - It can&#39;t be a MemoryMatches array because the MemoryValue*&#39;s could be turned into Identity&#39;s.
139     HashMap&lt;Value*, Matches&gt; m_map;
140 };
141 
142 struct ImpureBlockData {
143     void dump(PrintStream&amp; out) const
144     {
145         out.print(
146             &quot;{reads = &quot;, reads, &quot;, writes = &quot;, writes, &quot;, storesAtHead = &quot;, storesAtHead,
147             &quot;, memoryValuesAtTail = &quot;, memoryValuesAtTail, &quot;}&quot;);
148     }
149 
150     RangeSet&lt;HeapRange&gt; reads; // This only gets used for forward store elimination.
151     RangeSet&lt;HeapRange&gt; writes; // This gets used for both load and store elimination.
152     bool fence;
153 
154     MemoryValueMap storesAtHead;
155     MemoryValueMap memoryValuesAtTail;
156 };
157 
158 class CSE {
159 public:
160     CSE(Procedure&amp; proc)
161         : m_proc(proc)
162         , m_dominators(proc.dominators())
163         , m_impureBlockData(proc.size())
164         , m_insertionSet(proc)
165     {
166     }
167 
168     bool run()
169     {
170         if (B3EliminateCommonSubexpressionsInternal::verbose)
171             dataLog(&quot;B3 before CSE:\n&quot;, m_proc);
172 
173         m_proc.resetValueOwners();
174 
175         // Summarize the impure effects of each block, and the impure values available at the end of
176         // each block. This doesn&#39;t edit code yet.
177         for (BasicBlock* block : m_proc) {
178             ImpureBlockData&amp; data = m_impureBlockData[block];
179             for (Value* value : *block) {
180                 Effects effects = value-&gt;effects();
181                 MemoryValue* memory = value-&gt;as&lt;MemoryValue&gt;();
182 
183                 if (memory &amp;&amp; memory-&gt;isStore()
184                     &amp;&amp; !data.reads.overlaps(memory-&gt;range())
185                     &amp;&amp; !data.writes.overlaps(memory-&gt;range())
186                     &amp;&amp; (!data.fence || !memory-&gt;hasFence()))
187                     data.storesAtHead.add(memory);
188                 data.reads.add(effects.reads);
189 
190                 if (HeapRange writes = effects.writes)
191                     clobber(data, writes);
192                 data.fence |= effects.fence;
193 
194                 if (memory)
195                     data.memoryValuesAtTail.add(memory);
196             }
197 
198             if (B3EliminateCommonSubexpressionsInternal::verbose)
199                 dataLog(&quot;Block &quot;, *block, &quot;: &quot;, data, &quot;\n&quot;);
200         }
201 
202         // Perform CSE. This edits code.
203         Vector&lt;BasicBlock*&gt; postOrder = m_proc.blocksInPostOrder();
204         for (unsigned i = postOrder.size(); i--;) {
205             m_block = postOrder[i];
206             if (B3EliminateCommonSubexpressionsInternal::verbose)
207                 dataLog(&quot;Looking at &quot;, *m_block, &quot;:\n&quot;);
208 
209             m_data = ImpureBlockData();
210             for (m_index = 0; m_index &lt; m_block-&gt;size(); ++m_index) {
211                 m_value = m_block-&gt;at(m_index);
212                 process();
213             }
214             m_insertionSet.execute(m_block);
215             m_impureBlockData[m_block] = m_data;
216         }
217 
218         // The previous pass might have requested that we insert code in some basic block other than
219         // the one that it was looking at. This inserts them.
220         for (BasicBlock* block : m_proc) {
221             for (unsigned valueIndex = 0; valueIndex &lt; block-&gt;size(); ++valueIndex) {
222                 auto iter = m_sets.find(block-&gt;at(valueIndex));
223                 if (iter == m_sets.end())
224                     continue;
225 
226                 for (Value* value : iter-&gt;value)
227                     m_insertionSet.insertValue(valueIndex + 1, value);
228             }
229             m_insertionSet.execute(block);
230         }
231 
232         if (B3EliminateCommonSubexpressionsInternal::verbose)
233             dataLog(&quot;B3 after CSE:\n&quot;, m_proc);
234 
235         return m_changed;
236     }
237 
238 private:
239     void process()
240     {
241         m_value-&gt;performSubstitution();
242 
243         if (m_pureCSE.process(m_value, m_dominators)) {
244             ASSERT(!m_value-&gt;effects().writes);
245             m_changed = true;
246             return;
247         }
248 
249         MemoryValue* memory = m_value-&gt;as&lt;MemoryValue&gt;();
250         if (memory &amp;&amp; processMemoryBeforeClobber(memory))
251             return;
252 
253         if (HeapRange writes = m_value-&gt;effects().writes)
254             clobber(m_data, writes);
255 
256         if (memory)
257             processMemoryAfterClobber(memory);
258     }
259 
260     // Return true if we got rid of the operation. If you changed IR in this function, you have to
261     // set m_changed even if you also return true.
262     bool processMemoryBeforeClobber(MemoryValue* memory)
263     {
264         Value* value = memory-&gt;child(0);
265         Value* ptr = memory-&gt;lastChild();
266         HeapRange range = memory-&gt;range();
267         Value::OffsetType offset = memory-&gt;offset();
268 
269         switch (memory-&gt;opcode()) {
270         case Store8:
271             return handleStoreBeforeClobber(
272                 ptr, range,
273                 [&amp;] (MemoryValue* candidate) -&gt; bool {
274                     return candidate-&gt;offset() == offset
275                         &amp;&amp; ((candidate-&gt;opcode() == Store8 &amp;&amp; candidate-&gt;child(0) == value)
276                             || ((candidate-&gt;opcode() == Load8Z || candidate-&gt;opcode() == Load8S)
277                                 &amp;&amp; candidate == value));
278                 });
279         case Store16:
280             return handleStoreBeforeClobber(
281                 ptr, range,
282                 [&amp;] (MemoryValue* candidate) -&gt; bool {
283                     return candidate-&gt;offset() == offset
284                         &amp;&amp; ((candidate-&gt;opcode() == Store16 &amp;&amp; candidate-&gt;child(0) == value)
285                             || ((candidate-&gt;opcode() == Load16Z || candidate-&gt;opcode() == Load16S)
286                                 &amp;&amp; candidate == value));
287                 });
288         case Store:
289             return handleStoreBeforeClobber(
290                 ptr, range,
291                 [&amp;] (MemoryValue* candidate) -&gt; bool {
292                     return candidate-&gt;offset() == offset
293                         &amp;&amp; ((candidate-&gt;opcode() == Store &amp;&amp; candidate-&gt;child(0) == value)
294                             || (candidate-&gt;opcode() == Load &amp;&amp; candidate == value));
295                 });
296         default:
297             return false;
298         }
299     }
300 
301     void clobber(ImpureBlockData&amp; data, HeapRange writes)
302     {
303         data.writes.add(writes);
304 
305         data.memoryValuesAtTail.removeIf(
306             [&amp;] (MemoryValue* memory) {
307                 return memory-&gt;range().overlaps(writes);
308             });
309     }
310 
311     void processMemoryAfterClobber(MemoryValue* memory)
312     {
313         Value* ptr = memory-&gt;lastChild();
314         HeapRange range = memory-&gt;range();
315         Value::OffsetType offset = memory-&gt;offset();
316         Type type = memory-&gt;type();
317 
318         // FIXME: Empower this to insert more casts and shifts. For example, a Load8 could match a
319         // Store and mask the result. You could even have:
320         //
321         // Store(@value, @ptr, offset = 0)
322         // Load8Z(@ptr, offset = 2)
323         //
324         // Which could be turned into something like this:
325         //
326         // Store(@value, @ptr, offset = 0)
327         // ZShr(@value, 16)
328 
329         switch (memory-&gt;opcode()) {
330         case Load8Z: {
331             handleMemoryValue(
332                 ptr, range,
333                 [&amp;] (MemoryValue* candidate) -&gt; bool {
334                     return candidate-&gt;offset() == offset
335                         &amp;&amp; (candidate-&gt;opcode() == Load8Z || candidate-&gt;opcode() == Store8);
336                 },
337                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
338                     if (match-&gt;opcode() == Store8) {
339                         Value* mask = m_proc.add&lt;Const32Value&gt;(m_value-&gt;origin(), 0xff);
340                         fixups.append(mask);
341                         Value* zext = m_proc.add&lt;Value&gt;(
342                             BitAnd, m_value-&gt;origin(), match-&gt;child(0), mask);
343                         fixups.append(zext);
344                         return zext;
345                     }
346                     return nullptr;
347                 });
348             break;
349         }
350 
351         case Load8S: {
352             handleMemoryValue(
353                 ptr, range,
354                 [&amp;] (MemoryValue* candidate) -&gt; bool {
355                     return candidate-&gt;offset() == offset
356                         &amp;&amp; (candidate-&gt;opcode() == Load8S || candidate-&gt;opcode() == Store8);
357                 },
358                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
359                     if (match-&gt;opcode() == Store8) {
360                         Value* sext = m_proc.add&lt;Value&gt;(
361                             SExt8, m_value-&gt;origin(), match-&gt;child(0));
362                         fixups.append(sext);
363                         return sext;
364                     }
365                     return nullptr;
366                 });
367             break;
368         }
369 
370         case Load16Z: {
371             handleMemoryValue(
372                 ptr, range,
373                 [&amp;] (MemoryValue* candidate) -&gt; bool {
374                     return candidate-&gt;offset() == offset
375                         &amp;&amp; (candidate-&gt;opcode() == Load16Z || candidate-&gt;opcode() == Store16);
376                 },
377                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
378                     if (match-&gt;opcode() == Store16) {
379                         Value* mask = m_proc.add&lt;Const32Value&gt;(m_value-&gt;origin(), 0xffff);
380                         fixups.append(mask);
381                         Value* zext = m_proc.add&lt;Value&gt;(
382                             BitAnd, m_value-&gt;origin(), match-&gt;child(0), mask);
383                         fixups.append(zext);
384                         return zext;
385                     }
386                     return nullptr;
387                 });
388             break;
389         }
390 
391         case Load16S: {
392             handleMemoryValue(
393                 ptr, range, [&amp;] (MemoryValue* candidate) -&gt; bool {
394                     return candidate-&gt;offset() == offset
395                         &amp;&amp; (candidate-&gt;opcode() == Load16S || candidate-&gt;opcode() == Store16);
396                 },
397                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
398                     if (match-&gt;opcode() == Store16) {
399                         Value* sext = m_proc.add&lt;Value&gt;(
400                             SExt16, m_value-&gt;origin(), match-&gt;child(0));
401                         fixups.append(sext);
402                         return sext;
403                     }
404                     return nullptr;
405                 });
406             break;
407         }
408 
409         case Load: {
410             handleMemoryValue(
411                 ptr, range,
412                 [&amp;] (MemoryValue* candidate) -&gt; bool {
413                     if (B3EliminateCommonSubexpressionsInternal::verbose)
414                         dataLog(&quot;        Consdering &quot;, pointerDump(candidate), &quot;\n&quot;);
415                     if (candidate-&gt;offset() != offset)
416                         return false;
417 
418                     if (B3EliminateCommonSubexpressionsInternal::verbose)
419                         dataLog(&quot;            offset ok.\n&quot;);
420 
421                     if (candidate-&gt;opcode() == Load &amp;&amp; candidate-&gt;type() == type)
422                         return true;
423 
424                     if (B3EliminateCommonSubexpressionsInternal::verbose)
425                         dataLog(&quot;            not a load with ok type.\n&quot;);
426 
427                     if (candidate-&gt;opcode() == Store &amp;&amp; candidate-&gt;child(0)-&gt;type() == type)
428                         return true;
429 
430                     if (B3EliminateCommonSubexpressionsInternal::verbose)
431                         dataLog(&quot;            not a store with ok type.\n&quot;);
432 
433                     return false;
434                 });
435             break;
436         }
437 
438         case Store8: {
439             handleStoreAfterClobber(
440                 ptr, range,
441                 [&amp;] (MemoryValue* candidate) -&gt; bool {
442                     return candidate-&gt;opcode() == Store8
443                         &amp;&amp; candidate-&gt;offset() == offset;
444                 });
445             break;
446         }
447 
448         case Store16: {
449             handleStoreAfterClobber(
450                 ptr, range,
451                 [&amp;] (MemoryValue* candidate) -&gt; bool {
452                     return candidate-&gt;opcode() == Store16
453                         &amp;&amp; candidate-&gt;offset() == offset;
454                 });
455             break;
456         }
457 
458         case Store: {
459             handleStoreAfterClobber(
460                 ptr, range,
461                 [&amp;] (MemoryValue* candidate) -&gt; bool {
462                     return candidate-&gt;opcode() == Store
463                         &amp;&amp; candidate-&gt;offset() == offset;
464                 });
465             break;
466         }
467 
468         default:
469             break;
470         }
471     }
472 
473     template&lt;typename Filter&gt;
474     bool handleStoreBeforeClobber(Value* ptr, HeapRange range, const Filter&amp; filter)
475     {
476         MemoryMatches matches = findMemoryValue(ptr, range, filter);
477         if (matches.isEmpty())
478             return false;
479 
480         m_value-&gt;replaceWithNop();
481         m_changed = true;
482         return true;
483     }
484 
485     template&lt;typename Filter&gt;
486     void handleStoreAfterClobber(Value* ptr, HeapRange range, const Filter&amp; filter)
487     {
488         if (!m_value-&gt;traps() &amp;&amp; findStoreAfterClobber(ptr, range, filter)) {
489             m_value-&gt;replaceWithNop();
490             m_changed = true;
491             return;
492         }
493 
494         m_data.memoryValuesAtTail.add(m_value-&gt;as&lt;MemoryValue&gt;());
495     }
496 
497     template&lt;typename Filter&gt;
498     bool findStoreAfterClobber(Value* ptr, HeapRange range, const Filter&amp; filter)
499     {
500         if (m_value-&gt;as&lt;MemoryValue&gt;()-&gt;hasFence())
501             return false;
502 
503         // We can eliminate a store if every forward path hits a store to the same location before
504         // hitting any operation that observes the store. This search seems like it should be
505         // expensive, but in the overwhelming majority of cases it will almost immediately hit an
506         // operation that interferes.
507 
508         if (B3EliminateCommonSubexpressionsInternal::verbose)
509             dataLog(*m_value, &quot;: looking forward for stores to &quot;, *ptr, &quot;...\n&quot;);
510 
511         // First search forward in this basic block.
512         // FIXME: It would be cool to get rid of this linear search. It&#39;s not super critical since
513         // we will probably bail out very quickly, but it *is* annoying.
514         for (unsigned index = m_index + 1; index &lt; m_block-&gt;size(); ++index) {
515             Value* value = m_block-&gt;at(index);
516 
517             if (MemoryValue* memoryValue = value-&gt;as&lt;MemoryValue&gt;()) {
518                 if (memoryValue-&gt;lastChild() == ptr &amp;&amp; filter(memoryValue))
519                     return true;
520             }
521 
522             Effects effects = value-&gt;effects();
523             if (effects.reads.overlaps(range) || effects.writes.overlaps(range))
524                 return false;
525         }
526 
527         if (!m_block-&gt;numSuccessors())
528             return false;
529 
530         BlockWorklist worklist;
531         worklist.pushAll(m_block-&gt;successorBlocks());
532 
533         while (BasicBlock* block = worklist.pop()) {
534             ImpureBlockData&amp; data = m_impureBlockData[block];
535 
536             MemoryValue* match = data.storesAtHead.find(ptr, filter);
537             if (match &amp;&amp; match != m_value)
538                 continue;
539 
540             if (data.writes.overlaps(range) || data.reads.overlaps(range))
541                 return false;
542 
543             if (!block-&gt;numSuccessors())
544                 return false;
545 
546             worklist.pushAll(block-&gt;successorBlocks());
547         }
548 
549         return true;
550     }
551 
552     template&lt;typename Filter&gt;
553     void handleMemoryValue(Value* ptr, HeapRange range, const Filter&amp; filter)
554     {
555         handleMemoryValue(
556             ptr, range, filter,
557             [] (MemoryValue*, Vector&lt;Value*&gt;&amp;) -&gt; Value* {
558                 return nullptr;
559             });
560     }
561 
562     template&lt;typename Filter, typename Replace&gt;
563     void handleMemoryValue(
564         Value* ptr, HeapRange range, const Filter&amp; filter, const Replace&amp; replace)
565     {
566         MemoryMatches matches = findMemoryValue(ptr, range, filter);
567         if (replaceMemoryValue(matches, replace))
568             return;
569         m_data.memoryValuesAtTail.add(m_value-&gt;as&lt;MemoryValue&gt;());
570     }
571 
572     template&lt;typename Replace&gt;
573     bool replaceMemoryValue(const MemoryMatches&amp; matches, const Replace&amp; replace)
574     {
575         if (matches.isEmpty())
576             return false;
577 
578         if (B3EliminateCommonSubexpressionsInternal::verbose)
579             dataLog(&quot;Eliminating &quot;, *m_value, &quot; due to &quot;, pointerListDump(matches), &quot;\n&quot;);
580 
581         m_changed = true;
582 
583         if (matches.size() == 1) {
584             MemoryValue* dominatingMatch = matches[0];
585             RELEASE_ASSERT(m_dominators.dominates(dominatingMatch-&gt;owner, m_block));
586 
587             if (B3EliminateCommonSubexpressionsInternal::verbose)
588                 dataLog(&quot;    Eliminating using &quot;, *dominatingMatch, &quot;\n&quot;);
589             Vector&lt;Value*&gt; extraValues;
590             if (Value* value = replace(dominatingMatch, extraValues)) {
591                 for (Value* extraValue : extraValues)
592                     m_insertionSet.insertValue(m_index, extraValue);
593                 m_value-&gt;replaceWithIdentity(value);
594             } else {
595                 if (dominatingMatch-&gt;isStore())
596                     m_value-&gt;replaceWithIdentity(dominatingMatch-&gt;child(0));
597                 else
598                     m_value-&gt;replaceWithIdentity(dominatingMatch);
599             }
600             return true;
601         }
602 
603         // FIXME: It would be way better if this phase just did SSA calculation directly.
604         // Right now we&#39;re relying on the fact that CSE&#39;s position in the phase order is
605         // almost right before SSA fixup.
606 
607         Variable* variable = m_proc.addVariable(m_value-&gt;type());
608 
609         VariableValue* get = m_insertionSet.insert&lt;VariableValue&gt;(
610             m_index, Get, m_value-&gt;origin(), variable);
611         if (B3EliminateCommonSubexpressionsInternal::verbose)
612             dataLog(&quot;    Inserting get of value: &quot;, *get, &quot;\n&quot;);
613         m_value-&gt;replaceWithIdentity(get);
614 
615         for (MemoryValue* match : matches) {
616             Vector&lt;Value*&gt;&amp; sets = m_sets.add(match, Vector&lt;Value*&gt;()).iterator-&gt;value;
617 
618             Value* value = replace(match, sets);
619             if (!value) {
620                 if (match-&gt;isStore())
621                     value = match-&gt;child(0);
622                 else
623                     value = match;
624             }
625 
626             Value* set = m_proc.add&lt;VariableValue&gt;(Set, m_value-&gt;origin(), variable, value);
627             sets.append(set);
628         }
629 
630         return true;
631     }
632 
633     template&lt;typename Filter&gt;
634     MemoryMatches findMemoryValue(Value* ptr, HeapRange range, const Filter&amp; filter)
635     {
636         if (B3EliminateCommonSubexpressionsInternal::verbose) {
637             dataLog(*m_value, &quot;: looking backward for &quot;, *ptr, &quot;...\n&quot;);
638             dataLog(&quot;    Full value: &quot;, deepDump(m_value), &quot;\n&quot;);
639         }
640 
641         if (m_value-&gt;as&lt;MemoryValue&gt;()-&gt;hasFence()) {
642             if (B3EliminateCommonSubexpressionsInternal::verbose)
643                 dataLog(&quot;    Giving up because fences.\n&quot;);
644             return { };
645         }
646 
647         if (MemoryValue* match = m_data.memoryValuesAtTail.find(ptr, filter)) {
648             if (B3EliminateCommonSubexpressionsInternal::verbose)
649                 dataLog(&quot;    Found &quot;, *match, &quot; locally.\n&quot;);
650             return { match };
651         }
652 
653         if (m_data.writes.overlaps(range)) {
654             if (B3EliminateCommonSubexpressionsInternal::verbose)
655                 dataLog(&quot;    Giving up because of writes.\n&quot;);
656             return { };
657         }
658 
659         BlockWorklist worklist;
660         worklist.pushAll(m_block-&gt;predecessors());
661 
662         MemoryMatches matches;
663 
664         while (BasicBlock* block = worklist.pop()) {
665             if (B3EliminateCommonSubexpressionsInternal::verbose)
666                 dataLog(&quot;    Looking at &quot;, *block, &quot;\n&quot;);
667 
668             ImpureBlockData&amp; data = m_impureBlockData[block];
669 
670             MemoryValue* match = data.memoryValuesAtTail.find(ptr, filter);
671             if (B3EliminateCommonSubexpressionsInternal::verbose)
672                 dataLog(&quot;    Consdering match: &quot;, pointerDump(match), &quot;\n&quot;);
673             if (match &amp;&amp; match != m_value) {
674                 if (B3EliminateCommonSubexpressionsInternal::verbose)
675                     dataLog(&quot;    Found match: &quot;, *match, &quot;\n&quot;);
676                 matches.append(match);
677                 continue;
678             }
679 
680             if (data.writes.overlaps(range)) {
681                 if (B3EliminateCommonSubexpressionsInternal::verbose)
682                     dataLog(&quot;    Giving up because of writes.\n&quot;);
683                 return { };
684             }
685 
686             if (!block-&gt;numPredecessors()) {
687                 if (B3EliminateCommonSubexpressionsInternal::verbose)
688                     dataLog(&quot;    Giving up because it&#39;s live at root.\n&quot;);
689                 // This essentially proves that this is live at the prologue. That means that we
690                 // cannot reliably optimize this case.
691                 return { };
692             }
693 
694             worklist.pushAll(block-&gt;predecessors());
695         }
696 
697         if (B3EliminateCommonSubexpressionsInternal::verbose)
698             dataLog(&quot;    Got matches: &quot;, pointerListDump(matches), &quot;\n&quot;);
699         return matches;
700     }
701 
702     Procedure&amp; m_proc;
703 
704     Dominators&amp; m_dominators;
705     PureCSE m_pureCSE;
706 
707     IndexMap&lt;BasicBlock*, ImpureBlockData&gt; m_impureBlockData;
708 
709     ImpureBlockData m_data;
710 
711     BasicBlock* m_block;
712     unsigned m_index;
713     Value* m_value;
714 
715     HashMap&lt;Value*, Vector&lt;Value*&gt;&gt; m_sets;
716 
717     InsertionSet m_insertionSet;
718 
719     bool m_changed { false };
720 };
721 
722 } // anonymous namespace
723 
724 bool eliminateCommonSubexpressions(Procedure&amp; proc)
725 {
726     PhaseScope phaseScope(proc, &quot;eliminateCommonSubexpressions&quot;);
727 
728     CSE cse(proc);
729     return cse.run();
730 }
731 
732 } } // namespace JSC::B3
733 
734 #endif // ENABLE(B3_JIT)
735 
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>