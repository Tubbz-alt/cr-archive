<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmAirIRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;WasmAirIRGenerator.h&quot;
  28 
  29 #if ENABLE(WEBASSEMBLY)
  30 
  31 #include &quot;AirCode.h&quot;
  32 #include &quot;AirGenerate.h&quot;
  33 #include &quot;AirHelpers.h&quot;
  34 #include &quot;AirOpcodeUtils.h&quot;
  35 #include &quot;AirValidate.h&quot;
  36 #include &quot;AllowMacroScratchRegisterUsageIf.h&quot;
  37 #include &quot;B3CCallValue.h&quot;
  38 #include &quot;B3CheckSpecial.h&quot;
  39 #include &quot;B3CheckValue.h&quot;
  40 #include &quot;B3PatchpointSpecial.h&quot;
  41 #include &quot;B3Procedure.h&quot;
  42 #include &quot;B3ProcedureInlines.h&quot;
  43 #include &quot;BinarySwitch.h&quot;
  44 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
  45 #include &quot;JSCInlines.h&quot;
  46 #include &quot;JSWebAssemblyInstance.h&quot;
  47 #include &quot;ScratchRegisterAllocator.h&quot;
  48 #include &quot;VirtualRegister.h&quot;
  49 #include &quot;WasmCallingConvention.h&quot;
  50 #include &quot;WasmContextInlines.h&quot;
  51 #include &quot;WasmExceptionType.h&quot;
  52 #include &quot;WasmFunctionParser.h&quot;
  53 #include &quot;WasmInstance.h&quot;
  54 #include &quot;WasmMemory.h&quot;
  55 #include &quot;WasmOMGPlan.h&quot;
  56 #include &quot;WasmOSREntryData.h&quot;
  57 #include &quot;WasmOpcodeOrigin.h&quot;
  58 #include &quot;WasmOperations.h&quot;
  59 #include &quot;WasmSignatureInlines.h&quot;
  60 #include &quot;WasmThunks.h&quot;
  61 #include &lt;limits&gt;
  62 #include &lt;wtf/Box.h&gt;
  63 #include &lt;wtf/Optional.h&gt;
  64 #include &lt;wtf/StdLibExtras.h&gt;
  65 
  66 namespace JSC { namespace Wasm {
  67 
  68 using namespace B3::Air;
  69 
  70 struct ConstrainedTmp {
  71     ConstrainedTmp() = default;
  72     ConstrainedTmp(Tmp tmp)
  73         : ConstrainedTmp(tmp, tmp.isReg() ? B3::ValueRep::reg(tmp.reg()) : B3::ValueRep::SomeRegister)
  74     { }
  75 
  76     ConstrainedTmp(Tmp tmp, B3::ValueRep rep)
  77         : tmp(tmp)
  78         , rep(rep)
  79     {
  80     }
  81 
  82     explicit operator bool() const { return !!tmp; }
  83 
  84     Tmp tmp;
  85     B3::ValueRep rep;
  86 };
  87 
  88 class TypedTmp {
  89 public:
  90     constexpr TypedTmp()
  91         : m_tmp()
  92         , m_type(Type::Void)
  93     { }
  94 
  95     TypedTmp(Tmp tmp, Type type)
  96         : m_tmp(tmp)
  97         , m_type(type)
  98     { }
  99 
 100     TypedTmp(const TypedTmp&amp;) = default;
 101     TypedTmp(TypedTmp&amp;&amp;) = default;
 102     TypedTmp&amp; operator=(TypedTmp&amp;&amp;) = default;
 103     TypedTmp&amp; operator=(const TypedTmp&amp;) = default;
 104 
 105     bool operator==(const TypedTmp&amp; other) const
 106     {
 107         return m_tmp == other.m_tmp &amp;&amp; m_type == other.m_type;
 108     }
 109     bool operator!=(const TypedTmp&amp; other) const
 110     {
 111         return !(*this == other);
 112     }
 113 
 114     explicit operator bool() const { return !!tmp(); }
 115 
 116     operator Tmp() const { return tmp(); }
 117     operator Arg() const { return Arg(tmp()); }
 118     Tmp tmp() const { return m_tmp; }
 119     Type type() const { return m_type; }
 120 
 121     void dump(PrintStream&amp; out) const
 122     {
 123         out.print(&quot;(&quot;, m_tmp, &quot;, &quot;, m_type, &quot;)&quot;);
 124     }
 125 
 126 private:
 127 
 128     Tmp m_tmp;
 129     Type m_type;
 130 };
 131 
 132 class AirIRGenerator {
 133 public:
 134     using ExpressionType = TypedTmp;
 135     using ResultList = Vector&lt;ExpressionType, 8&gt;;
 136 
 137     struct ControlData {
 138         ControlData(B3::Origin origin, BlockSignature result, ResultList resultTmps, BlockType type, BasicBlock* continuation, BasicBlock* special = nullptr)
 139             : controlBlockType(type)
 140             , continuation(continuation)
 141             , special(special)
 142             , results(resultTmps)
 143             , returnType(result)
 144         {
 145             UNUSED_PARAM(origin);
 146         }
 147 
 148         ControlData()
 149         {
 150         }
 151 
 152         static bool isIf(const ControlData&amp; control) { return control.blockType() == BlockType::If; }
 153         static bool isTopLevel(const ControlData&amp; control) { return control.blockType() == BlockType::TopLevel; }
 154 
 155         void dump(PrintStream&amp; out) const
 156         {
 157             switch (blockType()) {
 158             case BlockType::If:
 159                 out.print(&quot;If:       &quot;);
 160                 break;
 161             case BlockType::Block:
 162                 out.print(&quot;Block:    &quot;);
 163                 break;
 164             case BlockType::Loop:
 165                 out.print(&quot;Loop:     &quot;);
 166                 break;
 167             case BlockType::TopLevel:
 168                 out.print(&quot;TopLevel: &quot;);
 169                 break;
 170             }
 171             out.print(&quot;Continuation: &quot;, *continuation, &quot;, Special: &quot;);
 172             if (special)
 173                 out.print(*special);
 174             else
 175                 out.print(&quot;None&quot;);
 176 
 177             CommaPrinter comma(&quot;, &quot;, &quot; Result Tmps: [&quot;);
 178             for (const auto&amp; tmp : results)
 179                 out.print(comma, tmp);
 180             if (comma.didPrint())
 181                 out.print(&quot;]&quot;);
 182         }
 183 
 184         BlockType blockType() const { return controlBlockType; }
 185         BlockSignature signature() const { return returnType; }
 186 
 187         BasicBlock* targetBlockForBranch()
 188         {
 189             if (blockType() == BlockType::Loop)
 190                 return special;
 191             return continuation;
 192         }
 193 
 194         void convertIfToBlock()
 195         {
 196             ASSERT(blockType() == BlockType::If);
 197             controlBlockType = BlockType::Block;
 198             special = nullptr;
 199         }
 200 
 201         SignatureArgCount branchTargetArity() const
 202         {
 203             if (blockType() == BlockType::Loop)
 204                 return returnType-&gt;argumentCount();
 205             return returnType-&gt;returnCount();
 206         }
 207 
 208         Type branchTargetType(unsigned i) const
 209         {
 210             ASSERT(i &lt; branchTargetArity());
 211             if (blockType() == BlockType::Loop)
 212                 return returnType-&gt;argument(i);
 213             return returnType-&gt;returnType(i);
 214         }
 215 
 216     private:
 217         friend class AirIRGenerator;
 218         BlockType controlBlockType;
 219         BasicBlock* continuation;
 220         BasicBlock* special;
 221         ResultList results;
 222         BlockSignature returnType;
 223     };
 224 
 225     using ControlType = ControlData;
 226 
 227     using ControlEntry = FunctionParser&lt;AirIRGenerator&gt;::ControlEntry;
 228     using ControlStack = FunctionParser&lt;AirIRGenerator&gt;::ControlStack;
 229     using Stack = FunctionParser&lt;AirIRGenerator&gt;::Stack;
 230     using TypedExpression = FunctionParser&lt;AirIRGenerator&gt;::TypedExpression;
 231 
 232     using ErrorType = String;
 233     using UnexpectedResult = Unexpected&lt;ErrorType&gt;;
 234     using Result = Expected&lt;std::unique_ptr&lt;InternalFunction&gt;, ErrorType&gt;;
 235     using PartialResult = Expected&lt;void, ErrorType&gt;;
 236 
 237     static_assert(std::is_same_v&lt;ResultList, FunctionParser&lt;AirIRGenerator&gt;::ResultList&gt;);
 238 
 239     static ExpressionType emptyExpression() { return { }; };
 240 
 241     template &lt;typename ...Args&gt;
 242     NEVER_INLINE UnexpectedResult WARN_UNUSED_RETURN fail(Args... args) const
 243     {
 244         using namespace FailureHelper; // See ADL comment in WasmParser.h.
 245         return UnexpectedResult(makeString(&quot;WebAssembly.Module failed compiling: &quot;_s, makeString(args)...));
 246     }
 247 
 248 #define WASM_COMPILE_FAIL_IF(condition, ...) do { \
 249         if (UNLIKELY(condition))                  \
 250             return fail(__VA_ARGS__);             \
 251     } while (0)
 252 
 253     AirIRGenerator(const ModuleInformation&amp;, B3::Procedure&amp;, InternalFunction*, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp;, MemoryMode, unsigned functionIndex, TierUpCount*, const Signature&amp;);
 254 
 255     PartialResult WARN_UNUSED_RETURN addArguments(const Signature&amp;);
 256     PartialResult WARN_UNUSED_RETURN addLocal(Type, uint32_t);
 257     ExpressionType addConstant(Type, uint64_t);
 258     ExpressionType addConstant(BasicBlock*, Type, uint64_t);
 259     ExpressionType addBottom(BasicBlock*, Type);
 260 
 261     // References
 262     PartialResult WARN_UNUSED_RETURN addRefIsNull(ExpressionType value, ExpressionType&amp; result);
 263     PartialResult WARN_UNUSED_RETURN addRefFunc(uint32_t index, ExpressionType&amp; result);
 264 
 265     // Tables
 266     PartialResult WARN_UNUSED_RETURN addTableGet(unsigned, ExpressionType index, ExpressionType&amp; result);
 267     PartialResult WARN_UNUSED_RETURN addTableSet(unsigned, ExpressionType index, ExpressionType value);
 268     PartialResult WARN_UNUSED_RETURN addTableSize(unsigned, ExpressionType&amp; result);
 269     PartialResult WARN_UNUSED_RETURN addTableGrow(unsigned, ExpressionType fill, ExpressionType delta, ExpressionType&amp; result);
 270     PartialResult WARN_UNUSED_RETURN addTableFill(unsigned, ExpressionType offset, ExpressionType fill, ExpressionType count);
 271 
 272     // Locals
 273     PartialResult WARN_UNUSED_RETURN getLocal(uint32_t index, ExpressionType&amp; result);
 274     PartialResult WARN_UNUSED_RETURN setLocal(uint32_t index, ExpressionType value);
 275 
 276     // Globals
 277     PartialResult WARN_UNUSED_RETURN getGlobal(uint32_t index, ExpressionType&amp; result);
 278     PartialResult WARN_UNUSED_RETURN setGlobal(uint32_t index, ExpressionType value);
 279 
 280     // Memory
 281     PartialResult WARN_UNUSED_RETURN load(LoadOpType, ExpressionType pointer, ExpressionType&amp; result, uint32_t offset);
 282     PartialResult WARN_UNUSED_RETURN store(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 283     PartialResult WARN_UNUSED_RETURN addGrowMemory(ExpressionType delta, ExpressionType&amp; result);
 284     PartialResult WARN_UNUSED_RETURN addCurrentMemory(ExpressionType&amp; result);
 285 
 286     // Basic operators
 287     template&lt;OpType&gt;
 288     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType arg, ExpressionType&amp; result);
 289     template&lt;OpType&gt;
 290     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType left, ExpressionType right, ExpressionType&amp; result);
 291     PartialResult WARN_UNUSED_RETURN addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType&amp; result);
 292 
 293     // Control flow
 294     ControlData WARN_UNUSED_RETURN addTopLevel(BlockSignature);
 295     PartialResult WARN_UNUSED_RETURN addBlock(BlockSignature, Stack&amp; enclosingStack, ControlType&amp; newBlock, Stack&amp; newStack);
 296     PartialResult WARN_UNUSED_RETURN addLoop(BlockSignature, Stack&amp; enclosingStack, ControlType&amp; block, Stack&amp; newStack, uint32_t loopIndex);
 297     PartialResult WARN_UNUSED_RETURN addIf(ExpressionType condition, BlockSignature, Stack&amp; enclosingStack, ControlType&amp; result, Stack&amp; newStack);
 298     PartialResult WARN_UNUSED_RETURN addElse(ControlData&amp;, const Stack&amp;);
 299     PartialResult WARN_UNUSED_RETURN addElseToUnreachable(ControlData&amp;);
 300 
 301     PartialResult WARN_UNUSED_RETURN addReturn(const ControlData&amp;, const Stack&amp; returnValues);
 302     PartialResult WARN_UNUSED_RETURN addBranch(ControlData&amp;, ExpressionType condition, const Stack&amp; returnValues);
 303     PartialResult WARN_UNUSED_RETURN addSwitch(ExpressionType condition, const Vector&lt;ControlData*&gt;&amp; targets, ControlData&amp; defaultTargets, const Stack&amp; expressionStack);
 304     PartialResult WARN_UNUSED_RETURN endBlock(ControlEntry&amp;, Stack&amp; expressionStack);
 305     PartialResult WARN_UNUSED_RETURN addEndToUnreachable(ControlEntry&amp;, const Stack&amp; expressionStack = { });
 306 
 307     PartialResult WARN_UNUSED_RETURN endTopLevel(BlockSignature, const Stack&amp;) { return { }; }
 308 
 309     // Calls
 310     PartialResult WARN_UNUSED_RETURN addCall(uint32_t calleeIndex, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results);
 311     PartialResult WARN_UNUSED_RETURN addCallIndirect(unsigned tableIndex, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results);
 312     PartialResult WARN_UNUSED_RETURN addUnreachable();
 313     B3::PatchpointValue* WARN_UNUSED_RETURN emitCallPatchpoint(BasicBlock*, const Signature&amp;, const ResultList&amp; results, const Vector&lt;TypedTmp&gt;&amp; args, Vector&lt;ConstrainedTmp&gt;&amp;&amp; extraArgs = { });
 314 
 315     PartialResult addShift(Type, B3::Air::Opcode, ExpressionType value, ExpressionType shift, ExpressionType&amp; result);
 316     PartialResult addIntegerSub(B3::Air::Opcode, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 317     PartialResult addFloatingPointAbs(B3::Air::Opcode, ExpressionType value, ExpressionType&amp; result);
 318     PartialResult addFloatingPointBinOp(Type, B3::Air::Opcode, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 319 
 320     void dump(const ControlStack&amp;, const Stack* expressionStack);
 321     void setParser(FunctionParser&lt;AirIRGenerator&gt;* parser) { m_parser = parser; };
 322     void didFinishParsingLocals() { }
 323     void didPopValueFromStack() { }
 324 
 325     const Bag&lt;B3::PatchpointValue*&gt;&amp; patchpoints() const
 326     {
 327         return m_patchpoints;
 328     }
 329 
 330 private:
 331     B3::Type toB3ResultType(BlockSignature returnType);
 332     ALWAYS_INLINE void validateInst(Inst&amp; inst)
 333     {
 334         if (ASSERT_ENABLED) {
 335             if (!inst.isValidForm()) {
 336                 dataLogLn(&quot;Inst validation failed:&quot;);
 337                 dataLogLn(inst, &quot;\n&quot;);
 338                 if (inst.origin)
 339                     dataLogLn(deepDump(inst.origin), &quot;\n&quot;);
 340                 CRASH();
 341             }
 342         }
 343     }
 344 
 345     static Arg extractArg(const TypedTmp&amp; tmp) { return tmp.tmp(); }
 346     static Arg extractArg(const Tmp&amp; tmp) { return Arg(tmp); }
 347     static Arg extractArg(const Arg&amp; arg) { return arg; }
 348 
 349     template&lt;typename... Arguments&gt;
 350     void append(BasicBlock* block, Kind kind, Arguments&amp;&amp;... arguments)
 351     {
 352         // FIXME: Find a way to use origin here.
 353         auto&amp; inst = block-&gt;append(kind, nullptr, extractArg(arguments)...);
 354         validateInst(inst);
 355     }
 356 
 357     template&lt;typename... Arguments&gt;
 358     void append(Kind kind, Arguments&amp;&amp;... arguments)
 359     {
 360         append(m_currentBlock, kind, std::forward&lt;Arguments&gt;(arguments)...);
 361     }
 362 
 363     template&lt;typename... Arguments&gt;
 364     void appendEffectful(B3::Air::Opcode op, Arguments&amp;&amp;... arguments)
 365     {
 366         Kind kind = op;
 367         kind.effects = true;
 368         append(m_currentBlock, kind, std::forward&lt;Arguments&gt;(arguments)...);
 369     }
 370 
 371     Tmp newTmp(B3::Bank bank)
 372     {
 373         switch (bank) {
 374         case B3::GP:
 375             if (m_freeGPs.size())
 376                 return m_freeGPs.takeLast();
 377             break;
 378         case B3::FP:
 379             if (m_freeFPs.size())
 380                 return m_freeFPs.takeLast();
 381             break;
 382         }
 383         return m_code.newTmp(bank);
 384     }
 385 
 386     TypedTmp g32() { return { newTmp(B3::GP), Type::I32 }; }
 387     TypedTmp g64() { return { newTmp(B3::GP), Type::I64 }; }
 388     TypedTmp gAnyref() { return { newTmp(B3::GP), Type::Anyref }; }
 389     TypedTmp gFuncref() { return { newTmp(B3::GP), Type::Funcref }; }
 390     TypedTmp f32() { return { newTmp(B3::FP), Type::F32 }; }
 391     TypedTmp f64() { return { newTmp(B3::FP), Type::F64 }; }
 392 
 393     TypedTmp tmpForType(Type type)
 394     {
 395         switch (type) {
 396         case Type::I32:
 397             return g32();
 398         case Type::I64:
 399             return g64();
 400         case Type::Funcref:
 401             return gFuncref();
 402         case Type::Anyref:
 403             return gAnyref();
 404         case Type::F32:
 405             return f32();
 406         case Type::F64:
 407             return f64();
 408         case Type::Void:
 409             return { };
 410         default:
 411             RELEASE_ASSERT_NOT_REACHED();
 412         }
 413     }
 414 
 415     ResultList tmpsForSignature(BlockSignature signature)
 416     {
 417         ResultList result(signature-&gt;returnCount());
 418         for (unsigned i = 0; i &lt; signature-&gt;returnCount(); ++i)
 419             result[i] = tmpForType(signature-&gt;returnType(i));
 420         return result;
 421     }
 422 
 423     B3::PatchpointValue* addPatchpoint(B3::Type type)
 424     {
 425         auto* result = m_proc.add&lt;B3::PatchpointValue&gt;(type, B3::Origin());
 426         if (UNLIKELY(shouldDumpIRAtEachPhase(B3::AirMode)))
 427             m_patchpoints.add(result);
 428         return result;
 429     }
 430 
 431     template &lt;typename ...Args&gt;
 432     void emitPatchpoint(B3::PatchpointValue* patch, Tmp result, Args... theArgs)
 433     {
 434         emitPatchpoint(m_currentBlock, patch, result, std::forward&lt;Args&gt;(theArgs)...);
 435     }
 436 
 437     template &lt;typename ...Args&gt;
 438     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result, Args... theArgs)
 439     {
 440         emitPatchpoint(basicBlock, patch, Vector&lt;Tmp, 8&gt; { result }, Vector&lt;ConstrainedTmp, sizeof...(Args)&gt;::from(theArgs...));
 441     }
 442 
 443     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result)
 444     {
 445         emitPatchpoint(basicBlock, patch, Vector&lt;Tmp, 8&gt; { result }, Vector&lt;ConstrainedTmp&gt;());
 446     }
 447 
 448     template &lt;typename ResultTmpType, size_t inlineSize&gt;
 449     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, const Vector&lt;ResultTmpType, 8&gt;&amp;  results, Vector&lt;ConstrainedTmp, inlineSize&gt;&amp;&amp; args)
 450     {
 451         if (!m_patchpointSpecial)
 452             m_patchpointSpecial = static_cast&lt;B3::PatchpointSpecial*&gt;(m_code.addSpecial(makeUnique&lt;B3::PatchpointSpecial&gt;()));
 453 
 454         auto toTmp = [&amp;] (ResultTmpType tmp) {
 455             if constexpr (std::is_same_v&lt;ResultTmpType, Tmp&gt;)
 456                 return tmp;
 457             else
 458                 return tmp.tmp();
 459         };
 460 
 461         Inst inst(Patch, patch, Arg::special(m_patchpointSpecial));
 462         Vector&lt;Inst, 1&gt; resultMovs;
 463         switch (patch-&gt;type().kind()) {
 464         case B3::Void:
 465             break;
 466         default: {
 467             ASSERT(results.size());
 468             for (unsigned i = 0; i &lt; results.size(); ++i) {
 469                 switch (patch-&gt;resultConstraints[i].kind()) {
 470                 case B3::ValueRep::StackArgument: {
 471                     Arg arg = Arg::callArg(patch-&gt;resultConstraints[i].offsetFromSP());
 472                     inst.args.append(arg);
 473                     resultMovs.append(Inst(B3::Air::moveForType(m_proc.typeAtOffset(patch-&gt;type(), i)), nullptr, arg, toTmp(results[i])));
 474                     break;
 475                 }
 476                 case B3::ValueRep::Register: {
 477                     inst.args.append(Tmp(patch-&gt;resultConstraints[i].reg()));
 478                     resultMovs.append(Inst(B3::Air::relaxedMoveForType(m_proc.typeAtOffset(patch-&gt;type(), i)), nullptr, Tmp(patch-&gt;resultConstraints[i].reg()), toTmp(results[i])));
 479                     break;
 480                 }
 481                 case B3::ValueRep::SomeRegister: {
 482                     inst.args.append(toTmp(results[i]));
 483                     break;
 484                 }
 485                 default:
 486                     RELEASE_ASSERT_NOT_REACHED();
 487                 }
 488             }
 489         }
 490         }
 491 
 492         for (unsigned i = 0; i &lt; args.size(); ++i) {
 493             ConstrainedTmp&amp; tmp = args[i];
 494             // FIXME: This is less than ideal to create dummy values just to satisfy Air&#39;s
 495             // validation. We should abstrcat Patch enough so ValueRep&#39;s don&#39;t need to be
 496             // backed by Values.
 497             // https://bugs.webkit.org/show_bug.cgi?id=194040
 498             B3::Value* dummyValue = m_proc.addConstant(B3::Origin(), tmp.tmp.isGP() ? B3::Int64 : B3::Double, 0);
 499             patch-&gt;append(dummyValue, tmp.rep);
 500             switch (tmp.rep.kind()) {
 501             case B3::ValueRep::ColdAny: // B3::Value propagates ColdAny information and later Air will allocate appropriate stack.
 502             case B3::ValueRep::SomeRegister:
 503                 inst.args.append(tmp.tmp);
 504                 break;
 505             case B3::ValueRep::Register:
 506                 patch-&gt;earlyClobbered().clear(tmp.rep.reg());
 507                 append(basicBlock, tmp.tmp.isGP() ? Move : MoveDouble, tmp.tmp, tmp.rep.reg());
 508                 inst.args.append(Tmp(tmp.rep.reg()));
 509                 break;
 510             case B3::ValueRep::StackArgument: {
 511                 ASSERT(!patch-&gt;effects.terminal);
 512                 Arg arg = Arg::callArg(tmp.rep.offsetFromSP());
 513                 append(basicBlock, tmp.tmp.isGP() ? Move : MoveDouble, tmp.tmp, arg);
 514                 ASSERT(arg.canRepresent(patch-&gt;child(i)-&gt;type()));
 515                 inst.args.append(arg);
 516                 break;
 517             }
 518             default:
 519                 RELEASE_ASSERT_NOT_REACHED();
 520             }
 521         }
 522 
 523         for (auto valueRep : patch-&gt;resultConstraints) {
 524             if (valueRep.isReg())
 525                 patch-&gt;lateClobbered().clear(valueRep.reg());
 526         }
 527         for (unsigned i = patch-&gt;numGPScratchRegisters; i--;)
 528             inst.args.append(g64().tmp());
 529         for (unsigned i = patch-&gt;numFPScratchRegisters; i--;)
 530             inst.args.append(f64().tmp());
 531 
 532         validateInst(inst);
 533         basicBlock-&gt;append(WTFMove(inst));
 534         for (Inst result : resultMovs) {
 535             validateInst(result);
 536             basicBlock-&gt;append(WTFMove(result));
 537         }
 538     }
 539 
 540     template &lt;typename Branch, typename Generator&gt;
 541     void emitCheck(const Branch&amp; makeBranch, const Generator&amp; generator)
 542     {
 543         // We fail along the truthy edge of &#39;branch&#39;.
 544         Inst branch = makeBranch();
 545 
 546         // FIXME: Make a hashmap of these.
 547         B3::CheckSpecial::Key key(branch);
 548         B3::CheckSpecial* special = static_cast&lt;B3::CheckSpecial*&gt;(m_code.addSpecial(makeUnique&lt;B3::CheckSpecial&gt;(key)));
 549 
 550         // FIXME: Remove the need for dummy values
 551         // https://bugs.webkit.org/show_bug.cgi?id=194040
 552         B3::Value* dummyPredicate = m_proc.addConstant(B3::Origin(), B3::Int32, 42);
 553         B3::CheckValue* checkValue = m_proc.add&lt;B3::CheckValue&gt;(B3::Check, B3::Origin(), dummyPredicate);
 554         checkValue-&gt;setGenerator(generator);
 555 
 556         Inst inst(Patch, checkValue, Arg::special(special));
 557         inst.args.appendVector(branch.args);
 558         m_currentBlock-&gt;append(WTFMove(inst));
 559     }
 560 
 561     template &lt;typename Func, typename ...Args&gt;
 562     void emitCCall(Func func, TypedTmp result, Args... args)
 563     {
 564         emitCCall(m_currentBlock, func, result, std::forward&lt;Args&gt;(args)...);
 565     }
 566     template &lt;typename Func, typename ...Args&gt;
 567     void emitCCall(BasicBlock* block, Func func, TypedTmp result, Args... theArgs)
 568     {
 569         B3::Type resultType = B3::Void;
 570         if (result) {
 571             switch (result.type()) {
 572             case Type::I32:
 573                 resultType = B3::Int32;
 574                 break;
 575             case Type::I64:
 576             case Type::Anyref:
 577             case Type::Funcref:
 578                 resultType = B3::Int64;
 579                 break;
 580             case Type::F32:
 581                 resultType = B3::Float;
 582                 break;
 583             case Type::F64:
 584                 resultType = B3::Double;
 585                 break;
 586             default:
 587                 RELEASE_ASSERT_NOT_REACHED();
 588             }
 589         }
 590 
 591         auto makeDummyValue = [&amp;] (Tmp tmp) {
 592             // FIXME: This is less than ideal to create dummy values just to satisfy Air&#39;s
 593             // validation. We should abstrcat CCall enough so we&#39;re not reliant on arguments
 594             // to the B3::CCallValue.
 595             // https://bugs.webkit.org/show_bug.cgi?id=194040
 596             if (tmp.isGP())
 597                 return m_proc.addConstant(B3::Origin(), B3::Int64, 0);
 598             return m_proc.addConstant(B3::Origin(), B3::Double, 0);
 599         };
 600 
 601         B3::Value* dummyFunc = m_proc.addConstant(B3::Origin(), B3::Int64, bitwise_cast&lt;uintptr_t&gt;(func));
 602         B3::Value* origin = m_proc.add&lt;B3::CCallValue&gt;(resultType, B3::Origin(), B3::Effects::none(), dummyFunc, makeDummyValue(theArgs)...);
 603 
 604         Inst inst(CCall, origin);
 605 
 606         Tmp callee = g64();
 607         append(block, Move, Arg::immPtr(tagCFunctionPtr&lt;void*&gt;(func, B3CCallPtrTag)), callee);
 608         inst.args.append(callee);
 609 
 610         if (result)
 611             inst.args.append(result.tmp());
 612 
 613         for (Tmp tmp : Vector&lt;Tmp, sizeof...(Args)&gt;::from(theArgs.tmp()...))
 614             inst.args.append(tmp);
 615 
 616         block-&gt;append(WTFMove(inst));
 617     }
 618 
 619     static B3::Air::Opcode moveOpForValueType(Type type)
 620     {
 621         switch (type) {
 622         case Type::I32:
 623             return Move32;
 624         case Type::I64:
 625         case Type::Anyref:
 626         case Type::Funcref:
 627             return Move;
 628         case Type::F32:
 629             return MoveFloat;
 630         case Type::F64:
 631             return MoveDouble;
 632         default:
 633             RELEASE_ASSERT_NOT_REACHED();
 634         }
 635     }
 636 
 637     void emitThrowException(CCallHelpers&amp;, ExceptionType);
 638 
 639     void emitEntryTierUpCheck();
 640     void emitLoopTierUpCheck(uint32_t loopIndex, const Stack&amp; enclosingStack);
 641 
 642     void emitWriteBarrierForJSWrapper();
 643     ExpressionType emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOp);
 644     ExpressionType emitLoadOp(LoadOpType, ExpressionType pointer, uint32_t offset);
 645     void emitStoreOp(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 646 
 647     void unify(const ExpressionType dst, const ExpressionType source);
 648     void unifyValuesWithBlock(const Stack&amp; resultStack, const ResultList&amp; stack);
 649 
 650     template &lt;typename IntType&gt;
 651     void emitChecksForModOrDiv(bool isSignedDiv, ExpressionType left, ExpressionType right);
 652 
 653     template &lt;typename IntType&gt;
 654     void emitModOrDiv(bool isDiv, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 655 
 656     enum class MinOrMax { Min, Max };
 657 
 658     PartialResult addFloatingPointMinOrMax(Type, MinOrMax, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 659 
 660     int32_t WARN_UNUSED_RETURN fixupPointerPlusOffset(ExpressionType&amp;, uint32_t);
 661 
 662     void restoreWasmContextInstance(BasicBlock*, TypedTmp);
 663     enum class RestoreCachedStackLimit { No, Yes };
 664     void restoreWebAssemblyGlobalState(RestoreCachedStackLimit, const MemoryInformation&amp;, TypedTmp instance, BasicBlock*);
 665 
 666     B3::Origin origin();
 667 
 668     uint32_t outerLoopIndex() const
 669     {
 670         if (m_outerLoops.isEmpty())
 671             return UINT32_MAX;
 672         return m_outerLoops.last();
 673     }
 674 
 675     FunctionParser&lt;AirIRGenerator&gt;* m_parser { nullptr };
 676     const ModuleInformation&amp; m_info;
 677     const MemoryMode m_mode { MemoryMode::BoundsChecking };
 678     const unsigned m_functionIndex { UINT_MAX };
 679     TierUpCount* m_tierUp { nullptr };
 680 
 681     B3::Procedure&amp; m_proc;
 682     Code&amp; m_code;
 683     Vector&lt;uint32_t&gt; m_outerLoops;
 684     BasicBlock* m_currentBlock { nullptr };
 685     BasicBlock* m_rootBlock { nullptr };
 686     Vector&lt;TypedTmp&gt; m_locals;
 687     Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; m_unlinkedWasmToWasmCalls; // List each call site and the function index whose address it should be patched with.
 688     GPRReg m_memoryBaseGPR { InvalidGPRReg };
 689     GPRReg m_memorySizeGPR { InvalidGPRReg };
 690     GPRReg m_wasmContextInstanceGPR { InvalidGPRReg };
 691     bool m_makesCalls { false };
 692 
 693     Vector&lt;Tmp, 8&gt; m_freeGPs;
 694     Vector&lt;Tmp, 8&gt; m_freeFPs;
 695 
 696     HashMap&lt;BlockSignature, B3::Type&gt; m_tupleMap;
 697     // This is only filled if we are dumping IR.
 698     Bag&lt;B3::PatchpointValue*&gt; m_patchpoints;
 699 
 700     TypedTmp m_instanceValue; // Always use the accessor below to ensure the instance value is materialized when used.
 701     bool m_usesInstanceValue { false };
 702     TypedTmp instanceValue()
 703     {
 704         m_usesInstanceValue = true;
 705         return m_instanceValue;
 706     }
 707 
 708     uint32_t m_maxNumJSCallArguments { 0 };
 709     unsigned m_numImportFunctions;
 710 
 711     B3::PatchpointSpecial* m_patchpointSpecial { nullptr };
 712 };
 713 
 714 // Memory accesses in WebAssembly have unsigned 32-bit offsets, whereas they have signed 32-bit offsets in B3.
 715 int32_t AirIRGenerator::fixupPointerPlusOffset(ExpressionType&amp; ptr, uint32_t offset)
 716 {
 717     if (static_cast&lt;uint64_t&gt;(offset) &gt; static_cast&lt;uint64_t&gt;(std::numeric_limits&lt;int32_t&gt;::max())) {
 718         auto previousPtr = ptr;
 719         ptr = g64();
 720         auto constant = g64();
 721         append(Move, Arg::bigImm(offset), constant);
 722         append(Add64, constant, previousPtr, ptr);
 723         return 0;
 724     }
 725     return offset;
 726 }
 727 
 728 void AirIRGenerator::restoreWasmContextInstance(BasicBlock* block, TypedTmp instance)
 729 {
 730     if (Context::useFastTLS()) {
 731         auto* patchpoint = addPatchpoint(B3::Void);
 732         if (CCallHelpers::storeWasmContextInstanceNeedsMacroScratchRegister())
 733             patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
 734         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 735             AllowMacroScratchRegisterUsageIf allowScratch(jit, CCallHelpers::storeWasmContextInstanceNeedsMacroScratchRegister());
 736             jit.storeWasmContextInstance(params[0].gpr());
 737         });
 738         emitPatchpoint(block, patchpoint, Tmp(), instance);
 739         return;
 740     }
 741 
 742     // FIXME: Because WasmToWasm call clobbers wasmContextInstance register and does not restore it, we need to restore it in the caller side.
 743     // This prevents us from using ArgumentReg to this (logically) immutable pinned register.
 744     auto* patchpoint = addPatchpoint(B3::Void);
 745     B3::Effects effects = B3::Effects::none();
 746     effects.writesPinned = true;
 747     effects.reads = B3::HeapRange::top();
 748     patchpoint-&gt;effects = effects;
 749     patchpoint-&gt;clobberLate(RegisterSet(m_wasmContextInstanceGPR));
 750     GPRReg wasmContextInstanceGPR = m_wasmContextInstanceGPR;
 751     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; param) {
 752         jit.move(param[0].gpr(), wasmContextInstanceGPR);
 753     });
 754     emitPatchpoint(block, patchpoint, Tmp(), instance);
 755 }
 756 
 757 AirIRGenerator::AirIRGenerator(const ModuleInformation&amp; info, B3::Procedure&amp; procedure, InternalFunction* compilation, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; unlinkedWasmToWasmCalls, MemoryMode mode, unsigned functionIndex, TierUpCount* tierUp, const Signature&amp; signature)
 758     : m_info(info)
 759     , m_mode(mode)
 760     , m_functionIndex(functionIndex)
 761     , m_tierUp(tierUp)
 762     , m_proc(procedure)
 763     , m_code(m_proc.code())
 764     , m_unlinkedWasmToWasmCalls(unlinkedWasmToWasmCalls)
 765     , m_numImportFunctions(info.importFunctionCount())
 766 {
 767     m_currentBlock = m_code.addBlock();
 768     m_rootBlock = m_currentBlock;
 769 
 770     // FIXME we don&#39;t really need to pin registers here if there&#39;s no memory. It makes wasm -&gt; wasm thunks simpler for now. https://bugs.webkit.org/show_bug.cgi?id=166623
 771     const PinnedRegisterInfo&amp; pinnedRegs = PinnedRegisterInfo::get();
 772 
 773     m_memoryBaseGPR = pinnedRegs.baseMemoryPointer;
 774     m_code.pinRegister(m_memoryBaseGPR);
 775 
 776     m_wasmContextInstanceGPR = pinnedRegs.wasmContextInstancePointer;
 777     if (!Context::useFastTLS())
 778         m_code.pinRegister(m_wasmContextInstanceGPR);
 779 
 780     if (mode != MemoryMode::Signaling) {
 781         m_memorySizeGPR = pinnedRegs.sizeRegister;
 782         m_code.pinRegister(m_memorySizeGPR);
 783     }
 784 
 785     if (info.memory) {
 786         switch (m_mode) {
 787         case MemoryMode::BoundsChecking:
 788             break;
 789         case MemoryMode::Signaling:
 790             // Most memory accesses in signaling mode don&#39;t do an explicit
 791             // exception check because they can rely on fault handling to detect
 792             // out-of-bounds accesses. FaultSignalHandler nonetheless needs the
 793             // thunk to exist so that it can jump to that thunk.
 794             if (UNLIKELY(!Thunks::singleton().stub(throwExceptionFromWasmThunkGenerator)))
 795                 CRASH();
 796             break;
 797         }
 798     }
 799 
 800     m_code.setNumEntrypoints(1);
 801 
 802     GPRReg contextInstance = Context::useFastTLS() ? wasmCallingConvention().prologueScratchGPRs[1] : m_wasmContextInstanceGPR;
 803 
 804     Ref&lt;B3::Air::PrologueGenerator&gt; prologueGenerator = createSharedTask&lt;B3::Air::PrologueGeneratorFunction&gt;([=] (CCallHelpers&amp; jit, B3::Air::Code&amp; code) {
 805         AllowMacroScratchRegisterUsage allowScratch(jit);
 806         code.emitDefaultPrologue(jit);
 807 
 808         {
 809             GPRReg calleeGPR = wasmCallingConvention().prologueScratchGPRs[0];
 810             auto moveLocation = jit.moveWithPatch(MacroAssembler::TrustedImmPtr(nullptr), calleeGPR);
 811             jit.addLinkTask([compilation, moveLocation] (LinkBuffer&amp; linkBuffer) {
 812                 compilation-&gt;calleeMoveLocation = linkBuffer.locationOf&lt;WasmEntryPtrTag&gt;(moveLocation);
 813             });
 814             jit.emitPutToCallFrameHeader(calleeGPR, CallFrameSlot::callee);
 815             jit.emitPutToCallFrameHeader(nullptr, CallFrameSlot::codeBlock);
 816         }
 817 
 818         {
 819             const Checked&lt;int32_t&gt; wasmFrameSize = m_code.frameSize();
 820             const unsigned minimumParentCheckSize = WTF::roundUpToMultipleOf(stackAlignmentBytes(), 1024);
 821             const unsigned extraFrameSize = WTF::roundUpToMultipleOf(stackAlignmentBytes(), std::max&lt;uint32_t&gt;(
 822                 // This allows us to elide stack checks for functions that are terminal nodes in the call
 823                 // tree, (e.g they don&#39;t make any calls) and have a small enough frame size. This works by
 824                 // having any such terminal node have its parent caller include some extra size in its
 825                 // own check for it. The goal here is twofold:
 826                 // 1. Emit less code.
 827                 // 2. Try to speed things up by skipping stack checks.
 828                 minimumParentCheckSize,
 829                 // This allows us to elide stack checks in the Wasm -&gt; Embedder call IC stub. Since these will
 830                 // spill all arguments to the stack, we ensure that a stack check here covers the
 831                 // stack that such a stub would use.
 832                 (Checked&lt;uint32_t&gt;(m_maxNumJSCallArguments) * sizeof(Register) + jsCallingConvention().headerSizeInBytes).unsafeGet()
 833             ));
 834             const int32_t checkSize = m_makesCalls ? (wasmFrameSize + extraFrameSize).unsafeGet() : wasmFrameSize.unsafeGet();
 835             bool needUnderflowCheck = static_cast&lt;unsigned&gt;(checkSize) &gt; Options::reservedZoneSize();
 836             bool needsOverflowCheck = m_makesCalls || wasmFrameSize &gt;= minimumParentCheckSize || needUnderflowCheck;
 837 
 838             // This allows leaf functions to not do stack checks if their frame size is within
 839             // certain limits since their caller would have already done the check.
 840             if (needsOverflowCheck) {
 841                 GPRReg scratch = wasmCallingConvention().prologueScratchGPRs[0];
 842 
 843                 if (Context::useFastTLS())
 844                     jit.loadWasmContextInstance(contextInstance);
 845 
 846                 jit.addPtr(CCallHelpers::TrustedImm32(-checkSize), GPRInfo::callFrameRegister, scratch);
 847                 MacroAssembler::JumpList overflow;
 848                 if (UNLIKELY(needUnderflowCheck))
 849                     overflow.append(jit.branchPtr(CCallHelpers::Above, scratch, GPRInfo::callFrameRegister));
 850                 overflow.append(jit.branchPtr(CCallHelpers::Below, scratch, CCallHelpers::Address(contextInstance, Instance::offsetOfCachedStackLimit())));
 851                 jit.addLinkTask([overflow] (LinkBuffer&amp; linkBuffer) {
 852                     linkBuffer.link(overflow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(throwStackOverflowFromWasmThunkGenerator).code()));
 853                 });
 854             } else if (m_usesInstanceValue &amp;&amp; Context::useFastTLS()) {
 855                 // No overflow check is needed, but the instance values still needs to be correct.
 856                 jit.loadWasmContextInstance(contextInstance);
 857             }
 858         }
 859     });
 860 
 861     m_code.setPrologueForEntrypoint(0, WTFMove(prologueGenerator));
 862 
 863     if (Context::useFastTLS()) {
 864         m_instanceValue = g64();
 865         // FIXME: Would be nice to only do this if we use instance value.
 866         append(Move, Tmp(contextInstance), m_instanceValue);
 867     } else
 868         m_instanceValue = { Tmp(contextInstance), Type::I64 };
 869 
 870     ASSERT(!m_locals.size());
 871     m_locals.grow(signature.argumentCount());
 872     for (unsigned i = 0; i &lt; signature.argumentCount(); ++i) {
 873         Type type = signature.argument(i);
 874         m_locals[i] = tmpForType(type);
 875     }
 876 
 877     CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(signature, CallRole::Callee);
 878 
 879     for (unsigned i = 0; i &lt; wasmCallInfo.params.size(); ++i) {
 880         B3::ValueRep location = wasmCallInfo.params[i];
 881         Arg arg = location.isReg() ? Arg(Tmp(location.reg())) : Arg::addr(Tmp(GPRInfo::callFrameRegister), location.offsetFromFP());
 882         switch (signature.argument(i)) {
 883         case Type::I32:
 884             append(Move32, arg, m_locals[i]);
 885             break;
 886         case Type::I64:
 887         case Type::Anyref:
 888         case Type::Funcref:
 889             append(Move, arg, m_locals[i]);
 890             break;
 891         case Type::F32:
 892             append(MoveFloat, arg, m_locals[i]);
 893             break;
 894         case Type::F64:
 895             append(MoveDouble, arg, m_locals[i]);
 896             break;
 897         default:
 898             RELEASE_ASSERT_NOT_REACHED();
 899         }
 900     }
 901 
 902     emitEntryTierUpCheck();
 903 }
 904 
 905 B3::Type AirIRGenerator::toB3ResultType(BlockSignature returnType)
 906 {
 907     if (returnType-&gt;returnsVoid())
 908         return B3::Void;
 909 
 910     if (returnType-&gt;returnCount() == 1)
 911         return toB3Type(returnType-&gt;returnType(0));
 912 
 913     auto result = m_tupleMap.ensure(returnType, [&amp;] {
 914         Vector&lt;B3::Type&gt; result;
 915         for (unsigned i = 0; i &lt; returnType-&gt;returnCount(); ++i)
 916             result.append(toB3Type(returnType-&gt;returnType(i)));
 917         return m_proc.addTuple(WTFMove(result));
 918     });
 919     return result.iterator-&gt;value;
 920 }
 921 
 922 void AirIRGenerator::restoreWebAssemblyGlobalState(RestoreCachedStackLimit restoreCachedStackLimit, const MemoryInformation&amp; memory, TypedTmp instance, BasicBlock* block)
 923 {
 924     restoreWasmContextInstance(block, instance);
 925 
 926     if (restoreCachedStackLimit == RestoreCachedStackLimit::Yes) {
 927         // The Instance caches the stack limit, but also knows where its canonical location is.
 928         static_assert(sizeof(decltype(static_cast&lt;Instance*&gt;(nullptr)-&gt;cachedStackLimit())) == sizeof(uint64_t), &quot;&quot;);
 929 
 930         RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfPointerToActualStackLimit(), B3::Width64));
 931         RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfCachedStackLimit(), B3::Width64));
 932         auto temp = g64();
 933         append(block, Move, Arg::addr(instanceValue(), Instance::offsetOfPointerToActualStackLimit()), temp);
 934         append(block, Move, Arg::addr(temp), temp);
 935         append(block, Move, temp, Arg::addr(instanceValue(), Instance::offsetOfCachedStackLimit()));
 936     }
 937 
 938     if (!!memory) {
 939         const PinnedRegisterInfo* pinnedRegs = &amp;PinnedRegisterInfo::get();
 940         RegisterSet clobbers;
 941         clobbers.set(pinnedRegs-&gt;baseMemoryPointer);
 942         clobbers.set(pinnedRegs-&gt;sizeRegister);
 943         if (!isARM64())
 944             clobbers.set(RegisterSet::macroScratchRegisters());
 945 
 946         auto* patchpoint = addPatchpoint(B3::Void);
 947         B3::Effects effects = B3::Effects::none();
 948         effects.writesPinned = true;
 949         effects.reads = B3::HeapRange::top();
 950         patchpoint-&gt;effects = effects;
 951         patchpoint-&gt;clobber(clobbers);
 952         patchpoint-&gt;numGPScratchRegisters = Gigacage::isEnabled(Gigacage::Primitive) ? 1 : 0;
 953 
 954         patchpoint-&gt;setGenerator([pinnedRegs] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 955             AllowMacroScratchRegisterUsage allowScratch(jit);
 956             GPRReg baseMemory = pinnedRegs-&gt;baseMemoryPointer;
 957             GPRReg scratchOrSize = Gigacage::isEnabled(Gigacage::Primitive) ? params.gpScratch(0) : pinnedRegs-&gt;sizeRegister;
 958 
 959             jit.loadPtr(CCallHelpers::Address(params[0].gpr(), Instance::offsetOfCachedMemorySize()), pinnedRegs-&gt;sizeRegister);
 960             jit.loadPtr(CCallHelpers::Address(params[0].gpr(), Instance::offsetOfCachedMemory()), baseMemory);
 961 
 962             jit.cageConditionally(Gigacage::Primitive, baseMemory, pinnedRegs-&gt;sizeRegister, scratchOrSize);
 963         });
 964 
 965         emitPatchpoint(block, patchpoint, Tmp(), instance);
 966     }
 967 }
 968 
 969 void AirIRGenerator::emitThrowException(CCallHelpers&amp; jit, ExceptionType type)
 970 {
 971     jit.move(CCallHelpers::TrustedImm32(static_cast&lt;uint32_t&gt;(type)), GPRInfo::argumentGPR1);
 972     auto jumpToExceptionStub = jit.jump();
 973 
 974     jit.addLinkTask([jumpToExceptionStub] (LinkBuffer&amp; linkBuffer) {
 975         linkBuffer.link(jumpToExceptionStub, CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(throwExceptionFromWasmThunkGenerator).code()));
 976     });
 977 }
 978 
 979 auto AirIRGenerator::addLocal(Type type, uint32_t count) -&gt; PartialResult
 980 {
 981     size_t newSize = m_locals.size() + count;
 982     ASSERT(!(CheckedUint32(count) + m_locals.size()).hasOverflowed());
 983     ASSERT(newSize &lt;= maxFunctionLocals);
 984     WASM_COMPILE_FAIL_IF(!m_locals.tryReserveCapacity(newSize), &quot;can&#39;t allocate memory for &quot;, newSize, &quot; locals&quot;);
 985 
 986     for (uint32_t i = 0; i &lt; count; ++i) {
 987         auto local = tmpForType(type);
 988         m_locals.uncheckedAppend(local);
 989         switch (type) {
 990         case Type::Anyref:
 991         case Type::Funcref:
 992             append(Move, Arg::imm(JSValue::encode(jsNull())), local);
 993             break;
 994         case Type::I32:
 995         case Type::I64: {
 996             append(Xor64, local, local);
 997             break;
 998         }
 999         case Type::F32:
1000         case Type::F64: {
1001             auto temp = g64();
1002             // IEEE 754 &quot;0&quot; is just int32/64 zero.
1003             append(Xor64, temp, temp);
1004             append(type == Type::F32 ? Move32ToFloat : Move64ToDouble, temp, local);
1005             break;
1006         }
1007         default:
1008             RELEASE_ASSERT_NOT_REACHED();
1009         }
1010     }
1011     return { };
1012 }
1013 
1014 auto AirIRGenerator::addConstant(Type type, uint64_t value) -&gt; ExpressionType
1015 {
1016     return addConstant(m_currentBlock, type, value);
1017 }
1018 
1019 auto AirIRGenerator::addConstant(BasicBlock* block, Type type, uint64_t value) -&gt; ExpressionType
1020 {
1021     auto result = tmpForType(type);
1022     switch (type) {
1023     case Type::I32:
1024     case Type::I64:
1025     case Type::Anyref:
1026     case Type::Funcref:
1027         append(block, Move, Arg::bigImm(value), result);
1028         break;
1029     case Type::F32:
1030     case Type::F64: {
1031         auto tmp = g64();
1032         append(block, Move, Arg::bigImm(value), tmp);
1033         append(block, type == Type::F32 ? Move32ToFloat : Move64ToDouble, tmp, result);
1034         break;
1035     }
1036 
1037     default:
1038         RELEASE_ASSERT_NOT_REACHED();
1039     }
1040 
1041     return result;
1042 }
1043 
1044 auto AirIRGenerator::addBottom(BasicBlock* block, Type type) -&gt; ExpressionType
1045 {
1046     append(block, B3::Air::Oops);
1047     return addConstant(type, 0);
1048 }
1049 
1050 auto AirIRGenerator::addArguments(const Signature&amp; signature) -&gt; PartialResult
1051 {
1052     RELEASE_ASSERT(m_locals.size() == signature.argumentCount()); // We handle arguments in the prologue
1053     return { };
1054 }
1055 
1056 auto AirIRGenerator::addRefIsNull(ExpressionType value, ExpressionType&amp; result) -&gt; PartialResult
1057 {
1058     ASSERT(value.tmp());
1059     result = tmpForType(Type::I32);
1060     auto tmp = g64();
1061 
1062     append(Move, Arg::bigImm(JSValue::encode(jsNull())), tmp);
1063     append(Compare64, Arg::relCond(MacroAssembler::Equal), value, tmp, result);
1064 
1065     return { };
1066 }
1067 
1068 auto AirIRGenerator::addRefFunc(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
1069 {
1070     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1071     result = tmpForType(Type::Funcref);
1072     emitCCall(&amp;operationWasmRefFunc, result, instanceValue(), addConstant(Type::I32, index));
1073 
1074     return { };
1075 }
1076 
1077 auto AirIRGenerator::addTableGet(unsigned tableIndex, ExpressionType index, ExpressionType&amp; result) -&gt; PartialResult
1078 {
1079     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1080     ASSERT(index.tmp());
1081     ASSERT(index.type() == Type::I32);
1082     result = tmpForType(m_info.tables[tableIndex].wasmType());
1083 
1084     emitCCall(&amp;operationGetWasmTableElement, result, instanceValue(), addConstant(Type::I32, tableIndex), index);
1085     emitCheck([&amp;] {
1086         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), result, result);
1087     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1088         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
1089     });
1090 
1091     return { };
1092 }
1093 
1094 auto AirIRGenerator::addTableSet(unsigned tableIndex, ExpressionType index, ExpressionType value) -&gt; PartialResult
1095 {
1096     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1097     ASSERT(index.tmp());
1098     ASSERT(index.type() == Type::I32);
1099     ASSERT(value.tmp());
1100 
1101     auto shouldThrow = g32();
1102     emitCCall(&amp;operationSetWasmTableElement, shouldThrow, instanceValue(), addConstant(Type::I32, tableIndex), index, value);
1103 
1104     emitCheck([&amp;] {
1105         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), shouldThrow, shouldThrow);
1106     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1107         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
1108     });
1109 
1110     return { };
1111 }
1112 
1113 auto AirIRGenerator::addTableSize(unsigned tableIndex, ExpressionType&amp; result) -&gt; PartialResult
1114 {
1115     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1116     result = tmpForType(Type::I32);
1117 
1118     emitCCall(&amp;operationGetWasmTableSize, result, instanceValue(), addConstant(Type::I32, tableIndex));
1119 
1120     return { };
1121 }
1122 
1123 auto AirIRGenerator::addTableGrow(unsigned tableIndex, ExpressionType fill, ExpressionType delta, ExpressionType&amp; result) -&gt; PartialResult
1124 {
1125     ASSERT(fill.tmp());
1126     ASSERT(isSubtype(fill.type(), m_info.tables[tableIndex].wasmType()));
1127     ASSERT(delta.tmp());
1128     ASSERT(delta.type() == Type::I32);
1129     result = tmpForType(Type::I32);
1130 
1131     emitCCall(&amp;operationWasmTableGrow, result, instanceValue(), addConstant(Type::I32, tableIndex), fill, delta);
1132 
1133     return { };
1134 }
1135 
1136 auto AirIRGenerator::addTableFill(unsigned tableIndex, ExpressionType offset, ExpressionType fill, ExpressionType count) -&gt; PartialResult
1137 {
1138     ASSERT(fill.tmp());
1139     ASSERT(isSubtype(fill.type(), m_info.tables[tableIndex].wasmType()));
1140     ASSERT(offset.tmp());
1141     ASSERT(offset.type() == Type::I32);
1142     ASSERT(count.tmp());
1143     ASSERT(count.type() == Type::I32);
1144 
1145     auto result = tmpForType(Type::I32);
1146     emitCCall(&amp;operationWasmTableFill, result, instanceValue(), addConstant(Type::I32, tableIndex), offset, fill, count);
1147 
1148     emitCheck([&amp;] {
1149         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), result, result);
1150     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1151         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
1152     });
1153 
1154     return { };
1155 }
1156 
1157 auto AirIRGenerator::getLocal(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
1158 {
1159     ASSERT(m_locals[index].tmp());
1160     result = tmpForType(m_locals[index].type());
1161     append(moveOpForValueType(m_locals[index].type()), m_locals[index].tmp(), result);
1162     return { };
1163 }
1164 
1165 auto AirIRGenerator::addUnreachable() -&gt; PartialResult
1166 {
1167     B3::PatchpointValue* unreachable = addPatchpoint(B3::Void);
1168     unreachable-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1169         this-&gt;emitThrowException(jit, ExceptionType::Unreachable);
1170     });
1171     unreachable-&gt;effects.terminal = true;
1172     emitPatchpoint(unreachable, Tmp());
1173     return { };
1174 }
1175 
1176 auto AirIRGenerator::addGrowMemory(ExpressionType delta, ExpressionType&amp; result) -&gt; PartialResult
1177 {
1178     result = g32();
1179     emitCCall(&amp;operationGrowMemory, result, TypedTmp { Tmp(GPRInfo::callFrameRegister), Type::I64 }, instanceValue(), delta);
1180     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::No, m_info.memory, instanceValue(), m_currentBlock);
1181 
1182     return { };
1183 }
1184 
1185 auto AirIRGenerator::addCurrentMemory(ExpressionType&amp; result) -&gt; PartialResult
1186 {
1187     static_assert(sizeof(decltype(static_cast&lt;Memory*&gt;(nullptr)-&gt;size())) == sizeof(uint64_t), &quot;codegen relies on this size&quot;);
1188 
1189     auto temp1 = g64();
1190     auto temp2 = g64();
1191 
1192     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfCachedMemorySize(), B3::Width64));
1193     append(Move, Arg::addr(instanceValue(), Instance::offsetOfCachedMemorySize()), temp1);
1194     constexpr uint32_t shiftValue = 16;
1195     static_assert(PageCount::pageSize == 1ull &lt;&lt; shiftValue, &quot;This must hold for the code below to be correct.&quot;);
1196     append(Move, Arg::imm(16), temp2);
1197     addShift(Type::I32, Urshift64, temp1, temp2, result);
1198     append(Move32, result, result);
1199 
1200     return { };
1201 }
1202 
1203 auto AirIRGenerator::setLocal(uint32_t index, ExpressionType value) -&gt; PartialResult
1204 {
1205     ASSERT(m_locals[index].tmp());
1206     append(moveOpForValueType(m_locals[index].type()), value, m_locals[index].tmp());
1207     return { };
1208 }
1209 
1210 auto AirIRGenerator::getGlobal(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
1211 {
1212     const Wasm::GlobalInformation&amp; global = m_info.globals[index];
1213     Type type = global.type;
1214 
1215     result = tmpForType(type);
1216 
1217     auto temp = g64();
1218 
1219     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfGlobals(), B3::Width64));
1220     append(Move, Arg::addr(instanceValue(), Instance::offsetOfGlobals()), temp);
1221 
1222     int32_t offset = safeCast&lt;int32_t&gt;(index * sizeof(Register));
1223     switch (global.bindingMode) {
1224     case Wasm::GlobalInformation::BindingMode::EmbeddedInInstance:
1225         if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
1226             append(moveOpForValueType(type), Arg::addr(temp, offset), result);
1227         else {
1228             auto temp2 = g64();
1229             append(Move, Arg::bigImm(offset), temp2);
1230             append(Add64, temp2, temp, temp);
1231             append(moveOpForValueType(type), Arg::addr(temp), result);
1232         }
1233         break;
1234     case Wasm::GlobalInformation::BindingMode::Portable:
1235         ASSERT(global.mutability == Wasm::GlobalInformation::Mutability::Mutable);
1236         if (Arg::isValidAddrForm(offset, B3::Width64))
1237             append(Move, Arg::addr(temp, offset), temp);
1238         else {
1239             auto temp2 = g64();
1240             append(Move, Arg::bigImm(offset), temp2);
1241             append(Add64, temp2, temp, temp);
1242             append(Move, Arg::addr(temp), temp);
1243         }
1244         append(moveOpForValueType(type), Arg::addr(temp), result);
1245         break;
1246     }
1247     return { };
1248 }
1249 
1250 auto AirIRGenerator::setGlobal(uint32_t index, ExpressionType value) -&gt; PartialResult
1251 {
1252     auto temp = g64();
1253 
1254     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfGlobals(), B3::Width64));
1255     append(Move, Arg::addr(instanceValue(), Instance::offsetOfGlobals()), temp);
1256 
1257     const Wasm::GlobalInformation&amp; global = m_info.globals[index];
1258     Type type = global.type;
1259 
1260     int32_t offset = safeCast&lt;int32_t&gt;(index * sizeof(Register));
1261     switch (global.bindingMode) {
1262     case Wasm::GlobalInformation::BindingMode::EmbeddedInInstance:
1263         if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
1264             append(moveOpForValueType(type), value, Arg::addr(temp, offset));
1265         else {
1266             auto temp2 = g64();
1267             append(Move, Arg::bigImm(offset), temp2);
1268             append(Add64, temp2, temp, temp);
1269             append(moveOpForValueType(type), value, Arg::addr(temp));
1270         }
1271         if (isSubtype(type, Anyref))
1272             emitWriteBarrierForJSWrapper();
1273         break;
1274     case Wasm::GlobalInformation::BindingMode::Portable:
1275         ASSERT(global.mutability == Wasm::GlobalInformation::Mutability::Mutable);
1276         if (Arg::isValidAddrForm(offset, B3::Width64))
1277             append(Move, Arg::addr(temp, offset), temp);
1278         else {
1279             auto temp2 = g64();
1280             append(Move, Arg::bigImm(offset), temp2);
1281             append(Add64, temp2, temp, temp);
1282             append(Move, Arg::addr(temp), temp);
1283         }
1284         append(moveOpForValueType(type), value, Arg::addr(temp));
1285         // We emit a write-barrier onto JSWebAssemblyGlobal, not JSWebAssemblyInstance.
1286         if (isSubtype(type, Anyref)) {
1287             auto cell = g64();
1288             auto vm = g64();
1289             auto cellState = g32();
1290             auto threshold = g32();
1291 
1292             BasicBlock* fenceCheckPath = m_code.addBlock();
1293             BasicBlock* fencePath = m_code.addBlock();
1294             BasicBlock* doSlowPath = m_code.addBlock();
1295             BasicBlock* continuation = m_code.addBlock();
1296 
1297             append(Move, Arg::addr(instanceValue(), Instance::offsetOfOwner()), cell);
1298             append(Move, Arg::addr(cell, JSWebAssemblyInstance::offsetOfVM()), vm);
1299 
1300             append(Move, Arg::addr(temp, Wasm::Global::offsetOfOwner() - Wasm::Global::offsetOfValue()), cell);
1301             append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
1302             append(Move32, Arg::addr(vm, VM::offsetOfHeapBarrierThreshold()), threshold);
1303 
1304             append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, threshold);
1305             m_currentBlock-&gt;setSuccessors(continuation, fenceCheckPath);
1306             m_currentBlock = fenceCheckPath;
1307 
1308             append(Load8, Arg::addr(vm, VM::offsetOfHeapMutatorShouldBeFenced()), threshold);
1309             append(BranchTest32, Arg::resCond(MacroAssembler::Zero), threshold, threshold);
1310             m_currentBlock-&gt;setSuccessors(doSlowPath, fencePath);
1311             m_currentBlock = fencePath;
1312 
1313             auto* doFence = addPatchpoint(B3::Void);
1314             doFence-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1315                 jit.memoryFence();
1316             });
1317             emitPatchpoint(doFence, Tmp());
1318 
1319             append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
1320             append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, Arg::imm(blackThreshold));
1321             m_currentBlock-&gt;setSuccessors(continuation, doSlowPath);
1322             m_currentBlock = doSlowPath;
1323 
1324             emitCCall(&amp;operationWasmWriteBarrierSlowPath, TypedTmp(), cell, vm);
1325             append(Jump);
1326             m_currentBlock-&gt;setSuccessors(continuation);
1327             m_currentBlock = continuation;
1328         }
1329         break;
1330     }
1331 
1332     return { };
1333 }
1334 
1335 inline void AirIRGenerator::emitWriteBarrierForJSWrapper()
1336 {
1337     auto cell = g64();
1338     auto vm = g64();
1339     auto cellState = g32();
1340     auto threshold = g32();
1341 
1342     BasicBlock* fenceCheckPath = m_code.addBlock();
1343     BasicBlock* fencePath = m_code.addBlock();
1344     BasicBlock* doSlowPath = m_code.addBlock();
1345     BasicBlock* continuation = m_code.addBlock();
1346 
1347     append(Move, Arg::addr(instanceValue(), Instance::offsetOfOwner()), cell);
1348     append(Move, Arg::addr(cell, JSWebAssemblyInstance::offsetOfVM()), vm);
1349     append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
1350     append(Move32, Arg::addr(vm, VM::offsetOfHeapBarrierThreshold()), threshold);
1351 
1352     append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, threshold);
1353     m_currentBlock-&gt;setSuccessors(continuation, fenceCheckPath);
1354     m_currentBlock = fenceCheckPath;
1355 
1356     append(Load8, Arg::addr(vm, VM::offsetOfHeapMutatorShouldBeFenced()), threshold);
1357     append(BranchTest32, Arg::resCond(MacroAssembler::Zero), threshold, threshold);
1358     m_currentBlock-&gt;setSuccessors(doSlowPath, fencePath);
1359     m_currentBlock = fencePath;
1360 
1361     auto* doFence = addPatchpoint(B3::Void);
1362     doFence-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1363         jit.memoryFence();
1364     });
1365     emitPatchpoint(doFence, Tmp());
1366 
1367     append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
1368     append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, Arg::imm(blackThreshold));
1369     m_currentBlock-&gt;setSuccessors(continuation, doSlowPath);
1370     m_currentBlock = doSlowPath;
1371 
1372     emitCCall(&amp;operationWasmWriteBarrierSlowPath, TypedTmp(), cell, vm);
1373     append(Jump);
1374     m_currentBlock-&gt;setSuccessors(continuation);
1375     m_currentBlock = continuation;
1376 }
1377 
1378 inline AirIRGenerator::ExpressionType AirIRGenerator::emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOperation)
1379 {
1380     ASSERT(m_memoryBaseGPR);
1381 
1382     auto result = g64();
1383     append(Move32, pointer, result);
1384 
1385     switch (m_mode) {
1386     case MemoryMode::BoundsChecking: {
1387         // We&#39;re not using signal handling at all, we must therefore check that no memory access exceeds the current memory size.
1388         ASSERT(m_memorySizeGPR);
1389         ASSERT(sizeOfOperation + offset &gt; offset);
1390         auto temp = g64();
1391         append(Move, Arg::bigImm(static_cast&lt;uint64_t&gt;(sizeOfOperation) + offset - 1), temp);
1392         append(Add64, result, temp);
1393 
1394         emitCheck([&amp;] {
1395             return Inst(Branch64, nullptr, Arg::relCond(MacroAssembler::AboveOrEqual), temp, Tmp(m_memorySizeGPR));
1396         }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1397             this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1398         });
1399         break;
1400     }
1401 
1402     case MemoryMode::Signaling: {
1403         // We&#39;ve virtually mapped 4GiB+redzone for this memory. Only the user-allocated pages are addressable, contiguously in range [0, current],
1404         // and everything above is mapped PROT_NONE. We don&#39;t need to perform any explicit bounds check in the 4GiB range because WebAssembly register
1405         // memory accesses are 32-bit. However WebAssembly register + offset accesses perform the addition in 64-bit which can push an access above
1406         // the 32-bit limit (the offset is unsigned 32-bit). The redzone will catch most small offsets, and we&#39;ll explicitly bounds check any
1407         // register + large offset access. We don&#39;t think this will be generated frequently.
1408         //
1409         // We could check that register + large offset doesn&#39;t exceed 4GiB+redzone since that&#39;s technically the limit we need to avoid overflowing the
1410         // PROT_NONE region, but it&#39;s better if we use a smaller immediate because it can codegens better. We know that anything equal to or greater
1411         // than the declared &#39;maximum&#39; will trap, so we can compare against that number. If there was no declared &#39;maximum&#39; then we still know that
1412         // any access equal to or greater than 4GiB will trap, no need to add the redzone.
1413         if (offset &gt;= Memory::fastMappedRedzoneBytes()) {
1414             uint64_t maximum = m_info.memory.maximum() ? m_info.memory.maximum().bytes() : std::numeric_limits&lt;uint32_t&gt;::max();
1415             auto temp = g64();
1416             append(Move, Arg::bigImm(static_cast&lt;uint64_t&gt;(sizeOfOperation) + offset - 1), temp);
1417             append(Add64, result, temp);
1418             auto sizeMax = addConstant(Type::I64, maximum);
1419 
1420             emitCheck([&amp;] {
1421                 return Inst(Branch64, nullptr, Arg::relCond(MacroAssembler::AboveOrEqual), temp, sizeMax);
1422             }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1423                 this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1424             });
1425         }
1426         break;
1427     }
1428     }
1429 
1430     append(Add64, Tmp(m_memoryBaseGPR), result);
1431     return result;
1432 }
1433 
1434 inline uint32_t sizeOfLoadOp(LoadOpType op)
1435 {
1436     switch (op) {
1437     case LoadOpType::I32Load8S:
1438     case LoadOpType::I32Load8U:
1439     case LoadOpType::I64Load8S:
1440     case LoadOpType::I64Load8U:
1441         return 1;
1442     case LoadOpType::I32Load16S:
1443     case LoadOpType::I64Load16S:
1444     case LoadOpType::I32Load16U:
1445     case LoadOpType::I64Load16U:
1446         return 2;
1447     case LoadOpType::I32Load:
1448     case LoadOpType::I64Load32S:
1449     case LoadOpType::I64Load32U:
1450     case LoadOpType::F32Load:
1451         return 4;
1452     case LoadOpType::I64Load:
1453     case LoadOpType::F64Load:
1454         return 8;
1455     }
1456     RELEASE_ASSERT_NOT_REACHED();
1457 }
1458 
1459 inline TypedTmp AirIRGenerator::emitLoadOp(LoadOpType op, ExpressionType pointer, uint32_t uoffset)
1460 {
1461     uint32_t offset = fixupPointerPlusOffset(pointer, uoffset);
1462 
1463     TypedTmp immTmp;
1464     TypedTmp newPtr;
1465     TypedTmp result;
1466 
1467     Arg addrArg;
1468     if (Arg::isValidAddrForm(offset, B3::widthForBytes(sizeOfLoadOp(op))))
1469         addrArg = Arg::addr(pointer, offset);
1470     else {
1471         immTmp = g64();
1472         newPtr = g64();
1473         append(Move, Arg::bigImm(offset), immTmp);
1474         append(Add64, immTmp, pointer, newPtr);
1475         addrArg = Arg::addr(newPtr);
1476     }
1477 
1478     switch (op) {
1479     case LoadOpType::I32Load8S: {
1480         result = g32();
1481         appendEffectful(Load8SignedExtendTo32, addrArg, result);
1482         break;
1483     }
1484 
1485     case LoadOpType::I64Load8S: {
1486         result = g64();
1487         appendEffectful(Load8SignedExtendTo32, addrArg, result);
1488         append(SignExtend32ToPtr, result, result);
1489         break;
1490     }
1491 
1492     case LoadOpType::I32Load8U: {
1493         result = g32();
1494         appendEffectful(Load8, addrArg, result);
1495         break;
1496     }
1497 
1498     case LoadOpType::I64Load8U: {
1499         result = g64();
1500         appendEffectful(Load8, addrArg, result);
1501         break;
1502     }
1503 
1504     case LoadOpType::I32Load16S: {
1505         result = g32();
1506         appendEffectful(Load16SignedExtendTo32, addrArg, result);
1507         break;
1508     }
1509 
1510     case LoadOpType::I64Load16S: {
1511         result = g64();
1512         appendEffectful(Load16SignedExtendTo32, addrArg, result);
1513         append(SignExtend32ToPtr, result, result);
1514         break;
1515     }
1516 
1517     case LoadOpType::I32Load16U: {
1518         result = g32();
1519         appendEffectful(Load16, addrArg, result);
1520         break;
1521     }
1522 
1523     case LoadOpType::I64Load16U: {
1524         result = g64();
1525         appendEffectful(Load16, addrArg, result);
1526         break;
1527     }
1528 
1529     case LoadOpType::I32Load:
1530         result = g32();
1531         appendEffectful(Move32, addrArg, result);
1532         break;
1533 
1534     case LoadOpType::I64Load32U: {
1535         result = g64();
1536         appendEffectful(Move32, addrArg, result);
1537         break;
1538     }
1539 
1540     case LoadOpType::I64Load32S: {
1541         result = g64();
1542         appendEffectful(Move32, addrArg, result);
1543         append(SignExtend32ToPtr, result, result);
1544         break;
1545     }
1546 
1547     case LoadOpType::I64Load: {
1548         result = g64();
1549         appendEffectful(Move, addrArg, result);
1550         break;
1551     }
1552 
1553     case LoadOpType::F32Load: {
1554         result = f32();
1555         appendEffectful(MoveFloat, addrArg, result);
1556         break;
1557     }
1558 
1559     case LoadOpType::F64Load: {
1560         result = f64();
1561         appendEffectful(MoveDouble, addrArg, result);
1562         break;
1563     }
1564     }
1565 
1566     return result;
1567 }
1568 
1569 auto AirIRGenerator::load(LoadOpType op, ExpressionType pointer, ExpressionType&amp; result, uint32_t offset) -&gt; PartialResult
1570 {
1571     ASSERT(pointer.tmp().isGP());
1572 
1573     if (UNLIKELY(sumOverflows&lt;uint32_t&gt;(offset, sizeOfLoadOp(op)))) {
1574         // FIXME: Even though this is provably out of bounds, it&#39;s not a validation error, so we have to handle it
1575         // as a runtime exception. However, this may change: https://bugs.webkit.org/show_bug.cgi?id=166435
1576         auto* patch = addPatchpoint(B3::Void);
1577         patch-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1578             this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1579         });
1580         emitPatchpoint(patch, Tmp());
1581 
1582         // We won&#39;t reach here, so we just pick a random reg.
1583         switch (op) {
1584         case LoadOpType::I32Load8S:
1585         case LoadOpType::I32Load16S:
1586         case LoadOpType::I32Load:
1587         case LoadOpType::I32Load16U:
1588         case LoadOpType::I32Load8U:
1589             result = g32();
1590             break;
1591         case LoadOpType::I64Load8S:
1592         case LoadOpType::I64Load8U:
1593         case LoadOpType::I64Load16S:
1594         case LoadOpType::I64Load32U:
1595         case LoadOpType::I64Load32S:
1596         case LoadOpType::I64Load:
1597         case LoadOpType::I64Load16U:
1598             result = g64();
1599             break;
1600         case LoadOpType::F32Load:
1601             result = f32();
1602             break;
1603         case LoadOpType::F64Load:
1604             result = f64();
1605             break;
1606         }
1607     } else
1608         result = emitLoadOp(op, emitCheckAndPreparePointer(pointer, offset, sizeOfLoadOp(op)), offset);
1609 
1610     return { };
1611 }
1612 
1613 inline uint32_t sizeOfStoreOp(StoreOpType op)
1614 {
1615     switch (op) {
1616     case StoreOpType::I32Store8:
1617     case StoreOpType::I64Store8:
1618         return 1;
1619     case StoreOpType::I32Store16:
1620     case StoreOpType::I64Store16:
1621         return 2;
1622     case StoreOpType::I32Store:
1623     case StoreOpType::I64Store32:
1624     case StoreOpType::F32Store:
1625         return 4;
1626     case StoreOpType::I64Store:
1627     case StoreOpType::F64Store:
1628         return 8;
1629     }
1630     RELEASE_ASSERT_NOT_REACHED();
1631 }
1632 
1633 
1634 inline void AirIRGenerator::emitStoreOp(StoreOpType op, ExpressionType pointer, ExpressionType value, uint32_t uoffset)
1635 {
1636     uint32_t offset = fixupPointerPlusOffset(pointer, uoffset);
1637 
1638     TypedTmp immTmp;
1639     TypedTmp newPtr;
1640 
1641     Arg addrArg;
1642     if (Arg::isValidAddrForm(offset, B3::widthForBytes(sizeOfStoreOp(op))))
1643         addrArg = Arg::addr(pointer, offset);
1644     else {
1645         immTmp = g64();
1646         newPtr = g64();
1647         append(Move, Arg::bigImm(offset), immTmp);
1648         append(Add64, immTmp, pointer, newPtr);
1649         addrArg = Arg::addr(newPtr);
1650     }
1651 
1652     switch (op) {
1653     case StoreOpType::I64Store8:
1654     case StoreOpType::I32Store8:
1655         append(Store8, value, addrArg);
1656         return;
1657 
1658     case StoreOpType::I64Store16:
1659     case StoreOpType::I32Store16:
1660         append(Store16, value, addrArg);
1661         return;
1662 
1663     case StoreOpType::I64Store32:
1664     case StoreOpType::I32Store:
1665         append(Move32, value, addrArg);
1666         return;
1667 
1668     case StoreOpType::I64Store:
1669         append(Move, value, addrArg);
1670         return;
1671 
1672     case StoreOpType::F32Store:
1673         append(MoveFloat, value, addrArg);
1674         return;
1675 
1676     case StoreOpType::F64Store:
1677         append(MoveDouble, value, addrArg);
1678         return;
1679     }
1680 
1681     RELEASE_ASSERT_NOT_REACHED();
1682 }
1683 
1684 auto AirIRGenerator::store(StoreOpType op, ExpressionType pointer, ExpressionType value, uint32_t offset) -&gt; PartialResult
1685 {
1686     ASSERT(pointer.tmp().isGP());
1687 
1688     if (UNLIKELY(sumOverflows&lt;uint32_t&gt;(offset, sizeOfStoreOp(op)))) {
1689         // FIXME: Even though this is provably out of bounds, it&#39;s not a validation error, so we have to handle it
1690         // as a runtime exception. However, this may change: https://bugs.webkit.org/show_bug.cgi?id=166435
1691         auto* throwException = addPatchpoint(B3::Void);
1692         throwException-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1693             this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1694         });
1695         emitPatchpoint(throwException, Tmp());
1696     } else
1697         emitStoreOp(op, emitCheckAndPreparePointer(pointer, offset, sizeOfStoreOp(op)), value, offset);
1698 
1699     return { };
1700 }
1701 
1702 auto AirIRGenerator::addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType&amp; result) -&gt; PartialResult
1703 {
1704     ASSERT(nonZero.type() == zero.type());
1705     result = tmpForType(nonZero.type());
1706     append(moveOpForValueType(nonZero.type()), nonZero, result);
1707 
1708     BasicBlock* isZero = m_code.addBlock();
1709     BasicBlock* continuation = m_code.addBlock();
1710 
1711     append(BranchTest32, Arg::resCond(MacroAssembler::Zero), condition, condition);
1712     m_currentBlock-&gt;setSuccessors(isZero, continuation);
1713 
1714     append(isZero, moveOpForValueType(zero.type()), zero, result);
1715     append(isZero, Jump);
1716     isZero-&gt;setSuccessors(continuation);
1717 
1718     m_currentBlock = continuation;
1719 
1720     return { };
1721 }
1722 
1723 void AirIRGenerator::emitEntryTierUpCheck()
1724 {
1725     if (!m_tierUp)
1726         return;
1727 
1728     auto countdownPtr = g64();
1729 
1730     append(Move, Arg::bigImm(reinterpret_cast&lt;uint64_t&gt;(&amp;m_tierUp-&gt;m_counter)), countdownPtr);
1731 
1732     auto* patch = addPatchpoint(B3::Void);
1733     B3::Effects effects = B3::Effects::none();
1734     effects.reads = B3::HeapRange::top();
1735     effects.writes = B3::HeapRange::top();
1736     patch-&gt;effects = effects;
1737     patch-&gt;clobber(RegisterSet::macroScratchRegisters());
1738 
1739     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1740         AllowMacroScratchRegisterUsage allowScratch(jit);
1741 
1742         CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(TierUpCount::functionEntryIncrement()), CCallHelpers::Address(params[0].gpr()));
1743         CCallHelpers::Label tierUpResume = jit.label();
1744 
1745         params.addLatePath([=] (CCallHelpers&amp; jit) {
1746             tierUp.link(&amp;jit);
1747 
1748             const unsigned extraPaddingBytes = 0;
1749             RegisterSet registersToSpill = { };
1750             registersToSpill.add(GPRInfo::argumentGPR1);
1751             unsigned numberOfStackBytesUsedForRegisterPreservation = ScratchRegisterAllocator::preserveRegistersToStackForCall(jit, registersToSpill, extraPaddingBytes);
1752 
1753             jit.move(MacroAssembler::TrustedImm32(m_functionIndex), GPRInfo::argumentGPR1);
1754             MacroAssembler::Call call = jit.nearCall();
1755 
1756             ScratchRegisterAllocator::restoreRegistersFromStackForCall(jit, registersToSpill, RegisterSet(), numberOfStackBytesUsedForRegisterPreservation, extraPaddingBytes);
1757             jit.jump(tierUpResume);
1758 
1759             jit.addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
1760                 MacroAssembler::repatchNearCall(linkBuffer.locationOfNearCall&lt;NoPtrTag&gt;(call), CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(triggerOMGEntryTierUpThunkGenerator).code()));
1761             });
1762         });
1763     });
1764 
1765     emitPatchpoint(patch, Tmp(), countdownPtr);
1766 }
1767 
1768 void AirIRGenerator::emitLoopTierUpCheck(uint32_t loopIndex, const Stack&amp; enclosingStack)
1769 {
1770     uint32_t outerLoopIndex = this-&gt;outerLoopIndex();
1771     m_outerLoops.append(loopIndex);
1772 
1773     if (!m_tierUp)
1774         return;
1775 
1776     ASSERT(m_tierUp-&gt;osrEntryTriggers().size() == loopIndex);
1777     m_tierUp-&gt;osrEntryTriggers().append(TierUpCount::TriggerReason::DontTrigger);
1778     m_tierUp-&gt;outerLoops().append(outerLoopIndex);
1779 
1780     auto countdownPtr = g64();
1781 
1782     append(Move, Arg::bigImm(reinterpret_cast&lt;uint64_t&gt;(&amp;m_tierUp-&gt;m_counter)), countdownPtr);
1783 
1784     auto* patch = addPatchpoint(B3::Void);
1785     B3::Effects effects = B3::Effects::none();
1786     effects.reads = B3::HeapRange::top();
1787     effects.writes = B3::HeapRange::top();
1788     effects.exitsSideways = true;
1789     patch-&gt;effects = effects;
1790 
1791     patch-&gt;clobber(RegisterSet::macroScratchRegisters());
1792     RegisterSet clobberLate;
1793     clobberLate.add(GPRInfo::argumentGPR0);
1794     patch-&gt;clobberLate(clobberLate);
1795 
1796     Vector&lt;ConstrainedTmp&gt; patchArgs;
1797     patchArgs.append(countdownPtr);
1798 
1799     for (auto&amp; local : m_locals)
1800         patchArgs.append(ConstrainedTmp(local, B3::ValueRep::ColdAny));
1801     for (unsigned controlIndex = 0; controlIndex &lt; m_parser-&gt;controlStack().size(); ++controlIndex) {
1802         Stack&amp; expressionStack = m_parser-&gt;controlStack()[controlIndex].enclosedExpressionStack;
1803         for (TypedExpression value : expressionStack)
1804             patchArgs.append(ConstrainedTmp(value.value(), B3::ValueRep::ColdAny));
1805     }
1806     for (TypedExpression value : enclosingStack)
1807         patchArgs.append(ConstrainedTmp(value.value(), B3::ValueRep::ColdAny));
1808 
1809     TierUpCount::TriggerReason* forceEntryTrigger = &amp;(m_tierUp-&gt;osrEntryTriggers().last());
1810     static_assert(!static_cast&lt;uint8_t&gt;(TierUpCount::TriggerReason::DontTrigger), &quot;the JIT code assumes non-zero means &#39;enter&#39;&quot;);
1811     static_assert(sizeof(TierUpCount::TriggerReason) == 1, &quot;branchTest8 assumes this size&quot;);
1812     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1813         AllowMacroScratchRegisterUsage allowScratch(jit);
1814         CCallHelpers::Jump forceOSREntry = jit.branchTest8(CCallHelpers::NonZero, CCallHelpers::AbsoluteAddress(forceEntryTrigger));
1815         CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(TierUpCount::loopIncrement()), CCallHelpers::Address(params[0].gpr()));
1816         MacroAssembler::Label tierUpResume = jit.label();
1817 
1818         OSREntryData&amp; osrEntryData = m_tierUp-&gt;addOSREntryData(m_functionIndex, loopIndex);
1819         // First argument is the countdown location.
1820         for (unsigned index = 1; index &lt; params.value()-&gt;numChildren(); ++index)
1821             osrEntryData.values().constructAndAppend(params[index], params.value()-&gt;child(index)-&gt;type());
1822         OSREntryData* osrEntryDataPtr = &amp;osrEntryData;
1823 
1824         params.addLatePath([=] (CCallHelpers&amp; jit) {
1825             AllowMacroScratchRegisterUsage allowScratch(jit);
1826             forceOSREntry.link(&amp;jit);
1827             tierUp.link(&amp;jit);
1828 
1829             jit.probe(operationWasmTriggerOSREntryNow, osrEntryDataPtr);
1830             jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::argumentGPR0).linkTo(tierUpResume, &amp;jit);
1831             jit.farJump(GPRInfo::argumentGPR1, WasmEntryPtrTag);
1832         });
1833     });
1834 
1835     emitPatchpoint(m_currentBlock, patch, ResultList { }, WTFMove(patchArgs));
1836 }
1837 
1838 AirIRGenerator::ControlData AirIRGenerator::addTopLevel(BlockSignature signature)
1839 {
1840     return ControlData(B3::Origin(), signature, tmpsForSignature(signature), BlockType::TopLevel, m_code.addBlock());
1841 }
1842 
1843 auto AirIRGenerator::addLoop(BlockSignature signature, Stack&amp; enclosingStack, ControlType&amp; block, Stack&amp; newStack, uint32_t loopIndex) -&gt; PartialResult
1844 {
1845     BasicBlock* body = m_code.addBlock();
1846     BasicBlock* continuation = m_code.addBlock();
1847 
1848     splitStack(signature, enclosingStack, newStack);
1849     ResultList results;
1850     results.reserveInitialCapacity(newStack.size());
1851     for (auto item : newStack)
1852         results.uncheckedAppend(item);
1853     block = ControlData(origin(), signature, WTFMove(results), BlockType::Loop, continuation, body);
1854 
1855     append(Jump);
1856     m_currentBlock-&gt;setSuccessors(body);
1857 
1858     m_currentBlock = body;
1859     emitLoopTierUpCheck(loopIndex, enclosingStack);
1860 
1861     return { };
1862 }
1863 
1864 auto AirIRGenerator::addBlock(BlockSignature signature, Stack&amp; enclosingStack, ControlType&amp; newBlock, Stack&amp; newStack) -&gt; PartialResult
1865 {
1866     splitStack(signature, enclosingStack, newStack);
1867     newBlock = ControlData(origin(), signature, tmpsForSignature(signature), BlockType::Block, m_code.addBlock());
1868     return { };
1869 }
1870 
1871 auto AirIRGenerator::addIf(ExpressionType condition, BlockSignature signature, Stack&amp; enclosingStack, ControlType&amp; result, Stack&amp; newStack) -&gt; PartialResult
1872 {
1873     BasicBlock* taken = m_code.addBlock();
1874     BasicBlock* notTaken = m_code.addBlock();
1875     BasicBlock* continuation = m_code.addBlock();
1876 
1877     // Wasm bools are i32.
1878     append(BranchTest32, Arg::resCond(MacroAssembler::NonZero), condition, condition);
1879     m_currentBlock-&gt;setSuccessors(taken, notTaken);
1880 
1881     m_currentBlock = taken;
1882     splitStack(signature, enclosingStack, newStack);
1883     result = ControlData(origin(), signature, tmpsForSignature(signature), BlockType::If, continuation, notTaken);
1884     return { };
1885 }
1886 
1887 auto AirIRGenerator::addElse(ControlData&amp; data, const Stack&amp; currentStack) -&gt; PartialResult
1888 {
1889     unifyValuesWithBlock(currentStack, data.results);
1890     append(Jump);
1891     m_currentBlock-&gt;setSuccessors(data.continuation);
1892     return addElseToUnreachable(data);
1893 }
1894 
1895 auto AirIRGenerator::addElseToUnreachable(ControlData&amp; data) -&gt; PartialResult
1896 {
1897     ASSERT(data.blockType() == BlockType::If);
1898     m_currentBlock = data.special;
1899     data.convertIfToBlock();
1900     return { };
1901 }
1902 
1903 auto AirIRGenerator::addReturn(const ControlData&amp; data, const Stack&amp; returnValues) -&gt; PartialResult
1904 {
1905     CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(*data.signature(), CallRole::Callee);
1906     if (!wasmCallInfo.results.size()) {
1907         append(RetVoid);
1908         return { };
1909     }
1910 
1911     B3::PatchpointValue* patch = addPatchpoint(B3::Void);
1912     patch-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1913         auto calleeSaves = params.code().calleeSaveRegisterAtOffsetList();
1914 
1915         for (RegisterAtOffset calleeSave : calleeSaves)
1916             jit.load64ToReg(CCallHelpers::Address(GPRInfo::callFrameRegister, calleeSave.offset()), calleeSave.reg());
1917 
1918         jit.emitFunctionEpilogue();
1919         jit.ret();
1920     });
1921     patch-&gt;effects.terminal = true;
1922 
1923     ASSERT(returnValues.size() &gt;= wasmCallInfo.results.size());
1924     unsigned offset = returnValues.size() - wasmCallInfo.results.size();
1925     Vector&lt;ConstrainedTmp, 8&gt; returnConstraints;
1926     for (unsigned i = 0; i &lt; wasmCallInfo.results.size(); ++i) {
1927         B3::ValueRep rep = wasmCallInfo.results[i];
1928         TypedTmp tmp = returnValues[offset + i];
1929 
1930         if (rep.isStack()) {
1931             append(moveForType(toB3Type(tmp.type())), tmp, Arg::addr(Tmp(GPRInfo::callFrameRegister), rep.offsetFromFP()));
1932             continue;
1933         }
1934 
1935         ASSERT(rep.isReg());
1936         if (data.signature()-&gt;returnType(i) == I32)
1937             append(Move32, tmp, tmp);
1938         returnConstraints.append(ConstrainedTmp(tmp, wasmCallInfo.results[i]));
1939     }
1940 
1941     emitPatchpoint(m_currentBlock, patch, ResultList { }, WTFMove(returnConstraints));
1942     return { };
1943 }
1944 
1945 // NOTE: All branches in Wasm are on 32-bit ints
1946 
1947 auto AirIRGenerator::addBranch(ControlData&amp; data, ExpressionType condition, const Stack&amp; returnValues) -&gt; PartialResult
1948 {
1949     unifyValuesWithBlock(returnValues, data.results);
1950 
1951     BasicBlock* target = data.targetBlockForBranch();
1952     if (condition) {
1953         BasicBlock* continuation = m_code.addBlock();
1954         append(BranchTest32, Arg::resCond(MacroAssembler::NonZero), condition, condition);
1955         m_currentBlock-&gt;setSuccessors(target, continuation);
1956         m_currentBlock = continuation;
1957     } else {
1958         append(Jump);
1959         m_currentBlock-&gt;setSuccessors(target);
1960     }
1961 
1962     return { };
1963 }
1964 
1965 auto AirIRGenerator::addSwitch(ExpressionType condition, const Vector&lt;ControlData*&gt;&amp; targets, ControlData&amp; defaultTarget, const Stack&amp; expressionStack) -&gt; PartialResult
1966 {
1967     auto&amp; successors = m_currentBlock-&gt;successors();
1968     ASSERT(successors.isEmpty());
1969     for (const auto&amp; target : targets) {
1970         unifyValuesWithBlock(expressionStack, target-&gt;results);
1971         successors.append(target-&gt;targetBlockForBranch());
1972     }
1973     unifyValuesWithBlock(expressionStack, defaultTarget.results);
1974     successors.append(defaultTarget.targetBlockForBranch());
1975 
1976     ASSERT(condition.type() == Type::I32);
1977 
1978     // FIXME: We should consider dynamically switching between a jump table
1979     // and a binary switch depending on the number of successors.
1980     // https://bugs.webkit.org/show_bug.cgi?id=194477
1981 
1982     size_t numTargets = targets.size();
1983 
1984     auto* patchpoint = addPatchpoint(B3::Void);
1985     patchpoint-&gt;effects = B3::Effects::none();
1986     patchpoint-&gt;effects.terminal = true;
1987     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
1988 
1989     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1990         AllowMacroScratchRegisterUsage allowScratch(jit);
1991 
1992         Vector&lt;int64_t&gt; cases;
1993         cases.reserveInitialCapacity(numTargets);
1994         for (size_t i = 0; i &lt; numTargets; ++i)
1995             cases.uncheckedAppend(i);
1996 
1997         GPRReg valueReg = params[0].gpr();
1998         BinarySwitch binarySwitch(valueReg, cases, BinarySwitch::Int32);
1999 
2000         Vector&lt;CCallHelpers::Jump&gt; caseJumps;
2001         caseJumps.resize(numTargets);
2002 
2003         while (binarySwitch.advance(jit)) {
2004             unsigned value = binarySwitch.caseValue();
2005             unsigned index = binarySwitch.caseIndex();
2006             ASSERT_UNUSED(value, value == index);
2007             ASSERT(index &lt; numTargets);
2008             caseJumps[index] = jit.jump();
2009         }
2010 
2011         CCallHelpers::JumpList fallThrough = binarySwitch.fallThrough();
2012 
2013         Vector&lt;Box&lt;CCallHelpers::Label&gt;&gt; successorLabels = params.successorLabels();
2014         ASSERT(successorLabels.size() == caseJumps.size() + 1);
2015 
2016         params.addLatePath([=, caseJumps = WTFMove(caseJumps), successorLabels = WTFMove(successorLabels)] (CCallHelpers&amp; jit) {
2017             for (size_t i = 0; i &lt; numTargets; ++i)
2018                 caseJumps[i].linkTo(*successorLabels[i], &amp;jit);
2019             fallThrough.linkTo(*successorLabels[numTargets], &amp;jit);
2020         });
2021     });
2022 
2023     emitPatchpoint(patchpoint, TypedTmp(), condition);
2024 
2025     return { };
2026 }
2027 
2028 auto AirIRGenerator::endBlock(ControlEntry&amp; entry, Stack&amp; expressionStack) -&gt; PartialResult
2029 {
2030     ControlData&amp; data = entry.controlData;
2031 
2032     if (data.blockType() != BlockType::Loop)
2033         unifyValuesWithBlock(expressionStack, data.results);
2034     append(Jump);
2035     m_currentBlock-&gt;setSuccessors(data.continuation);
2036 
2037     return addEndToUnreachable(entry, expressionStack);
2038 }
2039 
2040 
2041 auto AirIRGenerator::addEndToUnreachable(ControlEntry&amp; entry, const Stack&amp; expressionStack) -&gt; PartialResult
2042 {
2043     ControlData&amp; data = entry.controlData;
2044     m_currentBlock = data.continuation;
2045 
2046     if (data.blockType() == BlockType::If) {
2047         append(data.special, Jump);
2048         data.special-&gt;setSuccessors(m_currentBlock);
2049     }
2050 
2051     if (data.blockType() == BlockType::Loop) {
2052         m_outerLoops.removeLast();
2053         for (unsigned i = 0; i &lt; data.signature()-&gt;returnCount(); ++i) {
2054             if (i &lt; expressionStack.size())
2055                 entry.enclosedExpressionStack.append(expressionStack[i]);
2056             else {
2057                 Type type = data.signature()-&gt;returnType(i);
2058                 entry.enclosedExpressionStack.constructAndAppend(type, addBottom(m_currentBlock, type));
2059             }
2060         }
2061     } else {
2062         for (unsigned i = 0; i &lt; data.signature()-&gt;returnCount(); ++i)
2063             entry.enclosedExpressionStack.constructAndAppend(data.signature()-&gt;returnType(i), data.results[i]);
2064     }
2065 
2066     // TopLevel does not have any code after this so we need to make sure we emit a return here.
2067     if (data.blockType() == BlockType::TopLevel)
2068         return addReturn(data, entry.enclosedExpressionStack);
2069 
2070     return { };
2071 }
2072 
2073 B3::PatchpointValue* AirIRGenerator::emitCallPatchpoint(BasicBlock* block, const Signature&amp; signature, const ResultList&amp; results, const Vector&lt;TypedTmp&gt;&amp; args, Vector&lt;ConstrainedTmp&gt;&amp;&amp; patchArgs)
2074 {
2075     auto* patchpoint = addPatchpoint(toB3ResultType(&amp;signature));
2076     patchpoint-&gt;effects.writesPinned = true;
2077     patchpoint-&gt;effects.readsPinned = true;
2078     patchpoint-&gt;clobberEarly(RegisterSet::macroScratchRegisters());
2079     patchpoint-&gt;clobberLate(RegisterSet::volatileRegistersForJSCall());
2080 
2081     CallInformation locations = wasmCallingConvention().callInformationFor(signature);
2082     m_code.requestCallArgAreaSizeInBytes(WTF::roundUpToMultipleOf(stackAlignmentBytes(), locations.headerAndArgumentStackSizeInBytes));
2083 
2084     size_t offset = patchArgs.size();
2085     Checked&lt;size_t&gt; newSize = checkedSum&lt;size_t&gt;(patchArgs.size(), args.size());
2086     RELEASE_ASSERT(!newSize.hasOverflowed());
2087 
2088     patchArgs.grow(newSize.unsafeGet());
2089     for (unsigned i = 0; i &lt; args.size(); ++i)
2090         patchArgs[i + offset] = ConstrainedTmp(args[i], locations.params[i]);
2091 
2092     if (patchpoint-&gt;type() != B3::Void)
2093         patchpoint-&gt;resultConstraints = WTFMove(locations.results);
2094     emitPatchpoint(block, patchpoint, results, WTFMove(patchArgs));
2095     return patchpoint;
2096 }
2097 
2098 auto AirIRGenerator::addCall(uint32_t functionIndex, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results) -&gt; PartialResult
2099 {
2100     ASSERT(signature.argumentCount() == args.size());
2101 
2102     m_makesCalls = true;
2103 
2104     for (unsigned i = 0; i &lt; signature.returnCount(); ++i)
2105         results.append(tmpForType(signature.returnType(i)));
2106 
2107     Vector&lt;UnlinkedWasmToWasmCall&gt;* unlinkedWasmToWasmCalls = &amp;m_unlinkedWasmToWasmCalls;
2108 
2109     if (m_info.isImportedFunctionFromFunctionIndexSpace(functionIndex)) {
2110         m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast&lt;uint32_t&gt;(args.size()));
2111 
2112         auto currentInstance = g64();
2113         append(Move, instanceValue(), currentInstance);
2114 
2115         auto targetInstance = g64();
2116 
2117         // FIXME: We should have better isel here.
2118         // https://bugs.webkit.org/show_bug.cgi?id=193999
2119         append(Move, Arg::bigImm(Instance::offsetOfTargetInstance(functionIndex)), targetInstance);
2120         append(Add64, instanceValue(), targetInstance);
2121         append(Move, Arg::addr(targetInstance), targetInstance);
2122 
2123         BasicBlock* isWasmBlock = m_code.addBlock();
2124         BasicBlock* isEmbedderBlock = m_code.addBlock();
2125         BasicBlock* continuation = m_code.addBlock();
2126 
2127         append(BranchTest64, Arg::resCond(MacroAssembler::NonZero), targetInstance, targetInstance);
2128         m_currentBlock-&gt;setSuccessors(isWasmBlock, isEmbedderBlock);
2129 
2130         {
2131             auto* patchpoint = emitCallPatchpoint(isWasmBlock, signature, results, args);
2132             // We need to clobber all potential pinned registers since we might be leaving the instance.
2133             // We pessimistically assume we could be calling to something that is bounds checking.
2134             // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
2135             patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2136 
2137             patchpoint-&gt;setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2138                 AllowMacroScratchRegisterUsage allowScratch(jit);
2139                 CCallHelpers::Call call = jit.threadSafePatchableNearCall();
2140                 jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer&amp; linkBuffer) {
2141                     unlinkedWasmToWasmCalls-&gt;append({ linkBuffer.locationOfNearCall&lt;WasmEntryPtrTag&gt;(call), functionIndex });
2142                 });
2143             });
2144 
2145             append(isWasmBlock, Jump);
2146             isWasmBlock-&gt;setSuccessors(continuation);
2147         }
2148 
2149         {
2150             auto jumpDestination = g64();
2151             append(isEmbedderBlock, Move, Arg::bigImm(Instance::offsetOfWasmToEmbedderStub(functionIndex)), jumpDestination);
2152             append(isEmbedderBlock, Add64, instanceValue(), jumpDestination);
2153             append(isEmbedderBlock, Move, Arg::addr(jumpDestination), jumpDestination);
2154 
2155             Vector&lt;ConstrainedTmp&gt; jumpArgs;
2156             jumpArgs.append({ jumpDestination, B3::ValueRep::SomeRegister });
2157             auto* patchpoint = emitCallPatchpoint(isEmbedderBlock, signature, results, args, WTFMove(jumpArgs));
2158             // We need to clobber all potential pinned registers since we might be leaving the instance.
2159             // We pessimistically assume we could be calling to something that is bounds checking.
2160             // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
2161             patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2162             patchpoint-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2163                 AllowMacroScratchRegisterUsage allowScratch(jit);
2164                 jit.call(params[params.proc().resultCount(params.value()-&gt;type())].gpr(), WasmEntryPtrTag);
2165             });
2166 
2167             append(isEmbedderBlock, Jump);
2168             isEmbedderBlock-&gt;setSuccessors(continuation);
2169         }
2170 
2171         m_currentBlock = continuation;
2172         // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
2173         restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, currentInstance, continuation);
2174     } else {
2175         auto* patchpoint = emitCallPatchpoint(m_currentBlock, signature, results, args);
2176         // We need to clobber the size register since the LLInt always bounds checks
2177         if (m_mode == MemoryMode::Signaling)
2178             patchpoint-&gt;clobberLate(RegisterSet { PinnedRegisterInfo::get().sizeRegister });
2179         patchpoint-&gt;setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2180             AllowMacroScratchRegisterUsage allowScratch(jit);
2181             CCallHelpers::Call call = jit.threadSafePatchableNearCall();
2182             jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer&amp; linkBuffer) {
2183                 unlinkedWasmToWasmCalls-&gt;append({ linkBuffer.locationOfNearCall&lt;WasmEntryPtrTag&gt;(call), functionIndex });
2184             });
2185         });
2186     }
2187 
2188     return { };
2189 }
2190 
2191 auto AirIRGenerator::addCallIndirect(unsigned tableIndex, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results) -&gt; PartialResult
2192 {
2193     ExpressionType calleeIndex = args.takeLast();
2194     ASSERT(signature.argumentCount() == args.size());
2195     ASSERT(m_info.tableCount() &gt; tableIndex);
2196     ASSERT(m_info.tables[tableIndex].type() == TableElementType::Funcref);
2197 
2198     m_makesCalls = true;
2199     // Note: call indirect can call either WebAssemblyFunction or WebAssemblyWrapperFunction. Because
2200     // WebAssemblyWrapperFunction is like calling into the embedder, we conservatively assume all call indirects
2201     // can be to the embedder for our stack check calculation.
2202     m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast&lt;uint32_t&gt;(args.size()));
2203 
2204     auto currentInstance = g64();
2205     append(Move, instanceValue(), currentInstance);
2206 
2207     ExpressionType callableFunctionBuffer = g64();
2208     ExpressionType instancesBuffer = g64();
2209     ExpressionType callableFunctionBufferLength = g64();
2210     {
2211         RELEASE_ASSERT(Arg::isValidAddrForm(FuncRefTable::offsetOfFunctions(), B3::Width64));
2212         RELEASE_ASSERT(Arg::isValidAddrForm(FuncRefTable::offsetOfInstances(), B3::Width64));
2213         RELEASE_ASSERT(Arg::isValidAddrForm(FuncRefTable::offsetOfLength(), B3::Width64));
2214 
2215         if (UNLIKELY(!Arg::isValidAddrForm(Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex), B3::Width64))) {
2216             append(Move, Arg::bigImm(Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex)), callableFunctionBufferLength);
2217             append(Add64, instanceValue(), callableFunctionBufferLength);
2218             append(Move, Arg::addr(callableFunctionBufferLength), callableFunctionBufferLength);
2219         } else
2220             append(Move, Arg::addr(instanceValue(), Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex)), callableFunctionBufferLength);
2221         append(Move, Arg::addr(callableFunctionBufferLength, FuncRefTable::offsetOfFunctions()), callableFunctionBuffer);
2222         append(Move, Arg::addr(callableFunctionBufferLength, FuncRefTable::offsetOfInstances()), instancesBuffer);
2223         append(Move32, Arg::addr(callableFunctionBufferLength, Table::offsetOfLength()), callableFunctionBufferLength);
2224     }
2225 
2226     append(Move32, calleeIndex, calleeIndex);
2227 
2228     // Check the index we are looking for is valid.
2229     emitCheck([&amp;] {
2230         return Inst(Branch32, nullptr, Arg::relCond(MacroAssembler::AboveOrEqual), calleeIndex, callableFunctionBufferLength);
2231     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2232         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsCallIndirect);
2233     });
2234 
2235     ExpressionType calleeCode = g64();
2236     {
2237         ExpressionType calleeSignatureIndex = g64();
2238         // Compute the offset in the table index space we are looking for.
2239         append(Move, Arg::imm(sizeof(WasmToWasmImportableFunction)), calleeSignatureIndex);
2240         append(Mul64, calleeIndex, calleeSignatureIndex);
2241         append(Add64, callableFunctionBuffer, calleeSignatureIndex);
2242 
2243         append(Move, Arg::addr(calleeSignatureIndex, WasmToWasmImportableFunction::offsetOfEntrypointLoadLocation()), calleeCode); // Pointer to callee code.
2244 
2245         // Check that the WasmToWasmImportableFunction is initialized. We trap if it isn&#39;t. An &quot;invalid&quot; SignatureIndex indicates it&#39;s not initialized.
2246         // FIXME: when we have trap handlers, we can just let the call fail because Signature::invalidIndex is 0. https://bugs.webkit.org/show_bug.cgi?id=177210
2247         static_assert(sizeof(WasmToWasmImportableFunction::signatureIndex) == sizeof(uint64_t), &quot;Load codegen assumes i64&quot;);
2248 
2249         // FIXME: This seems dumb to do two checks just for a nicer error message.
2250         // We should move just to use a single branch and then figure out what
2251         // error to use in the exception handler.
2252 
2253         append(Move, Arg::addr(calleeSignatureIndex, WasmToWasmImportableFunction::offsetOfSignatureIndex()), calleeSignatureIndex);
2254 
2255         emitCheck([&amp;] {
2256             static_assert(Signature::invalidIndex == 0, &quot;&quot;);
2257             return Inst(BranchTest64, nullptr, Arg::resCond(MacroAssembler::Zero), calleeSignatureIndex, calleeSignatureIndex);
2258         }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2259             this-&gt;emitThrowException(jit, ExceptionType::NullTableEntry);
2260         });
2261 
2262         ExpressionType expectedSignatureIndex = g64();
2263         append(Move, Arg::bigImm(SignatureInformation::get(signature)), expectedSignatureIndex);
2264         emitCheck([&amp;] {
2265             return Inst(Branch64, nullptr, Arg::relCond(MacroAssembler::NotEqual), calleeSignatureIndex, expectedSignatureIndex);
2266         }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2267             this-&gt;emitThrowException(jit, ExceptionType::BadSignature);
2268         });
2269     }
2270 
2271     // Do a context switch if needed.
2272     {
2273         auto newContextInstance = g64();
2274         append(Move, Arg::index(instancesBuffer, calleeIndex, 8, 0), newContextInstance);
2275 
2276         BasicBlock* doContextSwitch = m_code.addBlock();
2277         BasicBlock* continuation = m_code.addBlock();
2278 
2279         append(Branch64, Arg::relCond(MacroAssembler::Equal), newContextInstance, instanceValue());
2280         m_currentBlock-&gt;setSuccessors(continuation, doContextSwitch);
2281 
2282         auto* patchpoint = addPatchpoint(B3::Void);
2283         patchpoint-&gt;effects.writesPinned = true;
2284         // We pessimistically assume we&#39;re calling something with BoundsChecking memory.
2285         // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
2286         patchpoint-&gt;clobber(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2287         patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2288         patchpoint-&gt;numGPScratchRegisters = Gigacage::isEnabled(Gigacage::Primitive) ? 1 : 0;
2289 
2290         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2291             AllowMacroScratchRegisterUsage allowScratch(jit);
2292             GPRReg newContextInstance = params[0].gpr();
2293             GPRReg oldContextInstance = params[1].gpr();
2294             const PinnedRegisterInfo&amp; pinnedRegs = PinnedRegisterInfo::get();
2295             GPRReg baseMemory = pinnedRegs.baseMemoryPointer;
2296             ASSERT(newContextInstance != baseMemory);
2297             jit.loadPtr(CCallHelpers::Address(oldContextInstance, Instance::offsetOfCachedStackLimit()), baseMemory);
2298             jit.storePtr(baseMemory, CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedStackLimit()));
2299             jit.storeWasmContextInstance(newContextInstance);
2300             // FIXME: We should support more than one memory size register
2301             //   see: https://bugs.webkit.org/show_bug.cgi?id=162952
2302             ASSERT(pinnedRegs.sizeRegister != newContextInstance);
2303             GPRReg scratchOrSize = Gigacage::isEnabled(Gigacage::Primitive) ? params.gpScratch(0) : pinnedRegs.sizeRegister;
2304 
2305             jit.loadPtr(CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedMemorySize()), pinnedRegs.sizeRegister); // Memory size.
2306             jit.loadPtr(CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedMemory()), baseMemory); // Memory::void*.
2307 
2308             jit.cageConditionally(Gigacage::Primitive, baseMemory, pinnedRegs.sizeRegister, scratchOrSize);
2309         });
2310 
2311         emitPatchpoint(doContextSwitch, patchpoint, Tmp(), newContextInstance, instanceValue());
2312         append(doContextSwitch, Jump);
2313         doContextSwitch-&gt;setSuccessors(continuation);
2314 
2315         m_currentBlock = continuation;
2316     }
2317 
2318     append(Move, Arg::addr(calleeCode), calleeCode);
2319 
2320     Vector&lt;ConstrainedTmp&gt; extraArgs;
2321     extraArgs.append(calleeCode);
2322 
2323     for (unsigned i = 0; i &lt; signature.returnCount(); ++i)
2324         results.append(tmpForType(signature.returnType(i)));
2325 
2326     auto* patchpoint = emitCallPatchpoint(m_currentBlock, signature, results, args, WTFMove(extraArgs));
2327 
2328     // We need to clobber all potential pinned registers since we might be leaving the instance.
2329     // We pessimistically assume we&#39;re always calling something that is bounds checking so
2330     // because the wasm-&gt;wasm thunk unconditionally overrides the size registers.
2331     // FIXME: We should not have to do this, but the wasm-&gt;wasm stub assumes it can
2332     // use all the pinned registers as scratch: https://bugs.webkit.org/show_bug.cgi?id=172181
2333 
2334     patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2335 
2336     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2337         AllowMacroScratchRegisterUsage allowScratch(jit);
2338         jit.call(params[params.proc().resultCount(params.value()-&gt;type())].gpr(), WasmEntryPtrTag);
2339     });
2340 
2341     // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
2342     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, currentInstance, m_currentBlock);
2343 
2344     return { };
2345 }
2346 
2347 void AirIRGenerator::unify(const ExpressionType dst, const ExpressionType source)
2348 {
2349     ASSERT(isSubtype(source.type(), dst.type()));
2350     append(moveOpForValueType(dst.type()), source, dst);
2351 }
2352 
2353 void AirIRGenerator::unifyValuesWithBlock(const Stack&amp; resultStack, const ResultList&amp; result)
2354 {
2355     ASSERT(result.size() &lt;= resultStack.size());
2356 
2357     for (size_t i = 0; i &lt; result.size(); ++i)
2358         unify(result[result.size() - 1 - i], resultStack[resultStack.size() - 1 - i]);
2359 }
2360 
2361 static void dumpExpressionStack(const CommaPrinter&amp; comma, const AirIRGenerator::Stack&amp; expressionStack)
2362 {
2363     dataLog(comma, &quot;ExpressionStack:&quot;);
2364     for (const auto&amp; expression : expressionStack)
2365         dataLog(comma, expression.value());
2366 }
2367 
2368 void AirIRGenerator::dump(const ControlStack&amp; controlStack, const Stack* stack)
2369 {
2370     dataLogLn(&quot;Processing Graph:&quot;);
2371     dataLog(m_code);
2372     dataLogLn(&quot;With current block:&quot;, *m_currentBlock);
2373     dataLogLn(&quot;Control stack:&quot;);
2374     for (size_t i = controlStack.size(); i--;) {
2375         dataLog(&quot;  &quot;, controlStack[i].controlData, &quot;: &quot;);
2376         CommaPrinter comma(&quot;, &quot;, &quot;&quot;);
2377         dumpExpressionStack(comma, *stack);
2378         stack = &amp;controlStack[i].enclosedExpressionStack;
2379         dataLogLn();
2380     }
2381     dataLogLn(&quot;\n&quot;);
2382 }
2383 
2384 auto AirIRGenerator::origin() -&gt; B3::Origin
2385 {
2386     // FIXME: We should implement a way to give Inst&#39;s an origin.
2387     return B3::Origin();
2388 }
2389 
2390 Expected&lt;std::unique_ptr&lt;InternalFunction&gt;, String&gt; parseAndCompileAir(CompilationContext&amp; compilationContext, const FunctionData&amp; function, const Signature&amp; signature, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; unlinkedWasmToWasmCalls, const ModuleInformation&amp; info, MemoryMode mode, uint32_t functionIndex, TierUpCount* tierUp)
2391 {
2392     auto result = makeUnique&lt;InternalFunction&gt;();
2393 
2394     compilationContext.embedderEntrypointJIT = makeUnique&lt;CCallHelpers&gt;();
2395     compilationContext.wasmEntrypointJIT = makeUnique&lt;CCallHelpers&gt;();
2396 
2397     B3::Procedure procedure;
2398     Code&amp; code = procedure.code();
2399 
2400     procedure.setOriginPrinter([] (PrintStream&amp; out, B3::Origin origin) {
2401         if (origin.data())
2402             out.print(&quot;Wasm: &quot;, bitwise_cast&lt;OpcodeOrigin&gt;(origin));
2403     });
2404 
2405     // This means we cannot use either StackmapGenerationParams::usedRegisters() or
2406     // StackmapGenerationParams::unavailableRegisters(). In exchange for this concession, we
2407     // don&#39;t strictly need to run Air::reportUsedRegisters(), which saves a bit of CPU time at
2408     // optLevel=1.
2409     procedure.setNeedsUsedRegisters(false);
2410 
2411     procedure.setOptLevel(Options::webAssemblyBBQAirOptimizationLevel());
2412 
2413     AirIRGenerator irGenerator(info, procedure, result.get(), unlinkedWasmToWasmCalls, mode, functionIndex, tierUp, signature);
2414     FunctionParser&lt;AirIRGenerator&gt; parser(irGenerator, function.data.data(), function.data.size(), signature, info);
2415     WASM_FAIL_IF_HELPER_FAILS(parser.parse());
2416 
2417 
2418     for (BasicBlock* block : code) {
2419         for (size_t i = 0; i &lt; block-&gt;numSuccessors(); ++i)
2420             block-&gt;successorBlock(i)-&gt;addPredecessor(block);
2421     }
2422 
2423     {
2424         if (UNLIKELY(shouldDumpIRAtEachPhase(B3::AirMode))) {
2425             dataLogLn(&quot;Generated patchpoints&quot;);
2426             for (B3::PatchpointValue** patch : irGenerator.patchpoints())
2427                 dataLogLn(deepDump(procedure, *patch));
2428         }
2429 
2430         B3::Air::prepareForGeneration(code);
2431         B3::Air::generate(code, *compilationContext.wasmEntrypointJIT);
2432         compilationContext.wasmEntrypointByproducts = procedure.releaseByproducts();
2433         result-&gt;entrypoint.calleeSaveRegisters = code.calleeSaveRegisterAtOffsetList();
2434     }
2435 
2436     return result;
2437 }
2438 
2439 template &lt;typename IntType&gt;
2440 void AirIRGenerator::emitChecksForModOrDiv(bool isSignedDiv, ExpressionType left, ExpressionType right)
2441 {
2442     static_assert(sizeof(IntType) == 4 || sizeof(IntType) == 8, &quot;&quot;);
2443 
2444     emitCheck([&amp;] {
2445         return Inst(sizeof(IntType) == 4 ? BranchTest32 : BranchTest64, nullptr, Arg::resCond(MacroAssembler::Zero), right, right);
2446     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2447         this-&gt;emitThrowException(jit, ExceptionType::DivisionByZero);
2448     });
2449 
2450     if (isSignedDiv) {
2451         ASSERT(std::is_signed&lt;IntType&gt;::value);
2452         IntType min = std::numeric_limits&lt;IntType&gt;::min();
2453 
2454         // FIXME: Better isel for compare with imms here.
2455         // https://bugs.webkit.org/show_bug.cgi?id=193999
2456         auto minTmp = sizeof(IntType) == 4 ? g32() : g64();
2457         auto negOne = sizeof(IntType) == 4 ? g32() : g64();
2458 
2459         B3::Air::Opcode op = sizeof(IntType) == 4 ? Compare32 : Compare64;
2460         append(Move, Arg::bigImm(static_cast&lt;uint64_t&gt;(min)), minTmp);
2461         append(op, Arg::relCond(MacroAssembler::Equal), left, minTmp, minTmp);
2462 
2463         append(Move, Arg::imm(-1), negOne);
2464         append(op, Arg::relCond(MacroAssembler::Equal), right, negOne, negOne);
2465 
2466         emitCheck([&amp;] {
2467             return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), minTmp, negOne);
2468         },
2469         [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2470             this-&gt;emitThrowException(jit, ExceptionType::IntegerOverflow);
2471         });
2472     }
2473 }
2474 
2475 template &lt;typename IntType&gt;
2476 void AirIRGenerator::emitModOrDiv(bool isDiv, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result)
2477 {
2478     static_assert(sizeof(IntType) == 4 || sizeof(IntType) == 8, &quot;&quot;);
2479 
2480     result = sizeof(IntType) == 4 ? g32() : g64();
2481 
2482     bool isSigned = std::is_signed&lt;IntType&gt;::value;
2483 
2484     if (isARM64()) {
2485         B3::Air::Opcode div;
2486         switch (sizeof(IntType)) {
2487         case 4:
2488             div = isSigned ? Div32 : UDiv32;
2489             break;
2490         case 8:
2491             div = isSigned ? Div64 : UDiv64;
2492             break;
2493         }
2494 
2495         append(div, lhs, rhs, result);
2496 
2497         if (!isDiv) {
2498             append(sizeof(IntType) == 4 ? Mul32 : Mul64, result, rhs, result);
2499             append(sizeof(IntType) == 4 ? Sub32 : Sub64, lhs, result, result);
2500         }
2501 
2502         return;
2503     }
2504 
2505 #if CPU(X86_64)
2506     Tmp eax(X86Registers::eax);
2507     Tmp edx(X86Registers::edx);
2508 
2509     if (isSigned) {
2510         B3::Air::Opcode convertToDoubleWord;
2511         B3::Air::Opcode div;
2512         switch (sizeof(IntType)) {
2513         case 4:
2514             convertToDoubleWord = X86ConvertToDoubleWord32;
2515             div = X86Div32;
2516             break;
2517         case 8:
2518             convertToDoubleWord = X86ConvertToQuadWord64;
2519             div = X86Div64;
2520             break;
2521         default:
2522             RELEASE_ASSERT_NOT_REACHED();
2523         }
2524 
2525         // We implement &quot;res = Div&lt;Chill&gt;/Mod&lt;Chill&gt;(num, den)&quot; as follows:
2526         //
2527         //     if (den + 1 &lt;=_unsigned 1) {
2528         //         if (!den) {
2529         //             res = 0;
2530         //             goto done;
2531         //         }
2532         //         if (num == -2147483648) {
2533         //             res = isDiv ? num : 0;
2534         //             goto done;
2535         //         }
2536         //     }
2537         //     res = num (/ or %) dev;
2538         // done:
2539 
2540         BasicBlock* denIsGood = m_code.addBlock();
2541         BasicBlock* denMayBeBad = m_code.addBlock();
2542         BasicBlock* denNotZero = m_code.addBlock();
2543         BasicBlock* continuation = m_code.addBlock();
2544 
2545         auto temp = sizeof(IntType) == 4 ? g32() : g64();
2546         auto one = addConstant(sizeof(IntType) == 4 ? Type::I32 : Type::I64, 1);
2547 
2548         append(sizeof(IntType) == 4 ? Add32 : Add64, rhs, one, temp);
2549         append(sizeof(IntType) == 4 ? Branch32 : Branch64, Arg::relCond(MacroAssembler::Above), temp, one);
2550         m_currentBlock-&gt;setSuccessors(denIsGood, denMayBeBad);
2551 
2552         append(denMayBeBad, Xor64, result, result);
2553         append(denMayBeBad, sizeof(IntType) == 4 ? BranchTest32 : BranchTest64, Arg::resCond(MacroAssembler::Zero), rhs, rhs);
2554         denMayBeBad-&gt;setSuccessors(continuation, denNotZero);
2555 
2556         auto min = addConstant(denNotZero, sizeof(IntType) == 4 ? Type::I32 : Type::I64, std::numeric_limits&lt;IntType&gt;::min());
2557         if (isDiv)
2558             append(denNotZero, sizeof(IntType) == 4 ? Move32 : Move, min, result);
2559         else {
2560             // Result is zero, as set above...
2561         }
2562         append(denNotZero, sizeof(IntType) == 4 ? Branch32 : Branch64, Arg::relCond(MacroAssembler::Equal), lhs, min);
2563         denNotZero-&gt;setSuccessors(continuation, denIsGood);
2564 
2565         auto divResult = isDiv ? eax : edx;
2566         append(denIsGood, Move, lhs, eax);
2567         append(denIsGood, convertToDoubleWord, eax, edx);
2568         append(denIsGood, div, eax, edx, rhs);
2569         append(denIsGood, sizeof(IntType) == 4 ? Move32 : Move, divResult, result);
2570         append(denIsGood, Jump);
2571         denIsGood-&gt;setSuccessors(continuation);
2572 
2573         m_currentBlock = continuation;
2574         return;
2575     }
2576 
2577     B3::Air::Opcode div = sizeof(IntType) == 4 ? X86UDiv32 : X86UDiv64;
2578 
2579     Tmp divResult = isDiv ? eax : edx;
2580 
2581     append(Move, lhs, eax);
2582     append(Xor64, edx, edx);
2583     append(div, eax, edx, rhs);
2584     append(sizeof(IntType) == 4 ? Move32 : Move, divResult, result);
2585 #else
2586     RELEASE_ASSERT_NOT_REACHED();
2587 #endif
2588 }
2589 
2590 template&lt;&gt;
2591 auto AirIRGenerator::addOp&lt;OpType::I32DivS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2592 {
2593     emitChecksForModOrDiv&lt;int32_t&gt;(true, left, right);
2594     emitModOrDiv&lt;int32_t&gt;(true, left, right, result);
2595     return { };
2596 }
2597 
2598 template&lt;&gt;
2599 auto AirIRGenerator::addOp&lt;OpType::I32RemS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2600 {
2601     emitChecksForModOrDiv&lt;int32_t&gt;(false, left, right);
2602     emitModOrDiv&lt;int32_t&gt;(false, left, right, result);
2603     return { };
2604 }
2605 
2606 template&lt;&gt;
2607 auto AirIRGenerator::addOp&lt;OpType::I32DivU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2608 {
2609     emitChecksForModOrDiv&lt;uint32_t&gt;(false, left, right);
2610     emitModOrDiv&lt;uint32_t&gt;(true, left, right, result);
2611     return { };
2612 }
2613 
2614 template&lt;&gt;
2615 auto AirIRGenerator::addOp&lt;OpType::I32RemU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2616 {
2617     emitChecksForModOrDiv&lt;uint32_t&gt;(false, left, right);
2618     emitModOrDiv&lt;uint32_t&gt;(false, left, right, result);
2619     return { };
2620 }
2621 
2622 template&lt;&gt;
2623 auto AirIRGenerator::addOp&lt;OpType::I64DivS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2624 {
2625     emitChecksForModOrDiv&lt;int64_t&gt;(true, left, right);
2626     emitModOrDiv&lt;int64_t&gt;(true, left, right, result);
2627     return { };
2628 }
2629 
2630 template&lt;&gt;
2631 auto AirIRGenerator::addOp&lt;OpType::I64RemS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2632 {
2633     emitChecksForModOrDiv&lt;int64_t&gt;(false, left, right);
2634     emitModOrDiv&lt;int64_t&gt;(false, left, right, result);
2635     return { };
2636 }
2637 
2638 template&lt;&gt;
2639 auto AirIRGenerator::addOp&lt;OpType::I64DivU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2640 {
2641     emitChecksForModOrDiv&lt;uint64_t&gt;(false, left, right);
2642     emitModOrDiv&lt;uint64_t&gt;(true, left, right, result);
2643     return { };
2644 }
2645 
2646 template&lt;&gt;
2647 auto AirIRGenerator::addOp&lt;OpType::I64RemU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2648 {
2649     emitChecksForModOrDiv&lt;uint64_t&gt;(false, left, right);
2650     emitModOrDiv&lt;uint64_t&gt;(false, left, right, result);
2651     return { };
2652 }
2653 
2654 template&lt;&gt;
2655 auto AirIRGenerator::addOp&lt;OpType::I32Ctz&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2656 {
2657     auto* patchpoint = addPatchpoint(B3::Int32);
2658     patchpoint-&gt;effects = B3::Effects::none();
2659     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2660         jit.countTrailingZeros32(params[1].gpr(), params[0].gpr());
2661     });
2662     result = g32();
2663     emitPatchpoint(patchpoint, result, arg);
2664     return { };
2665 }
2666 
2667 template&lt;&gt;
2668 auto AirIRGenerator::addOp&lt;OpType::I64Ctz&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2669 {
2670     auto* patchpoint = addPatchpoint(B3::Int64);
2671     patchpoint-&gt;effects = B3::Effects::none();
2672     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2673         jit.countTrailingZeros64(params[1].gpr(), params[0].gpr());
2674     });
2675     result = g64();
2676     emitPatchpoint(patchpoint, result, arg);
2677     return { };
2678 }
2679 
2680 template&lt;&gt;
2681 auto AirIRGenerator::addOp&lt;OpType::I32Popcnt&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2682 {
2683     result = g32();
2684 
2685 #if CPU(X86_64)
2686     if (MacroAssembler::supportsCountPopulation()) {
2687         auto* patchpoint = addPatchpoint(B3::Int32);
2688         patchpoint-&gt;effects = B3::Effects::none();
2689         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2690             jit.countPopulation32(params[1].gpr(), params[0].gpr());
2691         });
2692         emitPatchpoint(patchpoint, result, arg);
2693         return { };
2694     }
2695 #endif
2696 
2697     emitCCall(&amp;operationPopcount32, result, arg);
2698     return { };
2699 }
2700 
2701 template&lt;&gt;
2702 auto AirIRGenerator::addOp&lt;OpType::I64Popcnt&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2703 {
2704     result = g64();
2705 
2706 #if CPU(X86_64)
2707     if (MacroAssembler::supportsCountPopulation()) {
2708         auto* patchpoint = addPatchpoint(B3::Int64);
2709         patchpoint-&gt;effects = B3::Effects::none();
2710         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2711             jit.countPopulation64(params[1].gpr(), params[0].gpr());
2712         });
2713         emitPatchpoint(patchpoint, result, arg);
2714         return { };
2715     }
2716 #endif
2717 
2718     emitCCall(&amp;operationPopcount64, result, arg);
2719     return { };
2720 }
2721 
2722 template&lt;&gt;
2723 auto AirIRGenerator::addOp&lt;F64ConvertUI64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2724 {
2725     auto* patchpoint = addPatchpoint(B3::Double);
2726     patchpoint-&gt;effects = B3::Effects::none();
2727     if (isX86())
2728         patchpoint-&gt;numGPScratchRegisters = 1;
2729     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2730     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2731         AllowMacroScratchRegisterUsage allowScratch(jit);
2732 #if CPU(X86_64)
2733         jit.convertUInt64ToDouble(params[1].gpr(), params[0].fpr(), params.gpScratch(0));
2734 #else
2735         jit.convertUInt64ToDouble(params[1].gpr(), params[0].fpr());
2736 #endif
2737     });
2738     result = f64();
2739     emitPatchpoint(patchpoint, result, arg);
2740     return { };
2741 }
2742 
2743 template&lt;&gt;
2744 auto AirIRGenerator::addOp&lt;OpType::F32ConvertUI64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2745 {
2746     auto* patchpoint = addPatchpoint(B3::Float);
2747     patchpoint-&gt;effects = B3::Effects::none();
2748     if (isX86())
2749         patchpoint-&gt;numGPScratchRegisters = 1;
2750     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2751     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2752         AllowMacroScratchRegisterUsage allowScratch(jit);
2753 #if CPU(X86_64)
2754         jit.convertUInt64ToFloat(params[1].gpr(), params[0].fpr(), params.gpScratch(0));
2755 #else
2756         jit.convertUInt64ToFloat(params[1].gpr(), params[0].fpr());
2757 #endif
2758     });
2759     result = f32();
2760     emitPatchpoint(patchpoint, result, arg);
2761     return { };
2762 }
2763 
2764 template&lt;&gt;
2765 auto AirIRGenerator::addOp&lt;OpType::F64Nearest&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2766 {
2767     auto* patchpoint = addPatchpoint(B3::Double);
2768     patchpoint-&gt;effects = B3::Effects::none();
2769     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2770         jit.roundTowardNearestIntDouble(params[1].fpr(), params[0].fpr());
2771     });
2772     result = f64();
2773     emitPatchpoint(patchpoint, result, arg);
2774     return { };
2775 }
2776 
2777 template&lt;&gt;
2778 auto AirIRGenerator::addOp&lt;OpType::F32Nearest&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2779 {
2780     auto* patchpoint = addPatchpoint(B3::Float);
2781     patchpoint-&gt;effects = B3::Effects::none();
2782     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2783         jit.roundTowardNearestIntFloat(params[1].fpr(), params[0].fpr());
2784     });
2785     result = f32();
2786     emitPatchpoint(patchpoint, result, arg);
2787     return { };
2788 }
2789 
2790 template&lt;&gt;
2791 auto AirIRGenerator::addOp&lt;OpType::F64Trunc&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2792 {
2793     auto* patchpoint = addPatchpoint(B3::Double);
2794     patchpoint-&gt;effects = B3::Effects::none();
2795     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2796         jit.roundTowardZeroDouble(params[1].fpr(), params[0].fpr());
2797     });
2798     result = f64();
2799     emitPatchpoint(patchpoint, result, arg);
2800     return { };
2801 }
2802 
2803 template&lt;&gt;
2804 auto AirIRGenerator::addOp&lt;OpType::F32Trunc&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2805 {
2806     auto* patchpoint = addPatchpoint(B3::Float);
2807     patchpoint-&gt;effects = B3::Effects::none();
2808     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2809         jit.roundTowardZeroFloat(params[1].fpr(), params[0].fpr());
2810     });
2811     result = f32();
2812     emitPatchpoint(patchpoint, result, arg);
2813     return { };
2814 }
2815 
2816 template&lt;&gt;
2817 auto AirIRGenerator::addOp&lt;OpType::I32TruncSF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2818 {
2819     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2820     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2821 
2822     auto temp1 = g32();
2823     auto temp2 = g32();
2824     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2825     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2826     append(Or32, temp1, temp2);
2827 
2828     emitCheck([&amp;] {
2829         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2830     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2831         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2832     });
2833 
2834     auto* patchpoint = addPatchpoint(B3::Int32);
2835     patchpoint-&gt;effects = B3::Effects::none();
2836     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2837         jit.truncateDoubleToInt32(params[1].fpr(), params[0].gpr());
2838     });
2839     result = g32();
2840     emitPatchpoint(patchpoint, result, arg);
2841 
2842     return { };
2843 }
2844 
2845 template&lt;&gt;
2846 auto AirIRGenerator::addOp&lt;OpType::I32TruncSF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2847 {
2848     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(-static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2849     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2850 
2851     auto temp1 = g32();
2852     auto temp2 = g32();
2853     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2854     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2855     append(Or32, temp1, temp2);
2856 
2857     emitCheck([&amp;] {
2858         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2859     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2860         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2861     });
2862 
2863     auto* patchpoint = addPatchpoint(B3::Int32);
2864     patchpoint-&gt;effects = B3::Effects::none();
2865     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2866         jit.truncateFloatToInt32(params[1].fpr(), params[0].gpr());
2867     });
2868     result = g32();
2869     emitPatchpoint(patchpoint, result, arg);
2870     return { };
2871 }
2872 
2873 
2874 template&lt;&gt;
2875 auto AirIRGenerator::addOp&lt;OpType::I32TruncUF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2876 {
2877     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min()) * -2.0));
2878     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-1.0));
2879 
2880     auto temp1 = g32();
2881     auto temp2 = g32();
2882     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2883     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2884     append(Or32, temp1, temp2);
2885 
2886     emitCheck([&amp;] {
2887         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2888     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2889         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2890     });
2891 
2892     auto* patchpoint = addPatchpoint(B3::Int32);
2893     patchpoint-&gt;effects = B3::Effects::none();
2894     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2895         jit.truncateDoubleToUint32(params[1].fpr(), params[0].gpr());
2896     });
2897     result = g32();
2898     emitPatchpoint(patchpoint, result, arg);
2899     return { };
2900 }
2901 
2902 template&lt;&gt;
2903 auto AirIRGenerator::addOp&lt;OpType::I32TruncUF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2904 {
2905     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min()) * static_cast&lt;float&gt;(-2.0)));
2906     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-1.0)));
2907 
2908     auto temp1 = g32();
2909     auto temp2 = g32();
2910     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2911     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2912     append(Or32, temp1, temp2);
2913 
2914     emitCheck([&amp;] {
2915         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2916     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2917         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2918     });
2919 
2920     auto* patchpoint = addPatchpoint(B3::Int32);
2921     patchpoint-&gt;effects = B3::Effects::none();
2922     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2923         jit.truncateFloatToUint32(params[1].fpr(), params[0].gpr());
2924     });
2925     result = g32();
2926     emitPatchpoint(patchpoint, result, arg);
2927     return { };
2928 }
2929 
2930 template&lt;&gt;
2931 auto AirIRGenerator::addOp&lt;OpType::I64TruncSF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2932 {
2933     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2934     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2935 
2936     auto temp1 = g32();
2937     auto temp2 = g32();
2938     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2939     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2940     append(Or32, temp1, temp2);
2941 
2942     emitCheck([&amp;] {
2943         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2944     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2945         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2946     });
2947 
2948     auto* patchpoint = addPatchpoint(B3::Int64);
2949     patchpoint-&gt;effects = B3::Effects::none();
2950     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2951         jit.truncateDoubleToInt64(params[1].fpr(), params[0].gpr());
2952     });
2953 
2954     result = g64();
2955     emitPatchpoint(patchpoint, result, arg);
2956     return { };
2957 }
2958 
2959 template&lt;&gt;
2960 auto AirIRGenerator::addOp&lt;OpType::I64TruncUF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2961 {
2962     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min()) * -2.0));
2963     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-1.0));
2964 
2965     auto temp1 = g32();
2966     auto temp2 = g32();
2967     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2968     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2969     append(Or32, temp1, temp2);
2970 
2971     emitCheck([&amp;] {
2972         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2973     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2974         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2975     });
2976 
2977     TypedTmp signBitConstant;
2978     if (isX86())
2979         signBitConstant = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;uint64_t&gt;::max() - std::numeric_limits&lt;int64_t&gt;::max())));
2980 
2981     Vector&lt;ConstrainedTmp&gt; args;
2982     auto* patchpoint = addPatchpoint(B3::Int64);
2983     patchpoint-&gt;effects = B3::Effects::none();
2984     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2985     args.append(arg);
2986     if (isX86()) {
2987         args.append(signBitConstant);
2988         patchpoint-&gt;numFPScratchRegisters = 1;
2989     }
2990     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2991         AllowMacroScratchRegisterUsage allowScratch(jit);
2992         FPRReg scratch = InvalidFPRReg;
2993         FPRReg constant = InvalidFPRReg;
2994         if (isX86()) {
2995             scratch = params.fpScratch(0);
2996             constant = params[2].fpr();
2997         }
2998         jit.truncateDoubleToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
2999     });
3000 
3001     result = g64();
3002     emitPatchpoint(m_currentBlock, patchpoint, Vector&lt;TypedTmp, 8&gt; { result }, WTFMove(args));
3003     return { };
3004 }
3005 
3006 template&lt;&gt;
3007 auto AirIRGenerator::addOp&lt;OpType::I64TruncSF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
3008 {
3009     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(-static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
3010     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
3011 
3012     auto temp1 = g32();
3013     auto temp2 = g32();
3014     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
3015     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
3016     append(Or32, temp1, temp2);
3017 
3018     emitCheck([&amp;] {
3019         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
3020     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
3021         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
3022     });
3023 
3024     auto* patchpoint = addPatchpoint(B3::Int64);
3025     patchpoint-&gt;effects = B3::Effects::none();
3026     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
3027         jit.truncateFloatToInt64(params[1].fpr(), params[0].gpr());
3028     });
3029     result = g64();
3030     emitPatchpoint(patchpoint, result, arg);
3031     return { };
3032 }
3033 
3034 template&lt;&gt;
3035 auto AirIRGenerator::addOp&lt;OpType::I64TruncUF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
3036 {
3037     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min()) * static_cast&lt;float&gt;(-2.0)));
3038     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-1.0)));
3039 
3040     auto temp1 = g32();
3041     auto temp2 = g32();
3042     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
3043     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
3044     append(Or32, temp1, temp2);
3045 
3046     emitCheck([&amp;] {
3047         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
3048     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
3049         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
3050     });
3051 
3052     TypedTmp signBitConstant;
3053     if (isX86())
3054         signBitConstant = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;uint64_t&gt;::max() - std::numeric_limits&lt;int64_t&gt;::max())));
3055 
3056     auto* patchpoint = addPatchpoint(B3::Int64);
3057     patchpoint-&gt;effects = B3::Effects::none();
3058     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
3059     Vector&lt;ConstrainedTmp, 2&gt; args;
3060     args.append(arg);
3061     if (isX86()) {
3062         args.append(signBitConstant);
3063         patchpoint-&gt;numFPScratchRegisters = 1;
3064     }
3065     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
3066         AllowMacroScratchRegisterUsage allowScratch(jit);
3067         FPRReg scratch = InvalidFPRReg;
3068         FPRReg constant = InvalidFPRReg;
3069         if (isX86()) {
3070             scratch = params.fpScratch(0);
3071             constant = params[2].fpr();
3072         }
3073         jit.truncateFloatToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
3074     });
3075 
3076     result = g64();
3077     emitPatchpoint(m_currentBlock, patchpoint, Vector&lt;TypedTmp, 8&gt; { result }, WTFMove(args));
3078 
3079     return { };
3080 }
3081 
3082 auto AirIRGenerator::addShift(Type type, B3::Air::Opcode op, ExpressionType value, ExpressionType shift, ExpressionType&amp; result) -&gt; PartialResult
3083 {
3084     ASSERT(type == Type::I64 || type == Type::I32);
3085     result = tmpForType(type);
3086 
3087     if (isValidForm(op, Arg::Tmp, Arg::Tmp, Arg::Tmp)) {
3088         append(op, value, shift, result);
3089         return { };
3090     }
3091 
3092 #if CPU(X86_64)
3093     Tmp ecx = Tmp(X86Registers::ecx);
3094     append(Move, value, result);
3095     append(Move, shift, ecx);
3096     append(op, ecx, result);
3097 #else
3098     RELEASE_ASSERT_NOT_REACHED();
3099 #endif
3100     return { };
3101 }
3102 
3103 auto AirIRGenerator::addIntegerSub(B3::Air::Opcode op, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result) -&gt; PartialResult
3104 {
3105     ASSERT(op == Sub32 || op == Sub64);
3106 
3107     result = op == Sub32 ? g32() : g64();
3108 
3109     if (isValidForm(op, Arg::Tmp, Arg::Tmp, Arg::Tmp)) {
3110         append(op, lhs, rhs, result);
3111         return { };
3112     }
3113 
3114     RELEASE_ASSERT(isX86());
3115     // Sub a, b
3116     // means
3117     // b = b Sub a
3118     append(Move, lhs, result);
3119     append(op, rhs, result);
3120     return { };
3121 }
3122 
3123 auto AirIRGenerator::addFloatingPointAbs(B3::Air::Opcode op, ExpressionType value, ExpressionType&amp; result) -&gt; PartialResult
3124 {
3125     RELEASE_ASSERT(op == AbsFloat || op == AbsDouble);
3126 
3127     result = op == AbsFloat ? f32() : f64();
3128 
3129     if (isValidForm(op, Arg::Tmp, Arg::Tmp)) {
3130         append(op, value, result);
3131         return { };
3132     }
3133 
3134     RELEASE_ASSERT(isX86());
3135 
3136     if (op == AbsFloat) {
3137         auto constant = g32();
3138         append(Move, Arg::imm(static_cast&lt;uint32_t&gt;(~(1ull &lt;&lt; 31))), constant);
3139         append(Move32ToFloat, constant, result);
3140         append(AndFloat, value, result);
3141     } else {
3142         auto constant = g64();
3143         append(Move, Arg::bigImm(~(1ull &lt;&lt; 63)), constant);
3144         append(Move64ToDouble, constant, result);
3145         append(AndDouble, value, result);
3146     }
3147     return { };
3148 }
3149 
3150 auto AirIRGenerator::addFloatingPointBinOp(Type type, B3::Air::Opcode op, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result) -&gt; PartialResult
3151 {
3152     ASSERT(type == Type::F32 || type == Type::F64);
3153     result = tmpForType(type);
3154 
3155     if (isValidForm(op, Arg::Tmp, Arg::Tmp, Arg::Tmp)) {
3156         append(op, lhs, rhs, result);
3157         return { };
3158     }
3159 
3160     RELEASE_ASSERT(isX86());
3161 
3162     // Op a, b
3163     // means
3164     // b = b Op a
3165     append(moveOpForValueType(type), lhs, result);
3166     append(op, rhs, result);
3167     return { };
3168 }
3169 
3170 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Ceil&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3171 {
3172     result = f32();
3173     append(CeilFloat, arg0, result);
3174     return { };
3175 }
3176 
3177 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3178 {
3179     result = g32();
3180     append(Mul32, arg0, arg1, result);
3181     return { };
3182 }
3183 
3184 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3185 {
3186     return addIntegerSub(Sub32, arg0, arg1, result);
3187 }
3188 
3189 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Le&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3190 {
3191     result = g32();
3192     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqual), arg0, arg1, result);
3193     return { };
3194 }
3195 
3196 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32DemoteF64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3197 {
3198     result = f32();
3199     append(ConvertDoubleToFloat, arg0, result);
3200     return { };
3201 }
3202 
3203 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Min&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3204 {
3205     return addFloatingPointMinOrMax(F32, MinOrMax::Min, arg0, arg1, result);
3206 }
3207 
3208 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3209 {
3210     result = g32();
3211     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleNotEqualOrUnordered), arg0, arg1, result);
3212     return { };
3213 }
3214 
3215 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Lt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3216 {
3217     result = g32();
3218     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThan), arg0, arg1, result);
3219     return { };
3220 }
3221 
3222 auto AirIRGenerator::addFloatingPointMinOrMax(Type floatType, MinOrMax minOrMax, ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3223 {
3224     ASSERT(floatType == F32 || floatType == F64);
3225     result = tmpForType(floatType);
3226 
3227     BasicBlock* isEqual = m_code.addBlock();
3228     BasicBlock* notEqual = m_code.addBlock();
3229     BasicBlock* isLessThan = m_code.addBlock();
3230     BasicBlock* notLessThan = m_code.addBlock();
3231     BasicBlock* isGreaterThan = m_code.addBlock();
3232     BasicBlock* isNaN = m_code.addBlock();
3233     BasicBlock* continuation = m_code.addBlock();
3234 
3235     auto branchOp = floatType == F32 ? BranchFloat : BranchDouble;
3236     append(m_currentBlock, branchOp, Arg::doubleCond(MacroAssembler::DoubleEqual), arg0, arg1);
3237     m_currentBlock-&gt;setSuccessors(isEqual, notEqual);
3238 
3239     append(notEqual, branchOp, Arg::doubleCond(MacroAssembler::DoubleLessThan), arg0, arg1);
3240     notEqual-&gt;setSuccessors(isLessThan, notLessThan);
3241 
3242     append(notLessThan, branchOp, Arg::doubleCond(MacroAssembler::DoubleGreaterThan), arg0, arg1);
3243     notLessThan-&gt;setSuccessors(isGreaterThan, isNaN);
3244 
3245     auto andOp = floatType == F32 ? AndFloat : AndDouble;
3246     auto orOp = floatType == F32 ? OrFloat : OrDouble;
3247     append(isEqual, minOrMax == MinOrMax::Max ? andOp : orOp, arg0, arg1, result);
3248     append(isEqual, Jump);
3249     isEqual-&gt;setSuccessors(continuation);
3250 
3251     auto isLessThanResult = minOrMax == MinOrMax::Max ? arg1 : arg0;
3252     append(isLessThan, moveOpForValueType(floatType), isLessThanResult, result);
3253     append(isLessThan, Jump);
3254     isLessThan-&gt;setSuccessors(continuation);
3255 
3256     auto isGreaterThanResult = minOrMax == MinOrMax::Max ? arg0 : arg1;
3257     append(isGreaterThan, moveOpForValueType(floatType), isGreaterThanResult, result);
3258     append(isGreaterThan, Jump);
3259     isGreaterThan-&gt;setSuccessors(continuation);
3260 
3261     auto addOp = floatType == F32 ? AddFloat : AddDouble;
3262     append(isNaN, addOp, arg0, arg1, result);
3263     append(isNaN, Jump);
3264     isNaN-&gt;setSuccessors(continuation);
3265 
3266     m_currentBlock = continuation;
3267 
3268     return { };
3269 }
3270 
3271 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Max&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3272 {
3273     return addFloatingPointMinOrMax(F32, MinOrMax::Max, arg0, arg1, result);
3274 }
3275 
3276 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3277 {
3278     return addFloatingPointBinOp(Type::F64, MulDouble, arg0, arg1, result);
3279 }
3280 
3281 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Div&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3282 {
3283     return addFloatingPointBinOp(Type::F32, DivFloat, arg0, arg1, result);
3284 }
3285 
3286 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Clz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3287 {
3288     result = g32();
3289     append(CountLeadingZeros32, arg0, result);
3290     return { };
3291 }
3292 
3293 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Copysign&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3294 {
3295     // FIXME: We can have better codegen here for the imms and two operand forms on x86
3296     // https://bugs.webkit.org/show_bug.cgi?id=193999
3297     result = f32();
3298     auto temp1 = g32();
3299     auto sign = g32();
3300     auto value = g32();
3301 
3302     // FIXME: Try to use Imm where possible:
3303     // https://bugs.webkit.org/show_bug.cgi?id=193999
3304     append(MoveFloatTo32, arg1, temp1);
3305     append(Move, Arg::bigImm(0x80000000), sign);
3306     append(And32, temp1, sign, sign);
3307 
3308     append(MoveDoubleTo64, arg0, temp1);
3309     append(Move, Arg::bigImm(0x7fffffff), value);
3310     append(And32, temp1, value, value);
3311 
3312     append(Or32, sign, value, value);
3313     append(Move32ToFloat, value, result);
3314 
3315     return { };
3316 }
3317 
3318 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ConvertUI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3319 {
3320     result = f64();
3321     auto temp = g64();
3322     append(Move32, arg0, temp);
3323     append(ConvertInt64ToDouble, temp, result);
3324     return { };
3325 }
3326 
3327 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ReinterpretI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3328 {
3329     result = f32();
3330     append(Move32ToFloat, arg0, result);
3331     return { };
3332 }
3333 
3334 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64And&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3335 {
3336     result = g64();
3337     append(And64, arg0, arg1, result);
3338     return { };
3339 }
3340 
3341 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3342 {
3343     result = g32();
3344     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleNotEqualOrUnordered), arg0, arg1, result);
3345     return { };
3346 }
3347 
3348 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Gt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3349 {
3350     result = g32();
3351     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThan), arg0, arg1, result);
3352     return { };
3353 }
3354 
3355 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Sqrt&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3356 {
3357     result = f32();
3358     append(SqrtFloat, arg0, result);
3359     return { };
3360 }
3361 
3362 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Ge&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3363 {
3364     result = g32();
3365     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqual), arg0, arg1, result);
3366     return { };
3367 }
3368 
3369 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3370 {
3371     result = g32();
3372     append(Compare64, Arg::relCond(MacroAssembler::GreaterThan), arg0, arg1, result);
3373     return { };
3374 }
3375 
3376 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3377 {
3378     result = g32();
3379     append(Compare64, Arg::relCond(MacroAssembler::Above), arg0, arg1, result);
3380     return { };
3381 }
3382 
3383 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Eqz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3384 {
3385     result = g32();
3386     append(Test64, Arg::resCond(MacroAssembler::Zero), arg0, arg0, result);
3387     return { };
3388 }
3389 
3390 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Div&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3391 {
3392     return addFloatingPointBinOp(Type::F64, DivDouble, arg0, arg1, result);
3393 }
3394 
3395 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3396 {
3397     result = f32();
3398     append(AddFloat, arg0, arg1, result);
3399     return { };
3400 }
3401 
3402 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Or&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3403 {
3404     result = g64();
3405     append(Or64, arg0, arg1, result);
3406     return { };
3407 }
3408 
3409 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3410 {
3411     result = g32();
3412     append(Compare32, Arg::relCond(MacroAssembler::BelowOrEqual), arg0, arg1, result);
3413     return { };
3414 }
3415 
3416 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3417 {
3418     result = g32();
3419     append(Compare32, Arg::relCond(MacroAssembler::LessThanOrEqual), arg0, arg1, result);
3420     return { };
3421 }
3422 
3423 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3424 {
3425     result = g32();
3426     append(Compare64, Arg::relCond(MacroAssembler::NotEqual), arg0, arg1, result);
3427     return { };
3428 }
3429 
3430 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Clz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3431 {
3432     result = g64();
3433     append(CountLeadingZeros64, arg0, result);
3434     return { };
3435 }
3436 
3437 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Neg&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3438 {
3439     result = f32();
3440     if (isValidForm(NegateFloat, Arg::Tmp, Arg::Tmp))
3441         append(NegateFloat, arg0, result);
3442     else {
3443         auto constant = addConstant(Type::I32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-0.0)));
3444         auto temp = g32();
3445         append(MoveFloatTo32, arg0, temp);
3446         append(Xor32, constant, temp);
3447         append(Move32ToFloat, temp, result);
3448     }
3449     return { };
3450 }
3451 
3452 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32And&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3453 {
3454     result = g32();
3455     append(And32, arg0, arg1, result);
3456     return { };
3457 }
3458 
3459 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3460 {
3461     result = g32();
3462     append(Compare32, Arg::relCond(MacroAssembler::Below), arg0, arg1, result);
3463     return { };
3464 }
3465 
3466 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Rotr&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3467 {
3468     return addShift(Type::I64, RotateRight64, arg0, arg1, result);
3469 }
3470 
3471 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Abs&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3472 {
3473     return addFloatingPointAbs(AbsDouble, arg0, result);
3474 }
3475 
3476 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3477 {
3478     result = g32();
3479     append(Compare32, Arg::relCond(MacroAssembler::LessThan), arg0, arg1, result);
3480     return { };
3481 }
3482 
3483 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3484 {
3485     result = g32();
3486     append(Compare32, Arg::relCond(MacroAssembler::Equal), arg0, arg1, result);
3487     return { };
3488 }
3489 
3490 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Copysign&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3491 {
3492     // FIXME: We can have better codegen here for the imms and two operand forms on x86
3493     // https://bugs.webkit.org/show_bug.cgi?id=193999
3494     result = f64();
3495     auto temp1 = g64();
3496     auto sign = g64();
3497     auto value = g64();
3498 
3499     append(MoveDoubleTo64, arg1, temp1);
3500     append(Move, Arg::bigImm(0x8000000000000000), sign);
3501     append(And64, temp1, sign, sign);
3502 
3503     append(MoveDoubleTo64, arg0, temp1);
3504     append(Move, Arg::bigImm(0x7fffffffffffffff), value);
3505     append(And64, temp1, value, value);
3506 
3507     append(Or64, sign, value, value);
3508     append(Move64ToDouble, value, result);
3509 
3510     return { };
3511 }
3512 
3513 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ConvertSI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3514 {
3515     result = f32();
3516     append(ConvertInt64ToFloat, arg0, result);
3517     return { };
3518 }
3519 
3520 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Rotl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3521 {
3522     if (isARM64()) {
3523         // ARM64 doesn&#39;t have a rotate left.
3524         auto newShift = g64();
3525         append(Move, arg1, newShift);
3526         append(Neg64, newShift);
3527         return addShift(Type::I64, RotateRight64, arg0, newShift, result);
3528     } else
3529         return addShift(Type::I64, RotateLeft64, arg0, arg1, result);
3530 }
3531 
3532 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Lt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3533 {
3534     result = g32();
3535     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThan), arg0, arg1, result);
3536     return { };
3537 }
3538 
3539 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ConvertSI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3540 {
3541     result = f64();
3542     append(ConvertInt32ToDouble, arg0, result);
3543     return { };
3544 }
3545 
3546 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3547 {
3548     result = g32();
3549     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleEqual), arg0, arg1, result);
3550     return { };
3551 }
3552 
3553 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Le&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3554 {
3555     result = g32();
3556     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqual), arg0, arg1, result);
3557     return { };
3558 }
3559 
3560 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Ge&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3561 {
3562     result = g32();
3563     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqual), arg0, arg1, result);
3564     return { };
3565 }
3566 
3567 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32ShrU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3568 {
3569     return addShift(Type::I32, Urshift32, arg0, arg1, result);
3570 }
3571 
3572 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ConvertUI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3573 {
3574     result = f32();
3575     auto temp = g64();
3576     append(Move32, arg0, temp);
3577     append(ConvertInt64ToFloat, temp, result);
3578     return { };
3579 }
3580 
3581 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32ShrS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3582 {
3583     return addShift(Type::I32, Rshift32, arg0, arg1, result);
3584 }
3585 
3586 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3587 {
3588     result = g32();
3589     append(Compare32, Arg::relCond(MacroAssembler::AboveOrEqual), arg0, arg1, result);
3590     return { };
3591 }
3592 
3593 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Ceil&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3594 {
3595     result = f64();
3596     append(CeilDouble, arg0, result);
3597     return { };
3598 }
3599 
3600 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3601 {
3602     result = g32();
3603     append(Compare32, Arg::relCond(MacroAssembler::GreaterThanOrEqual), arg0, arg1, result);
3604     return { };
3605 }
3606 
3607 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Shl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3608 {
3609     return addShift(Type::I32, Lshift32, arg0, arg1, result);
3610 }
3611 
3612 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Floor&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3613 {
3614     result = f64();
3615     append(FloorDouble, arg0, result);
3616     return { };
3617 }
3618 
3619 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Xor&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3620 {
3621     result = g32();
3622     append(Xor32, arg0, arg1, result);
3623     return { };
3624 }
3625 
3626 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Abs&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3627 {
3628     return addFloatingPointAbs(AbsFloat, arg0, result);
3629 }
3630 
3631 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Min&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3632 {
3633     return addFloatingPointMinOrMax(F64, MinOrMax::Min, arg0, arg1, result);
3634 }
3635 
3636 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3637 {
3638     result = f32();
3639     append(MulFloat, arg0, arg1, result);
3640     return { };
3641 }
3642 
3643 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3644 {
3645     return addIntegerSub(Sub64, arg0, arg1, result);
3646 }
3647 
3648 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32ReinterpretF32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3649 {
3650     result = g32();
3651     append(MoveFloatTo32, arg0, result);
3652     return { };
3653 }
3654 
3655 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3656 {
3657     result = g32();
3658     append(Add32, arg0, arg1, result);
3659     return { };
3660 }
3661 
3662 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3663 {
3664     return addFloatingPointBinOp(Type::F64, SubDouble, arg0, arg1, result);
3665 }
3666 
3667 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Or&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3668 {
3669     result = g32();
3670     append(Or32, arg0, arg1, result);
3671     return { };
3672 }
3673 
3674 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3675 {
3676     result = g32();
3677     append(Compare64, Arg::relCond(MacroAssembler::Below), arg0, arg1, result);
3678     return { };
3679 }
3680 
3681 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3682 {
3683     result = g32();
3684     append(Compare64, Arg::relCond(MacroAssembler::LessThan), arg0, arg1, result);
3685     return { };
3686 }
3687 
3688 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ConvertSI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3689 {
3690     result = f64();
3691     append(ConvertInt64ToDouble, arg0, result);
3692     return { };
3693 }
3694 
3695 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Xor&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3696 {
3697     result = g64();
3698     append(Xor64, arg0, arg1, result);
3699     return { };
3700 }
3701 
3702 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3703 {
3704     result = g32();
3705     append(Compare64, Arg::relCond(MacroAssembler::AboveOrEqual), arg0, arg1, result);
3706     return { };
3707 }
3708 
3709 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3710 {
3711     result = g64();
3712     append(Mul64, arg0, arg1, result);
3713     return { };
3714 }
3715 
3716 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3717 {
3718     result = f32();
3719     if (isValidForm(SubFloat, Arg::Tmp, Arg::Tmp, Arg::Tmp))
3720         append(SubFloat, arg0, arg1, result);
3721     else {
3722         RELEASE_ASSERT(isX86());
3723         append(MoveFloat, arg0, result);
3724         append(SubFloat, arg1, result);
3725     }
3726     return { };
3727 }
3728 
3729 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64PromoteF32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3730 {
3731     result = f64();
3732     append(ConvertFloatToDouble, arg0, result);
3733     return { };
3734 }
3735 
3736 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3737 {
3738     result = f64();
3739     append(AddDouble, arg0, arg1, result);
3740     return { };
3741 }
3742 
3743 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3744 {
3745     result = g32();
3746     append(Compare64, Arg::relCond(MacroAssembler::GreaterThanOrEqual), arg0, arg1, result);
3747     return { };
3748 }
3749 
3750 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ExtendUI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3751 {
3752     result = g64();
3753     append(Move32, arg0, result);
3754     return { };
3755 }
3756 
3757 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3758 {
3759     result = g32();
3760     RELEASE_ASSERT(arg0 &amp;&amp; arg1);
3761     append(Compare32, Arg::relCond(MacroAssembler::NotEqual), arg0, arg1, result);
3762     return { };
3763 }
3764 
3765 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ReinterpretI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3766 {
3767     result = f64();
3768     append(Move64ToDouble, arg0, result);
3769     return { };
3770 }
3771 
3772 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3773 {
3774     result = g32();
3775     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleEqual), arg0, arg1, result);
3776     return { };
3777 }
3778 
3779 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3780 {
3781     result = g32();
3782     append(Compare64, Arg::relCond(MacroAssembler::Equal), arg0, arg1, result);
3783     return { };
3784 }
3785 
3786 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Floor&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3787 {
3788     result = f32();
3789     append(FloorFloat, arg0, result);
3790     return { };
3791 }
3792 
3793 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ConvertSI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3794 {
3795     result = f32();
3796     append(ConvertInt32ToFloat, arg0, result);
3797     return { };
3798 }
3799 
3800 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Eqz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3801 {
3802     result = g32();
3803     append(Test32, Arg::resCond(MacroAssembler::Zero), arg0, arg0, result);
3804     return { };
3805 }
3806 
3807 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ReinterpretF64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3808 {
3809     result = g64();
3810     append(MoveDoubleTo64, arg0, result);
3811     return { };
3812 }
3813 
3814 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ShrS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3815 {
3816     return addShift(Type::I64, Rshift64, arg0, arg1, result);
3817 }
3818 
3819 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ShrU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3820 {
3821     return addShift(Type::I64, Urshift64, arg0, arg1, result);
3822 }
3823 
3824 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Sqrt&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3825 {
3826     result = f64();
3827     append(SqrtDouble, arg0, result);
3828     return { };
3829 }
3830 
3831 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Shl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3832 {
3833     return addShift(Type::I64, Lshift64, arg0, arg1, result);
3834 }
3835 
3836 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Gt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3837 {
3838     result = g32();
3839     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThan), arg0, arg1, result);
3840     return { };
3841 }
3842 
3843 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32WrapI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3844 {
3845     result = g32();
3846     append(Move32, arg0, result);
3847     return { };
3848 }
3849 
3850 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Rotl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3851 {
3852     if (isARM64()) {
3853         // ARM64 doesn&#39;t have a rotate left.
3854         auto newShift = g64();
3855         append(Move, arg1, newShift);
3856         append(Neg64, newShift);
3857         return addShift(Type::I32, RotateRight32, arg0, newShift, result);
3858     } else
3859         return addShift(Type::I32, RotateLeft32, arg0, arg1, result);
3860 }
3861 
3862 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Rotr&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3863 {
3864     return addShift(Type::I32, RotateRight32, arg0, arg1, result);
3865 }
3866 
3867 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3868 {
3869     result = g32();
3870     append(Compare32, Arg::relCond(MacroAssembler::Above), arg0, arg1, result);
3871     return { };
3872 }
3873 
3874 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ExtendSI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3875 {
3876     result = g64();
3877     append(SignExtend32ToPtr, arg0, result);
3878     return { };
3879 }
3880 
3881 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3882 {
3883     result = g32();
3884     append(Compare32, Arg::relCond(MacroAssembler::GreaterThan), arg0, arg1, result);
3885     return { };
3886 }
3887 
3888 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Neg&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3889 {
3890     result = f64();
3891     if (isValidForm(NegateDouble, Arg::Tmp, Arg::Tmp))
3892         append(NegateDouble, arg0, result);
3893     else {
3894         auto constant = addConstant(Type::I64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(-0.0)));
3895         auto temp = g64();
3896         append(MoveDoubleTo64, arg0, temp);
3897         append(Xor64, constant, temp);
3898         append(Move64ToDouble, temp, result);
3899     }
3900     return { };
3901 }
3902 
3903 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Max&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3904 {
3905     return addFloatingPointMinOrMax(F64, MinOrMax::Max, arg0, arg1, result);
3906 }
3907 
3908 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3909 {
3910     result = g32();
3911     append(Compare64, Arg::relCond(MacroAssembler::BelowOrEqual), arg0, arg1, result);
3912     return { };
3913 }
3914 
3915 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3916 {
3917     result = g32();
3918     append(Compare64, Arg::relCond(MacroAssembler::LessThanOrEqual), arg0, arg1, result);
3919     return { };
3920 }
3921 
3922 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3923 {
3924     result = g64();
3925     append(Add64, arg0, arg1, result);
3926     return { };
3927 }
3928 
3929 } } // namespace JSC::Wasm
3930 
3931 #endif // ENABLE(WEBASSEMBLY)
    </pre>
  </body>
</html>