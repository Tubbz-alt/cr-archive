diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp
@@ -29,10 +29,11 @@
 #if ENABLE(B3_JIT)
 
 #include "AirBlockInsertionSet.h"
 #include "AirCCallSpecial.h"
 #include "AirCode.h"
+#include "AirHelpers.h"
 #include "AirInsertionSet.h"
 #include "AirInstInlines.h"
 #include "AirPrintSpecial.h"
 #include "AirStackSlot.h"
 #include "B3ArgumentRegValue.h"
@@ -61,27 +62,30 @@
 #include "B3WasmAddressValue.h"
 #include <wtf/IndexMap.h>
 #include <wtf/IndexSet.h>
 #include <wtf/ListDump.h>
 
-#if ASSERT_DISABLED
+#if !ASSERT_ENABLED
 IGNORE_RETURN_TYPE_WARNINGS_BEGIN
 #endif
 
 namespace JSC { namespace B3 {
 
 namespace {
 
 namespace B3LowerToAirInternal {
-static const bool verbose = false;
+static constexpr bool verbose = false;
 }
 
 using Arg = Air::Arg;
 using Inst = Air::Inst;
 using Code = Air::Code;
 using Tmp = Air::Tmp;
 
+using Air::moveForType;
+using Air::relaxedMoveForType;
+
 // FIXME: We wouldn't need this if Air supported Width modifiers in Air::Kind.
 // https://bugs.webkit.org/show_bug.cgi?id=169247
 #define OPCODE_FOR_WIDTH(opcode, width) ( \
     (width) == Width8 ? Air::opcode ## 8 : \
     (width) == Width16 ? Air::opcode ## 16 :    \
@@ -153,14 +157,14 @@
         }
 
         for (B3::StackSlot* stack : m_procedure.stackSlots())
             m_stackToStack.add(stack, m_code.addStackSlot(stack));
         for (Variable* variable : m_procedure.variables()) {
-            auto addResult = m_variableToTmps.add(variable, Vector<Tmp, 1>(m_procedure.returnCount(variable->type())));
+            auto addResult = m_variableToTmps.add(variable, Vector<Tmp, 1>(m_procedure.resultCount(variable->type())));
             ASSERT(addResult.isNewEntry);
-            for (unsigned i = 0; i < m_procedure.returnCount(variable->type()); ++i)
-                addResult.iterator->value[i] = tmpForType(variable->type().isNumeric() ? variable->type() : m_procedure.extractFromTuple(variable->type(), i));
+            for (unsigned i = 0; i < m_procedure.resultCount(variable->type()); ++i)
+                addResult.iterator->value[i] = tmpForType(m_procedure.typeAtOffset(variable->type(), i));
         }
 
         // Figure out which blocks are not rare.
         m_fastWorklist.push(m_procedure[0]);
         while (B3::BasicBlock* block = m_fastWorklist.pop()) {
@@ -529,11 +533,11 @@
 
         auto fallback = [&] () -> Arg {
             return Arg::addr(tmp(address), offset);
         };
 
-        static const unsigned lotsOfUses = 10; // This is arbitrary and we should tune it eventually.
+        static constexpr unsigned lotsOfUses = 10; // This is arbitrary and we should tune it eventually.
 
         // Only match if the address value isn't used in some large number of places.
         if (m_useCounts.numUses(address) > lotsOfUses)
             return fallback();
 
@@ -1171,66 +1175,10 @@
         kind.effects |= memory->traps();
 
         append(createStore(kind, memory->child(0), dest));
     }
 
-    Air::Opcode moveForType(Type type)
-    {
-        using namespace Air;
-        switch (type.kind()) {
-        case Int32:
-            return Move32;
-        case Int64:
-            RELEASE_ASSERT(is64Bit());
-            return Move;
-        case Float:
-            return MoveFloat;
-        case Double:
-            return MoveDouble;
-        case Void:
-        case Tuple:
-            break;
-        }
-        RELEASE_ASSERT_NOT_REACHED();
-        return Air::Oops;
-    }
-
-    Air::Opcode relaxedMoveForType(Type type)
-    {
-        using namespace Air;
-        switch (type.kind()) {
-        case Int32:
-        case Int64:
-            // For Int32, we could return Move or Move32. It's a trade-off.
-            //
-            // Move32: Using Move32 guarantees that we use the narrower move, but in cases where the
-            //     register allocator can't prove that the variables involved are 32-bit, this will
-            //     disable coalescing.
-            //
-            // Move: Using Move guarantees that the register allocator can coalesce normally, but in
-            //     cases where it can't prove that the variables are 32-bit and it doesn't coalesce,
-            //     this will force us to use a full 64-bit Move instead of the slightly cheaper
-            //     32-bit Move32.
-            //
-            // Coalescing is a lot more profitable than turning Move into Move32. So, it's better to
-            // use Move here because in cases where the register allocator cannot prove that
-            // everything is 32-bit, we still get coalescing.
-            return Move;
-        case Float:
-            // MoveFloat is always coalescable and we never convert MoveDouble to MoveFloat, so we
-            // should use MoveFloat when we know that the temporaries involved are 32-bit.
-            return MoveFloat;
-        case Double:
-            return MoveDouble;
-        case Void:
-        case Tuple:
-            break;
-        }
-        RELEASE_ASSERT_NOT_REACHED();
-        return Air::Oops;
-    }
-
 #if ENABLE(MASM_PROBE)
     template<typename... Arguments>
     void print(Arguments&&... arguments)
     {
         Value* origin = m_value;
@@ -2338,11 +2286,11 @@
         Air::BasicBlock* failBlock = nullptr;
         if (!isBranch) {
             failBlock = newBlock();
             failure = failBlock;
         }
-        Air::BasicBlock* strongFailBlock;
+        Air::BasicBlock* strongFailBlock = nullptr;
         if (isStrong && hasFence)
             strongFailBlock = newBlock();
         Air::FrequentedBlock comparisonFail = failure;
         Air::FrequentedBlock weakFail;
         if (isStrong) {
@@ -2747,11 +2695,11 @@
             if (m_value->child(1)->isInt(0xffff)) {
                 appendUnOp<ZeroExtend16To32, ZeroExtend16To32>(m_value->child(0));
                 return;
             }
 
-            if (m_value->child(1)->isInt(0xffffffff)) {
+            if (m_value->child(1)->isInt64(0xffffffff) || m_value->child(1)->isInt32(0xffffffff)) {
                 appendUnOp<Move32, Move32>(m_value->child(0));
                 return;
             }
 
             appendBinOp<And32, And64, AndDouble, AndFloat, Commutative>(
@@ -3738,10 +3686,10 @@
     lowerToAir.run();
 }
 
 } } // namespace JSC::B3
 
-#if ASSERT_DISABLED
+#if !ASSERT_ENABLED
 IGNORE_RETURN_TYPE_WARNINGS_END
 #endif
 
 #endif // ENABLE(B3_JIT)
