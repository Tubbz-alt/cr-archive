<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGCSEPhase.h&quot;
 28 
 29 #if ENABLE(DFG_JIT)
 30 
 31 #include &quot;DFGAbstractHeap.h&quot;
 32 #include &quot;DFGBlockMapInlines.h&quot;
 33 #include &quot;DFGClobberSet.h&quot;
 34 #include &quot;DFGClobberize.h&quot;
 35 #include &quot;DFGDominators.h&quot;
 36 #include &quot;DFGGraph.h&quot;
 37 #include &quot;DFGPhase.h&quot;
 38 #include &quot;JSCInlines.h&quot;
 39 #include &lt;array&gt;
 40 
 41 namespace JSC { namespace DFG {
 42 
 43 // This file contains two CSE implementations: local and global. LocalCSE typically runs when we&#39;re
 44 // in DFG mode, i.e. we want to compile quickly. LocalCSE contains a lot of optimizations for
 45 // compile time. GlobalCSE, on the other hand, is fairly straight-forward. It will find more
 46 // optimization opportunities by virtue of being global.
 47 
 48 namespace {
 49 
 50 namespace DFGCSEPhaseInternal {
<a name="2" id="anc2"></a><span class="line-modified"> 51 static constexpr bool verbose = false;</span>
 52 }
 53 
 54 class ImpureDataSlot {
 55     WTF_MAKE_NONCOPYABLE(ImpureDataSlot);
 56     WTF_MAKE_FAST_ALLOCATED;
 57 public:
 58     ImpureDataSlot(HeapLocation key, LazyNode value, unsigned hash)
 59         : key(key), value(value), hash(hash)
 60     { }
 61 
 62     HeapLocation key;
 63     LazyNode value;
 64     unsigned hash;
 65 };
 66 
 67 struct ImpureDataSlotHash : public DefaultHash&lt;std::unique_ptr&lt;ImpureDataSlot&gt;&gt;::Hash {
 68     static unsigned hash(const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; key)
 69     {
 70         return key-&gt;hash;
 71     }
 72 
 73     static bool equal(const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; a, const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; b)
 74     {
 75         // The ImpureDataSlot are unique per table per HeapLocation. This lets us compare the key
 76         // by just comparing the pointers of the unique ImpureDataSlots.
 77         ASSERT(a != b || a-&gt;key == b-&gt;key);
 78         return a == b;
 79     }
 80 };
 81 
 82 struct ImpureDataTranslator {
 83     static unsigned hash(const HeapLocation&amp; key)
 84     {
 85         return key.hash();
 86     }
 87 
 88     static bool equal(const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot, const HeapLocation&amp; key)
 89     {
 90         if (!slot)
 91             return false;
 92         if (HashTraits&lt;std::unique_ptr&lt;ImpureDataSlot&gt;&gt;::isDeletedValue(slot))
 93             return false;
 94         return slot-&gt;key == key;
 95     }
 96 
 97     static void translate(std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot, const HeapLocation&amp; key, unsigned hashCode)
 98     {
 99         new (NotNull, std::addressof(slot)) std::unique_ptr&lt;ImpureDataSlot&gt;(new ImpureDataSlot {key, LazyNode(), hashCode});
100     }
101 };
102 
103 class ImpureMap {
104     WTF_MAKE_FAST_ALLOCATED;
105     WTF_MAKE_NONCOPYABLE(ImpureMap);
106 public:
107     ImpureMap() = default;
108 
109     ImpureMap(ImpureMap&amp;&amp; other)
110     {
111         m_abstractHeapStackMap.swap(other.m_abstractHeapStackMap);
112         m_fallbackStackMap.swap(other.m_fallbackStackMap);
113         m_heapMap.swap(other.m_heapMap);
114 #if !defined(NDEBUG)
115         m_debugImpureData.swap(other.m_debugImpureData);
116 #endif
117     }
118 
119     const ImpureDataSlot* add(const HeapLocation&amp; location, const LazyNode&amp; node)
120     {
121         const ImpureDataSlot* result = addImpl(location, node);
122 
123 #if !defined(NDEBUG)
124         auto addResult = m_debugImpureData.add(location, node);
125         ASSERT(!!result == !addResult.isNewEntry);
126 #endif
127         return result;
128     }
129 
130     LazyNode get(const HeapLocation&amp; location) const
131     {
132         LazyNode result = getImpl(location);
133 #if !defined(NDEBUG)
134         ASSERT(result == m_debugImpureData.get(location));
135 #endif
136         return result;
137     }
138 
139     void clobber(AbstractHeap heap, bool clobberConservatively)
140     {
141         switch (heap.kind()) {
142         case World: {
143             clear();
144             break;
145         }
146         case SideState:
147             break;
148         case Stack: {
149             ASSERT(!heap.payload().isTop());
<a name="3" id="anc3"></a><span class="line-modified">150             m_abstractHeapStackMap.remove(heap.payload().value());</span>

151             if (clobberConservatively)
152                 m_fallbackStackMap.clear();
153             else
154                 clobber(m_fallbackStackMap, heap);
155             break;
156         }
157         default:
158             if (clobberConservatively)
159                 m_heapMap.clear();
160             else
161                 clobber(m_heapMap, heap);
162             break;
163         }
164 #if !defined(NDEBUG)
165         m_debugImpureData.removeIf([heap, clobberConservatively, this](const HashMap&lt;HeapLocation, LazyNode&gt;::KeyValuePairType&amp; pair) -&gt; bool {
166             switch (heap.kind()) {
167             case World:
168             case SideState:
169                 break;
170             case Stack: {
171                 if (!clobberConservatively)
172                     break;
173                 if (pair.key.heap().kind() == Stack) {
<a name="4" id="anc4"></a><span class="line-modified">174                     auto iterator = m_abstractHeapStackMap.find(pair.key.heap().payload().value());</span>
175                     if (iterator != m_abstractHeapStackMap.end() &amp;&amp; iterator-&gt;value-&gt;key == pair.key)
176                         return false;
177                     return true;
178                 }
179                 break;
180             }
181             default: {
182                 if (!clobberConservatively)
183                     break;
184                 AbstractHeapKind kind = pair.key.heap().kind();
185                 if (kind != World &amp;&amp; kind != SideState &amp;&amp; kind != Stack)
186                     return true;
187                 break;
188             }
189             }
190             return heap.overlaps(pair.key.heap());
191         });
192         ASSERT(m_debugImpureData.size()
193             == (m_heapMap.size()
194                 + m_abstractHeapStackMap.size()
195                 + m_fallbackStackMap.size()));
196 
197         const bool verifyClobber = false;
198         if (verifyClobber) {
199             for (auto&amp; pair : m_debugImpureData)
200                 ASSERT(!!get(pair.key));
201         }
202 #endif
203     }
204 
205     void clear()
206     {
207         m_abstractHeapStackMap.clear();
208         m_fallbackStackMap.clear();
209         m_heapMap.clear();
210 #if !defined(NDEBUG)
211         m_debugImpureData.clear();
212 #endif
213     }
214 
215 private:
216     typedef HashSet&lt;std::unique_ptr&lt;ImpureDataSlot&gt;, ImpureDataSlotHash&gt; Map;
217 
218     const ImpureDataSlot* addImpl(const HeapLocation&amp; location, const LazyNode&amp; node)
219     {
220         switch (location.heap().kind()) {
221         case World:
222         case SideState:
223             RELEASE_ASSERT_NOT_REACHED();
224         case Stack: {
225             AbstractHeap abstractHeap = location.heap();
226             if (abstractHeap.payload().isTop())
227                 return add(m_fallbackStackMap, location, node);
<a name="5" id="anc5"></a><span class="line-modified">228             auto addResult = m_abstractHeapStackMap.add(abstractHeap.payload().value(), nullptr);</span>

229             if (addResult.isNewEntry) {
230                 addResult.iterator-&gt;value.reset(new ImpureDataSlot {location, node, 0});
231                 return nullptr;
232             }
233             if (addResult.iterator-&gt;value-&gt;key == location)
234                 return addResult.iterator-&gt;value.get();
235             return add(m_fallbackStackMap, location, node);
236         }
237         default:
238             return add(m_heapMap, location, node);
239         }
240         return nullptr;
241     }
242 
243     LazyNode getImpl(const HeapLocation&amp; location) const
244     {
245         switch (location.heap().kind()) {
246         case World:
247         case SideState:
248             RELEASE_ASSERT_NOT_REACHED();
249         case Stack: {
<a name="6" id="anc6"></a><span class="line-modified">250             auto iterator = m_abstractHeapStackMap.find(location.heap().payload().value());</span>

251             if (iterator != m_abstractHeapStackMap.end()
252                 &amp;&amp; iterator-&gt;value-&gt;key == location)
253                 return iterator-&gt;value-&gt;value;
254             return get(m_fallbackStackMap, location);
255         }
256         default:
257             return get(m_heapMap, location);
258         }
259         return LazyNode();
260     }
261 
262     static const ImpureDataSlot* add(Map&amp; map, const HeapLocation&amp; location, const LazyNode&amp; node)
263     {
264         auto result = map.add&lt;ImpureDataTranslator&gt;(location);
265         if (result.isNewEntry) {
266             (*result.iterator)-&gt;value = node;
267             return nullptr;
268         }
269         return result.iterator-&gt;get();
270     }
271 
272     static LazyNode get(const Map&amp; map, const HeapLocation&amp; location)
273     {
274         auto iterator = map.find&lt;ImpureDataTranslator&gt;(location);
275         if (iterator != map.end())
276             return (*iterator)-&gt;value;
277         return LazyNode();
278     }
279 
280     static void clobber(Map&amp; map, AbstractHeap heap)
281     {
282         map.removeIf([heap](const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot) -&gt; bool {
283             return heap.overlaps(slot-&gt;key.heap());
284         });
285     }
286 
287     // The majority of Impure Stack Slots are unique per value.
288     // This is very useful for fast clobber(), we can just remove the slot addressed by AbstractHeap
289     // in O(1).
290     //
291     // When there are conflict, any additional HeapLocation is added in the fallback map.
292     // This works well because fallbackStackMap remains tiny.
293     //
294     // One cannot assume a unique ImpureData is in m_abstractHeapStackMap. It may have been
295     // a duplicate in the past and now only live in m_fallbackStackMap.
296     //
297     // Obviously, TOP always goes into m_fallbackStackMap since it does not have a unique value.
<a name="7" id="anc7"></a><span class="line-modified">298     HashMap&lt;int64_t, std::unique_ptr&lt;ImpureDataSlot&gt;, DefaultHash&lt;int64_t&gt;::Hash, WTF::SignedWithZeroKeyHashTraits&lt;int64_t&gt;&gt; m_abstractHeapStackMap;</span>
299     Map m_fallbackStackMap;
300 
301     Map m_heapMap;
302 
303 #if !defined(NDEBUG)
304     HashMap&lt;HeapLocation, LazyNode&gt; m_debugImpureData;
305 #endif
306 };
307 
308 class LocalCSEPhase : public Phase {
309 public:
310     LocalCSEPhase(Graph&amp; graph)
311         : Phase(graph, &quot;local common subexpression elimination&quot;)
312         , m_smallBlock(graph)
313         , m_largeBlock(graph)
314         , m_hugeBlock(graph)
315     {
316     }
317 
318     bool run()
319     {
320         ASSERT(m_graph.m_fixpointState == FixpointNotConverged);
321         ASSERT(m_graph.m_form == ThreadedCPS || m_graph.m_form == LoadStore);
322 
323         bool changed = false;
324 
325         m_graph.clearReplacements();
326 
327         for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
328             BasicBlock* block = m_graph.block(blockIndex);
329             if (!block)
330                 continue;
331 
332             if (block-&gt;size() &lt;= SmallMaps::capacity)
333                 changed |= m_smallBlock.run(block);
334             else if (block-&gt;size() &lt;= Options::maxDFGNodesInBasicBlockForPreciseAnalysis())
335                 changed |= m_largeBlock.run(block);
336             else
337                 changed |= m_hugeBlock.run(block);
338         }
339 
340         return changed;
341     }
342 
343 private:
344     class SmallMaps {
345     public:
346         // This permits SmallMaps to be used for blocks that have up to 100 nodes. In practice,
347         // fewer than half of the nodes in a block have pure defs, and even fewer have impure defs.
348         // Thus, a capacity limit of 100 probably means that somewhere around ~40 things may end up
349         // in one of these &quot;small&quot; list-based maps. That number still seems largeish, except that
350         // the overhead of HashMaps can be quite high currently: clearing them, or even removing
351         // enough things from them, deletes (or resizes) their backing store eagerly. Hence
352         // HashMaps induce a lot of malloc traffic.
<a name="8" id="anc8"></a><span class="line-modified">353         static constexpr unsigned capacity = 100;</span>
354 
355         SmallMaps()
356             : m_pureLength(0)
357             , m_impureLength(0)
358         {
359         }
360 
361         void clear()
362         {
363             m_pureLength = 0;
364             m_impureLength = 0;
365         }
366 
367         void write(AbstractHeap heap)
368         {
369             if (heap.kind() == SideState)
370                 return;
371 
372             for (unsigned i = 0; i &lt; m_impureLength; ++i) {
373                 if (heap.overlaps(m_impureMap[i].key.heap()))
374                     m_impureMap[i--] = m_impureMap[--m_impureLength];
375             }
376         }
377 
378         Node* addPure(PureValue value, Node* node)
379         {
380             for (unsigned i = m_pureLength; i--;) {
381                 if (m_pureMap[i].key == value)
382                     return m_pureMap[i].value;
383             }
384 
<a name="9" id="anc9"></a><span class="line-modified">385             RELEASE_ASSERT(m_pureLength &lt; capacity);</span>
386             m_pureMap[m_pureLength++] = WTF::KeyValuePair&lt;PureValue, Node*&gt;(value, node);
387             return nullptr;
388         }
389 
390         LazyNode findReplacement(HeapLocation location)
391         {
392             for (unsigned i = m_impureLength; i--;) {
393                 if (m_impureMap[i].key == location)
394                     return m_impureMap[i].value;
395             }
396             return nullptr;
397         }
398 
399         LazyNode addImpure(HeapLocation location, LazyNode node)
400         {
401             // FIXME: If we are using small maps, we must not def() derived values.
402             // For now the only derived values we def() are constant-based.
403             if (location.index() &amp;&amp; !location.index().isNode())
404                 return nullptr;
405             if (LazyNode result = findReplacement(location))
406                 return result;
<a name="10" id="anc10"></a><span class="line-modified">407             RELEASE_ASSERT(m_impureLength &lt; capacity);</span>
408             m_impureMap[m_impureLength++] = WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt;(location, node);
409             return nullptr;
410         }
411 
412     private:
413         WTF::KeyValuePair&lt;PureValue, Node*&gt; m_pureMap[capacity];
414         WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt; m_impureMap[capacity];
415         unsigned m_pureLength;
416         unsigned m_impureLength;
417     };
418 
419     class LargeMaps {
420     public:
421         LargeMaps()
422         {
423         }
424 
425         void clear()
426         {
427             m_pureMap.clear();
428             m_impureMap.clear();
429         }
430 
431         void write(AbstractHeap heap)
432         {
433             bool clobberConservatively = false;
434             m_impureMap.clobber(heap, clobberConservatively);
435         }
436 
437         Node* addPure(PureValue value, Node* node)
438         {
439             auto result = m_pureMap.add(value, node);
440             if (result.isNewEntry)
441                 return nullptr;
442             return result.iterator-&gt;value;
443         }
444 
445         LazyNode findReplacement(HeapLocation location)
446         {
447             return m_impureMap.get(location);
448         }
449 
450         LazyNode addImpure(const HeapLocation&amp; location, const LazyNode&amp; node)
451         {
452             if (const ImpureDataSlot* slot = m_impureMap.add(location, node))
453                 return slot-&gt;value;
454             return LazyNode();
455         }
456 
457     private:
458         HashMap&lt;PureValue, Node*&gt; m_pureMap;
459         ImpureMap m_impureMap;
460     };
461 
462     // This is used only for huge basic blocks. Our usual CSE is quadratic complexity for # of DFG nodes in a basic block.
463     // HugeMaps model results conservatively to avoid an O(N^2) algorithm. In particular, we clear all the slots of the specified heap kind
464     // in ImpureMap instead of iterating slots and removing a matched slot. This change makes the complexity O(N).
465     // FIXME: We can make LargeMap O(N) without introducing conservative behavior if we track clobbering by hierarchical epochs.
466     // https://bugs.webkit.org/show_bug.cgi?id=200014
467     class HugeMaps {
468     public:
469         HugeMaps() = default;
470 
471         void clear()
472         {
473             m_pureMap.clear();
474             m_impureMap.clear();
475         }
476 
477         void write(AbstractHeap heap)
478         {
479             bool clobberConservatively = true;
480             m_impureMap.clobber(heap, clobberConservatively);
481         }
482 
483         Node* addPure(PureValue value, Node* node)
484         {
485             auto result = m_pureMap.add(value, node);
486             if (result.isNewEntry)
487                 return nullptr;
488             return result.iterator-&gt;value;
489         }
490 
491         LazyNode findReplacement(HeapLocation location)
492         {
493             return m_impureMap.get(location);
494         }
495 
496         LazyNode addImpure(const HeapLocation&amp; location, const LazyNode&amp; node)
497         {
498             if (const ImpureDataSlot* slot = m_impureMap.add(location, node))
499                 return slot-&gt;value;
500             return LazyNode();
501         }
502 
503     private:
504         HashMap&lt;PureValue, Node*&gt; m_pureMap;
505         ImpureMap m_impureMap;
506     };
507 
508     template&lt;typename Maps&gt;
509     class BlockCSE {
510     public:
511         BlockCSE(Graph&amp; graph)
512             : m_graph(graph)
513             , m_insertionSet(graph)
514         {
515         }
516 
517         bool run(BasicBlock* block)
518         {
519             m_maps.clear();
520             m_changed = false;
521             m_block = block;
522 
523             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
524                 m_node = block-&gt;at(nodeIndex);
525                 m_graph.performSubstitution(m_node);
526 
527                 if (m_node-&gt;op() == Identity || m_node-&gt;op() == IdentityWithProfile) {
528                     m_node-&gt;replaceWith(m_graph, m_node-&gt;child1().node());
529                     m_changed = true;
530                 } else {
531                     // This rule only makes sense for local CSE, since in SSA form we have already
532                     // factored the bounds check out of the PutByVal. It&#39;s kind of gross, but we
533                     // still have reason to believe that PutByValAlias is a good optimization and
534                     // that it&#39;s better to do it with a single node rather than separating out the
535                     // CheckInBounds.
536                     if (m_node-&gt;op() == PutByVal || m_node-&gt;op() == PutByValDirect) {
537                         HeapLocation heap;
538 
539                         Node* base = m_graph.varArgChild(m_node, 0).node();
540                         Node* index = m_graph.varArgChild(m_node, 1).node();
541                         LocationKind indexedPropertyLoc = indexedPropertyLocForResultType(m_node-&gt;result());
542 
543                         ArrayMode mode = m_node-&gt;arrayMode();
544                         switch (mode.type()) {
545                         case Array::Int32:
546                             if (!mode.isInBounds())
547                                 break;
548                             heap = HeapLocation(indexedPropertyLoc, IndexedInt32Properties, base, index);
549                             break;
550 
551                         case Array::Double: {
552                             if (!mode.isInBounds())
553                                 break;
554                             LocationKind kind = mode.isSaneChain() ? IndexedPropertyDoubleSaneChainLoc : IndexedPropertyDoubleLoc;
555                             heap = HeapLocation(kind, IndexedDoubleProperties, base, index);
556                             break;
557                         }
558 
559                         case Array::Contiguous:
560                             if (!mode.isInBounds())
561                                 break;
562                             heap = HeapLocation(indexedPropertyLoc, IndexedContiguousProperties, base, index);
563                             break;
564 
565                         case Array::Int8Array:
566                         case Array::Int16Array:
567                         case Array::Int32Array:
568                         case Array::Uint8Array:
569                         case Array::Uint8ClampedArray:
570                         case Array::Uint16Array:
571                         case Array::Uint32Array:
572                         case Array::Float32Array:
573                         case Array::Float64Array:
574                             if (!mode.isInBounds())
575                                 break;
576                             heap = HeapLocation(
577                                 indexedPropertyLoc, TypedArrayProperties, base, index);
578                             break;
579 
580                         default:
581                             break;
582                         }
583 
584                         if (!!heap &amp;&amp; m_maps.findReplacement(heap))
585                             m_node-&gt;setOp(PutByValAlias);
586                     }
587 
588                     clobberize(m_graph, m_node, *this);
589                 }
590             }
591 
592             m_insertionSet.execute(block);
593 
594             return m_changed;
595         }
596 
597         void read(AbstractHeap) { }
598 
599         void write(AbstractHeap heap)
600         {
601             m_maps.write(heap);
602         }
603 
604         void def(PureValue value)
605         {
606             Node* match = m_maps.addPure(value, m_node);
607             if (!match)
608                 return;
609 
610             m_node-&gt;replaceWith(m_graph, match);
611             m_changed = true;
612         }
613 
614         void def(const HeapLocation&amp; location, const LazyNode&amp; value)
615         {
616             LazyNode match = m_maps.addImpure(location, value);
617             if (!match)
618                 return;
619 
620             if (m_node-&gt;op() == GetLocal) {
621                 // Usually the CPS rethreading phase does this. But it&#39;s OK for us to mess with
622                 // locals so long as:
623                 //
624                 // - We dethread the graph. Any changes we make may invalidate the assumptions of
625                 //   our CPS form, particularly if this GetLocal is linked to the variablesAtTail.
626                 //
627                 // - We don&#39;t introduce a Phantom for the child of the GetLocal. This wouldn&#39;t be
628                 //   totally wrong but it would pessimize the code. Just because there is a
629                 //   GetLocal doesn&#39;t mean that the child was live. Simply rerouting the all uses
630                 //   of this GetLocal will preserve the live-at-exit information just fine.
631                 //
632                 // We accomplish the latter by just clearing the child; then the Phantom that we
633                 // introduce won&#39;t have children and so it will eventually just be deleted.
634 
635                 m_node-&gt;child1() = Edge();
636                 m_graph.dethread();
637             }
638 
639             if (value.isNode() &amp;&amp; value.asNode() == m_node) {
640                 match.ensureIsNode(m_insertionSet, m_block, 0)-&gt;owner = m_block;
641                 ASSERT(match.isNode());
642                 m_node-&gt;replaceWith(m_graph, match.asNode());
643                 m_changed = true;
644             }
645         }
646 
647     private:
648         Graph&amp; m_graph;
649 
650         bool m_changed;
651         Node* m_node;
652         BasicBlock* m_block;
653 
654         Maps m_maps;
655 
656         InsertionSet m_insertionSet;
657     };
658 
659     BlockCSE&lt;SmallMaps&gt; m_smallBlock;
660     BlockCSE&lt;LargeMaps&gt; m_largeBlock;
661     BlockCSE&lt;HugeMaps&gt; m_hugeBlock;
662 };
663 
664 class GlobalCSEPhase : public Phase {
665 public:
666     GlobalCSEPhase(Graph&amp; graph)
667         : Phase(graph, &quot;global common subexpression elimination&quot;)
668         , m_impureDataMap(graph)
669         , m_insertionSet(graph)
670     {
671     }
672 
673     bool run()
674     {
675         ASSERT(m_graph.m_fixpointState == FixpointNotConverged);
676         ASSERT(m_graph.m_form == SSA);
677 
678         m_graph.initializeNodeOwners();
679         m_graph.ensureSSADominators();
680 
681         m_preOrder = m_graph.blocksInPreOrder();
682 
683         // First figure out what gets clobbered by blocks. Node that this uses the preOrder list
684         // for convenience only.
685         for (unsigned i = m_preOrder.size(); i--;) {
686             m_block = m_preOrder[i];
687             m_impureData = &amp;m_impureDataMap[m_block];
688             for (unsigned nodeIndex = m_block-&gt;size(); nodeIndex--;)
689                 addWrites(m_graph, m_block-&gt;at(nodeIndex), m_impureData-&gt;writes);
690         }
691 
692         // Based on my experience doing this before, what follows might have to be made iterative.
693         // Right now it doesn&#39;t have to be iterative because everything is dominator-bsed. But when
694         // validation is enabled, we check if iterating would find new CSE opportunities.
695 
696         bool changed = iterate();
697 
698         // FIXME: It should be possible to assert that CSE will not find any new opportunities if you
699         // run it a second time. Unfortunately, we cannot assert this right now. Note that if we did
700         // this, we&#39;d have to first reset all of our state.
701         // https://bugs.webkit.org/show_bug.cgi?id=145853
702 
703         return changed;
704     }
705 
706     bool iterate()
707     {
708         if (DFGCSEPhaseInternal::verbose)
709             dataLog(&quot;Performing iteration.\n&quot;);
710 
711         m_changed = false;
712         m_graph.clearReplacements();
713 
714         for (unsigned i = 0; i &lt; m_preOrder.size(); ++i) {
715             m_block = m_preOrder[i];
716             m_impureData = &amp;m_impureDataMap[m_block];
717             m_writesSoFar.clear();
718 
719             if (DFGCSEPhaseInternal::verbose)
720                 dataLog(&quot;Processing block &quot;, *m_block, &quot;:\n&quot;);
721 
722             for (unsigned nodeIndex = 0; nodeIndex &lt; m_block-&gt;size(); ++nodeIndex) {
723                 m_nodeIndex = nodeIndex;
724                 m_node = m_block-&gt;at(nodeIndex);
725                 if (DFGCSEPhaseInternal::verbose)
726                     dataLog(&quot;  Looking at node &quot;, m_node, &quot;:\n&quot;);
727 
728                 m_graph.performSubstitution(m_node);
729 
730                 if (m_node-&gt;op() == Identity || m_node-&gt;op() == IdentityWithProfile) {
731                     m_node-&gt;replaceWith(m_graph, m_node-&gt;child1().node());
732                     m_changed = true;
733                 } else
734                     clobberize(m_graph, m_node, *this);
735             }
736 
737             m_insertionSet.execute(m_block);
738 
739             m_impureData-&gt;didVisit = true;
740         }
741 
742         return m_changed;
743     }
744 
745     void read(AbstractHeap) { }
746 
747     void write(AbstractHeap heap)
748     {
749         bool clobberConservatively = false;
750         m_impureData-&gt;availableAtTail.clobber(heap, clobberConservatively);
751         m_writesSoFar.add(heap);
752     }
753 
754     void def(PureValue value)
755     {
756         // With pure values we do not have to worry about the possibility of some control flow path
757         // clobbering the value. So, we just search for all of the like values that have been
758         // computed. We pick one that is in a block that dominates ours. Note that this means that
759         // a PureValue will map to a list of nodes, since there may be many places in the control
760         // flow graph that compute a value but only one of them that dominates us. We may build up
761         // a large list of nodes that compute some value in the case of gnarly control flow. This
762         // is probably OK.
763 
764         auto result = m_pureValues.add(value, Vector&lt;Node*&gt;());
765         if (result.isNewEntry) {
766             result.iterator-&gt;value.append(m_node);
767             return;
768         }
769 
770         for (unsigned i = result.iterator-&gt;value.size(); i--;) {
771             Node* candidate = result.iterator-&gt;value[i];
772             if (m_graph.m_ssaDominators-&gt;dominates(candidate-&gt;owner, m_block)) {
773                 m_node-&gt;replaceWith(m_graph, candidate);
774                 m_changed = true;
775                 return;
776             }
777         }
778 
779         result.iterator-&gt;value.append(m_node);
780     }
781 
782     LazyNode findReplacement(HeapLocation location)
783     {
784         // At this instant, our &quot;availableAtTail&quot; reflects the set of things that are available in
785         // this block so far. We check this map to find block-local CSE opportunities before doing
786         // a global search.
787         LazyNode match = m_impureData-&gt;availableAtTail.get(location);
788         if (!!match) {
789             if (DFGCSEPhaseInternal::verbose)
790                 dataLog(&quot;      Found local match: &quot;, match, &quot;\n&quot;);
791             return match;
792         }
793 
794         // If it&#39;s not available at this point in the block, and at some prior point in the block
795         // we have clobbered this heap location, then there is no point in doing a global search.
796         if (m_writesSoFar.overlaps(location.heap())) {
797             if (DFGCSEPhaseInternal::verbose)
798                 dataLog(&quot;      Not looking globally because of local clobber: &quot;, m_writesSoFar, &quot;\n&quot;);
799             return nullptr;
800         }
801 
802         // This perfoms a backward search over the control flow graph to find some possible
803         // non-local def() that matches our heap location. Such a match is only valid if there does
804         // not exist any path from that def() to our block that contains a write() that overlaps
805         // our heap. This algorithm looks for both of these things (the matching def and the
806         // overlapping writes) in one backwards DFS pass.
807         //
808         // This starts by looking at the starting block&#39;s predecessors, and then it continues along
809         // their predecessors. As soon as this finds a possible def() - one that defines the heap
810         // location we want while dominating our starting block - it assumes that this one must be
811         // the match. It then lets the DFS over predecessors complete, but it doesn&#39;t add the
812         // def()&#39;s predecessors; this ensures that any blocks we visit thereafter are on some path
813         // from the def() to us. As soon as the DFG finds a write() that overlaps the location&#39;s
814         // heap, it stops, assuming that there is no possible match. Note that the write() case may
815         // trigger before we find a def(), or after. Either way, the write() case causes this
816         // function to immediately return nullptr.
817         //
818         // If the write() is found before we find the def(), then we know that any def() we would
819         // find would have a path to us that trips over the write() and hence becomes invalid. This
820         // is just a direct outcome of us looking for a def() that dominates us. Given a block A
821         // that dominates block B - so that A is the one that would have the def() and B is our
822         // starting block - we know that any other block must either be on the path from A to B, or
823         // it must be on a path from the root to A, but not both. So, if we haven&#39;t found A yet but
824         // we already have found a block C that has a write(), then C must be on some path from A
825         // to B, which means that A&#39;s def() is invalid for our purposes. Hence, before we find the
826         // def(), stopping on write() is the right thing to do.
827         //
828         // Stopping on write() is also the right thing to do after we find the def(). After we find
829         // the def(), we don&#39;t add that block&#39;s predecessors to the search worklist. That means
830         // that henceforth the only blocks we will see in the search are blocks on the path from
831         // the def() to us. If any such block has a write() that clobbers our heap then we should
832         // give up.
833         //
834         // Hence this graph search algorithm ends up being deceptively simple: any overlapping
835         // write() causes us to immediately return nullptr, and a matching def() means that we just
836         // record it and neglect to visit its precessors.
837 
838         Vector&lt;BasicBlock*, 8&gt; worklist;
839         Vector&lt;BasicBlock*, 8&gt; seenList;
840         BitVector seen;
841 
842         for (unsigned i = m_block-&gt;predecessors.size(); i--;) {
843             BasicBlock* predecessor = m_block-&gt;predecessors[i];
844             if (!seen.get(predecessor-&gt;index)) {
845                 worklist.append(predecessor);
846                 seen.set(predecessor-&gt;index);
847             }
848         }
849 
850         while (!worklist.isEmpty()) {
851             BasicBlock* block = worklist.takeLast();
852             seenList.append(block);
853 
854             if (DFGCSEPhaseInternal::verbose)
855                 dataLog(&quot;      Searching in block &quot;, *block, &quot;\n&quot;);
856             ImpureBlockData&amp; data = m_impureDataMap[block];
857 
858             // We require strict domination because this would only see things in our own block if
859             // they came *after* our position in the block. Clearly, while our block dominates
860             // itself, the things in the block after us don&#39;t dominate us.
861             if (m_graph.m_ssaDominators-&gt;strictlyDominates(block, m_block)) {
862                 if (DFGCSEPhaseInternal::verbose)
863                     dataLog(&quot;        It strictly dominates.\n&quot;);
864                 DFG_ASSERT(m_graph, m_node, data.didVisit);
865                 DFG_ASSERT(m_graph, m_node, !match);
866                 match = data.availableAtTail.get(location);
867                 if (DFGCSEPhaseInternal::verbose)
868                     dataLog(&quot;        Availability: &quot;, match, &quot;\n&quot;);
869                 if (!!match) {
870                     // Don&#39;t examine the predecessors of a match. At this point we just want to
871                     // establish that other blocks on the path from here to there don&#39;t clobber
872                     // the location we&#39;re interested in.
873                     continue;
874                 }
875             }
876 
877             if (DFGCSEPhaseInternal::verbose)
878                 dataLog(&quot;        Dealing with write set &quot;, data.writes, &quot;\n&quot;);
879             if (data.writes.overlaps(location.heap())) {
880                 if (DFGCSEPhaseInternal::verbose)
881                     dataLog(&quot;        Clobbered.\n&quot;);
882                 return nullptr;
883             }
884 
885             for (unsigned i = block-&gt;predecessors.size(); i--;) {
886                 BasicBlock* predecessor = block-&gt;predecessors[i];
887                 if (!seen.get(predecessor-&gt;index)) {
888                     worklist.append(predecessor);
889                     seen.set(predecessor-&gt;index);
890                 }
891             }
892         }
893 
894         if (!match)
895             return nullptr;
896 
897         // Cache the results for next time. We cache them both for this block and for all of our
898         // predecessors, since even though we&#39;ve already visited our predecessors, our predecessors
899         // probably have successors other than us.
900         // FIXME: Consider caching failed searches as well, when match is null. It&#39;s not clear that
901         // the reduction in compile time would warrant the increase in complexity, though.
902         // https://bugs.webkit.org/show_bug.cgi?id=134876
903         for (BasicBlock* block : seenList)
904             m_impureDataMap[block].availableAtTail.add(location, match);
905         m_impureData-&gt;availableAtTail.add(location, match);
906 
907         return match;
908     }
909 
910     void def(HeapLocation location, LazyNode value)
911     {
912         if (DFGCSEPhaseInternal::verbose)
913             dataLog(&quot;    Got heap location def: &quot;, location, &quot; -&gt; &quot;, value, &quot;\n&quot;);
914 
915         LazyNode match = findReplacement(location);
916 
917         if (DFGCSEPhaseInternal::verbose)
918             dataLog(&quot;      Got match: &quot;, match, &quot;\n&quot;);
919 
920         if (!match) {
921             if (DFGCSEPhaseInternal::verbose)
922                 dataLog(&quot;      Adding at-tail mapping: &quot;, location, &quot; -&gt; &quot;, value, &quot;\n&quot;);
923             auto result = m_impureData-&gt;availableAtTail.add(location, value);
924             ASSERT_UNUSED(result, !result);
925             return;
926         }
927 
928         if (value.isNode() &amp;&amp; value.asNode() == m_node) {
929             if (!match.isNode()) {
930                 // We need to properly record the constant in order to use an existing one if applicable.
931                 // This ensures that re-running GCSE will not find new optimizations.
932                 match.ensureIsNode(m_insertionSet, m_block, m_nodeIndex)-&gt;owner = m_block;
933                 auto result = m_pureValues.add(PureValue(match.asNode(), match-&gt;constant()), Vector&lt;Node*&gt;());
934                 bool replaced = false;
935                 if (!result.isNewEntry) {
936                     for (unsigned i = result.iterator-&gt;value.size(); i--;) {
937                         Node* candidate = result.iterator-&gt;value[i];
938                         if (m_graph.m_ssaDominators-&gt;dominates(candidate-&gt;owner, m_block)) {
939                             ASSERT(candidate);
940                             match-&gt;replaceWith(m_graph, candidate);
941                             match.setNode(candidate);
942                             replaced = true;
943                             break;
944                         }
945                     }
946                 }
947                 if (!replaced)
948                     result.iterator-&gt;value.append(match.asNode());
949             }
950             ASSERT(match.asNode());
951             m_node-&gt;replaceWith(m_graph, match.asNode());
952             m_changed = true;
953         }
954     }
955 
956     struct ImpureBlockData {
957         ImpureBlockData()
958             : didVisit(false)
959         {
960         }
961 
962         ClobberSet writes;
963         ImpureMap availableAtTail;
964         bool didVisit;
965     };
966 
967     Vector&lt;BasicBlock*&gt; m_preOrder;
968 
969     PureMultiMap m_pureValues;
970     BlockMap&lt;ImpureBlockData&gt; m_impureDataMap;
971 
972     BasicBlock* m_block;
973     Node* m_node;
974     unsigned m_nodeIndex;
975     ImpureBlockData* m_impureData;
976     ClobberSet m_writesSoFar;
977     InsertionSet m_insertionSet;
978 
979     bool m_changed;
980 };
981 
982 } // anonymous namespace
983 
984 bool performLocalCSE(Graph&amp; graph)
985 {
986     return runPhase&lt;LocalCSEPhase&gt;(graph);
987 }
988 
989 bool performGlobalCSE(Graph&amp; graph)
990 {
991     return runPhase&lt;GlobalCSEPhase&gt;(graph);
992 }
993 
994 } } // namespace JSC::DFG
995 
996 #endif // ENABLE(DFG_JIT)
<a name="11" id="anc11"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="11" type="hidden" />
</body>
</html>