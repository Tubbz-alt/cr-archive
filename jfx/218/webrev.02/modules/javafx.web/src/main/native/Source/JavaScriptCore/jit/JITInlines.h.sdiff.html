<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="JITInlineCacheGenerator.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITLeftShiftGenerator.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #if ENABLE(JIT)

 29 #include &quot;JSCInlines.h&quot;
 30 
 31 namespace JSC {
 32 
<span class="line-removed"> 33 inline MacroAssembler::JumpList JIT::emitDoubleGetByVal(const Instruction* instruction, PatchableJump&amp; badType)</span>
<span class="line-removed"> 34 {</span>
<span class="line-removed"> 35 #if USE(JSVALUE64)</span>
<span class="line-removed"> 36     JSValueRegs result = JSValueRegs(regT0);</span>
<span class="line-removed"> 37 #else</span>
<span class="line-removed"> 38     JSValueRegs result = JSValueRegs(regT1, regT0);</span>
<span class="line-removed"> 39 #endif</span>
<span class="line-removed"> 40     JumpList slowCases = emitDoubleLoad(instruction, badType);</span>
<span class="line-removed"> 41     boxDouble(fpRegT0, result);</span>
<span class="line-removed"> 42     return slowCases;</span>
<span class="line-removed"> 43 }</span>
<span class="line-removed"> 44 </span>
 45 ALWAYS_INLINE MacroAssembler::JumpList JIT::emitLoadForArrayMode(const Instruction* currentInstruction, JITArrayMode arrayMode, PatchableJump&amp; badType)
 46 {
 47     switch (arrayMode) {
 48     case JITInt32:
 49         return emitInt32Load(currentInstruction, badType);
 50     case JITDouble:
 51         return emitDoubleLoad(currentInstruction, badType);
 52     case JITContiguous:
 53         return emitContiguousLoad(currentInstruction, badType);
 54     case JITArrayStorage:
 55         return emitArrayStorageLoad(currentInstruction, badType);
 56     default:
 57         break;
 58     }
 59     RELEASE_ASSERT_NOT_REACHED();
 60     return MacroAssembler::JumpList();
 61 }
 62 
<span class="line-modified"> 63 inline MacroAssembler::JumpList JIT::emitContiguousGetByVal(const Instruction* instruction, PatchableJump&amp; badType, IndexingType expectedShape)</span>
 64 {
<span class="line-modified"> 65     return emitContiguousLoad(instruction, badType, expectedShape);</span>
 66 }
 67 
<span class="line-modified"> 68 inline MacroAssembler::JumpList JIT::emitArrayStorageGetByVal(const Instruction* instruction, PatchableJump&amp; badType)</span>
 69 {
<span class="line-modified"> 70     return emitArrayStorageLoad(instruction, badType);</span>
<span class="line-removed"> 71 }</span>
<span class="line-removed"> 72 </span>
<span class="line-removed"> 73 ALWAYS_INLINE bool JIT::isOperandConstantDouble(int src)</span>
<span class="line-removed"> 74 {</span>
<span class="line-removed"> 75     return m_codeBlock-&gt;isConstantRegisterIndex(src) &amp;&amp; getConstantOperand(src).isDouble();</span>
<span class="line-removed"> 76 }</span>
<span class="line-removed"> 77 </span>
<span class="line-removed"> 78 ALWAYS_INLINE JSValue JIT::getConstantOperand(int src)</span>
<span class="line-removed"> 79 {</span>
<span class="line-removed"> 80     ASSERT(m_codeBlock-&gt;isConstantRegisterIndex(src));</span>
 81     return m_codeBlock-&gt;getConstant(src);
 82 }
 83 
<span class="line-modified"> 84 ALWAYS_INLINE void JIT::emitPutIntToCallFrameHeader(RegisterID from, int entry)</span>
 85 {

 86 #if USE(JSVALUE32_64)
 87     store32(TrustedImm32(JSValue::Int32Tag), tagFor(entry));
 88     store32(from, payloadFor(entry));
 89 #else
 90     store64(from, addressFor(entry));
 91 #endif
 92 }
 93 
 94 ALWAYS_INLINE void JIT::emitLoadCharacterString(RegisterID src, RegisterID dst, JumpList&amp; failures)
 95 {
 96     failures.append(branchIfNotString(src));
 97     loadPtr(MacroAssembler::Address(src, JSString::offsetOfValue()), dst);
 98     failures.append(branchIfRopeStringImpl(dst));
 99     failures.append(branch32(NotEqual, MacroAssembler::Address(dst, StringImpl::lengthMemoryOffset()), TrustedImm32(1)));
<span class="line-modified">100     loadPtr(MacroAssembler::Address(dst, StringImpl::flagsOffset()), regT1);</span>
<span class="line-modified">101     loadPtr(MacroAssembler::Address(dst, StringImpl::dataOffset()), dst);</span>
<span class="line-modified">102 </span>
<span class="line-modified">103     JumpList is16Bit;</span>
<span class="line-modified">104     JumpList cont8Bit;</span>
<span class="line-removed">105     is16Bit.append(branchTest32(Zero, regT1, TrustedImm32(StringImpl::flagIs8Bit())));</span>
<span class="line-removed">106     load8(MacroAssembler::Address(dst, 0), dst);</span>
<span class="line-removed">107     cont8Bit.append(jump());</span>
108     is16Bit.link(this);
<span class="line-modified">109     load16(MacroAssembler::Address(dst, 0), dst);</span>
<span class="line-modified">110     cont8Bit.link(this);</span>
111 }
112 
113 ALWAYS_INLINE JIT::Call JIT::emitNakedCall(CodePtr&lt;NoPtrTag&gt; target)
114 {
<span class="line-modified">115     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
116     Call nakedCall = nearCall();
<span class="line-modified">117     m_calls.append(CallRecord(nakedCall, m_bytecodeOffset, FunctionPtr&lt;OperationPtrTag&gt;(target.retagged&lt;OperationPtrTag&gt;())));</span>
118     return nakedCall;
119 }
120 
121 ALWAYS_INLINE JIT::Call JIT::emitNakedTailCall(CodePtr&lt;NoPtrTag&gt; target)
122 {
<span class="line-modified">123     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
124     Call nakedCall = nearTailCall();
<span class="line-modified">125     m_calls.append(CallRecord(nakedCall, m_bytecodeOffset, FunctionPtr&lt;OperationPtrTag&gt;(target.retagged&lt;OperationPtrTag&gt;())));</span>
126     return nakedCall;
127 }
128 
129 ALWAYS_INLINE void JIT::updateTopCallFrame()
130 {
<span class="line-modified">131     ASSERT(static_cast&lt;int&gt;(m_bytecodeOffset) &gt;= 0);</span>
<span class="line-modified">132 #if USE(JSVALUE32_64)</span>
<span class="line-removed">133     const Instruction* instruction = m_codeBlock-&gt;instructions().at(m_bytecodeOffset).ptr();</span>
<span class="line-removed">134     uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed">135 #else</span>
<span class="line-removed">136     uint32_t locationBits = CallSiteIndex(m_bytecodeOffset).bits();</span>
<span class="line-removed">137 #endif</span>
<span class="line-removed">138     store32(TrustedImm32(locationBits), tagFor(CallFrameSlot::argumentCount));</span>
139 
140     // FIXME: It&#39;s not clear that this is needed. JITOperations tend to update the top call frame on
141     // the C++ side.
142     // https://bugs.webkit.org/show_bug.cgi?id=155693
143     storePtr(callFrameRegister, &amp;m_vm-&gt;topCallFrame);
144 }
145 
146 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheck(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
147 {
148     updateTopCallFrame();
149     MacroAssembler::Call call = appendCall(function);
150     exceptionCheck();
151     return call;
152 }
153 
154 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
155 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckAndSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
156 {
157     updateTopCallFrame();
158     MacroAssembler::Call call = appendCallWithSlowPathReturnType(function);
159     exceptionCheck();
160     return call;
161 }
162 #endif
163 
164 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
165 {
166     updateTopCallFrame(); // The callee is responsible for setting topCallFrame to their caller
167     MacroAssembler::Call call = appendCall(function);
168     exceptionCheckWithCallFrameRollback();
169     return call;
170 }
171 
<span class="line-modified">172 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, int dst)</span>
173 {
174     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
175 #if USE(JSVALUE64)
176     emitPutVirtualRegister(dst, returnValueGPR);
177 #else
178     emitStore(dst, returnValueGPR2, returnValueGPR);
179 #endif
180     return call;
181 }
182 
183 template&lt;typename Metadata&gt;
<span class="line-modified">184 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata&amp; metadata, const FunctionPtr&lt;CFunctionPtrTag&gt; function, int dst)</span>
185 {
186     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
187     emitValueProfilingSite(metadata);
188 #if USE(JSVALUE64)
189     emitPutVirtualRegister(dst, returnValueGPR);
190 #else
191     emitStore(dst, returnValueGPR2, returnValueGPR);
192 #endif
193     return call;
194 }
195 
<span class="line-modified">196 ALWAYS_INLINE void JIT::linkSlowCaseIfNotJSCell(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, int vReg)</span>
197 {
<span class="line-modified">198     if (!m_codeBlock-&gt;isKnownNotImmediate(vReg))</span>
199         linkSlowCase(iter);
200 }
201 
<span class="line-modified">202 ALWAYS_INLINE void JIT::linkAllSlowCasesForBytecodeOffset(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, unsigned bytecodeOffset)</span>
203 {
<span class="line-modified">204     while (iter != slowCases.end() &amp;&amp; iter-&gt;to == bytecodeOffset)</span>
205         linkSlowCase(iter);
206 }
207 







208 ALWAYS_INLINE void JIT::addSlowCase(Jump jump)
209 {
<span class="line-modified">210     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
211 
<span class="line-modified">212     m_slowCases.append(SlowCaseEntry(jump, m_bytecodeOffset));</span>
213 }
214 
215 ALWAYS_INLINE void JIT::addSlowCase(const JumpList&amp; jumpList)
216 {
<span class="line-modified">217     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
218 
219     for (const Jump&amp; jump : jumpList.jumps())
<span class="line-modified">220         m_slowCases.append(SlowCaseEntry(jump, m_bytecodeOffset));</span>
221 }
222 
223 ALWAYS_INLINE void JIT::addSlowCase()
224 {
<span class="line-modified">225     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
226 
227     Jump emptyJump; // Doing it this way to make Windows happy.
<span class="line-modified">228     m_slowCases.append(SlowCaseEntry(emptyJump, m_bytecodeOffset));</span>
229 }
230 
231 ALWAYS_INLINE void JIT::addJump(Jump jump, int relativeOffset)
232 {
<span class="line-modified">233     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
234 
<span class="line-modified">235     m_jmpTable.append(JumpTable(jump, m_bytecodeOffset + relativeOffset));</span>
236 }
237 
238 ALWAYS_INLINE void JIT::addJump(const JumpList&amp; jumpList, int relativeOffset)
239 {
<span class="line-modified">240     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
241 
242     for (auto&amp; jump : jumpList.jumps())
243         addJump(jump, relativeOffset);
244 }
245 
246 ALWAYS_INLINE void JIT::emitJumpSlowToHot(Jump jump, int relativeOffset)
247 {
<span class="line-modified">248     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
249 
<span class="line-modified">250     jump.linkTo(m_labels[m_bytecodeOffset + relativeOffset], this);</span>
251 }
252 
253 #if ENABLE(SAMPLING_FLAGS)
254 ALWAYS_INLINE void JIT::setSamplingFlag(int32_t flag)
255 {
256     ASSERT(flag &gt;= 1);
257     ASSERT(flag &lt;= 32);
258     or32(TrustedImm32(1u &lt;&lt; (flag - 1)), AbsoluteAddress(SamplingFlags::addressOfFlags()));
259 }
260 
261 ALWAYS_INLINE void JIT::clearSamplingFlag(int32_t flag)
262 {
263     ASSERT(flag &gt;= 1);
264     ASSERT(flag &lt;= 32);
265     and32(TrustedImm32(~(1u &lt;&lt; (flag - 1))), AbsoluteAddress(SamplingFlags::addressOfFlags()));
266 }
267 #endif
268 
269 #if ENABLE(SAMPLING_COUNTERS)
270 ALWAYS_INLINE void JIT::emitCount(AbstractSamplingCounter&amp; counter, int32_t count)
</pre>
<hr />
<pre>
286     storePtr(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;encodeSample(instruction, inHostFunction)), m_interpreter-&gt;sampler()-&gt;sampleSlot());
287 }
288 #endif
289 #endif
290 
291 #if ENABLE(CODEBLOCK_SAMPLING)
292 #if CPU(X86_64)
293 ALWAYS_INLINE void JIT::sampleCodeBlock(CodeBlock* codeBlock)
294 {
295     move(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;codeBlockSlot()), X86Registers::ecx);
296     storePtr(TrustedImmPtr(codeBlock), X86Registers::ecx);
297 }
298 #else
299 ALWAYS_INLINE void JIT::sampleCodeBlock(CodeBlock* codeBlock)
300 {
301     storePtr(TrustedImmPtr(codeBlock), m_interpreter-&gt;sampler()-&gt;codeBlockSlot());
302 }
303 #endif
304 #endif
305 
<span class="line-modified">306 ALWAYS_INLINE bool JIT::isOperandConstantChar(int src)</span>
307 {
<span class="line-modified">308     return m_codeBlock-&gt;isConstantRegisterIndex(src) &amp;&amp; getConstantOperand(src).isString() &amp;&amp; asString(getConstantOperand(src).asCell())-&gt;length() == 1;</span>
309 }
310 
311 inline void JIT::emitValueProfilingSite(ValueProfile&amp; valueProfile)
312 {
313     ASSERT(shouldEmitProfiling());
314 
315     const RegisterID value = regT0;
316 #if USE(JSVALUE32_64)
317     const RegisterID valueTag = regT1;
318 #endif
319 
320     // We&#39;re in a simple configuration: only one bucket, so we can just do a direct
321     // store.
322 #if USE(JSVALUE64)
323     store64(value, valueProfile.m_buckets);
324 #else
325     EncodedValueDescriptor* descriptor = bitwise_cast&lt;EncodedValueDescriptor*&gt;(valueProfile.m_buckets);
326     store32(value, &amp;descriptor-&gt;asBits.payload);
327     store32(valueTag, &amp;descriptor-&gt;asBits.tag);
328 #endif
</pre>
<hr />
<pre>
365 }
366 
367 inline JITArrayMode JIT::chooseArrayMode(ArrayProfile* profile)
368 {
369     auto arrayProfileSaw = [] (ArrayModes arrayModes, IndexingType capability) {
370         return arrayModesIncludeIgnoringTypedArrays(arrayModes, capability);
371     };
372 
373     ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
374     profile-&gt;computeUpdatedPrediction(locker, m_codeBlock);
375     ArrayModes arrayModes = profile-&gt;observedArrayModes(locker);
376     if (arrayProfileSaw(arrayModes, DoubleShape))
377         return JITDouble;
378     if (arrayProfileSaw(arrayModes, Int32Shape))
379         return JITInt32;
380     if (arrayProfileSaw(arrayModes, ArrayStorageShape))
381         return JITArrayStorage;
382     return JITContiguous;
383 }
384 
<span class="line-modified">385 ALWAYS_INLINE int32_t JIT::getOperandConstantInt(int src)</span>
386 {
387     return getConstantOperand(src).asInt32();
388 }
389 
<span class="line-modified">390 ALWAYS_INLINE double JIT::getOperandConstantDouble(int src)</span>
391 {
392     return getConstantOperand(src).asDouble();
393 }
394 
<span class="line-modified">395 ALWAYS_INLINE void JIT::emitInitRegister(int dst)</span>
396 {
397     storeTrustedValue(jsUndefined(), addressFor(dst));
398 }
399 
400 #if USE(JSVALUE32_64)
401 
<span class="line-modified">402 inline void JIT::emitLoadTag(int index, RegisterID tag)</span>
403 {
<span class="line-modified">404     if (m_codeBlock-&gt;isConstantRegisterIndex(index)) {</span>
<span class="line-modified">405         move(Imm32(getConstantOperand(index).tag()), tag);</span>









406         return;
407     }
408 
<span class="line-modified">409     load32(tagFor(index), tag);</span>
410 }
411 
<span class="line-modified">412 inline void JIT::emitLoadPayload(int index, RegisterID payload)</span>
413 {
<span class="line-modified">414     if (m_codeBlock-&gt;isConstantRegisterIndex(index)) {</span>
<span class="line-modified">415         move(Imm32(getConstantOperand(index).payload()), payload);</span>
416         return;
417     }
418 
<span class="line-modified">419     load32(payloadFor(index), payload);</span>
420 }
421 
422 inline void JIT::emitLoad(const JSValue&amp; v, RegisterID tag, RegisterID payload)
423 {
424     move(Imm32(v.payload()), payload);
425     move(Imm32(v.tag()), tag);
426 }
427 
<span class="line-modified">428 ALWAYS_INLINE void JIT::emitGetVirtualRegister(int src, JSValueRegs dst)</span>
429 {
430     emitLoad(src, dst.tagGPR(), dst.payloadGPR());
431 }
432 
<span class="line-modified">433 ALWAYS_INLINE void JIT::emitPutVirtualRegister(int dst, JSValueRegs from)</span>
434 {
435     emitStore(dst, from.tagGPR(), from.payloadGPR());
436 }
437 
<span class="line-modified">438 inline void JIT::emitLoad(int index, RegisterID tag, RegisterID payload, RegisterID base)</span>
439 {
440     RELEASE_ASSERT(tag != payload);
441 
442     if (base == callFrameRegister) {
443         RELEASE_ASSERT(payload != base);
<span class="line-modified">444         emitLoadPayload(index, payload);</span>
<span class="line-modified">445         emitLoadTag(index, tag);</span>
446         return;
447     }
448 
<span class="line-removed">449     VirtualRegister target { index };</span>
450     if (payload == base) { // avoid stomping base
<span class="line-modified">451         load32(tagFor(target, base), tag);</span>
<span class="line-modified">452         load32(payloadFor(target, base), payload);</span>
453         return;
454     }
455 
<span class="line-modified">456     load32(payloadFor(target, base), payload);</span>
<span class="line-modified">457     load32(tagFor(target, base), tag);</span>
<span class="line-removed">458 }</span>
<span class="line-removed">459 </span>
<span class="line-removed">460 inline void JIT::emitLoad2(int index1, RegisterID tag1, RegisterID payload1, int index2, RegisterID tag2, RegisterID payload2)</span>
<span class="line-removed">461 {</span>
<span class="line-removed">462     emitLoad(index2, tag2, payload2);</span>
<span class="line-removed">463     emitLoad(index1, tag1, payload1);</span>
<span class="line-removed">464 }</span>
<span class="line-removed">465 </span>
<span class="line-removed">466 inline void JIT::emitLoadDouble(int index, FPRegisterID value)</span>
<span class="line-removed">467 {</span>
<span class="line-removed">468     if (m_codeBlock-&gt;isConstantRegisterIndex(index)) {</span>
<span class="line-removed">469         WriteBarrier&lt;Unknown&gt;&amp; inConstantPool = m_codeBlock-&gt;constantRegister(index);</span>
<span class="line-removed">470         loadDouble(TrustedImmPtr(&amp;inConstantPool), value);</span>
<span class="line-removed">471     } else</span>
<span class="line-removed">472         loadDouble(addressFor(index), value);</span>
473 }
474 
<span class="line-modified">475 inline void JIT::emitLoadInt32ToDouble(int index, FPRegisterID value)</span>
476 {
<span class="line-modified">477     if (m_codeBlock-&gt;isConstantRegisterIndex(index)) {</span>
<span class="line-modified">478         WriteBarrier&lt;Unknown&gt;&amp; inConstantPool = m_codeBlock-&gt;constantRegister(index);</span>
<span class="line-removed">479         char* bytePointer = reinterpret_cast&lt;char*&gt;(&amp;inConstantPool);</span>
<span class="line-removed">480         convertInt32ToDouble(AbsoluteAddress(bytePointer + OBJECT_OFFSETOF(JSValue, u.asBits.payload)), value);</span>
<span class="line-removed">481     } else</span>
<span class="line-removed">482         convertInt32ToDouble(payloadFor(index), value);</span>
483 }
484 
<span class="line-modified">485 inline void JIT::emitStore(int index, RegisterID tag, RegisterID payload, RegisterID base)</span>
486 {
<span class="line-modified">487     VirtualRegister target { index };</span>
<span class="line-modified">488     store32(payload, payloadFor(target, base));</span>
<span class="line-removed">489     store32(tag, tagFor(target, base));</span>
490 }
491 
<span class="line-modified">492 inline void JIT::emitStoreInt32(int index, RegisterID payload, bool indexIsInt32)</span>
493 {
<span class="line-modified">494     store32(payload, payloadFor(index));</span>
495     if (!indexIsInt32)
<span class="line-modified">496         store32(TrustedImm32(JSValue::Int32Tag), tagFor(index));</span>
497 }
498 
<span class="line-modified">499 inline void JIT::emitStoreInt32(int index, TrustedImm32 payload, bool indexIsInt32)</span>
500 {
<span class="line-modified">501     store32(payload, payloadFor(index));</span>
502     if (!indexIsInt32)
<span class="line-modified">503         store32(TrustedImm32(JSValue::Int32Tag), tagFor(index));</span>
504 }
505 
<span class="line-modified">506 inline void JIT::emitStoreCell(int index, RegisterID payload, bool indexIsCell)</span>
507 {
<span class="line-modified">508     store32(payload, payloadFor(index));</span>
509     if (!indexIsCell)
<span class="line-modified">510         store32(TrustedImm32(JSValue::CellTag), tagFor(index));</span>
511 }
512 
<span class="line-modified">513 inline void JIT::emitStoreBool(int index, RegisterID payload, bool indexIsBool)</span>
514 {
<span class="line-modified">515     store32(payload, payloadFor(index));</span>
516     if (!indexIsBool)
<span class="line-modified">517         store32(TrustedImm32(JSValue::BooleanTag), tagFor(index));</span>
518 }
519 
<span class="line-modified">520 inline void JIT::emitStoreDouble(int index, FPRegisterID value)</span>
521 {
<span class="line-modified">522     storeDouble(value, addressFor(index));</span>
523 }
524 
<span class="line-modified">525 inline void JIT::emitStore(int index, const JSValue constant, RegisterID base)</span>
526 {
<span class="line-modified">527     VirtualRegister target { index };</span>
<span class="line-modified">528     store32(Imm32(constant.payload()), payloadFor(target, base));</span>
<span class="line-removed">529     store32(Imm32(constant.tag()), tagFor(target, base));</span>
530 }
531 
<span class="line-modified">532 inline void JIT::emitJumpSlowCaseIfNotJSCell(int virtualRegisterIndex)</span>
533 {
<span class="line-modified">534     if (!m_codeBlock-&gt;isKnownNotImmediate(virtualRegisterIndex)) {</span>
<span class="line-modified">535         if (m_codeBlock-&gt;isConstantRegisterIndex(virtualRegisterIndex))</span>
536             addSlowCase(jump());
537         else
<span class="line-modified">538             addSlowCase(emitJumpIfNotJSCell(virtualRegisterIndex));</span>
539     }
540 }
541 
<span class="line-modified">542 inline void JIT::emitJumpSlowCaseIfNotJSCell(int virtualRegisterIndex, RegisterID tag)</span>
543 {
<span class="line-modified">544     if (!m_codeBlock-&gt;isKnownNotImmediate(virtualRegisterIndex)) {</span>
<span class="line-modified">545         if (m_codeBlock-&gt;isConstantRegisterIndex(virtualRegisterIndex))</span>
546             addSlowCase(jump());
547         else
548             addSlowCase(branchIfNotCell(tag));
549     }
550 }
551 
<span class="line-modified">552 ALWAYS_INLINE bool JIT::isOperandConstantInt(int src)</span>
553 {
<span class="line-modified">554     return m_codeBlock-&gt;isConstantRegisterIndex(src) &amp;&amp; getConstantOperand(src).isInt32();</span>
555 }
556 
<span class="line-modified">557 ALWAYS_INLINE bool JIT::getOperandConstantInt(int op1, int op2, int&amp; op, int32_t&amp; constant)</span>
558 {
559     if (isOperandConstantInt(op1)) {
560         constant = getConstantOperand(op1).asInt32();
561         op = op2;
562         return true;
563     }
564 
565     if (isOperandConstantInt(op2)) {
566         constant = getConstantOperand(op2).asInt32();
567         op = op1;
568         return true;
569     }
570 
571     return false;
572 }
573 
574 #else // USE(JSVALUE32_64)
575 
576 // get arg puts an arg from the SF register array into a h/w register
<span class="line-modified">577 ALWAYS_INLINE void JIT::emitGetVirtualRegister(int src, RegisterID dst)</span>
578 {
<span class="line-modified">579     ASSERT(m_bytecodeOffset != std::numeric_limits&lt;unsigned&gt;::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.</span>
580 
<span class="line-modified">581     if (m_codeBlock-&gt;isConstantRegisterIndex(src)) {</span>
582         JSValue value = m_codeBlock-&gt;getConstant(src);
583         if (!value.isNumber())
584             move(TrustedImm64(JSValue::encode(value)), dst);
585         else
586             move(Imm64(JSValue::encode(value)), dst);
587         return;
588     }
589 
590     load64(addressFor(src), dst);
591 }
592 
<span class="line-modified">593 ALWAYS_INLINE void JIT::emitGetVirtualRegister(int src, JSValueRegs dst)</span>
594 {
595     emitGetVirtualRegister(src, dst.payloadGPR());
596 }
597 
<span class="line-modified">598 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, RegisterID dst)</span>
<span class="line-removed">599 {</span>
<span class="line-removed">600     emitGetVirtualRegister(src.offset(), dst);</span>
<span class="line-removed">601 }</span>
<span class="line-removed">602 </span>
<span class="line-removed">603 ALWAYS_INLINE void JIT::emitGetVirtualRegisters(int src1, RegisterID dst1, int src2, RegisterID dst2)</span>
604 {
605     emitGetVirtualRegister(src1, dst1);
606     emitGetVirtualRegister(src2, dst2);
607 }
608 
<span class="line-modified">609 ALWAYS_INLINE void JIT::emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2)</span>
<span class="line-removed">610 {</span>
<span class="line-removed">611     emitGetVirtualRegisters(src1.offset(), dst1, src2.offset(), dst2);</span>
<span class="line-removed">612 }</span>
<span class="line-removed">613 </span>
<span class="line-removed">614 ALWAYS_INLINE bool JIT::isOperandConstantInt(int src)</span>
615 {
<span class="line-modified">616     return m_codeBlock-&gt;isConstantRegisterIndex(src) &amp;&amp; getConstantOperand(src).isInt32();</span>
617 }
618 
<span class="line-modified">619 ALWAYS_INLINE void JIT::emitPutVirtualRegister(int dst, RegisterID from)</span>
620 {
621     store64(from, addressFor(dst));
622 }
623 
<span class="line-modified">624 ALWAYS_INLINE void JIT::emitPutVirtualRegister(int dst, JSValueRegs from)</span>
625 {
626     emitPutVirtualRegister(dst, from.payloadGPR());
627 }
628 
<span class="line-removed">629 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, RegisterID from)</span>
<span class="line-removed">630 {</span>
<span class="line-removed">631     emitPutVirtualRegister(dst.offset(), from);</span>
<span class="line-removed">632 }</span>
<span class="line-removed">633 </span>
634 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfBothJSCells(RegisterID reg1, RegisterID reg2, RegisterID scratch)
635 {
636     move(reg1, scratch);
637     or64(reg2, scratch);
638     return branchIfCell(scratch);
639 }
640 
641 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfJSCell(RegisterID reg)
642 {
643     addSlowCase(branchIfCell(reg));
644 }
645 
646 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg)
647 {
648     addSlowCase(branchIfNotCell(reg));
649 }
650 
<span class="line-modified">651 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg, int vReg)</span>
652 {
653     if (!m_codeBlock-&gt;isKnownNotImmediate(vReg))
654         emitJumpSlowCaseIfNotJSCell(reg);
655 }
656 
<span class="line-removed">657 inline void JIT::emitLoadDouble(int index, FPRegisterID value)</span>
<span class="line-removed">658 {</span>
<span class="line-removed">659     if (m_codeBlock-&gt;isConstantRegisterIndex(index)) {</span>
<span class="line-removed">660         WriteBarrier&lt;Unknown&gt;&amp; inConstantPool = m_codeBlock-&gt;constantRegister(index);</span>
<span class="line-removed">661         loadDouble(TrustedImmPtr(&amp;inConstantPool), value);</span>
<span class="line-removed">662     } else</span>
<span class="line-removed">663         loadDouble(addressFor(index), value);</span>
<span class="line-removed">664 }</span>
<span class="line-removed">665 </span>
<span class="line-removed">666 inline void JIT::emitLoadInt32ToDouble(int index, FPRegisterID value)</span>
<span class="line-removed">667 {</span>
<span class="line-removed">668     if (m_codeBlock-&gt;isConstantRegisterIndex(index)) {</span>
<span class="line-removed">669         ASSERT(isOperandConstantInt(index));</span>
<span class="line-removed">670         convertInt32ToDouble(Imm32(getConstantOperand(index).asInt32()), value);</span>
<span class="line-removed">671     } else</span>
<span class="line-removed">672         convertInt32ToDouble(addressFor(index), value);</span>
<span class="line-removed">673 }</span>
<span class="line-removed">674 </span>
675 ALWAYS_INLINE JIT::PatchableJump JIT::emitPatchableJumpIfNotInt(RegisterID reg)
676 {
<span class="line-modified">677     return patchableBranch64(Below, reg, tagTypeNumberRegister);</span>
678 }
679 
680 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
681 {
682     move(reg1, scratch);
683     and64(reg2, scratch);
684     return branchIfNotInt32(scratch);
685 }
686 
687 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotInt(RegisterID reg)
688 {
689     addSlowCase(branchIfNotInt32(reg));
690 }
691 
692 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
693 {
694     addSlowCase(emitJumpIfNotInt(reg1, reg2, scratch));
695 }
696 
697 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotNumber(RegisterID reg)
</pre>
<hr />
<pre>
703 
704 ALWAYS_INLINE int JIT::jumpTarget(const Instruction* instruction, int target)
705 {
706     if (target)
707         return target;
708     return m_codeBlock-&gt;outOfLineJumpOffset(instruction);
709 }
710 
711 ALWAYS_INLINE GetPutInfo JIT::copiedGetPutInfo(OpPutToScope bytecode)
712 {
713     unsigned key = bytecode.m_metadataID + 1; // HashMap doesn&#39;t like 0 as a key
714     auto iterator = m_copiedGetPutInfos.find(key);
715     if (iterator != m_copiedGetPutInfos.end())
716         return GetPutInfo(iterator-&gt;value);
717     GetPutInfo getPutInfo = bytecode.metadata(m_codeBlock).m_getPutInfo;
718     m_copiedGetPutInfos.add(key, getPutInfo.operand());
719     return getPutInfo;
720 }
721 
722 template&lt;typename BinaryOp&gt;
<span class="line-modified">723 ALWAYS_INLINE ArithProfile JIT::copiedArithProfile(BinaryOp bytecode)</span>
724 {
<span class="line-modified">725     uint64_t key = static_cast&lt;uint64_t&gt;(BinaryOp::opcodeID) &lt;&lt; 32 | static_cast&lt;uint64_t&gt;(bytecode.m_metadataID);</span>
726     auto iterator = m_copiedArithProfiles.find(key);
727     if (iterator != m_copiedArithProfiles.end())
728         return iterator-&gt;value;
<span class="line-modified">729     ArithProfile arithProfile = bytecode.metadata(m_codeBlock).m_arithProfile;</span>
730     m_copiedArithProfiles.add(key, arithProfile);
731     return arithProfile;
732 }
733 
734 } // namespace JSC
735 
736 #endif // ENABLE(JIT)
</pre>
</td>
<td>
<hr />
<pre>
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #if ENABLE(JIT)
<span class="line-added"> 29 #include &quot;CommonSlowPathsInlines.h&quot;</span>
 30 #include &quot;JSCInlines.h&quot;
 31 
 32 namespace JSC {
 33 












 34 ALWAYS_INLINE MacroAssembler::JumpList JIT::emitLoadForArrayMode(const Instruction* currentInstruction, JITArrayMode arrayMode, PatchableJump&amp; badType)
 35 {
 36     switch (arrayMode) {
 37     case JITInt32:
 38         return emitInt32Load(currentInstruction, badType);
 39     case JITDouble:
 40         return emitDoubleLoad(currentInstruction, badType);
 41     case JITContiguous:
 42         return emitContiguousLoad(currentInstruction, badType);
 43     case JITArrayStorage:
 44         return emitArrayStorageLoad(currentInstruction, badType);
 45     default:
 46         break;
 47     }
 48     RELEASE_ASSERT_NOT_REACHED();
 49     return MacroAssembler::JumpList();
 50 }
 51 
<span class="line-modified"> 52 ALWAYS_INLINE bool JIT::isOperandConstantDouble(VirtualRegister src)</span>
 53 {
<span class="line-modified"> 54     return src.isConstant() &amp;&amp; getConstantOperand(src).isDouble();</span>
 55 }
 56 
<span class="line-modified"> 57 ALWAYS_INLINE JSValue JIT::getConstantOperand(VirtualRegister src)</span>
 58 {
<span class="line-modified"> 59     ASSERT(src.isConstant());</span>










 60     return m_codeBlock-&gt;getConstant(src);
 61 }
 62 
<span class="line-modified"> 63 ALWAYS_INLINE void JIT::emitPutIntToCallFrameHeader(RegisterID from, VirtualRegister entry)</span>
 64 {
<span class="line-added"> 65     ASSERT(entry.isHeader());</span>
 66 #if USE(JSVALUE32_64)
 67     store32(TrustedImm32(JSValue::Int32Tag), tagFor(entry));
 68     store32(from, payloadFor(entry));
 69 #else
 70     store64(from, addressFor(entry));
 71 #endif
 72 }
 73 
 74 ALWAYS_INLINE void JIT::emitLoadCharacterString(RegisterID src, RegisterID dst, JumpList&amp; failures)
 75 {
 76     failures.append(branchIfNotString(src));
 77     loadPtr(MacroAssembler::Address(src, JSString::offsetOfValue()), dst);
 78     failures.append(branchIfRopeStringImpl(dst));
 79     failures.append(branch32(NotEqual, MacroAssembler::Address(dst, StringImpl::lengthMemoryOffset()), TrustedImm32(1)));
<span class="line-modified"> 80     loadPtr(MacroAssembler::Address(dst, StringImpl::dataOffset()), regT1);</span>
<span class="line-modified"> 81 </span>
<span class="line-modified"> 82     auto is16Bit = branchTest32(Zero, Address(dst, StringImpl::flagsOffset()), TrustedImm32(StringImpl::flagIs8Bit()));</span>
<span class="line-modified"> 83     load8(MacroAssembler::Address(regT1, 0), dst);</span>
<span class="line-modified"> 84     auto done = jump();</span>



 85     is16Bit.link(this);
<span class="line-modified"> 86     load16(MacroAssembler::Address(regT1, 0), dst);</span>
<span class="line-modified"> 87     done.link(this);</span>
 88 }
 89 
 90 ALWAYS_INLINE JIT::Call JIT::emitNakedCall(CodePtr&lt;NoPtrTag&gt; target)
 91 {
<span class="line-modified"> 92     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
 93     Call nakedCall = nearCall();
<span class="line-modified"> 94     m_calls.append(CallRecord(nakedCall, m_bytecodeIndex, FunctionPtr&lt;OperationPtrTag&gt;(target.retagged&lt;OperationPtrTag&gt;())));</span>
 95     return nakedCall;
 96 }
 97 
 98 ALWAYS_INLINE JIT::Call JIT::emitNakedTailCall(CodePtr&lt;NoPtrTag&gt; target)
 99 {
<span class="line-modified">100     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
101     Call nakedCall = nearTailCall();
<span class="line-modified">102     m_calls.append(CallRecord(nakedCall, m_bytecodeIndex, FunctionPtr&lt;OperationPtrTag&gt;(target.retagged&lt;OperationPtrTag&gt;())));</span>
103     return nakedCall;
104 }
105 
106 ALWAYS_INLINE void JIT::updateTopCallFrame()
107 {
<span class="line-modified">108     uint32_t locationBits = CallSiteIndex(m_bytecodeIndex).bits();</span>
<span class="line-modified">109     store32(TrustedImm32(locationBits), tagFor(CallFrameSlot::argumentCountIncludingThis));</span>






110 
111     // FIXME: It&#39;s not clear that this is needed. JITOperations tend to update the top call frame on
112     // the C++ side.
113     // https://bugs.webkit.org/show_bug.cgi?id=155693
114     storePtr(callFrameRegister, &amp;m_vm-&gt;topCallFrame);
115 }
116 
117 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheck(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
118 {
119     updateTopCallFrame();
120     MacroAssembler::Call call = appendCall(function);
121     exceptionCheck();
122     return call;
123 }
124 
125 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
126 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckAndSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
127 {
128     updateTopCallFrame();
129     MacroAssembler::Call call = appendCallWithSlowPathReturnType(function);
130     exceptionCheck();
131     return call;
132 }
133 #endif
134 
135 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
136 {
137     updateTopCallFrame(); // The callee is responsible for setting topCallFrame to their caller
138     MacroAssembler::Call call = appendCall(function);
139     exceptionCheckWithCallFrameRollback();
140     return call;
141 }
142 
<span class="line-modified">143 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr&lt;CFunctionPtrTag&gt; function, VirtualRegister dst)</span>
144 {
145     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
146 #if USE(JSVALUE64)
147     emitPutVirtualRegister(dst, returnValueGPR);
148 #else
149     emitStore(dst, returnValueGPR2, returnValueGPR);
150 #endif
151     return call;
152 }
153 
154 template&lt;typename Metadata&gt;
<span class="line-modified">155 ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata&amp; metadata, const FunctionPtr&lt;CFunctionPtrTag&gt; function, VirtualRegister dst)</span>
156 {
157     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
158     emitValueProfilingSite(metadata);
159 #if USE(JSVALUE64)
160     emitPutVirtualRegister(dst, returnValueGPR);
161 #else
162     emitStore(dst, returnValueGPR2, returnValueGPR);
163 #endif
164     return call;
165 }
166 
<span class="line-modified">167 ALWAYS_INLINE void JIT::linkSlowCaseIfNotJSCell(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, VirtualRegister reg)</span>
168 {
<span class="line-modified">169     if (!m_codeBlock-&gt;isKnownNotImmediate(reg))</span>
170         linkSlowCase(iter);
171 }
172 
<span class="line-modified">173 ALWAYS_INLINE void JIT::linkAllSlowCasesForBytecodeIndex(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, BytecodeIndex bytecodeIndex)</span>
174 {
<span class="line-modified">175     while (iter != slowCases.end() &amp;&amp; iter-&gt;to == bytecodeIndex)</span>
176         linkSlowCase(iter);
177 }
178 
<span class="line-added">179 ALWAYS_INLINE bool JIT::hasAnySlowCases(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, BytecodeIndex bytecodeIndex)</span>
<span class="line-added">180 {</span>
<span class="line-added">181     if (iter != slowCases.end() &amp;&amp; iter-&gt;to == bytecodeIndex)</span>
<span class="line-added">182         return true;</span>
<span class="line-added">183     return false;</span>
<span class="line-added">184 }</span>
<span class="line-added">185 </span>
186 ALWAYS_INLINE void JIT::addSlowCase(Jump jump)
187 {
<span class="line-modified">188     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
189 
<span class="line-modified">190     m_slowCases.append(SlowCaseEntry(jump, m_bytecodeIndex));</span>
191 }
192 
193 ALWAYS_INLINE void JIT::addSlowCase(const JumpList&amp; jumpList)
194 {
<span class="line-modified">195     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
196 
197     for (const Jump&amp; jump : jumpList.jumps())
<span class="line-modified">198         m_slowCases.append(SlowCaseEntry(jump, m_bytecodeIndex));</span>
199 }
200 
201 ALWAYS_INLINE void JIT::addSlowCase()
202 {
<span class="line-modified">203     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
204 
205     Jump emptyJump; // Doing it this way to make Windows happy.
<span class="line-modified">206     m_slowCases.append(SlowCaseEntry(emptyJump, m_bytecodeIndex));</span>
207 }
208 
209 ALWAYS_INLINE void JIT::addJump(Jump jump, int relativeOffset)
210 {
<span class="line-modified">211     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
212 
<span class="line-modified">213     m_jmpTable.append(JumpTable(jump, m_bytecodeIndex.offset() + relativeOffset));</span>
214 }
215 
216 ALWAYS_INLINE void JIT::addJump(const JumpList&amp; jumpList, int relativeOffset)
217 {
<span class="line-modified">218     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
219 
220     for (auto&amp; jump : jumpList.jumps())
221         addJump(jump, relativeOffset);
222 }
223 
224 ALWAYS_INLINE void JIT::emitJumpSlowToHot(Jump jump, int relativeOffset)
225 {
<span class="line-modified">226     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
227 
<span class="line-modified">228     jump.linkTo(m_labels[m_bytecodeIndex.offset() + relativeOffset], this);</span>
229 }
230 
231 #if ENABLE(SAMPLING_FLAGS)
232 ALWAYS_INLINE void JIT::setSamplingFlag(int32_t flag)
233 {
234     ASSERT(flag &gt;= 1);
235     ASSERT(flag &lt;= 32);
236     or32(TrustedImm32(1u &lt;&lt; (flag - 1)), AbsoluteAddress(SamplingFlags::addressOfFlags()));
237 }
238 
239 ALWAYS_INLINE void JIT::clearSamplingFlag(int32_t flag)
240 {
241     ASSERT(flag &gt;= 1);
242     ASSERT(flag &lt;= 32);
243     and32(TrustedImm32(~(1u &lt;&lt; (flag - 1))), AbsoluteAddress(SamplingFlags::addressOfFlags()));
244 }
245 #endif
246 
247 #if ENABLE(SAMPLING_COUNTERS)
248 ALWAYS_INLINE void JIT::emitCount(AbstractSamplingCounter&amp; counter, int32_t count)
</pre>
<hr />
<pre>
264     storePtr(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;encodeSample(instruction, inHostFunction)), m_interpreter-&gt;sampler()-&gt;sampleSlot());
265 }
266 #endif
267 #endif
268 
269 #if ENABLE(CODEBLOCK_SAMPLING)
270 #if CPU(X86_64)
271 ALWAYS_INLINE void JIT::sampleCodeBlock(CodeBlock* codeBlock)
272 {
273     move(TrustedImmPtr(m_interpreter-&gt;sampler()-&gt;codeBlockSlot()), X86Registers::ecx);
274     storePtr(TrustedImmPtr(codeBlock), X86Registers::ecx);
275 }
276 #else
277 ALWAYS_INLINE void JIT::sampleCodeBlock(CodeBlock* codeBlock)
278 {
279     storePtr(TrustedImmPtr(codeBlock), m_interpreter-&gt;sampler()-&gt;codeBlockSlot());
280 }
281 #endif
282 #endif
283 
<span class="line-modified">284 ALWAYS_INLINE bool JIT::isOperandConstantChar(VirtualRegister src)</span>
285 {
<span class="line-modified">286     return src.isConstant() &amp;&amp; getConstantOperand(src).isString() &amp;&amp; asString(getConstantOperand(src).asCell())-&gt;length() == 1;</span>
287 }
288 
289 inline void JIT::emitValueProfilingSite(ValueProfile&amp; valueProfile)
290 {
291     ASSERT(shouldEmitProfiling());
292 
293     const RegisterID value = regT0;
294 #if USE(JSVALUE32_64)
295     const RegisterID valueTag = regT1;
296 #endif
297 
298     // We&#39;re in a simple configuration: only one bucket, so we can just do a direct
299     // store.
300 #if USE(JSVALUE64)
301     store64(value, valueProfile.m_buckets);
302 #else
303     EncodedValueDescriptor* descriptor = bitwise_cast&lt;EncodedValueDescriptor*&gt;(valueProfile.m_buckets);
304     store32(value, &amp;descriptor-&gt;asBits.payload);
305     store32(valueTag, &amp;descriptor-&gt;asBits.tag);
306 #endif
</pre>
<hr />
<pre>
343 }
344 
345 inline JITArrayMode JIT::chooseArrayMode(ArrayProfile* profile)
346 {
347     auto arrayProfileSaw = [] (ArrayModes arrayModes, IndexingType capability) {
348         return arrayModesIncludeIgnoringTypedArrays(arrayModes, capability);
349     };
350 
351     ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
352     profile-&gt;computeUpdatedPrediction(locker, m_codeBlock);
353     ArrayModes arrayModes = profile-&gt;observedArrayModes(locker);
354     if (arrayProfileSaw(arrayModes, DoubleShape))
355         return JITDouble;
356     if (arrayProfileSaw(arrayModes, Int32Shape))
357         return JITInt32;
358     if (arrayProfileSaw(arrayModes, ArrayStorageShape))
359         return JITArrayStorage;
360     return JITContiguous;
361 }
362 
<span class="line-modified">363 ALWAYS_INLINE int32_t JIT::getOperandConstantInt(VirtualRegister src)</span>
364 {
365     return getConstantOperand(src).asInt32();
366 }
367 
<span class="line-modified">368 ALWAYS_INLINE double JIT::getOperandConstantDouble(VirtualRegister src)</span>
369 {
370     return getConstantOperand(src).asDouble();
371 }
372 
<span class="line-modified">373 ALWAYS_INLINE void JIT::emitInitRegister(VirtualRegister dst)</span>
374 {
375     storeTrustedValue(jsUndefined(), addressFor(dst));
376 }
377 
378 #if USE(JSVALUE32_64)
379 
<span class="line-modified">380 inline void JIT::emitLoadDouble(VirtualRegister reg, FPRegisterID value)</span>
381 {
<span class="line-modified">382     if (reg.isConstant()) {</span>
<span class="line-modified">383         WriteBarrier&lt;Unknown&gt;&amp; inConstantPool = m_codeBlock-&gt;constantRegister(reg);</span>
<span class="line-added">384         loadDouble(TrustedImmPtr(&amp;inConstantPool), value);</span>
<span class="line-added">385     } else</span>
<span class="line-added">386         loadDouble(addressFor(reg), value);</span>
<span class="line-added">387 }</span>
<span class="line-added">388 </span>
<span class="line-added">389 inline void JIT::emitLoadTag(VirtualRegister reg, RegisterID tag)</span>
<span class="line-added">390 {</span>
<span class="line-added">391     if (reg.isConstant()) {</span>
<span class="line-added">392         move(Imm32(getConstantOperand(reg).tag()), tag);</span>
393         return;
394     }
395 
<span class="line-modified">396     load32(tagFor(reg), tag);</span>
397 }
398 
<span class="line-modified">399 inline void JIT::emitLoadPayload(VirtualRegister reg, RegisterID payload)</span>
400 {
<span class="line-modified">401     if (reg.isConstant()) {</span>
<span class="line-modified">402         move(Imm32(getConstantOperand(reg).payload()), payload);</span>
403         return;
404     }
405 
<span class="line-modified">406     load32(payloadFor(reg), payload);</span>
407 }
408 
409 inline void JIT::emitLoad(const JSValue&amp; v, RegisterID tag, RegisterID payload)
410 {
411     move(Imm32(v.payload()), payload);
412     move(Imm32(v.tag()), tag);
413 }
414 
<span class="line-modified">415 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst)</span>
416 {
417     emitLoad(src, dst.tagGPR(), dst.payloadGPR());
418 }
419 
<span class="line-modified">420 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, JSValueRegs from)</span>
421 {
422     emitStore(dst, from.tagGPR(), from.payloadGPR());
423 }
424 
<span class="line-modified">425 inline void JIT::emitLoad(VirtualRegister reg, RegisterID tag, RegisterID payload, RegisterID base)</span>
426 {
427     RELEASE_ASSERT(tag != payload);
428 
429     if (base == callFrameRegister) {
430         RELEASE_ASSERT(payload != base);
<span class="line-modified">431         emitLoadPayload(reg, payload);</span>
<span class="line-modified">432         emitLoadTag(reg, tag);</span>
433         return;
434     }
435 

436     if (payload == base) { // avoid stomping base
<span class="line-modified">437         load32(tagFor(reg, base), tag);</span>
<span class="line-modified">438         load32(payloadFor(reg, base), payload);</span>
439         return;
440     }
441 
<span class="line-modified">442     load32(payloadFor(reg, base), payload);</span>
<span class="line-modified">443     load32(tagFor(reg, base), tag);</span>















444 }
445 
<span class="line-modified">446 inline void JIT::emitLoad2(VirtualRegister reg1, RegisterID tag1, RegisterID payload1, VirtualRegister reg2, RegisterID tag2, RegisterID payload2)</span>
447 {
<span class="line-modified">448     emitLoad(reg2, tag2, payload2);</span>
<span class="line-modified">449     emitLoad(reg1, tag1, payload1);</span>




450 }
451 
<span class="line-modified">452 inline void JIT::emitStore(VirtualRegister reg, RegisterID tag, RegisterID payload, RegisterID base)</span>
453 {
<span class="line-modified">454     store32(payload, payloadFor(reg, base));</span>
<span class="line-modified">455     store32(tag, tagFor(reg, base));</span>

456 }
457 
<span class="line-modified">458 inline void JIT::emitStoreInt32(VirtualRegister reg, RegisterID payload, bool indexIsInt32)</span>
459 {
<span class="line-modified">460     store32(payload, payloadFor(reg));</span>
461     if (!indexIsInt32)
<span class="line-modified">462         store32(TrustedImm32(JSValue::Int32Tag), tagFor(reg));</span>
463 }
464 
<span class="line-modified">465 inline void JIT::emitStoreInt32(VirtualRegister reg, TrustedImm32 payload, bool indexIsInt32)</span>
466 {
<span class="line-modified">467     store32(payload, payloadFor(reg));</span>
468     if (!indexIsInt32)
<span class="line-modified">469         store32(TrustedImm32(JSValue::Int32Tag), tagFor(reg));</span>
470 }
471 
<span class="line-modified">472 inline void JIT::emitStoreCell(VirtualRegister reg, RegisterID payload, bool indexIsCell)</span>
473 {
<span class="line-modified">474     store32(payload, payloadFor(reg));</span>
475     if (!indexIsCell)
<span class="line-modified">476         store32(TrustedImm32(JSValue::CellTag), tagFor(reg));</span>
477 }
478 
<span class="line-modified">479 inline void JIT::emitStoreBool(VirtualRegister reg, RegisterID payload, bool indexIsBool)</span>
480 {
<span class="line-modified">481     store32(payload, payloadFor(reg));</span>
482     if (!indexIsBool)
<span class="line-modified">483         store32(TrustedImm32(JSValue::BooleanTag), tagFor(reg));</span>
484 }
485 
<span class="line-modified">486 inline void JIT::emitStoreDouble(VirtualRegister reg, FPRegisterID value)</span>
487 {
<span class="line-modified">488     storeDouble(value, addressFor(reg));</span>
489 }
490 
<span class="line-modified">491 inline void JIT::emitStore(VirtualRegister reg, const JSValue constant, RegisterID base)</span>
492 {
<span class="line-modified">493     store32(Imm32(constant.payload()), payloadFor(reg, base));</span>
<span class="line-modified">494     store32(Imm32(constant.tag()), tagFor(reg, base));</span>

495 }
496 
<span class="line-modified">497 inline void JIT::emitJumpSlowCaseIfNotJSCell(VirtualRegister reg)</span>
498 {
<span class="line-modified">499     if (!m_codeBlock-&gt;isKnownNotImmediate(reg)) {</span>
<span class="line-modified">500         if (reg.isConstant())</span>
501             addSlowCase(jump());
502         else
<span class="line-modified">503             addSlowCase(emitJumpIfNotJSCell(reg));</span>
504     }
505 }
506 
<span class="line-modified">507 inline void JIT::emitJumpSlowCaseIfNotJSCell(VirtualRegister reg, RegisterID tag)</span>
508 {
<span class="line-modified">509     if (!m_codeBlock-&gt;isKnownNotImmediate(reg)) {</span>
<span class="line-modified">510         if (reg.isConstant())</span>
511             addSlowCase(jump());
512         else
513             addSlowCase(branchIfNotCell(tag));
514     }
515 }
516 
<span class="line-modified">517 ALWAYS_INLINE bool JIT::isOperandConstantInt(VirtualRegister src)</span>
518 {
<span class="line-modified">519     return src.isConstant() &amp;&amp; getConstantOperand(src).isInt32();</span>
520 }
521 
<span class="line-modified">522 ALWAYS_INLINE bool JIT::getOperandConstantInt(VirtualRegister op1, VirtualRegister op2, VirtualRegister&amp; op, int32_t&amp; constant)</span>
523 {
524     if (isOperandConstantInt(op1)) {
525         constant = getConstantOperand(op1).asInt32();
526         op = op2;
527         return true;
528     }
529 
530     if (isOperandConstantInt(op2)) {
531         constant = getConstantOperand(op2).asInt32();
532         op = op1;
533         return true;
534     }
535 
536     return false;
537 }
538 
539 #else // USE(JSVALUE32_64)
540 
541 // get arg puts an arg from the SF register array into a h/w register
<span class="line-modified">542 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, RegisterID dst)</span>
543 {
<span class="line-modified">544     ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.</span>
545 
<span class="line-modified">546     if (src.isConstant()) {</span>
547         JSValue value = m_codeBlock-&gt;getConstant(src);
548         if (!value.isNumber())
549             move(TrustedImm64(JSValue::encode(value)), dst);
550         else
551             move(Imm64(JSValue::encode(value)), dst);
552         return;
553     }
554 
555     load64(addressFor(src), dst);
556 }
557 
<span class="line-modified">558 ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst)</span>
559 {
560     emitGetVirtualRegister(src, dst.payloadGPR());
561 }
562 
<span class="line-modified">563 ALWAYS_INLINE void JIT::emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2)</span>





564 {
565     emitGetVirtualRegister(src1, dst1);
566     emitGetVirtualRegister(src2, dst2);
567 }
568 
<span class="line-modified">569 ALWAYS_INLINE bool JIT::isOperandConstantInt(VirtualRegister src)</span>





570 {
<span class="line-modified">571     return src.isConstant() &amp;&amp; getConstantOperand(src).isInt32();</span>
572 }
573 
<span class="line-modified">574 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, RegisterID from)</span>
575 {
576     store64(from, addressFor(dst));
577 }
578 
<span class="line-modified">579 ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, JSValueRegs from)</span>
580 {
581     emitPutVirtualRegister(dst, from.payloadGPR());
582 }
583 





584 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfBothJSCells(RegisterID reg1, RegisterID reg2, RegisterID scratch)
585 {
586     move(reg1, scratch);
587     or64(reg2, scratch);
588     return branchIfCell(scratch);
589 }
590 
591 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfJSCell(RegisterID reg)
592 {
593     addSlowCase(branchIfCell(reg));
594 }
595 
596 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg)
597 {
598     addSlowCase(branchIfNotCell(reg));
599 }
600 
<span class="line-modified">601 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg, VirtualRegister vReg)</span>
602 {
603     if (!m_codeBlock-&gt;isKnownNotImmediate(vReg))
604         emitJumpSlowCaseIfNotJSCell(reg);
605 }
606 


















607 ALWAYS_INLINE JIT::PatchableJump JIT::emitPatchableJumpIfNotInt(RegisterID reg)
608 {
<span class="line-modified">609     return patchableBranch64(Below, reg, numberTagRegister);</span>
610 }
611 
612 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
613 {
614     move(reg1, scratch);
615     and64(reg2, scratch);
616     return branchIfNotInt32(scratch);
617 }
618 
619 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotInt(RegisterID reg)
620 {
621     addSlowCase(branchIfNotInt32(reg));
622 }
623 
624 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
625 {
626     addSlowCase(emitJumpIfNotInt(reg1, reg2, scratch));
627 }
628 
629 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotNumber(RegisterID reg)
</pre>
<hr />
<pre>
635 
636 ALWAYS_INLINE int JIT::jumpTarget(const Instruction* instruction, int target)
637 {
638     if (target)
639         return target;
640     return m_codeBlock-&gt;outOfLineJumpOffset(instruction);
641 }
642 
643 ALWAYS_INLINE GetPutInfo JIT::copiedGetPutInfo(OpPutToScope bytecode)
644 {
645     unsigned key = bytecode.m_metadataID + 1; // HashMap doesn&#39;t like 0 as a key
646     auto iterator = m_copiedGetPutInfos.find(key);
647     if (iterator != m_copiedGetPutInfos.end())
648         return GetPutInfo(iterator-&gt;value);
649     GetPutInfo getPutInfo = bytecode.metadata(m_codeBlock).m_getPutInfo;
650     m_copiedGetPutInfos.add(key, getPutInfo.operand());
651     return getPutInfo;
652 }
653 
654 template&lt;typename BinaryOp&gt;
<span class="line-modified">655 ALWAYS_INLINE BinaryArithProfile JIT::copiedArithProfile(BinaryOp bytecode)</span>
656 {
<span class="line-modified">657     uint64_t key = (static_cast&lt;uint64_t&gt;(BinaryOp::opcodeID) + 1) &lt;&lt; 32 | static_cast&lt;uint64_t&gt;(bytecode.m_metadataID);</span>
658     auto iterator = m_copiedArithProfiles.find(key);
659     if (iterator != m_copiedArithProfiles.end())
660         return iterator-&gt;value;
<span class="line-modified">661     BinaryArithProfile arithProfile = bytecode.metadata(m_codeBlock).m_arithProfile;</span>
662     m_copiedArithProfiles.add(key, arithProfile);
663     return arithProfile;
664 }
665 
666 } // namespace JSC
667 
668 #endif // ENABLE(JIT)
</pre>
</td>
</tr>
</table>
<center><a href="JITInlineCacheGenerator.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITLeftShiftGenerator.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>