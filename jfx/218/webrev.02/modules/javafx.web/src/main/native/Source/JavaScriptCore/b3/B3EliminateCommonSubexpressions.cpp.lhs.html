<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3EliminateCommonSubexpressions.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (C) 2016-2017 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;B3EliminateCommonSubexpressions.h&quot;
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;B3BasicBlockInlines.h&quot;
 32 #include &quot;B3BlockWorklist.h&quot;
 33 #include &quot;B3Dominators.h&quot;
 34 #include &quot;B3HeapRange.h&quot;
 35 #include &quot;B3InsertionSetInlines.h&quot;
 36 #include &quot;B3MemoryValue.h&quot;
 37 #include &quot;B3PhaseScope.h&quot;
 38 #include &quot;B3ProcedureInlines.h&quot;
 39 #include &quot;B3PureCSE.h&quot;
 40 #include &quot;B3SlotBaseValue.h&quot;
 41 #include &quot;B3StackSlot.h&quot;
 42 #include &quot;B3ValueKey.h&quot;
 43 #include &quot;B3ValueInlines.h&quot;
 44 #include &quot;B3Variable.h&quot;
 45 #include &quot;B3VariableValue.h&quot;
<a name="2" id="anc2"></a><span class="line-removed"> 46 #include &quot;DFGGraph.h&quot;</span>
 47 #include &lt;wtf/CommaPrinter.h&gt;
 48 #include &lt;wtf/HashMap.h&gt;
 49 #include &lt;wtf/ListDump.h&gt;
 50 #include &lt;wtf/RangeSet.h&gt;
 51 
 52 namespace JSC { namespace B3 {
 53 
 54 namespace {
 55 
 56 namespace B3EliminateCommonSubexpressionsInternal {
<a name="3" id="anc3"></a><span class="line-modified"> 57 static const bool verbose = false;</span>
 58 }
 59 
 60 // FIXME: We could treat Patchpoints with a non-empty set of reads as a &quot;memory value&quot; and somehow
 61 // eliminate redundant ones. We would need some way of determining if two patchpoints are replacable.
 62 // It doesn&#39;t seem right to use the reads set for this. We could use the generator, but that feels
 63 // lame because the FTL will pretty much use a unique generator for each patchpoint even when two
 64 // patchpoints have the same semantics as far as CSE would be concerned. We could invent something
 65 // like a &quot;value ID&quot; for patchpoints. By default, each one gets a unique value ID, but FTL could force
 66 // some patchpoints to share the same one as a signal that they will return the same value if executed
 67 // in the same heap with the same inputs.
 68 
 69 typedef Vector&lt;MemoryValue*, 1&gt; MemoryMatches;
 70 
 71 class MemoryValueMap {
 72 public:
 73     MemoryValueMap() { }
 74 
 75     void add(MemoryValue* memory)
 76     {
 77         Matches&amp; matches = m_map.add(memory-&gt;lastChild(), Matches()).iterator-&gt;value;
 78         if (matches.contains(memory))
 79             return;
 80         matches.append(memory);
 81     }
 82 
 83     template&lt;typename Functor&gt;
 84     void removeIf(const Functor&amp; functor)
 85     {
 86         m_map.removeIf(
 87             [&amp;] (HashMap&lt;Value*, Matches&gt;::KeyValuePairType&amp; entry) -&gt; bool {
 88                 entry.value.removeAllMatching(
 89                     [&amp;] (Value* value) -&gt; bool {
 90                         if (MemoryValue* memory = value-&gt;as&lt;MemoryValue&gt;())
 91                             return functor(memory);
 92                         return true;
 93                     });
 94                 return entry.value.isEmpty();
 95             });
 96     }
 97 
 98     Matches* find(Value* ptr)
 99     {
100         auto iter = m_map.find(ptr);
101         if (iter == m_map.end())
102             return nullptr;
103         return &amp;iter-&gt;value;
104     }
105 
106     template&lt;typename Functor&gt;
107     MemoryValue* find(Value* ptr, const Functor&amp; functor)
108     {
109         if (B3EliminateCommonSubexpressionsInternal::verbose)
110             dataLog(&quot;        Looking for &quot;, pointerDump(ptr), &quot; in &quot;, *this, &quot;\n&quot;);
111         if (Matches* matches = find(ptr)) {
112             if (B3EliminateCommonSubexpressionsInternal::verbose)
113                 dataLog(&quot;        Matches: &quot;, pointerListDump(*matches), &quot;\n&quot;);
114             for (Value* candidateValue : *matches) {
115                 if (B3EliminateCommonSubexpressionsInternal::verbose)
116                     dataLog(&quot;        Having candidate: &quot;, pointerDump(candidateValue), &quot;\n&quot;);
117                 if (MemoryValue* candidateMemory = candidateValue-&gt;as&lt;MemoryValue&gt;()) {
118                     if (functor(candidateMemory))
119                         return candidateMemory;
120                 }
121             }
122         }
123         return nullptr;
124     }
125 
126     void dump(PrintStream&amp; out) const
127     {
128         out.print(&quot;{&quot;);
129         CommaPrinter comma;
130         for (auto&amp; entry : m_map)
131             out.print(comma, pointerDump(entry.key), &quot;=&gt;&quot;, pointerListDump(entry.value));
132         out.print(&quot;}&quot;);
133     }
134 
135 private:
136     // This uses Matches for two reasons:
137     // - It cannot be a MemoryValue* because the key is imprecise. Many MemoryValues could have the
138     //   same key while being unaliased.
139     // - It can&#39;t be a MemoryMatches array because the MemoryValue*&#39;s could be turned into Identity&#39;s.
140     HashMap&lt;Value*, Matches&gt; m_map;
141 };
142 
143 struct ImpureBlockData {
144     void dump(PrintStream&amp; out) const
145     {
146         out.print(
147             &quot;{reads = &quot;, reads, &quot;, writes = &quot;, writes, &quot;, storesAtHead = &quot;, storesAtHead,
148             &quot;, memoryValuesAtTail = &quot;, memoryValuesAtTail, &quot;}&quot;);
149     }
150 
151     RangeSet&lt;HeapRange&gt; reads; // This only gets used for forward store elimination.
152     RangeSet&lt;HeapRange&gt; writes; // This gets used for both load and store elimination.
153     bool fence;
154 
155     MemoryValueMap storesAtHead;
156     MemoryValueMap memoryValuesAtTail;
157 };
158 
159 class CSE {
160 public:
161     CSE(Procedure&amp; proc)
162         : m_proc(proc)
163         , m_dominators(proc.dominators())
164         , m_impureBlockData(proc.size())
165         , m_insertionSet(proc)
166     {
167     }
168 
169     bool run()
170     {
171         if (B3EliminateCommonSubexpressionsInternal::verbose)
172             dataLog(&quot;B3 before CSE:\n&quot;, m_proc);
173 
174         m_proc.resetValueOwners();
175 
176         // Summarize the impure effects of each block, and the impure values available at the end of
177         // each block. This doesn&#39;t edit code yet.
178         for (BasicBlock* block : m_proc) {
179             ImpureBlockData&amp; data = m_impureBlockData[block];
180             for (Value* value : *block) {
181                 Effects effects = value-&gt;effects();
182                 MemoryValue* memory = value-&gt;as&lt;MemoryValue&gt;();
183 
184                 if (memory &amp;&amp; memory-&gt;isStore()
185                     &amp;&amp; !data.reads.overlaps(memory-&gt;range())
186                     &amp;&amp; !data.writes.overlaps(memory-&gt;range())
187                     &amp;&amp; (!data.fence || !memory-&gt;hasFence()))
188                     data.storesAtHead.add(memory);
189                 data.reads.add(effects.reads);
190 
191                 if (HeapRange writes = effects.writes)
192                     clobber(data, writes);
193                 data.fence |= effects.fence;
194 
195                 if (memory)
196                     data.memoryValuesAtTail.add(memory);
197             }
198 
199             if (B3EliminateCommonSubexpressionsInternal::verbose)
200                 dataLog(&quot;Block &quot;, *block, &quot;: &quot;, data, &quot;\n&quot;);
201         }
202 
203         // Perform CSE. This edits code.
204         Vector&lt;BasicBlock*&gt; postOrder = m_proc.blocksInPostOrder();
205         for (unsigned i = postOrder.size(); i--;) {
206             m_block = postOrder[i];
207             if (B3EliminateCommonSubexpressionsInternal::verbose)
208                 dataLog(&quot;Looking at &quot;, *m_block, &quot;:\n&quot;);
209 
210             m_data = ImpureBlockData();
211             for (m_index = 0; m_index &lt; m_block-&gt;size(); ++m_index) {
212                 m_value = m_block-&gt;at(m_index);
213                 process();
214             }
215             m_insertionSet.execute(m_block);
216             m_impureBlockData[m_block] = m_data;
217         }
218 
219         // The previous pass might have requested that we insert code in some basic block other than
220         // the one that it was looking at. This inserts them.
221         for (BasicBlock* block : m_proc) {
222             for (unsigned valueIndex = 0; valueIndex &lt; block-&gt;size(); ++valueIndex) {
223                 auto iter = m_sets.find(block-&gt;at(valueIndex));
224                 if (iter == m_sets.end())
225                     continue;
226 
227                 for (Value* value : iter-&gt;value)
228                     m_insertionSet.insertValue(valueIndex + 1, value);
229             }
230             m_insertionSet.execute(block);
231         }
232 
233         if (B3EliminateCommonSubexpressionsInternal::verbose)
234             dataLog(&quot;B3 after CSE:\n&quot;, m_proc);
235 
236         return m_changed;
237     }
238 
239 private:
240     void process()
241     {
242         m_value-&gt;performSubstitution();
243 
244         if (m_pureCSE.process(m_value, m_dominators)) {
245             ASSERT(!m_value-&gt;effects().writes);
246             m_changed = true;
247             return;
248         }
249 
250         MemoryValue* memory = m_value-&gt;as&lt;MemoryValue&gt;();
251         if (memory &amp;&amp; processMemoryBeforeClobber(memory))
252             return;
253 
254         if (HeapRange writes = m_value-&gt;effects().writes)
255             clobber(m_data, writes);
256 
257         if (memory)
258             processMemoryAfterClobber(memory);
259     }
260 
261     // Return true if we got rid of the operation. If you changed IR in this function, you have to
262     // set m_changed even if you also return true.
263     bool processMemoryBeforeClobber(MemoryValue* memory)
264     {
265         Value* value = memory-&gt;child(0);
266         Value* ptr = memory-&gt;lastChild();
267         HeapRange range = memory-&gt;range();
268         Value::OffsetType offset = memory-&gt;offset();
269 
270         switch (memory-&gt;opcode()) {
271         case Store8:
272             return handleStoreBeforeClobber(
273                 ptr, range,
274                 [&amp;] (MemoryValue* candidate) -&gt; bool {
275                     return candidate-&gt;offset() == offset
276                         &amp;&amp; ((candidate-&gt;opcode() == Store8 &amp;&amp; candidate-&gt;child(0) == value)
277                             || ((candidate-&gt;opcode() == Load8Z || candidate-&gt;opcode() == Load8S)
278                                 &amp;&amp; candidate == value));
279                 });
280         case Store16:
281             return handleStoreBeforeClobber(
282                 ptr, range,
283                 [&amp;] (MemoryValue* candidate) -&gt; bool {
284                     return candidate-&gt;offset() == offset
285                         &amp;&amp; ((candidate-&gt;opcode() == Store16 &amp;&amp; candidate-&gt;child(0) == value)
286                             || ((candidate-&gt;opcode() == Load16Z || candidate-&gt;opcode() == Load16S)
287                                 &amp;&amp; candidate == value));
288                 });
289         case Store:
290             return handleStoreBeforeClobber(
291                 ptr, range,
292                 [&amp;] (MemoryValue* candidate) -&gt; bool {
293                     return candidate-&gt;offset() == offset
294                         &amp;&amp; ((candidate-&gt;opcode() == Store &amp;&amp; candidate-&gt;child(0) == value)
295                             || (candidate-&gt;opcode() == Load &amp;&amp; candidate == value));
296                 });
297         default:
298             return false;
299         }
300     }
301 
302     void clobber(ImpureBlockData&amp; data, HeapRange writes)
303     {
304         data.writes.add(writes);
305 
306         data.memoryValuesAtTail.removeIf(
307             [&amp;] (MemoryValue* memory) {
308                 return memory-&gt;range().overlaps(writes);
309             });
310     }
311 
312     void processMemoryAfterClobber(MemoryValue* memory)
313     {
314         Value* ptr = memory-&gt;lastChild();
315         HeapRange range = memory-&gt;range();
316         Value::OffsetType offset = memory-&gt;offset();
317         Type type = memory-&gt;type();
318 
319         // FIXME: Empower this to insert more casts and shifts. For example, a Load8 could match a
320         // Store and mask the result. You could even have:
321         //
322         // Store(@value, @ptr, offset = 0)
323         // Load8Z(@ptr, offset = 2)
324         //
325         // Which could be turned into something like this:
326         //
327         // Store(@value, @ptr, offset = 0)
328         // ZShr(@value, 16)
329 
330         switch (memory-&gt;opcode()) {
331         case Load8Z: {
332             handleMemoryValue(
333                 ptr, range,
334                 [&amp;] (MemoryValue* candidate) -&gt; bool {
335                     return candidate-&gt;offset() == offset
336                         &amp;&amp; (candidate-&gt;opcode() == Load8Z || candidate-&gt;opcode() == Store8);
337                 },
338                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
339                     if (match-&gt;opcode() == Store8) {
340                         Value* mask = m_proc.add&lt;Const32Value&gt;(m_value-&gt;origin(), 0xff);
341                         fixups.append(mask);
342                         Value* zext = m_proc.add&lt;Value&gt;(
343                             BitAnd, m_value-&gt;origin(), match-&gt;child(0), mask);
344                         fixups.append(zext);
345                         return zext;
346                     }
347                     return nullptr;
348                 });
349             break;
350         }
351 
352         case Load8S: {
353             handleMemoryValue(
354                 ptr, range,
355                 [&amp;] (MemoryValue* candidate) -&gt; bool {
356                     return candidate-&gt;offset() == offset
357                         &amp;&amp; (candidate-&gt;opcode() == Load8S || candidate-&gt;opcode() == Store8);
358                 },
359                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
360                     if (match-&gt;opcode() == Store8) {
361                         Value* sext = m_proc.add&lt;Value&gt;(
362                             SExt8, m_value-&gt;origin(), match-&gt;child(0));
363                         fixups.append(sext);
364                         return sext;
365                     }
366                     return nullptr;
367                 });
368             break;
369         }
370 
371         case Load16Z: {
372             handleMemoryValue(
373                 ptr, range,
374                 [&amp;] (MemoryValue* candidate) -&gt; bool {
375                     return candidate-&gt;offset() == offset
376                         &amp;&amp; (candidate-&gt;opcode() == Load16Z || candidate-&gt;opcode() == Store16);
377                 },
378                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
379                     if (match-&gt;opcode() == Store16) {
380                         Value* mask = m_proc.add&lt;Const32Value&gt;(m_value-&gt;origin(), 0xffff);
381                         fixups.append(mask);
382                         Value* zext = m_proc.add&lt;Value&gt;(
383                             BitAnd, m_value-&gt;origin(), match-&gt;child(0), mask);
384                         fixups.append(zext);
385                         return zext;
386                     }
387                     return nullptr;
388                 });
389             break;
390         }
391 
392         case Load16S: {
393             handleMemoryValue(
394                 ptr, range, [&amp;] (MemoryValue* candidate) -&gt; bool {
395                     return candidate-&gt;offset() == offset
396                         &amp;&amp; (candidate-&gt;opcode() == Load16S || candidate-&gt;opcode() == Store16);
397                 },
398                 [&amp;] (MemoryValue* match, Vector&lt;Value*&gt;&amp; fixups) -&gt; Value* {
399                     if (match-&gt;opcode() == Store16) {
400                         Value* sext = m_proc.add&lt;Value&gt;(
401                             SExt16, m_value-&gt;origin(), match-&gt;child(0));
402                         fixups.append(sext);
403                         return sext;
404                     }
405                     return nullptr;
406                 });
407             break;
408         }
409 
410         case Load: {
411             handleMemoryValue(
412                 ptr, range,
413                 [&amp;] (MemoryValue* candidate) -&gt; bool {
414                     if (B3EliminateCommonSubexpressionsInternal::verbose)
415                         dataLog(&quot;        Consdering &quot;, pointerDump(candidate), &quot;\n&quot;);
416                     if (candidate-&gt;offset() != offset)
417                         return false;
418 
419                     if (B3EliminateCommonSubexpressionsInternal::verbose)
420                         dataLog(&quot;            offset ok.\n&quot;);
421 
422                     if (candidate-&gt;opcode() == Load &amp;&amp; candidate-&gt;type() == type)
423                         return true;
424 
425                     if (B3EliminateCommonSubexpressionsInternal::verbose)
426                         dataLog(&quot;            not a load with ok type.\n&quot;);
427 
428                     if (candidate-&gt;opcode() == Store &amp;&amp; candidate-&gt;child(0)-&gt;type() == type)
429                         return true;
430 
431                     if (B3EliminateCommonSubexpressionsInternal::verbose)
432                         dataLog(&quot;            not a store with ok type.\n&quot;);
433 
434                     return false;
435                 });
436             break;
437         }
438 
439         case Store8: {
440             handleStoreAfterClobber(
441                 ptr, range,
442                 [&amp;] (MemoryValue* candidate) -&gt; bool {
443                     return candidate-&gt;opcode() == Store8
444                         &amp;&amp; candidate-&gt;offset() == offset;
445                 });
446             break;
447         }
448 
449         case Store16: {
450             handleStoreAfterClobber(
451                 ptr, range,
452                 [&amp;] (MemoryValue* candidate) -&gt; bool {
453                     return candidate-&gt;opcode() == Store16
454                         &amp;&amp; candidate-&gt;offset() == offset;
455                 });
456             break;
457         }
458 
459         case Store: {
460             handleStoreAfterClobber(
461                 ptr, range,
462                 [&amp;] (MemoryValue* candidate) -&gt; bool {
463                     return candidate-&gt;opcode() == Store
464                         &amp;&amp; candidate-&gt;offset() == offset;
465                 });
466             break;
467         }
468 
469         default:
470             break;
471         }
472     }
473 
474     template&lt;typename Filter&gt;
475     bool handleStoreBeforeClobber(Value* ptr, HeapRange range, const Filter&amp; filter)
476     {
477         MemoryMatches matches = findMemoryValue(ptr, range, filter);
478         if (matches.isEmpty())
479             return false;
480 
481         m_value-&gt;replaceWithNop();
482         m_changed = true;
483         return true;
484     }
485 
486     template&lt;typename Filter&gt;
487     void handleStoreAfterClobber(Value* ptr, HeapRange range, const Filter&amp; filter)
488     {
489         if (!m_value-&gt;traps() &amp;&amp; findStoreAfterClobber(ptr, range, filter)) {
490             m_value-&gt;replaceWithNop();
491             m_changed = true;
492             return;
493         }
494 
495         m_data.memoryValuesAtTail.add(m_value-&gt;as&lt;MemoryValue&gt;());
496     }
497 
498     template&lt;typename Filter&gt;
499     bool findStoreAfterClobber(Value* ptr, HeapRange range, const Filter&amp; filter)
500     {
501         if (m_value-&gt;as&lt;MemoryValue&gt;()-&gt;hasFence())
502             return false;
503 
504         // We can eliminate a store if every forward path hits a store to the same location before
505         // hitting any operation that observes the store. This search seems like it should be
506         // expensive, but in the overwhelming majority of cases it will almost immediately hit an
507         // operation that interferes.
508 
509         if (B3EliminateCommonSubexpressionsInternal::verbose)
510             dataLog(*m_value, &quot;: looking forward for stores to &quot;, *ptr, &quot;...\n&quot;);
511 
512         // First search forward in this basic block.
513         // FIXME: It would be cool to get rid of this linear search. It&#39;s not super critical since
514         // we will probably bail out very quickly, but it *is* annoying.
515         for (unsigned index = m_index + 1; index &lt; m_block-&gt;size(); ++index) {
516             Value* value = m_block-&gt;at(index);
517 
518             if (MemoryValue* memoryValue = value-&gt;as&lt;MemoryValue&gt;()) {
519                 if (memoryValue-&gt;lastChild() == ptr &amp;&amp; filter(memoryValue))
520                     return true;
521             }
522 
523             Effects effects = value-&gt;effects();
524             if (effects.reads.overlaps(range) || effects.writes.overlaps(range))
525                 return false;
526         }
527 
528         if (!m_block-&gt;numSuccessors())
529             return false;
530 
531         BlockWorklist worklist;
532         worklist.pushAll(m_block-&gt;successorBlocks());
533 
534         while (BasicBlock* block = worklist.pop()) {
535             ImpureBlockData&amp; data = m_impureBlockData[block];
536 
537             MemoryValue* match = data.storesAtHead.find(ptr, filter);
538             if (match &amp;&amp; match != m_value)
539                 continue;
540 
541             if (data.writes.overlaps(range) || data.reads.overlaps(range))
542                 return false;
543 
544             if (!block-&gt;numSuccessors())
545                 return false;
546 
547             worklist.pushAll(block-&gt;successorBlocks());
548         }
549 
550         return true;
551     }
552 
553     template&lt;typename Filter&gt;
554     void handleMemoryValue(Value* ptr, HeapRange range, const Filter&amp; filter)
555     {
556         handleMemoryValue(
557             ptr, range, filter,
558             [] (MemoryValue*, Vector&lt;Value*&gt;&amp;) -&gt; Value* {
559                 return nullptr;
560             });
561     }
562 
563     template&lt;typename Filter, typename Replace&gt;
564     void handleMemoryValue(
565         Value* ptr, HeapRange range, const Filter&amp; filter, const Replace&amp; replace)
566     {
567         MemoryMatches matches = findMemoryValue(ptr, range, filter);
568         if (replaceMemoryValue(matches, replace))
569             return;
570         m_data.memoryValuesAtTail.add(m_value-&gt;as&lt;MemoryValue&gt;());
571     }
572 
573     template&lt;typename Replace&gt;
574     bool replaceMemoryValue(const MemoryMatches&amp; matches, const Replace&amp; replace)
575     {
576         if (matches.isEmpty())
577             return false;
578 
579         if (B3EliminateCommonSubexpressionsInternal::verbose)
580             dataLog(&quot;Eliminating &quot;, *m_value, &quot; due to &quot;, pointerListDump(matches), &quot;\n&quot;);
581 
582         m_changed = true;
583 
584         if (matches.size() == 1) {
585             MemoryValue* dominatingMatch = matches[0];
586             RELEASE_ASSERT(m_dominators.dominates(dominatingMatch-&gt;owner, m_block));
587 
588             if (B3EliminateCommonSubexpressionsInternal::verbose)
589                 dataLog(&quot;    Eliminating using &quot;, *dominatingMatch, &quot;\n&quot;);
590             Vector&lt;Value*&gt; extraValues;
591             if (Value* value = replace(dominatingMatch, extraValues)) {
592                 for (Value* extraValue : extraValues)
593                     m_insertionSet.insertValue(m_index, extraValue);
594                 m_value-&gt;replaceWithIdentity(value);
595             } else {
596                 if (dominatingMatch-&gt;isStore())
597                     m_value-&gt;replaceWithIdentity(dominatingMatch-&gt;child(0));
598                 else
599                     m_value-&gt;replaceWithIdentity(dominatingMatch);
600             }
601             return true;
602         }
603 
604         // FIXME: It would be way better if this phase just did SSA calculation directly.
605         // Right now we&#39;re relying on the fact that CSE&#39;s position in the phase order is
606         // almost right before SSA fixup.
607 
608         Variable* variable = m_proc.addVariable(m_value-&gt;type());
609 
610         VariableValue* get = m_insertionSet.insert&lt;VariableValue&gt;(
611             m_index, Get, m_value-&gt;origin(), variable);
612         if (B3EliminateCommonSubexpressionsInternal::verbose)
613             dataLog(&quot;    Inserting get of value: &quot;, *get, &quot;\n&quot;);
614         m_value-&gt;replaceWithIdentity(get);
615 
616         for (MemoryValue* match : matches) {
617             Vector&lt;Value*&gt;&amp; sets = m_sets.add(match, Vector&lt;Value*&gt;()).iterator-&gt;value;
618 
619             Value* value = replace(match, sets);
620             if (!value) {
621                 if (match-&gt;isStore())
622                     value = match-&gt;child(0);
623                 else
624                     value = match;
625             }
626 
627             Value* set = m_proc.add&lt;VariableValue&gt;(Set, m_value-&gt;origin(), variable, value);
628             sets.append(set);
629         }
630 
631         return true;
632     }
633 
634     template&lt;typename Filter&gt;
635     MemoryMatches findMemoryValue(Value* ptr, HeapRange range, const Filter&amp; filter)
636     {
637         if (B3EliminateCommonSubexpressionsInternal::verbose) {
638             dataLog(*m_value, &quot;: looking backward for &quot;, *ptr, &quot;...\n&quot;);
639             dataLog(&quot;    Full value: &quot;, deepDump(m_value), &quot;\n&quot;);
640         }
641 
642         if (m_value-&gt;as&lt;MemoryValue&gt;()-&gt;hasFence()) {
643             if (B3EliminateCommonSubexpressionsInternal::verbose)
644                 dataLog(&quot;    Giving up because fences.\n&quot;);
645             return { };
646         }
647 
648         if (MemoryValue* match = m_data.memoryValuesAtTail.find(ptr, filter)) {
649             if (B3EliminateCommonSubexpressionsInternal::verbose)
650                 dataLog(&quot;    Found &quot;, *match, &quot; locally.\n&quot;);
651             return { match };
652         }
653 
654         if (m_data.writes.overlaps(range)) {
655             if (B3EliminateCommonSubexpressionsInternal::verbose)
656                 dataLog(&quot;    Giving up because of writes.\n&quot;);
657             return { };
658         }
659 
660         BlockWorklist worklist;
661         worklist.pushAll(m_block-&gt;predecessors());
662 
663         MemoryMatches matches;
664 
665         while (BasicBlock* block = worklist.pop()) {
666             if (B3EliminateCommonSubexpressionsInternal::verbose)
667                 dataLog(&quot;    Looking at &quot;, *block, &quot;\n&quot;);
668 
669             ImpureBlockData&amp; data = m_impureBlockData[block];
670 
671             MemoryValue* match = data.memoryValuesAtTail.find(ptr, filter);
672             if (B3EliminateCommonSubexpressionsInternal::verbose)
673                 dataLog(&quot;    Consdering match: &quot;, pointerDump(match), &quot;\n&quot;);
674             if (match &amp;&amp; match != m_value) {
675                 if (B3EliminateCommonSubexpressionsInternal::verbose)
676                     dataLog(&quot;    Found match: &quot;, *match, &quot;\n&quot;);
677                 matches.append(match);
678                 continue;
679             }
680 
681             if (data.writes.overlaps(range)) {
682                 if (B3EliminateCommonSubexpressionsInternal::verbose)
683                     dataLog(&quot;    Giving up because of writes.\n&quot;);
684                 return { };
685             }
686 
687             if (!block-&gt;numPredecessors()) {
688                 if (B3EliminateCommonSubexpressionsInternal::verbose)
689                     dataLog(&quot;    Giving up because it&#39;s live at root.\n&quot;);
690                 // This essentially proves that this is live at the prologue. That means that we
691                 // cannot reliably optimize this case.
692                 return { };
693             }
694 
695             worklist.pushAll(block-&gt;predecessors());
696         }
697 
698         if (B3EliminateCommonSubexpressionsInternal::verbose)
699             dataLog(&quot;    Got matches: &quot;, pointerListDump(matches), &quot;\n&quot;);
700         return matches;
701     }
702 
703     Procedure&amp; m_proc;
704 
705     Dominators&amp; m_dominators;
706     PureCSE m_pureCSE;
707 
708     IndexMap&lt;BasicBlock*, ImpureBlockData&gt; m_impureBlockData;
709 
710     ImpureBlockData m_data;
711 
712     BasicBlock* m_block;
713     unsigned m_index;
714     Value* m_value;
715 
716     HashMap&lt;Value*, Vector&lt;Value*&gt;&gt; m_sets;
717 
718     InsertionSet m_insertionSet;
719 
720     bool m_changed { false };
721 };
722 
723 } // anonymous namespace
724 
725 bool eliminateCommonSubexpressions(Procedure&amp; proc)
726 {
727     PhaseScope phaseScope(proc, &quot;eliminateCommonSubexpressions&quot;);
728 
729     CSE cse(proc);
730     return cse.run();
731 }
732 
733 } } // namespace JSC::B3
734 
735 #endif // ENABLE(B3_JIT)
736 
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>