<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="LLIntThunks.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="LowLevelInterpreter.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
<span class="line-modified">   1 # Copyrsght (C) 2011-2019 Apple Inc. All rights reserved.</span>
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
</pre>
<hr />
<pre>
  57 #   predicate of an &quot;if&quot; is assumed to be a #define that is available
  58 #   during code gen. So you can&#39;t use &quot;if&quot; for computation in a macro, but
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #
<span class="line-removed">  77 #  - pc holds the (native) program counter on 32-bits ARM architectures (ARMv7)</span>
<span class="line-removed">  78 #</span>
  79 #  - t0, t1, t2, t3, t4, and optionally t5, t6, and t7 are temporary registers that can get trashed on
  80 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  81 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  82 #
  83 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  84 #  two distinct registers
  85 #
  86 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  87 #  thus are pairwise distinct registers. Be mindful that:
  88 #    + On X86, there are no argument registers. a0 and a1 are edx and
  89 #    ecx following the fastcall convention, but you should still use the stack
  90 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  91 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  92 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  93 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  94 #    this for you.
  95 #
  96 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  97 #  you should be mindful of that in functions that are called directly from C.
  98 #  If you need more registers, you should push and pop them like a good
  99 #  assembly citizen, because any other register will be callee-saved on X86.
 100 #
 101 # You can additionally assume:
 102 #
 103 #  - a3, t2, t3, t4 and t5 are never return registers; t0, t1, a0, a1 and a2
 104 #  can be return registers.
 105 #
 106 #  - t4 and t5 are never argument registers, t3 can only be a3, t1 can only be
 107 #  a1; but t0 and t2 can be either a0 or a2.
 108 #
<span class="line-modified"> 109 #  - On 64 bits, there are callee-save registers named csr0, csr1, ... csrN.</span>
 110 #  The last three csr registers are used used to store the PC base and
<span class="line-modified"> 111 #  two special tag values. Don&#39;t use them for anything else.</span>
 112 #
 113 # Additional platform-specific details (you shouldn&#39;t rely on this remaining
 114 # true):
 115 #
 116 #  - For consistency with the baseline JIT, t0 is always r0 (and t1 is always
 117 #  r1 on 32 bits platforms). You should use the r version when you need return
 118 #  registers, and the t version otherwise: code using t0 (or t1) should still
 119 #  work if swapped with e.g. t3, while code using r0 (or r1) should not. There
 120 #  *may* be legacy code relying on this.
 121 #
 122 #  - On all platforms other than X86, t0 can only be a0 and t2 can only be a2.
 123 #
 124 #  - On all platforms other than X86 and X86_64, a2 is not a return register.
 125 #  a2 is r0 on X86 (because we have so few registers) and r1 on X86_64 (because
 126 #  the ABI enforces it).
 127 #
 128 # The following floating-point registers are available:
 129 #
 130 #  - ft0-ft5 are temporary floating-point registers that get trashed on calls,
 131 #  and are pairwise distinct.
</pre>
<hr />
<pre>
 148 if ARMv7k
 149 end
 150 if ARMv7s
 151 end
 152 
 153 # These declarations must match interpreter/JSStack.h.
 154 
 155 const PtrSize = constexpr (sizeof(void*))
 156 const MachineRegisterSize = constexpr (sizeof(CPURegister))
 157 const SlotSize = constexpr (sizeof(Register))
 158 
 159 if JSVALUE64
 160     const CallFrameHeaderSlots = 5
 161 else
 162     const CallFrameHeaderSlots = 4
 163     const CallFrameAlignSlots = 1
 164 end
 165 
 166 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 167 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)

 168 
 169 const StackAlignment = constexpr (stackAlignmentBytes())
 170 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 171 const StackAlignmentMask = StackAlignment - 1
 172 
 173 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 174 
 175 const CallerFrame = 0
 176 const ReturnPC = CallerFrame + MachineRegisterSize
 177 const CodeBlock = ReturnPC + MachineRegisterSize
 178 const Callee = CodeBlock + SlotSize
<span class="line-modified"> 179 const ArgumentCount = Callee + SlotSize</span>
<span class="line-modified"> 180 const ThisArgumentOffset = ArgumentCount + SlotSize</span>
 181 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 182 const CallFrameHeaderSize = ThisArgumentOffset
 183 
 184 const MetadataOffsetTable16Offset = 0
 185 const MetadataOffsetTable32Offset = constexpr UnlinkedMetadataTable::s_offset16TableSize

 186 
 187 # Some value representation constants.
 188 if JSVALUE64
<span class="line-modified"> 189     const TagBitTypeOther = constexpr TagBitTypeOther</span>
<span class="line-modified"> 190     const TagBitBool      = constexpr TagBitBool</span>
<span class="line-modified"> 191     const TagBitUndefined = constexpr TagBitUndefined</span>
<span class="line-modified"> 192     const ValueEmpty      = constexpr ValueEmpty</span>
<span class="line-modified"> 193     const ValueFalse      = constexpr ValueFalse</span>
<span class="line-modified"> 194     const ValueTrue       = constexpr ValueTrue</span>
<span class="line-modified"> 195     const ValueUndefined  = constexpr ValueUndefined</span>
<span class="line-modified"> 196     const ValueNull       = constexpr ValueNull</span>
<span class="line-modified"> 197     const TagTypeNumber   = constexpr TagTypeNumber</span>
<span class="line-modified"> 198     const TagMask         = constexpr TagMask</span>
 199 else
 200     const Int32Tag = constexpr JSValue::Int32Tag
 201     const BooleanTag = constexpr JSValue::BooleanTag
 202     const NullTag = constexpr JSValue::NullTag
 203     const UndefinedTag = constexpr JSValue::UndefinedTag
 204     const CellTag = constexpr JSValue::CellTag
 205     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 206     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 207     const LowestTag = constexpr JSValue::LowestTag
 208 end
 209 
 210 if JSVALUE64
 211     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 212     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 213 end
 214 
<span class="line-removed"> 215 const CallOpCodeSize = constexpr op_call_length</span>
<span class="line-removed"> 216 </span>
 217 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 218 
 219 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 220     const CalleeSaveSpaceAsVirtualRegisters = 4
 221 elsif C_LOOP or C_LOOP_WIN
 222     const CalleeSaveSpaceAsVirtualRegisters = 1
 223 elsif ARMv7
 224     const CalleeSaveSpaceAsVirtualRegisters = 1
 225 elsif MIPS
 226     const CalleeSaveSpaceAsVirtualRegisters = 1
 227 else
 228     const CalleeSaveSpaceAsVirtualRegisters = 0
 229 end
 230 
 231 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 232 
 233 
 234 # Watchpoint states
 235 const ClearWatchpoint = constexpr ClearWatchpoint
 236 const IsWatched = constexpr IsWatched
 237 const IsInvalidated = constexpr IsInvalidated
 238 
 239 # ShadowChicken data
 240 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 241 
<span class="line-modified"> 242 # ArithProfile data</span>
<span class="line-modified"> 243 const ArithProfileInt = constexpr (ArithProfile::observedUnaryInt().bits())</span>
<span class="line-modified"> 244 const ArithProfileNumber = constexpr (ArithProfile::observedUnaryNumber().bits())</span>
<span class="line-modified"> 245 const ArithProfileIntInt = constexpr (ArithProfile::observedBinaryIntInt().bits())</span>
<span class="line-modified"> 246 const ArithProfileNumberInt = constexpr (ArithProfile::observedBinaryNumberInt().bits())</span>
<span class="line-modified"> 247 const ArithProfileIntNumber = constexpr (ArithProfile::observedBinaryIntNumber().bits())</span>
<span class="line-modified"> 248 const ArithProfileNumberNumber = constexpr (ArithProfile::observedBinaryNumberNumber().bits())</span>


 249 
 250 # Pointer Tags
 251 const BytecodePtrTag = constexpr BytecodePtrTag
 252 const JSEntryPtrTag = constexpr JSEntryPtrTag
 253 const ExceptionHandlerPtrTag = constexpr ExceptionHandlerPtrTag
 254 const NoPtrTag = constexpr NoPtrTag
 255 const SlowPathPtrTag = constexpr SlowPathPtrTag
 256 
 257 # Some register conventions.





 258 if JSVALUE64
<span class="line-removed"> 259     # - Use a pair of registers to represent the PC: one register for the</span>
<span class="line-removed"> 260     #   base of the bytecodes, and one register for the index.</span>
<span class="line-removed"> 261     # - The PC base (or PB for short) must be stored in a callee-save register.</span>
<span class="line-removed"> 262     # - C calls are still given the Instruction* rather than the PC index.</span>
<span class="line-removed"> 263     #   This requires an add before the call, and a sub after.</span>
 264     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 265     if ARM64 or ARM64E
 266         const metadataTable = csr6
 267         const PB = csr7
<span class="line-modified"> 268         const tagTypeNumber = csr8</span>
<span class="line-modified"> 269         const tagMask = csr9</span>
 270     elsif X86_64
 271         const metadataTable = csr1
 272         const PB = csr2
<span class="line-modified"> 273         const tagTypeNumber = csr3</span>
<span class="line-modified"> 274         const tagMask = csr4</span>
 275     elsif X86_64_WIN
 276         const metadataTable = csr3
 277         const PB = csr4
<span class="line-modified"> 278         const tagTypeNumber = csr5</span>
<span class="line-modified"> 279         const tagMask = csr6</span>
 280     elsif C_LOOP or C_LOOP_WIN
 281         const PB = csr0
<span class="line-modified"> 282         const tagTypeNumber = csr1</span>
<span class="line-modified"> 283         const tagMask = csr2</span>
 284         const metadataTable = csr3
 285     end
 286 
 287 else
 288     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 289     if C_LOOP or C_LOOP_WIN

 290         const metadataTable = csr3
 291     elsif ARMv7
 292         const metadataTable = csr0

 293     elsif MIPS
 294         const metadataTable = csr0

 295     else
 296         error
 297     end
 298 end
 299 





























 300 macro dispatch(advanceReg)
 301     addp advanceReg, PC
 302     nextInstruction()
 303 end
 304 
 305 macro dispatchIndirect(offsetReg)
 306     dispatch(offsetReg)
 307 end
 308 
<span class="line-modified"> 309 macro dispatchOp(size, opcodeName)</span>
 310     macro dispatchNarrow()
<span class="line-modified"> 311         dispatch(constexpr %opcodeName%_length)</span>
 312     end
 313 
 314     macro dispatchWide16()
<span class="line-modified"> 315         dispatch(constexpr %opcodeName%_length * 2 + 1)</span>
 316     end
 317 
 318     macro dispatchWide32()
<span class="line-modified"> 319         dispatch(constexpr %opcodeName%_length * 4 + 1)</span>
 320     end
 321 
 322     size(dispatchNarrow, dispatchWide16, dispatchWide32, macro (dispatch) dispatch() end)
 323 end
 324 





 325 macro getu(size, opcodeStruct, fieldName, dst)
 326     size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)
 327         getu(opcodeStruct, fieldName, dst)
 328     end)
 329 end
 330 
 331 macro get(size, opcodeStruct, fieldName, dst)
 332     size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)
 333         get(opcodeStruct, fieldName, dst)
 334     end)
 335 end
 336 
 337 macro narrow(narrowFn, wide16Fn, wide32Fn, k)
 338     k(narrowFn)
 339 end
 340 
 341 macro wide16(narrowFn, wide16Fn, wide32Fn, k)
 342     k(wide16Fn)
 343 end
 344 
 345 macro wide32(narrowFn, wide16Fn, wide32Fn, k)
 346     k(wide32Fn)
 347 end
 348 
 349 macro metadata(size, opcode, dst, scratch)
 350     loadh (constexpr %opcode%::opcodeID * 2 + MetadataOffsetTable16Offset)[metadataTable], dst # offset = metadataTable&lt;uint16_t*&gt;[opcodeID]
 351     btinz dst, .setUpOffset
 352     loadi (constexpr %opcode%::opcodeID * 4 + MetadataOffsetTable32Offset)[metadataTable], dst # offset = metadataTable&lt;uint32_t*&gt;[opcodeID]
 353 .setUpOffset:
 354     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 355     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 356     addi scratch, dst # offset += scratch
 357     addp metadataTable, dst # return &amp;metadataTable[offset]
 358 end
 359 
<span class="line-modified"> 360 macro jumpImpl(targetOffsetReg)</span>
 361     btiz targetOffsetReg, .outOfLineJumpTarget
 362     dispatchIndirect(targetOffsetReg)
 363 .outOfLineJumpTarget:
 364     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 365     nextInstruction()
 366 end
 367 
 368 macro commonOp(label, prologue, fn)
 369 _%label%:
 370     prologue()
 371     fn(narrow)




 372 
 373 # FIXME: We cannot enable wide16 bytecode in Windows CLoop. With MSVC, as CLoop::execute gets larger code
 374 # size, CLoop::execute gets higher stack height requirement. This makes CLoop::execute takes 160KB stack
 375 # per call, causes stack overflow error easily. For now, we disable wide16 optimization for Windows CLoop.
 376 # https://bugs.webkit.org/show_bug.cgi?id=198283
 377 if not C_LOOP_WIN
 378 _%label%_wide16:
 379     prologue()
 380     fn(wide16)




 381 end
 382 
 383 _%label%_wide32:
 384     prologue()
 385     fn(wide32)




 386 end
 387 
 388 macro op(l, fn)
 389     commonOp(l, macro () end, macro (size)
 390         size(fn, macro() end, macro() end, macro(gen) gen() end)
 391     end)
 392 end
 393 
 394 macro llintOp(opcodeName, opcodeStruct, fn)
 395     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 396         macro getImpl(fieldName, dst)
 397             get(size, opcodeStruct, fieldName, dst)
 398         end
 399 
 400         macro dispatchImpl()
 401             dispatchOp(size, opcodeName)
 402         end
 403 
 404         fn(size, getImpl, dispatchImpl)
 405     end)
</pre>
<hr />
<pre>
 409     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 410         makeReturn(get, dispatch, macro (return)
 411             fn(size, get, dispatch, return)
 412         end)
 413     end)
 414 end
 415 
 416 macro llintOpWithMetadata(opcodeName, opcodeStruct, fn)
 417     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
 418         macro meta(dst, scratch)
 419             metadata(size, opcodeStruct, dst, scratch)
 420         end
 421         fn(size, get, dispatch, meta, return)
 422     end)
 423 end
 424 
 425 macro llintOpWithJump(opcodeName, opcodeStruct, impl)
 426     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 427         macro jump(fieldName)
 428             get(fieldName, t0)
<span class="line-modified"> 429             jumpImpl(t0)</span>
 430         end
 431 
 432         impl(size, get, jump, dispatch)
 433     end)
 434 end
 435 
 436 macro llintOpWithProfile(opcodeName, opcodeStruct, fn)
 437     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 438         makeReturnProfiled(opcodeStruct, get, metadata, dispatch, macro (returnProfiled)
 439             fn(size, get, dispatch, returnProfiled)
 440         end)
 441     end)
 442 end
 443 
 444 
 445 if X86_64_WIN
 446     const extraTempReg = t0
 447 else
 448     const extraTempReg = t5
 449 end
</pre>
<hr />
<pre>
 487 
 488 const FirstTypedArrayType = constexpr FirstTypedArrayType
 489 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 490 
 491 # Type flags constants.
 492 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 493 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 494 
 495 # Bytecode operand constants.
 496 const FirstConstantRegisterIndexNarrow = constexpr FirstConstantRegisterIndex8
 497 const FirstConstantRegisterIndexWide16 = constexpr FirstConstantRegisterIndex16
 498 const FirstConstantRegisterIndexWide32 = constexpr FirstConstantRegisterIndex
 499 
 500 # Code type constants.
 501 const GlobalCode = constexpr GlobalCode
 502 const EvalCode = constexpr EvalCode
 503 const FunctionCode = constexpr FunctionCode
 504 const ModuleCode = constexpr ModuleCode
 505 
 506 # The interpreter steals the tag word of the argument count.
<span class="line-modified"> 507 const LLIntReturnPC = ArgumentCount + TagOffset</span>
 508 
 509 # String flags.
 510 const isRopeInPointer = constexpr JSString::isRopeInPointer
 511 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 512 
 513 # Copied from PropertyOffset.h
 514 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 515 
 516 # ResolveType
 517 const GlobalProperty = constexpr GlobalProperty
 518 const GlobalVar = constexpr GlobalVar
 519 const GlobalLexicalVar = constexpr GlobalLexicalVar
 520 const ClosureVar = constexpr ClosureVar
 521 const LocalClosureVar = constexpr LocalClosureVar
 522 const ModuleVar = constexpr ModuleVar
 523 const GlobalPropertyWithVarInjectionChecks = constexpr GlobalPropertyWithVarInjectionChecks
 524 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 525 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 526 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 527 
 528 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 529 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 530 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 531 const NotInitialization = constexpr InitializationMode::NotInitialization
 532 
 533 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 534 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 535 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter


 536 
 537 const BlackThreshold = constexpr blackThreshold
 538 
 539 const VectorBufferOffset = Vector::m_buffer
 540 const VectorSizeOffset = Vector::m_size
 541 
 542 # Some common utilities.
 543 macro crash()
 544     if C_LOOP or C_LOOP_WIN
 545         cloopCrash
 546     else
 547         call _llint_crash
 548     end
 549 end
 550 
 551 macro assert(assertion)
 552     if ASSERT_ENABLED
 553         assertion(.ok)
 554         crash()
 555     .ok:
 556     end
 557 end
 558 








 559 # The probe macro can be used to insert some debugging code without perturbing scalar
 560 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 561 # C probe callback function should not trash floating point registers.
 562 #
 563 # The macro you pass to probe() can pass whatever registers you like to your probe
 564 # callback function. However, you need to be mindful of which of the registers are
 565 # also used as argument registers, and ensure that you don&#39;t trash the register value
 566 # before storing it in the probe callback argument register that you desire.
 567 #
 568 # Here&#39;s an example of how it&#39;s used:
 569 #
 570 #     probe(
 571 #         macro()
<span class="line-modified"> 572 #             move cfr, a0 # pass the ExecState* as arg0.</span>
 573 #             move t0, a1 # pass the value of register t0 as arg1.
 574 #             call _cProbeCallbackFunction # to do whatever you want.
 575 #         end
 576 #     )
 577 #
 578 if X86_64 or ARM64 or ARM64E or ARMv7
 579     macro probe(action)
 580         # save all the registers that the LLInt may use.
 581         if ARM64 or ARM64E or ARMv7
 582             push cfr, lr
 583         end
 584         push a0, a1
 585         push a2, a3
 586         push t0, t1
 587         push t2, t3
 588         push t4, t5
 589         if ARM64 or ARM64E
 590             push csr0, csr1
 591             push csr2, csr3
 592             push csr4, csr5
 593             push csr6, csr7
 594             push csr8, csr9
 595         elsif ARMv7
<span class="line-modified"> 596             push csr0</span>
 597         end
 598 
 599         action()
 600 
 601         # restore all the registers we saved previously.
 602         if ARM64 or ARM64E
 603             pop csr9, csr8
 604             pop csr7, csr6
 605             pop csr5, csr4
 606             pop csr3, csr2
 607             pop csr1, csr0
 608         elsif ARMv7
<span class="line-modified"> 609             pop csr0</span>
 610         end
 611         pop t5, t4
 612         pop t3, t2
 613         pop t1, t0
 614         pop a3, a2
 615         pop a1, a0
 616         if ARM64 or ARM64E or ARMv7
 617             pop lr, cfr
 618         end
 619     end
 620 else
 621     macro probe(action)
 622     end
 623 end
 624 
 625 macro checkStackPointerAlignment(tempReg, location)
 626     if ASSERT_ENABLED
 627         if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
 628             # ARM64 and ARM64E will check for us!
 629             # C_LOOP or C_LOOP_WIN does not need the alignment, and can use a little perf
</pre>
<hr />
<pre>
 632             if ARMv7
 633                 # ARM can&#39;t do logical ops with the sp as a source
 634                 move sp, tempReg
 635                 andp StackAlignmentMask, tempReg
 636             else
 637                 andp sp, StackAlignmentMask, tempReg
 638             end
 639             btpz tempReg, .stackPointerOkay
 640             move location, tempReg
 641             break
 642         .stackPointerOkay:
 643         end
 644     end
 645 end
 646 
 647 if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 648     const CalleeSaveRegisterCount = 0
 649 elsif ARMv7
 650     const CalleeSaveRegisterCount = 7
 651 elsif MIPS
<span class="line-modified"> 652     const CalleeSaveRegisterCount = 2</span>
 653 elsif X86 or X86_WIN
 654     const CalleeSaveRegisterCount = 3
 655 end
 656 
 657 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 658 
 659 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 660 # callee save registers rounded up to keep the stack aligned
 661 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 662 
 663 macro pushCalleeSaves()
 664     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 665     elsif ARMv7
 666         emit &quot;push {r4-r6, r8-r11}&quot;
 667     elsif MIPS
<span class="line-modified"> 668         emit &quot;addiu $sp, $sp, -8&quot;</span>
 669         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
<span class="line-modified"> 670         emit &quot;sw $s4, 4($sp)&quot;</span>

 671         # save $gp to $s4 so that we can restore it after a function call
 672         emit &quot;move $s4, $gp&quot;
 673     elsif X86
 674         emit &quot;push %esi&quot;
 675         emit &quot;push %edi&quot;
 676         emit &quot;push %ebx&quot;
 677     elsif X86_WIN
 678         emit &quot;push esi&quot;
 679         emit &quot;push edi&quot;
 680         emit &quot;push ebx&quot;
 681     end
 682 end
 683 
 684 macro popCalleeSaves()
 685     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 686     elsif ARMv7
 687         emit &quot;pop {r4-r6, r8-r11}&quot;
 688     elsif MIPS
 689         emit &quot;lw $s0, 0($sp)&quot;
<span class="line-modified"> 690         emit &quot;lw $s4, 4($sp)&quot;</span>
<span class="line-modified"> 691         emit &quot;addiu $sp, $sp, 8&quot;</span>

 692     elsif X86
 693         emit &quot;pop %ebx&quot;
 694         emit &quot;pop %edi&quot;
 695         emit &quot;pop %esi&quot;
 696     elsif X86_WIN
 697         emit &quot;pop ebx&quot;
 698         emit &quot;pop edi&quot;
 699         emit &quot;pop esi&quot;
 700     end
 701 end
 702 
 703 macro preserveCallerPCAndCFR()
 704     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 705         push lr
 706         push cfr
 707     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 708         push cfr
 709     elsif ARM64 or ARM64E
 710         push cfr, lr
 711     else
</pre>
<hr />
<pre>
 713     end
 714     move sp, cfr
 715 end
 716 
 717 macro restoreCallerPCAndCFR()
 718     move cfr, sp
 719     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 720         pop cfr
 721         pop lr
 722     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 723         pop cfr
 724     elsif ARM64 or ARM64E
 725         pop lr, cfr
 726     end
 727 end
 728 
 729 macro preserveCalleeSavesUsedByLLInt()
 730     subp CalleeSaveSpaceStackAligned, sp
 731     if C_LOOP or C_LOOP_WIN
 732         storep metadataTable, -PtrSize[cfr]
<span class="line-modified"> 733     elsif ARMv7 or MIPS</span>








 734         storep metadataTable, -4[cfr]




 735     elsif ARM64 or ARM64E
 736         emit &quot;stp x27, x28, [x29, #-16]&quot;
 737         emit &quot;stp x25, x26, [x29, #-32]&quot;
 738     elsif X86
 739     elsif X86_WIN
 740     elsif X86_64
 741         storep csr4, -8[cfr]
 742         storep csr3, -16[cfr]
 743         storep csr2, -24[cfr]
 744         storep csr1, -32[cfr]
 745     elsif X86_64_WIN
 746         storep csr6, -8[cfr]
 747         storep csr5, -16[cfr]
 748         storep csr4, -24[cfr]
 749         storep csr3, -32[cfr]
 750     end
 751 end
 752 
 753 macro restoreCalleeSavesUsedByLLInt()
 754     if C_LOOP or C_LOOP_WIN
 755         loadp -PtrSize[cfr], metadataTable
<span class="line-modified"> 756     elsif ARMv7 or MIPS</span>


 757         loadp -4[cfr], metadataTable




 758     elsif ARM64 or ARM64E
 759         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 760         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 761     elsif X86
 762     elsif X86_WIN
 763     elsif X86_64
 764         loadp -32[cfr], csr1
 765         loadp -24[cfr], csr2
 766         loadp -16[cfr], csr3
 767         loadp -8[cfr], csr4
 768     elsif X86_64_WIN
 769         loadp -32[cfr], csr3
 770         loadp -24[cfr], csr4
 771         loadp -16[cfr], csr5
 772         loadp -8[cfr], csr6
 773     end
 774 end
 775 
<span class="line-modified"> 776 macro copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(vm, temp)</span>
 777     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
<span class="line-modified"> 778         loadp VM::topEntryFrame[vm], temp</span>
<span class="line-modified"> 779         vmEntryRecord(temp, temp)</span>
<span class="line-removed"> 780         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp</span>
 781         if ARM64 or ARM64E
<span class="line-modified"> 782             storeq csr0, [temp]</span>
<span class="line-modified"> 783             storeq csr1, 8[temp]</span>
<span class="line-modified"> 784             storeq csr2, 16[temp]</span>
<span class="line-modified"> 785             storeq csr3, 24[temp]</span>
<span class="line-modified"> 786             storeq csr4, 32[temp]</span>
<span class="line-modified"> 787             storeq csr5, 40[temp]</span>
<span class="line-modified"> 788             storeq csr6, 48[temp]</span>
<span class="line-modified"> 789             storeq csr7, 56[temp]</span>
<span class="line-modified"> 790             storeq csr8, 64[temp]</span>
<span class="line-modified"> 791             storeq csr9, 72[temp]</span>
<span class="line-modified"> 792             stored csfr0, 80[temp]</span>
<span class="line-modified"> 793             stored csfr1, 88[temp]</span>
<span class="line-modified"> 794             stored csfr2, 96[temp]</span>
<span class="line-modified"> 795             stored csfr3, 104[temp]</span>
<span class="line-modified"> 796             stored csfr4, 112[temp]</span>
<span class="line-modified"> 797             stored csfr5, 120[temp]</span>
<span class="line-modified"> 798             stored csfr6, 128[temp]</span>
<span class="line-modified"> 799             stored csfr7, 136[temp]</span>
 800         elsif X86_64
<span class="line-modified"> 801             storeq csr0, [temp]</span>
<span class="line-modified"> 802             storeq csr1, 8[temp]</span>
<span class="line-modified"> 803             storeq csr2, 16[temp]</span>
<span class="line-modified"> 804             storeq csr3, 24[temp]</span>
<span class="line-modified"> 805             storeq csr4, 32[temp]</span>
 806         elsif X86_64_WIN
<span class="line-modified"> 807             storeq csr0, [temp]</span>
<span class="line-modified"> 808             storeq csr1, 8[temp]</span>
<span class="line-modified"> 809             storeq csr2, 16[temp]</span>
<span class="line-modified"> 810             storeq csr3, 24[temp]</span>
<span class="line-modified"> 811             storeq csr4, 32[temp]</span>
<span class="line-modified"> 812             storeq csr5, 40[temp]</span>
<span class="line-modified"> 813             storeq csr6, 48[temp]</span>
 814         elsif ARMv7 or MIPS
<span class="line-modified"> 815             storep csr0, [temp]</span>

 816         end
 817     end
 818 end
 819 







 820 macro restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(vm, temp)
 821     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 822         loadp VM::topEntryFrame[vm], temp
 823         vmEntryRecord(temp, temp)
 824         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 825         if ARM64 or ARM64E
 826             loadq [temp], csr0
 827             loadq 8[temp], csr1
 828             loadq 16[temp], csr2
 829             loadq 24[temp], csr3
 830             loadq 32[temp], csr4
 831             loadq 40[temp], csr5
 832             loadq 48[temp], csr6
 833             loadq 56[temp], csr7
 834             loadq 64[temp], csr8
 835             loadq 72[temp], csr9
 836             loadd 80[temp], csfr0
 837             loadd 88[temp], csfr1
 838             loadd 96[temp], csfr2
 839             loadd 104[temp], csfr3
 840             loadd 112[temp], csfr4
 841             loadd 120[temp], csfr5
 842             loadd 128[temp], csfr6
 843             loadd 136[temp], csfr7
 844         elsif X86_64
 845             loadq [temp], csr0
 846             loadq 8[temp], csr1
 847             loadq 16[temp], csr2
 848             loadq 24[temp], csr3
 849             loadq 32[temp], csr4
 850         elsif X86_64_WIN
 851             loadq [temp], csr0
 852             loadq 8[temp], csr1
 853             loadq 16[temp], csr2
 854             loadq 24[temp], csr3
 855             loadq 32[temp], csr4
 856             loadq 40[temp], csr5
 857             loadq 48[temp], csr6
 858         elsif ARMv7 or MIPS
 859             loadp [temp], csr0

 860         end
 861     end
 862 end
 863 
 864 macro preserveReturnAddressAfterCall(destinationRegister)
 865     if C_LOOP or C_LOOP_WIN or ARMv7 or ARM64 or ARM64E or MIPS
 866         # In C_LOOP or C_LOOP_WIN case, we&#39;re only preserving the bytecode vPC.
 867         move lr, destinationRegister
 868     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 869         pop destinationRegister
 870     else
 871         error
 872     end
 873 end
 874 
 875 macro functionPrologue()
 876     tagReturnAddress sp
 877     if X86 or X86_WIN or X86_64 or X86_64_WIN
 878         push cfr
 879     elsif ARM64 or ARM64E
</pre>
<hr />
<pre>
 906     addp maxFrameExtentForSlowPathCall, size
 907 end
 908 
 909 macro restoreStackPointerAfterCall()
 910     loadp CodeBlock[cfr], t2
 911     getFrameRegisterSizeForCodeBlock(t2, t2)
 912     if ARMv7
 913         subp cfr, t2, t2
 914         move t2, sp
 915     else
 916         subp cfr, t2, sp
 917     end
 918 end
 919 
 920 macro traceExecution()
 921     if TRACING
 922         callSlowPath(_llint_trace)
 923     end
 924 end
 925 
<span class="line-modified"> 926 macro callTargetFunction(size, opcodeStruct, dispatch, callee, callPtrTag)</span>






















 927     if C_LOOP or C_LOOP_WIN
 928         cloopCallJSFunction callee
 929     else
 930         call callee, callPtrTag
 931     end










 932     restoreStackPointerAfterCall()
 933     dispatchAfterCall(size, opcodeStruct, dispatch)
 934 end
 935 
 936 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
 937     addp CallerFrameAndPCSize, sp
 938 end
 939 
 940 # sp points to the new frame
 941 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
 942     restoreCalleeSavesUsedByLLInt()
 943 
<span class="line-modified"> 944     loadi PayloadOffset + ArgumentCount[cfr], temp2</span>
 945     loadp CodeBlock[cfr], temp1
 946     loadi CodeBlock::m_numParameters[temp1], temp1
 947     bilteq temp1, temp2, .noArityFixup
 948     move temp1, temp2
 949 
 950 .noArityFixup:
 951     # We assume &lt; 2^28 arguments
 952     muli SlotSize, temp2
 953     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 954     andi ~StackAlignmentMask, temp2
 955 
 956     move cfr, temp1
 957     addp temp2, temp1
 958 
<span class="line-modified"> 959     loadi PayloadOffset + ArgumentCount[sp], temp2</span>
 960     # We assume &lt; 2^28 arguments
 961     muli SlotSize, temp2
 962     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 963     andi ~StackAlignmentMask, temp2
 964 
 965     if ARMv7 or ARM64 or ARM64E or C_LOOP or C_LOOP_WIN or MIPS
 966         addp CallerFrameAndPCSize, sp
 967         subi CallerFrameAndPCSize, temp2
 968         loadp CallerFrameAndPC::returnPC[cfr], lr
 969     else
 970         addp PtrSize, sp
 971         subi PtrSize, temp2
 972         loadp PtrSize[cfr], temp3
 973         storep temp3, [sp]
 974     end
 975 
 976     if ARM64E
 977         addp 16, cfr, temp3
 978         untagReturnAddress temp3
 979     end
</pre>
<hr />
<pre>
 981     subp temp2, temp1
 982     loadp [cfr], cfr
 983 
 984 .copyLoop:
 985     if ARM64 and not ADDRESS64
 986         subi MachineRegisterSize, temp2
 987         loadq [sp, temp2, 1], temp3
 988         storeq temp3, [temp1, temp2, 1]
 989         btinz temp2, .copyLoop
 990     else
 991         subi PtrSize, temp2
 992         loadp [sp, temp2, 1], temp3
 993         storep temp3, [temp1, temp2, 1]
 994         btinz temp2, .copyLoop
 995     end
 996 
 997     move temp1, sp
 998     jmp callee, callPtrTag
 999 end
1000 
<span class="line-modified">1001 macro slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)</span>
1002     callCallSlowPath(
1003         slowPath,
1004         # Those are r0 and r1
1005         macro (callee, calleeFramePtr)
1006             btpz calleeFramePtr, .dontUpdateSP
1007             move calleeFramePtr, sp
1008             prepareCall(callee, t2, t3, t4, SlowPathPtrTag)
1009         .dontUpdateSP:
<span class="line-modified">1010             callTargetFunction(size, opcodeStruct, dispatch, callee, SlowPathPtrTag)</span>
1011         end)
1012 end
1013 









1014 macro arrayProfile(offset, cellAndIndexingType, metadata, scratch)
1015     const cell = cellAndIndexingType
1016     const indexingType = cellAndIndexingType 
1017     loadi JSCell::m_structureID[cell], scratch
1018     storei scratch, offset + ArrayProfile::m_lastSeenStructureID[metadata]
1019     loadb JSCell::m_indexingTypeAndMisc[cell], indexingType
1020 end
1021 
1022 macro skipIfIsRememberedOrInEden(cell, slowPath)
1023     memfence
1024     bba JSCell::m_cellState[cell], BlackThreshold, .done
1025     slowPath()
1026 .done:
1027 end
1028 
1029 macro notifyWrite(set, slow)
1030     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1031 end
1032 
1033 macro checkSwitchToJIT(increment, action)
</pre>
<hr />
<pre>
1040 macro checkSwitchToJITForEpilogue()
1041     checkSwitchToJIT(
1042         10,
1043         macro ()
1044             callSlowPath(_llint_replace)
1045         end)
1046 end
1047 
1048 macro assertNotConstant(size, index)
1049     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
1050         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1051     end)
1052 end
1053 
1054 macro functionForCallCodeBlockGetter(targetRegister)
1055     if JSVALUE64
1056         loadp Callee[cfr], targetRegister
1057     else
1058         loadp Callee + PayloadOffset[cfr], targetRegister
1059     end
<span class="line-modified">1060     loadp JSFunction::m_executable[targetRegister], targetRegister</span>



1061     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1062     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1063 end
1064 
1065 macro functionForConstructCodeBlockGetter(targetRegister)
1066     if JSVALUE64
1067         loadp Callee[cfr], targetRegister
1068     else
1069         loadp Callee + PayloadOffset[cfr], targetRegister
1070     end
<span class="line-modified">1071     loadp JSFunction::m_executable[targetRegister], targetRegister</span>



1072     loadp FunctionExecutable::m_codeBlockForConstruct[targetRegister], targetRegister
1073     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1074 end
1075 
1076 macro notFunctionCodeBlockGetter(targetRegister)
1077     loadp CodeBlock[cfr], targetRegister
1078 end
1079 
1080 macro functionCodeBlockSetter(sourceRegister)
1081     storep sourceRegister, CodeBlock[cfr]
1082 end
1083 
1084 macro notFunctionCodeBlockSetter(sourceRegister)
1085     # Nothing to do!
1086 end
1087 










1088 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1089 # in t1. May also trigger prologue entry OSR.
1090 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1091     # Set up the call frame and check if we should OSR.
1092     tagReturnAddress sp
1093     preserveCallerPCAndCFR()
1094 
1095     if TRACING
1096         subp maxFrameExtentForSlowPathCall, sp
1097         callSlowPath(traceSlowPath)
1098         addp maxFrameExtentForSlowPathCall, sp
1099     end
1100     codeBlockGetter(t1)

1101     if not (C_LOOP or C_LOOP_WIN)
1102         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1103         if JSVALUE64
1104             move cfr, a0
1105             move PC, a1
1106             cCall2(osrSlowPath)
1107         else
1108             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1109             # Temporarily align stack pointer for this call.
1110             subp 8, sp
1111             move cfr, a0
1112             move PC, a1
1113             cCall2(osrSlowPath)
1114             addp 8, sp
1115         end
1116         btpz r0, .recover
1117         move cfr, sp # restore the previous sp
1118         # pop the callerFrame since we will jump to a function that wants to save it
1119         if ARM64 or ARM64E
1120             pop lr, cfr
1121             untagReturnAddress sp
1122         elsif ARMv7 or MIPS
1123             pop cfr
1124             pop lr
1125         else
1126             pop cfr
1127         end
1128         jmp r0, JSEntryPtrTag
1129     .recover:
<span class="line-modified">1130         codeBlockGetter(t1)</span>
1131     .continue:
1132     end
1133 
<span class="line-removed">1134     codeBlockSetter(t1)</span>
<span class="line-removed">1135 </span>
1136     preserveCalleeSavesUsedByLLInt()
1137 
1138     # Set up the PC.
<span class="line-modified">1139     if JSVALUE64</span>
<span class="line-modified">1140         loadp CodeBlock::m_instructionsRawPointer[t1], PB</span>
<span class="line-removed">1141         move 0, PC</span>
<span class="line-removed">1142     else</span>
<span class="line-removed">1143         loadp CodeBlock::m_instructionsRawPointer[t1], PC</span>
<span class="line-removed">1144     end</span>
1145 
1146     # Get new sp in t0 and check stack height.
1147     getFrameRegisterSizeForCodeBlock(t1, t0)
1148     subp cfr, t0, t0
1149     bpa t0, cfr, .needStackCheck
1150     loadp CodeBlock::m_vm[t1], t2
1151     if C_LOOP or C_LOOP_WIN
1152         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1153     else
1154         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1155     end
1156 
1157 .needStackCheck:
1158     # Stack height check failed - need to call a slow_path.
1159     # Set up temporary stack pointer for call including callee saves
1160     subp maxFrameExtentForSlowPathCall, sp
1161     callSlowPath(_llint_stack_check)
1162     bpeq r1, 0, .stackHeightOKGetCodeBlock
1163 
1164     # We&#39;re throwing before the frame is fully set up. This frame will be
1165     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1166     # start unwinding. We need to do this before we change the cfr.
1167     restoreCalleeSavesUsedByLLInt()
1168 
1169     move r1, cfr
1170     jmp _llint_throw_from_slow_path_trampoline
1171 
1172 .stackHeightOKGetCodeBlock:
1173     # Stack check slow path returned that the stack was ok.
1174     # Since they were clobbered, need to get CodeBlock and new sp
<span class="line-modified">1175     codeBlockGetter(t1)</span>
1176     getFrameRegisterSizeForCodeBlock(t1, t0)
1177     subp cfr, t0, t0
1178 
1179 .stackHeightOK:
1180     if X86_64 or ARM64
1181         # We need to start zeroing from sp as it has been adjusted after saving callee saves.
1182         move sp, t2
1183         move t0, sp
1184 .zeroStackLoop:
1185         bpeq sp, t2, .zeroStackDone
1186         subp PtrSize, t2
1187         storep 0, [t2]
1188         jmp .zeroStackLoop
1189 .zeroStackDone:
1190     else
1191         move t0, sp
1192     end
1193 
1194     loadp CodeBlock::m_metadata[t1], metadataTable
1195 
1196     if JSVALUE64
<span class="line-modified">1197         move TagTypeNumber, tagTypeNumber</span>
<span class="line-modified">1198         addq TagBitTypeOther, tagTypeNumber, tagMask</span>
1199     end
1200 end
1201 
1202 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
1203 # Must call dispatch(0) after calling this.
1204 macro functionInitialization(profileArgSkip)
1205     # Profile the arguments. Unfortunately, we have no choice but to do this. This
1206     # code is pretty horrendous because of the difference in ordering between
1207     # arguments and value profiles, the desire to have a simple loop-down-to-zero
1208     # loop, and the desire to use only three registers so as to preserve the PC and
1209     # the code block. It is likely that this code should be rewritten in a more
1210     # optimal way for architectures that have more than five registers available
1211     # for arbitrary use in the interpreter.
1212     loadi CodeBlock::m_numParameters[t1], t0
1213     addp -profileArgSkip, t0 # Use addi because that&#39;s what has the peephole
1214     assert(macro (ok) bpgteq t0, 0, ok end)
1215     btpz t0, .argumentProfileDone
1216     loadp CodeBlock::m_argumentValueProfiles + RefCountedArray::m_data[t1], t3
1217     btpz t3, .argumentProfileDone # When we can&#39;t JIT, we don&#39;t allocate any argument value profiles.
1218     mulp sizeof ValueProfile, t0, t2 # Aaaaahhhh! Need strength reduction!
</pre>
<hr />
<pre>
1262 
1263 if C_LOOP or C_LOOP_WIN
1264     _llint_vm_entry_to_native:
1265 else
1266     global _vmEntryToNative
1267     _vmEntryToNative:
1268 end
1269     doVMEntry(makeHostFunctionCall)
1270 
1271 
1272 if not (C_LOOP or C_LOOP_WIN)
1273     # void sanitizeStackForVMImpl(VM* vm)
1274     global _sanitizeStackForVMImpl
1275     _sanitizeStackForVMImpl:
1276         tagReturnAddress sp
1277         # We need three non-aliased caller-save registers. We are guaranteed
1278         # this for a0, a1 and a2 on all architectures.
1279         if X86 or X86_WIN
1280             loadp 4[sp], a0
1281         end
<span class="line-modified">1282         const vm = a0</span>
1283         const address = a1
1284         const zeroValue = a2
1285     
<span class="line-modified">1286         loadp VM::m_lastStackTop[vm], address</span>




1287         bpbeq sp, address, .zeroFillDone
<span class="line-modified">1288     </span>

1289         move 0, zeroValue
1290     .zeroFillLoop:
1291         storep zeroValue, [address]
1292         addp PtrSize, address
<span class="line-modified">1293         bpa sp, address, .zeroFillLoop</span>
1294 
1295     .zeroFillDone:
<span class="line-modified">1296         move sp, address</span>
<span class="line-removed">1297         storep address, VM::m_lastStackTop[vm]</span>
1298         ret
1299     
1300     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1301     global _vmEntryRecord
1302     _vmEntryRecord:
1303         tagReturnAddress sp
1304         if X86 or X86_WIN
1305             loadp 4[sp], a0
1306         end
1307 
1308         vmEntryRecord(a0, r0)
1309         ret
1310 end
1311 
1312 if C_LOOP or C_LOOP_WIN
1313     # Dummy entry point the C Loop uses to initialize.
1314     _llint_entry:
1315         crash()
1316 else
<span class="line-modified">1317     macro initPCRelative(pcBase)</span>
1318         if X86_64 or X86_64_WIN or X86 or X86_WIN
<span class="line-modified">1319             call _relativePCBase</span>
<span class="line-modified">1320         _relativePCBase:</span>
1321             pop pcBase
1322         elsif ARM64 or ARM64E
1323         elsif ARMv7
<span class="line-modified">1324         _relativePCBase:</span>
1325             move pc, pcBase
1326             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1327         elsif MIPS
<span class="line-modified">1328             la _relativePCBase, pcBase</span>
1329             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
<span class="line-modified">1330         _relativePCBase:</span>
1331         end
<span class="line-modified">1332 end</span>
1333 
<span class="line-modified">1334 # The PC base is in t3, as this is what _llint_entry leaves behind through</span>
<span class="line-modified">1335 # initPCRelative(t3)</span>
<span class="line-modified">1336 macro setEntryAddress(index, label)</span>
<span class="line-modified">1337     setEntryAddressCommon(index, label, a0)</span>
<span class="line-modified">1338 end</span>




























1339 
<span class="line-removed">1340 macro setEntryAddressWide16(index, label)</span>
<span class="line-removed">1341      setEntryAddressCommon(index, label, a1)</span>
<span class="line-removed">1342 end</span>
1343 
<span class="line-removed">1344 macro setEntryAddressWide32(index, label)</span>
<span class="line-removed">1345      setEntryAddressCommon(index, label, a2)</span>
<span class="line-removed">1346 end</span>
1347 
<span class="line-modified">1348 macro setEntryAddressCommon(index, label, map)</span>
<span class="line-modified">1349     if X86_64</span>
<span class="line-modified">1350         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-modified">1351         move index, t5</span>
<span class="line-removed">1352         storep t4, [map, t5, 8]</span>
<span class="line-removed">1353     elsif X86_64_WIN</span>
<span class="line-removed">1354         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-removed">1355         move index, t0</span>
<span class="line-removed">1356         storep t4, [map, t0, 8]</span>
<span class="line-removed">1357     elsif X86 or X86_WIN</span>
<span class="line-removed">1358         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-removed">1359         move index, t5</span>
<span class="line-removed">1360         storep t4, [map, t5, 4]</span>
<span class="line-removed">1361     elsif ARM64 or ARM64E</span>
<span class="line-removed">1362         pcrtoaddr label, t3</span>
<span class="line-removed">1363         move index, t4</span>
<span class="line-removed">1364         storep t3, [map, t4, PtrSize]</span>
<span class="line-removed">1365     elsif ARMv7</span>
<span class="line-removed">1366         mvlbl (label - _relativePCBase), t4</span>
<span class="line-removed">1367         addp t4, t3, t4</span>
<span class="line-removed">1368         move index, t5</span>
<span class="line-removed">1369         storep t4, [map, t5, 4]</span>
<span class="line-removed">1370     elsif MIPS</span>
<span class="line-removed">1371         la label, t4</span>
<span class="line-removed">1372         la _relativePCBase, t3</span>
<span class="line-removed">1373         subp t3, t4</span>
<span class="line-removed">1374         addp t4, t3, t4</span>
<span class="line-removed">1375         move index, t5</span>
<span class="line-removed">1376         storep t4, [map, t5, 4]</span>
<span class="line-removed">1377     end</span>
<span class="line-removed">1378 end</span>
1379 
<span class="line-modified">1380 global _llint_entry</span>
<span class="line-modified">1381 # Entry point for the llint to initialize.</span>
<span class="line-modified">1382 _llint_entry:</span>
<span class="line-modified">1383     functionPrologue()</span>
<span class="line-modified">1384     pushCalleeSaves()</span>
<span class="line-modified">1385     if X86 or X86_WIN</span>
<span class="line-modified">1386         loadp 20[sp], a0</span>
<span class="line-modified">1387         loadp 24[sp], a1</span>
<span class="line-modified">1388         loadp 28[sp], a2</span>
1389     end
1390 
<span class="line-removed">1391     initPCRelative(t3)</span>
1392 
<span class="line-modified">1393     # Include generated bytecode initialization file.</span>
<span class="line-modified">1394     include InitBytecodes</span>








1395 
<span class="line-modified">1396     popCalleeSaves()</span>
<span class="line-modified">1397     functionEpilogue()</span>
<span class="line-modified">1398     ret</span>




1399 end
1400 







1401 _llint_op_wide16:
1402     nextInstructionWide16()
1403 
1404 _llint_op_wide32:
1405     nextInstructionWide32()
1406 
1407 macro noWide(label)
<span class="line-modified">1408 _llint_%label%_wide16:</span>
1409     crash()
1410 
<span class="line-modified">1411 _llint_%label%_wide32:</span>
1412     crash()
1413 end
1414 
<span class="line-modified">1415 noWide(op_wide16)</span>
<span class="line-modified">1416 noWide(op_wide32)</span>
<span class="line-modified">1417 noWide(op_enter)</span>
1418 
1419 op(llint_program_prologue, macro ()
1420     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1421     dispatch(0)
1422 end)
1423 
1424 
1425 op(llint_module_program_prologue, macro ()
1426     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1427     dispatch(0)
1428 end)
1429 
1430 
1431 op(llint_eval_prologue, macro ()
1432     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1433     dispatch(0)
1434 end)
1435 
1436 
1437 op(llint_function_for_call_prologue, macro ()
</pre>
<hr />
<pre>
1466 end)
1467 
1468 
1469 # Value-representation-specific code.
1470 if JSVALUE64
1471     include LowLevelInterpreter64
1472 else
1473     include LowLevelInterpreter32_64
1474 end
1475 
1476 
1477 # Value-representation-agnostic code.
1478 macro slowPathOp(opcodeName)
1479     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1480         callSlowPath(_slow_path_%opcodeName%)
1481         dispatch()
1482     end)
1483 end
1484 
1485 slowPathOp(create_cloned_arguments)

1486 slowPathOp(create_direct_arguments)
1487 slowPathOp(create_lexical_environment)
1488 slowPathOp(create_rest)
1489 slowPathOp(create_scoped_arguments)
1490 slowPathOp(create_this)



1491 slowPathOp(define_accessor_property)
1492 slowPathOp(define_data_property)
1493 slowPathOp(enumerator_generic_pname)
1494 slowPathOp(enumerator_structure_pname)
1495 slowPathOp(get_by_id_with_this)
1496 slowPathOp(get_by_val_with_this)
1497 slowPathOp(get_direct_pname)
1498 slowPathOp(get_enumerable_length)
1499 slowPathOp(get_property_enumerator)
1500 slowPathOp(greater)
1501 slowPathOp(greatereq)
1502 slowPathOp(has_generic_property)
1503 slowPathOp(has_indexed_property)
1504 slowPathOp(has_structure_property)
1505 slowPathOp(in_by_id)
1506 slowPathOp(in_by_val)
1507 slowPathOp(is_function)
1508 slowPathOp(is_object_or_null)
1509 slowPathOp(less)
1510 slowPathOp(lesseq)
1511 slowPathOp(mod)
1512 slowPathOp(new_array_buffer)
1513 slowPathOp(new_array_with_spread)
1514 slowPathOp(pow)
1515 slowPathOp(push_with_scope)
1516 slowPathOp(put_by_id_with_this)
1517 slowPathOp(put_by_val_with_this)
1518 slowPathOp(resolve_scope_for_hoisting_func_decl_in_eval)
1519 slowPathOp(spread)
1520 slowPathOp(strcat)
1521 slowPathOp(throw_static_error)
1522 slowPathOp(to_index_string)
1523 slowPathOp(typeof)
1524 slowPathOp(unreachable)


1525 
1526 macro llintSlowPathOp(opcodeName)
1527     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1528         callSlowPath(_llint_slow_path_%opcodeName%)
1529         dispatch()
1530     end)
1531 end
1532 
1533 llintSlowPathOp(del_by_id)
1534 llintSlowPathOp(del_by_val)
1535 llintSlowPathOp(instanceof)
1536 llintSlowPathOp(instanceof_custom)
1537 llintSlowPathOp(new_array)
1538 llintSlowPathOp(new_array_with_size)
1539 llintSlowPathOp(new_async_func)
1540 llintSlowPathOp(new_async_func_exp)
1541 llintSlowPathOp(new_async_generator_func)
1542 llintSlowPathOp(new_async_generator_func_exp)
1543 llintSlowPathOp(new_func)
1544 llintSlowPathOp(new_func_exp)
</pre>
<hr />
<pre>
1648 
1649 
1650 equalityJumpOp(
1651     jneq, OpJneq,
1652     macro (left, right, target) bineq left, right, target end)
1653 
1654 
1655 compareUnsignedJumpOp(
1656     jbelow, OpJbelow,
1657     macro (left, right, target) bib left, right, target end)
1658 
1659 
1660 compareUnsignedJumpOp(
1661     jbeloweq, OpJbeloweq,
1662     macro (left, right, target) bibeq left, right, target end)
1663 
1664 
1665 preOp(inc, OpInc,
1666     macro (value, slow) baddio 1, value, slow end)
1667 
<span class="line-removed">1668 </span>
1669 preOp(dec, OpDec,
1670     macro (value, slow) bsubio 1, value, slow end)
1671 
1672 
1673 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
<span class="line-modified">1674     # CheckTraps.</span>





1675     loadp CodeBlock[cfr], t1
1676     loadp CodeBlock::m_vm[t1], t1
<span class="line-modified">1677     btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps</span>

1678 .afterHandlingTraps:
<span class="line-removed">1679     checkSwitchToJITForLoop()</span>
1680     dispatch()
1681 .handleTraps:
<span class="line-modified">1682     callTrapHandler(_llint_throw_from_slow_path_trampoline)</span>
1683     jmp .afterHandlingTraps


1684 end)
1685 
1686 
1687 # Returns the packet pointer in t0.
1688 macro acquireShadowChickenPacket(slow)
1689     loadp CodeBlock[cfr], t1
1690     loadp CodeBlock::m_vm[t1], t1
1691     loadp VM::m_shadowChicken[t1], t2
1692     loadp ShadowChicken::m_logCursor[t2], t0
1693     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1694     addp sizeof ShadowChicken::Packet, t0, t1
1695     storep t1, ShadowChicken::m_logCursor[t2]
1696 end
1697 
1698 
1699 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1700     dispatch()
1701 end)
1702 
1703 
</pre>
<hr />
<pre>
1706     arrayProfileForCall(OpCall, getu)
1707 end)
1708 
1709 
1710 macro callOp(opcodeName, opcodeStruct, prepareCall, fn)
1711     commonCallOp(op_%opcodeName%, _llint_slow_path_%opcodeName%, opcodeStruct, prepareCall, fn)
1712 end
1713 
1714 
1715 callOp(tail_call, OpTailCall, prepareForTailCall, macro (getu, metadata)
1716     arrayProfileForCall(OpTailCall, getu)
1717     checkSwitchToJITForEpilogue()
1718     # reload metadata since checkSwitchToJITForEpilogue() might have trashed t5
1719     metadata(t5, t0)
1720 end)
1721 
1722 
1723 callOp(construct, OpConstruct, prepareForRegularCall, macro (getu, metadata) end)
1724 
1725 
<span class="line-modified">1726 macro doCallVarargs(size, opcodeStruct, dispatch, frameSlowPath, slowPath, prepareCall)</span>








1727     callSlowPath(frameSlowPath)
1728     branchIfException(_llint_throw_from_slow_path_trampoline)
1729     # calleeFrame in r1
1730     if JSVALUE64
1731         move r1, sp
1732     else
1733         # The calleeFrame is not stack aligned, move down by CallerFrameAndPCSize to align
1734         if ARMv7
1735             subp r1, CallerFrameAndPCSize, t2
1736             move t2, sp
1737         else
1738             subp r1, CallerFrameAndPCSize, sp
1739         end
1740     end
<span class="line-modified">1741     slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)</span>
1742 end
1743 
1744 
1745 llintOp(op_call_varargs, OpCallVarargs, macro (size, get, dispatch)
<span class="line-modified">1746     doCallVarargs(size, OpCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_call_varargs, prepareForRegularCall)</span>
1747 end)
1748 
1749 llintOp(op_tail_call_varargs, OpTailCallVarargs, macro (size, get, dispatch)
1750     checkSwitchToJITForEpilogue()
1751     # We lie and perform the tail call instead of preparing it since we can&#39;t
1752     # prepare the frame for a call opcode
<span class="line-modified">1753     doCallVarargs(size, OpTailCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_tail_call_varargs, prepareForTailCall)</span>
1754 end)
1755 
1756 
1757 llintOp(op_tail_call_forward_arguments, OpTailCallForwardArguments, macro (size, get, dispatch)
1758     checkSwitchToJITForEpilogue()
1759     # We lie and perform the tail call instead of preparing it since we can&#39;t
1760     # prepare the frame for a call opcode
<span class="line-modified">1761     doCallVarargs(size, OpTailCallForwardArguments, dispatch, _llint_slow_path_size_frame_for_forward_arguments, _llint_slow_path_tail_call_forward_arguments, prepareForTailCall)</span>
1762 end)
1763 
1764 
1765 llintOp(op_construct_varargs, OpConstructVarargs, macro (size, get, dispatch)
<span class="line-modified">1766     doCallVarargs(size, OpConstructVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_construct_varargs, prepareForRegularCall)</span>
1767 end)
1768 
1769 
1770 # Eval is executed in one of two modes:
1771 #
1772 # 1) We find that we&#39;re really invoking eval() in which case the
1773 #    execution is perfomed entirely inside the slow_path, and it
1774 #    returns the PC of a function that just returns the return value
1775 #    that the eval returned.
1776 #
1777 # 2) We find that we&#39;re invoking something called eval() that is not
1778 #    the real eval. Then the slow_path returns the PC of the thing to
1779 #    call, and we call it.
1780 #
1781 # This allows us to handle two cases, which would require a total of
1782 # up to four pieces of state that cannot be easily packed into two
1783 # registers (C functions can return up to two registers, easily):
1784 #
1785 # - The call frame register. This may or may not have been modified
1786 #   by the slow_path, but the convention is that it returns it. It&#39;s not
1787 #   totally clear if that&#39;s necessary, since the cfr is callee save.
1788 #   But that&#39;s our style in this here interpreter so we stick with it.
1789 #
1790 # - A bit to say if the slow_path successfully executed the eval and has
1791 #   the return value, or did not execute the eval but has a PC for us
1792 #   to call.
1793 #
1794 # - Either:
1795 #   - The JS return value (two registers), or
1796 #
1797 #   - The PC to call.
1798 #
1799 # It turns out to be easier to just always have this return the cfr
1800 # and a PC to call, and that PC may be a dummy thunk that just
1801 # returns the JS value that the eval returned.
1802 
1803 _llint_op_call_eval:
1804     slowPathForCall(

1805         narrow,
1806         OpCallEval,
1807         macro () dispatchOp(narrow, op_call_eval) end,
1808         _llint_slow_path_call_eval,
1809         prepareForRegularCall)
1810 
1811 _llint_op_call_eval_wide16:
1812     slowPathForCall(

1813         wide16,
1814         OpCallEval,
1815         macro () dispatchOp(wide16, op_call_eval) end,
1816         _llint_slow_path_call_eval_wide16,
1817         prepareForRegularCall)
1818 
1819 _llint_op_call_eval_wide32:
1820     slowPathForCall(

1821         wide32,
1822         OpCallEval,
1823         macro () dispatchOp(wide32, op_call_eval) end,
1824         _llint_slow_path_call_eval_wide32,
1825         prepareForRegularCall)
1826 
1827 
1828 commonOp(llint_generic_return_point, macro () end, macro (size)
1829     dispatchAfterCall(size, OpCallEval, macro ()
1830         dispatchOp(size, op_call_eval)
1831     end)
1832 end)
1833 
1834 
1835 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
1836     dispatch()
1837 end)
1838 
1839 
1840 llintOp(op_yield, OpYield, macro (unused, unused, unused)
</pre>
<hr />
<pre>
1860 op(llint_native_call_trampoline, macro ()
1861     nativeCallTrampoline(NativeExecutable::m_function)
1862 end)
1863 
1864 
1865 op(llint_native_construct_trampoline, macro ()
1866     nativeCallTrampoline(NativeExecutable::m_constructor)
1867 end)
1868 
1869 
1870 op(llint_internal_function_call_trampoline, macro ()
1871     internalFunctionCallTrampoline(InternalFunction::m_functionForCall)
1872 end)
1873 
1874 
1875 op(llint_internal_function_construct_trampoline, macro ()
1876     internalFunctionCallTrampoline(InternalFunction::m_functionForConstruct)
1877 end)
1878 
1879 















































1880 # Lastly, make sure that we can link even though we don&#39;t support all opcodes.
1881 # These opcodes should never arise when using LLInt or either JIT. We assert
1882 # as much.
1883 
1884 macro notSupported()
1885     if ASSERT_ENABLED
1886         crash()
1887     else
1888         # We should use whatever the smallest possible instruction is, just to
1889         # ensure that there is a gap between instruction labels. If multiple
1890         # smallest instructions exist, we should pick the one that is most
1891         # likely result in execution being halted. Currently that is the break
1892         # instruction on all architectures we&#39;re interested in. (Break is int3
1893         # on Intel, which is 1 byte, and bkpt on ARMv7, which is 2 bytes.)
1894         break
1895     end
1896 end




























</pre>
</td>
<td>
<hr />
<pre>
<span class="line-modified">   1 # Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
</pre>
<hr />
<pre>
  57 #   predicate of an &quot;if&quot; is assumed to be a #define that is available
  58 #   during code gen. So you can&#39;t use &quot;if&quot; for computation in a macro, but
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #


  77 #  - t0, t1, t2, t3, t4, and optionally t5, t6, and t7 are temporary registers that can get trashed on
  78 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  79 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  80 #
  81 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  82 #  two distinct registers
  83 #
  84 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  85 #  thus are pairwise distinct registers. Be mindful that:
  86 #    + On X86, there are no argument registers. a0 and a1 are edx and
  87 #    ecx following the fastcall convention, but you should still use the stack
  88 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  89 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  90 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  91 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  92 #    this for you.
  93 #
  94 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  95 #  you should be mindful of that in functions that are called directly from C.
  96 #  If you need more registers, you should push and pop them like a good
  97 #  assembly citizen, because any other register will be callee-saved on X86.
  98 #
  99 # You can additionally assume:
 100 #
 101 #  - a3, t2, t3, t4 and t5 are never return registers; t0, t1, a0, a1 and a2
 102 #  can be return registers.
 103 #
 104 #  - t4 and t5 are never argument registers, t3 can only be a3, t1 can only be
 105 #  a1; but t0 and t2 can be either a0 or a2.
 106 #
<span class="line-modified"> 107 #  - There are callee-save registers named csr0, csr1, ... csrN.</span>
 108 #  The last three csr registers are used used to store the PC base and
<span class="line-modified"> 109 #  two special tag values (on 64-bits only). Don&#39;t use them for anything else.</span>
 110 #
 111 # Additional platform-specific details (you shouldn&#39;t rely on this remaining
 112 # true):
 113 #
 114 #  - For consistency with the baseline JIT, t0 is always r0 (and t1 is always
 115 #  r1 on 32 bits platforms). You should use the r version when you need return
 116 #  registers, and the t version otherwise: code using t0 (or t1) should still
 117 #  work if swapped with e.g. t3, while code using r0 (or r1) should not. There
 118 #  *may* be legacy code relying on this.
 119 #
 120 #  - On all platforms other than X86, t0 can only be a0 and t2 can only be a2.
 121 #
 122 #  - On all platforms other than X86 and X86_64, a2 is not a return register.
 123 #  a2 is r0 on X86 (because we have so few registers) and r1 on X86_64 (because
 124 #  the ABI enforces it).
 125 #
 126 # The following floating-point registers are available:
 127 #
 128 #  - ft0-ft5 are temporary floating-point registers that get trashed on calls,
 129 #  and are pairwise distinct.
</pre>
<hr />
<pre>
 146 if ARMv7k
 147 end
 148 if ARMv7s
 149 end
 150 
 151 # These declarations must match interpreter/JSStack.h.
 152 
 153 const PtrSize = constexpr (sizeof(void*))
 154 const MachineRegisterSize = constexpr (sizeof(CPURegister))
 155 const SlotSize = constexpr (sizeof(Register))
 156 
 157 if JSVALUE64
 158     const CallFrameHeaderSlots = 5
 159 else
 160     const CallFrameHeaderSlots = 4
 161     const CallFrameAlignSlots = 1
 162 end
 163 
 164 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 165 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)
<span class="line-added"> 166 const JSInternalFieldObjectImpl_internalFields = JSInternalFieldObjectImpl::m_internalFields</span>
 167 
 168 const StackAlignment = constexpr (stackAlignmentBytes())
 169 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 170 const StackAlignmentMask = StackAlignment - 1
 171 
 172 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 173 
 174 const CallerFrame = 0
 175 const ReturnPC = CallerFrame + MachineRegisterSize
 176 const CodeBlock = ReturnPC + MachineRegisterSize
 177 const Callee = CodeBlock + SlotSize
<span class="line-modified"> 178 const ArgumentCountIncludingThis = Callee + SlotSize</span>
<span class="line-modified"> 179 const ThisArgumentOffset = ArgumentCountIncludingThis + SlotSize</span>
 180 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 181 const CallFrameHeaderSize = ThisArgumentOffset
 182 
 183 const MetadataOffsetTable16Offset = 0
 184 const MetadataOffsetTable32Offset = constexpr UnlinkedMetadataTable::s_offset16TableSize
<span class="line-added"> 185 const NumberOfJSOpcodeIDs = constexpr numOpcodeIDs</span>
 186 
 187 # Some value representation constants.
 188 if JSVALUE64
<span class="line-modified"> 189     const TagOther        = constexpr JSValue::OtherTag</span>
<span class="line-modified"> 190     const TagBool         = constexpr JSValue::BoolTag</span>
<span class="line-modified"> 191     const TagUndefined    = constexpr JSValue::UndefinedTag</span>
<span class="line-modified"> 192     const ValueEmpty      = constexpr JSValue::ValueEmpty</span>
<span class="line-modified"> 193     const ValueFalse      = constexpr JSValue::ValueFalse</span>
<span class="line-modified"> 194     const ValueTrue       = constexpr JSValue::ValueTrue</span>
<span class="line-modified"> 195     const ValueUndefined  = constexpr JSValue::ValueUndefined</span>
<span class="line-modified"> 196     const ValueNull       = constexpr JSValue::ValueNull</span>
<span class="line-modified"> 197     const TagNumber       = constexpr JSValue::NumberTag</span>
<span class="line-modified"> 198     const NotCellMask     = constexpr JSValue::NotCellMask</span>
 199 else
 200     const Int32Tag = constexpr JSValue::Int32Tag
 201     const BooleanTag = constexpr JSValue::BooleanTag
 202     const NullTag = constexpr JSValue::NullTag
 203     const UndefinedTag = constexpr JSValue::UndefinedTag
 204     const CellTag = constexpr JSValue::CellTag
 205     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 206     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 207     const LowestTag = constexpr JSValue::LowestTag
 208 end
 209 
 210 if JSVALUE64
 211     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 212     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 213 end
 214 


 215 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 216 
 217 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 218     const CalleeSaveSpaceAsVirtualRegisters = 4
 219 elsif C_LOOP or C_LOOP_WIN
 220     const CalleeSaveSpaceAsVirtualRegisters = 1
 221 elsif ARMv7
 222     const CalleeSaveSpaceAsVirtualRegisters = 1
 223 elsif MIPS
 224     const CalleeSaveSpaceAsVirtualRegisters = 1
 225 else
 226     const CalleeSaveSpaceAsVirtualRegisters = 0
 227 end
 228 
 229 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 230 
 231 
 232 # Watchpoint states
 233 const ClearWatchpoint = constexpr ClearWatchpoint
 234 const IsWatched = constexpr IsWatched
 235 const IsInvalidated = constexpr IsInvalidated
 236 
 237 # ShadowChicken data
 238 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 239 
<span class="line-modified"> 240 # UnaryArithProfile data</span>
<span class="line-modified"> 241 const ArithProfileInt = constexpr (UnaryArithProfile::observedIntBits())</span>
<span class="line-modified"> 242 const ArithProfileNumber = constexpr (UnaryArithProfile::observedNumberBits())</span>
<span class="line-modified"> 243 </span>
<span class="line-modified"> 244 # BinaryArithProfile data</span>
<span class="line-modified"> 245 const ArithProfileIntInt = constexpr (BinaryArithProfile::observedIntIntBits())</span>
<span class="line-modified"> 246 const ArithProfileNumberInt = constexpr (BinaryArithProfile::observedNumberIntBits())</span>
<span class="line-added"> 247 const ArithProfileIntNumber = constexpr (BinaryArithProfile::observedIntNumberBits())</span>
<span class="line-added"> 248 const ArithProfileNumberNumber = constexpr (BinaryArithProfile::observedNumberNumberBits())</span>
 249 
 250 # Pointer Tags
 251 const BytecodePtrTag = constexpr BytecodePtrTag
 252 const JSEntryPtrTag = constexpr JSEntryPtrTag
 253 const ExceptionHandlerPtrTag = constexpr ExceptionHandlerPtrTag
 254 const NoPtrTag = constexpr NoPtrTag
 255 const SlowPathPtrTag = constexpr SlowPathPtrTag
 256 
 257 # Some register conventions.
<span class="line-added"> 258 # - We use a pair of registers to represent the PC: one register for the</span>
<span class="line-added"> 259 #   base of the bytecodes, and one register for the index.</span>
<span class="line-added"> 260 # - The PC base (or PB for short) must be stored in a callee-save register.</span>
<span class="line-added"> 261 # - C calls are still given the Instruction* rather than the PC index.</span>
<span class="line-added"> 262 #   This requires an add before the call, and a sub after.</span>
 263 if JSVALUE64





 264     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 265     if ARM64 or ARM64E
 266         const metadataTable = csr6
 267         const PB = csr7
<span class="line-modified"> 268         const numberTag = csr8</span>
<span class="line-modified"> 269         const notCellMask = csr9</span>
 270     elsif X86_64
 271         const metadataTable = csr1
 272         const PB = csr2
<span class="line-modified"> 273         const numberTag = csr3</span>
<span class="line-modified"> 274         const notCellMask = csr4</span>
 275     elsif X86_64_WIN
 276         const metadataTable = csr3
 277         const PB = csr4
<span class="line-modified"> 278         const numberTag = csr5</span>
<span class="line-modified"> 279         const notCellMask = csr6</span>
 280     elsif C_LOOP or C_LOOP_WIN
 281         const PB = csr0
<span class="line-modified"> 282         const numberTag = csr1</span>
<span class="line-modified"> 283         const notCellMask = csr2</span>
 284         const metadataTable = csr3
 285     end
 286 
 287 else
 288     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 289     if C_LOOP or C_LOOP_WIN
<span class="line-added"> 290         const PB = csr0</span>
 291         const metadataTable = csr3
 292     elsif ARMv7
 293         const metadataTable = csr0
<span class="line-added"> 294         const PB = csr1</span>
 295     elsif MIPS
 296         const metadataTable = csr0
<span class="line-added"> 297         const PB = csr1</span>
 298     else
 299         error
 300     end
 301 end
 302 
<span class="line-added"> 303 if GIGACAGE_ENABLED</span>
<span class="line-added"> 304     const GigacagePrimitiveBasePtrOffset = constexpr Gigacage::offsetOfPrimitiveGigacageBasePtr</span>
<span class="line-added"> 305     const GigacageJSValueBasePtrOffset = constexpr Gigacage::offsetOfJSValueGigacageBasePtr</span>
<span class="line-added"> 306 end</span>
<span class="line-added"> 307 </span>
<span class="line-added"> 308 # Opcode offsets</span>
<span class="line-added"> 309 const OpcodeIDNarrowSize = 1 # OpcodeID</span>
<span class="line-added"> 310 const OpcodeIDWide16Size = 2 # Wide16 Prefix + OpcodeID</span>
<span class="line-added"> 311 const OpcodeIDWide32Size = 2 # Wide32 Prefix + OpcodeID</span>
<span class="line-added"> 312 </span>
<span class="line-added"> 313 </span>
<span class="line-added"> 314 macro nextInstruction()</span>
<span class="line-added"> 315     loadb [PB, PC, 1], t0</span>
<span class="line-added"> 316     leap _g_opcodeMap, t1</span>
<span class="line-added"> 317     jmp [t1, t0, PtrSize], BytecodePtrTag</span>
<span class="line-added"> 318 end</span>
<span class="line-added"> 319 </span>
<span class="line-added"> 320 macro nextInstructionWide16()</span>
<span class="line-added"> 321     loadb OpcodeIDNarrowSize[PB, PC, 1], t0</span>
<span class="line-added"> 322     leap _g_opcodeMapWide16, t1</span>
<span class="line-added"> 323     jmp [t1, t0, PtrSize], BytecodePtrTag</span>
<span class="line-added"> 324 end</span>
<span class="line-added"> 325 </span>
<span class="line-added"> 326 macro nextInstructionWide32()</span>
<span class="line-added"> 327     loadb OpcodeIDNarrowSize[PB, PC, 1], t0</span>
<span class="line-added"> 328     leap _g_opcodeMapWide32, t1</span>
<span class="line-added"> 329     jmp [t1, t0, PtrSize], BytecodePtrTag</span>
<span class="line-added"> 330 end</span>
<span class="line-added"> 331 </span>
 332 macro dispatch(advanceReg)
 333     addp advanceReg, PC
 334     nextInstruction()
 335 end
 336 
 337 macro dispatchIndirect(offsetReg)
 338     dispatch(offsetReg)
 339 end
 340 
<span class="line-modified"> 341 macro genericDispatchOp(dispatch, size, opcodeName)</span>
 342     macro dispatchNarrow()
<span class="line-modified"> 343         dispatch((constexpr %opcodeName%_length - 1) * 1 + OpcodeIDNarrowSize)</span>
 344     end
 345 
 346     macro dispatchWide16()
<span class="line-modified"> 347         dispatch((constexpr %opcodeName%_length - 1) * 2 + OpcodeIDWide16Size)</span>
 348     end
 349 
 350     macro dispatchWide32()
<span class="line-modified"> 351         dispatch((constexpr %opcodeName%_length - 1) * 4 + OpcodeIDWide32Size)</span>
 352     end
 353 
 354     size(dispatchNarrow, dispatchWide16, dispatchWide32, macro (dispatch) dispatch() end)
 355 end
 356 
<span class="line-added"> 357 macro dispatchOp(size, opcodeName)</span>
<span class="line-added"> 358     genericDispatchOp(dispatch, size, opcodeName)</span>
<span class="line-added"> 359 end</span>
<span class="line-added"> 360 </span>
<span class="line-added"> 361 </span>
 362 macro getu(size, opcodeStruct, fieldName, dst)
 363     size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)
 364         getu(opcodeStruct, fieldName, dst)
 365     end)
 366 end
 367 
 368 macro get(size, opcodeStruct, fieldName, dst)
 369     size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)
 370         get(opcodeStruct, fieldName, dst)
 371     end)
 372 end
 373 
 374 macro narrow(narrowFn, wide16Fn, wide32Fn, k)
 375     k(narrowFn)
 376 end
 377 
 378 macro wide16(narrowFn, wide16Fn, wide32Fn, k)
 379     k(wide16Fn)
 380 end
 381 
 382 macro wide32(narrowFn, wide16Fn, wide32Fn, k)
 383     k(wide32Fn)
 384 end
 385 
 386 macro metadata(size, opcode, dst, scratch)
 387     loadh (constexpr %opcode%::opcodeID * 2 + MetadataOffsetTable16Offset)[metadataTable], dst # offset = metadataTable&lt;uint16_t*&gt;[opcodeID]
 388     btinz dst, .setUpOffset
 389     loadi (constexpr %opcode%::opcodeID * 4 + MetadataOffsetTable32Offset)[metadataTable], dst # offset = metadataTable&lt;uint32_t*&gt;[opcodeID]
 390 .setUpOffset:
 391     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 392     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 393     addi scratch, dst # offset += scratch
 394     addp metadataTable, dst # return &amp;metadataTable[offset]
 395 end
 396 
<span class="line-modified"> 397 macro jumpImpl(dispatchIndirect, targetOffsetReg)</span>
 398     btiz targetOffsetReg, .outOfLineJumpTarget
 399     dispatchIndirect(targetOffsetReg)
 400 .outOfLineJumpTarget:
 401     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 402     nextInstruction()
 403 end
 404 
 405 macro commonOp(label, prologue, fn)
 406 _%label%:
 407     prologue()
 408     fn(narrow)
<span class="line-added"> 409     if ASSERT_ENABLED</span>
<span class="line-added"> 410         break</span>
<span class="line-added"> 411         break</span>
<span class="line-added"> 412     end</span>
 413 
 414 # FIXME: We cannot enable wide16 bytecode in Windows CLoop. With MSVC, as CLoop::execute gets larger code
 415 # size, CLoop::execute gets higher stack height requirement. This makes CLoop::execute takes 160KB stack
 416 # per call, causes stack overflow error easily. For now, we disable wide16 optimization for Windows CLoop.
 417 # https://bugs.webkit.org/show_bug.cgi?id=198283
 418 if not C_LOOP_WIN
 419 _%label%_wide16:
 420     prologue()
 421     fn(wide16)
<span class="line-added"> 422     if ASSERT_ENABLED</span>
<span class="line-added"> 423         break</span>
<span class="line-added"> 424         break</span>
<span class="line-added"> 425     end</span>
 426 end
 427 
 428 _%label%_wide32:
 429     prologue()
 430     fn(wide32)
<span class="line-added"> 431     if ASSERT_ENABLED</span>
<span class="line-added"> 432         break</span>
<span class="line-added"> 433         break</span>
<span class="line-added"> 434     end</span>
 435 end
 436 
 437 macro op(l, fn)
 438     commonOp(l, macro () end, macro (size)
 439         size(fn, macro() end, macro() end, macro(gen) gen() end)
 440     end)
 441 end
 442 
 443 macro llintOp(opcodeName, opcodeStruct, fn)
 444     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 445         macro getImpl(fieldName, dst)
 446             get(size, opcodeStruct, fieldName, dst)
 447         end
 448 
 449         macro dispatchImpl()
 450             dispatchOp(size, opcodeName)
 451         end
 452 
 453         fn(size, getImpl, dispatchImpl)
 454     end)
</pre>
<hr />
<pre>
 458     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 459         makeReturn(get, dispatch, macro (return)
 460             fn(size, get, dispatch, return)
 461         end)
 462     end)
 463 end
 464 
 465 macro llintOpWithMetadata(opcodeName, opcodeStruct, fn)
 466     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
 467         macro meta(dst, scratch)
 468             metadata(size, opcodeStruct, dst, scratch)
 469         end
 470         fn(size, get, dispatch, meta, return)
 471     end)
 472 end
 473 
 474 macro llintOpWithJump(opcodeName, opcodeStruct, impl)
 475     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 476         macro jump(fieldName)
 477             get(fieldName, t0)
<span class="line-modified"> 478             jumpImpl(dispatchIndirect, t0)</span>
 479         end
 480 
 481         impl(size, get, jump, dispatch)
 482     end)
 483 end
 484 
 485 macro llintOpWithProfile(opcodeName, opcodeStruct, fn)
 486     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 487         makeReturnProfiled(opcodeStruct, get, metadata, dispatch, macro (returnProfiled)
 488             fn(size, get, dispatch, returnProfiled)
 489         end)
 490     end)
 491 end
 492 
 493 
 494 if X86_64_WIN
 495     const extraTempReg = t0
 496 else
 497     const extraTempReg = t5
 498 end
</pre>
<hr />
<pre>
 536 
 537 const FirstTypedArrayType = constexpr FirstTypedArrayType
 538 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 539 
 540 # Type flags constants.
 541 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 542 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 543 
 544 # Bytecode operand constants.
 545 const FirstConstantRegisterIndexNarrow = constexpr FirstConstantRegisterIndex8
 546 const FirstConstantRegisterIndexWide16 = constexpr FirstConstantRegisterIndex16
 547 const FirstConstantRegisterIndexWide32 = constexpr FirstConstantRegisterIndex
 548 
 549 # Code type constants.
 550 const GlobalCode = constexpr GlobalCode
 551 const EvalCode = constexpr EvalCode
 552 const FunctionCode = constexpr FunctionCode
 553 const ModuleCode = constexpr ModuleCode
 554 
 555 # The interpreter steals the tag word of the argument count.
<span class="line-modified"> 556 const LLIntReturnPC = ArgumentCountIncludingThis + TagOffset</span>
 557 
 558 # String flags.
 559 const isRopeInPointer = constexpr JSString::isRopeInPointer
 560 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 561 
 562 # Copied from PropertyOffset.h
 563 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 564 
 565 # ResolveType
 566 const GlobalProperty = constexpr GlobalProperty
 567 const GlobalVar = constexpr GlobalVar
 568 const GlobalLexicalVar = constexpr GlobalLexicalVar
 569 const ClosureVar = constexpr ClosureVar
 570 const LocalClosureVar = constexpr LocalClosureVar
 571 const ModuleVar = constexpr ModuleVar
 572 const GlobalPropertyWithVarInjectionChecks = constexpr GlobalPropertyWithVarInjectionChecks
 573 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 574 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 575 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 576 
 577 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 578 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 579 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 580 const NotInitialization = constexpr InitializationMode::NotInitialization
 581 
 582 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 583 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 584 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter
<span class="line-added"> 585 const PreciseAllocationHeaderSize = constexpr (PreciseAllocation::headerSize())</span>
<span class="line-added"> 586 const PreciseAllocationVMOffset = (PreciseAllocation::m_weakSet + WeakSet::m_vm - PreciseAllocationHeaderSize)</span>
 587 
 588 const BlackThreshold = constexpr blackThreshold
 589 
 590 const VectorBufferOffset = Vector::m_buffer
 591 const VectorSizeOffset = Vector::m_size
 592 
 593 # Some common utilities.
 594 macro crash()
 595     if C_LOOP or C_LOOP_WIN
 596         cloopCrash
 597     else
 598         call _llint_crash
 599     end
 600 end
 601 
 602 macro assert(assertion)
 603     if ASSERT_ENABLED
 604         assertion(.ok)
 605         crash()
 606     .ok:
 607     end
 608 end
 609 
<span class="line-added"> 610 macro assert_with(assertion, crash)</span>
<span class="line-added"> 611     if ASSERT_ENABLED</span>
<span class="line-added"> 612         assertion(.ok)</span>
<span class="line-added"> 613         crash()</span>
<span class="line-added"> 614     .ok:</span>
<span class="line-added"> 615     end</span>
<span class="line-added"> 616 end</span>
<span class="line-added"> 617 </span>
 618 # The probe macro can be used to insert some debugging code without perturbing scalar
 619 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 620 # C probe callback function should not trash floating point registers.
 621 #
 622 # The macro you pass to probe() can pass whatever registers you like to your probe
 623 # callback function. However, you need to be mindful of which of the registers are
 624 # also used as argument registers, and ensure that you don&#39;t trash the register value
 625 # before storing it in the probe callback argument register that you desire.
 626 #
 627 # Here&#39;s an example of how it&#39;s used:
 628 #
 629 #     probe(
 630 #         macro()
<span class="line-modified"> 631 #             move cfr, a0 # pass the CallFrame* as arg0.</span>
 632 #             move t0, a1 # pass the value of register t0 as arg1.
 633 #             call _cProbeCallbackFunction # to do whatever you want.
 634 #         end
 635 #     )
 636 #
 637 if X86_64 or ARM64 or ARM64E or ARMv7
 638     macro probe(action)
 639         # save all the registers that the LLInt may use.
 640         if ARM64 or ARM64E or ARMv7
 641             push cfr, lr
 642         end
 643         push a0, a1
 644         push a2, a3
 645         push t0, t1
 646         push t2, t3
 647         push t4, t5
 648         if ARM64 or ARM64E
 649             push csr0, csr1
 650             push csr2, csr3
 651             push csr4, csr5
 652             push csr6, csr7
 653             push csr8, csr9
 654         elsif ARMv7
<span class="line-modified"> 655             push csr0, csr1</span>
 656         end
 657 
 658         action()
 659 
 660         # restore all the registers we saved previously.
 661         if ARM64 or ARM64E
 662             pop csr9, csr8
 663             pop csr7, csr6
 664             pop csr5, csr4
 665             pop csr3, csr2
 666             pop csr1, csr0
 667         elsif ARMv7
<span class="line-modified"> 668             pop csr1, csr0</span>
 669         end
 670         pop t5, t4
 671         pop t3, t2
 672         pop t1, t0
 673         pop a3, a2
 674         pop a1, a0
 675         if ARM64 or ARM64E or ARMv7
 676             pop lr, cfr
 677         end
 678     end
 679 else
 680     macro probe(action)
 681     end
 682 end
 683 
 684 macro checkStackPointerAlignment(tempReg, location)
 685     if ASSERT_ENABLED
 686         if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
 687             # ARM64 and ARM64E will check for us!
 688             # C_LOOP or C_LOOP_WIN does not need the alignment, and can use a little perf
</pre>
<hr />
<pre>
 691             if ARMv7
 692                 # ARM can&#39;t do logical ops with the sp as a source
 693                 move sp, tempReg
 694                 andp StackAlignmentMask, tempReg
 695             else
 696                 andp sp, StackAlignmentMask, tempReg
 697             end
 698             btpz tempReg, .stackPointerOkay
 699             move location, tempReg
 700             break
 701         .stackPointerOkay:
 702         end
 703     end
 704 end
 705 
 706 if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 707     const CalleeSaveRegisterCount = 0
 708 elsif ARMv7
 709     const CalleeSaveRegisterCount = 7
 710 elsif MIPS
<span class="line-modified"> 711     const CalleeSaveRegisterCount = 3</span>
 712 elsif X86 or X86_WIN
 713     const CalleeSaveRegisterCount = 3
 714 end
 715 
 716 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 717 
 718 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 719 # callee save registers rounded up to keep the stack aligned
 720 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 721 
 722 macro pushCalleeSaves()
 723     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 724     elsif ARMv7
 725         emit &quot;push {r4-r6, r8-r11}&quot;
 726     elsif MIPS
<span class="line-modified"> 727         emit &quot;addiu $sp, $sp, -12&quot;</span>
 728         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
<span class="line-modified"> 729         emit &quot;sw $s1, 4($sp)&quot; # csr1/PB</span>
<span class="line-added"> 730         emit &quot;sw $s4, 8($sp)&quot;</span>
 731         # save $gp to $s4 so that we can restore it after a function call
 732         emit &quot;move $s4, $gp&quot;
 733     elsif X86
 734         emit &quot;push %esi&quot;
 735         emit &quot;push %edi&quot;
 736         emit &quot;push %ebx&quot;
 737     elsif X86_WIN
 738         emit &quot;push esi&quot;
 739         emit &quot;push edi&quot;
 740         emit &quot;push ebx&quot;
 741     end
 742 end
 743 
 744 macro popCalleeSaves()
 745     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 746     elsif ARMv7
 747         emit &quot;pop {r4-r6, r8-r11}&quot;
 748     elsif MIPS
 749         emit &quot;lw $s0, 0($sp)&quot;
<span class="line-modified"> 750         emit &quot;lw $s1, 4($sp)&quot;</span>
<span class="line-modified"> 751         emit &quot;lw $s4, 8($sp)&quot;</span>
<span class="line-added"> 752         emit &quot;addiu $sp, $sp, 12&quot;</span>
 753     elsif X86
 754         emit &quot;pop %ebx&quot;
 755         emit &quot;pop %edi&quot;
 756         emit &quot;pop %esi&quot;
 757     elsif X86_WIN
 758         emit &quot;pop ebx&quot;
 759         emit &quot;pop edi&quot;
 760         emit &quot;pop esi&quot;
 761     end
 762 end
 763 
 764 macro preserveCallerPCAndCFR()
 765     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 766         push lr
 767         push cfr
 768     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 769         push cfr
 770     elsif ARM64 or ARM64E
 771         push cfr, lr
 772     else
</pre>
<hr />
<pre>
 774     end
 775     move sp, cfr
 776 end
 777 
 778 macro restoreCallerPCAndCFR()
 779     move cfr, sp
 780     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 781         pop cfr
 782         pop lr
 783     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 784         pop cfr
 785     elsif ARM64 or ARM64E
 786         pop lr, cfr
 787     end
 788 end
 789 
 790 macro preserveCalleeSavesUsedByLLInt()
 791     subp CalleeSaveSpaceStackAligned, sp
 792     if C_LOOP or C_LOOP_WIN
 793         storep metadataTable, -PtrSize[cfr]
<span class="line-modified"> 794 </span>
<span class="line-added"> 795     # Next ARMv7 and MIPS differ in how we store metadataTable and PB,</span>
<span class="line-added"> 796     # because this codes needs to be in sync with how registers are</span>
<span class="line-added"> 797     # restored in Baseline JIT (specifically in emitRestoreCalleeSavesFor).</span>
<span class="line-added"> 798     # emitRestoreCalleeSavesFor restores registers in order instead of by name.</span>
<span class="line-added"> 799     # However, ARMv7 and MIPS differ in the order in which registers are assigned</span>
<span class="line-added"> 800     # to metadataTable and PB, therefore they can also not have the same saving</span>
<span class="line-added"> 801     # order.</span>
<span class="line-added"> 802     elsif ARMv7</span>
 803         storep metadataTable, -4[cfr]
<span class="line-added"> 804         storep PB, -8[cfr]</span>
<span class="line-added"> 805     elsif MIPS</span>
<span class="line-added"> 806         storep PB, -4[cfr]</span>
<span class="line-added"> 807         storep metadataTable, -8[cfr]</span>
 808     elsif ARM64 or ARM64E
 809         emit &quot;stp x27, x28, [x29, #-16]&quot;
 810         emit &quot;stp x25, x26, [x29, #-32]&quot;
 811     elsif X86
 812     elsif X86_WIN
 813     elsif X86_64
 814         storep csr4, -8[cfr]
 815         storep csr3, -16[cfr]
 816         storep csr2, -24[cfr]
 817         storep csr1, -32[cfr]
 818     elsif X86_64_WIN
 819         storep csr6, -8[cfr]
 820         storep csr5, -16[cfr]
 821         storep csr4, -24[cfr]
 822         storep csr3, -32[cfr]
 823     end
 824 end
 825 
 826 macro restoreCalleeSavesUsedByLLInt()
 827     if C_LOOP or C_LOOP_WIN
 828         loadp -PtrSize[cfr], metadataTable
<span class="line-modified"> 829     # To understand why ARMv7 and MIPS differ in restore order,</span>
<span class="line-added"> 830     # see comment in preserveCalleeSavesUsedByLLInt</span>
<span class="line-added"> 831     elsif ARMv7</span>
 832         loadp -4[cfr], metadataTable
<span class="line-added"> 833         loadp -8[cfr], PB</span>
<span class="line-added"> 834     elsif MIPS</span>
<span class="line-added"> 835         loadp -4[cfr], PB</span>
<span class="line-added"> 836         loadp -8[cfr], metadataTable</span>
 837     elsif ARM64 or ARM64E
 838         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 839         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 840     elsif X86
 841     elsif X86_WIN
 842     elsif X86_64
 843         loadp -32[cfr], csr1
 844         loadp -24[cfr], csr2
 845         loadp -16[cfr], csr3
 846         loadp -8[cfr], csr4
 847     elsif X86_64_WIN
 848         loadp -32[cfr], csr3
 849         loadp -24[cfr], csr4
 850         loadp -16[cfr], csr5
 851         loadp -8[cfr], csr6
 852     end
 853 end
 854 
<span class="line-modified"> 855 macro copyCalleeSavesToEntryFrameCalleeSavesBuffer(entryFrame)</span>
 856     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
<span class="line-modified"> 857         vmEntryRecord(entryFrame, entryFrame)</span>
<span class="line-modified"> 858         leap VMEntryRecord::calleeSaveRegistersBuffer[entryFrame], entryFrame</span>

 859         if ARM64 or ARM64E
<span class="line-modified"> 860             storeq csr0, [entryFrame]</span>
<span class="line-modified"> 861             storeq csr1, 8[entryFrame]</span>
<span class="line-modified"> 862             storeq csr2, 16[entryFrame]</span>
<span class="line-modified"> 863             storeq csr3, 24[entryFrame]</span>
<span class="line-modified"> 864             storeq csr4, 32[entryFrame]</span>
<span class="line-modified"> 865             storeq csr5, 40[entryFrame]</span>
<span class="line-modified"> 866             storeq csr6, 48[entryFrame]</span>
<span class="line-modified"> 867             storeq csr7, 56[entryFrame]</span>
<span class="line-modified"> 868             storeq csr8, 64[entryFrame]</span>
<span class="line-modified"> 869             storeq csr9, 72[entryFrame]</span>
<span class="line-modified"> 870             stored csfr0, 80[entryFrame]</span>
<span class="line-modified"> 871             stored csfr1, 88[entryFrame]</span>
<span class="line-modified"> 872             stored csfr2, 96[entryFrame]</span>
<span class="line-modified"> 873             stored csfr3, 104[entryFrame]</span>
<span class="line-modified"> 874             stored csfr4, 112[entryFrame]</span>
<span class="line-modified"> 875             stored csfr5, 120[entryFrame]</span>
<span class="line-modified"> 876             stored csfr6, 128[entryFrame]</span>
<span class="line-modified"> 877             stored csfr7, 136[entryFrame]</span>
 878         elsif X86_64
<span class="line-modified"> 879             storeq csr0, [entryFrame]</span>
<span class="line-modified"> 880             storeq csr1, 8[entryFrame]</span>
<span class="line-modified"> 881             storeq csr2, 16[entryFrame]</span>
<span class="line-modified"> 882             storeq csr3, 24[entryFrame]</span>
<span class="line-modified"> 883             storeq csr4, 32[entryFrame]</span>
 884         elsif X86_64_WIN
<span class="line-modified"> 885             storeq csr0, [entryFrame]</span>
<span class="line-modified"> 886             storeq csr1, 8[entryFrame]</span>
<span class="line-modified"> 887             storeq csr2, 16[entryFrame]</span>
<span class="line-modified"> 888             storeq csr3, 24[entryFrame]</span>
<span class="line-modified"> 889             storeq csr4, 32[entryFrame]</span>
<span class="line-modified"> 890             storeq csr5, 40[entryFrame]</span>
<span class="line-modified"> 891             storeq csr6, 48[entryFrame]</span>
 892         elsif ARMv7 or MIPS
<span class="line-modified"> 893             storep csr0, [entryFrame]</span>
<span class="line-added"> 894             storep csr1, 4[entryFrame]</span>
 895         end
 896     end
 897 end
 898 
<span class="line-added"> 899 macro copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(vm, temp)</span>
<span class="line-added"> 900     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS</span>
<span class="line-added"> 901         loadp VM::topEntryFrame[vm], temp</span>
<span class="line-added"> 902         copyCalleeSavesToEntryFrameCalleeSavesBuffer(temp)</span>
<span class="line-added"> 903     end</span>
<span class="line-added"> 904 end</span>
<span class="line-added"> 905 </span>
 906 macro restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(vm, temp)
 907     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 908         loadp VM::topEntryFrame[vm], temp
 909         vmEntryRecord(temp, temp)
 910         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 911         if ARM64 or ARM64E
 912             loadq [temp], csr0
 913             loadq 8[temp], csr1
 914             loadq 16[temp], csr2
 915             loadq 24[temp], csr3
 916             loadq 32[temp], csr4
 917             loadq 40[temp], csr5
 918             loadq 48[temp], csr6
 919             loadq 56[temp], csr7
 920             loadq 64[temp], csr8
 921             loadq 72[temp], csr9
 922             loadd 80[temp], csfr0
 923             loadd 88[temp], csfr1
 924             loadd 96[temp], csfr2
 925             loadd 104[temp], csfr3
 926             loadd 112[temp], csfr4
 927             loadd 120[temp], csfr5
 928             loadd 128[temp], csfr6
 929             loadd 136[temp], csfr7
 930         elsif X86_64
 931             loadq [temp], csr0
 932             loadq 8[temp], csr1
 933             loadq 16[temp], csr2
 934             loadq 24[temp], csr3
 935             loadq 32[temp], csr4
 936         elsif X86_64_WIN
 937             loadq [temp], csr0
 938             loadq 8[temp], csr1
 939             loadq 16[temp], csr2
 940             loadq 24[temp], csr3
 941             loadq 32[temp], csr4
 942             loadq 40[temp], csr5
 943             loadq 48[temp], csr6
 944         elsif ARMv7 or MIPS
 945             loadp [temp], csr0
<span class="line-added"> 946             loadp 4[temp], csr1</span>
 947         end
 948     end
 949 end
 950 
 951 macro preserveReturnAddressAfterCall(destinationRegister)
 952     if C_LOOP or C_LOOP_WIN or ARMv7 or ARM64 or ARM64E or MIPS
 953         # In C_LOOP or C_LOOP_WIN case, we&#39;re only preserving the bytecode vPC.
 954         move lr, destinationRegister
 955     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 956         pop destinationRegister
 957     else
 958         error
 959     end
 960 end
 961 
 962 macro functionPrologue()
 963     tagReturnAddress sp
 964     if X86 or X86_WIN or X86_64 or X86_64_WIN
 965         push cfr
 966     elsif ARM64 or ARM64E
</pre>
<hr />
<pre>
 993     addp maxFrameExtentForSlowPathCall, size
 994 end
 995 
 996 macro restoreStackPointerAfterCall()
 997     loadp CodeBlock[cfr], t2
 998     getFrameRegisterSizeForCodeBlock(t2, t2)
 999     if ARMv7
1000         subp cfr, t2, t2
1001         move t2, sp
1002     else
1003         subp cfr, t2, sp
1004     end
1005 end
1006 
1007 macro traceExecution()
1008     if TRACING
1009         callSlowPath(_llint_trace)
1010     end
1011 end
1012 
<span class="line-modified">1013 macro defineOSRExitReturnLabel(opcodeName, size)</span>
<span class="line-added">1014     macro defineNarrow()</span>
<span class="line-added">1015         if not C_LOOP_WIN</span>
<span class="line-added">1016             _%opcodeName%_return_location:</span>
<span class="line-added">1017         end</span>
<span class="line-added">1018     end</span>
<span class="line-added">1019 </span>
<span class="line-added">1020     macro defineWide16()</span>
<span class="line-added">1021         if not C_LOOP_WIN</span>
<span class="line-added">1022             _%opcodeName%_return_location_wide16:</span>
<span class="line-added">1023         end</span>
<span class="line-added">1024     end</span>
<span class="line-added">1025 </span>
<span class="line-added">1026     macro defineWide32()</span>
<span class="line-added">1027         if not C_LOOP_WIN</span>
<span class="line-added">1028             _%opcodeName%_return_location_wide32:</span>
<span class="line-added">1029         end</span>
<span class="line-added">1030     end</span>
<span class="line-added">1031 </span>
<span class="line-added">1032     size(defineNarrow, defineWide16, defineWide32, macro (f) f() end)</span>
<span class="line-added">1033 end</span>
<span class="line-added">1034 </span>
<span class="line-added">1035 macro callTargetFunction(opcodeName, size, opcodeStruct, dispatch, callee, callPtrTag)</span>
1036     if C_LOOP or C_LOOP_WIN
1037         cloopCallJSFunction callee
1038     else
1039         call callee, callPtrTag
1040     end
<span class="line-added">1041 </span>
<span class="line-added">1042     if ARMv7 or MIPS</span>
<span class="line-added">1043         # It is required in ARMv7 and MIPs because global label definitions</span>
<span class="line-added">1044         # for those architectures generates a set of instructions</span>
<span class="line-added">1045         # that can clobber LLInt execution, resulting in unexpected</span>
<span class="line-added">1046         # crashes.</span>
<span class="line-added">1047         restoreStackPointerAfterCall()</span>
<span class="line-added">1048         dispatchAfterCall(size, opcodeStruct, dispatch)</span>
<span class="line-added">1049     end</span>
<span class="line-added">1050     defineOSRExitReturnLabel(opcodeName, size)</span>
1051     restoreStackPointerAfterCall()
1052     dispatchAfterCall(size, opcodeStruct, dispatch)
1053 end
1054 
1055 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
1056     addp CallerFrameAndPCSize, sp
1057 end
1058 
1059 # sp points to the new frame
1060 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
1061     restoreCalleeSavesUsedByLLInt()
1062 
<span class="line-modified">1063     loadi PayloadOffset + ArgumentCountIncludingThis[cfr], temp2</span>
1064     loadp CodeBlock[cfr], temp1
1065     loadi CodeBlock::m_numParameters[temp1], temp1
1066     bilteq temp1, temp2, .noArityFixup
1067     move temp1, temp2
1068 
1069 .noArityFixup:
1070     # We assume &lt; 2^28 arguments
1071     muli SlotSize, temp2
1072     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
1073     andi ~StackAlignmentMask, temp2
1074 
1075     move cfr, temp1
1076     addp temp2, temp1
1077 
<span class="line-modified">1078     loadi PayloadOffset + ArgumentCountIncludingThis[sp], temp2</span>
1079     # We assume &lt; 2^28 arguments
1080     muli SlotSize, temp2
1081     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
1082     andi ~StackAlignmentMask, temp2
1083 
1084     if ARMv7 or ARM64 or ARM64E or C_LOOP or C_LOOP_WIN or MIPS
1085         addp CallerFrameAndPCSize, sp
1086         subi CallerFrameAndPCSize, temp2
1087         loadp CallerFrameAndPC::returnPC[cfr], lr
1088     else
1089         addp PtrSize, sp
1090         subi PtrSize, temp2
1091         loadp PtrSize[cfr], temp3
1092         storep temp3, [sp]
1093     end
1094 
1095     if ARM64E
1096         addp 16, cfr, temp3
1097         untagReturnAddress temp3
1098     end
</pre>
<hr />
<pre>
1100     subp temp2, temp1
1101     loadp [cfr], cfr
1102 
1103 .copyLoop:
1104     if ARM64 and not ADDRESS64
1105         subi MachineRegisterSize, temp2
1106         loadq [sp, temp2, 1], temp3
1107         storeq temp3, [temp1, temp2, 1]
1108         btinz temp2, .copyLoop
1109     else
1110         subi PtrSize, temp2
1111         loadp [sp, temp2, 1], temp3
1112         storep temp3, [temp1, temp2, 1]
1113         btinz temp2, .copyLoop
1114     end
1115 
1116     move temp1, sp
1117     jmp callee, callPtrTag
1118 end
1119 
<span class="line-modified">1120 macro slowPathForCall(opcodeName, size, opcodeStruct, dispatch, slowPath, prepareCall)</span>
1121     callCallSlowPath(
1122         slowPath,
1123         # Those are r0 and r1
1124         macro (callee, calleeFramePtr)
1125             btpz calleeFramePtr, .dontUpdateSP
1126             move calleeFramePtr, sp
1127             prepareCall(callee, t2, t3, t4, SlowPathPtrTag)
1128         .dontUpdateSP:
<span class="line-modified">1129             callTargetFunction(%opcodeName%_slow, size, opcodeStruct, dispatch, callee, SlowPathPtrTag)</span>
1130         end)
1131 end
1132 
<span class="line-added">1133 macro getterSetterOSRExitReturnPoint(opName, size)</span>
<span class="line-added">1134     crash() # We don&#39;t reach this in straight line code. We only reach it via returning to the code below when reconstructing stack frames during OSR exit.</span>
<span class="line-added">1135 </span>
<span class="line-added">1136     defineOSRExitReturnLabel(opName, size)</span>
<span class="line-added">1137 </span>
<span class="line-added">1138     restoreStackPointerAfterCall()</span>
<span class="line-added">1139     loadi LLIntReturnPC[cfr], PC</span>
<span class="line-added">1140 end</span>
<span class="line-added">1141 </span>
1142 macro arrayProfile(offset, cellAndIndexingType, metadata, scratch)
1143     const cell = cellAndIndexingType
1144     const indexingType = cellAndIndexingType 
1145     loadi JSCell::m_structureID[cell], scratch
1146     storei scratch, offset + ArrayProfile::m_lastSeenStructureID[metadata]
1147     loadb JSCell::m_indexingTypeAndMisc[cell], indexingType
1148 end
1149 
1150 macro skipIfIsRememberedOrInEden(cell, slowPath)
1151     memfence
1152     bba JSCell::m_cellState[cell], BlackThreshold, .done
1153     slowPath()
1154 .done:
1155 end
1156 
1157 macro notifyWrite(set, slow)
1158     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1159 end
1160 
1161 macro checkSwitchToJIT(increment, action)
</pre>
<hr />
<pre>
1168 macro checkSwitchToJITForEpilogue()
1169     checkSwitchToJIT(
1170         10,
1171         macro ()
1172             callSlowPath(_llint_replace)
1173         end)
1174 end
1175 
1176 macro assertNotConstant(size, index)
1177     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
1178         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1179     end)
1180 end
1181 
1182 macro functionForCallCodeBlockGetter(targetRegister)
1183     if JSVALUE64
1184         loadp Callee[cfr], targetRegister
1185     else
1186         loadp Callee + PayloadOffset[cfr], targetRegister
1187     end
<span class="line-modified">1188     loadp JSFunction::m_executableOrRareData[targetRegister], targetRegister</span>
<span class="line-added">1189     btpz targetRegister, (constexpr JSFunction::rareDataTag), .isExecutable</span>
<span class="line-added">1190     loadp (FunctionRareData::m_executable - (constexpr JSFunction::rareDataTag))[targetRegister], targetRegister</span>
<span class="line-added">1191 .isExecutable:</span>
1192     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1193     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1194 end
1195 
1196 macro functionForConstructCodeBlockGetter(targetRegister)
1197     if JSVALUE64
1198         loadp Callee[cfr], targetRegister
1199     else
1200         loadp Callee + PayloadOffset[cfr], targetRegister
1201     end
<span class="line-modified">1202     loadp JSFunction::m_executableOrRareData[targetRegister], targetRegister</span>
<span class="line-added">1203     btpz targetRegister, (constexpr JSFunction::rareDataTag), .isExecutable</span>
<span class="line-added">1204     loadp (FunctionRareData::m_executable - (constexpr JSFunction::rareDataTag))[targetRegister], targetRegister</span>
<span class="line-added">1205 .isExecutable:</span>
1206     loadp FunctionExecutable::m_codeBlockForConstruct[targetRegister], targetRegister
1207     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1208 end
1209 
1210 macro notFunctionCodeBlockGetter(targetRegister)
1211     loadp CodeBlock[cfr], targetRegister
1212 end
1213 
1214 macro functionCodeBlockSetter(sourceRegister)
1215     storep sourceRegister, CodeBlock[cfr]
1216 end
1217 
1218 macro notFunctionCodeBlockSetter(sourceRegister)
1219     # Nothing to do!
1220 end
1221 
<span class="line-added">1222 macro convertCalleeToVM(callee)</span>
<span class="line-added">1223     btpnz callee, (constexpr PreciseAllocation::halfAlignment), .preciseAllocation</span>
<span class="line-added">1224     andp MarkedBlockMask, callee</span>
<span class="line-added">1225     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[callee], callee</span>
<span class="line-added">1226     jmp .done</span>
<span class="line-added">1227 .preciseAllocation:</span>
<span class="line-added">1228     loadp PreciseAllocationVMOffset[callee], callee</span>
<span class="line-added">1229 .done:</span>
<span class="line-added">1230 end</span>
<span class="line-added">1231 </span>
1232 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1233 # in t1. May also trigger prologue entry OSR.
1234 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1235     # Set up the call frame and check if we should OSR.
1236     tagReturnAddress sp
1237     preserveCallerPCAndCFR()
1238 
1239     if TRACING
1240         subp maxFrameExtentForSlowPathCall, sp
1241         callSlowPath(traceSlowPath)
1242         addp maxFrameExtentForSlowPathCall, sp
1243     end
1244     codeBlockGetter(t1)
<span class="line-added">1245     codeBlockSetter(t1)</span>
1246     if not (C_LOOP or C_LOOP_WIN)
1247         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1248         if JSVALUE64
1249             move cfr, a0
1250             move PC, a1
1251             cCall2(osrSlowPath)
1252         else
1253             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1254             # Temporarily align stack pointer for this call.
1255             subp 8, sp
1256             move cfr, a0
1257             move PC, a1
1258             cCall2(osrSlowPath)
1259             addp 8, sp
1260         end
1261         btpz r0, .recover
1262         move cfr, sp # restore the previous sp
1263         # pop the callerFrame since we will jump to a function that wants to save it
1264         if ARM64 or ARM64E
1265             pop lr, cfr
1266             untagReturnAddress sp
1267         elsif ARMv7 or MIPS
1268             pop cfr
1269             pop lr
1270         else
1271             pop cfr
1272         end
1273         jmp r0, JSEntryPtrTag
1274     .recover:
<span class="line-modified">1275         notFunctionCodeBlockGetter(t1)</span>
1276     .continue:
1277     end
1278 


1279     preserveCalleeSavesUsedByLLInt()
1280 
1281     # Set up the PC.
<span class="line-modified">1282     loadp CodeBlock::m_instructionsRawPointer[t1], PB</span>
<span class="line-modified">1283     move 0, PC</span>




1284 
1285     # Get new sp in t0 and check stack height.
1286     getFrameRegisterSizeForCodeBlock(t1, t0)
1287     subp cfr, t0, t0
1288     bpa t0, cfr, .needStackCheck
1289     loadp CodeBlock::m_vm[t1], t2
1290     if C_LOOP or C_LOOP_WIN
1291         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1292     else
1293         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1294     end
1295 
1296 .needStackCheck:
1297     # Stack height check failed - need to call a slow_path.
1298     # Set up temporary stack pointer for call including callee saves
1299     subp maxFrameExtentForSlowPathCall, sp
1300     callSlowPath(_llint_stack_check)
1301     bpeq r1, 0, .stackHeightOKGetCodeBlock
1302 
1303     # We&#39;re throwing before the frame is fully set up. This frame will be
1304     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1305     # start unwinding. We need to do this before we change the cfr.
1306     restoreCalleeSavesUsedByLLInt()
1307 
1308     move r1, cfr
1309     jmp _llint_throw_from_slow_path_trampoline
1310 
1311 .stackHeightOKGetCodeBlock:
1312     # Stack check slow path returned that the stack was ok.
1313     # Since they were clobbered, need to get CodeBlock and new sp
<span class="line-modified">1314     notFunctionCodeBlockGetter(t1)</span>
1315     getFrameRegisterSizeForCodeBlock(t1, t0)
1316     subp cfr, t0, t0
1317 
1318 .stackHeightOK:
1319     if X86_64 or ARM64
1320         # We need to start zeroing from sp as it has been adjusted after saving callee saves.
1321         move sp, t2
1322         move t0, sp
1323 .zeroStackLoop:
1324         bpeq sp, t2, .zeroStackDone
1325         subp PtrSize, t2
1326         storep 0, [t2]
1327         jmp .zeroStackLoop
1328 .zeroStackDone:
1329     else
1330         move t0, sp
1331     end
1332 
1333     loadp CodeBlock::m_metadata[t1], metadataTable
1334 
1335     if JSVALUE64
<span class="line-modified">1336         move TagNumber, numberTag</span>
<span class="line-modified">1337         addq TagOther, numberTag, notCellMask</span>
1338     end
1339 end
1340 
1341 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
1342 # Must call dispatch(0) after calling this.
1343 macro functionInitialization(profileArgSkip)
1344     # Profile the arguments. Unfortunately, we have no choice but to do this. This
1345     # code is pretty horrendous because of the difference in ordering between
1346     # arguments and value profiles, the desire to have a simple loop-down-to-zero
1347     # loop, and the desire to use only three registers so as to preserve the PC and
1348     # the code block. It is likely that this code should be rewritten in a more
1349     # optimal way for architectures that have more than five registers available
1350     # for arbitrary use in the interpreter.
1351     loadi CodeBlock::m_numParameters[t1], t0
1352     addp -profileArgSkip, t0 # Use addi because that&#39;s what has the peephole
1353     assert(macro (ok) bpgteq t0, 0, ok end)
1354     btpz t0, .argumentProfileDone
1355     loadp CodeBlock::m_argumentValueProfiles + RefCountedArray::m_data[t1], t3
1356     btpz t3, .argumentProfileDone # When we can&#39;t JIT, we don&#39;t allocate any argument value profiles.
1357     mulp sizeof ValueProfile, t0, t2 # Aaaaahhhh! Need strength reduction!
</pre>
<hr />
<pre>
1401 
1402 if C_LOOP or C_LOOP_WIN
1403     _llint_vm_entry_to_native:
1404 else
1405     global _vmEntryToNative
1406     _vmEntryToNative:
1407 end
1408     doVMEntry(makeHostFunctionCall)
1409 
1410 
1411 if not (C_LOOP or C_LOOP_WIN)
1412     # void sanitizeStackForVMImpl(VM* vm)
1413     global _sanitizeStackForVMImpl
1414     _sanitizeStackForVMImpl:
1415         tagReturnAddress sp
1416         # We need three non-aliased caller-save registers. We are guaranteed
1417         # this for a0, a1 and a2 on all architectures.
1418         if X86 or X86_WIN
1419             loadp 4[sp], a0
1420         end
<span class="line-modified">1421         const vmOrStartSP = a0</span>
1422         const address = a1
1423         const zeroValue = a2
1424     
<span class="line-modified">1425         loadp VM::m_lastStackTop[vmOrStartSP], address</span>
<span class="line-added">1426         move sp, zeroValue</span>
<span class="line-added">1427         storep zeroValue, VM::m_lastStackTop[vmOrStartSP]</span>
<span class="line-added">1428         move sp, vmOrStartSP</span>
<span class="line-added">1429 </span>
1430         bpbeq sp, address, .zeroFillDone
<span class="line-modified">1431         move address, sp</span>
<span class="line-added">1432 </span>
1433         move 0, zeroValue
1434     .zeroFillLoop:
1435         storep zeroValue, [address]
1436         addp PtrSize, address
<span class="line-modified">1437         bpa vmOrStartSP, address, .zeroFillLoop</span>
1438 
1439     .zeroFillDone:
<span class="line-modified">1440         move vmOrStartSP, sp</span>

1441         ret
1442     
1443     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1444     global _vmEntryRecord
1445     _vmEntryRecord:
1446         tagReturnAddress sp
1447         if X86 or X86_WIN
1448             loadp 4[sp], a0
1449         end
1450 
1451         vmEntryRecord(a0, r0)
1452         ret
1453 end
1454 
1455 if C_LOOP or C_LOOP_WIN
1456     # Dummy entry point the C Loop uses to initialize.
1457     _llint_entry:
1458         crash()
1459 else
<span class="line-modified">1460     macro initPCRelative(kind, pcBase)</span>
1461         if X86_64 or X86_64_WIN or X86 or X86_WIN
<span class="line-modified">1462             call _%kind%_relativePCBase</span>
<span class="line-modified">1463         _%kind%_relativePCBase:</span>
1464             pop pcBase
1465         elsif ARM64 or ARM64E
1466         elsif ARMv7
<span class="line-modified">1467         _%kind%_relativePCBase:</span>
1468             move pc, pcBase
1469             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1470         elsif MIPS
<span class="line-modified">1471             la _%kind%_relativePCBase, pcBase</span>
1472             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
<span class="line-modified">1473         _%kind%_relativePCBase:</span>
1474         end
<span class="line-modified">1475     end</span>
1476 
<span class="line-modified">1477     # The PC base is in t3, as this is what _llint_entry leaves behind through</span>
<span class="line-modified">1478     # initPCRelative(t3)</span>
<span class="line-modified">1479     macro setEntryAddressCommon(kind, index, label, map)</span>
<span class="line-modified">1480         if X86_64</span>
<span class="line-modified">1481             leap (label - _%kind%_relativePCBase)[t3], t4</span>
<span class="line-added">1482             move index, t5</span>
<span class="line-added">1483             storep t4, [map, t5, 8]</span>
<span class="line-added">1484         elsif X86_64_WIN</span>
<span class="line-added">1485             leap (label - _%kind%_relativePCBase)[t3], t4</span>
<span class="line-added">1486             move index, t0</span>
<span class="line-added">1487             storep t4, [map, t0, 8]</span>
<span class="line-added">1488         elsif X86 or X86_WIN</span>
<span class="line-added">1489             leap (label - _%kind%_relativePCBase)[t3], t4</span>
<span class="line-added">1490             move index, t5</span>
<span class="line-added">1491             storep t4, [map, t5, 4]</span>
<span class="line-added">1492         elsif ARM64 or ARM64E</span>
<span class="line-added">1493             pcrtoaddr label, t3</span>
<span class="line-added">1494             move index, t4</span>
<span class="line-added">1495             storep t3, [map, t4, PtrSize]</span>
<span class="line-added">1496         elsif ARMv7</span>
<span class="line-added">1497             mvlbl (label - _%kind%_relativePCBase), t4</span>
<span class="line-added">1498             addp t4, t3, t4</span>
<span class="line-added">1499             move index, t5</span>
<span class="line-added">1500             storep t4, [map, t5, 4]</span>
<span class="line-added">1501         elsif MIPS</span>
<span class="line-added">1502             la label, t4</span>
<span class="line-added">1503             la _%kind%_relativePCBase, t3</span>
<span class="line-added">1504             subp t3, t4</span>
<span class="line-added">1505             addp t4, t3, t4</span>
<span class="line-added">1506             move index, t5</span>
<span class="line-added">1507             storep t4, [map, t5, 4]</span>
<span class="line-added">1508         end</span>
<span class="line-added">1509     end</span>
1510 



1511 



1512 
<span class="line-modified">1513     macro includeEntriesAtOffset(kind, fn)</span>
<span class="line-modified">1514         macro setEntryAddress(index, label)</span>
<span class="line-modified">1515             setEntryAddressCommon(kind, index, label, a0)</span>
<span class="line-modified">1516         end</span>



























1517 
<span class="line-modified">1518         macro setEntryAddressWide16(index, label)</span>
<span class="line-modified">1519              setEntryAddressCommon(kind, index, label, a1)</span>
<span class="line-modified">1520         end</span>
<span class="line-modified">1521 </span>
<span class="line-modified">1522         macro setEntryAddressWide32(index, label)</span>
<span class="line-modified">1523              setEntryAddressCommon(kind, index, label, a2)</span>
<span class="line-modified">1524         end</span>
<span class="line-modified">1525 </span>
<span class="line-modified">1526         fn()</span>
1527     end
1528 

1529 
<span class="line-modified">1530 macro entry(kind, initialize)</span>
<span class="line-modified">1531     global _%kind%_entry</span>
<span class="line-added">1532     _%kind%_entry:</span>
<span class="line-added">1533         functionPrologue()</span>
<span class="line-added">1534         pushCalleeSaves()</span>
<span class="line-added">1535         if X86 or X86_WIN</span>
<span class="line-added">1536             loadp 20[sp], a0</span>
<span class="line-added">1537             loadp 24[sp], a1</span>
<span class="line-added">1538             loadp 28[sp], a2</span>
<span class="line-added">1539         end</span>
1540 
<span class="line-modified">1541         initPCRelative(kind, t3)</span>
<span class="line-modified">1542 </span>
<span class="line-modified">1543         # Include generated bytecode initialization file.</span>
<span class="line-added">1544         includeEntriesAtOffset(kind, initialize)</span>
<span class="line-added">1545         popCalleeSaves()</span>
<span class="line-added">1546         functionEpilogue()</span>
<span class="line-added">1547         ret</span>
1548 end
1549 
<span class="line-added">1550 # Entry point for the llint to initialize.</span>
<span class="line-added">1551 entry(llint, macro()</span>
<span class="line-added">1552     include InitBytecodes</span>
<span class="line-added">1553 end)</span>
<span class="line-added">1554 </span>
<span class="line-added">1555 end // not (C_LOOP or C_LOOP_WIN)</span>
<span class="line-added">1556 </span>
1557 _llint_op_wide16:
1558     nextInstructionWide16()
1559 
1560 _llint_op_wide32:
1561     nextInstructionWide32()
1562 
1563 macro noWide(label)
<span class="line-modified">1564 _%label%_wide16:</span>
1565     crash()
1566 
<span class="line-modified">1567 _%label%_wide32:</span>
1568     crash()
1569 end
1570 
<span class="line-modified">1571 noWide(llint_op_wide16)</span>
<span class="line-modified">1572 noWide(llint_op_wide32)</span>
<span class="line-modified">1573 noWide(llint_op_enter)</span>
1574 
1575 op(llint_program_prologue, macro ()
1576     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1577     dispatch(0)
1578 end)
1579 
1580 
1581 op(llint_module_program_prologue, macro ()
1582     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1583     dispatch(0)
1584 end)
1585 
1586 
1587 op(llint_eval_prologue, macro ()
1588     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1589     dispatch(0)
1590 end)
1591 
1592 
1593 op(llint_function_for_call_prologue, macro ()
</pre>
<hr />
<pre>
1622 end)
1623 
1624 
1625 # Value-representation-specific code.
1626 if JSVALUE64
1627     include LowLevelInterpreter64
1628 else
1629     include LowLevelInterpreter32_64
1630 end
1631 
1632 
1633 # Value-representation-agnostic code.
1634 macro slowPathOp(opcodeName)
1635     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1636         callSlowPath(_slow_path_%opcodeName%)
1637         dispatch()
1638     end)
1639 end
1640 
1641 slowPathOp(create_cloned_arguments)
<span class="line-added">1642 slowPathOp(create_arguments_butterfly)</span>
1643 slowPathOp(create_direct_arguments)
1644 slowPathOp(create_lexical_environment)
1645 slowPathOp(create_rest)
1646 slowPathOp(create_scoped_arguments)
1647 slowPathOp(create_this)
<span class="line-added">1648 slowPathOp(create_promise)</span>
<span class="line-added">1649 slowPathOp(create_generator)</span>
<span class="line-added">1650 slowPathOp(create_async_generator)</span>
1651 slowPathOp(define_accessor_property)
1652 slowPathOp(define_data_property)
1653 slowPathOp(enumerator_generic_pname)
1654 slowPathOp(enumerator_structure_pname)
1655 slowPathOp(get_by_id_with_this)
1656 slowPathOp(get_by_val_with_this)
1657 slowPathOp(get_direct_pname)
1658 slowPathOp(get_enumerable_length)
1659 slowPathOp(get_property_enumerator)
1660 slowPathOp(greater)
1661 slowPathOp(greatereq)
1662 slowPathOp(has_generic_property)
1663 slowPathOp(has_indexed_property)
1664 slowPathOp(has_structure_property)
1665 slowPathOp(in_by_id)
1666 slowPathOp(in_by_val)
1667 slowPathOp(is_function)
1668 slowPathOp(is_object_or_null)
1669 slowPathOp(less)
1670 slowPathOp(lesseq)
1671 slowPathOp(mod)
1672 slowPathOp(new_array_buffer)
1673 slowPathOp(new_array_with_spread)
1674 slowPathOp(pow)
1675 slowPathOp(push_with_scope)
1676 slowPathOp(put_by_id_with_this)
1677 slowPathOp(put_by_val_with_this)
1678 slowPathOp(resolve_scope_for_hoisting_func_decl_in_eval)
1679 slowPathOp(spread)
1680 slowPathOp(strcat)
1681 slowPathOp(throw_static_error)
1682 slowPathOp(to_index_string)
1683 slowPathOp(typeof)
1684 slowPathOp(unreachable)
<span class="line-added">1685 slowPathOp(new_promise)</span>
<span class="line-added">1686 slowPathOp(new_generator)</span>
1687 
1688 macro llintSlowPathOp(opcodeName)
1689     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1690         callSlowPath(_llint_slow_path_%opcodeName%)
1691         dispatch()
1692     end)
1693 end
1694 
1695 llintSlowPathOp(del_by_id)
1696 llintSlowPathOp(del_by_val)
1697 llintSlowPathOp(instanceof)
1698 llintSlowPathOp(instanceof_custom)
1699 llintSlowPathOp(new_array)
1700 llintSlowPathOp(new_array_with_size)
1701 llintSlowPathOp(new_async_func)
1702 llintSlowPathOp(new_async_func_exp)
1703 llintSlowPathOp(new_async_generator_func)
1704 llintSlowPathOp(new_async_generator_func_exp)
1705 llintSlowPathOp(new_func)
1706 llintSlowPathOp(new_func_exp)
</pre>
<hr />
<pre>
1810 
1811 
1812 equalityJumpOp(
1813     jneq, OpJneq,
1814     macro (left, right, target) bineq left, right, target end)
1815 
1816 
1817 compareUnsignedJumpOp(
1818     jbelow, OpJbelow,
1819     macro (left, right, target) bib left, right, target end)
1820 
1821 
1822 compareUnsignedJumpOp(
1823     jbeloweq, OpJbeloweq,
1824     macro (left, right, target) bibeq left, right, target end)
1825 
1826 
1827 preOp(inc, OpInc,
1828     macro (value, slow) baddio 1, value, slow end)
1829 

1830 preOp(dec, OpDec,
1831     macro (value, slow) bsubio 1, value, slow end)
1832 
1833 
1834 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
<span class="line-modified">1835     checkSwitchToJITForLoop()</span>
<span class="line-added">1836     dispatch()</span>
<span class="line-added">1837 end)</span>
<span class="line-added">1838 </span>
<span class="line-added">1839 </span>
<span class="line-added">1840 llintOp(op_check_traps, OpCheckTraps, macro (unused, unused, dispatch)</span>
1841     loadp CodeBlock[cfr], t1
1842     loadp CodeBlock::m_vm[t1], t1
<span class="line-modified">1843     loadb VM::m_traps+VMTraps::m_needTrapHandling[t1], t0</span>
<span class="line-added">1844     btpnz t0, .handleTraps</span>
1845 .afterHandlingTraps:

1846     dispatch()
1847 .handleTraps:
<span class="line-modified">1848     callTrapHandler(.throwHandler)</span>
1849     jmp .afterHandlingTraps
<span class="line-added">1850 .throwHandler:</span>
<span class="line-added">1851     jmp _llint_throw_from_slow_path_trampoline</span>
1852 end)
1853 
1854 
1855 # Returns the packet pointer in t0.
1856 macro acquireShadowChickenPacket(slow)
1857     loadp CodeBlock[cfr], t1
1858     loadp CodeBlock::m_vm[t1], t1
1859     loadp VM::m_shadowChicken[t1], t2
1860     loadp ShadowChicken::m_logCursor[t2], t0
1861     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1862     addp sizeof ShadowChicken::Packet, t0, t1
1863     storep t1, ShadowChicken::m_logCursor[t2]
1864 end
1865 
1866 
1867 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1868     dispatch()
1869 end)
1870 
1871 
</pre>
<hr />
<pre>
1874     arrayProfileForCall(OpCall, getu)
1875 end)
1876 
1877 
1878 macro callOp(opcodeName, opcodeStruct, prepareCall, fn)
1879     commonCallOp(op_%opcodeName%, _llint_slow_path_%opcodeName%, opcodeStruct, prepareCall, fn)
1880 end
1881 
1882 
1883 callOp(tail_call, OpTailCall, prepareForTailCall, macro (getu, metadata)
1884     arrayProfileForCall(OpTailCall, getu)
1885     checkSwitchToJITForEpilogue()
1886     # reload metadata since checkSwitchToJITForEpilogue() might have trashed t5
1887     metadata(t5, t0)
1888 end)
1889 
1890 
1891 callOp(construct, OpConstruct, prepareForRegularCall, macro (getu, metadata) end)
1892 
1893 
<span class="line-modified">1894 macro branchIfException(exceptionTarget)</span>
<span class="line-added">1895     loadp CodeBlock[cfr], t3</span>
<span class="line-added">1896     loadp CodeBlock::m_vm[t3], t3</span>
<span class="line-added">1897     btpz VM::m_exception[t3], .noException</span>
<span class="line-added">1898     jmp exceptionTarget</span>
<span class="line-added">1899 .noException:</span>
<span class="line-added">1900 end</span>
<span class="line-added">1901 </span>
<span class="line-added">1902 macro doCallVarargs(opcodeName, size, opcodeStruct, dispatch, frameSlowPath, slowPath, prepareCall)</span>
1903     callSlowPath(frameSlowPath)
1904     branchIfException(_llint_throw_from_slow_path_trampoline)
1905     # calleeFrame in r1
1906     if JSVALUE64
1907         move r1, sp
1908     else
1909         # The calleeFrame is not stack aligned, move down by CallerFrameAndPCSize to align
1910         if ARMv7
1911             subp r1, CallerFrameAndPCSize, t2
1912             move t2, sp
1913         else
1914             subp r1, CallerFrameAndPCSize, sp
1915         end
1916     end
<span class="line-modified">1917     slowPathForCall(opcodeName, size, opcodeStruct, dispatch, slowPath, prepareCall)</span>
1918 end
1919 
1920 
1921 llintOp(op_call_varargs, OpCallVarargs, macro (size, get, dispatch)
<span class="line-modified">1922     doCallVarargs(op_call_varargs, size, OpCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_call_varargs, prepareForRegularCall)</span>
1923 end)
1924 
1925 llintOp(op_tail_call_varargs, OpTailCallVarargs, macro (size, get, dispatch)
1926     checkSwitchToJITForEpilogue()
1927     # We lie and perform the tail call instead of preparing it since we can&#39;t
1928     # prepare the frame for a call opcode
<span class="line-modified">1929     doCallVarargs(op_tail_call_varargs, size, OpTailCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_tail_call_varargs, prepareForTailCall)</span>
1930 end)
1931 
1932 
1933 llintOp(op_tail_call_forward_arguments, OpTailCallForwardArguments, macro (size, get, dispatch)
1934     checkSwitchToJITForEpilogue()
1935     # We lie and perform the tail call instead of preparing it since we can&#39;t
1936     # prepare the frame for a call opcode
<span class="line-modified">1937     doCallVarargs(op_tail_call_forward_arguments, size, OpTailCallForwardArguments, dispatch, _llint_slow_path_size_frame_for_forward_arguments, _llint_slow_path_tail_call_forward_arguments, prepareForTailCall)</span>
1938 end)
1939 
1940 
1941 llintOp(op_construct_varargs, OpConstructVarargs, macro (size, get, dispatch)
<span class="line-modified">1942     doCallVarargs(op_construct_varargs, size, OpConstructVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_construct_varargs, prepareForRegularCall)</span>
1943 end)
1944 
1945 
1946 # Eval is executed in one of two modes:
1947 #
1948 # 1) We find that we&#39;re really invoking eval() in which case the
1949 #    execution is perfomed entirely inside the slow_path, and it
1950 #    returns the PC of a function that just returns the return value
1951 #    that the eval returned.
1952 #
1953 # 2) We find that we&#39;re invoking something called eval() that is not
1954 #    the real eval. Then the slow_path returns the PC of the thing to
1955 #    call, and we call it.
1956 #
1957 # This allows us to handle two cases, which would require a total of
1958 # up to four pieces of state that cannot be easily packed into two
1959 # registers (C functions can return up to two registers, easily):
1960 #
1961 # - The call frame register. This may or may not have been modified
1962 #   by the slow_path, but the convention is that it returns it. It&#39;s not
1963 #   totally clear if that&#39;s necessary, since the cfr is callee save.
1964 #   But that&#39;s our style in this here interpreter so we stick with it.
1965 #
1966 # - A bit to say if the slow_path successfully executed the eval and has
1967 #   the return value, or did not execute the eval but has a PC for us
1968 #   to call.
1969 #
1970 # - Either:
1971 #   - The JS return value (two registers), or
1972 #
1973 #   - The PC to call.
1974 #
1975 # It turns out to be easier to just always have this return the cfr
1976 # and a PC to call, and that PC may be a dummy thunk that just
1977 # returns the JS value that the eval returned.
1978 
1979 _llint_op_call_eval:
1980     slowPathForCall(
<span class="line-added">1981         op_call_eval_narrow,</span>
1982         narrow,
1983         OpCallEval,
1984         macro () dispatchOp(narrow, op_call_eval) end,
1985         _llint_slow_path_call_eval,
1986         prepareForRegularCall)
1987 
1988 _llint_op_call_eval_wide16:
1989     slowPathForCall(
<span class="line-added">1990         op_call_eval_wide16,</span>
1991         wide16,
1992         OpCallEval,
1993         macro () dispatchOp(wide16, op_call_eval) end,
1994         _llint_slow_path_call_eval_wide16,
1995         prepareForRegularCall)
1996 
1997 _llint_op_call_eval_wide32:
1998     slowPathForCall(
<span class="line-added">1999         op_call_eval_wide32,</span>
2000         wide32,
2001         OpCallEval,
2002         macro () dispatchOp(wide32, op_call_eval) end,
2003         _llint_slow_path_call_eval_wide32,
2004         prepareForRegularCall)
2005 
2006 
2007 commonOp(llint_generic_return_point, macro () end, macro (size)
2008     dispatchAfterCall(size, OpCallEval, macro ()
2009         dispatchOp(size, op_call_eval)
2010     end)
2011 end)
2012 
2013 
2014 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
2015     dispatch()
2016 end)
2017 
2018 
2019 llintOp(op_yield, OpYield, macro (unused, unused, unused)
</pre>
<hr />
<pre>
2039 op(llint_native_call_trampoline, macro ()
2040     nativeCallTrampoline(NativeExecutable::m_function)
2041 end)
2042 
2043 
2044 op(llint_native_construct_trampoline, macro ()
2045     nativeCallTrampoline(NativeExecutable::m_constructor)
2046 end)
2047 
2048 
2049 op(llint_internal_function_call_trampoline, macro ()
2050     internalFunctionCallTrampoline(InternalFunction::m_functionForCall)
2051 end)
2052 
2053 
2054 op(llint_internal_function_construct_trampoline, macro ()
2055     internalFunctionCallTrampoline(InternalFunction::m_functionForConstruct)
2056 end)
2057 
2058 
<span class="line-added">2059 op(checkpoint_osr_exit_from_inlined_call_trampoline, macro ()</span>
<span class="line-added">2060     if (JSVALUE64 and not (C_LOOP or C_LOOP_WIN)) or ARMv7 or MIPS</span>
<span class="line-added">2061         restoreStackPointerAfterCall()</span>
<span class="line-added">2062 </span>
<span class="line-added">2063         # Make sure we move r0 to a1 first since r0 might be the same as a0, for instance, on arm.</span>
<span class="line-added">2064         if ARMv7 or MIPS</span>
<span class="line-added">2065             # Given _slow_path_checkpoint_osr_exit_from_inlined_call has</span>
<span class="line-added">2066             # parameters as CallFrame* and EncodedJSValue,</span>
<span class="line-added">2067             # we need to store call result on a2, a3 and call frame on a0,</span>
<span class="line-added">2068             # leaving a1 as dummy value.</span>
<span class="line-added">2069             move r1, a3</span>
<span class="line-added">2070             move r0, a2</span>
<span class="line-added">2071             move cfr, a0</span>
<span class="line-added">2072             # We don&#39;t call saveStateForCCall() because we are going to use the bytecodeIndex from our side state.</span>
<span class="line-added">2073             cCall4(_slow_path_checkpoint_osr_exit_from_inlined_call)</span>
<span class="line-added">2074         else</span>
<span class="line-added">2075             move r0, a1</span>
<span class="line-added">2076             move cfr, a0</span>
<span class="line-added">2077             # We don&#39;t call saveStateForCCall() because we are going to use the bytecodeIndex from our side state.</span>
<span class="line-added">2078             cCall2(_slow_path_checkpoint_osr_exit_from_inlined_call)</span>
<span class="line-added">2079         end</span>
<span class="line-added">2080 </span>
<span class="line-added">2081         restoreStateAfterCCall()</span>
<span class="line-added">2082         branchIfException(_llint_throw_from_slow_path_trampoline)</span>
<span class="line-added">2083         jmp r1, JSEntryPtrTag</span>
<span class="line-added">2084     else</span>
<span class="line-added">2085         notSupported()</span>
<span class="line-added">2086     end</span>
<span class="line-added">2087 end)</span>
<span class="line-added">2088 </span>
<span class="line-added">2089 op(checkpoint_osr_exit_trampoline, macro ()</span>
<span class="line-added">2090     # FIXME: We can probably dispatch to the checkpoint handler directly but this was easier</span>
<span class="line-added">2091     # and probably doesn&#39;t matter for performance.</span>
<span class="line-added">2092     if (JSVALUE64 and not (C_LOOP or C_LOOP_WIN)) or ARMv7 or MIPS</span>
<span class="line-added">2093         restoreStackPointerAfterCall()</span>
<span class="line-added">2094 </span>
<span class="line-added">2095         move cfr, a0</span>
<span class="line-added">2096         # We don&#39;t call saveStateForCCall() because we are going to use the bytecodeIndex from our side state.</span>
<span class="line-added">2097         cCall2(_slow_path_checkpoint_osr_exit)</span>
<span class="line-added">2098         restoreStateAfterCCall()</span>
<span class="line-added">2099         branchIfException(_llint_throw_from_slow_path_trampoline)</span>
<span class="line-added">2100         jmp r1, JSEntryPtrTag</span>
<span class="line-added">2101     else</span>
<span class="line-added">2102         notSupported()</span>
<span class="line-added">2103     end</span>
<span class="line-added">2104 end)</span>
<span class="line-added">2105 </span>
2106 # Lastly, make sure that we can link even though we don&#39;t support all opcodes.
2107 # These opcodes should never arise when using LLInt or either JIT. We assert
2108 # as much.
2109 
2110 macro notSupported()
2111     if ASSERT_ENABLED
2112         crash()
2113     else
2114         # We should use whatever the smallest possible instruction is, just to
2115         # ensure that there is a gap between instruction labels. If multiple
2116         # smallest instructions exist, we should pick the one that is most
2117         # likely result in execution being halted. Currently that is the break
2118         # instruction on all architectures we&#39;re interested in. (Break is int3
2119         # on Intel, which is 1 byte, and bkpt on ARMv7, which is 2 bytes.)
2120         break
2121     end
2122 end
<span class="line-added">2123 </span>
<span class="line-added">2124 // FIXME: We should not need the X86_64_WIN condition here, since WEBASSEMBLY should already be false on Windows</span>
<span class="line-added">2125 // https://bugs.webkit.org/show_bug.cgi?id=203716</span>
<span class="line-added">2126 if WEBASSEMBLY and not X86_64_WIN</span>
<span class="line-added">2127 </span>
<span class="line-added">2128 entry(wasm, macro()</span>
<span class="line-added">2129     include InitWasm</span>
<span class="line-added">2130 end)</span>
<span class="line-added">2131 </span>
<span class="line-added">2132 macro wasmScope()</span>
<span class="line-added">2133     # Wrap the script in a macro since it overwrites some of the LLInt macros,</span>
<span class="line-added">2134     # but we don&#39;t want to interfere with the LLInt opcodes</span>
<span class="line-added">2135     include WebAssembly</span>
<span class="line-added">2136 end</span>
<span class="line-added">2137 wasmScope()</span>
<span class="line-added">2138 </span>
<span class="line-added">2139 else</span>
<span class="line-added">2140 </span>
<span class="line-added">2141 # These need to be defined even when WebAssembly is disabled</span>
<span class="line-added">2142 op(wasm_function_prologue, macro ()</span>
<span class="line-added">2143     crash()</span>
<span class="line-added">2144 end)</span>
<span class="line-added">2145 </span>
<span class="line-added">2146 op(wasm_function_prologue_no_tls, macro ()</span>
<span class="line-added">2147     crash()</span>
<span class="line-added">2148 end)</span>
<span class="line-added">2149 </span>
<span class="line-added">2150 end</span>
</pre>
</td>
</tr>
</table>
<center><a href="LLIntThunks.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="LowLevelInterpreter.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>