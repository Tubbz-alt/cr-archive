<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2017-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;Subspace.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;AllocatorInlines.h&quot;
 31 #include &quot;BlockDirectoryInlines.h&quot;
 32 #include &quot;JSCInlines.h&quot;
 33 #include &quot;LocalAllocatorInlines.h&quot;
 34 #include &quot;MarkedBlockInlines.h&quot;
<a name="1" id="anc1"></a><span class="line-added"> 35 #include &quot;MarkedSpaceInlines.h&quot;</span>
 36 #include &quot;PreventCollectionScope.h&quot;
 37 #include &quot;SubspaceInlines.h&quot;
 38 
 39 namespace JSC {
 40 
 41 CompleteSubspace::CompleteSubspace(CString name, Heap&amp; heap, HeapCellType* heapCellType, AlignedMemoryAllocator* alignedMemoryAllocator)
 42     : Subspace(name, heap)
 43 {
 44     initialize(heapCellType, alignedMemoryAllocator);
 45 }
 46 
 47 CompleteSubspace::~CompleteSubspace()
 48 {
 49 }
 50 
 51 Allocator CompleteSubspace::allocatorFor(size_t size, AllocatorForMode mode)
 52 {
 53     return allocatorForNonVirtual(size, mode);
 54 }
 55 
 56 void* CompleteSubspace::allocate(VM&amp; vm, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
 57 {
 58     return allocateNonVirtual(vm, size, deferralContext, failureMode);
 59 }
 60 
 61 Allocator CompleteSubspace::allocatorForSlow(size_t size)
 62 {
 63     size_t index = MarkedSpace::sizeClassToIndex(size);
 64     size_t sizeClass = MarkedSpace::s_sizeClassForSizeStep[index];
 65     if (!sizeClass)
 66         return Allocator();
 67 
 68     // This is written in such a way that it&#39;s OK for the JIT threads to end up here if they want
 69     // to generate code that uses some allocator that hadn&#39;t been used yet. Note that a possibly-
 70     // just-as-good solution would be to return null if we&#39;re in the JIT since the JIT treats null
 71     // allocator as &quot;please always take the slow path&quot;. But, that could lead to performance
 72     // surprises and the algorithm here is pretty easy. Only this code has to hold the lock, to
 73     // prevent simultaneously BlockDirectory creations from multiple threads. This code ensures
 74     // that any &quot;forEachAllocator&quot; traversals will only see this allocator after it&#39;s initialized
 75     // enough: it will have
 76     auto locker = holdLock(m_space.directoryLock());
 77     if (Allocator allocator = m_allocatorForSizeStep[index])
 78         return allocator;
 79 
 80     if (false)
 81         dataLog(&quot;Creating BlockDirectory/LocalAllocator for &quot;, m_name, &quot;, &quot;, attributes(), &quot;, &quot;, sizeClass, &quot;.\n&quot;);
 82 
<a name="2" id="anc2"></a><span class="line-modified"> 83     std::unique_ptr&lt;BlockDirectory&gt; uniqueDirectory = makeUnique&lt;BlockDirectory&gt;(sizeClass);</span>

 84     BlockDirectory* directory = uniqueDirectory.get();
 85     m_directories.append(WTFMove(uniqueDirectory));
 86 
 87     directory-&gt;setSubspace(this);
 88     m_space.addBlockDirectory(locker, directory);
 89 
 90     std::unique_ptr&lt;LocalAllocator&gt; uniqueLocalAllocator =
 91         makeUnique&lt;LocalAllocator&gt;(directory);
 92     LocalAllocator* localAllocator = uniqueLocalAllocator.get();
 93     m_localAllocators.append(WTFMove(uniqueLocalAllocator));
 94 
 95     Allocator allocator(localAllocator);
 96 
 97     index = MarkedSpace::sizeClassToIndex(sizeClass);
 98     for (;;) {
 99         if (MarkedSpace::s_sizeClassForSizeStep[index] != sizeClass)
100             break;
101 
102         m_allocatorForSizeStep[index] = allocator;
103 
104         if (!index--)
105             break;
106     }
107 
108     directory-&gt;setNextDirectoryInSubspace(m_firstDirectory);
<a name="3" id="anc3"></a><span class="line-modified">109     m_alignedMemoryAllocator-&gt;registerDirectory(m_space.heap(), directory);</span>
110     WTF::storeStoreFence();
111     m_firstDirectory = directory;
112     return allocator;
113 }
114 
115 void* CompleteSubspace::allocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
116 {
117     void* result = tryAllocateSlow(vm, size, deferralContext);
118     if (failureMode == AllocationFailureMode::Assert)
119         RELEASE_ASSERT(result);
120     return result;
121 }
122 
123 void* CompleteSubspace::tryAllocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext)
124 {
125     if (validateDFGDoesGC)
126         RELEASE_ASSERT(vm.heap.expectDoesGC());
127 
128     sanitizeStackForVM(vm);
129 
130     if (Allocator allocator = allocatorFor(size, AllocatorForMode::EnsureAllocator))
<a name="4" id="anc4"></a><span class="line-modified">131         return allocator.allocate(vm.heap, deferralContext, AllocationFailureMode::ReturnNull);</span>
132 
<a name="5" id="anc5"></a><span class="line-modified">133     if (size &lt;= Options::preciseAllocationCutoff()</span>
134         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
135         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
136         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
137         RELEASE_ASSERT_NOT_REACHED();
138     }
139 
140     vm.heap.collectIfNecessaryOrDefer(deferralContext);
141 
142     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
<a name="6" id="anc6"></a><span class="line-modified">143     PreciseAllocation* allocation = PreciseAllocation::tryCreate(vm.heap, size, this, m_space.m_preciseAllocations.size());</span>
144     if (!allocation)
145         return nullptr;
146 
<a name="7" id="anc7"></a><span class="line-modified">147     m_space.m_preciseAllocations.append(allocation);</span>
<span class="line-modified">148     if (auto* set = m_space.preciseAllocationSet())</span>
<span class="line-added">149         set-&gt;add(allocation-&gt;cell());</span>
<span class="line-added">150     ASSERT(allocation-&gt;indexInSpace() == m_space.m_preciseAllocations.size() - 1);</span>
151     vm.heap.didAllocate(size);
152     m_space.m_capacity += size;
153 
<a name="8" id="anc8"></a><span class="line-modified">154     m_preciseAllocations.append(allocation);</span>
155 
156     return allocation-&gt;cell();
157 }
158 
<a name="9" id="anc9"></a><span class="line-modified">159 void* CompleteSubspace::reallocatePreciseAllocationNonVirtual(VM&amp; vm, HeapCell* oldCell, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)</span>
160 {
161     if (validateDFGDoesGC)
162         RELEASE_ASSERT(vm.heap.expectDoesGC());
163 
164     // The following conditions are met in Butterfly for example.
<a name="10" id="anc10"></a><span class="line-modified">165     ASSERT(oldCell-&gt;isPreciseAllocation());</span>
166 
<a name="11" id="anc11"></a><span class="line-modified">167     PreciseAllocation* oldAllocation = &amp;oldCell-&gt;preciseAllocation();</span>
168     ASSERT(oldAllocation-&gt;cellSize() &lt;= size);
169     ASSERT(oldAllocation-&gt;weakSet().isTriviallyDestructible());
170     ASSERT(oldAllocation-&gt;attributes().destruction == DoesNotNeedDestruction);
171     ASSERT(oldAllocation-&gt;attributes().cellKind == HeapCell::Auxiliary);
172     ASSERT(size &gt; MarkedSpace::largeCutoff);
173 
174     sanitizeStackForVM(vm);
175 
<a name="12" id="anc12"></a><span class="line-modified">176     if (size &lt;= Options::preciseAllocationCutoff()</span>
177         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
178         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
179         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
180         RELEASE_ASSERT_NOT_REACHED();
181     }
182 
183     vm.heap.collectIfNecessaryOrDefer(deferralContext);
184 
185     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
186     size_t difference = size - oldAllocation-&gt;cellSize();
187     unsigned oldIndexInSpace = oldAllocation-&gt;indexInSpace();
188     if (oldAllocation-&gt;isOnList())
189         oldAllocation-&gt;remove();
190 
<a name="13" id="anc13"></a><span class="line-modified">191     PreciseAllocation* allocation = oldAllocation-&gt;tryReallocate(size, this);</span>
192     if (!allocation) {
193         RELEASE_ASSERT(failureMode != AllocationFailureMode::Assert);
<a name="14" id="anc14"></a><span class="line-modified">194         m_preciseAllocations.append(oldAllocation);</span>
195         return nullptr;
196     }
197     ASSERT(oldIndexInSpace == allocation-&gt;indexInSpace());
198 
<a name="15" id="anc15"></a><span class="line-modified">199     // If reallocation changes the address, we should update HashSet.</span>
<span class="line-added">200     if (oldAllocation != allocation) {</span>
<span class="line-added">201         if (auto* set = m_space.preciseAllocationSet()) {</span>
<span class="line-added">202             set-&gt;remove(oldAllocation-&gt;cell());</span>
<span class="line-added">203             set-&gt;add(allocation-&gt;cell());</span>
<span class="line-added">204         }</span>
<span class="line-added">205     }</span>
<span class="line-added">206 </span>
<span class="line-added">207     m_space.m_preciseAllocations[oldIndexInSpace] = allocation;</span>
208     vm.heap.didAllocate(difference);
209     m_space.m_capacity += difference;
210 
<a name="16" id="anc16"></a><span class="line-modified">211     m_preciseAllocations.append(allocation);</span>
212 
213     return allocation-&gt;cell();
214 }
215 
216 } // namespace JSC
217 
<a name="17" id="anc17"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="17" type="hidden" />
</body>
</html>