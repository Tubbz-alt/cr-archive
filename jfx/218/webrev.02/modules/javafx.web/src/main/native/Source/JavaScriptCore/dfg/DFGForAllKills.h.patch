diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGForAllKills.h b/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGForAllKills.h
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGForAllKills.h
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGForAllKills.h
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2015 Apple Inc. All rights reserved.
+ * Copyright (C) 2015-2019 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -30,10 +30,14 @@
 #include "DFGOSRAvailabilityAnalysisPhase.h"
 #include "FullBytecodeLiveness.h"
 
 namespace JSC { namespace DFG {
 
+namespace ForAllKillsInternal {
+constexpr bool verbose = false;
+}
+
 // Utilities for finding the last points where a node is live in DFG SSA. This accounts for liveness due
 // to OSR exit. This is usually used for enumerating over all of the program points where a node is live,
 // by exploring all blocks where the node is live at tail and then exploring all program points where the
 // node is killed. A prerequisite to using these utilities is having liveness and OSR availability
 // computed.
@@ -51,63 +55,83 @@
         return;
     }
 
     CodeOrigin after = nodeAfter->origin.forExit;
 
-    VirtualRegister alreadyNoted;
+    Operand alreadyNoted;
     // If we MovHint something that is live at the time, then we kill the old value.
     if (nodeAfter->containsMovHint()) {
-        VirtualRegister reg = nodeAfter->unlinkedLocal();
-        if (graph.isLiveInBytecode(reg, after)) {
-            functor(reg);
-            alreadyNoted = reg;
+        Operand operand = nodeAfter->unlinkedOperand();
+        if (graph.isLiveInBytecode(operand, after)) {
+            functor(operand);
+            alreadyNoted = operand;
         }
     }
 
     if (before == after)
         return;
 
     // It's easier to do this if the inline call frames are the same. This is way faster than the
     // other loop, below.
     auto* beforeInlineCallFrame = before.inlineCallFrame();
     if (beforeInlineCallFrame == after.inlineCallFrame()) {
-        int stackOffset = beforeInlineCallFrame ? beforeInlineCallFrame->stackOffset : 0;
         CodeBlock* codeBlock = graph.baselineCodeBlockFor(beforeInlineCallFrame);
+        if (after.bytecodeIndex().checkpoint()) {
+            ASSERT(before.bytecodeIndex().checkpoint() != after.bytecodeIndex().checkpoint());
+            ASSERT_WITH_MESSAGE(before.bytecodeIndex().offset() == after.bytecodeIndex().offset(), "When the DFG does code motion it should change the forExit origin to match the surrounding bytecodes.");
+
+            auto liveBefore = tmpLivenessForCheckpoint(*codeBlock, before.bytecodeIndex());
+            auto liveAfter = tmpLivenessForCheckpoint(*codeBlock, after.bytecodeIndex());
+            liveAfter.invert();
+            liveBefore.filter(liveAfter);
+
+            liveBefore.forEachSetBit([&] (size_t tmp) {
+                functor(remapOperand(beforeInlineCallFrame, Operand::tmp(tmp)));
+            });
+            // No locals can die at a checkpoint.
+            return;
+        }
+
         FullBytecodeLiveness& fullLiveness = graph.livenessFor(codeBlock);
-        const FastBitVector& liveBefore = fullLiveness.getLiveness(before.bytecodeIndex());
-        const FastBitVector& liveAfter = fullLiveness.getLiveness(after.bytecodeIndex());
+        const FastBitVector& liveBefore = fullLiveness.getLiveness(before.bytecodeIndex(), LivenessCalculationPoint::BeforeUse);
+        const FastBitVector& liveAfter = fullLiveness.getLiveness(after.bytecodeIndex(), LivenessCalculationPoint::BeforeUse);
 
         (liveBefore & ~liveAfter).forEachSetBit(
             [&] (size_t relativeLocal) {
-                functor(virtualRegisterForLocal(relativeLocal) + stackOffset);
+                functor(remapOperand(beforeInlineCallFrame, virtualRegisterForLocal(relativeLocal)));
             });
         return;
     }
 
+    ASSERT_WITH_MESSAGE(!after.bytecodeIndex().checkpoint(), "Transitioning across a checkpoint but before and after don't share an inlineCallFrame.");
+
     // Detect kills the super conservative way: it is killed if it was live before and dead after.
-    BitVector liveAfter = graph.localsLiveInBytecode(after);
-    graph.forAllLocalsLiveInBytecode(
+    BitVector liveAfter = graph.localsAndTmpsLiveInBytecode(after);
+    unsigned numLocals = graph.block(0)->variablesAtHead.numberOfLocals();
+    graph.forAllLocalsAndTmpsLiveInBytecode(
         before,
-        [&] (VirtualRegister reg) {
-            if (reg == alreadyNoted)
+        [&] (Operand operand) {
+            if (operand == alreadyNoted)
                 return;
-            if (liveAfter.get(reg.toLocal()))
+            unsigned offset = operand.isTmp() ? numLocals + operand.value() : operand.toLocal();
+            if (liveAfter.get(offset))
                 return;
-            functor(reg);
+            functor(operand);
         });
 }
 
 // Tells you all of the nodes that would no longer be live across the node at this nodeIndex.
 template<typename Functor>
 void forAllKilledNodesAtNodeIndex(
     Graph& graph, AvailabilityMap& availabilityMap, BasicBlock* block, unsigned nodeIndex,
     const Functor& functor)
 {
-    static const unsigned seenInClosureFlag = 1;
-    static const unsigned calledFunctorFlag = 2;
+    static constexpr unsigned seenInClosureFlag = 1;
+    static constexpr unsigned calledFunctorFlag = 2;
     HashMap<Node*, unsigned> flags;
 
+    ASSERT(nodeIndex);
     Node* node = block->at(nodeIndex);
 
     graph.doToChildren(
         node,
         [&] (Edge edge) {
@@ -118,19 +142,17 @@
                     result |= calledFunctorFlag;
                 }
             }
         });
 
-    Node* before = nullptr;
-    if (nodeIndex)
-        before = block->at(nodeIndex - 1);
+    Node* before = block->at(nodeIndex - 1);
 
     forAllKilledOperands(
         graph, before, node,
-        [&] (VirtualRegister reg) {
+        [&] (Operand operand) {
             availabilityMap.closeStartingWithLocal(
-                reg,
+                operand,
                 [&] (Node* node) -> bool {
                     return flags.get(node) & seenInClosureFlag;
                 },
                 [&] (Node* node) -> bool {
                     auto& resultFlags = flags.add(node, 0).iterator->value;
@@ -157,10 +179,11 @@
     LocalOSRAvailabilityCalculator localAvailability(graph);
     localAvailability.beginBlock(block);
     // Start at the second node, because the functor is expected to only inspect nodes from the start of
     // the block up to nodeIndex (exclusive), so if nodeIndex is zero then the functor has nothing to do.
     for (unsigned nodeIndex = 1; nodeIndex < block->size(); ++nodeIndex) {
+        dataLogLnIf(ForAllKillsInternal::verbose, "local availability at index: ", nodeIndex, " ", localAvailability.m_availability);
         forAllKilledNodesAtNodeIndex(
             graph, localAvailability.m_availability, block, nodeIndex,
             [&] (Node* node) {
                 functor(nodeIndex, node);
             });
