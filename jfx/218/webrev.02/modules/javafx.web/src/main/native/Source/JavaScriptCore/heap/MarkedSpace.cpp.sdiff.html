<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="MarkedBlockInlines.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedSpace.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 10  *  This library is distributed in the hope that it will be useful,
 11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 13  *  Lesser General Public License for more details.
 14  *
 15  *  You should have received a copy of the GNU Lesser General Public
 16  *  License along with this library; if not, write to the Free Software
 17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 18  *
 19  */
 20 
 21 #include &quot;config.h&quot;
 22 #include &quot;MarkedSpace.h&quot;
 23 
 24 #include &quot;BlockDirectoryInlines.h&quot;
 25 #include &quot;FunctionCodeBlock.h&quot;
 26 #include &quot;IncrementalSweeper.h&quot;
 27 #include &quot;JSObject.h&quot;
 28 #include &quot;JSCInlines.h&quot;
 29 #include &quot;MarkedBlockInlines.h&quot;

 30 #include &lt;wtf/ListDump.h&gt;
 31 
 32 namespace JSC {
 33 
 34 std::array&lt;size_t, MarkedSpace::numSizeClasses&gt; MarkedSpace::s_sizeClassForSizeStep;
 35 
 36 namespace {
 37 
 38 const Vector&lt;size_t&gt;&amp; sizeClasses()
 39 {
 40     static Vector&lt;size_t&gt;* result;
 41     static std::once_flag once;
 42     std::call_once(
 43         once,
 44         [] {
 45             result = new Vector&lt;size_t&gt;();
 46 
<span class="line-modified"> 47             if (Options::dumpSizeClasses()) {</span>
 48                 dataLog(&quot;Block size: &quot;, MarkedBlock::blockSize, &quot;\n&quot;);
 49                 dataLog(&quot;Footer size: &quot;, sizeof(MarkedBlock::Footer), &quot;\n&quot;);
 50             }
 51 
 52             auto add = [&amp;] (size_t sizeClass) {
 53                 sizeClass = WTF::roundUpToMultipleOf&lt;MarkedBlock::atomSize&gt;(sizeClass);
<span class="line-modified"> 54                 if (Options::dumpSizeClasses())</span>
<span class="line-removed"> 55                     dataLog(&quot;Adding JSC MarkedSpace size class: &quot;, sizeClass, &quot;\n&quot;);</span>
 56                 // Perform some validation as we go.
 57                 RELEASE_ASSERT(!(sizeClass % MarkedSpace::sizeStep));
 58                 if (result-&gt;isEmpty())
 59                     RELEASE_ASSERT(sizeClass == MarkedSpace::sizeStep);
 60                 result-&gt;append(sizeClass);
 61             };
 62 
 63             // This is a definition of the size classes in our GC. It must define all of the
 64             // size classes from sizeStep up to largeCutoff.
 65 
 66             // Have very precise size classes for the small stuff. This is a loop to make it easy to reduce
 67             // atomSize.
 68             for (size_t size = MarkedSpace::sizeStep; size &lt; MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep)
 69                 add(size);
 70 
 71             // We want to make sure that the remaining size classes minimize internal fragmentation (i.e.
 72             // the wasted space at the tail end of a MarkedBlock) while proceeding roughly in an exponential
 73             // way starting at just above the precise size classes to four cells per block.
 74 
<span class="line-modified"> 75             if (Options::dumpSizeClasses())</span>
<span class="line-removed"> 76                 dataLog(&quot;    Marked block payload size: &quot;, static_cast&lt;size_t&gt;(MarkedSpace::blockPayload), &quot;\n&quot;);</span>
 77 
 78             for (unsigned i = 0; ; ++i) {
 79                 double approximateSize = MarkedSpace::preciseCutoff * pow(Options::sizeClassProgression(), i);
<span class="line-modified"> 80 </span>
<span class="line-removed"> 81                 if (Options::dumpSizeClasses())</span>
<span class="line-removed"> 82                     dataLog(&quot;    Next size class as a double: &quot;, approximateSize, &quot;\n&quot;);</span>
 83 
 84                 size_t approximateSizeInBytes = static_cast&lt;size_t&gt;(approximateSize);
<span class="line-modified"> 85 </span>
<span class="line-removed"> 86                 if (Options::dumpSizeClasses())</span>
<span class="line-removed"> 87                     dataLog(&quot;    Next size class as bytes: &quot;, approximateSizeInBytes, &quot;\n&quot;);</span>
 88 
 89                 // Make sure that the computer did the math correctly.
 90                 RELEASE_ASSERT(approximateSizeInBytes &gt;= MarkedSpace::preciseCutoff);
 91 
 92                 if (approximateSizeInBytes &gt; MarkedSpace::largeCutoff)
 93                     break;
 94 
 95                 size_t sizeClass =
 96                     WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(approximateSizeInBytes);
<span class="line-modified"> 97 </span>
<span class="line-removed"> 98                 if (Options::dumpSizeClasses())</span>
<span class="line-removed"> 99                     dataLog(&quot;    Size class: &quot;, sizeClass, &quot;\n&quot;);</span>
100 
101                 // Optimize the size class so that there isn&#39;t any slop at the end of the block&#39;s
102                 // payload.
103                 unsigned cellsPerBlock = MarkedSpace::blockPayload / sizeClass;
104                 size_t possiblyBetterSizeClass = (MarkedSpace::blockPayload / cellsPerBlock) &amp; ~(MarkedSpace::sizeStep - 1);
<span class="line-modified">105 </span>
<span class="line-removed">106                 if (Options::dumpSizeClasses())</span>
<span class="line-removed">107                     dataLog(&quot;    Possibly better size class: &quot;, possiblyBetterSizeClass, &quot;\n&quot;);</span>
108 
109                 // The size class we just came up with is better than the other one if it reduces
110                 // total wastage assuming we only allocate cells of that size.
111                 size_t originalWastage = MarkedSpace::blockPayload - cellsPerBlock * sizeClass;
112                 size_t newWastage = (possiblyBetterSizeClass - sizeClass) * cellsPerBlock;
<span class="line-modified">113 </span>
<span class="line-removed">114                 if (Options::dumpSizeClasses())</span>
<span class="line-removed">115                     dataLog(&quot;    Original wastage: &quot;, originalWastage, &quot;, new wastage: &quot;, newWastage, &quot;\n&quot;);</span>
116 
117                 size_t betterSizeClass;
118                 if (newWastage &gt; originalWastage)
119                     betterSizeClass = sizeClass;
120                 else
121                     betterSizeClass = possiblyBetterSizeClass;
122 
<span class="line-modified">123                 if (Options::dumpSizeClasses())</span>
<span class="line-removed">124                     dataLog(&quot;    Choosing size class: &quot;, betterSizeClass, &quot;\n&quot;);</span>
125 
126                 if (betterSizeClass == result-&gt;last()) {
127                     // Defense for when expStep is small.
128                     continue;
129                 }
130 
131                 // This is usually how we get out of the loop.
132                 if (betterSizeClass &gt; MarkedSpace::largeCutoff
<span class="line-modified">133                     || betterSizeClass &gt; Options::largeAllocationCutoff())</span>
134                     break;
135 
136                 add(betterSizeClass);
137             }
138 
139             // Manually inject size classes for objects we know will be allocated in high volume.
140             // FIXME: All of these things should have IsoSubspaces.
141             // https://bugs.webkit.org/show_bug.cgi?id=179876
<span class="line-modified">142             add(sizeof(UnlinkedFunctionCodeBlock));</span>
<span class="line-removed">143             add(sizeof(JSString));</span>
144 
145             {
146                 // Sort and deduplicate.
147                 std::sort(result-&gt;begin(), result-&gt;end());
148                 auto it = std::unique(result-&gt;begin(), result-&gt;end());
149                 result-&gt;shrinkCapacity(it - result-&gt;begin());
150             }
151 
<span class="line-modified">152             if (Options::dumpSizeClasses())</span>
<span class="line-removed">153                 dataLog(&quot;JSC Heap MarkedSpace size class dump: &quot;, listDump(*result), &quot;\n&quot;);</span>
154 
155             // We have an optimiation in MarkedSpace::optimalSizeFor() that assumes things about
156             // the size class table. This checks our results against that function&#39;s assumptions.
157             for (size_t size = MarkedSpace::sizeStep, i = 0; size &lt;= MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep, i++)
158                 RELEASE_ASSERT(result-&gt;at(i) == size);
159         });
160     return *result;
161 }
162 
163 template&lt;typename TableType, typename SizeClassCons, typename DefaultCons&gt;
164 void buildSizeClassTable(TableType&amp; table, const SizeClassCons&amp; cons, const DefaultCons&amp; defaultCons)
165 {
166     size_t nextIndex = 0;
167     for (size_t sizeClass : sizeClasses()) {
168         auto entry = cons(sizeClass);
169         size_t index = MarkedSpace::sizeClassToIndex(sizeClass);
170         for (size_t i = nextIndex; i &lt;= index; ++i)
171             table[i] = entry;
172         nextIndex = index + 1;
173     }
</pre>
<hr />
<pre>
179 } // anonymous namespace
180 
181 void MarkedSpace::initializeSizeClassForStepSize()
182 {
183     static std::once_flag flag;
184     std::call_once(
185         flag,
186         [] {
187             buildSizeClassTable(
188                 s_sizeClassForSizeStep,
189                 [&amp;] (size_t sizeClass) -&gt; size_t {
190                     return sizeClass;
191                 },
192                 [&amp;] (size_t sizeClass) -&gt; size_t {
193                     return sizeClass;
194                 });
195         });
196 }
197 
198 MarkedSpace::MarkedSpace(Heap* heap)
<span class="line-removed">199     : m_heap(heap)</span>
200 {

201     initializeSizeClassForStepSize();
202 }
203 
204 MarkedSpace::~MarkedSpace()
205 {
206     ASSERT(!m_blocks.set().size());
207 }
208 
209 void MarkedSpace::freeMemory()
210 {
211     forEachBlock(
212         [&amp;] (MarkedBlock::Handle* block) {
213             freeBlock(block);
214         });
<span class="line-modified">215     for (LargeAllocation* allocation : m_largeAllocations)</span>
216         allocation-&gt;destroy();





217 }
218 
219 void MarkedSpace::lastChanceToFinalize()
220 {
221     forEachDirectory(
222         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
223             directory.lastChanceToFinalize();
224             return IterationStatus::Continue;
225         });
<span class="line-modified">226     for (LargeAllocation* allocation : m_largeAllocations)</span>
227         allocation-&gt;lastChanceToFinalize();

228 }
229 
<span class="line-modified">230 void MarkedSpace::sweep()</span>
231 {
<span class="line-modified">232     m_heap-&gt;sweeper().stopSweeping();</span>
233     forEachDirectory(
234         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
235             directory.sweep();
236             return IterationStatus::Continue;
237         });
238 }
239 
<span class="line-modified">240 void MarkedSpace::sweepLargeAllocations()</span>
241 {
<span class="line-modified">242     RELEASE_ASSERT(m_largeAllocationsNurseryOffset == m_largeAllocations.size());</span>
<span class="line-modified">243     unsigned srcIndex = m_largeAllocationsNurseryOffsetForSweep;</span>
244     unsigned dstIndex = srcIndex;
<span class="line-modified">245     while (srcIndex &lt; m_largeAllocations.size()) {</span>
<span class="line-modified">246         LargeAllocation* allocation = m_largeAllocations[srcIndex++];</span>
247         allocation-&gt;sweep();
248         if (allocation-&gt;isEmpty()) {
<span class="line-modified">249             m_capacity -= allocation-&gt;cellSize();</span>
<span class="line-modified">250             allocation-&gt;destroy();</span>






251             continue;
252         }
253         allocation-&gt;setIndexInSpace(dstIndex);
<span class="line-modified">254         m_largeAllocations[dstIndex++] = allocation;</span>
255     }
<span class="line-modified">256     m_largeAllocations.shrink(dstIndex);</span>
<span class="line-modified">257     m_largeAllocationsNurseryOffset = m_largeAllocations.size();</span>
258 }
259 
260 void MarkedSpace::prepareForAllocation()
261 {
<span class="line-modified">262     ASSERT(!Thread::mayBeGCThread() || m_heap-&gt;worldIsStopped());</span>
263     for (Subspace* subspace : m_subspaces)
264         subspace-&gt;prepareForAllocation();
265 
266     m_activeWeakSets.takeFrom(m_newActiveWeakSets);
267 
<span class="line-modified">268     if (m_heap-&gt;collectionScope() == CollectionScope::Eden)</span>
<span class="line-modified">269         m_largeAllocationsNurseryOffsetForSweep = m_largeAllocationsNurseryOffset;</span>
270     else
<span class="line-modified">271         m_largeAllocationsNurseryOffsetForSweep = 0;</span>
<span class="line-modified">272     m_largeAllocationsNurseryOffset = m_largeAllocations.size();</span>







273 }
274 
275 void MarkedSpace::visitWeakSets(SlotVisitor&amp; visitor)
276 {
277     auto visit = [&amp;] (WeakSet* weakSet) {
278         weakSet-&gt;visit(visitor);
279     };
280 
281     m_newActiveWeakSets.forEach(visit);
282 
<span class="line-modified">283     if (m_heap-&gt;collectionScope() == CollectionScope::Full)</span>
284         m_activeWeakSets.forEach(visit);
285 }
286 
287 void MarkedSpace::reapWeakSets()
288 {
289     auto visit = [&amp;] (WeakSet* weakSet) {
290         weakSet-&gt;reap();
291     };
292 
293     m_newActiveWeakSets.forEach(visit);
294 
<span class="line-modified">295     if (m_heap-&gt;collectionScope() == CollectionScope::Full)</span>
296         m_activeWeakSets.forEach(visit);
297 }
298 
299 void MarkedSpace::stopAllocating()
300 {
301     ASSERT(!isIterating());
302     forEachDirectory(
303         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
304             directory.stopAllocating();
305             return IterationStatus::Continue;
306         });
307 }
308 
309 void MarkedSpace::stopAllocatingForGood()
310 {
311     ASSERT(!isIterating());
312     forEachDirectory(
313         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
314             directory.stopAllocatingForGood();
315             return IterationStatus::Continue;
316         });
317 }
318 
319 void MarkedSpace::prepareForConservativeScan()
320 {
<span class="line-modified">321     m_largeAllocationsForThisCollectionBegin = m_largeAllocations.begin() + m_largeAllocationsOffsetForThisCollection;</span>
<span class="line-modified">322     m_largeAllocationsForThisCollectionSize = m_largeAllocations.size() - m_largeAllocationsOffsetForThisCollection;</span>
<span class="line-modified">323     m_largeAllocationsForThisCollectionEnd = m_largeAllocations.end();</span>
<span class="line-modified">324     RELEASE_ASSERT(m_largeAllocationsForThisCollectionEnd == m_largeAllocationsForThisCollectionBegin + m_largeAllocationsForThisCollectionSize);</span>
325 
326     std::sort(
<span class="line-modified">327         m_largeAllocationsForThisCollectionBegin, m_largeAllocationsForThisCollectionEnd,</span>
<span class="line-modified">328         [&amp;] (LargeAllocation* a, LargeAllocation* b) {</span>
329             return a &lt; b;
330         });
<span class="line-modified">331     unsigned index = m_largeAllocationsOffsetForThisCollection;</span>
<span class="line-modified">332     for (auto* start = m_largeAllocationsForThisCollectionBegin; start != m_largeAllocationsForThisCollectionEnd; ++start, ++index) {</span>
333         (*start)-&gt;setIndexInSpace(index);
<span class="line-modified">334         ASSERT(m_largeAllocations[index] == *start);</span>
<span class="line-modified">335         ASSERT(m_largeAllocations[index]-&gt;indexInSpace() == index);</span>
336     }
337 }
338 
339 void MarkedSpace::prepareForMarking()
340 {
<span class="line-modified">341     if (m_heap-&gt;collectionScope() == CollectionScope::Eden)</span>
<span class="line-modified">342         m_largeAllocationsOffsetForThisCollection = m_largeAllocationsNurseryOffset;</span>
343     else
<span class="line-modified">344         m_largeAllocationsOffsetForThisCollection = 0;</span>
345 }
346 
347 void MarkedSpace::resumeAllocating()
348 {
349     forEachDirectory(
350         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
351             directory.resumeAllocating();
352             return IterationStatus::Continue;
353         });
<span class="line-modified">354     // Nothing to do for LargeAllocations.</span>
355 }
356 
357 bool MarkedSpace::isPagedOut(MonotonicTime deadline)
358 {
359     bool result = false;
360     forEachDirectory(
361         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
362             if (directory.isPagedOut(deadline)) {
363                 result = true;
364                 return IterationStatus::Done;
365             }
366             return IterationStatus::Continue;
367         });
<span class="line-modified">368     // FIXME: Consider taking LargeAllocations into account here.</span>
369     return result;
370 }
371 
372 void MarkedSpace::freeBlock(MarkedBlock::Handle* block)
373 {
374     block-&gt;directory()-&gt;removeBlock(block);
375     m_capacity -= MarkedBlock::blockSize;
376     m_blocks.remove(&amp;block-&gt;block());
377     delete block;
378 }
379 
380 void MarkedSpace::freeOrShrinkBlock(MarkedBlock::Handle* block)
381 {
382     if (!block-&gt;isEmpty()) {
383         block-&gt;shrink();
384         return;
385     }
386 
387     freeBlock(block);
388 }
389 
390 void MarkedSpace::shrink()
391 {
392     forEachDirectory(
393         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
394             directory.shrink();
395             return IterationStatus::Continue;
396         });
397 }
398 
399 void MarkedSpace::beginMarking()
400 {
<span class="line-modified">401     if (m_heap-&gt;collectionScope() == CollectionScope::Full) {</span>
402         forEachDirectory(
403             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
404                 directory.beginMarkingForFullCollection();
405                 return IterationStatus::Continue;
406             });
407 
408         if (UNLIKELY(nextVersion(m_markingVersion) == initialVersion)) {
409             forEachBlock(
410                 [&amp;] (MarkedBlock::Handle* handle) {
411                     handle-&gt;block().resetMarks();
412                 });
413         }
414 
415         m_markingVersion = nextVersion(m_markingVersion);
416 
<span class="line-modified">417         for (LargeAllocation* allocation : m_largeAllocations)</span>
418             allocation-&gt;flip();
419     }
420 
<span class="line-modified">421     if (!ASSERT_DISABLED) {</span>
422         forEachBlock(
423             [&amp;] (MarkedBlock::Handle* block) {
424                 if (block-&gt;areMarksStale())
425                     return;
426                 ASSERT(!block-&gt;isFreeListed());
427             });
428     }
429 
430     m_isMarking = true;
431 }
432 
433 void MarkedSpace::endMarking()
434 {
435     if (UNLIKELY(nextVersion(m_newlyAllocatedVersion) == initialVersion)) {
436         forEachBlock(
437             [&amp;] (MarkedBlock::Handle* handle) {
438                 handle-&gt;block().resetAllocated();
439             });
440     }
441 
442     m_newlyAllocatedVersion = nextVersion(m_newlyAllocatedVersion);
443 
<span class="line-modified">444     for (unsigned i = m_largeAllocationsOffsetForThisCollection; i &lt; m_largeAllocations.size(); ++i)</span>
<span class="line-modified">445         m_largeAllocations[i]-&gt;clearNewlyAllocated();</span>
446 
<span class="line-modified">447     if (!ASSERT_DISABLED) {</span>
<span class="line-modified">448         for (LargeAllocation* allocation : m_largeAllocations)</span>
449             ASSERT_UNUSED(allocation, !allocation-&gt;isNewlyAllocated());
450     }
451 
452     forEachDirectory(
453         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
454             directory.endMarking();
455             return IterationStatus::Continue;
456         });
457 
458     m_isMarking = false;
459 }
460 
461 void MarkedSpace::willStartIterating()
462 {
463     ASSERT(!isIterating());
464     stopAllocating();
465     m_isIterating = true;
466 }
467 
468 void MarkedSpace::didFinishIterating()
469 {
470     ASSERT(isIterating());
471     resumeAllocating();
472     m_isIterating = false;
473 }
474 
475 size_t MarkedSpace::objectCount()
476 {
477     size_t result = 0;
478     forEachBlock(
479         [&amp;] (MarkedBlock::Handle* block) {
480             result += block-&gt;markCount();
481         });
<span class="line-modified">482     for (LargeAllocation* allocation : m_largeAllocations) {</span>
483         if (allocation-&gt;isMarked())
484             result++;
485     }
486     return result;
487 }
488 
489 size_t MarkedSpace::size()
490 {
491     size_t result = 0;
492     forEachBlock(
493         [&amp;] (MarkedBlock::Handle* block) {
494             result += block-&gt;markCount() * block-&gt;cellSize();
495         });
<span class="line-modified">496     for (LargeAllocation* allocation : m_largeAllocations) {</span>
497         if (allocation-&gt;isMarked())
498             result += allocation-&gt;cellSize();
499     }
500     return result;
501 }
502 
503 size_t MarkedSpace::capacity()
504 {
505     return m_capacity;
506 }
507 
508 void MarkedSpace::addActiveWeakSet(WeakSet* weakSet)
509 {
510     // We conservatively assume that the WeakSet should belong in the new set. In fact, some weak
511     // sets might contain new weak handles even though they are tied to old objects. This slightly
512     // increases the amount of scanning that an eden collection would have to do, but the effect
513     // ought to be small.
514     m_newActiveWeakSets.append(weakSet);
515 }
516 
517 void MarkedSpace::didAddBlock(MarkedBlock::Handle* block)
518 {
519     // WARNING: This function is called before block is fully initialized. The block will not know
520     // its cellSize() or attributes(). The latter implies that you can&#39;t ask things like
521     // needsDestruction().
522     m_capacity += MarkedBlock::blockSize;
523     m_blocks.add(&amp;block-&gt;block());
524 }
525 
526 void MarkedSpace::didAllocateInBlock(MarkedBlock::Handle* block)
527 {
528     if (block-&gt;weakSet().isOnList()) {
529         block-&gt;weakSet().remove();
530         m_newActiveWeakSets.append(&amp;block-&gt;weakSet());
531     }
532 }
533 
534 void MarkedSpace::snapshotUnswept()
535 {
<span class="line-modified">536     if (m_heap-&gt;collectionScope() == CollectionScope::Eden) {</span>
537         forEachDirectory(
538             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
539                 directory.snapshotUnsweptForEdenCollection();
540                 return IterationStatus::Continue;
541             });
542     } else {
543         forEachDirectory(
544             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
545                 directory.snapshotUnsweptForFullCollection();
546                 return IterationStatus::Continue;
547             });
548     }
549 }
550 
551 void MarkedSpace::assertNoUnswept()
552 {
<span class="line-modified">553     if (ASSERT_DISABLED)</span>
554         return;
555     forEachDirectory(
556         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
557             directory.assertNoUnswept();
558             return IterationStatus::Continue;
559         });
560 }
561 
562 void MarkedSpace::dumpBits(PrintStream&amp; out)
563 {
564     forEachDirectory(
565         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
566             out.print(&quot;Bits for &quot;, directory, &quot;:\n&quot;);
567             directory.dumpBits(out);
568             return IterationStatus::Continue;
569         });
570 }
571 
572 void MarkedSpace::addBlockDirectory(const AbstractLocker&amp;, BlockDirectory* directory)
573 {
</pre>
</td>
<td>
<hr />
<pre>
 10  *  This library is distributed in the hope that it will be useful,
 11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 13  *  Lesser General Public License for more details.
 14  *
 15  *  You should have received a copy of the GNU Lesser General Public
 16  *  License along with this library; if not, write to the Free Software
 17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 18  *
 19  */
 20 
 21 #include &quot;config.h&quot;
 22 #include &quot;MarkedSpace.h&quot;
 23 
 24 #include &quot;BlockDirectoryInlines.h&quot;
 25 #include &quot;FunctionCodeBlock.h&quot;
 26 #include &quot;IncrementalSweeper.h&quot;
 27 #include &quot;JSObject.h&quot;
 28 #include &quot;JSCInlines.h&quot;
 29 #include &quot;MarkedBlockInlines.h&quot;
<span class="line-added"> 30 #include &quot;MarkedSpaceInlines.h&quot;</span>
 31 #include &lt;wtf/ListDump.h&gt;
 32 
 33 namespace JSC {
 34 
 35 std::array&lt;size_t, MarkedSpace::numSizeClasses&gt; MarkedSpace::s_sizeClassForSizeStep;
 36 
 37 namespace {
 38 
 39 const Vector&lt;size_t&gt;&amp; sizeClasses()
 40 {
 41     static Vector&lt;size_t&gt;* result;
 42     static std::once_flag once;
 43     std::call_once(
 44         once,
 45         [] {
 46             result = new Vector&lt;size_t&gt;();
 47 
<span class="line-modified"> 48             if (UNLIKELY(Options::dumpSizeClasses())) {</span>
 49                 dataLog(&quot;Block size: &quot;, MarkedBlock::blockSize, &quot;\n&quot;);
 50                 dataLog(&quot;Footer size: &quot;, sizeof(MarkedBlock::Footer), &quot;\n&quot;);
 51             }
 52 
 53             auto add = [&amp;] (size_t sizeClass) {
 54                 sizeClass = WTF::roundUpToMultipleOf&lt;MarkedBlock::atomSize&gt;(sizeClass);
<span class="line-modified"> 55                 dataLogLnIf(Options::dumpSizeClasses(), &quot;Adding JSC MarkedSpace size class: &quot;, sizeClass);</span>

 56                 // Perform some validation as we go.
 57                 RELEASE_ASSERT(!(sizeClass % MarkedSpace::sizeStep));
 58                 if (result-&gt;isEmpty())
 59                     RELEASE_ASSERT(sizeClass == MarkedSpace::sizeStep);
 60                 result-&gt;append(sizeClass);
 61             };
 62 
 63             // This is a definition of the size classes in our GC. It must define all of the
 64             // size classes from sizeStep up to largeCutoff.
 65 
 66             // Have very precise size classes for the small stuff. This is a loop to make it easy to reduce
 67             // atomSize.
 68             for (size_t size = MarkedSpace::sizeStep; size &lt; MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep)
 69                 add(size);
 70 
 71             // We want to make sure that the remaining size classes minimize internal fragmentation (i.e.
 72             // the wasted space at the tail end of a MarkedBlock) while proceeding roughly in an exponential
 73             // way starting at just above the precise size classes to four cells per block.
 74 
<span class="line-modified"> 75             dataLogLnIf(Options::dumpSizeClasses(), &quot;    Marked block payload size: &quot;, static_cast&lt;size_t&gt;(MarkedSpace::blockPayload));</span>

 76 
 77             for (unsigned i = 0; ; ++i) {
 78                 double approximateSize = MarkedSpace::preciseCutoff * pow(Options::sizeClassProgression(), i);
<span class="line-modified"> 79                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Next size class as a double: &quot;, approximateSize);</span>


 80 
 81                 size_t approximateSizeInBytes = static_cast&lt;size_t&gt;(approximateSize);
<span class="line-modified"> 82                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Next size class as bytes: &quot;, approximateSizeInBytes);</span>


 83 
 84                 // Make sure that the computer did the math correctly.
 85                 RELEASE_ASSERT(approximateSizeInBytes &gt;= MarkedSpace::preciseCutoff);
 86 
 87                 if (approximateSizeInBytes &gt; MarkedSpace::largeCutoff)
 88                     break;
 89 
 90                 size_t sizeClass =
 91                     WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(approximateSizeInBytes);
<span class="line-modified"> 92                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Size class: &quot;, sizeClass);</span>


 93 
 94                 // Optimize the size class so that there isn&#39;t any slop at the end of the block&#39;s
 95                 // payload.
 96                 unsigned cellsPerBlock = MarkedSpace::blockPayload / sizeClass;
 97                 size_t possiblyBetterSizeClass = (MarkedSpace::blockPayload / cellsPerBlock) &amp; ~(MarkedSpace::sizeStep - 1);
<span class="line-modified"> 98                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Possibly better size class: &quot;, possiblyBetterSizeClass);</span>


 99 
100                 // The size class we just came up with is better than the other one if it reduces
101                 // total wastage assuming we only allocate cells of that size.
102                 size_t originalWastage = MarkedSpace::blockPayload - cellsPerBlock * sizeClass;
103                 size_t newWastage = (possiblyBetterSizeClass - sizeClass) * cellsPerBlock;
<span class="line-modified">104                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Original wastage: &quot;, originalWastage, &quot;, new wastage: &quot;, newWastage);</span>


105 
106                 size_t betterSizeClass;
107                 if (newWastage &gt; originalWastage)
108                     betterSizeClass = sizeClass;
109                 else
110                     betterSizeClass = possiblyBetterSizeClass;
111 
<span class="line-modified">112                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Choosing size class: &quot;, betterSizeClass);</span>

113 
114                 if (betterSizeClass == result-&gt;last()) {
115                     // Defense for when expStep is small.
116                     continue;
117                 }
118 
119                 // This is usually how we get out of the loop.
120                 if (betterSizeClass &gt; MarkedSpace::largeCutoff
<span class="line-modified">121                     || betterSizeClass &gt; Options::preciseAllocationCutoff())</span>
122                     break;
123 
124                 add(betterSizeClass);
125             }
126 
127             // Manually inject size classes for objects we know will be allocated in high volume.
128             // FIXME: All of these things should have IsoSubspaces.
129             // https://bugs.webkit.org/show_bug.cgi?id=179876
<span class="line-modified">130             add(256);</span>

131 
132             {
133                 // Sort and deduplicate.
134                 std::sort(result-&gt;begin(), result-&gt;end());
135                 auto it = std::unique(result-&gt;begin(), result-&gt;end());
136                 result-&gt;shrinkCapacity(it - result-&gt;begin());
137             }
138 
<span class="line-modified">139             dataLogLnIf(Options::dumpSizeClasses(), &quot;JSC Heap MarkedSpace size class dump: &quot;, listDump(*result));</span>

140 
141             // We have an optimiation in MarkedSpace::optimalSizeFor() that assumes things about
142             // the size class table. This checks our results against that function&#39;s assumptions.
143             for (size_t size = MarkedSpace::sizeStep, i = 0; size &lt;= MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep, i++)
144                 RELEASE_ASSERT(result-&gt;at(i) == size);
145         });
146     return *result;
147 }
148 
149 template&lt;typename TableType, typename SizeClassCons, typename DefaultCons&gt;
150 void buildSizeClassTable(TableType&amp; table, const SizeClassCons&amp; cons, const DefaultCons&amp; defaultCons)
151 {
152     size_t nextIndex = 0;
153     for (size_t sizeClass : sizeClasses()) {
154         auto entry = cons(sizeClass);
155         size_t index = MarkedSpace::sizeClassToIndex(sizeClass);
156         for (size_t i = nextIndex; i &lt;= index; ++i)
157             table[i] = entry;
158         nextIndex = index + 1;
159     }
</pre>
<hr />
<pre>
165 } // anonymous namespace
166 
167 void MarkedSpace::initializeSizeClassForStepSize()
168 {
169     static std::once_flag flag;
170     std::call_once(
171         flag,
172         [] {
173             buildSizeClassTable(
174                 s_sizeClassForSizeStep,
175                 [&amp;] (size_t sizeClass) -&gt; size_t {
176                     return sizeClass;
177                 },
178                 [&amp;] (size_t sizeClass) -&gt; size_t {
179                     return sizeClass;
180                 });
181         });
182 }
183 
184 MarkedSpace::MarkedSpace(Heap* heap)

185 {
<span class="line-added">186     ASSERT_UNUSED(heap, heap == &amp;this-&gt;heap());</span>
187     initializeSizeClassForStepSize();
188 }
189 
190 MarkedSpace::~MarkedSpace()
191 {
192     ASSERT(!m_blocks.set().size());
193 }
194 
195 void MarkedSpace::freeMemory()
196 {
197     forEachBlock(
198         [&amp;] (MarkedBlock::Handle* block) {
199             freeBlock(block);
200         });
<span class="line-modified">201     for (PreciseAllocation* allocation : m_preciseAllocations)</span>
202         allocation-&gt;destroy();
<span class="line-added">203     forEachSubspace([&amp;](Subspace&amp; subspace) {</span>
<span class="line-added">204         if (subspace.isIsoSubspace())</span>
<span class="line-added">205             static_cast&lt;IsoSubspace&amp;&gt;(subspace).destroyLowerTierFreeList();</span>
<span class="line-added">206         return IterationStatus::Continue;</span>
<span class="line-added">207     });</span>
208 }
209 
210 void MarkedSpace::lastChanceToFinalize()
211 {
212     forEachDirectory(
213         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
214             directory.lastChanceToFinalize();
215             return IterationStatus::Continue;
216         });
<span class="line-modified">217     for (PreciseAllocation* allocation : m_preciseAllocations)</span>
218         allocation-&gt;lastChanceToFinalize();
<span class="line-added">219     // We do not call lastChanceToFinalize for lower-tier swept cells since we need nothing to do.</span>
220 }
221 
<span class="line-modified">222 void MarkedSpace::sweepBlocks()</span>
223 {
<span class="line-modified">224     heap().sweeper().stopSweeping();</span>
225     forEachDirectory(
226         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
227             directory.sweep();
228             return IterationStatus::Continue;
229         });
230 }
231 
<span class="line-modified">232 void MarkedSpace::sweepPreciseAllocations()</span>
233 {
<span class="line-modified">234     RELEASE_ASSERT(m_preciseAllocationsNurseryOffset == m_preciseAllocations.size());</span>
<span class="line-modified">235     unsigned srcIndex = m_preciseAllocationsNurseryOffsetForSweep;</span>
236     unsigned dstIndex = srcIndex;
<span class="line-modified">237     while (srcIndex &lt; m_preciseAllocations.size()) {</span>
<span class="line-modified">238         PreciseAllocation* allocation = m_preciseAllocations[srcIndex++];</span>
239         allocation-&gt;sweep();
240         if (allocation-&gt;isEmpty()) {
<span class="line-modified">241             if (auto* set = preciseAllocationSet())</span>
<span class="line-modified">242                 set-&gt;remove(allocation-&gt;cell());</span>
<span class="line-added">243             if (allocation-&gt;isLowerTier())</span>
<span class="line-added">244                 static_cast&lt;IsoSubspace*&gt;(allocation-&gt;subspace())-&gt;sweepLowerTierCell(allocation);</span>
<span class="line-added">245             else {</span>
<span class="line-added">246                 m_capacity -= allocation-&gt;cellSize();</span>
<span class="line-added">247                 allocation-&gt;destroy();</span>
<span class="line-added">248             }</span>
249             continue;
250         }
251         allocation-&gt;setIndexInSpace(dstIndex);
<span class="line-modified">252         m_preciseAllocations[dstIndex++] = allocation;</span>
253     }
<span class="line-modified">254     m_preciseAllocations.shrink(dstIndex);</span>
<span class="line-modified">255     m_preciseAllocationsNurseryOffset = m_preciseAllocations.size();</span>
256 }
257 
258 void MarkedSpace::prepareForAllocation()
259 {
<span class="line-modified">260     ASSERT(!Thread::mayBeGCThread() || heap().worldIsStopped());</span>
261     for (Subspace* subspace : m_subspaces)
262         subspace-&gt;prepareForAllocation();
263 
264     m_activeWeakSets.takeFrom(m_newActiveWeakSets);
265 
<span class="line-modified">266     if (heap().collectionScope() == CollectionScope::Eden)</span>
<span class="line-modified">267         m_preciseAllocationsNurseryOffsetForSweep = m_preciseAllocationsNurseryOffset;</span>
268     else
<span class="line-modified">269         m_preciseAllocationsNurseryOffsetForSweep = 0;</span>
<span class="line-modified">270     m_preciseAllocationsNurseryOffset = m_preciseAllocations.size();</span>
<span class="line-added">271 }</span>
<span class="line-added">272 </span>
<span class="line-added">273 void MarkedSpace::enablePreciseAllocationTracking()</span>
<span class="line-added">274 {</span>
<span class="line-added">275     m_preciseAllocationSet = makeUnique&lt;HashSet&lt;HeapCell*&gt;&gt;();</span>
<span class="line-added">276     for (auto* allocation : m_preciseAllocations)</span>
<span class="line-added">277         m_preciseAllocationSet-&gt;add(allocation-&gt;cell());</span>
278 }
279 
280 void MarkedSpace::visitWeakSets(SlotVisitor&amp; visitor)
281 {
282     auto visit = [&amp;] (WeakSet* weakSet) {
283         weakSet-&gt;visit(visitor);
284     };
285 
286     m_newActiveWeakSets.forEach(visit);
287 
<span class="line-modified">288     if (heap().collectionScope() == CollectionScope::Full)</span>
289         m_activeWeakSets.forEach(visit);
290 }
291 
292 void MarkedSpace::reapWeakSets()
293 {
294     auto visit = [&amp;] (WeakSet* weakSet) {
295         weakSet-&gt;reap();
296     };
297 
298     m_newActiveWeakSets.forEach(visit);
299 
<span class="line-modified">300     if (heap().collectionScope() == CollectionScope::Full)</span>
301         m_activeWeakSets.forEach(visit);
302 }
303 
304 void MarkedSpace::stopAllocating()
305 {
306     ASSERT(!isIterating());
307     forEachDirectory(
308         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
309             directory.stopAllocating();
310             return IterationStatus::Continue;
311         });
312 }
313 
314 void MarkedSpace::stopAllocatingForGood()
315 {
316     ASSERT(!isIterating());
317     forEachDirectory(
318         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
319             directory.stopAllocatingForGood();
320             return IterationStatus::Continue;
321         });
322 }
323 
324 void MarkedSpace::prepareForConservativeScan()
325 {
<span class="line-modified">326     m_preciseAllocationsForThisCollectionBegin = m_preciseAllocations.begin() + m_preciseAllocationsOffsetForThisCollection;</span>
<span class="line-modified">327     m_preciseAllocationsForThisCollectionSize = m_preciseAllocations.size() - m_preciseAllocationsOffsetForThisCollection;</span>
<span class="line-modified">328     m_preciseAllocationsForThisCollectionEnd = m_preciseAllocations.end();</span>
<span class="line-modified">329     RELEASE_ASSERT(m_preciseAllocationsForThisCollectionEnd == m_preciseAllocationsForThisCollectionBegin + m_preciseAllocationsForThisCollectionSize);</span>
330 
331     std::sort(
<span class="line-modified">332         m_preciseAllocationsForThisCollectionBegin, m_preciseAllocationsForThisCollectionEnd,</span>
<span class="line-modified">333         [&amp;] (PreciseAllocation* a, PreciseAllocation* b) {</span>
334             return a &lt; b;
335         });
<span class="line-modified">336     unsigned index = m_preciseAllocationsOffsetForThisCollection;</span>
<span class="line-modified">337     for (auto* start = m_preciseAllocationsForThisCollectionBegin; start != m_preciseAllocationsForThisCollectionEnd; ++start, ++index) {</span>
338         (*start)-&gt;setIndexInSpace(index);
<span class="line-modified">339         ASSERT(m_preciseAllocations[index] == *start);</span>
<span class="line-modified">340         ASSERT(m_preciseAllocations[index]-&gt;indexInSpace() == index);</span>
341     }
342 }
343 
344 void MarkedSpace::prepareForMarking()
345 {
<span class="line-modified">346     if (heap().collectionScope() == CollectionScope::Eden)</span>
<span class="line-modified">347         m_preciseAllocationsOffsetForThisCollection = m_preciseAllocationsNurseryOffset;</span>
348     else
<span class="line-modified">349         m_preciseAllocationsOffsetForThisCollection = 0;</span>
350 }
351 
352 void MarkedSpace::resumeAllocating()
353 {
354     forEachDirectory(
355         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
356             directory.resumeAllocating();
357             return IterationStatus::Continue;
358         });
<span class="line-modified">359     // Nothing to do for PreciseAllocations.</span>
360 }
361 
362 bool MarkedSpace::isPagedOut(MonotonicTime deadline)
363 {
364     bool result = false;
365     forEachDirectory(
366         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
367             if (directory.isPagedOut(deadline)) {
368                 result = true;
369                 return IterationStatus::Done;
370             }
371             return IterationStatus::Continue;
372         });
<span class="line-modified">373     // FIXME: Consider taking PreciseAllocations into account here.</span>
374     return result;
375 }
376 
377 void MarkedSpace::freeBlock(MarkedBlock::Handle* block)
378 {
379     block-&gt;directory()-&gt;removeBlock(block);
380     m_capacity -= MarkedBlock::blockSize;
381     m_blocks.remove(&amp;block-&gt;block());
382     delete block;
383 }
384 
385 void MarkedSpace::freeOrShrinkBlock(MarkedBlock::Handle* block)
386 {
387     if (!block-&gt;isEmpty()) {
388         block-&gt;shrink();
389         return;
390     }
391 
392     freeBlock(block);
393 }
394 
395 void MarkedSpace::shrink()
396 {
397     forEachDirectory(
398         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
399             directory.shrink();
400             return IterationStatus::Continue;
401         });
402 }
403 
404 void MarkedSpace::beginMarking()
405 {
<span class="line-modified">406     if (heap().collectionScope() == CollectionScope::Full) {</span>
407         forEachDirectory(
408             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
409                 directory.beginMarkingForFullCollection();
410                 return IterationStatus::Continue;
411             });
412 
413         if (UNLIKELY(nextVersion(m_markingVersion) == initialVersion)) {
414             forEachBlock(
415                 [&amp;] (MarkedBlock::Handle* handle) {
416                     handle-&gt;block().resetMarks();
417                 });
418         }
419 
420         m_markingVersion = nextVersion(m_markingVersion);
421 
<span class="line-modified">422         for (PreciseAllocation* allocation : m_preciseAllocations)</span>
423             allocation-&gt;flip();
424     }
425 
<span class="line-modified">426     if (ASSERT_ENABLED) {</span>
427         forEachBlock(
428             [&amp;] (MarkedBlock::Handle* block) {
429                 if (block-&gt;areMarksStale())
430                     return;
431                 ASSERT(!block-&gt;isFreeListed());
432             });
433     }
434 
435     m_isMarking = true;
436 }
437 
438 void MarkedSpace::endMarking()
439 {
440     if (UNLIKELY(nextVersion(m_newlyAllocatedVersion) == initialVersion)) {
441         forEachBlock(
442             [&amp;] (MarkedBlock::Handle* handle) {
443                 handle-&gt;block().resetAllocated();
444             });
445     }
446 
447     m_newlyAllocatedVersion = nextVersion(m_newlyAllocatedVersion);
448 
<span class="line-modified">449     for (unsigned i = m_preciseAllocationsOffsetForThisCollection; i &lt; m_preciseAllocations.size(); ++i)</span>
<span class="line-modified">450         m_preciseAllocations[i]-&gt;clearNewlyAllocated();</span>
451 
<span class="line-modified">452     if (ASSERT_ENABLED) {</span>
<span class="line-modified">453         for (PreciseAllocation* allocation : m_preciseAllocations)</span>
454             ASSERT_UNUSED(allocation, !allocation-&gt;isNewlyAllocated());
455     }
456 
457     forEachDirectory(
458         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
459             directory.endMarking();
460             return IterationStatus::Continue;
461         });
462 
463     m_isMarking = false;
464 }
465 
466 void MarkedSpace::willStartIterating()
467 {
468     ASSERT(!isIterating());
469     stopAllocating();
470     m_isIterating = true;
471 }
472 
473 void MarkedSpace::didFinishIterating()
474 {
475     ASSERT(isIterating());
476     resumeAllocating();
477     m_isIterating = false;
478 }
479 
480 size_t MarkedSpace::objectCount()
481 {
482     size_t result = 0;
483     forEachBlock(
484         [&amp;] (MarkedBlock::Handle* block) {
485             result += block-&gt;markCount();
486         });
<span class="line-modified">487     for (PreciseAllocation* allocation : m_preciseAllocations) {</span>
488         if (allocation-&gt;isMarked())
489             result++;
490     }
491     return result;
492 }
493 
494 size_t MarkedSpace::size()
495 {
496     size_t result = 0;
497     forEachBlock(
498         [&amp;] (MarkedBlock::Handle* block) {
499             result += block-&gt;markCount() * block-&gt;cellSize();
500         });
<span class="line-modified">501     for (PreciseAllocation* allocation : m_preciseAllocations) {</span>
502         if (allocation-&gt;isMarked())
503             result += allocation-&gt;cellSize();
504     }
505     return result;
506 }
507 
508 size_t MarkedSpace::capacity()
509 {
510     return m_capacity;
511 }
512 
513 void MarkedSpace::addActiveWeakSet(WeakSet* weakSet)
514 {
515     // We conservatively assume that the WeakSet should belong in the new set. In fact, some weak
516     // sets might contain new weak handles even though they are tied to old objects. This slightly
517     // increases the amount of scanning that an eden collection would have to do, but the effect
518     // ought to be small.
519     m_newActiveWeakSets.append(weakSet);
520 }
521 
522 void MarkedSpace::didAddBlock(MarkedBlock::Handle* block)
523 {
524     // WARNING: This function is called before block is fully initialized. The block will not know
525     // its cellSize() or attributes(). The latter implies that you can&#39;t ask things like
526     // needsDestruction().
527     m_capacity += MarkedBlock::blockSize;
528     m_blocks.add(&amp;block-&gt;block());
529 }
530 
531 void MarkedSpace::didAllocateInBlock(MarkedBlock::Handle* block)
532 {
533     if (block-&gt;weakSet().isOnList()) {
534         block-&gt;weakSet().remove();
535         m_newActiveWeakSets.append(&amp;block-&gt;weakSet());
536     }
537 }
538 
539 void MarkedSpace::snapshotUnswept()
540 {
<span class="line-modified">541     if (heap().collectionScope() == CollectionScope::Eden) {</span>
542         forEachDirectory(
543             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
544                 directory.snapshotUnsweptForEdenCollection();
545                 return IterationStatus::Continue;
546             });
547     } else {
548         forEachDirectory(
549             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
550                 directory.snapshotUnsweptForFullCollection();
551                 return IterationStatus::Continue;
552             });
553     }
554 }
555 
556 void MarkedSpace::assertNoUnswept()
557 {
<span class="line-modified">558     if (!ASSERT_ENABLED)</span>
559         return;
560     forEachDirectory(
561         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
562             directory.assertNoUnswept();
563             return IterationStatus::Continue;
564         });
565 }
566 
567 void MarkedSpace::dumpBits(PrintStream&amp; out)
568 {
569     forEachDirectory(
570         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
571             out.print(&quot;Bits for &quot;, directory, &quot;:\n&quot;);
572             directory.dumpBits(out);
573             return IterationStatus::Continue;
574         });
575 }
576 
577 void MarkedSpace::addBlockDirectory(const AbstractLocker&amp;, BlockDirectory* directory)
578 {
</pre>
</td>
</tr>
</table>
<center><a href="MarkedBlockInlines.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedSpace.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>