<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/WebAssembly.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 # Copyright (C) 2019 Apple Inc. All rights reserved.
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 # Calling conventions
  25 const CalleeSaveSpaceAsVirtualRegisters = constexpr Wasm::numberOfLLIntCalleeSaveRegisters
  26 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
  27 const WasmEntryPtrTag = constexpr WasmEntryPtrTag
  28 
  29 if HAVE_FAST_TLS
  30     const WTF_WASM_CONTEXT_KEY = constexpr WTF_WASM_CONTEXT_KEY
  31 end
  32 
  33 if X86_64
  34     const NumberOfWasmArgumentGPRs = 6
  35 elsif ARM64 or ARM64E
  36     const NumberOfWasmArgumentGPRs = 8
  37 else
  38     error
  39 end
  40 const NumberOfWasmArgumentFPRs = 8
  41 const NumberOfWasmArguments = NumberOfWasmArgumentGPRs + NumberOfWasmArgumentFPRs
  42 
  43 # All callee saves must match the definition in WasmCallee.cpp
  44 
  45 # These must match the definition in WasmMemoryInformation.cpp
  46 const wasmInstance = csr0
  47 const memoryBase = csr3
  48 const memorySize = csr4
  49 
  50 # This must match the definition in LowLevelInterpreter.asm
  51 if X86_64
  52     const PB = csr2
  53 elsif ARM64 or ARM64E
  54     const PB = csr7
  55 else
  56     error
  57 end
  58 
  59 macro forEachArgumentGPR(fn)
  60     fn(0 * 8, wa0)
  61     fn(1 * 8, wa1)
  62     fn(2 * 8, wa2)
  63     fn(3 * 8, wa3)
  64     fn(4 * 8, wa4)
  65     fn(5 * 8, wa5)
  66     if ARM64 or ARM64E
  67         fn(6 * 8, wa6)
  68         fn(7 * 8, wa7)
  69     end
  70 end
  71 
  72 macro forEachArgumentFPR(fn)
  73     fn((NumberOfWasmArgumentGPRs + 0) * 8, wfa0)
  74     fn((NumberOfWasmArgumentGPRs + 1) * 8, wfa1)
  75     fn((NumberOfWasmArgumentGPRs + 2) * 8, wfa2)
  76     fn((NumberOfWasmArgumentGPRs + 3) * 8, wfa3)
  77     fn((NumberOfWasmArgumentGPRs + 4) * 8, wfa4)
  78     fn((NumberOfWasmArgumentGPRs + 5) * 8, wfa5)
  79     fn((NumberOfWasmArgumentGPRs + 6) * 8, wfa6)
  80     fn((NumberOfWasmArgumentGPRs + 7) * 8, wfa7)
  81 end
  82 
  83 # Helper macros
  84 # FIXME: Eventually this should be unified with the JS versions
  85 # https://bugs.webkit.org/show_bug.cgi?id=203656
  86 
  87 macro wasmDispatch(advanceReg)
  88     addp advanceReg, PC
  89     wasmNextInstruction()
  90 end
  91 
  92 macro wasmDispatchIndirect(offsetReg)
  93     wasmDispatch(offsetReg)
  94 end
  95 
  96 macro wasmNextInstruction()
  97     loadb [PB, PC, 1], t0
  98     leap _g_opcodeMap, t1
  99     jmp NumberOfJSOpcodeIDs * PtrSize[t1, t0, PtrSize], BytecodePtrTag
 100 end
 101 
 102 macro wasmNextInstructionWide16()
 103     loadb OpcodeIDNarrowSize[PB, PC, 1], t0
 104     leap _g_opcodeMapWide16, t1
 105     jmp NumberOfJSOpcodeIDs * PtrSize[t1, t0, PtrSize], BytecodePtrTag
 106 end
 107 
 108 macro wasmNextInstructionWide32()
 109     loadb OpcodeIDNarrowSize[PB, PC, 1], t0
 110     leap _g_opcodeMapWide32, t1
 111     jmp NumberOfJSOpcodeIDs * PtrSize[t1, t0, PtrSize], BytecodePtrTag
 112 end
 113 
 114 macro checkSwitchToJIT(increment, action)
 115     loadp CodeBlock[cfr], ws0
 116     baddis increment, Wasm::FunctionCodeBlock::m_tierUpCounter + Wasm::LLIntTierUpCounter::m_counter[ws0], .continue
 117     action()
 118     .continue:
 119 end
 120 
 121 macro checkSwitchToJITForPrologue(codeBlockRegister)
 122     checkSwitchToJIT(
 123         5,
 124         macro()
 125             move cfr, a0
 126             move PC, a1
 127             move wasmInstance, a2
 128             cCall4(_slow_path_wasm_prologue_osr)
 129             btpz r0, .recover
 130             move r0, ws0
 131 
 132             forEachArgumentGPR(macro (offset, gpr)
 133                 loadq -offset - 8 - CalleeSaveSpaceAsVirtualRegisters * 8[cfr], gpr
 134             end)
 135 
 136             forEachArgumentFPR(macro (offset, fpr)
 137                 loadd -offset - 8 - CalleeSaveSpaceAsVirtualRegisters * 8[cfr], fpr
 138             end)
 139 
 140             restoreCalleeSavesUsedByWasm()
 141             restoreCallerPCAndCFR()
 142             untagReturnAddress sp
 143             jmp ws0, WasmEntryPtrTag
 144         .recover:
 145             notFunctionCodeBlockGetter(codeBlockRegister)
 146         end)
 147 end
 148 
 149 macro checkSwitchToJITForLoop()
 150     checkSwitchToJIT(
 151         1,
 152         macro()
 153             storei PC, ArgumentCountIncludingThis + TagOffset[cfr]
 154             prepareStateForCCall()
 155             move cfr, a0
 156             move PC, a1
 157             move wasmInstance, a2
 158             cCall4(_slow_path_wasm_loop_osr)
 159             btpz r1, .recover
 160             restoreCalleeSavesUsedByWasm()
 161             restoreCallerPCAndCFR()
 162             untagReturnAddress sp
 163             move r0, a0
 164             jmp r1, WasmEntryPtrTag
 165         .recover:
 166             loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
 167         end)
 168 end
 169 
 170 macro checkSwitchToJITForEpilogue()
 171     checkSwitchToJIT(
 172         10,
 173         macro ()
 174             callWasmSlowPath(_slow_path_wasm_epilogue_osr)
 175         end)
 176 end
 177 
 178 # Wasm specific helpers
 179 
 180 macro preserveCalleeSavesUsedByWasm()
 181     # NOTE: We intentionally don&#39;t save memoryBase and memorySize here. See the comment
 182     # in restoreCalleeSavesUsedByWasm() below for why.
 183     subp CalleeSaveSpaceStackAligned, sp
 184     if ARM64 or ARM64E
 185         emit &quot;stp x19, x26, [x29, #-16]&quot;
 186     elsif X86_64
 187         storep PB, -0x8[cfr]
 188         storep wasmInstance, -0x10[cfr]
 189     else
 190         error
 191     end
 192 end
 193 
 194 macro restoreCalleeSavesUsedByWasm()
 195     # NOTE: We intentionally don&#39;t restore memoryBase and memorySize here. These are saved
 196     # and restored when entering Wasm by the JSToWasm wrapper and changes to them are meant
 197     # to be observable within the same Wasm module.
 198     if ARM64 or ARM64E
 199         emit &quot;ldp x19, x26, [x29, #-16]&quot;
 200     elsif X86_64
 201         loadp -0x8[cfr], PB
 202         loadp -0x10[cfr], wasmInstance
 203     else
 204         error
 205     end
 206 end
 207 
 208 macro loadWasmInstanceFromTLS()
 209 if  HAVE_FAST_TLS
 210     tls_loadp WTF_WASM_CONTEXT_KEY, wasmInstance
 211 else
 212     crash()
 213 end
 214 end
 215 
 216 macro storeWasmInstanceToTLS(instance)
 217 if  HAVE_FAST_TLS
 218     tls_storep instance, WTF_WASM_CONTEXT_KEY
 219 else
 220     crash()
 221 end
 222 end
 223 
 224 macro reloadMemoryRegistersFromInstance(instance, scratch1, scratch2)
 225     loadp Wasm::Instance::m_cachedMemory[instance], memoryBase
 226     loadi Wasm::Instance::m_cachedMemorySize[instance], memorySize
 227     cagedPrimitive(memoryBase, memorySize, scratch1, scratch2)
 228 end
 229 
 230 macro throwException(exception)
 231     storei constexpr Wasm::ExceptionType::%exception%, ArgumentCountIncludingThis + PayloadOffset[cfr]
 232     jmp _wasm_throw_from_slow_path_trampoline
 233 end
 234 
 235 macro callWasmSlowPath(slowPath)
 236     prepareStateForCCall()
 237     move cfr, a0
 238     move PC, a1
 239     move wasmInstance, a2
 240     cCall4(slowPath)
 241     restoreStateAfterCCall()
 242 end
 243 
 244 macro callWasmCallSlowPath(slowPath, action)
 245     storei PC, ArgumentCountIncludingThis + TagOffset[cfr]
 246     prepareStateForCCall()
 247     move cfr, a0
 248     move PC, a1
 249     move wasmInstance, a2
 250     cCall4(slowPath)
 251     action(r0, r1)
 252 end
 253 
 254 macro wasmPrologue(codeBlockGetter, codeBlockSetter, loadWasmInstance)
 255     # Set up the call frame and check if we should OSR.
 256     tagReturnAddress sp
 257     preserveCallerPCAndCFR()
 258     preserveCalleeSavesUsedByWasm()
 259     loadWasmInstance()
 260     reloadMemoryRegistersFromInstance(wasmInstance, ws0, ws1)
 261 
 262     codeBlockGetter(ws0)
 263     codeBlockSetter(ws0)
 264 
 265     # Get new sp in ws1 and check stack height.
 266     loadi Wasm::FunctionCodeBlock::m_numCalleeLocals[ws0], ws1
 267     lshiftp 3, ws1
 268     addp maxFrameExtentForSlowPathCall, ws1
 269     subp cfr, ws1, ws1
 270 
 271     bpa ws1, cfr, .stackOverflow
 272     bpbeq Wasm::Instance::m_cachedStackLimit[wasmInstance], ws1, .stackHeightOK
 273 
 274 .stackOverflow:
 275     throwException(StackOverflow)
 276 
 277 .stackHeightOK:
 278     move ws1, sp
 279 
 280     forEachArgumentGPR(macro (offset, gpr)
 281         storeq gpr, -offset - 8 - CalleeSaveSpaceAsVirtualRegisters * 8[cfr]
 282     end)
 283 
 284     forEachArgumentFPR(macro (offset, fpr)
 285         stored fpr, -offset - 8 - CalleeSaveSpaceAsVirtualRegisters * 8[cfr]
 286     end)
 287 
 288     checkSwitchToJITForPrologue(ws0)
 289 
 290     # Set up the PC.
 291     loadp Wasm::FunctionCodeBlock::m_instructionsRawPointer[ws0], PB
 292     move 0, PC
 293 
 294     loadi Wasm::FunctionCodeBlock::m_numVars[ws0], ws1
 295     subi NumberOfWasmArguments + CalleeSaveSpaceAsVirtualRegisters, ws1
 296     btiz ws1, .zeroInitializeLocalsDone
 297     negi ws1
 298     sxi2q ws1, ws1
 299     leap (NumberOfWasmArguments + CalleeSaveSpaceAsVirtualRegisters + 1) * -8[cfr], ws0
 300 .zeroInitializeLocalsLoop:
 301     addq 1, ws1
 302     storeq 0, [ws0, ws1, 8]
 303     btqz ws1, .zeroInitializeLocalsDone
 304     jmp .zeroInitializeLocalsLoop
 305 .zeroInitializeLocalsDone:
 306 end
 307 
 308 macro traceExecution()
 309     if TRACING
 310         callWasmSlowPath(_slow_path_wasm_trace)
 311     end
 312 end
 313 
 314 # Less convenient, but required for opcodes that collide with reserved instructions (e.g. wasm_nop)
 315 macro unprefixedWasmOp(opcodeName, opcodeStruct, fn)
 316     commonOp(opcodeName, traceExecution, macro(size)
 317         fn(macro(fn2)
 318             fn2(opcodeName, opcodeStruct, size)
 319         end)
 320     end)
 321 end
 322 
 323 macro wasmOp(opcodeName, opcodeStruct, fn)
 324     unprefixedWasmOp(wasm_%opcodeName%, opcodeStruct, fn)
 325 end
 326 
 327 # Same as unprefixedWasmOp, necessary for e.g. wasm_call
 328 macro unprefixedSlowWasmOp(opcodeName)
 329     unprefixedWasmOp(opcodeName, unusedOpcodeStruct, macro(ctx)
 330         callWasmSlowPath(_slow_path_%opcodeName%)
 331         dispatch(ctx)
 332     end)
 333 end
 334 
 335 macro slowWasmOp(opcodeName)
 336     unprefixedSlowWasmOp(wasm_%opcodeName%)
 337 end
 338 
 339 # Macro version of load operations: mload[suffix]
 340 # loads field from the instruction stream and performs load[suffix] to dst
 341 macro firstConstantRegisterIndex(ctx, fn)
 342     ctx(macro(opcodeName, opcodeStruct, size)
 343         size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, fn)
 344     end)
 345 end
 346 
 347 macro loadConstantOrVariable(ctx, index, loader)
 348     firstConstantRegisterIndex(ctx, macro (firstConstantIndex)
 349         bpgteq index, firstConstantIndex, .constant
 350         loader([cfr, index, 8])
 351         jmp .done
 352     .constant:
 353         loadp CodeBlock[cfr], t6
 354         loadp Wasm::FunctionCodeBlock::m_constants + VectorBufferOffset[t6], t6
 355         subp firstConstantIndex, index
 356         loader([t6, index, 8])
 357     .done:
 358     end)
 359 end
 360 
 361 macro mloadq(ctx, field, dst)
 362     wgets(ctx, field, dst)
 363     loadConstantOrVariable(ctx, dst, macro (from)
 364         loadq from, dst
 365     end)
 366 end
 367 
 368 macro mloadi(ctx, field, dst)
 369     wgets(ctx, field, dst)
 370     loadConstantOrVariable(ctx, dst, macro (from)
 371         loadi from, dst
 372     end)
 373 end
 374 
 375 macro mloadp(ctx, field, dst)
 376     wgets(ctx, field, dst)
 377     loadConstantOrVariable(ctx, dst, macro (from)
 378         loadp from, dst
 379     end)
 380 end
 381 
 382 macro mloadf(ctx, field, dst)
 383     wgets(ctx, field, t5)
 384     loadConstantOrVariable(ctx, t5, macro (from)
 385         loadf from, dst
 386     end)
 387 end
 388 
 389 macro mloadd(ctx, field, dst)
 390     wgets(ctx, field, t5)
 391     loadConstantOrVariable(ctx, t5, macro (from)
 392         loadd from, dst
 393     end)
 394 end
 395 
 396 # Typed returns
 397 
 398 macro returnq(ctx, value)
 399     wgets(ctx, m_dst, t5)
 400     storeq value, [cfr, t5, 8]
 401     dispatch(ctx)
 402 end
 403 
 404 macro returni(ctx, value)
 405     wgets(ctx, m_dst, t5)
 406     storei value, [cfr, t5, 8]
 407     dispatch(ctx)
 408 end
 409 
 410 macro returnf(ctx, value)
 411     wgets(ctx, m_dst, t5)
 412     storef value, [cfr, t5, 8]
 413     dispatch(ctx)
 414 end
 415 
 416 macro returnd(ctx, value)
 417     wgets(ctx, m_dst, t5)
 418     stored value, [cfr, t5, 8]
 419     dispatch(ctx)
 420 end
 421 
 422 # Wasm wrapper of get/getu that operate on ctx
 423 macro wgets(ctx, field, dst)
 424     ctx(macro(opcodeName, opcodeStruct, size)
 425         size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)
 426             get(opcodeStruct, field, dst)
 427         end)
 428     end)
 429 end
 430 
 431 macro wgetu(ctx, field, dst)
 432     ctx(macro(opcodeName, opcodeStruct, size)
 433         size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)
 434             getu(opcodeStruct, field, dst)
 435         end)
 436     end)
 437 end
 438 
 439 # Control flow helpers
 440 
 441 macro dispatch(ctx)
 442     ctx(macro(opcodeName, opcodeStruct, size)
 443         genericDispatchOp(wasmDispatch, size, opcodeName)
 444     end)
 445 end
 446 
 447 macro jump(ctx, target)
 448     wgets(ctx, target, t0)
 449     btiz t0, .outOfLineJumpTarget
 450     wasmDispatchIndirect(t0)
 451 .outOfLineJumpTarget:
 452     callWasmSlowPath(_slow_path_wasm_out_of_line_jump_target)
 453     wasmNextInstruction()
 454 end
 455 
 456 macro doReturn()
 457     restoreCalleeSavesUsedByWasm()
 458     restoreCallerPCAndCFR()
 459     ret
 460 end
 461 
 462 # Entry point
 463 
 464 macro wasmCodeBlockGetter(targetRegister)
 465     loadp Callee[cfr], targetRegister
 466     andp ~3, targetRegister
 467     loadp Wasm::LLIntCallee::m_codeBlock[targetRegister], targetRegister
 468 end
 469 
 470 op(wasm_function_prologue, macro ()
 471     if not WEBASSEMBLY or not JSVALUE64 or C_LOOP or C_LOOP_WIN
 472         error
 473     end
 474 
 475     wasmPrologue(wasmCodeBlockGetter, functionCodeBlockSetter, loadWasmInstanceFromTLS)
 476     wasmNextInstruction()
 477 end)
 478 
 479 op(wasm_function_prologue_no_tls, macro ()
 480     if not WEBASSEMBLY or not JSVALUE64 or C_LOOP or C_LOOP_WIN
 481         error
 482     end
 483 
 484     wasmPrologue(wasmCodeBlockGetter, functionCodeBlockSetter, macro () end)
 485     wasmNextInstruction()
 486 end)
 487 
 488 op(wasm_throw_from_slow_path_trampoline, macro ()
 489     loadp Wasm::Instance::m_pointerToTopEntryFrame[wasmInstance], t5
 490     loadp [t5], t5
 491     copyCalleeSavesToEntryFrameCalleeSavesBuffer(t5)
 492 
 493     move cfr, a0
 494     addp PB, PC, a1
 495     move wasmInstance, a2
 496     # Slow paths and the throwException macro store the exception code in the ArgumentCountIncludingThis slot
 497     loadi ArgumentCountIncludingThis + PayloadOffset[cfr], a3
 498     cCall4(_slow_path_wasm_throw_exception)
 499 
 500     jmp r0, ExceptionHandlerPtrTag
 501 end)
 502 
 503 # Disable wide version of narrow-only opcodes
 504 noWide(wasm_enter)
 505 noWide(wasm_wide16)
 506 noWide(wasm_wide32)
 507 
 508 # Opcodes that always invoke the slow path
 509 
 510 slowWasmOp(ref_func)
 511 slowWasmOp(table_get)
 512 slowWasmOp(table_set)
 513 slowWasmOp(table_size)
 514 slowWasmOp(table_fill)
 515 slowWasmOp(table_grow)
 516 slowWasmOp(set_global_ref)
 517 slowWasmOp(set_global_ref_portable_binding)
 518 
 519 wasmOp(grow_memory, WasmGrowMemory, macro(ctx)
 520     callWasmSlowPath(_slow_path_wasm_grow_memory)
 521     reloadMemoryRegistersFromInstance(wasmInstance, ws0, ws1)
 522     dispatch(ctx)
 523 end)
 524 
 525 # Opcodes that should eventually be shared with JS llint
 526 
 527 _wasm_wide16:
 528     wasmNextInstructionWide16()
 529 
 530 _wasm_wide32:
 531     wasmNextInstructionWide32()
 532 
 533 _wasm_enter:
 534     traceExecution()
 535     checkStackPointerAlignment(t2, 0xdead00e1)
 536     loadp CodeBlock[cfr], t2                // t2&lt;CodeBlock&gt; = cfr.CodeBlock
 537     loadi Wasm::FunctionCodeBlock::m_numVars[t2], t2      // t2&lt;size_t&gt; = t2&lt;CodeBlock&gt;.m_numVars
 538     subq CalleeSaveSpaceAsVirtualRegisters + NumberOfWasmArguments, t2
 539     btiz t2, .opEnterDone
 540     move cfr, t1
 541     subq CalleeSaveSpaceAsVirtualRegisters * 8 + NumberOfWasmArguments * 8, t1
 542     negi t2
 543     sxi2q t2, t2
 544 .opEnterLoop:
 545     storeq 0, [t1, t2, 8]
 546     addq 1, t2
 547     btqnz t2, .opEnterLoop
 548 .opEnterDone:
 549     wasmDispatchIndirect(1)
 550 
 551 unprefixedWasmOp(wasm_nop, WasmNop, macro(ctx)
 552     dispatch(ctx)
 553 end)
 554 
 555 wasmOp(loop_hint, WasmLoopHint, macro(ctx)
 556     checkSwitchToJITForLoop()
 557     dispatch(ctx)
 558 end)
 559 
 560 wasmOp(mov, WasmMov, macro(ctx)
 561     mloadq(ctx, m_src, t0)
 562     returnq(ctx, t0)
 563 end)
 564 
 565 wasmOp(jtrue, WasmJtrue, macro(ctx)
 566     mloadi(ctx, m_condition, t0)
 567     btiz t0, .continue
 568     jump(ctx, m_targetLabel)
 569 .continue:
 570     dispatch(ctx)
 571 end)
 572 
 573 wasmOp(jfalse, WasmJfalse, macro(ctx)
 574     mloadi(ctx, m_condition, t0)
 575     btinz t0, .continue
 576     jump(ctx, m_targetLabel)
 577 .continue:
 578     dispatch(ctx)
 579 end)
 580 
 581 wasmOp(switch, WasmSwitch, macro(ctx)
 582     mloadi(ctx, m_scrutinee, t0)
 583     wgetu(ctx, m_tableIndex, t1)
 584 
 585     loadp CodeBlock[cfr], t2
 586     loadp Wasm::FunctionCodeBlock::m_jumpTables + VectorBufferOffset[t2], t2
 587     muli sizeof Wasm::FunctionCodeBlock::JumpTable, t1
 588     addp t1, t2
 589 
 590     loadi VectorSizeOffset[t2], t3
 591     bib t0, t3, .inBounds
 592 
 593 .outOfBounds:
 594     subi t3, 1, t0
 595 
 596 .inBounds:
 597     loadp VectorBufferOffset[t2], t2
 598     muli sizeof Wasm::FunctionCodeBlock::JumpTableEntry, t0
 599 
 600     loadi Wasm::FunctionCodeBlock::JumpTableEntry::startOffset[t2, t0], t1
 601     loadi Wasm::FunctionCodeBlock::JumpTableEntry::dropCount[t2, t0], t3
 602     loadi Wasm::FunctionCodeBlock::JumpTableEntry::keepCount[t2, t0], t5
 603     dropKeep(t1, t3, t5)
 604 
 605     loadis Wasm::FunctionCodeBlock::JumpTableEntry::target[t2, t0], t3
 606     assert(macro(ok) btinz t3, .ok end)
 607     wasmDispatchIndirect(t3)
 608 end)
 609 
 610 unprefixedWasmOp(wasm_jmp, WasmJmp, macro(ctx)
 611     jump(ctx, m_targetLabel)
 612 end)
 613 
 614 unprefixedWasmOp(wasm_ret, WasmRet, macro(ctx)
 615     checkSwitchToJITForEpilogue()
 616     forEachArgumentGPR(macro (offset, gpr)
 617         loadq -offset - 8 - CalleeSaveSpaceAsVirtualRegisters * 8[cfr], gpr
 618     end)
 619     forEachArgumentFPR(macro (offset, fpr)
 620         loadd -offset - 8 - CalleeSaveSpaceAsVirtualRegisters * 8[cfr], fpr
 621     end)
 622     doReturn()
 623 end)
 624 
 625 # Wasm specific bytecodes
 626 
 627 wasmOp(unreachable, WasmUnreachable, macro(ctx)
 628     throwException(Unreachable)
 629 end)
 630 
 631 wasmOp(ret_void, WasmRetVoid, macro(ctx)
 632     checkSwitchToJITForEpilogue()
 633     doReturn()
 634 end)
 635 
 636 wasmOp(ref_is_null, WasmRefIsNull, macro(ctx)
 637     mloadp(ctx, m_ref, t0)
 638     cqeq t0, ValueNull, t0
 639     returni(ctx, t0)
 640 end)
 641 
 642 wasmOp(get_global, WasmGetGlobal, macro(ctx)
 643     loadp Wasm::Instance::m_globals[wasmInstance], t0
 644     wgetu(ctx, m_globalIndex, t1)
 645     loadq [t0, t1, 8], t0
 646     returnq(ctx, t0)
 647 end)
 648 
 649 wasmOp(set_global, WasmSetGlobal, macro(ctx)
 650     loadp Wasm::Instance::m_globals[wasmInstance], t0
 651     wgetu(ctx, m_globalIndex, t1)
 652     mloadq(ctx, m_value, t2)
 653     storeq t2, [t0, t1, 8]
 654     dispatch(ctx)
 655 end)
 656 
 657 wasmOp(get_global_portable_binding, WasmGetGlobalPortableBinding, macro(ctx)
 658     loadp Wasm::Instance::m_globals[wasmInstance], t0
 659     wgetu(ctx, m_globalIndex, t1)
 660     loadq [t0, t1, 8], t0
 661     loadq [t0], t0
 662     returnq(ctx, t0)
 663 end)
 664 
 665 wasmOp(set_global_portable_binding, WasmSetGlobalPortableBinding, macro(ctx)
 666     loadp Wasm::Instance::m_globals[wasmInstance], t0
 667     wgetu(ctx, m_globalIndex, t1)
 668     mloadq(ctx, m_value, t2)
 669     loadq [t0, t1, 8], t0
 670     storeq t2, [t0]
 671     dispatch(ctx)
 672 end)
 673 
 674 macro slowPathForWasmCall(ctx, slowPath, storeWasmInstance)
 675     callWasmCallSlowPath(
 676         slowPath,
 677         # callee is r0 and targetWasmInstance is r1
 678         macro (callee, targetWasmInstance)
 679             move callee, ws0
 680 
 681             loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
 682 
 683             # the call might throw (e.g. indirect call with bad signature)
 684             btpz targetWasmInstance, .throw
 685 
 686             wgetu(ctx, m_stackOffset, ws1)
 687             lshifti 3, ws1
 688             subp cfr, ws1, sp
 689 
 690             wgetu(ctx, m_numberOfStackArgs, ws1)
 691 
 692             # Preserve the current instance
 693             move wasmInstance, PB
 694 
 695             storeWasmInstance(targetWasmInstance)
 696             reloadMemoryRegistersFromInstance(targetWasmInstance, wa0, wa1)
 697 
 698             # Load registers from stack
 699             forEachArgumentGPR(macro (offset, gpr)
 700                 loadq CallFrameHeaderSize + offset[sp, ws1, 8], gpr
 701             end)
 702 
 703             forEachArgumentFPR(macro (offset, fpr)
 704                 loadd CallFrameHeaderSize + offset[sp, ws1, 8], fpr
 705             end)
 706 
 707             addp CallerFrameAndPCSize, sp
 708             call ws0, SlowPathPtrTag
 709 
 710             loadp CodeBlock[cfr], ws1
 711             loadi Wasm::FunctionCodeBlock::m_numCalleeLocals[ws1], ws1
 712             lshiftp 3, ws1
 713             addp maxFrameExtentForSlowPathCall, ws1
 714             subp cfr, ws1, sp
 715 
 716             # We need to set PC to load information from the instruction stream, but we
 717             # need to preserve its current value since it might contain a return value
 718             move PC, memoryBase
 719             move PB, wasmInstance
 720             loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
 721             loadp CodeBlock[cfr], PB
 722             loadp Wasm::FunctionCodeBlock::m_instructionsRawPointer[PB], PB
 723 
 724             wgetu(ctx, m_stackOffset, ws1)
 725             lshifti 3, ws1
 726             negi ws1
 727             sxi2q ws1, ws1
 728             addp cfr, ws1
 729 
 730             # Argument registers are also return registers, so they must be stored to the stack
 731             # in case they contain return values.
 732             wgetu(ctx, m_numberOfStackArgs, ws0)
 733             move memoryBase, PC
 734             forEachArgumentGPR(macro (offset, gpr)
 735                 storeq gpr, CallFrameHeaderSize + offset[ws1, ws0, 8]
 736             end)
 737 
 738             forEachArgumentFPR(macro (offset, fpr)
 739                 stored fpr, CallFrameHeaderSize + offset[ws1, ws0, 8]
 740             end)
 741 
 742             loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
 743 
 744             storeWasmInstance(wasmInstance)
 745             reloadMemoryRegistersFromInstance(wasmInstance, ws0, ws1)
 746 
 747             # Restore stack limit
 748             loadp Wasm::Instance::m_pointerToActualStackLimit[wasmInstance], t5
 749             loadp [t5], t5
 750             storep t5, Wasm::Instance::m_cachedStackLimit[wasmInstance]
 751 
 752             dispatch(ctx)
 753 
 754         .throw:
 755             restoreStateAfterCCall()
 756             dispatch(ctx)
 757         end)
 758 end
 759 
 760 unprefixedWasmOp(wasm_call, WasmCall, macro(ctx)
 761     slowPathForWasmCall(ctx, _slow_path_wasm_call, storeWasmInstanceToTLS)
 762 end)
 763 
 764 unprefixedWasmOp(wasm_call_no_tls, WasmCallNoTls, macro(ctx)
 765     slowPathForWasmCall(ctx, _slow_path_wasm_call_no_tls, macro(targetInstance) move targetInstance, wasmInstance end)
 766 end)
 767 
 768 wasmOp(call_indirect, WasmCallIndirect, macro(ctx)
 769     slowPathForWasmCall(ctx, _slow_path_wasm_call_indirect, storeWasmInstanceToTLS)
 770 end)
 771 
 772 wasmOp(call_indirect_no_tls, WasmCallIndirectNoTls, macro(ctx)
 773     slowPathForWasmCall(ctx, _slow_path_wasm_call_indirect_no_tls, macro(targetInstance) move targetInstance, wasmInstance end)
 774 end)
 775 
 776 wasmOp(current_memory, WasmCurrentMemory, macro(ctx)
 777     loadp Wasm::Instance::m_cachedMemorySize[wasmInstance], t0
 778     urshiftq 16, t0
 779     returnq(ctx, t0)
 780 end)
 781 
 782 wasmOp(select, WasmSelect, macro(ctx)
 783     mloadi(ctx, m_condition, t0)
 784     btiz t0, .isZero
 785     mloadq(ctx, m_nonZero, t0)
 786     returnq(ctx, t0)
 787 .isZero:
 788     mloadq(ctx, m_zero, t0)
 789     returnq(ctx, t0)
 790 end)
 791 
 792 # uses offset as scratch and returns result on pointer
 793 macro emitCheckAndPreparePointer(ctx, pointer, offset, size)
 794     leap size - 1[pointer, offset], t5
 795     bpb t5, memorySize, .continuation
 796     throwException(OutOfBoundsMemoryAccess)
 797 .continuation:
 798     addp memoryBase, pointer
 799 end
 800 
 801 macro wasmLoadOp(name, struct, size, fn)
 802     wasmOp(name, struct, macro(ctx)
 803         mloadi(ctx, m_pointer, t0)
 804         wgetu(ctx, m_offset, t1)
 805         emitCheckAndPreparePointer(ctx, t0, t1, size)
 806         fn([t0, t1], t2)
 807         returnq(ctx, t2)
 808     end)
 809 end
 810 
 811 wasmLoadOp(load8_u, WasmLoad8U, 1, macro(mem, dst) loadb mem, dst end)
 812 wasmLoadOp(load16_u, WasmLoad16U, 2, macro(mem, dst) loadh mem, dst end)
 813 wasmLoadOp(load32_u, WasmLoad32U, 4, macro(mem, dst) loadi mem, dst end)
 814 wasmLoadOp(load64_u, WasmLoad64U, 8, macro(mem, dst) loadq mem, dst end)
 815 
 816 wasmLoadOp(i32_load8_s, WasmI32Load8S, 1, macro(mem, dst) loadbsi mem, dst end)
 817 wasmLoadOp(i64_load8_s, WasmI64Load8S, 1, macro(mem, dst) loadbsq mem, dst end)
 818 wasmLoadOp(i32_load16_s, WasmI32Load16S, 2, macro(mem, dst) loadhsi mem, dst end)
 819 wasmLoadOp(i64_load16_s, WasmI64Load16S, 2, macro(mem, dst) loadhsq mem, dst end)
 820 wasmLoadOp(i64_load32_s, WasmI64Load32S, 4, macro(mem, dst) loadis mem, dst end)
 821 
 822 macro wasmStoreOp(name, struct, size, fn)
 823     wasmOp(name, struct, macro(ctx)
 824         mloadi(ctx, m_pointer, t0)
 825         wgetu(ctx, m_offset, t1)
 826         emitCheckAndPreparePointer(ctx, t0, t1, size)
 827         mloadq(ctx, m_value, t2)
 828         fn(t2, [t0, t1])
 829         dispatch(ctx)
 830     end)
 831 end
 832 
 833 wasmStoreOp(store8, WasmStore8, 1, macro(value, mem) storeb value, mem end)
 834 wasmStoreOp(store16, WasmStore16, 2, macro(value, mem) storeh value, mem end)
 835 wasmStoreOp(store32, WasmStore32, 4, macro(value, mem) storei value, mem end)
 836 wasmStoreOp(store64, WasmStore64, 8, macro(value, mem) storeq value, mem end)
 837 
 838 # Opcodes that don&#39;t have the `b3op` entry in wasm.json. This should be kept in sync
 839 
 840 wasmOp(i32_div_s, WasmI32DivS, macro (ctx)
 841     mloadi(ctx, m_lhs, t0)
 842     mloadi(ctx, m_rhs, t1)
 843 
 844     btiz t1, .throwDivisionByZero
 845 
 846     bineq t1, -1, .safe
 847     bieq t0, constexpr INT32_MIN, .throwIntegerOverflow
 848 
 849 .safe:
 850     if X86_64
 851         # FIXME: Add a way to static_asset that t0 is rax and t2 is rdx
 852         # https://bugs.webkit.org/show_bug.cgi?id=203692
 853         cdqi
 854         idivi t1
 855     elsif ARM64 or ARM64E
 856         divis t1, t0
 857     else
 858         error
 859     end
 860     returni(ctx, t0)
 861 
 862 .throwDivisionByZero:
 863     throwException(DivisionByZero)
 864 
 865 .throwIntegerOverflow:
 866     throwException(IntegerOverflow)
 867 end)
 868 
 869 wasmOp(i32_div_u, WasmI32DivU, macro (ctx)
 870     mloadi(ctx, m_lhs, t0)
 871     mloadi(ctx, m_rhs, t1)
 872 
 873     btiz t1, .throwDivisionByZero
 874 
 875     if X86_64
 876         xori t2, t2
 877         udivi t1
 878     elsif ARM64 or ARM64E
 879         divi t1, t0
 880     else
 881         error
 882     end
 883     returni(ctx, t0)
 884 
 885 .throwDivisionByZero:
 886     throwException(DivisionByZero)
 887 end)
 888 
 889 wasmOp(i32_rem_s, WasmI32RemS, macro (ctx)
 890     mloadi(ctx, m_lhs, t0)
 891     mloadi(ctx, m_rhs, t1)
 892 
 893     btiz t1, .throwDivisionByZero
 894 
 895     bineq t1, -1, .safe
 896     bineq t0, constexpr INT32_MIN, .safe
 897 
 898     move 0, t2
 899     jmp .return
 900 
 901 .safe:
 902     if X86_64
 903         # FIXME: Add a way to static_asset that t0 is rax and t2 is rdx
 904         # https://bugs.webkit.org/show_bug.cgi?id=203692
 905         cdqi
 906         idivi t1
 907     elsif ARM64 or ARM64E
 908         divis t1, t0, t2
 909         muli t1, t2
 910         subi t0, t2, t2
 911     else
 912         error
 913     end
 914 
 915 .return:
 916     returni(ctx, t2)
 917 
 918 .throwDivisionByZero:
 919     throwException(DivisionByZero)
 920 end)
 921 
 922 wasmOp(i32_rem_u, WasmI32RemU, macro (ctx)
 923     mloadi(ctx, m_lhs, t0)
 924     mloadi(ctx, m_rhs, t1)
 925 
 926     btiz t1, .throwDivisionByZero
 927 
 928     if X86_64
 929         xori t2, t2
 930         udivi t1
 931     elsif ARM64 or ARM64E
 932         divi t1, t0, t2
 933         muli t1, t2
 934         subi t0, t2, t2
 935     else
 936         error
 937     end
 938     returni(ctx, t2)
 939 
 940 .throwDivisionByZero:
 941     throwException(DivisionByZero)
 942 end)
 943 
 944 wasmOp(i32_ctz, WasmI32Ctz, macro (ctx)
 945     mloadq(ctx, m_operand, t0)
 946     tzcnti t0, t0
 947     returni(ctx, t0)
 948 end)
 949 
 950 wasmOp(i32_popcnt, WasmI32Popcnt, macro (ctx)
 951     mloadi(ctx, m_operand, a1)
 952     prepareStateForCCall()
 953     move PC, a0
 954     cCall2(_slow_path_wasm_popcount)
 955     restoreStateAfterCCall()
 956     returni(ctx, r1)
 957 end)
 958 
 959 wasmOp(i64_div_s, WasmI64DivS, macro (ctx)
 960     mloadq(ctx, m_lhs, t0)
 961     mloadq(ctx, m_rhs, t1)
 962 
 963     btqz t1, .throwDivisionByZero
 964 
 965     bqneq t1, -1, .safe
 966     bqeq t0, constexpr INT64_MIN, .throwIntegerOverflow
 967 
 968 .safe:
 969     if X86_64
 970         # FIXME: Add a way to static_asset that t0 is rax and t2 is rdx
 971         # https://bugs.webkit.org/show_bug.cgi?id=203692
 972         cqoq
 973         idivq t1
 974     elsif ARM64 or ARM64E
 975         divqs t1, t0
 976     else
 977         error
 978     end
 979     returnq(ctx, t0)
 980 
 981 .throwDivisionByZero:
 982     throwException(DivisionByZero)
 983 
 984 .throwIntegerOverflow:
 985     throwException(IntegerOverflow)
 986 end)
 987 
 988 wasmOp(i64_div_u, WasmI64DivU, macro (ctx)
 989     mloadq(ctx, m_lhs, t0)
 990     mloadq(ctx, m_rhs, t1)
 991 
 992     btqz t1, .throwDivisionByZero
 993 
 994     if X86_64
 995         xorq t2, t2
 996         udivq t1
 997     elsif ARM64 or ARM64E
 998         divq t1, t0
 999     else
1000         error
1001     end
1002     returnq(ctx, t0)
1003 
1004 .throwDivisionByZero:
1005     throwException(DivisionByZero)
1006 end)
1007 
1008 wasmOp(i64_rem_s, WasmI64RemS, macro (ctx)
1009     mloadq(ctx, m_lhs, t0)
1010     mloadq(ctx, m_rhs, t1)
1011 
1012     btqz t1, .throwDivisionByZero
1013 
1014     bqneq t1, -1, .safe
1015     bqneq t0, constexpr INT64_MIN, .safe
1016 
1017     move 0, t2
1018     jmp .return
1019 
1020 .safe:
1021     if X86_64
1022         # FIXME: Add a way to static_asset that t0 is rax and t2 is rdx
1023         # https://bugs.webkit.org/show_bug.cgi?id=203692
1024         cqoq
1025         idivq t1
1026     elsif ARM64 or ARM64E
1027         divqs t1, t0, t2
1028         mulq t1, t2
1029         subq t0, t2, t2
1030     else
1031         error
1032     end
1033 
1034 .return:
1035     returnq(ctx, t2) # rdx has the remainder
1036 
1037 .throwDivisionByZero:
1038     throwException(DivisionByZero)
1039 end)
1040 
1041 wasmOp(i64_rem_u, WasmI64RemU, macro (ctx)
1042     mloadq(ctx, m_lhs, t0)
1043     mloadq(ctx, m_rhs, t1)
1044 
1045     btqz t1, .throwDivisionByZero
1046 
1047     if X86_64
1048         xorq t2, t2
1049         udivq t1
1050     elsif ARM64 or ARM64E
1051         divq t1, t0, t2
1052         mulq t1, t2
1053         subq t0, t2, t2
1054     else
1055         error
1056     end
1057     returnq(ctx, t2)
1058 
1059 .throwDivisionByZero:
1060     throwException(DivisionByZero)
1061 end)
1062 
1063 wasmOp(i64_ctz, WasmI64Ctz, macro (ctx)
1064     mloadq(ctx, m_operand, t0)
1065     tzcntq t0, t0
1066     returnq(ctx, t0)
1067 end)
1068 
1069 wasmOp(i64_popcnt, WasmI64Popcnt, macro (ctx)
1070     mloadq(ctx, m_operand, a1)
1071     prepareStateForCCall()
1072     move PC, a0
1073     cCall2(_slow_path_wasm_popcountll)
1074     restoreStateAfterCCall()
1075     returnq(ctx, r1)
1076 end)
1077 
1078 wasmOp(f32_trunc, WasmF32Trunc, macro (ctx)
1079     mloadf(ctx, m_operand, ft0)
1080     truncatef ft0, ft0
1081     returnf(ctx, ft0)
1082 end)
1083 
1084 wasmOp(f32_nearest, WasmF32Nearest, macro (ctx)
1085     mloadf(ctx, m_operand, ft0)
1086     roundf ft0, ft0
1087     returnf(ctx, ft0)
1088 end)
1089 
1090 wasmOp(f64_trunc, WasmF64Trunc, macro (ctx)
1091     mloadd(ctx, m_operand, ft0)
1092     truncated ft0, ft0
1093     returnd(ctx, ft0)
1094 end)
1095 
1096 wasmOp(f64_nearest, WasmF64Nearest, macro (ctx)
1097     mloadd(ctx, m_operand, ft0)
1098     roundd ft0, ft0
1099     returnd(ctx, ft0)
1100 end)
1101 
1102 wasmOp(i32_trunc_s_f32, WasmI32TruncSF32, macro (ctx)
1103     mloadf(ctx, m_operand, ft0)
1104 
1105     move 0xcf000000, t0 # INT32_MIN
1106     fi2f t0, ft1
1107     bfltun ft0, ft1, .outOfBoundsTrunc
1108 
1109     move 0x4f000000, t0 # -INT32_MIN
1110     fi2f t0, ft1
1111     bfgtequn ft0, ft1, .outOfBoundsTrunc
1112 
1113     truncatef2is ft0, t0
1114     returni(ctx, t0)
1115 
1116 .outOfBoundsTrunc:
1117     throwException(OutOfBoundsTrunc)
1118 end)
1119 
1120 wasmOp(i32_trunc_s_f64, WasmI32TruncSF64, macro (ctx)
1121     mloadd(ctx, m_operand, ft0)
1122 
1123     move 0xc1e0000000000000, t0 # INT32_MIN
1124     fq2d t0, ft1
1125     bdltun ft0, ft1, .outOfBoundsTrunc
1126 
1127     move 0x41e0000000000000, t0 # -INT32_MIN
1128     fq2d t0, ft1
1129     bdgtequn ft0, ft1, .outOfBoundsTrunc
1130 
1131     truncated2is ft0, t0
1132     returni(ctx, t0)
1133 
1134 .outOfBoundsTrunc:
1135     throwException(OutOfBoundsTrunc)
1136 end)
1137 
1138 wasmOp(i32_trunc_u_f32, WasmI32TruncUF32, macro (ctx)
1139     mloadf(ctx, m_operand, ft0)
1140 
1141     move 0xbf800000, t0 # -1.0
1142     fi2f t0, ft1
1143     bfltequn ft0, ft1, .outOfBoundsTrunc
1144 
1145     move 0x4f800000, t0 # INT32_MIN * -2.0
1146     fi2f t0, ft1
1147     bfgtequn ft0, ft1, .outOfBoundsTrunc
1148 
1149     truncatef2i ft0, t0
1150     returni(ctx, t0)
1151 
1152 .outOfBoundsTrunc:
1153     throwException(OutOfBoundsTrunc)
1154 end)
1155 
1156 wasmOp(i32_trunc_u_f64, WasmI32TruncUF64, macro (ctx)
1157     mloadd(ctx, m_operand, ft0)
1158 
1159     move 0xbff0000000000000, t0 # -1.0
1160     fq2d t0, ft1
1161     bdltequn ft0, ft1, .outOfBoundsTrunc
1162 
1163     move 0x41f0000000000000, t0 # INT32_MIN * -2.0
1164     fq2d t0, ft1
1165     bdgtequn ft0, ft1, .outOfBoundsTrunc
1166 
1167     truncated2i ft0, t0
1168     returni(ctx, t0)
1169 
1170 .outOfBoundsTrunc:
1171     throwException(OutOfBoundsTrunc)
1172 end)
1173 
1174 wasmOp(i64_trunc_s_f32, WasmI64TruncSF32, macro (ctx)
1175     mloadd(ctx, m_operand, ft0)
1176 
1177     move 0xdf000000, t0 # INT64_MIN
1178     fi2f t0, ft1
1179     bfltun ft0, ft1, .outOfBoundsTrunc
1180 
1181     move 0x5f000000, t0 # -INT64_MIN
1182     fi2f t0, ft1
1183     bfgtequn ft0, ft1, .outOfBoundsTrunc
1184 
1185     truncatef2qs ft0, t0
1186     returnq(ctx, t0)
1187 
1188 .outOfBoundsTrunc:
1189     throwException(OutOfBoundsTrunc)
1190 end)
1191 
1192 wasmOp(i64_trunc_s_f64, WasmI64TruncSF64, macro (ctx)
1193     mloadd(ctx, m_operand, ft0)
1194 
1195     move 0xc3e0000000000000, t0 # INT64_MIN
1196     fq2d t0, ft1
1197     bdltun ft0, ft1, .outOfBoundsTrunc
1198 
1199     move 0x43e0000000000000, t0 # -INT64_MIN
1200     fq2d t0, ft1
1201     bdgtequn ft0, ft1, .outOfBoundsTrunc
1202 
1203     truncated2qs ft0, t0
1204     returnq(ctx, t0)
1205 
1206 .outOfBoundsTrunc:
1207     throwException(OutOfBoundsTrunc)
1208 end)
1209 
1210 wasmOp(i64_trunc_u_f32, WasmI64TruncUF32, macro (ctx)
1211     mloadf(ctx, m_operand, ft0)
1212 
1213     move 0xbf800000, t0 # -1.0
1214     fi2f t0, ft1
1215     bfltequn ft0, ft1, .outOfBoundsTrunc
1216 
1217     move 0x5f800000, t0 # INT64_MIN * -2.0
1218     fi2f t0, ft1
1219     bfgtequn ft0, ft1, .outOfBoundsTrunc
1220 
1221     truncatef2q ft0, t0
1222     returnq(ctx, t0)
1223 
1224 .outOfBoundsTrunc:
1225     throwException(OutOfBoundsTrunc)
1226 end)
1227 
1228 wasmOp(i64_trunc_u_f64, WasmI64TruncUF64, macro (ctx)
1229     mloadd(ctx, m_operand, ft0)
1230 
1231     move 0xbff0000000000000, t0 # -1.0
1232     fq2d t0, ft1
1233     bdltequn ft0, ft1, .outOfBoundsTrunc
1234 
1235     move 0x43f0000000000000, t0 # INT64_MIN * -2.0
1236     fq2d t0, ft1
1237     bdgtequn ft0, ft1, .outOfBoundsTrunc
1238 
1239     truncated2q ft0, t0
1240     returnq(ctx, t0)
1241 
1242 .outOfBoundsTrunc:
1243     throwException(OutOfBoundsTrunc)
1244 end)
1245 
1246 wasmOp(f32_convert_u_i64, WasmF32ConvertUI64, macro (ctx)
1247     mloadq(ctx, m_operand, t0)
1248     if X86_64
1249         cq2f t0, t1, ft0
1250     elsif ARM64 or ARM64E
1251         cq2f t0, ft0
1252     else
1253         error
1254     end
1255     returnf(ctx, ft0)
1256 end)
1257 
1258 wasmOp(f64_convert_u_i64, WasmF64ConvertUI64, macro (ctx)
1259     mloadq(ctx, m_operand, t0)
1260     if X86_64
1261         cq2d t0, t1, ft0
1262     elsif ARM64 or ARM64E
1263         cq2d t0, ft0
1264     else
1265         error
1266     end
1267     returnd(ctx, ft0)
1268 end)
1269 
1270 wasmOp(i32_eqz, WasmI32Eqz, macro(ctx)
1271     mloadi(ctx, m_operand, t0)
1272     cieq t0, 0, t0
1273     returni(ctx, t0)
1274 end)
1275 
1276 wasmOp(i64_shl, WasmI64Shl, macro(ctx)
1277     mloadq(ctx, m_lhs, t0)
1278     mloadi(ctx, m_rhs, t1)
1279     lshiftq t1, t0
1280     returnq(ctx, t0)
1281 end)
1282 
1283 wasmOp(i64_shr_u, WasmI64ShrU, macro(ctx)
1284     mloadq(ctx, m_lhs, t0)
1285     mloadi(ctx, m_rhs, t1)
1286     urshiftq t1, t0
1287     returnq(ctx, t0)
1288 end)
1289 
1290 wasmOp(i64_shr_s, WasmI64ShrS, macro(ctx)
1291     mloadq(ctx, m_lhs, t0)
1292     mloadi(ctx, m_rhs, t1)
1293     rshiftq t1, t0
1294     returnq(ctx, t0)
1295 end)
1296 
1297 wasmOp(i64_rotr, WasmI64Rotr, macro(ctx)
1298     mloadq(ctx, m_lhs, t0)
1299     mloadi(ctx, m_rhs, t1)
1300     rrotateq t1, t0
1301     returnq(ctx, t0)
1302 end)
1303 
1304 wasmOp(i64_rotl, WasmI64Rotl, macro(ctx)
1305     mloadq(ctx, m_lhs, t0)
1306     mloadi(ctx, m_rhs, t1)
1307     lrotateq t1, t0
1308     returnq(ctx, t0)
1309 end)
1310 
1311 wasmOp(i64_eqz, WasmI64Eqz, macro(ctx)
1312     mloadq(ctx, m_operand, t0)
1313     cqeq t0, 0, t0
1314     returni(ctx, t0)
1315 end)
1316 
1317 wasmOp(f32_min, WasmF32Min, macro(ctx)
1318     mloadf(ctx, m_lhs, ft0)
1319     mloadf(ctx, m_rhs, ft1)
1320 
1321     bfeq ft0, ft1, .equal
1322     bflt ft0, ft1, .lt
1323     bfgt ft0, ft1, .return
1324 
1325 .NaN:
1326     addf ft0, ft1
1327     jmp .return
1328 
1329 .equal:
1330     orf ft0, ft1
1331     jmp .return
1332 
1333 .lt:
1334     moved ft0, ft1
1335 
1336 .return:
1337     returnf(ctx, ft1)
1338 end)
1339 
1340 wasmOp(f32_max, WasmF32Max, macro(ctx)
1341     mloadf(ctx, m_lhs, ft0)
1342     mloadf(ctx, m_rhs, ft1)
1343 
1344     bfeq ft1, ft0, .equal
1345     bflt ft1, ft0, .lt
1346     bfgt ft1, ft0, .return
1347 
1348 .NaN:
1349     addf ft0, ft1
1350     jmp .return
1351 
1352 .equal:
1353     andf ft0, ft1
1354     jmp .return
1355 
1356 .lt:
1357     moved ft0, ft1
1358 
1359 .return:
1360     returnf(ctx, ft1)
1361 end)
1362 
1363 wasmOp(f32_copysign, WasmF32Copysign, macro(ctx)
1364     mloadf(ctx, m_lhs, ft0)
1365     mloadf(ctx, m_rhs, ft1)
1366 
1367     ff2i ft1, t1
1368     move 0x80000000, t2
1369     andi t2, t1
1370 
1371     ff2i ft0, t0
1372     move 0x7fffffff, t2
1373     andi t2, t0
1374 
1375     ori t1, t0
1376     fi2f t0, ft0
1377     returnf(ctx, ft0)
1378 end)
1379 
1380 wasmOp(f64_min, WasmF64Min, macro(ctx)
1381     mloadd(ctx, m_lhs, ft0)
1382     mloadd(ctx, m_rhs, ft1)
1383 
1384     bdeq ft0, ft1, .equal
1385     bdlt ft0, ft1, .lt
1386     bdgt ft0, ft1, .return
1387 
1388 .NaN:
1389     addd ft0, ft1
1390     jmp .return
1391 
1392 .equal:
1393     ord ft0, ft1
1394     jmp .return
1395 
1396 .lt:
1397     moved ft0, ft1
1398 
1399 .return:
1400     returnd(ctx, ft1)
1401 end)
1402 
1403 wasmOp(f64_max, WasmF64Max, macro(ctx)
1404     mloadd(ctx, m_lhs, ft0)
1405     mloadd(ctx, m_rhs, ft1)
1406 
1407     bdeq ft1, ft0, .equal
1408     bdlt ft1, ft0, .lt
1409     bdgt ft1, ft0, .return
1410 
1411 .NaN:
1412     addd ft0, ft1
1413     jmp .return
1414 
1415 .equal:
1416     andd ft0, ft1
1417     jmp .return
1418 
1419 .lt:
1420     moved ft0, ft1
1421 
1422 .return:
1423     returnd(ctx, ft1)
1424 end)
1425 
1426 wasmOp(f64_copysign, WasmF64Copysign, macro(ctx)
1427     mloadd(ctx, m_lhs, ft0)
1428     mloadd(ctx, m_rhs, ft1)
1429 
1430     fd2q ft1, t1
1431     move 0x8000000000000000, t2
1432     andq t2, t1
1433 
1434     fd2q ft0, t0
1435     move 0x7fffffffffffffff, t2
1436     andq t2, t0
1437 
1438     orq t1, t0
1439     fq2d t0, ft0
1440     returnd(ctx, ft0)
1441 end)
1442 
1443 wasmOp(f32_convert_u_i32, WasmF32ConvertUI32, macro(ctx)
1444     mloadi(ctx, m_operand, t0)
1445     ci2f t0, ft0
1446     returnf(ctx, ft0)
1447 end)
1448 
1449 wasmOp(f64_convert_u_i32, WasmF64ConvertUI32, macro(ctx)
1450     mloadi(ctx, m_operand, t0)
1451     ci2d t0, ft0
1452     returnd(ctx, ft0)
1453 end)
1454 
1455 wasmOp(i32_add, WasmI32Add, macro(ctx)
1456     mloadi(ctx, m_lhs, t0)
1457     mloadi(ctx, m_rhs, t1)
1458     addi t0, t1, t2
1459     returni(ctx, t2)
1460 end)
1461 
1462 wasmOp(i32_sub, WasmI32Sub, macro(ctx)
1463     mloadi(ctx, m_lhs, t0)
1464     mloadi(ctx, m_rhs, t1)
1465     subi t1, t0
1466     returni(ctx, t0)
1467 end)
1468 
1469 wasmOp(i32_mul, WasmI32Mul, macro(ctx)
1470     mloadi(ctx, m_lhs, t0)
1471     mloadi(ctx, m_rhs, t1)
1472     muli t0, t1
1473     returni(ctx, t1)
1474 end)
1475 
1476 wasmOp(i32_and, WasmI32And, macro(ctx)
1477     mloadi(ctx, m_lhs, t0)
1478     mloadi(ctx, m_rhs, t1)
1479     andi t0, t1
1480     returni(ctx, t1)
1481 end)
1482 
1483 wasmOp(i32_or, WasmI32Or, macro(ctx)
1484     mloadi(ctx, m_lhs, t0)
1485     mloadi(ctx, m_rhs, t1)
1486     ori t0, t1
1487     returni(ctx, t1)
1488 end)
1489 
1490 wasmOp(i32_xor, WasmI32Xor, macro(ctx)
1491     mloadi(ctx, m_lhs, t0)
1492     mloadi(ctx, m_rhs, t1)
1493     xori t0, t1
1494     returni(ctx, t1)
1495 end)
1496 
1497 wasmOp(i32_shl, WasmI32Shl, macro(ctx)
1498     mloadi(ctx, m_lhs, t0)
1499     mloadi(ctx, m_rhs, t1)
1500     lshifti t1, t0
1501     returni(ctx, t0)
1502 end)
1503 
1504 wasmOp(i32_shr_u, WasmI32ShrU, macro(ctx)
1505     mloadi(ctx, m_lhs, t0)
1506     mloadi(ctx, m_rhs, t1)
1507     urshifti t1, t0
1508     returni(ctx, t0)
1509 end)
1510 
1511 wasmOp(i32_shr_s, WasmI32ShrS, macro(ctx)
1512     mloadi(ctx, m_lhs, t0)
1513     mloadi(ctx, m_rhs, t1)
1514     rshifti t1, t0
1515     returni(ctx, t0)
1516 end)
1517 
1518 wasmOp(i32_rotr, WasmI32Rotr, macro(ctx)
1519     mloadi(ctx, m_lhs, t0)
1520     mloadi(ctx, m_rhs, t1)
1521     rrotatei t1, t0
1522     returni(ctx, t0)
1523 end)
1524 
1525 wasmOp(i32_rotl, WasmI32Rotl, macro(ctx)
1526     mloadi(ctx, m_lhs, t0)
1527     mloadi(ctx, m_rhs, t1)
1528     lrotatei t1, t0
1529     returni(ctx, t0)
1530 end)
1531 
1532 wasmOp(i32_eq, WasmI32Eq, macro(ctx)
1533     mloadi(ctx, m_lhs, t0)
1534     mloadi(ctx, m_rhs, t1)
1535     cieq t0, t1, t2
1536     andi 1, t2
1537     returni(ctx, t2)
1538 end)
1539 
1540 wasmOp(i32_ne, WasmI32Ne, macro(ctx)
1541     mloadi(ctx, m_lhs, t0)
1542     mloadi(ctx, m_rhs, t1)
1543     cineq t0, t1, t2
1544     andi 1, t2
1545     returni(ctx, t2)
1546 end)
1547 
1548 wasmOp(i32_lt_s, WasmI32LtS, macro(ctx)
1549     mloadi(ctx, m_lhs, t0)
1550     mloadi(ctx, m_rhs, t1)
1551     cilt t0, t1, t2
1552     andi 1, t2
1553     returni(ctx, t2)
1554 end)
1555 
1556 wasmOp(i32_le_s, WasmI32LeS, macro(ctx)
1557     mloadi(ctx, m_lhs, t0)
1558     mloadi(ctx, m_rhs, t1)
1559     cilteq t0, t1, t2
1560     andi 1, t2
1561     returni(ctx, t2)
1562 end)
1563 
1564 wasmOp(i32_lt_u, WasmI32LtU, macro(ctx)
1565     mloadi(ctx, m_lhs, t0)
1566     mloadi(ctx, m_rhs, t1)
1567     cib t0, t1, t2
1568     andi 1, t2
1569     returni(ctx, t2)
1570 end)
1571 
1572 wasmOp(i32_le_u, WasmI32LeU, macro(ctx)
1573     mloadi(ctx, m_lhs, t0)
1574     mloadi(ctx, m_rhs, t1)
1575     cibeq t0, t1, t2
1576     andi 1, t2
1577     returni(ctx, t2)
1578 end)
1579 
1580 wasmOp(i32_gt_s, WasmI32GtS, macro(ctx)
1581     mloadi(ctx, m_lhs, t0)
1582     mloadi(ctx, m_rhs, t1)
1583     cigt t0, t1, t2
1584     andi 1, t2
1585     returni(ctx, t2)
1586 end)
1587 
1588 wasmOp(i32_ge_s, WasmI32GeS, macro(ctx)
1589     mloadi(ctx, m_lhs, t0)
1590     mloadi(ctx, m_rhs, t1)
1591     cigteq t0, t1, t2
1592     andi 1, t2
1593     returni(ctx, t2)
1594 end)
1595 
1596 wasmOp(i32_gt_u, WasmI32GtU, macro(ctx)
1597     mloadi(ctx, m_lhs, t0)
1598     mloadi(ctx, m_rhs, t1)
1599     cia t0, t1, t2
1600     andi 1, t2
1601     returni(ctx, t2)
1602 end)
1603 
1604 wasmOp(i32_ge_u, WasmI32GeU, macro(ctx)
1605     mloadi(ctx, m_lhs, t0)
1606     mloadi(ctx, m_rhs, t1)
1607     ciaeq t0, t1, t2
1608     andi 1, t2
1609     returni(ctx, t2)
1610 end)
1611 
1612 wasmOp(i32_clz, WasmI32Clz, macro(ctx)
1613     mloadi(ctx, m_operand, t0)
1614     lzcnti t0, t1
1615     returni(ctx, t1)
1616 end)
1617 
1618 wasmOp(i64_add, WasmI64Add, macro(ctx)
1619     mloadq(ctx, m_lhs, t0)
1620     mloadq(ctx, m_rhs, t1)
1621     addq t0, t1
1622     returnq(ctx, t1)
1623 end)
1624 
1625 wasmOp(i64_sub, WasmI64Sub, macro(ctx)
1626     mloadq(ctx, m_lhs, t0)
1627     mloadq(ctx, m_rhs, t1)
1628     subq t1, t0
1629     returnq(ctx, t0)
1630 end)
1631 
1632 wasmOp(i64_mul, WasmI64Mul, macro(ctx)
1633     mloadq(ctx, m_lhs, t0)
1634     mloadq(ctx, m_rhs, t1)
1635     mulq t0, t1
1636     returnq(ctx, t1)
1637 end)
1638 
1639 wasmOp(i64_and, WasmI64And, macro(ctx)
1640     mloadq(ctx, m_lhs, t0)
1641     mloadq(ctx, m_rhs, t1)
1642     andq t0, t1
1643     returnq(ctx, t1)
1644 end)
1645 
1646 wasmOp(i64_or, WasmI64Or, macro(ctx)
1647     mloadq(ctx, m_lhs, t0)
1648     mloadq(ctx, m_rhs, t1)
1649     orq t0, t1
1650     returnq(ctx, t1)
1651 end)
1652 
1653 wasmOp(i64_xor, WasmI64Xor, macro(ctx)
1654     mloadq(ctx, m_lhs, t0)
1655     mloadq(ctx, m_rhs, t1)
1656     xorq t0, t1
1657     returnq(ctx, t1)
1658 end)
1659 
1660 wasmOp(i64_eq, WasmI64Eq, macro(ctx)
1661     mloadq(ctx, m_lhs, t0)
1662     mloadq(ctx, m_rhs, t1)
1663     cqeq t0, t1, t2
1664     andi 1, t2
1665     returni(ctx, t2)
1666 end)
1667 
1668 wasmOp(i64_ne, WasmI64Ne, macro(ctx)
1669     mloadq(ctx, m_lhs, t0)
1670     mloadq(ctx, m_rhs, t1)
1671     cqneq t0, t1, t2
1672     andi 1, t2
1673     returni(ctx, t2)
1674 end)
1675 
1676 wasmOp(i64_lt_s, WasmI64LtS, macro(ctx)
1677     mloadq(ctx, m_lhs, t0)
1678     mloadq(ctx, m_rhs, t1)
1679     cqlt t0, t1, t2
1680     andi 1, t2
1681     returni(ctx, t2)
1682 end)
1683 
1684 wasmOp(i64_le_s, WasmI64LeS, macro(ctx)
1685     mloadq(ctx, m_lhs, t0)
1686     mloadq(ctx, m_rhs, t1)
1687     cqlteq t0, t1, t2
1688     andi 1, t2
1689     returni(ctx, t2)
1690 end)
1691 
1692 wasmOp(i64_lt_u, WasmI64LtU, macro(ctx)
1693     mloadq(ctx, m_lhs, t0)
1694     mloadq(ctx, m_rhs, t1)
1695     cqb t0, t1, t2
1696     andi 1, t2
1697     returni(ctx, t2)
1698 end)
1699 
1700 wasmOp(i64_le_u, WasmI64LeU, macro(ctx)
1701     mloadq(ctx, m_lhs, t0)
1702     mloadq(ctx, m_rhs, t1)
1703     cqbeq t0, t1, t2
1704     andi 1, t2
1705     returni(ctx, t2)
1706 end)
1707 
1708 wasmOp(i64_gt_s, WasmI64GtS, macro(ctx)
1709     mloadq(ctx, m_lhs, t0)
1710     mloadq(ctx, m_rhs, t1)
1711     cqgt t0, t1, t2
1712     andi 1, t2
1713     returni(ctx, t2)
1714 end)
1715 
1716 wasmOp(i64_ge_s, WasmI64GeS, macro(ctx)
1717     mloadq(ctx, m_lhs, t0)
1718     mloadq(ctx, m_rhs, t1)
1719     cqgteq t0, t1, t2
1720     andi 1, t2
1721     returni(ctx, t2)
1722 end)
1723 
1724 wasmOp(i64_gt_u, WasmI64GtU, macro(ctx)
1725     mloadq(ctx, m_lhs, t0)
1726     mloadq(ctx, m_rhs, t1)
1727     cqa t0, t1, t2
1728     andi 1, t2
1729     returni(ctx, t2)
1730 end)
1731 
1732 wasmOp(i64_ge_u, WasmI64GeU, macro(ctx)
1733     mloadq(ctx, m_lhs, t0)
1734     mloadq(ctx, m_rhs, t1)
1735     cqaeq t0, t1, t2
1736     andi 1, t2
1737     returni(ctx, t2)
1738 end)
1739 
1740 wasmOp(i64_clz, WasmI64Clz, macro(ctx)
1741     mloadq(ctx, m_operand, t0)
1742     lzcntq t0, t1
1743     returnq(ctx, t1)
1744 end)
1745 
1746 wasmOp(f32_add, WasmF32Add, macro(ctx)
1747     mloadf(ctx, m_lhs, ft0)
1748     mloadf(ctx, m_rhs, ft1)
1749     addf ft0, ft1
1750     returnf(ctx, ft1)
1751 end)
1752 
1753 wasmOp(f32_sub, WasmF32Sub, macro(ctx)
1754     mloadf(ctx, m_lhs, ft0)
1755     mloadf(ctx, m_rhs, ft1)
1756     subf ft1, ft0
1757     returnf(ctx, ft0)
1758 end)
1759 
1760 wasmOp(f32_mul, WasmF32Mul, macro(ctx)
1761     mloadf(ctx, m_lhs, ft0)
1762     mloadf(ctx, m_rhs, ft1)
1763     mulf ft0, ft1
1764     returnf(ctx, ft1)
1765 end)
1766 
1767 wasmOp(f32_div, WasmF32Div, macro(ctx)
1768     mloadf(ctx, m_lhs, ft0)
1769     mloadf(ctx, m_rhs, ft1)
1770     divf ft1, ft0
1771     returnf(ctx, ft0)
1772 end)
1773 
1774 wasmOp(f32_abs, WasmF32Abs, macro(ctx)
1775     mloadf(ctx, m_operand, ft0)
1776     absf ft0, ft1
1777     returnf(ctx, ft1)
1778 end)
1779 
1780 wasmOp(f32_neg, WasmF32Neg, macro(ctx)
1781     mloadf(ctx, m_operand, ft0)
1782     negf ft0, ft1
1783     returnf(ctx, ft1)
1784 end)
1785 
1786 wasmOp(f32_ceil, WasmF32Ceil, macro(ctx)
1787     mloadf(ctx, m_operand, ft0)
1788     ceilf ft0, ft1
1789     returnf(ctx, ft1)
1790 end)
1791 
1792 wasmOp(f32_floor, WasmF32Floor, macro(ctx)
1793     mloadf(ctx, m_operand, ft0)
1794     floorf ft0, ft1
1795     returnf(ctx, ft1)
1796 end)
1797 
1798 wasmOp(f32_sqrt, WasmF32Sqrt, macro(ctx)
1799     mloadf(ctx, m_operand, ft0)
1800     sqrtf ft0, ft1
1801     returnf(ctx, ft1)
1802 end)
1803 
1804 wasmOp(f32_eq, WasmF32Eq, macro(ctx)
1805     mloadf(ctx, m_lhs, ft0)
1806     mloadf(ctx, m_rhs, ft1)
1807     cfeq ft0, ft1, t0
1808     returni(ctx, t0)
1809 end)
1810 
1811 wasmOp(f32_ne, WasmF32Ne, macro(ctx)
1812     mloadf(ctx, m_lhs, ft0)
1813     mloadf(ctx, m_rhs, ft1)
1814     cfnequn ft0, ft1, t0
1815     returni(ctx, t0)
1816 end)
1817 
1818 wasmOp(f32_lt, WasmF32Lt, macro(ctx)
1819     mloadf(ctx, m_lhs, ft0)
1820     mloadf(ctx, m_rhs, ft1)
1821     cflt ft0, ft1, t0
1822     returni(ctx, t0)
1823 end)
1824 
1825 wasmOp(f32_le, WasmF32Le, macro(ctx)
1826     mloadf(ctx, m_lhs, ft0)
1827     mloadf(ctx, m_rhs, ft1)
1828     cflteq ft0, ft1, t0
1829     returni(ctx, t0)
1830 end)
1831 
1832 wasmOp(f32_gt, WasmF32Gt, macro(ctx)
1833     mloadf(ctx, m_lhs, ft0)
1834     mloadf(ctx, m_rhs, ft1)
1835     cfgt ft0, ft1, t0
1836     returni(ctx, t0)
1837 end)
1838 
1839 wasmOp(f32_ge, WasmF32Ge, macro(ctx)
1840     mloadf(ctx, m_lhs, ft0)
1841     mloadf(ctx, m_rhs, ft1)
1842     cfgteq ft0, ft1, t0
1843     returni(ctx, t0)
1844 end)
1845 
1846 wasmOp(f64_add, WasmF64Add, macro(ctx)
1847     mloadd(ctx, m_lhs, ft0)
1848     mloadd(ctx, m_rhs, ft1)
1849     addd ft0, ft1
1850     returnd(ctx, ft1)
1851 end)
1852 
1853 wasmOp(f64_sub, WasmF64Sub, macro(ctx)
1854     mloadd(ctx, m_lhs, ft0)
1855     mloadd(ctx, m_rhs, ft1)
1856     subd ft1, ft0
1857     returnd(ctx, ft0)
1858 end)
1859 
1860 wasmOp(f64_mul, WasmF64Mul, macro(ctx)
1861     mloadd(ctx, m_lhs, ft0)
1862     mloadd(ctx, m_rhs, ft1)
1863     muld ft0, ft1
1864     returnd(ctx, ft1)
1865 end)
1866 
1867 wasmOp(f64_div, WasmF64Div, macro(ctx)
1868     mloadd(ctx, m_lhs, ft0)
1869     mloadd(ctx, m_rhs, ft1)
1870     divd ft1, ft0
1871     returnd(ctx, ft0)
1872 end)
1873 
1874 wasmOp(f64_abs, WasmF64Abs, macro(ctx)
1875     mloadd(ctx, m_operand, ft0)
1876     absd ft0, ft1
1877     returnd(ctx, ft1)
1878 end)
1879 
1880 wasmOp(f64_neg, WasmF64Neg, macro(ctx)
1881     mloadd(ctx, m_operand, ft0)
1882     negd ft0, ft1
1883     returnd(ctx, ft1)
1884 end)
1885 
1886 wasmOp(f64_ceil, WasmF64Ceil, macro(ctx)
1887     mloadd(ctx, m_operand, ft0)
1888     ceild ft0, ft1
1889     returnd(ctx, ft1)
1890 end)
1891 
1892 wasmOp(f64_floor, WasmF64Floor, macro(ctx)
1893     mloadd(ctx, m_operand, ft0)
1894     floord ft0, ft1
1895     returnd(ctx, ft1)
1896 end)
1897 
1898 wasmOp(f64_sqrt, WasmF64Sqrt, macro(ctx)
1899     mloadd(ctx, m_operand, ft0)
1900     sqrtd ft0, ft1
1901     returnd(ctx, ft1)
1902 end)
1903 
1904 wasmOp(f64_eq, WasmF64Eq, macro(ctx)
1905     mloadd(ctx, m_lhs, ft0)
1906     mloadd(ctx, m_rhs, ft1)
1907     cdeq ft0, ft1, t0
1908     returni(ctx, t0)
1909 end)
1910 
1911 wasmOp(f64_ne, WasmF64Ne, macro(ctx)
1912     mloadd(ctx, m_lhs, ft0)
1913     mloadd(ctx, m_rhs, ft1)
1914     cdnequn ft0, ft1, t0
1915     returni(ctx, t0)
1916 end)
1917 
1918 wasmOp(f64_lt, WasmF64Lt, macro(ctx)
1919     mloadd(ctx, m_lhs, ft0)
1920     mloadd(ctx, m_rhs, ft1)
1921     cdlt ft0, ft1, t0
1922     returni(ctx, t0)
1923 end)
1924 
1925 wasmOp(f64_le, WasmF64Le, macro(ctx)
1926     mloadd(ctx, m_lhs, ft0)
1927     mloadd(ctx, m_rhs, ft1)
1928     cdlteq ft0, ft1, t0
1929     returni(ctx, t0)
1930 end)
1931 
1932 wasmOp(f64_gt, WasmF64Gt, macro(ctx)
1933     mloadd(ctx, m_lhs, ft0)
1934     mloadd(ctx, m_rhs, ft1)
1935     cdgt ft0, ft1, t0
1936     returni(ctx, t0)
1937 end)
1938 
1939 wasmOp(f64_ge, WasmF64Ge, macro(ctx)
1940     mloadd(ctx, m_lhs, ft0)
1941     mloadd(ctx, m_rhs, ft1)
1942     cdgteq ft0, ft1, t0
1943     returni(ctx, t0)
1944 end)
1945 
1946 wasmOp(i32_wrap_i64, WasmI32WrapI64, macro(ctx)
1947     mloadq(ctx, m_operand, t0)
1948     returni(ctx, t0)
1949 end)
1950 
1951 wasmOp(i64_extend_s_i32, WasmI64ExtendSI32, macro(ctx)
1952     mloadi(ctx, m_operand, t0)
1953     sxi2q t0, t1
1954     returnq(ctx, t1)
1955 end)
1956 
1957 wasmOp(i64_extend_u_i32, WasmI64ExtendUI32, macro(ctx)
1958     mloadi(ctx, m_operand, t0)
1959     zxi2q t0, t1
1960     returnq(ctx, t1)
1961 end)
1962 
1963 wasmOp(f32_convert_s_i32, WasmF32ConvertSI32, macro(ctx)
1964     mloadi(ctx, m_operand, t0)
1965     ci2fs t0, ft0
1966     returnf(ctx, ft0)
1967 end)
1968 
1969 wasmOp(f32_convert_s_i64, WasmF32ConvertSI64, macro(ctx)
1970     mloadq(ctx, m_operand, t0)
1971     cq2fs t0, ft0
1972     returnf(ctx, ft0)
1973 end)
1974 
1975 wasmOp(f32_demote_f64, WasmF32DemoteF64, macro(ctx)
1976     mloadd(ctx, m_operand, ft0)
1977     cd2f ft0, ft1
1978     returnf(ctx, ft1)
1979 end)
1980 
1981 wasmOp(f32_reinterpret_i32, WasmF32ReinterpretI32, macro(ctx)
1982     mloadi(ctx, m_operand, t0)
1983     fi2f t0, ft0
1984     returnf(ctx, ft0)
1985 end)
1986 
1987 wasmOp(f64_convert_s_i32, WasmF64ConvertSI32, macro(ctx)
1988     mloadi(ctx, m_operand, t0)
1989     ci2ds t0, ft0
1990     returnd(ctx, ft0)
1991 end)
1992 
1993 wasmOp(f64_convert_s_i64, WasmF64ConvertSI64, macro(ctx)
1994     mloadq(ctx, m_operand, t0)
1995     cq2ds t0, ft0
1996     returnd(ctx, ft0)
1997 end)
1998 
1999 wasmOp(f64_promote_f32, WasmF64PromoteF32, macro(ctx)
2000     mloadf(ctx, m_operand, ft0)
2001     cf2d ft0, ft1
2002     returnd(ctx, ft1)
2003 end)
2004 
2005 wasmOp(f64_reinterpret_i64, WasmF64ReinterpretI64, macro(ctx)
2006     mloadq(ctx, m_operand, t0)
2007     fq2d t0, ft0
2008     returnd(ctx, ft0)
2009 end)
2010 
2011 wasmOp(i32_reinterpret_f32, WasmI32ReinterpretF32, macro(ctx)
2012     mloadf(ctx, m_operand, ft0)
2013     ff2i ft0, t0
2014     returni(ctx, t0)
2015 end)
2016 
2017 wasmOp(i64_reinterpret_f64, WasmI64ReinterpretF64, macro(ctx)
2018     mloadd(ctx, m_operand, ft0)
2019     fd2q ft0, t0
2020     returnq(ctx, t0)
2021 end)
2022 
2023 macro dropKeep(startOffset, drop, keep)
2024     lshifti 3, startOffset
2025     subp cfr, startOffset, startOffset
2026     negi drop
2027     sxi2q drop, drop
2028 
2029 .copyLoop:
2030     btiz keep, .done
2031     loadq [startOffset, drop, 8], t6
2032     storeq t6, [startOffset]
2033     subi 1, keep
2034     subp 8, startOffset
2035     jmp .copyLoop
2036 
2037 .done:
2038 end
2039 
2040 wasmOp(drop_keep, WasmDropKeep, macro(ctx)
2041     wgetu(ctx, m_startOffset, t0)
2042     wgetu(ctx, m_dropCount, t1)
2043     wgetu(ctx, m_keepCount, t2)
2044 
2045     dropKeep(t0, t1, t2)
2046 
2047     dispatch(ctx)
2048 end)
    </pre>
  </body>
</html>