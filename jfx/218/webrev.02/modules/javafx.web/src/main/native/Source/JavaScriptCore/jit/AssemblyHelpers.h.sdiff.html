<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/AssemblyHelpers.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AssemblyHelpers.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="BinarySwitch.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/AssemblyHelpers.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(JIT)
  29 
  30 #include &quot;CodeBlock.h&quot;
  31 #include &quot;EntryFrame.h&quot;
  32 #include &quot;FPRInfo.h&quot;
  33 #include &quot;GPRInfo.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InlineCallFrame.h&quot;
  36 #include &quot;JITAllocator.h&quot;
  37 #include &quot;JITCode.h&quot;

  38 #include &quot;MacroAssembler.h&quot;
  39 #include &quot;MarkedSpace.h&quot;
  40 #include &quot;RegisterAtOffsetList.h&quot;
  41 #include &quot;RegisterSet.h&quot;
  42 #include &quot;StackAlignment.h&quot;
  43 #include &quot;TagRegistersMode.h&quot;
  44 #include &quot;TypeofType.h&quot;
  45 #include &quot;VM.h&quot;
  46 
  47 namespace JSC {
  48 
<span class="line-modified">  49 typedef void (*V_DebugOperation_EPP)(ExecState*, void*, void*);</span>
  50 
  51 class AssemblyHelpers : public MacroAssembler {
  52 public:
  53     AssemblyHelpers(CodeBlock* codeBlock)
  54         : m_codeBlock(codeBlock)
  55         , m_baselineCodeBlock(codeBlock ? codeBlock-&gt;baselineAlternative() : 0)
  56     {
  57         if (m_codeBlock) {
  58             ASSERT(m_baselineCodeBlock);
  59             ASSERT(!m_baselineCodeBlock-&gt;alternative());
  60             ASSERT(m_baselineCodeBlock-&gt;jitType() == JITType::None || JITCode::isBaselineCode(m_baselineCodeBlock-&gt;jitType()));
  61         }
  62     }
  63 
  64     CodeBlock* codeBlock() { return m_codeBlock; }
  65     VM&amp; vm() { return m_codeBlock-&gt;vm(); }
  66     AssemblerType_T&amp; assembler() { return m_assembler; }
  67 








  68     void checkStackPointerAlignment()
  69     {
  70         // This check is both unneeded and harder to write correctly for ARM64
  71 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM64)
  72         Jump stackPointerAligned = branchTestPtr(Zero, stackPointerRegister, TrustedImm32(0xf));
  73         abortWithReason(AHStackPointerMisaligned);
  74         stackPointerAligned.link(this);
  75 #endif
  76     }
  77 




































  78     template&lt;typename T&gt;
  79     void storeCell(T cell, Address address)
  80     {
  81 #if USE(JSVALUE64)
  82         store64(cell, address);
  83 #else
  84         store32(cell, address.withOffset(PayloadOffset));
  85         store32(TrustedImm32(JSValue::CellTag), address.withOffset(TagOffset));
  86 #endif
  87     }
  88 
  89     void loadCell(Address address, GPRReg gpr)
  90     {
  91 #if USE(JSVALUE64)
  92         load64(address, gpr);
  93 #else
  94         load32(address.withOffset(PayloadOffset), gpr);
  95 #endif
  96     }
  97 
</pre>
<hr />
<pre>
 317         RegisterSet dontRestoreRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 318         unsigned registerCount = calleeSaves-&gt;size();
 319 
 320         for (unsigned i = 0; i &lt; registerCount; i++) {
 321             RegisterAtOffset entry = calleeSaves-&gt;at(i);
 322             if (dontRestoreRegisters.get(entry.reg()))
 323                 continue;
 324             loadPtr(Address(framePointerRegister, entry.offset()), entry.reg().gpr());
 325         }
 326     }
 327 
 328     void emitSaveCalleeSaves()
 329     {
 330         emitSaveCalleeSavesFor(codeBlock());
 331     }
 332 
 333     void emitSaveThenMaterializeTagRegisters()
 334     {
 335 #if USE(JSVALUE64)
 336 #if CPU(ARM64)
<span class="line-modified"> 337         pushPair(GPRInfo::tagTypeNumberRegister, GPRInfo::tagMaskRegister);</span>
 338 #else
<span class="line-modified"> 339         push(GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 340         push(GPRInfo::tagMaskRegister);</span>
 341 #endif
 342         emitMaterializeTagCheckRegisters();
 343 #endif
 344     }
 345 
 346     void emitRestoreCalleeSaves()
 347     {
 348         emitRestoreCalleeSavesFor(codeBlock());
 349     }
 350 
 351     void emitRestoreSavedTagRegisters()
 352     {
 353 #if USE(JSVALUE64)
 354 #if CPU(ARM64)
<span class="line-modified"> 355         popPair(GPRInfo::tagTypeNumberRegister, GPRInfo::tagMaskRegister);</span>
 356 #else
<span class="line-modified"> 357         pop(GPRInfo::tagMaskRegister);</span>
<span class="line-modified"> 358         pop(GPRInfo::tagTypeNumberRegister);</span>
 359 #endif
 360 #endif
 361     }
 362 
 363     // If you use this, be aware that vmGPR will get trashed.
 364     void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(GPRReg vmGPR)
 365     {
 366 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 367         loadPtr(Address(vmGPR, VM::topEntryFrameOffset()), vmGPR);
 368         copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(vmGPR);
 369 #else
 370         UNUSED_PARAM(vmGPR);
 371 #endif
 372     }
 373 
 374     void copyCalleeSavesToEntryFrameCalleeSavesBuffer(EntryFrame*&amp; topEntryFrame)
 375     {
 376 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 377         const TempRegisterSet&amp; usedRegisters = { RegisterSet::stubUnavailableRegisters() };
 378         GPRReg temp1 = usedRegisters.getFreeGPR(0);
</pre>
<hr />
<pre>
 435                 if (currentFrameEntry) {
 436                     // Load calleeSave from stack into temp register
 437                     fpRegToStore = fpTemp;
 438                     loadDouble(Address(framePointerRegister, currentFrameEntry-&gt;offset()), fpRegToStore);
 439                 } else
 440                     // Just store callee save directly
 441                     fpRegToStore = entry.reg().fpr();
 442 
 443                 storeDouble(fpRegToStore, Address(temp1, entry.offset()));
 444             }
 445         }
 446 #else
 447         UNUSED_PARAM(topEntryFrame);
 448         UNUSED_PARAM(usedRegisters);
 449 #endif
 450     }
 451 
 452     void emitMaterializeTagCheckRegisters()
 453     {
 454 #if USE(JSVALUE64)
<span class="line-modified"> 455         move(MacroAssembler::TrustedImm64(TagTypeNumber), GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 456         orPtr(MacroAssembler::TrustedImm32(TagBitTypeOther), GPRInfo::tagTypeNumberRegister, GPRInfo::tagMaskRegister);</span>
 457 #endif
 458     }
 459 
<span class="line-modified"> 460     void clearStackFrame(GPRReg currentTop, GPRReg newTop, GPRReg temp, unsigned frameSize)</span>
<span class="line-removed"> 461     {</span>
<span class="line-removed"> 462         ASSERT(frameSize % stackAlignmentBytes() == 0);</span>
<span class="line-removed"> 463         if (frameSize &lt;= 128) {</span>
<span class="line-removed"> 464             for (unsigned offset = 0; offset &lt; frameSize; offset += sizeof(CPURegister))</span>
<span class="line-removed"> 465                 storePtr(TrustedImm32(0), Address(currentTop, -8 - offset));</span>
<span class="line-removed"> 466         } else {</span>
<span class="line-removed"> 467             constexpr unsigned storeBytesPerIteration = stackAlignmentBytes();</span>
<span class="line-removed"> 468             constexpr unsigned storesPerIteration = storeBytesPerIteration / sizeof(CPURegister);</span>
<span class="line-removed"> 469 </span>
<span class="line-removed"> 470             move(currentTop, temp);</span>
<span class="line-removed"> 471             Label zeroLoop = label();</span>
<span class="line-removed"> 472             subPtr(TrustedImm32(storeBytesPerIteration), temp);</span>
<span class="line-removed"> 473 #if CPU(ARM64)</span>
<span class="line-removed"> 474             static_assert(storesPerIteration == 2, &quot;clearStackFrame() for ARM64 assumes stack is 16 byte aligned&quot;);</span>
<span class="line-removed"> 475             storePair64(ARM64Registers::zr, ARM64Registers::zr, temp);</span>
<span class="line-removed"> 476 #else</span>
<span class="line-removed"> 477             for (unsigned i = storesPerIteration; i-- != 0;)</span>
<span class="line-removed"> 478                 storePtr(TrustedImm32(0), Address(temp, sizeof(CPURegister) * i));</span>
<span class="line-removed"> 479 #endif</span>
<span class="line-removed"> 480             branchPtr(NotEqual, temp, newTop).linkTo(zeroLoop, this);</span>
<span class="line-removed"> 481         }</span>
<span class="line-removed"> 482     }</span>
<span class="line-removed"> 483 </span>
<span class="line-removed"> 484 #if CPU(X86_64) || CPU(X86)</span>
 485     static constexpr size_t prologueStackPointerDelta()
 486     {
 487         // Prologue only saves the framePointerRegister
 488         return sizeof(void*);
 489     }
 490 
 491     void emitFunctionPrologue()
 492     {
 493         push(framePointerRegister);
 494         move(stackPointerRegister, framePointerRegister);
 495     }
 496 
 497     void emitFunctionEpilogueWithEmptyFrame()
 498     {
 499         pop(framePointerRegister);
 500     }
 501 
 502     void emitFunctionEpilogue()
 503     {
 504         move(framePointerRegister, stackPointerRegister);
 505         pop(framePointerRegister);
 506     }
 507 
 508     void preserveReturnAddressAfterCall(GPRReg reg)
 509     {
 510         pop(reg);
 511     }
 512 
 513     void restoreReturnAddressBeforeReturn(GPRReg reg)
 514     {
 515         push(reg);
 516     }
 517 
 518     void restoreReturnAddressBeforeReturn(Address address)
 519     {
 520         push(address);
 521     }
<span class="line-modified"> 522 #endif // CPU(X86_64) || CPU(X86)</span>
 523 
 524 #if CPU(ARM_THUMB2) || CPU(ARM64)
 525     static constexpr size_t prologueStackPointerDelta()
 526     {
 527         // Prologue saves the framePointerRegister and linkRegister
 528         return 2 * sizeof(void*);
 529     }
 530 
 531     void emitFunctionPrologue()
 532     {
 533         tagReturnAddress();
 534         pushPair(framePointerRegister, linkRegister);
 535         move(stackPointerRegister, framePointerRegister);
 536     }
 537 
 538     void emitFunctionEpilogueWithEmptyFrame()
 539     {
 540         popPair(framePointerRegister, linkRegister);
 541     }
 542 
</pre>
<hr />
<pre>
 585         move(framePointerRegister, stackPointerRegister);
 586         emitFunctionEpilogueWithEmptyFrame();
 587     }
 588 
 589     ALWAYS_INLINE void preserveReturnAddressAfterCall(RegisterID reg)
 590     {
 591         move(returnAddressRegister, reg);
 592     }
 593 
 594     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(RegisterID reg)
 595     {
 596         move(reg, returnAddressRegister);
 597     }
 598 
 599     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(Address address)
 600     {
 601         loadPtr(address, returnAddressRegister);
 602     }
 603 #endif
 604 
<span class="line-modified"> 605     void emitGetFromCallFrameHeaderPtr(int entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 606     {
<span class="line-modified"> 607         loadPtr(Address(from, entry * sizeof(Register)), to);</span>
 608     }
<span class="line-modified"> 609     void emitGetFromCallFrameHeader32(int entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 610     {
<span class="line-modified"> 611         load32(Address(from, entry * sizeof(Register)), to);</span>
 612     }
 613 #if USE(JSVALUE64)
<span class="line-modified"> 614     void emitGetFromCallFrameHeader64(int entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 615     {
<span class="line-modified"> 616         load64(Address(from, entry * sizeof(Register)), to);</span>
 617     }
 618 #endif // USE(JSVALUE64)
<span class="line-modified"> 619     void emitPutToCallFrameHeader(GPRReg from, int entry)</span>
 620     {
<span class="line-modified"> 621         storePtr(from, Address(GPRInfo::callFrameRegister, entry * sizeof(Register)));</span>
 622     }
 623 
<span class="line-modified"> 624     void emitPutToCallFrameHeader(void* value, int entry)</span>
 625     {
<span class="line-modified"> 626         storePtr(TrustedImmPtr(value), Address(GPRInfo::callFrameRegister, entry * sizeof(Register)));</span>
 627     }
 628 
 629     void emitGetCallerFrameFromCallFrameHeaderPtr(RegisterID to)
 630     {
 631         loadPtr(Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), to);
 632     }
 633     void emitPutCallerFrameToCallFrameHeader(RegisterID from)
 634     {
 635         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()));
 636     }
 637 
 638     void emitPutReturnPCToCallFrameHeader(RegisterID from)
 639     {
 640         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 641     }
 642     void emitPutReturnPCToCallFrameHeader(TrustedImmPtr from)
 643     {
 644         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 645     }
 646 
 647     // emitPutToCallFrameHeaderBeforePrologue() and related are used to access callee frame header
 648     // fields before the code from emitFunctionPrologue() has executed.
 649     // First, the access is via the stack pointer. Second, the address calculation must also take
 650     // into account that the stack pointer may not have been adjusted down for the return PC and/or
 651     // caller&#39;s frame pointer. On some platforms, the callee is responsible for pushing the
 652     // &quot;link register&quot; containing the return address in the function prologue.
 653 #if USE(JSVALUE64)
<span class="line-modified"> 654     void emitPutToCallFrameHeaderBeforePrologue(GPRReg from, int entry)</span>
 655     {
<span class="line-modified"> 656         storePtr(from, Address(stackPointerRegister, entry * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta()));</span>
 657     }
 658 #else
<span class="line-modified"> 659     void emitPutPayloadToCallFrameHeaderBeforePrologue(GPRReg from, int entry)</span>
 660     {
<span class="line-modified"> 661         storePtr(from, Address(stackPointerRegister, entry * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta() + OBJECT_OFFSETOF(EncodedValueDescriptor, asBits.payload)));</span>
 662     }
 663 
<span class="line-modified"> 664     void emitPutTagToCallFrameHeaderBeforePrologue(TrustedImm32 tag, int entry)</span>
 665     {
<span class="line-modified"> 666         storePtr(tag, Address(stackPointerRegister, entry * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta() + OBJECT_OFFSETOF(EncodedValueDescriptor, asBits.tag)));</span>
 667     }
 668 #endif
 669 
 670     JumpList branchIfNotEqual(JSValueRegs regs, JSValue value)
 671     {
 672 #if USE(JSVALUE64)
 673         return branch64(NotEqual, regs.gpr(), TrustedImm64(JSValue::encode(value)));
 674 #else
 675         JumpList result;
 676         result.append(branch32(NotEqual, regs.tagGPR(), TrustedImm32(value.tag())));
 677         if (value.isEmpty() || value.isUndefinedOrNull())
 678             return result; // These don&#39;t have anything interesting in the payload.
 679         result.append(branch32(NotEqual, regs.payloadGPR(), TrustedImm32(value.payload())));
 680         return result;
 681 #endif
 682     }
 683 
 684     Jump branchIfEqual(JSValueRegs regs, JSValue value)
 685     {
 686 #if USE(JSVALUE64)
 687         return branch64(Equal, regs.gpr(), TrustedImm64(JSValue::encode(value)));
 688 #else
 689         Jump notEqual;
 690         // These don&#39;t have anything interesting in the payload.
 691         if (!value.isEmpty() &amp;&amp; !value.isUndefinedOrNull())
 692             notEqual = branch32(NotEqual, regs.payloadGPR(), TrustedImm32(value.payload()));
 693         Jump result = branch32(Equal, regs.tagGPR(), TrustedImm32(value.tag()));
 694         if (notEqual.isSet())
 695             notEqual.link(this);
 696         return result;
 697 #endif
 698     }
 699 
 700     Jump branchIfNotCell(GPRReg reg, TagRegistersMode mode = HaveTagRegisters)
 701     {
 702 #if USE(JSVALUE64)
 703         if (mode == HaveTagRegisters)
<span class="line-modified"> 704             return branchTest64(NonZero, reg, GPRInfo::tagMaskRegister);</span>
<span class="line-modified"> 705         return branchTest64(NonZero, reg, TrustedImm64(TagMask));</span>
 706 #else
 707         UNUSED_PARAM(mode);
 708         return branch32(MacroAssembler::NotEqual, reg, TrustedImm32(JSValue::CellTag));
 709 #endif
 710     }
 711 
 712     Jump branchIfNotCell(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 713     {
 714 #if USE(JSVALUE64)
 715         return branchIfNotCell(regs.gpr(), mode);
 716 #else
 717         return branchIfNotCell(regs.tagGPR(), mode);
 718 #endif
 719     }
 720 
 721     Jump branchIfCell(GPRReg reg, TagRegistersMode mode = HaveTagRegisters)
 722     {
 723 #if USE(JSVALUE64)
 724         if (mode == HaveTagRegisters)
<span class="line-modified"> 725             return branchTest64(Zero, reg, GPRInfo::tagMaskRegister);</span>
<span class="line-modified"> 726         return branchTest64(Zero, reg, TrustedImm64(TagMask));</span>
 727 #else
 728         UNUSED_PARAM(mode);
 729         return branch32(MacroAssembler::Equal, reg, TrustedImm32(JSValue::CellTag));
 730 #endif
 731     }
 732     Jump branchIfCell(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 733     {
 734 #if USE(JSVALUE64)
 735         return branchIfCell(regs.gpr(), mode);
 736 #else
 737         return branchIfCell(regs.tagGPR(), mode);
 738 #endif
 739     }
 740 
 741     Jump branchIfOther(JSValueRegs regs, GPRReg tempGPR)
 742     {
 743 #if USE(JSVALUE64)
 744         move(regs.gpr(), tempGPR);
<span class="line-modified"> 745         and64(TrustedImm32(~TagBitUndefined), tempGPR);</span>
<span class="line-modified"> 746         return branch64(Equal, tempGPR, TrustedImm64(ValueNull));</span>
 747 #else
 748         or32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 749         return branch32(Equal, tempGPR, TrustedImm32(JSValue::NullTag));
 750 #endif
 751     }
 752 
 753     Jump branchIfNotOther(JSValueRegs regs, GPRReg tempGPR)
 754     {
 755 #if USE(JSVALUE64)
 756         move(regs.gpr(), tempGPR);
<span class="line-modified"> 757         and64(TrustedImm32(~TagBitUndefined), tempGPR);</span>
<span class="line-modified"> 758         return branch64(NotEqual, tempGPR, TrustedImm64(ValueNull));</span>
 759 #else
 760         or32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 761         return branch32(NotEqual, tempGPR, TrustedImm32(JSValue::NullTag));
 762 #endif
 763     }
 764 
 765     Jump branchIfInt32(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 766     {
 767 #if USE(JSVALUE64)
 768         if (mode == HaveTagRegisters)
<span class="line-modified"> 769             return branch64(AboveOrEqual, gpr, GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 770         return branch64(AboveOrEqual, gpr, TrustedImm64(TagTypeNumber));</span>
 771 #else
 772         UNUSED_PARAM(mode);
 773         return branch32(Equal, gpr, TrustedImm32(JSValue::Int32Tag));
 774 #endif
 775     }
 776 
 777     Jump branchIfInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 778     {
 779 #if USE(JSVALUE64)
 780         return branchIfInt32(regs.gpr(), mode);
 781 #else
 782         return branchIfInt32(regs.tagGPR(), mode);
 783 #endif
 784     }
 785 
 786     Jump branchIfNotInt32(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 787     {
 788 #if USE(JSVALUE64)
 789         if (mode == HaveTagRegisters)
<span class="line-modified"> 790             return branch64(Below, gpr, GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 791         return branch64(Below, gpr, TrustedImm64(TagTypeNumber));</span>
 792 #else
 793         UNUSED_PARAM(mode);
 794         return branch32(NotEqual, gpr, TrustedImm32(JSValue::Int32Tag));
 795 #endif
 796     }
 797 
 798     Jump branchIfNotInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 799     {
 800 #if USE(JSVALUE64)
 801         return branchIfNotInt32(regs.gpr(), mode);
 802 #else
 803         return branchIfNotInt32(regs.tagGPR(), mode);
 804 #endif
 805     }
 806 
 807     // Note that the tempGPR is not used in 64-bit mode.
 808     Jump branchIfNumber(JSValueRegs regs, GPRReg tempGPR, TagRegistersMode mode = HaveTagRegisters)
 809     {
 810 #if USE(JSVALUE64)
 811         UNUSED_PARAM(tempGPR);
 812         return branchIfNumber(regs.gpr(), mode);
 813 #else
 814         UNUSED_PARAM(mode);
 815         ASSERT(tempGPR != InvalidGPRReg);
 816         add32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 817         return branch32(Below, tempGPR, TrustedImm32(JSValue::LowestTag + 1));
 818 #endif
 819     }
 820 
 821 #if USE(JSVALUE64)
 822     Jump branchIfNumber(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 823     {
 824         if (mode == HaveTagRegisters)
<span class="line-modified"> 825             return branchTest64(NonZero, gpr, GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 826         return branchTest64(NonZero, gpr, TrustedImm64(TagTypeNumber));</span>
 827     }
 828 #endif
 829 
 830     // Note that the tempGPR is not used in 64-bit mode.
 831     Jump branchIfNotNumber(JSValueRegs regs, GPRReg tempGPR, TagRegistersMode mode = HaveTagRegisters)
 832     {
 833 #if USE(JSVALUE64)
 834         UNUSED_PARAM(tempGPR);
 835         return branchIfNotNumber(regs.gpr(), mode);
 836 #else
 837         UNUSED_PARAM(mode);
 838         add32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 839         return branch32(AboveOrEqual, tempGPR, TrustedImm32(JSValue::LowestTag + 1));
 840 #endif
 841     }
 842 
 843 #if USE(JSVALUE64)
 844     Jump branchIfNotNumber(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 845     {
 846         if (mode == HaveTagRegisters)
<span class="line-modified"> 847             return branchTest64(Zero, gpr, GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 848         return branchTest64(Zero, gpr, TrustedImm64(TagTypeNumber));</span>
 849     }
 850 #endif
 851 
 852     Jump branchIfNotDoubleKnownNotInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 853     {
 854 #if USE(JSVALUE64)
 855         if (mode == HaveTagRegisters)
<span class="line-modified"> 856             return branchTest64(Zero, regs.gpr(), GPRInfo::tagTypeNumberRegister);</span>
<span class="line-modified"> 857         return branchTest64(Zero, regs.gpr(), TrustedImm64(TagTypeNumber));</span>
 858 #else
 859         UNUSED_PARAM(mode);
 860         return branch32(AboveOrEqual, regs.tagGPR(), TrustedImm32(JSValue::LowestTag));
 861 #endif
 862     }
 863 
 864     // Note that the tempGPR is not used in 32-bit mode.
 865     Jump branchIfBoolean(GPRReg gpr, GPRReg tempGPR)
 866     {
 867 #if USE(JSVALUE64)
 868         ASSERT(tempGPR != InvalidGPRReg);
 869         move(gpr, tempGPR);
<span class="line-modified"> 870         xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), tempGPR);</span>
 871         return branchTest64(Zero, tempGPR, TrustedImm32(static_cast&lt;int32_t&gt;(~1)));
 872 #else
 873         UNUSED_PARAM(tempGPR);
 874         return branch32(Equal, gpr, TrustedImm32(JSValue::BooleanTag));
 875 #endif
 876     }
 877 
 878     // Note that the tempGPR is not used in 32-bit mode.
 879     Jump branchIfBoolean(JSValueRegs regs, GPRReg tempGPR)
 880     {
 881 #if USE(JSVALUE64)
 882         return branchIfBoolean(regs.gpr(), tempGPR);
 883 #else
 884         return branchIfBoolean(regs.tagGPR(), tempGPR);
 885 #endif
 886     }
 887 
 888     // Note that the tempGPR is not used in 32-bit mode.
 889     Jump branchIfNotBoolean(GPRReg gpr, GPRReg tempGPR)
 890     {
 891 #if USE(JSVALUE64)
 892         ASSERT(tempGPR != InvalidGPRReg);
 893         move(gpr, tempGPR);
<span class="line-modified"> 894         xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), tempGPR);</span>
 895         return branchTest64(NonZero, tempGPR, TrustedImm32(static_cast&lt;int32_t&gt;(~1)));
 896 #else
 897         UNUSED_PARAM(tempGPR);
 898         return branch32(NotEqual, gpr, TrustedImm32(JSValue::BooleanTag));
 899 #endif
 900     }
 901 
 902     // Note that the tempGPR is not used in 32-bit mode.
 903     Jump branchIfNotBoolean(JSValueRegs regs, GPRReg tempGPR)
 904     {
 905 #if USE(JSVALUE64)
 906         return branchIfNotBoolean(regs.gpr(), tempGPR);
 907 #else
 908         return branchIfNotBoolean(regs.tagGPR(), tempGPR);
 909 #endif
 910     }
 911 
 912     Jump branchIfObject(GPRReg cellGPR)
 913     {
 914         return branch8(
</pre>
<hr />
<pre>
1086     }
1087 
1088     static Address addressForByteOffset(ptrdiff_t byteOffset)
1089     {
1090         return Address(GPRInfo::callFrameRegister, byteOffset);
1091     }
1092     static Address addressFor(VirtualRegister virtualRegister, GPRReg baseReg)
1093     {
1094         ASSERT(virtualRegister.isValid());
1095         return Address(baseReg, virtualRegister.offset() * sizeof(Register));
1096     }
1097     static Address addressFor(VirtualRegister virtualRegister)
1098     {
1099         // NB. It&#39;s tempting on some architectures to sometimes use an offset from the stack
1100         // register because for some offsets that will encode to a smaller instruction. But we
1101         // cannot do this. We use this in places where the stack pointer has been moved to some
1102         // unpredictable location.
1103         ASSERT(virtualRegister.isValid());
1104         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register));
1105     }
<span class="line-modified">1106     static Address addressFor(int operand)</span>
1107     {
<span class="line-modified">1108         return addressFor(static_cast&lt;VirtualRegister&gt;(operand));</span>

1109     }
1110 
1111     static Address tagFor(VirtualRegister virtualRegister, GPRReg baseGPR)
1112     {
1113         ASSERT(virtualRegister.isValid());
1114         return Address(baseGPR, virtualRegister.offset() * sizeof(Register) + TagOffset);
1115     }
1116     static Address tagFor(VirtualRegister virtualRegister)
1117     {
1118         ASSERT(virtualRegister.isValid());
1119         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register) + TagOffset);
1120     }
<span class="line-modified">1121     static Address tagFor(int operand)</span>
1122     {
<span class="line-modified">1123         return tagFor(static_cast&lt;VirtualRegister&gt;(operand));</span>

1124     }
1125 
1126     static Address payloadFor(VirtualRegister virtualRegister, GPRReg baseGPR)
1127     {
1128         ASSERT(virtualRegister.isValid());
1129         return Address(baseGPR, virtualRegister.offset() * sizeof(Register) + PayloadOffset);
1130     }
1131     static Address payloadFor(VirtualRegister virtualRegister)
1132     {
1133         ASSERT(virtualRegister.isValid());
1134         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register) + PayloadOffset);
1135     }
<span class="line-modified">1136     static Address payloadFor(int operand)</span>
1137     {
<span class="line-modified">1138         return payloadFor(static_cast&lt;VirtualRegister&gt;(operand));</span>

1139     }
1140 
1141     // Access to our fixed callee CallFrame.
<span class="line-modified">1142     static Address calleeFrameSlot(int slot)</span>
1143     {
<span class="line-modified">1144         ASSERT(slot &gt;= CallerFrameAndPC::sizeInRegisters);</span>
<span class="line-modified">1145         return Address(stackPointerRegister, sizeof(Register) * (slot - CallerFrameAndPC::sizeInRegisters));</span>
1146     }
1147 
1148     // Access to our fixed callee CallFrame.
1149     static Address calleeArgumentSlot(int argument)
1150     {
<span class="line-modified">1151         return calleeFrameSlot(virtualRegisterForArgument(argument).offset());</span>
1152     }
1153 
<span class="line-modified">1154     static Address calleeFrameTagSlot(int slot)</span>
1155     {
1156         return calleeFrameSlot(slot).withOffset(TagOffset);
1157     }
1158 
<span class="line-modified">1159     static Address calleeFramePayloadSlot(int slot)</span>
1160     {
1161         return calleeFrameSlot(slot).withOffset(PayloadOffset);
1162     }
1163 
1164     static Address calleeArgumentTagSlot(int argument)
1165     {
1166         return calleeArgumentSlot(argument).withOffset(TagOffset);
1167     }
1168 
1169     static Address calleeArgumentPayloadSlot(int argument)
1170     {
1171         return calleeArgumentSlot(argument).withOffset(PayloadOffset);
1172     }
1173 
1174     static Address calleeFrameCallerFrame()
1175     {
<span class="line-modified">1176         return calleeFrameSlot(0).withOffset(CallFrame::callerFrameOffset());</span>
1177     }
1178 
1179     static GPRReg selectScratchGPR(RegisterSet preserved)
1180     {
1181         GPRReg registers[] = {
1182             GPRInfo::regT0,
1183             GPRInfo::regT1,
1184             GPRInfo::regT2,
1185             GPRInfo::regT3,
1186             GPRInfo::regT4,
1187             GPRInfo::regT5,
1188         };
1189 
1190         for (GPRReg reg : registers) {
1191             if (!preserved.contains(reg))
1192                 return reg;
1193         }
1194         RELEASE_ASSERT_NOT_REACHED();
1195         return InvalidGPRReg;
1196     }
</pre>
<hr />
<pre>
1212     {
1213         if (regs.tagGPR() != InvalidGPRReg)
1214             set.set(regs.tagGPR());
1215         if (regs.payloadGPR() != InvalidGPRReg)
1216             set.set(regs.payloadGPR());
1217         constructRegisterSet(set, args...);
1218     }
1219 
1220     template&lt;typename... Regs&gt;
1221     static void constructRegisterSet(RegisterSet&amp; set, GPRReg reg, Regs... args)
1222     {
1223         if (reg != InvalidGPRReg)
1224             set.set(reg);
1225         constructRegisterSet(set, args...);
1226     }
1227 
1228     // Add a debug call. This call has no effect on JIT code execution state.
1229     void debugCall(VM&amp;, V_DebugOperation_EPP function, void* argument);
1230 
1231     // These methods JIT generate dynamic, debug-only checks - akin to ASSERTs.
<span class="line-modified">1232 #if !ASSERT_DISABLED</span>
1233     void jitAssertIsInt32(GPRReg);
1234     void jitAssertIsJSInt32(GPRReg);
1235     void jitAssertIsJSNumber(GPRReg);
1236     void jitAssertIsJSDouble(GPRReg);
1237     void jitAssertIsCell(GPRReg);
1238     void jitAssertHasValidCallFrame();
1239     void jitAssertIsNull(GPRReg);
1240     void jitAssertTagsInPlace();
1241     void jitAssertArgumentCountSane();
1242 #else
1243     void jitAssertIsInt32(GPRReg) { }
1244     void jitAssertIsJSInt32(GPRReg) { }
1245     void jitAssertIsJSNumber(GPRReg) { }
1246     void jitAssertIsJSDouble(GPRReg) { }
1247     void jitAssertIsCell(GPRReg) { }
1248     void jitAssertHasValidCallFrame() { }
1249     void jitAssertIsNull(GPRReg) { }
1250     void jitAssertTagsInPlace() { }
1251     void jitAssertArgumentCountSane() { }
1252 #endif
1253 
1254     void jitReleaseAssertNoException(VM&amp;);
1255 
1256     void incrementSuperSamplerCount();
1257     void decrementSuperSamplerCount();
1258 
1259     void purifyNaN(FPRReg);
1260 
1261     // These methods convert between doubles, and doubles boxed and JSValues.
1262 #if USE(JSVALUE64)
1263     GPRReg boxDouble(FPRReg fpr, GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
1264     {
1265         moveDoubleTo64(fpr, gpr);
1266         if (mode == DoNotHaveTagRegisters)
<span class="line-modified">1267             sub64(TrustedImm64(TagTypeNumber), gpr);</span>
1268         else {
<span class="line-modified">1269             sub64(GPRInfo::tagTypeNumberRegister, gpr);</span>
1270             jitAssertIsJSDouble(gpr);
1271         }
1272         return gpr;
1273     }
1274     FPRReg unboxDoubleWithoutAssertions(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
1275     {
<span class="line-modified">1276         add64(GPRInfo::tagTypeNumberRegister, gpr, resultGPR);</span>
1277         move64ToDouble(resultGPR, fpr);
1278         return fpr;
1279     }
1280     FPRReg unboxDouble(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
1281     {
1282         jitAssertIsJSDouble(gpr);
1283         return unboxDoubleWithoutAssertions(gpr, resultGPR, fpr);
1284     }
1285 
1286     void boxDouble(FPRReg fpr, JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
1287     {
1288         boxDouble(fpr, regs.gpr(), mode);
1289     }
1290 
1291     void unboxDoubleNonDestructive(JSValueRegs regs, FPRReg destFPR, GPRReg resultGPR, FPRReg)
1292     {
1293         unboxDouble(regs.payloadGPR(), resultGPR, destFPR);
1294     }
1295 
1296     // Here are possible arrangements of source, target, scratch:
1297     // - source, target, scratch can all be separate registers.
1298     // - source and target can be the same but scratch is separate.
1299     // - target and scratch can be the same but source is separate.
1300     void boxInt52(GPRReg source, GPRReg target, GPRReg scratch, FPRReg fpScratch)
1301     {
1302         // Is it an int32?
1303         signExtend32ToPtr(source, scratch);
1304         Jump isInt32 = branch64(Equal, source, scratch);
1305 
1306         // Nope, it&#39;s not, but regT0 contains the int64 value.
1307         convertInt64ToDouble(source, fpScratch);
1308         boxDouble(fpScratch, target);
1309         Jump done = jump();
1310 
1311         isInt32.link(this);
1312         zeroExtend32ToPtr(source, target);
<span class="line-modified">1313         or64(GPRInfo::tagTypeNumberRegister, target);</span>
1314 
1315         done.link(this);
1316     }
1317 #endif
1318 
1319 #if USE(JSVALUE32_64)
1320     void boxDouble(FPRReg fpr, GPRReg tagGPR, GPRReg payloadGPR)
1321     {
1322         moveDoubleToInts(fpr, payloadGPR, tagGPR);
1323     }
1324     void unboxDouble(GPRReg tagGPR, GPRReg payloadGPR, FPRReg fpr, FPRReg scratchFPR)
1325     {
1326         moveIntsToDouble(payloadGPR, tagGPR, fpr, scratchFPR);
1327     }
1328 
1329     void boxDouble(FPRReg fpr, JSValueRegs regs)
1330     {
1331         boxDouble(fpr, regs.tagGPR(), regs.payloadGPR());
1332     }
1333     void unboxDouble(JSValueRegs regs, FPRReg fpr, FPRReg scratchFPR)
1334     {
1335         unboxDouble(regs.tagGPR(), regs.payloadGPR(), fpr, scratchFPR);
1336     }
1337 
1338     void unboxDoubleNonDestructive(const JSValueRegs regs, FPRReg destFPR, GPRReg, FPRReg scratchFPR)
1339     {
1340         unboxDouble(regs, destFPR, scratchFPR);
1341     }
1342 #endif
1343 
1344     void boxBooleanPayload(GPRReg boolGPR, GPRReg payloadGPR)
1345     {
1346 #if USE(JSVALUE64)
<span class="line-modified">1347         add32(TrustedImm32(ValueFalse), boolGPR, payloadGPR);</span>
1348 #else
1349         move(boolGPR, payloadGPR);
1350 #endif
1351     }
1352 
1353     void boxBooleanPayload(bool value, GPRReg payloadGPR)
1354     {
1355 #if USE(JSVALUE64)
<span class="line-modified">1356         move(TrustedImm32(ValueFalse + value), payloadGPR);</span>
1357 #else
1358         move(TrustedImm32(value), payloadGPR);
1359 #endif
1360     }
1361 
1362     void boxBoolean(GPRReg boolGPR, JSValueRegs boxedRegs)
1363     {
1364         boxBooleanPayload(boolGPR, boxedRegs.payloadGPR());
1365 #if USE(JSVALUE32_64)
1366         move(TrustedImm32(JSValue::BooleanTag), boxedRegs.tagGPR());
1367 #endif
1368     }
1369 
1370     void boxBoolean(bool value, JSValueRegs boxedRegs)
1371     {
1372         boxBooleanPayload(value, boxedRegs.payloadGPR());
1373 #if USE(JSVALUE32_64)
1374         move(TrustedImm32(JSValue::BooleanTag), boxedRegs.tagGPR());
1375 #endif
1376     }
1377 
1378     void boxInt32(GPRReg intGPR, JSValueRegs boxedRegs, TagRegistersMode mode = HaveTagRegisters)
1379     {
1380 #if USE(JSVALUE64)
1381         if (mode == DoNotHaveTagRegisters) {
1382             move(intGPR, boxedRegs.gpr());
<span class="line-modified">1383             or64(TrustedImm64(TagTypeNumber), boxedRegs.gpr());</span>
1384         } else
<span class="line-modified">1385             or64(GPRInfo::tagTypeNumberRegister, intGPR, boxedRegs.gpr());</span>
1386 #else
1387         UNUSED_PARAM(mode);
1388         move(intGPR, boxedRegs.payloadGPR());
1389         move(TrustedImm32(JSValue::Int32Tag), boxedRegs.tagGPR());
1390 #endif
1391     }
1392 
1393     void boxCell(GPRReg cellGPR, JSValueRegs boxedRegs)
1394     {
1395 #if USE(JSVALUE64)
1396         move(cellGPR, boxedRegs.gpr());
1397 #else
1398         move(cellGPR, boxedRegs.payloadGPR());
1399         move(TrustedImm32(JSValue::CellTag), boxedRegs.tagGPR());
1400 #endif
1401     }
1402 
1403     void callExceptionFuzz(VM&amp;);
1404 
1405     enum ExceptionCheckKind { NormalExceptionCheck, InvertedExceptionCheck };
</pre>
<hr />
<pre>
1465     static VirtualRegister argumentsStart(InlineCallFrame* inlineCallFrame)
1466     {
1467         if (!inlineCallFrame)
1468             return VirtualRegister(CallFrame::argumentOffset(0));
1469         if (inlineCallFrame-&gt;argumentsWithFixup.size() &lt;= 1)
1470             return virtualRegisterForLocal(0);
1471         ValueRecovery recovery = inlineCallFrame-&gt;argumentsWithFixup[1];
1472         RELEASE_ASSERT(recovery.technique() == DisplacedInJSStack);
1473         return recovery.virtualRegister();
1474     }
1475 
1476     static VirtualRegister argumentsStart(const CodeOrigin&amp; codeOrigin)
1477     {
1478         return argumentsStart(codeOrigin.inlineCallFrame());
1479     }
1480 
1481     static VirtualRegister argumentCount(InlineCallFrame* inlineCallFrame)
1482     {
1483         ASSERT(!inlineCallFrame || inlineCallFrame-&gt;isVarargs());
1484         if (!inlineCallFrame)
<span class="line-modified">1485             return VirtualRegister(CallFrameSlot::argumentCount);</span>
1486         return inlineCallFrame-&gt;argumentCountRegister;
1487     }
1488 
1489     static VirtualRegister argumentCount(const CodeOrigin&amp; codeOrigin)
1490     {
1491         return argumentCount(codeOrigin.inlineCallFrame());
1492     }
1493 
1494     void emitLoadStructure(VM&amp;, RegisterID source, RegisterID dest, RegisterID scratch);
1495 
1496     void emitStoreStructureWithTypeInfo(TrustedImmPtr structure, RegisterID dest, RegisterID)
1497     {
1498         emitStoreStructureWithTypeInfo(*this, structure, dest);
1499     }
1500 
1501     void emitStoreStructureWithTypeInfo(RegisterID structure, RegisterID dest, RegisterID scratch)
1502     {
1503 #if USE(JSVALUE64)
1504         load64(MacroAssembler::Address(structure, Structure::structureIDOffset()), scratch);
1505         store64(scratch, MacroAssembler::Address(dest, JSCell::structureIDOffset()));
</pre>
<hr />
<pre>
1571         }
1572 #endif
1573         andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1574         addPtr(TrustedImmPtr(Gigacage::basePtr(kind)), storage);
1575 #if CPU(ARM64E)
1576         if (kind == Gigacage::Primitive)
1577             bitFieldInsert64(storage, 0, 64 - numberOfPACBits, tempReg);
1578 #endif
1579 
1580 #else
1581         UNUSED_PARAM(kind);
1582         UNUSED_PARAM(storage);
1583 #endif
1584     }
1585 
1586     // length may be the same register as scratch.
1587     void cageConditionally(Gigacage::Kind kind, GPRReg storage, GPRReg length, GPRReg scratch)
1588     {
1589 #if GIGACAGE_ENABLED
1590         if (Gigacage::isEnabled(kind)) {
<span class="line-modified">1591             if (kind != Gigacage::Primitive || Gigacage::isDisablingPrimitiveGigacageDisabled())</span>
1592                 cageWithoutUntagging(kind, storage);
1593             else {
1594 #if CPU(ARM64E)
1595                 if (length == scratch)
1596                     scratch = getCachedMemoryTempRegisterIDAndInvalidate();
1597 #endif
<span class="line-modified">1598                 loadPtr(&amp;Gigacage::basePtr(kind), scratch);</span>
1599                 Jump done = branchTest64(Zero, scratch);
1600 #if CPU(ARM64E)
1601                 GPRReg tempReg = getCachedDataTempRegisterIDAndInvalidate();
1602                 move(storage, tempReg);
1603                 ASSERT(LogicalImmediate::create64(Gigacage::mask(kind)).isValid());
1604                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), tempReg);
1605                 addPtr(scratch, tempReg);
1606                 bitFieldInsert64(tempReg, 0, 64 - numberOfPACBits, storage);
1607 #else
1608                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1609                 addPtr(scratch, storage);
1610 #endif // CPU(ARM64E)
1611                 done.link(this);
1612             }
1613         }
1614 #endif
1615 
1616 #if CPU(ARM64E)
1617         if (kind == Gigacage::Primitive)
1618             untagArrayPtr(length, storage);
1619 #endif
1620         UNUSED_PARAM(kind);
1621         UNUSED_PARAM(storage);
1622         UNUSED_PARAM(length);
1623         UNUSED_PARAM(scratch);
1624     }
1625 
1626     void emitComputeButterflyIndexingMask(GPRReg vectorLengthGPR, GPRReg scratchGPR, GPRReg resultGPR)
1627     {
1628         ASSERT(scratchGPR != resultGPR);
1629         Jump done;
<span class="line-removed">1630         if (isX86() &amp;&amp; !isX86_64()) {</span>
<span class="line-removed">1631             Jump nonZero = branchTest32(NonZero, vectorLengthGPR);</span>
<span class="line-removed">1632             move(TrustedImm32(0), resultGPR);</span>
<span class="line-removed">1633             done = jump();</span>
<span class="line-removed">1634             nonZero.link(this);</span>
<span class="line-removed">1635         }</span>
1636         // If vectorLength == 0 then clz will return 32 on both ARM and x86. On 64-bit systems, we can then do a 64-bit right shift on a 32-bit -1 to get a 0 mask for zero vectorLength. On 32-bit ARM, shift masks with 0xff, which means it will still create a 0 mask.
1637         countLeadingZeros32(vectorLengthGPR, scratchGPR);
1638         move(TrustedImm32(-1), resultGPR);
1639         urshiftPtr(scratchGPR, resultGPR);
1640         if (done.isSet())
1641             done.link(this);
1642     }
1643 
1644     // If for whatever reason the butterfly is going to change vector length this function does NOT
1645     // update the indexing mask.
1646     void nukeStructureAndStoreButterfly(VM&amp; vm, GPRReg butterfly, GPRReg object)
1647     {
1648         if (isX86()) {
1649             or32(TrustedImm32(bitwise_cast&lt;int32_t&gt;(nukedStructureIDBit())), Address(object, JSCell::structureIDOffset()));
1650             storePtr(butterfly, Address(object, JSObject::butterflyOffset()));
1651             return;
1652         }
1653 
1654         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1655         or32(TrustedImm32(bitwise_cast&lt;int32_t&gt;(nukedStructureIDBit())), Address(object, JSCell::structureIDOffset()));
</pre>
<hr />
<pre>
1740         notBigInt.link(this);
1741         functor(TypeofType::Symbol, false);
1742 
1743         notCell.link(this);
1744 
1745         Jump notNumber = branchIfNotNumber(regs, tempGPR);
1746         functor(TypeofType::Number, false);
1747         notNumber.link(this);
1748 
1749         JumpList notNull = branchIfNotEqual(regs, jsNull());
1750         functor(TypeofType::Object, false);
1751         notNull.link(this);
1752 
1753         Jump notBoolean = branchIfNotBoolean(regs, tempGPR);
1754         functor(TypeofType::Boolean, false);
1755         notBoolean.link(this);
1756 
1757         functor(TypeofType::Undefined, true);
1758     }
1759 
<span class="line-modified">1760     void emitDumbVirtualCall(VM&amp;, CallLinkInfo*);</span>
1761 
1762     void makeSpaceOnStackForCCall();
1763     void reclaimSpaceOnStackForCCall();
1764 
1765 #if USE(JSVALUE64)
1766     void emitRandomThunk(JSGlobalObject*, GPRReg scratch0, GPRReg scratch1, GPRReg scratch2, FPRReg result);
1767     void emitRandomThunk(VM&amp;, GPRReg scratch0, GPRReg scratch1, GPRReg scratch2, GPRReg scratch3, FPRReg result);
1768 #endif
1769 
1770     // Call this if you know that the value held in allocatorGPR is non-null. This DOES NOT mean
1771     // that allocator is non-null; allocator can be null as a signal that we don&#39;t know what the
1772     // value of allocatorGPR is. Additionally, if the allocator is not null, then there is no need
1773     // to populate allocatorGPR - this code will ignore the contents of allocatorGPR.
1774     void emitAllocateWithNonNullAllocator(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, GPRReg scratchGPR, JumpList&amp; slowPath);
1775 
1776     void emitAllocate(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, GPRReg scratchGPR, JumpList&amp; slowPath);
1777 
1778     template&lt;typename StructureType&gt;
1779     void emitAllocateJSCell(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure, GPRReg scratchGPR, JumpList&amp; slowPath)
1780     {
</pre>
<hr />
<pre>
1852     void emitInitializeInlineStorage(GPRReg baseGPR, GPRReg inlineCapacity)
1853     {
1854         Jump empty = branchTest32(Zero, inlineCapacity);
1855         Label loop = label();
1856         sub32(TrustedImm32(1), inlineCapacity);
1857         storeTrustedValue(JSValue(), BaseIndex(baseGPR, inlineCapacity, TimesEight, JSObject::offsetOfInlineStorage()));
1858         branchTest32(NonZero, inlineCapacity).linkTo(loop, this);
1859         empty.link(this);
1860     }
1861 
1862     void emitInitializeOutOfLineStorage(GPRReg butterflyGPR, unsigned outOfLineCapacity)
1863     {
1864         for (unsigned i = 0; i &lt; outOfLineCapacity; ++i)
1865             storeTrustedValue(JSValue(), Address(butterflyGPR, -sizeof(IndexingHeader) - (i + 1) * sizeof(EncodedJSValue)));
1866     }
1867 
1868 #if USE(JSVALUE64)
1869     void wangsInt64Hash(GPRReg inputAndResult, GPRReg scratch);
1870 #endif
1871 
<span class="line-removed">1872     // This assumes that index and length are 32-bit. This also assumes that they are already</span>
<span class="line-removed">1873     // zero-extended. Also this does not clobber index, which is useful in the baseline JIT. This</span>
<span class="line-removed">1874     // permits length and result to be in the same register.</span>
<span class="line-removed">1875     void emitPreparePreciseIndexMask32(GPRReg index, GPRReg length, GPRReg result);</span>
<span class="line-removed">1876 </span>
1877 #if ENABLE(WEBASSEMBLY)
1878     void loadWasmContextInstance(GPRReg dst);
1879     void storeWasmContextInstance(GPRReg src);
1880     static bool loadWasmContextInstanceNeedsMacroScratchRegister();
1881     static bool storeWasmContextInstanceNeedsMacroScratchRegister();
1882 #endif
1883 
1884 protected:
1885     void copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(GPRReg calleeSavesBuffer);
1886 
1887     CodeBlock* m_codeBlock;
1888     CodeBlock* m_baselineCodeBlock;
1889 };
1890 
1891 } // namespace JSC
1892 
1893 #endif // ENABLE(JIT)
</pre>
</td>
<td>
<hr />
<pre>
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(JIT)
  29 
  30 #include &quot;CodeBlock.h&quot;
  31 #include &quot;EntryFrame.h&quot;
  32 #include &quot;FPRInfo.h&quot;
  33 #include &quot;GPRInfo.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InlineCallFrame.h&quot;
  36 #include &quot;JITAllocator.h&quot;
  37 #include &quot;JITCode.h&quot;
<span class="line-added">  38 #include &quot;JSCellInlines.h&quot;</span>
  39 #include &quot;MacroAssembler.h&quot;
  40 #include &quot;MarkedSpace.h&quot;
  41 #include &quot;RegisterAtOffsetList.h&quot;
  42 #include &quot;RegisterSet.h&quot;
  43 #include &quot;StackAlignment.h&quot;
  44 #include &quot;TagRegistersMode.h&quot;
  45 #include &quot;TypeofType.h&quot;
  46 #include &quot;VM.h&quot;
  47 
  48 namespace JSC {
  49 
<span class="line-modified">  50 typedef void (*V_DebugOperation_EPP)(CallFrame*, void*, void*);</span>
  51 
  52 class AssemblyHelpers : public MacroAssembler {
  53 public:
  54     AssemblyHelpers(CodeBlock* codeBlock)
  55         : m_codeBlock(codeBlock)
  56         , m_baselineCodeBlock(codeBlock ? codeBlock-&gt;baselineAlternative() : 0)
  57     {
  58         if (m_codeBlock) {
  59             ASSERT(m_baselineCodeBlock);
  60             ASSERT(!m_baselineCodeBlock-&gt;alternative());
  61             ASSERT(m_baselineCodeBlock-&gt;jitType() == JITType::None || JITCode::isBaselineCode(m_baselineCodeBlock-&gt;jitType()));
  62         }
  63     }
  64 
  65     CodeBlock* codeBlock() { return m_codeBlock; }
  66     VM&amp; vm() { return m_codeBlock-&gt;vm(); }
  67     AssemblerType_T&amp; assembler() { return m_assembler; }
  68 
<span class="line-added">  69     void prepareCallOperation(VM&amp; vm)</span>
<span class="line-added">  70     {</span>
<span class="line-added">  71         UNUSED_PARAM(vm);</span>
<span class="line-added">  72 #if !USE(BUILTIN_FRAME_ADDRESS) || ASSERT_ENABLED</span>
<span class="line-added">  73         storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);</span>
<span class="line-added">  74 #endif</span>
<span class="line-added">  75     }</span>
<span class="line-added">  76 </span>
  77     void checkStackPointerAlignment()
  78     {
  79         // This check is both unneeded and harder to write correctly for ARM64
  80 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM64)
  81         Jump stackPointerAligned = branchTestPtr(Zero, stackPointerRegister, TrustedImm32(0xf));
  82         abortWithReason(AHStackPointerMisaligned);
  83         stackPointerAligned.link(this);
  84 #endif
  85     }
  86 
<span class="line-added">  87 #if USE(JSVALUE64)</span>
<span class="line-added">  88     void store64FromReg(Reg src, Address dst)</span>
<span class="line-added">  89     {</span>
<span class="line-added">  90         if (src.isFPR())</span>
<span class="line-added">  91             storeDouble(src.fpr(), dst);</span>
<span class="line-added">  92         else</span>
<span class="line-added">  93             store64(src.gpr(), dst);</span>
<span class="line-added">  94     }</span>
<span class="line-added">  95 #endif</span>
<span class="line-added">  96 </span>
<span class="line-added">  97     void store32FromReg(Reg src, Address dst)</span>
<span class="line-added">  98     {</span>
<span class="line-added">  99         if (src.isFPR())</span>
<span class="line-added"> 100             storeFloat(src.fpr(), dst);</span>
<span class="line-added"> 101         else</span>
<span class="line-added"> 102             store32(src.gpr(), dst);</span>
<span class="line-added"> 103     }</span>
<span class="line-added"> 104 </span>
<span class="line-added"> 105 #if USE(JSVALUE64)</span>
<span class="line-added"> 106     void load64ToReg(Address src, Reg dst)</span>
<span class="line-added"> 107     {</span>
<span class="line-added"> 108         if (dst.isFPR())</span>
<span class="line-added"> 109             loadDouble(src, dst.fpr());</span>
<span class="line-added"> 110         else</span>
<span class="line-added"> 111             load64(src, dst.gpr());</span>
<span class="line-added"> 112     }</span>
<span class="line-added"> 113 #endif</span>
<span class="line-added"> 114 </span>
<span class="line-added"> 115     void load32ToReg(Address src, Reg dst)</span>
<span class="line-added"> 116     {</span>
<span class="line-added"> 117         if (dst.isFPR())</span>
<span class="line-added"> 118             loadFloat(src, dst.fpr());</span>
<span class="line-added"> 119         else</span>
<span class="line-added"> 120             load32(src, dst.gpr());</span>
<span class="line-added"> 121     }</span>
<span class="line-added"> 122 </span>
 123     template&lt;typename T&gt;
 124     void storeCell(T cell, Address address)
 125     {
 126 #if USE(JSVALUE64)
 127         store64(cell, address);
 128 #else
 129         store32(cell, address.withOffset(PayloadOffset));
 130         store32(TrustedImm32(JSValue::CellTag), address.withOffset(TagOffset));
 131 #endif
 132     }
 133 
 134     void loadCell(Address address, GPRReg gpr)
 135     {
 136 #if USE(JSVALUE64)
 137         load64(address, gpr);
 138 #else
 139         load32(address.withOffset(PayloadOffset), gpr);
 140 #endif
 141     }
 142 
</pre>
<hr />
<pre>
 362         RegisterSet dontRestoreRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 363         unsigned registerCount = calleeSaves-&gt;size();
 364 
 365         for (unsigned i = 0; i &lt; registerCount; i++) {
 366             RegisterAtOffset entry = calleeSaves-&gt;at(i);
 367             if (dontRestoreRegisters.get(entry.reg()))
 368                 continue;
 369             loadPtr(Address(framePointerRegister, entry.offset()), entry.reg().gpr());
 370         }
 371     }
 372 
 373     void emitSaveCalleeSaves()
 374     {
 375         emitSaveCalleeSavesFor(codeBlock());
 376     }
 377 
 378     void emitSaveThenMaterializeTagRegisters()
 379     {
 380 #if USE(JSVALUE64)
 381 #if CPU(ARM64)
<span class="line-modified"> 382         pushPair(GPRInfo::numberTagRegister, GPRInfo::notCellMaskRegister);</span>
 383 #else
<span class="line-modified"> 384         push(GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 385         push(GPRInfo::notCellMaskRegister);</span>
 386 #endif
 387         emitMaterializeTagCheckRegisters();
 388 #endif
 389     }
 390 
 391     void emitRestoreCalleeSaves()
 392     {
 393         emitRestoreCalleeSavesFor(codeBlock());
 394     }
 395 
 396     void emitRestoreSavedTagRegisters()
 397     {
 398 #if USE(JSVALUE64)
 399 #if CPU(ARM64)
<span class="line-modified"> 400         popPair(GPRInfo::numberTagRegister, GPRInfo::notCellMaskRegister);</span>
 401 #else
<span class="line-modified"> 402         pop(GPRInfo::notCellMaskRegister);</span>
<span class="line-modified"> 403         pop(GPRInfo::numberTagRegister);</span>
 404 #endif
 405 #endif
 406     }
 407 
 408     // If you use this, be aware that vmGPR will get trashed.
 409     void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(GPRReg vmGPR)
 410     {
 411 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 412         loadPtr(Address(vmGPR, VM::topEntryFrameOffset()), vmGPR);
 413         copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(vmGPR);
 414 #else
 415         UNUSED_PARAM(vmGPR);
 416 #endif
 417     }
 418 
 419     void copyCalleeSavesToEntryFrameCalleeSavesBuffer(EntryFrame*&amp; topEntryFrame)
 420     {
 421 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 422         const TempRegisterSet&amp; usedRegisters = { RegisterSet::stubUnavailableRegisters() };
 423         GPRReg temp1 = usedRegisters.getFreeGPR(0);
</pre>
<hr />
<pre>
 480                 if (currentFrameEntry) {
 481                     // Load calleeSave from stack into temp register
 482                     fpRegToStore = fpTemp;
 483                     loadDouble(Address(framePointerRegister, currentFrameEntry-&gt;offset()), fpRegToStore);
 484                 } else
 485                     // Just store callee save directly
 486                     fpRegToStore = entry.reg().fpr();
 487 
 488                 storeDouble(fpRegToStore, Address(temp1, entry.offset()));
 489             }
 490         }
 491 #else
 492         UNUSED_PARAM(topEntryFrame);
 493         UNUSED_PARAM(usedRegisters);
 494 #endif
 495     }
 496 
 497     void emitMaterializeTagCheckRegisters()
 498     {
 499 #if USE(JSVALUE64)
<span class="line-modified"> 500         move(MacroAssembler::TrustedImm64(JSValue::NumberTag), GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 501         orPtr(MacroAssembler::TrustedImm32(JSValue::OtherTag), GPRInfo::numberTagRegister, GPRInfo::notCellMaskRegister);</span>
 502 #endif
 503     }
 504 
<span class="line-modified"> 505 #if CPU(X86_64)</span>
























 506     static constexpr size_t prologueStackPointerDelta()
 507     {
 508         // Prologue only saves the framePointerRegister
 509         return sizeof(void*);
 510     }
 511 
 512     void emitFunctionPrologue()
 513     {
 514         push(framePointerRegister);
 515         move(stackPointerRegister, framePointerRegister);
 516     }
 517 
 518     void emitFunctionEpilogueWithEmptyFrame()
 519     {
 520         pop(framePointerRegister);
 521     }
 522 
 523     void emitFunctionEpilogue()
 524     {
 525         move(framePointerRegister, stackPointerRegister);
 526         pop(framePointerRegister);
 527     }
 528 
 529     void preserveReturnAddressAfterCall(GPRReg reg)
 530     {
 531         pop(reg);
 532     }
 533 
 534     void restoreReturnAddressBeforeReturn(GPRReg reg)
 535     {
 536         push(reg);
 537     }
 538 
 539     void restoreReturnAddressBeforeReturn(Address address)
 540     {
 541         push(address);
 542     }
<span class="line-modified"> 543 #endif // CPU(X86_64)</span>
 544 
 545 #if CPU(ARM_THUMB2) || CPU(ARM64)
 546     static constexpr size_t prologueStackPointerDelta()
 547     {
 548         // Prologue saves the framePointerRegister and linkRegister
 549         return 2 * sizeof(void*);
 550     }
 551 
 552     void emitFunctionPrologue()
 553     {
 554         tagReturnAddress();
 555         pushPair(framePointerRegister, linkRegister);
 556         move(stackPointerRegister, framePointerRegister);
 557     }
 558 
 559     void emitFunctionEpilogueWithEmptyFrame()
 560     {
 561         popPair(framePointerRegister, linkRegister);
 562     }
 563 
</pre>
<hr />
<pre>
 606         move(framePointerRegister, stackPointerRegister);
 607         emitFunctionEpilogueWithEmptyFrame();
 608     }
 609 
 610     ALWAYS_INLINE void preserveReturnAddressAfterCall(RegisterID reg)
 611     {
 612         move(returnAddressRegister, reg);
 613     }
 614 
 615     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(RegisterID reg)
 616     {
 617         move(reg, returnAddressRegister);
 618     }
 619 
 620     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(Address address)
 621     {
 622         loadPtr(address, returnAddressRegister);
 623     }
 624 #endif
 625 
<span class="line-modified"> 626     void emitGetFromCallFrameHeaderPtr(VirtualRegister entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 627     {
<span class="line-modified"> 628         loadPtr(Address(from, entry.offset() * sizeof(Register)), to);</span>
 629     }
<span class="line-modified"> 630     void emitGetFromCallFrameHeader32(VirtualRegister entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 631     {
<span class="line-modified"> 632         load32(Address(from, entry.offset() * sizeof(Register)), to);</span>
 633     }
 634 #if USE(JSVALUE64)
<span class="line-modified"> 635     void emitGetFromCallFrameHeader64(VirtualRegister entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 636     {
<span class="line-modified"> 637         load64(Address(from, entry.offset() * sizeof(Register)), to);</span>
 638     }
 639 #endif // USE(JSVALUE64)
<span class="line-modified"> 640     void emitPutToCallFrameHeader(GPRReg from, VirtualRegister entry)</span>
 641     {
<span class="line-modified"> 642         storePtr(from, Address(GPRInfo::callFrameRegister, entry.offset() * sizeof(Register)));</span>
 643     }
 644 
<span class="line-modified"> 645     void emitPutToCallFrameHeader(void* value, VirtualRegister entry)</span>
 646     {
<span class="line-modified"> 647         storePtr(TrustedImmPtr(value), Address(GPRInfo::callFrameRegister, entry.offset() * sizeof(Register)));</span>
 648     }
 649 
 650     void emitGetCallerFrameFromCallFrameHeaderPtr(RegisterID to)
 651     {
 652         loadPtr(Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), to);
 653     }
 654     void emitPutCallerFrameToCallFrameHeader(RegisterID from)
 655     {
 656         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()));
 657     }
 658 
 659     void emitPutReturnPCToCallFrameHeader(RegisterID from)
 660     {
 661         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 662     }
 663     void emitPutReturnPCToCallFrameHeader(TrustedImmPtr from)
 664     {
 665         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 666     }
 667 
 668     // emitPutToCallFrameHeaderBeforePrologue() and related are used to access callee frame header
 669     // fields before the code from emitFunctionPrologue() has executed.
 670     // First, the access is via the stack pointer. Second, the address calculation must also take
 671     // into account that the stack pointer may not have been adjusted down for the return PC and/or
 672     // caller&#39;s frame pointer. On some platforms, the callee is responsible for pushing the
 673     // &quot;link register&quot; containing the return address in the function prologue.
 674 #if USE(JSVALUE64)
<span class="line-modified"> 675     void emitPutToCallFrameHeaderBeforePrologue(GPRReg from, VirtualRegister entry)</span>
 676     {
<span class="line-modified"> 677         storePtr(from, Address(stackPointerRegister, entry.offset() * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta()));</span>
 678     }
 679 #else
<span class="line-modified"> 680     void emitPutPayloadToCallFrameHeaderBeforePrologue(GPRReg from, VirtualRegister entry)</span>
 681     {
<span class="line-modified"> 682         storePtr(from, Address(stackPointerRegister, entry.offset() * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta() + OBJECT_OFFSETOF(EncodedValueDescriptor, asBits.payload)));</span>
 683     }
 684 
<span class="line-modified"> 685     void emitPutTagToCallFrameHeaderBeforePrologue(TrustedImm32 tag, VirtualRegister entry)</span>
 686     {
<span class="line-modified"> 687         storePtr(tag, Address(stackPointerRegister, entry.offset() * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta() + OBJECT_OFFSETOF(EncodedValueDescriptor, asBits.tag)));</span>
 688     }
 689 #endif
 690 
 691     JumpList branchIfNotEqual(JSValueRegs regs, JSValue value)
 692     {
 693 #if USE(JSVALUE64)
 694         return branch64(NotEqual, regs.gpr(), TrustedImm64(JSValue::encode(value)));
 695 #else
 696         JumpList result;
 697         result.append(branch32(NotEqual, regs.tagGPR(), TrustedImm32(value.tag())));
 698         if (value.isEmpty() || value.isUndefinedOrNull())
 699             return result; // These don&#39;t have anything interesting in the payload.
 700         result.append(branch32(NotEqual, regs.payloadGPR(), TrustedImm32(value.payload())));
 701         return result;
 702 #endif
 703     }
 704 
 705     Jump branchIfEqual(JSValueRegs regs, JSValue value)
 706     {
 707 #if USE(JSVALUE64)
 708         return branch64(Equal, regs.gpr(), TrustedImm64(JSValue::encode(value)));
 709 #else
 710         Jump notEqual;
 711         // These don&#39;t have anything interesting in the payload.
 712         if (!value.isEmpty() &amp;&amp; !value.isUndefinedOrNull())
 713             notEqual = branch32(NotEqual, regs.payloadGPR(), TrustedImm32(value.payload()));
 714         Jump result = branch32(Equal, regs.tagGPR(), TrustedImm32(value.tag()));
 715         if (notEqual.isSet())
 716             notEqual.link(this);
 717         return result;
 718 #endif
 719     }
 720 
 721     Jump branchIfNotCell(GPRReg reg, TagRegistersMode mode = HaveTagRegisters)
 722     {
 723 #if USE(JSVALUE64)
 724         if (mode == HaveTagRegisters)
<span class="line-modified"> 725             return branchTest64(NonZero, reg, GPRInfo::notCellMaskRegister);</span>
<span class="line-modified"> 726         return branchTest64(NonZero, reg, TrustedImm64(JSValue::NotCellMask));</span>
 727 #else
 728         UNUSED_PARAM(mode);
 729         return branch32(MacroAssembler::NotEqual, reg, TrustedImm32(JSValue::CellTag));
 730 #endif
 731     }
 732 
 733     Jump branchIfNotCell(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 734     {
 735 #if USE(JSVALUE64)
 736         return branchIfNotCell(regs.gpr(), mode);
 737 #else
 738         return branchIfNotCell(regs.tagGPR(), mode);
 739 #endif
 740     }
 741 
 742     Jump branchIfCell(GPRReg reg, TagRegistersMode mode = HaveTagRegisters)
 743     {
 744 #if USE(JSVALUE64)
 745         if (mode == HaveTagRegisters)
<span class="line-modified"> 746             return branchTest64(Zero, reg, GPRInfo::notCellMaskRegister);</span>
<span class="line-modified"> 747         return branchTest64(Zero, reg, TrustedImm64(JSValue::NotCellMask));</span>
 748 #else
 749         UNUSED_PARAM(mode);
 750         return branch32(MacroAssembler::Equal, reg, TrustedImm32(JSValue::CellTag));
 751 #endif
 752     }
 753     Jump branchIfCell(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 754     {
 755 #if USE(JSVALUE64)
 756         return branchIfCell(regs.gpr(), mode);
 757 #else
 758         return branchIfCell(regs.tagGPR(), mode);
 759 #endif
 760     }
 761 
 762     Jump branchIfOther(JSValueRegs regs, GPRReg tempGPR)
 763     {
 764 #if USE(JSVALUE64)
 765         move(regs.gpr(), tempGPR);
<span class="line-modified"> 766         and64(TrustedImm32(~JSValue::UndefinedTag), tempGPR);</span>
<span class="line-modified"> 767         return branch64(Equal, tempGPR, TrustedImm64(JSValue::ValueNull));</span>
 768 #else
 769         or32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 770         return branch32(Equal, tempGPR, TrustedImm32(JSValue::NullTag));
 771 #endif
 772     }
 773 
 774     Jump branchIfNotOther(JSValueRegs regs, GPRReg tempGPR)
 775     {
 776 #if USE(JSVALUE64)
 777         move(regs.gpr(), tempGPR);
<span class="line-modified"> 778         and64(TrustedImm32(~JSValue::UndefinedTag), tempGPR);</span>
<span class="line-modified"> 779         return branch64(NotEqual, tempGPR, TrustedImm64(JSValue::ValueNull));</span>
 780 #else
 781         or32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 782         return branch32(NotEqual, tempGPR, TrustedImm32(JSValue::NullTag));
 783 #endif
 784     }
 785 
 786     Jump branchIfInt32(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 787     {
 788 #if USE(JSVALUE64)
 789         if (mode == HaveTagRegisters)
<span class="line-modified"> 790             return branch64(AboveOrEqual, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 791         return branch64(AboveOrEqual, gpr, TrustedImm64(JSValue::NumberTag));</span>
 792 #else
 793         UNUSED_PARAM(mode);
 794         return branch32(Equal, gpr, TrustedImm32(JSValue::Int32Tag));
 795 #endif
 796     }
 797 
 798     Jump branchIfInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 799     {
 800 #if USE(JSVALUE64)
 801         return branchIfInt32(regs.gpr(), mode);
 802 #else
 803         return branchIfInt32(regs.tagGPR(), mode);
 804 #endif
 805     }
 806 
 807     Jump branchIfNotInt32(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 808     {
 809 #if USE(JSVALUE64)
 810         if (mode == HaveTagRegisters)
<span class="line-modified"> 811             return branch64(Below, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 812         return branch64(Below, gpr, TrustedImm64(JSValue::NumberTag));</span>
 813 #else
 814         UNUSED_PARAM(mode);
 815         return branch32(NotEqual, gpr, TrustedImm32(JSValue::Int32Tag));
 816 #endif
 817     }
 818 
 819     Jump branchIfNotInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 820     {
 821 #if USE(JSVALUE64)
 822         return branchIfNotInt32(regs.gpr(), mode);
 823 #else
 824         return branchIfNotInt32(regs.tagGPR(), mode);
 825 #endif
 826     }
 827 
 828     // Note that the tempGPR is not used in 64-bit mode.
 829     Jump branchIfNumber(JSValueRegs regs, GPRReg tempGPR, TagRegistersMode mode = HaveTagRegisters)
 830     {
 831 #if USE(JSVALUE64)
 832         UNUSED_PARAM(tempGPR);
 833         return branchIfNumber(regs.gpr(), mode);
 834 #else
 835         UNUSED_PARAM(mode);
 836         ASSERT(tempGPR != InvalidGPRReg);
 837         add32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 838         return branch32(Below, tempGPR, TrustedImm32(JSValue::LowestTag + 1));
 839 #endif
 840     }
 841 
 842 #if USE(JSVALUE64)
 843     Jump branchIfNumber(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 844     {
 845         if (mode == HaveTagRegisters)
<span class="line-modified"> 846             return branchTest64(NonZero, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 847         return branchTest64(NonZero, gpr, TrustedImm64(JSValue::NumberTag));</span>
 848     }
 849 #endif
 850 
 851     // Note that the tempGPR is not used in 64-bit mode.
 852     Jump branchIfNotNumber(JSValueRegs regs, GPRReg tempGPR, TagRegistersMode mode = HaveTagRegisters)
 853     {
 854 #if USE(JSVALUE64)
 855         UNUSED_PARAM(tempGPR);
 856         return branchIfNotNumber(regs.gpr(), mode);
 857 #else
 858         UNUSED_PARAM(mode);
 859         add32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 860         return branch32(AboveOrEqual, tempGPR, TrustedImm32(JSValue::LowestTag + 1));
 861 #endif
 862     }
 863 
 864 #if USE(JSVALUE64)
 865     Jump branchIfNotNumber(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 866     {
 867         if (mode == HaveTagRegisters)
<span class="line-modified"> 868             return branchTest64(Zero, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 869         return branchTest64(Zero, gpr, TrustedImm64(JSValue::NumberTag));</span>
 870     }
 871 #endif
 872 
 873     Jump branchIfNotDoubleKnownNotInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 874     {
 875 #if USE(JSVALUE64)
 876         if (mode == HaveTagRegisters)
<span class="line-modified"> 877             return branchTest64(Zero, regs.gpr(), GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 878         return branchTest64(Zero, regs.gpr(), TrustedImm64(JSValue::NumberTag));</span>
 879 #else
 880         UNUSED_PARAM(mode);
 881         return branch32(AboveOrEqual, regs.tagGPR(), TrustedImm32(JSValue::LowestTag));
 882 #endif
 883     }
 884 
 885     // Note that the tempGPR is not used in 32-bit mode.
 886     Jump branchIfBoolean(GPRReg gpr, GPRReg tempGPR)
 887     {
 888 #if USE(JSVALUE64)
 889         ASSERT(tempGPR != InvalidGPRReg);
 890         move(gpr, tempGPR);
<span class="line-modified"> 891         xor64(TrustedImm32(JSValue::ValueFalse), tempGPR);</span>
 892         return branchTest64(Zero, tempGPR, TrustedImm32(static_cast&lt;int32_t&gt;(~1)));
 893 #else
 894         UNUSED_PARAM(tempGPR);
 895         return branch32(Equal, gpr, TrustedImm32(JSValue::BooleanTag));
 896 #endif
 897     }
 898 
 899     // Note that the tempGPR is not used in 32-bit mode.
 900     Jump branchIfBoolean(JSValueRegs regs, GPRReg tempGPR)
 901     {
 902 #if USE(JSVALUE64)
 903         return branchIfBoolean(regs.gpr(), tempGPR);
 904 #else
 905         return branchIfBoolean(regs.tagGPR(), tempGPR);
 906 #endif
 907     }
 908 
 909     // Note that the tempGPR is not used in 32-bit mode.
 910     Jump branchIfNotBoolean(GPRReg gpr, GPRReg tempGPR)
 911     {
 912 #if USE(JSVALUE64)
 913         ASSERT(tempGPR != InvalidGPRReg);
 914         move(gpr, tempGPR);
<span class="line-modified"> 915         xor64(TrustedImm32(JSValue::ValueFalse), tempGPR);</span>
 916         return branchTest64(NonZero, tempGPR, TrustedImm32(static_cast&lt;int32_t&gt;(~1)));
 917 #else
 918         UNUSED_PARAM(tempGPR);
 919         return branch32(NotEqual, gpr, TrustedImm32(JSValue::BooleanTag));
 920 #endif
 921     }
 922 
 923     // Note that the tempGPR is not used in 32-bit mode.
 924     Jump branchIfNotBoolean(JSValueRegs regs, GPRReg tempGPR)
 925     {
 926 #if USE(JSVALUE64)
 927         return branchIfNotBoolean(regs.gpr(), tempGPR);
 928 #else
 929         return branchIfNotBoolean(regs.tagGPR(), tempGPR);
 930 #endif
 931     }
 932 
 933     Jump branchIfObject(GPRReg cellGPR)
 934     {
 935         return branch8(
</pre>
<hr />
<pre>
1107     }
1108 
1109     static Address addressForByteOffset(ptrdiff_t byteOffset)
1110     {
1111         return Address(GPRInfo::callFrameRegister, byteOffset);
1112     }
1113     static Address addressFor(VirtualRegister virtualRegister, GPRReg baseReg)
1114     {
1115         ASSERT(virtualRegister.isValid());
1116         return Address(baseReg, virtualRegister.offset() * sizeof(Register));
1117     }
1118     static Address addressFor(VirtualRegister virtualRegister)
1119     {
1120         // NB. It&#39;s tempting on some architectures to sometimes use an offset from the stack
1121         // register because for some offsets that will encode to a smaller instruction. But we
1122         // cannot do this. We use this in places where the stack pointer has been moved to some
1123         // unpredictable location.
1124         ASSERT(virtualRegister.isValid());
1125         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register));
1126     }
<span class="line-modified">1127     static Address addressFor(Operand operand)</span>
1128     {
<span class="line-modified">1129         ASSERT(!operand.isTmp());</span>
<span class="line-added">1130         return addressFor(operand.virtualRegister());</span>
1131     }
1132 
1133     static Address tagFor(VirtualRegister virtualRegister, GPRReg baseGPR)
1134     {
1135         ASSERT(virtualRegister.isValid());
1136         return Address(baseGPR, virtualRegister.offset() * sizeof(Register) + TagOffset);
1137     }
1138     static Address tagFor(VirtualRegister virtualRegister)
1139     {
1140         ASSERT(virtualRegister.isValid());
1141         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register) + TagOffset);
1142     }
<span class="line-modified">1143     static Address tagFor(Operand operand)</span>
1144     {
<span class="line-modified">1145         ASSERT(!operand.isTmp());</span>
<span class="line-added">1146         return tagFor(operand.virtualRegister());</span>
1147     }
1148 
1149     static Address payloadFor(VirtualRegister virtualRegister, GPRReg baseGPR)
1150     {
1151         ASSERT(virtualRegister.isValid());
1152         return Address(baseGPR, virtualRegister.offset() * sizeof(Register) + PayloadOffset);
1153     }
1154     static Address payloadFor(VirtualRegister virtualRegister)
1155     {
1156         ASSERT(virtualRegister.isValid());
1157         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register) + PayloadOffset);
1158     }
<span class="line-modified">1159     static Address payloadFor(Operand operand)</span>
1160     {
<span class="line-modified">1161         ASSERT(!operand.isTmp());</span>
<span class="line-added">1162         return payloadFor(operand.virtualRegister());</span>
1163     }
1164 
1165     // Access to our fixed callee CallFrame.
<span class="line-modified">1166     static Address calleeFrameSlot(VirtualRegister slot)</span>
1167     {
<span class="line-modified">1168         ASSERT(slot.offset() &gt;= CallerFrameAndPC::sizeInRegisters);</span>
<span class="line-modified">1169         return Address(stackPointerRegister, sizeof(Register) * (slot - CallerFrameAndPC::sizeInRegisters).offset());</span>
1170     }
1171 
1172     // Access to our fixed callee CallFrame.
1173     static Address calleeArgumentSlot(int argument)
1174     {
<span class="line-modified">1175         return calleeFrameSlot(virtualRegisterForArgumentIncludingThis(argument));</span>
1176     }
1177 
<span class="line-modified">1178     static Address calleeFrameTagSlot(VirtualRegister slot)</span>
1179     {
1180         return calleeFrameSlot(slot).withOffset(TagOffset);
1181     }
1182 
<span class="line-modified">1183     static Address calleeFramePayloadSlot(VirtualRegister slot)</span>
1184     {
1185         return calleeFrameSlot(slot).withOffset(PayloadOffset);
1186     }
1187 
1188     static Address calleeArgumentTagSlot(int argument)
1189     {
1190         return calleeArgumentSlot(argument).withOffset(TagOffset);
1191     }
1192 
1193     static Address calleeArgumentPayloadSlot(int argument)
1194     {
1195         return calleeArgumentSlot(argument).withOffset(PayloadOffset);
1196     }
1197 
1198     static Address calleeFrameCallerFrame()
1199     {
<span class="line-modified">1200         return calleeFrameSlot(VirtualRegister(0)).withOffset(CallFrame::callerFrameOffset());</span>
1201     }
1202 
1203     static GPRReg selectScratchGPR(RegisterSet preserved)
1204     {
1205         GPRReg registers[] = {
1206             GPRInfo::regT0,
1207             GPRInfo::regT1,
1208             GPRInfo::regT2,
1209             GPRInfo::regT3,
1210             GPRInfo::regT4,
1211             GPRInfo::regT5,
1212         };
1213 
1214         for (GPRReg reg : registers) {
1215             if (!preserved.contains(reg))
1216                 return reg;
1217         }
1218         RELEASE_ASSERT_NOT_REACHED();
1219         return InvalidGPRReg;
1220     }
</pre>
<hr />
<pre>
1236     {
1237         if (regs.tagGPR() != InvalidGPRReg)
1238             set.set(regs.tagGPR());
1239         if (regs.payloadGPR() != InvalidGPRReg)
1240             set.set(regs.payloadGPR());
1241         constructRegisterSet(set, args...);
1242     }
1243 
1244     template&lt;typename... Regs&gt;
1245     static void constructRegisterSet(RegisterSet&amp; set, GPRReg reg, Regs... args)
1246     {
1247         if (reg != InvalidGPRReg)
1248             set.set(reg);
1249         constructRegisterSet(set, args...);
1250     }
1251 
1252     // Add a debug call. This call has no effect on JIT code execution state.
1253     void debugCall(VM&amp;, V_DebugOperation_EPP function, void* argument);
1254 
1255     // These methods JIT generate dynamic, debug-only checks - akin to ASSERTs.
<span class="line-modified">1256 #if ASSERT_ENABLED</span>
1257     void jitAssertIsInt32(GPRReg);
1258     void jitAssertIsJSInt32(GPRReg);
1259     void jitAssertIsJSNumber(GPRReg);
1260     void jitAssertIsJSDouble(GPRReg);
1261     void jitAssertIsCell(GPRReg);
1262     void jitAssertHasValidCallFrame();
1263     void jitAssertIsNull(GPRReg);
1264     void jitAssertTagsInPlace();
1265     void jitAssertArgumentCountSane();
1266 #else
1267     void jitAssertIsInt32(GPRReg) { }
1268     void jitAssertIsJSInt32(GPRReg) { }
1269     void jitAssertIsJSNumber(GPRReg) { }
1270     void jitAssertIsJSDouble(GPRReg) { }
1271     void jitAssertIsCell(GPRReg) { }
1272     void jitAssertHasValidCallFrame() { }
1273     void jitAssertIsNull(GPRReg) { }
1274     void jitAssertTagsInPlace() { }
1275     void jitAssertArgumentCountSane() { }
1276 #endif
1277 
1278     void jitReleaseAssertNoException(VM&amp;);
1279 
1280     void incrementSuperSamplerCount();
1281     void decrementSuperSamplerCount();
1282 
1283     void purifyNaN(FPRReg);
1284 
1285     // These methods convert between doubles, and doubles boxed and JSValues.
1286 #if USE(JSVALUE64)
1287     GPRReg boxDouble(FPRReg fpr, GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
1288     {
1289         moveDoubleTo64(fpr, gpr);
1290         if (mode == DoNotHaveTagRegisters)
<span class="line-modified">1291             sub64(TrustedImm64(JSValue::NumberTag), gpr);</span>
1292         else {
<span class="line-modified">1293             sub64(GPRInfo::numberTagRegister, gpr);</span>
1294             jitAssertIsJSDouble(gpr);
1295         }
1296         return gpr;
1297     }
1298     FPRReg unboxDoubleWithoutAssertions(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
1299     {
<span class="line-modified">1300         add64(GPRInfo::numberTagRegister, gpr, resultGPR);</span>
1301         move64ToDouble(resultGPR, fpr);
1302         return fpr;
1303     }
1304     FPRReg unboxDouble(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
1305     {
1306         jitAssertIsJSDouble(gpr);
1307         return unboxDoubleWithoutAssertions(gpr, resultGPR, fpr);
1308     }
1309 
1310     void boxDouble(FPRReg fpr, JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
1311     {
1312         boxDouble(fpr, regs.gpr(), mode);
1313     }
1314 
1315     void unboxDoubleNonDestructive(JSValueRegs regs, FPRReg destFPR, GPRReg resultGPR, FPRReg)
1316     {
1317         unboxDouble(regs.payloadGPR(), resultGPR, destFPR);
1318     }
1319 
1320     // Here are possible arrangements of source, target, scratch:
1321     // - source, target, scratch can all be separate registers.
1322     // - source and target can be the same but scratch is separate.
1323     // - target and scratch can be the same but source is separate.
1324     void boxInt52(GPRReg source, GPRReg target, GPRReg scratch, FPRReg fpScratch)
1325     {
1326         // Is it an int32?
1327         signExtend32ToPtr(source, scratch);
1328         Jump isInt32 = branch64(Equal, source, scratch);
1329 
1330         // Nope, it&#39;s not, but regT0 contains the int64 value.
1331         convertInt64ToDouble(source, fpScratch);
1332         boxDouble(fpScratch, target);
1333         Jump done = jump();
1334 
1335         isInt32.link(this);
1336         zeroExtend32ToPtr(source, target);
<span class="line-modified">1337         or64(GPRInfo::numberTagRegister, target);</span>
1338 
1339         done.link(this);
1340     }
1341 #endif
1342 
1343 #if USE(JSVALUE32_64)
1344     void boxDouble(FPRReg fpr, GPRReg tagGPR, GPRReg payloadGPR)
1345     {
1346         moveDoubleToInts(fpr, payloadGPR, tagGPR);
1347     }
1348     void unboxDouble(GPRReg tagGPR, GPRReg payloadGPR, FPRReg fpr, FPRReg scratchFPR)
1349     {
1350         moveIntsToDouble(payloadGPR, tagGPR, fpr, scratchFPR);
1351     }
1352 
1353     void boxDouble(FPRReg fpr, JSValueRegs regs)
1354     {
1355         boxDouble(fpr, regs.tagGPR(), regs.payloadGPR());
1356     }
1357     void unboxDouble(JSValueRegs regs, FPRReg fpr, FPRReg scratchFPR)
1358     {
1359         unboxDouble(regs.tagGPR(), regs.payloadGPR(), fpr, scratchFPR);
1360     }
1361 
1362     void unboxDoubleNonDestructive(const JSValueRegs regs, FPRReg destFPR, GPRReg, FPRReg scratchFPR)
1363     {
1364         unboxDouble(regs, destFPR, scratchFPR);
1365     }
1366 #endif
1367 
1368     void boxBooleanPayload(GPRReg boolGPR, GPRReg payloadGPR)
1369     {
1370 #if USE(JSVALUE64)
<span class="line-modified">1371         add32(TrustedImm32(JSValue::ValueFalse), boolGPR, payloadGPR);</span>
1372 #else
1373         move(boolGPR, payloadGPR);
1374 #endif
1375     }
1376 
1377     void boxBooleanPayload(bool value, GPRReg payloadGPR)
1378     {
1379 #if USE(JSVALUE64)
<span class="line-modified">1380         move(TrustedImm32(JSValue::ValueFalse + value), payloadGPR);</span>
1381 #else
1382         move(TrustedImm32(value), payloadGPR);
1383 #endif
1384     }
1385 
1386     void boxBoolean(GPRReg boolGPR, JSValueRegs boxedRegs)
1387     {
1388         boxBooleanPayload(boolGPR, boxedRegs.payloadGPR());
1389 #if USE(JSVALUE32_64)
1390         move(TrustedImm32(JSValue::BooleanTag), boxedRegs.tagGPR());
1391 #endif
1392     }
1393 
1394     void boxBoolean(bool value, JSValueRegs boxedRegs)
1395     {
1396         boxBooleanPayload(value, boxedRegs.payloadGPR());
1397 #if USE(JSVALUE32_64)
1398         move(TrustedImm32(JSValue::BooleanTag), boxedRegs.tagGPR());
1399 #endif
1400     }
1401 
1402     void boxInt32(GPRReg intGPR, JSValueRegs boxedRegs, TagRegistersMode mode = HaveTagRegisters)
1403     {
1404 #if USE(JSVALUE64)
1405         if (mode == DoNotHaveTagRegisters) {
1406             move(intGPR, boxedRegs.gpr());
<span class="line-modified">1407             or64(TrustedImm64(JSValue::NumberTag), boxedRegs.gpr());</span>
1408         } else
<span class="line-modified">1409             or64(GPRInfo::numberTagRegister, intGPR, boxedRegs.gpr());</span>
1410 #else
1411         UNUSED_PARAM(mode);
1412         move(intGPR, boxedRegs.payloadGPR());
1413         move(TrustedImm32(JSValue::Int32Tag), boxedRegs.tagGPR());
1414 #endif
1415     }
1416 
1417     void boxCell(GPRReg cellGPR, JSValueRegs boxedRegs)
1418     {
1419 #if USE(JSVALUE64)
1420         move(cellGPR, boxedRegs.gpr());
1421 #else
1422         move(cellGPR, boxedRegs.payloadGPR());
1423         move(TrustedImm32(JSValue::CellTag), boxedRegs.tagGPR());
1424 #endif
1425     }
1426 
1427     void callExceptionFuzz(VM&amp;);
1428 
1429     enum ExceptionCheckKind { NormalExceptionCheck, InvertedExceptionCheck };
</pre>
<hr />
<pre>
1489     static VirtualRegister argumentsStart(InlineCallFrame* inlineCallFrame)
1490     {
1491         if (!inlineCallFrame)
1492             return VirtualRegister(CallFrame::argumentOffset(0));
1493         if (inlineCallFrame-&gt;argumentsWithFixup.size() &lt;= 1)
1494             return virtualRegisterForLocal(0);
1495         ValueRecovery recovery = inlineCallFrame-&gt;argumentsWithFixup[1];
1496         RELEASE_ASSERT(recovery.technique() == DisplacedInJSStack);
1497         return recovery.virtualRegister();
1498     }
1499 
1500     static VirtualRegister argumentsStart(const CodeOrigin&amp; codeOrigin)
1501     {
1502         return argumentsStart(codeOrigin.inlineCallFrame());
1503     }
1504 
1505     static VirtualRegister argumentCount(InlineCallFrame* inlineCallFrame)
1506     {
1507         ASSERT(!inlineCallFrame || inlineCallFrame-&gt;isVarargs());
1508         if (!inlineCallFrame)
<span class="line-modified">1509             return CallFrameSlot::argumentCountIncludingThis;</span>
1510         return inlineCallFrame-&gt;argumentCountRegister;
1511     }
1512 
1513     static VirtualRegister argumentCount(const CodeOrigin&amp; codeOrigin)
1514     {
1515         return argumentCount(codeOrigin.inlineCallFrame());
1516     }
1517 
1518     void emitLoadStructure(VM&amp;, RegisterID source, RegisterID dest, RegisterID scratch);
1519 
1520     void emitStoreStructureWithTypeInfo(TrustedImmPtr structure, RegisterID dest, RegisterID)
1521     {
1522         emitStoreStructureWithTypeInfo(*this, structure, dest);
1523     }
1524 
1525     void emitStoreStructureWithTypeInfo(RegisterID structure, RegisterID dest, RegisterID scratch)
1526     {
1527 #if USE(JSVALUE64)
1528         load64(MacroAssembler::Address(structure, Structure::structureIDOffset()), scratch);
1529         store64(scratch, MacroAssembler::Address(dest, JSCell::structureIDOffset()));
</pre>
<hr />
<pre>
1595         }
1596 #endif
1597         andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1598         addPtr(TrustedImmPtr(Gigacage::basePtr(kind)), storage);
1599 #if CPU(ARM64E)
1600         if (kind == Gigacage::Primitive)
1601             bitFieldInsert64(storage, 0, 64 - numberOfPACBits, tempReg);
1602 #endif
1603 
1604 #else
1605         UNUSED_PARAM(kind);
1606         UNUSED_PARAM(storage);
1607 #endif
1608     }
1609 
1610     // length may be the same register as scratch.
1611     void cageConditionally(Gigacage::Kind kind, GPRReg storage, GPRReg length, GPRReg scratch)
1612     {
1613 #if GIGACAGE_ENABLED
1614         if (Gigacage::isEnabled(kind)) {
<span class="line-modified">1615             if (kind != Gigacage::Primitive || Gigacage::isDisablingPrimitiveGigacageForbidden())</span>
1616                 cageWithoutUntagging(kind, storage);
1617             else {
1618 #if CPU(ARM64E)
1619                 if (length == scratch)
1620                     scratch = getCachedMemoryTempRegisterIDAndInvalidate();
1621 #endif
<span class="line-modified">1622                 loadPtr(Gigacage::addressOfBasePtr(kind), scratch);</span>
1623                 Jump done = branchTest64(Zero, scratch);
1624 #if CPU(ARM64E)
1625                 GPRReg tempReg = getCachedDataTempRegisterIDAndInvalidate();
1626                 move(storage, tempReg);
1627                 ASSERT(LogicalImmediate::create64(Gigacage::mask(kind)).isValid());
1628                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), tempReg);
1629                 addPtr(scratch, tempReg);
1630                 bitFieldInsert64(tempReg, 0, 64 - numberOfPACBits, storage);
1631 #else
1632                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1633                 addPtr(scratch, storage);
1634 #endif // CPU(ARM64E)
1635                 done.link(this);
1636             }
1637         }
1638 #endif
1639 
1640 #if CPU(ARM64E)
1641         if (kind == Gigacage::Primitive)
1642             untagArrayPtr(length, storage);
1643 #endif
1644         UNUSED_PARAM(kind);
1645         UNUSED_PARAM(storage);
1646         UNUSED_PARAM(length);
1647         UNUSED_PARAM(scratch);
1648     }
1649 
1650     void emitComputeButterflyIndexingMask(GPRReg vectorLengthGPR, GPRReg scratchGPR, GPRReg resultGPR)
1651     {
1652         ASSERT(scratchGPR != resultGPR);
1653         Jump done;






1654         // If vectorLength == 0 then clz will return 32 on both ARM and x86. On 64-bit systems, we can then do a 64-bit right shift on a 32-bit -1 to get a 0 mask for zero vectorLength. On 32-bit ARM, shift masks with 0xff, which means it will still create a 0 mask.
1655         countLeadingZeros32(vectorLengthGPR, scratchGPR);
1656         move(TrustedImm32(-1), resultGPR);
1657         urshiftPtr(scratchGPR, resultGPR);
1658         if (done.isSet())
1659             done.link(this);
1660     }
1661 
1662     // If for whatever reason the butterfly is going to change vector length this function does NOT
1663     // update the indexing mask.
1664     void nukeStructureAndStoreButterfly(VM&amp; vm, GPRReg butterfly, GPRReg object)
1665     {
1666         if (isX86()) {
1667             or32(TrustedImm32(bitwise_cast&lt;int32_t&gt;(nukedStructureIDBit())), Address(object, JSCell::structureIDOffset()));
1668             storePtr(butterfly, Address(object, JSObject::butterflyOffset()));
1669             return;
1670         }
1671 
1672         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1673         or32(TrustedImm32(bitwise_cast&lt;int32_t&gt;(nukedStructureIDBit())), Address(object, JSCell::structureIDOffset()));
</pre>
<hr />
<pre>
1758         notBigInt.link(this);
1759         functor(TypeofType::Symbol, false);
1760 
1761         notCell.link(this);
1762 
1763         Jump notNumber = branchIfNotNumber(regs, tempGPR);
1764         functor(TypeofType::Number, false);
1765         notNumber.link(this);
1766 
1767         JumpList notNull = branchIfNotEqual(regs, jsNull());
1768         functor(TypeofType::Object, false);
1769         notNull.link(this);
1770 
1771         Jump notBoolean = branchIfNotBoolean(regs, tempGPR);
1772         functor(TypeofType::Boolean, false);
1773         notBoolean.link(this);
1774 
1775         functor(TypeofType::Undefined, true);
1776     }
1777 
<span class="line-modified">1778     void emitDumbVirtualCall(VM&amp;, JSGlobalObject*, CallLinkInfo*);</span>
1779 
1780     void makeSpaceOnStackForCCall();
1781     void reclaimSpaceOnStackForCCall();
1782 
1783 #if USE(JSVALUE64)
1784     void emitRandomThunk(JSGlobalObject*, GPRReg scratch0, GPRReg scratch1, GPRReg scratch2, FPRReg result);
1785     void emitRandomThunk(VM&amp;, GPRReg scratch0, GPRReg scratch1, GPRReg scratch2, GPRReg scratch3, FPRReg result);
1786 #endif
1787 
1788     // Call this if you know that the value held in allocatorGPR is non-null. This DOES NOT mean
1789     // that allocator is non-null; allocator can be null as a signal that we don&#39;t know what the
1790     // value of allocatorGPR is. Additionally, if the allocator is not null, then there is no need
1791     // to populate allocatorGPR - this code will ignore the contents of allocatorGPR.
1792     void emitAllocateWithNonNullAllocator(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, GPRReg scratchGPR, JumpList&amp; slowPath);
1793 
1794     void emitAllocate(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, GPRReg scratchGPR, JumpList&amp; slowPath);
1795 
1796     template&lt;typename StructureType&gt;
1797     void emitAllocateJSCell(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure, GPRReg scratchGPR, JumpList&amp; slowPath)
1798     {
</pre>
<hr />
<pre>
1870     void emitInitializeInlineStorage(GPRReg baseGPR, GPRReg inlineCapacity)
1871     {
1872         Jump empty = branchTest32(Zero, inlineCapacity);
1873         Label loop = label();
1874         sub32(TrustedImm32(1), inlineCapacity);
1875         storeTrustedValue(JSValue(), BaseIndex(baseGPR, inlineCapacity, TimesEight, JSObject::offsetOfInlineStorage()));
1876         branchTest32(NonZero, inlineCapacity).linkTo(loop, this);
1877         empty.link(this);
1878     }
1879 
1880     void emitInitializeOutOfLineStorage(GPRReg butterflyGPR, unsigned outOfLineCapacity)
1881     {
1882         for (unsigned i = 0; i &lt; outOfLineCapacity; ++i)
1883             storeTrustedValue(JSValue(), Address(butterflyGPR, -sizeof(IndexingHeader) - (i + 1) * sizeof(EncodedJSValue)));
1884     }
1885 
1886 #if USE(JSVALUE64)
1887     void wangsInt64Hash(GPRReg inputAndResult, GPRReg scratch);
1888 #endif
1889 





1890 #if ENABLE(WEBASSEMBLY)
1891     void loadWasmContextInstance(GPRReg dst);
1892     void storeWasmContextInstance(GPRReg src);
1893     static bool loadWasmContextInstanceNeedsMacroScratchRegister();
1894     static bool storeWasmContextInstanceNeedsMacroScratchRegister();
1895 #endif
1896 
1897 protected:
1898     void copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(GPRReg calleeSavesBuffer);
1899 
1900     CodeBlock* m_codeBlock;
1901     CodeBlock* m_baselineCodeBlock;
1902 };
1903 
1904 } // namespace JSC
1905 
1906 #endif // ENABLE(JIT)
</pre>
</td>
</tr>
</table>
<center><a href="AssemblyHelpers.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="BinarySwitch.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>