<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="ICStats.h.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JIT.h.cdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 72,27 ***</span>
      MacroAssembler::repatchCall(
          CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
          newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  }
  
<span class="line-modified">! JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, unsigned loopOSREntryBytecodeOffset)</span>
      : JSInterfaceJIT(&amp;vm, codeBlock)
      , m_interpreter(vm.interpreter)
      , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
<span class="line-removed">-     , m_bytecodeOffset(std::numeric_limits&lt;unsigned&gt;::max())</span>
      , m_pcToCodeOriginMapBuilder(vm)
      , m_canBeOptimized(false)
      , m_shouldEmitProfiling(false)
<span class="line-modified">!     , m_shouldUseIndexMasking(Options::enableSpectreMitigations())</span>
<span class="line-removed">-     , m_loopOSREntryBytecodeOffset(loopOSREntryBytecodeOffset)</span>
  {
  }
  
  JIT::~JIT()
  {
  }
  
  void JIT::emitNotifyWrite(WatchpointSet* set)
  {
      if (!set || set-&gt;state() == IsInvalidated) {
          addSlowCase(Jump());
          return;
<span class="line-new-header">--- 72,45 ---</span>
      MacroAssembler::repatchCall(
          CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
          newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  }
  
<span class="line-modified">! JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, BytecodeIndex loopOSREntryBytecodeIndex)</span>
      : JSInterfaceJIT(&amp;vm, codeBlock)
      , m_interpreter(vm.interpreter)
      , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
      , m_pcToCodeOriginMapBuilder(vm)
      , m_canBeOptimized(false)
      , m_shouldEmitProfiling(false)
<span class="line-modified">!     , m_loopOSREntryBytecodeIndex(loopOSREntryBytecodeIndex)</span>
  {
  }
  
  JIT::~JIT()
  {
  }
  
<span class="line-added">+ #if ENABLE(DFG_JIT)</span>
<span class="line-added">+ void JIT::emitEnterOptimizationCheck()</span>
<span class="line-added">+ {</span>
<span class="line-added">+     if (!canBeOptimized())</span>
<span class="line-added">+         return;</span>
<span class="line-added">+ </span>
<span class="line-added">+     JumpList skipOptimize;</span>
<span class="line-added">+ </span>
<span class="line-added">+     skipOptimize.append(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));</span>
<span class="line-added">+     ASSERT(!m_bytecodeIndex.offset());</span>
<span class="line-added">+ </span>
<span class="line-added">+     copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
<span class="line-added">+ </span>
<span class="line-added">+     callOperation(operationOptimize, &amp;vm(), m_bytecodeIndex.asBits());</span>
<span class="line-added">+     skipOptimize.append(branchTestPtr(Zero, returnValueGPR));</span>
<span class="line-added">+     farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-added">+     skipOptimize.link(this);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
  void JIT::emitNotifyWrite(WatchpointSet* set)
  {
      if (!set || set-&gt;state() == IsInvalidated) {
          addSlowCase(Jump());
          return;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 106,38 ***</span>
      addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
  }
  
  void JIT::assertStackPointerOffset()
  {
<span class="line-modified">!     if (ASSERT_DISABLED)</span>
          return;
  
      addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
      Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
      breakpoint();
      ok.link(this);
  }
  
  #define NEXT_OPCODE(name) \
<span class="line-modified">!     m_bytecodeOffset += currentInstruction-&gt;size(); \</span>
      break;
  
  #define DEFINE_SLOW_OP(name) \
      case op_##name: { \
<span class="line-modified">!         if (m_bytecodeOffset &gt;= startBytecodeOffset) { \</span>
              JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
              slowPathCall.call(); \
          } \
<span class="line-modified">!         NEXT_OPCODE(op_##name); \</span>
      }
  
  #define DEFINE_OP(name) \
      case name: { \
<span class="line-modified">!         if (m_bytecodeOffset &gt;= startBytecodeOffset) { \</span>
              emit_##name(currentInstruction); \
          } \
<span class="line-modified">!         NEXT_OPCODE(name); \</span>
      }
  
  #define DEFINE_SLOWCASE_OP(name) \
      case name: { \
          emitSlow_##name(currentInstruction, iter); \
<span class="line-new-header">--- 124,43 ---</span>
      addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
  }
  
  void JIT::assertStackPointerOffset()
  {
<span class="line-modified">!     if (!ASSERT_ENABLED)</span>
          return;
  
      addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
      Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
      breakpoint();
      ok.link(this);
  }
  
  #define NEXT_OPCODE(name) \
<span class="line-modified">!     m_bytecodeIndex = BytecodeIndex(m_bytecodeIndex.offset() + currentInstruction-&gt;size()); \</span>
      break;
  
<span class="line-added">+ #define NEXT_OPCODE_IN_MAIN(name) \</span>
<span class="line-added">+     if (previousSlowCasesSize != m_slowCases.size()) \</span>
<span class="line-added">+         ++m_bytecodeCountHavingSlowCase; \</span>
<span class="line-added">+     NEXT_OPCODE(name)</span>
<span class="line-added">+ </span>
  #define DEFINE_SLOW_OP(name) \
      case op_##name: { \
<span class="line-modified">!         if (m_bytecodeIndex &gt;= startBytecodeIndex) { \</span>
              JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
              slowPathCall.call(); \
          } \
<span class="line-modified">!         NEXT_OPCODE_IN_MAIN(op_##name); \</span>
      }
  
  #define DEFINE_OP(name) \
      case name: { \
<span class="line-modified">!         if (m_bytecodeIndex &gt;= startBytecodeIndex) { \</span>
              emit_##name(currentInstruction); \
          } \
<span class="line-modified">!         NEXT_OPCODE_IN_MAIN(name); \</span>
      }
  
  #define DEFINE_SLOWCASE_OP(name) \
      case name: { \
          emitSlow_##name(currentInstruction, iter); \
</pre>
<hr />
<pre>
<span class="line-old-header">*** 170,88 ***</span>
      unsigned instructionCount = m_codeBlock-&gt;instructions().size();
  
      m_callLinkInfoIndex = 0;
  
      VM&amp; vm = m_codeBlock-&gt;vm();
<span class="line-modified">!     unsigned startBytecodeOffset = 0;</span>
<span class="line-modified">!     if (m_loopOSREntryBytecodeOffset &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {</span>
          // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
          // This optimization would be invalid otherwise. When the LLInt determines it wants to
          // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
          // was executing at when it kicked off our compilation. We only need to compile code for
          // anything reachable from that bytecode offset.
  
          // We only bother building the bytecode graph if it could save time and executable
          // memory. We pick an arbitrary offset where we deem this is profitable.
<span class="line-modified">!         if (m_loopOSREntryBytecodeOffset &gt;= 200) {</span>
              // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
              // Instead, we just find the minimum bytecode offset that is reachable, and
              // compile code from that bytecode offset onwards.
  
              BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
<span class="line-modified">!             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeOffset);</span>
              RELEASE_ASSERT(block);
  
              GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
<span class="line-modified">!             startBytecodeOffset = UINT_MAX;</span>
              worklist.push(block);
  
              while (BytecodeBasicBlock* block = worklist.pop()) {
<span class="line-modified">!                 startBytecodeOffset = std::min(startBytecodeOffset, block-&gt;leaderOffset());</span>
<span class="line-modified">!                 worklist.pushAll(block-&gt;successors());</span>
  
                  // Also add catch blocks for bytecodes that throw.
                  if (m_codeBlock-&gt;numberOfExceptionHandlers()) {
                      for (unsigned bytecodeOffset = block-&gt;leaderOffset(); bytecodeOffset &lt; block-&gt;leaderOffset() + block-&gt;totalLength();) {
                          auto instruction = instructions.at(bytecodeOffset);
<span class="line-modified">!                         if (auto* handler = m_codeBlock-&gt;handlerForBytecodeOffset(bytecodeOffset))</span>
                              worklist.push(graph.findBasicBlockWithLeaderOffset(handler-&gt;target));
  
                          bytecodeOffset += instruction-&gt;size();
                      }
                  }
              }
          }
      }
  
<span class="line-modified">!     for (m_bytecodeOffset = 0; m_bytecodeOffset &lt; instructionCount; ) {</span>
<span class="line-modified">!         if (m_bytecodeOffset == startBytecodeOffset &amp;&amp; startBytecodeOffset &gt; 0) {</span>
              // We&#39;ve proven all bytecode instructions up until here are unreachable.
              // Let&#39;s ensure that by crashing if it&#39;s ever hit.
              breakpoint();
          }
  
          if (m_disassembler)
<span class="line-modified">!             m_disassembler-&gt;setForBytecodeMainPath(m_bytecodeOffset, label());</span>
<span class="line-modified">!         const Instruction* currentInstruction = instructions.at(m_bytecodeOffset).ptr();</span>
<span class="line-modified">!         ASSERT_WITH_MESSAGE(currentInstruction-&gt;size(), &quot;privateCompileMainPass gone bad @ %d&quot;, m_bytecodeOffset);</span>
  
<span class="line-modified">!         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));</span>
  
  #if ENABLE(OPCODE_SAMPLING)
<span class="line-modified">!         if (m_bytecodeOffset &gt; 0) // Avoid the overhead of sampling op_enter twice.</span>
              sampleInstruction(currentInstruction);
  #endif
  
<span class="line-modified">!         m_labels[m_bytecodeOffset] = label();</span>
  
          if (JITInternal::verbose)
<span class="line-modified">!             dataLogF(&quot;Old JIT emitting code for bc#%u at offset 0x%lx.\n&quot;, m_bytecodeOffset, (long)debugOffset());</span>
  
          OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
  
          if (UNLIKELY(m_compilation)) {
              add64(
                  TrustedImm32(1),
                  AbsoluteAddress(m_compilation-&gt;executionCounterFor(Profiler::OriginStack(Profiler::Origin(
<span class="line-modified">!                     m_compilation-&gt;bytecodes(), m_bytecodeOffset)))-&gt;address()));</span>
          }
  
          if (Options::eagerlyUpdateTopCallFrame())
              updateTopCallFrame();
  
<span class="line-modified">!         unsigned bytecodeOffset = m_bytecodeOffset;</span>
  #if ENABLE(MASM_PROBE)
          if (UNLIKELY(Options::traceBaselineJITExecution())) {
              CodeBlock* codeBlock = m_codeBlock;
              probe([=] (Probe::Context&amp; ctx) {
                  dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
<span class="line-new-header">--- 193,91 ---</span>
      unsigned instructionCount = m_codeBlock-&gt;instructions().size();
  
      m_callLinkInfoIndex = 0;
  
      VM&amp; vm = m_codeBlock-&gt;vm();
<span class="line-modified">!     BytecodeIndex startBytecodeIndex(0);</span>
<span class="line-modified">!     if (m_loopOSREntryBytecodeIndex &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {</span>
          // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
          // This optimization would be invalid otherwise. When the LLInt determines it wants to
          // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
          // was executing at when it kicked off our compilation. We only need to compile code for
          // anything reachable from that bytecode offset.
  
          // We only bother building the bytecode graph if it could save time and executable
          // memory. We pick an arbitrary offset where we deem this is profitable.
<span class="line-modified">!         if (m_loopOSREntryBytecodeIndex.offset() &gt;= 200) {</span>
              // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
              // Instead, we just find the minimum bytecode offset that is reachable, and
              // compile code from that bytecode offset onwards.
  
              BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
<span class="line-modified">!             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeIndex.offset());</span>
              RELEASE_ASSERT(block);
  
              GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
<span class="line-modified">!             startBytecodeIndex = BytecodeIndex();</span>
              worklist.push(block);
  
              while (BytecodeBasicBlock* block = worklist.pop()) {
<span class="line-modified">!                 startBytecodeIndex = BytecodeIndex(std::min(startBytecodeIndex.offset(), block-&gt;leaderOffset()));</span>
<span class="line-modified">!                 for (unsigned successorIndex : block-&gt;successors())</span>
<span class="line-added">+                     worklist.push(&amp;graph[successorIndex]);</span>
  
                  // Also add catch blocks for bytecodes that throw.
                  if (m_codeBlock-&gt;numberOfExceptionHandlers()) {
                      for (unsigned bytecodeOffset = block-&gt;leaderOffset(); bytecodeOffset &lt; block-&gt;leaderOffset() + block-&gt;totalLength();) {
                          auto instruction = instructions.at(bytecodeOffset);
<span class="line-modified">!                         if (auto* handler = m_codeBlock-&gt;handlerForBytecodeIndex(BytecodeIndex(bytecodeOffset)))</span>
                              worklist.push(graph.findBasicBlockWithLeaderOffset(handler-&gt;target));
  
                          bytecodeOffset += instruction-&gt;size();
                      }
                  }
              }
          }
      }
  
<span class="line-modified">!     m_bytecodeCountHavingSlowCase = 0;</span>
<span class="line-modified">!     for (m_bytecodeIndex = BytecodeIndex(0); m_bytecodeIndex.offset() &lt; instructionCount; ) {</span>
<span class="line-added">+         unsigned previousSlowCasesSize = m_slowCases.size();</span>
<span class="line-added">+         if (m_bytecodeIndex == startBytecodeIndex &amp;&amp; startBytecodeIndex.offset() &gt; 0) {</span>
              // We&#39;ve proven all bytecode instructions up until here are unreachable.
              // Let&#39;s ensure that by crashing if it&#39;s ever hit.
              breakpoint();
          }
  
          if (m_disassembler)
<span class="line-modified">!             m_disassembler-&gt;setForBytecodeMainPath(m_bytecodeIndex.offset(), label());</span>
<span class="line-modified">!         const Instruction* currentInstruction = instructions.at(m_bytecodeIndex).ptr();</span>
<span class="line-modified">!         ASSERT(currentInstruction-&gt;size());</span>
  
<span class="line-modified">!         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));</span>
  
  #if ENABLE(OPCODE_SAMPLING)
<span class="line-modified">!         if (m_bytecodeIndex &gt; 0) // Avoid the overhead of sampling op_enter twice.</span>
              sampleInstruction(currentInstruction);
  #endif
  
<span class="line-modified">!         m_labels[m_bytecodeIndex.offset()] = label();</span>
  
          if (JITInternal::verbose)
<span class="line-modified">!             dataLogLn(&quot;Old JIT emitting code for &quot;, m_bytecodeIndex, &quot; at offset &quot;, (long)debugOffset());</span>
  
          OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
  
          if (UNLIKELY(m_compilation)) {
              add64(
                  TrustedImm32(1),
                  AbsoluteAddress(m_compilation-&gt;executionCounterFor(Profiler::OriginStack(Profiler::Origin(
<span class="line-modified">!                     m_compilation-&gt;bytecodes(), m_bytecodeIndex)))-&gt;address()));</span>
          }
  
          if (Options::eagerlyUpdateTopCallFrame())
              updateTopCallFrame();
  
<span class="line-modified">!         unsigned bytecodeOffset = m_bytecodeIndex.offset();</span>
  #if ENABLE(MASM_PROBE)
          if (UNLIKELY(Options::traceBaselineJITExecution())) {
              CodeBlock* codeBlock = m_codeBlock;
              probe([=] (Probe::Context&amp; ctx) {
                  dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 287,11 ***</span>
<span class="line-new-header">--- 313,17 ---</span>
          DEFINE_SLOW_OP(get_property_enumerator)
          DEFINE_SLOW_OP(to_index_string)
          DEFINE_SLOW_OP(create_direct_arguments)
          DEFINE_SLOW_OP(create_scoped_arguments)
          DEFINE_SLOW_OP(create_cloned_arguments)
<span class="line-added">+         DEFINE_SLOW_OP(create_arguments_butterfly)</span>
          DEFINE_SLOW_OP(create_rest)
<span class="line-added">+         DEFINE_SLOW_OP(create_promise)</span>
<span class="line-added">+         DEFINE_SLOW_OP(new_promise)</span>
<span class="line-added">+         DEFINE_SLOW_OP(create_generator)</span>
<span class="line-added">+         DEFINE_SLOW_OP(create_async_generator)</span>
<span class="line-added">+         DEFINE_SLOW_OP(new_generator)</span>
          DEFINE_SLOW_OP(pow)
  
          DEFINE_OP(op_add)
          DEFINE_OP(op_bitnot)
          DEFINE_OP(op_bitand)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 361,10 ***</span>
<span class="line-new-header">--- 393,11 ---</span>
          DEFINE_OP(op_jnstricteq)
          DEFINE_OP(op_jbelow)
          DEFINE_OP(op_jbeloweq)
          DEFINE_OP(op_jtrue)
          DEFINE_OP(op_loop_hint)
<span class="line-added">+         DEFINE_OP(op_check_traps)</span>
          DEFINE_OP(op_nop)
          DEFINE_OP(op_super_sampler_begin)
          DEFINE_OP(op_super_sampler_end)
          DEFINE_OP(op_lshift)
          DEFINE_OP(op_mod)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 398,10 ***</span>
<span class="line-new-header">--- 431,14 ---</span>
          DEFINE_OP(op_put_getter_by_id)
          DEFINE_OP(op_put_setter_by_id)
          DEFINE_OP(op_put_getter_setter_by_id)
          DEFINE_OP(op_put_getter_by_val)
          DEFINE_OP(op_put_setter_by_val)
<span class="line-added">+         DEFINE_OP(op_to_property_key)</span>
<span class="line-added">+ </span>
<span class="line-added">+         DEFINE_OP(op_get_internal_field)</span>
<span class="line-added">+         DEFINE_OP(op_put_internal_field)</span>
  
          DEFINE_OP(op_ret)
          DEFINE_OP(op_rshift)
          DEFINE_OP(op_unsigned)
          DEFINE_OP(op_urshift)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 411,10 ***</span>
<span class="line-new-header">--- 448,11 ---</span>
          DEFINE_OP(op_switch_char)
          DEFINE_OP(op_switch_imm)
          DEFINE_OP(op_switch_string)
          DEFINE_OP(op_throw)
          DEFINE_OP(op_to_number)
<span class="line-added">+         DEFINE_OP(op_to_numeric)</span>
          DEFINE_OP(op_to_string)
          DEFINE_OP(op_to_object)
          DEFINE_OP(op_to_primitive)
  
          DEFINE_OP(op_resolve_scope)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 441,11 ***</span>
  
      RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
  
  #ifndef NDEBUG
      // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified">!     m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max();</span>
  #endif
  }
  
  void JIT::privateCompileLinkPass()
  {
<span class="line-new-header">--- 479,11 ---</span>
  
      RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
  
  #ifndef NDEBUG
      // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified">!     m_bytecodeIndex = BytecodeIndex();</span>
  #endif
  }
  
  void JIT::privateCompileLinkPass()
  {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 456,40 ***</span>
  }
  
  void JIT::privateCompileSlowCases()
  {
      m_getByIdIndex = 0;
      m_getByIdWithThisIndex = 0;
      m_putByIdIndex = 0;
      m_inByIdIndex = 0;
      m_instanceOfIndex = 0;
      m_byValInstructionIndex = 0;
      m_callLinkInfoIndex = 0;
  
      for (Vector&lt;SlowCaseEntry&gt;::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
<span class="line-modified">!         m_bytecodeOffset = iter-&gt;to;</span>
  
<span class="line-modified">!         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));</span>
  
<span class="line-modified">!         unsigned firstTo = m_bytecodeOffset;</span>
  
<span class="line-modified">!         const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(m_bytecodeOffset).ptr();</span>
  
<span class="line-modified">!         RareCaseProfile* rareCaseProfile = 0;</span>
          if (shouldEmitProfiling())
<span class="line-modified">!             rareCaseProfile = m_codeBlock-&gt;addRareCaseProfile(m_bytecodeOffset);</span>
  
          if (JITInternal::verbose)
<span class="line-modified">!             dataLogF(&quot;Old JIT emitting slow code for bc#%u at offset 0x%lx.\n&quot;, m_bytecodeOffset, (long)debugOffset());</span>
  
          if (m_disassembler)
<span class="line-modified">!             m_disassembler-&gt;setForBytecodeSlowPath(m_bytecodeOffset, label());</span>
  
  #if ENABLE(MASM_PROBE)
          if (UNLIKELY(Options::traceBaselineJITExecution())) {
              OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
<span class="line-modified">!             unsigned bytecodeOffset = m_bytecodeOffset;</span>
              CodeBlock* codeBlock = m_codeBlock;
              probe([=] (Probe::Context&amp; ctx) {
                  dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] SLOW &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
              });
          }
<span class="line-new-header">--- 494,46 ---</span>
  }
  
  void JIT::privateCompileSlowCases()
  {
      m_getByIdIndex = 0;
<span class="line-added">+     m_getByValIndex = 0;</span>
      m_getByIdWithThisIndex = 0;
      m_putByIdIndex = 0;
      m_inByIdIndex = 0;
      m_instanceOfIndex = 0;
      m_byValInstructionIndex = 0;
      m_callLinkInfoIndex = 0;
  
<span class="line-added">+     RefCountedArray&lt;RareCaseProfile&gt; rareCaseProfiles;</span>
<span class="line-added">+     if (shouldEmitProfiling())</span>
<span class="line-added">+         rareCaseProfiles = RefCountedArray&lt;RareCaseProfile&gt;(m_bytecodeCountHavingSlowCase);</span>
<span class="line-added">+ </span>
<span class="line-added">+     unsigned bytecodeCountHavingSlowCase = 0;</span>
      for (Vector&lt;SlowCaseEntry&gt;::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
<span class="line-modified">!         m_bytecodeIndex = iter-&gt;to;</span>
  
<span class="line-modified">!         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));</span>
  
<span class="line-modified">!         BytecodeIndex firstTo = m_bytecodeIndex;</span>
  
<span class="line-modified">!         const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(m_bytecodeIndex).ptr();</span>
  
<span class="line-modified">!         RareCaseProfile* rareCaseProfile = nullptr;</span>
          if (shouldEmitProfiling())
<span class="line-modified">!             rareCaseProfile = &amp;rareCaseProfiles.at(bytecodeCountHavingSlowCase);</span>
  
          if (JITInternal::verbose)
<span class="line-modified">!             dataLogLn(&quot;Old JIT emitting slow code for &quot;, m_bytecodeIndex, &quot; at offset &quot;, (long)debugOffset());</span>
  
          if (m_disassembler)
<span class="line-modified">!             m_disassembler-&gt;setForBytecodeSlowPath(m_bytecodeIndex.offset(), label());</span>
  
  #if ENABLE(MASM_PROBE)
          if (UNLIKELY(Options::traceBaselineJITExecution())) {
              OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
<span class="line-modified">!             unsigned bytecodeOffset = m_bytecodeIndex.offset();</span>
              CodeBlock* codeBlock = m_codeBlock;
              probe([=] (Probe::Context&amp; ctx) {
                  dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] SLOW &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
              });
          }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 525,11 ***</span>
          DEFINE_SLOWCASE_OP(op_jeq)
          DEFINE_SLOWCASE_OP(op_jneq)
          DEFINE_SLOWCASE_OP(op_jstricteq)
          DEFINE_SLOWCASE_OP(op_jnstricteq)
          DEFINE_SLOWCASE_OP(op_loop_hint)
<span class="line-modified">!         DEFINE_SLOWCASE_OP(op_enter)</span>
          DEFINE_SLOWCASE_OP(op_mod)
          DEFINE_SLOWCASE_OP(op_mul)
          DEFINE_SLOWCASE_OP(op_negate)
          DEFINE_SLOWCASE_OP(op_neq)
          DEFINE_SLOWCASE_OP(op_new_object)
<span class="line-new-header">--- 569,11 ---</span>
          DEFINE_SLOWCASE_OP(op_jeq)
          DEFINE_SLOWCASE_OP(op_jneq)
          DEFINE_SLOWCASE_OP(op_jstricteq)
          DEFINE_SLOWCASE_OP(op_jnstricteq)
          DEFINE_SLOWCASE_OP(op_loop_hint)
<span class="line-modified">!         DEFINE_SLOWCASE_OP(op_check_traps)</span>
          DEFINE_SLOWCASE_OP(op_mod)
          DEFINE_SLOWCASE_OP(op_mul)
          DEFINE_SLOWCASE_OP(op_negate)
          DEFINE_SLOWCASE_OP(op_neq)
          DEFINE_SLOWCASE_OP(op_new_object)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 551,22 ***</span>
<span class="line-new-header">--- 595,27 ---</span>
          DEFINE_SLOWCASE_SLOW_OP(lshift)
          DEFINE_SLOWCASE_SLOW_OP(rshift)
          DEFINE_SLOWCASE_SLOW_OP(urshift)
          DEFINE_SLOWCASE_SLOW_OP(div)
          DEFINE_SLOWCASE_SLOW_OP(create_this)
<span class="line-added">+         DEFINE_SLOWCASE_SLOW_OP(create_promise)</span>
<span class="line-added">+         DEFINE_SLOWCASE_SLOW_OP(create_generator)</span>
<span class="line-added">+         DEFINE_SLOWCASE_SLOW_OP(create_async_generator)</span>
          DEFINE_SLOWCASE_SLOW_OP(to_this)
          DEFINE_SLOWCASE_SLOW_OP(to_primitive)
          DEFINE_SLOWCASE_SLOW_OP(to_number)
<span class="line-added">+         DEFINE_SLOWCASE_SLOW_OP(to_numeric)</span>
          DEFINE_SLOWCASE_SLOW_OP(to_string)
          DEFINE_SLOWCASE_SLOW_OP(to_object)
          DEFINE_SLOWCASE_SLOW_OP(not)
          DEFINE_SLOWCASE_SLOW_OP(stricteq)
          DEFINE_SLOWCASE_SLOW_OP(nstricteq)
          DEFINE_SLOWCASE_SLOW_OP(get_direct_pname)
          DEFINE_SLOWCASE_SLOW_OP(has_structure_property)
          DEFINE_SLOWCASE_SLOW_OP(resolve_scope)
          DEFINE_SLOWCASE_SLOW_OP(check_tdz)
<span class="line-added">+         DEFINE_SLOWCASE_SLOW_OP(to_property_key)</span>
  
          default:
              RELEASE_ASSERT_NOT_REACHED();
          }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 578,22 ***</span>
  
          if (shouldEmitProfiling())
              add32(TrustedImm32(1), AbsoluteAddress(&amp;rareCaseProfile-&gt;m_counter));
  
          emitJumpSlowToHot(jump(), 0);
      }
  
      RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
      RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
      RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
      RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
      RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
      RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
  
  #ifndef NDEBUG
      // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified">!     m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max();</span>
  #endif
  }
  
  void JIT::compileWithoutLinking(JITCompilationEffort effort)
  {
<span class="line-new-header">--- 627,27 ---</span>
  
          if (shouldEmitProfiling())
              add32(TrustedImm32(1), AbsoluteAddress(&amp;rareCaseProfile-&gt;m_counter));
  
          emitJumpSlowToHot(jump(), 0);
<span class="line-added">+         ++bytecodeCountHavingSlowCase;</span>
      }
  
<span class="line-added">+     RELEASE_ASSERT(bytecodeCountHavingSlowCase == m_bytecodeCountHavingSlowCase);</span>
      RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
      RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
      RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
      RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
      RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
      RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
  
<span class="line-added">+     if (shouldEmitProfiling())</span>
<span class="line-added">+         m_codeBlock-&gt;setRareCaseProfiles(WTFMove(rareCaseProfiles));</span>
<span class="line-added">+ </span>
  #ifndef NDEBUG
      // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified">!     m_bytecodeIndex = BytecodeIndex();</span>
  #endif
  }
  
  void JIT::compileWithoutLinking(JITCompilationEffort effort)
  {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 640,11 ***</span>
                  m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
                  Profiler::Baseline));
          m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
      }
  
<span class="line-modified">!     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(0, nullptr));</span>
  
      Label entryLabel(this);
      if (m_disassembler)
          m_disassembler-&gt;setStartOfCode(entryLabel);
  
<span class="line-new-header">--- 694,11 ---</span>
                  m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
                  Profiler::Baseline));
          m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
      }
  
<span class="line-modified">!     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(BytecodeIndex(0)));</span>
  
      Label entryLabel(this);
      if (m_disassembler)
          m_disassembler-&gt;setStartOfCode(entryLabel);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 670,18 ***</span>
          stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
      stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm-&gt;addressOfSoftStackLimit()), regT1));
  
      move(regT1, stackPointerRegister);
      checkStackPointerAlignment();
<span class="line-removed">-     if (Options::zeroStackFrame())</span>
<span class="line-removed">-         clearStackFrame(callFrameRegister, stackPointerRegister, regT0, maxFrameSize);</span>
  
      emitSaveCalleeSaves();
      emitMaterializeTagCheckRegisters();
  
      if (m_codeBlock-&gt;codeType() == FunctionCode) {
<span class="line-modified">!         ASSERT(m_bytecodeOffset == std::numeric_limits&lt;unsigned&gt;::max());</span>
          if (shouldEmitProfiling()) {
              for (int argument = 0; argument &lt; m_codeBlock-&gt;numParameters(); ++argument) {
                  // If this is a constructor, then we want to put in a dummy profiling site (to
                  // keep things consistent) but we don&#39;t actually want to record the dummy value.
                  if (m_codeBlock-&gt;isConstructor() &amp;&amp; !argument)
<span class="line-new-header">--- 724,16 ---</span>
          stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
      stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm-&gt;addressOfSoftStackLimit()), regT1));
  
      move(regT1, stackPointerRegister);
      checkStackPointerAlignment();
  
      emitSaveCalleeSaves();
      emitMaterializeTagCheckRegisters();
  
      if (m_codeBlock-&gt;codeType() == FunctionCode) {
<span class="line-modified">!         ASSERT(!m_bytecodeIndex);</span>
          if (shouldEmitProfiling()) {
              for (int argument = 0; argument &lt; m_codeBlock-&gt;numParameters(); ++argument) {
                  // If this is a constructor, then we want to put in a dummy profiling site (to
                  // keep things consistent) but we don&#39;t actually want to record the dummy value.
                  if (m_codeBlock-&gt;isConstructor() &amp;&amp; !argument)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 707,11 ***</span>
      if (m_disassembler)
          m_disassembler-&gt;setEndOfSlowPath(label());
      m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
  
      stackOverflow.link(this);
<span class="line-modified">!     m_bytecodeOffset = 0;</span>
      if (maxFrameExtentForSlowPathCall)
          addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
      callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
  
      // If the number of parameters is 1, we never require arity fixup.
<span class="line-new-header">--- 759,11 ---</span>
      if (m_disassembler)
          m_disassembler-&gt;setEndOfSlowPath(label());
      m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
  
      stackOverflow.link(this);
<span class="line-modified">!     m_bytecodeIndex = BytecodeIndex(0);</span>
      if (maxFrameExtentForSlowPathCall)
          addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
      callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
  
      // If the number of parameters is 1, we never require arity fixup.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 720,26 ***</span>
          m_arityCheck = label();
          store8(TrustedImm32(0), &amp;m_codeBlock-&gt;m_shouldAlwaysBeInlined);
          emitFunctionPrologue();
          emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
  
<span class="line-modified">!         load32(payloadFor(CallFrameSlot::argumentCount), regT1);</span>
          branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock-&gt;m_numParameters)).linkTo(beginLabel, this);
  
<span class="line-modified">!         m_bytecodeOffset = 0;</span>
  
          if (maxFrameExtentForSlowPathCall)
              addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
<span class="line-modified">!         callOperationWithCallFrameRollbackOnException(m_codeBlock-&gt;isConstructor() ? operationConstructArityCheck : operationCallArityCheck);</span>
          if (maxFrameExtentForSlowPathCall)
              addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
          branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
          move(returnValueGPR, GPRInfo::argumentGPR0);
          emitNakedCall(m_vm-&gt;getCTIStub(arityFixupGenerator).retaggedCode&lt;NoPtrTag&gt;());
  
<span class="line-modified">! #if !ASSERT_DISABLED</span>
<span class="line-modified">!         m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max(); // Reset this, in order to guard its use with ASSERTs.</span>
  #endif
  
          jump(beginLabel);
      } else
          m_arityCheck = entryLabel; // Never require arity fixup.
<span class="line-new-header">--- 772,26 ---</span>
          m_arityCheck = label();
          store8(TrustedImm32(0), &amp;m_codeBlock-&gt;m_shouldAlwaysBeInlined);
          emitFunctionPrologue();
          emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
  
<span class="line-modified">!         load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT1);</span>
          branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock-&gt;m_numParameters)).linkTo(beginLabel, this);
  
<span class="line-modified">!         m_bytecodeIndex = BytecodeIndex(0);</span>
  
          if (maxFrameExtentForSlowPathCall)
              addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
<span class="line-modified">!         callOperationWithCallFrameRollbackOnException(m_codeBlock-&gt;isConstructor() ? operationConstructArityCheck : operationCallArityCheck, m_codeBlock-&gt;globalObject());</span>
          if (maxFrameExtentForSlowPathCall)
              addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
          branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
          move(returnValueGPR, GPRInfo::argumentGPR0);
          emitNakedCall(m_vm-&gt;getCTIStub(arityFixupGenerator).retaggedCode&lt;NoPtrTag&gt;());
  
<span class="line-modified">! #if ASSERT_ENABLED</span>
<span class="line-modified">!         m_bytecodeIndex = BytecodeIndex(); // Reset this, in order to guard its use with ASSERTs.</span>
  #endif
  
          jump(beginLabel);
      } else
          m_arityCheck = entryLabel; // Never require arity fixup.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 775,11 ***</span>
      if (patchBuffer.didFailToAllocate())
          return CompilationFailed;
  
      // Translate vPC offsets into addresses in JIT generated code, for switch tables.
      for (auto&amp; record : m_switches) {
<span class="line-modified">!         unsigned bytecodeOffset = record.bytecodeOffset;</span>
  
          if (record.type != SwitchRecord::String) {
              ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
              ASSERT(record.jumpTable.simpleJumpTable-&gt;branchOffsets.size() == record.jumpTable.simpleJumpTable-&gt;ctiOffsets.size());
  
<span class="line-new-header">--- 827,11 ---</span>
      if (patchBuffer.didFailToAllocate())
          return CompilationFailed;
  
      // Translate vPC offsets into addresses in JIT generated code, for switch tables.
      for (auto&amp; record : m_switches) {
<span class="line-modified">!         unsigned bytecodeOffset = record.bytecodeIndex.offset();</span>
  
          if (record.type != SwitchRecord::String) {
              ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
              ASSERT(record.jumpTable.simpleJumpTable-&gt;branchOffsets.size() == record.jumpTable.simpleJumpTable-&gt;ctiOffsets.size());
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 818,10 ***</span>
<span class="line-new-header">--- 870,11 ---</span>
          if (record.callee)
              patchBuffer.link(record.from, record.callee);
      }
  
      finalizeInlineCaches(m_getByIds, patchBuffer);
<span class="line-added">+     finalizeInlineCaches(m_getByVals, patchBuffer);</span>
      finalizeInlineCaches(m_getByIdsWithThis, patchBuffer);
      finalizeInlineCaches(m_putByIds, patchBuffer);
      finalizeInlineCaches(m_inByIds, patchBuffer);
      finalizeInlineCaches(m_instanceOfs, patchBuffer);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 857,21 ***</span>
              CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.callReturnLocation)),
              CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathBegin)),
              patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathOther));
      }
  
<span class="line-modified">!     JITCodeMap jitCodeMap;</span>
<span class="line-modified">!     for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {</span>
<span class="line-modified">!         if (m_labels[bytecodeOffset].isSet())</span>
<span class="line-modified">!             jitCodeMap.append(bytecodeOffset, patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));</span>
      }
<span class="line-removed">-     jitCodeMap.finish();</span>
<span class="line-removed">-     m_codeBlock-&gt;setJITCodeMap(WTFMove(jitCodeMap));</span>
  
      MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
  
<span class="line-modified">!     if (Options::dumpDisassembly()) {</span>
          m_disassembler-&gt;dump(patchBuffer);
          patchBuffer.didAlreadyDisassemble();
      }
      if (UNLIKELY(m_compilation)) {
          if (Options::disassembleBaselineForProfiler())
<span class="line-new-header">--- 910,22 ---</span>
              CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.callReturnLocation)),
              CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathBegin)),
              patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathOther));
      }
  
<span class="line-modified">!     {</span>
<span class="line-modified">!         JITCodeMapBuilder jitCodeMapBuilder;</span>
<span class="line-modified">!         for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {</span>
<span class="line-modified">!             if (m_labels[bytecodeOffset].isSet())</span>
<span class="line-added">+                 jitCodeMapBuilder.append(BytecodeIndex(bytecodeOffset), patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));</span>
<span class="line-added">+         }</span>
<span class="line-added">+         m_codeBlock-&gt;setJITCodeMap(jitCodeMapBuilder.finalize());</span>
      }
  
      MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
  
<span class="line-modified">!     if (UNLIKELY(Options::dumpDisassembly())) {</span>
          m_disassembler-&gt;dump(patchBuffer);
          patchBuffer.didAlreadyDisassemble();
      }
      if (UNLIKELY(m_compilation)) {
          if (Options::disassembleBaselineForProfiler())
</pre>
<hr />
<pre>
<span class="line-old-header">*** 888,11 ***</span>
  
      m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
          static_cast&lt;double&gt;(result.size()) /
          static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));
  
<span class="line-modified">!     m_codeBlock-&gt;shrinkToFit(CodeBlock::LateShrink);</span>
      m_codeBlock-&gt;setJITCode(
          adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));
  
      if (JITInternal::verbose)
          dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
<span class="line-new-header">--- 942,14 ---</span>
  
      m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
          static_cast&lt;double&gt;(result.size()) /
          static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));
  
<span class="line-modified">!     {</span>
<span class="line-added">+         ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);</span>
<span class="line-added">+         m_codeBlock-&gt;shrinkToFit(locker, CodeBlock::ShrinkMode::LateShrink);</span>
<span class="line-added">+     }</span>
      m_codeBlock-&gt;setJITCode(
          adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));
  
      if (JITInternal::verbose)
          dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
</pre>
<hr />
<pre>
<span class="line-old-header">*** 912,40 ***</span>
      if (!m_exceptionChecksWithCallFrameRollback.empty()) {
          m_exceptionChecksWithCallFrameRollback.link(this);
  
          copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
  
<span class="line-modified">!         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).</span>
<span class="line-removed">- </span>
          move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified">!         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);</span>
<span class="line-modified">! </span>
<span class="line-removed">- #if CPU(X86)</span>
<span class="line-removed">-         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!</span>
<span class="line-removed">-         poke(GPRInfo::argumentGPR0);</span>
<span class="line-removed">-         poke(GPRInfo::argumentGPR1, 1);</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">-         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));</span>
          jumpToExceptionHandler(vm());
      }
  
      if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
          m_exceptionHandler = label();
          m_exceptionChecks.link(this);
  
          copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
  
<span class="line-modified">!         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).</span>
          move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified">!         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);</span>
<span class="line-modified">! </span>
<span class="line-removed">- #if CPU(X86)</span>
<span class="line-removed">-         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!</span>
<span class="line-removed">-         poke(GPRInfo::argumentGPR0);</span>
<span class="line-removed">-         poke(GPRInfo::argumentGPR1, 1);</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">-         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));</span>
          jumpToExceptionHandler(vm());
      }
  }
  
  void JIT::doMainThreadPreparationBeforeCompile()
<span class="line-new-header">--- 969,27 ---</span>
      if (!m_exceptionChecksWithCallFrameRollback.empty()) {
          m_exceptionChecksWithCallFrameRollback.link(this);
  
          copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
  
<span class="line-modified">!         // operationLookupExceptionHandlerFromCallerFrame is passed one argument, the VM*.</span>
          move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified">!         prepareCallOperation(vm());</span>
<span class="line-modified">!         m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandlerFromCallerFrame)));</span>
          jumpToExceptionHandler(vm());
      }
  
      if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
          m_exceptionHandler = label();
          m_exceptionChecks.link(this);
  
          copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
  
<span class="line-modified">!         // operationLookupExceptionHandler is passed one argument, the VM*.</span>
          move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified">!         prepareCallOperation(vm());</span>
<span class="line-modified">!         m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandler)));</span>
          jumpToExceptionHandler(vm());
      }
  }
  
  void JIT::doMainThreadPreparationBeforeCompile()
</pre>
<center><a href="ICStats.h.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JIT.h.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>