<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITArithmetic.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;ArithProfile.h&quot;
  32 #include &quot;BytecodeGenerator.h&quot;
  33 #include &quot;CodeBlock.h&quot;
  34 #include &quot;JITAddGenerator.h&quot;
  35 #include &quot;JITBitAndGenerator.h&quot;
  36 #include &quot;JITBitOrGenerator.h&quot;
  37 #include &quot;JITBitXorGenerator.h&quot;
  38 #include &quot;JITDivGenerator.h&quot;
  39 #include &quot;JITInlines.h&quot;
  40 #include &quot;JITLeftShiftGenerator.h&quot;
  41 #include &quot;JITMathIC.h&quot;
  42 #include &quot;JITMulGenerator.h&quot;
  43 #include &quot;JITNegGenerator.h&quot;
  44 #include &quot;JITOperations.h&quot;
  45 #include &quot;JITSubGenerator.h&quot;
  46 #include &quot;JSArray.h&quot;
  47 #include &quot;JSFunction.h&quot;
  48 #include &quot;Interpreter.h&quot;
  49 #include &quot;JSCInlines.h&quot;
  50 #include &quot;LinkBuffer.h&quot;
  51 #include &quot;ResultType.h&quot;
  52 #include &quot;SlowPathCall.h&quot;
  53 
  54 namespace JSC {
  55 
  56 void JIT::emit_op_jless(const Instruction* currentInstruction)
  57 {
  58     emit_compareAndJump&lt;OpJless&gt;(currentInstruction, LessThan);
  59 }
  60 
  61 void JIT::emit_op_jlesseq(const Instruction* currentInstruction)
  62 {
  63     emit_compareAndJump&lt;OpJlesseq&gt;(currentInstruction, LessThanOrEqual);
  64 }
  65 
  66 void JIT::emit_op_jgreater(const Instruction* currentInstruction)
  67 {
  68     emit_compareAndJump&lt;OpJgreater&gt;(currentInstruction, GreaterThan);
  69 }
  70 
  71 void JIT::emit_op_jgreatereq(const Instruction* currentInstruction)
  72 {
  73     emit_compareAndJump&lt;OpJgreatereq&gt;(currentInstruction, GreaterThanOrEqual);
  74 }
  75 
  76 void JIT::emit_op_jnless(const Instruction* currentInstruction)
  77 {
  78     emit_compareAndJump&lt;OpJnless&gt;(currentInstruction, GreaterThanOrEqual);
  79 }
  80 
  81 void JIT::emit_op_jnlesseq(const Instruction* currentInstruction)
  82 {
  83     emit_compareAndJump&lt;OpJnlesseq&gt;(currentInstruction, GreaterThan);
  84 }
  85 
  86 void JIT::emit_op_jngreater(const Instruction* currentInstruction)
  87 {
  88     emit_compareAndJump&lt;OpJngreater&gt;(currentInstruction, LessThanOrEqual);
  89 }
  90 
  91 void JIT::emit_op_jngreatereq(const Instruction* currentInstruction)
  92 {
  93     emit_compareAndJump&lt;OpJngreatereq&gt;(currentInstruction, LessThan);
  94 }
  95 
  96 void JIT::emitSlow_op_jless(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
  97 {
  98     emit_compareAndJumpSlow&lt;OpJless&gt;(currentInstruction, DoubleLessThan, operationCompareLess, false, iter);
  99 }
 100 
 101 void JIT::emitSlow_op_jlesseq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 102 {
 103     emit_compareAndJumpSlow&lt;OpJlesseq&gt;(currentInstruction, DoubleLessThanOrEqual, operationCompareLessEq, false, iter);
 104 }
 105 
 106 void JIT::emitSlow_op_jgreater(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 107 {
 108     emit_compareAndJumpSlow&lt;OpJgreater&gt;(currentInstruction, DoubleGreaterThan, operationCompareGreater, false, iter);
 109 }
 110 
 111 void JIT::emitSlow_op_jgreatereq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 112 {
 113     emit_compareAndJumpSlow&lt;OpJgreatereq&gt;(currentInstruction, DoubleGreaterThanOrEqual, operationCompareGreaterEq, false, iter);
 114 }
 115 
 116 void JIT::emitSlow_op_jnless(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 117 {
 118     emit_compareAndJumpSlow&lt;OpJnless&gt;(currentInstruction, DoubleGreaterThanOrEqualOrUnordered, operationCompareLess, true, iter);
 119 }
 120 
 121 void JIT::emitSlow_op_jnlesseq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 122 {
 123     emit_compareAndJumpSlow&lt;OpJnlesseq&gt;(currentInstruction, DoubleGreaterThanOrUnordered, operationCompareLessEq, true, iter);
 124 }
 125 
 126 void JIT::emitSlow_op_jngreater(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 127 {
 128     emit_compareAndJumpSlow&lt;OpJngreater&gt;(currentInstruction, DoubleLessThanOrEqualOrUnordered, operationCompareGreater, true, iter);
 129 }
 130 
 131 void JIT::emitSlow_op_jngreatereq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 132 {
 133     emit_compareAndJumpSlow&lt;OpJngreatereq&gt;(currentInstruction, DoubleLessThanOrUnordered, operationCompareGreaterEq, true, iter);
 134 }
 135 
 136 void JIT::emit_op_below(const Instruction* currentInstruction)
 137 {
 138     emit_compareUnsigned&lt;OpBelow&gt;(currentInstruction, Below);
 139 }
 140 
 141 void JIT::emit_op_beloweq(const Instruction* currentInstruction)
 142 {
 143     emit_compareUnsigned&lt;OpBeloweq&gt;(currentInstruction, BelowOrEqual);
 144 }
 145 
 146 void JIT::emit_op_jbelow(const Instruction* currentInstruction)
 147 {
 148     emit_compareUnsignedAndJump&lt;OpJbelow&gt;(currentInstruction, Below);
 149 }
 150 
 151 void JIT::emit_op_jbeloweq(const Instruction* currentInstruction)
 152 {
 153     emit_compareUnsignedAndJump&lt;OpJbeloweq&gt;(currentInstruction, BelowOrEqual);
 154 }
 155 
 156 #if USE(JSVALUE64)
 157 
 158 void JIT::emit_op_unsigned(const Instruction* currentInstruction)
 159 {
 160     auto bytecode = currentInstruction-&gt;as&lt;OpUnsigned&gt;();
 161     VirtualRegister result = bytecode.m_dst;
 162     VirtualRegister op1 = bytecode.m_operand;
 163 
 164     emitGetVirtualRegister(op1, regT0);
 165     emitJumpSlowCaseIfNotInt(regT0);
 166     addSlowCase(branch32(LessThan, regT0, TrustedImm32(0)));
 167     boxInt32(regT0, JSValueRegs { regT0 });
 168     emitPutVirtualRegister(result, regT0);
 169 }
 170 
 171 template&lt;typename Op&gt;
 172 void JIT::emit_compareAndJump(const Instruction* instruction, RelationalCondition condition)
 173 {
 174     auto bytecode = instruction-&gt;as&lt;Op&gt;();
 175     VirtualRegister op1 = bytecode.m_lhs;
 176     VirtualRegister op2 = bytecode.m_rhs;
 177     unsigned target = jumpTarget(instruction, bytecode.m_targetLabel);
 178     emit_compareAndJumpImpl(op1, op2, target, condition);
 179 }
 180 
 181 void JIT::emit_compareAndJumpImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, RelationalCondition condition)
 182 {
 183     // We generate inline code for the following cases in the fast path:
 184     // - int immediate to constant int immediate
 185     // - constant int immediate to int immediate
 186     // - int immediate to int immediate
 187 
 188     bool disallowAllocation = false;
 189     if (isOperandConstantChar(op1)) {
 190         emitGetVirtualRegister(op2, regT0);
 191         addSlowCase(branchIfNotCell(regT0));
 192         JumpList failures;
 193         emitLoadCharacterString(regT0, regT0, failures);
 194         addSlowCase(failures);
 195         addJump(branch32(commute(condition), regT0, Imm32(asString(getConstantOperand(op1))-&gt;tryGetValue(disallowAllocation)[0])), target);
 196         return;
 197     }
 198     if (isOperandConstantChar(op2)) {
 199         emitGetVirtualRegister(op1, regT0);
 200         addSlowCase(branchIfNotCell(regT0));
 201         JumpList failures;
 202         emitLoadCharacterString(regT0, regT0, failures);
 203         addSlowCase(failures);
 204         addJump(branch32(condition, regT0, Imm32(asString(getConstantOperand(op2))-&gt;tryGetValue(disallowAllocation)[0])), target);
 205         return;
 206     }
 207     if (isOperandConstantInt(op2)) {
 208         emitGetVirtualRegister(op1, regT0);
 209         emitJumpSlowCaseIfNotInt(regT0);
 210         int32_t op2imm = getOperandConstantInt(op2);
 211         addJump(branch32(condition, regT0, Imm32(op2imm)), target);
 212         return;
 213     }
 214     if (isOperandConstantInt(op1)) {
 215         emitGetVirtualRegister(op2, regT1);
 216         emitJumpSlowCaseIfNotInt(regT1);
 217         int32_t op1imm = getOperandConstantInt(op1);
 218         addJump(branch32(commute(condition), regT1, Imm32(op1imm)), target);
 219         return;
 220     }
 221 
 222     emitGetVirtualRegisters(op1, regT0, op2, regT1);
 223     emitJumpSlowCaseIfNotInt(regT0);
 224     emitJumpSlowCaseIfNotInt(regT1);
 225 
 226     addJump(branch32(condition, regT0, regT1), target);
 227 }
 228 
 229 template&lt;typename Op&gt;
 230 void JIT::emit_compareUnsignedAndJump(const Instruction* instruction, RelationalCondition condition)
 231 {
 232     auto bytecode = instruction-&gt;as&lt;Op&gt;();
 233     VirtualRegister op1 = bytecode.m_lhs;
 234     VirtualRegister op2 = bytecode.m_rhs;
 235     unsigned target = jumpTarget(instruction, bytecode.m_targetLabel);
 236     emit_compareUnsignedAndJumpImpl(op1, op2, target, condition);
 237 }
 238 
 239 void JIT::emit_compareUnsignedAndJumpImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, RelationalCondition condition)
 240 {
 241     if (isOperandConstantInt(op2)) {
 242         emitGetVirtualRegister(op1, regT0);
 243         int32_t op2imm = getOperandConstantInt(op2);
 244         addJump(branch32(condition, regT0, Imm32(op2imm)), target);
 245     } else if (isOperandConstantInt(op1)) {
 246         emitGetVirtualRegister(op2, regT1);
 247         int32_t op1imm = getOperandConstantInt(op1);
 248         addJump(branch32(commute(condition), regT1, Imm32(op1imm)), target);
 249     } else {
 250         emitGetVirtualRegisters(op1, regT0, op2, regT1);
 251         addJump(branch32(condition, regT0, regT1), target);
 252     }
 253 }
 254 
 255 template&lt;typename Op&gt;
 256 void JIT::emit_compareUnsigned(const Instruction* instruction, RelationalCondition condition)
 257 {
 258     auto bytecode = instruction-&gt;as&lt;Op&gt;();
 259     VirtualRegister dst = bytecode.m_dst;
 260     VirtualRegister op1 = bytecode.m_lhs;
 261     VirtualRegister op2 = bytecode.m_rhs;
 262     emit_compareUnsignedImpl(dst, op1, op2, condition);
 263 }
 264 
 265 void JIT::emit_compareUnsignedImpl(VirtualRegister dst, VirtualRegister op1, VirtualRegister op2, RelationalCondition condition)
 266 {
 267     if (isOperandConstantInt(op2)) {
 268         emitGetVirtualRegister(op1, regT0);
 269         int32_t op2imm = getOperandConstantInt(op2);
 270         compare32(condition, regT0, Imm32(op2imm), regT0);
 271     } else if (isOperandConstantInt(op1)) {
 272         emitGetVirtualRegister(op2, regT0);
 273         int32_t op1imm = getOperandConstantInt(op1);
 274         compare32(commute(condition), regT0, Imm32(op1imm), regT0);
 275     } else {
 276         emitGetVirtualRegisters(op1, regT0, op2, regT1);
 277         compare32(condition, regT0, regT1, regT0);
 278     }
 279     boxBoolean(regT0, JSValueRegs { regT0 });
 280     emitPutVirtualRegister(dst);
 281 }
 282 
 283 template&lt;typename Op&gt;
 284 void JIT::emit_compareAndJumpSlow(const Instruction* instruction, DoubleCondition condition, size_t (JIT_OPERATION *operation)(JSGlobalObject*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 285 {
 286     auto bytecode = instruction-&gt;as&lt;Op&gt;();
 287     VirtualRegister op1 = bytecode.m_lhs;
 288     VirtualRegister op2 = bytecode.m_rhs;
 289     unsigned target = jumpTarget(instruction, bytecode.m_targetLabel);
 290     emit_compareAndJumpSlowImpl(op1, op2, target, instruction-&gt;size(), condition, operation, invert, iter);
 291 }
 292 
 293 void JIT::emit_compareAndJumpSlowImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, size_t instructionSize, DoubleCondition condition, size_t (JIT_OPERATION *operation)(JSGlobalObject*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 294 {
 295 
 296     // We generate inline code for the following cases in the slow path:
 297     // - floating-point number to constant int immediate
 298     // - constant int immediate to floating-point number
 299     // - floating-point number to floating-point number.
 300     if (isOperandConstantChar(op1) || isOperandConstantChar(op2)) {
 301         linkAllSlowCases(iter);
 302 
 303         emitGetVirtualRegister(op1, argumentGPR0);
 304         emitGetVirtualRegister(op2, argumentGPR1);
 305         callOperation(operation, TrustedImmPtr(m_codeBlock-&gt;globalObject()), argumentGPR0, argumentGPR1);
 306         emitJumpSlowToHot(branchTest32(invert ? Zero : NonZero, returnValueGPR), target);
 307         return;
 308     }
 309 
 310     if (isOperandConstantInt(op2)) {
 311         linkAllSlowCases(iter);
 312 
 313         if (supportsFloatingPoint()) {
 314             Jump fail1 = branchIfNotNumber(regT0);
 315             add64(numberTagRegister, regT0);
 316             move64ToDouble(regT0, fpRegT0);
 317 
 318             int32_t op2imm = getConstantOperand(op2).asInt32();
 319 
 320             move(Imm32(op2imm), regT1);
 321             convertInt32ToDouble(regT1, fpRegT1);
 322 
 323             emitJumpSlowToHot(branchDouble(condition, fpRegT0, fpRegT1), target);
 324 
 325             emitJumpSlowToHot(jump(), instructionSize);
 326 
 327             fail1.link(this);
 328         }
 329 
 330         emitGetVirtualRegister(op2, regT1);
 331         callOperation(operation, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
 332         emitJumpSlowToHot(branchTest32(invert ? Zero : NonZero, returnValueGPR), target);
 333         return;
 334     }
 335 
 336     if (isOperandConstantInt(op1)) {
 337         linkAllSlowCases(iter);
 338 
 339         if (supportsFloatingPoint()) {
 340             Jump fail1 = branchIfNotNumber(regT1);
 341             add64(numberTagRegister, regT1);
 342             move64ToDouble(regT1, fpRegT1);
 343 
 344             int32_t op1imm = getConstantOperand(op1).asInt32();
 345 
 346             move(Imm32(op1imm), regT0);
 347             convertInt32ToDouble(regT0, fpRegT0);
 348 
 349             emitJumpSlowToHot(branchDouble(condition, fpRegT0, fpRegT1), target);
 350 
 351             emitJumpSlowToHot(jump(), instructionSize);
 352 
 353             fail1.link(this);
 354         }
 355 
 356         emitGetVirtualRegister(op1, regT2);
 357         callOperation(operation, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT2, regT1);
 358         emitJumpSlowToHot(branchTest32(invert ? Zero : NonZero, returnValueGPR), target);
 359         return;
 360     }
 361 
 362     linkSlowCase(iter); // LHS is not Int.
 363 
 364     if (supportsFloatingPoint()) {
 365         Jump fail1 = branchIfNotNumber(regT0);
 366         Jump fail2 = branchIfNotNumber(regT1);
 367         Jump fail3 = branchIfInt32(regT1);
 368         add64(numberTagRegister, regT0);
 369         add64(numberTagRegister, regT1);
 370         move64ToDouble(regT0, fpRegT0);
 371         move64ToDouble(regT1, fpRegT1);
 372 
 373         emitJumpSlowToHot(branchDouble(condition, fpRegT0, fpRegT1), target);
 374 
 375         emitJumpSlowToHot(jump(), instructionSize);
 376 
 377         fail1.link(this);
 378         fail2.link(this);
 379         fail3.link(this);
 380     }
 381 
 382     linkSlowCase(iter); // RHS is not Int.
 383     callOperation(operation, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
 384     emitJumpSlowToHot(branchTest32(invert ? Zero : NonZero, returnValueGPR), target);
 385 }
 386 
 387 void JIT::emit_op_inc(const Instruction* currentInstruction)
 388 {
 389     auto bytecode = currentInstruction-&gt;as&lt;OpInc&gt;();
 390     VirtualRegister srcDst = bytecode.m_srcDst;
 391 
 392     emitGetVirtualRegister(srcDst, regT0);
 393     emitJumpSlowCaseIfNotInt(regT0);
 394     addSlowCase(branchAdd32(Overflow, TrustedImm32(1), regT0));
 395     boxInt32(regT0, JSValueRegs { regT0 });
 396     emitPutVirtualRegister(srcDst);
 397 }
 398 
 399 void JIT::emit_op_dec(const Instruction* currentInstruction)
 400 {
 401     auto bytecode = currentInstruction-&gt;as&lt;OpDec&gt;();
 402     VirtualRegister srcDst = bytecode.m_srcDst;
 403 
 404     emitGetVirtualRegister(srcDst, regT0);
 405     emitJumpSlowCaseIfNotInt(regT0);
 406     addSlowCase(branchSub32(Overflow, TrustedImm32(1), regT0));
 407     boxInt32(regT0, JSValueRegs { regT0 });
 408     emitPutVirtualRegister(srcDst);
 409 }
 410 
 411 /* ------------------------------ BEGIN: OP_MOD ------------------------------ */
 412 
 413 #if CPU(X86_64)
 414 
 415 void JIT::emit_op_mod(const Instruction* currentInstruction)
 416 {
 417     auto bytecode = currentInstruction-&gt;as&lt;OpMod&gt;();
 418     VirtualRegister result = bytecode.m_dst;
 419     VirtualRegister op1 = bytecode.m_lhs;
 420     VirtualRegister op2 = bytecode.m_rhs;
 421 
 422     // Make sure registers are correct for x86 IDIV instructions.
 423     ASSERT(regT0 == X86Registers::eax);
 424     auto edx = X86Registers::edx;
 425     auto ecx = X86Registers::ecx;
 426     ASSERT(regT4 != edx);
 427     ASSERT(regT4 != ecx);
 428 
 429     emitGetVirtualRegisters(op1, regT4, op2, ecx);
 430     emitJumpSlowCaseIfNotInt(regT4);
 431     emitJumpSlowCaseIfNotInt(ecx);
 432 
 433     move(regT4, regT0);
 434     addSlowCase(branchTest32(Zero, ecx));
 435     Jump denominatorNotNeg1 = branch32(NotEqual, ecx, TrustedImm32(-1));
 436     addSlowCase(branch32(Equal, regT0, TrustedImm32(-2147483647-1)));
 437     denominatorNotNeg1.link(this);
 438     x86ConvertToDoubleWord32();
 439     x86Div32(ecx);
 440     Jump numeratorPositive = branch32(GreaterThanOrEqual, regT4, TrustedImm32(0));
 441     addSlowCase(branchTest32(Zero, edx));
 442     numeratorPositive.link(this);
 443     boxInt32(edx, JSValueRegs { regT0 });
 444     emitPutVirtualRegister(result);
 445 }
 446 
 447 void JIT::emitSlow_op_mod(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 448 {
 449     linkAllSlowCases(iter);
 450 
 451     JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_mod);
 452     slowPathCall.call();
 453 }
 454 
 455 #else // CPU(X86_64)
 456 
 457 void JIT::emit_op_mod(const Instruction* currentInstruction)
 458 {
 459     JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_mod);
 460     slowPathCall.call();
 461 }
 462 
 463 void JIT::emitSlow_op_mod(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;)
 464 {
 465     UNREACHABLE_FOR_PLATFORM();
 466 }
 467 
 468 #endif // CPU(X86_64)
 469 
 470 /* ------------------------------ END: OP_MOD ------------------------------ */
 471 
 472 #endif // USE(JSVALUE64)
 473 
 474 void JIT::emit_op_negate(const Instruction* currentInstruction)
 475 {
 476     UnaryArithProfile* arithProfile = &amp;currentInstruction-&gt;as&lt;OpNegate&gt;().metadata(m_codeBlock).m_arithProfile;
 477     JITNegIC* negateIC = m_codeBlock-&gt;addJITNegIC(arithProfile);
 478     m_instructionToMathIC.add(currentInstruction, negateIC);
 479     emitMathICFast&lt;OpNegate&gt;(negateIC, currentInstruction, operationArithNegateProfiled, operationArithNegate);
 480 }
 481 
 482 void JIT::emitSlow_op_negate(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 483 {
 484     linkAllSlowCases(iter);
 485 
 486     JITNegIC* negIC = bitwise_cast&lt;JITNegIC*&gt;(m_instructionToMathIC.get(currentInstruction));
 487     emitMathICSlow&lt;OpNegate&gt;(negIC, currentInstruction, operationArithNegateProfiledOptimize, operationArithNegateProfiled, operationArithNegateOptimize);
 488 }
 489 
 490 template&lt;typename Op, typename SnippetGenerator&gt;
 491 void JIT::emitBitBinaryOpFastPath(const Instruction* currentInstruction, ProfilingPolicy profilingPolicy)
 492 {
 493     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 494     VirtualRegister result = bytecode.m_dst;
 495     VirtualRegister op1 = bytecode.m_lhs;
 496     VirtualRegister op2 = bytecode.m_rhs;
 497 
 498 #if USE(JSVALUE64)
 499     JSValueRegs leftRegs = JSValueRegs(regT0);
 500     JSValueRegs rightRegs = JSValueRegs(regT1);
 501     JSValueRegs resultRegs = leftRegs;
 502     GPRReg scratchGPR = regT2;
 503 #else
 504     JSValueRegs leftRegs = JSValueRegs(regT1, regT0);
 505     JSValueRegs rightRegs = JSValueRegs(regT3, regT2);
 506     JSValueRegs resultRegs = leftRegs;
 507     GPRReg scratchGPR = regT4;
 508 #endif
 509 
 510     SnippetOperand leftOperand;
 511     SnippetOperand rightOperand;
 512 
 513     if (isOperandConstantInt(op1))
 514         leftOperand.setConstInt32(getOperandConstantInt(op1));
 515     else if (isOperandConstantInt(op2))
 516         rightOperand.setConstInt32(getOperandConstantInt(op2));
 517 
 518     RELEASE_ASSERT(!leftOperand.isConst() || !rightOperand.isConst());
 519 
 520     if (!leftOperand.isConst())
 521         emitGetVirtualRegister(op1, leftRegs);
 522     if (!rightOperand.isConst())
 523         emitGetVirtualRegister(op2, rightRegs);
 524 
 525     SnippetGenerator gen(leftOperand, rightOperand, resultRegs, leftRegs, rightRegs, scratchGPR);
 526 
 527     gen.generateFastPath(*this);
 528 
 529     ASSERT(gen.didEmitFastPath());
 530     gen.endJumpList().link(this);
 531     if (profilingPolicy == ProfilingPolicy::ShouldEmitProfiling)
 532         emitValueProfilingSiteIfProfiledOpcode(bytecode);
 533     emitPutVirtualRegister(result, resultRegs);
 534 
 535     addSlowCase(gen.slowPathJumpList());
 536 }
 537 
 538 void JIT::emit_op_bitnot(const Instruction* currentInstruction)
 539 {
 540     auto bytecode = currentInstruction-&gt;as&lt;OpBitnot&gt;();
 541     VirtualRegister result = bytecode.m_dst;
 542     VirtualRegister op1 = bytecode.m_operand;
 543 
 544 #if USE(JSVALUE64)
 545     JSValueRegs leftRegs = JSValueRegs(regT0);
 546 #else
 547     JSValueRegs leftRegs = JSValueRegs(regT1, regT0);
 548 #endif
 549 
 550     emitGetVirtualRegister(op1, leftRegs);
 551 
 552     addSlowCase(branchIfNotInt32(leftRegs));
 553     not32(leftRegs.payloadGPR());
 554 #if USE(JSVALUE64)
 555     boxInt32(leftRegs.payloadGPR(), leftRegs);
 556 #endif
 557 
 558     emitValueProfilingSiteIfProfiledOpcode(bytecode);
 559 
 560     emitPutVirtualRegister(result, leftRegs);
 561 }
 562 
 563 void JIT::emit_op_bitand(const Instruction* currentInstruction)
 564 {
 565     emitBitBinaryOpFastPath&lt;OpBitand, JITBitAndGenerator&gt;(currentInstruction, ProfilingPolicy::ShouldEmitProfiling);
 566 }
 567 
 568 void JIT::emit_op_bitor(const Instruction* currentInstruction)
 569 {
 570     emitBitBinaryOpFastPath&lt;OpBitor, JITBitOrGenerator&gt;(currentInstruction, ProfilingPolicy::ShouldEmitProfiling);
 571 }
 572 
 573 void JIT::emit_op_bitxor(const Instruction* currentInstruction)
 574 {
 575     emitBitBinaryOpFastPath&lt;OpBitxor, JITBitXorGenerator&gt;(currentInstruction, ProfilingPolicy::ShouldEmitProfiling);
 576 }
 577 
 578 void JIT::emit_op_lshift(const Instruction* currentInstruction)
 579 {
 580     emitBitBinaryOpFastPath&lt;OpLshift, JITLeftShiftGenerator&gt;(currentInstruction);
 581 }
 582 
 583 void JIT::emitRightShiftFastPath(const Instruction* currentInstruction, OpcodeID opcodeID)
 584 {
 585     ASSERT(opcodeID == op_rshift || opcodeID == op_urshift);
 586     switch (opcodeID) {
 587     case op_rshift:
 588         emitRightShiftFastPath&lt;OpRshift&gt;(currentInstruction, JITRightShiftGenerator::SignedShift);
 589         break;
 590     case op_urshift:
 591         emitRightShiftFastPath&lt;OpUrshift&gt;(currentInstruction, JITRightShiftGenerator::UnsignedShift);
 592         break;
 593     default:
 594         ASSERT_NOT_REACHED();
 595     }
 596 }
 597 
 598 template&lt;typename Op&gt;
 599 void JIT::emitRightShiftFastPath(const Instruction* currentInstruction, JITRightShiftGenerator::ShiftType snippetShiftType)
 600 {
 601     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 602     VirtualRegister result = bytecode.m_dst;
 603     VirtualRegister op1 = bytecode.m_lhs;
 604     VirtualRegister op2 = bytecode.m_rhs;
 605 
 606 #if USE(JSVALUE64)
 607     JSValueRegs leftRegs = JSValueRegs(regT0);
 608     JSValueRegs rightRegs = JSValueRegs(regT1);
 609     JSValueRegs resultRegs = leftRegs;
 610     GPRReg scratchGPR = regT2;
 611     FPRReg scratchFPR = InvalidFPRReg;
 612 #else
 613     JSValueRegs leftRegs = JSValueRegs(regT1, regT0);
 614     JSValueRegs rightRegs = JSValueRegs(regT3, regT2);
 615     JSValueRegs resultRegs = leftRegs;
 616     GPRReg scratchGPR = regT4;
 617     FPRReg scratchFPR = fpRegT2;
 618 #endif
 619 
 620     SnippetOperand leftOperand;
 621     SnippetOperand rightOperand;
 622 
 623     if (isOperandConstantInt(op1))
 624         leftOperand.setConstInt32(getOperandConstantInt(op1));
 625     else if (isOperandConstantInt(op2))
 626         rightOperand.setConstInt32(getOperandConstantInt(op2));
 627 
 628     RELEASE_ASSERT(!leftOperand.isConst() || !rightOperand.isConst());
 629 
 630     if (!leftOperand.isConst())
 631         emitGetVirtualRegister(op1, leftRegs);
 632     if (!rightOperand.isConst())
 633         emitGetVirtualRegister(op2, rightRegs);
 634 
 635     JITRightShiftGenerator gen(leftOperand, rightOperand, resultRegs, leftRegs, rightRegs,
 636         fpRegT0, scratchGPR, scratchFPR, snippetShiftType);
 637 
 638     gen.generateFastPath(*this);
 639 
 640     ASSERT(gen.didEmitFastPath());
 641     gen.endJumpList().link(this);
 642     emitPutVirtualRegister(result, resultRegs);
 643 
 644     addSlowCase(gen.slowPathJumpList());
 645 }
 646 
 647 void JIT::emit_op_rshift(const Instruction* currentInstruction)
 648 {
 649     emitRightShiftFastPath(currentInstruction, op_rshift);
 650 }
 651 
 652 void JIT::emit_op_urshift(const Instruction* currentInstruction)
 653 {
 654     emitRightShiftFastPath(currentInstruction, op_urshift);
 655 }
 656 
 657 void JIT::emit_op_add(const Instruction* currentInstruction)
 658 {
 659     BinaryArithProfile* arithProfile = &amp;currentInstruction-&gt;as&lt;OpAdd&gt;().metadata(m_codeBlock).m_arithProfile;
 660     JITAddIC* addIC = m_codeBlock-&gt;addJITAddIC(arithProfile);
 661     m_instructionToMathIC.add(currentInstruction, addIC);
 662     emitMathICFast&lt;OpAdd&gt;(addIC, currentInstruction, operationValueAddProfiled, operationValueAdd);
 663 }
 664 
 665 void JIT::emitSlow_op_add(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 666 {
 667     linkAllSlowCases(iter);
 668 
 669     JITAddIC* addIC = bitwise_cast&lt;JITAddIC*&gt;(m_instructionToMathIC.get(currentInstruction));
 670     emitMathICSlow&lt;OpAdd&gt;(addIC, currentInstruction, operationValueAddProfiledOptimize, operationValueAddProfiled, operationValueAddOptimize);
 671 }
 672 
 673 template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
 674 void JIT::emitMathICFast(JITUnaryMathIC&lt;Generator&gt;* mathIC, const Instruction* currentInstruction, ProfiledFunction profiledFunction, NonProfiledFunction nonProfiledFunction)
 675 {
 676     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 677     VirtualRegister result = bytecode.m_dst;
 678     VirtualRegister operand = bytecode.m_operand;
 679 
 680 #if USE(JSVALUE64)
 681     // ArithNegate benefits from using the same register as src and dst.
 682     // Since regT1==argumentGPR1, using regT1 avoid shuffling register to call the slow path.
 683     JSValueRegs srcRegs = JSValueRegs(regT1);
 684     JSValueRegs resultRegs = JSValueRegs(regT1);
 685     GPRReg scratchGPR = regT2;
 686 #else
 687     JSValueRegs srcRegs = JSValueRegs(regT1, regT0);
 688     JSValueRegs resultRegs = JSValueRegs(regT3, regT2);
 689     GPRReg scratchGPR = regT4;
 690 #endif
 691 
 692 #if ENABLE(MATH_IC_STATS)
 693     auto inlineStart = label();
 694 #endif
 695 
 696     mathIC-&gt;m_generator = Generator(resultRegs, srcRegs, scratchGPR);
 697 
 698     emitGetVirtualRegister(operand, srcRegs);
 699 
 700     MathICGenerationState&amp; mathICGenerationState = m_instructionToMathICGenerationState.add(currentInstruction, MathICGenerationState()).iterator-&gt;value;
 701 
 702     bool generatedInlineCode = mathIC-&gt;generateInline(*this, mathICGenerationState);
 703     if (!generatedInlineCode) {
 704         UnaryArithProfile* arithProfile = mathIC-&gt;arithProfile();
 705         if (arithProfile &amp;&amp; shouldEmitProfiling())
 706             callOperationWithResult(profiledFunction, resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), srcRegs, arithProfile);
 707         else
 708             callOperationWithResult(nonProfiledFunction, resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), srcRegs);
 709     } else
 710         addSlowCase(mathICGenerationState.slowPathJumps);
 711 
 712 #if ENABLE(MATH_IC_STATS)
 713     auto inlineEnd = label();
 714     addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
 715         size_t size = linkBuffer.locationOf(inlineEnd).executableAddress&lt;char*&gt;() - linkBuffer.locationOf(inlineStart).executableAddress&lt;char*&gt;();
 716         mathIC-&gt;m_generatedCodeSize += size;
 717     });
 718 #endif
 719 
 720     emitPutVirtualRegister(result, resultRegs);
 721 }
 722 
 723 template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
 724 void JIT::emitMathICFast(JITBinaryMathIC&lt;Generator&gt;* mathIC, const Instruction* currentInstruction, ProfiledFunction profiledFunction, NonProfiledFunction nonProfiledFunction)
 725 {
 726     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 727     VirtualRegister result = bytecode.m_dst;
 728     VirtualRegister op1 = bytecode.m_lhs;
 729     VirtualRegister op2 = bytecode.m_rhs;
 730 
 731 #if USE(JSVALUE64)
 732     JSValueRegs leftRegs = JSValueRegs(regT1);
 733     JSValueRegs rightRegs = JSValueRegs(regT2);
 734     JSValueRegs resultRegs = JSValueRegs(regT0);
 735     GPRReg scratchGPR = regT3;
 736     FPRReg scratchFPR = fpRegT2;
 737 #else
 738     JSValueRegs leftRegs = JSValueRegs(regT1, regT0);
 739     JSValueRegs rightRegs = JSValueRegs(regT3, regT2);
 740     JSValueRegs resultRegs = leftRegs;
 741     GPRReg scratchGPR = regT4;
 742     FPRReg scratchFPR = fpRegT2;
 743 #endif
 744 
 745     SnippetOperand leftOperand(bytecode.m_operandTypes.first());
 746     SnippetOperand rightOperand(bytecode.m_operandTypes.second());
 747 
 748     if (isOperandConstantInt(op1))
 749         leftOperand.setConstInt32(getOperandConstantInt(op1));
 750     else if (isOperandConstantInt(op2))
 751         rightOperand.setConstInt32(getOperandConstantInt(op2));
 752 
 753     RELEASE_ASSERT(!leftOperand.isConst() || !rightOperand.isConst());
 754 
 755     mathIC-&gt;m_generator = Generator(leftOperand, rightOperand, resultRegs, leftRegs, rightRegs, fpRegT0, fpRegT1, scratchGPR, scratchFPR);
 756 
 757     ASSERT(!(Generator::isLeftOperandValidConstant(leftOperand) &amp;&amp; Generator::isRightOperandValidConstant(rightOperand)));
 758 
 759     if (!Generator::isLeftOperandValidConstant(leftOperand))
 760         emitGetVirtualRegister(op1, leftRegs);
 761     if (!Generator::isRightOperandValidConstant(rightOperand))
 762         emitGetVirtualRegister(op2, rightRegs);
 763 
 764 #if ENABLE(MATH_IC_STATS)
 765     auto inlineStart = label();
 766 #endif
 767 
 768     MathICGenerationState&amp; mathICGenerationState = m_instructionToMathICGenerationState.add(currentInstruction, MathICGenerationState()).iterator-&gt;value;
 769 
 770     bool generatedInlineCode = mathIC-&gt;generateInline(*this, mathICGenerationState);
 771     if (!generatedInlineCode) {
 772         if (leftOperand.isConst())
 773             emitGetVirtualRegister(op1, leftRegs);
 774         else if (rightOperand.isConst())
 775             emitGetVirtualRegister(op2, rightRegs);
 776         BinaryArithProfile* arithProfile = mathIC-&gt;arithProfile();
 777         if (arithProfile &amp;&amp; shouldEmitProfiling())
 778             callOperationWithResult(profiledFunction, resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), leftRegs, rightRegs, arithProfile);
 779         else
 780             callOperationWithResult(nonProfiledFunction, resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), leftRegs, rightRegs);
 781     } else
 782         addSlowCase(mathICGenerationState.slowPathJumps);
 783 
 784 #if ENABLE(MATH_IC_STATS)
 785     auto inlineEnd = label();
 786     addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
 787         size_t size = linkBuffer.locationOf(inlineEnd).executableAddress&lt;char*&gt;() - linkBuffer.locationOf(inlineStart).executableAddress&lt;char*&gt;();
 788         mathIC-&gt;m_generatedCodeSize += size;
 789     });
 790 #endif
 791 
 792     emitPutVirtualRegister(result, resultRegs);
 793 }
 794 
 795 template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
 796 void JIT::emitMathICSlow(JITUnaryMathIC&lt;Generator&gt;* mathIC, const Instruction* currentInstruction, ProfiledRepatchFunction profiledRepatchFunction, ProfiledFunction profiledFunction, RepatchFunction repatchFunction)
 797 {
 798     MathICGenerationState&amp; mathICGenerationState = m_instructionToMathICGenerationState.find(currentInstruction)-&gt;value;
 799     mathICGenerationState.slowPathStart = label();
 800 
 801     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 802     VirtualRegister result = bytecode.m_dst;
 803 
 804 #if USE(JSVALUE64)
 805     JSValueRegs srcRegs = JSValueRegs(regT1);
 806     JSValueRegs resultRegs = JSValueRegs(regT0);
 807 #else
 808     JSValueRegs srcRegs = JSValueRegs(regT1, regT0);
 809     JSValueRegs resultRegs = JSValueRegs(regT3, regT2);
 810 #endif
 811 
 812 #if ENABLE(MATH_IC_STATS)
 813     auto slowPathStart = label();
 814 #endif
 815 
 816     UnaryArithProfile* arithProfile = mathIC-&gt;arithProfile();
 817     if (arithProfile &amp;&amp; shouldEmitProfiling()) {
 818         if (mathICGenerationState.shouldSlowPathRepatch)
 819             mathICGenerationState.slowPathCall = callOperationWithResult(reinterpret_cast&lt;J_JITOperation_GJMic&gt;(profiledRepatchFunction), resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), srcRegs, TrustedImmPtr(mathIC));
 820         else
 821             mathICGenerationState.slowPathCall = callOperationWithResult(profiledFunction, resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), srcRegs, arithProfile);
 822     } else
 823         mathICGenerationState.slowPathCall = callOperationWithResult(reinterpret_cast&lt;J_JITOperation_GJMic&gt;(repatchFunction), resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), srcRegs, TrustedImmPtr(mathIC));
 824 
 825 #if ENABLE(MATH_IC_STATS)
 826     auto slowPathEnd = label();
 827     addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
 828         size_t size = linkBuffer.locationOf(slowPathEnd).executableAddress&lt;char*&gt;() - linkBuffer.locationOf(slowPathStart).executableAddress&lt;char*&gt;();
 829         mathIC-&gt;m_generatedCodeSize += size;
 830     });
 831 #endif
 832 
 833     emitPutVirtualRegister(result, resultRegs);
 834 
 835     addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
 836         MathICGenerationState&amp; mathICGenerationState = m_instructionToMathICGenerationState.find(currentInstruction)-&gt;value;
 837         mathIC-&gt;finalizeInlineCode(mathICGenerationState, linkBuffer);
 838     });
 839 }
 840 
 841 template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
 842 void JIT::emitMathICSlow(JITBinaryMathIC&lt;Generator&gt;* mathIC, const Instruction* currentInstruction, ProfiledRepatchFunction profiledRepatchFunction, ProfiledFunction profiledFunction, RepatchFunction repatchFunction)
 843 {
 844     MathICGenerationState&amp; mathICGenerationState = m_instructionToMathICGenerationState.find(currentInstruction)-&gt;value;
 845     mathICGenerationState.slowPathStart = label();
 846 
 847     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 848     VirtualRegister result = bytecode.m_dst;
 849     VirtualRegister op1 = bytecode.m_lhs;
 850     VirtualRegister op2 = bytecode.m_rhs;
 851 
 852 #if USE(JSVALUE64)
 853     JSValueRegs leftRegs = JSValueRegs(regT1);
 854     JSValueRegs rightRegs = JSValueRegs(regT2);
 855     JSValueRegs resultRegs = JSValueRegs(regT0);
 856 #else
 857     JSValueRegs leftRegs = JSValueRegs(regT1, regT0);
 858     JSValueRegs rightRegs = JSValueRegs(regT3, regT2);
 859     JSValueRegs resultRegs = leftRegs;
 860 #endif
 861 
 862     SnippetOperand leftOperand(bytecode.m_operandTypes.first());
 863     SnippetOperand rightOperand(bytecode.m_operandTypes.second());
 864 
 865     if (isOperandConstantInt(op1))
 866         leftOperand.setConstInt32(getOperandConstantInt(op1));
 867     else if (isOperandConstantInt(op2))
 868         rightOperand.setConstInt32(getOperandConstantInt(op2));
 869 
 870     ASSERT(!(Generator::isLeftOperandValidConstant(leftOperand) &amp;&amp; Generator::isRightOperandValidConstant(rightOperand)));
 871 
 872     if (Generator::isLeftOperandValidConstant(leftOperand))
 873         emitGetVirtualRegister(op1, leftRegs);
 874     else if (Generator::isRightOperandValidConstant(rightOperand))
 875         emitGetVirtualRegister(op2, rightRegs);
 876 
 877 #if ENABLE(MATH_IC_STATS)
 878     auto slowPathStart = label();
 879 #endif
 880 
 881     BinaryArithProfile* arithProfile = mathIC-&gt;arithProfile();
 882     if (arithProfile &amp;&amp; shouldEmitProfiling()) {
 883         if (mathICGenerationState.shouldSlowPathRepatch)
 884             mathICGenerationState.slowPathCall = callOperationWithResult(bitwise_cast&lt;J_JITOperation_GJJMic&gt;(profiledRepatchFunction), resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), leftRegs, rightRegs, TrustedImmPtr(mathIC));
 885         else
 886             mathICGenerationState.slowPathCall = callOperationWithResult(profiledFunction, resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), leftRegs, rightRegs, arithProfile);
 887     } else
 888         mathICGenerationState.slowPathCall = callOperationWithResult(bitwise_cast&lt;J_JITOperation_GJJMic&gt;(repatchFunction), resultRegs, TrustedImmPtr(m_codeBlock-&gt;globalObject()), leftRegs, rightRegs, TrustedImmPtr(mathIC));
 889 
 890 #if ENABLE(MATH_IC_STATS)
 891     auto slowPathEnd = label();
 892     addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
 893         size_t size = linkBuffer.locationOf(slowPathEnd).executableAddress&lt;char*&gt;() - linkBuffer.locationOf(slowPathStart).executableAddress&lt;char*&gt;();
 894         mathIC-&gt;m_generatedCodeSize += size;
 895     });
 896 #endif
 897 
 898     emitPutVirtualRegister(result, resultRegs);
 899 
 900     addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
 901         MathICGenerationState&amp; mathICGenerationState = m_instructionToMathICGenerationState.find(currentInstruction)-&gt;value;
 902         mathIC-&gt;finalizeInlineCode(mathICGenerationState, linkBuffer);
 903     });
 904 }
 905 
 906 void JIT::emit_op_div(const Instruction* currentInstruction)
 907 {
 908     auto bytecode = currentInstruction-&gt;as&lt;OpDiv&gt;();
 909     VirtualRegister result = bytecode.m_dst;
 910     VirtualRegister op1 = bytecode.m_lhs;
 911     VirtualRegister op2 = bytecode.m_rhs;
 912 
 913 #if USE(JSVALUE64)
 914     JSValueRegs leftRegs = JSValueRegs(regT0);
 915     JSValueRegs rightRegs = JSValueRegs(regT1);
 916     JSValueRegs resultRegs = leftRegs;
 917     GPRReg scratchGPR = regT2;
 918 #else
 919     JSValueRegs leftRegs = JSValueRegs(regT1, regT0);
 920     JSValueRegs rightRegs = JSValueRegs(regT3, regT2);
 921     JSValueRegs resultRegs = leftRegs;
 922     GPRReg scratchGPR = regT4;
 923 #endif
 924     FPRReg scratchFPR = fpRegT2;
 925 
 926     BinaryArithProfile* arithProfile = nullptr;
 927     if (shouldEmitProfiling())
 928         arithProfile = &amp;currentInstruction-&gt;as&lt;OpDiv&gt;().metadata(m_codeBlock).m_arithProfile;
 929 
 930     SnippetOperand leftOperand(bytecode.m_operandTypes.first());
 931     SnippetOperand rightOperand(bytecode.m_operandTypes.second());
 932 
 933     if (isOperandConstantInt(op1))
 934         leftOperand.setConstInt32(getOperandConstantInt(op1));
 935 #if USE(JSVALUE64)
 936     else if (isOperandConstantDouble(op1))
 937         leftOperand.setConstDouble(getOperandConstantDouble(op1));
 938 #endif
 939     else if (isOperandConstantInt(op2))
 940         rightOperand.setConstInt32(getOperandConstantInt(op2));
 941 #if USE(JSVALUE64)
 942     else if (isOperandConstantDouble(op2))
 943         rightOperand.setConstDouble(getOperandConstantDouble(op2));
 944 #endif
 945 
 946     RELEASE_ASSERT(!leftOperand.isConst() || !rightOperand.isConst());
 947 
 948     if (!leftOperand.isConst())
 949         emitGetVirtualRegister(op1, leftRegs);
 950     if (!rightOperand.isConst())
 951         emitGetVirtualRegister(op2, rightRegs);
 952 
 953     JITDivGenerator gen(leftOperand, rightOperand, resultRegs, leftRegs, rightRegs,
 954         fpRegT0, fpRegT1, scratchGPR, scratchFPR, arithProfile);
 955 
 956     gen.generateFastPath(*this);
 957 
 958     if (gen.didEmitFastPath()) {
 959         gen.endJumpList().link(this);
 960         emitPutVirtualRegister(result, resultRegs);
 961 
 962         addSlowCase(gen.slowPathJumpList());
 963     } else {
 964         ASSERT(gen.endJumpList().empty());
 965         ASSERT(gen.slowPathJumpList().empty());
 966         JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_div);
 967         slowPathCall.call();
 968     }
 969 }
 970 
 971 void JIT::emit_op_mul(const Instruction* currentInstruction)
 972 {
 973     BinaryArithProfile* arithProfile = &amp;currentInstruction-&gt;as&lt;OpMul&gt;().metadata(m_codeBlock).m_arithProfile;
 974     JITMulIC* mulIC = m_codeBlock-&gt;addJITMulIC(arithProfile);
 975     m_instructionToMathIC.add(currentInstruction, mulIC);
 976     emitMathICFast&lt;OpMul&gt;(mulIC, currentInstruction, operationValueMulProfiled, operationValueMul);
 977 }
 978 
 979 void JIT::emitSlow_op_mul(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 980 {
 981     linkAllSlowCases(iter);
 982 
 983     JITMulIC* mulIC = bitwise_cast&lt;JITMulIC*&gt;(m_instructionToMathIC.get(currentInstruction));
 984     emitMathICSlow&lt;OpMul&gt;(mulIC, currentInstruction, operationValueMulProfiledOptimize, operationValueMulProfiled, operationValueMulOptimize);
 985 }
 986 
 987 void JIT::emit_op_sub(const Instruction* currentInstruction)
 988 {
 989     BinaryArithProfile* arithProfile = &amp;currentInstruction-&gt;as&lt;OpSub&gt;().metadata(m_codeBlock).m_arithProfile;
 990     JITSubIC* subIC = m_codeBlock-&gt;addJITSubIC(arithProfile);
 991     m_instructionToMathIC.add(currentInstruction, subIC);
 992     emitMathICFast&lt;OpSub&gt;(subIC, currentInstruction, operationValueSubProfiled, operationValueSub);
 993 }
 994 
 995 void JIT::emitSlow_op_sub(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 996 {
 997     linkAllSlowCases(iter);
 998 
 999     JITSubIC* subIC = bitwise_cast&lt;JITSubIC*&gt;(m_instructionToMathIC.get(currentInstruction));
1000     emitMathICSlow&lt;OpSub&gt;(subIC, currentInstruction, operationValueSubProfiledOptimize, operationValueSubProfiled, operationValueSubOptimize);
1001 }
1002 
1003 /* ------------------------------ END: OP_ADD, OP_SUB, OP_MUL, OP_POW ------------------------------ */
1004 
1005 } // namespace JSC
1006 
1007 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>