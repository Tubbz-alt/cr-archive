<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CodeBlock.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlockHash.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  66 #include &quot;PutPropertySlot.h&quot;
  67 #include &quot;ValueProfile.h&quot;
  68 #include &quot;VirtualRegister.h&quot;
  69 #include &quot;Watchpoint.h&quot;
  70 #include &lt;wtf/Bag.h&gt;
  71 #include &lt;wtf/FastMalloc.h&gt;
  72 #include &lt;wtf/RefCountedArray.h&gt;
  73 #include &lt;wtf/RefPtr.h&gt;
  74 #include &lt;wtf/SegmentedVector.h&gt;
  75 #include &lt;wtf/Vector.h&gt;
  76 #include &lt;wtf/text/WTFString.h&gt;
  77 
  78 namespace JSC {
  79 
  80 #if ENABLE(DFG_JIT)
  81 namespace DFG {
  82 struct OSRExitState;
  83 } // namespace DFG
  84 #endif
  85 


  86 class BytecodeLivenessAnalysis;
  87 class CodeBlockSet;
  88 class ExecutableToCodeBlockEdge;
  89 class JSModuleEnvironment;
  90 class LLIntOffsetsExtractor;
  91 class LLIntPrototypeLoadAdaptiveStructureWatchpoint;
  92 class MetadataTable;
  93 class PCToCodeOriginMap;
  94 class RegisterAtOffsetList;
  95 class StructureStubInfo;
  96 


  97 enum class AccessType : int8_t;
  98 
<span class="line-removed">  99 struct ArithProfile;</span>
 100 struct OpCatch;
 101 
 102 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 103 
 104 class CodeBlock : public JSCell {
 105     typedef JSCell Base;
 106     friend class BytecodeLivenessAnalysis;
 107     friend class JIT;
 108     friend class LLIntOffsetsExtractor;
 109 
 110 public:
 111 
 112     enum CopyParsedBlockTag { CopyParsedBlock };
 113 
<span class="line-modified"> 114     static const unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;</span>
<span class="line-modified"> 115     static const bool needsDestruction = true;</span>
 116 
 117     template&lt;typename, SubspaceAccess&gt;
<span class="line-modified"> 118     static IsoSubspace* subspaceFor(VM&amp;) { return nullptr; }</span>





 119 
 120     DECLARE_INFO;
 121 
 122 protected:
 123     CodeBlock(VM&amp;, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);
 124     CodeBlock(VM&amp;, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 125 
 126     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 127     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 128 
 129     void finishCreationCommon(VM&amp;);
 130 
 131     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 132 
 133 public:
 134     JS_EXPORT_PRIVATE ~CodeBlock();
 135 
 136     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 137 
 138     CString inferredName() const;
 139     CodeBlockHash hash() const;
 140     bool hasHash() const;
 141     bool isSafeToComputeHash() const;
 142     CString hashAsStringIfPossible() const;
 143     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 144     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
 145     void dumpAssumingJITType(PrintStream&amp;, JITType) const;
 146     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 147 
 148     MetadataTable* metadataTable() const { return m_metadata.get(); }
 149 
 150     int numParameters() const { return m_numParameters; }
 151     void setNumParameters(int newValue);
 152 
 153     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 154 
 155     int numCalleeLocals() const { return m_numCalleeLocals; }
 156 
 157     int numVars() const { return m_numVars; }

 158 
 159     int* addressOfNumParameters() { return &amp;m_numParameters; }
 160     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 161 
 162     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 163     void setAlternative(VM&amp;, CodeBlock*);
 164 
 165     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 166     {
 167         Functor f(std::forward&lt;Functor&gt;(functor));
 168         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 169         codeBlocks.append(this);
 170 
 171         while (!codeBlocks.isEmpty()) {
 172             CodeBlock* currentCodeBlock = codeBlocks.takeLast();
 173             f(currentCodeBlock);
 174 
 175             if (CodeBlock* alternative = currentCodeBlock-&gt;alternative())
 176                 codeBlocks.append(alternative);
 177             if (CodeBlock* osrEntryBlock = currentCodeBlock-&gt;specialOSREntryBlockOrNull())
</pre>
<hr />
<pre>
 206     void dumpBytecode(PrintStream&amp;);
 207     void dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; = ICStatusMap());
 208     void dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; = ICStatusMap());
 209 
 210     void dumpExceptionHandlers(PrintStream&amp;);
 211     void printStructures(PrintStream&amp;, const Instruction*);
 212     void printStructure(PrintStream&amp;, const char* name, const Instruction*, int operand);
 213 
 214     void dumpMathICStats();
 215 
 216     bool isStrictMode() const { return m_unlinkedCode-&gt;isStrictMode(); }
 217     bool isConstructor() const { return m_unlinkedCode-&gt;isConstructor(); }
 218     ECMAMode ecmaMode() const { return isStrictMode() ? StrictMode : NotStrictMode; }
 219     CodeType codeType() const { return m_unlinkedCode-&gt;codeType(); }
 220 
 221     JSParserScriptMode scriptMode() const { return m_unlinkedCode-&gt;scriptMode(); }
 222 
 223     bool hasInstalledVMTrapBreakpoints() const;
 224     bool installVMTrapBreakpoints();
 225 
<span class="line-modified"> 226     inline bool isKnownNotImmediate(int index)</span>
 227     {
<span class="line-modified"> 228         if (index == thisRegister().offset() &amp;&amp; !isStrictMode())</span>
 229             return true;
 230 
<span class="line-modified"> 231         if (isConstantRegisterIndex(index))</span>
<span class="line-modified"> 232             return getConstant(index).isCell();</span>
 233 
 234         return false;
 235     }
 236 
<span class="line-modified"> 237     ALWAYS_INLINE bool isTemporaryRegisterIndex(int index)</span>
 238     {
<span class="line-modified"> 239         return index &gt;= m_numVars;</span>
 240     }
 241 
<span class="line-modified"> 242     HandlerInfo* handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler = RequiredHandler::AnyHandler);</span>
 243     HandlerInfo* handlerForIndex(unsigned, RequiredHandler = RequiredHandler::AnyHandler);
 244     void removeExceptionHandlerForCallSite(DisposableCallSiteIndex);
<span class="line-modified"> 245     unsigned lineNumberForBytecodeOffset(unsigned bytecodeOffset);</span>
<span class="line-modified"> 246     unsigned columnNumberForBytecodeOffset(unsigned bytecodeOffset);</span>
<span class="line-modified"> 247     void expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot,</span>
 248         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 249 
<span class="line-modified"> 250     Optional&lt;unsigned&gt; bytecodeOffsetFromCallSiteIndex(CallSiteIndex);</span>
 251 





 252     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 253     void getICStatusMap(ICStatusMap&amp; result);
 254 
 255 #if ENABLE(JIT)
 256     struct JITData {
 257         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 258 
 259         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 260         Bag&lt;JITAddIC&gt; m_addICs;
 261         Bag&lt;JITMulIC&gt; m_mulICs;
 262         Bag&lt;JITNegIC&gt; m_negICs;
 263         Bag&lt;JITSubIC&gt; m_subICs;
 264         Bag&lt;ByValInfo&gt; m_byValInfos;
 265         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
 266         SentinelLinkedList&lt;CallLinkInfo, PackedRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;
 267         SentinelLinkedList&lt;PolymorphicCallNode, PackedRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;
<span class="line-modified"> 268         SegmentedVector&lt;RareCaseProfile, 8&gt; m_rareCaseProfiles;</span>
 269         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 270         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 271         JITCodeMap m_jitCodeMap;
 272     };
 273 
 274     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 275     {
 276         if (LIKELY(m_jitData))
 277             return *m_jitData;
 278         return ensureJITDataSlow(locker);
 279     }
 280     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 281 
<span class="line-modified"> 282     JITAddIC* addJITAddIC(ArithProfile*);</span>
<span class="line-modified"> 283     JITMulIC* addJITMulIC(ArithProfile*);</span>
<span class="line-modified"> 284     JITNegIC* addJITNegIC(ArithProfile*);</span>
<span class="line-modified"> 285     JITSubIC* addJITSubIC(ArithProfile*);</span>
 286 
 287     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 288     JITAddIC* addMathIC(ArithProfile* profile) { return addJITAddIC(profile); }</span>
 289 
 290     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 291     JITMulIC* addMathIC(ArithProfile* profile) { return addJITMulIC(profile); }</span>
 292 
 293     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 294     JITNegIC* addMathIC(ArithProfile* profile) { return addJITNegIC(profile); }</span>
 295 
 296     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 297     JITSubIC* addMathIC(ArithProfile* profile) { return addJITSubIC(profile); }</span>
 298 
 299     StructureStubInfo* addStubInfo(AccessType);
 300 
 301     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 302     // stub info.
 303     StructureStubInfo* findStubInfo(CodeOrigin);
 304 
 305     ByValInfo* addByValInfo();
 306 
 307     CallLinkInfo* addCallLinkInfo();
 308 
 309     // This is a slow function call used primarily for compiling OSR exits in the case
 310     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 311     // looking for a CallLinkInfoMap to amortize the cost of calling this.
<span class="line-modified"> 312     CallLinkInfo* getCallLinkInfoForBytecodeIndex(unsigned bytecodeIndex);</span>
 313 
 314     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 315     {
 316         ConcurrentJSLocker locker(m_lock);
 317         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
 318     }
 319     const JITCodeMap&amp; jitCodeMap()
 320     {
 321         ConcurrentJSLocker locker(m_lock);
 322         return ensureJITData(locker).m_jitCodeMap;
 323     }
 324 
 325     void setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp;);
 326     Optional&lt;CodeOrigin&gt; findPC(void* pc);
 327 
 328     void setCalleeSaveRegisters(RegisterSet);
 329     void setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt;);
 330 
<span class="line-modified"> 331     RareCaseProfile* addRareCaseProfile(int bytecodeOffset);</span>
<span class="line-modified"> 332     RareCaseProfile* rareCaseProfileForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);</span>
<span class="line-modified"> 333     unsigned rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);</span>
 334 
<span class="line-modified"> 335     bool likelyToTakeSlowCase(int bytecodeOffset)</span>
 336     {
 337         if (!hasBaselineJITProfiling())
 338             return false;
 339         ConcurrentJSLocker locker(m_lock);
<span class="line-modified"> 340         unsigned value = rareCaseProfileCountForBytecodeOffset(locker, bytecodeOffset);</span>
 341         return value &gt;= Options::likelyToTakeSlowCaseMinimumCount();
 342     }
 343 
<span class="line-modified"> 344     bool couldTakeSlowCase(int bytecodeOffset)</span>
 345     {
 346         if (!hasBaselineJITProfiling())
 347             return false;
 348         ConcurrentJSLocker locker(m_lock);
<span class="line-modified"> 349         unsigned value = rareCaseProfileCountForBytecodeOffset(locker, bytecodeOffset);</span>
 350         return value &gt;= Options::couldTakeSlowCaseMinimumCount();
 351     }
 352 
 353     // We call this when we want to reattempt compiling something with the baseline JIT. Ideally
 354     // the baseline JIT would not add data to CodeBlock, but instead it would put its data into
 355     // a newly created JITCode, which could be thrown away if we bail on JIT compilation. Then we
 356     // would be able to get rid of this silly function.
 357     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=159061
 358     void resetJITData();
 359 #endif // ENABLE(JIT)
 360 
 361     void unlinkIncomingCalls();
 362 
 363 #if ENABLE(JIT)
<span class="line-modified"> 364     void linkIncomingCall(ExecState* callerFrame, CallLinkInfo*);</span>
<span class="line-modified"> 365     void linkIncomingPolymorphicCall(ExecState* callerFrame, PolymorphicCallNode*);</span>
 366 #endif // ENABLE(JIT)
 367 
<span class="line-modified"> 368     void linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo*);</span>
 369 
 370     const Instruction* outOfLineJumpTarget(const Instruction* pc);




 371     int outOfLineJumpOffset(const Instruction* pc);
 372     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 373     {
 374         return outOfLineJumpOffset(instruction.ptr());
 375     }
 376 
 377     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 378     {
 379         const auto* instructionsBegin = instructions().at(0).ptr();
 380         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 381         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 382         return returnAddress - instructionsBegin;
 383     }
 384 





 385     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 386 
 387     size_t predictedMachineCodeSize();
 388 
 389     unsigned instructionsSize() const { return instructions().size(); }
 390     unsigned bytecodeCost() const { return m_bytecodeCost; }
 391 
 392     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 393     CodeBlock* newReplacement();
 394 
 395     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 396     {
<span class="line-removed"> 397         ASSERT(heap()-&gt;isDeferred());</span>
 398         if (!code-&gt;isShared())
 399             heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());
 400 
 401         ConcurrentJSLocker locker(m_lock);
 402         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 403         m_jitCode = WTFMove(code);
 404     }
 405 
 406     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 407     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
 408     JITType jitType() const
 409     {
 410         JITCode* jitCode = m_jitCode.get();
 411         WTF::loadLoadFence();
 412         JITType result = JITCode::jitTypeFor(jitCode);
 413         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 414         return result;
 415     }
 416 
 417     bool hasBaselineJITProfiling() const
 418     {
 419         return jitType() == JITType::BaselineJIT;
 420     }
 421 
 422 #if ENABLE(JIT)
 423     CodeBlock* replacement();
 424 
 425     DFG::CapabilityLevel computeCapabilityLevel();
 426     DFG::CapabilityLevel capabilityLevel();
 427     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 428 


 429     bool hasOptimizedReplacement(JITType typeToReplace);
 430     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 431 #endif
 432 
 433     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 434 
 435     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 436 
 437     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 438 
 439     VM&amp; vm() const { return *m_vm; }
 440 
 441     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 442 
 443     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 444 
 445     void setScopeRegister(VirtualRegister scopeRegister)
 446     {
 447         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 448         m_scopeRegister = scopeRegister;
</pre>
<hr />
<pre>
 466 
 467     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 468     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 469 
 470     String nameForRegister(VirtualRegister);
 471 
 472     unsigned numberOfArgumentValueProfiles()
 473     {
 474         ASSERT(m_numParameters &gt;= 0);
 475         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm().canUseJIT());
 476         return m_argumentValueProfiles.size();
 477     }
 478 
 479     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 480     {
 481         ASSERT(vm().canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.
 482         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];
 483         return result;
 484     }
 485 
<span class="line-modified"> 486     ValueProfile&amp; valueProfileForBytecodeOffset(int bytecodeOffset);</span>
<span class="line-modified"> 487     SpeculatedType valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);</span>
 488 
 489     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 490     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 491     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 492     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 493     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 494 
<span class="line-modified"> 495     ArithProfile* arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset);</span>
<span class="line-modified"> 496     ArithProfile* arithProfileForPC(const Instruction*);</span>


 497 
<span class="line-modified"> 498     bool couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset);</span>
 499 
<span class="line-modified"> 500     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset);</span>
<span class="line-modified"> 501     ArrayProfile* getArrayProfile(unsigned bytecodeOffset);</span>
 502 
 503     // Exception handling support
 504 
 505     size_t numberOfExceptionHandlers() const { return m_rareData ? m_rareData-&gt;m_exceptionHandlers.size() : 0; }
 506     HandlerInfo&amp; exceptionHandler(int index) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_exceptionHandlers[index]; }
 507 
 508     bool hasExpressionInfo() { return m_unlinkedCode-&gt;hasExpressionInfo(); }
 509 
 510 #if ENABLE(DFG_JIT)
 511     Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; codeOrigins();
 512 
 513     // Having code origins implies that there has been some inlining.
 514     bool hasCodeOrigins()
 515     {
 516         return JITCode::isOptimizingJIT(jitType());
 517     }
 518 
 519     bool canGetCodeOrigin(CallSiteIndex index)
 520     {
 521         if (!hasCodeOrigins())
</pre>
<hr />
<pre>
 529     }
 530 
 531     CompressedLazyOperandValueProfileHolder&amp; lazyOperandValueProfiles(const ConcurrentJSLocker&amp;)
 532     {
 533         return m_lazyOperandValueProfiles;
 534     }
 535 #endif // ENABLE(DFG_JIT)
 536 
 537     // Constant Pool
 538 #if ENABLE(DFG_JIT)
 539     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers() + numberOfDFGIdentifiers(); }
 540     size_t numberOfDFGIdentifiers() const;
 541     const Identifier&amp; identifier(int index) const;
 542 #else
 543     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers(); }
 544     const Identifier&amp; identifier(int index) const { return m_unlinkedCode-&gt;identifier(index); }
 545 #endif
 546 
 547     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants() { return m_constantRegisters; }
 548     Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation() { return m_constantsSourceCodeRepresentation; }
<span class="line-modified"> 549     unsigned addConstant(JSValue v)</span>
 550     {
 551         unsigned result = m_constantRegisters.size();
 552         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 553         m_constantRegisters.last().set(*m_vm, this, v);
 554         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 555         return result;
 556     }
 557 
<span class="line-modified"> 558     unsigned addConstantLazily()</span>
 559     {
 560         unsigned result = m_constantRegisters.size();
 561         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 562         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 563         return result;
 564     }
 565 
 566     const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constantRegisters() { return m_constantRegisters; }
<span class="line-modified"> 567     WriteBarrier&lt;Unknown&gt;&amp; constantRegister(int index) { return m_constantRegisters[index - FirstConstantRegisterIndex]; }</span>
<span class="line-modified"> 568     static ALWAYS_INLINE bool isConstantRegisterIndex(int index) { return index &gt;= FirstConstantRegisterIndex; }</span>
<span class="line-modified"> 569     ALWAYS_INLINE JSValue getConstant(int index) const { return m_constantRegisters[index - FirstConstantRegisterIndex].get(); }</span>
<span class="line-removed"> 570     ALWAYS_INLINE SourceCodeRepresentation constantSourceCodeRepresentation(int index) const { return m_constantsSourceCodeRepresentation[index - FirstConstantRegisterIndex]; }</span>
 571 
 572     FunctionExecutable* functionDecl(int index) { return m_functionDecls[index].get(); }
 573     int numberOfFunctionDecls() { return m_functionDecls.size(); }
 574     FunctionExecutable* functionExpr(int index) { return m_functionExprs[index].get(); }
 575 
 576     const BitVector&amp; bitVector(size_t i) { return m_unlinkedCode-&gt;bitVector(i); }
 577 
 578     Heap* heap() const { return &amp;m_vm-&gt;heap; }
 579     JSGlobalObject* globalObject() { return m_globalObject.get(); }
 580 
 581     JSGlobalObject* globalObjectFor(CodeOrigin);
 582 
 583     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 584     {
 585         return m_unlinkedCode-&gt;livenessAnalysis(this);
 586     }
 587 
 588     void validate();
 589 
 590     // Jump Tables
 591 
 592     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }
<span class="line-removed"> 593     SimpleJumpTable&amp; addSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_switchJumpTables.append(SimpleJumpTable()); return m_rareData-&gt;m_switchJumpTables.last(); }</span>
 594     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 595     void clearSwitchJumpTables()
 596     {
 597         if (!m_rareData)
 598             return;
 599         m_rareData-&gt;m_switchJumpTables.clear();
 600     }
 601 #if ENABLE(DFG_JIT)
 602     void addSwitchJumpTableFromProfiledCodeBlock(SimpleJumpTable&amp; profiled)
 603     {
 604         createRareDataIfNecessary();
 605         m_rareData-&gt;m_switchJumpTables.append(profiled.cloneNonJITPart());
 606     }
 607 #endif
 608 
 609     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }
<span class="line-removed"> 610     StringJumpTable&amp; addStringSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_stringSwitchJumpTables.append(StringJumpTable()); return m_rareData-&gt;m_stringSwitchJumpTables.last(); }</span>
 611     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 612 
 613     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 614 
<span class="line-modified"> 615     enum ShrinkMode {</span>
 616         // Shrink prior to generating machine code that may point directly into vectors.
 617         EarlyShrink,
 618 
 619         // Shrink after generating machine code, and after possibly creating new vectors
 620         // and appending to others. At this time it is not safe to shrink certain vectors
 621         // because we would have generated machine code that references them directly.
<span class="line-modified"> 622         LateShrink</span>
 623     };
<span class="line-modified"> 624     void shrinkToFit(ShrinkMode);</span>
 625 
 626     // Functions for controlling when JITting kicks in, in a mixed mode
 627     // execution world.
 628 
 629     bool checkIfJITThresholdReached()
 630     {
 631         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 632     }
 633 
 634     void dontJITAnytimeSoon()
 635     {
 636         m_llintExecuteCounter.deferIndefinitely();
 637     }
 638 
 639     int32_t thresholdForJIT(int32_t threshold);
 640     void jitAfterWarmUp();
 641     void jitSoon();
 642 
 643     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 644     {
</pre>
<hr />
<pre>
 740     // frames that are still executing this CodeBlock. The value here
 741     // is tuned to strike a balance between the cost of OSR entry
 742     // (which is too high to warrant making every loop back edge to
 743     // trigger OSR immediately) and the cost of executing baseline
 744     // code (which is high enough that we don&#39;t necessarily want to
 745     // have a full warm-up). The intuition for calling this instead of
 746     // optimizeNextInvocation() is for the case of recursive functions
 747     // with loops. Consider that there may be N call frames of some
 748     // recursive function, for a reasonably large value of N. The top
 749     // one triggers optimization, and then returns, and then all of
 750     // the others return. We don&#39;t want optimization to be triggered on
 751     // each return, as that would be superfluous. It only makes sense
 752     // to trigger optimization if one of those functions becomes hot
 753     // in the baseline code.
 754     void optimizeSoon();
 755 
 756     void forceOptimizationSlowPathConcurrently();
 757 
 758     void setOptimizationThresholdBasedOnCompilationResult(CompilationResult);
 759 

 760     uint32_t osrExitCounter() const { return m_osrExitCounter; }
 761 
 762     void countOSRExit() { m_osrExitCounter++; }
 763 
 764     enum class OptimizeAction { None, ReoptimizeNow };
 765 #if ENABLE(DFG_JIT)
 766     OptimizeAction updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp;);
 767 #endif
 768 
 769     static ptrdiff_t offsetOfOSRExitCounter() { return OBJECT_OFFSETOF(CodeBlock, m_osrExitCounter); }
 770 
 771     uint32_t adjustedExitCountThreshold(uint32_t desiredThreshold);
 772     uint32_t exitCountThresholdForReoptimization();
 773     uint32_t exitCountThresholdForReoptimizationFromLoop();
 774     bool shouldReoptimizeNow();
 775     bool shouldReoptimizeFromLoopNow();
 776 
 777 #else // No JIT
 778     void optimizeAfterWarmUp() { }
 779     unsigned numberOfDFGCompiles() { return 0; }
 780 #endif
 781 
 782     bool shouldOptimizeNow();
 783     void updateAllValueProfilePredictions();
 784     void updateAllArrayPredictions();
 785     void updateAllPredictions();
 786 
 787     unsigned frameRegisterCount();
 788     int stackPointerOffset();
 789 
<span class="line-modified"> 790     bool hasOpDebugForLineAndColumn(unsigned line, unsigned column);</span>
 791 
 792     bool hasDebuggerRequests() const { return m_debuggerRequests; }
 793     void* debuggerRequestsAddress() { return &amp;m_debuggerRequests; }
 794 
 795     void addBreakpoint(unsigned numBreakpoints);
 796     void removeBreakpoint(unsigned numBreakpoints)
 797     {
 798         ASSERT(m_numBreakpoints &gt;= numBreakpoints);
 799         m_numBreakpoints -= numBreakpoints;
 800     }
 801 
 802     enum SteppingMode {
 803         SteppingModeDisabled,
 804         SteppingModeEnabled
 805     };
 806     void setSteppingMode(SteppingMode);
 807 
 808     void clearDebuggerRequests()
 809     {
 810         m_steppingMode = SteppingModeDisabled;
</pre>
<hr />
<pre>
 826     // locking. This is crucial since executing the inline cache is effectively
 827     // &quot;querying&quot; it.
 828     //
 829     // Another exception to the rules is that the GC can do whatever it wants
 830     // without holding any locks, because the GC is guaranteed to wait until any
 831     // concurrent compilation threads finish what they&#39;re doing.
 832     mutable ConcurrentJSLock m_lock;
 833 
 834     bool m_shouldAlwaysBeInlined; // Not a bitfield because the JIT wants to store to it.
 835 
 836 #if ENABLE(JIT)
 837     unsigned m_capabilityLevelState : 2; // DFG::CapabilityLevel
 838 #endif
 839 
 840     bool m_allTransitionsHaveBeenMarked : 1; // Initialized and used on every GC.
 841 
 842     bool m_didFailJITCompilation : 1;
 843     bool m_didFailFTLCompilation : 1;
 844     bool m_hasBeenCompiledWithFTL : 1;
 845 



 846     // Internal methods for use by validation code. It would be private if it wasn&#39;t
 847     // for the fact that we use it from anonymous namespaces.
 848     void beginValidationDidFail();
 849     NO_RETURN_DUE_TO_CRASH void endValidationDidFail();
 850 
 851     struct RareData {
<span class="line-modified"> 852         WTF_MAKE_FAST_ALLOCATED;</span>
 853     public:
 854         Vector&lt;HandlerInfo&gt; m_exceptionHandlers;
 855 
 856         // Jump Tables
 857         Vector&lt;SimpleJumpTable&gt; m_switchJumpTables;
 858         Vector&lt;StringJumpTable&gt; m_stringSwitchJumpTables;
 859 
<span class="line-modified"> 860         Vector&lt;std::unique_ptr&lt;ValueProfileAndOperandBuffer&gt;&gt; m_catchProfiles;</span>
 861 
 862         DirectEvalCodeCache m_directEvalCodeCache;
 863     };
 864 
 865     void clearExceptionHandlers()
 866     {
 867         if (m_rareData)
 868             m_rareData-&gt;m_exceptionHandlers.clear();
 869     }
 870 
 871     void appendExceptionHandler(const HandlerInfo&amp; handler)
 872     {
 873         createRareDataIfNecessary(); // We may be handling the exception of an inlined call frame.
 874         m_rareData-&gt;m_exceptionHandlers.append(handler);
 875     }
 876 
 877     DisposableCallSiteIndex newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite);
 878 
<span class="line-modified"> 879     void ensureCatchLivenessIsComputedForBytecodeOffset(InstructionStream::Offset bytecodeOffset);</span>
 880 
 881     bool hasTailCalls() const { return m_unlinkedCode-&gt;hasTailCalls(); }
 882 
 883     template&lt;typename Metadata&gt;
 884     Metadata&amp; metadata(OpcodeID opcodeID, unsigned metadataID)
 885     {
 886         ASSERT(m_metadata);
 887         return bitwise_cast&lt;Metadata*&gt;(m_metadata-&gt;get(opcodeID))[metadataID];
 888     }
 889 
 890     size_t metadataSizeInBytes()
 891     {
 892         return m_unlinkedCode-&gt;metadataSizeInBytes();
 893     }
 894 



 895 protected:
 896     void finalizeLLIntInlineCaches();
 897 #if ENABLE(JIT)
 898     void finalizeBaselineJITInlineCaches();
 899 #endif
 900 #if ENABLE(DFG_JIT)
 901     void tallyFrequentExitSites();
 902 #else
 903     void tallyFrequentExitSites() { }
 904 #endif
 905 
 906 private:
 907     friend class CodeBlockSet;
 908     friend class ExecutableToCodeBlockEdge;
 909 
 910     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 911 
 912     CodeBlock* specialOSREntryBlockOrNull();
 913 
<span class="line-modified"> 914     void noticeIncomingCall(ExecState* callerFrame);</span>
 915 
 916     double optimizationThresholdScalingFactor();
 917 
 918     void updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);
 919 
<span class="line-modified"> 920     void setConstantIdentifierSetRegisters(VM&amp;, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants);</span>
 921 
<span class="line-modified"> 922     void setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable);</span>
 923 
<span class="line-modified"> 924     void replaceConstant(int index, JSValue value)</span>
 925     {
<span class="line-modified"> 926         ASSERT(isConstantRegisterIndex(index) &amp;&amp; static_cast&lt;size_t&gt;(index - FirstConstantRegisterIndex) &lt; m_constantRegisters.size());</span>
<span class="line-modified"> 927         m_constantRegisters[index - FirstConstantRegisterIndex].set(*m_vm, this, value);</span>
 928     }
 929 
 930     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
 931     bool shouldJettisonDueToWeakReference(VM&amp;);
 932     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 933 
 934     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 935     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 936 
 937     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 938     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 939     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 940 
 941     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 942     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
<span class="line-modified"> 943     ValueProfile* tryGetValueProfileForBytecodeOffset(int bytecodeOffset);</span>
 944 
 945     Seconds timeSinceCreation()
 946     {
 947         return MonotonicTime::now() - m_creationTime;
 948     }
 949 
 950     void createRareDataIfNecessary()
 951     {
 952         if (!m_rareData) {
 953             auto rareData = makeUnique&lt;RareData&gt;();
 954             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 955             m_rareData = WTFMove(rareData);
 956         }
 957     }
 958 
 959     void insertBasicBlockBoundariesForControlFlowProfiler();
<span class="line-modified"> 960     void ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp;, InstructionStream::Offset);</span>
 961 
 962     int m_numCalleeLocals;
 963     int m_numVars;
 964     int m_numParameters;
 965     int m_numberOfArgumentsToSkip { 0 };
 966     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 967     union {
 968         unsigned m_debuggerRequests;
 969         struct {
 970             unsigned m_hasDebuggerStatement : 1;
 971             unsigned m_steppingMode : 1;
 972             unsigned m_numBreakpoints : 30;
 973         };
 974     };
 975     unsigned m_bytecodeCost { 0 };
 976     VirtualRegister m_scopeRegister;
 977     mutable CodeBlockHash m_hash;
 978 
 979     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
 980     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
</pre>
<hr />
<pre>
1007     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
1008 
1009     WriteBarrier&lt;CodeBlock&gt; m_alternative;
1010 
1011     BaselineExecutionCounter m_llintExecuteCounter;
1012 
1013     BaselineExecutionCounter m_jitExecuteCounter;
1014     uint32_t m_osrExitCounter;
1015 
1016     uint16_t m_optimizationDelayCounter;
1017     uint16_t m_reoptimizationRetryCounter;
1018 
1019     RefPtr&lt;MetadataTable&gt; m_metadata;
1020 
1021     MonotonicTime m_creationTime;
1022     double m_previousCounter { 0 };
1023 
1024     std::unique_ptr&lt;RareData&gt; m_rareData;
1025 };
1026 
<span class="line-removed">1027 inline Register&amp; ExecState::r(int index)</span>
<span class="line-removed">1028 {</span>
<span class="line-removed">1029     CodeBlock* codeBlock = this-&gt;codeBlock();</span>
<span class="line-removed">1030     if (codeBlock-&gt;isConstantRegisterIndex(index))</span>
<span class="line-removed">1031         return *reinterpret_cast&lt;Register*&gt;(&amp;codeBlock-&gt;constantRegister(index));</span>
<span class="line-removed">1032     return this[index];</span>
<span class="line-removed">1033 }</span>
<span class="line-removed">1034 </span>
<span class="line-removed">1035 inline Register&amp; ExecState::r(VirtualRegister reg)</span>
<span class="line-removed">1036 {</span>
<span class="line-removed">1037     return r(reg.offset());</span>
<span class="line-removed">1038 }</span>
<span class="line-removed">1039 </span>
<span class="line-removed">1040 inline Register&amp; ExecState::uncheckedR(int index)</span>
<span class="line-removed">1041 {</span>
<span class="line-removed">1042     RELEASE_ASSERT(index &lt; FirstConstantRegisterIndex);</span>
<span class="line-removed">1043     return this[index];</span>
<span class="line-removed">1044 }</span>
<span class="line-removed">1045 </span>
<span class="line-removed">1046 inline Register&amp; ExecState::uncheckedR(VirtualRegister reg)</span>
<span class="line-removed">1047 {</span>
<span class="line-removed">1048     return uncheckedR(reg.offset());</span>
<span class="line-removed">1049 }</span>
<span class="line-removed">1050 </span>
1051 template &lt;typename ExecutableType&gt;
1052 Exception* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)
1053 {
1054     if (hasJITCodeFor(kind)) {
1055         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1056             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1057         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1058             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1059         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1060             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1061         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1062             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1063         else
1064             RELEASE_ASSERT_NOT_REACHED();
1065         return nullptr;
1066     }
1067     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1068 }
1069 
1070 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
</pre>
</td>
<td>
<hr />
<pre>
  66 #include &quot;PutPropertySlot.h&quot;
  67 #include &quot;ValueProfile.h&quot;
  68 #include &quot;VirtualRegister.h&quot;
  69 #include &quot;Watchpoint.h&quot;
  70 #include &lt;wtf/Bag.h&gt;
  71 #include &lt;wtf/FastMalloc.h&gt;
  72 #include &lt;wtf/RefCountedArray.h&gt;
  73 #include &lt;wtf/RefPtr.h&gt;
  74 #include &lt;wtf/SegmentedVector.h&gt;
  75 #include &lt;wtf/Vector.h&gt;
  76 #include &lt;wtf/text/WTFString.h&gt;
  77 
  78 namespace JSC {
  79 
  80 #if ENABLE(DFG_JIT)
  81 namespace DFG {
  82 struct OSRExitState;
  83 } // namespace DFG
  84 #endif
  85 
<span class="line-added">  86 class UnaryArithProfile;</span>
<span class="line-added">  87 class BinaryArithProfile;</span>
  88 class BytecodeLivenessAnalysis;
  89 class CodeBlockSet;
  90 class ExecutableToCodeBlockEdge;
  91 class JSModuleEnvironment;
  92 class LLIntOffsetsExtractor;
  93 class LLIntPrototypeLoadAdaptiveStructureWatchpoint;
  94 class MetadataTable;
  95 class PCToCodeOriginMap;
  96 class RegisterAtOffsetList;
  97 class StructureStubInfo;
  98 
<span class="line-added">  99 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(CodeBlockRareData);</span>
<span class="line-added"> 100 </span>
 101 enum class AccessType : int8_t;
 102 

 103 struct OpCatch;
 104 
 105 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 106 
 107 class CodeBlock : public JSCell {
 108     typedef JSCell Base;
 109     friend class BytecodeLivenessAnalysis;
 110     friend class JIT;
 111     friend class LLIntOffsetsExtractor;
 112 
 113 public:
 114 
 115     enum CopyParsedBlockTag { CopyParsedBlock };
 116 
<span class="line-modified"> 117     static constexpr unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;</span>
<span class="line-modified"> 118     static constexpr bool needsDestruction = true;</span>
 119 
 120     template&lt;typename, SubspaceAccess&gt;
<span class="line-modified"> 121     static void subspaceFor(VM&amp;)</span>
<span class="line-added"> 122     {</span>
<span class="line-added"> 123         RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added"> 124     }</span>
<span class="line-added"> 125     // GC strongly assumes CodeBlock is not a PreciseAllocation for now.</span>
<span class="line-added"> 126     static constexpr uint8_t numberOfLowerTierCells = 0;</span>
 127 
 128     DECLARE_INFO;
 129 
 130 protected:
 131     CodeBlock(VM&amp;, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);
 132     CodeBlock(VM&amp;, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 133 
 134     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 135     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 136 
 137     void finishCreationCommon(VM&amp;);
 138 
 139     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 140 
 141 public:
 142     JS_EXPORT_PRIVATE ~CodeBlock();
 143 
 144     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 145 
 146     CString inferredName() const;
 147     CodeBlockHash hash() const;
 148     bool hasHash() const;
 149     bool isSafeToComputeHash() const;
 150     CString hashAsStringIfPossible() const;
 151     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 152     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
 153     void dumpAssumingJITType(PrintStream&amp;, JITType) const;
 154     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 155 
 156     MetadataTable* metadataTable() const { return m_metadata.get(); }
 157 
 158     int numParameters() const { return m_numParameters; }
 159     void setNumParameters(int newValue);
 160 
 161     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 162 
 163     int numCalleeLocals() const { return m_numCalleeLocals; }
 164 
 165     int numVars() const { return m_numVars; }
<span class="line-added"> 166     int numTmps() const { return m_unlinkedCode-&gt;hasCheckpoints() * maxNumCheckpointTmps; }</span>
 167 
 168     int* addressOfNumParameters() { return &amp;m_numParameters; }
 169     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 170 
 171     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 172     void setAlternative(VM&amp;, CodeBlock*);
 173 
 174     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 175     {
 176         Functor f(std::forward&lt;Functor&gt;(functor));
 177         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 178         codeBlocks.append(this);
 179 
 180         while (!codeBlocks.isEmpty()) {
 181             CodeBlock* currentCodeBlock = codeBlocks.takeLast();
 182             f(currentCodeBlock);
 183 
 184             if (CodeBlock* alternative = currentCodeBlock-&gt;alternative())
 185                 codeBlocks.append(alternative);
 186             if (CodeBlock* osrEntryBlock = currentCodeBlock-&gt;specialOSREntryBlockOrNull())
</pre>
<hr />
<pre>
 215     void dumpBytecode(PrintStream&amp;);
 216     void dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; = ICStatusMap());
 217     void dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; = ICStatusMap());
 218 
 219     void dumpExceptionHandlers(PrintStream&amp;);
 220     void printStructures(PrintStream&amp;, const Instruction*);
 221     void printStructure(PrintStream&amp;, const char* name, const Instruction*, int operand);
 222 
 223     void dumpMathICStats();
 224 
 225     bool isStrictMode() const { return m_unlinkedCode-&gt;isStrictMode(); }
 226     bool isConstructor() const { return m_unlinkedCode-&gt;isConstructor(); }
 227     ECMAMode ecmaMode() const { return isStrictMode() ? StrictMode : NotStrictMode; }
 228     CodeType codeType() const { return m_unlinkedCode-&gt;codeType(); }
 229 
 230     JSParserScriptMode scriptMode() const { return m_unlinkedCode-&gt;scriptMode(); }
 231 
 232     bool hasInstalledVMTrapBreakpoints() const;
 233     bool installVMTrapBreakpoints();
 234 
<span class="line-modified"> 235     inline bool isKnownNotImmediate(VirtualRegister reg)</span>
 236     {
<span class="line-modified"> 237         if (reg == thisRegister() &amp;&amp; !isStrictMode())</span>
 238             return true;
 239 
<span class="line-modified"> 240         if (reg.isConstant())</span>
<span class="line-modified"> 241             return getConstant(reg).isCell();</span>
 242 
 243         return false;
 244     }
 245 
<span class="line-modified"> 246     ALWAYS_INLINE bool isTemporaryRegister(VirtualRegister reg)</span>
 247     {
<span class="line-modified"> 248         return reg.offset() &gt;= m_numVars;</span>
 249     }
 250 
<span class="line-modified"> 251     HandlerInfo* handlerForBytecodeIndex(BytecodeIndex, RequiredHandler = RequiredHandler::AnyHandler);</span>
 252     HandlerInfo* handlerForIndex(unsigned, RequiredHandler = RequiredHandler::AnyHandler);
 253     void removeExceptionHandlerForCallSite(DisposableCallSiteIndex);
<span class="line-modified"> 254     unsigned lineNumberForBytecodeIndex(BytecodeIndex);</span>
<span class="line-modified"> 255     unsigned columnNumberForBytecodeIndex(BytecodeIndex);</span>
<span class="line-modified"> 256     void expressionRangeForBytecodeIndex(BytecodeIndex, int&amp; divot,</span>
 257         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 258 
<span class="line-modified"> 259     Optional&lt;BytecodeIndex&gt; bytecodeIndexFromCallSiteIndex(CallSiteIndex);</span>
 260 
<span class="line-added"> 261     // Because we might throw out baseline JIT code and all its baseline JIT data (m_jitData),</span>
<span class="line-added"> 262     // you need to be careful about the lifetime of when you use the return value of this function.</span>
<span class="line-added"> 263     // The return value may have raw pointers into this data structure that gets thrown away.</span>
<span class="line-added"> 264     // Specifically, you need to ensure that no GC can be finalized (typically that means no</span>
<span class="line-added"> 265     // allocations) between calling this and the last use of it.</span>
 266     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 267     void getICStatusMap(ICStatusMap&amp; result);
 268 
 269 #if ENABLE(JIT)
 270     struct JITData {
 271         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 272 
 273         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 274         Bag&lt;JITAddIC&gt; m_addICs;
 275         Bag&lt;JITMulIC&gt; m_mulICs;
 276         Bag&lt;JITNegIC&gt; m_negICs;
 277         Bag&lt;JITSubIC&gt; m_subICs;
 278         Bag&lt;ByValInfo&gt; m_byValInfos;
 279         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
 280         SentinelLinkedList&lt;CallLinkInfo, PackedRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;
 281         SentinelLinkedList&lt;PolymorphicCallNode, PackedRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;
<span class="line-modified"> 282         RefCountedArray&lt;RareCaseProfile&gt; m_rareCaseProfiles;</span>
 283         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 284         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 285         JITCodeMap m_jitCodeMap;
 286     };
 287 
 288     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 289     {
 290         if (LIKELY(m_jitData))
 291             return *m_jitData;
 292         return ensureJITDataSlow(locker);
 293     }
 294     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 295 
<span class="line-modified"> 296     JITAddIC* addJITAddIC(BinaryArithProfile*);</span>
<span class="line-modified"> 297     JITMulIC* addJITMulIC(BinaryArithProfile*);</span>
<span class="line-modified"> 298     JITNegIC* addJITNegIC(UnaryArithProfile*);</span>
<span class="line-modified"> 299     JITSubIC* addJITSubIC(BinaryArithProfile*);</span>
 300 
 301     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 302     JITAddIC* addMathIC(BinaryArithProfile* profile) { return addJITAddIC(profile); }</span>
 303 
 304     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 305     JITMulIC* addMathIC(BinaryArithProfile* profile) { return addJITMulIC(profile); }</span>
 306 
 307     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 308     JITNegIC* addMathIC(UnaryArithProfile* profile) { return addJITNegIC(profile); }</span>
 309 
 310     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 311     JITSubIC* addMathIC(BinaryArithProfile* profile) { return addJITSubIC(profile); }</span>
 312 
 313     StructureStubInfo* addStubInfo(AccessType);
 314 
 315     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 316     // stub info.
 317     StructureStubInfo* findStubInfo(CodeOrigin);
 318 
 319     ByValInfo* addByValInfo();
 320 
 321     CallLinkInfo* addCallLinkInfo();
 322 
 323     // This is a slow function call used primarily for compiling OSR exits in the case
 324     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 325     // looking for a CallLinkInfoMap to amortize the cost of calling this.
<span class="line-modified"> 326     CallLinkInfo* getCallLinkInfoForBytecodeIndex(BytecodeIndex);</span>
 327 
 328     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 329     {
 330         ConcurrentJSLocker locker(m_lock);
 331         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
 332     }
 333     const JITCodeMap&amp; jitCodeMap()
 334     {
 335         ConcurrentJSLocker locker(m_lock);
 336         return ensureJITData(locker).m_jitCodeMap;
 337     }
 338 
 339     void setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp;);
 340     Optional&lt;CodeOrigin&gt; findPC(void* pc);
 341 
 342     void setCalleeSaveRegisters(RegisterSet);
 343     void setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt;);
 344 
<span class="line-modified"> 345     void setRareCaseProfiles(RefCountedArray&lt;RareCaseProfile&gt;&amp;&amp;);</span>
<span class="line-modified"> 346     RareCaseProfile* rareCaseProfileForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex);</span>
<span class="line-modified"> 347     unsigned rareCaseProfileCountForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex);</span>
 348 
<span class="line-modified"> 349     bool likelyToTakeSlowCase(BytecodeIndex bytecodeIndex)</span>
 350     {
 351         if (!hasBaselineJITProfiling())
 352             return false;
 353         ConcurrentJSLocker locker(m_lock);
<span class="line-modified"> 354         unsigned value = rareCaseProfileCountForBytecodeIndex(locker, bytecodeIndex);</span>
 355         return value &gt;= Options::likelyToTakeSlowCaseMinimumCount();
 356     }
 357 
<span class="line-modified"> 358     bool couldTakeSlowCase(BytecodeIndex bytecodeIndex)</span>
 359     {
 360         if (!hasBaselineJITProfiling())
 361             return false;
 362         ConcurrentJSLocker locker(m_lock);
<span class="line-modified"> 363         unsigned value = rareCaseProfileCountForBytecodeIndex(locker, bytecodeIndex);</span>
 364         return value &gt;= Options::couldTakeSlowCaseMinimumCount();
 365     }
 366 
 367     // We call this when we want to reattempt compiling something with the baseline JIT. Ideally
 368     // the baseline JIT would not add data to CodeBlock, but instead it would put its data into
 369     // a newly created JITCode, which could be thrown away if we bail on JIT compilation. Then we
 370     // would be able to get rid of this silly function.
 371     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=159061
 372     void resetJITData();
 373 #endif // ENABLE(JIT)
 374 
 375     void unlinkIncomingCalls();
 376 
 377 #if ENABLE(JIT)
<span class="line-modified"> 378     void linkIncomingCall(CallFrame* callerFrame, CallLinkInfo*);</span>
<span class="line-modified"> 379     void linkIncomingPolymorphicCall(CallFrame* callerFrame, PolymorphicCallNode*);</span>
 380 #endif // ENABLE(JIT)
 381 
<span class="line-modified"> 382     void linkIncomingCall(CallFrame* callerFrame, LLIntCallLinkInfo*);</span>
 383 
 384     const Instruction* outOfLineJumpTarget(const Instruction* pc);
<span class="line-added"> 385     int outOfLineJumpOffset(InstructionStream::Offset offset)</span>
<span class="line-added"> 386     {</span>
<span class="line-added"> 387         return m_unlinkedCode-&gt;outOfLineJumpOffset(offset);</span>
<span class="line-added"> 388     }</span>
 389     int outOfLineJumpOffset(const Instruction* pc);
 390     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 391     {
 392         return outOfLineJumpOffset(instruction.ptr());
 393     }
 394 
 395     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 396     {
 397         const auto* instructionsBegin = instructions().at(0).ptr();
 398         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 399         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 400         return returnAddress - instructionsBegin;
 401     }
 402 
<span class="line-added"> 403     inline BytecodeIndex bytecodeIndex(const Instruction* returnAddress)</span>
<span class="line-added"> 404     {</span>
<span class="line-added"> 405         return BytecodeIndex(bytecodeOffset(returnAddress));</span>
<span class="line-added"> 406     }</span>
<span class="line-added"> 407 </span>
 408     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 409 
 410     size_t predictedMachineCodeSize();
 411 
 412     unsigned instructionsSize() const { return instructions().size(); }
 413     unsigned bytecodeCost() const { return m_bytecodeCost; }
 414 
 415     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 416     CodeBlock* newReplacement();
 417 
 418     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 419     {

 420         if (!code-&gt;isShared())
 421             heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());
 422 
 423         ConcurrentJSLocker locker(m_lock);
 424         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 425         m_jitCode = WTFMove(code);
 426     }
 427 
 428     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 429     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
 430     JITType jitType() const
 431     {
 432         JITCode* jitCode = m_jitCode.get();
 433         WTF::loadLoadFence();
 434         JITType result = JITCode::jitTypeFor(jitCode);
 435         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 436         return result;
 437     }
 438 
 439     bool hasBaselineJITProfiling() const
 440     {
 441         return jitType() == JITType::BaselineJIT;
 442     }
 443 
 444 #if ENABLE(JIT)
 445     CodeBlock* replacement();
 446 
 447     DFG::CapabilityLevel computeCapabilityLevel();
 448     DFG::CapabilityLevel capabilityLevel();
 449     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 450 
<span class="line-added"> 451     CodeBlock* optimizedReplacement(JITType typeToReplace);</span>
<span class="line-added"> 452     CodeBlock* optimizedReplacement(); // the typeToReplace is my JITType</span>
 453     bool hasOptimizedReplacement(JITType typeToReplace);
 454     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 455 #endif
 456 
 457     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 458 
 459     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 460 
 461     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 462 
 463     VM&amp; vm() const { return *m_vm; }
 464 
 465     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 466 
 467     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 468 
 469     void setScopeRegister(VirtualRegister scopeRegister)
 470     {
 471         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 472         m_scopeRegister = scopeRegister;
</pre>
<hr />
<pre>
 490 
 491     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 492     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 493 
 494     String nameForRegister(VirtualRegister);
 495 
 496     unsigned numberOfArgumentValueProfiles()
 497     {
 498         ASSERT(m_numParameters &gt;= 0);
 499         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm().canUseJIT());
 500         return m_argumentValueProfiles.size();
 501     }
 502 
 503     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 504     {
 505         ASSERT(vm().canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.
 506         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];
 507         return result;
 508     }
 509 
<span class="line-modified"> 510     ValueProfile&amp; valueProfileForBytecodeIndex(BytecodeIndex);</span>
<span class="line-modified"> 511     SpeculatedType valueProfilePredictionForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex);</span>
 512 
 513     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 514     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 515     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 516     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 517     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 518 
<span class="line-modified"> 519     BinaryArithProfile* binaryArithProfileForBytecodeIndex(BytecodeIndex);</span>
<span class="line-modified"> 520     UnaryArithProfile* unaryArithProfileForBytecodeIndex(BytecodeIndex);</span>
<span class="line-added"> 521     BinaryArithProfile* binaryArithProfileForPC(const Instruction*);</span>
<span class="line-added"> 522     UnaryArithProfile* unaryArithProfileForPC(const Instruction*);</span>
 523 
<span class="line-modified"> 524     bool couldTakeSpecialArithFastCase(BytecodeIndex bytecodeOffset);</span>
 525 
<span class="line-modified"> 526     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, BytecodeIndex);</span>
<span class="line-modified"> 527     ArrayProfile* getArrayProfile(BytecodeIndex);</span>
 528 
 529     // Exception handling support
 530 
 531     size_t numberOfExceptionHandlers() const { return m_rareData ? m_rareData-&gt;m_exceptionHandlers.size() : 0; }
 532     HandlerInfo&amp; exceptionHandler(int index) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_exceptionHandlers[index]; }
 533 
 534     bool hasExpressionInfo() { return m_unlinkedCode-&gt;hasExpressionInfo(); }
 535 
 536 #if ENABLE(DFG_JIT)
 537     Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; codeOrigins();
 538 
 539     // Having code origins implies that there has been some inlining.
 540     bool hasCodeOrigins()
 541     {
 542         return JITCode::isOptimizingJIT(jitType());
 543     }
 544 
 545     bool canGetCodeOrigin(CallSiteIndex index)
 546     {
 547         if (!hasCodeOrigins())
</pre>
<hr />
<pre>
 555     }
 556 
 557     CompressedLazyOperandValueProfileHolder&amp; lazyOperandValueProfiles(const ConcurrentJSLocker&amp;)
 558     {
 559         return m_lazyOperandValueProfiles;
 560     }
 561 #endif // ENABLE(DFG_JIT)
 562 
 563     // Constant Pool
 564 #if ENABLE(DFG_JIT)
 565     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers() + numberOfDFGIdentifiers(); }
 566     size_t numberOfDFGIdentifiers() const;
 567     const Identifier&amp; identifier(int index) const;
 568 #else
 569     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers(); }
 570     const Identifier&amp; identifier(int index) const { return m_unlinkedCode-&gt;identifier(index); }
 571 #endif
 572 
 573     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants() { return m_constantRegisters; }
 574     Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation() { return m_constantsSourceCodeRepresentation; }
<span class="line-modified"> 575     unsigned addConstant(const ConcurrentJSLocker&amp;, JSValue v)</span>
 576     {
 577         unsigned result = m_constantRegisters.size();
 578         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 579         m_constantRegisters.last().set(*m_vm, this, v);
 580         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 581         return result;
 582     }
 583 
<span class="line-modified"> 584     unsigned addConstantLazily(const ConcurrentJSLocker&amp;)</span>
 585     {
 586         unsigned result = m_constantRegisters.size();
 587         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 588         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 589         return result;
 590     }
 591 
 592     const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constantRegisters() { return m_constantRegisters; }
<span class="line-modified"> 593     WriteBarrier&lt;Unknown&gt;&amp; constantRegister(VirtualRegister reg) { return m_constantRegisters[reg.toConstantIndex()]; }</span>
<span class="line-modified"> 594     ALWAYS_INLINE JSValue getConstant(VirtualRegister reg) const { return m_constantRegisters[reg.toConstantIndex()].get(); }</span>
<span class="line-modified"> 595     ALWAYS_INLINE SourceCodeRepresentation constantSourceCodeRepresentation(VirtualRegister reg) const { return m_constantsSourceCodeRepresentation[reg.toConstantIndex()]; }</span>

 596 
 597     FunctionExecutable* functionDecl(int index) { return m_functionDecls[index].get(); }
 598     int numberOfFunctionDecls() { return m_functionDecls.size(); }
 599     FunctionExecutable* functionExpr(int index) { return m_functionExprs[index].get(); }
 600 
 601     const BitVector&amp; bitVector(size_t i) { return m_unlinkedCode-&gt;bitVector(i); }
 602 
 603     Heap* heap() const { return &amp;m_vm-&gt;heap; }
 604     JSGlobalObject* globalObject() { return m_globalObject.get(); }
 605 
 606     JSGlobalObject* globalObjectFor(CodeOrigin);
 607 
 608     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 609     {
 610         return m_unlinkedCode-&gt;livenessAnalysis(this);
 611     }
 612 
 613     void validate();
 614 
 615     // Jump Tables
 616 
 617     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }

 618     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 619     void clearSwitchJumpTables()
 620     {
 621         if (!m_rareData)
 622             return;
 623         m_rareData-&gt;m_switchJumpTables.clear();
 624     }
 625 #if ENABLE(DFG_JIT)
 626     void addSwitchJumpTableFromProfiledCodeBlock(SimpleJumpTable&amp; profiled)
 627     {
 628         createRareDataIfNecessary();
 629         m_rareData-&gt;m_switchJumpTables.append(profiled.cloneNonJITPart());
 630     }
 631 #endif
 632 
 633     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }

 634     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 635 
 636     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 637 
<span class="line-modified"> 638     enum class ShrinkMode {</span>
 639         // Shrink prior to generating machine code that may point directly into vectors.
 640         EarlyShrink,
 641 
 642         // Shrink after generating machine code, and after possibly creating new vectors
 643         // and appending to others. At this time it is not safe to shrink certain vectors
 644         // because we would have generated machine code that references them directly.
<span class="line-modified"> 645         LateShrink,</span>
 646     };
<span class="line-modified"> 647     void shrinkToFit(const ConcurrentJSLocker&amp;, ShrinkMode);</span>
 648 
 649     // Functions for controlling when JITting kicks in, in a mixed mode
 650     // execution world.
 651 
 652     bool checkIfJITThresholdReached()
 653     {
 654         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 655     }
 656 
 657     void dontJITAnytimeSoon()
 658     {
 659         m_llintExecuteCounter.deferIndefinitely();
 660     }
 661 
 662     int32_t thresholdForJIT(int32_t threshold);
 663     void jitAfterWarmUp();
 664     void jitSoon();
 665 
 666     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 667     {
</pre>
<hr />
<pre>
 763     // frames that are still executing this CodeBlock. The value here
 764     // is tuned to strike a balance between the cost of OSR entry
 765     // (which is too high to warrant making every loop back edge to
 766     // trigger OSR immediately) and the cost of executing baseline
 767     // code (which is high enough that we don&#39;t necessarily want to
 768     // have a full warm-up). The intuition for calling this instead of
 769     // optimizeNextInvocation() is for the case of recursive functions
 770     // with loops. Consider that there may be N call frames of some
 771     // recursive function, for a reasonably large value of N. The top
 772     // one triggers optimization, and then returns, and then all of
 773     // the others return. We don&#39;t want optimization to be triggered on
 774     // each return, as that would be superfluous. It only makes sense
 775     // to trigger optimization if one of those functions becomes hot
 776     // in the baseline code.
 777     void optimizeSoon();
 778 
 779     void forceOptimizationSlowPathConcurrently();
 780 
 781     void setOptimizationThresholdBasedOnCompilationResult(CompilationResult);
 782 
<span class="line-added"> 783     BytecodeIndex bytecodeIndexForExit(BytecodeIndex) const;</span>
 784     uint32_t osrExitCounter() const { return m_osrExitCounter; }
 785 
 786     void countOSRExit() { m_osrExitCounter++; }
 787 
 788     enum class OptimizeAction { None, ReoptimizeNow };
 789 #if ENABLE(DFG_JIT)
 790     OptimizeAction updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp;);
 791 #endif
 792 
 793     static ptrdiff_t offsetOfOSRExitCounter() { return OBJECT_OFFSETOF(CodeBlock, m_osrExitCounter); }
 794 
 795     uint32_t adjustedExitCountThreshold(uint32_t desiredThreshold);
 796     uint32_t exitCountThresholdForReoptimization();
 797     uint32_t exitCountThresholdForReoptimizationFromLoop();
 798     bool shouldReoptimizeNow();
 799     bool shouldReoptimizeFromLoopNow();
 800 
 801 #else // No JIT
 802     void optimizeAfterWarmUp() { }
 803     unsigned numberOfDFGCompiles() { return 0; }
 804 #endif
 805 
 806     bool shouldOptimizeNow();
 807     void updateAllValueProfilePredictions();
 808     void updateAllArrayPredictions();
 809     void updateAllPredictions();
 810 
 811     unsigned frameRegisterCount();
 812     int stackPointerOffset();
 813 
<span class="line-modified"> 814     bool hasOpDebugForLineAndColumn(unsigned line, Optional&lt;unsigned&gt; column);</span>
 815 
 816     bool hasDebuggerRequests() const { return m_debuggerRequests; }
 817     void* debuggerRequestsAddress() { return &amp;m_debuggerRequests; }
 818 
 819     void addBreakpoint(unsigned numBreakpoints);
 820     void removeBreakpoint(unsigned numBreakpoints)
 821     {
 822         ASSERT(m_numBreakpoints &gt;= numBreakpoints);
 823         m_numBreakpoints -= numBreakpoints;
 824     }
 825 
 826     enum SteppingMode {
 827         SteppingModeDisabled,
 828         SteppingModeEnabled
 829     };
 830     void setSteppingMode(SteppingMode);
 831 
 832     void clearDebuggerRequests()
 833     {
 834         m_steppingMode = SteppingModeDisabled;
</pre>
<hr />
<pre>
 850     // locking. This is crucial since executing the inline cache is effectively
 851     // &quot;querying&quot; it.
 852     //
 853     // Another exception to the rules is that the GC can do whatever it wants
 854     // without holding any locks, because the GC is guaranteed to wait until any
 855     // concurrent compilation threads finish what they&#39;re doing.
 856     mutable ConcurrentJSLock m_lock;
 857 
 858     bool m_shouldAlwaysBeInlined; // Not a bitfield because the JIT wants to store to it.
 859 
 860 #if ENABLE(JIT)
 861     unsigned m_capabilityLevelState : 2; // DFG::CapabilityLevel
 862 #endif
 863 
 864     bool m_allTransitionsHaveBeenMarked : 1; // Initialized and used on every GC.
 865 
 866     bool m_didFailJITCompilation : 1;
 867     bool m_didFailFTLCompilation : 1;
 868     bool m_hasBeenCompiledWithFTL : 1;
 869 
<span class="line-added"> 870     bool m_hasLinkedOSRExit : 1;</span>
<span class="line-added"> 871     bool m_isEligibleForLLIntDowngrade : 1;</span>
<span class="line-added"> 872 </span>
 873     // Internal methods for use by validation code. It would be private if it wasn&#39;t
 874     // for the fact that we use it from anonymous namespaces.
 875     void beginValidationDidFail();
 876     NO_RETURN_DUE_TO_CRASH void endValidationDidFail();
 877 
 878     struct RareData {
<span class="line-modified"> 879         WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(CodeBlockRareData);</span>
 880     public:
 881         Vector&lt;HandlerInfo&gt; m_exceptionHandlers;
 882 
 883         // Jump Tables
 884         Vector&lt;SimpleJumpTable&gt; m_switchJumpTables;
 885         Vector&lt;StringJumpTable&gt; m_stringSwitchJumpTables;
 886 
<span class="line-modified"> 887         Vector&lt;std::unique_ptr&lt;ValueProfileAndVirtualRegisterBuffer&gt;&gt; m_catchProfiles;</span>
 888 
 889         DirectEvalCodeCache m_directEvalCodeCache;
 890     };
 891 
 892     void clearExceptionHandlers()
 893     {
 894         if (m_rareData)
 895             m_rareData-&gt;m_exceptionHandlers.clear();
 896     }
 897 
 898     void appendExceptionHandler(const HandlerInfo&amp; handler)
 899     {
 900         createRareDataIfNecessary(); // We may be handling the exception of an inlined call frame.
 901         m_rareData-&gt;m_exceptionHandlers.append(handler);
 902     }
 903 
 904     DisposableCallSiteIndex newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite);
 905 
<span class="line-modified"> 906     void ensureCatchLivenessIsComputedForBytecodeIndex(BytecodeIndex);</span>
 907 
 908     bool hasTailCalls() const { return m_unlinkedCode-&gt;hasTailCalls(); }
 909 
 910     template&lt;typename Metadata&gt;
 911     Metadata&amp; metadata(OpcodeID opcodeID, unsigned metadataID)
 912     {
 913         ASSERT(m_metadata);
 914         return bitwise_cast&lt;Metadata*&gt;(m_metadata-&gt;get(opcodeID))[metadataID];
 915     }
 916 
 917     size_t metadataSizeInBytes()
 918     {
 919         return m_unlinkedCode-&gt;metadataSizeInBytes();
 920     }
 921 
<span class="line-added"> 922     MetadataTable* metadataTable() { return m_metadata.get(); }</span>
<span class="line-added"> 923     const void* instructionsRawPointer() { return m_instructionsRawPointer; }</span>
<span class="line-added"> 924 </span>
 925 protected:
 926     void finalizeLLIntInlineCaches();
 927 #if ENABLE(JIT)
 928     void finalizeBaselineJITInlineCaches();
 929 #endif
 930 #if ENABLE(DFG_JIT)
 931     void tallyFrequentExitSites();
 932 #else
 933     void tallyFrequentExitSites() { }
 934 #endif
 935 
 936 private:
 937     friend class CodeBlockSet;
 938     friend class ExecutableToCodeBlockEdge;
 939 
 940     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 941 
 942     CodeBlock* specialOSREntryBlockOrNull();
 943 
<span class="line-modified"> 944     void noticeIncomingCall(CallFrame* callerFrame);</span>
 945 
 946     double optimizationThresholdScalingFactor();
 947 
 948     void updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);
 949 
<span class="line-modified"> 950     void setConstantIdentifierSetRegisters(VM&amp;, const RefCountedArray&lt;ConstantIdentifierSetEntry&gt;&amp; constants);</span>
 951 
<span class="line-modified"> 952     void setConstantRegisters(const RefCountedArray&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const RefCountedArray&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable);</span>
 953 
<span class="line-modified"> 954     void replaceConstant(VirtualRegister reg, JSValue value)</span>
 955     {
<span class="line-modified"> 956         ASSERT(reg.isConstant() &amp;&amp; static_cast&lt;size_t&gt;(reg.toConstantIndex()) &lt; m_constantRegisters.size());</span>
<span class="line-modified"> 957         m_constantRegisters[reg.toConstantIndex()].set(*m_vm, this, value);</span>
 958     }
 959 
 960     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
 961     bool shouldJettisonDueToWeakReference(VM&amp;);
 962     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 963 
 964     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 965     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 966 
 967     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 968     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 969     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 970 
 971     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 972     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
<span class="line-modified"> 973     ValueProfile* tryGetValueProfileForBytecodeIndex(BytecodeIndex);</span>
 974 
 975     Seconds timeSinceCreation()
 976     {
 977         return MonotonicTime::now() - m_creationTime;
 978     }
 979 
 980     void createRareDataIfNecessary()
 981     {
 982         if (!m_rareData) {
 983             auto rareData = makeUnique&lt;RareData&gt;();
 984             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 985             m_rareData = WTFMove(rareData);
 986         }
 987     }
 988 
 989     void insertBasicBlockBoundariesForControlFlowProfiler();
<span class="line-modified"> 990     void ensureCatchLivenessIsComputedForBytecodeIndexSlow(const OpCatch&amp;, BytecodeIndex);</span>
 991 
 992     int m_numCalleeLocals;
 993     int m_numVars;
 994     int m_numParameters;
 995     int m_numberOfArgumentsToSkip { 0 };
 996     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 997     union {
 998         unsigned m_debuggerRequests;
 999         struct {
1000             unsigned m_hasDebuggerStatement : 1;
1001             unsigned m_steppingMode : 1;
1002             unsigned m_numBreakpoints : 30;
1003         };
1004     };
1005     unsigned m_bytecodeCost { 0 };
1006     VirtualRegister m_scopeRegister;
1007     mutable CodeBlockHash m_hash;
1008 
1009     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
1010     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
</pre>
<hr />
<pre>
1037     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
1038 
1039     WriteBarrier&lt;CodeBlock&gt; m_alternative;
1040 
1041     BaselineExecutionCounter m_llintExecuteCounter;
1042 
1043     BaselineExecutionCounter m_jitExecuteCounter;
1044     uint32_t m_osrExitCounter;
1045 
1046     uint16_t m_optimizationDelayCounter;
1047     uint16_t m_reoptimizationRetryCounter;
1048 
1049     RefPtr&lt;MetadataTable&gt; m_metadata;
1050 
1051     MonotonicTime m_creationTime;
1052     double m_previousCounter { 0 };
1053 
1054     std::unique_ptr&lt;RareData&gt; m_rareData;
1055 };
1056 
























1057 template &lt;typename ExecutableType&gt;
1058 Exception* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)
1059 {
1060     if (hasJITCodeFor(kind)) {
1061         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1062             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1063         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1064             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1065         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1066             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1067         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1068             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1069         else
1070             RELEASE_ASSERT_NOT_REACHED();
1071         return nullptr;
1072     }
1073     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1074 }
1075 
1076 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
</pre>
</td>
</tr>
</table>
<center><a href="CodeBlock.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlockHash.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>