diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/InlineAccess.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/InlineAccess.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/InlineAccess.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/InlineAccess.cpp
@@ -150,13 +150,13 @@
 
 
 template <typename Function>
 ALWAYS_INLINE static bool linkCodeInline(const char* name, CCallHelpers& jit, StructureStubInfo& stubInfo, const Function& function)
 {
-    if (jit.m_assembler.buffer().codeSize() <= stubInfo.patch.inlineSize()) {
+    if (jit.m_assembler.buffer().codeSize() <= stubInfo.inlineSize()) {
         bool needsBranchCompaction = true;
-        LinkBuffer linkBuffer(jit, stubInfo.patch.start, stubInfo.patch.inlineSize(), JITCompilationMustSucceed, needsBranchCompaction);
+        LinkBuffer linkBuffer(jit, stubInfo.start, stubInfo.inlineSize(), JITCompilationMustSucceed, needsBranchCompaction);
         ASSERT(linkBuffer.isValid());
         function(linkBuffer);
         FINALIZE_CODE(linkBuffer, NoPtrTag, "InlineAccessType: '%s'", name);
         return true;
     }
@@ -167,22 +167,25 @@
     // of randomness. It's helpful to flip this on when running tests or browsing
     // the web just to see how often it fails. You don't want an IC size that always fails.
     constexpr bool failIfCantInline = false;
     if (failIfCantInline) {
         dataLog("Failure for: ", name, "\n");
-        dataLog("real size: ", jit.m_assembler.buffer().codeSize(), " inline size:", stubInfo.patch.inlineSize(), "\n");
+        dataLog("real size: ", jit.m_assembler.buffer().codeSize(), " inline size:", stubInfo.inlineSize(), "\n");
         CRASH();
     }
 
     return false;
 }
 
 bool InlineAccess::generateSelfPropertyAccess(StructureStubInfo& stubInfo, Structure* structure, PropertyOffset offset)
 {
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     CCallHelpers jit;
 
-    GPRReg base = stubInfo.baseGPR();
+    GPRReg base = stubInfo.baseGPR;
     JSValueRegs value = stubInfo.valueRegs();
 
     auto branchToSlowPath = jit.patchableBranch32(
         MacroAssembler::NotEqual,
         MacroAssembler::Address(base, JSCell::structureIDOffset()),
@@ -197,23 +200,23 @@
 
     jit.loadValue(
         MacroAssembler::Address(storage, offsetRelativeToBase(offset)), value);
 
     bool linkedCodeInline = linkCodeInline("property access", jit, stubInfo, [&] (LinkBuffer& linkBuffer) {
-        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation());
+        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation);
     });
     return linkedCodeInline;
 }
 
 ALWAYS_INLINE static GPRReg getScratchRegister(StructureStubInfo& stubInfo)
 {
-    ScratchRegisterAllocator allocator(stubInfo.patch.usedRegisters);
-    allocator.lock(stubInfo.baseGPR());
-    allocator.lock(stubInfo.patch.valueGPR);
+    ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+    allocator.lock(stubInfo.baseGPR);
+    allocator.lock(stubInfo.valueGPR);
 #if USE(JSVALUE32_64)
-    allocator.lock(stubInfo.patch.baseTagGPR);
-    allocator.lock(stubInfo.patch.valueTagGPR);
+    allocator.lock(stubInfo.baseTagGPR);
+    allocator.lock(stubInfo.valueTagGPR);
 #endif
     GPRReg scratch = allocator.allocateScratchGPR();
     if (allocator.didReuseRegisters())
         return InvalidGPRReg;
     return scratch;
@@ -224,23 +227,29 @@
     return getScratchRegister(stubInfo) != InvalidGPRReg;
 }
 
 bool InlineAccess::canGenerateSelfPropertyReplace(StructureStubInfo& stubInfo, PropertyOffset offset)
 {
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     if (isInlineOffset(offset))
         return true;
 
     return hasFreeRegister(stubInfo);
 }
 
 bool InlineAccess::generateSelfPropertyReplace(StructureStubInfo& stubInfo, Structure* structure, PropertyOffset offset)
 {
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     ASSERT(canGenerateSelfPropertyReplace(stubInfo, offset));
 
     CCallHelpers jit;
 
-    GPRReg base = stubInfo.baseGPR();
+    GPRReg base = stubInfo.baseGPR;
     JSValueRegs value = stubInfo.valueRegs();
 
     auto branchToSlowPath = jit.patchableBranch32(
         MacroAssembler::NotEqual,
         MacroAssembler::Address(base, JSCell::structureIDOffset()),
@@ -257,32 +266,38 @@
 
     jit.storeValue(
         value, MacroAssembler::Address(storage, offsetRelativeToBase(offset)));
 
     bool linkedCodeInline = linkCodeInline("property replace", jit, stubInfo, [&] (LinkBuffer& linkBuffer) {
-        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation());
+        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation);
     });
     return linkedCodeInline;
 }
 
 bool InlineAccess::isCacheableArrayLength(StructureStubInfo& stubInfo, JSArray* array)
 {
     ASSERT(array->indexingType() & IsArray);
 
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     if (!hasFreeRegister(stubInfo))
         return false;
 
     return !hasAnyArrayStorage(array->indexingType()) && array->indexingType() != ArrayClass;
 }
 
 bool InlineAccess::generateArrayLength(StructureStubInfo& stubInfo, JSArray* array)
 {
     ASSERT(isCacheableArrayLength(stubInfo, array));
 
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     CCallHelpers jit;
 
-    GPRReg base = stubInfo.baseGPR();
+    GPRReg base = stubInfo.baseGPR;
     JSValueRegs value = stubInfo.valueRegs();
     GPRReg scratch = getScratchRegister(stubInfo);
 
     jit.load8(CCallHelpers::Address(base, JSCell::indexingTypeAndMiscOffset()), scratch);
     jit.and32(CCallHelpers::TrustedImm32(IndexingTypeMask), scratch);
@@ -291,27 +306,33 @@
     jit.loadPtr(CCallHelpers::Address(base, JSObject::butterflyOffset()), value.payloadGPR());
     jit.load32(CCallHelpers::Address(value.payloadGPR(), ArrayStorage::lengthOffset()), value.payloadGPR());
     jit.boxInt32(value.payloadGPR(), value);
 
     bool linkedCodeInline = linkCodeInline("array length", jit, stubInfo, [&] (LinkBuffer& linkBuffer) {
-        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation());
+        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation);
     });
     return linkedCodeInline;
 }
 
 bool InlineAccess::isCacheableStringLength(StructureStubInfo& stubInfo)
 {
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     return hasFreeRegister(stubInfo);
 }
 
 bool InlineAccess::generateStringLength(StructureStubInfo& stubInfo)
 {
     ASSERT(isCacheableStringLength(stubInfo));
 
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
     CCallHelpers jit;
 
-    GPRReg base = stubInfo.baseGPR();
+    GPRReg base = stubInfo.baseGPR;
     JSValueRegs value = stubInfo.valueRegs();
     GPRReg scratch = getScratchRegister(stubInfo);
 
     auto branchToSlowPath = jit.patchableBranch8(
         CCallHelpers::NotEqual,
@@ -328,31 +349,34 @@
 
     done.link(&jit);
     jit.boxInt32(value.payloadGPR(), value);
 
     bool linkedCodeInline = linkCodeInline("string length", jit, stubInfo, [&] (LinkBuffer& linkBuffer) {
-        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation());
+        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation);
     });
     return linkedCodeInline;
 }
 
 
 bool InlineAccess::generateSelfInAccess(StructureStubInfo& stubInfo, Structure* structure)
 {
     CCallHelpers jit;
 
-    GPRReg base = stubInfo.baseGPR();
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+
+    GPRReg base = stubInfo.baseGPR;
     JSValueRegs value = stubInfo.valueRegs();
 
     auto branchToSlowPath = jit.patchableBranch32(
         MacroAssembler::NotEqual,
         MacroAssembler::Address(base, JSCell::structureIDOffset()),
         MacroAssembler::TrustedImm32(bitwise_cast<uint32_t>(structure->id())));
     jit.boxBoolean(true, value);
 
     bool linkedCodeInline = linkCodeInline("in access", jit, stubInfo, [&] (LinkBuffer& linkBuffer) {
-        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation());
+        linkBuffer.link(branchToSlowPath, stubInfo.slowPathStartLocation);
     });
     return linkedCodeInline;
 }
 
 void InlineAccess::rewireStubAsJump(StructureStubInfo& stubInfo, CodeLocationLabel<JITStubRoutinePtrTag> target)
@@ -361,11 +385,11 @@
 
     auto jump = jit.jump();
 
     // We don't need a nop sled here because nobody should be jumping into the middle of an IC.
     bool needsBranchCompaction = false;
-    LinkBuffer linkBuffer(jit, stubInfo.patch.start, jit.m_assembler.buffer().codeSize(), JITCompilationMustSucceed, needsBranchCompaction);
+    LinkBuffer linkBuffer(jit, stubInfo.start, jit.m_assembler.buffer().codeSize(), JITCompilationMustSucceed, needsBranchCompaction);
     RELEASE_ASSERT(linkBuffer.isValid());
     linkBuffer.link(jump, target);
 
     FINALIZE_CODE(linkBuffer, NoPtrTag, "InlineAccess: linking constant jump");
 }
