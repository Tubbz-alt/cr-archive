<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/runtime/SymbolTable.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (C) 2007-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  *
  8  * 1.  Redistributions of source code must retain the above copyright
  9  *     notice, this list of conditions and the following disclaimer.
 10  * 2.  Redistributions in binary form must reproduce the above copyright
 11  *     notice, this list of conditions and the following disclaimer in the
 12  *     documentation and/or other materials provided with the distribution.
 13  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
 14  *     its contributors may be used to endorse or promote products derived
 15  *     from this software without specific prior written permission.
 16  *
 17  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
 18  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 19  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 20  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
 21  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 22  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 23  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 24  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 25  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 26  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 27  */
 28 
 29 #pragma once
 30 
 31 #include &quot;ConcurrentJSLock.h&quot;
 32 #include &quot;ConstantMode.h&quot;
 33 #include &quot;InferredValue.h&quot;
 34 #include &quot;JSObject.h&quot;
 35 #include &quot;ScopedArgumentsTable.h&quot;
 36 #include &quot;TypeLocation.h&quot;
 37 #include &quot;VarOffset.h&quot;
 38 #include &quot;Watchpoint.h&quot;
 39 #include &lt;memory&gt;
 40 #include &lt;wtf/HashTraits.h&gt;
 41 #include &lt;wtf/text/UniquedStringImpl.h&gt;
 42 
 43 namespace JSC {
 44 
 45 class SymbolTable;
 46 
<a name="2" id="anc2"></a><span class="line-added"> 47 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(SymbolTableEntryFatEntry);</span>
<span class="line-added"> 48 </span>
 49 static ALWAYS_INLINE int missingSymbolMarker() { return std::numeric_limits&lt;int&gt;::max(); }
 50 
 51 // The bit twiddling in this class assumes that every register index is a
 52 // reasonably small positive or negative number, and therefore has its high
 53 // four bits all set or all unset.
 54 
 55 // In addition to implementing semantics-mandated variable attributes and
 56 // implementation-mandated variable indexing, this class also implements
 57 // watchpoints to be used for JIT optimizations. Because watchpoints are
 58 // meant to be relatively rare, this class optimizes heavily for the case
 59 // that they are not being used. To that end, this class uses the thin-fat
 60 // idiom: either it is thin, in which case it contains an in-place encoded
 61 // word that consists of attributes, the index, and a bit saying that it is
 62 // thin; or it is fat, in which case it contains a pointer to a malloc&#39;d
 63 // data structure and a bit saying that it is fat. The malloc&#39;d data
 64 // structure will be malloced a second time upon copy, to preserve the
 65 // property that in-place edits to SymbolTableEntry do not manifest in any
 66 // copies. However, the malloc&#39;d FatEntry data structure contains a ref-
 67 // counted pointer to a shared WatchpointSet. Thus, in-place edits of the
 68 // WatchpointSet will manifest in all copies. Here&#39;s a picture:
 69 //
 70 // SymbolTableEntry --&gt; FatEntry --&gt; WatchpointSet
 71 //
 72 // If you make a copy of a SymbolTableEntry, you will have:
 73 //
 74 // original: SymbolTableEntry --&gt; FatEntry --&gt; WatchpointSet
 75 // copy:     SymbolTableEntry --&gt; FatEntry -----^
 76 
 77 struct SymbolTableEntry {
 78     friend class CachedSymbolTableEntry;
 79 
 80 private:
 81     static VarOffset varOffsetFromBits(intptr_t bits)
 82     {
 83         VarKind kind;
 84         intptr_t kindBits = bits &amp; KindBitsMask;
 85         if (kindBits &lt;= UnwatchableScopeKindBits)
 86             kind = VarKind::Scope;
 87         else if (kindBits == StackKindBits)
 88             kind = VarKind::Stack;
 89         else
 90             kind = VarKind::DirectArgument;
 91         return VarOffset::assemble(kind, static_cast&lt;int&gt;(bits &gt;&gt; FlagBits));
 92     }
 93 
 94     static ScopeOffset scopeOffsetFromBits(intptr_t bits)
 95     {
 96         ASSERT((bits &amp; KindBitsMask) &lt;= UnwatchableScopeKindBits);
 97         return ScopeOffset(static_cast&lt;int&gt;(bits &gt;&gt; FlagBits));
 98     }
 99 
100 public:
101 
102     // Use the SymbolTableEntry::Fast class, either via implicit cast or by calling
103     // getFast(), when you (1) only care about isNull(), getIndex(), and isReadOnly(),
104     // and (2) you are in a hot path where you need to minimize the number of times
105     // that you branch on isFat() when getting the bits().
106     class Fast {
107     public:
108         Fast()
109             : m_bits(SlimFlag)
110         {
111         }
112 
113         ALWAYS_INLINE Fast(const SymbolTableEntry&amp; entry)
114             : m_bits(entry.bits())
115         {
116         }
117 
118         bool isNull() const
119         {
120             return !(m_bits &amp; ~SlimFlag);
121         }
122 
123         VarOffset varOffset() const
124         {
125             return varOffsetFromBits(m_bits);
126         }
127 
128         // Asserts if the offset is anything but a scope offset. This structures the assertions
129         // in a way that may result in better code, even in release, than doing
130         // varOffset().scopeOffset().
131         ScopeOffset scopeOffset() const
132         {
133             return scopeOffsetFromBits(m_bits);
134         }
135 
136         bool isReadOnly() const
137         {
138             return m_bits &amp; ReadOnlyFlag;
139         }
140 
141         bool isDontEnum() const
142         {
143             return m_bits &amp; DontEnumFlag;
144         }
145 
146         unsigned getAttributes() const
147         {
148             unsigned attributes = 0;
149             if (isReadOnly())
150                 attributes |= PropertyAttribute::ReadOnly;
151             if (isDontEnum())
152                 attributes |= PropertyAttribute::DontEnum;
153             return attributes;
154         }
155 
156         bool isFat() const
157         {
158             return !(m_bits &amp; SlimFlag);
159         }
160 
161     private:
162         friend struct SymbolTableEntry;
163         intptr_t m_bits;
164     };
165 
166     SymbolTableEntry()
167         : m_bits(SlimFlag)
168     {
169     }
170 
171     SymbolTableEntry(VarOffset offset)
172         : m_bits(SlimFlag)
173     {
174         ASSERT(isValidVarOffset(offset));
175         pack(offset, true, false, false);
176     }
177 
178     SymbolTableEntry(VarOffset offset, unsigned attributes)
179         : m_bits(SlimFlag)
180     {
181         ASSERT(isValidVarOffset(offset));
182         pack(offset, true, attributes &amp; PropertyAttribute::ReadOnly, attributes &amp; PropertyAttribute::DontEnum);
183     }
184 
185     ~SymbolTableEntry()
186     {
187         freeFatEntry();
188     }
189 
190     SymbolTableEntry(const SymbolTableEntry&amp; other)
191         : m_bits(SlimFlag)
192     {
193         *this = other;
194     }
195 
196     SymbolTableEntry&amp; operator=(const SymbolTableEntry&amp; other)
197     {
198         if (UNLIKELY(other.isFat()))
199             return copySlow(other);
200         freeFatEntry();
201         m_bits = other.m_bits;
202         return *this;
203     }
204 
205     SymbolTableEntry(SymbolTableEntry&amp;&amp; other)
206         : m_bits(SlimFlag)
207     {
208         swap(other);
209     }
210 
211     SymbolTableEntry&amp; operator=(SymbolTableEntry&amp;&amp; other)
212     {
213         swap(other);
214         return *this;
215     }
216 
217     void swap(SymbolTableEntry&amp; other)
218     {
219         std::swap(m_bits, other.m_bits);
220     }
221 
222     bool isNull() const
223     {
224         return !(bits() &amp; ~SlimFlag);
225     }
226 
227     VarOffset varOffset() const
228     {
229         return varOffsetFromBits(bits());
230     }
231 
232     bool isWatchable() const
233     {
234         return (m_bits &amp; KindBitsMask) == ScopeKindBits &amp;&amp; VM::canUseJIT();
235     }
236 
237     // Asserts if the offset is anything but a scope offset. This structures the assertions
238     // in a way that may result in better code, even in release, than doing
239     // varOffset().scopeOffset().
240     ScopeOffset scopeOffset() const
241     {
242         return scopeOffsetFromBits(bits());
243     }
244 
245     ALWAYS_INLINE Fast getFast() const
246     {
247         return Fast(*this);
248     }
249 
250     ALWAYS_INLINE Fast getFast(bool&amp; wasFat) const
251     {
252         Fast result;
253         wasFat = isFat();
254         if (wasFat)
255             result.m_bits = fatEntry()-&gt;m_bits | SlimFlag;
256         else
257             result.m_bits = m_bits;
258         return result;
259     }
260 
261     unsigned getAttributes() const
262     {
263         return getFast().getAttributes();
264     }
265 
266     void setAttributes(unsigned attributes)
267     {
268         pack(varOffset(), isWatchable(), attributes &amp; PropertyAttribute::ReadOnly, attributes &amp; PropertyAttribute::DontEnum);
269     }
270 
271     bool isReadOnly() const
272     {
273         return bits() &amp; ReadOnlyFlag;
274     }
275 
276     ConstantMode constantMode() const
277     {
278         return modeForIsConstant(isReadOnly());
279     }
280 
281     bool isDontEnum() const
282     {
283         return bits() &amp; DontEnumFlag;
284     }
285 
286     void disableWatching(VM&amp; vm)
287     {
288         if (WatchpointSet* set = watchpointSet())
289             set-&gt;invalidate(vm, &quot;Disabling watching in symbol table&quot;);
290         if (varOffset().isScope())
291             pack(varOffset(), false, isReadOnly(), isDontEnum());
292     }
293 
294     void prepareToWatch();
295 
296     // This watchpoint set is initialized clear, and goes through the following state transitions:
297     //
298     // First write to this var, in any scope that has this symbol table: Clear-&gt;IsWatched.
299     //
300     // Second write to this var, in any scope that has this symbol table: IsWatched-&gt;IsInvalidated.
301     //
302     // We ensure that we touch the set (i.e. trigger its state transition) after we do the write. This
303     // means that if you&#39;re in the compiler thread, and you:
304     //
305     // 1) Observe that the set IsWatched and commit to adding your watchpoint.
306     // 2) Load a value from any scope that has this watchpoint set.
307     //
308     // Then you can be sure that that value is either going to be the correct value for that var forever,
309     // or the watchpoint set will invalidate and you&#39;ll get fired.
310     //
311     // It&#39;s possible to write a program that first creates multiple scopes with the same var, and then
312     // initializes that var in just one of them. This means that a compilation could constant-fold to one
313     // of the scopes that still has an undefined value for this variable. That&#39;s fine, because at that
314     // point any write to any of the instances of that variable would fire the watchpoint.
315     //
316     // Note that watchpointSet() returns nullptr if JIT is disabled.
317     WatchpointSet* watchpointSet()
318     {
319         if (!isFat())
320             return nullptr;
321         return fatEntry()-&gt;m_watchpoints.get();
322     }
323 
324 private:
325     static const intptr_t SlimFlag = 0x1;
326     static const intptr_t ReadOnlyFlag = 0x2;
327     static const intptr_t DontEnumFlag = 0x4;
328     static const intptr_t NotNullFlag = 0x8;
329     static const intptr_t KindBitsMask = 0x30;
330     static const intptr_t ScopeKindBits = 0x00;
331     static const intptr_t UnwatchableScopeKindBits = 0x10;
332     static const intptr_t StackKindBits = 0x20;
333     static const intptr_t DirectArgumentKindBits = 0x30;
334     static const intptr_t FlagBits = 6;
335 
336     class FatEntry {
<a name="3" id="anc3"></a><span class="line-modified">337         WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(SymbolTableEntryFatEntry);</span>
338     public:
339         FatEntry(intptr_t bits)
340             : m_bits(bits &amp; ~SlimFlag)
341         {
342         }
343 
344         intptr_t m_bits; // always has FatFlag set and exactly matches what the bits would have been if this wasn&#39;t fat.
345 
346         RefPtr&lt;WatchpointSet&gt; m_watchpoints;
347     };
348 
349     SymbolTableEntry&amp; copySlow(const SymbolTableEntry&amp;);
350 
351     bool isFat() const
352     {
353         return !(m_bits &amp; SlimFlag);
354     }
355 
356     const FatEntry* fatEntry() const
357     {
358         ASSERT(isFat());
359         return bitwise_cast&lt;const FatEntry*&gt;(m_bits);
360     }
361 
362     FatEntry* fatEntry()
363     {
364         ASSERT(isFat());
365         return bitwise_cast&lt;FatEntry*&gt;(m_bits);
366     }
367 
368     FatEntry* inflate()
369     {
370         if (LIKELY(isFat()))
371             return fatEntry();
372         return inflateSlow();
373     }
374 
375     FatEntry* inflateSlow();
376 
377     ALWAYS_INLINE intptr_t bits() const
378     {
379         if (isFat())
380             return fatEntry()-&gt;m_bits;
381         return m_bits;
382     }
383 
384     ALWAYS_INLINE intptr_t&amp; bits()
385     {
386         if (isFat())
387             return fatEntry()-&gt;m_bits;
388         return m_bits;
389     }
390 
391     void freeFatEntry()
392     {
393         if (LIKELY(!isFat()))
394             return;
395         freeFatEntrySlow();
396     }
397 
398     JS_EXPORT_PRIVATE void freeFatEntrySlow();
399 
400     void pack(VarOffset offset, bool isWatchable, bool readOnly, bool dontEnum)
401     {
402         ASSERT(!isFat());
403         intptr_t&amp; bitsRef = bits();
404         bitsRef =
405             (static_cast&lt;intptr_t&gt;(offset.rawOffset()) &lt;&lt; FlagBits) | NotNullFlag | SlimFlag;
406         if (readOnly)
407             bitsRef |= ReadOnlyFlag;
408         if (dontEnum)
409             bitsRef |= DontEnumFlag;
410         switch (offset.kind()) {
411         case VarKind::Scope:
412             if (isWatchable)
413                 bitsRef |= ScopeKindBits;
414             else
415                 bitsRef |= UnwatchableScopeKindBits;
416             break;
417         case VarKind::Stack:
418             bitsRef |= StackKindBits;
419             break;
420         case VarKind::DirectArgument:
421             bitsRef |= DirectArgumentKindBits;
422             break;
423         default:
424             RELEASE_ASSERT_NOT_REACHED();
425             break;
426         }
427     }
428 
429     static bool isValidVarOffset(VarOffset offset)
430     {
431         return ((static_cast&lt;intptr_t&gt;(offset.rawOffset()) &lt;&lt; FlagBits) &gt;&gt; FlagBits) == static_cast&lt;intptr_t&gt;(offset.rawOffset());
432     }
433 
434     intptr_t m_bits;
435 };
436 
437 struct SymbolTableIndexHashTraits : HashTraits&lt;SymbolTableEntry&gt; {
<a name="4" id="anc4"></a><span class="line-modified">438     static constexpr bool needsDestruction = true;</span>
439 };
440 
441 class SymbolTable final : public JSCell {
442     friend class CachedSymbolTable;
443 
444 public:
445     typedef JSCell Base;
<a name="5" id="anc5"></a><span class="line-modified">446     static constexpr unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;</span>
447 
448     typedef HashMap&lt;RefPtr&lt;UniquedStringImpl&gt;, SymbolTableEntry, IdentifierRepHash, HashTraits&lt;RefPtr&lt;UniquedStringImpl&gt;&gt;, SymbolTableIndexHashTraits&gt; Map;
449     typedef HashMap&lt;RefPtr&lt;UniquedStringImpl&gt;, GlobalVariableID, IdentifierRepHash&gt; UniqueIDMap;
450     typedef HashMap&lt;RefPtr&lt;UniquedStringImpl&gt;, RefPtr&lt;TypeSet&gt;, IdentifierRepHash&gt; UniqueTypeSetMap;
451     typedef HashMap&lt;VarOffset, RefPtr&lt;UniquedStringImpl&gt;&gt; OffsetToVariableMap;
452     typedef Vector&lt;SymbolTableEntry*&gt; LocalToEntryVec;
453 
454     template&lt;typename CellType, SubspaceAccess&gt;
455     static IsoSubspace* subspaceFor(VM&amp; vm)
456     {
457         return &amp;vm.symbolTableSpace;
458     }
459 
460     static SymbolTable* create(VM&amp; vm)
461     {
462         SymbolTable* symbolTable = new (NotNull, allocateCell&lt;SymbolTable&gt;(vm.heap)) SymbolTable(vm);
463         symbolTable-&gt;finishCreation(vm);
464         return symbolTable;
465     }
466 
<a name="6" id="anc6"></a><span class="line-modified">467     static constexpr bool needsDestruction = true;</span>
468     static void destroy(JSCell*);
469 
470     static Structure* createStructure(VM&amp; vm, JSGlobalObject* globalObject, JSValue prototype)
471     {
472         return Structure::create(vm, globalObject, prototype, TypeInfo(CellType, StructureFlags), info());
473     }
474 
475     // You must hold the lock until after you&#39;re done with the iterator.
476     Map::iterator find(const ConcurrentJSLocker&amp;, UniquedStringImpl* key)
477     {
478         return m_map.find(key);
479     }
480 
481     Map::iterator find(const GCSafeConcurrentJSLocker&amp;, UniquedStringImpl* key)
482     {
483         return m_map.find(key);
484     }
485 
486     SymbolTableEntry get(const ConcurrentJSLocker&amp;, UniquedStringImpl* key)
487     {
488         return m_map.get(key);
489     }
490 
491     SymbolTableEntry get(UniquedStringImpl* key)
492     {
493         ConcurrentJSLocker locker(m_lock);
494         return get(locker, key);
495     }
496 
497     SymbolTableEntry inlineGet(const ConcurrentJSLocker&amp;, UniquedStringImpl* key)
498     {
499         return m_map.inlineGet(key);
500     }
501 
502     SymbolTableEntry inlineGet(UniquedStringImpl* key)
503     {
504         ConcurrentJSLocker locker(m_lock);
505         return inlineGet(locker, key);
506     }
507 
508     Map::iterator begin(const ConcurrentJSLocker&amp;)
509     {
510         return m_map.begin();
511     }
512 
513     Map::iterator end(const ConcurrentJSLocker&amp;)
514     {
515         return m_map.end();
516     }
517 
518     Map::iterator end(const GCSafeConcurrentJSLocker&amp;)
519     {
520         return m_map.end();
521     }
522 
523     size_t size(const ConcurrentJSLocker&amp;) const
524     {
525         return m_map.size();
526     }
527 
528     size_t size() const
529     {
530         ConcurrentJSLocker locker(m_lock);
531         return size(locker);
532     }
533 
534     ScopeOffset maxScopeOffset() const
535     {
536         return m_maxScopeOffset;
537     }
538 
539     void didUseScopeOffset(ScopeOffset offset)
540     {
541         if (!m_maxScopeOffset || m_maxScopeOffset &lt; offset)
542             m_maxScopeOffset = offset;
543     }
544 
545     void didUseVarOffset(VarOffset offset)
546     {
547         if (offset.isScope())
548             didUseScopeOffset(offset.scopeOffset());
549     }
550 
551     unsigned scopeSize() const
552     {
553         ScopeOffset maxScopeOffset = this-&gt;maxScopeOffset();
554 
555         // Do some calculation that relies on invalid scope offset plus one being zero.
556         unsigned fastResult = maxScopeOffset.offsetUnchecked() + 1;
557 
558         // Assert that this works.
559         ASSERT(fastResult == (!maxScopeOffset ? 0 : maxScopeOffset.offset() + 1));
560 
561         return fastResult;
562     }
563 
564     ScopeOffset nextScopeOffset() const
565     {
566         return ScopeOffset(scopeSize());
567     }
568 
569     ScopeOffset takeNextScopeOffset(const ConcurrentJSLocker&amp;)
570     {
571         ScopeOffset result = nextScopeOffset();
572         m_maxScopeOffset = result;
573         return result;
574     }
575 
576     ScopeOffset takeNextScopeOffset()
577     {
578         ConcurrentJSLocker locker(m_lock);
579         return takeNextScopeOffset(locker);
580     }
581 
582     template&lt;typename Entry&gt;
583     void add(const ConcurrentJSLocker&amp;, UniquedStringImpl* key, Entry&amp;&amp; entry)
584     {
585         RELEASE_ASSERT(!m_localToEntry);
586         didUseVarOffset(entry.varOffset());
587         Map::AddResult result = m_map.add(key, std::forward&lt;Entry&gt;(entry));
588         ASSERT_UNUSED(result, result.isNewEntry);
589     }
590 
591     template&lt;typename Entry&gt;
592     void add(UniquedStringImpl* key, Entry&amp;&amp; entry)
593     {
594         ConcurrentJSLocker locker(m_lock);
595         add(locker, key, std::forward&lt;Entry&gt;(entry));
596     }
597 
598     template&lt;typename Entry&gt;
599     void set(const ConcurrentJSLocker&amp;, UniquedStringImpl* key, Entry&amp;&amp; entry)
600     {
601         RELEASE_ASSERT(!m_localToEntry);
602         didUseVarOffset(entry.varOffset());
603         m_map.set(key, std::forward&lt;Entry&gt;(entry));
604     }
605 
606     template&lt;typename Entry&gt;
607     void set(UniquedStringImpl* key, Entry&amp;&amp; entry)
608     {
609         ConcurrentJSLocker locker(m_lock);
610         set(locker, key, std::forward&lt;Entry&gt;(entry));
611     }
612 
613     bool contains(const ConcurrentJSLocker&amp;, UniquedStringImpl* key)
614     {
615         return m_map.contains(key);
616     }
617 
618     bool contains(UniquedStringImpl* key)
619     {
620         ConcurrentJSLocker locker(m_lock);
621         return contains(locker, key);
622     }
623 
624     // The principle behind ScopedArgumentsTable modifications is that we will create one and
625     // leave it unlocked - thereby allowing in-place changes - until someone asks for a pointer to
626     // the table. Then, we will lock it. Then both our future changes and their future changes
627     // will first have to make a copy. This discipline means that usually when we create a
628     // ScopedArguments object, we don&#39;t have to make a copy of the ScopedArgumentsTable - instead
629     // we just take a reference to one that we already have.
630 
631     uint32_t argumentsLength() const
632     {
633         if (!m_arguments)
634             return 0;
635         return m_arguments-&gt;length();
636     }
637 
638     void setArgumentsLength(VM&amp; vm, uint32_t length)
639     {
640         if (UNLIKELY(!m_arguments))
641             m_arguments.set(vm, this, ScopedArgumentsTable::create(vm, length));
642         else
643             m_arguments.set(vm, this, m_arguments-&gt;setLength(vm, length));
644     }
645 
646     ScopeOffset argumentOffset(uint32_t i) const
647     {
648         ASSERT_WITH_SECURITY_IMPLICATION(m_arguments);
649         return m_arguments-&gt;get(i);
650     }
651 
652     void setArgumentOffset(VM&amp; vm, uint32_t i, ScopeOffset offset)
653     {
654         ASSERT_WITH_SECURITY_IMPLICATION(m_arguments);
655         m_arguments.set(vm, this, m_arguments-&gt;set(vm, i, offset));
656     }
657 
658     ScopedArgumentsTable* arguments() const
659     {
660         if (!m_arguments)
661             return nullptr;
662         m_arguments-&gt;lock();
663         return m_arguments.get();
664     }
665 
666     const LocalToEntryVec&amp; localToEntry(const ConcurrentJSLocker&amp;);
667     SymbolTableEntry* entryFor(const ConcurrentJSLocker&amp;, ScopeOffset);
668 
669     GlobalVariableID uniqueIDForVariable(const ConcurrentJSLocker&amp;, UniquedStringImpl* key, VM&amp;);
670     GlobalVariableID uniqueIDForOffset(const ConcurrentJSLocker&amp;, VarOffset, VM&amp;);
671     RefPtr&lt;TypeSet&gt; globalTypeSetForOffset(const ConcurrentJSLocker&amp;, VarOffset, VM&amp;);
672     RefPtr&lt;TypeSet&gt; globalTypeSetForVariable(const ConcurrentJSLocker&amp;, UniquedStringImpl* key, VM&amp;);
673 
674     bool usesNonStrictEval() const { return m_usesNonStrictEval; }
675     void setUsesNonStrictEval(bool usesNonStrictEval) { m_usesNonStrictEval = usesNonStrictEval; }
676 
677     bool isNestedLexicalScope() const { return m_nestedLexicalScope; }
678     void markIsNestedLexicalScope() { ASSERT(scopeType() == LexicalScope); m_nestedLexicalScope = true; }
679 
680     enum ScopeType {
681         VarScope,
682         GlobalLexicalScope,
683         LexicalScope,
684         CatchScope,
685         FunctionNameScope
686     };
687     void setScopeType(ScopeType type) { m_scopeType = type; }
688     ScopeType scopeType() const { return static_cast&lt;ScopeType&gt;(m_scopeType); }
689 
690     SymbolTable* cloneScopePart(VM&amp;);
691 
692     void prepareForTypeProfiling(const ConcurrentJSLocker&amp;);
693 
694     CodeBlock* rareDataCodeBlock();
695     void setRareDataCodeBlock(CodeBlock*);
696 
697     InferredValue&lt;JSScope&gt;&amp; singleton() { return m_singleton; }
698 
699     void notifyCreation(VM&amp; vm, JSScope* scope, const char* reason)
700     {
701         m_singleton.notifyWrite(vm, this, scope, reason);
702     }
703 
704     static void visitChildren(JSCell*, SlotVisitor&amp;);
705 
706     DECLARE_EXPORT_INFO;
707 
708     void finalizeUnconditionally(VM&amp;);
709 
710 private:
711     JS_EXPORT_PRIVATE SymbolTable(VM&amp;);
712     ~SymbolTable();
713 
714     JS_EXPORT_PRIVATE void finishCreation(VM&amp;);
715 
716     Map m_map;
717     ScopeOffset m_maxScopeOffset;
718 public:
719     mutable ConcurrentJSLock m_lock;
720 private:
721     unsigned m_usesNonStrictEval : 1;
722     unsigned m_nestedLexicalScope : 1; // Non-function LexicalScope.
723     unsigned m_scopeType : 3; // ScopeType
724 
725     struct SymbolTableRareData {
726         WTF_MAKE_STRUCT_FAST_ALLOCATED;
727         UniqueIDMap m_uniqueIDMap;
728         OffsetToVariableMap m_offsetToVariableMap;
729         UniqueTypeSetMap m_uniqueTypeSetMap;
730         WriteBarrier&lt;CodeBlock&gt; m_codeBlock;
731     };
732     std::unique_ptr&lt;SymbolTableRareData&gt; m_rareData;
733 
734     WriteBarrier&lt;ScopedArgumentsTable&gt; m_arguments;
735     InferredValue&lt;JSScope&gt; m_singleton;
736 
737     std::unique_ptr&lt;LocalToEntryVec&gt; m_localToEntry;
738 };
739 
740 } // namespace JSC
<a name="7" id="anc7"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="7" type="hidden" />
</body>
</html>