diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm
@@ -21,51 +21,40 @@
 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 # THE POSSIBILITY OF SUCH DAMAGE.
 
 
 # Utilities.
-
-macro nextInstruction()
-    loadb [PB, PC, 1], t0
-    leap _g_opcodeMap, t1
-    jmp [t1, t0, PtrSize], BytecodePtrTag
-end
-
-macro nextInstructionWide16()
-    loadh 1[PB, PC, 1], t0
-    leap _g_opcodeMapWide16, t1
-    jmp [t1, t0, PtrSize], BytecodePtrTag
+macro storePC()
+    storei PC, LLIntReturnPC[cfr]
 end
 
-macro nextInstructionWide32()
-    loadi 1[PB, PC, 1], t0
-    leap _g_opcodeMapWide32, t1
-    jmp [t1, t0, PtrSize], BytecodePtrTag
+macro loadPC()
+    loadi LLIntReturnPC[cfr], PC
 end
 
 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
-    loadb constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
+    loadb constexpr %opcodeStruct%_%fieldName%_index + OpcodeIDNarrowSize[PB, PC, 1], dst
 end
 
 macro getOperandNarrow(opcodeStruct, fieldName, dst)
-    loadbsq constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
+    loadbsq constexpr %opcodeStruct%_%fieldName%_index + OpcodeIDNarrowSize[PB, PC, 1], dst
 end
 
 macro getuOperandWide16(opcodeStruct, fieldName, dst)
-    loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst
+    loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + OpcodeIDWide16Size[PB, PC, 1], dst
 end
 
 macro getOperandWide16(opcodeStruct, fieldName, dst)
-    loadhsq constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst
+    loadhsq constexpr %opcodeStruct%_%fieldName%_index * 2 + OpcodeIDWide16Size[PB, PC, 1], dst
 end
 
 macro getuOperandWide32(opcodeStruct, fieldName, dst)
-    loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
+    loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + OpcodeIDWide32Size[PB, PC, 1], dst
 end
 
 macro getOperandWide32(opcodeStruct, fieldName, dst)
-    loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
+    loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + OpcodeIDWide32Size[PB, PC, 1], dst
 end
 
 macro makeReturn(get, dispatch, fn)
     fn(macro (value)
         move value, t2
@@ -88,12 +77,13 @@
 
 macro valueProfile(opcodeStruct, metadata, value)
     storeq value, %opcodeStruct%::Metadata::m_profile.m_buckets[metadata]
 end
 
+# After calling, calling bytecode is claiming input registers are not used.
 macro dispatchAfterCall(size, opcodeStruct, dispatch)
-    loadi ArgumentCount + TagOffset[cfr], PC
+    loadPC()
     loadp CodeBlock[cfr], PB
     loadp CodeBlock::m_instructionsRawPointer[PB], PB
     get(size, opcodeStruct, m_dst, t1)
     storeq r0, [cfr, t1, 8]
     metadata(size, opcodeStruct, t2, t1)
@@ -214,11 +204,11 @@
 .stackHeightOK:
     move t3, sp
     move (constexpr ProtoCallFrame::numberOfRegisters), t3
 
 .copyHeaderLoop:
-    # Copy the CodeBlock/Callee/ArgumentCount/|this| from protoCallFrame into the callee frame.
+    # Copy the CodeBlock/Callee/ArgumentCountIncludingThis/|this| from protoCallFrame into the callee frame.
     subi 1, t3
     loadq [protoCallFrame, t3, 8], extraTempReg
     storeq extraTempReg, CodeBlock[sp, t3, 8]
     btinz t3, .copyHeaderLoop
 
@@ -253,11 +243,11 @@
     end
     storep cfr, VM::topEntryFrame[vm]
 
     checkStackPointerAlignment(extraTempReg, 0xbad0dc02)
 
-    makeCall(entry, t3, t4)
+    makeCall(entry, protoCallFrame, t3, t4)
 
     # We may have just made a call into a JS function, so we can't rely on sp
     # for anything but the fact that our own locals (ie the VMEntryRecord) are
     # not below it. It also still has to be aligned, though.
     checkStackPointerAlignment(t2, 0xbad0dc03)
@@ -295,41 +285,43 @@
     functionEpilogue()
     ret
 end
 
 
-macro makeJavaScriptCall(entry, temp, unused)
+# a0, a2, t3, t4
+macro makeJavaScriptCall(entry, protoCallFrame, temp1, temp2)
     addp 16, sp
     if C_LOOP or C_LOOP_WIN
         cloopCallJSFunction entry
     else
         call entry, JSEntryPtrTag
     end
     subp 16, sp
 end
 
-macro makeHostFunctionCall(entry, temp, unused)
-    move entry, temp
+# a0, a2, t3, t4
+macro makeHostFunctionCall(entry, protoCallFrame, temp1, temp2)
+    move entry, temp1
     storep cfr, [sp]
-    move sp, a0
+    loadp ProtoCallFrame::globalObject[protoCallFrame], a0
+    move sp, a1
     if C_LOOP or C_LOOP_WIN
         storep lr, 8[sp]
-        cloopCallNative temp
+        cloopCallNative temp1
     elsif X86_64_WIN
         # We need to allocate 32 bytes on the stack for the shadow space.
         subp 32, sp
-        call temp, JSEntryPtrTag
+        call temp1, JSEntryPtrTag
         addp 32, sp
     else
-        call temp, JSEntryPtrTag
+        call temp1, JSEntryPtrTag
     end
 end
 
 op(handleUncaughtException, macro ()
     loadp Callee[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    convertCalleeToVM(t3)
     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
     storep 0, VM::callFrameForCatch[t3]
 
     loadp VM::topEntryFrame[t3], cfr
     vmEntryRecord(cfr, t2)
@@ -383,44 +375,44 @@
     move PC, a1
     cCall4(_llint_trace_value)
     restoreStateAfterCCall()
 end
 
-# Call a slow path for call call opcodes.
+# Call a slow path for call opcodes.
 macro callCallSlowPath(slowPath, action)
-    storei PC, ArgumentCount + TagOffset[cfr]
+    storePC()
     prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(slowPath)
     action(r0, r1)
 end
 
 macro callTrapHandler(throwHandler)
-    storei PC, ArgumentCount + TagOffset[cfr]
+    storePC()
     prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(_llint_slow_path_handle_traps)
     btpnz r0, throwHandler
-    loadi ArgumentCount + TagOffset[cfr], PC
+    loadi LLIntReturnPC[cfr], PC
 end
 
 macro checkSwitchToJITForLoop()
     checkSwitchToJIT(
         1,
         macro()
-            storei PC, ArgumentCount + TagOffset[cfr]
+            storePC()
             prepareStateForCCall()
             move cfr, a0
             move PC, a1
             cCall2(_llint_loop_osr)
             btpz r0, .recover
             move r1, sp
             jmp r0, JSEntryPtrTag
         .recover:
-            loadi ArgumentCount + TagOffset[cfr], PC
+            loadPC()
         end)
 end
 
 macro cage(basePtr, mask, ptr, scratch)
     if GIGACAGE_ENABLED and not (C_LOOP or C_LOOP_WIN)
@@ -438,11 +430,11 @@
         move ptr, scratch2
     else
         const source = ptr
     end
     if GIGACAGE_ENABLED
-        cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, source, scratch)
+        cage(_g_gigacageConfig + Gigacage::Config::basePtrs + GigacagePrimitiveBasePtrOffset, constexpr Gigacage::primitiveGigacageMask, source, scratch)
         if ARM64E
             const numberOfPACBits = constexpr MacroAssembler::numberOfPACBits
             bfiq scratch2, 0, 64 - numberOfPACBits, ptr
         end
     end
@@ -451,65 +443,84 @@
     end
 end
 
 macro loadCagedJSValue(source, dest, scratchOrLength)
     loadp source, dest
-    cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, dest, scratchOrLength)
+    if GIGACAGE_ENABLED
+        cage(_g_gigacageConfig + Gigacage::Config::basePtrs + GigacageJSValueBasePtrOffset, constexpr Gigacage::jsValueGigacageMask, dest, scratchOrLength)
+    end
 end
 
 macro loadVariable(get, fieldName, valueReg)
     get(fieldName, valueReg)
     loadq [cfr, valueReg, 8], valueReg
 end
 
+# Index and value must be different registers. Index may be clobbered.
+macro loadConstant(size, index, value)
+    macro loadNarrow()
+        loadp CodeBlock[cfr], value
+        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
+        loadq -(FirstConstantRegisterIndexNarrow * 8)[value, index, 8], value
+    end
+
+    macro loadWide16()
+        loadp CodeBlock[cfr], value
+        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
+        loadq -(FirstConstantRegisterIndexWide16 * 8)[value, index, 8], value
+    end
+
+    macro loadWide32()
+        loadp CodeBlock[cfr], value
+        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
+        subp FirstConstantRegisterIndexWide32, index
+        loadq [value, index, 8], value
+    end
+
+    size(loadNarrow, loadWide16, loadWide32, macro (load) load() end)
+end
+
 # Index and value must be different registers. Index may be clobbered.
 macro loadConstantOrVariable(size, index, value)
     macro loadNarrow()
         bpgteq index, FirstConstantRegisterIndexNarrow, .constant
         loadq [cfr, index, 8], value
         jmp .done
     .constant:
-        loadp CodeBlock[cfr], value
-        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
-        loadq -(FirstConstantRegisterIndexNarrow * 8)[value, index, 8], value
+        loadConstant(size, index, value)
     .done:
     end
 
     macro loadWide16()
         bpgteq index, FirstConstantRegisterIndexWide16, .constant
         loadq [cfr, index, 8], value
         jmp .done
     .constant:
-        loadp CodeBlock[cfr], value
-        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
-        loadq -(FirstConstantRegisterIndexWide16 * 8)[value, index, 8], value
+        loadConstant(size, index, value)
     .done:
     end
 
     macro loadWide32()
         bpgteq index, FirstConstantRegisterIndexWide32, .constant
         loadq [cfr, index, 8], value
         jmp .done
     .constant:
-        loadp CodeBlock[cfr], value
-        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
-        subp FirstConstantRegisterIndexWide32, index
-        loadq [value, index, 8], value
+        loadConstant(size, index, value)
     .done:
     end
 
     size(loadNarrow, loadWide16, loadWide32, macro (load) load() end)
 end
 
 macro loadConstantOrVariableInt32(size, index, value, slow)
     loadConstantOrVariable(size, index, value)
-    bqb value, tagTypeNumber, slow
+    bqb value, numberTag, slow
 end
 
 macro loadConstantOrVariableCell(size, index, value, slow)
     loadConstantOrVariable(size, index, value)
-    btqnz value, tagMask, slow
+    btqnz value, notCellMask, slow
 end
 
 macro writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)
     skipIfIsRememberedOrInEden(
         cell,
@@ -521,10 +532,17 @@
             pop PC, PB
             reloadAfterSlowPath()
         end)
 end
 
+macro writeBarrierOnCellAndValueWithReload(cell, value, reloadAfterSlowPath)
+    btqnz value, notCellMask, .writeBarrierDone
+    btqz value, .writeBarrierDone
+    writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)
+.writeBarrierDone:
+end
+
 macro writeBarrierOnOperandWithReload(size, get, cellFieldName, reloadAfterSlowPath)
     get(cellFieldName, t1)
     loadConstantOrVariableCell(size, t1, t2, .writeBarrierDone)
     writeBarrierOnCellWithReload(t2, reloadAfterSlowPath)
 .writeBarrierDone:
@@ -588,11 +606,11 @@
 
 # Entrypoints into the interpreter.
 
 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
 macro functionArityCheck(doneLabel, slowPath)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     biaeq t0, CodeBlock::m_numParameters[t1], doneLabel
     prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(slowPath)   # This slowPath has the protocol: r0 = 0 => no error, r0 != 0 => error
@@ -607,11 +625,11 @@
     jmp _llint_throw_from_slow_path_trampoline
 
 .noError:
     move r1, t1 # r1 contains slotsToAdd.
     btiz t1, .continue
-    loadi PayloadOffset + ArgumentCount[cfr], t2
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t2
     addi CallFrameHeaderSlots, t2
 
     // Check if there are some unaligned slots we can use
     move t1, t3
     andi StackAlignmentSlots - 1, t3
@@ -667,25 +685,17 @@
     loadp CodeBlock::m_instructionsRawPointer[t1], PB
     move 0, PC
     jmp doneLabel
 end
 
-macro branchIfException(label)
-    loadp Callee[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
-    btpz VM::m_exception[t3], .noException
-    jmp label
-.noException:
-end
-
 # Instruction implementations
+
 _llint_op_enter:
     traceExecution()
     checkStackPointerAlignment(t2, 0xdead00e1)
-    loadp CodeBlock[cfr], t3                // t3<CodeBlock> = cfr.CodeBlock
-    loadi CodeBlock::m_numVars[t3], t2      // t2<size_t> = t3<CodeBlock>.m_numVars
+    loadp CodeBlock[cfr], t2                // t2<CodeBlock> = cfr.CodeBlock
+    loadi CodeBlock::m_numVars[t2], t2      // t2<size_t> = t2<CodeBlock>.m_numVars
     subq CalleeSaveSpaceAsVirtualRegisters, t2
     move cfr, t1
     subq CalleeSaveSpaceAsVirtualRegisters * 8, t1
     btiz t2, .opEnterDone
     move ValueUndefined, t0
@@ -694,37 +704,30 @@
 .opEnterLoop:
     storeq t0, [t1, t2, 8]
     addq 1, t2
     btqnz t2, .opEnterLoop
 .opEnterDone:
-    writeBarrierOnCellWithReload(t3, macro ()
-        loadp CodeBlock[cfr], t3 # Reload CodeBlock
-    end)
-    loadp CodeBlock::m_vm[t3], t1
-    btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps
-.afterHandlingTraps:
+    callSlowPath(_slow_path_enter)
     dispatchOp(narrow, op_enter)
-.handleTraps:
-    callTrapHandler(_llint_throw_from_slow_path_trampoline)
-    jmp .afterHandlingTraps
+
 
 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
     get(m_index, t2)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     bilteq t0, t2, .opGetArgumentOutOfBounds
     loadq ThisArgumentOffset[cfr, t2, 8], t0
     return(t0)
 
 .opGetArgumentOutOfBounds:
     return(ValueUndefined)
 end)
 
 
 llintOpWithReturn(op_argument_count, OpArgumentCount, macro (size, get, dispatch, return)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     subi 1, t0
-    orq TagTypeNumber, t0
+    orq TagNumber, t0
     return(t0)
 end)
 
 
 llintOpWithReturn(op_get_scope, OpGetScope, macro (size, get, dispatch, return)
@@ -735,11 +738,11 @@
 
 
 llintOpWithMetadata(op_to_this, OpToThis, macro (size, get, dispatch, metadata, return)
     get(m_srcDst, t0)
     loadq [cfr, t0, 8], t0
-    btqnz t0, tagMask, .opToThisSlow
+    btqnz t0, notCellMask, .opToThisSlow
     bbneq JSCell::m_type[t0], FinalObjectType, .opToThisSlow
     loadi JSCell::m_structureID[t0], t1
     metadata(t2, t3)
     loadi OpToThis::Metadata::m_cachedStructureID[t2], t2
     bineq t1, t2, .opToThisSlow
@@ -802,22 +805,22 @@
 
 macro equalNullComparisonOp(opcodeName, opcodeStruct, fn)
     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
         get(m_operand, t0)
         loadq [cfr, t0, 8], t0
-        btqnz t0, tagMask, .immediate
+        btqnz t0, notCellMask, .immediate
         btbnz JSCell::m_flags[t0], MasqueradesAsUndefined, .masqueradesAsUndefined
         move 0, t0
         jmp .done
     .masqueradesAsUndefined:
         loadStructureWithScratch(t0, t2, t1, t3)
         loadp CodeBlock[cfr], t0
         loadp CodeBlock::m_globalObject[t0], t0
         cpeq Structure::m_globalObject[t2], t0, t0
         jmp .done
     .immediate:
-        andq ~TagBitUndefined, t0
+        andq ~TagUndefined, t0
         cqeq t0, ValueNull, t0
     .done:
         fn(t0)
         return(t0)
     end)
@@ -832,11 +835,11 @@
 
 
 llintOpWithReturn(op_is_undefined_or_null, OpIsUndefinedOrNull, macro (size, get, dispatch, return)
     get(m_operand, t1)
     loadConstantOrVariable(size, t1, t0)
-    andq ~TagBitUndefined, t0
+    andq ~TagUndefined, t0
     cqeq t0, ValueNull, t0
     orq ValueFalse, t0
     return(t0)
 end)
 
@@ -847,16 +850,16 @@
         get(m_lhs, t2)
         loadConstantOrVariable(size, t0, t1)
         loadConstantOrVariable(size, t2, t0)
         move t0, t2
         orq t1, t2
-        btqz t2, tagMask, .slow
-        bqaeq t0, tagTypeNumber, .leftOK
-        btqnz t0, tagTypeNumber, .slow
+        btqz t2, notCellMask, .slow
+        bqaeq t0, numberTag, .leftOK
+        btqnz t0, numberTag, .slow
     .leftOK:
-        bqaeq t1, tagTypeNumber, .rightOK
-        btqnz t1, tagTypeNumber, .slow
+        bqaeq t1, numberTag, .rightOK
+        btqnz t1, numberTag, .slow
     .rightOK:
         equalityOperation(t0, t1, t0)
         orq ValueFalse, t0
         return(t0)
 
@@ -881,16 +884,16 @@
         get(m_rhs, t3)
         loadConstantOrVariable(size, t2, t0)
         loadConstantOrVariable(size, t3, t1)
         move t0, t2
         orq t1, t2
-        btqz t2, tagMask, .slow
-        bqaeq t0, tagTypeNumber, .leftOK
-        btqnz t0, tagTypeNumber, .slow
+        btqz t2, notCellMask, .slow
+        bqaeq t0, numberTag, .leftOK
+        btqnz t0, numberTag, .slow
     .leftOK:
-        bqaeq t1, tagTypeNumber, .rightOK
-        btqnz t1, tagTypeNumber, .slow
+        bqaeq t1, numberTag, .rightOK
+        btqnz t1, numberTag, .slow
     .rightOK:
         equalityOperation(t0, t1, .jumpTarget)
         dispatch()
 
     .jumpTarget:
@@ -908,44 +911,64 @@
 
 
 strictEqualityJumpOp(jnstricteq, OpJnstricteq,
     macro (left, right, target) bqneq left, right, target end)
 
+macro preOp(opcodeName, opcodeStruct, integerOperation)
+    llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
+        macro updateArithProfile(type)
+            orh type, %opcodeStruct%::Metadata::m_arithProfile + UnaryArithProfile::m_bits[t1]
+        end
 
-macro preOp(opcodeName, opcodeStruct, arithmeticOperation)
-    llintOp(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch)
         get(m_srcDst, t0)
-        loadq [cfr, t0, 8], t1
-        bqb t1, tagTypeNumber, .slow
-        arithmeticOperation(t1, .slow)
-        orq tagTypeNumber, t1
-        storeq t1, [cfr, t0, 8]
+        loadq [cfr, t0, 8], t3
+        metadata(t1, t2)
+        # Metadata in t1, srcDst in t3
+        bqb t3, numberTag, .slow
+        integerOperation(t3, .slow)
+        orq numberTag, t3
+        storeq t3, [cfr, t0, 8]
+        updateArithProfile(ArithProfileInt)
         dispatch()
+
     .slow:
         callSlowPath(_slow_path_%opcodeName%)
         dispatch()
     end)
 end
 
 llintOpWithProfile(op_to_number, OpToNumber, macro (size, get, dispatch, return)
     get(m_operand, t0)
     loadConstantOrVariable(size, t0, t2)
-    bqaeq t2, tagTypeNumber, .opToNumberIsImmediate
-    btqz t2, tagTypeNumber, .opToNumberSlow
+    bqaeq t2, numberTag, .opToNumberIsImmediate
+    btqz t2, numberTag, .opToNumberSlow
 .opToNumberIsImmediate:
     return(t2)
 
 .opToNumberSlow:
     callSlowPath(_slow_path_to_number)
     dispatch()
 end)
 
+llintOpWithProfile(op_to_numeric, OpToNumeric, macro (size, get, dispatch, return)
+    get(m_operand, t0)
+    loadConstantOrVariable(size, t0, t2)
+    bqaeq t2, numberTag, .opToNumericIsImmediate
+    btqz t2, numberTag, .opToNumericSlow
+.opToNumericIsImmediate:
+    return(t2)
+
+.opToNumericSlow:
+    callSlowPath(_slow_path_to_numeric)
+    dispatch()
+end)
+
 
 llintOpWithReturn(op_to_string, OpToString, macro (size, get, dispatch, return)
     get(m_operand, t1)
     loadConstantOrVariable(size, t1, t0)
-    btqnz t0, tagMask, .opToStringSlow
+    btqnz t0, notCellMask, .opToStringSlow
     bbneq JSCell::m_type[t0], StringType, .opToStringSlow
 .opToStringIsString:
     return(t0)
 
 .opToStringSlow:
@@ -955,37 +978,39 @@
 
 
 llintOpWithProfile(op_to_object, OpToObject, macro (size, get, dispatch, return)
     get(m_operand, t0)
     loadConstantOrVariable(size, t0, t2)
-    btqnz t2, tagMask, .opToObjectSlow
+    btqnz t2, notCellMask, .opToObjectSlow
     bbb JSCell::m_type[t2], ObjectType, .opToObjectSlow
     return(t2)
 
 .opToObjectSlow:
     callSlowPath(_slow_path_to_object)
     dispatch()
 end)
 
 
 llintOpWithMetadata(op_negate, OpNegate, macro (size, get, dispatch, metadata, return)
+
+    macro updateArithProfile(type)
+        orh type, OpNegate::Metadata::m_arithProfile + UnaryArithProfile::m_bits[t1]
+    end
+
     get(m_operand, t0)
     loadConstantOrVariable(size, t0, t3)
     metadata(t1, t2)
-    loadi OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t1], t2
-    bqb t3, tagTypeNumber, .opNegateNotInt
+    bqb t3, numberTag, .opNegateNotInt
     btiz t3, 0x7fffffff, .opNegateSlow
     negi t3
-    orq tagTypeNumber, t3
-    ori ArithProfileInt, t2
-    storei t2, OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t1]
+    orq numberTag, t3
+    updateArithProfile(ArithProfileInt)
     return(t3)
 .opNegateNotInt:
-    btqz t3, tagTypeNumber, .opNegateSlow
+    btqz t3, numberTag, .opNegateSlow
     xorq 0x8000000000000000, t3
-    ori ArithProfileNumber, t2
-    storei t2, OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t1]
+    updateArithProfile(ArithProfileNumber)
     return(t3)
 
 .opNegateSlow:
     callSlowPath(_slow_path_negate)
     dispatch()
@@ -995,58 +1020,58 @@
 macro binaryOpCustomStore(opcodeName, opcodeStruct, integerOperationAndStore, doubleOperation)
     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
         metadata(t5, t0)
 
         macro profile(type)
-            ori type, %opcodeStruct%::Metadata::m_arithProfile + ArithProfile::m_bits[t5]
+            orh type, %opcodeStruct%::Metadata::m_arithProfile + BinaryArithProfile::m_bits[t5]
         end
 
         get(m_rhs, t0)
         get(m_lhs, t2)
         loadConstantOrVariable(size, t0, t1)
         loadConstantOrVariable(size, t2, t0)
-        bqb t0, tagTypeNumber, .op1NotInt
-        bqb t1, tagTypeNumber, .op2NotInt
+        bqb t0, numberTag, .op1NotInt
+        bqb t1, numberTag, .op2NotInt
         get(m_dst, t2)
         integerOperationAndStore(t1, t0, .slow, t2)
 
         profile(ArithProfileIntInt)
         dispatch()
 
     .op1NotInt:
         # First operand is definitely not an int, the second operand could be anything.
-        btqz t0, tagTypeNumber, .slow
-        bqaeq t1, tagTypeNumber, .op1NotIntOp2Int
-        btqz t1, tagTypeNumber, .slow
-        addq tagTypeNumber, t1
+        btqz t0, numberTag, .slow
+        bqaeq t1, numberTag, .op1NotIntOp2Int
+        btqz t1, numberTag, .slow
+        addq numberTag, t1
         fq2d t1, ft1
         profile(ArithProfileNumberNumber)
         jmp .op1NotIntReady
     .op1NotIntOp2Int:
         profile(ArithProfileNumberInt)
-        ci2d t1, ft1
+        ci2ds t1, ft1
     .op1NotIntReady:
         get(m_dst, t2)
-        addq tagTypeNumber, t0
+        addq numberTag, t0
         fq2d t0, ft0
         doubleOperation(ft1, ft0)
         fd2q ft0, t0
-        subq tagTypeNumber, t0
+        subq numberTag, t0
         storeq t0, [cfr, t2, 8]
         dispatch()
 
     .op2NotInt:
         # First operand is definitely an int, the second is definitely not.
         get(m_dst, t2)
-        btqz t1, tagTypeNumber, .slow
+        btqz t1, numberTag, .slow
         profile(ArithProfileIntNumber)
-        ci2d t0, ft0
-        addq tagTypeNumber, t1
+        ci2ds t0, ft0
+        addq numberTag, t1
         fq2d t1, ft1
         doubleOperation(ft1, ft0)
         fd2q ft0, t0
-        subq tagTypeNumber, t0
+        subq numberTag, t0
         storeq t0, [cfr, t2, 8]
         dispatch()
 
     .slow:
         callSlowPath(_slow_path_%opcodeName%)
@@ -1068,11 +1093,11 @@
             move left, t3
             move right, t0
             cdqi
             idivi t3
             btinz t1, slow
-            orq tagTypeNumber, t0
+            orq numberTag, t0
             storeq t0, [cfr, index, 8]
         end,
         macro (left, right) divd left, right end)
 else
     slowPathOp(div)
@@ -1086,21 +1111,21 @@
         bmulio left, t3, slow
         btinz t3, .done
         bilt left, 0, slow
         bilt right, 0, slow
     .done:
-        orq tagTypeNumber, t3
+        orq numberTag, t3
         storeq t3, [cfr, index, 8]
     end,
     macro (left, right) muld left, right end)
 
 
 macro binaryOp(opcodeName, opcodeStruct, integerOperation, doubleOperation)
     binaryOpCustomStore(opcodeName, opcodeStruct,
         macro (left, right, slow, index)
             integerOperation(left, right, slow)
-            orq tagTypeNumber, right
+            orq numberTag, right
             storeq right, [cfr, index, 8]
         end,
         doubleOperation)
 end
 
@@ -1129,14 +1154,14 @@
     opKind(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
         get(m_rhs, t0)
         get(m_lhs, t2)
         loadConstantOrVariable(size, t0, t1)
         loadConstantOrVariable(size, t2, t0)
-        bqb t0, tagTypeNumber, .slow
-        bqb t1, tagTypeNumber, .slow
+        bqb t0, numberTag, .slow
+        bqb t1, numberTag, .slow
         operation(t1, t0)
-        orq tagTypeNumber, t0
+        orq numberTag, t0
         return(t0)
 
     .slow:
         callSlowPath(_slow_path_%opcodeName%)
         dispatch()
@@ -1153,11 +1178,11 @@
 
 bitOpProfiled(lshift, OpLshift,
     macro (left, right) lshifti left, right end)
 
 
-bitOp(rshift, OpRshift,
+bitOpProfiled(rshift, OpRshift,
     macro (left, right) rshifti left, right end)
 
 
 bitOp(urshift, OpUrshift,
     macro (left, right) urshifti left, right end)
@@ -1173,11 +1198,11 @@
 
 llintOpWithProfile(op_bitnot, OpBitnot, macro (size, get, dispatch, return)
     get(m_operand, t0)
     loadConstantOrVariableInt32(size, t0, t3, .opBitNotSlow)
     noti t3
-    orq tagTypeNumber, t3
+    orq numberTag, t3
     return(t3)
 .opBitNotSlow:
     callSlowPath(_slow_path_bitnot)
     dispatch()
 end)
@@ -1216,11 +1241,11 @@
 
 
 llintOpWithReturn(op_is_undefined, OpIsUndefined, macro (size, get, dispatch, return)
     get(m_operand, t1)
     loadConstantOrVariable(size, t1, t0)
-    btqz t0, tagMask, .opIsUndefinedCell
+    btqz t0, notCellMask, .opIsUndefinedCell
     cqeq t0, ValueUndefined, t3
     orq ValueFalse, t3
     return(t3)
 .opIsUndefinedCell:
     btbnz JSCell::m_flags[t0], MasqueradesAsUndefined, .masqueradesAsUndefined
@@ -1247,21 +1272,21 @@
 
 
 llintOpWithReturn(op_is_number, OpIsNumber, macro (size, get, dispatch, return)
     get(m_operand, t1)
     loadConstantOrVariable(size, t1, t0)
-    tqnz t0, tagTypeNumber, t1
+    tqnz t0, numberTag, t1
     orq ValueFalse, t1
     return(t1)
 end)
 
 
 llintOpWithReturn(op_is_cell_with_type, OpIsCellWithType, macro (size, get, dispatch, return)
     getu(size, OpIsCellWithType, m_type, t0)
     get(m_operand, t1)
     loadConstantOrVariable(size, t1, t3)
-    btqnz t3, tagMask, .notCellCase
+    btqnz t3, notCellMask, .notCellCase
     cbeq JSCell::m_type[t3], t0, t1
     orq ValueFalse, t1
     return(t1)
 .notCellCase:
     return(ValueFalse)
@@ -1269,11 +1294,11 @@
 
 
 llintOpWithReturn(op_is_object, OpIsObject, macro (size, get, dispatch, return)
     get(m_operand, t1)
     loadConstantOrVariable(size, t1, t0)
-    btqnz t0, tagMask, .opIsObjectNotCell
+    btqnz t0, notCellMask, .opIsObjectNotCell
     cbaeq JSCell::m_type[t0], ObjectType, t1
     orq ValueFalse, t1
     return(t1)
 .opIsObjectNotCell:
     return(ValueFalse)
@@ -1321,11 +1346,10 @@
 .opGetByIdDirectSlow:
     callSlowPath(_llint_slow_path_get_by_id_direct)
     dispatch()
 end)
 
-
 llintOpWithMetadata(op_get_by_id, OpGetById, macro (size, get, dispatch, metadata, return)
     metadata(t2, t1)
     loadb OpGetById::Metadata::m_modeMetadata.mode[t2], t1
     get(m_base, t0)
     loadConstantOrVariableCell(size, t0, t3, .opGetByIdSlow)
@@ -1358,11 +1382,11 @@
     btiz t0, IsArray, .opGetByIdSlow
     btiz t0, IndexingShapeMask, .opGetByIdSlow
     loadCagedJSValue(JSObject::m_butterfly[t3], t0, t1)
     loadi -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], t0
     bilt t0, 0, .opGetByIdSlow
-    orq tagTypeNumber, t0
+    orq numberTag, t0
     valueProfile(OpGetById, t2, t0)
     return(t0)
 
 .opGetByIdUnset:
     loadi JSCell::m_structureID[t3], t1
@@ -1372,10 +1396,17 @@
     return(ValueUndefined)
 
 .opGetByIdSlow:
     callSlowPath(_llint_slow_path_get_by_id)
     dispatch()
+
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_get_by_id, size)
+    metadata(t2, t3)
+    valueProfile(OpGetById, t2, r0)
+    return(r0)
+
 end)
 
 
 llintOpWithMetadata(op_put_by_id, OpPutById, macro (size, get, dispatch, metadata, return)
     get(m_base, t3)
@@ -1396,35 +1427,43 @@
     # If we have a chain, we need to check it. t0 is the base. We may clobber t1 to use it as
     # scratch.
     loadp OpPutById::Metadata::m_structureChain[t5], t3
     btpz t3, .opPutByIdTransitionDirect
 
-    structureIDToStructureWithScratch(t2, t1, t3)
+    loadp CodeBlock[cfr], t1
+    loadp CodeBlock::m_vm[t1], t1
+    loadp VM::heap + Heap::m_structureIDTable + StructureIDTable::m_table[t1], t1
+
+    macro structureIDToStructureWithScratchAndTable(structureIDThenStructure, table, scratch)
+        move structureIDThenStructure, scratch
+        rshifti NumberOfStructureIDEntropyBits, scratch
+        loadp [table, scratch, PtrSize], scratch
+        lshiftp StructureEntropyBitsShift, structureIDThenStructure
+        xorp scratch, structureIDThenStructure
+    end
 
-    # reload the StructureChain since we used t3 as a scratch above
-    loadp OpPutById::Metadata::m_structureChain[t5], t3
+    structureIDToStructureWithScratchAndTable(t2, t1, t0)
 
     loadp StructureChain::m_vector[t3], t3
     assert(macro (ok) btpnz t3, ok end)
 
     loadq Structure::m_prototype[t2], t2
     bqeq t2, ValueNull, .opPutByIdTransitionChainDone
 .opPutByIdTransitionChainLoop:
-    # At this point, t2 contains a prototye, and [t3] contains the Structure* that we want that
-    # prototype to have. We don't want to have to load the Structure* for t2. Instead, we load
-    # the Structure* from [t3], and then we compare its id to the id in the header of t2.
-    loadp [t3], t1
     loadi JSCell::m_structureID[t2], t2
-    # Now, t1 has the Structure* and t2 has the StructureID that we want that Structure* to have.
-    bineq t2, Structure::m_blob + StructureIDBlob::u.fields.structureID[t1], .opPutByIdSlow
-    addp PtrSize, t3
-    loadq Structure::m_prototype[t1], t2
+    bineq t2, [t3], .opPutByIdSlow
+    addp 4, t3
+    structureIDToStructureWithScratchAndTable(t2, t1, t0)
+    loadq Structure::m_prototype[t2], t2
     bqneq t2, ValueNull, .opPutByIdTransitionChainLoop
 
 .opPutByIdTransitionChainDone:
     # Reload the new structure, since we clobbered it above.
     loadi OpPutById::Metadata::m_newStructureID[t5], t1
+    # Reload base into t0
+    get(m_base, t3)
+    loadConstantOrVariable(size, t3, t0)
 
 .opPutByIdTransitionDirect:
     storei t1, JSCell::m_structureID[t0]
     writeBarrierOnOperandWithReload(size, get, m_base, macro ()
         # Reload metadata into t5
@@ -1444,10 +1483,15 @@
     dispatch()
 
 .opPutByIdSlow:
     callSlowPath(_llint_slow_path_put_by_id)
     dispatch()
+
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_put_by_id, size)
+    dispatch()
+
 end)
 
 
 llintOpWithMetadata(op_get_by_val, OpGetByVal, macro (size, get, dispatch, metadata, return)
     macro finishGetByVal(result, scratch)
@@ -1456,17 +1500,17 @@
         valueProfile(OpGetByVal, t5, result)
         dispatch()
     end
 
     macro finishIntGetByVal(result, scratch)
-        orq tagTypeNumber, result
+        orq numberTag, result
         finishGetByVal(result, scratch)
     end
 
     macro finishDoubleGetByVal(result, scratch1, scratch2)
         fd2q result, scratch1
-        subq tagTypeNumber, scratch1
+        subq numberTag, scratch1
         finishGetByVal(scratch1, scratch2)
     end
 
     metadata(t5, t2)
 
@@ -1478,12 +1522,12 @@
 
     get(m_property, t3)
     loadConstantOrVariableInt32(size, t3, t1, .opGetByValSlow)
     sxi2q t1, t1
 
-    loadCagedJSValue(JSObject::m_butterfly[t0], t3, tagTypeNumber)
-    move TagTypeNumber, tagTypeNumber
+    loadCagedJSValue(JSObject::m_butterfly[t0], t3, numberTag)
+    move TagNumber, numberTag
 
     andi IndexingShapeMask, t2
     bieq t2, Int32Shape, .opGetByValIsContiguous
     bineq t2, ContiguousShape, .opGetByValNotContiguous
 
@@ -1499,11 +1543,11 @@
     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
     get(m_dst, t0)
     loadd [t3, t1, 8], ft0
     bdnequn ft0, ft0, .opGetByValSlow
     fd2q ft0, t2
-    subq tagTypeNumber, t2
+    subq numberTag, t2
     jmp .opGetByValDone
     
 .opGetByValNotDouble:
     subi ArrayStorageShape, t2
     bia t2, SlowPutArrayStorageShape - ArrayStorageShape, .opGetByValNotIndexedStorage
@@ -1615,14 +1659,21 @@
     finishDoubleGetByVal(ft0, t0, t1)
 
 .opGetByValSlow:
     callSlowPath(_llint_slow_path_get_by_val)
     dispatch()
+
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_get_by_val, size)
+    metadata(t5, t2)
+    valueProfile(OpGetByVal, t5, r0)
+    return(r0)
+
 end)
 
 
-macro putByValOp(opcodeName, opcodeStruct)
+macro putByValOp(opcodeName, opcodeStruct, osrExitPoint)
     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
         macro contiguousPutByVal(storeCallback)
             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], .outOfBounds
         .storeResult:
             get(m_value, t2)
@@ -1643,33 +1694,33 @@
         metadata(t5, t0)
         arrayProfile(%opcodeStruct%::Metadata::m_arrayProfile, t2, t5, t0)
         get(m_property, t0)
         loadConstantOrVariableInt32(size, t0, t3, .opPutByValSlow)
         sxi2q t3, t3
-        loadCagedJSValue(JSObject::m_butterfly[t1], t0, tagTypeNumber)
-        move TagTypeNumber, tagTypeNumber
+        loadCagedJSValue(JSObject::m_butterfly[t1], t0, numberTag)
+        move TagNumber, numberTag
         btinz t2, CopyOnWrite, .opPutByValSlow
         andi IndexingShapeMask, t2
         bineq t2, Int32Shape, .opPutByValNotInt32
         contiguousPutByVal(
             macro (operand, scratch, address)
                 loadConstantOrVariable(size, operand, scratch)
-                bqb scratch, tagTypeNumber, .opPutByValSlow
+                bqb scratch, numberTag, .opPutByValSlow
                 storeq scratch, address
                 writeBarrierOnOperands(size, get, m_base, m_value)
             end)
 
     .opPutByValNotInt32:
         bineq t2, DoubleShape, .opPutByValNotDouble
         contiguousPutByVal(
             macro (operand, scratch, address)
                 loadConstantOrVariable(size, operand, scratch)
-                bqb scratch, tagTypeNumber, .notInt
-                ci2d scratch, ft0
+                bqb scratch, numberTag, .notInt
+                ci2ds scratch, ft0
                 jmp .ready
             .notInt:
-                addq tagTypeNumber, scratch
+                addq numberTag, scratch
                 fq2d scratch, ft0
                 bdnequn ft0, ft0, .opPutByValSlow
             .ready:
                 stored ft0, address
                 writeBarrierOnOperands(size, get, m_base, m_value)
@@ -1706,16 +1757,23 @@
     .opPutByValOutOfBounds:
         storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_outOfBounds[t5]
     .opPutByValSlow:
         callSlowPath(_llint_slow_path_%opcodeName%)
         dispatch()
+
+        osrExitPoint(size, dispatch)
+
     end)
 end
 
-putByValOp(put_by_val, OpPutByVal)
+putByValOp(put_by_val, OpPutByVal, macro (size, dispatch)
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_put_by_val, size)
+    dispatch()
+end)
 
-putByValOp(put_by_val_direct, OpPutByValDirect)
+putByValOp(put_by_val_direct, OpPutByValDirect, macro (a, b) end)
 
 
 macro llintJumpTrueOrFalseOp(opcodeName, opcodeStruct, conditionOp)
     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
         get(m_condition, t1)
@@ -1737,20 +1795,20 @@
 macro equalNullJumpOp(opcodeName, opcodeStruct, cellHandler, immediateHandler)
     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
         get(m_value, t0)
         assertNotConstant(size, t0)
         loadq [cfr, t0, 8], t0
-        btqnz t0, tagMask, .immediate
+        btqnz t0, notCellMask, .immediate
         loadStructureWithScratch(t0, t2, t1, t3)
         cellHandler(t2, JSCell::m_flags[t0], .target)
         dispatch()
 
     .target:
         jump(m_targetLabel)
 
     .immediate:
-        andq ~TagBitUndefined, t0
+        andq ~TagUndefined, t0
         immediateHandler(t0, .target)
         dispatch()
     end)
 end
 
@@ -1776,11 +1834,11 @@
 
 macro undefinedOrNullJumpOp(opcodeName, opcodeStruct, fn)
     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
         get(m_value, t1)
         loadConstantOrVariable(size, t1, t0)
-        andq ~TagBitUndefined, t0
+        andq ~TagUndefined, t0
         fn(t0, .target)
         dispatch()
 
     .target:
         jump(m_targetLabel)
@@ -1793,55 +1851,53 @@
 undefinedOrNullJumpOp(jnundefined_or_null, OpJnundefinedOrNull,
     macro (value, target) bqneq value, ValueNull, target end)
 
 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
     get(m_value, t0)
-    getu(size, OpJneqPtr, m_specialPointer, t1)
-    loadp CodeBlock[cfr], t2
-    loadp CodeBlock::m_globalObject[t2], t2
-    loadp JSGlobalObject::m_specialPointers[t2, t1, PtrSize], t1
-    bpneq t1, [cfr, t0, 8], .opJneqPtrTarget
+    get(m_specialPointer, t1)
+    loadConstant(size, t1, t2)
+    bpneq t2, [cfr, t0, 8], .opJneqPtrTarget
     dispatch()
 
 .opJneqPtrTarget:
     metadata(t5, t0)
     storeb 1, OpJneqPtr::Metadata::m_hasJumped[t5]
     get(m_targetLabel, t0)
-    jumpImpl(t0)
+    jumpImpl(dispatchIndirect, t0)
 end)
 
 
 macro compareJumpOp(opcodeName, opcodeStruct, integerCompare, doubleCompare)
     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
         get(m_lhs, t2)
         get(m_rhs, t3)
         loadConstantOrVariable(size, t2, t0)
         loadConstantOrVariable(size, t3, t1)
-        bqb t0, tagTypeNumber, .op1NotInt
-        bqb t1, tagTypeNumber, .op2NotInt
+        bqb t0, numberTag, .op1NotInt
+        bqb t1, numberTag, .op2NotInt
         integerCompare(t0, t1, .jumpTarget)
         dispatch()
 
     .op1NotInt:
-        btqz t0, tagTypeNumber, .slow
-        bqb t1, tagTypeNumber, .op1NotIntOp2NotInt
-        ci2d t1, ft1
+        btqz t0, numberTag, .slow
+        bqb t1, numberTag, .op1NotIntOp2NotInt
+        ci2ds t1, ft1
         jmp .op1NotIntReady
     .op1NotIntOp2NotInt:
-        btqz t1, tagTypeNumber, .slow
-        addq tagTypeNumber, t1
+        btqz t1, numberTag, .slow
+        addq numberTag, t1
         fq2d t1, ft1
     .op1NotIntReady:
-        addq tagTypeNumber, t0
+        addq numberTag, t0
         fq2d t0, ft0
         doubleCompare(ft0, ft1, .jumpTarget)
         dispatch()
 
     .op2NotInt:
-        ci2d t0, ft0
-        btqz t1, tagTypeNumber, .slow
-        addq tagTypeNumber, t1
+        ci2ds t0, ft0
+        btqz t1, numberTag, .slow
+        addq numberTag, t1
         fq2d t1, ft1
         doubleCompare(ft0, ft1, .jumpTarget)
         dispatch()
 
     .jumpTarget:
@@ -1908,20 +1964,20 @@
     loadp CodeBlock[cfr], t2
     loadp CodeBlock::m_rareData[t2], t2
     muli sizeof SimpleJumpTable, t3
     loadp CodeBlock::RareData::m_switchJumpTables + VectorBufferOffset[t2], t2
     addp t3, t2
-    bqb t1, tagTypeNumber, .opSwitchImmNotInt
+    bqb t1, numberTag, .opSwitchImmNotInt
     subi SimpleJumpTable::min[t2], t1
     biaeq t1, SimpleJumpTable::branchOffsets + VectorSizeOffset[t2], .opSwitchImmFallThrough
     loadp SimpleJumpTable::branchOffsets + VectorBufferOffset[t2], t3
     loadis [t3, t1, 4], t1
     btiz t1, .opSwitchImmFallThrough
     dispatchIndirect(t1)
 
 .opSwitchImmNotInt:
-    btqnz t1, tagTypeNumber, .opSwitchImmSlow   # Go slow if it's a double.
+    btqnz t1, numberTag, .opSwitchImmSlow   # Go slow if it's a double.
 .opSwitchImmFallThrough:
     jump(m_defaultOffset)
 
 .opSwitchImmSlow:
     callSlowPath(_llint_slow_path_switch_imm)
@@ -1936,11 +1992,11 @@
     loadp CodeBlock[cfr], t2
     loadp CodeBlock::m_rareData[t2], t2
     muli sizeof SimpleJumpTable, t3
     loadp CodeBlock::RareData::m_switchJumpTables + VectorBufferOffset[t2], t2
     addp t3, t2
-    btqnz t1, tagMask, .opSwitchCharFallThrough
+    btqnz t1, notCellMask, .opSwitchCharFallThrough
     bbneq JSCell::m_type[t1], StringType, .opSwitchCharFallThrough
     loadp JSString::m_fiber[t1], t0
     btpnz t0, isRopeInPointer, .opSwitchOnRope
     bineq StringImpl::m_length[t0], 1, .opSwitchCharFallThrough
     loadp StringImpl::m_data8[t0], t1
@@ -1972,11 +2028,11 @@
 # we assume t5 contains the metadata, and we should not scratch that
 macro arrayProfileForCall(opcodeStruct, getu)
     getu(m_argv, t3)
     negp t3
     loadq ThisArgumentOffset[cfr, t3, 8], t0
-    btqnz t0, tagMask, .done
+    btqnz t0, notCellMask, .done
     loadi JSCell::m_structureID[t0], t3
     storei t3, %opcodeStruct%::Metadata::m_callLinkInfo.m_arrayProfile.m_lastSeenStructureID[t5]
 .done:
 end
 
@@ -1996,18 +2052,18 @@
         lshifti 3, t3
         negp t3
         addp cfr, t3
         storeq t2, Callee[t3]
         getu(size, opcodeStruct, m_argc, t2)
-        storei PC, ArgumentCount + TagOffset[cfr]
-        storei t2, ArgumentCount + PayloadOffset[t3]
+        storePC()
+        storei t2, ArgumentCountIncludingThis + PayloadOffset[t3]
         move t3, sp
         prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
-        callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
+        callTargetFunction(opcodeName, size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
 
     .opCallSlow:
-        slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
+        slowPathForCall(opcodeName, size, opcodeStruct, dispatch, slowPath, prepareCall)
     end)
 end
 
 llintOp(op_ret, OpRet, macro (size, get, dispatch)
     checkSwitchToJITForEpilogue()
@@ -2018,30 +2074,46 @@
 
 
 llintOpWithReturn(op_to_primitive, OpToPrimitive, macro (size, get, dispatch, return)
     get(m_src, t2)
     loadConstantOrVariable(size, t2, t0)
-    btqnz t0, tagMask, .opToPrimitiveIsImm
+    btqnz t0, notCellMask, .opToPrimitiveIsImm
     bbaeq JSCell::m_type[t0], ObjectType, .opToPrimitiveSlowCase
 .opToPrimitiveIsImm:
     return(t0)
 
 .opToPrimitiveSlowCase:
     callSlowPath(_slow_path_to_primitive)
     dispatch()
 end)
 
 
+llintOpWithReturn(op_to_property_key, OpToPropertyKey, macro (size, get, dispatch, return)
+    get(m_src, t2)
+    loadConstantOrVariable(size, t2, t0)
+
+    btqnz t0, notCellMask, .opToPropertyKeySlow
+    bbeq JSCell::m_type[t0], SymbolType, .done
+    bbneq JSCell::m_type[t0], StringType, .opToPropertyKeySlow
+
+.done:
+    return(t0)
+
+.opToPropertyKeySlow:
+    callSlowPath(_slow_path_to_property_key)
+    dispatch()
+end)
+
+
 commonOp(llint_op_catch, macro() end, macro (size)
     # This is where we end up from the JIT's throw trampoline (because the
     # machine code return address will be set to _llint_op_catch), and from
     # the interpreter's throw trampoline (see _llint_throw_trampoline).
     # The throwing code must have known that we were throwing to the interpreter,
     # and have set VM::targetInterpreterPCForThrow.
     loadp Callee[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    convertCalleeToVM(t3)
     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
     loadp VM::callFrameForCatch[t3], cfr
     storep 0, VM::callFrameForCatch[t3]
     restoreStackPointerAfterCall()
 
@@ -2054,13 +2126,12 @@
     callSlowPath(_llint_slow_path_check_if_exception_is_uncatchable_and_notify_profiler)
     bpeq r1, 0, .isCatchableException
     jmp _llint_throw_from_slow_path_trampoline
 
 .isCatchableException:
-    loadp Callee[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    loadp CodeBlock[cfr], t3
+    loadp CodeBlock::m_vm[t3], t3
 
     loadp VM::m_exception[t3], t0
     storep 0, VM::m_exception[t3]
     get(size, OpCatch, m_exception, t2)
     storeq t0, [cfr, t2, 8]
@@ -2086,22 +2157,20 @@
 end)
 
 
 op(llint_throw_from_slow_path_trampoline, macro ()
     loadp Callee[cfr], t1
-    andp MarkedBlockMask, t1
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
+    convertCalleeToVM(t1)
     copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(t1, t2)
 
     callSlowPath(_llint_slow_path_handle_exception)
 
     # When throwing from the interpreter (i.e. throwing from LLIntSlowPaths), so
     # the throw target is not necessarily interpreted code, we come to here.
     # This essentially emulates the JIT's throwing protocol.
     loadp Callee[cfr], t1
-    andp MarkedBlockMask, t1
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
+    convertCalleeToVM(t1)
     jmp VM::targetMachinePCForThrow[t1], ExceptionHandlerPtrTag
 end)
 
 
 op(llint_throw_during_call_trampoline, macro ()
@@ -2109,39 +2178,40 @@
     jmp _llint_throw_from_slow_path_trampoline
 end)
 
 
 macro nativeCallTrampoline(executableOffsetToFunction)
-
     functionPrologue()
     storep 0, CodeBlock[cfr]
-    loadp Callee[cfr], t0
-    andp MarkedBlockMask, t0, t1
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
-    storep cfr, VM::topCallFrame[t1]
+    loadp Callee[cfr], a0
+    loadp JSFunction::m_executableOrRareData[a0], a2
+    btpz a2, (constexpr JSFunction::rareDataTag), .isExecutable
+    loadp (FunctionRareData::m_executable - (constexpr JSFunction::rareDataTag))[a2], a2
+.isExecutable:
+    loadp JSFunction::m_scope[a0], a0
+    loadp JSGlobalObject::m_vm[a0], a1
+    storep cfr, VM::topCallFrame[a1]
     if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
         storep lr, ReturnPC[cfr]
     end
-    move cfr, a0
-    loadp Callee[cfr], t1
-    loadp JSFunction::m_executable[t1], t1
+    move cfr, a1
     checkStackPointerAlignment(t3, 0xdead0001)
     if C_LOOP or C_LOOP_WIN
-        cloopCallNative executableOffsetToFunction[t1]
+        cloopCallNative executableOffsetToFunction[a2]
     else
         if X86_64_WIN
             subp 32, sp
-            call executableOffsetToFunction[t1], JSEntryPtrTag
+            call executableOffsetToFunction[a2], JSEntryPtrTag
             addp 32, sp
         else
-            call executableOffsetToFunction[t1], JSEntryPtrTag
+            call executableOffsetToFunction[a2], JSEntryPtrTag
         end
     end
 
     loadp Callee[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    loadp JSFunction::m_scope[t3], t3
+    loadp JSGlobalObject::m_vm[t3], t3
 
     btpnz VM::m_exception[t3], .handleException
 
     functionEpilogue()
     ret
@@ -2152,35 +2222,34 @@
 end
 
 macro internalFunctionCallTrampoline(offsetOfFunction)
     functionPrologue()
     storep 0, CodeBlock[cfr]
-    loadp Callee[cfr], t0
-    andp MarkedBlockMask, t0, t1
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
-    storep cfr, VM::topCallFrame[t1]
+    loadp Callee[cfr], a2
+    loadp InternalFunction::m_globalObject[a2], a0
+    loadp JSGlobalObject::m_vm[a0], a1
+    storep cfr, VM::topCallFrame[a1]
     if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
         storep lr, ReturnPC[cfr]
     end
-    move cfr, a0
-    loadp Callee[cfr], t1
+    move cfr, a1
     checkStackPointerAlignment(t3, 0xdead0001)
     if C_LOOP or C_LOOP_WIN
-        cloopCallNative offsetOfFunction[t1]
+        cloopCallNative offsetOfFunction[a2]
     else
         if X86_64_WIN
             subp 32, sp
-            call offsetOfFunction[t1], JSEntryPtrTag
+            call offsetOfFunction[a2], JSEntryPtrTag
             addp 32, sp
         else
-            call offsetOfFunction[t1], JSEntryPtrTag
+            call offsetOfFunction[a2], JSEntryPtrTag
         end
     end
 
     loadp Callee[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    loadp InternalFunction::m_globalObject[t3], t3
+    loadp JSGlobalObject::m_vm[t3], t3
 
     btpnz VM::m_exception[t3], .handleException
 
     functionEpilogue()
     ret
@@ -2540,11 +2609,11 @@
     # Store the TypeLocation onto the log entry.
     metadata(t5, t3)
     loadp OpProfileType::Metadata::m_typeLocation[t5], t3
     storep t3, TypeProfilerLog::LogEntry::location[t2]
 
-    btqz t0, tagMask, .opProfileTypeIsCell
+    btqz t0, notCellMask, .opProfileTypeIsCell
     storei 0, TypeProfilerLog::LogEntry::structureID[t2]
     jmp .opProfileTypeSkipIsCell
 .opProfileTypeIsCell:
     loadi JSCell::m_structureID[t0], t3
     storei t3, TypeProfilerLog::LogEntry::structureID[t2]
@@ -2570,23 +2639,41 @@
     dispatch()
 end)
 
 
 llintOpWithReturn(op_get_rest_length, OpGetRestLength, macro (size, get, dispatch, return)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     subi 1, t0
     getu(size, OpGetRestLength, m_numParametersToSkip, t1)
     bilteq t0, t1, .storeZero
     subi t1, t0
     jmp .boxUp
 .storeZero:
     move 0, t0
 .boxUp:
-    orq tagTypeNumber, t0
+    orq numberTag, t0
+    return(t0)
+end)
+
+
+llintOpWithProfile(op_get_internal_field, OpGetInternalField, macro (size, get, dispatch, return)
+    loadVariable(get, m_base, t1)
+    getu(size, OpGetInternalField, m_index, t2)
+    loadq JSInternalFieldObjectImpl_internalFields[t1, t2, SlotSize], t0
     return(t0)
 end)
 
+llintOp(op_put_internal_field, OpPutInternalField, macro (size, get, dispatch)
+    loadVariable(get, m_base, t0)
+    get(m_value, t1)
+    loadConstantOrVariable(size, t1, t2)
+    getu(size, OpPutInternalField, m_index, t1)
+    storeq t2, JSInternalFieldObjectImpl_internalFields[t0, t1, SlotSize]
+    writeBarrierOnCellAndValueWithReload(t0, t2, macro() end)
+    dispatch()
+end)
+
 
 llintOp(op_log_shadow_chicken_prologue, OpLogShadowChickenPrologue, macro (size, get, dispatch)
     acquireShadowChickenPacket(.opLogShadowChickenPrologueSlow)
     storep cfr, ShadowChicken::Packet::frame[t0]
     loadp CallerFrame[cfr], t1
