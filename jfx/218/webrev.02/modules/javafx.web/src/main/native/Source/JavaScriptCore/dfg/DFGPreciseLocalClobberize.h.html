<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGPreciseLocalClobberize.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2014-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #if ENABLE(DFG_JIT)
 29 
 30 #include &quot;DFGClobberize.h&quot;
 31 
 32 namespace JSC { namespace DFG {
 33 
 34 template&lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor&gt;
 35 class PreciseLocalClobberizeAdaptor {
 36 public:
 37     PreciseLocalClobberizeAdaptor(
 38         Graph&amp; graph, Node* node,
 39         const ReadFunctor&amp; read, const WriteFunctor&amp; write, const DefFunctor&amp; def)
 40         : m_graph(graph)
 41         , m_node(node)
 42         , m_read(read)
 43         , m_unconditionalWrite(write)
 44         , m_def(def)
 45     {
 46     }
 47 
 48     void read(AbstractHeap heap)
 49     {
 50         if (heap.kind() == Stack) {
 51             if (heap.payload().isTop()) {
 52                 readTop();
 53                 return;
 54             }
 55 
 56             callIfAppropriate(m_read, heap.operand());
 57             return;
 58         }
 59 
 60         if (heap.overlaps(Stack)) {
 61             readTop();
 62             return;
 63         }
 64     }
 65 
 66     void write(AbstractHeap heap)
 67     {
 68         // We expect stack writes to already be precisely characterized by DFG::clobberize().
 69         if (heap.kind() == Stack) {
 70             RELEASE_ASSERT(!heap.payload().isTop());
 71             callIfAppropriate(m_unconditionalWrite, heap.operand());
 72             return;
 73         }
 74 
 75         RELEASE_ASSERT(!heap.overlaps(Stack));
 76     }
 77 
 78     void def(PureValue)
 79     {
 80         // PureValue defs never have anything to do with locals, so ignore this.
 81     }
 82 
 83     void def(HeapLocation location, LazyNode node)
 84     {
 85         if (location.kind() != StackLoc)
 86             return;
 87 
 88         RELEASE_ASSERT(location.heap().kind() == Stack);
 89 
 90         m_def(location.heap().operand(), node);
 91     }
 92 
 93 private:
 94     template&lt;typename Functor&gt;
 95     void callIfAppropriate(const Functor&amp; functor, Operand operand)
 96     {
 97         if (operand.isLocal() &amp;&amp; static_cast&lt;unsigned&gt;(operand.toLocal()) &gt;= m_graph.block(0)-&gt;variablesAtHead.numberOfLocals())
 98             return;
 99 
100         if (operand.isArgument() &amp;&amp; !operand.isHeader() &amp;&amp; static_cast&lt;unsigned&gt;(operand.toArgument()) &gt;= m_graph.block(0)-&gt;variablesAtHead.numberOfArguments())
101             return;
102 
103         functor(operand);
104     }
105 
106     void readTop()
107     {
108         auto readFrame = [&amp;] (InlineCallFrame* inlineCallFrame, unsigned numberOfArgumentsToSkip) {
109             if (!inlineCallFrame) {
110                 // Read the outermost arguments and argument count.
111                 for (unsigned i = numberOfArgumentsToSkip; i &lt; static_cast&lt;unsigned&gt;(m_graph.m_codeBlock-&gt;numParameters()); i++)
112                     m_read(virtualRegisterForArgumentIncludingThis(i));
113                 m_read(VirtualRegister(CallFrameSlot::argumentCountIncludingThis));
114                 return;
115             }
116 
117             for (unsigned i = numberOfArgumentsToSkip; i &lt; inlineCallFrame-&gt;argumentsWithFixup.size(); i++)
118                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgumentIncludingThis(i).offset()));
119             if (inlineCallFrame-&gt;isVarargs())
120                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis));
121         };
122 
123         auto readSpread = [&amp;] (Node* spread) {
124             ASSERT(spread-&gt;op() == Spread || spread-&gt;op() == PhantomSpread);
125             if (!spread-&gt;child1()-&gt;isPhantomAllocation())
126                 return;
127 
128             ASSERT(spread-&gt;child1()-&gt;op() == PhantomCreateRest || spread-&gt;child1()-&gt;op() == PhantomNewArrayBuffer);
129             if (spread-&gt;child1()-&gt;op() == PhantomNewArrayBuffer) {
130                 // This reads from a constant buffer.
131                 return;
132             }
133             InlineCallFrame* inlineCallFrame = spread-&gt;child1()-&gt;origin.semantic.inlineCallFrame();
134             unsigned numberOfArgumentsToSkip = spread-&gt;child1()-&gt;numberOfArgumentsToSkip();
135             readFrame(inlineCallFrame, numberOfArgumentsToSkip);
136         };
137 
138         auto readNewArrayWithSpreadNode = [&amp;] (Node* arrayWithSpread) {
139             ASSERT(arrayWithSpread-&gt;op() == NewArrayWithSpread || arrayWithSpread-&gt;op() == PhantomNewArrayWithSpread);
140             BitVector* bitVector = arrayWithSpread-&gt;bitVector();
141             for (unsigned i = 0; i &lt; arrayWithSpread-&gt;numChildren(); i++) {
142                 if (bitVector-&gt;get(i)) {
143                     Node* child = m_graph.varArgChild(arrayWithSpread, i).node();
144                     if (child-&gt;op() == PhantomSpread)
145                         readSpread(child);
146                 }
147             }
148         };
149 
150         switch (m_node-&gt;op()) {
151         case ForwardVarargs:
152         case CallForwardVarargs:
153         case ConstructForwardVarargs:
154         case TailCallForwardVarargs:
155         case TailCallForwardVarargsInlinedCaller:
156         case GetMyArgumentByVal:
157         case GetMyArgumentByValOutOfBounds:
158         case CreateDirectArguments:
159         case CreateScopedArguments:
160         case CreateClonedArguments:
161         case CreateArgumentsButterfly:
162         case PhantomDirectArguments:
163         case PhantomClonedArguments:
164         case GetRestLength:
165         case CreateRest: {
166             bool isForwardingNode = false;
167             bool isPhantomNode = false;
168             switch (m_node-&gt;op()) {
169             case ForwardVarargs:
170             case CallForwardVarargs:
171             case ConstructForwardVarargs:
172             case TailCallForwardVarargs:
173             case TailCallForwardVarargsInlinedCaller:
174                 isForwardingNode = true;
175                 break;
176             case PhantomDirectArguments:
177             case PhantomClonedArguments:
178                 isPhantomNode = true;
179                 break;
180             default:
181                 break;
182             }
183 
184             if (isPhantomNode &amp;&amp; m_graph.m_plan.isFTL())
185                 break;
186 
187             if (isForwardingNode &amp;&amp; m_node-&gt;hasArgumentsChild() &amp;&amp; m_node-&gt;argumentsChild()
188                 &amp;&amp; (m_node-&gt;argumentsChild()-&gt;op() == PhantomNewArrayWithSpread || m_node-&gt;argumentsChild()-&gt;op() == PhantomSpread)) {
189                 if (m_node-&gt;argumentsChild()-&gt;op() == PhantomNewArrayWithSpread)
190                     readNewArrayWithSpreadNode(m_node-&gt;argumentsChild().node());
191                 else
192                     readSpread(m_node-&gt;argumentsChild().node());
193             } else {
194                 InlineCallFrame* inlineCallFrame;
195                 if (m_node-&gt;hasArgumentsChild() &amp;&amp; m_node-&gt;argumentsChild())
196                     inlineCallFrame = m_node-&gt;argumentsChild()-&gt;origin.semantic.inlineCallFrame();
197                 else
198                     inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame();
199 
200                 unsigned numberOfArgumentsToSkip = 0;
201                 if (m_node-&gt;op() == GetMyArgumentByVal || m_node-&gt;op() == GetMyArgumentByValOutOfBounds) {
202                     // The value of numberOfArgumentsToSkip guarantees that GetMyArgumentByVal* will never
203                     // read any arguments below the number of arguments to skip. For example, if numberOfArgumentsToSkip is 2,
204                     // we will never read argument 0 or argument 1.
205                     numberOfArgumentsToSkip = m_node-&gt;numberOfArgumentsToSkip();
206                 }
207 
208                 readFrame(inlineCallFrame, numberOfArgumentsToSkip);
209             }
210 
211             break;
212         }
213 
214         case Spread:
215             readSpread(m_node);
216             break;
217 
218         case NewArrayWithSpread: {
219             readNewArrayWithSpreadNode(m_node);
220             break;
221         }
222 
223         case GetArgument: {
224             InlineCallFrame* inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame();
225             unsigned indexIncludingThis = m_node-&gt;argumentIndex();
226             if (!inlineCallFrame) {
227                 if (indexIncludingThis &lt; static_cast&lt;unsigned&gt;(m_graph.m_codeBlock-&gt;numParameters()))
228                     m_read(virtualRegisterForArgumentIncludingThis(indexIncludingThis));
229                 m_read(VirtualRegister(CallFrameSlot::argumentCountIncludingThis));
230                 break;
231             }
232 
233             ASSERT_WITH_MESSAGE(inlineCallFrame-&gt;isVarargs(), &quot;GetArgument is only used for InlineCallFrame if the call frame is varargs.&quot;);
234             if (indexIncludingThis &lt; inlineCallFrame-&gt;argumentsWithFixup.size())
235                 m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgumentIncludingThis(indexIncludingThis).offset()));
236             m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis));
237             break;
238         }
239 
240         default: {
241             // All of the outermost arguments, except this, are read in sloppy mode.
242             if (!m_graph.m_codeBlock-&gt;isStrictMode()) {
243                 for (unsigned i = m_graph.m_codeBlock-&gt;numParameters(); i--;)
244                     m_read(virtualRegisterForArgumentIncludingThis(i));
245             }
246 
247             // The stack header is read.
248             for (unsigned i = 0; i &lt; CallFrameSlot::thisArgument; ++i)
249                 m_read(VirtualRegister(i));
250 
251             // Read all of the inline arguments and call frame headers that we didn&#39;t already capture.
252             for (InlineCallFrame* inlineCallFrame = m_node-&gt;origin.semantic.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;getCallerInlineFrameSkippingTailCalls()) {
253                 if (!inlineCallFrame-&gt;isStrictMode()) {
254                     for (unsigned i = inlineCallFrame-&gt;argumentsWithFixup.size(); i--;)
255                         m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + virtualRegisterForArgumentIncludingThis(i).offset()));
256                 }
257                 if (inlineCallFrame-&gt;isClosureCall)
258                     m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee));
259                 if (inlineCallFrame-&gt;isVarargs())
260                     m_read(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis));
261             }
262             break;
263         } }
264     }
265 
266     Graph&amp; m_graph;
267     Node* m_node;
268     const ReadFunctor&amp; m_read;
269     const WriteFunctor&amp; m_unconditionalWrite;
270     const DefFunctor&amp; m_def;
271 };
272 
273 template&lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor&gt;
274 void preciseLocalClobberize(
275     Graph&amp; graph, Node* node,
276     const ReadFunctor&amp; read, const WriteFunctor&amp; write, const DefFunctor&amp; def)
277 {
278     PreciseLocalClobberizeAdaptor&lt;ReadFunctor, WriteFunctor, DefFunctor&gt;
279         adaptor(graph, node, read, write, def);
280     clobberize(graph, node, adaptor);
281 }
282 
283 } } // namespace JSC::DFG
284 
285 #endif // ENABLE(DFG_JIT)
    </pre>
  </body>
</html>