diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp
@@ -30,10 +30,11 @@
 #include "AllocatorInlines.h"
 #include "BlockDirectoryInlines.h"
 #include "JSCInlines.h"
 #include "LocalAllocatorInlines.h"
 #include "MarkedBlockInlines.h"
+#include "MarkedSpaceInlines.h"
 #include "PreventCollectionScope.h"
 #include "SubspaceInlines.h"
 
 namespace JSC {
 
@@ -77,12 +78,11 @@
         return allocator;
 
     if (false)
         dataLog("Creating BlockDirectory/LocalAllocator for ", m_name, ", ", attributes(), ", ", sizeClass, ".\n");
 
-    std::unique_ptr<BlockDirectory> uniqueDirectory =
-        makeUnique<BlockDirectory>(m_space.heap(), sizeClass);
+    std::unique_ptr<BlockDirectory> uniqueDirectory = makeUnique<BlockDirectory>(sizeClass);
     BlockDirectory* directory = uniqueDirectory.get();
     m_directories.append(WTFMove(uniqueDirectory));
 
     directory->setSubspace(this);
     m_space.addBlockDirectory(locker, directory);
@@ -104,11 +104,11 @@
         if (!index--)
             break;
     }
 
     directory->setNextDirectoryInSubspace(m_firstDirectory);
-    m_alignedMemoryAllocator->registerDirectory(directory);
+    m_alignedMemoryAllocator->registerDirectory(m_space.heap(), directory);
     WTF::storeStoreFence();
     m_firstDirectory = directory;
     return allocator;
 }
 
@@ -126,54 +126,56 @@
         RELEASE_ASSERT(vm.heap.expectDoesGC());
 
     sanitizeStackForVM(vm);
 
     if (Allocator allocator = allocatorFor(size, AllocatorForMode::EnsureAllocator))
-        return allocator.allocate(deferralContext, AllocationFailureMode::ReturnNull);
+        return allocator.allocate(vm.heap, deferralContext, AllocationFailureMode::ReturnNull);
 
-    if (size <= Options::largeAllocationCutoff()
+    if (size <= Options::preciseAllocationCutoff()
         && size <= MarkedSpace::largeCutoff) {
         dataLog("FATAL: attampting to allocate small object using large allocation.\n");
         dataLog("Requested allocation size: ", size, "\n");
         RELEASE_ASSERT_NOT_REACHED();
     }
 
     vm.heap.collectIfNecessaryOrDefer(deferralContext);
 
     size = WTF::roundUpToMultipleOf<MarkedSpace::sizeStep>(size);
-    LargeAllocation* allocation = LargeAllocation::tryCreate(vm.heap, size, this, m_space.m_largeAllocations.size());
+    PreciseAllocation* allocation = PreciseAllocation::tryCreate(vm.heap, size, this, m_space.m_preciseAllocations.size());
     if (!allocation)
         return nullptr;
 
-    m_space.m_largeAllocations.append(allocation);
-    ASSERT(allocation->indexInSpace() == m_space.m_largeAllocations.size() - 1);
+    m_space.m_preciseAllocations.append(allocation);
+    if (auto* set = m_space.preciseAllocationSet())
+        set->add(allocation->cell());
+    ASSERT(allocation->indexInSpace() == m_space.m_preciseAllocations.size() - 1);
     vm.heap.didAllocate(size);
     m_space.m_capacity += size;
 
-    m_largeAllocations.append(allocation);
+    m_preciseAllocations.append(allocation);
 
     return allocation->cell();
 }
 
-void* CompleteSubspace::reallocateLargeAllocationNonVirtual(VM& vm, HeapCell* oldCell, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
+void* CompleteSubspace::reallocatePreciseAllocationNonVirtual(VM& vm, HeapCell* oldCell, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
 {
     if (validateDFGDoesGC)
         RELEASE_ASSERT(vm.heap.expectDoesGC());
 
     // The following conditions are met in Butterfly for example.
-    ASSERT(oldCell->isLargeAllocation());
+    ASSERT(oldCell->isPreciseAllocation());
 
-    LargeAllocation* oldAllocation = &oldCell->largeAllocation();
+    PreciseAllocation* oldAllocation = &oldCell->preciseAllocation();
     ASSERT(oldAllocation->cellSize() <= size);
     ASSERT(oldAllocation->weakSet().isTriviallyDestructible());
     ASSERT(oldAllocation->attributes().destruction == DoesNotNeedDestruction);
     ASSERT(oldAllocation->attributes().cellKind == HeapCell::Auxiliary);
     ASSERT(size > MarkedSpace::largeCutoff);
 
     sanitizeStackForVM(vm);
 
-    if (size <= Options::largeAllocationCutoff()
+    if (size <= Options::preciseAllocationCutoff()
         && size <= MarkedSpace::largeCutoff) {
         dataLog("FATAL: attampting to allocate small object using large allocation.\n");
         dataLog("Requested allocation size: ", size, "\n");
         RELEASE_ASSERT_NOT_REACHED();
     }
@@ -184,23 +186,31 @@
     size_t difference = size - oldAllocation->cellSize();
     unsigned oldIndexInSpace = oldAllocation->indexInSpace();
     if (oldAllocation->isOnList())
         oldAllocation->remove();
 
-    LargeAllocation* allocation = oldAllocation->tryReallocate(size, this);
+    PreciseAllocation* allocation = oldAllocation->tryReallocate(size, this);
     if (!allocation) {
         RELEASE_ASSERT(failureMode != AllocationFailureMode::Assert);
-        m_largeAllocations.append(oldAllocation);
+        m_preciseAllocations.append(oldAllocation);
         return nullptr;
     }
     ASSERT(oldIndexInSpace == allocation->indexInSpace());
 
-    m_space.m_largeAllocations[oldIndexInSpace] = allocation;
+    // If reallocation changes the address, we should update HashSet.
+    if (oldAllocation != allocation) {
+        if (auto* set = m_space.preciseAllocationSet()) {
+            set->remove(oldAllocation->cell());
+            set->add(allocation->cell());
+        }
+    }
+
+    m_space.m_preciseAllocations[oldIndexInSpace] = allocation;
     vm.heap.didAllocate(difference);
     m_space.m_capacity += difference;
 
-    m_largeAllocations.append(allocation);
+    m_preciseAllocations.append(allocation);
 
     return allocation->cell();
 }
 
 } // namespace JSC
