<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CallFrameShuffler64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ExecutableAllocator.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 75      mach_vm_address_t *target_address,
 76      mach_vm_size_t size,
 77      mach_vm_offset_t mask,
 78      int flags,
 79      vm_map_t src_task,
 80      mach_vm_address_t src_address,
 81      boolean_t copy,
 82      vm_prot_t *cur_protection,
 83      vm_prot_t *max_protection,
 84      vm_inherit_t inheritance
 85      );
 86 }
 87 
 88 #endif
 89 
 90 namespace JSC {
 91 
 92 using namespace WTF;
 93 
 94 #if defined(FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB) &amp;&amp; FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB &gt; 0
<span class="line-modified"> 95 static const size_t fixedExecutableMemoryPoolSize = FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB * 1024 * 1024;</span>
 96 #elif CPU(ARM)
<span class="line-modified"> 97 static const size_t fixedExecutableMemoryPoolSize = 16 * 1024 * 1024;</span>
 98 #elif CPU(ARM64)
<span class="line-modified"> 99 static const size_t fixedExecutableMemoryPoolSize = 128 * 1024 * 1024;</span>
100 #elif CPU(X86_64)
<span class="line-modified">101 static const size_t fixedExecutableMemoryPoolSize = 1024 * 1024 * 1024;</span>
102 #else
<span class="line-modified">103 static const size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;</span>
104 #endif
105 
106 #if CPU(ARM)
<span class="line-modified">107 static const double executablePoolReservationFraction = 0.15;</span>
108 #else
<span class="line-modified">109 static const double executablePoolReservationFraction = 0.25;</span>
110 #endif
111 
<span class="line-removed">112 #if ENABLE(SEPARATED_WX_HEAP)</span>
<span class="line-removed">113 JS_EXPORT_PRIVATE bool useFastPermisionsJITCopy { false };</span>
<span class="line-removed">114 JS_EXPORT_PRIVATE JITWriteSeparateHeapsFunction jitWriteSeparateHeapsFunction;</span>
<span class="line-removed">115 #endif</span>
<span class="line-removed">116 </span>
<span class="line-removed">117 #if !USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION) &amp;&amp; HAVE(REMAP_JIT)</span>
<span class="line-removed">118 static uintptr_t startOfFixedWritableMemoryPool;</span>
<span class="line-removed">119 #endif</span>
<span class="line-removed">120 </span>
<span class="line-removed">121 class FixedVMPoolExecutableAllocator;</span>
<span class="line-removed">122 static FixedVMPoolExecutableAllocator* allocator = nullptr;</span>
<span class="line-removed">123 </span>
<span class="line-removed">124 static bool s_isJITEnabled = true;</span>
125 static bool isJITEnabled()
126 {

127 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
<span class="line-modified">128     return processHasEntitlement(&quot;dynamic-codesigning&quot;) &amp;&amp; s_isJITEnabled;</span>
129 #else
<span class="line-modified">130     return s_isJITEnabled;</span>
131 #endif
132 }
133 
134 void ExecutableAllocator::setJITEnabled(bool enabled)
135 {
<span class="line-modified">136     ASSERT(!allocator);</span>
<span class="line-modified">137     if (s_isJITEnabled == enabled)</span>

138         return;
139 
<span class="line-modified">140     s_isJITEnabled = enabled;</span>
141 
142 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
143     if (!enabled) {











144         constexpr size_t size = 1;
145         constexpr int protection = PROT_READ | PROT_WRITE | PROT_EXEC;
146         constexpr int flags = MAP_PRIVATE | MAP_ANON | MAP_JIT;
147         constexpr int fd = OSAllocator::JSJITCodePages;
148         void* allocation = mmap(nullptr, size, protection, flags, fd, 0);
149         const void* executableMemoryAllocationFailure = reinterpret_cast&lt;void*&gt;(-1);
150         RELEASE_ASSERT_WITH_MESSAGE(allocation &amp;&amp; allocation != executableMemoryAllocationFailure, &quot;We should not have allocated executable memory before disabling the JIT.&quot;);
151         RELEASE_ASSERT_WITH_MESSAGE(!munmap(allocation, size), &quot;Unmapping executable memory should succeed so we do not have any executable memory in the address space&quot;);
152         RELEASE_ASSERT_WITH_MESSAGE(mmap(nullptr, size, protection, flags, fd, 0) == executableMemoryAllocationFailure, &quot;Allocating executable memory should fail after setJITEnabled(false) is called.&quot;);
153     }
154 #endif
155 }
156 
157 class FixedVMPoolExecutableAllocator final : public MetaAllocator {
158     WTF_MAKE_FAST_ALLOCATED;
159 public:
160     FixedVMPoolExecutableAllocator()
161         : MetaAllocator(jitAllocationGranule) // round up all allocations to 32 bytes
162     {
163         if (!isJITEnabled())
</pre>
<hr />
<pre>
176             // This makes the following JIT code logging broken and some of JIT code is not recorded correctly.
177             // To avoid this problem, we use committed reservation if we need perf JITDump logging.
178             if (Options::logJITCodeForPerf())
179                 return PageReservation::reserveAndCommitWithGuardPages(reservationSize, OSAllocator::JSJITCodePages, EXECUTABLE_POOL_WRITABLE, true);
180 #endif
181             return PageReservation::reserveWithGuardPages(reservationSize, OSAllocator::JSJITCodePages, EXECUTABLE_POOL_WRITABLE, true);
182         };
183 
184         m_reservation = tryCreatePageReservation(reservationSize);
185         if (m_reservation) {
186             ASSERT(m_reservation.size() == reservationSize);
187             void* reservationBase = m_reservation.base();
188 
189 #if ENABLE(FAST_JIT_PERMISSIONS) &amp;&amp; !ENABLE(SEPARATED_WX_HEAP)
190             RELEASE_ASSERT(os_thread_self_restrict_rwx_is_supported());
191             os_thread_self_restrict_rwx_to_rx();
192 
193 #else // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
194 #if ENABLE(FAST_JIT_PERMISSIONS)
195             if (os_thread_self_restrict_rwx_is_supported()) {
<span class="line-modified">196                 useFastPermisionsJITCopy = true;</span>
197                 os_thread_self_restrict_rwx_to_rx();
198             } else
199 #endif
200             if (Options::useSeparatedWXHeap()) {
201                 // First page of our JIT allocation is reserved.
202                 ASSERT(reservationSize &gt;= pageSize() * 2);
203                 reservationBase = (void*)((uintptr_t)reservationBase + pageSize());
204                 reservationSize -= pageSize();
205                 initializeSeparatedWXHeaps(m_reservation.base(), pageSize(), reservationBase, reservationSize);
206             }
207 #endif // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
208 
209             addFreshFreeSpace(reservationBase, reservationSize);
210 
211             ASSERT(bytesReserved() == reservationSize); // Since our executable memory is fixed-sized, bytesReserved is never changed after initialization.
212 
213             void* reservationEnd = reinterpret_cast&lt;uint8_t*&gt;(reservationBase) + reservationSize;
214 
<span class="line-modified">215             m_memoryStart = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationBase));</span>
<span class="line-modified">216             m_memoryEnd = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationEnd));</span>
217         }
218     }
219 
220     virtual ~FixedVMPoolExecutableAllocator();
221 
<span class="line-modified">222     void* memoryStart() { return m_memoryStart.untaggedExecutableAddress(); }</span>
<span class="line-modified">223     void* memoryEnd() { return m_memoryEnd.untaggedExecutableAddress(); }</span>
224     bool isJITPC(void* pc) { return memoryStart() &lt;= pc &amp;&amp; pc &lt; memoryEnd(); }
225 
226 protected:
227     FreeSpacePtr allocateNewSpace(size_t&amp;) override
228     {
229         // We&#39;re operating in a fixed pool, so new allocation is always prohibited.
230         return nullptr;
231     }
232 
233     void notifyNeedPage(void* page, size_t count) override
234     {
235 #if USE(MADV_FREE_FOR_JIT_MEMORY)
236         UNUSED_PARAM(page);
237         UNUSED_PARAM(count);
238 #else
239         m_reservation.commit(page, pageSize() * count);
240 #endif
241     }
242 
243     void notifyPageIsFree(void* page, size_t count) override
</pre>
<hr />
<pre>
285         int result = 0;
286 
287 #if USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
288         // Prevent reading the write thunk code.
289         result = vm_protect(mach_task_self(), reinterpret_cast&lt;vm_address_t&gt;(stubBase), stubSize, true, VM_PROT_EXECUTE);
290         RELEASE_ASSERT(!result);
291 #endif
292 
293         // Prevent writing into the executable JIT mapping.
294         result = vm_protect(mach_task_self(), reinterpret_cast&lt;vm_address_t&gt;(jitBase), jitSize, true, VM_PROT_READ | VM_PROT_EXECUTE);
295         RELEASE_ASSERT(!result);
296 
297         // Prevent execution in the writable JIT mapping.
298         result = vm_protect(mach_task_self(), static_cast&lt;vm_address_t&gt;(writableAddr), jitSize, true, VM_PROT_READ | VM_PROT_WRITE);
299         RELEASE_ASSERT(!result);
300 
301         // Zero out writableAddr to avoid leaking the address of the writable mapping.
302         memset_s(&amp;writableAddr, sizeof(writableAddr), 0, sizeof(writableAddr));
303 
304 #if ENABLE(SEPARATED_WX_HEAP)
<span class="line-modified">305         jitWriteSeparateHeapsFunction = reinterpret_cast&lt;JITWriteSeparateHeapsFunction&gt;(writeThunk.code().executableAddress());</span>
306 #endif
307     }
308 
309 #if CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
310     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; jitWriteThunkGenerator(void* writableAddr, void* stubBase, size_t stubSize)
311     {
312         using namespace ARM64Registers;
313         using TrustedImm32 = MacroAssembler::TrustedImm32;
314 
315         MacroAssembler jit;
316 
317         jit.tagReturnAddress();
318         jit.move(MacroAssembler::TrustedImmPtr(writableAddr), x7);
319         jit.addPtr(x7, x0);
320 
321         jit.move(x0, x3);
322         MacroAssembler::Jump smallCopy = jit.branch64(MacroAssembler::Below, x2, MacroAssembler::TrustedImm64(64));
323 
324         jit.add64(TrustedImm32(32), x3);
325         jit.and64(TrustedImm32(-32), x3);
</pre>
<hr />
<pre>
364         MacroAssembler::Jump local2 = jit.branchAdd64(MacroAssembler::Equal, TrustedImm32(8), x2);
365         MacroAssembler::Label local1 = jit.label();
366         jit.load8(x1, PostIndex(1), x6);
367         jit.store8(x6, x3, PostIndex(1));
368         jit.branchSub64(MacroAssembler::NotEqual, TrustedImm32(1), x2).linkTo(local1, &amp;jit);
369         local2.link(&amp;jit);
370         jit.ret();
371 
372         auto stubBaseCodePtr = MacroAssemblerCodePtr&lt;LinkBufferPtrTag&gt;(tagCodePtr&lt;LinkBufferPtrTag&gt;(stubBase));
373         LinkBuffer linkBuffer(jit, stubBaseCodePtr, stubSize);
374         // We don&#39;t use FINALIZE_CODE() for two reasons.
375         // The first is that we don&#39;t want the writeable address, as disassembled instructions,
376         // to appear in the console or anywhere in memory, via the PrintStream buffer.
377         // The second is we can&#39;t guarantee that the code is readable when using the
378         // asyncDisassembly option as our caller will set our pages execute only.
379         return linkBuffer.finalizeCodeWithoutDisassembly&lt;JITThunkPtrTag&gt;();
380     }
381 #else // not CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
382     static void genericWriteToJITRegion(off_t offset, const void* data, size_t dataSize)
383     {
<span class="line-modified">384         memcpy((void*)(startOfFixedWritableMemoryPool + offset), data, dataSize);</span>
385     }
386 
387     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; jitWriteThunkGenerator(void* address, void*, size_t)
388     {
<span class="line-modified">389         startOfFixedWritableMemoryPool = reinterpret_cast&lt;uintptr_t&gt;(address);</span>
390         void* function = reinterpret_cast&lt;void*&gt;(&amp;genericWriteToJITRegion);
391 #if CPU(ARM_THUMB2)
392         // Handle thumb offset
393         uintptr_t functionAsInt = reinterpret_cast&lt;uintptr_t&gt;(function);
394         functionAsInt -= 1;
395         function = reinterpret_cast&lt;void*&gt;(functionAsInt);
396 #endif
397         auto codePtr = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;(tagCFunctionPtr&lt;JITThunkPtrTag&gt;(function));
398         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(codePtr);
399     }
400 #endif // CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
401 
402 #else // OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
403     void initializeSeparatedWXHeaps(void*, size_t, void*, size_t)
404     {
405     }
406 #endif
407 
408 private:
409     PageReservation m_reservation;
<span class="line-removed">410     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryStart;</span>
<span class="line-removed">411     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryEnd;</span>
412 };
413 
414 FixedVMPoolExecutableAllocator::~FixedVMPoolExecutableAllocator()
415 {
416     m_reservation.deallocate();
417 }
418 



419 void ExecutableAllocator::initializeUnderlyingAllocator()
420 {
<span class="line-modified">421     ASSERT(!allocator);</span>
<span class="line-modified">422     allocator = new FixedVMPoolExecutableAllocator();</span>
<span class="line-modified">423     CodeProfiling::notifyAllocator(allocator);</span>

424 }
425 
426 bool ExecutableAllocator::isValid() const
427 {

428     if (!allocator)
429         return Base::isValid();
430     return !!allocator-&gt;bytesReserved();
431 }
432 
433 bool ExecutableAllocator::underMemoryPressure()
434 {

435     if (!allocator)
436         return Base::underMemoryPressure();
437     return allocator-&gt;bytesAllocated() &gt; allocator-&gt;bytesReserved() / 2;
438 }
439 
440 double ExecutableAllocator::memoryPressureMultiplier(size_t addedMemoryUsage)
441 {

442     if (!allocator)
443         return Base::memoryPressureMultiplier(addedMemoryUsage);
444     ASSERT(allocator-&gt;bytesAllocated() &lt;= allocator-&gt;bytesReserved());
445     size_t bytesAllocated = allocator-&gt;bytesAllocated() + addedMemoryUsage;
446     size_t bytesAvailable = static_cast&lt;size_t&gt;(
447         allocator-&gt;bytesReserved() * (1 - executablePoolReservationFraction));
448     if (bytesAllocated &gt;= bytesAvailable)
449         bytesAllocated = bytesAvailable;
450     double result = 1.0;
451     size_t divisor = bytesAvailable - bytesAllocated;
452     if (divisor)
453         result = static_cast&lt;double&gt;(bytesAvailable) / divisor;
454     if (result &lt; 1.0)
455         result = 1.0;
456     return result;
457 }
458 
459 RefPtr&lt;ExecutableMemoryHandle&gt; ExecutableAllocator::allocate(size_t sizeInBytes, void* ownerUID, JITCompilationEffort effort)
460 {

461     if (!allocator)
462         return Base::allocate(sizeInBytes, ownerUID, effort);
463     if (Options::logExecutableAllocation()) {
464         MetaAllocator::Statistics stats = allocator-&gt;currentStatistics();
465         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with &quot;, stats.bytesAllocated, &quot; bytes allocated, &quot;, stats.bytesReserved, &quot; bytes reserved, and &quot;, stats.bytesCommitted, &quot; committed.\n&quot;);
466     }
467 
468     if (effort != JITCompilationCanFail &amp;&amp; Options::reportMustSucceedExecutableAllocations()) {
469         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with JITCompilationMustSucceed.\n&quot;);
470         WTFReportBacktrace();
471     }
472 
473     if (effort == JITCompilationCanFail
474         &amp;&amp; doExecutableAllocationFuzzingIfEnabled() == PretendToFailExecutableAllocation)
475         return nullptr;
476 
477     if (effort == JITCompilationCanFail) {
478         // Don&#39;t allow allocations if we are down to reserve.
479         size_t bytesAllocated = allocator-&gt;bytesAllocated() + sizeInBytes;
480         size_t bytesAvailable = static_cast&lt;size_t&gt;(
481             allocator-&gt;bytesReserved() * (1 - executablePoolReservationFraction));
482         if (bytesAllocated &gt; bytesAvailable) {
483             if (Options::logExecutableAllocation())
484                 dataLog(&quot;Allocation failed because bytes allocated &quot;, bytesAllocated,  &quot; &gt; &quot;, bytesAvailable, &quot; bytes available.\n&quot;);
485             return nullptr;
486         }
487     }
488 
489     RefPtr&lt;ExecutableMemoryHandle&gt; result = allocator-&gt;allocate(sizeInBytes, ownerUID);
490     if (!result) {
491         if (effort != JITCompilationCanFail) {
492             dataLog(&quot;Ran out of executable memory while allocating &quot;, sizeInBytes, &quot; bytes.\n&quot;);
493             CRASH();
494         }
495         return nullptr;
496     }
497 
<span class="line-removed">498 #if CPU(ARM64E)</span>
499     void* start = allocator-&gt;memoryStart();
500     void* end = allocator-&gt;memoryEnd();
501     void* resultStart = result-&gt;start().untaggedPtr();
502     void* resultEnd = result-&gt;end().untaggedPtr();
503     RELEASE_ASSERT(start &lt;= resultStart &amp;&amp; resultStart &lt; end);
504     RELEASE_ASSERT(start &lt; resultEnd &amp;&amp; resultEnd &lt;= end);
<span class="line-removed">505 #endif</span>
506     return result;
507 }
508 
509 bool ExecutableAllocator::isValidExecutableMemory(const AbstractLocker&amp; locker, void* address)
510 {

511     if (!allocator)
512         return Base::isValidExecutableMemory(locker, address);
513     return allocator-&gt;isInAllocatedMemory(locker, address);
514 }
515 
516 Lock&amp; ExecutableAllocator::getLock() const
517 {

518     if (!allocator)
519         return Base::getLock();
520     return allocator-&gt;getLock();
521 }
522 
523 size_t ExecutableAllocator::committedByteCount()
524 {

525     if (!allocator)
526         return Base::committedByteCount();
527     return allocator-&gt;bytesCommitted();
528 }
529 
530 #if ENABLE(META_ALLOCATOR_PROFILE)
531 void ExecutableAllocator::dumpProfile()
532 {

533     if (!allocator)
534         return;
535     allocator-&gt;dumpProfile();
536 }
537 #endif
538 
539 void* startOfFixedExecutableMemoryPoolImpl()
540 {

541     if (!allocator)
542         return nullptr;
543     return allocator-&gt;memoryStart();
544 }
545 
546 void* endOfFixedExecutableMemoryPoolImpl()
547 {

548     if (!allocator)
549         return nullptr;
550     return allocator-&gt;memoryEnd();
551 }
552 
553 bool isJITPC(void* pc)
554 {

555     return allocator &amp;&amp; allocator-&gt;isJITPC(pc);
556 }
557 
558 void dumpJITMemory(const void* dst, const void* src, size_t size)
559 {
<span class="line-modified">560     ASSERT(Options::dumpJITMemoryPath());</span>
561 
562 #if OS(DARWIN)
563     static int fd = -1;
564     static uint8_t* buffer;
565     static constexpr size_t bufferSize = fixedExecutableMemoryPoolSize;
566     static size_t offset = 0;
567     static Lock dumpJITMemoryLock;
568     static bool needsToFlush = false;
569     static auto flush = [](const AbstractLocker&amp;) {
570         if (fd == -1) {
571             String path = Options::dumpJITMemoryPath();
572             path = path.replace(&quot;%pid&quot;, String::number(getCurrentProcessID()));
573             fd = open(FileSystem::fileSystemRepresentation(path).data(), O_CREAT | O_TRUNC | O_APPEND | O_WRONLY | O_EXLOCK | O_NONBLOCK, 0666);
574             RELEASE_ASSERT(fd != -1);
575         }
576         write(fd, buffer, offset);
577         offset = 0;
578         needsToFlush = false;
579     };
580 
</pre>
<hr />
<pre>
618     uint64_t size64 = size;
619     TraceScope(DumpJITMemoryStart, DumpJITMemoryStop, time, dst64, size64);
620     write(locker, &amp;time, sizeof(time));
621     write(locker, &amp;dst64, sizeof(dst64));
622     write(locker, &amp;size64, sizeof(size64));
623     write(locker, src, size);
624 #else
625     UNUSED_PARAM(dst);
626     UNUSED_PARAM(src);
627     UNUSED_PARAM(size);
628     RELEASE_ASSERT_NOT_REACHED();
629 #endif
630 }
631 
632 } // namespace JSC
633 
634 #endif // ENABLE(JIT)
635 
636 namespace JSC {
637 
<span class="line-modified">638 static ExecutableAllocator* executableAllocator;</span>
<span class="line-modified">639 </span>

640 void ExecutableAllocator::initialize()
641 {
<span class="line-modified">642     executableAllocator = new ExecutableAllocator;</span>

643 }
644 
645 ExecutableAllocator&amp; ExecutableAllocator::singleton()
646 {
<span class="line-modified">647     ASSERT(executableAllocator);</span>
<span class="line-modified">648     return *executableAllocator;</span>
649 }
650 
651 } // namespace JSC
</pre>
</td>
<td>
<hr />
<pre>
 75      mach_vm_address_t *target_address,
 76      mach_vm_size_t size,
 77      mach_vm_offset_t mask,
 78      int flags,
 79      vm_map_t src_task,
 80      mach_vm_address_t src_address,
 81      boolean_t copy,
 82      vm_prot_t *cur_protection,
 83      vm_prot_t *max_protection,
 84      vm_inherit_t inheritance
 85      );
 86 }
 87 
 88 #endif
 89 
 90 namespace JSC {
 91 
 92 using namespace WTF;
 93 
 94 #if defined(FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB) &amp;&amp; FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB &gt; 0
<span class="line-modified"> 95 static constexpr size_t fixedExecutableMemoryPoolSize = FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB * 1024 * 1024;</span>
 96 #elif CPU(ARM)
<span class="line-modified"> 97 static constexpr size_t fixedExecutableMemoryPoolSize = 16 * 1024 * 1024;</span>
 98 #elif CPU(ARM64)
<span class="line-modified"> 99 static constexpr size_t fixedExecutableMemoryPoolSize = 128 * 1024 * 1024;</span>
100 #elif CPU(X86_64)
<span class="line-modified">101 static constexpr size_t fixedExecutableMemoryPoolSize = 1024 * 1024 * 1024;</span>
102 #else
<span class="line-modified">103 static constexpr size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;</span>
104 #endif
105 
106 #if CPU(ARM)
<span class="line-modified">107 static constexpr double executablePoolReservationFraction = 0.15;</span>
108 #else
<span class="line-modified">109 static constexpr double executablePoolReservationFraction = 0.25;</span>
110 #endif
111 













112 static bool isJITEnabled()
113 {
<span class="line-added">114     bool jitEnabled = !g_jscConfig.jitDisabled;</span>
115 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
<span class="line-modified">116     return processHasEntitlement(&quot;dynamic-codesigning&quot;) &amp;&amp; jitEnabled;</span>
117 #else
<span class="line-modified">118     return jitEnabled;</span>
119 #endif
120 }
121 
122 void ExecutableAllocator::setJITEnabled(bool enabled)
123 {
<span class="line-modified">124     bool jitEnabled = !g_jscConfig.jitDisabled;</span>
<span class="line-modified">125     ASSERT(!g_jscConfig.fixedVMPoolExecutableAllocator);</span>
<span class="line-added">126     if (jitEnabled == enabled)</span>
127         return;
128 
<span class="line-modified">129     g_jscConfig.jitDisabled = !enabled;</span>
130 
131 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
132     if (!enabled) {
<span class="line-added">133         // Because of an OS quirk, even after the JIT region has been unmapped,</span>
<span class="line-added">134         // the OS thinks that region is reserved, and as such, can cause Gigacage</span>
<span class="line-added">135         // allocation to fail. We work around this by initializing the Gigacage</span>
<span class="line-added">136         // first.</span>
<span class="line-added">137         // Note: when called, setJITEnabled() is always called extra early in the</span>
<span class="line-added">138         // process bootstrap. Under normal operation (when setJITEnabled() isn&#39;t</span>
<span class="line-added">139         // called at all), we will naturally initialize the Gigacage before we</span>
<span class="line-added">140         // allocate the JIT region. Hence, this workaround is merely ensuring the</span>
<span class="line-added">141         // same behavior of allocation ordering.</span>
<span class="line-added">142         Gigacage::ensureGigacage();</span>
<span class="line-added">143 </span>
144         constexpr size_t size = 1;
145         constexpr int protection = PROT_READ | PROT_WRITE | PROT_EXEC;
146         constexpr int flags = MAP_PRIVATE | MAP_ANON | MAP_JIT;
147         constexpr int fd = OSAllocator::JSJITCodePages;
148         void* allocation = mmap(nullptr, size, protection, flags, fd, 0);
149         const void* executableMemoryAllocationFailure = reinterpret_cast&lt;void*&gt;(-1);
150         RELEASE_ASSERT_WITH_MESSAGE(allocation &amp;&amp; allocation != executableMemoryAllocationFailure, &quot;We should not have allocated executable memory before disabling the JIT.&quot;);
151         RELEASE_ASSERT_WITH_MESSAGE(!munmap(allocation, size), &quot;Unmapping executable memory should succeed so we do not have any executable memory in the address space&quot;);
152         RELEASE_ASSERT_WITH_MESSAGE(mmap(nullptr, size, protection, flags, fd, 0) == executableMemoryAllocationFailure, &quot;Allocating executable memory should fail after setJITEnabled(false) is called.&quot;);
153     }
154 #endif
155 }
156 
157 class FixedVMPoolExecutableAllocator final : public MetaAllocator {
158     WTF_MAKE_FAST_ALLOCATED;
159 public:
160     FixedVMPoolExecutableAllocator()
161         : MetaAllocator(jitAllocationGranule) // round up all allocations to 32 bytes
162     {
163         if (!isJITEnabled())
</pre>
<hr />
<pre>
176             // This makes the following JIT code logging broken and some of JIT code is not recorded correctly.
177             // To avoid this problem, we use committed reservation if we need perf JITDump logging.
178             if (Options::logJITCodeForPerf())
179                 return PageReservation::reserveAndCommitWithGuardPages(reservationSize, OSAllocator::JSJITCodePages, EXECUTABLE_POOL_WRITABLE, true);
180 #endif
181             return PageReservation::reserveWithGuardPages(reservationSize, OSAllocator::JSJITCodePages, EXECUTABLE_POOL_WRITABLE, true);
182         };
183 
184         m_reservation = tryCreatePageReservation(reservationSize);
185         if (m_reservation) {
186             ASSERT(m_reservation.size() == reservationSize);
187             void* reservationBase = m_reservation.base();
188 
189 #if ENABLE(FAST_JIT_PERMISSIONS) &amp;&amp; !ENABLE(SEPARATED_WX_HEAP)
190             RELEASE_ASSERT(os_thread_self_restrict_rwx_is_supported());
191             os_thread_self_restrict_rwx_to_rx();
192 
193 #else // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
194 #if ENABLE(FAST_JIT_PERMISSIONS)
195             if (os_thread_self_restrict_rwx_is_supported()) {
<span class="line-modified">196                 g_jscConfig.useFastPermisionsJITCopy = true;</span>
197                 os_thread_self_restrict_rwx_to_rx();
198             } else
199 #endif
200             if (Options::useSeparatedWXHeap()) {
201                 // First page of our JIT allocation is reserved.
202                 ASSERT(reservationSize &gt;= pageSize() * 2);
203                 reservationBase = (void*)((uintptr_t)reservationBase + pageSize());
204                 reservationSize -= pageSize();
205                 initializeSeparatedWXHeaps(m_reservation.base(), pageSize(), reservationBase, reservationSize);
206             }
207 #endif // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
208 
209             addFreshFreeSpace(reservationBase, reservationSize);
210 
211             ASSERT(bytesReserved() == reservationSize); // Since our executable memory is fixed-sized, bytesReserved is never changed after initialization.
212 
213             void* reservationEnd = reinterpret_cast&lt;uint8_t*&gt;(reservationBase) + reservationSize;
214 
<span class="line-modified">215             g_jscConfig.startExecutableMemory = tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationBase);</span>
<span class="line-modified">216             g_jscConfig.endExecutableMemory = tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationEnd);</span>
217         }
218     }
219 
220     virtual ~FixedVMPoolExecutableAllocator();
221 
<span class="line-modified">222     void* memoryStart() { return untagCodePtr&lt;ExecutableMemoryPtrTag&gt;(g_jscConfig.startExecutableMemory); }</span>
<span class="line-modified">223     void* memoryEnd() { return untagCodePtr&lt;ExecutableMemoryPtrTag&gt;(g_jscConfig.endExecutableMemory); }</span>
224     bool isJITPC(void* pc) { return memoryStart() &lt;= pc &amp;&amp; pc &lt; memoryEnd(); }
225 
226 protected:
227     FreeSpacePtr allocateNewSpace(size_t&amp;) override
228     {
229         // We&#39;re operating in a fixed pool, so new allocation is always prohibited.
230         return nullptr;
231     }
232 
233     void notifyNeedPage(void* page, size_t count) override
234     {
235 #if USE(MADV_FREE_FOR_JIT_MEMORY)
236         UNUSED_PARAM(page);
237         UNUSED_PARAM(count);
238 #else
239         m_reservation.commit(page, pageSize() * count);
240 #endif
241     }
242 
243     void notifyPageIsFree(void* page, size_t count) override
</pre>
<hr />
<pre>
285         int result = 0;
286 
287 #if USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
288         // Prevent reading the write thunk code.
289         result = vm_protect(mach_task_self(), reinterpret_cast&lt;vm_address_t&gt;(stubBase), stubSize, true, VM_PROT_EXECUTE);
290         RELEASE_ASSERT(!result);
291 #endif
292 
293         // Prevent writing into the executable JIT mapping.
294         result = vm_protect(mach_task_self(), reinterpret_cast&lt;vm_address_t&gt;(jitBase), jitSize, true, VM_PROT_READ | VM_PROT_EXECUTE);
295         RELEASE_ASSERT(!result);
296 
297         // Prevent execution in the writable JIT mapping.
298         result = vm_protect(mach_task_self(), static_cast&lt;vm_address_t&gt;(writableAddr), jitSize, true, VM_PROT_READ | VM_PROT_WRITE);
299         RELEASE_ASSERT(!result);
300 
301         // Zero out writableAddr to avoid leaking the address of the writable mapping.
302         memset_s(&amp;writableAddr, sizeof(writableAddr), 0, sizeof(writableAddr));
303 
304 #if ENABLE(SEPARATED_WX_HEAP)
<span class="line-modified">305         g_jscConfig.jitWriteSeparateHeaps = reinterpret_cast&lt;JITWriteSeparateHeapsFunction&gt;(writeThunk.code().executableAddress());</span>
306 #endif
307     }
308 
309 #if CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
310     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; jitWriteThunkGenerator(void* writableAddr, void* stubBase, size_t stubSize)
311     {
312         using namespace ARM64Registers;
313         using TrustedImm32 = MacroAssembler::TrustedImm32;
314 
315         MacroAssembler jit;
316 
317         jit.tagReturnAddress();
318         jit.move(MacroAssembler::TrustedImmPtr(writableAddr), x7);
319         jit.addPtr(x7, x0);
320 
321         jit.move(x0, x3);
322         MacroAssembler::Jump smallCopy = jit.branch64(MacroAssembler::Below, x2, MacroAssembler::TrustedImm64(64));
323 
324         jit.add64(TrustedImm32(32), x3);
325         jit.and64(TrustedImm32(-32), x3);
</pre>
<hr />
<pre>
364         MacroAssembler::Jump local2 = jit.branchAdd64(MacroAssembler::Equal, TrustedImm32(8), x2);
365         MacroAssembler::Label local1 = jit.label();
366         jit.load8(x1, PostIndex(1), x6);
367         jit.store8(x6, x3, PostIndex(1));
368         jit.branchSub64(MacroAssembler::NotEqual, TrustedImm32(1), x2).linkTo(local1, &amp;jit);
369         local2.link(&amp;jit);
370         jit.ret();
371 
372         auto stubBaseCodePtr = MacroAssemblerCodePtr&lt;LinkBufferPtrTag&gt;(tagCodePtr&lt;LinkBufferPtrTag&gt;(stubBase));
373         LinkBuffer linkBuffer(jit, stubBaseCodePtr, stubSize);
374         // We don&#39;t use FINALIZE_CODE() for two reasons.
375         // The first is that we don&#39;t want the writeable address, as disassembled instructions,
376         // to appear in the console or anywhere in memory, via the PrintStream buffer.
377         // The second is we can&#39;t guarantee that the code is readable when using the
378         // asyncDisassembly option as our caller will set our pages execute only.
379         return linkBuffer.finalizeCodeWithoutDisassembly&lt;JITThunkPtrTag&gt;();
380     }
381 #else // not CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
382     static void genericWriteToJITRegion(off_t offset, const void* data, size_t dataSize)
383     {
<span class="line-modified">384         memcpy((void*)(g_jscConfig.startOfFixedWritableMemoryPool + offset), data, dataSize);</span>
385     }
386 
387     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; jitWriteThunkGenerator(void* address, void*, size_t)
388     {
<span class="line-modified">389         g_jscConfig.startOfFixedWritableMemoryPool = reinterpret_cast&lt;uintptr_t&gt;(address);</span>
390         void* function = reinterpret_cast&lt;void*&gt;(&amp;genericWriteToJITRegion);
391 #if CPU(ARM_THUMB2)
392         // Handle thumb offset
393         uintptr_t functionAsInt = reinterpret_cast&lt;uintptr_t&gt;(function);
394         functionAsInt -= 1;
395         function = reinterpret_cast&lt;void*&gt;(functionAsInt);
396 #endif
397         auto codePtr = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;(tagCFunctionPtr&lt;JITThunkPtrTag&gt;(function));
398         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(codePtr);
399     }
400 #endif // CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
401 
402 #else // OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
403     void initializeSeparatedWXHeaps(void*, size_t, void*, size_t)
404     {
405     }
406 #endif
407 
408 private:
409     PageReservation m_reservation;


410 };
411 
412 FixedVMPoolExecutableAllocator::~FixedVMPoolExecutableAllocator()
413 {
414     m_reservation.deallocate();
415 }
416 
<span class="line-added">417 // Keep this pointer in a mutable global variable to help Leaks find it.</span>
<span class="line-added">418 // But we do not use this pointer.</span>
<span class="line-added">419 static FixedVMPoolExecutableAllocator* globalFixedVMPoolExecutableAllocatorToWorkAroundLeaks = nullptr;</span>
420 void ExecutableAllocator::initializeUnderlyingAllocator()
421 {
<span class="line-modified">422     RELEASE_ASSERT(!g_jscConfig.fixedVMPoolExecutableAllocator);</span>
<span class="line-modified">423     g_jscConfig.fixedVMPoolExecutableAllocator = new FixedVMPoolExecutableAllocator();</span>
<span class="line-modified">424     globalFixedVMPoolExecutableAllocatorToWorkAroundLeaks = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
<span class="line-added">425     CodeProfiling::notifyAllocator(g_jscConfig.fixedVMPoolExecutableAllocator);</span>
426 }
427 
428 bool ExecutableAllocator::isValid() const
429 {
<span class="line-added">430     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
431     if (!allocator)
432         return Base::isValid();
433     return !!allocator-&gt;bytesReserved();
434 }
435 
436 bool ExecutableAllocator::underMemoryPressure()
437 {
<span class="line-added">438     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
439     if (!allocator)
440         return Base::underMemoryPressure();
441     return allocator-&gt;bytesAllocated() &gt; allocator-&gt;bytesReserved() / 2;
442 }
443 
444 double ExecutableAllocator::memoryPressureMultiplier(size_t addedMemoryUsage)
445 {
<span class="line-added">446     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
447     if (!allocator)
448         return Base::memoryPressureMultiplier(addedMemoryUsage);
449     ASSERT(allocator-&gt;bytesAllocated() &lt;= allocator-&gt;bytesReserved());
450     size_t bytesAllocated = allocator-&gt;bytesAllocated() + addedMemoryUsage;
451     size_t bytesAvailable = static_cast&lt;size_t&gt;(
452         allocator-&gt;bytesReserved() * (1 - executablePoolReservationFraction));
453     if (bytesAllocated &gt;= bytesAvailable)
454         bytesAllocated = bytesAvailable;
455     double result = 1.0;
456     size_t divisor = bytesAvailable - bytesAllocated;
457     if (divisor)
458         result = static_cast&lt;double&gt;(bytesAvailable) / divisor;
459     if (result &lt; 1.0)
460         result = 1.0;
461     return result;
462 }
463 
464 RefPtr&lt;ExecutableMemoryHandle&gt; ExecutableAllocator::allocate(size_t sizeInBytes, void* ownerUID, JITCompilationEffort effort)
465 {
<span class="line-added">466     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
467     if (!allocator)
468         return Base::allocate(sizeInBytes, ownerUID, effort);
469     if (Options::logExecutableAllocation()) {
470         MetaAllocator::Statistics stats = allocator-&gt;currentStatistics();
471         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with &quot;, stats.bytesAllocated, &quot; bytes allocated, &quot;, stats.bytesReserved, &quot; bytes reserved, and &quot;, stats.bytesCommitted, &quot; committed.\n&quot;);
472     }
473 
474     if (effort != JITCompilationCanFail &amp;&amp; Options::reportMustSucceedExecutableAllocations()) {
475         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with JITCompilationMustSucceed.\n&quot;);
476         WTFReportBacktrace();
477     }
478 
479     if (effort == JITCompilationCanFail
480         &amp;&amp; doExecutableAllocationFuzzingIfEnabled() == PretendToFailExecutableAllocation)
481         return nullptr;
482 
483     if (effort == JITCompilationCanFail) {
484         // Don&#39;t allow allocations if we are down to reserve.
485         size_t bytesAllocated = allocator-&gt;bytesAllocated() + sizeInBytes;
486         size_t bytesAvailable = static_cast&lt;size_t&gt;(
487             allocator-&gt;bytesReserved() * (1 - executablePoolReservationFraction));
488         if (bytesAllocated &gt; bytesAvailable) {
489             if (Options::logExecutableAllocation())
490                 dataLog(&quot;Allocation failed because bytes allocated &quot;, bytesAllocated,  &quot; &gt; &quot;, bytesAvailable, &quot; bytes available.\n&quot;);
491             return nullptr;
492         }
493     }
494 
495     RefPtr&lt;ExecutableMemoryHandle&gt; result = allocator-&gt;allocate(sizeInBytes, ownerUID);
496     if (!result) {
497         if (effort != JITCompilationCanFail) {
498             dataLog(&quot;Ran out of executable memory while allocating &quot;, sizeInBytes, &quot; bytes.\n&quot;);
499             CRASH();
500         }
501         return nullptr;
502     }
503 

504     void* start = allocator-&gt;memoryStart();
505     void* end = allocator-&gt;memoryEnd();
506     void* resultStart = result-&gt;start().untaggedPtr();
507     void* resultEnd = result-&gt;end().untaggedPtr();
508     RELEASE_ASSERT(start &lt;= resultStart &amp;&amp; resultStart &lt; end);
509     RELEASE_ASSERT(start &lt; resultEnd &amp;&amp; resultEnd &lt;= end);

510     return result;
511 }
512 
513 bool ExecutableAllocator::isValidExecutableMemory(const AbstractLocker&amp; locker, void* address)
514 {
<span class="line-added">515     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
516     if (!allocator)
517         return Base::isValidExecutableMemory(locker, address);
518     return allocator-&gt;isInAllocatedMemory(locker, address);
519 }
520 
521 Lock&amp; ExecutableAllocator::getLock() const
522 {
<span class="line-added">523     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
524     if (!allocator)
525         return Base::getLock();
526     return allocator-&gt;getLock();
527 }
528 
529 size_t ExecutableAllocator::committedByteCount()
530 {
<span class="line-added">531     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
532     if (!allocator)
533         return Base::committedByteCount();
534     return allocator-&gt;bytesCommitted();
535 }
536 
537 #if ENABLE(META_ALLOCATOR_PROFILE)
538 void ExecutableAllocator::dumpProfile()
539 {
<span class="line-added">540     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
541     if (!allocator)
542         return;
543     allocator-&gt;dumpProfile();
544 }
545 #endif
546 
547 void* startOfFixedExecutableMemoryPoolImpl()
548 {
<span class="line-added">549     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
550     if (!allocator)
551         return nullptr;
552     return allocator-&gt;memoryStart();
553 }
554 
555 void* endOfFixedExecutableMemoryPoolImpl()
556 {
<span class="line-added">557     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
558     if (!allocator)
559         return nullptr;
560     return allocator-&gt;memoryEnd();
561 }
562 
563 bool isJITPC(void* pc)
564 {
<span class="line-added">565     auto* allocator = g_jscConfig.fixedVMPoolExecutableAllocator;</span>
566     return allocator &amp;&amp; allocator-&gt;isJITPC(pc);
567 }
568 
569 void dumpJITMemory(const void* dst, const void* src, size_t size)
570 {
<span class="line-modified">571     RELEASE_ASSERT(Options::dumpJITMemoryPath());</span>
572 
573 #if OS(DARWIN)
574     static int fd = -1;
575     static uint8_t* buffer;
576     static constexpr size_t bufferSize = fixedExecutableMemoryPoolSize;
577     static size_t offset = 0;
578     static Lock dumpJITMemoryLock;
579     static bool needsToFlush = false;
580     static auto flush = [](const AbstractLocker&amp;) {
581         if (fd == -1) {
582             String path = Options::dumpJITMemoryPath();
583             path = path.replace(&quot;%pid&quot;, String::number(getCurrentProcessID()));
584             fd = open(FileSystem::fileSystemRepresentation(path).data(), O_CREAT | O_TRUNC | O_APPEND | O_WRONLY | O_EXLOCK | O_NONBLOCK, 0666);
585             RELEASE_ASSERT(fd != -1);
586         }
587         write(fd, buffer, offset);
588         offset = 0;
589         needsToFlush = false;
590     };
591 
</pre>
<hr />
<pre>
629     uint64_t size64 = size;
630     TraceScope(DumpJITMemoryStart, DumpJITMemoryStop, time, dst64, size64);
631     write(locker, &amp;time, sizeof(time));
632     write(locker, &amp;dst64, sizeof(dst64));
633     write(locker, &amp;size64, sizeof(size64));
634     write(locker, src, size);
635 #else
636     UNUSED_PARAM(dst);
637     UNUSED_PARAM(src);
638     UNUSED_PARAM(size);
639     RELEASE_ASSERT_NOT_REACHED();
640 #endif
641 }
642 
643 } // namespace JSC
644 
645 #endif // ENABLE(JIT)
646 
647 namespace JSC {
648 
<span class="line-modified">649 // Keep this pointer in a mutable global variable to help Leaks find it.</span>
<span class="line-modified">650 // But we do not use this pointer.</span>
<span class="line-added">651 static ExecutableAllocator* globalExecutableAllocatorToWorkAroundLeaks = nullptr;</span>
652 void ExecutableAllocator::initialize()
653 {
<span class="line-modified">654     g_jscConfig.executableAllocator = new ExecutableAllocator;</span>
<span class="line-added">655     globalExecutableAllocatorToWorkAroundLeaks = g_jscConfig.executableAllocator;</span>
656 }
657 
658 ExecutableAllocator&amp; ExecutableAllocator::singleton()
659 {
<span class="line-modified">660     ASSERT(g_jscConfig.executableAllocator);</span>
<span class="line-modified">661     return *g_jscConfig.executableAllocator;</span>
662 }
663 
664 } // namespace JSC
</pre>
</td>
</tr>
</table>
<center><a href="CallFrameShuffler64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ExecutableAllocator.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>