diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp
@@ -72,27 +72,45 @@
     MacroAssembler::repatchCall(
         CodeLocationCall<NoPtrTag>(MacroAssemblerCodePtr<NoPtrTag>(returnAddress)),
         newCalleeFunction.retagged<OperationPtrTag>());
 }
 
-JIT::JIT(VM& vm, CodeBlock* codeBlock, unsigned loopOSREntryBytecodeOffset)
+JIT::JIT(VM& vm, CodeBlock* codeBlock, BytecodeIndex loopOSREntryBytecodeIndex)
     : JSInterfaceJIT(&vm, codeBlock)
     , m_interpreter(vm.interpreter)
     , m_labels(codeBlock ? codeBlock->instructions().size() : 0)
-    , m_bytecodeOffset(std::numeric_limits<unsigned>::max())
     , m_pcToCodeOriginMapBuilder(vm)
     , m_canBeOptimized(false)
     , m_shouldEmitProfiling(false)
-    , m_shouldUseIndexMasking(Options::enableSpectreMitigations())
-    , m_loopOSREntryBytecodeOffset(loopOSREntryBytecodeOffset)
+    , m_loopOSREntryBytecodeIndex(loopOSREntryBytecodeIndex)
 {
 }
 
 JIT::~JIT()
 {
 }
 
+#if ENABLE(DFG_JIT)
+void JIT::emitEnterOptimizationCheck()
+{
+    if (!canBeOptimized())
+        return;
+
+    JumpList skipOptimize;
+
+    skipOptimize.append(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock->addressOfJITExecuteCounter())));
+    ASSERT(!m_bytecodeIndex.offset());
+
+    copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
+
+    callOperation(operationOptimize, &vm(), m_bytecodeIndex.asBits());
+    skipOptimize.append(branchTestPtr(Zero, returnValueGPR));
+    farJump(returnValueGPR, GPRInfo::callFrameRegister);
+    skipOptimize.link(this);
+}
+#endif
+
 void JIT::emitNotifyWrite(WatchpointSet* set)
 {
     if (!set || set->state() == IsInvalidated) {
         addSlowCase(Jump());
         return;
@@ -106,38 +124,43 @@
     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 }
 
 void JIT::assertStackPointerOffset()
 {
-    if (ASSERT_DISABLED)
+    if (!ASSERT_ENABLED)
         return;
 
     addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
     Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
     breakpoint();
     ok.link(this);
 }
 
 #define NEXT_OPCODE(name) \
-    m_bytecodeOffset += currentInstruction->size(); \
+    m_bytecodeIndex = BytecodeIndex(m_bytecodeIndex.offset() + currentInstruction->size()); \
     break;
 
+#define NEXT_OPCODE_IN_MAIN(name) \
+    if (previousSlowCasesSize != m_slowCases.size()) \
+        ++m_bytecodeCountHavingSlowCase; \
+    NEXT_OPCODE(name)
+
 #define DEFINE_SLOW_OP(name) \
     case op_##name: { \
-        if (m_bytecodeOffset >= startBytecodeOffset) { \
+        if (m_bytecodeIndex >= startBytecodeIndex) { \
             JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
             slowPathCall.call(); \
         } \
-        NEXT_OPCODE(op_##name); \
+        NEXT_OPCODE_IN_MAIN(op_##name); \
     }
 
 #define DEFINE_OP(name) \
     case name: { \
-        if (m_bytecodeOffset >= startBytecodeOffset) { \
+        if (m_bytecodeIndex >= startBytecodeIndex) { \
             emit_##name(currentInstruction); \
         } \
-        NEXT_OPCODE(name); \
+        NEXT_OPCODE_IN_MAIN(name); \
     }
 
 #define DEFINE_SLOWCASE_OP(name) \
     case name: { \
         emitSlow_##name(currentInstruction, iter); \
@@ -170,88 +193,91 @@
     unsigned instructionCount = m_codeBlock->instructions().size();
 
     m_callLinkInfoIndex = 0;
 
     VM& vm = m_codeBlock->vm();
-    unsigned startBytecodeOffset = 0;
-    if (m_loopOSREntryBytecodeOffset && (m_codeBlock->inherits<ProgramCodeBlock>(vm) || m_codeBlock->inherits<ModuleProgramCodeBlock>(vm))) {
+    BytecodeIndex startBytecodeIndex(0);
+    if (m_loopOSREntryBytecodeIndex && (m_codeBlock->inherits<ProgramCodeBlock>(vm) || m_codeBlock->inherits<ModuleProgramCodeBlock>(vm))) {
         // We can only do this optimization because we execute ProgramCodeBlock's exactly once.
         // This optimization would be invalid otherwise. When the LLInt determines it wants to
         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
         // was executing at when it kicked off our compilation. We only need to compile code for
         // anything reachable from that bytecode offset.
 
         // We only bother building the bytecode graph if it could save time and executable
         // memory. We pick an arbitrary offset where we deem this is profitable.
-        if (m_loopOSREntryBytecodeOffset >= 200) {
+        if (m_loopOSREntryBytecodeIndex.offset() >= 200) {
             // As a simplification, we don't find all bytecode ranges that are unreachable.
             // Instead, we just find the minimum bytecode offset that is reachable, and
             // compile code from that bytecode offset onwards.
 
             BytecodeGraph graph(m_codeBlock, m_codeBlock->instructions());
-            BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeOffset);
+            BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeIndex.offset());
             RELEASE_ASSERT(block);
 
             GraphNodeWorklist<BytecodeBasicBlock*> worklist;
-            startBytecodeOffset = UINT_MAX;
+            startBytecodeIndex = BytecodeIndex();
             worklist.push(block);
 
             while (BytecodeBasicBlock* block = worklist.pop()) {
-                startBytecodeOffset = std::min(startBytecodeOffset, block->leaderOffset());
-                worklist.pushAll(block->successors());
+                startBytecodeIndex = BytecodeIndex(std::min(startBytecodeIndex.offset(), block->leaderOffset()));
+                for (unsigned successorIndex : block->successors())
+                    worklist.push(&graph[successorIndex]);
 
                 // Also add catch blocks for bytecodes that throw.
                 if (m_codeBlock->numberOfExceptionHandlers()) {
                     for (unsigned bytecodeOffset = block->leaderOffset(); bytecodeOffset < block->leaderOffset() + block->totalLength();) {
                         auto instruction = instructions.at(bytecodeOffset);
-                        if (auto* handler = m_codeBlock->handlerForBytecodeOffset(bytecodeOffset))
+                        if (auto* handler = m_codeBlock->handlerForBytecodeIndex(BytecodeIndex(bytecodeOffset)))
                             worklist.push(graph.findBasicBlockWithLeaderOffset(handler->target));
 
                         bytecodeOffset += instruction->size();
                     }
                 }
             }
         }
     }
 
-    for (m_bytecodeOffset = 0; m_bytecodeOffset < instructionCount; ) {
-        if (m_bytecodeOffset == startBytecodeOffset && startBytecodeOffset > 0) {
+    m_bytecodeCountHavingSlowCase = 0;
+    for (m_bytecodeIndex = BytecodeIndex(0); m_bytecodeIndex.offset() < instructionCount; ) {
+        unsigned previousSlowCasesSize = m_slowCases.size();
+        if (m_bytecodeIndex == startBytecodeIndex && startBytecodeIndex.offset() > 0) {
             // We've proven all bytecode instructions up until here are unreachable.
             // Let's ensure that by crashing if it's ever hit.
             breakpoint();
         }
 
         if (m_disassembler)
-            m_disassembler->setForBytecodeMainPath(m_bytecodeOffset, label());
-        const Instruction* currentInstruction = instructions.at(m_bytecodeOffset).ptr();
-        ASSERT_WITH_MESSAGE(currentInstruction->size(), "privateCompileMainPass gone bad @ %d", m_bytecodeOffset);
+            m_disassembler->setForBytecodeMainPath(m_bytecodeIndex.offset(), label());
+        const Instruction* currentInstruction = instructions.at(m_bytecodeIndex).ptr();
+        ASSERT(currentInstruction->size());
 
-        m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));
+        m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));
 
 #if ENABLE(OPCODE_SAMPLING)
-        if (m_bytecodeOffset > 0) // Avoid the overhead of sampling op_enter twice.
+        if (m_bytecodeIndex > 0) // Avoid the overhead of sampling op_enter twice.
             sampleInstruction(currentInstruction);
 #endif
 
-        m_labels[m_bytecodeOffset] = label();
+        m_labels[m_bytecodeIndex.offset()] = label();
 
         if (JITInternal::verbose)
-            dataLogF("Old JIT emitting code for bc#%u at offset 0x%lx.\n", m_bytecodeOffset, (long)debugOffset());
+            dataLogLn("Old JIT emitting code for ", m_bytecodeIndex, " at offset ", (long)debugOffset());
 
         OpcodeID opcodeID = currentInstruction->opcodeID();
 
         if (UNLIKELY(m_compilation)) {
             add64(
                 TrustedImm32(1),
                 AbsoluteAddress(m_compilation->executionCounterFor(Profiler::OriginStack(Profiler::Origin(
-                    m_compilation->bytecodes(), m_bytecodeOffset)))->address()));
+                    m_compilation->bytecodes(), m_bytecodeIndex)))->address()));
         }
 
         if (Options::eagerlyUpdateTopCallFrame())
             updateTopCallFrame();
 
-        unsigned bytecodeOffset = m_bytecodeOffset;
+        unsigned bytecodeOffset = m_bytecodeIndex.offset();
 #if ENABLE(MASM_PROBE)
         if (UNLIKELY(Options::traceBaselineJITExecution())) {
             CodeBlock* codeBlock = m_codeBlock;
             probe([=] (Probe::Context& ctx) {
                 dataLogLn("JIT [", bytecodeOffset, "] ", opcodeNames[opcodeID], " cfr ", RawPointer(ctx.fp()), " @ ", codeBlock);
@@ -287,11 +313,17 @@
         DEFINE_SLOW_OP(get_property_enumerator)
         DEFINE_SLOW_OP(to_index_string)
         DEFINE_SLOW_OP(create_direct_arguments)
         DEFINE_SLOW_OP(create_scoped_arguments)
         DEFINE_SLOW_OP(create_cloned_arguments)
+        DEFINE_SLOW_OP(create_arguments_butterfly)
         DEFINE_SLOW_OP(create_rest)
+        DEFINE_SLOW_OP(create_promise)
+        DEFINE_SLOW_OP(new_promise)
+        DEFINE_SLOW_OP(create_generator)
+        DEFINE_SLOW_OP(create_async_generator)
+        DEFINE_SLOW_OP(new_generator)
         DEFINE_SLOW_OP(pow)
 
         DEFINE_OP(op_add)
         DEFINE_OP(op_bitnot)
         DEFINE_OP(op_bitand)
@@ -361,10 +393,11 @@
         DEFINE_OP(op_jnstricteq)
         DEFINE_OP(op_jbelow)
         DEFINE_OP(op_jbeloweq)
         DEFINE_OP(op_jtrue)
         DEFINE_OP(op_loop_hint)
+        DEFINE_OP(op_check_traps)
         DEFINE_OP(op_nop)
         DEFINE_OP(op_super_sampler_begin)
         DEFINE_OP(op_super_sampler_end)
         DEFINE_OP(op_lshift)
         DEFINE_OP(op_mod)
@@ -398,10 +431,14 @@
         DEFINE_OP(op_put_getter_by_id)
         DEFINE_OP(op_put_setter_by_id)
         DEFINE_OP(op_put_getter_setter_by_id)
         DEFINE_OP(op_put_getter_by_val)
         DEFINE_OP(op_put_setter_by_val)
+        DEFINE_OP(op_to_property_key)
+
+        DEFINE_OP(op_get_internal_field)
+        DEFINE_OP(op_put_internal_field)
 
         DEFINE_OP(op_ret)
         DEFINE_OP(op_rshift)
         DEFINE_OP(op_unsigned)
         DEFINE_OP(op_urshift)
@@ -411,10 +448,11 @@
         DEFINE_OP(op_switch_char)
         DEFINE_OP(op_switch_imm)
         DEFINE_OP(op_switch_string)
         DEFINE_OP(op_throw)
         DEFINE_OP(op_to_number)
+        DEFINE_OP(op_to_numeric)
         DEFINE_OP(op_to_string)
         DEFINE_OP(op_to_object)
         DEFINE_OP(op_to_primitive)
 
         DEFINE_OP(op_resolve_scope)
@@ -441,11 +479,11 @@
 
     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 
 #ifndef NDEBUG
     // Reset this, in order to guard its use with ASSERTs.
-    m_bytecodeOffset = std::numeric_limits<unsigned>::max();
+    m_bytecodeIndex = BytecodeIndex();
 #endif
 }
 
 void JIT::privateCompileLinkPass()
 {
@@ -456,40 +494,46 @@
 }
 
 void JIT::privateCompileSlowCases()
 {
     m_getByIdIndex = 0;
+    m_getByValIndex = 0;
     m_getByIdWithThisIndex = 0;
     m_putByIdIndex = 0;
     m_inByIdIndex = 0;
     m_instanceOfIndex = 0;
     m_byValInstructionIndex = 0;
     m_callLinkInfoIndex = 0;
 
+    RefCountedArray<RareCaseProfile> rareCaseProfiles;
+    if (shouldEmitProfiling())
+        rareCaseProfiles = RefCountedArray<RareCaseProfile>(m_bytecodeCountHavingSlowCase);
+
+    unsigned bytecodeCountHavingSlowCase = 0;
     for (Vector<SlowCaseEntry>::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
-        m_bytecodeOffset = iter->to;
+        m_bytecodeIndex = iter->to;
 
-        m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));
+        m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));
 
-        unsigned firstTo = m_bytecodeOffset;
+        BytecodeIndex firstTo = m_bytecodeIndex;
 
-        const Instruction* currentInstruction = m_codeBlock->instructions().at(m_bytecodeOffset).ptr();
+        const Instruction* currentInstruction = m_codeBlock->instructions().at(m_bytecodeIndex).ptr();
 
-        RareCaseProfile* rareCaseProfile = 0;
+        RareCaseProfile* rareCaseProfile = nullptr;
         if (shouldEmitProfiling())
-            rareCaseProfile = m_codeBlock->addRareCaseProfile(m_bytecodeOffset);
+            rareCaseProfile = &rareCaseProfiles.at(bytecodeCountHavingSlowCase);
 
         if (JITInternal::verbose)
-            dataLogF("Old JIT emitting slow code for bc#%u at offset 0x%lx.\n", m_bytecodeOffset, (long)debugOffset());
+            dataLogLn("Old JIT emitting slow code for ", m_bytecodeIndex, " at offset ", (long)debugOffset());
 
         if (m_disassembler)
-            m_disassembler->setForBytecodeSlowPath(m_bytecodeOffset, label());
+            m_disassembler->setForBytecodeSlowPath(m_bytecodeIndex.offset(), label());
 
 #if ENABLE(MASM_PROBE)
         if (UNLIKELY(Options::traceBaselineJITExecution())) {
             OpcodeID opcodeID = currentInstruction->opcodeID();
-            unsigned bytecodeOffset = m_bytecodeOffset;
+            unsigned bytecodeOffset = m_bytecodeIndex.offset();
             CodeBlock* codeBlock = m_codeBlock;
             probe([=] (Probe::Context& ctx) {
                 dataLogLn("JIT [", bytecodeOffset, "] SLOW ", opcodeNames[opcodeID], " cfr ", RawPointer(ctx.fp()), " @ ", codeBlock);
             });
         }
@@ -525,11 +569,11 @@
         DEFINE_SLOWCASE_OP(op_jeq)
         DEFINE_SLOWCASE_OP(op_jneq)
         DEFINE_SLOWCASE_OP(op_jstricteq)
         DEFINE_SLOWCASE_OP(op_jnstricteq)
         DEFINE_SLOWCASE_OP(op_loop_hint)
-        DEFINE_SLOWCASE_OP(op_enter)
+        DEFINE_SLOWCASE_OP(op_check_traps)
         DEFINE_SLOWCASE_OP(op_mod)
         DEFINE_SLOWCASE_OP(op_mul)
         DEFINE_SLOWCASE_OP(op_negate)
         DEFINE_SLOWCASE_OP(op_neq)
         DEFINE_SLOWCASE_OP(op_new_object)
@@ -551,22 +595,27 @@
         DEFINE_SLOWCASE_SLOW_OP(lshift)
         DEFINE_SLOWCASE_SLOW_OP(rshift)
         DEFINE_SLOWCASE_SLOW_OP(urshift)
         DEFINE_SLOWCASE_SLOW_OP(div)
         DEFINE_SLOWCASE_SLOW_OP(create_this)
+        DEFINE_SLOWCASE_SLOW_OP(create_promise)
+        DEFINE_SLOWCASE_SLOW_OP(create_generator)
+        DEFINE_SLOWCASE_SLOW_OP(create_async_generator)
         DEFINE_SLOWCASE_SLOW_OP(to_this)
         DEFINE_SLOWCASE_SLOW_OP(to_primitive)
         DEFINE_SLOWCASE_SLOW_OP(to_number)
+        DEFINE_SLOWCASE_SLOW_OP(to_numeric)
         DEFINE_SLOWCASE_SLOW_OP(to_string)
         DEFINE_SLOWCASE_SLOW_OP(to_object)
         DEFINE_SLOWCASE_SLOW_OP(not)
         DEFINE_SLOWCASE_SLOW_OP(stricteq)
         DEFINE_SLOWCASE_SLOW_OP(nstricteq)
         DEFINE_SLOWCASE_SLOW_OP(get_direct_pname)
         DEFINE_SLOWCASE_SLOW_OP(has_structure_property)
         DEFINE_SLOWCASE_SLOW_OP(resolve_scope)
         DEFINE_SLOWCASE_SLOW_OP(check_tdz)
+        DEFINE_SLOWCASE_SLOW_OP(to_property_key)
 
         default:
             RELEASE_ASSERT_NOT_REACHED();
         }
 
@@ -578,22 +627,27 @@
 
         if (shouldEmitProfiling())
             add32(TrustedImm32(1), AbsoluteAddress(&rareCaseProfile->m_counter));
 
         emitJumpSlowToHot(jump(), 0);
+        ++bytecodeCountHavingSlowCase;
     }
 
+    RELEASE_ASSERT(bytecodeCountHavingSlowCase == m_bytecodeCountHavingSlowCase);
     RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
     RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
     RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
     RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
     RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 
+    if (shouldEmitProfiling())
+        m_codeBlock->setRareCaseProfiles(WTFMove(rareCaseProfiles));
+
 #ifndef NDEBUG
     // Reset this, in order to guard its use with ASSERTs.
-    m_bytecodeOffset = std::numeric_limits<unsigned>::max();
+    m_bytecodeIndex = BytecodeIndex();
 #endif
 }
 
 void JIT::compileWithoutLinking(JITCompilationEffort effort)
 {
@@ -640,11 +694,11 @@
                 m_vm->m_perBytecodeProfiler->ensureBytecodesFor(m_codeBlock),
                 Profiler::Baseline));
         m_compilation->addProfiledBytecodes(*m_vm->m_perBytecodeProfiler, m_codeBlock);
     }
 
-    m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(0, nullptr));
+    m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(BytecodeIndex(0)));
 
     Label entryLabel(this);
     if (m_disassembler)
         m_disassembler->setStartOfCode(entryLabel);
 
@@ -670,18 +724,16 @@
         stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
     stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm->addressOfSoftStackLimit()), regT1));
 
     move(regT1, stackPointerRegister);
     checkStackPointerAlignment();
-    if (Options::zeroStackFrame())
-        clearStackFrame(callFrameRegister, stackPointerRegister, regT0, maxFrameSize);
 
     emitSaveCalleeSaves();
     emitMaterializeTagCheckRegisters();
 
     if (m_codeBlock->codeType() == FunctionCode) {
-        ASSERT(m_bytecodeOffset == std::numeric_limits<unsigned>::max());
+        ASSERT(!m_bytecodeIndex);
         if (shouldEmitProfiling()) {
             for (int argument = 0; argument < m_codeBlock->numParameters(); ++argument) {
                 // If this is a constructor, then we want to put in a dummy profiling site (to
                 // keep things consistent) but we don't actually want to record the dummy value.
                 if (m_codeBlock->isConstructor() && !argument)
@@ -707,11 +759,11 @@
     if (m_disassembler)
         m_disassembler->setEndOfSlowPath(label());
     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 
     stackOverflow.link(this);
-    m_bytecodeOffset = 0;
+    m_bytecodeIndex = BytecodeIndex(0);
     if (maxFrameExtentForSlowPathCall)
         addPtr(TrustedImm32(-static_cast<int32_t>(maxFrameExtentForSlowPathCall)), stackPointerRegister);
     callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
 
     // If the number of parameters is 1, we never require arity fixup.
@@ -720,26 +772,26 @@
         m_arityCheck = label();
         store8(TrustedImm32(0), &m_codeBlock->m_shouldAlwaysBeInlined);
         emitFunctionPrologue();
         emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 
-        load32(payloadFor(CallFrameSlot::argumentCount), regT1);
+        load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT1);
         branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock->m_numParameters)).linkTo(beginLabel, this);
 
-        m_bytecodeOffset = 0;
+        m_bytecodeIndex = BytecodeIndex(0);
 
         if (maxFrameExtentForSlowPathCall)
             addPtr(TrustedImm32(-static_cast<int32_t>(maxFrameExtentForSlowPathCall)), stackPointerRegister);
-        callOperationWithCallFrameRollbackOnException(m_codeBlock->isConstructor() ? operationConstructArityCheck : operationCallArityCheck);
+        callOperationWithCallFrameRollbackOnException(m_codeBlock->isConstructor() ? operationConstructArityCheck : operationCallArityCheck, m_codeBlock->globalObject());
         if (maxFrameExtentForSlowPathCall)
             addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
         branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
         move(returnValueGPR, GPRInfo::argumentGPR0);
         emitNakedCall(m_vm->getCTIStub(arityFixupGenerator).retaggedCode<NoPtrTag>());
 
-#if !ASSERT_DISABLED
-        m_bytecodeOffset = std::numeric_limits<unsigned>::max(); // Reset this, in order to guard its use with ASSERTs.
+#if ASSERT_ENABLED
+        m_bytecodeIndex = BytecodeIndex(); // Reset this, in order to guard its use with ASSERTs.
 #endif
 
         jump(beginLabel);
     } else
         m_arityCheck = entryLabel; // Never require arity fixup.
@@ -775,11 +827,11 @@
     if (patchBuffer.didFailToAllocate())
         return CompilationFailed;
 
     // Translate vPC offsets into addresses in JIT generated code, for switch tables.
     for (auto& record : m_switches) {
-        unsigned bytecodeOffset = record.bytecodeOffset;
+        unsigned bytecodeOffset = record.bytecodeIndex.offset();
 
         if (record.type != SwitchRecord::String) {
             ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
             ASSERT(record.jumpTable.simpleJumpTable->branchOffsets.size() == record.jumpTable.simpleJumpTable->ctiOffsets.size());
 
@@ -818,10 +870,11 @@
         if (record.callee)
             patchBuffer.link(record.from, record.callee);
     }
 
     finalizeInlineCaches(m_getByIds, patchBuffer);
+    finalizeInlineCaches(m_getByVals, patchBuffer);
     finalizeInlineCaches(m_getByIdsWithThis, patchBuffer);
     finalizeInlineCaches(m_putByIds, patchBuffer);
     finalizeInlineCaches(m_inByIds, patchBuffer);
     finalizeInlineCaches(m_instanceOfs, patchBuffer);
 
@@ -857,21 +910,22 @@
             CodeLocationLabel<JSInternalPtrTag>(patchBuffer.locationOfNearCall<JSInternalPtrTag>(compilationInfo.callReturnLocation)),
             CodeLocationLabel<JSInternalPtrTag>(patchBuffer.locationOf<JSInternalPtrTag>(compilationInfo.hotPathBegin)),
             patchBuffer.locationOfNearCall<JSInternalPtrTag>(compilationInfo.hotPathOther));
     }
 
-    JITCodeMap jitCodeMap;
-    for (unsigned bytecodeOffset = 0; bytecodeOffset < m_labels.size(); ++bytecodeOffset) {
-        if (m_labels[bytecodeOffset].isSet())
-            jitCodeMap.append(bytecodeOffset, patchBuffer.locationOf<JSEntryPtrTag>(m_labels[bytecodeOffset]));
+    {
+        JITCodeMapBuilder jitCodeMapBuilder;
+        for (unsigned bytecodeOffset = 0; bytecodeOffset < m_labels.size(); ++bytecodeOffset) {
+            if (m_labels[bytecodeOffset].isSet())
+                jitCodeMapBuilder.append(BytecodeIndex(bytecodeOffset), patchBuffer.locationOf<JSEntryPtrTag>(m_labels[bytecodeOffset]));
+        }
+        m_codeBlock->setJITCodeMap(jitCodeMapBuilder.finalize());
     }
-    jitCodeMap.finish();
-    m_codeBlock->setJITCodeMap(WTFMove(jitCodeMap));
 
     MacroAssemblerCodePtr<JSEntryPtrTag> withArityCheck = patchBuffer.locationOf<JSEntryPtrTag>(m_arityCheck);
 
-    if (Options::dumpDisassembly()) {
+    if (UNLIKELY(Options::dumpDisassembly())) {
         m_disassembler->dump(patchBuffer);
         patchBuffer.didAlreadyDisassemble();
     }
     if (UNLIKELY(m_compilation)) {
         if (Options::disassembleBaselineForProfiler())
@@ -888,11 +942,14 @@
 
     m_vm->machineCodeBytesPerBytecodeWordForBaselineJIT->add(
         static_cast<double>(result.size()) /
         static_cast<double>(m_codeBlock->instructionsSize()));
 
-    m_codeBlock->shrinkToFit(CodeBlock::LateShrink);
+    {
+        ConcurrentJSLocker locker(m_codeBlock->m_lock);
+        m_codeBlock->shrinkToFit(locker, CodeBlock::ShrinkMode::LateShrink);
+    }
     m_codeBlock->setJITCode(
         adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));
 
     if (JITInternal::verbose)
         dataLogF("JIT generated code for %p at [%p, %p).\n", m_codeBlock, result.executableMemory()->start().untaggedPtr(), result.executableMemory()->end().untaggedPtr());
@@ -912,40 +969,27 @@
     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
         m_exceptionChecksWithCallFrameRollback.link(this);
 
         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 
-        // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).
-
+        // operationLookupExceptionHandlerFromCallerFrame is passed one argument, the VM*.
         move(TrustedImmPtr(&vm()), GPRInfo::argumentGPR0);
-        move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
-
-#if CPU(X86)
-        // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
-        poke(GPRInfo::argumentGPR0);
-        poke(GPRInfo::argumentGPR1, 1);
-#endif
-        m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits<unsigned>::max(), FunctionPtr<OperationPtrTag>(lookupExceptionHandlerFromCallerFrame)));
+        prepareCallOperation(vm());
+        m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr<OperationPtrTag>(operationLookupExceptionHandlerFromCallerFrame)));
         jumpToExceptionHandler(vm());
     }
 
     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
         m_exceptionHandler = label();
         m_exceptionChecks.link(this);
 
         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 
-        // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).
+        // operationLookupExceptionHandler is passed one argument, the VM*.
         move(TrustedImmPtr(&vm()), GPRInfo::argumentGPR0);
-        move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
-
-#if CPU(X86)
-        // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
-        poke(GPRInfo::argumentGPR0);
-        poke(GPRInfo::argumentGPR1, 1);
-#endif
-        m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits<unsigned>::max(), FunctionPtr<OperationPtrTag>(lookupExceptionHandler)));
+        prepareCallOperation(vm());
+        m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr<OperationPtrTag>(operationLookupExceptionHandler)));
         jumpToExceptionHandler(vm());
     }
 }
 
 void JIT::doMainThreadPreparationBeforeCompile()
