<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/ARM64Assembler.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="../Sources.txt.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ARM64Registers.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/ARM64Assembler.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1477         sub&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1478     }
1479 
1480     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1481     ALWAYS_INLINE void ngc(RegisterID rd, RegisterID rm)
1482     {
1483         sbc&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm);
1484     }
1485 
1486     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1487     ALWAYS_INLINE void ngc(RegisterID rd, RegisterID rm, ShiftType shift, int amount)
1488     {
1489         sbc&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1490     }
1491 
1492     ALWAYS_INLINE void nop()
1493     {
1494         insn(nopPseudo());
1495     }
1496 
<span class="line-modified">1497     template &lt;typename CopyFunction&gt;</span>
<span class="line-modified">1498     static void fillNops(void* base, size_t size, CopyFunction copy)</span>



1499     {
1500         RELEASE_ASSERT(!(size % sizeof(int32_t)));
1501         size_t n = size / sizeof(int32_t);
1502         for (int32_t* ptr = static_cast&lt;int32_t*&gt;(base); n--;) {
1503             int insn = nopPseudo();
1504             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(ptr) == ptr);
1505             copy(ptr++, &amp;insn, sizeof(int));
1506         }
1507     }
1508 
1509     ALWAYS_INLINE void dmbISH()
1510     {
1511         insn(0xd5033bbf);
1512     }
1513 
1514     ALWAYS_INLINE void dmbISHST()
1515     {
1516         insn(0xd5033abf);
1517     }
1518 
</pre>
<hr />
<pre>
2535         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition));
2536     }
2537 
2538     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition, bool is64Bit, RegisterID compareRegister)
2539     {
2540         ASSERT(to.isSet());
2541         ASSERT(from.isSet());
2542         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition, is64Bit, compareRegister));
2543     }
2544 
2545     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition, unsigned bitNumber, RegisterID compareRegister)
2546     {
2547         ASSERT(to.isSet());
2548         ASSERT(from.isSet());
2549         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition, bitNumber, compareRegister));
2550     }
2551 
2552     static void linkJump(void* code, AssemblerLabel from, void* to)
2553     {
2554         ASSERT(from.isSet());
<span class="line-modified">2555         relinkJumpOrCall&lt;false&gt;(addressOf(code, from), addressOf(code, from), to);</span>
2556     }
2557 
2558     static void linkCall(void* code, AssemblerLabel from, void* to)
2559     {
2560         ASSERT(from.isSet());
<span class="line-modified">2561         linkJumpOrCall&lt;true&gt;(addressOf(code, from) - 1, addressOf(code, from) - 1, to);</span>
2562     }
2563 
2564     static void linkPointer(void* code, AssemblerLabel where, void* valuePtr)
2565     {
2566         linkPointer(addressOf(code, where), valuePtr);
2567     }
2568 
2569     static void replaceWithVMHalt(void* where)
2570     {
2571         // This should try to write to null which should always Segfault.
2572         int insn = dataCacheZeroVirtualAddress(ARM64Registers::zr);
2573         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2574         performJITMemcpy(where, &amp;insn, sizeof(int));
2575         cacheFlush(where, sizeof(int));
2576     }
2577 
2578     static void replaceWithJump(void* where, void* to)
2579     {
2580         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(where)) &gt;&gt; 2;
2581         ASSERT(static_cast&lt;int&gt;(offset) == offset);
</pre>
<hr />
<pre>
2598     static void replaceWithLoad(void* where)
2599     {
2600         Datasize sf;
2601         AddOp op;
2602         SetFlags S;
2603         int shift;
2604         int imm12;
2605         RegisterID rn;
2606         RegisterID rd;
2607         if (disassembleAddSubtractImmediate(where, sf, op, S, shift, imm12, rn, rd)) {
2608             ASSERT(sf == Datasize_64);
2609             ASSERT(op == AddOp_ADD);
2610             ASSERT(!S);
2611             ASSERT(!shift);
2612             ASSERT(!(imm12 &amp; ~0xff8));
2613             int insn = loadStoreRegisterUnsignedImmediate(MemOpSize_64, false, MemOp_LOAD, encodePositiveImmediate&lt;64&gt;(imm12), rn, rd);
2614             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2615             performJITMemcpy(where, &amp;insn, sizeof(int));
2616             cacheFlush(where, sizeof(int));
2617         }
<span class="line-modified">2618 #if !ASSERT_DISABLED</span>
2619         else {
2620             MemOpSize size;
2621             bool V;
2622             MemOp opc;
2623             int imm12;
2624             RegisterID rn;
2625             RegisterID rt;
2626             ASSERT(disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt));
2627             ASSERT(size == MemOpSize_64);
2628             ASSERT(!V);
2629             ASSERT(opc == MemOp_LOAD);
2630             ASSERT(!(imm12 &amp; ~0x1ff));
2631         }
<span class="line-modified">2632 #endif</span>
2633     }
2634 
2635     static void replaceWithAddressComputation(void* where)
2636     {
2637         MemOpSize size;
2638         bool V;
2639         MemOp opc;
2640         int imm12;
2641         RegisterID rn;
2642         RegisterID rt;
2643         if (disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt)) {
2644             ASSERT(size == MemOpSize_64);
2645             ASSERT(!V);
2646             ASSERT(opc == MemOp_LOAD);
2647             ASSERT(!(imm12 &amp; ~0x1ff));
2648             int insn = addSubtractImmediate(Datasize_64, AddOp_ADD, DontSetFlags, 0, imm12 * sizeof(void*), rn, rt);
2649             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2650             performJITMemcpy(where, &amp;insn, sizeof(int));
2651             cacheFlush(where, sizeof(int));
2652         }
<span class="line-modified">2653 #if !ASSERT_DISABLED</span>
2654         else {
2655             Datasize sf;
2656             AddOp op;
2657             SetFlags S;
2658             int shift;
2659             int imm12;
2660             RegisterID rn;
2661             RegisterID rd;
2662             ASSERT(disassembleAddSubtractImmediate(where, sf, op, S, shift, imm12, rn, rd));
2663             ASSERT(sf == Datasize_64);
2664             ASSERT(op == AddOp_ADD);
2665             ASSERT(!S);
2666             ASSERT(!shift);
2667             ASSERT(!(imm12 &amp; ~0xff8));
2668         }
<span class="line-modified">2669 #endif</span>
2670     }
2671 
2672     static void repatchPointer(void* where, void* valuePtr)
2673     {
2674         linkPointer(static_cast&lt;int*&gt;(where), valuePtr, true);
2675     }
2676 
2677     static void setPointer(int* address, void* valuePtr, RegisterID rd, bool flush)
2678     {
2679         uintptr_t value = reinterpret_cast&lt;uintptr_t&gt;(valuePtr);
2680         int buffer[3];
2681         buffer[0] = moveWideImediate(Datasize_64, MoveWideOp_Z, 0, getHalfword(value, 0), rd);
2682         buffer[1] = moveWideImediate(Datasize_64, MoveWideOp_K, 1, getHalfword(value, 1), rd);
2683         buffer[2] = moveWideImediate(Datasize_64, MoveWideOp_K, 2, getHalfword(value, 2), rd);
2684         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(address) == address);
2685         performJITMemcpy(address, buffer, sizeof(int) * 3);
2686 
2687         if (flush)
2688             cacheFlush(address, sizeof(int) * 3);
2689     }
</pre>
<hr />
<pre>
2737         expected = disassembleMoveWideImediate(address + 2, sf, opc, hw, imm16, rd);
2738         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_K &amp;&amp; hw == 2 &amp;&amp; rd == rdFirst);
2739         result |= static_cast&lt;uintptr_t&gt;(imm16) &lt;&lt; 32;
2740 #endif
2741 
2742         return reinterpret_cast&lt;void*&gt;(result);
2743     }
2744 
2745     static void* readCallTarget(void* from)
2746     {
2747         return readPointer(reinterpret_cast&lt;int*&gt;(from) - (isAddress64Bit() ? 4 : 3));
2748     }
2749 
2750     // The static relink, repatch, and replace methods can use can
2751     // use |from| for both the write and executable address for call
2752     // and jump patching as they&#39;re modifying existing (linked) code,
2753     // so the address being provided is correct for relative address
2754     // computation.
2755     static void relinkJump(void* from, void* to)
2756     {
<span class="line-modified">2757         relinkJumpOrCall&lt;false&gt;(reinterpret_cast&lt;int*&gt;(from), reinterpret_cast&lt;const int*&gt;(from), to);</span>
2758         cacheFlush(from, sizeof(int));
2759     }
2760 
2761     static void relinkJumpToNop(void* from)
2762     {
2763         relinkJump(from, static_cast&lt;char*&gt;(from) + 4);
2764     }
2765 
2766     static void relinkCall(void* from, void* to)
2767     {
<span class="line-modified">2768         relinkJumpOrCall&lt;true&gt;(reinterpret_cast&lt;int*&gt;(from) - 1, reinterpret_cast&lt;const int*&gt;(from) - 1, to);</span>
2769         cacheFlush(reinterpret_cast&lt;int*&gt;(from) - 1, sizeof(int));
2770     }
2771 
2772     static void repatchCompact(void* where, int32_t value)
2773     {
2774         ASSERT(!(value &amp; ~0x3ff8));
2775 
2776         MemOpSize size;
2777         bool V;
2778         MemOp opc;
2779         int imm12;
2780         RegisterID rn;
2781         RegisterID rt;
2782         bool expected = disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt);
2783         ASSERT_UNUSED(expected, expected &amp;&amp; size &gt;= MemOpSize_32 &amp;&amp; !V &amp;&amp; opc == MemOp_LOAD); // expect 32/64 bit load to GPR.
2784 
2785         if (size == MemOpSize_32)
2786             imm12 = encodePositiveImmediate&lt;32&gt;(value);
2787         else
2788             imm12 = encodePositiveImmediate&lt;64&gt;(value);
2789         int insn = loadStoreRegisterUnsignedImmediate(size, V, opc, imm12, rn, rt);
2790         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2791         performJITMemcpy(where, &amp;insn, sizeof(int));
2792 
2793         cacheFlush(where, sizeof(int));
2794     }
2795 
2796     unsigned debugOffset() { return m_buffer.debugOffset(); }
2797 
2798 #if OS(LINUX) &amp;&amp; COMPILER(GCC_COMPATIBLE)
2799     static inline void linuxPageFlush(uintptr_t begin, uintptr_t end)
2800     {
2801         __builtin___clear_cache(reinterpret_cast&lt;char*&gt;(begin), reinterpret_cast&lt;char*&gt;(end));
2802     }
2803 #endif
2804 
2805     static void cacheFlush(void* code, size_t size)
2806     {
<span class="line-modified">2807 #if OS(IOS_FAMILY)</span>
2808         sys_cache_control(kCacheFunctionPrepareForExecution, code, size);
2809 #elif OS(FUCHSIA)
2810         zx_cache_flush(code, size, ZX_CACHE_FLUSH_INSN);
2811 #elif OS(LINUX)
2812         size_t page = pageSize();
2813         uintptr_t current = reinterpret_cast&lt;uintptr_t&gt;(code);
2814         uintptr_t end = current + size;
2815         uintptr_t firstPageEnd = (current &amp; ~(page - 1)) + page;
2816 
2817         if (end &lt;= firstPageEnd) {
2818             linuxPageFlush(current, end);
2819             return;
2820         }
2821 
2822         linuxPageFlush(current, firstPageEnd);
2823 
2824         for (current = firstPageEnd; current + page &lt; end; current += page)
2825             linuxPageFlush(current, current + page);
2826 
2827         linuxPageFlush(current, end);
</pre>
<hr />
<pre>
2893         default:
2894             ASSERT_NOT_REACHED();
2895         }
2896 
2897         return LinkJumpNoCondition;
2898     }
2899 
2900     static JumpLinkType computeJumpType(LinkRecord&amp; record, const uint8_t* from, const uint8_t* to)
2901     {
2902         JumpLinkType linkType = computeJumpType(record.type(), from, to);
2903         record.setLinkType(linkType);
2904         return linkType;
2905     }
2906 
2907     Vector&lt;LinkRecord, 0, UnsafeVectorOverflow&gt;&amp; jumpsToLink()
2908     {
2909         std::sort(m_jumpsToLink.begin(), m_jumpsToLink.end(), linkRecordSourceComparator);
2910         return m_jumpsToLink;
2911     }
2912 
<span class="line-modified">2913 #if CPU(ARM64E)</span>
<span class="line-modified">2914     class CopyFunction {</span>
<span class="line-removed">2915         typedef void* (*Func)(void*, const void*, size_t);</span>
<span class="line-removed">2916     public:</span>
<span class="line-removed">2917         CopyFunction(Func func)</span>
<span class="line-removed">2918             : m_func(func)</span>
<span class="line-removed">2919         {</span>
<span class="line-removed">2920             assertIsNullOrTaggedWith(func, CopyFunctionPtrTag);</span>
<span class="line-removed">2921         }</span>
<span class="line-removed">2922 </span>
<span class="line-removed">2923         void* operator()(void* dst, const void* src, size_t size)</span>
<span class="line-removed">2924         {</span>
<span class="line-removed">2925             return ptrauth_auth_function(m_func, ptrauth_key_process_dependent_code, CopyFunctionPtrTag)(dst, src, size);</span>
<span class="line-removed">2926         }</span>
<span class="line-removed">2927 </span>
<span class="line-removed">2928     private:</span>
<span class="line-removed">2929         Func m_func;</span>
<span class="line-removed">2930     };</span>
<span class="line-removed">2931 #else</span>
<span class="line-removed">2932     typedef void* (*CopyFunction)(void*, const void*, size_t);</span>
<span class="line-removed">2933 #endif</span>
<span class="line-removed">2934 </span>
<span class="line-removed">2935     static void ALWAYS_INLINE link(LinkRecord&amp; record, uint8_t* from, const uint8_t* fromInstruction8, uint8_t* to, CopyFunction copy)</span>
2936     {
2937         const int* fromInstruction = reinterpret_cast&lt;const int*&gt;(fromInstruction8);
2938         switch (record.linkType()) {
2939         case LinkJumpNoCondition:
<span class="line-modified">2940             linkJumpOrCall&lt;false&gt;(reinterpret_cast&lt;int*&gt;(from), fromInstruction, to, copy);</span>
2941             break;
2942         case LinkJumpConditionDirect:
<span class="line-modified">2943             linkConditionalBranch&lt;true&gt;(record.condition(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to, copy);</span>
2944             break;
2945         case LinkJumpCondition:
<span class="line-modified">2946             linkConditionalBranch&lt;false&gt;(record.condition(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to, copy);</span>
2947             break;
2948         case LinkJumpCompareAndBranchDirect:
<span class="line-modified">2949             linkCompareAndBranch&lt;true&gt;(record.condition(), record.is64Bit(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to, copy);</span>
2950             break;
2951         case LinkJumpCompareAndBranch:
<span class="line-modified">2952             linkCompareAndBranch&lt;false&gt;(record.condition(), record.is64Bit(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to, copy);</span>
2953             break;
2954         case LinkJumpTestBitDirect:
<span class="line-modified">2955             linkTestAndBranch&lt;true&gt;(record.condition(), record.bitNumber(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to, copy);</span>
2956             break;
2957         case LinkJumpTestBit:
<span class="line-modified">2958             linkTestAndBranch&lt;false&gt;(record.condition(), record.bitNumber(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to, copy);</span>
2959             break;
2960         default:
2961             ASSERT_NOT_REACHED();
2962             break;
2963         }
2964     }
2965 
2966 protected:
2967     template&lt;Datasize size&gt;
2968     static bool checkMovk(int insn, int _hw, RegisterID _rd)
2969     {
2970         Datasize sf;
2971         MoveWideOp opc;
2972         int hw;
2973         uint16_t imm16;
2974         RegisterID rd;
2975         bool expected = disassembleMoveWideImediate(&amp;insn, sf, opc, hw, imm16, rd);
2976 
2977         return expected
2978             &amp;&amp; sf == size
2979             &amp;&amp; opc == MoveWideOp_K
2980             &amp;&amp; hw == _hw
2981             &amp;&amp; rd == _rd;
2982     }
2983 
2984     static void linkPointer(int* address, void* valuePtr, bool flush = false)
2985     {
2986         Datasize sf;
2987         MoveWideOp opc;
2988         int hw;
2989         uint16_t imm16;
2990         RegisterID rd;
2991         bool expected = disassembleMoveWideImediate(address, sf, opc, hw, imm16, rd);
2992         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_Z &amp;&amp; !hw);
2993         ASSERT(checkMovk&lt;Datasize_64&gt;(address[1], 1, rd));
2994         ASSERT(checkMovk&lt;Datasize_64&gt;(address[2], 2, rd));
2995 
2996         setPointer(address, valuePtr, rd, flush);
2997     }
2998 
<span class="line-modified">2999     template&lt;bool isCall&gt;</span>
<span class="line-modified">3000     static void linkJumpOrCall(int* from, const int* fromInstruction, void* to, CopyFunction copy = tagCFunctionPtr&lt;CopyFunctionPtrTag&gt;(performJITMemcpy))</span>
3001     {


3002         bool link;
3003         int imm26;
3004         bool isUnconditionalBranchImmediateOrNop = disassembleUnconditionalBranchImmediate(from, link, imm26) || disassembleNop(from);
3005 
3006         ASSERT_UNUSED(isUnconditionalBranchImmediateOrNop, isUnconditionalBranchImmediateOrNop);

3007         ASSERT_UNUSED(isCall, (link == isCall) || disassembleNop(from));
3008         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3009         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3010         assertIsNotTagged(to);
3011         assertIsNotTagged(fromInstruction);
3012         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3013         ASSERT(static_cast&lt;int&gt;(offset) == offset);
3014 
3015         int insn = unconditionalBranchImmediate(isCall, static_cast&lt;int&gt;(offset));
3016         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3017         copy(from, &amp;insn, sizeof(int));
3018     }
3019 
<span class="line-modified">3020     template&lt;bool isDirect&gt;</span>
<span class="line-modified">3021     static void linkCompareAndBranch(Condition condition, bool is64Bit, RegisterID rt, int* from, const int* fromInstruction, void* to, CopyFunction copy = tagCFunctionPtr&lt;CopyFunctionPtrTag&gt;(performJITMemcpy))</span>
3022     {
3023         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3024         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3025         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3026         ASSERT(isInt&lt;26&gt;(offset));
3027 
3028         bool useDirect = isInt&lt;19&gt;(offset);
<span class="line-modified">3029         ASSERT(!isDirect || useDirect);</span>
3030 
<span class="line-modified">3031         if (useDirect || isDirect) {</span>
3032             int insn = compareAndBranchImmediate(is64Bit ? Datasize_64 : Datasize_32, condition == ConditionNE, static_cast&lt;int&gt;(offset), rt);
3033             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3034             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3035             if (!isDirect) {</span>
3036                 insn = nopPseudo();
3037                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3038                 copy(from + 1, &amp;insn, sizeof(int));
3039             }
3040         } else {
3041             int insn = compareAndBranchImmediate(is64Bit ? Datasize_64 : Datasize_32, invert(condition) == ConditionNE, 2, rt);
3042             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3043             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3044             linkJumpOrCall&lt;false&gt;(from + 1, fromInstruction + 1, to, copy);</span>
3045         }
3046     }
3047 
<span class="line-modified">3048     template&lt;bool isDirect&gt;</span>
<span class="line-modified">3049     static void linkConditionalBranch(Condition condition, int* from, const int* fromInstruction, void* to, CopyFunction copy = tagCFunctionPtr&lt;CopyFunctionPtrTag&gt;(performJITMemcpy))</span>
3050     {
3051         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3052         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3053         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3054         ASSERT(isInt&lt;26&gt;(offset));
3055 
3056         bool useDirect = isInt&lt;19&gt;(offset);
<span class="line-modified">3057         ASSERT(!isDirect || useDirect);</span>
3058 
<span class="line-modified">3059         if (useDirect || isDirect) {</span>
3060             int insn = conditionalBranchImmediate(static_cast&lt;int&gt;(offset), condition);
3061             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3062             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3063             if (!isDirect) {</span>
3064                 insn = nopPseudo();
3065                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3066                 copy(from + 1, &amp;insn, sizeof(int));
3067             }
3068         } else {
3069             int insn = conditionalBranchImmediate(2, invert(condition));
3070             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3071             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3072             linkJumpOrCall&lt;false&gt;(from + 1, fromInstruction + 1, to, copy);</span>
3073         }
3074     }
3075 
<span class="line-modified">3076     template&lt;bool isDirect&gt;</span>
<span class="line-modified">3077     static void linkTestAndBranch(Condition condition, unsigned bitNumber, RegisterID rt, int* from, const int* fromInstruction, void* to, CopyFunction copy = tagCFunctionPtr&lt;CopyFunctionPtrTag&gt;(performJITMemcpy))</span>
3078     {
3079         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3080         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3081         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3082         ASSERT(static_cast&lt;int&gt;(offset) == offset);
3083         ASSERT(isInt&lt;26&gt;(offset));
3084 
3085         bool useDirect = isInt&lt;14&gt;(offset);
<span class="line-modified">3086         ASSERT(!isDirect || useDirect);</span>
3087 
<span class="line-modified">3088         if (useDirect || isDirect) {</span>
3089             int insn = testAndBranchImmediate(condition == ConditionNE, static_cast&lt;int&gt;(bitNumber), static_cast&lt;int&gt;(offset), rt);
3090             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3091             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3092             if (!isDirect) {</span>
3093                 insn = nopPseudo();
3094                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3095                 copy(from + 1, &amp;insn, sizeof(int));
3096             }
3097         } else {
3098             int insn = testAndBranchImmediate(invert(condition) == ConditionNE, static_cast&lt;int&gt;(bitNumber), 2, rt);
3099             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3100             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3101             linkJumpOrCall&lt;false&gt;(from + 1, fromInstruction + 1, to, copy);</span>
3102         }
3103     }
3104 
<span class="line-modified">3105     template&lt;bool isCall&gt;</span>
3106     static void relinkJumpOrCall(int* from, const int* fromInstruction, void* to)
3107     {
<span class="line-modified">3108         if (!isCall &amp;&amp; disassembleNop(from)) {</span>

3109             unsigned op01;
3110             int imm19;
3111             Condition condition;
3112             bool isConditionalBranchImmediate = disassembleConditionalBranchImmediate(from - 1, op01, imm19, condition);
3113 
3114             if (isConditionalBranchImmediate) {
3115                 ASSERT_UNUSED(op01, !op01);
<span class="line-modified">3116                 ASSERT_UNUSED(isCall, !isCall);</span>
3117 
3118                 if (imm19 == 8)
3119                     condition = invert(condition);
3120 
<span class="line-modified">3121                 linkConditionalBranch&lt;false&gt;(condition, from - 1, fromInstruction - 1, to);</span>
3122                 return;
3123             }
3124 
3125             Datasize opSize;
3126             bool op;
3127             RegisterID rt;
3128             bool isCompareAndBranchImmediate = disassembleCompareAndBranchImmediate(from - 1, opSize, op, imm19, rt);
3129 
3130             if (isCompareAndBranchImmediate) {
3131                 if (imm19 == 8)
3132                     op = !op;
3133 
<span class="line-modified">3134                 linkCompareAndBranch&lt;false&gt;(op ? ConditionNE : ConditionEQ, opSize == Datasize_64, rt, from - 1, fromInstruction - 1, to);</span>
3135                 return;
3136             }
3137 
3138             int imm14;
3139             unsigned bitNumber;
3140             bool isTestAndBranchImmediate = disassembleTestAndBranchImmediate(from - 1, op, bitNumber, imm14, rt);
3141 
3142             if (isTestAndBranchImmediate) {
3143                 if (imm14 == 8)
3144                     op = !op;
3145 
<span class="line-modified">3146                 linkTestAndBranch&lt;false&gt;(op ? ConditionNE : ConditionEQ, bitNumber, rt, from - 1, fromInstruction - 1, to);</span>
3147                 return;
3148             }
3149         }
3150 
<span class="line-modified">3151         linkJumpOrCall&lt;isCall&gt;(from, fromInstruction, to);</span>
3152     }
3153 
3154     static int* addressOf(void* code, AssemblerLabel label)
3155     {
3156         return reinterpret_cast&lt;int*&gt;(static_cast&lt;char*&gt;(code) + label.m_offset);
3157     }
3158 
3159     static RegisterID disassembleXOrSp(int reg) { return reg == 31 ? ARM64Registers::sp : static_cast&lt;RegisterID&gt;(reg); }
3160     static RegisterID disassembleXOrZr(int reg) { return reg == 31 ? ARM64Registers::zr : static_cast&lt;RegisterID&gt;(reg); }
3161     static RegisterID disassembleXOrZrOrSp(bool useZr, int reg) { return reg == 31 ? (useZr ? ARM64Registers::zr : ARM64Registers::sp) : static_cast&lt;RegisterID&gt;(reg); }
3162 
3163     static bool disassembleAddSubtractImmediate(void* address, Datasize&amp; sf, AddOp&amp; op, SetFlags&amp; S, int&amp; shift, int&amp; imm12, RegisterID&amp; rn, RegisterID&amp; rd)
3164     {
3165         int insn = *static_cast&lt;int*&gt;(address);
3166         sf = static_cast&lt;Datasize&gt;((insn &gt;&gt; 31) &amp; 1);
3167         op = static_cast&lt;AddOp&gt;((insn &gt;&gt; 30) &amp; 1);
3168         S = static_cast&lt;SetFlags&gt;((insn &gt;&gt; 29) &amp; 1);
3169         shift = (insn &gt;&gt; 22) &amp; 3;
3170         imm12 = (insn &gt;&gt; 10) &amp; 0x3ff;
3171         rn = disassembleXOrSp((insn &gt;&gt; 5) &amp; 0x1f);
</pre>
<hr />
<pre>
3227         int insn = *static_cast&lt;int*&gt;(address);
3228         op = (insn &gt;&gt; 24) &amp; 0x1;
3229         imm14 = (insn &lt;&lt; 13) &gt;&gt; 18;
3230         bitNumber = static_cast&lt;unsigned&gt;((((insn &gt;&gt; 26) &amp; 0x20)) | ((insn &gt;&gt; 19) &amp; 0x1f));
3231         rt = static_cast&lt;RegisterID&gt;(insn &amp; 0x1f);
3232         return (insn &amp; 0x7e000000) == 0x36000000;
3233 
3234     }
3235 
3236     static bool disassembleUnconditionalBranchImmediate(void* address, bool&amp; op, int&amp; imm26)
3237     {
3238         int insn = *static_cast&lt;int*&gt;(address);
3239         op = (insn &gt;&gt; 31) &amp; 1;
3240         imm26 = (insn &lt;&lt; 6) &gt;&gt; 6;
3241         return (insn &amp; 0x7c000000) == 0x14000000;
3242     }
3243 
3244     static int xOrSp(RegisterID reg)
3245     {
3246         ASSERT(!isZr(reg));
<span class="line-modified">3247         ASSERT(!isIOS() || reg != ARM64Registers::x18);</span>
3248         return reg;
3249     }
3250     static int xOrZr(RegisterID reg)
3251     {
3252         ASSERT(!isSp(reg));
<span class="line-modified">3253         ASSERT(!isIOS() || reg != ARM64Registers::x18);</span>
3254         return reg &amp; 31;
3255     }
3256     static FPRegisterID xOrZrAsFPR(RegisterID reg) { return static_cast&lt;FPRegisterID&gt;(xOrZr(reg)); }
3257     static int xOrZrOrSp(bool useZr, RegisterID reg) { return useZr ? xOrZr(reg) : xOrSp(reg); }
3258 
3259     ALWAYS_INLINE void insn(int instruction)
3260     {
3261         m_buffer.putInt(instruction);
3262     }
3263 
3264     ALWAYS_INLINE static int addSubtractExtendedRegister(Datasize sf, AddOp op, SetFlags S, RegisterID rm, ExtendType option, int imm3, RegisterID rn, RegisterID rd)
3265     {
3266         ASSERT(imm3 &lt; 5);
3267         // The only allocated values for opt is 0.
3268         const int opt = 0;
3269         return (0x0b200000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | opt &lt;&lt; 22 | xOrZr(rm) &lt;&lt; 16 | option &lt;&lt; 13 | (imm3 &amp; 0x7) &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | xOrZrOrSp(S, rd));
3270     }
3271 
3272     ALWAYS_INLINE static int addSubtractImmediate(Datasize sf, AddOp op, SetFlags S, int shift, int imm12, RegisterID rn, RegisterID rd)
3273     {
</pre>
</td>
<td>
<hr />
<pre>
1477         sub&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1478     }
1479 
1480     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1481     ALWAYS_INLINE void ngc(RegisterID rd, RegisterID rm)
1482     {
1483         sbc&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm);
1484     }
1485 
1486     template&lt;int datasize, SetFlags setFlags = DontSetFlags&gt;
1487     ALWAYS_INLINE void ngc(RegisterID rd, RegisterID rm, ShiftType shift, int amount)
1488     {
1489         sbc&lt;datasize, setFlags&gt;(rd, ARM64Registers::zr, rm, shift, amount);
1490     }
1491 
1492     ALWAYS_INLINE void nop()
1493     {
1494         insn(nopPseudo());
1495     }
1496 
<span class="line-modified">1497     enum BranchTargetType { DirectBranch, IndirectBranch  };</span>
<span class="line-modified">1498     using CopyFunction = void*(&amp;)(void*, const void*, size_t);</span>
<span class="line-added">1499 </span>
<span class="line-added">1500     template &lt;CopyFunction copy&gt;</span>
<span class="line-added">1501     static void fillNops(void* base, size_t size)</span>
1502     {
1503         RELEASE_ASSERT(!(size % sizeof(int32_t)));
1504         size_t n = size / sizeof(int32_t);
1505         for (int32_t* ptr = static_cast&lt;int32_t*&gt;(base); n--;) {
1506             int insn = nopPseudo();
1507             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(ptr) == ptr);
1508             copy(ptr++, &amp;insn, sizeof(int));
1509         }
1510     }
1511 
1512     ALWAYS_INLINE void dmbISH()
1513     {
1514         insn(0xd5033bbf);
1515     }
1516 
1517     ALWAYS_INLINE void dmbISHST()
1518     {
1519         insn(0xd5033abf);
1520     }
1521 
</pre>
<hr />
<pre>
2538         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition));
2539     }
2540 
2541     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition, bool is64Bit, RegisterID compareRegister)
2542     {
2543         ASSERT(to.isSet());
2544         ASSERT(from.isSet());
2545         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition, is64Bit, compareRegister));
2546     }
2547 
2548     void linkJump(AssemblerLabel from, AssemblerLabel to, JumpType type, Condition condition, unsigned bitNumber, RegisterID compareRegister)
2549     {
2550         ASSERT(to.isSet());
2551         ASSERT(from.isSet());
2552         m_jumpsToLink.append(LinkRecord(from.m_offset, to.m_offset, type, condition, bitNumber, compareRegister));
2553     }
2554 
2555     static void linkJump(void* code, AssemblerLabel from, void* to)
2556     {
2557         ASSERT(from.isSet());
<span class="line-modified">2558         relinkJumpOrCall&lt;BranchType_JMP&gt;(addressOf(code, from), addressOf(code, from), to);</span>
2559     }
2560 
2561     static void linkCall(void* code, AssemblerLabel from, void* to)
2562     {
2563         ASSERT(from.isSet());
<span class="line-modified">2564         linkJumpOrCall&lt;BranchType_CALL&gt;(addressOf(code, from) - 1, addressOf(code, from) - 1, to);</span>
2565     }
2566 
2567     static void linkPointer(void* code, AssemblerLabel where, void* valuePtr)
2568     {
2569         linkPointer(addressOf(code, where), valuePtr);
2570     }
2571 
2572     static void replaceWithVMHalt(void* where)
2573     {
2574         // This should try to write to null which should always Segfault.
2575         int insn = dataCacheZeroVirtualAddress(ARM64Registers::zr);
2576         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2577         performJITMemcpy(where, &amp;insn, sizeof(int));
2578         cacheFlush(where, sizeof(int));
2579     }
2580 
2581     static void replaceWithJump(void* where, void* to)
2582     {
2583         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(where)) &gt;&gt; 2;
2584         ASSERT(static_cast&lt;int&gt;(offset) == offset);
</pre>
<hr />
<pre>
2601     static void replaceWithLoad(void* where)
2602     {
2603         Datasize sf;
2604         AddOp op;
2605         SetFlags S;
2606         int shift;
2607         int imm12;
2608         RegisterID rn;
2609         RegisterID rd;
2610         if (disassembleAddSubtractImmediate(where, sf, op, S, shift, imm12, rn, rd)) {
2611             ASSERT(sf == Datasize_64);
2612             ASSERT(op == AddOp_ADD);
2613             ASSERT(!S);
2614             ASSERT(!shift);
2615             ASSERT(!(imm12 &amp; ~0xff8));
2616             int insn = loadStoreRegisterUnsignedImmediate(MemOpSize_64, false, MemOp_LOAD, encodePositiveImmediate&lt;64&gt;(imm12), rn, rd);
2617             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2618             performJITMemcpy(where, &amp;insn, sizeof(int));
2619             cacheFlush(where, sizeof(int));
2620         }
<span class="line-modified">2621 #if ASSERT_ENABLED</span>
2622         else {
2623             MemOpSize size;
2624             bool V;
2625             MemOp opc;
2626             int imm12;
2627             RegisterID rn;
2628             RegisterID rt;
2629             ASSERT(disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt));
2630             ASSERT(size == MemOpSize_64);
2631             ASSERT(!V);
2632             ASSERT(opc == MemOp_LOAD);
2633             ASSERT(!(imm12 &amp; ~0x1ff));
2634         }
<span class="line-modified">2635 #endif // ASSERT_ENABLED</span>
2636     }
2637 
2638     static void replaceWithAddressComputation(void* where)
2639     {
2640         MemOpSize size;
2641         bool V;
2642         MemOp opc;
2643         int imm12;
2644         RegisterID rn;
2645         RegisterID rt;
2646         if (disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt)) {
2647             ASSERT(size == MemOpSize_64);
2648             ASSERT(!V);
2649             ASSERT(opc == MemOp_LOAD);
2650             ASSERT(!(imm12 &amp; ~0x1ff));
2651             int insn = addSubtractImmediate(Datasize_64, AddOp_ADD, DontSetFlags, 0, imm12 * sizeof(void*), rn, rt);
2652             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2653             performJITMemcpy(where, &amp;insn, sizeof(int));
2654             cacheFlush(where, sizeof(int));
2655         }
<span class="line-modified">2656 #if ASSERT_ENABLED</span>
2657         else {
2658             Datasize sf;
2659             AddOp op;
2660             SetFlags S;
2661             int shift;
2662             int imm12;
2663             RegisterID rn;
2664             RegisterID rd;
2665             ASSERT(disassembleAddSubtractImmediate(where, sf, op, S, shift, imm12, rn, rd));
2666             ASSERT(sf == Datasize_64);
2667             ASSERT(op == AddOp_ADD);
2668             ASSERT(!S);
2669             ASSERT(!shift);
2670             ASSERT(!(imm12 &amp; ~0xff8));
2671         }
<span class="line-modified">2672 #endif // ASSERT_ENABLED</span>
2673     }
2674 
2675     static void repatchPointer(void* where, void* valuePtr)
2676     {
2677         linkPointer(static_cast&lt;int*&gt;(where), valuePtr, true);
2678     }
2679 
2680     static void setPointer(int* address, void* valuePtr, RegisterID rd, bool flush)
2681     {
2682         uintptr_t value = reinterpret_cast&lt;uintptr_t&gt;(valuePtr);
2683         int buffer[3];
2684         buffer[0] = moveWideImediate(Datasize_64, MoveWideOp_Z, 0, getHalfword(value, 0), rd);
2685         buffer[1] = moveWideImediate(Datasize_64, MoveWideOp_K, 1, getHalfword(value, 1), rd);
2686         buffer[2] = moveWideImediate(Datasize_64, MoveWideOp_K, 2, getHalfword(value, 2), rd);
2687         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(address) == address);
2688         performJITMemcpy(address, buffer, sizeof(int) * 3);
2689 
2690         if (flush)
2691             cacheFlush(address, sizeof(int) * 3);
2692     }
</pre>
<hr />
<pre>
2740         expected = disassembleMoveWideImediate(address + 2, sf, opc, hw, imm16, rd);
2741         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_K &amp;&amp; hw == 2 &amp;&amp; rd == rdFirst);
2742         result |= static_cast&lt;uintptr_t&gt;(imm16) &lt;&lt; 32;
2743 #endif
2744 
2745         return reinterpret_cast&lt;void*&gt;(result);
2746     }
2747 
2748     static void* readCallTarget(void* from)
2749     {
2750         return readPointer(reinterpret_cast&lt;int*&gt;(from) - (isAddress64Bit() ? 4 : 3));
2751     }
2752 
2753     // The static relink, repatch, and replace methods can use can
2754     // use |from| for both the write and executable address for call
2755     // and jump patching as they&#39;re modifying existing (linked) code,
2756     // so the address being provided is correct for relative address
2757     // computation.
2758     static void relinkJump(void* from, void* to)
2759     {
<span class="line-modified">2760         relinkJumpOrCall&lt;BranchType_JMP&gt;(reinterpret_cast&lt;int*&gt;(from), reinterpret_cast&lt;const int*&gt;(from), to);</span>
2761         cacheFlush(from, sizeof(int));
2762     }
2763 
2764     static void relinkJumpToNop(void* from)
2765     {
2766         relinkJump(from, static_cast&lt;char*&gt;(from) + 4);
2767     }
2768 
2769     static void relinkCall(void* from, void* to)
2770     {
<span class="line-modified">2771         relinkJumpOrCall&lt;BranchType_CALL&gt;(reinterpret_cast&lt;int*&gt;(from) - 1, reinterpret_cast&lt;const int*&gt;(from) - 1, to);</span>
2772         cacheFlush(reinterpret_cast&lt;int*&gt;(from) - 1, sizeof(int));
2773     }
2774 
2775     static void repatchCompact(void* where, int32_t value)
2776     {
2777         ASSERT(!(value &amp; ~0x3ff8));
2778 
2779         MemOpSize size;
2780         bool V;
2781         MemOp opc;
2782         int imm12;
2783         RegisterID rn;
2784         RegisterID rt;
2785         bool expected = disassembleLoadStoreRegisterUnsignedImmediate(where, size, V, opc, imm12, rn, rt);
2786         ASSERT_UNUSED(expected, expected &amp;&amp; size &gt;= MemOpSize_32 &amp;&amp; !V &amp;&amp; opc == MemOp_LOAD); // expect 32/64 bit load to GPR.
2787 
2788         if (size == MemOpSize_32)
2789             imm12 = encodePositiveImmediate&lt;32&gt;(value);
2790         else
2791             imm12 = encodePositiveImmediate&lt;64&gt;(value);
2792         int insn = loadStoreRegisterUnsignedImmediate(size, V, opc, imm12, rn, rt);
2793         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(where) == where);
2794         performJITMemcpy(where, &amp;insn, sizeof(int));
2795 
2796         cacheFlush(where, sizeof(int));
2797     }
2798 
2799     unsigned debugOffset() { return m_buffer.debugOffset(); }
2800 
2801 #if OS(LINUX) &amp;&amp; COMPILER(GCC_COMPATIBLE)
2802     static inline void linuxPageFlush(uintptr_t begin, uintptr_t end)
2803     {
2804         __builtin___clear_cache(reinterpret_cast&lt;char*&gt;(begin), reinterpret_cast&lt;char*&gt;(end));
2805     }
2806 #endif
2807 
2808     static void cacheFlush(void* code, size_t size)
2809     {
<span class="line-modified">2810 #if OS(DARWIN)</span>
2811         sys_cache_control(kCacheFunctionPrepareForExecution, code, size);
2812 #elif OS(FUCHSIA)
2813         zx_cache_flush(code, size, ZX_CACHE_FLUSH_INSN);
2814 #elif OS(LINUX)
2815         size_t page = pageSize();
2816         uintptr_t current = reinterpret_cast&lt;uintptr_t&gt;(code);
2817         uintptr_t end = current + size;
2818         uintptr_t firstPageEnd = (current &amp; ~(page - 1)) + page;
2819 
2820         if (end &lt;= firstPageEnd) {
2821             linuxPageFlush(current, end);
2822             return;
2823         }
2824 
2825         linuxPageFlush(current, firstPageEnd);
2826 
2827         for (current = firstPageEnd; current + page &lt; end; current += page)
2828             linuxPageFlush(current, current + page);
2829 
2830         linuxPageFlush(current, end);
</pre>
<hr />
<pre>
2896         default:
2897             ASSERT_NOT_REACHED();
2898         }
2899 
2900         return LinkJumpNoCondition;
2901     }
2902 
2903     static JumpLinkType computeJumpType(LinkRecord&amp; record, const uint8_t* from, const uint8_t* to)
2904     {
2905         JumpLinkType linkType = computeJumpType(record.type(), from, to);
2906         record.setLinkType(linkType);
2907         return linkType;
2908     }
2909 
2910     Vector&lt;LinkRecord, 0, UnsafeVectorOverflow&gt;&amp; jumpsToLink()
2911     {
2912         std::sort(m_jumpsToLink.begin(), m_jumpsToLink.end(), linkRecordSourceComparator);
2913         return m_jumpsToLink;
2914     }
2915 
<span class="line-modified">2916     template&lt;CopyFunction copy&gt;</span>
<span class="line-modified">2917     static void ALWAYS_INLINE link(LinkRecord&amp; record, uint8_t* from, const uint8_t* fromInstruction8, uint8_t* to)</span>





















2918     {
2919         const int* fromInstruction = reinterpret_cast&lt;const int*&gt;(fromInstruction8);
2920         switch (record.linkType()) {
2921         case LinkJumpNoCondition:
<span class="line-modified">2922             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);</span>
2923             break;
2924         case LinkJumpConditionDirect:
<span class="line-modified">2925             linkConditionalBranch&lt;DirectBranch, copy&gt;(record.condition(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);</span>
2926             break;
2927         case LinkJumpCondition:
<span class="line-modified">2928             linkConditionalBranch&lt;IndirectBranch, copy&gt;(record.condition(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to);</span>
2929             break;
2930         case LinkJumpCompareAndBranchDirect:
<span class="line-modified">2931             linkCompareAndBranch&lt;DirectBranch, copy&gt;(record.condition(), record.is64Bit(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);</span>
2932             break;
2933         case LinkJumpCompareAndBranch:
<span class="line-modified">2934             linkCompareAndBranch&lt;IndirectBranch, copy&gt;(record.condition(), record.is64Bit(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to);</span>
2935             break;
2936         case LinkJumpTestBitDirect:
<span class="line-modified">2937             linkTestAndBranch&lt;DirectBranch, copy&gt;(record.condition(), record.bitNumber(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from), fromInstruction, to);</span>
2938             break;
2939         case LinkJumpTestBit:
<span class="line-modified">2940             linkTestAndBranch&lt;IndirectBranch, copy&gt;(record.condition(), record.bitNumber(), record.compareRegister(), reinterpret_cast&lt;int*&gt;(from) - 1, fromInstruction - 1, to);</span>
2941             break;
2942         default:
2943             ASSERT_NOT_REACHED();
2944             break;
2945         }
2946     }
2947 
2948 protected:
2949     template&lt;Datasize size&gt;
2950     static bool checkMovk(int insn, int _hw, RegisterID _rd)
2951     {
2952         Datasize sf;
2953         MoveWideOp opc;
2954         int hw;
2955         uint16_t imm16;
2956         RegisterID rd;
2957         bool expected = disassembleMoveWideImediate(&amp;insn, sf, opc, hw, imm16, rd);
2958 
2959         return expected
2960             &amp;&amp; sf == size
2961             &amp;&amp; opc == MoveWideOp_K
2962             &amp;&amp; hw == _hw
2963             &amp;&amp; rd == _rd;
2964     }
2965 
2966     static void linkPointer(int* address, void* valuePtr, bool flush = false)
2967     {
2968         Datasize sf;
2969         MoveWideOp opc;
2970         int hw;
2971         uint16_t imm16;
2972         RegisterID rd;
2973         bool expected = disassembleMoveWideImediate(address, sf, opc, hw, imm16, rd);
2974         ASSERT_UNUSED(expected, expected &amp;&amp; sf &amp;&amp; opc == MoveWideOp_Z &amp;&amp; !hw);
2975         ASSERT(checkMovk&lt;Datasize_64&gt;(address[1], 1, rd));
2976         ASSERT(checkMovk&lt;Datasize_64&gt;(address[2], 2, rd));
2977 
2978         setPointer(address, valuePtr, rd, flush);
2979     }
2980 
<span class="line-modified">2981     template&lt;BranchType type, CopyFunction copy = performJITMemcpy&gt;</span>
<span class="line-modified">2982     static void linkJumpOrCall(int* from, const int* fromInstruction, void* to)</span>
2983     {
<span class="line-added">2984         static_assert(type == BranchType_JMP || type == BranchType_CALL, &quot;&quot;);</span>
<span class="line-added">2985 </span>
2986         bool link;
2987         int imm26;
2988         bool isUnconditionalBranchImmediateOrNop = disassembleUnconditionalBranchImmediate(from, link, imm26) || disassembleNop(from);
2989 
2990         ASSERT_UNUSED(isUnconditionalBranchImmediateOrNop, isUnconditionalBranchImmediateOrNop);
<span class="line-added">2991         constexpr bool isCall = (type == BranchType_CALL);</span>
2992         ASSERT_UNUSED(isCall, (link == isCall) || disassembleNop(from));
2993         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
2994         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
2995         assertIsNotTagged(to);
2996         assertIsNotTagged(fromInstruction);
2997         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
2998         ASSERT(static_cast&lt;int&gt;(offset) == offset);
2999 
3000         int insn = unconditionalBranchImmediate(isCall, static_cast&lt;int&gt;(offset));
3001         RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3002         copy(from, &amp;insn, sizeof(int));
3003     }
3004 
<span class="line-modified">3005     template&lt;BranchTargetType type, CopyFunction copy = performJITMemcpy&gt;</span>
<span class="line-modified">3006     static void linkCompareAndBranch(Condition condition, bool is64Bit, RegisterID rt, int* from, const int* fromInstruction, void* to)</span>
3007     {
3008         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3009         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3010         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3011         ASSERT(isInt&lt;26&gt;(offset));
3012 
3013         bool useDirect = isInt&lt;19&gt;(offset);
<span class="line-modified">3014         ASSERT(type == IndirectBranch || useDirect);</span>
3015 
<span class="line-modified">3016         if (useDirect || type == DirectBranch) {</span>
3017             int insn = compareAndBranchImmediate(is64Bit ? Datasize_64 : Datasize_32, condition == ConditionNE, static_cast&lt;int&gt;(offset), rt);
3018             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3019             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3020             if (type == IndirectBranch) {</span>
3021                 insn = nopPseudo();
3022                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3023                 copy(from + 1, &amp;insn, sizeof(int));
3024             }
3025         } else {
3026             int insn = compareAndBranchImmediate(is64Bit ? Datasize_64 : Datasize_32, invert(condition) == ConditionNE, 2, rt);
3027             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3028             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3029             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(from + 1, fromInstruction + 1, to);</span>
3030         }
3031     }
3032 
<span class="line-modified">3033     template&lt;BranchTargetType type, CopyFunction copy = performJITMemcpy&gt;</span>
<span class="line-modified">3034     static void linkConditionalBranch(Condition condition, int* from, const int* fromInstruction, void* to)</span>
3035     {
3036         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3037         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3038         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3039         ASSERT(isInt&lt;26&gt;(offset));
3040 
3041         bool useDirect = isInt&lt;19&gt;(offset);
<span class="line-modified">3042         ASSERT(type == IndirectBranch || useDirect);</span>
3043 
<span class="line-modified">3044         if (useDirect || type == DirectBranch) {</span>
3045             int insn = conditionalBranchImmediate(static_cast&lt;int&gt;(offset), condition);
3046             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3047             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3048             if (type == IndirectBranch) {</span>
3049                 insn = nopPseudo();
3050                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3051                 copy(from + 1, &amp;insn, sizeof(int));
3052             }
3053         } else {
3054             int insn = conditionalBranchImmediate(2, invert(condition));
3055             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3056             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3057             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(from + 1, fromInstruction + 1, to);</span>
3058         }
3059     }
3060 
<span class="line-modified">3061     template&lt;BranchTargetType type, CopyFunction copy = performJITMemcpy&gt;</span>
<span class="line-modified">3062     static void linkTestAndBranch(Condition condition, unsigned bitNumber, RegisterID rt, int* from, const int* fromInstruction, void* to)</span>
3063     {
3064         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(from) &amp; 3));
3065         ASSERT(!(reinterpret_cast&lt;intptr_t&gt;(to) &amp; 3));
3066         intptr_t offset = (reinterpret_cast&lt;intptr_t&gt;(to) - reinterpret_cast&lt;intptr_t&gt;(fromInstruction)) &gt;&gt; 2;
3067         ASSERT(static_cast&lt;int&gt;(offset) == offset);
3068         ASSERT(isInt&lt;26&gt;(offset));
3069 
3070         bool useDirect = isInt&lt;14&gt;(offset);
<span class="line-modified">3071         ASSERT(type == IndirectBranch || useDirect);</span>
3072 
<span class="line-modified">3073         if (useDirect || type == DirectBranch) {</span>
3074             int insn = testAndBranchImmediate(condition == ConditionNE, static_cast&lt;int&gt;(bitNumber), static_cast&lt;int&gt;(offset), rt);
3075             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3076             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3077             if (type == IndirectBranch) {</span>
3078                 insn = nopPseudo();
3079                 RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from + 1) == (from + 1));
3080                 copy(from + 1, &amp;insn, sizeof(int));
3081             }
3082         } else {
3083             int insn = testAndBranchImmediate(invert(condition) == ConditionNE, static_cast&lt;int&gt;(bitNumber), 2, rt);
3084             RELEASE_ASSERT(roundUpToMultipleOf&lt;instructionSize&gt;(from) == from);
3085             copy(from, &amp;insn, sizeof(int));
<span class="line-modified">3086             linkJumpOrCall&lt;BranchType_JMP, copy&gt;(from + 1, fromInstruction + 1, to);</span>
3087         }
3088     }
3089 
<span class="line-modified">3090     template&lt;BranchType type&gt;</span>
3091     static void relinkJumpOrCall(int* from, const int* fromInstruction, void* to)
3092     {
<span class="line-modified">3093         static_assert(type == BranchType_JMP || type == BranchType_CALL, &quot;&quot;);</span>
<span class="line-added">3094         if ((type == BranchType_JMP) &amp;&amp; disassembleNop(from)) {</span>
3095             unsigned op01;
3096             int imm19;
3097             Condition condition;
3098             bool isConditionalBranchImmediate = disassembleConditionalBranchImmediate(from - 1, op01, imm19, condition);
3099 
3100             if (isConditionalBranchImmediate) {
3101                 ASSERT_UNUSED(op01, !op01);
<span class="line-modified">3102                 ASSERT(type == BranchType_JMP);</span>
3103 
3104                 if (imm19 == 8)
3105                     condition = invert(condition);
3106 
<span class="line-modified">3107                 linkConditionalBranch&lt;IndirectBranch&gt;(condition, from - 1, fromInstruction - 1, to);</span>
3108                 return;
3109             }
3110 
3111             Datasize opSize;
3112             bool op;
3113             RegisterID rt;
3114             bool isCompareAndBranchImmediate = disassembleCompareAndBranchImmediate(from - 1, opSize, op, imm19, rt);
3115 
3116             if (isCompareAndBranchImmediate) {
3117                 if (imm19 == 8)
3118                     op = !op;
3119 
<span class="line-modified">3120                 linkCompareAndBranch&lt;IndirectBranch&gt;(op ? ConditionNE : ConditionEQ, opSize == Datasize_64, rt, from - 1, fromInstruction - 1, to);</span>
3121                 return;
3122             }
3123 
3124             int imm14;
3125             unsigned bitNumber;
3126             bool isTestAndBranchImmediate = disassembleTestAndBranchImmediate(from - 1, op, bitNumber, imm14, rt);
3127 
3128             if (isTestAndBranchImmediate) {
3129                 if (imm14 == 8)
3130                     op = !op;
3131 
<span class="line-modified">3132                 linkTestAndBranch&lt;IndirectBranch&gt;(op ? ConditionNE : ConditionEQ, bitNumber, rt, from - 1, fromInstruction - 1, to);</span>
3133                 return;
3134             }
3135         }
3136 
<span class="line-modified">3137         linkJumpOrCall&lt;type&gt;(from, fromInstruction, to);</span>
3138     }
3139 
3140     static int* addressOf(void* code, AssemblerLabel label)
3141     {
3142         return reinterpret_cast&lt;int*&gt;(static_cast&lt;char*&gt;(code) + label.m_offset);
3143     }
3144 
3145     static RegisterID disassembleXOrSp(int reg) { return reg == 31 ? ARM64Registers::sp : static_cast&lt;RegisterID&gt;(reg); }
3146     static RegisterID disassembleXOrZr(int reg) { return reg == 31 ? ARM64Registers::zr : static_cast&lt;RegisterID&gt;(reg); }
3147     static RegisterID disassembleXOrZrOrSp(bool useZr, int reg) { return reg == 31 ? (useZr ? ARM64Registers::zr : ARM64Registers::sp) : static_cast&lt;RegisterID&gt;(reg); }
3148 
3149     static bool disassembleAddSubtractImmediate(void* address, Datasize&amp; sf, AddOp&amp; op, SetFlags&amp; S, int&amp; shift, int&amp; imm12, RegisterID&amp; rn, RegisterID&amp; rd)
3150     {
3151         int insn = *static_cast&lt;int*&gt;(address);
3152         sf = static_cast&lt;Datasize&gt;((insn &gt;&gt; 31) &amp; 1);
3153         op = static_cast&lt;AddOp&gt;((insn &gt;&gt; 30) &amp; 1);
3154         S = static_cast&lt;SetFlags&gt;((insn &gt;&gt; 29) &amp; 1);
3155         shift = (insn &gt;&gt; 22) &amp; 3;
3156         imm12 = (insn &gt;&gt; 10) &amp; 0x3ff;
3157         rn = disassembleXOrSp((insn &gt;&gt; 5) &amp; 0x1f);
</pre>
<hr />
<pre>
3213         int insn = *static_cast&lt;int*&gt;(address);
3214         op = (insn &gt;&gt; 24) &amp; 0x1;
3215         imm14 = (insn &lt;&lt; 13) &gt;&gt; 18;
3216         bitNumber = static_cast&lt;unsigned&gt;((((insn &gt;&gt; 26) &amp; 0x20)) | ((insn &gt;&gt; 19) &amp; 0x1f));
3217         rt = static_cast&lt;RegisterID&gt;(insn &amp; 0x1f);
3218         return (insn &amp; 0x7e000000) == 0x36000000;
3219 
3220     }
3221 
3222     static bool disassembleUnconditionalBranchImmediate(void* address, bool&amp; op, int&amp; imm26)
3223     {
3224         int insn = *static_cast&lt;int*&gt;(address);
3225         op = (insn &gt;&gt; 31) &amp; 1;
3226         imm26 = (insn &lt;&lt; 6) &gt;&gt; 6;
3227         return (insn &amp; 0x7c000000) == 0x14000000;
3228     }
3229 
3230     static int xOrSp(RegisterID reg)
3231     {
3232         ASSERT(!isZr(reg));
<span class="line-modified">3233         ASSERT(!isDarwin() || reg != ARM64Registers::x18);</span>
3234         return reg;
3235     }
3236     static int xOrZr(RegisterID reg)
3237     {
3238         ASSERT(!isSp(reg));
<span class="line-modified">3239         ASSERT(!isDarwin() || reg != ARM64Registers::x18);</span>
3240         return reg &amp; 31;
3241     }
3242     static FPRegisterID xOrZrAsFPR(RegisterID reg) { return static_cast&lt;FPRegisterID&gt;(xOrZr(reg)); }
3243     static int xOrZrOrSp(bool useZr, RegisterID reg) { return useZr ? xOrZr(reg) : xOrSp(reg); }
3244 
3245     ALWAYS_INLINE void insn(int instruction)
3246     {
3247         m_buffer.putInt(instruction);
3248     }
3249 
3250     ALWAYS_INLINE static int addSubtractExtendedRegister(Datasize sf, AddOp op, SetFlags S, RegisterID rm, ExtendType option, int imm3, RegisterID rn, RegisterID rd)
3251     {
3252         ASSERT(imm3 &lt; 5);
3253         // The only allocated values for opt is 0.
3254         const int opt = 0;
3255         return (0x0b200000 | sf &lt;&lt; 31 | op &lt;&lt; 30 | S &lt;&lt; 29 | opt &lt;&lt; 22 | xOrZr(rm) &lt;&lt; 16 | option &lt;&lt; 13 | (imm3 &amp; 0x7) &lt;&lt; 10 | xOrSp(rn) &lt;&lt; 5 | xOrZrOrSp(S, rd));
3256     }
3257 
3258     ALWAYS_INLINE static int addSubtractImmediate(Datasize sf, AddOp op, SetFlags S, int shift, int imm12, RegisterID rn, RegisterID rd)
3259     {
</pre>
</td>
</tr>
</table>
<center><a href="../Sources.txt.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ARM64Registers.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>