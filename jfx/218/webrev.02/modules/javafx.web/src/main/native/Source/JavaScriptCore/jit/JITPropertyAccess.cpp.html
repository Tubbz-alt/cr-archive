<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITPropertyAccess.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;CodeBlock.h&quot;
  32 #include &quot;DirectArguments.h&quot;
  33 #include &quot;GCAwareJITStubRoutine.h&quot;
  34 #include &quot;GetterSetter.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSFunction.h&quot;
  39 #include &quot;JSLexicalEnvironment.h&quot;
  40 #include &quot;JSPromise.h&quot;
  41 #include &quot;LinkBuffer.h&quot;
  42 #include &quot;OpcodeInlines.h&quot;
  43 #include &quot;ResultType.h&quot;
  44 #include &quot;ScopedArguments.h&quot;
  45 #include &quot;ScopedArgumentsTable.h&quot;
  46 #include &quot;SlowPathCall.h&quot;
  47 #include &quot;StructureStubInfo.h&quot;
  48 #include &quot;ThunkGenerators.h&quot;
  49 #include &lt;wtf/ScopedLambda.h&gt;
  50 #include &lt;wtf/StringPrintStream.h&gt;
  51 
  52 
  53 namespace JSC {
  54 #if USE(JSVALUE64)
  55 
  56 void JIT::emit_op_get_by_val(const Instruction* currentInstruction)
  57 {
  58     auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
  59     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  60     VirtualRegister dst = bytecode.m_dst;
  61     VirtualRegister base = bytecode.m_base;
  62     VirtualRegister property = bytecode.m_property;
  63     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
  64 
  65     emitGetVirtualRegister(base, regT0);
  66     emitGetVirtualRegister(property, regT1);
  67 
  68     if (metadata.m_seenIdentifiers.count() &gt; Options::getByValICMaxNumberOfIdentifiers()) {
  69         auto notCell = branchIfNotCell(regT0);
  70         emitArrayProfilingSiteWithCell(regT0, regT2, profile);
  71         notCell.link(this);
  72         callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByVal, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
  73     } else {
  74         emitJumpSlowCaseIfNotJSCell(regT0, base);
  75         emitArrayProfilingSiteWithCell(regT0, regT2, profile);
  76 
  77         JITGetByValGenerator gen(
  78             m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
  79             JSValueRegs(regT0), JSValueRegs(regT1), JSValueRegs(regT0));
  80         if (isOperandConstantInt(property))
  81             gen.stubInfo()-&gt;propertyIsInt32 = true;
  82         gen.generateFastPath(*this);
  83         addSlowCase(gen.slowPathJump());
  84         m_getByVals.append(gen);
  85 
  86         emitValueProfilingSite(bytecode.metadata(m_codeBlock));
  87         emitPutVirtualRegister(dst);
  88     }
  89 
  90 }
  91 
  92 void JIT::emitSlow_op_get_by_val(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
  93 {
  94     if (hasAnySlowCases(iter)) {
  95         auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
  96         VirtualRegister dst = bytecode.m_dst;
  97         auto&amp; metadata = bytecode.metadata(m_codeBlock);
  98         ArrayProfile* profile = &amp;metadata.m_arrayProfile;
  99 
 100         linkAllSlowCases(iter);
 101 
 102         JITGetByValGenerator&amp; gen = m_getByVals[m_getByValIndex];
 103         ++m_getByValIndex;
 104         Label coldPathBegin = label();
 105         Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByValOptimize, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), profile, regT0, regT1);
 106         gen.reportSlowPathCall(coldPathBegin, call);
 107     }
 108 }
 109 
 110 void JIT::emit_op_put_by_val_direct(const Instruction* currentInstruction)
 111 {
 112     emit_op_put_by_val&lt;OpPutByValDirect&gt;(currentInstruction);
 113 }
 114 
 115 template&lt;typename Op&gt;
 116 void JIT::emit_op_put_by_val(const Instruction* currentInstruction)
 117 {
 118     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 119     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 120     VirtualRegister base = bytecode.m_base;
 121     VirtualRegister property = bytecode.m_property;
 122     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
 123     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
 124 
 125     emitGetVirtualRegister(base, regT0);
 126     bool propertyNameIsIntegerConstant = isOperandConstantInt(property);
 127     if (propertyNameIsIntegerConstant)
 128         move(Imm32(getOperandConstantInt(property)), regT1);
 129     else
 130         emitGetVirtualRegister(property, regT1);
 131 
 132     emitJumpSlowCaseIfNotJSCell(regT0, base);
 133     PatchableJump notIndex;
 134     if (!propertyNameIsIntegerConstant) {
 135         notIndex = emitPatchableJumpIfNotInt(regT1);
 136         addSlowCase(notIndex);
 137         // See comment in op_get_by_val.
 138         zeroExtend32ToPtr(regT1, regT1);
 139     }
 140     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
 141 
 142     PatchableJump badType;
 143     JumpList slowCases;
 144 
 145     // FIXME: Maybe we should do this inline?
 146     addSlowCase(branchTest32(NonZero, regT2, TrustedImm32(CopyOnWrite)));
 147     and32(TrustedImm32(IndexingShapeMask), regT2);
 148 
 149     JITArrayMode mode = chooseArrayMode(profile);
 150     switch (mode) {
 151     case JITInt32:
 152         slowCases = emitInt32PutByVal(bytecode, badType);
 153         break;
 154     case JITDouble:
 155         slowCases = emitDoublePutByVal(bytecode, badType);
 156         break;
 157     case JITContiguous:
 158         slowCases = emitContiguousPutByVal(bytecode, badType);
 159         break;
 160     case JITArrayStorage:
 161         slowCases = emitArrayStoragePutByVal(bytecode, badType);
 162         break;
 163     default:
 164         CRASH();
 165         break;
 166     }
 167 
 168     addSlowCase(badType);
 169     addSlowCase(slowCases);
 170 
 171     Label done = label();
 172 
 173     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeIndex, notIndex, badType, mode, profile, done, done));
 174 }
 175 
 176 template&lt;typename Op&gt;
 177 JIT::JumpList JIT::emitGenericContiguousPutByVal(Op bytecode, PatchableJump&amp; badType, IndexingType indexingShape)
 178 {
 179     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 180     VirtualRegister value = bytecode.m_value;
 181     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
 182 
 183     JumpList slowCases;
 184 
 185     badType = patchableBranch32(NotEqual, regT2, TrustedImm32(indexingShape));
 186 
 187     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT2);
 188     Jump outOfBounds = branch32(AboveOrEqual, regT1, Address(regT2, Butterfly::offsetOfPublicLength()));
 189 
 190     Label storeResult = label();
 191     emitGetVirtualRegister(value, regT3);
 192     switch (indexingShape) {
 193     case Int32Shape:
 194         slowCases.append(branchIfNotInt32(regT3));
 195         store64(regT3, BaseIndex(regT2, regT1, TimesEight));
 196         break;
 197     case DoubleShape: {
 198         Jump notInt = branchIfNotInt32(regT3);
 199         convertInt32ToDouble(regT3, fpRegT0);
 200         Jump ready = jump();
 201         notInt.link(this);
 202         add64(numberTagRegister, regT3);
 203         move64ToDouble(regT3, fpRegT0);
 204         slowCases.append(branchIfNaN(fpRegT0));
 205         ready.link(this);
 206         storeDouble(fpRegT0, BaseIndex(regT2, regT1, TimesEight));
 207         break;
 208     }
 209     case ContiguousShape:
 210         store64(regT3, BaseIndex(regT2, regT1, TimesEight));
 211         emitWriteBarrier(bytecode.m_base, value, ShouldFilterValue);
 212         break;
 213     default:
 214         CRASH();
 215         break;
 216     }
 217 
 218     Jump done = jump();
 219     outOfBounds.link(this);
 220 
 221     slowCases.append(branch32(AboveOrEqual, regT1, Address(regT2, Butterfly::offsetOfVectorLength())));
 222 
 223     emitArrayProfileStoreToHoleSpecialCase(profile);
 224 
 225     add32(TrustedImm32(1), regT1, regT3);
 226     store32(regT3, Address(regT2, Butterfly::offsetOfPublicLength()));
 227     jump().linkTo(storeResult, this);
 228 
 229     done.link(this);
 230 
 231     return slowCases;
 232 }
 233 
 234 template&lt;typename Op&gt;
 235 JIT::JumpList JIT::emitArrayStoragePutByVal(Op bytecode, PatchableJump&amp; badType)
 236 {
 237     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 238     VirtualRegister value = bytecode.m_value;
 239     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
 240 
 241     JumpList slowCases;
 242 
 243     badType = patchableBranch32(NotEqual, regT2, TrustedImm32(ArrayStorageShape));
 244     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT2);
 245     slowCases.append(branch32(AboveOrEqual, regT1, Address(regT2, ArrayStorage::vectorLengthOffset())));
 246 
 247     Jump empty = branchTest64(Zero, BaseIndex(regT2, regT1, TimesEight, ArrayStorage::vectorOffset()));
 248 
 249     Label storeResult(this);
 250     emitGetVirtualRegister(value, regT3);
 251     store64(regT3, BaseIndex(regT2, regT1, TimesEight, ArrayStorage::vectorOffset()));
 252     emitWriteBarrier(bytecode.m_base, value, ShouldFilterValue);
 253     Jump end = jump();
 254 
 255     empty.link(this);
 256     emitArrayProfileStoreToHoleSpecialCase(profile);
 257     add32(TrustedImm32(1), Address(regT2, ArrayStorage::numValuesInVectorOffset()));
 258     branch32(Below, regT1, Address(regT2, ArrayStorage::lengthOffset())).linkTo(storeResult, this);
 259 
 260     add32(TrustedImm32(1), regT1);
 261     store32(regT1, Address(regT2, ArrayStorage::lengthOffset()));
 262     sub32(TrustedImm32(1), regT1);
 263     jump().linkTo(storeResult, this);
 264 
 265     end.link(this);
 266 
 267     return slowCases;
 268 }
 269 
 270 template&lt;typename Op&gt;
 271 JITPutByIdGenerator JIT::emitPutByValWithCachedId(ByValInfo* byValInfo, Op bytecode, PutKind putKind, const Identifier&amp; propertyName, JumpList&amp; doneCases, JumpList&amp; slowCases)
 272 {
 273     // base: regT0
 274     // property: regT1
 275     // scratch: regT2
 276 
 277     VirtualRegister base = bytecode.m_base;
 278     VirtualRegister value = bytecode.m_value;
 279 
 280     slowCases.append(branchIfNotCell(regT1));
 281     emitByValIdentifierCheck(byValInfo, regT1, regT1, propertyName, slowCases);
 282 
 283     // Write barrier breaks the registers. So after issuing the write barrier,
 284     // reload the registers.
 285     emitGetVirtualRegisters(base, regT0, value, regT1);
 286 
 287     JITPutByIdGenerator gen(
 288         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 289         JSValueRegs(regT0), JSValueRegs(regT1), regT2, m_codeBlock-&gt;ecmaMode(), putKind);
 290     gen.generateFastPath(*this);
 291     emitWriteBarrier(base, value, ShouldFilterBase);
 292     doneCases.append(jump());
 293 
 294     Label coldPathBegin = label();
 295     gen.slowPathJump().link(this);
 296 
 297     Call call = callOperation(gen.slowPathFunction(), TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT1, regT0, propertyName.impl());
 298     gen.reportSlowPathCall(coldPathBegin, call);
 299     doneCases.append(jump());
 300 
 301     return gen;
 302 }
 303 
 304 void JIT::emitSlow_op_put_by_val(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 305 {
 306     bool isDirect = currentInstruction-&gt;opcodeID() == op_put_by_val_direct;
 307     VirtualRegister base;
 308     VirtualRegister property;
 309     VirtualRegister value;
 310 
 311     auto load = [&amp;](auto bytecode) {
 312         base = bytecode.m_base;
 313         property = bytecode.m_property;
 314         value = bytecode.m_value;
 315     };
 316 
 317     if (isDirect)
 318         load(currentInstruction-&gt;as&lt;OpPutByValDirect&gt;());
 319     else
 320         load(currentInstruction-&gt;as&lt;OpPutByVal&gt;());
 321 
 322     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
 323 
 324     linkAllSlowCases(iter);
 325     Label slowPath = label();
 326 
 327     emitGetVirtualRegister(base, regT0);
 328     emitGetVirtualRegister(property, regT1);
 329     emitGetVirtualRegister(value, regT2);
 330     Call call = callOperation(isDirect ? operationDirectPutByValOptimize : operationPutByValOptimize, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, regT2, byValInfo);
 331 
 332     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
 333     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
 334     m_byValInstructionIndex++;
 335 }
 336 
 337 void JIT::emit_op_put_getter_by_id(const Instruction* currentInstruction)
 338 {
 339     auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterById&gt;();
 340     emitGetVirtualRegister(bytecode.m_base, regT0);
 341     int32_t options = bytecode.m_attributes;
 342     emitGetVirtualRegister(bytecode.m_accessor, regT1);
 343     callOperation(operationPutGetterById, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, m_codeBlock-&gt;identifier(bytecode.m_property).impl(), options, regT1);
 344 }
 345 
 346 void JIT::emit_op_put_setter_by_id(const Instruction* currentInstruction)
 347 {
 348     auto bytecode = currentInstruction-&gt;as&lt;OpPutSetterById&gt;();
 349     emitGetVirtualRegister(bytecode.m_base, regT0);
 350     int32_t options = bytecode.m_attributes;
 351     emitGetVirtualRegister(bytecode.m_accessor, regT1);
 352     callOperation(operationPutSetterById, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, m_codeBlock-&gt;identifier(bytecode.m_property).impl(), options, regT1);
 353 }
 354 
 355 void JIT::emit_op_put_getter_setter_by_id(const Instruction* currentInstruction)
 356 {
 357     auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterSetterById&gt;();
 358     emitGetVirtualRegister(bytecode.m_base, regT0);
 359     int32_t attribute = bytecode.m_attributes;
 360     emitGetVirtualRegister(bytecode.m_getter, regT1);
 361     emitGetVirtualRegister(bytecode.m_setter, regT2);
 362     callOperation(operationPutGetterSetter, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, m_codeBlock-&gt;identifier(bytecode.m_property).impl(), attribute, regT1, regT2);
 363 }
 364 
 365 void JIT::emit_op_put_getter_by_val(const Instruction* currentInstruction)
 366 {
 367     auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterByVal&gt;();
 368     emitGetVirtualRegister(bytecode.m_base, regT0);
 369     emitGetVirtualRegister(bytecode.m_property, regT1);
 370     int32_t attributes = bytecode.m_attributes;
 371     emitGetVirtualRegister(bytecode.m_accessor, regT2);
 372     callOperation(operationPutGetterByVal, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, attributes, regT2);
 373 }
 374 
 375 void JIT::emit_op_put_setter_by_val(const Instruction* currentInstruction)
 376 {
 377     auto bytecode = currentInstruction-&gt;as&lt;OpPutSetterByVal&gt;();
 378     emitGetVirtualRegister(bytecode.m_base, regT0);
 379     emitGetVirtualRegister(bytecode.m_property, regT1);
 380     int32_t attributes = bytecode.m_attributes;
 381     emitGetVirtualRegister(bytecode.m_accessor, regT2);
 382     callOperation(operationPutSetterByVal, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, attributes, regT2);
 383 }
 384 
 385 void JIT::emit_op_del_by_id(const Instruction* currentInstruction)
 386 {
 387     auto bytecode = currentInstruction-&gt;as&lt;OpDelById&gt;();
 388     VirtualRegister dst = bytecode.m_dst;
 389     VirtualRegister base = bytecode.m_base;
 390     int property = bytecode.m_property;
 391     emitGetVirtualRegister(base, regT0);
 392     callOperation(operationDeleteByIdJSResult, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, m_codeBlock-&gt;identifier(property).impl());
 393 }
 394 
 395 void JIT::emit_op_del_by_val(const Instruction* currentInstruction)
 396 {
 397     auto bytecode = currentInstruction-&gt;as&lt;OpDelByVal&gt;();
 398     VirtualRegister dst = bytecode.m_dst;
 399     VirtualRegister base = bytecode.m_base;
 400     VirtualRegister property = bytecode.m_property;
 401     emitGetVirtualRegister(base, regT0);
 402     emitGetVirtualRegister(property, regT1);
 403     callOperation(operationDeleteByValJSResult, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);
 404 }
 405 
 406 void JIT::emit_op_try_get_by_id(const Instruction* currentInstruction)
 407 {
 408     auto bytecode = currentInstruction-&gt;as&lt;OpTryGetById&gt;();
 409     VirtualRegister resultVReg = bytecode.m_dst;
 410     VirtualRegister baseVReg = bytecode.m_base;
 411     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 412 
 413     emitGetVirtualRegister(baseVReg, regT0);
 414 
 415     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 416 
 417     JITGetByIdGenerator gen(
 418         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 419         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::TryGetById);
 420     gen.generateFastPath(*this);
 421     addSlowCase(gen.slowPathJump());
 422     m_getByIds.append(gen);
 423 
 424     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 425     emitPutVirtualRegister(resultVReg);
 426 }
 427 
 428 void JIT::emitSlow_op_try_get_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 429 {
 430     linkAllSlowCases(iter);
 431 
 432     auto bytecode = currentInstruction-&gt;as&lt;OpTryGetById&gt;();
 433     VirtualRegister resultVReg = bytecode.m_dst;
 434     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 435 
 436     JITGetByIdGenerator&amp; gen = m_getByIds[m_getByIdIndex++];
 437 
 438     Label coldPathBegin = label();
 439 
 440     Call call = callOperation(operationTryGetByIdOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT0, ident-&gt;impl());
 441 
 442     gen.reportSlowPathCall(coldPathBegin, call);
 443 }
 444 
 445 void JIT::emit_op_get_by_id_direct(const Instruction* currentInstruction)
 446 {
 447     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdDirect&gt;();
 448     VirtualRegister resultVReg = bytecode.m_dst;
 449     VirtualRegister baseVReg = bytecode.m_base;
 450     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 451 
 452     emitGetVirtualRegister(baseVReg, regT0);
 453 
 454     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 455 
 456     JITGetByIdGenerator gen(
 457         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 458         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::GetByIdDirect);
 459     gen.generateFastPath(*this);
 460     addSlowCase(gen.slowPathJump());
 461     m_getByIds.append(gen);
 462 
 463     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 464     emitPutVirtualRegister(resultVReg);
 465 }
 466 
 467 void JIT::emitSlow_op_get_by_id_direct(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 468 {
 469     linkAllSlowCases(iter);
 470 
 471     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdDirect&gt;();
 472     VirtualRegister resultVReg = bytecode.m_dst;
 473     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 474 
 475     JITGetByIdGenerator&amp; gen = m_getByIds[m_getByIdIndex++];
 476 
 477     Label coldPathBegin = label();
 478 
 479     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdDirectOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT0, ident-&gt;impl());
 480 
 481     gen.reportSlowPathCall(coldPathBegin, call);
 482 }
 483 
 484 void JIT::emit_op_get_by_id(const Instruction* currentInstruction)
 485 {
 486     auto bytecode = currentInstruction-&gt;as&lt;OpGetById&gt;();
 487     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 488     VirtualRegister resultVReg = bytecode.m_dst;
 489     VirtualRegister baseVReg = bytecode.m_base;
 490     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 491 
 492     emitGetVirtualRegister(baseVReg, regT0);
 493 
 494     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 495 
 496     if (*ident == m_vm-&gt;propertyNames-&gt;length &amp;&amp; shouldEmitProfiling()) {
 497         Jump notArrayLengthMode = branch8(NotEqual, AbsoluteAddress(&amp;metadata.m_modeMetadata.mode), TrustedImm32(static_cast&lt;uint8_t&gt;(GetByIdMode::ArrayLength)));
 498         emitArrayProfilingSiteWithCell(regT0, regT1, &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile);
 499         notArrayLengthMode.link(this);
 500     }
 501 
 502     JITGetByIdGenerator gen(
 503         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 504         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::GetById);
 505     gen.generateFastPath(*this);
 506     addSlowCase(gen.slowPathJump());
 507     m_getByIds.append(gen);
 508 
 509     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 510     emitPutVirtualRegister(resultVReg);
 511 }
 512 
 513 void JIT::emit_op_get_by_id_with_this(const Instruction* currentInstruction)
 514 {
 515     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
 516     VirtualRegister resultVReg = bytecode.m_dst;
 517     VirtualRegister baseVReg = bytecode.m_base;
 518     VirtualRegister thisVReg = bytecode.m_thisValue;
 519     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 520 
 521     emitGetVirtualRegister(baseVReg, regT0);
 522     emitGetVirtualRegister(thisVReg, regT1);
 523     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 524     emitJumpSlowCaseIfNotJSCell(regT1, thisVReg);
 525 
 526     JITGetByIdWithThisGenerator gen(
 527         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 528         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), JSValueRegs(regT1));
 529     gen.generateFastPath(*this);
 530     addSlowCase(gen.slowPathJump());
 531     m_getByIdsWithThis.append(gen);
 532 
 533     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 534     emitPutVirtualRegister(resultVReg);
 535 }
 536 
 537 void JIT::emitSlow_op_get_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 538 {
 539     linkAllSlowCases(iter);
 540 
 541     auto bytecode = currentInstruction-&gt;as&lt;OpGetById&gt;();
 542     VirtualRegister resultVReg = bytecode.m_dst;
 543     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 544 
 545     JITGetByIdGenerator&amp; gen = m_getByIds[m_getByIdIndex++];
 546 
 547     Label coldPathBegin = label();
 548 
 549     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT0, ident-&gt;impl());
 550 
 551     gen.reportSlowPathCall(coldPathBegin, call);
 552 }
 553 
 554 void JIT::emitSlow_op_get_by_id_with_this(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 555 {
 556     linkAllSlowCases(iter);
 557 
 558     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
 559     VirtualRegister resultVReg = bytecode.m_dst;
 560     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 561 
 562     JITGetByIdWithThisGenerator&amp; gen = m_getByIdsWithThis[m_getByIdWithThisIndex++];
 563 
 564     Label coldPathBegin = label();
 565 
 566     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdWithThisOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT0, regT1, ident-&gt;impl());
 567 
 568     gen.reportSlowPathCall(coldPathBegin, call);
 569 }
 570 
 571 void JIT::emit_op_put_by_id(const Instruction* currentInstruction)
 572 {
 573     auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
 574     VirtualRegister baseVReg = bytecode.m_base;
 575     VirtualRegister valueVReg = bytecode.m_value;
 576     bool direct = !!(bytecode.m_flags &amp; PutByIdIsDirect);
 577 
 578     // In order to be able to patch both the Structure, and the object offset, we store one pointer,
 579     // to just after the arguments have been loaded into registers &#39;hotPathBegin&#39;, and we generate code
 580     // such that the Structure &amp; offset are always at the same distance from this.
 581 
 582     emitGetVirtualRegisters(baseVReg, regT0, valueVReg, regT1);
 583 
 584     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 585 
 586     JITPutByIdGenerator gen(
 587         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 588         JSValueRegs(regT0), JSValueRegs(regT1), regT2, m_codeBlock-&gt;ecmaMode(),
 589         direct ? Direct : NotDirect);
 590 
 591     gen.generateFastPath(*this);
 592     addSlowCase(gen.slowPathJump());
 593 
 594     emitWriteBarrier(baseVReg, valueVReg, ShouldFilterBase);
 595 
 596     m_putByIds.append(gen);
 597 }
 598 
 599 void JIT::emitSlow_op_put_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 600 {
 601     linkAllSlowCases(iter);
 602 
 603     auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
 604     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 605 
 606     Label coldPathBegin(this);
 607 
 608     JITPutByIdGenerator&amp; gen = m_putByIds[m_putByIdIndex++];
 609 
 610     Call call = callOperation(gen.slowPathFunction(), TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT1, regT0, ident-&gt;impl());
 611 
 612     gen.reportSlowPathCall(coldPathBegin, call);
 613 }
 614 
 615 void JIT::emit_op_in_by_id(const Instruction* currentInstruction)
 616 {
 617     auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
 618     VirtualRegister resultVReg = bytecode.m_dst;
 619     VirtualRegister baseVReg = bytecode.m_base;
 620     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 621 
 622     emitGetVirtualRegister(baseVReg, regT0);
 623 
 624     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 625 
 626     JITInByIdGenerator gen(
 627         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex), RegisterSet::stubUnavailableRegisters(),
 628         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0));
 629     gen.generateFastPath(*this);
 630     addSlowCase(gen.slowPathJump());
 631     m_inByIds.append(gen);
 632 
 633     emitPutVirtualRegister(resultVReg);
 634 }
 635 
 636 void JIT::emitSlow_op_in_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 637 {
 638     linkAllSlowCases(iter);
 639 
 640     auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
 641     VirtualRegister resultVReg = bytecode.m_dst;
 642     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 643 
 644     JITInByIdGenerator&amp; gen = m_inByIds[m_inByIdIndex++];
 645 
 646     Label coldPathBegin = label();
 647 
 648     Call call = callOperation(operationInByIdOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT0, ident-&gt;impl());
 649 
 650     gen.reportSlowPathCall(coldPathBegin, call);
 651 }
 652 
 653 void JIT::emitVarInjectionCheck(bool needsVarInjectionChecks)
 654 {
 655     if (!needsVarInjectionChecks)
 656         return;
 657     addSlowCase(branch8(Equal, AbsoluteAddress(m_codeBlock-&gt;globalObject()-&gt;varInjectionWatchpoint()-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 658 }
 659 
 660 void JIT::emitResolveClosure(VirtualRegister dst, VirtualRegister scope, bool needsVarInjectionChecks, unsigned depth)
 661 {
 662     emitVarInjectionCheck(needsVarInjectionChecks);
 663     emitGetVirtualRegister(scope, regT0);
 664     for (unsigned i = 0; i &lt; depth; ++i)
 665         loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 666     emitPutVirtualRegister(dst);
 667 }
 668 
 669 void JIT::emit_op_resolve_scope(const Instruction* currentInstruction)
 670 {
 671     auto bytecode = currentInstruction-&gt;as&lt;OpResolveScope&gt;();
 672     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 673     VirtualRegister dst = bytecode.m_dst;
 674     VirtualRegister scope = bytecode.m_scope;
 675     ResolveType resolveType = metadata.m_resolveType;
 676     unsigned depth = metadata.m_localScopeDepth;
 677 
 678     auto emitCode = [&amp;] (ResolveType resolveType) {
 679         switch (resolveType) {
 680         case GlobalProperty:
 681         case GlobalPropertyWithVarInjectionChecks: {
 682             JSScope* constantScope = JSScope::constantScopeForCodeBlock(resolveType, m_codeBlock);
 683             RELEASE_ASSERT(constantScope);
 684             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 685             load32(&amp;metadata.m_globalLexicalBindingEpoch, regT1);
 686             addSlowCase(branch32(NotEqual, AbsoluteAddress(m_codeBlock-&gt;globalObject()-&gt;addressOfGlobalLexicalBindingEpoch()), regT1));
 687             move(TrustedImmPtr(constantScope), regT0);
 688             emitPutVirtualRegister(dst);
 689             break;
 690         }
 691 
 692         case GlobalVar:
 693         case GlobalVarWithVarInjectionChecks:
 694         case GlobalLexicalVar:
 695         case GlobalLexicalVarWithVarInjectionChecks: {
 696             JSScope* constantScope = JSScope::constantScopeForCodeBlock(resolveType, m_codeBlock);
 697             RELEASE_ASSERT(constantScope);
 698             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 699             move(TrustedImmPtr(constantScope), regT0);
 700             emitPutVirtualRegister(dst);
 701             break;
 702         }
 703         case ClosureVar:
 704         case ClosureVarWithVarInjectionChecks:
 705             emitResolveClosure(dst, scope, needsVarInjectionChecks(resolveType), depth);
 706             break;
 707         case ModuleVar:
 708             move(TrustedImmPtr(metadata.m_lexicalEnvironment.get()), regT0);
 709             emitPutVirtualRegister(dst);
 710             break;
 711         case Dynamic:
 712             addSlowCase(jump());
 713             break;
 714         case LocalClosureVar:
 715         case UnresolvedProperty:
 716         case UnresolvedPropertyWithVarInjectionChecks:
 717             RELEASE_ASSERT_NOT_REACHED();
 718         }
 719     };
 720 
 721     switch (resolveType) {
 722     case GlobalProperty:
 723     case GlobalPropertyWithVarInjectionChecks: {
 724         JumpList skipToEnd;
 725         load32(&amp;metadata.m_resolveType, regT0);
 726 
 727         Jump notGlobalProperty = branch32(NotEqual, regT0, TrustedImm32(resolveType));
 728         emitCode(resolveType);
 729         skipToEnd.append(jump());
 730 
 731         notGlobalProperty.link(this);
 732         emitCode(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar);
 733 
 734         skipToEnd.link(this);
 735         break;
 736     }
 737     case UnresolvedProperty:
 738     case UnresolvedPropertyWithVarInjectionChecks: {
 739         JumpList skipToEnd;
 740         load32(&amp;metadata.m_resolveType, regT0);
 741 
 742         Jump notGlobalProperty = branch32(NotEqual, regT0, TrustedImm32(GlobalProperty));
 743         emitCode(GlobalProperty);
 744         skipToEnd.append(jump());
 745         notGlobalProperty.link(this);
 746 
 747         Jump notGlobalPropertyWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalPropertyWithVarInjectionChecks));
 748         emitCode(GlobalPropertyWithVarInjectionChecks);
 749         skipToEnd.append(jump());
 750         notGlobalPropertyWithVarInjections.link(this);
 751 
 752         Jump notGlobalLexicalVar = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVar));
 753         emitCode(GlobalLexicalVar);
 754         skipToEnd.append(jump());
 755         notGlobalLexicalVar.link(this);
 756 
 757         Jump notGlobalLexicalVarWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVarWithVarInjectionChecks));
 758         emitCode(GlobalLexicalVarWithVarInjectionChecks);
 759         skipToEnd.append(jump());
 760         notGlobalLexicalVarWithVarInjections.link(this);
 761 
 762         addSlowCase(jump());
 763         skipToEnd.link(this);
 764         break;
 765     }
 766 
 767     default:
 768         emitCode(resolveType);
 769         break;
 770     }
 771 }
 772 
 773 void JIT::emitLoadWithStructureCheck(VirtualRegister scope, Structure** structureSlot)
 774 {
 775     loadPtr(structureSlot, regT1);
 776     emitGetVirtualRegister(scope, regT0);
 777     addSlowCase(branchTestPtr(Zero, regT1));
 778     load32(Address(regT1, Structure::structureIDOffset()), regT1);
 779     addSlowCase(branch32(NotEqual, Address(regT0, JSCell::structureIDOffset()), regT1));
 780 }
 781 
 782 void JIT::emitGetVarFromPointer(JSValue* operand, GPRReg reg)
 783 {
 784     loadPtr(operand, reg);
 785 }
 786 
 787 void JIT::emitGetVarFromIndirectPointer(JSValue** operand, GPRReg reg)
 788 {
 789     loadPtr(operand, reg);
 790     loadPtr(reg, reg);
 791 }
 792 
 793 void JIT::emitGetClosureVar(VirtualRegister scope, uintptr_t operand)
 794 {
 795     emitGetVirtualRegister(scope, regT0);
 796     loadPtr(Address(regT0, JSLexicalEnvironment::offsetOfVariables() + operand * sizeof(Register)), regT0);
 797 }
 798 
 799 void JIT::emit_op_get_from_scope(const Instruction* currentInstruction)
 800 {
 801     auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
 802     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 803     VirtualRegister dst = bytecode.m_dst;
 804     VirtualRegister scope = bytecode.m_scope;
 805     ResolveType resolveType = metadata.m_getPutInfo.resolveType();
 806     Structure** structureSlot = metadata.m_structure.slot();
 807     uintptr_t* operandSlot = reinterpret_cast&lt;uintptr_t*&gt;(&amp;metadata.m_operand);
 808 
 809     auto emitCode = [&amp;] (ResolveType resolveType, bool indirectLoadForOperand) {
 810         switch (resolveType) {
 811         case GlobalProperty:
 812         case GlobalPropertyWithVarInjectionChecks: {
 813             emitLoadWithStructureCheck(scope, structureSlot); // Structure check covers var injection since we don&#39;t cache structures for anything but the GlobalObject. Additionally, resolve_scope handles checking for the var injection.
 814             GPRReg base = regT0;
 815             GPRReg result = regT0;
 816             GPRReg offset = regT1;
 817             GPRReg scratch = regT2;
 818 
 819             jitAssert(scopedLambda&lt;Jump(void)&gt;([&amp;] () -&gt; Jump {
 820                 return branchPtr(Equal, base, TrustedImmPtr(m_codeBlock-&gt;globalObject()));
 821             }));
 822 
 823             load32(operandSlot, offset);
 824             if (ASSERT_ENABLED) {
 825                 Jump isOutOfLine = branch32(GreaterThanOrEqual, offset, TrustedImm32(firstOutOfLineOffset));
 826                 abortWithReason(JITOffsetIsNotOutOfLine);
 827                 isOutOfLine.link(this);
 828             }
 829             loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
 830             neg32(offset);
 831             signExtend32ToPtr(offset, offset);
 832             load64(BaseIndex(scratch, offset, TimesEight, (firstOutOfLineOffset - 2) * sizeof(EncodedJSValue)), result);
 833             break;
 834         }
 835         case GlobalVar:
 836         case GlobalVarWithVarInjectionChecks:
 837         case GlobalLexicalVar:
 838         case GlobalLexicalVarWithVarInjectionChecks:
 839             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 840             if (indirectLoadForOperand)
 841                 emitGetVarFromIndirectPointer(bitwise_cast&lt;JSValue**&gt;(operandSlot), regT0);
 842             else
 843                 emitGetVarFromPointer(bitwise_cast&lt;JSValue*&gt;(*operandSlot), regT0);
 844             if (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks) // TDZ check.
 845                 addSlowCase(branchIfEmpty(regT0));
 846             break;
 847         case ClosureVar:
 848         case ClosureVarWithVarInjectionChecks:
 849             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 850             emitGetClosureVar(scope, *operandSlot);
 851             break;
 852         case Dynamic:
 853             addSlowCase(jump());
 854             break;
 855         case LocalClosureVar:
 856         case ModuleVar:
 857         case UnresolvedProperty:
 858         case UnresolvedPropertyWithVarInjectionChecks:
 859             RELEASE_ASSERT_NOT_REACHED();
 860         }
 861     };
 862 
 863     switch (resolveType) {
 864     case GlobalProperty:
 865     case GlobalPropertyWithVarInjectionChecks: {
 866         JumpList skipToEnd;
 867         load32(&amp;metadata.m_getPutInfo, regT0);
 868         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
 869 
 870         Jump isNotGlobalProperty = branch32(NotEqual, regT0, TrustedImm32(resolveType));
 871         emitCode(resolveType, false);
 872         skipToEnd.append(jump());
 873 
 874         isNotGlobalProperty.link(this);
 875         emitCode(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar, true);
 876 
 877         skipToEnd.link(this);
 878         break;
 879     }
 880     case UnresolvedProperty:
 881     case UnresolvedPropertyWithVarInjectionChecks: {
 882         JumpList skipToEnd;
 883         load32(&amp;metadata.m_getPutInfo, regT0);
 884         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
 885 
 886         Jump isGlobalProperty = branch32(Equal, regT0, TrustedImm32(GlobalProperty));
 887         Jump notGlobalPropertyWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalPropertyWithVarInjectionChecks));
 888         isGlobalProperty.link(this);
 889         emitCode(GlobalProperty, false);
 890         skipToEnd.append(jump());
 891         notGlobalPropertyWithVarInjections.link(this);
 892 
 893         Jump notGlobalLexicalVar = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVar));
 894         emitCode(GlobalLexicalVar, true);
 895         skipToEnd.append(jump());
 896         notGlobalLexicalVar.link(this);
 897 
 898         Jump notGlobalLexicalVarWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVarWithVarInjectionChecks));
 899         emitCode(GlobalLexicalVarWithVarInjectionChecks, true);
 900         skipToEnd.append(jump());
 901         notGlobalLexicalVarWithVarInjections.link(this);
 902 
 903         addSlowCase(jump());
 904 
 905         skipToEnd.link(this);
 906         break;
 907     }
 908 
 909     default:
 910         emitCode(resolveType, false);
 911         break;
 912     }
 913     emitPutVirtualRegister(dst);
 914     emitValueProfilingSite(metadata);
 915 }
 916 
 917 void JIT::emitSlow_op_get_from_scope(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 918 {
 919     linkAllSlowCases(iter);
 920 
 921     auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
 922     VirtualRegister dst = bytecode.m_dst;
 923     callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetFromScope, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), currentInstruction);
 924 }
 925 
 926 void JIT::emitPutGlobalVariable(JSValue* operand, VirtualRegister value, WatchpointSet* set)
 927 {
 928     emitGetVirtualRegister(value, regT0);
 929     emitNotifyWrite(set);
 930     storePtr(regT0, operand);
 931 }
 932 void JIT::emitPutGlobalVariableIndirect(JSValue** addressOfOperand, VirtualRegister value, WatchpointSet** indirectWatchpointSet)
 933 {
 934     emitGetVirtualRegister(value, regT0);
 935     loadPtr(indirectWatchpointSet, regT1);
 936     emitNotifyWrite(regT1);
 937     loadPtr(addressOfOperand, regT1);
 938     storePtr(regT0, regT1);
 939 }
 940 
 941 void JIT::emitPutClosureVar(VirtualRegister scope, uintptr_t operand, VirtualRegister value, WatchpointSet* set)
 942 {
 943     emitGetVirtualRegister(value, regT1);
 944     emitGetVirtualRegister(scope, regT0);
 945     emitNotifyWrite(set);
 946     storePtr(regT1, Address(regT0, JSLexicalEnvironment::offsetOfVariables() + operand * sizeof(Register)));
 947 }
 948 
 949 void JIT::emit_op_put_to_scope(const Instruction* currentInstruction)
 950 {
 951     auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
 952     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 953     VirtualRegister scope = bytecode.m_scope;
 954     VirtualRegister value = bytecode.m_value;
 955     GetPutInfo getPutInfo = copiedGetPutInfo(bytecode);
 956     ResolveType resolveType = getPutInfo.resolveType();
 957     Structure** structureSlot = metadata.m_structure.slot();
 958     uintptr_t* operandSlot = reinterpret_cast&lt;uintptr_t*&gt;(&amp;metadata.m_operand);
 959 
 960     auto emitCode = [&amp;] (ResolveType resolveType, bool indirectLoadForOperand) {
 961         switch (resolveType) {
 962         case GlobalProperty:
 963         case GlobalPropertyWithVarInjectionChecks: {
 964             emitLoadWithStructureCheck(scope, structureSlot); // Structure check covers var injection since we don&#39;t cache structures for anything but the GlobalObject. Additionally, resolve_scope handles checking for the var injection.
 965             emitGetVirtualRegister(value, regT2);
 966 
 967             jitAssert(scopedLambda&lt;Jump(void)&gt;([&amp;] () -&gt; Jump {
 968                 return branchPtr(Equal, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()));
 969             }));
 970 
 971             loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
 972             loadPtr(operandSlot, regT1);
 973             negPtr(regT1);
 974             storePtr(regT2, BaseIndex(regT0, regT1, TimesEight, (firstOutOfLineOffset - 2) * sizeof(EncodedJSValue)));
 975             emitWriteBarrier(m_codeBlock-&gt;globalObject(), value, ShouldFilterValue);
 976             break;
 977         }
 978         case GlobalVar:
 979         case GlobalVarWithVarInjectionChecks:
 980         case GlobalLexicalVar:
 981         case GlobalLexicalVarWithVarInjectionChecks: {
 982             JSScope* constantScope = JSScope::constantScopeForCodeBlock(resolveType, m_codeBlock);
 983             RELEASE_ASSERT(constantScope);
 984             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 985             if (!isInitialization(getPutInfo.initializationMode()) &amp;&amp; (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)) {
 986                 // We need to do a TDZ check here because we can&#39;t always prove we need to emit TDZ checks statically.
 987                 if (indirectLoadForOperand)
 988                     emitGetVarFromIndirectPointer(bitwise_cast&lt;JSValue**&gt;(operandSlot), regT0);
 989                 else
 990                     emitGetVarFromPointer(bitwise_cast&lt;JSValue*&gt;(*operandSlot), regT0);
 991                 addSlowCase(branchIfEmpty(regT0));
 992             }
 993             if (indirectLoadForOperand)
 994                 emitPutGlobalVariableIndirect(bitwise_cast&lt;JSValue**&gt;(operandSlot), value, &amp;metadata.m_watchpointSet);
 995             else
 996                 emitPutGlobalVariable(bitwise_cast&lt;JSValue*&gt;(*operandSlot), value, metadata.m_watchpointSet);
 997             emitWriteBarrier(constantScope, value, ShouldFilterValue);
 998             break;
 999         }
1000         case LocalClosureVar:
1001         case ClosureVar:
1002         case ClosureVarWithVarInjectionChecks:
1003             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
1004             emitPutClosureVar(scope, *operandSlot, value, metadata.m_watchpointSet);
1005             emitWriteBarrier(scope, value, ShouldFilterValue);
1006             break;
1007         case ModuleVar:
1008         case Dynamic:
1009             addSlowCase(jump());
1010             break;
1011         case UnresolvedProperty:
1012         case UnresolvedPropertyWithVarInjectionChecks:
1013             RELEASE_ASSERT_NOT_REACHED();
1014             break;
1015         }
1016     };
1017 
1018     switch (resolveType) {
1019     case GlobalProperty:
1020     case GlobalPropertyWithVarInjectionChecks: {
1021         JumpList skipToEnd;
1022         load32(&amp;metadata.m_getPutInfo, regT0);
1023         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
1024 
1025         Jump isGlobalProperty = branch32(Equal, regT0, TrustedImm32(resolveType));
1026         Jump isGlobalLexicalVar = branch32(Equal, regT0, TrustedImm32(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar));
1027         addSlowCase(jump()); // Dynamic, it can happen if we attempt to put a value to already-initialized const binding.
1028 
1029         isGlobalLexicalVar.link(this);
1030         emitCode(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar, true);
1031         skipToEnd.append(jump());
1032 
1033         isGlobalProperty.link(this);
1034         emitCode(resolveType, false);
1035         skipToEnd.link(this);
1036         break;
1037     }
1038     case UnresolvedProperty:
1039     case UnresolvedPropertyWithVarInjectionChecks: {
1040         JumpList skipToEnd;
1041         load32(&amp;metadata.m_getPutInfo, regT0);
1042         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
1043 
1044         Jump isGlobalProperty = branch32(Equal, regT0, TrustedImm32(GlobalProperty));
1045         Jump notGlobalPropertyWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalPropertyWithVarInjectionChecks));
1046         isGlobalProperty.link(this);
1047         emitCode(GlobalProperty, false);
1048         skipToEnd.append(jump());
1049         notGlobalPropertyWithVarInjections.link(this);
1050 
1051         Jump notGlobalLexicalVar = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVar));
1052         emitCode(GlobalLexicalVar, true);
1053         skipToEnd.append(jump());
1054         notGlobalLexicalVar.link(this);
1055 
1056         Jump notGlobalLexicalVarWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVarWithVarInjectionChecks));
1057         emitCode(GlobalLexicalVarWithVarInjectionChecks, true);
1058         skipToEnd.append(jump());
1059         notGlobalLexicalVarWithVarInjections.link(this);
1060 
1061         addSlowCase(jump());
1062 
1063         skipToEnd.link(this);
1064         break;
1065     }
1066 
1067     default:
1068         emitCode(resolveType, false);
1069         break;
1070     }
1071 }
1072 
1073 void JIT::emitSlow_op_put_to_scope(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1074 {
1075     linkAllSlowCases(iter);
1076 
1077     auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
1078     ResolveType resolveType = copiedGetPutInfo(bytecode).resolveType();
1079     if (resolveType == ModuleVar) {
1080         JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_throw_strict_mode_readonly_property_write_error);
1081         slowPathCall.call();
1082     } else
1083         callOperation(operationPutToScope, TrustedImmPtr(m_codeBlock-&gt;globalObject()), currentInstruction);
1084 }
1085 
1086 void JIT::emit_op_get_from_arguments(const Instruction* currentInstruction)
1087 {
1088     auto bytecode = currentInstruction-&gt;as&lt;OpGetFromArguments&gt;();
1089     VirtualRegister dst = bytecode.m_dst;
1090     VirtualRegister arguments = bytecode.m_arguments;
1091     int index = bytecode.m_index;
1092 
1093     emitGetVirtualRegister(arguments, regT0);
1094     load64(Address(regT0, DirectArguments::storageOffset() + index * sizeof(WriteBarrier&lt;Unknown&gt;)), regT0);
1095     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1096     emitPutVirtualRegister(dst);
1097 }
1098 
1099 void JIT::emit_op_put_to_arguments(const Instruction* currentInstruction)
1100 {
1101     auto bytecode = currentInstruction-&gt;as&lt;OpPutToArguments&gt;();
1102     VirtualRegister arguments = bytecode.m_arguments;
1103     int index = bytecode.m_index;
1104     VirtualRegister value = bytecode.m_value;
1105 
1106     emitGetVirtualRegister(arguments, regT0);
1107     emitGetVirtualRegister(value, regT1);
1108     store64(regT1, Address(regT0, DirectArguments::storageOffset() + index * sizeof(WriteBarrier&lt;Unknown&gt;)));
1109 
1110     emitWriteBarrier(arguments, value, ShouldFilterValue);
1111 }
1112 
1113 void JIT::emitWriteBarrier(VirtualRegister owner, VirtualRegister value, WriteBarrierMode mode)
1114 {
1115     Jump valueNotCell;
1116     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue) {
1117         emitGetVirtualRegister(value, regT0);
1118         valueNotCell = branchIfNotCell(regT0);
1119     }
1120 
1121     emitGetVirtualRegister(owner, regT0);
1122     Jump ownerNotCell;
1123     if (mode == ShouldFilterBaseAndValue || mode == ShouldFilterBase)
1124         ownerNotCell = branchIfNotCell(regT0);
1125 
1126     Jump ownerIsRememberedOrInEden = barrierBranch(vm(), regT0, regT1);
1127     callOperation(operationWriteBarrierSlowPath, &amp;vm(), regT0);
1128     ownerIsRememberedOrInEden.link(this);
1129 
1130     if (mode == ShouldFilterBaseAndValue || mode == ShouldFilterBase)
1131         ownerNotCell.link(this);
1132     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue)
1133         valueNotCell.link(this);
1134 }
1135 
1136 void JIT::emitWriteBarrier(JSCell* owner, VirtualRegister value, WriteBarrierMode mode)
1137 {
1138     emitGetVirtualRegister(value, regT0);
1139     Jump valueNotCell;
1140     if (mode == ShouldFilterValue)
1141         valueNotCell = branchIfNotCell(regT0);
1142 
1143     emitWriteBarrier(owner);
1144 
1145     if (mode == ShouldFilterValue)
1146         valueNotCell.link(this);
1147 }
1148 
1149 void JIT::emit_op_get_internal_field(const Instruction* currentInstruction)
1150 {
1151     auto bytecode = currentInstruction-&gt;as&lt;OpGetInternalField&gt;();
1152     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1153     VirtualRegister dst = bytecode.m_dst;
1154     VirtualRegister base = bytecode.m_base;
1155     unsigned index = bytecode.m_index;
1156 
1157     emitGetVirtualRegister(base, regT1);
1158     loadPtr(Address(regT1, JSInternalFieldObjectImpl&lt;&gt;::offsetOfInternalField(index)), regT0);
1159 
1160     emitValueProfilingSite(metadata);
1161     emitPutVirtualRegister(dst);
1162 }
1163 
1164 void JIT::emit_op_put_internal_field(const Instruction* currentInstruction)
1165 {
1166     auto bytecode = currentInstruction-&gt;as&lt;OpPutInternalField&gt;();
1167     VirtualRegister base = bytecode.m_base;
1168     VirtualRegister value = bytecode.m_value;
1169     unsigned index = bytecode.m_index;
1170 
1171     emitGetVirtualRegister(base, regT0);
1172     emitGetVirtualRegister(value, regT1);
1173     storePtr(regT1, Address(regT0, JSInternalFieldObjectImpl&lt;&gt;::offsetOfInternalField(index)));
1174     emitWriteBarrier(base, value, ShouldFilterValue);
1175 }
1176 
1177 #else // USE(JSVALUE64)
1178 
1179 void JIT::emitWriteBarrier(VirtualRegister owner, VirtualRegister value, WriteBarrierMode mode)
1180 {
1181     Jump valueNotCell;
1182     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue) {
1183         emitLoadTag(value, regT0);
1184         valueNotCell = branchIfNotCell(regT0);
1185     }
1186 
1187     emitLoad(owner, regT0, regT1);
1188     Jump ownerNotCell;
1189     if (mode == ShouldFilterBase || mode == ShouldFilterBaseAndValue)
1190         ownerNotCell = branchIfNotCell(regT0);
1191 
1192     Jump ownerIsRememberedOrInEden = barrierBranch(vm(), regT1, regT2);
1193     callOperation(operationWriteBarrierSlowPath, &amp;vm(), regT1);
1194     ownerIsRememberedOrInEden.link(this);
1195 
1196     if (mode == ShouldFilterBase || mode == ShouldFilterBaseAndValue)
1197         ownerNotCell.link(this);
1198     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue)
1199         valueNotCell.link(this);
1200 }
1201 
1202 void JIT::emitWriteBarrier(JSCell* owner, VirtualRegister value, WriteBarrierMode mode)
1203 {
1204     Jump valueNotCell;
1205     if (mode == ShouldFilterValue) {
1206         emitLoadTag(value, regT0);
1207         valueNotCell = branchIfNotCell(regT0);
1208     }
1209 
1210     emitWriteBarrier(owner);
1211 
1212     if (mode == ShouldFilterValue)
1213         valueNotCell.link(this);
1214 }
1215 
1216 #endif // USE(JSVALUE64)
1217 
1218 void JIT::emitWriteBarrier(JSCell* owner)
1219 {
1220     Jump ownerIsRememberedOrInEden = barrierBranch(vm(), owner, regT0);
1221     callOperation(operationWriteBarrierSlowPath, &amp;vm(), owner);
1222     ownerIsRememberedOrInEden.link(this);
1223 }
1224 
1225 void JIT::emitByValIdentifierCheck(ByValInfo* byValInfo, RegisterID cell, RegisterID scratch, const Identifier&amp; propertyName, JumpList&amp; slowCases)
1226 {
1227     if (propertyName.isSymbol())
1228         slowCases.append(branchPtr(NotEqual, cell, TrustedImmPtr(byValInfo-&gt;cachedSymbol.get())));
1229     else {
1230         slowCases.append(branchIfNotString(cell));
1231         loadPtr(Address(cell, JSString::offsetOfValue()), scratch);
1232         slowCases.append(branchPtr(NotEqual, scratch, TrustedImmPtr(propertyName.impl())));
1233     }
1234 }
1235 
1236 template&lt;typename Op&gt;
1237 void JIT::privateCompilePutByVal(const ConcurrentJSLocker&amp;, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1238 {
1239     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1240     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1241 
1242     PatchableJump badType;
1243     JumpList slowCases;
1244 
1245     bool needsLinkForWriteBarrier = false;
1246 
1247     switch (arrayMode) {
1248     case JITInt32:
1249         slowCases = emitInt32PutByVal(bytecode, badType);
1250         break;
1251     case JITDouble:
1252         slowCases = emitDoublePutByVal(bytecode, badType);
1253         break;
1254     case JITContiguous:
1255         slowCases = emitContiguousPutByVal(bytecode, badType);
1256         needsLinkForWriteBarrier = true;
1257         break;
1258     case JITArrayStorage:
1259         slowCases = emitArrayStoragePutByVal(bytecode, badType);
1260         needsLinkForWriteBarrier = true;
1261         break;
1262     default:
1263         TypedArrayType type = typedArrayTypeForJITArrayMode(arrayMode);
1264         if (isInt(type))
1265             slowCases = emitIntTypedArrayPutByVal(bytecode, badType, type);
1266         else
1267             slowCases = emitFloatTypedArrayPutByVal(bytecode, badType, type);
1268         break;
1269     }
1270 
1271     Jump done = jump();
1272 
1273     LinkBuffer patchBuffer(*this, m_codeBlock);
1274     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1275     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1276     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1277     if (needsLinkForWriteBarrier) {
1278         ASSERT(removeCodePtrTag(m_calls.last().callee.executableAddress()) == removeCodePtrTag(operationWriteBarrierSlowPath));
1279         patchBuffer.link(m_calls.last().from, m_calls.last().callee);
1280     }
1281 
1282     bool isDirect = currentInstruction-&gt;opcodeID() == op_put_by_val_direct;
1283     if (!isDirect) {
1284         byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1285             m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1286             &quot;Baseline put_by_val stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1287 
1288     } else {
1289         byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1290             m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1291             &quot;Baseline put_by_val_direct stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1292     }
1293     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1294     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(isDirect ? operationDirectPutByValGeneric : operationPutByValGeneric));
1295 }
1296 // This function is only consumed from another translation unit (JITOperations.cpp),
1297 // so we list off the two expected specializations in advance.
1298 template void JIT::privateCompilePutByVal&lt;OpPutByVal&gt;(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
1299 template void JIT::privateCompilePutByVal&lt;OpPutByValDirect&gt;(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
1300 
1301 template&lt;typename Op&gt;
1302 void JIT::privateCompilePutByValWithCachedId(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, PutKind putKind, const Identifier&amp; propertyName)
1303 {
1304     ASSERT((putKind == Direct &amp;&amp; Op::opcodeID == op_put_by_val_direct) || (putKind == NotDirect &amp;&amp; Op::opcodeID == op_put_by_val));
1305     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1306     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1307 
1308     JumpList doneCases;
1309     JumpList slowCases;
1310 
1311     JITPutByIdGenerator gen = emitPutByValWithCachedId(byValInfo, bytecode, putKind, propertyName, doneCases, slowCases);
1312 
1313     ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
1314     LinkBuffer patchBuffer(*this, m_codeBlock);
1315     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1316     patchBuffer.link(doneCases, byValInfo-&gt;badTypeDoneTarget);
1317     if (!m_exceptionChecks.empty())
1318         patchBuffer.link(m_exceptionChecks, byValInfo-&gt;exceptionHandler);
1319 
1320     for (const auto&amp; callSite : m_calls) {
1321         if (callSite.callee)
1322             patchBuffer.link(callSite.from, callSite.callee);
1323     }
1324     gen.finalize(patchBuffer, patchBuffer);
1325 
1326     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1327         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1328         &quot;Baseline put_by_val%s with cached property name &#39;%s&#39; stub for %s, return point %p&quot;, (putKind == Direct) ? &quot;_direct&quot; : &quot;&quot;, propertyName.impl()-&gt;utf8().data(), toCString(*m_codeBlock).data(), returnAddress.value());
1329     byValInfo-&gt;stubInfo = gen.stubInfo();
1330 
1331     MacroAssembler::repatchJump(byValInfo-&gt;notIndexJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1332     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(putKind == Direct ? operationDirectPutByValGeneric : operationPutByValGeneric));
1333 }
1334 // This function is only consumed from another translation unit (JITOperations.cpp),
1335 // so we list off the two expected specializations in advance.
1336 template void JIT::privateCompilePutByValWithCachedId&lt;OpPutByVal&gt;(ByValInfo*, ReturnAddressPtr, PutKind, const Identifier&amp;);
1337 template void JIT::privateCompilePutByValWithCachedId&lt;OpPutByValDirect&gt;(ByValInfo*, ReturnAddressPtr, PutKind, const Identifier&amp;);
1338 
1339 JIT::JumpList JIT::emitDoubleLoad(const Instruction*, PatchableJump&amp; badType)
1340 {
1341 #if USE(JSVALUE64)
1342     RegisterID base = regT0;
1343     RegisterID property = regT1;
1344     RegisterID indexing = regT2;
1345     RegisterID scratch = regT3;
1346 #else
1347     RegisterID base = regT0;
1348     RegisterID property = regT2;
1349     RegisterID indexing = regT1;
1350     RegisterID scratch = regT3;
1351 #endif
1352 
1353     JumpList slowCases;
1354 
1355     badType = patchableBranch32(NotEqual, indexing, TrustedImm32(DoubleShape));
1356     loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
1357     slowCases.append(branch32(AboveOrEqual, property, Address(scratch, Butterfly::offsetOfPublicLength())));
1358     loadDouble(BaseIndex(scratch, property, TimesEight), fpRegT0);
1359     slowCases.append(branchIfNaN(fpRegT0));
1360 
1361     return slowCases;
1362 }
1363 
1364 JIT::JumpList JIT::emitContiguousLoad(const Instruction*, PatchableJump&amp; badType, IndexingType expectedShape)
1365 {
1366 #if USE(JSVALUE64)
1367     RegisterID base = regT0;
1368     RegisterID property = regT1;
1369     RegisterID indexing = regT2;
1370     JSValueRegs result = JSValueRegs(regT0);
1371     RegisterID scratch = regT3;
1372 #else
1373     RegisterID base = regT0;
1374     RegisterID property = regT2;
1375     RegisterID indexing = regT1;
1376     JSValueRegs result = JSValueRegs(regT1, regT0);
1377     RegisterID scratch = regT3;
1378 #endif
1379 
1380     JumpList slowCases;
1381 
1382     badType = patchableBranch32(NotEqual, indexing, TrustedImm32(expectedShape));
1383     loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
1384     slowCases.append(branch32(AboveOrEqual, property, Address(scratch, Butterfly::offsetOfPublicLength())));
1385     loadValue(BaseIndex(scratch, property, TimesEight), result);
1386     slowCases.append(branchIfEmpty(result));
1387 
1388     return slowCases;
1389 }
1390 
1391 JIT::JumpList JIT::emitArrayStorageLoad(const Instruction*, PatchableJump&amp; badType)
1392 {
1393 #if USE(JSVALUE64)
1394     RegisterID base = regT0;
1395     RegisterID property = regT1;
1396     RegisterID indexing = regT2;
1397     JSValueRegs result = JSValueRegs(regT0);
1398     RegisterID scratch = regT3;
1399 #else
1400     RegisterID base = regT0;
1401     RegisterID property = regT2;
1402     RegisterID indexing = regT1;
1403     JSValueRegs result = JSValueRegs(regT1, regT0);
1404     RegisterID scratch = regT3;
1405 #endif
1406 
1407     JumpList slowCases;
1408 
1409     add32(TrustedImm32(-ArrayStorageShape), indexing, scratch);
1410     badType = patchableBranch32(Above, scratch, TrustedImm32(SlowPutArrayStorageShape - ArrayStorageShape));
1411 
1412     loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
1413     slowCases.append(branch32(AboveOrEqual, property, Address(scratch, ArrayStorage::vectorLengthOffset())));
1414 
1415     loadValue(BaseIndex(scratch, property, TimesEight, ArrayStorage::vectorOffset()), result);
1416     slowCases.append(branchIfEmpty(result));
1417 
1418     return slowCases;
1419 }
1420 
1421 template&lt;typename Op&gt;
1422 JIT::JumpList JIT::emitIntTypedArrayPutByVal(Op bytecode, PatchableJump&amp; badType, TypedArrayType type)
1423 {
1424     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1425     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1426     ASSERT(isInt(type));
1427 
1428     VirtualRegister value = bytecode.m_value;
1429 
1430 #if USE(JSVALUE64)
1431     RegisterID base = regT0;
1432     RegisterID property = regT1;
1433     RegisterID earlyScratch = regT3;
1434     RegisterID lateScratch = regT2;
1435     RegisterID lateScratch2 = regT4;
1436 #else
1437     RegisterID base = regT0;
1438     RegisterID property = regT2;
1439     RegisterID earlyScratch = regT3;
1440     RegisterID lateScratch = regT1;
1441     RegisterID lateScratch2 = regT4;
1442 #endif
1443 
1444     JumpList slowCases;
1445 
1446     load8(Address(base, JSCell::typeInfoTypeOffset()), earlyScratch);
1447     badType = patchableBranch32(NotEqual, earlyScratch, TrustedImm32(typeForTypedArrayType(type)));
1448     load32(Address(base, JSArrayBufferView::offsetOfLength()), lateScratch2);
1449     Jump inBounds = branch32(Below, property, lateScratch2);
1450     emitArrayProfileOutOfBoundsSpecialCase(profile);
1451     slowCases.append(jump());
1452     inBounds.link(this);
1453 
1454 #if USE(JSVALUE64)
1455     emitGetVirtualRegister(value, earlyScratch);
1456     slowCases.append(branchIfNotInt32(earlyScratch));
1457 #else
1458     emitLoad(value, lateScratch, earlyScratch);
1459     slowCases.append(branchIfNotInt32(lateScratch));
1460 #endif
1461 
1462     // We would be loading this into base as in get_by_val, except that the slow
1463     // path expects the base to be unclobbered.
1464     loadPtr(Address(base, JSArrayBufferView::offsetOfVector()), lateScratch);
1465     cageConditionally(Gigacage::Primitive, lateScratch, lateScratch2, lateScratch2);
1466 
1467     if (isClamped(type)) {
1468         ASSERT(elementSize(type) == 1);
1469         ASSERT(!JSC::isSigned(type));
1470         Jump inBounds = branch32(BelowOrEqual, earlyScratch, TrustedImm32(0xff));
1471         Jump tooBig = branch32(GreaterThan, earlyScratch, TrustedImm32(0xff));
1472         xor32(earlyScratch, earlyScratch);
1473         Jump clamped = jump();
1474         tooBig.link(this);
1475         move(TrustedImm32(0xff), earlyScratch);
1476         clamped.link(this);
1477         inBounds.link(this);
1478     }
1479 
1480     switch (elementSize(type)) {
1481     case 1:
1482         store8(earlyScratch, BaseIndex(lateScratch, property, TimesOne));
1483         break;
1484     case 2:
1485         store16(earlyScratch, BaseIndex(lateScratch, property, TimesTwo));
1486         break;
1487     case 4:
1488         store32(earlyScratch, BaseIndex(lateScratch, property, TimesFour));
1489         break;
1490     default:
1491         CRASH();
1492     }
1493 
1494     return slowCases;
1495 }
1496 
1497 template&lt;typename Op&gt;
1498 JIT::JumpList JIT::emitFloatTypedArrayPutByVal(Op bytecode, PatchableJump&amp; badType, TypedArrayType type)
1499 {
1500     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1501     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1502     ASSERT(isFloat(type));
1503 
1504     VirtualRegister value = bytecode.m_value;
1505 
1506 #if USE(JSVALUE64)
1507     RegisterID base = regT0;
1508     RegisterID property = regT1;
1509     RegisterID earlyScratch = regT3;
1510     RegisterID lateScratch = regT2;
1511     RegisterID lateScratch2 = regT4;
1512 #else
1513     RegisterID base = regT0;
1514     RegisterID property = regT2;
1515     RegisterID earlyScratch = regT3;
1516     RegisterID lateScratch = regT1;
1517     RegisterID lateScratch2 = regT4;
1518 #endif
1519 
1520     JumpList slowCases;
1521 
1522     load8(Address(base, JSCell::typeInfoTypeOffset()), earlyScratch);
1523     badType = patchableBranch32(NotEqual, earlyScratch, TrustedImm32(typeForTypedArrayType(type)));
1524     load32(Address(base, JSArrayBufferView::offsetOfLength()), lateScratch2);
1525     Jump inBounds = branch32(Below, property, lateScratch2);
1526     emitArrayProfileOutOfBoundsSpecialCase(profile);
1527     slowCases.append(jump());
1528     inBounds.link(this);
1529 
1530 #if USE(JSVALUE64)
1531     emitGetVirtualRegister(value, earlyScratch);
1532     Jump doubleCase = branchIfNotInt32(earlyScratch);
1533     convertInt32ToDouble(earlyScratch, fpRegT0);
1534     Jump ready = jump();
1535     doubleCase.link(this);
1536     slowCases.append(branchIfNotNumber(earlyScratch));
1537     add64(numberTagRegister, earlyScratch);
1538     move64ToDouble(earlyScratch, fpRegT0);
1539     ready.link(this);
1540 #else
1541     emitLoad(value, lateScratch, earlyScratch);
1542     Jump doubleCase = branchIfNotInt32(lateScratch);
1543     convertInt32ToDouble(earlyScratch, fpRegT0);
1544     Jump ready = jump();
1545     doubleCase.link(this);
1546     slowCases.append(branch32(Above, lateScratch, TrustedImm32(JSValue::LowestTag)));
1547     moveIntsToDouble(earlyScratch, lateScratch, fpRegT0, fpRegT1);
1548     ready.link(this);
1549 #endif
1550 
1551     // We would be loading this into base as in get_by_val, except that the slow
1552     // path expects the base to be unclobbered.
1553     loadPtr(Address(base, JSArrayBufferView::offsetOfVector()), lateScratch);
1554     cageConditionally(Gigacage::Primitive, lateScratch, lateScratch2, lateScratch2);
1555 
1556     switch (elementSize(type)) {
1557     case 4:
1558         convertDoubleToFloat(fpRegT0, fpRegT0);
1559         storeFloat(fpRegT0, BaseIndex(lateScratch, property, TimesFour));
1560         break;
1561     case 8:
1562         storeDouble(fpRegT0, BaseIndex(lateScratch, property, TimesEight));
1563         break;
1564     default:
1565         CRASH();
1566     }
1567 
1568     return slowCases;
1569 }
1570 
1571 template void JIT::emit_op_put_by_val&lt;OpPutByVal&gt;(const Instruction*);
1572 
1573 } // namespace JSC
1574 
1575 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>