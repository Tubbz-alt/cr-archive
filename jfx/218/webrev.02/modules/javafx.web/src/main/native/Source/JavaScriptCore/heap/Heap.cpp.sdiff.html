<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="HandleSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  63 #include &quot;PreventCollectionScope.h&quot;
  64 #include &quot;SamplingProfiler.h&quot;
  65 #include &quot;ShadowChicken.h&quot;
  66 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  67 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  68 #include &quot;StopIfNecessaryTimer.h&quot;
  69 #include &quot;SubspaceInlines.h&quot;
  70 #include &quot;SuperSampler.h&quot;
  71 #include &quot;SweepingScope.h&quot;
  72 #include &quot;SymbolTableInlines.h&quot;
  73 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  74 #include &quot;TypeProfiler.h&quot;
  75 #include &quot;TypeProfilerLog.h&quot;
  76 #include &quot;UnlinkedCodeBlock.h&quot;
  77 #include &quot;VM.h&quot;
  78 #include &quot;VisitCounter.h&quot;
  79 #include &quot;WasmMemory.h&quot;
  80 #include &quot;WeakMapImplInlines.h&quot;
  81 #include &quot;WeakSetInlines.h&quot;
  82 #include &lt;algorithm&gt;

  83 #include &lt;wtf/ListDump.h&gt;
  84 #include &lt;wtf/MainThread.h&gt;
  85 #include &lt;wtf/ParallelVectorIterator.h&gt;
  86 #include &lt;wtf/ProcessID.h&gt;
  87 #include &lt;wtf/RAMSize.h&gt;
  88 #include &lt;wtf/SimpleStats.h&gt;
  89 #include &lt;wtf/Threading.h&gt;
  90 
<span class="line-modified">  91 #if PLATFORM(IOS_FAMILY)</span>
  92 #include &lt;bmalloc/bmalloc.h&gt;
  93 #endif
  94 
  95 #if USE(FOUNDATION)
  96 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  97 #endif
  98 
  99 #ifdef JSC_GLIB_API_ENABLED
 100 #include &quot;JSCGLibWrapperObject.h&quot;
 101 #endif
 102 
 103 namespace JSC {
 104 
 105 namespace {
 106 
 107 bool verboseStop = false;
 108 
 109 double maxPauseMS(double thisPauseMS)
 110 {
 111     static double maxPauseMS;
 112     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 113     return maxPauseMS;
 114 }
 115 
 116 size_t minHeapSize(HeapType heapType, size_t ramSize)
 117 {
 118     if (heapType == LargeHeap) {
 119         double result = std::min(
 120             static_cast&lt;double&gt;(Options::largeHeapSize()),
 121             ramSize * Options::smallHeapRAMFraction());
 122         return static_cast&lt;size_t&gt;(result);
 123     }
 124     return Options::smallHeapSize();
 125 }
 126 
 127 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 128 {
 129     if (VM::isInMiniMode())
 130         return Options::miniVMHeapGrowthFactor() * heapSize;
 131 
<span class="line-modified"> 132 #if PLATFORM(IOS_FAMILY)</span>
 133     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 134     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 135         return Options::smallHeapGrowthFactor() * heapSize;
 136     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 137         return Options::mediumHeapGrowthFactor() * heapSize;
 138 #else
 139     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 140         return Options::smallHeapGrowthFactor() * heapSize;
 141     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 142         return Options::mediumHeapGrowthFactor() * heapSize;
 143 #endif
 144     return Options::largeHeapGrowthFactor() * heapSize;
 145 }
 146 
 147 bool isValidSharedInstanceThreadState(VM&amp; vm)
 148 {
 149     return vm.currentThreadIsHoldingAPILock();
 150 }
 151 
 152 bool isValidThreadState(VM&amp; vm)
</pre>
<hr />
<pre>
 354         [&amp;] (SlotVisitor&amp; visitor) {
 355             visitor.clearMarkStacks();
 356         });
 357     m_mutatorMarkStack-&gt;clear();
 358     m_raceMarkStack-&gt;clear();
 359 
 360     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 361         WeakBlock::destroy(*this, block);
 362 }
 363 
 364 bool Heap::isPagedOut(MonotonicTime deadline)
 365 {
 366     return m_objectSpace.isPagedOut(deadline);
 367 }
 368 
 369 void Heap::dumpHeapStatisticsAtVMDestruction()
 370 {
 371     unsigned counter = 0;
 372     m_objectSpace.forEachBlock([&amp;] (MarkedBlock::Handle* block) {
 373         unsigned live = 0;
<span class="line-modified"> 374         block-&gt;forEachCell([&amp;] (HeapCell* cell, HeapCell::Kind) {</span>
 375             if (cell-&gt;isLive())
 376                 live++;
 377             return IterationStatus::Continue;
 378         });
 379         dataLogLn(&quot;[&quot;, counter++, &quot;] &quot;, block-&gt;cellSize(), &quot;, &quot;, live, &quot; / &quot;, block-&gt;cellsPerBlock(), &quot; &quot;, static_cast&lt;double&gt;(live) / block-&gt;cellsPerBlock() * 100, &quot;% &quot;, block-&gt;attributes(), &quot; &quot;, block-&gt;subspace()-&gt;name());
<span class="line-modified"> 380         block-&gt;forEachCell([&amp;] (HeapCell* heapCell, HeapCell::Kind kind) {</span>
 381             if (heapCell-&gt;isLive() &amp;&amp; kind == HeapCell::Kind::JSCell) {
 382                 auto* cell = static_cast&lt;JSCell*&gt;(heapCell);
 383                 if (cell-&gt;isObject())
 384                     dataLogLn(&quot;    &quot;, JSValue((JSObject*)cell));
 385                 else
 386                     dataLogLn(&quot;    &quot;, *cell);
 387             }
 388             return IterationStatus::Continue;
 389         });
 390     });
 391 }
 392 
 393 // The VM is being destroyed and the collector will never run again.
 394 // Run all pending finalizers now because we won&#39;t get another chance.
 395 void Heap::lastChanceToFinalize()
 396 {
 397     MonotonicTime before;
<span class="line-modified"> 398     if (Options::logGC()) {</span>
 399         before = MonotonicTime::now();
 400         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 401     }
 402 
 403     m_isShuttingDown = true;
 404 
 405     RELEASE_ASSERT(!m_vm.entryScope);
 406     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 407 
 408     if (m_collectContinuouslyThread) {
 409         {
 410             LockHolder locker(m_collectContinuouslyLock);
 411             m_shouldStopCollectingContinuously = true;
 412             m_collectContinuouslyCondition.notifyOne();
 413         }
 414         m_collectContinuouslyThread-&gt;waitForCompletion();
 415     }
 416 
<span class="line-modified"> 417     if (Options::logGC())</span>
<span class="line-removed"> 418         dataLog(&quot;1&quot;);</span>
 419 
 420     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 421     // going to call into anything that starts collections. Still, this makes the algorithm more
 422     // obviously sound.
 423     m_isSafeToCollect = false;
 424 
<span class="line-modified"> 425     if (Options::logGC())</span>
<span class="line-removed"> 426         dataLog(&quot;2&quot;);</span>
 427 
 428     bool isCollecting;
 429     {
 430         auto locker = holdLock(*m_threadLock);
 431         RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 432         isCollecting = m_lastServedTicket &lt; m_lastGrantedTicket;
 433     }
 434     if (isCollecting) {
<span class="line-modified"> 435         if (Options::logGC())</span>
<span class="line-removed"> 436             dataLog(&quot;...]\n&quot;);</span>
 437 
 438         // Wait for the current collection to finish.
 439         waitForCollector(
 440             [&amp;] (const AbstractLocker&amp;) -&gt; bool {
 441                 RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 442                 return m_lastServedTicket == m_lastGrantedTicket;
 443             });
 444 
<span class="line-modified"> 445         if (Options::logGC())</span>
<span class="line-removed"> 446             dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);</span>
 447     }
<span class="line-modified"> 448     if (Options::logGC())</span>
<span class="line-removed"> 449         dataLog(&quot;3&quot;);</span>
 450 
 451     RELEASE_ASSERT(m_requests.isEmpty());
 452     RELEASE_ASSERT(m_lastServedTicket == m_lastGrantedTicket);
 453 
 454     // Carefully bring the thread down.
 455     bool stopped = false;
 456     {
 457         LockHolder locker(*m_threadLock);
 458         stopped = m_thread-&gt;tryStop(locker);
 459         m_threadShouldStop = true;
 460         if (!stopped)
 461             m_threadCondition-&gt;notifyOne(locker);
 462     }
 463 
<span class="line-modified"> 464     if (Options::logGC())</span>
<span class="line-removed"> 465         dataLog(&quot;4&quot;);</span>
 466 
 467     if (!stopped)
 468         m_thread-&gt;join();
 469 
<span class="line-modified"> 470     if (Options::logGC())</span>
<span class="line-removed"> 471         dataLog(&quot;5 &quot;);</span>
 472 
 473     if (UNLIKELY(Options::dumpHeapStatisticsAtVMDestruction()))
 474         dumpHeapStatisticsAtVMDestruction();
 475 
 476     m_arrayBuffers.lastChanceToFinalize();
 477     m_objectSpace.stopAllocatingForGood();
 478     m_objectSpace.lastChanceToFinalize();
 479     releaseDelayedReleasedObjects();
 480 
 481     sweepAllLogicallyEmptyWeakBlocks();
 482 
 483     m_objectSpace.freeMemory();
 484 
<span class="line-modified"> 485     if (Options::logGC())</span>
<span class="line-removed"> 486         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);</span>
 487 }
 488 
 489 void Heap::releaseDelayedReleasedObjects()
 490 {
 491 #if USE(FOUNDATION) || defined(JSC_GLIB_API_ENABLED)
 492     // We need to guard against the case that releasing an object can create more objects due to the
 493     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 494     // back here and could try to recursively release objects. We guard that with a recursive entry
 495     // count. Only the initial call will release objects, recursive calls simple return and let the
 496     // the initial call to the function take care of any objects created during release time.
 497     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 498     // and use a temp Vector for the actual releasing.
 499     if (!m_delayedReleaseRecursionCount++) {
 500         while (!m_delayedReleaseObjects.isEmpty()) {
 501             ASSERT(m_vm.currentThreadIsHoldingAPILock());
 502 
 503             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 504 
 505             {
 506                 // We need to drop locks before calling out to arbitrary code.
</pre>
<hr />
<pre>
 521 }
 522 
 523 void Heap::reportExtraMemoryAllocatedSlowCase(size_t size)
 524 {
 525     didAllocate(size);
 526     collectIfNecessaryOrDefer();
 527 }
 528 
 529 void Heap::deprecatedReportExtraMemorySlowCase(size_t size)
 530 {
 531     // FIXME: Change this to use SaturatedArithmetic when available.
 532     // https://bugs.webkit.org/show_bug.cgi?id=170411
 533     Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = m_deprecatedExtraMemorySize;
 534     checkedNewSize += size;
 535     m_deprecatedExtraMemorySize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
 536     reportExtraMemoryAllocatedSlowCase(size);
 537 }
 538 
 539 bool Heap::overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType)
 540 {
<span class="line-modified"> 541 #if PLATFORM(IOS_FAMILY)</span>
<span class="line-modified"> 542     if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_precentAvailableMemoryCachedCallCount &gt;= 100) {</span>
 543         m_overCriticalMemoryThreshold = bmalloc::api::percentAvailableMemoryInUse() &gt; Options::criticalGCMemoryThreshold();
<span class="line-modified"> 544         m_precentAvailableMemoryCachedCallCount = 0;</span>
 545     }
 546 
 547     return m_overCriticalMemoryThreshold;
 548 #else
 549     UNUSED_PARAM(memoryThresholdCallType);
 550     return false;
 551 #endif
 552 }
 553 
 554 void Heap::reportAbandonedObjectGraph()
 555 {
 556     // Our clients don&#39;t know exactly how much memory they
 557     // are abandoning so we just guess for them.
 558     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 559 
 560     // We want to accelerate the next collection. Because memory has just
 561     // been abandoned, the next collection has the potential to
 562     // be more profitable. Since allocation is the trigger for collection,
 563     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 564     if (m_fullActivityCallback) {
</pre>
<hr />
<pre>
 595     if (m_arrayBuffers.addReference(cell, buffer)) {
 596         collectIfNecessaryOrDefer();
 597         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 598     }
 599 }
 600 
 601 template&lt;typename CellType, typename CellSet&gt;
 602 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 603 {
 604     cellSet.forEachMarkedCell(
 605         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 606             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(vm());
 607         });
 608 }
 609 
 610 void Heap::finalizeUnconditionalFinalizers()
 611 {
 612     vm().builtinExecutables()-&gt;finalizeUnconditionally();
 613     finalizeMarkedUnconditionalFinalizers&lt;FunctionExecutable&gt;(vm().functionExecutableSpace.space);
 614     finalizeMarkedUnconditionalFinalizers&lt;SymbolTable&gt;(vm().symbolTableSpace);

 615     vm().forEachCodeBlockSpace(
 616         [&amp;] (auto&amp; space) {
 617             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 618         });
<span class="line-removed"> 619     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm().executableToCodeBlockEdgesWithFinalizers);</span>
 620     finalizeMarkedUnconditionalFinalizers&lt;StructureRareData&gt;(vm().structureRareDataSpace);
 621     finalizeMarkedUnconditionalFinalizers&lt;UnlinkedFunctionExecutable&gt;(vm().unlinkedFunctionExecutableSpace.set);
 622     if (vm().m_weakSetSpace)
 623         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm().m_weakSetSpace);
 624     if (vm().m_weakMapSpace)
 625         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm().m_weakMapSpace);
 626     if (vm().m_weakObjectRefSpace)
 627         finalizeMarkedUnconditionalFinalizers&lt;JSWeakObjectRef&gt;(*vm().m_weakObjectRefSpace);
 628     if (vm().m_errorInstanceSpace)
 629         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm().m_errorInstanceSpace);
 630 
 631 #if ENABLE(WEBASSEMBLY)
 632     if (vm().m_webAssemblyCodeBlockSpace)
 633         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm().m_webAssemblyCodeBlockSpace);
 634 #endif
 635 }
 636 
 637 void Heap::willStartIterating()
 638 {
 639     m_objectSpace.willStartIterating();
</pre>
<hr />
<pre>
 703 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 704 {
 705     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 706 }
 707 
 708 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 709 {
 710 #if ENABLE(C_LOOP)
 711     m_vm.interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);
 712 #else
 713     UNUSED_PARAM(roots);
 714 #endif
 715 }
 716 
 717 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 718 {
 719 #if ENABLE(DFG_JIT)
 720     if (!VM::canUseJIT())
 721         return;
 722     m_vm.gatherScratchBufferRoots(roots);

 723 #else
 724     UNUSED_PARAM(roots);
 725 #endif
 726 }
 727 
 728 void Heap::beginMarking()
 729 {
 730     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 731     m_jitStubRoutines-&gt;clearMarks();
 732     m_objectSpace.beginMarking();
 733     setMutatorShouldBeFenced(true);
 734 }
 735 
 736 void Heap::removeDeadCompilerWorklistEntries()
 737 {
 738 #if ENABLE(DFG_JIT)
 739     if (!VM::canUseJIT())
 740         return;
 741     for (unsigned i = DFG::numberOfWorklists(); i--;)
 742         DFG::existingWorklistForIndex(i).removeDeadPlans(m_vm);
</pre>
<hr />
<pre>
1032                 // return anyway, since we proved that the object was not marked at the time that
1033                 // we executed this slow path.
1034             }
1035 
1036             return;
1037         }
1038     } else
1039         ASSERT(isMarked(cell));
1040     // It could be that the object was *just* marked. This means that the collector may set the
1041     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
1042     // race with the collector here. If we win then this is accurate because the object _will_
1043     // get scanned again. If we lose then someone else will barrier the object again. That would
1044     // be unfortunate but not the end of the world.
1045     cell-&gt;setCellState(CellState::PossiblyGrey);
1046     m_mutatorMarkStack-&gt;append(cell);
1047 }
1048 
1049 void Heap::sweepSynchronously()
1050 {
1051     MonotonicTime before { };
<span class="line-modified">1052     if (Options::logGC()) {</span>
1053         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
1054         before = MonotonicTime::now();
1055     }
<span class="line-modified">1056     m_objectSpace.sweep();</span>
1057     m_objectSpace.shrink();
<span class="line-modified">1058     if (Options::logGC()) {</span>
1059         MonotonicTime after = MonotonicTime::now();
1060         dataLog(&quot;=&gt; &quot;, capacity() / 1024, &quot;kb, &quot;, (after - before).milliseconds(), &quot;ms&quot;);
1061     }
1062 }
1063 
1064 void Heap::collect(Synchronousness synchronousness, GCRequest request)
1065 {
1066     switch (synchronousness) {
1067     case Async:
1068         collectAsync(request);
1069         return;
1070     case Sync:
1071         collectSync(request);
1072         return;
1073     }
1074     RELEASE_ASSERT_NOT_REACHED();
1075 }
1076 
1077 void Heap::collectNow(Synchronousness synchronousness, GCRequest request)
1078 {
1079     if (validateDFGDoesGC)
1080         RELEASE_ASSERT(expectDoesGC());
1081 
1082     switch (synchronousness) {
1083     case Async: {
1084         collectAsync(request);
1085         stopIfNecessary();
1086         return;
1087     }
1088 
1089     case Sync: {
1090         collectSync(request);
1091 
1092         DeferGCForAWhile deferGC(*this);
1093         if (UNLIKELY(Options::useImmortalObjects()))
1094             sweeper().stopSweeping();
1095 
1096         bool alreadySweptInCollectSync = shouldSweepSynchronously();
1097         if (!alreadySweptInCollectSync) {
<span class="line-modified">1098             if (Options::logGC())</span>
<span class="line-removed">1099                 dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;);</span>
1100             sweepSynchronously();
<span class="line-modified">1101             if (Options::logGC())</span>
<span class="line-removed">1102                 dataLog(&quot;]\n&quot;);</span>
1103         }
1104         m_objectSpace.assertNoUnswept();
1105 
1106         sweepAllLogicallyEmptyWeakBlocks();
1107         return;
1108     } }
1109     RELEASE_ASSERT_NOT_REACHED();
1110 }
1111 
1112 void Heap::collectAsync(GCRequest request)
1113 {
1114     if (validateDFGDoesGC)
1115         RELEASE_ASSERT(expectDoesGC());
1116 
1117     if (!m_isSafeToCollect)
1118         return;
1119 
1120     bool alreadyRequested = false;
1121     {
1122         LockHolder locker(*m_threadLock);
</pre>
<hr />
<pre>
1246     // Check m_requests since the mutator calls this to poll what&#39;s going on.
1247     {
1248         auto locker = holdLock(*m_threadLock);
1249         if (m_requests.isEmpty())
1250             return false;
1251     }
1252 
1253     return changePhase(conn, CollectorPhase::Begin);
1254 }
1255 
1256 NEVER_INLINE bool Heap::runBeginPhase(GCConductor conn)
1257 {
1258     m_currentGCStartTime = MonotonicTime::now();
1259 
1260     {
1261         LockHolder locker(*m_threadLock);
1262         RELEASE_ASSERT(!m_requests.isEmpty());
1263         m_currentRequest = m_requests.first();
1264     }
1265 
<span class="line-modified">1266     if (Options::logGC())</span>
<span class="line-removed">1267         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: START &quot;, gcConductorShortName(conn), &quot; &quot;, capacity() / 1024, &quot;kb &quot;);</span>
1268 
1269     m_beforeGC = MonotonicTime::now();
1270 



1271     if (m_collectionScope) {
<span class="line-modified">1272         dataLog(&quot;Collection scope already set during GC: &quot;, *m_collectionScope, &quot;\n&quot;);</span>
1273         RELEASE_ASSERT_NOT_REACHED();
1274     }
1275 
1276     willStartCollection();
1277 
1278     if (UNLIKELY(m_verifier)) {
1279         // Verify that live objects from the last GC cycle haven&#39;t been corrupted by
1280         // mutators before we begin this new GC cycle.
1281         m_verifier-&gt;verify(HeapVerifier::Phase::BeforeGC);
1282 
1283         m_verifier-&gt;startGC();
1284         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::BeforeMarking);
1285     }
1286 
1287     prepareForMarking();
1288 
1289     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
1290         m_opaqueRoots.clear();
1291         m_collectorSlotVisitor-&gt;clearMarkStacks();
1292         m_mutatorMarkStack-&gt;clear();
</pre>
<hr />
<pre>
1313             }
1314 
1315             Thread::registerGCThread(GCThreadType::Helper);
1316 
1317             {
1318                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1319                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1320             }
1321 
1322             {
1323                 LockHolder locker(m_parallelSlotVisitorLock);
1324                 m_availableParallelSlotVisitors.append(slotVisitor);
1325             }
1326         });
1327 
1328     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1329 
1330     m_constraintSet-&gt;didStartMarking();
1331 
1332     m_scheduler-&gt;beginCollection();
<span class="line-modified">1333     if (Options::logGC())</span>
1334         m_scheduler-&gt;log();
1335 
1336     // After this, we will almost certainly fall through all of the &quot;slotVisitor.isEmpty()&quot;
1337     // checks because bootstrap would have put things into the visitor. So, we should fall
1338     // through to draining.
1339 
1340     if (!slotVisitor.didReachTermination()) {
1341         dataLog(&quot;Fatal: SlotVisitor should think that GC should terminate before constraint solving, but it does not think this.\n&quot;);
1342         dataLog(&quot;slotVisitor.isEmpty(): &quot;, slotVisitor.isEmpty(), &quot;\n&quot;);
1343         dataLog(&quot;slotVisitor.collectorMarkStack().isEmpty(): &quot;, slotVisitor.collectorMarkStack().isEmpty(), &quot;\n&quot;);
1344         dataLog(&quot;slotVisitor.mutatorMarkStack().isEmpty(): &quot;, slotVisitor.mutatorMarkStack().isEmpty(), &quot;\n&quot;);
1345         dataLog(&quot;m_numberOfActiveParallelMarkers: &quot;, m_numberOfActiveParallelMarkers, &quot;\n&quot;);
1346         dataLog(&quot;m_sharedCollectorMarkStack-&gt;isEmpty(): &quot;, m_sharedCollectorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1347         dataLog(&quot;m_sharedMutatorMarkStack-&gt;isEmpty(): &quot;, m_sharedMutatorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1348         dataLog(&quot;slotVisitor.didReachTermination(): &quot;, slotVisitor.didReachTermination(), &quot;\n&quot;);
1349         RELEASE_ASSERT_NOT_REACHED();
1350     }
1351 
1352     return changePhase(conn, CollectorPhase::Fixpoint);
1353 }
1354 
1355 NEVER_INLINE bool Heap::runFixpointPhase(GCConductor conn)
1356 {
1357     RELEASE_ASSERT(conn == GCConductor::Collector || m_currentThreadState);
1358 
1359     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1360 
<span class="line-modified">1361     if (Options::logGC()) {</span>
1362         HashMap&lt;const char*, size_t&gt; visitMap;
1363         forEachSlotVisitor(
1364             [&amp;] (SlotVisitor&amp; slotVisitor) {
1365                 visitMap.add(slotVisitor.codeName(), slotVisitor.bytesVisited() / 1024);
1366             });
1367 
1368         auto perVisitorDump = sortedMapDump(
1369             visitMap,
1370             [] (const char* a, const char* b) -&gt; bool {
1371                 return strcmp(a, b) &lt; 0;
1372             },
1373             &quot;:&quot;, &quot; &quot;);
1374 
1375         dataLog(&quot;v=&quot;, bytesVisited() / 1024, &quot;kb (&quot;, perVisitorDump, &quot;) o=&quot;, m_opaqueRoots.size(), &quot; b=&quot;, m_barriersExecuted, &quot; &quot;);
1376     }
1377 
1378     if (slotVisitor.didReachTermination()) {
1379         m_opaqueRoots.deleteOldTables();
1380 
1381         m_scheduler-&gt;didReachTermination();
</pre>
<hr />
<pre>
1385         // FIXME: Take m_mutatorDidRun into account when scheduling constraints. Most likely,
1386         // we don&#39;t have to execute root constraints again unless the mutator did run. At a
1387         // minimum, we could use this for work estimates - but it&#39;s probably more than just an
1388         // estimate.
1389         // https://bugs.webkit.org/show_bug.cgi?id=166828
1390 
1391         // Wondering what this does? Look at Heap::addCoreConstraints(). The DOM and others can also
1392         // add their own using Heap::addMarkingConstraint().
1393         bool converged = m_constraintSet-&gt;executeConvergence(slotVisitor);
1394 
1395         // FIXME: The slotVisitor.isEmpty() check is most likely not needed.
1396         // https://bugs.webkit.org/show_bug.cgi?id=180310
1397         if (converged &amp;&amp; slotVisitor.isEmpty()) {
1398             assertMarkStacksEmpty();
1399             return changePhase(conn, CollectorPhase::End);
1400         }
1401 
1402         m_scheduler-&gt;didExecuteConstraints();
1403     }
1404 
<span class="line-modified">1405     if (Options::logGC())</span>
<span class="line-removed">1406         dataLog(slotVisitor.collectorMarkStack().size(), &quot;+&quot;, m_mutatorMarkStack-&gt;size() + slotVisitor.mutatorMarkStack().size(), &quot; &quot;);</span>
1407 
1408     {
1409         ParallelModeEnabler enabler(slotVisitor);
1410         slotVisitor.drainInParallel(m_scheduler-&gt;timeToResume());
1411     }
1412 
1413     m_scheduler-&gt;synchronousDrainingDidStall();
1414 
1415     // This is kinda tricky. The termination check looks at:
1416     //
1417     // - Whether the marking threads are active. If they are not, this means that the marking threads&#39;
1418     //   SlotVisitors are empty.
1419     // - Whether the collector&#39;s slot visitor is empty.
1420     // - Whether the shared mark stacks are empty.
1421     //
1422     // This doesn&#39;t have to check the mutator SlotVisitor because that one becomes empty after every GC
1423     // work increment, so it must be empty now.
1424     if (slotVisitor.didReachTermination())
1425         return true; // This is like relooping to the top if runFixpointPhase().
1426 
1427     if (!m_scheduler-&gt;shouldResume())
1428         return true;
1429 
1430     m_scheduler-&gt;willResume();
1431 
<span class="line-modified">1432     if (Options::logGC()) {</span>
1433         double thisPauseMS = (MonotonicTime::now() - m_stopTime).milliseconds();
1434         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;)...]\n&quot;);
1435     }
1436 
1437     // Forgive the mutator for its past failures to keep up.
1438     // FIXME: Figure out if moving this to different places results in perf changes.
1439     m_incrementBalance = 0;
1440 
1441     return changePhase(conn, CollectorPhase::Concurrent);
1442 }
1443 
1444 NEVER_INLINE bool Heap::runConcurrentPhase(GCConductor conn)
1445 {
1446     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1447 
1448     switch (conn) {
1449     case GCConductor::Mutator: {
1450         // When the mutator has the conn, we poll runConcurrentPhase() on every time someone says
1451         // stopIfNecessary(), so on every allocation slow path. When that happens we poll if it&#39;s time
1452         // to stop and do some work.
</pre>
<hr />
<pre>
1456 
1457         // We could be coming from a collector phase that stuffed our SlotVisitor, so make sure we donate
1458         // everything. This is super cheap if the SlotVisitor is already empty.
1459         slotVisitor.donateAll();
1460         return false;
1461     }
1462     case GCConductor::Collector: {
1463         {
1464             ParallelModeEnabler enabler(slotVisitor);
1465             slotVisitor.drainInParallelPassively(m_scheduler-&gt;timeToStop());
1466         }
1467         return changePhase(conn, CollectorPhase::Reloop);
1468     } }
1469 
1470     RELEASE_ASSERT_NOT_REACHED();
1471     return false;
1472 }
1473 
1474 NEVER_INLINE bool Heap::runReloopPhase(GCConductor conn)
1475 {
<span class="line-modified">1476     if (Options::logGC())</span>
<span class="line-removed">1477         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;, gcConductorShortName(conn), &quot; &quot;);</span>
1478 
1479     m_scheduler-&gt;didStop();
1480 
<span class="line-modified">1481     if (Options::logGC())</span>
1482         m_scheduler-&gt;log();
1483 
1484     return changePhase(conn, CollectorPhase::Fixpoint);
1485 }
1486 
1487 NEVER_INLINE bool Heap::runEndPhase(GCConductor conn)
1488 {
1489     m_scheduler-&gt;endCollection();
1490 
1491     {
1492         auto locker = holdLock(m_markingMutex);
1493         m_parallelMarkersShouldExit = true;
1494         m_markingConditionVariable.notifyAll();
1495     }
1496     m_helperClient.finish();
1497 
1498     iterateExecutingAndCompilingCodeBlocks(
1499         [&amp;] (CodeBlock* codeBlock) {
1500             writeBarrier(codeBlock);
1501         });
1502 
1503     updateObjectCounts();
1504     endMarking();
1505 
1506     if (UNLIKELY(m_verifier)) {
1507         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1508         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1509     }
1510 
1511     if (vm().typeProfiler())
1512         vm().typeProfiler()-&gt;invalidateTypeSetCache(vm());
1513 


1514     reapWeakHandles();
1515     pruneStaleEntriesFromWeakGCMaps();
1516     sweepArrayBuffers();
1517     snapshotUnswept();
<span class="line-modified">1518     finalizeUnconditionalFinalizers();</span>
1519     removeDeadCompilerWorklistEntries();
1520     notifyIncrementalSweeper();
1521 
1522     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1523         [&amp;] (CodeBlock* codeBlock) {
1524             writeBarrier(codeBlock);
1525         });
1526     m_codeBlocks-&gt;clearCurrentlyExecuting();
1527 
1528     m_objectSpace.prepareForAllocation();
1529     updateAllocationLimits();
1530 
1531     if (UNLIKELY(m_verifier)) {
1532         m_verifier-&gt;trimDeadCells();
1533         m_verifier-&gt;verify(HeapVerifier::Phase::AfterGC);
1534     }
1535 
1536     didFinishCollection();
1537 
1538     if (m_currentRequest.didFinishEndPhase)
1539         m_currentRequest.didFinishEndPhase-&gt;run();
1540 
1541     if (false) {
1542         dataLog(&quot;Heap state after GC:\n&quot;);
1543         m_objectSpace.dumpBits();
1544     }
1545 
<span class="line-modified">1546     if (Options::logGC()) {</span>
1547         double thisPauseMS = (m_afterGC - m_stopTime).milliseconds();
1548         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;), cycle &quot;, (m_afterGC - m_beforeGC).milliseconds(), &quot;ms END]\n&quot;);
1549     }
1550 
1551     {
1552         auto locker = holdLock(*m_threadLock);
1553         m_requests.removeFirst();
1554         m_lastServedTicket++;
1555         clearMutatorWaiting();
1556     }
1557     ParkingLot::unparkAll(&amp;m_worldState);
1558 
<span class="line-modified">1559     if (false)</span>
<span class="line-removed">1560         dataLog(&quot;GC END!\n&quot;);</span>
1561 
1562     setNeedFinalize();
1563 
1564     m_lastGCStartTime = m_currentGCStartTime;
1565     m_lastGCEndTime = MonotonicTime::now();
1566     m_totalGCTime += m_lastGCEndTime - m_lastGCStartTime;
1567 
1568     return changePhase(conn, CollectorPhase::NotRunning);
1569 }
1570 
1571 bool Heap::changePhase(GCConductor conn, CollectorPhase nextPhase)
1572 {
1573     checkConn(conn);
1574 
1575     m_lastPhase = m_currentPhase;
1576     m_nextPhase = nextPhase;
1577 
1578     return finishChangingPhase(conn);
1579 }
1580 
</pre>
<hr />
<pre>
1641     m_worldIsStopped = true;
1642 
1643     forEachSlotVisitor(
1644         [&amp;] (SlotVisitor&amp; slotVisitor) {
1645             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1646         });
1647 
1648 #if ENABLE(JIT)
1649     if (VM::canUseJIT()) {
1650         DeferGCForAWhile awhile(*this);
1651         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm)
1652             &amp;&amp; conn == GCConductor::Collector)
1653             setGCDidJIT();
1654     }
1655 #endif // ENABLE(JIT)
1656     UNUSED_PARAM(conn);
1657 
1658     if (auto* shadowChicken = vm().shadowChicken())
1659         shadowChicken-&gt;update(vm(), vm().topCallFrame);
1660 
<span class="line-removed">1661     m_structureIDTable.flushOldTables();</span>
1662     m_objectSpace.stopAllocating();
1663 
1664     m_stopTime = MonotonicTime::now();
1665 }
1666 
1667 NEVER_INLINE void Heap::resumeThePeriphery()
1668 {
1669     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1670     // collection cycle or this is just a concurrent phase within a collection cycle:
1671     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1672     //   last active block.
1673     // - During collection cycle: it reinstates the last active block.
1674     m_objectSpace.resumeAllocating();
1675 
1676     m_barriersExecuted = 0;
1677 
1678     if (!m_worldIsStopped) {
1679         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
1680         RELEASE_ASSERT_NOT_REACHED();
1681     }
</pre>
<hr />
<pre>
2068 void Heap::setMutatorWaiting()
2069 {
2070     m_worldState.exchangeOr(mutatorWaitingBit);
2071 }
2072 
2073 void Heap::clearMutatorWaiting()
2074 {
2075     m_worldState.exchangeAnd(~mutatorWaitingBit);
2076 }
2077 
2078 void Heap::notifyThreadStopping(const AbstractLocker&amp;)
2079 {
2080     m_threadIsStopping = true;
2081     clearMutatorWaiting();
2082     ParkingLot::unparkAll(&amp;m_worldState);
2083 }
2084 
2085 void Heap::finalize()
2086 {
2087     MonotonicTime before;
<span class="line-modified">2088     if (Options::logGC()) {</span>
2089         before = MonotonicTime::now();
2090         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2091     }
2092 
2093     {
2094         SweepingScope sweepingScope(*this);
2095         deleteUnmarkedCompiledCode();
2096         deleteSourceProviderCaches();
2097         sweepInFinalize();
2098     }
2099 
2100     if (HasOwnPropertyCache* cache = vm().hasOwnPropertyCache())
2101         cache-&gt;clear();
2102 
2103     immutableButterflyToStringCache.clear();
2104 
2105     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
2106         callback.run(vm());
2107 
2108     if (shouldSweepSynchronously())
2109         sweepSynchronously();
2110 
<span class="line-modified">2111     if (Options::logGC()) {</span>
2112         MonotonicTime after = MonotonicTime::now();
2113         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2114     }
2115 }
2116 
2117 Heap::Ticket Heap::requestCollection(GCRequest request)
2118 {
2119     stopIfNecessary();
2120 
2121     ASSERT(vm().currentThreadIsHoldingAPILock());
2122     RELEASE_ASSERT(vm().atomStringTable() == Thread::current().atomStringTable());
2123 
2124     LockHolder locker(*m_threadLock);
2125     // We may be able to steal the conn. That only works if the collector is definitely not running
2126     // right now. This is an optimization that prevents the collector thread from ever starting in most
2127     // cases.
2128     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2129     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; !m_collectorThreadIsRunning) {
2130         if (false)
2131             dataLog(&quot;Taking the conn.\n&quot;);
2132         m_worldState.exchangeOr(mutatorHasConnBit);
2133     }
2134 
2135     m_requests.append(request);
2136     m_lastGrantedTicket++;
2137     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2138         m_threadCondition-&gt;notifyOne(locker);
2139     return m_lastGrantedTicket;
2140 }
2141 
2142 void Heap::waitForCollection(Ticket ticket)
2143 {
2144     waitForCollector(
2145         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2146             return m_lastServedTicket &gt;= ticket;
2147         });
2148 }
2149 
2150 void Heap::sweepInFinalize()
2151 {
<span class="line-modified">2152     m_objectSpace.sweepLargeAllocations();</span>
<span class="line-modified">2153     vm().eagerlySweptDestructibleObjectSpace.sweep();</span>




2154 }
2155 
2156 void Heap::suspendCompilerThreads()
2157 {
2158 #if ENABLE(DFG_JIT)
2159     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2160     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2161     // the worklists use AutomaticThreads anyway.
2162     if (!VM::canUseJIT())
2163         return;
2164     for (unsigned i = DFG::numberOfWorklists(); i--;)
2165         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2166 #endif
2167 }
2168 
2169 void Heap::willStartCollection()
2170 {
<span class="line-modified">2171     if (Options::logGC())</span>
<span class="line-removed">2172         dataLog(&quot;=&gt; &quot;);</span>
2173 
2174     if (shouldDoFullCollection()) {
2175         m_collectionScope = CollectionScope::Full;
2176         m_shouldDoFullCollection = false;
<span class="line-modified">2177         if (Options::logGC())</span>
<span class="line-removed">2178             dataLog(&quot;FullCollection, &quot;);</span>
<span class="line-removed">2179         if (false)</span>
<span class="line-removed">2180             dataLog(&quot;Full collection!\n&quot;);</span>
2181     } else {
2182         m_collectionScope = CollectionScope::Eden;
<span class="line-modified">2183         if (Options::logGC())</span>
<span class="line-removed">2184             dataLog(&quot;EdenCollection, &quot;);</span>
<span class="line-removed">2185         if (false)</span>
<span class="line-removed">2186             dataLog(&quot;Eden collection!\n&quot;);</span>
2187     }
2188     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2189         m_sizeBeforeLastFullCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2190         m_extraMemorySize = 0;
2191         m_deprecatedExtraMemorySize = 0;
2192 #if ENABLE(RESOURCE_USAGE)
2193         m_externalMemorySize = 0;
2194 #endif
2195 
2196         if (m_fullActivityCallback)
2197             m_fullActivityCallback-&gt;willCollect();
2198     } else {
2199         ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Eden);
2200         m_sizeBeforeLastEdenCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2201     }
2202 
2203     if (m_edenActivityCallback)
2204         m_edenActivityCallback-&gt;willCollect();
2205 
2206     for (auto* observer : m_observers)
</pre>
<hr />
<pre>
2237 }
2238 
2239 void Heap::deleteSourceProviderCaches()
2240 {
2241     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
2242         m_vm.clearSourceProviderCaches();
2243 }
2244 
2245 void Heap::notifyIncrementalSweeper()
2246 {
2247     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2248         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2249             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2250     }
2251 
2252     m_sweeper-&gt;startSweeping(*this);
2253 }
2254 
2255 void Heap::updateAllocationLimits()
2256 {
<span class="line-modified">2257     static const bool verbose = false;</span>
2258 
2259     if (verbose) {
2260         dataLog(&quot;\n&quot;);
2261         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2262     }
2263 
2264     // Calculate our current heap size threshold for the purpose of figuring out when we should
2265     // run another collection. This isn&#39;t the same as either size() or capacity(), though it should
2266     // be somewhere between the two. The key is to match the size calculations involved calls to
2267     // didAllocate(), while never dangerously underestimating capacity(). In extreme cases of
2268     // fragmentation, we may have size() much smaller than capacity().
2269     size_t currentHeapSize = 0;
2270 
2271     // For marked space, we use the total number of bytes visited. This matches the logic for
2272     // BlockDirectory&#39;s calls to didAllocate(), which effectively accounts for the total size of
2273     // objects allocated rather than blocks used. This will underestimate capacity(), and in case
2274     // of fragmentation, this may be substantial. Fortunately, marked space rarely fragments because
2275     // cells usually have a narrow range of sizes. So, the underestimation is probably OK.
2276     currentHeapSize += m_totalBytesVisited;
2277     if (verbose)
2278         dataLog(&quot;totalBytesVisited = &quot;, m_totalBytesVisited, &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2279 
2280     // It&#39;s up to the user to ensure that extraMemorySize() ends up corresponding to allocation-time
2281     // extra memory reporting.
2282     currentHeapSize += extraMemorySize();
<span class="line-modified">2283     if (!ASSERT_DISABLED) {</span>
2284         Checked&lt;size_t, RecordOverflow&gt; checkedCurrentHeapSize = m_totalBytesVisited;
2285         checkedCurrentHeapSize += extraMemorySize();
2286         ASSERT(!checkedCurrentHeapSize.hasOverflowed() &amp;&amp; checkedCurrentHeapSize.unsafeGet() == currentHeapSize);
2287     }
2288 
2289     if (verbose)
2290         dataLog(&quot;extraMemorySize() = &quot;, extraMemorySize(), &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2291 
2292     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2293         // To avoid pathological GC churn in very small and very large heaps, we set
2294         // the new allocation limit based on the current size of the heap, with a
2295         // fixed minimum.
2296         m_maxHeapSize = std::max(minHeapSize(m_heapType, m_ramSize), proportionalHeapSize(currentHeapSize, m_ramSize));
2297         if (verbose)
2298             dataLog(&quot;Full: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2299         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2300         if (verbose)
2301             dataLog(&quot;Full: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2302         m_sizeAfterLastFullCollect = currentHeapSize;
2303         if (verbose)
</pre>
<hr />
<pre>
2315         m_sizeAfterLastEdenCollect = currentHeapSize;
2316         if (verbose)
2317             dataLog(&quot;Eden: sizeAfterLastEdenCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2318         double edenToOldGenerationRatio = (double)m_maxEdenSize / (double)m_maxHeapSize;
2319         double minEdenToOldGenerationRatio = 1.0 / 3.0;
2320         if (edenToOldGenerationRatio &lt; minEdenToOldGenerationRatio)
2321             m_shouldDoFullCollection = true;
2322         // This seems suspect at first, but what it does is ensure that the nursery size is fixed.
2323         m_maxHeapSize += currentHeapSize - m_sizeAfterLastCollect;
2324         if (verbose)
2325             dataLog(&quot;Eden: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2326         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2327         if (verbose)
2328             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2329         if (m_fullActivityCallback) {
2330             ASSERT(currentHeapSize &gt;= m_sizeAfterLastFullCollect);
2331             m_fullActivityCallback-&gt;didAllocate(*this, currentHeapSize - m_sizeAfterLastFullCollect);
2332         }
2333     }
2334 
<span class="line-modified">2335 #if PLATFORM(IOS_FAMILY)</span>
2336     // Get critical memory threshold for next cycle.
2337     overCriticalMemoryThreshold(MemoryThresholdCallType::Direct);
2338 #endif
2339 
2340     m_sizeAfterLastCollect = currentHeapSize;
2341     if (verbose)
2342         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2343     m_bytesAllocatedThisCycle = 0;
2344 
<span class="line-modified">2345     if (Options::logGC())</span>
<span class="line-removed">2346         dataLog(&quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);</span>
2347 }
2348 
2349 void Heap::didFinishCollection()
2350 {
2351     m_afterGC = MonotonicTime::now();
2352     CollectionScope scope = *m_collectionScope;
2353     if (scope == CollectionScope::Full)
2354         m_lastFullGCLength = m_afterGC - m_beforeGC;
2355     else
2356         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2357 
2358 #if ENABLE(RESOURCE_USAGE)
2359     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2360 #endif
2361 
2362     if (HeapProfiler* heapProfiler = m_vm.heapProfiler()) {
2363         gatherExtraHeapData(*heapProfiler);
2364         removeDeadHeapSnapshotNodes(*heapProfiler);
2365     }
2366 
</pre>
<hr />
<pre>
2410 
2411 void Heap::didAllocate(size_t bytes)
2412 {
2413     if (m_edenActivityCallback)
2414         m_edenActivityCallback-&gt;didAllocate(*this, m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
2415     m_bytesAllocatedThisCycle += bytes;
2416     performIncrement(bytes);
2417 }
2418 
2419 bool Heap::isValidAllocation(size_t)
2420 {
2421     if (!isValidThreadState(m_vm))
2422         return false;
2423 
2424     if (isCurrentThreadBusy())
2425         return false;
2426 
2427     return true;
2428 }
2429 
<span class="line-modified">2430 void Heap::addFinalizer(JSCell* cell, Finalizer finalizer)</span>
2431 {
<span class="line-modified">2432     WeakSet::allocate(cell, &amp;m_finalizerOwner, reinterpret_cast&lt;void*&gt;(finalizer)); // Balanced by FinalizerOwner::finalize().</span>













2433 }
2434 
<span class="line-modified">2435 void Heap::FinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)</span>
2436 {


2437     HandleSlot slot = handle.slot();
<span class="line-removed">2438     Finalizer finalizer = reinterpret_cast&lt;Finalizer&gt;(context);</span>
2439     finalizer(slot-&gt;asCell());
2440     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
2441 }
2442 
2443 void Heap::collectNowFullIfNotDoneRecently(Synchronousness synchronousness)
2444 {
2445     if (!m_fullActivityCallback) {
2446         collectNow(synchronousness, CollectionScope::Full);
2447         return;
2448     }
2449 
2450     if (m_fullActivityCallback-&gt;didGCRecently()) {
2451         // A synchronous GC was already requested recently so we merely accelerate next collection.
2452         reportAbandonedObjectGraph();
2453         return;
2454     }
2455 
2456     m_fullActivityCallback-&gt;setDidGCRecently();
2457     collectNow(synchronousness, CollectionScope::Full);
2458 }
</pre>
<hr />
<pre>
2612         return;
2613     }
2614     if (!Options::useGC())
2615         return;
2616 
2617     if (mayNeedToStop()) {
2618         if (deferralContext)
2619             deferralContext-&gt;m_shouldGC = true;
2620         else if (isDeferred())
2621             m_didDeferGCWork = true;
2622         else
2623             stopIfNecessary();
2624     }
2625 
2626     if (UNLIKELY(Options::gcMaxHeapSize())) {
2627         if (m_bytesAllocatedThisCycle &lt;= Options::gcMaxHeapSize())
2628             return;
2629     } else {
2630         size_t bytesAllowedThisCycle = m_maxEdenSize;
2631 
<span class="line-modified">2632 #if PLATFORM(IOS_FAMILY)</span>
2633         if (overCriticalMemoryThreshold())
2634             bytesAllowedThisCycle = std::min(m_maxEdenSizeWhenCritical, bytesAllowedThisCycle);
2635 #endif
2636 
2637         if (m_bytesAllocatedThisCycle &lt;= bytesAllowedThisCycle)
2638             return;
2639     }
2640 
2641     if (deferralContext)
2642         deferralContext-&gt;m_shouldGC = true;
2643     else if (isDeferred())
2644         m_didDeferGCWork = true;
2645     else {
2646         collectAsync();
2647         stopIfNecessary(); // This will immediately start the collection if we have the conn.
2648     }
2649 }
2650 
2651 void Heap::decrementDeferralDepthAndGCIfNeededSlow()
2652 {
</pre>
<hr />
<pre>
2753                 slotVisitor.appendUnbarriered(m_vm.lastException());
2754             }
2755         },
2756         ConstraintVolatility::GreyedByExecution);
2757 
2758     m_constraintSet-&gt;add(
2759         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2760         [this] (SlotVisitor&amp; slotVisitor) {
2761             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2762             m_handleSet.visitStrongHandles(slotVisitor);
2763         },
2764         ConstraintVolatility::GreyedByExecution);
2765 
2766     m_constraintSet-&gt;add(
2767         &quot;D&quot;, &quot;Debugger&quot;,
2768         [this] (SlotVisitor&amp; slotVisitor) {
2769             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2770 
2771 #if ENABLE(SAMPLING_PROFILER)
2772             if (SamplingProfiler* samplingProfiler = m_vm.samplingProfiler()) {
<span class="line-modified">2773                 LockHolder locker(samplingProfiler-&gt;getLock());</span>
<span class="line-modified">2774                 samplingProfiler-&gt;processUnverifiedStackTraces();</span>
2775                 samplingProfiler-&gt;visit(slotVisitor);
2776                 if (Options::logGC() == GCLogging::Verbose)
2777                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2778             }
2779 #endif // ENABLE(SAMPLING_PROFILER)
2780 
2781             if (m_vm.typeProfiler())
2782                 m_vm.typeProfilerLog()-&gt;visit(slotVisitor);
2783 
2784             if (auto* shadowChicken = m_vm.shadowChicken())
2785                 shadowChicken-&gt;visitChildren(slotVisitor);
2786         },
2787         ConstraintVolatility::GreyedByExecution);
2788 
2789     m_constraintSet-&gt;add(
2790         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2791         [this] (SlotVisitor&amp; slotVisitor) {
2792             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2793             m_objectSpace.visitWeakSets(slotVisitor);
2794         },
</pre>
<hr />
<pre>
2851                     // Visit the CodeBlock as a constraint only if it&#39;s black.
2852                     if (isMarked(codeBlock)
2853                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2854                         slotVisitor.visitAsConstraint(codeBlock);
2855                 });
2856         },
2857         ConstraintVolatility::SeldomGreyed);
2858 
2859     m_constraintSet-&gt;add(makeUnique&lt;MarkStackMergingConstraint&gt;(*this));
2860 }
2861 
2862 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2863 {
2864     PreventCollectionScope preventCollectionScope(*this);
2865     m_constraintSet-&gt;add(WTFMove(constraint));
2866 }
2867 
2868 void Heap::notifyIsSafeToCollect()
2869 {
2870     MonotonicTime before;
<span class="line-modified">2871     if (Options::logGC()) {</span>
2872         before = MonotonicTime::now();
2873         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2874     }
2875 
2876     addCoreConstraints();
2877 
2878     m_isSafeToCollect = true;
2879 
2880     if (Options::collectContinuously()) {
2881         m_collectContinuouslyThread = Thread::create(
2882             &quot;JSC DEBUG Continuous GC&quot;,
2883             [this] () {
2884                 MonotonicTime initialTime = MonotonicTime::now();
2885                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2886                 while (!m_shouldStopCollectingContinuously) {
2887                     {
2888                         LockHolder locker(*m_threadLock);
2889                         if (m_requests.isEmpty()) {
2890                             m_requests.append(WTF::nullopt);
2891                             m_lastGrantedTicket++;
2892                             m_threadCondition-&gt;notifyOne(locker);
2893                         }
2894                     }
2895 
2896                     {
2897                         LockHolder locker(m_collectContinuouslyLock);
2898                         Seconds elapsed = MonotonicTime::now() - initialTime;
2899                         Seconds elapsedInPeriod = elapsed % period;
2900                         MonotonicTime timeToWakeUp =
2901                             initialTime + elapsed - elapsedInPeriod + period;
2902                         while (!hasElapsed(timeToWakeUp) &amp;&amp; !m_shouldStopCollectingContinuously) {
2903                             m_collectContinuouslyCondition.waitUntil(
2904                                 m_collectContinuouslyLock, timeToWakeUp);
2905                         }
2906                     }
2907                 }
2908             });
2909     }
2910 
<span class="line-modified">2911     if (Options::logGC())</span>
<span class="line-removed">2912         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);</span>
2913 }
2914 
2915 void Heap::preventCollection()
2916 {
2917     if (!m_isSafeToCollect)
2918         return;
2919 
2920     // This prevents the collectContinuously thread from starting a collection.
2921     m_collectContinuouslyLock.lock();
2922 
2923     // Wait for all collections to finish.
2924     waitForCollector(
2925         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2926             ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2927             return m_lastServedTicket == m_lastGrantedTicket;
2928         });
2929 
2930     // Now a collection can only start if this thread starts it.
2931     RELEASE_ASSERT(!m_collectionScope);
2932 }
</pre>
</td>
<td>
<hr />
<pre>
  63 #include &quot;PreventCollectionScope.h&quot;
  64 #include &quot;SamplingProfiler.h&quot;
  65 #include &quot;ShadowChicken.h&quot;
  66 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  67 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  68 #include &quot;StopIfNecessaryTimer.h&quot;
  69 #include &quot;SubspaceInlines.h&quot;
  70 #include &quot;SuperSampler.h&quot;
  71 #include &quot;SweepingScope.h&quot;
  72 #include &quot;SymbolTableInlines.h&quot;
  73 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  74 #include &quot;TypeProfiler.h&quot;
  75 #include &quot;TypeProfilerLog.h&quot;
  76 #include &quot;UnlinkedCodeBlock.h&quot;
  77 #include &quot;VM.h&quot;
  78 #include &quot;VisitCounter.h&quot;
  79 #include &quot;WasmMemory.h&quot;
  80 #include &quot;WeakMapImplInlines.h&quot;
  81 #include &quot;WeakSetInlines.h&quot;
  82 #include &lt;algorithm&gt;
<span class="line-added">  83 #include &lt;wtf/CryptographicallyRandomNumber.h&gt;</span>
  84 #include &lt;wtf/ListDump.h&gt;
  85 #include &lt;wtf/MainThread.h&gt;
  86 #include &lt;wtf/ParallelVectorIterator.h&gt;
  87 #include &lt;wtf/ProcessID.h&gt;
  88 #include &lt;wtf/RAMSize.h&gt;
  89 #include &lt;wtf/SimpleStats.h&gt;
  90 #include &lt;wtf/Threading.h&gt;
  91 
<span class="line-modified">  92 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)</span>
  93 #include &lt;bmalloc/bmalloc.h&gt;
  94 #endif
  95 
  96 #if USE(FOUNDATION)
  97 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  98 #endif
  99 
 100 #ifdef JSC_GLIB_API_ENABLED
 101 #include &quot;JSCGLibWrapperObject.h&quot;
 102 #endif
 103 
 104 namespace JSC {
 105 
 106 namespace {
 107 
 108 bool verboseStop = false;
 109 
 110 double maxPauseMS(double thisPauseMS)
 111 {
 112     static double maxPauseMS;
 113     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 114     return maxPauseMS;
 115 }
 116 
 117 size_t minHeapSize(HeapType heapType, size_t ramSize)
 118 {
 119     if (heapType == LargeHeap) {
 120         double result = std::min(
 121             static_cast&lt;double&gt;(Options::largeHeapSize()),
 122             ramSize * Options::smallHeapRAMFraction());
 123         return static_cast&lt;size_t&gt;(result);
 124     }
 125     return Options::smallHeapSize();
 126 }
 127 
 128 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 129 {
 130     if (VM::isInMiniMode())
 131         return Options::miniVMHeapGrowthFactor() * heapSize;
 132 
<span class="line-modified"> 133 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)</span>
 134     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 135     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 136         return Options::smallHeapGrowthFactor() * heapSize;
 137     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 138         return Options::mediumHeapGrowthFactor() * heapSize;
 139 #else
 140     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 141         return Options::smallHeapGrowthFactor() * heapSize;
 142     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 143         return Options::mediumHeapGrowthFactor() * heapSize;
 144 #endif
 145     return Options::largeHeapGrowthFactor() * heapSize;
 146 }
 147 
 148 bool isValidSharedInstanceThreadState(VM&amp; vm)
 149 {
 150     return vm.currentThreadIsHoldingAPILock();
 151 }
 152 
 153 bool isValidThreadState(VM&amp; vm)
</pre>
<hr />
<pre>
 355         [&amp;] (SlotVisitor&amp; visitor) {
 356             visitor.clearMarkStacks();
 357         });
 358     m_mutatorMarkStack-&gt;clear();
 359     m_raceMarkStack-&gt;clear();
 360 
 361     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 362         WeakBlock::destroy(*this, block);
 363 }
 364 
 365 bool Heap::isPagedOut(MonotonicTime deadline)
 366 {
 367     return m_objectSpace.isPagedOut(deadline);
 368 }
 369 
 370 void Heap::dumpHeapStatisticsAtVMDestruction()
 371 {
 372     unsigned counter = 0;
 373     m_objectSpace.forEachBlock([&amp;] (MarkedBlock::Handle* block) {
 374         unsigned live = 0;
<span class="line-modified"> 375         block-&gt;forEachCell([&amp;] (size_t, HeapCell* cell, HeapCell::Kind) {</span>
 376             if (cell-&gt;isLive())
 377                 live++;
 378             return IterationStatus::Continue;
 379         });
 380         dataLogLn(&quot;[&quot;, counter++, &quot;] &quot;, block-&gt;cellSize(), &quot;, &quot;, live, &quot; / &quot;, block-&gt;cellsPerBlock(), &quot; &quot;, static_cast&lt;double&gt;(live) / block-&gt;cellsPerBlock() * 100, &quot;% &quot;, block-&gt;attributes(), &quot; &quot;, block-&gt;subspace()-&gt;name());
<span class="line-modified"> 381         block-&gt;forEachCell([&amp;] (size_t, HeapCell* heapCell, HeapCell::Kind kind) {</span>
 382             if (heapCell-&gt;isLive() &amp;&amp; kind == HeapCell::Kind::JSCell) {
 383                 auto* cell = static_cast&lt;JSCell*&gt;(heapCell);
 384                 if (cell-&gt;isObject())
 385                     dataLogLn(&quot;    &quot;, JSValue((JSObject*)cell));
 386                 else
 387                     dataLogLn(&quot;    &quot;, *cell);
 388             }
 389             return IterationStatus::Continue;
 390         });
 391     });
 392 }
 393 
 394 // The VM is being destroyed and the collector will never run again.
 395 // Run all pending finalizers now because we won&#39;t get another chance.
 396 void Heap::lastChanceToFinalize()
 397 {
 398     MonotonicTime before;
<span class="line-modified"> 399     if (UNLIKELY(Options::logGC())) {</span>
 400         before = MonotonicTime::now();
 401         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 402     }
 403 
 404     m_isShuttingDown = true;
 405 
 406     RELEASE_ASSERT(!m_vm.entryScope);
 407     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 408 
 409     if (m_collectContinuouslyThread) {
 410         {
 411             LockHolder locker(m_collectContinuouslyLock);
 412             m_shouldStopCollectingContinuously = true;
 413             m_collectContinuouslyCondition.notifyOne();
 414         }
 415         m_collectContinuouslyThread-&gt;waitForCompletion();
 416     }
 417 
<span class="line-modified"> 418     dataLogIf(Options::logGC(), &quot;1&quot;);</span>

 419 
 420     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 421     // going to call into anything that starts collections. Still, this makes the algorithm more
 422     // obviously sound.
 423     m_isSafeToCollect = false;
 424 
<span class="line-modified"> 425     dataLogIf(Options::logGC(), &quot;2&quot;);</span>

 426 
 427     bool isCollecting;
 428     {
 429         auto locker = holdLock(*m_threadLock);
 430         RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 431         isCollecting = m_lastServedTicket &lt; m_lastGrantedTicket;
 432     }
 433     if (isCollecting) {
<span class="line-modified"> 434         dataLogIf(Options::logGC(), &quot;...]\n&quot;);</span>

 435 
 436         // Wait for the current collection to finish.
 437         waitForCollector(
 438             [&amp;] (const AbstractLocker&amp;) -&gt; bool {
 439                 RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 440                 return m_lastServedTicket == m_lastGrantedTicket;
 441             });
 442 
<span class="line-modified"> 443         dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);</span>

 444     }
<span class="line-modified"> 445     dataLogIf(Options::logGC(), &quot;3&quot;);</span>

 446 
 447     RELEASE_ASSERT(m_requests.isEmpty());
 448     RELEASE_ASSERT(m_lastServedTicket == m_lastGrantedTicket);
 449 
 450     // Carefully bring the thread down.
 451     bool stopped = false;
 452     {
 453         LockHolder locker(*m_threadLock);
 454         stopped = m_thread-&gt;tryStop(locker);
 455         m_threadShouldStop = true;
 456         if (!stopped)
 457             m_threadCondition-&gt;notifyOne(locker);
 458     }
 459 
<span class="line-modified"> 460     dataLogIf(Options::logGC(), &quot;4&quot;);</span>

 461 
 462     if (!stopped)
 463         m_thread-&gt;join();
 464 
<span class="line-modified"> 465     dataLogIf(Options::logGC(), &quot;5 &quot;);</span>

 466 
 467     if (UNLIKELY(Options::dumpHeapStatisticsAtVMDestruction()))
 468         dumpHeapStatisticsAtVMDestruction();
 469 
 470     m_arrayBuffers.lastChanceToFinalize();
 471     m_objectSpace.stopAllocatingForGood();
 472     m_objectSpace.lastChanceToFinalize();
 473     releaseDelayedReleasedObjects();
 474 
 475     sweepAllLogicallyEmptyWeakBlocks();
 476 
 477     m_objectSpace.freeMemory();
 478 
<span class="line-modified"> 479     dataLogIf(Options::logGC(), (MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);</span>

 480 }
 481 
 482 void Heap::releaseDelayedReleasedObjects()
 483 {
 484 #if USE(FOUNDATION) || defined(JSC_GLIB_API_ENABLED)
 485     // We need to guard against the case that releasing an object can create more objects due to the
 486     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 487     // back here and could try to recursively release objects. We guard that with a recursive entry
 488     // count. Only the initial call will release objects, recursive calls simple return and let the
 489     // the initial call to the function take care of any objects created during release time.
 490     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 491     // and use a temp Vector for the actual releasing.
 492     if (!m_delayedReleaseRecursionCount++) {
 493         while (!m_delayedReleaseObjects.isEmpty()) {
 494             ASSERT(m_vm.currentThreadIsHoldingAPILock());
 495 
 496             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 497 
 498             {
 499                 // We need to drop locks before calling out to arbitrary code.
</pre>
<hr />
<pre>
 514 }
 515 
 516 void Heap::reportExtraMemoryAllocatedSlowCase(size_t size)
 517 {
 518     didAllocate(size);
 519     collectIfNecessaryOrDefer();
 520 }
 521 
 522 void Heap::deprecatedReportExtraMemorySlowCase(size_t size)
 523 {
 524     // FIXME: Change this to use SaturatedArithmetic when available.
 525     // https://bugs.webkit.org/show_bug.cgi?id=170411
 526     Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = m_deprecatedExtraMemorySize;
 527     checkedNewSize += size;
 528     m_deprecatedExtraMemorySize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
 529     reportExtraMemoryAllocatedSlowCase(size);
 530 }
 531 
 532 bool Heap::overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType)
 533 {
<span class="line-modified"> 534 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)</span>
<span class="line-modified"> 535     if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_percentAvailableMemoryCachedCallCount &gt;= 100) {</span>
 536         m_overCriticalMemoryThreshold = bmalloc::api::percentAvailableMemoryInUse() &gt; Options::criticalGCMemoryThreshold();
<span class="line-modified"> 537         m_percentAvailableMemoryCachedCallCount = 0;</span>
 538     }
 539 
 540     return m_overCriticalMemoryThreshold;
 541 #else
 542     UNUSED_PARAM(memoryThresholdCallType);
 543     return false;
 544 #endif
 545 }
 546 
 547 void Heap::reportAbandonedObjectGraph()
 548 {
 549     // Our clients don&#39;t know exactly how much memory they
 550     // are abandoning so we just guess for them.
 551     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 552 
 553     // We want to accelerate the next collection. Because memory has just
 554     // been abandoned, the next collection has the potential to
 555     // be more profitable. Since allocation is the trigger for collection,
 556     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 557     if (m_fullActivityCallback) {
</pre>
<hr />
<pre>
 588     if (m_arrayBuffers.addReference(cell, buffer)) {
 589         collectIfNecessaryOrDefer();
 590         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 591     }
 592 }
 593 
 594 template&lt;typename CellType, typename CellSet&gt;
 595 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 596 {
 597     cellSet.forEachMarkedCell(
 598         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 599             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(vm());
 600         });
 601 }
 602 
 603 void Heap::finalizeUnconditionalFinalizers()
 604 {
 605     vm().builtinExecutables()-&gt;finalizeUnconditionally();
 606     finalizeMarkedUnconditionalFinalizers&lt;FunctionExecutable&gt;(vm().functionExecutableSpace.space);
 607     finalizeMarkedUnconditionalFinalizers&lt;SymbolTable&gt;(vm().symbolTableSpace);
<span class="line-added"> 608     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm().executableToCodeBlockEdgesWithFinalizers); // We run this before CodeBlock&#39;s unconditional finalizer since CodeBlock looks at the owner executable&#39;s installed CodeBlock in its finalizeUnconditionally.</span>
 609     vm().forEachCodeBlockSpace(
 610         [&amp;] (auto&amp; space) {
 611             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 612         });

 613     finalizeMarkedUnconditionalFinalizers&lt;StructureRareData&gt;(vm().structureRareDataSpace);
 614     finalizeMarkedUnconditionalFinalizers&lt;UnlinkedFunctionExecutable&gt;(vm().unlinkedFunctionExecutableSpace.set);
 615     if (vm().m_weakSetSpace)
 616         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm().m_weakSetSpace);
 617     if (vm().m_weakMapSpace)
 618         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm().m_weakMapSpace);
 619     if (vm().m_weakObjectRefSpace)
 620         finalizeMarkedUnconditionalFinalizers&lt;JSWeakObjectRef&gt;(*vm().m_weakObjectRefSpace);
 621     if (vm().m_errorInstanceSpace)
 622         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm().m_errorInstanceSpace);
 623 
 624 #if ENABLE(WEBASSEMBLY)
 625     if (vm().m_webAssemblyCodeBlockSpace)
 626         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm().m_webAssemblyCodeBlockSpace);
 627 #endif
 628 }
 629 
 630 void Heap::willStartIterating()
 631 {
 632     m_objectSpace.willStartIterating();
</pre>
<hr />
<pre>
 696 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 697 {
 698     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 699 }
 700 
 701 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 702 {
 703 #if ENABLE(C_LOOP)
 704     m_vm.interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);
 705 #else
 706     UNUSED_PARAM(roots);
 707 #endif
 708 }
 709 
 710 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 711 {
 712 #if ENABLE(DFG_JIT)
 713     if (!VM::canUseJIT())
 714         return;
 715     m_vm.gatherScratchBufferRoots(roots);
<span class="line-added"> 716     m_vm.scanSideState(roots);</span>
 717 #else
 718     UNUSED_PARAM(roots);
 719 #endif
 720 }
 721 
 722 void Heap::beginMarking()
 723 {
 724     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 725     m_jitStubRoutines-&gt;clearMarks();
 726     m_objectSpace.beginMarking();
 727     setMutatorShouldBeFenced(true);
 728 }
 729 
 730 void Heap::removeDeadCompilerWorklistEntries()
 731 {
 732 #if ENABLE(DFG_JIT)
 733     if (!VM::canUseJIT())
 734         return;
 735     for (unsigned i = DFG::numberOfWorklists(); i--;)
 736         DFG::existingWorklistForIndex(i).removeDeadPlans(m_vm);
</pre>
<hr />
<pre>
1026                 // return anyway, since we proved that the object was not marked at the time that
1027                 // we executed this slow path.
1028             }
1029 
1030             return;
1031         }
1032     } else
1033         ASSERT(isMarked(cell));
1034     // It could be that the object was *just* marked. This means that the collector may set the
1035     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
1036     // race with the collector here. If we win then this is accurate because the object _will_
1037     // get scanned again. If we lose then someone else will barrier the object again. That would
1038     // be unfortunate but not the end of the world.
1039     cell-&gt;setCellState(CellState::PossiblyGrey);
1040     m_mutatorMarkStack-&gt;append(cell);
1041 }
1042 
1043 void Heap::sweepSynchronously()
1044 {
1045     MonotonicTime before { };
<span class="line-modified">1046     if (UNLIKELY(Options::logGC())) {</span>
1047         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
1048         before = MonotonicTime::now();
1049     }
<span class="line-modified">1050     m_objectSpace.sweepBlocks();</span>
1051     m_objectSpace.shrink();
<span class="line-modified">1052     if (UNLIKELY(Options::logGC())) {</span>
1053         MonotonicTime after = MonotonicTime::now();
1054         dataLog(&quot;=&gt; &quot;, capacity() / 1024, &quot;kb, &quot;, (after - before).milliseconds(), &quot;ms&quot;);
1055     }
1056 }
1057 
1058 void Heap::collect(Synchronousness synchronousness, GCRequest request)
1059 {
1060     switch (synchronousness) {
1061     case Async:
1062         collectAsync(request);
1063         return;
1064     case Sync:
1065         collectSync(request);
1066         return;
1067     }
1068     RELEASE_ASSERT_NOT_REACHED();
1069 }
1070 
1071 void Heap::collectNow(Synchronousness synchronousness, GCRequest request)
1072 {
1073     if (validateDFGDoesGC)
1074         RELEASE_ASSERT(expectDoesGC());
1075 
1076     switch (synchronousness) {
1077     case Async: {
1078         collectAsync(request);
1079         stopIfNecessary();
1080         return;
1081     }
1082 
1083     case Sync: {
1084         collectSync(request);
1085 
1086         DeferGCForAWhile deferGC(*this);
1087         if (UNLIKELY(Options::useImmortalObjects()))
1088             sweeper().stopSweeping();
1089 
1090         bool alreadySweptInCollectSync = shouldSweepSynchronously();
1091         if (!alreadySweptInCollectSync) {
<span class="line-modified">1092             dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;);</span>

1093             sweepSynchronously();
<span class="line-modified">1094             dataLogIf(Options::logGC(), &quot;]\n&quot;);</span>

1095         }
1096         m_objectSpace.assertNoUnswept();
1097 
1098         sweepAllLogicallyEmptyWeakBlocks();
1099         return;
1100     } }
1101     RELEASE_ASSERT_NOT_REACHED();
1102 }
1103 
1104 void Heap::collectAsync(GCRequest request)
1105 {
1106     if (validateDFGDoesGC)
1107         RELEASE_ASSERT(expectDoesGC());
1108 
1109     if (!m_isSafeToCollect)
1110         return;
1111 
1112     bool alreadyRequested = false;
1113     {
1114         LockHolder locker(*m_threadLock);
</pre>
<hr />
<pre>
1238     // Check m_requests since the mutator calls this to poll what&#39;s going on.
1239     {
1240         auto locker = holdLock(*m_threadLock);
1241         if (m_requests.isEmpty())
1242             return false;
1243     }
1244 
1245     return changePhase(conn, CollectorPhase::Begin);
1246 }
1247 
1248 NEVER_INLINE bool Heap::runBeginPhase(GCConductor conn)
1249 {
1250     m_currentGCStartTime = MonotonicTime::now();
1251 
1252     {
1253         LockHolder locker(*m_threadLock);
1254         RELEASE_ASSERT(!m_requests.isEmpty());
1255         m_currentRequest = m_requests.first();
1256     }
1257 
<span class="line-modified">1258     dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: START &quot;, gcConductorShortName(conn), &quot; &quot;, capacity() / 1024, &quot;kb &quot;);</span>

1259 
1260     m_beforeGC = MonotonicTime::now();
1261 
<span class="line-added">1262     if (!Options::seedOfVMRandomForFuzzer())</span>
<span class="line-added">1263         vm().random().setSeed(cryptographicallyRandomNumber());</span>
<span class="line-added">1264 </span>
1265     if (m_collectionScope) {
<span class="line-modified">1266         dataLogLn(&quot;Collection scope already set during GC: &quot;, *m_collectionScope);</span>
1267         RELEASE_ASSERT_NOT_REACHED();
1268     }
1269 
1270     willStartCollection();
1271 
1272     if (UNLIKELY(m_verifier)) {
1273         // Verify that live objects from the last GC cycle haven&#39;t been corrupted by
1274         // mutators before we begin this new GC cycle.
1275         m_verifier-&gt;verify(HeapVerifier::Phase::BeforeGC);
1276 
1277         m_verifier-&gt;startGC();
1278         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::BeforeMarking);
1279     }
1280 
1281     prepareForMarking();
1282 
1283     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
1284         m_opaqueRoots.clear();
1285         m_collectorSlotVisitor-&gt;clearMarkStacks();
1286         m_mutatorMarkStack-&gt;clear();
</pre>
<hr />
<pre>
1307             }
1308 
1309             Thread::registerGCThread(GCThreadType::Helper);
1310 
1311             {
1312                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1313                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1314             }
1315 
1316             {
1317                 LockHolder locker(m_parallelSlotVisitorLock);
1318                 m_availableParallelSlotVisitors.append(slotVisitor);
1319             }
1320         });
1321 
1322     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1323 
1324     m_constraintSet-&gt;didStartMarking();
1325 
1326     m_scheduler-&gt;beginCollection();
<span class="line-modified">1327     if (UNLIKELY(Options::logGC()))</span>
1328         m_scheduler-&gt;log();
1329 
1330     // After this, we will almost certainly fall through all of the &quot;slotVisitor.isEmpty()&quot;
1331     // checks because bootstrap would have put things into the visitor. So, we should fall
1332     // through to draining.
1333 
1334     if (!slotVisitor.didReachTermination()) {
1335         dataLog(&quot;Fatal: SlotVisitor should think that GC should terminate before constraint solving, but it does not think this.\n&quot;);
1336         dataLog(&quot;slotVisitor.isEmpty(): &quot;, slotVisitor.isEmpty(), &quot;\n&quot;);
1337         dataLog(&quot;slotVisitor.collectorMarkStack().isEmpty(): &quot;, slotVisitor.collectorMarkStack().isEmpty(), &quot;\n&quot;);
1338         dataLog(&quot;slotVisitor.mutatorMarkStack().isEmpty(): &quot;, slotVisitor.mutatorMarkStack().isEmpty(), &quot;\n&quot;);
1339         dataLog(&quot;m_numberOfActiveParallelMarkers: &quot;, m_numberOfActiveParallelMarkers, &quot;\n&quot;);
1340         dataLog(&quot;m_sharedCollectorMarkStack-&gt;isEmpty(): &quot;, m_sharedCollectorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1341         dataLog(&quot;m_sharedMutatorMarkStack-&gt;isEmpty(): &quot;, m_sharedMutatorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1342         dataLog(&quot;slotVisitor.didReachTermination(): &quot;, slotVisitor.didReachTermination(), &quot;\n&quot;);
1343         RELEASE_ASSERT_NOT_REACHED();
1344     }
1345 
1346     return changePhase(conn, CollectorPhase::Fixpoint);
1347 }
1348 
1349 NEVER_INLINE bool Heap::runFixpointPhase(GCConductor conn)
1350 {
1351     RELEASE_ASSERT(conn == GCConductor::Collector || m_currentThreadState);
1352 
1353     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1354 
<span class="line-modified">1355     if (UNLIKELY(Options::logGC())) {</span>
1356         HashMap&lt;const char*, size_t&gt; visitMap;
1357         forEachSlotVisitor(
1358             [&amp;] (SlotVisitor&amp; slotVisitor) {
1359                 visitMap.add(slotVisitor.codeName(), slotVisitor.bytesVisited() / 1024);
1360             });
1361 
1362         auto perVisitorDump = sortedMapDump(
1363             visitMap,
1364             [] (const char* a, const char* b) -&gt; bool {
1365                 return strcmp(a, b) &lt; 0;
1366             },
1367             &quot;:&quot;, &quot; &quot;);
1368 
1369         dataLog(&quot;v=&quot;, bytesVisited() / 1024, &quot;kb (&quot;, perVisitorDump, &quot;) o=&quot;, m_opaqueRoots.size(), &quot; b=&quot;, m_barriersExecuted, &quot; &quot;);
1370     }
1371 
1372     if (slotVisitor.didReachTermination()) {
1373         m_opaqueRoots.deleteOldTables();
1374 
1375         m_scheduler-&gt;didReachTermination();
</pre>
<hr />
<pre>
1379         // FIXME: Take m_mutatorDidRun into account when scheduling constraints. Most likely,
1380         // we don&#39;t have to execute root constraints again unless the mutator did run. At a
1381         // minimum, we could use this for work estimates - but it&#39;s probably more than just an
1382         // estimate.
1383         // https://bugs.webkit.org/show_bug.cgi?id=166828
1384 
1385         // Wondering what this does? Look at Heap::addCoreConstraints(). The DOM and others can also
1386         // add their own using Heap::addMarkingConstraint().
1387         bool converged = m_constraintSet-&gt;executeConvergence(slotVisitor);
1388 
1389         // FIXME: The slotVisitor.isEmpty() check is most likely not needed.
1390         // https://bugs.webkit.org/show_bug.cgi?id=180310
1391         if (converged &amp;&amp; slotVisitor.isEmpty()) {
1392             assertMarkStacksEmpty();
1393             return changePhase(conn, CollectorPhase::End);
1394         }
1395 
1396         m_scheduler-&gt;didExecuteConstraints();
1397     }
1398 
<span class="line-modified">1399     dataLogIf(Options::logGC(), slotVisitor.collectorMarkStack().size(), &quot;+&quot;, m_mutatorMarkStack-&gt;size() + slotVisitor.mutatorMarkStack().size(), &quot; &quot;);</span>

1400 
1401     {
1402         ParallelModeEnabler enabler(slotVisitor);
1403         slotVisitor.drainInParallel(m_scheduler-&gt;timeToResume());
1404     }
1405 
1406     m_scheduler-&gt;synchronousDrainingDidStall();
1407 
1408     // This is kinda tricky. The termination check looks at:
1409     //
1410     // - Whether the marking threads are active. If they are not, this means that the marking threads&#39;
1411     //   SlotVisitors are empty.
1412     // - Whether the collector&#39;s slot visitor is empty.
1413     // - Whether the shared mark stacks are empty.
1414     //
1415     // This doesn&#39;t have to check the mutator SlotVisitor because that one becomes empty after every GC
1416     // work increment, so it must be empty now.
1417     if (slotVisitor.didReachTermination())
1418         return true; // This is like relooping to the top if runFixpointPhase().
1419 
1420     if (!m_scheduler-&gt;shouldResume())
1421         return true;
1422 
1423     m_scheduler-&gt;willResume();
1424 
<span class="line-modified">1425     if (UNLIKELY(Options::logGC())) {</span>
1426         double thisPauseMS = (MonotonicTime::now() - m_stopTime).milliseconds();
1427         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;)...]\n&quot;);
1428     }
1429 
1430     // Forgive the mutator for its past failures to keep up.
1431     // FIXME: Figure out if moving this to different places results in perf changes.
1432     m_incrementBalance = 0;
1433 
1434     return changePhase(conn, CollectorPhase::Concurrent);
1435 }
1436 
1437 NEVER_INLINE bool Heap::runConcurrentPhase(GCConductor conn)
1438 {
1439     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1440 
1441     switch (conn) {
1442     case GCConductor::Mutator: {
1443         // When the mutator has the conn, we poll runConcurrentPhase() on every time someone says
1444         // stopIfNecessary(), so on every allocation slow path. When that happens we poll if it&#39;s time
1445         // to stop and do some work.
</pre>
<hr />
<pre>
1449 
1450         // We could be coming from a collector phase that stuffed our SlotVisitor, so make sure we donate
1451         // everything. This is super cheap if the SlotVisitor is already empty.
1452         slotVisitor.donateAll();
1453         return false;
1454     }
1455     case GCConductor::Collector: {
1456         {
1457             ParallelModeEnabler enabler(slotVisitor);
1458             slotVisitor.drainInParallelPassively(m_scheduler-&gt;timeToStop());
1459         }
1460         return changePhase(conn, CollectorPhase::Reloop);
1461     } }
1462 
1463     RELEASE_ASSERT_NOT_REACHED();
1464     return false;
1465 }
1466 
1467 NEVER_INLINE bool Heap::runReloopPhase(GCConductor conn)
1468 {
<span class="line-modified">1469     dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;, gcConductorShortName(conn), &quot; &quot;);</span>

1470 
1471     m_scheduler-&gt;didStop();
1472 
<span class="line-modified">1473     if (UNLIKELY(Options::logGC()))</span>
1474         m_scheduler-&gt;log();
1475 
1476     return changePhase(conn, CollectorPhase::Fixpoint);
1477 }
1478 
1479 NEVER_INLINE bool Heap::runEndPhase(GCConductor conn)
1480 {
1481     m_scheduler-&gt;endCollection();
1482 
1483     {
1484         auto locker = holdLock(m_markingMutex);
1485         m_parallelMarkersShouldExit = true;
1486         m_markingConditionVariable.notifyAll();
1487     }
1488     m_helperClient.finish();
1489 
1490     iterateExecutingAndCompilingCodeBlocks(
1491         [&amp;] (CodeBlock* codeBlock) {
1492             writeBarrier(codeBlock);
1493         });
1494 
1495     updateObjectCounts();
1496     endMarking();
1497 
1498     if (UNLIKELY(m_verifier)) {
1499         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1500         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1501     }
1502 
1503     if (vm().typeProfiler())
1504         vm().typeProfiler()-&gt;invalidateTypeSetCache(vm());
1505 
<span class="line-added">1506     m_structureIDTable.flushOldTables();</span>
<span class="line-added">1507 </span>
1508     reapWeakHandles();
1509     pruneStaleEntriesFromWeakGCMaps();
1510     sweepArrayBuffers();
1511     snapshotUnswept();
<span class="line-modified">1512     finalizeUnconditionalFinalizers(); // We rely on these unconditional finalizers running before clearCurrentlyExecuting since CodeBlock&#39;s finalizer relies on querying currently executing.</span>
1513     removeDeadCompilerWorklistEntries();
1514     notifyIncrementalSweeper();
1515 
1516     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1517         [&amp;] (CodeBlock* codeBlock) {
1518             writeBarrier(codeBlock);
1519         });
1520     m_codeBlocks-&gt;clearCurrentlyExecuting();
1521 
1522     m_objectSpace.prepareForAllocation();
1523     updateAllocationLimits();
1524 
1525     if (UNLIKELY(m_verifier)) {
1526         m_verifier-&gt;trimDeadCells();
1527         m_verifier-&gt;verify(HeapVerifier::Phase::AfterGC);
1528     }
1529 
1530     didFinishCollection();
1531 
1532     if (m_currentRequest.didFinishEndPhase)
1533         m_currentRequest.didFinishEndPhase-&gt;run();
1534 
1535     if (false) {
1536         dataLog(&quot;Heap state after GC:\n&quot;);
1537         m_objectSpace.dumpBits();
1538     }
1539 
<span class="line-modified">1540     if (UNLIKELY(Options::logGC())) {</span>
1541         double thisPauseMS = (m_afterGC - m_stopTime).milliseconds();
1542         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;), cycle &quot;, (m_afterGC - m_beforeGC).milliseconds(), &quot;ms END]\n&quot;);
1543     }
1544 
1545     {
1546         auto locker = holdLock(*m_threadLock);
1547         m_requests.removeFirst();
1548         m_lastServedTicket++;
1549         clearMutatorWaiting();
1550     }
1551     ParkingLot::unparkAll(&amp;m_worldState);
1552 
<span class="line-modified">1553     dataLogLnIf(Options::logGC(), &quot;GC END!&quot;);</span>

1554 
1555     setNeedFinalize();
1556 
1557     m_lastGCStartTime = m_currentGCStartTime;
1558     m_lastGCEndTime = MonotonicTime::now();
1559     m_totalGCTime += m_lastGCEndTime - m_lastGCStartTime;
1560 
1561     return changePhase(conn, CollectorPhase::NotRunning);
1562 }
1563 
1564 bool Heap::changePhase(GCConductor conn, CollectorPhase nextPhase)
1565 {
1566     checkConn(conn);
1567 
1568     m_lastPhase = m_currentPhase;
1569     m_nextPhase = nextPhase;
1570 
1571     return finishChangingPhase(conn);
1572 }
1573 
</pre>
<hr />
<pre>
1634     m_worldIsStopped = true;
1635 
1636     forEachSlotVisitor(
1637         [&amp;] (SlotVisitor&amp; slotVisitor) {
1638             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1639         });
1640 
1641 #if ENABLE(JIT)
1642     if (VM::canUseJIT()) {
1643         DeferGCForAWhile awhile(*this);
1644         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm)
1645             &amp;&amp; conn == GCConductor::Collector)
1646             setGCDidJIT();
1647     }
1648 #endif // ENABLE(JIT)
1649     UNUSED_PARAM(conn);
1650 
1651     if (auto* shadowChicken = vm().shadowChicken())
1652         shadowChicken-&gt;update(vm(), vm().topCallFrame);
1653 

1654     m_objectSpace.stopAllocating();
1655 
1656     m_stopTime = MonotonicTime::now();
1657 }
1658 
1659 NEVER_INLINE void Heap::resumeThePeriphery()
1660 {
1661     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1662     // collection cycle or this is just a concurrent phase within a collection cycle:
1663     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1664     //   last active block.
1665     // - During collection cycle: it reinstates the last active block.
1666     m_objectSpace.resumeAllocating();
1667 
1668     m_barriersExecuted = 0;
1669 
1670     if (!m_worldIsStopped) {
1671         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
1672         RELEASE_ASSERT_NOT_REACHED();
1673     }
</pre>
<hr />
<pre>
2060 void Heap::setMutatorWaiting()
2061 {
2062     m_worldState.exchangeOr(mutatorWaitingBit);
2063 }
2064 
2065 void Heap::clearMutatorWaiting()
2066 {
2067     m_worldState.exchangeAnd(~mutatorWaitingBit);
2068 }
2069 
2070 void Heap::notifyThreadStopping(const AbstractLocker&amp;)
2071 {
2072     m_threadIsStopping = true;
2073     clearMutatorWaiting();
2074     ParkingLot::unparkAll(&amp;m_worldState);
2075 }
2076 
2077 void Heap::finalize()
2078 {
2079     MonotonicTime before;
<span class="line-modified">2080     if (UNLIKELY(Options::logGC())) {</span>
2081         before = MonotonicTime::now();
2082         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2083     }
2084 
2085     {
2086         SweepingScope sweepingScope(*this);
2087         deleteUnmarkedCompiledCode();
2088         deleteSourceProviderCaches();
2089         sweepInFinalize();
2090     }
2091 
2092     if (HasOwnPropertyCache* cache = vm().hasOwnPropertyCache())
2093         cache-&gt;clear();
2094 
2095     immutableButterflyToStringCache.clear();
2096 
2097     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
2098         callback.run(vm());
2099 
2100     if (shouldSweepSynchronously())
2101         sweepSynchronously();
2102 
<span class="line-modified">2103     if (UNLIKELY(Options::logGC())) {</span>
2104         MonotonicTime after = MonotonicTime::now();
2105         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2106     }
2107 }
2108 
2109 Heap::Ticket Heap::requestCollection(GCRequest request)
2110 {
2111     stopIfNecessary();
2112 
2113     ASSERT(vm().currentThreadIsHoldingAPILock());
2114     RELEASE_ASSERT(vm().atomStringTable() == Thread::current().atomStringTable());
2115 
2116     LockHolder locker(*m_threadLock);
2117     // We may be able to steal the conn. That only works if the collector is definitely not running
2118     // right now. This is an optimization that prevents the collector thread from ever starting in most
2119     // cases.
2120     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2121     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; !m_collectorThreadIsRunning) {
2122         if (false)
2123             dataLog(&quot;Taking the conn.\n&quot;);
2124         m_worldState.exchangeOr(mutatorHasConnBit);
2125     }
2126 
2127     m_requests.append(request);
2128     m_lastGrantedTicket++;
2129     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2130         m_threadCondition-&gt;notifyOne(locker);
2131     return m_lastGrantedTicket;
2132 }
2133 
2134 void Heap::waitForCollection(Ticket ticket)
2135 {
2136     waitForCollector(
2137         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2138             return m_lastServedTicket &gt;= ticket;
2139         });
2140 }
2141 
2142 void Heap::sweepInFinalize()
2143 {
<span class="line-modified">2144     m_objectSpace.sweepPreciseAllocations();</span>
<span class="line-modified">2145 #if ENABLE(WEBASSEMBLY)</span>
<span class="line-added">2146     // We hold onto a lot of memory, so it makes a lot of sense to be swept eagerly.</span>
<span class="line-added">2147     if (vm().m_webAssemblyMemorySpace)</span>
<span class="line-added">2148         vm().m_webAssemblyMemorySpace-&gt;sweep();</span>
<span class="line-added">2149 #endif</span>
2150 }
2151 
2152 void Heap::suspendCompilerThreads()
2153 {
2154 #if ENABLE(DFG_JIT)
2155     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2156     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2157     // the worklists use AutomaticThreads anyway.
2158     if (!VM::canUseJIT())
2159         return;
2160     for (unsigned i = DFG::numberOfWorklists(); i--;)
2161         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2162 #endif
2163 }
2164 
2165 void Heap::willStartCollection()
2166 {
<span class="line-modified">2167     dataLogIf(Options::logGC(), &quot;=&gt; &quot;);</span>

2168 
2169     if (shouldDoFullCollection()) {
2170         m_collectionScope = CollectionScope::Full;
2171         m_shouldDoFullCollection = false;
<span class="line-modified">2172         dataLogIf(Options::logGC(), &quot;FullCollection, &quot;);</span>



2173     } else {
2174         m_collectionScope = CollectionScope::Eden;
<span class="line-modified">2175         dataLogIf(Options::logGC(), &quot;EdenCollection, &quot;);</span>



2176     }
2177     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2178         m_sizeBeforeLastFullCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2179         m_extraMemorySize = 0;
2180         m_deprecatedExtraMemorySize = 0;
2181 #if ENABLE(RESOURCE_USAGE)
2182         m_externalMemorySize = 0;
2183 #endif
2184 
2185         if (m_fullActivityCallback)
2186             m_fullActivityCallback-&gt;willCollect();
2187     } else {
2188         ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Eden);
2189         m_sizeBeforeLastEdenCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2190     }
2191 
2192     if (m_edenActivityCallback)
2193         m_edenActivityCallback-&gt;willCollect();
2194 
2195     for (auto* observer : m_observers)
</pre>
<hr />
<pre>
2226 }
2227 
2228 void Heap::deleteSourceProviderCaches()
2229 {
2230     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
2231         m_vm.clearSourceProviderCaches();
2232 }
2233 
2234 void Heap::notifyIncrementalSweeper()
2235 {
2236     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2237         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2238             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2239     }
2240 
2241     m_sweeper-&gt;startSweeping(*this);
2242 }
2243 
2244 void Heap::updateAllocationLimits()
2245 {
<span class="line-modified">2246     static constexpr bool verbose = false;</span>
2247 
2248     if (verbose) {
2249         dataLog(&quot;\n&quot;);
2250         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2251     }
2252 
2253     // Calculate our current heap size threshold for the purpose of figuring out when we should
2254     // run another collection. This isn&#39;t the same as either size() or capacity(), though it should
2255     // be somewhere between the two. The key is to match the size calculations involved calls to
2256     // didAllocate(), while never dangerously underestimating capacity(). In extreme cases of
2257     // fragmentation, we may have size() much smaller than capacity().
2258     size_t currentHeapSize = 0;
2259 
2260     // For marked space, we use the total number of bytes visited. This matches the logic for
2261     // BlockDirectory&#39;s calls to didAllocate(), which effectively accounts for the total size of
2262     // objects allocated rather than blocks used. This will underestimate capacity(), and in case
2263     // of fragmentation, this may be substantial. Fortunately, marked space rarely fragments because
2264     // cells usually have a narrow range of sizes. So, the underestimation is probably OK.
2265     currentHeapSize += m_totalBytesVisited;
2266     if (verbose)
2267         dataLog(&quot;totalBytesVisited = &quot;, m_totalBytesVisited, &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2268 
2269     // It&#39;s up to the user to ensure that extraMemorySize() ends up corresponding to allocation-time
2270     // extra memory reporting.
2271     currentHeapSize += extraMemorySize();
<span class="line-modified">2272     if (ASSERT_ENABLED) {</span>
2273         Checked&lt;size_t, RecordOverflow&gt; checkedCurrentHeapSize = m_totalBytesVisited;
2274         checkedCurrentHeapSize += extraMemorySize();
2275         ASSERT(!checkedCurrentHeapSize.hasOverflowed() &amp;&amp; checkedCurrentHeapSize.unsafeGet() == currentHeapSize);
2276     }
2277 
2278     if (verbose)
2279         dataLog(&quot;extraMemorySize() = &quot;, extraMemorySize(), &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2280 
2281     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2282         // To avoid pathological GC churn in very small and very large heaps, we set
2283         // the new allocation limit based on the current size of the heap, with a
2284         // fixed minimum.
2285         m_maxHeapSize = std::max(minHeapSize(m_heapType, m_ramSize), proportionalHeapSize(currentHeapSize, m_ramSize));
2286         if (verbose)
2287             dataLog(&quot;Full: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2288         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2289         if (verbose)
2290             dataLog(&quot;Full: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2291         m_sizeAfterLastFullCollect = currentHeapSize;
2292         if (verbose)
</pre>
<hr />
<pre>
2304         m_sizeAfterLastEdenCollect = currentHeapSize;
2305         if (verbose)
2306             dataLog(&quot;Eden: sizeAfterLastEdenCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2307         double edenToOldGenerationRatio = (double)m_maxEdenSize / (double)m_maxHeapSize;
2308         double minEdenToOldGenerationRatio = 1.0 / 3.0;
2309         if (edenToOldGenerationRatio &lt; minEdenToOldGenerationRatio)
2310             m_shouldDoFullCollection = true;
2311         // This seems suspect at first, but what it does is ensure that the nursery size is fixed.
2312         m_maxHeapSize += currentHeapSize - m_sizeAfterLastCollect;
2313         if (verbose)
2314             dataLog(&quot;Eden: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2315         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2316         if (verbose)
2317             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2318         if (m_fullActivityCallback) {
2319             ASSERT(currentHeapSize &gt;= m_sizeAfterLastFullCollect);
2320             m_fullActivityCallback-&gt;didAllocate(*this, currentHeapSize - m_sizeAfterLastFullCollect);
2321         }
2322     }
2323 
<span class="line-modified">2324 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)</span>
2325     // Get critical memory threshold for next cycle.
2326     overCriticalMemoryThreshold(MemoryThresholdCallType::Direct);
2327 #endif
2328 
2329     m_sizeAfterLastCollect = currentHeapSize;
2330     if (verbose)
2331         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2332     m_bytesAllocatedThisCycle = 0;
2333 
<span class="line-modified">2334     dataLogIf(Options::logGC(), &quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);</span>

2335 }
2336 
2337 void Heap::didFinishCollection()
2338 {
2339     m_afterGC = MonotonicTime::now();
2340     CollectionScope scope = *m_collectionScope;
2341     if (scope == CollectionScope::Full)
2342         m_lastFullGCLength = m_afterGC - m_beforeGC;
2343     else
2344         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2345 
2346 #if ENABLE(RESOURCE_USAGE)
2347     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2348 #endif
2349 
2350     if (HeapProfiler* heapProfiler = m_vm.heapProfiler()) {
2351         gatherExtraHeapData(*heapProfiler);
2352         removeDeadHeapSnapshotNodes(*heapProfiler);
2353     }
2354 
</pre>
<hr />
<pre>
2398 
2399 void Heap::didAllocate(size_t bytes)
2400 {
2401     if (m_edenActivityCallback)
2402         m_edenActivityCallback-&gt;didAllocate(*this, m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
2403     m_bytesAllocatedThisCycle += bytes;
2404     performIncrement(bytes);
2405 }
2406 
2407 bool Heap::isValidAllocation(size_t)
2408 {
2409     if (!isValidThreadState(m_vm))
2410         return false;
2411 
2412     if (isCurrentThreadBusy())
2413         return false;
2414 
2415     return true;
2416 }
2417 
<span class="line-modified">2418 void Heap::addFinalizer(JSCell* cell, CFinalizer finalizer)</span>
2419 {
<span class="line-modified">2420     WeakSet::allocate(cell, &amp;m_cFinalizerOwner, bitwise_cast&lt;void*&gt;(finalizer)); // Balanced by CFinalizerOwner::finalize().</span>
<span class="line-added">2421 }</span>
<span class="line-added">2422 </span>
<span class="line-added">2423 void Heap::addFinalizer(JSCell* cell, LambdaFinalizer function)</span>
<span class="line-added">2424 {</span>
<span class="line-added">2425     WeakSet::allocate(cell, &amp;m_lambdaFinalizerOwner, function.leakImpl()); // Balanced by LambdaFinalizerOwner::finalize().</span>
<span class="line-added">2426 }</span>
<span class="line-added">2427 </span>
<span class="line-added">2428 void Heap::CFinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)</span>
<span class="line-added">2429 {</span>
<span class="line-added">2430     HandleSlot slot = handle.slot();</span>
<span class="line-added">2431     CFinalizer finalizer = bitwise_cast&lt;CFinalizer&gt;(context);</span>
<span class="line-added">2432     finalizer(slot-&gt;asCell());</span>
<span class="line-added">2433     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));</span>
2434 }
2435 
<span class="line-modified">2436 void Heap::LambdaFinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)</span>
2437 {
<span class="line-added">2438     LambdaFinalizer::Impl* impl = bitwise_cast&lt;LambdaFinalizer::Impl*&gt;(context);</span>
<span class="line-added">2439     LambdaFinalizer finalizer(impl);</span>
2440     HandleSlot slot = handle.slot();

2441     finalizer(slot-&gt;asCell());
2442     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
2443 }
2444 
2445 void Heap::collectNowFullIfNotDoneRecently(Synchronousness synchronousness)
2446 {
2447     if (!m_fullActivityCallback) {
2448         collectNow(synchronousness, CollectionScope::Full);
2449         return;
2450     }
2451 
2452     if (m_fullActivityCallback-&gt;didGCRecently()) {
2453         // A synchronous GC was already requested recently so we merely accelerate next collection.
2454         reportAbandonedObjectGraph();
2455         return;
2456     }
2457 
2458     m_fullActivityCallback-&gt;setDidGCRecently();
2459     collectNow(synchronousness, CollectionScope::Full);
2460 }
</pre>
<hr />
<pre>
2614         return;
2615     }
2616     if (!Options::useGC())
2617         return;
2618 
2619     if (mayNeedToStop()) {
2620         if (deferralContext)
2621             deferralContext-&gt;m_shouldGC = true;
2622         else if (isDeferred())
2623             m_didDeferGCWork = true;
2624         else
2625             stopIfNecessary();
2626     }
2627 
2628     if (UNLIKELY(Options::gcMaxHeapSize())) {
2629         if (m_bytesAllocatedThisCycle &lt;= Options::gcMaxHeapSize())
2630             return;
2631     } else {
2632         size_t bytesAllowedThisCycle = m_maxEdenSize;
2633 
<span class="line-modified">2634 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)</span>
2635         if (overCriticalMemoryThreshold())
2636             bytesAllowedThisCycle = std::min(m_maxEdenSizeWhenCritical, bytesAllowedThisCycle);
2637 #endif
2638 
2639         if (m_bytesAllocatedThisCycle &lt;= bytesAllowedThisCycle)
2640             return;
2641     }
2642 
2643     if (deferralContext)
2644         deferralContext-&gt;m_shouldGC = true;
2645     else if (isDeferred())
2646         m_didDeferGCWork = true;
2647     else {
2648         collectAsync();
2649         stopIfNecessary(); // This will immediately start the collection if we have the conn.
2650     }
2651 }
2652 
2653 void Heap::decrementDeferralDepthAndGCIfNeededSlow()
2654 {
</pre>
<hr />
<pre>
2755                 slotVisitor.appendUnbarriered(m_vm.lastException());
2756             }
2757         },
2758         ConstraintVolatility::GreyedByExecution);
2759 
2760     m_constraintSet-&gt;add(
2761         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2762         [this] (SlotVisitor&amp; slotVisitor) {
2763             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2764             m_handleSet.visitStrongHandles(slotVisitor);
2765         },
2766         ConstraintVolatility::GreyedByExecution);
2767 
2768     m_constraintSet-&gt;add(
2769         &quot;D&quot;, &quot;Debugger&quot;,
2770         [this] (SlotVisitor&amp; slotVisitor) {
2771             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2772 
2773 #if ENABLE(SAMPLING_PROFILER)
2774             if (SamplingProfiler* samplingProfiler = m_vm.samplingProfiler()) {
<span class="line-modified">2775                 auto locker = holdLock(samplingProfiler-&gt;getLock());</span>
<span class="line-modified">2776                 samplingProfiler-&gt;processUnverifiedStackTraces(locker);</span>
2777                 samplingProfiler-&gt;visit(slotVisitor);
2778                 if (Options::logGC() == GCLogging::Verbose)
2779                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2780             }
2781 #endif // ENABLE(SAMPLING_PROFILER)
2782 
2783             if (m_vm.typeProfiler())
2784                 m_vm.typeProfilerLog()-&gt;visit(slotVisitor);
2785 
2786             if (auto* shadowChicken = m_vm.shadowChicken())
2787                 shadowChicken-&gt;visitChildren(slotVisitor);
2788         },
2789         ConstraintVolatility::GreyedByExecution);
2790 
2791     m_constraintSet-&gt;add(
2792         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2793         [this] (SlotVisitor&amp; slotVisitor) {
2794             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2795             m_objectSpace.visitWeakSets(slotVisitor);
2796         },
</pre>
<hr />
<pre>
2853                     // Visit the CodeBlock as a constraint only if it&#39;s black.
2854                     if (isMarked(codeBlock)
2855                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2856                         slotVisitor.visitAsConstraint(codeBlock);
2857                 });
2858         },
2859         ConstraintVolatility::SeldomGreyed);
2860 
2861     m_constraintSet-&gt;add(makeUnique&lt;MarkStackMergingConstraint&gt;(*this));
2862 }
2863 
2864 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2865 {
2866     PreventCollectionScope preventCollectionScope(*this);
2867     m_constraintSet-&gt;add(WTFMove(constraint));
2868 }
2869 
2870 void Heap::notifyIsSafeToCollect()
2871 {
2872     MonotonicTime before;
<span class="line-modified">2873     if (UNLIKELY(Options::logGC())) {</span>
2874         before = MonotonicTime::now();
2875         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2876     }
2877 
2878     addCoreConstraints();
2879 
2880     m_isSafeToCollect = true;
2881 
2882     if (Options::collectContinuously()) {
2883         m_collectContinuouslyThread = Thread::create(
2884             &quot;JSC DEBUG Continuous GC&quot;,
2885             [this] () {
2886                 MonotonicTime initialTime = MonotonicTime::now();
2887                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2888                 while (!m_shouldStopCollectingContinuously) {
2889                     {
2890                         LockHolder locker(*m_threadLock);
2891                         if (m_requests.isEmpty()) {
2892                             m_requests.append(WTF::nullopt);
2893                             m_lastGrantedTicket++;
2894                             m_threadCondition-&gt;notifyOne(locker);
2895                         }
2896                     }
2897 
2898                     {
2899                         LockHolder locker(m_collectContinuouslyLock);
2900                         Seconds elapsed = MonotonicTime::now() - initialTime;
2901                         Seconds elapsedInPeriod = elapsed % period;
2902                         MonotonicTime timeToWakeUp =
2903                             initialTime + elapsed - elapsedInPeriod + period;
2904                         while (!hasElapsed(timeToWakeUp) &amp;&amp; !m_shouldStopCollectingContinuously) {
2905                             m_collectContinuouslyCondition.waitUntil(
2906                                 m_collectContinuouslyLock, timeToWakeUp);
2907                         }
2908                     }
2909                 }
2910             });
2911     }
2912 
<span class="line-modified">2913     dataLogIf(Options::logGC(), (MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);</span>

2914 }
2915 
2916 void Heap::preventCollection()
2917 {
2918     if (!m_isSafeToCollect)
2919         return;
2920 
2921     // This prevents the collectContinuously thread from starting a collection.
2922     m_collectContinuouslyLock.lock();
2923 
2924     // Wait for all collections to finish.
2925     waitForCollector(
2926         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2927             ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2928             return m_lastServedTicket == m_lastGrantedTicket;
2929         });
2930 
2931     // Now a collection can only start if this thread starts it.
2932     RELEASE_ASSERT(!m_collectionScope);
2933 }
</pre>
</td>
</tr>
</table>
<center><a href="HandleSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>