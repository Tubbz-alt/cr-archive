<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AirAllocateRegistersAndStackAndGenerateCode.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../index.html" target="_top">index</a> <a href="AirAllocateStackByGraphColoring.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersByGraphColoring.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2015-2017 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
 145             if (!isPrecolored(b)) {
 146                 ASSERT(!m_adjacencyList[b].contains(a));
 147                 m_adjacencyList[b].append(a);
 148             }
 149             return true;
 150         }
 151         return false;
 152     }
 153 
 154     template&lt;typename Function&gt;
 155     void forEachAdjacent(IndexType tmpIndex, Function function)
 156     {
 157         for (IndexType adjacentTmpIndex : m_adjacencyList[tmpIndex]) {
 158             if (!hasBeenSimplified(adjacentTmpIndex))
 159                 function(adjacentTmpIndex);
 160         }
 161     }
 162 
 163     bool hasBeenSimplified(IndexType tmpIndex)
 164     {
<span class="line-modified"> 165         if (!ASSERT_DISABLED) {</span>
 166             if (!!m_coalescedTmps[tmpIndex])
 167                 ASSERT(getAlias(tmpIndex) != tmpIndex);
 168         }
 169 
 170         return m_isOnSelectStack.quickGet(tmpIndex) || !!m_coalescedTmps[tmpIndex];
 171     }
 172 
 173     bool canBeSafelyCoalesced(IndexType u, IndexType v)
 174     {
 175         ASSERT(!isPrecolored(v));
 176         if (isPrecolored(u))
 177             return precoloredCoalescingHeuristic(u, v);
 178         return conservativeHeuristic(u, v);
 179     }
 180 
 181     bool conservativeHeuristic(IndexType u, IndexType v)
 182     {
 183         // This is using the Briggs&#39; conservative coalescing rule:
 184         // If the number of combined adjacent node with a degree &gt;= K is less than K,
 185         // it is safe to combine the two nodes. The reason is that we know that if the graph
 186         // is colorable, we have fewer than K adjacents with high order and there is a color
 187         // for the current node.
 188         ASSERT(u != v);
 189         ASSERT(!isPrecolored(u));
 190         ASSERT(!isPrecolored(v));
 191 
 192         const auto&amp; adjacentsOfU = m_adjacencyList[u];
 193         const auto&amp; adjacentsOfV = m_adjacencyList[v];
 194 
<span class="line-modified"> 195         if (adjacentsOfU.size() + adjacentsOfV.size() &lt; registerCount()) {</span>



 196             // Shortcut: if the total number of adjacents is less than the number of register, the condition is always met.
 197             return true;
 198         }
 199 
<span class="line-removed"> 200         HashSet&lt;IndexType&gt; highOrderAdjacents;</span>
<span class="line-removed"> 201 </span>
 202         for (IndexType adjacentTmpIndex : adjacentsOfU) {
 203             ASSERT(adjacentTmpIndex != v);
 204             ASSERT(adjacentTmpIndex != u);

 205             if (!hasBeenSimplified(adjacentTmpIndex) &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()) {
<span class="line-modified"> 206                 auto addResult = highOrderAdjacents.add(adjacentTmpIndex);</span>
<span class="line-modified"> 207                 if (addResult.isNewEntry &amp;&amp; highOrderAdjacents.size() &gt;= registerCount())</span>

 208                     return false;
<span class="line-modified"> 209             }</span>

 210         }



 211         for (IndexType adjacentTmpIndex : adjacentsOfV) {
 212             ASSERT(adjacentTmpIndex != u);
 213             ASSERT(adjacentTmpIndex != v);
<span class="line-modified"> 214             if (!hasBeenSimplified(adjacentTmpIndex) &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()) {</span>
<span class="line-modified"> 215                 auto addResult = highOrderAdjacents.add(adjacentTmpIndex);</span>
<span class="line-modified"> 216                 if (addResult.isNewEntry &amp;&amp; highOrderAdjacents.size() &gt;= registerCount())</span>




 217                     return false;
<span class="line-modified"> 218             }</span>

 219         }
 220 

 221         ASSERT(highOrderAdjacents.size() &lt; registerCount());
 222         return true;
 223     }
 224 
 225     bool precoloredCoalescingHeuristic(IndexType u, IndexType v)
 226     {
 227         if (traceDebug)
 228             dataLog(&quot;    Checking precoloredCoalescingHeuristic\n&quot;);
 229         ASSERT(isPrecolored(u));
 230         ASSERT(!isPrecolored(v));
 231 
 232         // If any adjacent of the non-colored node is not an adjacent of the colored node AND has a degree &gt;= K
 233         // there is a risk that this node needs to have the same color as our precolored node. If we coalesce such
 234         // move, we may create an uncolorable graph.
 235         const auto&amp; adjacentsOfV = m_adjacencyList[v];
 236         for (unsigned adjacentTmpIndex : adjacentsOfV) {
 237             if (!isPrecolored(adjacentTmpIndex)
 238                 &amp;&amp; !hasBeenSimplified(adjacentTmpIndex)
 239                 &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()
 240                 &amp;&amp; !hasInterferenceEdge(InterferenceEdge(u, adjacentTmpIndex)))
</pre>
<hr />
<pre>
 486             out.print(first(), &quot;&lt;=&gt;&quot;, second());
 487         }
 488 
 489     private:
 490         uint64_t m_value { 0 };
 491     };
 492 
 493     bool addInterferenceEdge(InterferenceEdge edge)
 494     {
 495         return m_interferenceEdges.add(edge).isNewEntry;
 496     }
 497 
 498     bool hasInterferenceEdge(InterferenceEdge edge)
 499     {
 500         return m_interferenceEdges.contains(edge);
 501     }
 502 
 503     struct InterferenceEdgeHash {
 504         static unsigned hash(const InterferenceEdge&amp; key) { return key.hash(); }
 505         static bool equal(const InterferenceEdge&amp; a, const InterferenceEdge&amp; b) { return a == b; }
<span class="line-modified"> 506         static const bool safeToCompareToEmptyOrDeleted = true;</span>
 507     };
 508     typedef SimpleClassHashTraits&lt;InterferenceEdge&gt; InterferenceEdgeHashTraits;
 509 
 510     Vector&lt;Reg&gt; m_regsInPriorityOrder;
 511     IndexType m_lastPrecoloredRegisterIndex { 0 };
 512 
 513     // The interference graph.
 514     HashSet&lt;InterferenceEdge, InterferenceEdgeHash, InterferenceEdgeHashTraits&gt; m_interferenceEdges;
 515 
 516     Vector&lt;Vector&lt;IndexType, 0, UnsafeVectorOverflow, 4&gt;, 0, UnsafeVectorOverflow&gt; m_adjacencyList;
 517     Vector&lt;IndexType, 0, UnsafeVectorOverflow&gt; m_degrees;
 518 
 519     using IndexTypeSet = HashSet&lt;IndexType, typename DefaultHash&lt;IndexType&gt;::Hash, WTF::UnsignedWithZeroKeyHashTraits&lt;IndexType&gt;&gt;;
 520 
 521     HashMap&lt;IndexType, IndexTypeSet, typename DefaultHash&lt;IndexType&gt;::Hash, WTF::UnsignedWithZeroKeyHashTraits&lt;IndexType&gt;&gt; m_biases;
 522 
 523     // Instead of keeping track of the move instructions, we just keep their operands around and use the index
 524     // in the vector as the &quot;identifier&quot; for the move.
 525     struct MoveOperands {
 526         IndexType srcIndex;
</pre>
<hr />
<pre>
 618         };
 619 
 620         // We first coalesce until we can&#39;t coalesce any more.
 621         do {
 622             changed = false;
 623             m_worklistMoves.forEachMove(coalesceMove);
 624         } while (changed);
 625         do {
 626             changed = false;
 627             m_worklistMoves.forEachLowPriorityMove(coalesceMove);
 628         } while (changed);
 629 
 630         // Then we create our select stack. The invariant we start with here is that nodes in
 631         // the interference graph with degree &gt;= k are on the spill list. Nodes with degree &lt; k
 632         // are on the simplify worklist. A node can move from the spill list to the simplify
 633         // list (but not the other way around, note that this is different than IRC because IRC
 634         // runs this while coalescing, but we do all our coalescing before this). Once a node is
 635         // added to the select stack, it&#39;s not on either list, but only on the select stack.
 636         // Once on the select stack, logically, it&#39;s no longer in the interference graph.
 637         auto assertInvariants = [&amp;] () {
<span class="line-modified"> 638             if (ASSERT_DISABLED)</span>
 639                 return;
 640             if (!shouldValidateIRAtEachPhase())
 641                 return;
 642 
 643             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 644             unsigned registerCount = this-&gt;registerCount();
 645             for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
 646                 if (getAlias(i) != i)
 647                     continue;
 648                 if (m_isOnSelectStack.contains(i)) {
 649                     ASSERT(!m_simplifyWorklist.contains(i) &amp;&amp; !m_spillWorklist.contains(i));
 650                     continue;
 651                 }
 652                 unsigned degree = m_degrees[i];
 653                 if (degree &gt;= registerCount) {
 654                     ASSERT(m_unspillableTmps.contains(i) || m_spillWorklist.contains(i));
 655                     ASSERT(!m_simplifyWorklist.contains(i));
 656                     continue;
 657                 }
 658                 ASSERT(m_simplifyWorklist.contains(i));
 659             }
 660         };
 661 
 662         makeInitialWorklist();
 663         assertInvariants();
 664         do {
 665             changed = false;
 666 
 667             while (m_simplifyWorklist.size()) {
 668                 simplify();
 669                 assertInvariants();
 670             }
 671 
 672             if (!m_spillWorklist.isEmpty()) {
 673                 selectSpill();
 674                 changed = true;
 675                 ASSERT(m_simplifyWorklist.size() == 1);
 676             }
 677         } while (changed);
 678 
<span class="line-modified"> 679         if (!ASSERT_DISABLED) {</span>
 680             ASSERT(!m_simplifyWorklist.size());
 681             ASSERT(m_spillWorklist.isEmpty());
 682             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 683             for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i)
 684                 ASSERT(hasBeenSimplified(i));
 685         }
 686 
 687         assignColors();
 688     }
 689 
 690 protected:
 691 
 692     bool coalesce(unsigned&amp; moveIndex)
 693     {
 694         const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
 695         IndexType u = getAlias(moveOperands.srcIndex);
 696         IndexType v = getAlias(moveOperands.dstIndex);
 697 
 698         if (isPrecolored(v))
 699             std::swap(u, v);
 700 
 701         if (traceDebug)
 702             dataLog(&quot;Coalescing move at index &quot;, moveIndex, &quot; u = &quot;, TmpMapper::tmpFromAbsoluteIndex(u), &quot; v = &quot;, TmpMapper::tmpFromAbsoluteIndex(v), &quot;    &quot;);
 703 
 704         if (u == v) {
 705             if (traceDebug)
 706                 dataLog(&quot;Already Coalesced. They&#39;re equal.\n&quot;);
 707             return false;
 708         }
 709 
 710         if (isPrecolored(v)
 711             || hasInterferenceEdge(InterferenceEdge(u, v))) {
 712 
 713             // No need to ever consider this move again if it interferes.
 714             // No coalescing will remove the interference.
 715             moveIndex = UINT_MAX;
 716 
<span class="line-modified"> 717             if (!ASSERT_DISABLED) {</span>
 718                 if (isPrecolored(v))
 719                     ASSERT(isPrecolored(u));
 720             }
 721 
 722             if (traceDebug)
 723                 dataLog(&quot;Constrained\n&quot;);
 724 
 725             return false;
 726         }
 727 
 728         if (canBeSafelyCoalesced(u, v)) {
 729             combine(u, v);
 730             m_hasCoalescedNonTrivialMove = true;
 731 
 732             if (traceDebug)
 733                 dataLog(&quot;    Safe Coalescing\n&quot;);
 734             return true;
 735         }
 736 
 737         addBias(u, v);
</pre>
<hr />
<pre>
 880         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_moveList;
 881         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_lowPriorityMoveList;
 882     };
 883 
 884     void decrementDegree(IndexType tmpIndex)
 885     {
 886         ASSERT(m_degrees[tmpIndex]);
 887         --m_degrees[tmpIndex];
 888     }
 889 
 890     void decrementDegreeInSimplification(IndexType tmpIndex)
 891     {
 892         ASSERT(m_degrees[tmpIndex]);
 893         unsigned oldDegree = m_degrees[tmpIndex]--;
 894 
 895         if (oldDegree == registerCount()) {
 896             ASSERT(m_degrees[tmpIndex] &lt; registerCount());
 897             if (traceDebug)
 898                 dataLogLn(&quot;Moving tmp &quot;, tmpIndex, &quot; from spill list to simplify list because it&#39;s degree is now less than k&quot;);
 899 
<span class="line-modified"> 900             if (!ASSERT_DISABLED)</span>
 901                 ASSERT(m_unspillableTmps.contains(tmpIndex) || m_spillWorklist.contains(tmpIndex));
 902             m_spillWorklist.quickClear(tmpIndex);
 903 
 904             ASSERT(!m_simplifyWorklist.contains(tmpIndex));
 905             m_simplifyWorklist.append(tmpIndex);
 906         }
 907     }
 908 
 909     void assignColors()
 910     {
 911         m_worklistMoves.clear();
 912         Base::assignColors();
 913     }
 914 
 915     // Set of &quot;move&quot; enabled for possible coalescing.
 916     MoveSet m_worklistMoves;
 917 };
 918 
 919 template &lt;typename IndexType, typename TmpMapper&gt;
 920 class IRC : public AbstractColoringAllocator&lt;IndexType, TmpMapper&gt; {
</pre>
<hr />
<pre>
1004         assignColors();
1005     }
1006 
1007 protected:
1008 
1009     void makeWorkList()
1010     {
1011         IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
1012         for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
1013             unsigned degree = m_degrees[i];
1014             if (degree &gt;= registerCount())
1015                 addToSpill(i);
1016             else if (!m_moveList[i].isEmpty())
1017                 m_freezeWorklist.add(i);
1018             else
1019                 m_simplifyWorklist.append(i);
1020         }
1021     }
1022 
1023     // Low-degree vertex can always be colored: just pick any of the color taken by any
<span class="line-modified">1024     // other adjacent verices.</span>
1025     // The &quot;Simplify&quot; phase takes a low-degree out of the interference graph to simplify it.
1026     void simplify()
1027     {
1028         IndexType lastIndex = m_simplifyWorklist.takeLast();
1029 
1030         ASSERT(!m_selectStack.contains(lastIndex));
1031         ASSERT(!m_isOnSelectStack.get(lastIndex));
1032         m_selectStack.append(lastIndex);
1033         m_isOnSelectStack.quickSet(lastIndex);
1034 
1035         forEachAdjacent(lastIndex, [this](IndexType adjacentTmpIndex) {
1036             decrementDegree(adjacentTmpIndex);
1037         });
1038     }
1039 
1040     void coalesce()
1041     {
1042         unsigned moveIndex = m_worklistMoves.takeLastMove();
1043         const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
1044         IndexType u = getAlias(moveOperands.srcIndex);
</pre>
<hr />
<pre>
1772 
1773         return true;
1774     }
1775 
1776     TmpWidth&amp; m_tmpWidth;
1777 };
1778 
1779 class GraphColoringRegisterAllocation {
1780 public:
1781     GraphColoringRegisterAllocation(Code&amp; code, UseCounts&lt;Tmp&gt;&amp; useCounts)
1782         : m_code(code)
1783         , m_useCounts(useCounts)
1784     {
1785     }
1786 
1787     void run()
1788     {
1789         padInterference(m_code);
1790 
1791         allocateOnBank&lt;GP&gt;();
<span class="line-removed">1792         m_numIterations = 0;</span>
1793         allocateOnBank&lt;FP&gt;();
1794 
1795         fixSpillsAfterTerminals(m_code);
<span class="line-removed">1796 </span>
<span class="line-removed">1797         if (reportStats)</span>
<span class="line-removed">1798             dataLog(&quot;Num iterations = &quot;, m_numIterations, &quot;\n&quot;);</span>
1799     }
1800 
1801 private:
1802     template&lt;Bank bank&gt;
1803     void allocateOnBank()
1804     {
1805         HashSet&lt;unsigned&gt; unspillableTmps = computeUnspillableTmps&lt;bank&gt;();
1806 
1807         // FIXME: If a Tmp is used only from a Scratch role and that argument is !admitsStack, then
1808         // we should add the Tmp to unspillableTmps. That will help avoid relooping only to turn the
1809         // Tmp into an unspillable Tmp.
1810         // https://bugs.webkit.org/show_bug.cgi?id=152699
1811 
<span class="line-modified">1812         while (true) {</span>
<span class="line-modified">1813             ++m_numIterations;</span>



1814 
1815             if (traceDebug)
<span class="line-modified">1816                 dataLog(&quot;Code at iteration &quot;, m_numIterations, &quot;:\n&quot;, m_code);</span>
1817 
1818             // FIXME: One way to optimize this code is to remove the recomputation inside the fixpoint.
1819             // We need to recompute because spilling adds tmps, but we could just update tmpWidth when we
1820             // add those tmps. Note that one easy way to remove the recomputation is to make any newly
1821             // added Tmps get the same use/def widths that the original Tmp got. But, this may hurt the
1822             // spill code we emit. Since we currently recompute TmpWidth after spilling, the newly
1823             // created Tmps may get narrower use/def widths. On the other hand, the spiller already
1824             // selects which move instruction to use based on the original Tmp&#39;s widths, so it may not
<span class="line-modified">1825             // matter than a subsequent iteration sees a coservative width for the new Tmps. Also, the</span>
1826             // recomputation may not actually be a performance problem; it&#39;s likely that a better way to
1827             // improve performance of TmpWidth is to replace its HashMap with something else. It&#39;s
1828             // possible that most of the TmpWidth overhead is from queries of TmpWidth rather than the
1829             // recomputation, in which case speeding up the lookup would be a bigger win.
1830             // https://bugs.webkit.org/show_bug.cgi?id=152478
1831             m_tmpWidth.recompute(m_code);
1832 
1833             auto doAllocation = [&amp;] (auto&amp; allocator) -&gt; bool {
1834                 allocator.allocate();
1835                 if (!allocator.requiresSpilling()) {
1836                     this-&gt;assignRegistersToTmp&lt;bank&gt;(allocator);
1837                     if (traceDebug)
<span class="line-modified">1838                         dataLog(&quot;Successfull allocation at iteration &quot;, m_numIterations, &quot;:\n&quot;, m_code);</span>
1839 
1840                     return true;
1841                 }
1842 
1843                 this-&gt;addSpillAndFill&lt;bank&gt;(allocator, unspillableTmps);
1844                 return false;
1845             };
1846 
<span class="line-removed">1847             bool done;</span>
1848             if (useIRC()) {
1849                 ColoringAllocator&lt;bank, IRC&gt; allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
1850                 done = doAllocation(allocator);
1851             } else {
1852                 ColoringAllocator&lt;bank, Briggs&gt; allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
1853                 done = doAllocation(allocator);
1854             }
<span class="line-removed">1855             if (done)</span>
<span class="line-removed">1856                 return;</span>
1857         }

1858     }
1859 
1860     template&lt;Bank bank&gt;
1861     HashSet&lt;unsigned&gt; computeUnspillableTmps()
1862     {
1863 
1864         HashSet&lt;unsigned&gt; unspillableTmps;
1865 
1866         struct Range {
1867             unsigned first { std::numeric_limits&lt;unsigned&gt;::max() };
1868             unsigned last { 0 };
1869             unsigned count { 0 };
1870             unsigned admitStackCount { 0 };
1871         };
1872 
1873         unsigned numTmps = m_code.numTmps(bank);
1874         unsigned arraySize = AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(numTmps);
1875 
1876         Vector&lt;Range, 0, UnsafeVectorOverflow&gt; ranges;
1877         ranges.fill(Range(), arraySize);
</pre>
<hr />
<pre>
2170                     Arg arg = Arg::stack(stackSlotEntry-&gt;value);
2171                     if (Arg::isAnyUse(role))
2172                         insertionSet.insert(instIndex, move, inst.origin, arg, tmp);
2173                     if (Arg::isAnyDef(role))
2174                         insertionSet.insert(instIndex + 1, move, inst.origin, tmp, arg);
2175                 });
2176             }
2177             insertionSet.execute(block);
2178 
2179             if (hasAliasedTmps) {
2180                 block-&gt;insts().removeAllMatching([&amp;] (const Inst&amp; inst) {
2181                     return allocator.isUselessMove(inst);
2182                 });
2183             }
2184         }
2185     }
2186 
2187     Code&amp; m_code;
2188     TmpWidth m_tmpWidth;
2189     UseCounts&lt;Tmp&gt;&amp; m_useCounts;
<span class="line-removed">2190     unsigned m_numIterations { 0 };</span>
2191 };
2192 
2193 } // anonymous namespace
2194 
2195 void allocateRegistersByGraphColoring(Code&amp; code)
2196 {
2197     PhaseScope phaseScope(code, &quot;allocateRegistersByGraphColoring&quot;);
2198 
2199     if (false)
2200         dataLog(&quot;Code before graph coloring:\n&quot;, code);
2201 
2202     UseCounts&lt;Tmp&gt; useCounts(code);
2203     GraphColoringRegisterAllocation graphColoringRegisterAllocation(code, useCounts);
2204     graphColoringRegisterAllocation.run();
2205 }
2206 
2207 } } } // namespace JSC::B3::Air
2208 
2209 #endif // ENABLE(B3_JIT)
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2015-2019 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
 145             if (!isPrecolored(b)) {
 146                 ASSERT(!m_adjacencyList[b].contains(a));
 147                 m_adjacencyList[b].append(a);
 148             }
 149             return true;
 150         }
 151         return false;
 152     }
 153 
 154     template&lt;typename Function&gt;
 155     void forEachAdjacent(IndexType tmpIndex, Function function)
 156     {
 157         for (IndexType adjacentTmpIndex : m_adjacencyList[tmpIndex]) {
 158             if (!hasBeenSimplified(adjacentTmpIndex))
 159                 function(adjacentTmpIndex);
 160         }
 161     }
 162 
 163     bool hasBeenSimplified(IndexType tmpIndex)
 164     {
<span class="line-modified"> 165         if (ASSERT_ENABLED) {</span>
 166             if (!!m_coalescedTmps[tmpIndex])
 167                 ASSERT(getAlias(tmpIndex) != tmpIndex);
 168         }
 169 
 170         return m_isOnSelectStack.quickGet(tmpIndex) || !!m_coalescedTmps[tmpIndex];
 171     }
 172 
 173     bool canBeSafelyCoalesced(IndexType u, IndexType v)
 174     {
 175         ASSERT(!isPrecolored(v));
 176         if (isPrecolored(u))
 177             return precoloredCoalescingHeuristic(u, v);
 178         return conservativeHeuristic(u, v);
 179     }
 180 
 181     bool conservativeHeuristic(IndexType u, IndexType v)
 182     {
 183         // This is using the Briggs&#39; conservative coalescing rule:
 184         // If the number of combined adjacent node with a degree &gt;= K is less than K,
 185         // it is safe to combine the two nodes. The reason is that we know that if the graph
 186         // is colorable, we have fewer than K adjacents with high order and there is a color
 187         // for the current node.
 188         ASSERT(u != v);
 189         ASSERT(!isPrecolored(u));
 190         ASSERT(!isPrecolored(v));
 191 
 192         const auto&amp; adjacentsOfU = m_adjacencyList[u];
 193         const auto&amp; adjacentsOfV = m_adjacencyList[v];
 194 
<span class="line-modified"> 195         Vector&lt;IndexType, MacroAssembler::numGPRs + MacroAssembler::numFPRs&gt; highOrderAdjacents;</span>
<span class="line-added"> 196         RELEASE_ASSERT(registerCount() &lt;= MacroAssembler::numGPRs + MacroAssembler::numFPRs);</span>
<span class="line-added"> 197         unsigned numCandidates = adjacentsOfU.size() + adjacentsOfV.size();</span>
<span class="line-added"> 198         if (numCandidates &lt; registerCount()) {</span>
 199             // Shortcut: if the total number of adjacents is less than the number of register, the condition is always met.
 200             return true;
 201         }
 202 


 203         for (IndexType adjacentTmpIndex : adjacentsOfU) {
 204             ASSERT(adjacentTmpIndex != v);
 205             ASSERT(adjacentTmpIndex != u);
<span class="line-added"> 206             numCandidates--;</span>
 207             if (!hasBeenSimplified(adjacentTmpIndex) &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()) {
<span class="line-modified"> 208                 ASSERT(std::find(highOrderAdjacents.begin(), highOrderAdjacents.end(), adjacentTmpIndex) == highOrderAdjacents.end());</span>
<span class="line-modified"> 209                 highOrderAdjacents.uncheckedAppend(adjacentTmpIndex);</span>
<span class="line-added"> 210                 if (highOrderAdjacents.size() &gt;= registerCount())</span>
 211                     return false;
<span class="line-modified"> 212             } else if (highOrderAdjacents.size() + numCandidates &lt; registerCount())</span>
<span class="line-added"> 213                 return true;</span>
 214         }
<span class="line-added"> 215         ASSERT(numCandidates == adjacentsOfV.size());</span>
<span class="line-added"> 216 </span>
<span class="line-added"> 217         auto iteratorEndHighOrderAdjacentsOfU = highOrderAdjacents.end();</span>
 218         for (IndexType adjacentTmpIndex : adjacentsOfV) {
 219             ASSERT(adjacentTmpIndex != u);
 220             ASSERT(adjacentTmpIndex != v);
<span class="line-modified"> 221             numCandidates--;</span>
<span class="line-modified"> 222             if (!hasBeenSimplified(adjacentTmpIndex)</span>
<span class="line-modified"> 223                 &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()</span>
<span class="line-added"> 224                 &amp;&amp; std::find(highOrderAdjacents.begin(), iteratorEndHighOrderAdjacentsOfU, adjacentTmpIndex) == iteratorEndHighOrderAdjacentsOfU) {</span>
<span class="line-added"> 225                 ASSERT(std::find(iteratorEndHighOrderAdjacentsOfU, highOrderAdjacents.end(), adjacentTmpIndex) == highOrderAdjacents.end());</span>
<span class="line-added"> 226                 highOrderAdjacents.uncheckedAppend(adjacentTmpIndex);</span>
<span class="line-added"> 227                 if (highOrderAdjacents.size() &gt;= registerCount())</span>
 228                     return false;
<span class="line-modified"> 229             } else if (highOrderAdjacents.size() + numCandidates &lt; registerCount())</span>
<span class="line-added"> 230                 return true;</span>
 231         }
 232 
<span class="line-added"> 233         ASSERT(!numCandidates);</span>
 234         ASSERT(highOrderAdjacents.size() &lt; registerCount());
 235         return true;
 236     }
 237 
 238     bool precoloredCoalescingHeuristic(IndexType u, IndexType v)
 239     {
 240         if (traceDebug)
 241             dataLog(&quot;    Checking precoloredCoalescingHeuristic\n&quot;);
 242         ASSERT(isPrecolored(u));
 243         ASSERT(!isPrecolored(v));
 244 
 245         // If any adjacent of the non-colored node is not an adjacent of the colored node AND has a degree &gt;= K
 246         // there is a risk that this node needs to have the same color as our precolored node. If we coalesce such
 247         // move, we may create an uncolorable graph.
 248         const auto&amp; adjacentsOfV = m_adjacencyList[v];
 249         for (unsigned adjacentTmpIndex : adjacentsOfV) {
 250             if (!isPrecolored(adjacentTmpIndex)
 251                 &amp;&amp; !hasBeenSimplified(adjacentTmpIndex)
 252                 &amp;&amp; m_degrees[adjacentTmpIndex] &gt;= registerCount()
 253                 &amp;&amp; !hasInterferenceEdge(InterferenceEdge(u, adjacentTmpIndex)))
</pre>
<hr />
<pre>
 499             out.print(first(), &quot;&lt;=&gt;&quot;, second());
 500         }
 501 
 502     private:
 503         uint64_t m_value { 0 };
 504     };
 505 
 506     bool addInterferenceEdge(InterferenceEdge edge)
 507     {
 508         return m_interferenceEdges.add(edge).isNewEntry;
 509     }
 510 
 511     bool hasInterferenceEdge(InterferenceEdge edge)
 512     {
 513         return m_interferenceEdges.contains(edge);
 514     }
 515 
 516     struct InterferenceEdgeHash {
 517         static unsigned hash(const InterferenceEdge&amp; key) { return key.hash(); }
 518         static bool equal(const InterferenceEdge&amp; a, const InterferenceEdge&amp; b) { return a == b; }
<span class="line-modified"> 519         static constexpr bool safeToCompareToEmptyOrDeleted = true;</span>
 520     };
 521     typedef SimpleClassHashTraits&lt;InterferenceEdge&gt; InterferenceEdgeHashTraits;
 522 
 523     Vector&lt;Reg&gt; m_regsInPriorityOrder;
 524     IndexType m_lastPrecoloredRegisterIndex { 0 };
 525 
 526     // The interference graph.
 527     HashSet&lt;InterferenceEdge, InterferenceEdgeHash, InterferenceEdgeHashTraits&gt; m_interferenceEdges;
 528 
 529     Vector&lt;Vector&lt;IndexType, 0, UnsafeVectorOverflow, 4&gt;, 0, UnsafeVectorOverflow&gt; m_adjacencyList;
 530     Vector&lt;IndexType, 0, UnsafeVectorOverflow&gt; m_degrees;
 531 
 532     using IndexTypeSet = HashSet&lt;IndexType, typename DefaultHash&lt;IndexType&gt;::Hash, WTF::UnsignedWithZeroKeyHashTraits&lt;IndexType&gt;&gt;;
 533 
 534     HashMap&lt;IndexType, IndexTypeSet, typename DefaultHash&lt;IndexType&gt;::Hash, WTF::UnsignedWithZeroKeyHashTraits&lt;IndexType&gt;&gt; m_biases;
 535 
 536     // Instead of keeping track of the move instructions, we just keep their operands around and use the index
 537     // in the vector as the &quot;identifier&quot; for the move.
 538     struct MoveOperands {
 539         IndexType srcIndex;
</pre>
<hr />
<pre>
 631         };
 632 
 633         // We first coalesce until we can&#39;t coalesce any more.
 634         do {
 635             changed = false;
 636             m_worklistMoves.forEachMove(coalesceMove);
 637         } while (changed);
 638         do {
 639             changed = false;
 640             m_worklistMoves.forEachLowPriorityMove(coalesceMove);
 641         } while (changed);
 642 
 643         // Then we create our select stack. The invariant we start with here is that nodes in
 644         // the interference graph with degree &gt;= k are on the spill list. Nodes with degree &lt; k
 645         // are on the simplify worklist. A node can move from the spill list to the simplify
 646         // list (but not the other way around, note that this is different than IRC because IRC
 647         // runs this while coalescing, but we do all our coalescing before this). Once a node is
 648         // added to the select stack, it&#39;s not on either list, but only on the select stack.
 649         // Once on the select stack, logically, it&#39;s no longer in the interference graph.
 650         auto assertInvariants = [&amp;] () {
<span class="line-modified"> 651             if (!ASSERT_ENABLED)</span>
 652                 return;
 653             if (!shouldValidateIRAtEachPhase())
 654                 return;
 655 
 656             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 657             unsigned registerCount = this-&gt;registerCount();
 658             for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
 659                 if (getAlias(i) != i)
 660                     continue;
 661                 if (m_isOnSelectStack.contains(i)) {
 662                     ASSERT(!m_simplifyWorklist.contains(i) &amp;&amp; !m_spillWorklist.contains(i));
 663                     continue;
 664                 }
 665                 unsigned degree = m_degrees[i];
 666                 if (degree &gt;= registerCount) {
 667                     ASSERT(m_unspillableTmps.contains(i) || m_spillWorklist.contains(i));
 668                     ASSERT(!m_simplifyWorklist.contains(i));
 669                     continue;
 670                 }
 671                 ASSERT(m_simplifyWorklist.contains(i));
 672             }
 673         };
 674 
 675         makeInitialWorklist();
 676         assertInvariants();
 677         do {
 678             changed = false;
 679 
 680             while (m_simplifyWorklist.size()) {
 681                 simplify();
 682                 assertInvariants();
 683             }
 684 
 685             if (!m_spillWorklist.isEmpty()) {
 686                 selectSpill();
 687                 changed = true;
 688                 ASSERT(m_simplifyWorklist.size() == 1);
 689             }
 690         } while (changed);
 691 
<span class="line-modified"> 692         if (ASSERT_ENABLED) {</span>
 693             ASSERT(!m_simplifyWorklist.size());
 694             ASSERT(m_spillWorklist.isEmpty());
 695             IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
 696             for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i)
 697                 ASSERT(hasBeenSimplified(i));
 698         }
 699 
 700         assignColors();
 701     }
 702 
 703 protected:
 704 
 705     bool coalesce(unsigned&amp; moveIndex)
 706     {
 707         const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
 708         IndexType u = getAlias(moveOperands.srcIndex);
 709         IndexType v = getAlias(moveOperands.dstIndex);
 710 
 711         if (isPrecolored(v))
 712             std::swap(u, v);
 713 
 714         if (traceDebug)
 715             dataLog(&quot;Coalescing move at index &quot;, moveIndex, &quot; u = &quot;, TmpMapper::tmpFromAbsoluteIndex(u), &quot; v = &quot;, TmpMapper::tmpFromAbsoluteIndex(v), &quot;    &quot;);
 716 
 717         if (u == v) {
 718             if (traceDebug)
 719                 dataLog(&quot;Already Coalesced. They&#39;re equal.\n&quot;);
 720             return false;
 721         }
 722 
 723         if (isPrecolored(v)
 724             || hasInterferenceEdge(InterferenceEdge(u, v))) {
 725 
 726             // No need to ever consider this move again if it interferes.
 727             // No coalescing will remove the interference.
 728             moveIndex = UINT_MAX;
 729 
<span class="line-modified"> 730             if (ASSERT_ENABLED) {</span>
 731                 if (isPrecolored(v))
 732                     ASSERT(isPrecolored(u));
 733             }
 734 
 735             if (traceDebug)
 736                 dataLog(&quot;Constrained\n&quot;);
 737 
 738             return false;
 739         }
 740 
 741         if (canBeSafelyCoalesced(u, v)) {
 742             combine(u, v);
 743             m_hasCoalescedNonTrivialMove = true;
 744 
 745             if (traceDebug)
 746                 dataLog(&quot;    Safe Coalescing\n&quot;);
 747             return true;
 748         }
 749 
 750         addBias(u, v);
</pre>
<hr />
<pre>
 893         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_moveList;
 894         Vector&lt;unsigned, 0, UnsafeVectorOverflow&gt; m_lowPriorityMoveList;
 895     };
 896 
 897     void decrementDegree(IndexType tmpIndex)
 898     {
 899         ASSERT(m_degrees[tmpIndex]);
 900         --m_degrees[tmpIndex];
 901     }
 902 
 903     void decrementDegreeInSimplification(IndexType tmpIndex)
 904     {
 905         ASSERT(m_degrees[tmpIndex]);
 906         unsigned oldDegree = m_degrees[tmpIndex]--;
 907 
 908         if (oldDegree == registerCount()) {
 909             ASSERT(m_degrees[tmpIndex] &lt; registerCount());
 910             if (traceDebug)
 911                 dataLogLn(&quot;Moving tmp &quot;, tmpIndex, &quot; from spill list to simplify list because it&#39;s degree is now less than k&quot;);
 912 
<span class="line-modified"> 913             if (ASSERT_ENABLED)</span>
 914                 ASSERT(m_unspillableTmps.contains(tmpIndex) || m_spillWorklist.contains(tmpIndex));
 915             m_spillWorklist.quickClear(tmpIndex);
 916 
 917             ASSERT(!m_simplifyWorklist.contains(tmpIndex));
 918             m_simplifyWorklist.append(tmpIndex);
 919         }
 920     }
 921 
 922     void assignColors()
 923     {
 924         m_worklistMoves.clear();
 925         Base::assignColors();
 926     }
 927 
 928     // Set of &quot;move&quot; enabled for possible coalescing.
 929     MoveSet m_worklistMoves;
 930 };
 931 
 932 template &lt;typename IndexType, typename TmpMapper&gt;
 933 class IRC : public AbstractColoringAllocator&lt;IndexType, TmpMapper&gt; {
</pre>
<hr />
<pre>
1017         assignColors();
1018     }
1019 
1020 protected:
1021 
1022     void makeWorkList()
1023     {
1024         IndexType firstNonRegIndex = m_lastPrecoloredRegisterIndex + 1;
1025         for (IndexType i = firstNonRegIndex; i &lt; m_degrees.size(); ++i) {
1026             unsigned degree = m_degrees[i];
1027             if (degree &gt;= registerCount())
1028                 addToSpill(i);
1029             else if (!m_moveList[i].isEmpty())
1030                 m_freezeWorklist.add(i);
1031             else
1032                 m_simplifyWorklist.append(i);
1033         }
1034     }
1035 
1036     // Low-degree vertex can always be colored: just pick any of the color taken by any
<span class="line-modified">1037     // other adjacent vertices.</span>
1038     // The &quot;Simplify&quot; phase takes a low-degree out of the interference graph to simplify it.
1039     void simplify()
1040     {
1041         IndexType lastIndex = m_simplifyWorklist.takeLast();
1042 
1043         ASSERT(!m_selectStack.contains(lastIndex));
1044         ASSERT(!m_isOnSelectStack.get(lastIndex));
1045         m_selectStack.append(lastIndex);
1046         m_isOnSelectStack.quickSet(lastIndex);
1047 
1048         forEachAdjacent(lastIndex, [this](IndexType adjacentTmpIndex) {
1049             decrementDegree(adjacentTmpIndex);
1050         });
1051     }
1052 
1053     void coalesce()
1054     {
1055         unsigned moveIndex = m_worklistMoves.takeLastMove();
1056         const MoveOperands&amp; moveOperands = m_coalescingCandidates[moveIndex];
1057         IndexType u = getAlias(moveOperands.srcIndex);
</pre>
<hr />
<pre>
1785 
1786         return true;
1787     }
1788 
1789     TmpWidth&amp; m_tmpWidth;
1790 };
1791 
1792 class GraphColoringRegisterAllocation {
1793 public:
1794     GraphColoringRegisterAllocation(Code&amp; code, UseCounts&lt;Tmp&gt;&amp; useCounts)
1795         : m_code(code)
1796         , m_useCounts(useCounts)
1797     {
1798     }
1799 
1800     void run()
1801     {
1802         padInterference(m_code);
1803 
1804         allocateOnBank&lt;GP&gt;();

1805         allocateOnBank&lt;FP&gt;();
1806 
1807         fixSpillsAfterTerminals(m_code);



1808     }
1809 
1810 private:
1811     template&lt;Bank bank&gt;
1812     void allocateOnBank()
1813     {
1814         HashSet&lt;unsigned&gt; unspillableTmps = computeUnspillableTmps&lt;bank&gt;();
1815 
1816         // FIXME: If a Tmp is used only from a Scratch role and that argument is !admitsStack, then
1817         // we should add the Tmp to unspillableTmps. That will help avoid relooping only to turn the
1818         // Tmp into an unspillable Tmp.
1819         // https://bugs.webkit.org/show_bug.cgi?id=152699
1820 
<span class="line-modified">1821         unsigned numIterations = 0;</span>
<span class="line-modified">1822         bool done = false;</span>
<span class="line-added">1823 </span>
<span class="line-added">1824         while (!done) {</span>
<span class="line-added">1825             ++numIterations;</span>
1826 
1827             if (traceDebug)
<span class="line-modified">1828                 dataLog(&quot;Code at iteration &quot;, numIterations, &quot;:\n&quot;, m_code);</span>
1829 
1830             // FIXME: One way to optimize this code is to remove the recomputation inside the fixpoint.
1831             // We need to recompute because spilling adds tmps, but we could just update tmpWidth when we
1832             // add those tmps. Note that one easy way to remove the recomputation is to make any newly
1833             // added Tmps get the same use/def widths that the original Tmp got. But, this may hurt the
1834             // spill code we emit. Since we currently recompute TmpWidth after spilling, the newly
1835             // created Tmps may get narrower use/def widths. On the other hand, the spiller already
1836             // selects which move instruction to use based on the original Tmp&#39;s widths, so it may not
<span class="line-modified">1837             // matter than a subsequent iteration sees a conservative width for the new Tmps. Also, the</span>
1838             // recomputation may not actually be a performance problem; it&#39;s likely that a better way to
1839             // improve performance of TmpWidth is to replace its HashMap with something else. It&#39;s
1840             // possible that most of the TmpWidth overhead is from queries of TmpWidth rather than the
1841             // recomputation, in which case speeding up the lookup would be a bigger win.
1842             // https://bugs.webkit.org/show_bug.cgi?id=152478
1843             m_tmpWidth.recompute(m_code);
1844 
1845             auto doAllocation = [&amp;] (auto&amp; allocator) -&gt; bool {
1846                 allocator.allocate();
1847                 if (!allocator.requiresSpilling()) {
1848                     this-&gt;assignRegistersToTmp&lt;bank&gt;(allocator);
1849                     if (traceDebug)
<span class="line-modified">1850                         dataLog(&quot;Successfull allocation at iteration &quot;, numIterations, &quot;:\n&quot;, m_code);</span>
1851 
1852                     return true;
1853                 }
1854 
1855                 this-&gt;addSpillAndFill&lt;bank&gt;(allocator, unspillableTmps);
1856                 return false;
1857             };
1858 

1859             if (useIRC()) {
1860                 ColoringAllocator&lt;bank, IRC&gt; allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
1861                 done = doAllocation(allocator);
1862             } else {
1863                 ColoringAllocator&lt;bank, Briggs&gt; allocator(m_code, m_tmpWidth, m_useCounts, unspillableTmps);
1864                 done = doAllocation(allocator);
1865             }


1866         }
<span class="line-added">1867         dataLogLnIf(reportStats, &quot;Num iterations = &quot;, numIterations, &quot; for bank: &quot;, bank);</span>
1868     }
1869 
1870     template&lt;Bank bank&gt;
1871     HashSet&lt;unsigned&gt; computeUnspillableTmps()
1872     {
1873 
1874         HashSet&lt;unsigned&gt; unspillableTmps;
1875 
1876         struct Range {
1877             unsigned first { std::numeric_limits&lt;unsigned&gt;::max() };
1878             unsigned last { 0 };
1879             unsigned count { 0 };
1880             unsigned admitStackCount { 0 };
1881         };
1882 
1883         unsigned numTmps = m_code.numTmps(bank);
1884         unsigned arraySize = AbsoluteTmpMapper&lt;bank&gt;::absoluteIndex(numTmps);
1885 
1886         Vector&lt;Range, 0, UnsafeVectorOverflow&gt; ranges;
1887         ranges.fill(Range(), arraySize);
</pre>
<hr />
<pre>
2180                     Arg arg = Arg::stack(stackSlotEntry-&gt;value);
2181                     if (Arg::isAnyUse(role))
2182                         insertionSet.insert(instIndex, move, inst.origin, arg, tmp);
2183                     if (Arg::isAnyDef(role))
2184                         insertionSet.insert(instIndex + 1, move, inst.origin, tmp, arg);
2185                 });
2186             }
2187             insertionSet.execute(block);
2188 
2189             if (hasAliasedTmps) {
2190                 block-&gt;insts().removeAllMatching([&amp;] (const Inst&amp; inst) {
2191                     return allocator.isUselessMove(inst);
2192                 });
2193             }
2194         }
2195     }
2196 
2197     Code&amp; m_code;
2198     TmpWidth m_tmpWidth;
2199     UseCounts&lt;Tmp&gt;&amp; m_useCounts;

2200 };
2201 
2202 } // anonymous namespace
2203 
2204 void allocateRegistersByGraphColoring(Code&amp; code)
2205 {
2206     PhaseScope phaseScope(code, &quot;allocateRegistersByGraphColoring&quot;);
2207 
2208     if (false)
2209         dataLog(&quot;Code before graph coloring:\n&quot;, code);
2210 
2211     UseCounts&lt;Tmp&gt; useCounts(code);
2212     GraphColoringRegisterAllocation graphColoringRegisterAllocation(code, useCounts);
2213     graphColoringRegisterAllocation.run();
2214 }
2215 
2216 } } } // namespace JSC::B3::Air
2217 
2218 #endif // ENABLE(B3_JIT)
</pre>
</td>
</tr>
</table>
<center><a href="AirAllocateRegistersAndStackAndGenerateCode.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../index.html" target="_top">index</a> <a href="AirAllocateStackByGraphColoring.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>