diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCode.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCode.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCode.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCode.cpp
@@ -56,11 +56,11 @@
 JITCode* JITCode::dfg()
 {
     return this;
 }
 
-void JITCode::shrinkToFit()
+void JITCode::shrinkToFit(const ConcurrentJSLocker&)
 {
     common.shrinkToFit();
     osrEntry.shrinkToFit();
     osrExit.shrinkToFit();
     speculationRecovery.shrinkToFit();
@@ -74,20 +74,18 @@
 {
     variableEventStream.reconstruct(
         codeBlock, codeOrigin, minifiedDFG, streamIndex, result);
 }
 
-void JITCode::reconstruct(
-    ExecState* exec, CodeBlock* codeBlock, CodeOrigin codeOrigin, unsigned streamIndex,
-    Operands<Optional<JSValue>>& result)
+void JITCode::reconstruct(CallFrame* callFrame, CodeBlock* codeBlock, CodeOrigin codeOrigin, unsigned streamIndex, Operands<Optional<JSValue>>& result)
 {
     Operands<ValueRecovery> recoveries;
     reconstruct(codeBlock, codeOrigin, streamIndex, recoveries);
 
     result = Operands<Optional<JSValue>>(OperandsLike, recoveries);
     for (size_t i = result.size(); i--;)
-        result[i] = recoveries[i].recover(exec);
+        result[i] = recoveries[i].recover(callFrame);
 }
 
 RegisterSet JITCode::liveRegistersToPreserveAtExceptionHandlingCallSite(CodeBlock* codeBlock, CallSiteIndex callSiteIndex)
 {
     for (OSRExit& exit : osrExit) {
@@ -128,50 +126,45 @@
 }
 
 void JITCode::optimizeNextInvocation(CodeBlock* codeBlock)
 {
     ASSERT(codeBlock->jitType() == JITType::DFGJIT);
-    if (Options::verboseOSR())
-        dataLog(*codeBlock, ": FTL-optimizing next invocation.\n");
+    dataLogLnIf(Options::verboseOSR(), *codeBlock, ": FTL-optimizing next invocation.");
     tierUpCounter.setNewThreshold(0, codeBlock);
 }
 
 void JITCode::dontOptimizeAnytimeSoon(CodeBlock* codeBlock)
 {
     ASSERT(codeBlock->jitType() == JITType::DFGJIT);
-    if (Options::verboseOSR())
-        dataLog(*codeBlock, ": Not FTL-optimizing anytime soon.\n");
+    dataLogLnIf(Options::verboseOSR(), *codeBlock, ": Not FTL-optimizing anytime soon.");
     tierUpCounter.deferIndefinitely();
 }
 
 void JITCode::optimizeAfterWarmUp(CodeBlock* codeBlock)
 {
     ASSERT(codeBlock->jitType() == JITType::DFGJIT);
-    if (Options::verboseOSR())
-        dataLog(*codeBlock, ": FTL-optimizing after warm-up.\n");
+    dataLogLnIf(Options::verboseOSR(), *codeBlock, ": FTL-optimizing after warm-up.");
     CodeBlock* baseline = codeBlock->baselineVersion();
     tierUpCounter.setNewThreshold(
         baseline->adjustedCounterValue(Options::thresholdForFTLOptimizeAfterWarmUp()),
         baseline);
 }
 
 void JITCode::optimizeSoon(CodeBlock* codeBlock)
 {
     ASSERT(codeBlock->jitType() == JITType::DFGJIT);
-    if (Options::verboseOSR())
-        dataLog(*codeBlock, ": FTL-optimizing soon.\n");
+    dataLogLnIf(Options::verboseOSR(), *codeBlock, ": FTL-optimizing soon.");
     CodeBlock* baseline = codeBlock->baselineVersion();
     tierUpCounter.setNewThreshold(
         baseline->adjustedCounterValue(Options::thresholdForFTLOptimizeSoon()),
         codeBlock);
 }
 
 void JITCode::forceOptimizationSlowPathConcurrently(CodeBlock* codeBlock)
 {
     ASSERT(codeBlock->jitType() == JITType::DFGJIT);
-    if (Options::verboseOSR())
-        dataLog(*codeBlock, ": Forcing slow path concurrently for FTL entry.\n");
+    dataLogLnIf(Options::verboseOSR(), *codeBlock, ": Forcing slow path concurrently for FTL entry.");
     tierUpCounter.forceSlowPathConcurrently();
 }
 
 void JITCode::setOptimizationThresholdBasedOnCompilationResult(
     CodeBlock* codeBlock, CompilationResult result)
@@ -204,21 +197,21 @@
 }
 
 void JITCode::setOSREntryBlock(VM& vm, const JSCell* owner, CodeBlock* osrEntryBlock)
 {
     if (Options::verboseOSR()) {
-        dataLog(RawPointer(this), ": Setting OSR entry block to ", RawPointer(osrEntryBlock), "\n");
-        dataLog("OSR entries will go to ", osrEntryBlock->jitCode()->ftlForOSREntry()->addressForCall(ArityCheckNotRequired), "\n");
+        dataLogLn(RawPointer(this), ": Setting OSR entry block to ", RawPointer(osrEntryBlock));
+        dataLogLn("OSR entries will go to ", osrEntryBlock->jitCode()->ftlForOSREntry()->addressForCall(ArityCheckNotRequired));
     }
     m_osrEntryBlock.set(vm, owner, osrEntryBlock);
 }
 
 void JITCode::clearOSREntryBlockAndResetThresholds(CodeBlock *dfgCodeBlock)
 {
     ASSERT(m_osrEntryBlock);
 
-    unsigned osrEntryBytecode = m_osrEntryBlock->jitCode()->ftlForOSREntry()->bytecodeIndex();
+    BytecodeIndex osrEntryBytecode = m_osrEntryBlock->jitCode()->ftlForOSREntry()->bytecodeIndex();
     m_osrEntryBlock.clear();
     osrEntryRetry = 0;
     tierUpEntryTriggers.set(osrEntryBytecode, JITCode::TriggerReason::DontTrigger);
     setOptimizationThresholdBasedOnCompilationResult(dfgCodeBlock, CompilationDeferred);
 }
@@ -253,11 +246,11 @@
     auto comparator = [] (const auto& a, const auto& b) {
         return a.m_bytecodeIndex < b.m_bytecodeIndex;
     };
     std::sort(osrEntry.begin(), osrEntry.end(), comparator);
 
-#if !ASSERT_DISABLED
+#if ASSERT_ENABLED
     auto verifyIsSorted = [&] (auto& osrVector) {
         for (unsigned i = 0; i + 1 < osrVector.size(); ++i)
             ASSERT(osrVector[i].m_bytecodeIndex <= osrVector[i + 1].m_bytecodeIndex);
     };
     verifyIsSorted(osrEntry);
