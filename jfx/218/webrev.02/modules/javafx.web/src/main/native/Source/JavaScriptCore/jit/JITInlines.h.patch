diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITInlines.h
@@ -24,26 +24,15 @@
  */
 
 #pragma once
 
 #if ENABLE(JIT)
+#include "CommonSlowPathsInlines.h"
 #include "JSCInlines.h"
 
 namespace JSC {
 
-inline MacroAssembler::JumpList JIT::emitDoubleGetByVal(const Instruction* instruction, PatchableJump& badType)
-{
-#if USE(JSVALUE64)
-    JSValueRegs result = JSValueRegs(regT0);
-#else
-    JSValueRegs result = JSValueRegs(regT1, regT0);
-#endif
-    JumpList slowCases = emitDoubleLoad(instruction, badType);
-    boxDouble(fpRegT0, result);
-    return slowCases;
-}
-
 ALWAYS_INLINE MacroAssembler::JumpList JIT::emitLoadForArrayMode(const Instruction* currentInstruction, JITArrayMode arrayMode, PatchableJump& badType)
 {
     switch (arrayMode) {
     case JITInt32:
         return emitInt32Load(currentInstruction, badType);
@@ -58,33 +47,24 @@
     }
     RELEASE_ASSERT_NOT_REACHED();
     return MacroAssembler::JumpList();
 }
 
-inline MacroAssembler::JumpList JIT::emitContiguousGetByVal(const Instruction* instruction, PatchableJump& badType, IndexingType expectedShape)
+ALWAYS_INLINE bool JIT::isOperandConstantDouble(VirtualRegister src)
 {
-    return emitContiguousLoad(instruction, badType, expectedShape);
+    return src.isConstant() && getConstantOperand(src).isDouble();
 }
 
-inline MacroAssembler::JumpList JIT::emitArrayStorageGetByVal(const Instruction* instruction, PatchableJump& badType)
+ALWAYS_INLINE JSValue JIT::getConstantOperand(VirtualRegister src)
 {
-    return emitArrayStorageLoad(instruction, badType);
-}
-
-ALWAYS_INLINE bool JIT::isOperandConstantDouble(int src)
-{
-    return m_codeBlock->isConstantRegisterIndex(src) && getConstantOperand(src).isDouble();
-}
-
-ALWAYS_INLINE JSValue JIT::getConstantOperand(int src)
-{
-    ASSERT(m_codeBlock->isConstantRegisterIndex(src));
+    ASSERT(src.isConstant());
     return m_codeBlock->getConstant(src);
 }
 
-ALWAYS_INLINE void JIT::emitPutIntToCallFrameHeader(RegisterID from, int entry)
+ALWAYS_INLINE void JIT::emitPutIntToCallFrameHeader(RegisterID from, VirtualRegister entry)
 {
+    ASSERT(entry.isHeader());
 #if USE(JSVALUE32_64)
     store32(TrustedImm32(JSValue::Int32Tag), tagFor(entry));
     store32(from, payloadFor(entry));
 #else
     store64(from, addressFor(entry));
@@ -95,49 +75,40 @@
 {
     failures.append(branchIfNotString(src));
     loadPtr(MacroAssembler::Address(src, JSString::offsetOfValue()), dst);
     failures.append(branchIfRopeStringImpl(dst));
     failures.append(branch32(NotEqual, MacroAssembler::Address(dst, StringImpl::lengthMemoryOffset()), TrustedImm32(1)));
-    loadPtr(MacroAssembler::Address(dst, StringImpl::flagsOffset()), regT1);
-    loadPtr(MacroAssembler::Address(dst, StringImpl::dataOffset()), dst);
-
-    JumpList is16Bit;
-    JumpList cont8Bit;
-    is16Bit.append(branchTest32(Zero, regT1, TrustedImm32(StringImpl::flagIs8Bit())));
-    load8(MacroAssembler::Address(dst, 0), dst);
-    cont8Bit.append(jump());
+    loadPtr(MacroAssembler::Address(dst, StringImpl::dataOffset()), regT1);
+
+    auto is16Bit = branchTest32(Zero, Address(dst, StringImpl::flagsOffset()), TrustedImm32(StringImpl::flagIs8Bit()));
+    load8(MacroAssembler::Address(regT1, 0), dst);
+    auto done = jump();
     is16Bit.link(this);
-    load16(MacroAssembler::Address(dst, 0), dst);
-    cont8Bit.link(this);
+    load16(MacroAssembler::Address(regT1, 0), dst);
+    done.link(this);
 }
 
 ALWAYS_INLINE JIT::Call JIT::emitNakedCall(CodePtr<NoPtrTag> target)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
     Call nakedCall = nearCall();
-    m_calls.append(CallRecord(nakedCall, m_bytecodeOffset, FunctionPtr<OperationPtrTag>(target.retagged<OperationPtrTag>())));
+    m_calls.append(CallRecord(nakedCall, m_bytecodeIndex, FunctionPtr<OperationPtrTag>(target.retagged<OperationPtrTag>())));
     return nakedCall;
 }
 
 ALWAYS_INLINE JIT::Call JIT::emitNakedTailCall(CodePtr<NoPtrTag> target)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
     Call nakedCall = nearTailCall();
-    m_calls.append(CallRecord(nakedCall, m_bytecodeOffset, FunctionPtr<OperationPtrTag>(target.retagged<OperationPtrTag>())));
+    m_calls.append(CallRecord(nakedCall, m_bytecodeIndex, FunctionPtr<OperationPtrTag>(target.retagged<OperationPtrTag>())));
     return nakedCall;
 }
 
 ALWAYS_INLINE void JIT::updateTopCallFrame()
 {
-    ASSERT(static_cast<int>(m_bytecodeOffset) >= 0);
-#if USE(JSVALUE32_64)
-    const Instruction* instruction = m_codeBlock->instructions().at(m_bytecodeOffset).ptr();
-    uint32_t locationBits = CallSiteIndex(instruction).bits();
-#else
-    uint32_t locationBits = CallSiteIndex(m_bytecodeOffset).bits();
-#endif
-    store32(TrustedImm32(locationBits), tagFor(CallFrameSlot::argumentCount));
+    uint32_t locationBits = CallSiteIndex(m_bytecodeIndex).bits();
+    store32(TrustedImm32(locationBits), tagFor(CallFrameSlot::argumentCountIncludingThis));
 
     // FIXME: It's not clear that this is needed. JITOperations tend to update the top call frame on
     // the C++ side.
     // https://bugs.webkit.org/show_bug.cgi?id=155693
     storePtr(callFrameRegister, &m_vm->topCallFrame);
@@ -167,11 +138,11 @@
     MacroAssembler::Call call = appendCall(function);
     exceptionCheckWithCallFrameRollback();
     return call;
 }
 
-ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr<CFunctionPtrTag> function, int dst)
+ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr<CFunctionPtrTag> function, VirtualRegister dst)
 {
     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
 #if USE(JSVALUE64)
     emitPutVirtualRegister(dst, returnValueGPR);
 #else
@@ -179,11 +150,11 @@
 #endif
     return call;
 }
 
 template<typename Metadata>
-ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata& metadata, const FunctionPtr<CFunctionPtrTag> function, int dst)
+ALWAYS_INLINE MacroAssembler::Call JIT::appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata& metadata, const FunctionPtr<CFunctionPtrTag> function, VirtualRegister dst)
 {
     MacroAssembler::Call call = appendCallWithExceptionCheck(function);
     emitValueProfilingSite(metadata);
 #if USE(JSVALUE64)
     emitPutVirtualRegister(dst, returnValueGPR);
@@ -191,65 +162,72 @@
     emitStore(dst, returnValueGPR2, returnValueGPR);
 #endif
     return call;
 }
 
-ALWAYS_INLINE void JIT::linkSlowCaseIfNotJSCell(Vector<SlowCaseEntry>::iterator& iter, int vReg)
+ALWAYS_INLINE void JIT::linkSlowCaseIfNotJSCell(Vector<SlowCaseEntry>::iterator& iter, VirtualRegister reg)
 {
-    if (!m_codeBlock->isKnownNotImmediate(vReg))
+    if (!m_codeBlock->isKnownNotImmediate(reg))
         linkSlowCase(iter);
 }
 
-ALWAYS_INLINE void JIT::linkAllSlowCasesForBytecodeOffset(Vector<SlowCaseEntry>& slowCases, Vector<SlowCaseEntry>::iterator& iter, unsigned bytecodeOffset)
+ALWAYS_INLINE void JIT::linkAllSlowCasesForBytecodeIndex(Vector<SlowCaseEntry>& slowCases, Vector<SlowCaseEntry>::iterator& iter, BytecodeIndex bytecodeIndex)
 {
-    while (iter != slowCases.end() && iter->to == bytecodeOffset)
+    while (iter != slowCases.end() && iter->to == bytecodeIndex)
         linkSlowCase(iter);
 }
 
+ALWAYS_INLINE bool JIT::hasAnySlowCases(Vector<SlowCaseEntry>& slowCases, Vector<SlowCaseEntry>::iterator& iter, BytecodeIndex bytecodeIndex)
+{
+    if (iter != slowCases.end() && iter->to == bytecodeIndex)
+        return true;
+    return false;
+}
+
 ALWAYS_INLINE void JIT::addSlowCase(Jump jump)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
-    m_slowCases.append(SlowCaseEntry(jump, m_bytecodeOffset));
+    m_slowCases.append(SlowCaseEntry(jump, m_bytecodeIndex));
 }
 
 ALWAYS_INLINE void JIT::addSlowCase(const JumpList& jumpList)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
     for (const Jump& jump : jumpList.jumps())
-        m_slowCases.append(SlowCaseEntry(jump, m_bytecodeOffset));
+        m_slowCases.append(SlowCaseEntry(jump, m_bytecodeIndex));
 }
 
 ALWAYS_INLINE void JIT::addSlowCase()
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
     Jump emptyJump; // Doing it this way to make Windows happy.
-    m_slowCases.append(SlowCaseEntry(emptyJump, m_bytecodeOffset));
+    m_slowCases.append(SlowCaseEntry(emptyJump, m_bytecodeIndex));
 }
 
 ALWAYS_INLINE void JIT::addJump(Jump jump, int relativeOffset)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
-    m_jmpTable.append(JumpTable(jump, m_bytecodeOffset + relativeOffset));
+    m_jmpTable.append(JumpTable(jump, m_bytecodeIndex.offset() + relativeOffset));
 }
 
 ALWAYS_INLINE void JIT::addJump(const JumpList& jumpList, int relativeOffset)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
     for (auto& jump : jumpList.jumps())
         addJump(jump, relativeOffset);
 }
 
 ALWAYS_INLINE void JIT::emitJumpSlowToHot(Jump jump, int relativeOffset)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
-    jump.linkTo(m_labels[m_bytecodeOffset + relativeOffset], this);
+    jump.linkTo(m_labels[m_bytecodeIndex.offset() + relativeOffset], this);
 }
 
 #if ENABLE(SAMPLING_FLAGS)
 ALWAYS_INLINE void JIT::setSamplingFlag(int32_t flag)
 {
@@ -301,13 +279,13 @@
     storePtr(TrustedImmPtr(codeBlock), m_interpreter->sampler()->codeBlockSlot());
 }
 #endif
 #endif
 
-ALWAYS_INLINE bool JIT::isOperandConstantChar(int src)
+ALWAYS_INLINE bool JIT::isOperandConstantChar(VirtualRegister src)
 {
-    return m_codeBlock->isConstantRegisterIndex(src) && getConstantOperand(src).isString() && asString(getConstantOperand(src).asCell())->length() == 1;
+    return src.isConstant() && getConstantOperand(src).isString() && asString(getConstantOperand(src).asCell())->length() == 1;
 }
 
 inline void JIT::emitValueProfilingSite(ValueProfile& valueProfile)
 {
     ASSERT(shouldEmitProfiling());
@@ -380,183 +358,170 @@
     if (arrayProfileSaw(arrayModes, ArrayStorageShape))
         return JITArrayStorage;
     return JITContiguous;
 }
 
-ALWAYS_INLINE int32_t JIT::getOperandConstantInt(int src)
+ALWAYS_INLINE int32_t JIT::getOperandConstantInt(VirtualRegister src)
 {
     return getConstantOperand(src).asInt32();
 }
 
-ALWAYS_INLINE double JIT::getOperandConstantDouble(int src)
+ALWAYS_INLINE double JIT::getOperandConstantDouble(VirtualRegister src)
 {
     return getConstantOperand(src).asDouble();
 }
 
-ALWAYS_INLINE void JIT::emitInitRegister(int dst)
+ALWAYS_INLINE void JIT::emitInitRegister(VirtualRegister dst)
 {
     storeTrustedValue(jsUndefined(), addressFor(dst));
 }
 
 #if USE(JSVALUE32_64)
 
-inline void JIT::emitLoadTag(int index, RegisterID tag)
+inline void JIT::emitLoadDouble(VirtualRegister reg, FPRegisterID value)
 {
-    if (m_codeBlock->isConstantRegisterIndex(index)) {
-        move(Imm32(getConstantOperand(index).tag()), tag);
+    if (reg.isConstant()) {
+        WriteBarrier<Unknown>& inConstantPool = m_codeBlock->constantRegister(reg);
+        loadDouble(TrustedImmPtr(&inConstantPool), value);
+    } else
+        loadDouble(addressFor(reg), value);
+}
+
+inline void JIT::emitLoadTag(VirtualRegister reg, RegisterID tag)
+{
+    if (reg.isConstant()) {
+        move(Imm32(getConstantOperand(reg).tag()), tag);
         return;
     }
 
-    load32(tagFor(index), tag);
+    load32(tagFor(reg), tag);
 }
 
-inline void JIT::emitLoadPayload(int index, RegisterID payload)
+inline void JIT::emitLoadPayload(VirtualRegister reg, RegisterID payload)
 {
-    if (m_codeBlock->isConstantRegisterIndex(index)) {
-        move(Imm32(getConstantOperand(index).payload()), payload);
+    if (reg.isConstant()) {
+        move(Imm32(getConstantOperand(reg).payload()), payload);
         return;
     }
 
-    load32(payloadFor(index), payload);
+    load32(payloadFor(reg), payload);
 }
 
 inline void JIT::emitLoad(const JSValue& v, RegisterID tag, RegisterID payload)
 {
     move(Imm32(v.payload()), payload);
     move(Imm32(v.tag()), tag);
 }
 
-ALWAYS_INLINE void JIT::emitGetVirtualRegister(int src, JSValueRegs dst)
+ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst)
 {
     emitLoad(src, dst.tagGPR(), dst.payloadGPR());
 }
 
-ALWAYS_INLINE void JIT::emitPutVirtualRegister(int dst, JSValueRegs from)
+ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, JSValueRegs from)
 {
     emitStore(dst, from.tagGPR(), from.payloadGPR());
 }
 
-inline void JIT::emitLoad(int index, RegisterID tag, RegisterID payload, RegisterID base)
+inline void JIT::emitLoad(VirtualRegister reg, RegisterID tag, RegisterID payload, RegisterID base)
 {
     RELEASE_ASSERT(tag != payload);
 
     if (base == callFrameRegister) {
         RELEASE_ASSERT(payload != base);
-        emitLoadPayload(index, payload);
-        emitLoadTag(index, tag);
+        emitLoadPayload(reg, payload);
+        emitLoadTag(reg, tag);
         return;
     }
 
-    VirtualRegister target { index };
     if (payload == base) { // avoid stomping base
-        load32(tagFor(target, base), tag);
-        load32(payloadFor(target, base), payload);
+        load32(tagFor(reg, base), tag);
+        load32(payloadFor(reg, base), payload);
         return;
     }
 
-    load32(payloadFor(target, base), payload);
-    load32(tagFor(target, base), tag);
-}
-
-inline void JIT::emitLoad2(int index1, RegisterID tag1, RegisterID payload1, int index2, RegisterID tag2, RegisterID payload2)
-{
-    emitLoad(index2, tag2, payload2);
-    emitLoad(index1, tag1, payload1);
-}
-
-inline void JIT::emitLoadDouble(int index, FPRegisterID value)
-{
-    if (m_codeBlock->isConstantRegisterIndex(index)) {
-        WriteBarrier<Unknown>& inConstantPool = m_codeBlock->constantRegister(index);
-        loadDouble(TrustedImmPtr(&inConstantPool), value);
-    } else
-        loadDouble(addressFor(index), value);
+    load32(payloadFor(reg, base), payload);
+    load32(tagFor(reg, base), tag);
 }
 
-inline void JIT::emitLoadInt32ToDouble(int index, FPRegisterID value)
+inline void JIT::emitLoad2(VirtualRegister reg1, RegisterID tag1, RegisterID payload1, VirtualRegister reg2, RegisterID tag2, RegisterID payload2)
 {
-    if (m_codeBlock->isConstantRegisterIndex(index)) {
-        WriteBarrier<Unknown>& inConstantPool = m_codeBlock->constantRegister(index);
-        char* bytePointer = reinterpret_cast<char*>(&inConstantPool);
-        convertInt32ToDouble(AbsoluteAddress(bytePointer + OBJECT_OFFSETOF(JSValue, u.asBits.payload)), value);
-    } else
-        convertInt32ToDouble(payloadFor(index), value);
+    emitLoad(reg2, tag2, payload2);
+    emitLoad(reg1, tag1, payload1);
 }
 
-inline void JIT::emitStore(int index, RegisterID tag, RegisterID payload, RegisterID base)
+inline void JIT::emitStore(VirtualRegister reg, RegisterID tag, RegisterID payload, RegisterID base)
 {
-    VirtualRegister target { index };
-    store32(payload, payloadFor(target, base));
-    store32(tag, tagFor(target, base));
+    store32(payload, payloadFor(reg, base));
+    store32(tag, tagFor(reg, base));
 }
 
-inline void JIT::emitStoreInt32(int index, RegisterID payload, bool indexIsInt32)
+inline void JIT::emitStoreInt32(VirtualRegister reg, RegisterID payload, bool indexIsInt32)
 {
-    store32(payload, payloadFor(index));
+    store32(payload, payloadFor(reg));
     if (!indexIsInt32)
-        store32(TrustedImm32(JSValue::Int32Tag), tagFor(index));
+        store32(TrustedImm32(JSValue::Int32Tag), tagFor(reg));
 }
 
-inline void JIT::emitStoreInt32(int index, TrustedImm32 payload, bool indexIsInt32)
+inline void JIT::emitStoreInt32(VirtualRegister reg, TrustedImm32 payload, bool indexIsInt32)
 {
-    store32(payload, payloadFor(index));
+    store32(payload, payloadFor(reg));
     if (!indexIsInt32)
-        store32(TrustedImm32(JSValue::Int32Tag), tagFor(index));
+        store32(TrustedImm32(JSValue::Int32Tag), tagFor(reg));
 }
 
-inline void JIT::emitStoreCell(int index, RegisterID payload, bool indexIsCell)
+inline void JIT::emitStoreCell(VirtualRegister reg, RegisterID payload, bool indexIsCell)
 {
-    store32(payload, payloadFor(index));
+    store32(payload, payloadFor(reg));
     if (!indexIsCell)
-        store32(TrustedImm32(JSValue::CellTag), tagFor(index));
+        store32(TrustedImm32(JSValue::CellTag), tagFor(reg));
 }
 
-inline void JIT::emitStoreBool(int index, RegisterID payload, bool indexIsBool)
+inline void JIT::emitStoreBool(VirtualRegister reg, RegisterID payload, bool indexIsBool)
 {
-    store32(payload, payloadFor(index));
+    store32(payload, payloadFor(reg));
     if (!indexIsBool)
-        store32(TrustedImm32(JSValue::BooleanTag), tagFor(index));
+        store32(TrustedImm32(JSValue::BooleanTag), tagFor(reg));
 }
 
-inline void JIT::emitStoreDouble(int index, FPRegisterID value)
+inline void JIT::emitStoreDouble(VirtualRegister reg, FPRegisterID value)
 {
-    storeDouble(value, addressFor(index));
+    storeDouble(value, addressFor(reg));
 }
 
-inline void JIT::emitStore(int index, const JSValue constant, RegisterID base)
+inline void JIT::emitStore(VirtualRegister reg, const JSValue constant, RegisterID base)
 {
-    VirtualRegister target { index };
-    store32(Imm32(constant.payload()), payloadFor(target, base));
-    store32(Imm32(constant.tag()), tagFor(target, base));
+    store32(Imm32(constant.payload()), payloadFor(reg, base));
+    store32(Imm32(constant.tag()), tagFor(reg, base));
 }
 
-inline void JIT::emitJumpSlowCaseIfNotJSCell(int virtualRegisterIndex)
+inline void JIT::emitJumpSlowCaseIfNotJSCell(VirtualRegister reg)
 {
-    if (!m_codeBlock->isKnownNotImmediate(virtualRegisterIndex)) {
-        if (m_codeBlock->isConstantRegisterIndex(virtualRegisterIndex))
+    if (!m_codeBlock->isKnownNotImmediate(reg)) {
+        if (reg.isConstant())
             addSlowCase(jump());
         else
-            addSlowCase(emitJumpIfNotJSCell(virtualRegisterIndex));
+            addSlowCase(emitJumpIfNotJSCell(reg));
     }
 }
 
-inline void JIT::emitJumpSlowCaseIfNotJSCell(int virtualRegisterIndex, RegisterID tag)
+inline void JIT::emitJumpSlowCaseIfNotJSCell(VirtualRegister reg, RegisterID tag)
 {
-    if (!m_codeBlock->isKnownNotImmediate(virtualRegisterIndex)) {
-        if (m_codeBlock->isConstantRegisterIndex(virtualRegisterIndex))
+    if (!m_codeBlock->isKnownNotImmediate(reg)) {
+        if (reg.isConstant())
             addSlowCase(jump());
         else
             addSlowCase(branchIfNotCell(tag));
     }
 }
 
-ALWAYS_INLINE bool JIT::isOperandConstantInt(int src)
+ALWAYS_INLINE bool JIT::isOperandConstantInt(VirtualRegister src)
 {
-    return m_codeBlock->isConstantRegisterIndex(src) && getConstantOperand(src).isInt32();
+    return src.isConstant() && getConstantOperand(src).isInt32();
 }
 
-ALWAYS_INLINE bool JIT::getOperandConstantInt(int op1, int op2, int& op, int32_t& constant)
+ALWAYS_INLINE bool JIT::getOperandConstantInt(VirtualRegister op1, VirtualRegister op2, VirtualRegister& op, int32_t& constant)
 {
     if (isOperandConstantInt(op1)) {
         constant = getConstantOperand(op1).asInt32();
         op = op2;
         return true;
@@ -572,15 +537,15 @@
 }
 
 #else // USE(JSVALUE32_64)
 
 // get arg puts an arg from the SF register array into a h/w register
-ALWAYS_INLINE void JIT::emitGetVirtualRegister(int src, RegisterID dst)
+ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, RegisterID dst)
 {
-    ASSERT(m_bytecodeOffset != std::numeric_limits<unsigned>::max()); // This method should only be called during hot/cold path generation, so that m_bytecodeOffset is set.
+    ASSERT(m_bytecodeIndex); // This method should only be called during hot/cold path generation, so that m_bytecodeIndex is set.
 
-    if (m_codeBlock->isConstantRegisterIndex(src)) {
+    if (src.isConstant()) {
         JSValue value = m_codeBlock->getConstant(src);
         if (!value.isNumber())
             move(TrustedImm64(JSValue::encode(value)), dst);
         else
             move(Imm64(JSValue::encode(value)), dst);
@@ -588,51 +553,36 @@
     }
 
     load64(addressFor(src), dst);
 }
 
-ALWAYS_INLINE void JIT::emitGetVirtualRegister(int src, JSValueRegs dst)
+ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst)
 {
     emitGetVirtualRegister(src, dst.payloadGPR());
 }
 
-ALWAYS_INLINE void JIT::emitGetVirtualRegister(VirtualRegister src, RegisterID dst)
-{
-    emitGetVirtualRegister(src.offset(), dst);
-}
-
-ALWAYS_INLINE void JIT::emitGetVirtualRegisters(int src1, RegisterID dst1, int src2, RegisterID dst2)
+ALWAYS_INLINE void JIT::emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2)
 {
     emitGetVirtualRegister(src1, dst1);
     emitGetVirtualRegister(src2, dst2);
 }
 
-ALWAYS_INLINE void JIT::emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2)
-{
-    emitGetVirtualRegisters(src1.offset(), dst1, src2.offset(), dst2);
-}
-
-ALWAYS_INLINE bool JIT::isOperandConstantInt(int src)
+ALWAYS_INLINE bool JIT::isOperandConstantInt(VirtualRegister src)
 {
-    return m_codeBlock->isConstantRegisterIndex(src) && getConstantOperand(src).isInt32();
+    return src.isConstant() && getConstantOperand(src).isInt32();
 }
 
-ALWAYS_INLINE void JIT::emitPutVirtualRegister(int dst, RegisterID from)
+ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, RegisterID from)
 {
     store64(from, addressFor(dst));
 }
 
-ALWAYS_INLINE void JIT::emitPutVirtualRegister(int dst, JSValueRegs from)
+ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, JSValueRegs from)
 {
     emitPutVirtualRegister(dst, from.payloadGPR());
 }
 
-ALWAYS_INLINE void JIT::emitPutVirtualRegister(VirtualRegister dst, RegisterID from)
-{
-    emitPutVirtualRegister(dst.offset(), from);
-}
-
 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfBothJSCells(RegisterID reg1, RegisterID reg2, RegisterID scratch)
 {
     move(reg1, scratch);
     or64(reg2, scratch);
     return branchIfCell(scratch);
@@ -646,37 +596,19 @@
 ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg)
 {
     addSlowCase(branchIfNotCell(reg));
 }
 
-ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg, int vReg)
+ALWAYS_INLINE void JIT::emitJumpSlowCaseIfNotJSCell(RegisterID reg, VirtualRegister vReg)
 {
     if (!m_codeBlock->isKnownNotImmediate(vReg))
         emitJumpSlowCaseIfNotJSCell(reg);
 }
 
-inline void JIT::emitLoadDouble(int index, FPRegisterID value)
-{
-    if (m_codeBlock->isConstantRegisterIndex(index)) {
-        WriteBarrier<Unknown>& inConstantPool = m_codeBlock->constantRegister(index);
-        loadDouble(TrustedImmPtr(&inConstantPool), value);
-    } else
-        loadDouble(addressFor(index), value);
-}
-
-inline void JIT::emitLoadInt32ToDouble(int index, FPRegisterID value)
-{
-    if (m_codeBlock->isConstantRegisterIndex(index)) {
-        ASSERT(isOperandConstantInt(index));
-        convertInt32ToDouble(Imm32(getConstantOperand(index).asInt32()), value);
-    } else
-        convertInt32ToDouble(addressFor(index), value);
-}
-
 ALWAYS_INLINE JIT::PatchableJump JIT::emitPatchableJumpIfNotInt(RegisterID reg)
 {
-    return patchableBranch64(Below, reg, tagTypeNumberRegister);
+    return patchableBranch64(Below, reg, numberTagRegister);
 }
 
 ALWAYS_INLINE JIT::Jump JIT::emitJumpIfNotInt(RegisterID reg1, RegisterID reg2, RegisterID scratch)
 {
     move(reg1, scratch);
@@ -718,17 +650,17 @@
     m_copiedGetPutInfos.add(key, getPutInfo.operand());
     return getPutInfo;
 }
 
 template<typename BinaryOp>
-ALWAYS_INLINE ArithProfile JIT::copiedArithProfile(BinaryOp bytecode)
+ALWAYS_INLINE BinaryArithProfile JIT::copiedArithProfile(BinaryOp bytecode)
 {
-    uint64_t key = static_cast<uint64_t>(BinaryOp::opcodeID) << 32 | static_cast<uint64_t>(bytecode.m_metadataID);
+    uint64_t key = (static_cast<uint64_t>(BinaryOp::opcodeID) + 1) << 32 | static_cast<uint64_t>(bytecode.m_metadataID);
     auto iterator = m_copiedArithProfiles.find(key);
     if (iterator != m_copiedArithProfiles.end())
         return iterator->value;
-    ArithProfile arithProfile = bytecode.metadata(m_codeBlock).m_arithProfile;
+    BinaryArithProfile arithProfile = bytecode.metadata(m_codeBlock).m_arithProfile;
     m_copiedArithProfiles.add(key, arithProfile);
     return arithProfile;
 }
 
 } // namespace JSC
