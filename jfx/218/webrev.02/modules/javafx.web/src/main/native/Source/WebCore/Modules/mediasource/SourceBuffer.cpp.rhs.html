<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/WebCore/Modules/mediasource/SourceBuffer.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2013 Google Inc. All rights reserved.
   3  * Copyright (C) 2013-2019 Apple Inc. All rights reserved.
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions are
   7  * met:
   8  *
   9  *     * Redistributions of source code must retain the above copyright
  10  * notice, this list of conditions and the following disclaimer.
  11  *     * Redistributions in binary form must reproduce the above
  12  * copyright notice, this list of conditions and the following disclaimer
  13  * in the documentation and/or other materials provided with the
  14  * distribution.
  15  *     * Neither the name of Google Inc. nor the names of its
  16  * contributors may be used to endorse or promote products derived from
  17  * this software without specific prior written permission.
  18  *
  19  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
  20  * &quot;AS IS&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
  21  * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
  22  * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
  23  * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
  24  * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
  25  * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
  26  * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
  27  * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  28  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  29  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  30  */
  31 
  32 #include &quot;config.h&quot;
  33 #include &quot;SourceBuffer.h&quot;
  34 
  35 #if ENABLE(MEDIA_SOURCE)
  36 
  37 #include &quot;AudioTrackList.h&quot;
  38 #include &quot;BufferSource.h&quot;
  39 #include &quot;Event.h&quot;
  40 #include &quot;EventNames.h&quot;
  41 #include &quot;GenericEventQueue.h&quot;
  42 #include &quot;HTMLMediaElement.h&quot;
  43 #include &quot;InbandTextTrack.h&quot;
<a name="1" id="anc1"></a><span class="line-added">  44 #include &quot;InbandTextTrackPrivate.h&quot;</span>
  45 #include &quot;Logging.h&quot;
  46 #include &quot;MediaDescription.h&quot;
  47 #include &quot;MediaSample.h&quot;
  48 #include &quot;MediaSource.h&quot;
  49 #include &quot;SampleMap.h&quot;
  50 #include &quot;SourceBufferList.h&quot;
  51 #include &quot;SourceBufferPrivate.h&quot;
  52 #include &quot;TextTrackList.h&quot;
  53 #include &quot;TimeRanges.h&quot;
  54 #include &quot;VideoTrackList.h&quot;
  55 #include &lt;JavaScriptCore/JSCInlines.h&gt;
  56 #include &lt;JavaScriptCore/JSLock.h&gt;
  57 #include &lt;JavaScriptCore/VM.h&gt;
  58 #include &lt;limits&gt;
  59 #include &lt;wtf/CheckedArithmetic.h&gt;
  60 #include &lt;wtf/IsoMallocInlines.h&gt;
<a name="2" id="anc2"></a><span class="line-added">  61 #include &lt;wtf/WeakPtr.h&gt;</span>
  62 
  63 namespace WebCore {
  64 
  65 WTF_MAKE_ISO_ALLOCATED_IMPL(SourceBuffer);
  66 
  67 static const double ExponentialMovingAverageCoefficient = 0.1;
  68 
<a name="3" id="anc3"></a><span class="line-added">  69 // Do not enqueue samples spanning a significant unbuffered gap.</span>
<span class="line-added">  70 // NOTE: one second is somewhat arbitrary. MediaSource::monitorSourceBuffers() is run</span>
<span class="line-added">  71 // on the playbackTimer, which is effectively every 350ms. Allowing &gt; 350ms gap between</span>
<span class="line-added">  72 // enqueued samples allows for situations where we overrun the end of a buffered range</span>
<span class="line-added">  73 // but don&#39;t notice for 350s of playback time, and the client can enqueue data for the</span>
<span class="line-added">  74 // new current time without triggering this early return.</span>
<span class="line-added">  75 // FIXME(135867): Make this gap detection logic less arbitrary.</span>
<span class="line-added">  76 static const MediaTime discontinuityTolerance = MediaTime(1, 1);</span>
<span class="line-added">  77 </span>
  78 struct SourceBuffer::TrackBuffer {
  79     MediaTime lastDecodeTimestamp;
  80     MediaTime greatestDecodeDuration;
  81     MediaTime lastFrameDuration;
  82     MediaTime highestPresentationTimestamp;
<a name="4" id="anc4"></a><span class="line-modified">  83     MediaTime highestEnqueuedPresentationTime;</span>
  84     MediaTime minimumEnqueuedPresentationTime;
  85     DecodeOrderSampleMap::KeyType lastEnqueuedDecodeKey;
<a name="5" id="anc5"></a><span class="line-modified">  86     MediaTime enqueueDiscontinuityBoundary { MediaTime::zeroTime() };</span>
  87     MediaTime roundedTimestampOffset;
  88     uint32_t lastFrameTimescale { 0 };
  89     bool needRandomAccessFlag { true };
  90     bool enabled { false };
  91     bool needsReenqueueing { false };
  92     bool needsMinimumUpcomingPresentationTimeUpdating { false };
  93     SampleMap samples;
  94     DecodeOrderSampleMap::MapType decodeQueue;
  95     RefPtr&lt;MediaDescription&gt; description;
  96     PlatformTimeRanges buffered;
  97 
  98     TrackBuffer()
  99         : lastDecodeTimestamp(MediaTime::invalidTime())
 100         , greatestDecodeDuration(MediaTime::invalidTime())
 101         , lastFrameDuration(MediaTime::invalidTime())
 102         , highestPresentationTimestamp(MediaTime::invalidTime())
<a name="6" id="anc6"></a><span class="line-modified"> 103         , highestEnqueuedPresentationTime(MediaTime::invalidTime())</span>
 104         , lastEnqueuedDecodeKey({MediaTime::invalidTime(), MediaTime::invalidTime()})
<a name="7" id="anc7"></a><span class="line-modified"> 105         , enqueueDiscontinuityBoundary(discontinuityTolerance)</span>
 106     {
 107     }
 108 };
 109 
 110 Ref&lt;SourceBuffer&gt; SourceBuffer::create(Ref&lt;SourceBufferPrivate&gt;&amp;&amp; sourceBufferPrivate, MediaSource* source)
 111 {
 112     auto sourceBuffer = adoptRef(*new SourceBuffer(WTFMove(sourceBufferPrivate), source));
 113     sourceBuffer-&gt;suspendIfNeeded();
 114     return sourceBuffer;
 115 }
 116 
 117 SourceBuffer::SourceBuffer(Ref&lt;SourceBufferPrivate&gt;&amp;&amp; sourceBufferPrivate, MediaSource* source)
 118     : ActiveDOMObject(source-&gt;scriptExecutionContext())
 119     , m_private(WTFMove(sourceBufferPrivate))
 120     , m_source(source)
<a name="8" id="anc8"></a><span class="line-modified"> 121     , m_asyncEventQueue(MainThreadGenericEventQueue::create(*this))</span>
 122     , m_appendBufferTimer(*this, &amp;SourceBuffer::appendBufferTimerFired)
 123     , m_appendWindowStart(MediaTime::zeroTime())
 124     , m_appendWindowEnd(MediaTime::positiveInfiniteTime())
 125     , m_groupStartTimestamp(MediaTime::invalidTime())
 126     , m_groupEndTimestamp(MediaTime::zeroTime())
 127     , m_buffered(TimeRanges::create())
 128     , m_appendState(WaitingForSegment)
 129     , m_timeOfBufferingMonitor(MonotonicTime::now())
 130     , m_pendingRemoveStart(MediaTime::invalidTime())
 131     , m_pendingRemoveEnd(MediaTime::invalidTime())
 132     , m_removeTimer(*this, &amp;SourceBuffer::removeTimerFired)
 133 #if !RELEASE_LOG_DISABLED
 134     , m_logger(m_private-&gt;sourceBufferLogger())
 135     , m_logIdentifier(m_private-&gt;sourceBufferLogIdentifier())
 136 #endif
 137 {
 138     ASSERT(m_source);
 139     ALWAYS_LOG(LOGIDENTIFIER);
 140 
 141     m_private-&gt;setClient(this);
 142 }
 143 
 144 SourceBuffer::~SourceBuffer()
 145 {
 146     ASSERT(isRemoved());
 147     ALWAYS_LOG(LOGIDENTIFIER);
 148 
 149     m_private-&gt;setClient(nullptr);
 150 }
 151 
 152 ExceptionOr&lt;Ref&lt;TimeRanges&gt;&gt; SourceBuffer::buffered() const
 153 {
 154     // Section 3.1 buffered attribute steps.
 155     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#attributes-1
 156     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source then throw an
 157     //    InvalidStateError exception and abort these steps.
 158     if (isRemoved())
 159         return Exception { InvalidStateError };
 160 
 161     // 2. Return a new static normalized TimeRanges object for the media segments buffered.
 162     return m_buffered-&gt;copy();
 163 }
 164 
 165 double SourceBuffer::timestampOffset() const
 166 {
 167     return m_timestampOffset.toDouble();
 168 }
 169 
 170 ExceptionOr&lt;void&gt; SourceBuffer::setTimestampOffset(double offset)
 171 {
 172     // Section 3.1 timestampOffset attribute setter steps.
 173     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#attributes-1
 174     // 1. Let new timestamp offset equal the new value being assigned to this attribute.
 175     // 2. If this object has been removed from the sourceBuffers attribute of the parent media source, then throw an
 176     //    InvalidStateError exception and abort these steps.
 177     // 3. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 178     if (isRemoved() || m_updating)
 179         return Exception { InvalidStateError };
 180 
 181     // 4. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
 182     // 4.1 Set the readyState attribute of the parent media source to &quot;open&quot;
 183     // 4.2 Queue a task to fire a simple event named sourceopen at the parent media source.
 184     m_source-&gt;openIfInEndedState();
 185 
 186     // 5. If the append state equals PARSING_MEDIA_SEGMENT, then throw an InvalidStateError and abort these steps.
 187     if (m_appendState == ParsingMediaSegment)
 188         return Exception { InvalidStateError };
 189 
 190     MediaTime newTimestampOffset = MediaTime::createWithDouble(offset);
 191 
 192     // 6. If the mode attribute equals &quot;sequence&quot;, then set the group start timestamp to new timestamp offset.
 193     if (m_mode == AppendMode::Sequence)
 194         m_groupStartTimestamp = newTimestampOffset;
 195 
 196     // 7. Update the attribute to the new value.
 197     m_timestampOffset = newTimestampOffset;
 198 
 199     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
 200         trackBuffer.lastFrameTimescale = 0;
 201         trackBuffer.roundedTimestampOffset = MediaTime::invalidTime();
 202     }
 203 
 204     return { };
 205 }
 206 
 207 double SourceBuffer::appendWindowStart() const
 208 {
 209     return m_appendWindowStart.toDouble();
 210 }
 211 
 212 ExceptionOr&lt;void&gt; SourceBuffer::setAppendWindowStart(double newValue)
 213 {
 214     // Section 3.1 appendWindowStart attribute setter steps.
 215     // W3C Editor&#39;s Draft 16 September 2016
 216     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-appendwindowstart
 217     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source,
 218     //    then throw an InvalidStateError  exception and abort these steps.
 219     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 220     if (isRemoved() || m_updating)
 221         return Exception { InvalidStateError };
 222 
 223     // 3. If the new value is less than 0 or greater than or equal to appendWindowEnd then
 224     //    throw an TypeError exception and abort these steps.
 225     if (newValue &lt; 0 || newValue &gt;= m_appendWindowEnd.toDouble())
 226         return Exception { TypeError };
 227 
 228     // 4. Update the attribute to the new value.
 229     m_appendWindowStart = MediaTime::createWithDouble(newValue);
 230 
 231     return { };
 232 }
 233 
 234 double SourceBuffer::appendWindowEnd() const
 235 {
 236     return m_appendWindowEnd.toDouble();
 237 }
 238 
 239 ExceptionOr&lt;void&gt; SourceBuffer::setAppendWindowEnd(double newValue)
 240 {
 241     // Section 3.1 appendWindowEnd attribute setter steps.
 242     // W3C Editor&#39;s Draft 16 September 2016
 243     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-appendwindowend
 244     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source,
 245     //    then throw an InvalidStateError exception and abort these steps.
 246     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 247     if (isRemoved() || m_updating)
 248         return Exception { InvalidStateError };
 249 
 250     // 3. If the new value equals NaN, then throw an TypeError and abort these steps.
 251     // 4. If the new value is less than or equal to appendWindowStart then throw an TypeError exception
 252     //    and abort these steps.
 253     if (std::isnan(newValue) || newValue &lt;= m_appendWindowStart.toDouble())
 254         return Exception { TypeError };
 255 
 256     // 5.. Update the attribute to the new value.
 257     m_appendWindowEnd = MediaTime::createWithDouble(newValue);
 258 
 259     return { };
 260 }
 261 
 262 ExceptionOr&lt;void&gt; SourceBuffer::appendBuffer(const BufferSource&amp; data)
 263 {
 264     return appendBufferInternal(static_cast&lt;const unsigned char*&gt;(data.data()), data.length());
 265 }
 266 
 267 void SourceBuffer::resetParserState()
 268 {
 269     // Section 3.5.2 Reset Parser State algorithm steps.
 270     // http://www.w3.org/TR/2014/CR-media-source-20140717/#sourcebuffer-reset-parser-state
 271     // 1. If the append state equals PARSING_MEDIA_SEGMENT and the input buffer contains some complete coded frames,
 272     //    then run the coded frame processing algorithm until all of these complete coded frames have been processed.
 273     // FIXME: If any implementation will work in pulling mode (instead of async push to SourceBufferPrivate, and forget)
 274     //     this should be handled somehow either here, or in m_private-&gt;abort();
 275 
 276     // 2. Unset the last decode timestamp on all track buffers.
 277     // 3. Unset the last frame duration on all track buffers.
 278     // 4. Unset the highest presentation timestamp on all track buffers.
 279     // 5. Set the need random access point flag on all track buffers to true.
 280     for (auto&amp; trackBufferPair : m_trackBufferMap.values()) {
 281         trackBufferPair.lastDecodeTimestamp = MediaTime::invalidTime();
 282         trackBufferPair.greatestDecodeDuration = MediaTime::invalidTime();
 283         trackBufferPair.lastFrameDuration = MediaTime::invalidTime();
 284         trackBufferPair.highestPresentationTimestamp = MediaTime::invalidTime();
 285         trackBufferPair.needRandomAccessFlag = true;
 286     }
 287     // 6. Remove all bytes from the input buffer.
 288     // Note: this is handled by abortIfUpdating()
 289     // 7. Set append state to WAITING_FOR_SEGMENT.
 290     m_appendState = WaitingForSegment;
 291 
 292     m_private-&gt;resetParserState();
 293 }
 294 
 295 ExceptionOr&lt;void&gt; SourceBuffer::abort()
 296 {
 297     // Section 3.2 abort() method steps.
 298     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-abort
 299     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source
 300     //    then throw an InvalidStateError exception and abort these steps.
 301     // 2. If the readyState attribute of the parent media source is not in the &quot;open&quot; state
 302     //    then throw an InvalidStateError exception and abort these steps.
 303     if (isRemoved() || !m_source-&gt;isOpen())
 304         return Exception { InvalidStateError };
 305 
 306     // 3. If the range removal algorithm is running, then throw an InvalidStateError exception and abort these steps.
 307     if (m_removeTimer.isActive())
 308         return Exception { InvalidStateError };
 309 
 310     // 4. If the sourceBuffer.updating attribute equals true, then run the following steps: ...
 311     abortIfUpdating();
 312 
 313     // 5. Run the reset parser state algorithm.
 314     resetParserState();
 315 
 316     // 6. Set appendWindowStart to the presentation start time.
 317     m_appendWindowStart = MediaTime::zeroTime();
 318 
 319     // 7. Set appendWindowEnd to positive Infinity.
 320     m_appendWindowEnd = MediaTime::positiveInfiniteTime();
 321 
 322     return { };
 323 }
 324 
 325 ExceptionOr&lt;void&gt; SourceBuffer::remove(double start, double end)
 326 {
 327     return remove(MediaTime::createWithDouble(start), MediaTime::createWithDouble(end));
 328 }
 329 
 330 ExceptionOr&lt;void&gt; SourceBuffer::remove(const MediaTime&amp; start, const MediaTime&amp; end)
 331 {
 332     DEBUG_LOG(LOGIDENTIFIER, &quot;start = &quot;, start, &quot;, end = &quot;, end);
 333 
 334     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-remove
 335     // Section 3.2 remove() method steps.
 336     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source then throw
 337     //    an InvalidStateError exception and abort these steps.
 338     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 339     if (isRemoved() || m_updating)
 340         return Exception { InvalidStateError };
 341 
 342     // 3. If duration equals NaN, then throw a TypeError exception and abort these steps.
 343     // 4. If start is negative or greater than duration, then throw a TypeError exception and abort these steps.
 344     // 5. If end is less than or equal to start or end equals NaN, then throw a TypeError exception and abort these steps.
 345     if (m_source-&gt;duration().isInvalid()
 346         || end.isInvalid()
 347         || start.isInvalid()
 348         || start &lt; MediaTime::zeroTime()
 349         || start &gt; m_source-&gt;duration()
 350         || end &lt;= start) {
 351         return Exception { TypeError };
 352     }
 353 
 354     // 6. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
 355     // 6.1. Set the readyState attribute of the parent media source to &quot;open&quot;
 356     // 6.2. Queue a task to fire a simple event named sourceopen at the parent media source .
 357     m_source-&gt;openIfInEndedState();
 358 
 359     // 7. Run the range removal algorithm with start and end as the start and end of the removal range.
 360     rangeRemoval(start, end);
 361 
 362     return { };
 363 }
 364 
 365 void SourceBuffer::rangeRemoval(const MediaTime&amp; start, const MediaTime&amp; end)
 366 {
 367     // 3.5.7 Range Removal
 368     // https://rawgit.com/w3c/media-source/7bbe4aa33c61ec025bc7acbd80354110f6a000f9/media-source.html#sourcebuffer-range-removal
 369     // 1. Let start equal the starting presentation timestamp for the removal range.
 370     // 2. Let end equal the end presentation timestamp for the removal range.
 371     // 3. Set the updating attribute to true.
 372     m_updating = true;
 373 
 374     // 4. Queue a task to fire a simple event named updatestart at this SourceBuffer object.
 375     scheduleEvent(eventNames().updatestartEvent);
 376 
 377     // 5. Return control to the caller and run the rest of the steps asynchronously.
 378     m_pendingRemoveStart = start;
 379     m_pendingRemoveEnd = end;
 380     m_removeTimer.startOneShot(0_s);
 381 }
 382 
 383 ExceptionOr&lt;void&gt; SourceBuffer::changeType(const String&amp; type)
 384 {
 385     // changeType() proposed API. See issue #155: &lt;https://github.com/w3c/media-source/issues/155&gt;
 386     // https://rawgit.com/wicg/media-source/codec-switching/index.html#dom-sourcebuffer-changetype
 387 
 388     // 1. If type is an empty string then throw a TypeError exception and abort these steps.
 389     if (type.isEmpty())
 390         return Exception { TypeError };
 391 
 392     // 2. If this object has been removed from the sourceBuffers attribute of the parent media source,
 393     // then throw an InvalidStateError exception and abort these steps.
 394     // 3. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 395     if (isRemoved() || m_updating)
 396         return Exception { InvalidStateError };
 397 
 398     // 4. If type contains a MIME type that is not supported or contains a MIME type that is not supported with
 399     // the types specified (currently or previously) of SourceBuffer objects in the sourceBuffers attribute of
 400     // the parent media source, then throw a NotSupportedError exception and abort these steps.
 401     ContentType contentType(type);
 402     if (!m_private-&gt;canSwitchToType(contentType))
 403         return Exception { NotSupportedError };
 404 
 405     // 5. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following
 406     // steps:
 407     // 5.1. Set the readyState attribute of the parent media source to &quot;open&quot;
 408     // 5.2. Queue a task to fire a simple event named sourceopen at the parent media source.
 409     m_source-&gt;openIfInEndedState();
 410 
 411     // 6. Run the reset parser state algorithm.
 412     resetParserState();
 413 
 414     // 7. Update the generate timestamps flag on this SourceBuffer object to the value in the &quot;Generate Timestamps
 415     // Flag&quot; column of the byte stream format registry [MSE-REGISTRY] entry that is associated with type.
 416     setShouldGenerateTimestamps(MediaSource::contentTypeShouldGenerateTimestamps(contentType));
 417 
 418     // ↳ If the generate timestamps flag equals true:
 419     // Set the mode attribute on this SourceBuffer object to &quot;sequence&quot;, including running the associated steps
 420     // for that attribute being set.
 421     if (m_shouldGenerateTimestamps)
 422         setMode(AppendMode::Sequence);
 423 
 424     // ↳ Otherwise:
 425     // Keep the previous value of the mode attribute on this SourceBuffer object, without running any associated
 426     // steps for that attribute being set.
 427     // NOTE: No-op.
 428 
 429     // 9. Set pending initialization segment for changeType flag to true.
 430     m_pendingInitializationSegmentForChangeType = true;
 431 
 432     return { };
 433 }
 434 
 435 void SourceBuffer::abortIfUpdating()
 436 {
 437     // Section 3.2 abort() method step 4 substeps.
 438     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-abort
 439 
 440     if (!m_updating)
 441         return;
 442 
 443     // 4.1. Abort the buffer append algorithm if it is running.
 444     m_appendBufferTimer.stop();
 445     m_pendingAppendData.clear();
 446     m_private-&gt;abort();
 447 
 448     // 4.2. Set the updating attribute to false.
 449     m_updating = false;
 450 
 451     // 4.3. Queue a task to fire a simple event named abort at this SourceBuffer object.
 452     scheduleEvent(eventNames().abortEvent);
 453 
 454     // 4.4. Queue a task to fire a simple event named updateend at this SourceBuffer object.
 455     scheduleEvent(eventNames().updateendEvent);
 456 }
 457 
 458 MediaTime SourceBuffer::highestPresentationTimestamp() const
 459 {
 460     MediaTime highestTime;
 461     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
 462         auto lastSampleIter = trackBuffer.samples.presentationOrder().rbegin();
 463         if (lastSampleIter == trackBuffer.samples.presentationOrder().rend())
 464             continue;
 465         highestTime = std::max(highestTime, lastSampleIter-&gt;first);
 466     }
 467     return highestTime;
 468 }
 469 
 470 void SourceBuffer::readyStateChanged()
 471 {
 472     updateBufferedFromTrackBuffers();
 473 }
 474 
 475 void SourceBuffer::removedFromMediaSource()
 476 {
 477     if (isRemoved())
 478         return;
 479 
 480     abortIfUpdating();
 481 
 482     for (auto&amp; trackBufferPair : m_trackBufferMap.values()) {
 483         trackBufferPair.samples.clear();
 484         trackBufferPair.decodeQueue.clear();
 485     }
 486 
 487     m_private-&gt;removedFromMediaSource();
 488     m_source = nullptr;
 489 }
 490 
 491 void SourceBuffer::seekToTime(const MediaTime&amp; time)
 492 {
 493     ALWAYS_LOG(LOGIDENTIFIER, time);
 494 
 495     for (auto&amp; trackBufferPair : m_trackBufferMap) {
 496         TrackBuffer&amp; trackBuffer = trackBufferPair.value;
 497         const AtomString&amp; trackID = trackBufferPair.key;
 498 
 499         trackBuffer.needsReenqueueing = true;
 500         reenqueueMediaForTime(trackBuffer, trackID, time);
 501     }
 502 }
 503 
 504 MediaTime SourceBuffer::sourceBufferPrivateFastSeekTimeForMediaTime(const MediaTime&amp; targetTime, const MediaTime&amp; negativeThreshold, const MediaTime&amp; positiveThreshold)
 505 {
 506     MediaTime seekTime = targetTime;
<a name="9" id="anc9"></a>

 507 
 508     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
 509         // Find the sample which contains the target time time.
 510         auto futureSyncSampleIterator = trackBuffer.samples.decodeOrder().findSyncSampleAfterPresentationTime(targetTime, positiveThreshold);
 511         auto pastSyncSampleIterator = trackBuffer.samples.decodeOrder().findSyncSamplePriorToPresentationTime(targetTime, negativeThreshold);
 512         auto upperBound = trackBuffer.samples.decodeOrder().end();
 513         auto lowerBound = trackBuffer.samples.decodeOrder().rend();
 514 
 515         if (futureSyncSampleIterator == upperBound &amp;&amp; pastSyncSampleIterator == lowerBound)
 516             continue;
 517 
 518         MediaTime futureSeekTime = MediaTime::positiveInfiniteTime();
 519         if (futureSyncSampleIterator != upperBound) {
 520             RefPtr&lt;MediaSample&gt;&amp; sample = futureSyncSampleIterator-&gt;second;
 521             futureSeekTime = sample-&gt;presentationTime();
 522         }
 523 
 524         MediaTime pastSeekTime = MediaTime::negativeInfiniteTime();
 525         if (pastSyncSampleIterator != lowerBound) {
 526             RefPtr&lt;MediaSample&gt;&amp; sample = pastSyncSampleIterator-&gt;second;
 527             pastSeekTime = sample-&gt;presentationTime();
 528         }
 529 
 530         MediaTime trackSeekTime = abs(targetTime - futureSeekTime) &lt; abs(targetTime - pastSeekTime) ? futureSeekTime : pastSeekTime;
 531         if (abs(targetTime - trackSeekTime) &gt; abs(targetTime - seekTime))
 532             seekTime = trackSeekTime;
 533     }
 534 
 535     return seekTime;
 536 }
 537 
 538 bool SourceBuffer::hasPendingActivity() const
 539 {
<a name="10" id="anc10"></a><span class="line-modified"> 540     return m_source || m_asyncEventQueue-&gt;hasPendingEvents();</span>



















 541 }
 542 
 543 void SourceBuffer::stop()
 544 {
<a name="11" id="anc11"></a>
 545     m_appendBufferTimer.stop();
 546     m_removeTimer.stop();
 547 }
 548 
<a name="12" id="anc12"></a>




 549 const char* SourceBuffer::activeDOMObjectName() const
 550 {
 551     return &quot;SourceBuffer&quot;;
 552 }
 553 
 554 bool SourceBuffer::isRemoved() const
 555 {
 556     return !m_source;
 557 }
 558 
 559 void SourceBuffer::scheduleEvent(const AtomString&amp; eventName)
 560 {
 561     auto event = Event::create(eventName, Event::CanBubble::No, Event::IsCancelable::No);
 562     event-&gt;setTarget(this);
 563 
<a name="13" id="anc13"></a><span class="line-modified"> 564     m_asyncEventQueue-&gt;enqueueEvent(WTFMove(event));</span>
 565 }
 566 
 567 ExceptionOr&lt;void&gt; SourceBuffer::appendBufferInternal(const unsigned char* data, unsigned size)
 568 {
 569     // Section 3.2 appendBuffer()
 570     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#widl-SourceBuffer-appendBuffer-void-ArrayBufferView-data
 571 
 572     // Step 1 is enforced by the caller.
 573     // 2. Run the prepare append algorithm.
 574     // Section 3.5.4 Prepare AppendAlgorithm
 575 
 576     // 1. If the SourceBuffer has been removed from the sourceBuffers attribute of the parent media source
 577     // then throw an InvalidStateError exception and abort these steps.
 578     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 579     if (isRemoved() || m_updating)
 580         return Exception { InvalidStateError };
 581 
 582     // 3. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
 583     // 3.1. Set the readyState attribute of the parent media source to &quot;open&quot;
 584     // 3.2. Queue a task to fire a simple event named sourceopen at the parent media source .
 585     m_source-&gt;openIfInEndedState();
 586 
 587     // 4. Run the coded frame eviction algorithm.
 588     evictCodedFrames(size);
 589 
 590     // FIXME: enable this code when MSE libraries have been updated to support it.
 591 #if USE(GSTREAMER)
 592     // 5. If the buffer full flag equals true, then throw a QuotaExceededError exception and abort these step.
 593     if (m_bufferFull) {
 594         ERROR_LOG(LOGIDENTIFIER, &quot;buffer full, failing with QuotaExceededError error&quot;);
 595         return Exception { QuotaExceededError };
 596     }
 597 #endif
 598 
 599     // NOTE: Return to 3.2 appendBuffer()
 600     // 3. Add data to the end of the input buffer.
 601     m_pendingAppendData.append(data, size);
 602 
 603     // 4. Set the updating attribute to true.
 604     m_updating = true;
 605 
 606     // 5. Queue a task to fire a simple event named updatestart at this SourceBuffer object.
 607     scheduleEvent(eventNames().updatestartEvent);
 608 
 609     // 6. Asynchronously run the buffer append algorithm.
 610     m_appendBufferTimer.startOneShot(0_s);
 611 
 612     reportExtraMemoryAllocated();
 613 
 614     return { };
 615 }
 616 
 617 void SourceBuffer::appendBufferTimerFired()
 618 {
 619     if (isRemoved())
 620         return;
 621 
 622     ASSERT(m_updating);
 623 
 624     // Section 3.5.5 Buffer Append Algorithm
 625     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#sourcebuffer-buffer-append
 626 
 627     // 1. Run the segment parser loop algorithm.
 628 
 629     // Section 3.5.1 Segment Parser Loop
 630     // https://dvcs.w3.org/hg/html-media/raw-file/tip/media-source/media-source.html#sourcebuffer-segment-parser-loop
 631     // When the segment parser loop algorithm is invoked, run the following steps:
 632 
 633     // 1. Loop Top: If the input buffer is empty, then jump to the need more data step below.
 634     if (!m_pendingAppendData.size()) {
 635         sourceBufferPrivateAppendComplete(AppendSucceeded);
 636         return;
 637     }
 638 
 639     // Manually clear out the m_pendingAppendData Vector, in case the platform implementation
 640     // rejects appending the buffer for whatever reason.
 641     // FIXME: The implementation should guarantee the move from this Vector, and we should
 642     // assert here to confirm that. See https://bugs.webkit.org/show_bug.cgi?id=178003.
 643     m_private-&gt;append(WTFMove(m_pendingAppendData));
 644     m_pendingAppendData.clear();
 645 }
 646 
 647 void SourceBuffer::sourceBufferPrivateAppendComplete(AppendResult result)
 648 {
 649     if (isRemoved())
 650         return;
 651 
 652     // Resolve the changes it TrackBuffers&#39; buffered ranges
 653     // into the SourceBuffer&#39;s buffered ranges
 654     updateBufferedFromTrackBuffers();
 655 
 656     // Section 3.5.5 Buffer Append Algorithm, ctd.
 657     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#sourcebuffer-buffer-append
 658 
 659     // 2. If the input buffer contains bytes that violate the SourceBuffer byte stream format specification,
 660     // then run the append error algorithm with the decode error parameter set to true and abort this algorithm.
 661     if (result == ParsingFailed) {
 662         ERROR_LOG(LOGIDENTIFIER, &quot;ParsingFailed&quot;);
 663         appendError(true);
 664         return;
 665     }
 666 
 667     // NOTE: Steps 3 - 6 enforced by sourceBufferPrivateDidReceiveInitializationSegment() and
 668     // sourceBufferPrivateDidReceiveSample below.
 669 
 670     // 7. Need more data: Return control to the calling algorithm.
 671 
 672     // NOTE: return to Section 3.5.5
 673     // 2.If the segment parser loop algorithm in the previous step was aborted, then abort this algorithm.
 674     if (result != AppendSucceeded)
 675         return;
 676 
 677     // 3. Set the updating attribute to false.
 678     m_updating = false;
 679 
 680     // 4. Queue a task to fire a simple event named update at this SourceBuffer object.
 681     scheduleEvent(eventNames().updateEvent);
 682 
 683     // 5. Queue a task to fire a simple event named updateend at this SourceBuffer object.
 684     scheduleEvent(eventNames().updateendEvent);
 685 
 686     if (m_source)
 687         m_source-&gt;monitorSourceBuffers();
 688 
 689     MediaTime currentMediaTime = m_source-&gt;currentTime();
 690     for (auto&amp; trackBufferPair : m_trackBufferMap) {
 691         TrackBuffer&amp; trackBuffer = trackBufferPair.value;
 692         const AtomString&amp; trackID = trackBufferPair.key;
 693 
 694         if (trackBuffer.needsReenqueueing) {
 695             DEBUG_LOG(LOGIDENTIFIER, &quot;reenqueuing at time &quot;, currentMediaTime);
 696             reenqueueMediaForTime(trackBuffer, trackID, currentMediaTime);
 697         } else
 698             provideMediaData(trackBuffer, trackID);
 699     }
 700 
 701     reportExtraMemoryAllocated();
 702     if (extraMemoryCost() &gt; this-&gt;maximumBufferSize())
 703         m_bufferFull = true;
 704 
 705     DEBUG_LOG(LOGIDENTIFIER);
 706 }
 707 
 708 void SourceBuffer::sourceBufferPrivateDidReceiveRenderingError(int error)
 709 {
 710 #if RELEASE_LOG_DISABLED
 711     UNUSED_PARAM(error);
 712 #endif
 713 
 714     ERROR_LOG(LOGIDENTIFIER, error);
 715 
 716     if (!isRemoved())
 717         m_source-&gt;streamEndedWithError(MediaSource::EndOfStreamError::Decode);
 718 }
 719 
 720 static bool decodeTimeComparator(const PresentationOrderSampleMap::MapType::value_type&amp; a, const PresentationOrderSampleMap::MapType::value_type&amp; b)
 721 {
 722     return a.second-&gt;decodeTime() &lt; b.second-&gt;decodeTime();
 723 }
 724 
 725 static PlatformTimeRanges removeSamplesFromTrackBuffer(const DecodeOrderSampleMap::MapType&amp; samples, SourceBuffer::TrackBuffer&amp; trackBuffer, const SourceBuffer* buffer, const char* logPrefix)
 726 {
 727 #if !RELEASE_LOG_DISABLED
 728     MediaTime earliestSample = MediaTime::positiveInfiniteTime();
 729     MediaTime latestSample = MediaTime::zeroTime();
 730     size_t bytesRemoved = 0;
 731     auto logIdentifier = WTF::Logger::LogSiteIdentifier(buffer-&gt;logClassName(), logPrefix, buffer-&gt;logIdentifier());
 732     auto&amp; logger = buffer-&gt;logger();
 733     auto willLog = logger.willLog(buffer-&gt;logChannel(), WTFLogLevel::Debug);
 734 #else
 735     UNUSED_PARAM(logPrefix);
 736     UNUSED_PARAM(buffer);
 737 #endif
 738 
 739     PlatformTimeRanges erasedRanges;
 740     for (const auto&amp; sampleIt : samples) {
 741         const DecodeOrderSampleMap::KeyType&amp; decodeKey = sampleIt.first;
 742 #if !RELEASE_LOG_DISABLED
 743         size_t startBufferSize = trackBuffer.samples.sizeInBytes();
 744 #endif
 745 
 746         const RefPtr&lt;MediaSample&gt;&amp; sample = sampleIt.second;
 747 
 748 #if !RELEASE_LOG_DISABLED
 749         if (willLog)
 750             logger.debug(buffer-&gt;logChannel(), logIdentifier, &quot;removing sample &quot;, *sampleIt.second);
 751 #endif
 752 
 753         // Remove the erased samples from the TrackBuffer sample map.
 754         trackBuffer.samples.removeSample(sample.get());
 755 
 756         // Also remove the erased samples from the TrackBuffer decodeQueue.
 757         trackBuffer.decodeQueue.erase(decodeKey);
 758 
 759         auto startTime = sample-&gt;presentationTime();
 760         auto endTime = startTime + sample-&gt;duration();
 761         erasedRanges.add(startTime, endTime);
 762 
 763 #if !RELEASE_LOG_DISABLED
 764         bytesRemoved += startBufferSize - trackBuffer.samples.sizeInBytes();
 765         if (startTime &lt; earliestSample)
 766             earliestSample = startTime;
 767         if (endTime &gt; latestSample)
 768             latestSample = endTime;
 769 #endif
 770     }
 771 
 772     // Because we may have added artificial padding in the buffered ranges when adding samples, we may
 773     // need to remove that padding when removing those same samples. Walk over the erased ranges looking
 774     // for unbuffered areas and expand erasedRanges to encompass those areas.
 775     PlatformTimeRanges additionalErasedRanges;
 776     for (unsigned i = 0; i &lt; erasedRanges.length(); ++i) {
 777         auto erasedStart = erasedRanges.start(i);
 778         auto erasedEnd = erasedRanges.end(i);
 779         auto startIterator = trackBuffer.samples.presentationOrder().reverseFindSampleBeforePresentationTime(erasedStart);
 780         if (startIterator == trackBuffer.samples.presentationOrder().rend())
 781             additionalErasedRanges.add(MediaTime::zeroTime(), erasedStart);
 782         else {
 783             auto&amp; previousSample = *startIterator-&gt;second;
 784             if (previousSample.presentationTime() + previousSample.duration() &lt; erasedStart)
 785                 additionalErasedRanges.add(previousSample.presentationTime() + previousSample.duration(), erasedStart);
 786         }
 787 
 788         auto endIterator = trackBuffer.samples.presentationOrder().findSampleStartingOnOrAfterPresentationTime(erasedEnd);
 789         if (endIterator == trackBuffer.samples.presentationOrder().end())
 790             additionalErasedRanges.add(erasedEnd, MediaTime::positiveInfiniteTime());
 791         else {
 792             auto&amp; nextSample = *endIterator-&gt;second;
 793             if (nextSample.presentationTime() &gt; erasedEnd)
 794                 additionalErasedRanges.add(erasedEnd, nextSample.presentationTime());
 795         }
 796     }
 797     if (additionalErasedRanges.length())
 798         erasedRanges.unionWith(additionalErasedRanges);
 799 
 800 #if !RELEASE_LOG_DISABLED
 801     if (bytesRemoved &amp;&amp; willLog)
 802         logger.debug(buffer-&gt;logChannel(), logIdentifier, &quot;removed &quot;, bytesRemoved, &quot;, start = &quot;, earliestSample, &quot;, end = &quot;, latestSample);
 803 #endif
 804 
 805     return erasedRanges;
 806 }
 807 
 808 void SourceBuffer::removeCodedFrames(const MediaTime&amp; start, const MediaTime&amp; end)
 809 {
 810     DEBUG_LOG(LOGIDENTIFIER, &quot;start = &quot;, start, &quot;, end = &quot;, end);
 811 
<a name="14" id="anc14"></a><span class="line-added"> 812     ASSERT(start &lt; end);</span>
<span class="line-added"> 813     if (start &gt;= end)</span>
<span class="line-added"> 814         return;</span>
<span class="line-added"> 815 </span>
 816     // 3.5.9 Coded Frame Removal Algorithm
 817     // https://dvcs.w3.org/hg/html-media/raw-file/tip/media-source/media-source.html#sourcebuffer-coded-frame-removal
 818 
 819     // 1. Let start be the starting presentation timestamp for the removal range.
<a name="15" id="anc15"></a>
 820     MediaTime currentMediaTime = m_source-&gt;currentTime();
 821 
 822     // 2. Let end be the end presentation timestamp for the removal range.
 823     // 3. For each track buffer in this source buffer, run the following steps:
 824     for (auto&amp; trackBufferKeyValue : m_trackBufferMap) {
 825         TrackBuffer&amp; trackBuffer = trackBufferKeyValue.value;
 826         AtomString trackID = trackBufferKeyValue.key;
 827 
 828         // 3.1. Let remove end timestamp be the current value of duration
 829         // 3.2 If this track buffer has a random access point timestamp that is greater than or equal to end, then update
 830         // remove end timestamp to that random access point timestamp.
 831         // NOTE: Step 3.2 will be incorrect for any random access point timestamp whose decode time is later than the sample at end,
 832         // but whose presentation time is less than the sample at end. Skip this step until step 3.3 below.
 833 
 834         // NOTE: To handle MediaSamples which may be an amalgamation of multiple shorter samples, find samples whose presentation
 835         // interval straddles the start and end times, and divide them if possible:
 836         auto divideSampleIfPossibleAtPresentationTime = [&amp;] (const MediaTime&amp; time) {
 837             auto sampleIterator = trackBuffer.samples.presentationOrder().findSampleContainingPresentationTime(time);
 838             if (sampleIterator == trackBuffer.samples.presentationOrder().end())
 839                 return;
 840             RefPtr&lt;MediaSample&gt; sample = sampleIterator-&gt;second;
 841             if (!sample-&gt;isDivisable())
 842                 return;
 843             std::pair&lt;RefPtr&lt;MediaSample&gt;, RefPtr&lt;MediaSample&gt;&gt; replacementSamples = sample-&gt;divide(time);
 844             if (!replacementSamples.first || !replacementSamples.second)
 845                 return;
 846             DEBUG_LOG(LOGIDENTIFIER, &quot;splitting sample &quot;, *sample, &quot; into &quot;, *replacementSamples.first, &quot; and &quot;, *replacementSamples.second);
 847             trackBuffer.samples.removeSample(sample.get());
 848             trackBuffer.samples.addSample(*replacementSamples.first);
 849             trackBuffer.samples.addSample(*replacementSamples.second);
 850         };
 851         divideSampleIfPossibleAtPresentationTime(start);
 852         divideSampleIfPossibleAtPresentationTime(end);
 853 
 854         auto removePresentationStart = trackBuffer.samples.presentationOrder().findSampleContainingOrAfterPresentationTime(start);
 855         auto removePresentationEnd = trackBuffer.samples.presentationOrder().findSampleStartingOnOrAfterPresentationTime(end);
 856         if (removePresentationStart == removePresentationEnd)
 857             continue;
 858 
 859         // 3.3 Remove all media data, from this track buffer, that contain starting timestamps greater than or equal to
 860         // start and less than the remove end timestamp.
 861         // NOTE: frames must be removed in decode order, so that all dependant frames between the frame to be removed
 862         // and the next sync sample frame are removed. But we must start from the first sample in decode order, not
 863         // presentation order.
 864         auto minmaxDecodeTimeIterPair = std::minmax_element(removePresentationStart, removePresentationEnd, decodeTimeComparator);
 865         auto&amp; firstSample = *minmaxDecodeTimeIterPair.first-&gt;second;
 866         auto&amp; lastSample = *minmaxDecodeTimeIterPair.second-&gt;second;
 867         auto removeDecodeStart = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey({firstSample.decodeTime(), firstSample.presentationTime()});
 868         auto removeDecodeLast = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey({lastSample.decodeTime(), lastSample.presentationTime()});
 869         auto removeDecodeEnd = trackBuffer.samples.decodeOrder().findSyncSampleAfterDecodeIterator(removeDecodeLast);
 870 
 871         DecodeOrderSampleMap::MapType erasedSamples(removeDecodeStart, removeDecodeEnd);
 872         PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(erasedSamples, trackBuffer, this, &quot;removeCodedFrames&quot;);
 873 
 874         // Only force the TrackBuffer to re-enqueue if the removed ranges overlap with enqueued and possibly
 875         // not yet displayed samples.
<a name="16" id="anc16"></a><span class="line-modified"> 876         if (trackBuffer.highestEnqueuedPresentationTime.isValid() &amp;&amp; currentMediaTime &lt; trackBuffer.highestEnqueuedPresentationTime) {</span>
<span class="line-modified"> 877             PlatformTimeRanges possiblyEnqueuedRanges(currentMediaTime, trackBuffer.highestEnqueuedPresentationTime);</span>
 878             possiblyEnqueuedRanges.intersectWith(erasedRanges);
 879             if (possiblyEnqueuedRanges.length()) {
 880                 trackBuffer.needsReenqueueing = true;
 881                 DEBUG_LOG(LOGIDENTIFIER, &quot;the range in removeCodedFrames() includes already enqueued samples, reenqueueing from &quot;, currentMediaTime);
 882                 reenqueueMediaForTime(trackBuffer, trackID, currentMediaTime);
 883             }
 884         }
 885 
 886         erasedRanges.invert();
 887         trackBuffer.buffered.intersectWith(erasedRanges);
 888         setBufferedDirty(true);
 889 
 890         // 3.4 If this object is in activeSourceBuffers, the current playback position is greater than or equal to start
 891         // and less than the remove end timestamp, and HTMLMediaElement.readyState is greater than HAVE_METADATA, then set
 892         // the HTMLMediaElement.readyState attribute to HAVE_METADATA and stall playback.
<a name="17" id="anc17"></a><span class="line-modified"> 893         if (m_active &amp;&amp; currentMediaTime &gt;= start &amp;&amp; currentMediaTime &lt; end &amp;&amp; m_private-&gt;readyState() &gt; MediaPlayer::ReadyState::HaveMetadata)</span>
<span class="line-modified"> 894             m_private-&gt;setReadyState(MediaPlayer::ReadyState::HaveMetadata);</span>
 895     }
 896 
 897     updateBufferedFromTrackBuffers();
 898 
 899     // 4. If buffer full flag equals true and this object is ready to accept more bytes, then set the buffer full flag to false.
 900     // No-op
 901 
 902     LOG(Media, &quot;SourceBuffer::removeCodedFrames(%p) - buffered = %s&quot;, this, toString(m_buffered-&gt;ranges()).utf8().data());
 903 }
 904 
 905 void SourceBuffer::removeTimerFired()
 906 {
 907     if (isRemoved())
 908         return;
 909 
 910     ASSERT(m_updating);
 911     ASSERT(m_pendingRemoveStart.isValid());
 912     ASSERT(m_pendingRemoveStart &lt; m_pendingRemoveEnd);
 913 
 914     // Section 3.5.7 Range Removal
 915     // http://w3c.github.io/media-source/#sourcebuffer-range-removal
 916 
 917     // 6. Run the coded frame removal algorithm with start and end as the start and end of the removal range.
 918     removeCodedFrames(m_pendingRemoveStart, m_pendingRemoveEnd);
 919 
 920     // 7. Set the updating attribute to false.
 921     m_updating = false;
 922     m_pendingRemoveStart = MediaTime::invalidTime();
 923     m_pendingRemoveEnd = MediaTime::invalidTime();
 924 
 925     // 8. Queue a task to fire a simple event named update at this SourceBuffer object.
 926     scheduleEvent(eventNames().updateEvent);
 927 
 928     // 9. Queue a task to fire a simple event named updateend at this SourceBuffer object.
 929     scheduleEvent(eventNames().updateendEvent);
 930 }
 931 
 932 void SourceBuffer::evictCodedFrames(size_t newDataSize)
 933 {
 934     // 3.5.13 Coded Frame Eviction Algorithm
 935     // http://www.w3.org/TR/media-source/#sourcebuffer-coded-frame-eviction
 936 
 937     if (isRemoved())
 938         return;
 939 
 940     // This algorithm is run to free up space in this source buffer when new data is appended.
 941     // 1. Let new data equal the data that is about to be appended to this SourceBuffer.
 942     // 2. If the buffer full flag equals false, then abort these steps.
 943     if (!m_bufferFull)
 944         return;
 945 
 946     size_t maximumBufferSize = this-&gt;maximumBufferSize();
 947 
 948     // 3. Let removal ranges equal a list of presentation time ranges that can be evicted from
 949     // the presentation to make room for the new data.
 950 
 951     // NOTE: begin by removing data from the beginning of the buffered ranges, 30 seconds at
 952     // a time, up to 30 seconds before currentTime.
 953     MediaTime thirtySeconds = MediaTime(30, 1);
 954     MediaTime currentTime = m_source-&gt;currentTime();
 955     MediaTime maximumRangeEnd = currentTime - thirtySeconds;
 956 
 957 #if !RELEASE_LOG_DISABLED
 958     DEBUG_LOG(LOGIDENTIFIER, &quot;currentTime = &quot;, m_source-&gt;currentTime(), &quot;, require &quot;, extraMemoryCost() + newDataSize, &quot; bytes, maximum buffer size is &quot;, maximumBufferSize);
 959     size_t initialBufferedSize = extraMemoryCost();
 960 #endif
 961 
 962     MediaTime rangeStart = MediaTime::zeroTime();
 963     MediaTime rangeEnd = rangeStart + thirtySeconds;
 964     while (rangeStart &lt; maximumRangeEnd) {
 965         // 4. For each range in removal ranges, run the coded frame removal algorithm with start and
 966         // end equal to the removal range start and end timestamp respectively.
 967         removeCodedFrames(rangeStart, std::min(rangeEnd, maximumRangeEnd));
 968         if (extraMemoryCost() + newDataSize &lt; maximumBufferSize) {
 969             m_bufferFull = false;
 970             break;
 971         }
 972 
 973         rangeStart += thirtySeconds;
 974         rangeEnd += thirtySeconds;
 975     }
 976 
 977 #if !RELEASE_LOG_DISABLED
 978     if (!m_bufferFull) {
 979         DEBUG_LOG(LOGIDENTIFIER, &quot;evicted &quot;, initialBufferedSize - extraMemoryCost());
 980         return;
 981     }
 982 #endif
 983 
 984     // If there still isn&#39;t enough free space and there buffers in time ranges after the current range (ie. there is a gap after
 985     // the current buffered range), delete 30 seconds at a time from duration back to the current time range or 30 seconds after
 986     // currenTime whichever we hit first.
 987     auto buffered = m_buffered-&gt;ranges();
 988     size_t currentTimeRange = buffered.find(currentTime);
 989     if (currentTimeRange == buffered.length() - 1) {
 990 #if !RELEASE_LOG_DISABLED
 991         ERROR_LOG(LOGIDENTIFIER, &quot;FAILED to free enough after evicting &quot;, initialBufferedSize - extraMemoryCost());
 992 #endif
 993         return;
 994     }
 995 
 996     MediaTime minimumRangeStart = currentTime + thirtySeconds;
 997 
 998     rangeEnd = m_source-&gt;duration();
 999     rangeStart = rangeEnd - thirtySeconds;
1000     while (rangeStart &gt; minimumRangeStart) {
1001 
1002         // Do not evict data from the time range that contains currentTime.
1003         size_t startTimeRange = buffered.find(rangeStart);
1004         if (currentTimeRange != notFound &amp;&amp; startTimeRange == currentTimeRange) {
1005             size_t endTimeRange = buffered.find(rangeEnd);
1006             if (currentTimeRange != notFound &amp;&amp; endTimeRange == currentTimeRange)
1007                 break;
1008 
1009             rangeEnd = buffered.start(endTimeRange);
1010         }
1011 
1012         // 4. For each range in removal ranges, run the coded frame removal algorithm with start and
1013         // end equal to the removal range start and end timestamp respectively.
1014         removeCodedFrames(std::max(minimumRangeStart, rangeStart), rangeEnd);
1015         if (extraMemoryCost() + newDataSize &lt; maximumBufferSize) {
1016             m_bufferFull = false;
1017             break;
1018         }
1019 
1020         rangeStart -= thirtySeconds;
1021         rangeEnd -= thirtySeconds;
1022     }
1023 
1024 #if !RELEASE_LOG_DISABLED
1025     if (m_bufferFull)
1026         ERROR_LOG(LOGIDENTIFIER, &quot;FAILED to free enough after evicting &quot;, initialBufferedSize - extraMemoryCost());
1027     else
1028         DEBUG_LOG(LOGIDENTIFIER, &quot;evicted &quot;, initialBufferedSize - extraMemoryCost());
1029 #endif
1030 }
1031 
1032 size_t SourceBuffer::maximumBufferSize() const
1033 {
1034     if (isRemoved())
1035         return 0;
1036 
1037     auto* element = m_source-&gt;mediaElement();
1038     if (!element)
1039         return 0;
1040 
1041     return element-&gt;maximumSourceBufferSize(*this);
1042 }
1043 
1044 VideoTrackList&amp; SourceBuffer::videoTracks()
1045 {
1046     if (!m_videoTracks)
<a name="18" id="anc18"></a><span class="line-modified">1047         m_videoTracks = VideoTrackList::create(makeWeakPtr(m_source-&gt;mediaElement()), scriptExecutionContext());</span>
1048     return *m_videoTracks;
1049 }
1050 
1051 AudioTrackList&amp; SourceBuffer::audioTracks()
1052 {
1053     if (!m_audioTracks)
<a name="19" id="anc19"></a><span class="line-modified">1054         m_audioTracks = AudioTrackList::create(makeWeakPtr(m_source-&gt;mediaElement()), scriptExecutionContext());</span>
1055     return *m_audioTracks;
1056 }
1057 
1058 TextTrackList&amp; SourceBuffer::textTracks()
1059 {
1060     if (!m_textTracks)
<a name="20" id="anc20"></a><span class="line-modified">1061         m_textTracks = TextTrackList::create(makeWeakPtr(m_source-&gt;mediaElement()), scriptExecutionContext());</span>
1062     return *m_textTracks;
1063 }
1064 
1065 void SourceBuffer::setActive(bool active)
1066 {
1067     if (m_active == active)
1068         return;
1069 
1070     m_active = active;
1071     m_private-&gt;setActive(active);
1072     if (!isRemoved())
1073         m_source-&gt;sourceBufferDidChangeActiveState(*this, active);
1074 }
1075 
1076 void SourceBuffer::sourceBufferPrivateDidReceiveInitializationSegment(const InitializationSegment&amp; segment)
1077 {
1078     if (isRemoved())
1079         return;
1080 
1081     ALWAYS_LOG(LOGIDENTIFIER);
1082 
1083     // 3.5.8 Initialization Segment Received (ctd)
1084     // https://rawgit.com/w3c/media-source/c3ad59c7a370d04430969ba73d18dc9bcde57a33/index.html#sourcebuffer-init-segment-received [Editor&#39;s Draft 09 January 2015]
1085 
1086     // 1. Update the duration attribute if it currently equals NaN:
1087     if (m_source-&gt;duration().isInvalid()) {
1088         // ↳ If the initialization segment contains a duration:
1089         //   Run the duration change algorithm with new duration set to the duration in the initialization segment.
1090         // ↳ Otherwise:
1091         //   Run the duration change algorithm with new duration set to positive Infinity.
1092         if (segment.duration.isValid() &amp;&amp; !segment.duration.isIndefinite())
1093             m_source-&gt;setDurationInternal(segment.duration);
1094         else
1095             m_source-&gt;setDurationInternal(MediaTime::positiveInfiniteTime());
1096     }
1097 
1098     // 2. If the initialization segment has no audio, video, or text tracks, then run the append error algorithm
1099     // with the decode error parameter set to true and abort these steps.
1100     if (segment.audioTracks.isEmpty() &amp;&amp; segment.videoTracks.isEmpty() &amp;&amp; segment.textTracks.isEmpty()) {
1101         appendError(true);
1102         return;
1103     }
1104 
1105     // 3. If the first initialization segment flag is true, then run the following steps:
1106     if (m_receivedFirstInitializationSegment) {
1107 
1108         // 3.1. Verify the following properties. If any of the checks fail then run the append error algorithm
1109         // with the decode error parameter set to true and abort these steps.
1110         if (!validateInitializationSegment(segment)) {
1111             appendError(true);
1112             return;
1113         }
1114         // 3.2 Add the appropriate track descriptions from this initialization segment to each of the track buffers.
1115         ASSERT(segment.audioTracks.size() == audioTracks().length());
1116         for (auto&amp; audioTrackInfo : segment.audioTracks) {
1117             if (audioTracks().length() == 1) {
1118                 audioTracks().item(0)-&gt;setPrivate(*audioTrackInfo.track);
1119                 break;
1120             }
1121 
1122             auto audioTrack = audioTracks().getTrackById(audioTrackInfo.track-&gt;id());
1123             ASSERT(audioTrack);
1124             audioTrack-&gt;setPrivate(*audioTrackInfo.track);
1125         }
1126 
1127         ASSERT(segment.videoTracks.size() == videoTracks().length());
1128         for (auto&amp; videoTrackInfo : segment.videoTracks) {
1129             if (videoTracks().length() == 1) {
1130                 videoTracks().item(0)-&gt;setPrivate(*videoTrackInfo.track);
1131                 break;
1132             }
1133 
1134             auto videoTrack = videoTracks().getTrackById(videoTrackInfo.track-&gt;id());
1135             ASSERT(videoTrack);
1136             videoTrack-&gt;setPrivate(*videoTrackInfo.track);
1137         }
1138 
1139         ASSERT(segment.textTracks.size() == textTracks().length());
1140         for (auto&amp; textTrackInfo : segment.textTracks) {
1141             if (textTracks().length() == 1) {
1142                 downcast&lt;InbandTextTrack&gt;(*textTracks().item(0)).setPrivate(*textTrackInfo.track);
1143                 break;
1144             }
1145 
1146             auto textTrack = textTracks().getTrackById(textTrackInfo.track-&gt;id());
1147             ASSERT(textTrack);
1148             downcast&lt;InbandTextTrack&gt;(*textTrack).setPrivate(*textTrackInfo.track);
1149         }
1150 
1151         // 3.3 Set the need random access point flag on all track buffers to true.
1152         for (auto&amp; trackBuffer : m_trackBufferMap.values())
1153             trackBuffer.needRandomAccessFlag = true;
1154     }
1155 
1156     // 4. Let active track flag equal false.
1157     bool activeTrackFlag = false;
1158 
1159     // 5. If the first initialization segment flag is false, then run the following steps:
1160     if (!m_receivedFirstInitializationSegment) {
1161         // 5.1 If the initialization segment contains tracks with codecs the user agent does not support,
1162         // then run the append error algorithm with the decode error parameter set to true and abort these steps.
1163         // NOTE: This check is the responsibility of the SourceBufferPrivate.
1164 
1165         // 5.2 For each audio track in the initialization segment, run following steps:
1166         for (auto&amp; audioTrackInfo : segment.audioTracks) {
1167             // FIXME: Implement steps 5.2.1-5.2.8.1 as per Editor&#39;s Draft 09 January 2015, and reorder this
1168             // 5.2.1 Let new audio track be a new AudioTrack object.
1169             // 5.2.2 Generate a unique ID and assign it to the id property on new video track.
1170             auto newAudioTrack = AudioTrack::create(*this, *audioTrackInfo.track);
1171             newAudioTrack-&gt;setSourceBuffer(this);
1172 
1173             // 5.2.3 If audioTracks.length equals 0, then run the following steps:
1174             if (!audioTracks().length()) {
1175                 // 5.2.3.1 Set the enabled property on new audio track to true.
1176                 newAudioTrack-&gt;setEnabled(true);
1177 
1178                 // 5.2.3.2 Set active track flag to true.
1179                 activeTrackFlag = true;
1180             }
1181 
1182             // 5.2.4 Add new audio track to the audioTracks attribute on this SourceBuffer object.
1183             // 5.2.5 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1184             // not cancelable, and that uses the TrackEvent interface, at the AudioTrackList object
1185             // referenced by the audioTracks attribute on this SourceBuffer object.
1186             audioTracks().append(newAudioTrack.copyRef());
1187 
1188             // 5.2.6 Add new audio track to the audioTracks attribute on the HTMLMediaElement.
1189             // 5.2.7 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1190             // not cancelable, and that uses the TrackEvent interface, at the AudioTrackList object
1191             // referenced by the audioTracks attribute on the HTMLMediaElement.
1192             m_source-&gt;mediaElement()-&gt;ensureAudioTracks().append(newAudioTrack.copyRef());
1193 
1194             // 5.2.8 Create a new track buffer to store coded frames for this track.
1195             ASSERT(!m_trackBufferMap.contains(newAudioTrack-&gt;id()));
1196             auto&amp; trackBuffer = m_trackBufferMap.add(newAudioTrack-&gt;id(), TrackBuffer()).iterator-&gt;value;
1197 
1198             // 5.2.9 Add the track description for this track to the track buffer.
1199             trackBuffer.description = audioTrackInfo.description;
1200 
1201             m_audioCodecs.append(trackBuffer.description-&gt;codec());
1202         }
1203 
1204         // 5.3 For each video track in the initialization segment, run following steps:
1205         for (auto&amp; videoTrackInfo : segment.videoTracks) {
1206             // FIXME: Implement steps 5.3.1-5.3.8.1 as per Editor&#39;s Draft 09 January 2015, and reorder this
1207             // 5.3.1 Let new video track be a new VideoTrack object.
1208             // 5.3.2 Generate a unique ID and assign it to the id property on new video track.
1209             auto newVideoTrack = VideoTrack::create(*this, *videoTrackInfo.track);
1210             newVideoTrack-&gt;setSourceBuffer(this);
1211 
1212             // 5.3.3 If videoTracks.length equals 0, then run the following steps:
1213             if (!videoTracks().length()) {
1214                 // 5.3.3.1 Set the selected property on new video track to true.
1215                 newVideoTrack-&gt;setSelected(true);
1216 
1217                 // 5.3.3.2 Set active track flag to true.
1218                 activeTrackFlag = true;
1219             }
1220 
1221             // 5.3.4 Add new video track to the videoTracks attribute on this SourceBuffer object.
1222             // 5.3.5 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1223             // not cancelable, and that uses the TrackEvent interface, at the VideoTrackList object
1224             // referenced by the videoTracks attribute on this SourceBuffer object.
1225             videoTracks().append(newVideoTrack.copyRef());
1226 
1227             // 5.3.6 Add new video track to the videoTracks attribute on the HTMLMediaElement.
1228             // 5.3.7 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1229             // not cancelable, and that uses the TrackEvent interface, at the VideoTrackList object
1230             // referenced by the videoTracks attribute on the HTMLMediaElement.
1231             m_source-&gt;mediaElement()-&gt;ensureVideoTracks().append(newVideoTrack.copyRef());
1232 
1233             // 5.3.8 Create a new track buffer to store coded frames for this track.
1234             ASSERT(!m_trackBufferMap.contains(newVideoTrack-&gt;id()));
1235             auto&amp; trackBuffer = m_trackBufferMap.add(newVideoTrack-&gt;id(), TrackBuffer()).iterator-&gt;value;
1236 
1237             // 5.3.9 Add the track description for this track to the track buffer.
1238             trackBuffer.description = videoTrackInfo.description;
1239 
1240             m_videoCodecs.append(trackBuffer.description-&gt;codec());
1241         }
1242 
1243         // 5.4 For each text track in the initialization segment, run following steps:
1244         for (auto&amp; textTrackInfo : segment.textTracks) {
1245             auto&amp; textTrackPrivate = *textTrackInfo.track;
1246 
1247             // FIXME: Implement steps 5.4.1-5.4.8.1 as per Editor&#39;s Draft 09 January 2015, and reorder this
1248             // 5.4.1 Let new text track be a new TextTrack object with its properties populated with the
1249             // appropriate information from the initialization segment.
1250             auto newTextTrack = InbandTextTrack::create(*scriptExecutionContext(), *this, textTrackPrivate);
1251 
1252             // 5.4.2 If the mode property on new text track equals &quot;showing&quot; or &quot;hidden&quot;, then set active
1253             // track flag to true.
1254             if (textTrackPrivate.mode() != InbandTextTrackPrivate::Disabled)
1255                 activeTrackFlag = true;
1256 
1257             // 5.4.3 Add new text track to the textTracks attribute on this SourceBuffer object.
1258             // 5.4.4 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1259             // not cancelable, and that uses the TrackEvent interface, at textTracks attribute on this
1260             // SourceBuffer object.
1261             textTracks().append(newTextTrack.get());
1262 
1263             // 5.4.5 Add new text track to the textTracks attribute on the HTMLMediaElement.
1264             // 5.4.6 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1265             // not cancelable, and that uses the TrackEvent interface, at the TextTrackList object
1266             // referenced by the textTracks attribute on the HTMLMediaElement.
1267             m_source-&gt;mediaElement()-&gt;ensureTextTracks().append(WTFMove(newTextTrack));
1268 
1269             // 5.4.7 Create a new track buffer to store coded frames for this track.
1270             ASSERT(!m_trackBufferMap.contains(textTrackPrivate.id()));
1271             auto&amp; trackBuffer = m_trackBufferMap.add(textTrackPrivate.id(), TrackBuffer()).iterator-&gt;value;
1272 
1273             // 5.4.8 Add the track description for this track to the track buffer.
1274             trackBuffer.description = textTrackInfo.description;
1275 
1276             m_textCodecs.append(trackBuffer.description-&gt;codec());
1277         }
1278 
1279         // 5.5 If active track flag equals true, then run the following steps:
1280         if (activeTrackFlag) {
1281             // 5.5.1 Add this SourceBuffer to activeSourceBuffers.
1282             // 5.5.2 Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1283             setActive(true);
1284         }
1285 
1286         // 5.6 Set first initialization segment flag to true.
1287         m_receivedFirstInitializationSegment = true;
1288     }
1289 
1290     // (Note: Issue #155 adds this step after step 5:)
1291     // 6. Set  pending initialization segment for changeType flag  to false.
1292     m_pendingInitializationSegmentForChangeType = false;
1293 
1294     // 6. If the HTMLMediaElement.readyState attribute is HAVE_NOTHING, then run the following steps:
<a name="21" id="anc21"></a><span class="line-modified">1295     if (m_private-&gt;readyState() == MediaPlayer::ReadyState::HaveNothing) {</span>
1296         // 6.1 If one or more objects in sourceBuffers have first initialization segment flag set to false, then abort these steps.
1297         for (auto&amp; sourceBuffer : *m_source-&gt;sourceBuffers()) {
1298             if (!sourceBuffer-&gt;m_receivedFirstInitializationSegment)
1299                 return;
1300         }
1301 
1302         // 6.2 Set the HTMLMediaElement.readyState attribute to HAVE_METADATA.
1303         // 6.3 Queue a task to fire a simple event named loadedmetadata at the media element.
<a name="22" id="anc22"></a><span class="line-modified">1304         m_private-&gt;setReadyState(MediaPlayer::ReadyState::HaveMetadata);</span>
1305     }
1306 
1307     // 7. If the active track flag equals true and the HTMLMediaElement.readyState
1308     // attribute is greater than HAVE_CURRENT_DATA, then set the HTMLMediaElement.readyState
1309     // attribute to HAVE_METADATA.
<a name="23" id="anc23"></a><span class="line-modified">1310     if (activeTrackFlag &amp;&amp; m_private-&gt;readyState() &gt; MediaPlayer::ReadyState::HaveCurrentData)</span>
<span class="line-modified">1311         m_private-&gt;setReadyState(MediaPlayer::ReadyState::HaveMetadata);</span>
1312 }
1313 
1314 bool SourceBuffer::validateInitializationSegment(const InitializationSegment&amp; segment)
1315 {
1316     // FIXME: ordering of all 3.5.X (X&gt;=7) functions needs to be updated to post-[24 July 2014 Editor&#39;s Draft] version
1317     // 3.5.8 Initialization Segment Received (ctd)
1318     // https://rawgit.com/w3c/media-source/c3ad59c7a370d04430969ba73d18dc9bcde57a33/index.html#sourcebuffer-init-segment-received [Editor&#39;s Draft 09 January 2015]
1319 
1320     // Note: those are checks from step 3.1
1321     //   * The number of audio, video, and text tracks match what was in the first initialization segment.
1322     if (segment.audioTracks.size() != audioTracks().length()
1323         || segment.videoTracks.size() != videoTracks().length()
1324         || segment.textTracks.size() != textTracks().length())
1325         return false;
1326 
1327     //   * The codecs for each track, match what was specified in the first initialization segment.
1328     // (Note: Issue #155 strikes out this check. For broad compatibility when this experimental feature
1329     // is not enabled, only perform this check if the &quot;pending initialization segment for changeType flag&quot;
1330     // is not set.)
1331     for (auto&amp; audioTrackInfo : segment.audioTracks) {
1332         if (m_audioCodecs.contains(audioTrackInfo.description-&gt;codec()))
1333             continue;
1334 
1335         if (!m_pendingInitializationSegmentForChangeType)
1336             return false;
1337 
1338         m_audioCodecs.append(audioTrackInfo.description-&gt;codec());
1339     }
1340 
1341     for (auto&amp; videoTrackInfo : segment.videoTracks) {
1342         if (m_videoCodecs.contains(videoTrackInfo.description-&gt;codec()))
1343             continue;
1344 
1345         if (!m_pendingInitializationSegmentForChangeType)
1346             return false;
1347 
1348         m_videoCodecs.append(videoTrackInfo.description-&gt;codec());
1349     }
1350 
1351     for (auto&amp; textTrackInfo : segment.textTracks) {
1352         if (m_textCodecs.contains(textTrackInfo.description-&gt;codec()))
1353             continue;
1354 
1355         if (!m_pendingInitializationSegmentForChangeType)
1356             return false;
1357 
1358         m_textCodecs.append(textTrackInfo.description-&gt;codec());
1359     }
1360 
1361     //   * If more than one track for a single type are present (ie 2 audio tracks), then the Track
1362     //   IDs match the ones in the first initialization segment.
1363     if (segment.audioTracks.size() &gt;= 2) {
1364         for (auto&amp; audioTrackInfo : segment.audioTracks) {
1365             if (!m_trackBufferMap.contains(audioTrackInfo.track-&gt;id()))
1366                 return false;
1367         }
1368     }
1369 
1370     if (segment.videoTracks.size() &gt;= 2) {
1371         for (auto&amp; videoTrackInfo : segment.videoTracks) {
1372             if (!m_trackBufferMap.contains(videoTrackInfo.track-&gt;id()))
1373                 return false;
1374         }
1375     }
1376 
1377     if (segment.textTracks.size() &gt;= 2) {
1378         for (auto&amp; textTrackInfo : segment.videoTracks) {
1379             if (!m_trackBufferMap.contains(textTrackInfo.track-&gt;id()))
1380                 return false;
1381         }
1382     }
1383 
1384     return true;
1385 }
1386 
1387 class SampleLessThanComparator {
1388 public:
1389     bool operator()(std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value1, std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value2)
1390     {
1391         return value1.first &lt; value2.first;
1392     }
1393 
1394     bool operator()(MediaTime value1, std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value2)
1395     {
1396         return value1 &lt; value2.first;
1397     }
1398 
1399     bool operator()(std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value1, MediaTime value2)
1400     {
1401         return value1.first &lt; value2;
1402     }
1403 };
1404 
1405 void SourceBuffer::appendError(bool decodeErrorParam)
1406 {
1407     // 3.5.3 Append Error Algorithm
1408     // https://rawgit.com/w3c/media-source/c3ad59c7a370d04430969ba73d18dc9bcde57a33/index.html#sourcebuffer-append-error [Editor&#39;s Draft 09 January 2015]
1409 
1410     ASSERT(m_updating);
1411     // 1. Run the reset parser state algorithm.
1412     resetParserState();
1413 
1414     // 2. Set the updating attribute to false.
1415     m_updating = false;
1416 
1417     // 3. Queue a task to fire a simple event named error at this SourceBuffer object.
1418     scheduleEvent(eventNames().errorEvent);
1419 
1420     // 4. Queue a task to fire a simple event named updateend at this SourceBuffer object.
1421     scheduleEvent(eventNames().updateendEvent);
1422 
1423     // 5. If decode error is true, then run the end of stream algorithm with the error parameter set to &quot;decode&quot;.
1424     if (decodeErrorParam)
1425         m_source-&gt;streamEndedWithError(MediaSource::EndOfStreamError::Decode);
1426 }
1427 
1428 void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample&amp; sample)
1429 {
1430     if (isRemoved())
1431         return;
1432 
1433     // 3.5.1 Segment Parser Loop
1434     // 6.1 If the first initialization segment received flag is false, (Note: Issue # 155 &amp; changeType()
1435     // algorithm) or the  pending initialization segment for changeType flag  is true, (End note)
1436     // then run the append error algorithm
1437     //     with the decode error parameter set to true and abort this algorithm.
1438     // Note: current design makes SourceBuffer somehow ignorant of append state - it&#39;s more a thing
1439     //  of SourceBufferPrivate. That&#39;s why this check can&#39;t really be done in appendInternal.
1440     //  unless we force some kind of design with state machine switching.
1441     if (!m_receivedFirstInitializationSegment || m_pendingInitializationSegmentForChangeType) {
1442         appendError(true);
1443         return;
1444     }
1445 
1446     // 3.5.8 Coded Frame Processing
1447     // http://www.w3.org/TR/media-source/#sourcebuffer-coded-frame-processing
1448 
1449     // When complete coded frames have been parsed by the segment parser loop then the following steps
1450     // are run:
1451     // 1. For each coded frame in the media segment run the following steps:
1452     // 1.1. Loop Top
1453     do {
1454         MediaTime presentationTimestamp;
1455         MediaTime decodeTimestamp;
1456 
1457         // NOTE: this is out-of-order, but we need the timescale from the
1458         // sample&#39;s duration for timestamp generation.
1459         // 1.2 Let frame duration be a double precision floating point representation of the coded frame&#39;s
1460         // duration in seconds.
1461         MediaTime frameDuration = sample.duration();
1462 
1463         if (m_shouldGenerateTimestamps) {
1464             // ↳ If generate timestamps flag equals true:
1465             // 1. Let presentation timestamp equal 0.
1466             // NOTE: Use the duration timscale for the presentation timestamp, as this will eliminate
1467             // timescale rounding when generating timestamps.
1468             presentationTimestamp = { 0, frameDuration.timeScale() };
1469 
1470             // 2. Let decode timestamp equal 0.
1471             decodeTimestamp = { 0, frameDuration.timeScale() };
1472         } else {
1473             // ↳ Otherwise:
1474             // 1. Let presentation timestamp be a double precision floating point representation of
1475             // the coded frame&#39;s presentation timestamp in seconds.
1476             presentationTimestamp = sample.presentationTime();
1477 
1478             // 2. Let decode timestamp be a double precision floating point representation of the coded frame&#39;s
1479             // decode timestamp in seconds.
1480             decodeTimestamp = sample.decodeTime();
1481         }
1482 
1483         // 1.3 If mode equals &quot;sequence&quot; and group start timestamp is set, then run the following steps:
1484         if (m_mode == AppendMode::Sequence &amp;&amp; m_groupStartTimestamp.isValid()) {
1485             // 1.3.1 Set timestampOffset equal to group start timestamp - presentation timestamp.
1486             m_timestampOffset = m_groupStartTimestamp;
1487 
1488             for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
1489                 trackBuffer.lastFrameTimescale = 0;
1490                 trackBuffer.roundedTimestampOffset = MediaTime::invalidTime();
1491             }
1492 
1493             // 1.3.2 Set group end timestamp equal to group start timestamp.
1494             m_groupEndTimestamp = m_groupStartTimestamp;
1495 
1496             // 1.3.3 Set the need random access point flag on all track buffers to true.
1497             for (auto&amp; trackBuffer : m_trackBufferMap.values())
1498                 trackBuffer.needRandomAccessFlag = true;
1499 
1500             // 1.3.4 Unset group start timestamp.
1501             m_groupStartTimestamp = MediaTime::invalidTime();
1502         }
1503 
1504         // NOTE: this is out-of-order, but we need TrackBuffer to be able to cache the results of timestamp offset rounding
1505         // 1.5 Let track buffer equal the track buffer that the coded frame will be added to.
1506         AtomString trackID = sample.trackID();
1507         auto it = m_trackBufferMap.find(trackID);
1508         if (it == m_trackBufferMap.end()) {
1509             // The client managed to append a sample with a trackID not present in the initialization
1510             // segment. This would be a good place to post an message to the developer console.
1511             didDropSample();
1512             return;
1513         }
1514         TrackBuffer&amp; trackBuffer = it-&gt;value;
1515 
1516         MediaTime microsecond(1, 1000000);
1517 
1518         auto roundTowardsTimeScaleWithRoundingMargin = [] (const MediaTime&amp; time, uint32_t timeScale, const MediaTime&amp; roundingMargin) {
1519             while (true) {
1520                 MediaTime roundedTime = time.toTimeScale(timeScale);
1521                 if (abs(roundedTime - time) &lt; roundingMargin || timeScale &gt;= MediaTime::MaximumTimeScale)
1522                     return roundedTime;
1523 
1524                 if (!WTF::safeMultiply(timeScale, 2, timeScale) || timeScale &gt; MediaTime::MaximumTimeScale)
1525                     timeScale = MediaTime::MaximumTimeScale;
1526             }
1527         };
1528 
1529         // 1.4 If timestampOffset is not 0, then run the following steps:
1530         if (m_timestampOffset) {
1531             if (!trackBuffer.roundedTimestampOffset.isValid() || presentationTimestamp.timeScale() != trackBuffer.lastFrameTimescale) {
1532                 trackBuffer.lastFrameTimescale = presentationTimestamp.timeScale();
1533                 trackBuffer.roundedTimestampOffset = roundTowardsTimeScaleWithRoundingMargin(m_timestampOffset, trackBuffer.lastFrameTimescale, microsecond);
1534             }
1535 
1536             // 1.4.1 Add timestampOffset to the presentation timestamp.
1537             presentationTimestamp += trackBuffer.roundedTimestampOffset;
1538 
1539             // 1.4.2 Add timestampOffset to the decode timestamp.
1540             decodeTimestamp += trackBuffer.roundedTimestampOffset;
1541         }
1542 
1543         // 1.6 ↳ If last decode timestamp for track buffer is set and decode timestamp is less than last
1544         // decode timestamp:
1545         // OR
1546         // ↳ If last decode timestamp for track buffer is set and the difference between decode timestamp and
1547         // last decode timestamp is greater than 2 times last frame duration:
1548         MediaTime decodeDurationToCheck = trackBuffer.greatestDecodeDuration;
1549 
1550         if (decodeDurationToCheck.isValid() &amp;&amp; trackBuffer.lastFrameDuration.isValid()
1551             &amp;&amp; (trackBuffer.lastFrameDuration &gt; decodeDurationToCheck))
1552             decodeDurationToCheck = trackBuffer.lastFrameDuration;
1553 
1554         if (trackBuffer.lastDecodeTimestamp.isValid() &amp;&amp; (decodeTimestamp &lt; trackBuffer.lastDecodeTimestamp
1555             || (decodeDurationToCheck.isValid() &amp;&amp; abs(decodeTimestamp - trackBuffer.lastDecodeTimestamp) &gt; (decodeDurationToCheck * 2)))) {
1556 
1557             // 1.6.1:
1558             if (m_mode == AppendMode::Segments) {
1559                 // ↳ If mode equals &quot;segments&quot;:
1560                 // Set group end timestamp to presentation timestamp.
1561                 m_groupEndTimestamp = presentationTimestamp;
1562             } else {
1563                 // ↳ If mode equals &quot;sequence&quot;:
1564                 // Set group start timestamp equal to the group end timestamp.
1565                 m_groupStartTimestamp = m_groupEndTimestamp;
1566             }
1567 
1568             for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
1569                 // 1.6.2 Unset the last decode timestamp on all track buffers.
1570                 trackBuffer.lastDecodeTimestamp = MediaTime::invalidTime();
1571                 // 1.6.3 Unset the last frame duration on all track buffers.
1572                 trackBuffer.greatestDecodeDuration = MediaTime::invalidTime();
1573                 trackBuffer.lastFrameDuration = MediaTime::invalidTime();
1574                 // 1.6.4 Unset the highest presentation timestamp on all track buffers.
1575                 trackBuffer.highestPresentationTimestamp = MediaTime::invalidTime();
1576                 // 1.6.5 Set the need random access point flag on all track buffers to true.
1577                 trackBuffer.needRandomAccessFlag = true;
1578             }
1579 
1580             // 1.6.6 Jump to the Loop Top step above to restart processing of the current coded frame.
1581             continue;
1582         }
1583 
1584         if (m_mode == AppendMode::Sequence) {
1585             // Use the generated timestamps instead of the sample&#39;s timestamps.
1586             sample.setTimestamps(presentationTimestamp, decodeTimestamp);
1587         } else if (trackBuffer.roundedTimestampOffset) {
1588             // Reflect the timestamp offset into the sample.
1589             sample.offsetTimestampsBy(trackBuffer.roundedTimestampOffset);
1590         }
1591 
1592         DEBUG_LOG(LOGIDENTIFIER, sample);
1593 
1594         // 1.7 Let frame end timestamp equal the sum of presentation timestamp and frame duration.
1595         MediaTime frameEndTimestamp = presentationTimestamp + frameDuration;
1596 
1597         // 1.8 If presentation timestamp is less than appendWindowStart, then set the need random access
1598         // point flag to true, drop the coded frame, and jump to the top of the loop to start processing
1599         // the next coded frame.
1600         // 1.9 If frame end timestamp is greater than appendWindowEnd, then set the need random access
1601         // point flag to true, drop the coded frame, and jump to the top of the loop to start processing
1602         // the next coded frame.
1603         if (presentationTimestamp &lt; m_appendWindowStart || frameEndTimestamp &gt; m_appendWindowEnd) {
1604             trackBuffer.needRandomAccessFlag = true;
1605             didDropSample();
1606             return;
1607         }
1608 
1609 
1610         // 1.10 If the decode timestamp is less than the presentation start time, then run the end of stream
1611         // algorithm with the error parameter set to &quot;decode&quot;, and abort these steps.
1612         // NOTE: Until &lt;https://www.w3.org/Bugs/Public/show_bug.cgi?id=27487&gt; is resolved, we will only check
1613         // the presentation timestamp.
1614         MediaTime presentationStartTime = MediaTime::zeroTime();
1615         if (presentationTimestamp &lt; presentationStartTime) {
1616             ERROR_LOG(LOGIDENTIFIER, &quot;failing because presentationTimestamp (&quot;, presentationTimestamp, &quot;) &lt; presentationStartTime (&quot;, presentationStartTime, &quot;)&quot;);
1617             m_source-&gt;streamEndedWithError(MediaSource::EndOfStreamError::Decode);
1618             return;
1619         }
1620 
1621         // 1.11 If the need random access point flag on track buffer equals true, then run the following steps:
1622         if (trackBuffer.needRandomAccessFlag) {
1623             // 1.11.1 If the coded frame is not a random access point, then drop the coded frame and jump
1624             // to the top of the loop to start processing the next coded frame.
1625             if (!sample.isSync()) {
1626                 didDropSample();
1627                 return;
1628             }
1629 
1630             // 1.11.2 Set the need random access point flag on track buffer to false.
1631             trackBuffer.needRandomAccessFlag = false;
1632         }
1633 
1634         // 1.12 Let spliced audio frame be an unset variable for holding audio splice information
1635         // 1.13 Let spliced timed text frame be an unset variable for holding timed text splice information
1636         // FIXME: Add support for sample splicing.
1637 
1638         SampleMap erasedSamples;
1639 
1640         // 1.14 If last decode timestamp for track buffer is unset and presentation timestamp falls
1641         // falls within the presentation interval of a coded frame in track buffer, then run the
1642         // following steps:
1643         if (trackBuffer.lastDecodeTimestamp.isInvalid()) {
1644             auto iter = trackBuffer.samples.presentationOrder().findSampleContainingPresentationTime(presentationTimestamp);
1645             if (iter != trackBuffer.samples.presentationOrder().end()) {
1646                 // 1.14.1 Let overlapped frame be the coded frame in track buffer that matches the condition above.
1647                 RefPtr&lt;MediaSample&gt; overlappedFrame = iter-&gt;second;
1648 
1649                 // 1.14.2 If track buffer contains audio coded frames:
1650                 // Run the audio splice frame algorithm and if a splice frame is returned, assign it to
1651                 // spliced audio frame.
1652                 // FIXME: Add support for sample splicing.
1653 
1654                 // If track buffer contains video coded frames:
1655                 if (trackBuffer.description &amp;&amp; trackBuffer.description-&gt;isVideo()) {
1656                     // 1.14.2.1 Let overlapped frame presentation timestamp equal the presentation timestamp
1657                     // of overlapped frame.
1658                     MediaTime overlappedFramePresentationTimestamp = overlappedFrame-&gt;presentationTime();
1659 
1660                     // 1.14.2.2 Let remove window timestamp equal overlapped frame presentation timestamp
1661                     // plus 1 microsecond.
1662                     MediaTime removeWindowTimestamp = overlappedFramePresentationTimestamp + microsecond;
1663 
1664                     // 1.14.2.3 If the presentation timestamp is less than the remove window timestamp,
1665                     // then remove overlapped frame and any coded frames that depend on it from track buffer.
1666                     if (presentationTimestamp &lt; removeWindowTimestamp)
1667                         erasedSamples.addSample(*iter-&gt;second);
1668                 }
1669 
1670                 // If track buffer contains timed text coded frames:
1671                 // Run the text splice frame algorithm and if a splice frame is returned, assign it to spliced timed text frame.
1672                 // FIXME: Add support for sample splicing.
1673             }
1674         }
1675 
1676         // 1.15 Remove existing coded frames in track buffer:
1677         // If highest presentation timestamp for track buffer is not set:
1678         if (trackBuffer.highestPresentationTimestamp.isInvalid()) {
1679             // Remove all coded frames from track buffer that have a presentation timestamp greater than or
1680             // equal to presentation timestamp and less than frame end timestamp.
1681             auto iter_pair = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimes(presentationTimestamp, frameEndTimestamp);
1682             if (iter_pair.first != trackBuffer.samples.presentationOrder().end())
1683                 erasedSamples.addRange(iter_pair.first, iter_pair.second);
1684         }
1685 
<a name="24" id="anc24"></a><span class="line-added">1686         // When appending media containing B-frames (media whose samples&#39; presentation timestamps</span>
<span class="line-added">1687         // do not increase monotonically, the prior erase steps could leave a sample in the trackBuffer</span>
<span class="line-added">1688         // which will be disconnected from its previous I-frame. If the incoming frame is an I-frame,</span>
<span class="line-added">1689         // remove all samples in decode order between the incoming I-frame&#39;s decode timestamp and the</span>
<span class="line-added">1690         // next I-frame. See &lt;https://github.com/w3c/media-source/issues/187&gt; for a discussion of what</span>
<span class="line-added">1691         // the how the MSE specification should handlie this secnario.</span>
<span class="line-added">1692         do {</span>
<span class="line-added">1693             if (!sample.isSync())</span>
<span class="line-added">1694                 break;</span>
<span class="line-added">1695 </span>
<span class="line-added">1696             DecodeOrderSampleMap::KeyType decodeKey(sample.decodeTime(), sample.presentationTime());</span>
<span class="line-added">1697             auto nextSampleInDecodeOrder = trackBuffer.samples.decodeOrder().findSampleAfterDecodeKey(decodeKey);</span>
<span class="line-added">1698             if (nextSampleInDecodeOrder == trackBuffer.samples.decodeOrder().end())</span>
<span class="line-added">1699                 break;</span>
<span class="line-added">1700 </span>
<span class="line-added">1701             if (nextSampleInDecodeOrder-&gt;second-&gt;isSync())</span>
<span class="line-added">1702                 break;</span>
<span class="line-added">1703 </span>
<span class="line-added">1704             auto nextSyncSample = trackBuffer.samples.decodeOrder().findSyncSampleAfterDecodeIterator(nextSampleInDecodeOrder);</span>
<span class="line-added">1705             INFO_LOG(LOGIDENTIFIER, &quot;Discovered out-of-order frames, from: &quot;, *nextSampleInDecodeOrder-&gt;second, &quot; to: &quot;, (nextSyncSample == trackBuffer.samples.decodeOrder().end() ? &quot;[end]&quot;_s : toString(*nextSyncSample-&gt;second)));</span>
<span class="line-added">1706             erasedSamples.addRange(nextSampleInDecodeOrder, nextSyncSample);</span>
<span class="line-added">1707         } while (false);</span>
<span class="line-added">1708 </span>
1709         // There are many files out there where the frame times are not perfectly contiguous and may have small overlaps
1710         // between the beginning of a frame and the end of the previous one; therefore a tolerance is needed whenever
1711         // durations are considered.
1712         // For instance, most WebM files are muxed rounded to the millisecond (the default TimecodeScale of the format)
1713         // but their durations use a finer timescale (causing a sub-millisecond overlap). More rarely, there are also
1714         // MP4 files with slightly off tfdt boxes, presenting a similar problem at the beginning of each fragment.
1715         const MediaTime contiguousFrameTolerance = MediaTime(1, 1000);
1716 
1717         // If highest presentation timestamp for track buffer is set and less than or equal to presentation timestamp
1718         if (trackBuffer.highestPresentationTimestamp.isValid() &amp;&amp; trackBuffer.highestPresentationTimestamp - contiguousFrameTolerance &lt;= presentationTimestamp) {
1719             // Remove all coded frames from track buffer that have a presentation timestamp greater than highest
1720             // presentation timestamp and less than or equal to frame end timestamp.
1721             do {
1722                 // NOTE: Searching from the end of the trackBuffer will be vastly more efficient if the search range is
1723                 // near the end of the buffered range. Use a linear-backwards search if the search range is within one
1724                 // frame duration of the end:
1725                 unsigned bufferedLength = trackBuffer.buffered.length();
1726                 if (!bufferedLength)
1727                     break;
1728 
1729                 MediaTime highestBufferedTime = trackBuffer.buffered.maximumBufferedTime();
1730                 MediaTime eraseBeginTime = trackBuffer.highestPresentationTimestamp - contiguousFrameTolerance;
1731                 MediaTime eraseEndTime = frameEndTimestamp - contiguousFrameTolerance;
1732 
1733                 PresentationOrderSampleMap::iterator_range range;
1734                 if (highestBufferedTime - trackBuffer.highestPresentationTimestamp &lt; trackBuffer.lastFrameDuration)
1735                     // If the new frame is at the end of the buffered ranges, perform a sequential scan from end (O(1)).
1736                     range = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimesFromEnd(eraseBeginTime, eraseEndTime);
1737                 else
1738                     // In any other case, perform a binary search (O(log(n)).
1739                     range = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimes(eraseBeginTime, eraseEndTime);
1740 
1741                 if (range.first != trackBuffer.samples.presentationOrder().end())
1742                     erasedSamples.addRange(range.first, range.second);
1743             } while(false);
1744         }
1745 
1746         // 1.16 Remove decoding dependencies of the coded frames removed in the previous step:
1747         DecodeOrderSampleMap::MapType dependentSamples;
1748         if (!erasedSamples.empty()) {
1749             // If detailed information about decoding dependencies is available:
1750             // FIXME: Add support for detailed dependency information
1751 
1752             // Otherwise: Remove all coded frames between the coded frames removed in the previous step
1753             // and the next random access point after those removed frames.
1754             auto firstDecodeIter = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey(erasedSamples.decodeOrder().begin()-&gt;first);
1755             auto lastDecodeIter = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey(erasedSamples.decodeOrder().rbegin()-&gt;first);
1756             auto nextSyncIter = trackBuffer.samples.decodeOrder().findSyncSampleAfterDecodeIterator(lastDecodeIter);
1757             dependentSamples.insert(firstDecodeIter, nextSyncIter);
1758 
1759             // NOTE: in the case of b-frames, the previous step may leave in place samples whose presentation
1760             // timestamp &lt; presentationTime, but whose decode timestamp &gt;= decodeTime. These will eventually cause
1761             // a decode error if left in place, so remove these samples as well.
1762             DecodeOrderSampleMap::KeyType decodeKey(sample.decodeTime(), sample.presentationTime());
1763             auto samplesWithHigherDecodeTimes = trackBuffer.samples.decodeOrder().findSamplesBetweenDecodeKeys(decodeKey, erasedSamples.decodeOrder().begin()-&gt;first);
1764             if (samplesWithHigherDecodeTimes.first != samplesWithHigherDecodeTimes.second)
1765                 dependentSamples.insert(samplesWithHigherDecodeTimes.first, samplesWithHigherDecodeTimes.second);
1766 
1767             PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(dependentSamples, trackBuffer, this, &quot;sourceBufferPrivateDidReceiveSample&quot;);
1768 
1769             // Only force the TrackBuffer to re-enqueue if the removed ranges overlap with enqueued and possibly
1770             // not yet displayed samples.
1771             MediaTime currentMediaTime = m_source-&gt;currentTime();
<a name="25" id="anc25"></a><span class="line-modified">1772             if (trackBuffer.highestEnqueuedPresentationTime.isValid() &amp;&amp; currentMediaTime &lt; trackBuffer.highestEnqueuedPresentationTime) {</span>
<span class="line-modified">1773                 PlatformTimeRanges possiblyEnqueuedRanges(currentMediaTime, trackBuffer.highestEnqueuedPresentationTime);</span>
1774                 possiblyEnqueuedRanges.intersectWith(erasedRanges);
1775                 if (possiblyEnqueuedRanges.length())
1776                     trackBuffer.needsReenqueueing = true;
1777             }
1778 
1779             erasedRanges.invert();
1780             trackBuffer.buffered.intersectWith(erasedRanges);
1781             setBufferedDirty(true);
1782         }
1783 
1784         // 1.17 If spliced audio frame is set:
1785         // Add spliced audio frame to the track buffer.
1786         // If spliced timed text frame is set:
1787         // Add spliced timed text frame to the track buffer.
1788         // FIXME: Add support for sample splicing.
1789 
1790         // Otherwise:
1791         // Add the coded frame with the presentation timestamp, decode timestamp, and frame duration to the track buffer.
1792         trackBuffer.samples.addSample(sample);
1793 
1794         // Note: The terminology here is confusing: &quot;enqueuing&quot; means providing a frame to the inner media framework.
<a name="26" id="anc26"></a><span class="line-modified">1795         // First, frames are inserted in the decode queue; later, at the end of the append some of the frames in the</span>
<span class="line-modified">1796         // decode may be &quot;enqueued&quot; (sent to the inner media framework) in `provideMediaData()`.</span>
1797         //
<a name="27" id="anc27"></a><span class="line-modified">1798         // In order to check whether a frame should be added to the decode queue we check that it does not precede any</span>
<span class="line-modified">1799         // frame already enqueued.</span>
<span class="line-added">1800         //</span>
<span class="line-added">1801         // Note that adding a frame to the decode queue is no guarantee that it will be actually enqueued at that point.</span>
<span class="line-added">1802         // If the frame is after the discontinuity boundary, the enqueueing algorithm will hold it there until samples</span>
<span class="line-added">1803         // with earlier timestamps are enqueued. The decode queue is not FIFO, but rather an ordered map.</span>
1804         DecodeOrderSampleMap::KeyType decodeKey(sample.decodeTime(), sample.presentationTime());
1805         if (trackBuffer.lastEnqueuedDecodeKey.first.isInvalid() || decodeKey &gt; trackBuffer.lastEnqueuedDecodeKey) {
1806             trackBuffer.decodeQueue.insert(DecodeOrderSampleMap::MapType::value_type(decodeKey, &amp;sample));
1807 
1808             if (trackBuffer.minimumEnqueuedPresentationTime.isValid() &amp;&amp; sample.presentationTime() &lt; trackBuffer.minimumEnqueuedPresentationTime)
1809                 trackBuffer.needsMinimumUpcomingPresentationTimeUpdating = true;
1810         }
1811 
1812         // NOTE: the spec considers &quot;Coded Frame Duration&quot; to be the presentation duration, but this is not necessarily equal
1813         // to the decoded duration. When comparing deltas between decode timestamps, the decode duration, not the presentation.
1814         if (trackBuffer.lastDecodeTimestamp.isValid()) {
1815             MediaTime lastDecodeDuration = decodeTimestamp - trackBuffer.lastDecodeTimestamp;
1816             if (!trackBuffer.greatestDecodeDuration.isValid() || lastDecodeDuration &gt; trackBuffer.greatestDecodeDuration)
1817                 trackBuffer.greatestDecodeDuration = lastDecodeDuration;
1818         }
1819 
1820         // 1.18 Set last decode timestamp for track buffer to decode timestamp.
1821         trackBuffer.lastDecodeTimestamp = decodeTimestamp;
1822 
1823         // 1.19 Set last frame duration for track buffer to frame duration.
1824         trackBuffer.lastFrameDuration = frameDuration;
1825 
1826         // 1.20 If highest presentation timestamp for track buffer is unset or frame end timestamp is greater
1827         // than highest presentation timestamp, then set highest presentation timestamp for track buffer
1828         // to frame end timestamp.
1829         if (trackBuffer.highestPresentationTimestamp.isInvalid() || frameEndTimestamp &gt; trackBuffer.highestPresentationTimestamp)
1830             trackBuffer.highestPresentationTimestamp = frameEndTimestamp;
1831 
1832         // 1.21 If frame end timestamp is greater than group end timestamp, then set group end timestamp equal
1833         // to frame end timestamp.
1834         if (m_groupEndTimestamp.isInvalid() || frameEndTimestamp &gt; m_groupEndTimestamp)
1835             m_groupEndTimestamp = frameEndTimestamp;
1836 
1837         // 1.22 If generate timestamps flag equals true, then set timestampOffset equal to frame end timestamp.
1838         if (m_shouldGenerateTimestamps) {
1839             m_timestampOffset = frameEndTimestamp;
1840             for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
1841                 trackBuffer.lastFrameTimescale = 0;
1842                 trackBuffer.roundedTimestampOffset = MediaTime::invalidTime();
1843             }
1844         }
1845 
1846         // Eliminate small gaps between buffered ranges by coalescing
1847         // disjoint ranges separated by less than a &quot;fudge factor&quot;.
1848         auto presentationEndTime = presentationTimestamp + frameDuration;
1849         auto nearestToPresentationStartTime = trackBuffer.buffered.nearest(presentationTimestamp);
1850         if (nearestToPresentationStartTime.isValid() &amp;&amp; (presentationTimestamp - nearestToPresentationStartTime).isBetween(MediaTime::zeroTime(), MediaSource::currentTimeFudgeFactor()))
1851             presentationTimestamp = nearestToPresentationStartTime;
1852 
1853         auto nearestToPresentationEndTime = trackBuffer.buffered.nearest(presentationEndTime);
1854         if (nearestToPresentationEndTime.isValid() &amp;&amp; (nearestToPresentationEndTime - presentationEndTime).isBetween(MediaTime::zeroTime(), MediaSource::currentTimeFudgeFactor()))
1855             presentationEndTime = nearestToPresentationEndTime;
1856 
1857         trackBuffer.buffered.add(presentationTimestamp, presentationEndTime);
1858         m_bufferedSinceLastMonitor += frameDuration.toDouble();
1859         setBufferedDirty(true);
1860 
1861         break;
1862     } while (1);
1863 
1864     // Steps 2-4 will be handled by MediaSource::monitorSourceBuffers()
1865 
1866     // 5. If the media segment contains data beyond the current duration, then run the duration change algorithm with new
1867     // duration set to the maximum of the current duration and the group end timestamp.
1868     if (m_groupEndTimestamp &gt; m_source-&gt;duration())
1869         m_source-&gt;setDurationInternal(m_groupEndTimestamp);
1870 }
1871 
1872 bool SourceBuffer::hasAudio() const
1873 {
1874     return m_audioTracks &amp;&amp; m_audioTracks-&gt;length();
1875 }
1876 
1877 bool SourceBuffer::hasVideo() const
1878 {
1879     return m_videoTracks &amp;&amp; m_videoTracks-&gt;length();
1880 }
1881 
1882 bool SourceBuffer::sourceBufferPrivateHasAudio() const
1883 {
1884     return hasAudio();
1885 }
1886 
1887 bool SourceBuffer::sourceBufferPrivateHasVideo() const
1888 {
1889     return hasVideo();
1890 }
1891 
1892 void SourceBuffer::videoTrackSelectedChanged(VideoTrack&amp; track)
1893 {
1894     // 2.4.5 Changes to selected/enabled track state
1895     // If the selected video track changes, then run the following steps:
1896     // 1. If the SourceBuffer associated with the previously selected video track is not associated with
1897     // any other enabled tracks, run the following steps:
1898     if (!track.selected()
1899         &amp;&amp; (!m_videoTracks || !m_videoTracks-&gt;isAnyTrackEnabled())
1900         &amp;&amp; (!m_audioTracks || !m_audioTracks-&gt;isAnyTrackEnabled())
1901         &amp;&amp; (!m_textTracks || !m_textTracks-&gt;isAnyTrackEnabled())) {
1902         // 1.1 Remove the SourceBuffer from activeSourceBuffers.
1903         // 1.2 Queue a task to fire a simple event named removesourcebuffer at activeSourceBuffers
1904         setActive(false);
1905     } else if (track.selected()) {
1906         // 2. If the SourceBuffer associated with the newly selected video track is not already in activeSourceBuffers,
1907         // run the following steps:
1908         // 2.1 Add the SourceBuffer to activeSourceBuffers.
1909         // 2.2 Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1910         setActive(true);
1911     }
1912 
1913     if (m_videoTracks &amp;&amp; m_videoTracks-&gt;contains(track))
1914         m_videoTracks-&gt;scheduleChangeEvent();
1915 
1916     if (!isRemoved())
1917         m_source-&gt;mediaElement()-&gt;videoTrackSelectedChanged(track);
1918 }
1919 
1920 void SourceBuffer::audioTrackEnabledChanged(AudioTrack&amp; track)
1921 {
1922     // 2.4.5 Changes to selected/enabled track state
1923     // If an audio track becomes disabled and the SourceBuffer associated with this track is not
1924     // associated with any other enabled or selected track, then run the following steps:
1925     if (!track.enabled()
1926         &amp;&amp; (!m_videoTracks || !m_videoTracks-&gt;isAnyTrackEnabled())
1927         &amp;&amp; (!m_audioTracks || !m_audioTracks-&gt;isAnyTrackEnabled())
1928         &amp;&amp; (!m_textTracks || !m_textTracks-&gt;isAnyTrackEnabled())) {
1929         // 1. Remove the SourceBuffer associated with the audio track from activeSourceBuffers
1930         // 2. Queue a task to fire a simple event named removesourcebuffer at activeSourceBuffers
1931         setActive(false);
1932     } else if (track.enabled()) {
1933         // If an audio track becomes enabled and the SourceBuffer associated with this track is
1934         // not already in activeSourceBuffers, then run the following steps:
1935         // 1. Add the SourceBuffer associated with the audio track to activeSourceBuffers
1936         // 2. Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1937         setActive(true);
1938     }
1939 
1940     if (m_audioTracks &amp;&amp; m_audioTracks-&gt;contains(track))
1941         m_audioTracks-&gt;scheduleChangeEvent();
1942 
1943     if (!isRemoved())
1944         m_source-&gt;mediaElement()-&gt;audioTrackEnabledChanged(track);
1945 }
1946 
1947 void SourceBuffer::textTrackModeChanged(TextTrack&amp; track)
1948 {
1949     // 2.4.5 Changes to selected/enabled track state
1950     // If a text track mode becomes &quot;disabled&quot; and the SourceBuffer associated with this track is not
1951     // associated with any other enabled or selected track, then run the following steps:
1952     if (track.mode() == TextTrack::Mode::Disabled
1953         &amp;&amp; (!m_videoTracks || !m_videoTracks-&gt;isAnyTrackEnabled())
1954         &amp;&amp; (!m_audioTracks || !m_audioTracks-&gt;isAnyTrackEnabled())
1955         &amp;&amp; (!m_textTracks || !m_textTracks-&gt;isAnyTrackEnabled())) {
1956         // 1. Remove the SourceBuffer associated with the audio track from activeSourceBuffers
1957         // 2. Queue a task to fire a simple event named removesourcebuffer at activeSourceBuffers
1958         setActive(false);
1959     } else {
1960         // If a text track mode becomes &quot;showing&quot; or &quot;hidden&quot; and the SourceBuffer associated with this
1961         // track is not already in activeSourceBuffers, then run the following steps:
1962         // 1. Add the SourceBuffer associated with the text track to activeSourceBuffers
1963         // 2. Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1964         setActive(true);
1965     }
1966 
1967     if (m_textTracks &amp;&amp; m_textTracks-&gt;contains(track))
1968         m_textTracks-&gt;scheduleChangeEvent();
1969 
1970     if (!isRemoved())
1971         m_source-&gt;mediaElement()-&gt;textTrackModeChanged(track);
1972 }
1973 
1974 void SourceBuffer::textTrackAddCue(TextTrack&amp; track, TextTrackCue&amp; cue)
1975 {
1976     if (!isRemoved())
1977         m_source-&gt;mediaElement()-&gt;textTrackAddCue(track, cue);
1978 }
1979 
1980 void SourceBuffer::textTrackAddCues(TextTrack&amp; track, const TextTrackCueList&amp; cueList)
1981 {
1982     if (!isRemoved())
1983         m_source-&gt;mediaElement()-&gt;textTrackAddCues(track, cueList);
1984 }
1985 
1986 void SourceBuffer::textTrackRemoveCue(TextTrack&amp; track, TextTrackCue&amp; cue)
1987 {
1988     if (!isRemoved())
1989         m_source-&gt;mediaElement()-&gt;textTrackRemoveCue(track, cue);
1990 }
1991 
1992 void SourceBuffer::textTrackRemoveCues(TextTrack&amp; track, const TextTrackCueList&amp; cueList)
1993 {
1994     if (!isRemoved())
1995         m_source-&gt;mediaElement()-&gt;textTrackRemoveCues(track, cueList);
1996 }
1997 
1998 void SourceBuffer::textTrackKindChanged(TextTrack&amp; track)
1999 {
2000     if (!isRemoved())
2001         m_source-&gt;mediaElement()-&gt;textTrackKindChanged(track);
2002 }
2003 
2004 void SourceBuffer::sourceBufferPrivateReenqueSamples(const AtomString&amp; trackID)
2005 {
2006     if (isRemoved())
2007         return;
2008 
2009     DEBUG_LOG(LOGIDENTIFIER);
2010     auto it = m_trackBufferMap.find(trackID);
2011     if (it == m_trackBufferMap.end())
2012         return;
2013 
2014     auto&amp; trackBuffer = it-&gt;value;
2015     trackBuffer.needsReenqueueing = true;
2016     reenqueueMediaForTime(trackBuffer, trackID, m_source-&gt;currentTime());
2017 }
2018 
2019 void SourceBuffer::sourceBufferPrivateDidBecomeReadyForMoreSamples(const AtomString&amp; trackID)
2020 {
2021     if (isRemoved())
2022         return;
2023 
2024     DEBUG_LOG(LOGIDENTIFIER);
2025     auto it = m_trackBufferMap.find(trackID);
2026     if (it == m_trackBufferMap.end())
2027         return;
2028 
2029     auto&amp; trackBuffer = it-&gt;value;
2030     if (!trackBuffer.needsReenqueueing &amp;&amp; !m_source-&gt;isSeeking())
2031         provideMediaData(trackBuffer, trackID);
2032 }
2033 
2034 void SourceBuffer::provideMediaData(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID)
2035 {
2036     if (m_source-&gt;isSeeking())
2037         return;
2038 
2039 #if !RELEASE_LOG_DISABLED
2040     unsigned enqueuedSamples = 0;
2041 #endif
2042 
2043     if (trackBuffer.needsMinimumUpcomingPresentationTimeUpdating)
2044         resetMinimumUpcomingPresentationTime(trackBuffer, trackID);
2045 
2046     while (!trackBuffer.decodeQueue.empty()) {
2047         if (!m_private-&gt;isReadyForMoreSamples(trackID)) {
2048             DEBUG_LOG(LOGIDENTIFIER, &quot;bailing early, track id &quot;, trackID, &quot; is not ready for more data&quot;);
2049             m_private-&gt;notifyClientWhenReadyForMoreSamples(trackID);
2050             break;
2051         }
2052 
2053         // FIXME(rdar://problem/20635969): Remove this re-entrancy protection when the aforementioned radar is resolved; protecting
2054         // against re-entrancy introduces a small inefficency when removing appended samples from the decode queue one at a time
2055         // rather than when all samples have been enqueued.
2056         auto sample = trackBuffer.decodeQueue.begin()-&gt;second;
2057 
<a name="28" id="anc28"></a><span class="line-modified">2058         if (sample-&gt;decodeTime() &gt; trackBuffer.enqueueDiscontinuityBoundary) {</span>
<span class="line-modified">2059             DEBUG_LOG(LOGIDENTIFIER, &quot;bailing early because of unbuffered gap, new sample: &quot;, sample-&gt;decodeTime(), &quot; &gt;= the current discontinuity boundary: &quot;, trackBuffer.enqueueDiscontinuityBoundary);</span>











2060             break;
2061         }
2062 
2063         // Remove the sample from the decode queue now.
2064         trackBuffer.decodeQueue.erase(trackBuffer.decodeQueue.begin());
2065 
<a name="29" id="anc29"></a><span class="line-modified">2066         MediaTime samplePresentationEnd = sample-&gt;presentationTime() + sample-&gt;duration();</span>
<span class="line-added">2067         if (trackBuffer.highestEnqueuedPresentationTime.isInvalid() || samplePresentationEnd &gt; trackBuffer.highestEnqueuedPresentationTime)</span>
<span class="line-added">2068             trackBuffer.highestEnqueuedPresentationTime = samplePresentationEnd;</span>
<span class="line-added">2069 </span>
2070         trackBuffer.lastEnqueuedDecodeKey = {sample-&gt;decodeTime(), sample-&gt;presentationTime()};
<a name="30" id="anc30"></a><span class="line-modified">2071         trackBuffer.enqueueDiscontinuityBoundary = sample-&gt;decodeTime() + sample-&gt;duration() + discontinuityTolerance;</span>
<span class="line-added">2072 </span>
2073         m_private-&gt;enqueueSample(sample.releaseNonNull(), trackID);
2074 #if !RELEASE_LOG_DISABLED
2075         ++enqueuedSamples;
2076 #endif
2077     }
2078 
2079     updateMinimumUpcomingPresentationTime(trackBuffer, trackID);
2080 
2081 #if !RELEASE_LOG_DISABLED
2082     DEBUG_LOG(LOGIDENTIFIER, &quot;enqueued &quot;, enqueuedSamples, &quot; samples, &quot;, static_cast&lt;size_t&gt;(trackBuffer.decodeQueue.size()), &quot; remaining&quot;);
2083 #endif
2084 
2085     trySignalAllSamplesInTrackEnqueued(trackID);
2086 }
2087 
2088 void SourceBuffer::updateMinimumUpcomingPresentationTime(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID)
2089 {
2090     if (!m_private-&gt;canSetMinimumUpcomingPresentationTime(trackID))
2091         return;
2092 
2093     if (trackBuffer.decodeQueue.empty()) {
2094         trackBuffer.minimumEnqueuedPresentationTime = MediaTime::invalidTime();
2095         m_private-&gt;clearMinimumUpcomingPresentationTime(trackID);
2096         return;
2097     }
2098 
2099     auto minPts = std::min_element(trackBuffer.decodeQueue.begin(), trackBuffer.decodeQueue.end(), [](auto&amp; left, auto&amp; right) -&gt; bool {
2100         return left.second-&gt;outputPresentationTime() &lt; right.second-&gt;outputPresentationTime();
2101     });
2102 
2103     if (minPts == trackBuffer.decodeQueue.end()) {
2104         trackBuffer.minimumEnqueuedPresentationTime = MediaTime::invalidTime();
2105         m_private-&gt;clearMinimumUpcomingPresentationTime(trackID);
2106         return;
2107     }
2108 
2109     trackBuffer.minimumEnqueuedPresentationTime = minPts-&gt;second-&gt;outputPresentationTime();
2110     m_private-&gt;setMinimumUpcomingPresentationTime(trackID, trackBuffer.minimumEnqueuedPresentationTime);
2111 }
2112 
2113 
2114 void SourceBuffer::resetMinimumUpcomingPresentationTime(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID)
2115 {
2116     if (!m_private-&gt;canSetMinimumUpcomingPresentationTime(trackID))
2117         return;
2118 
2119     trackBuffer.minimumEnqueuedPresentationTime = MediaTime::invalidTime();
2120     m_private-&gt;clearMinimumUpcomingPresentationTime(trackID);
2121 }
2122 
2123 void SourceBuffer::trySignalAllSamplesInTrackEnqueued(const AtomString&amp; trackID)
2124 {
2125     if (m_source-&gt;isEnded() &amp;&amp; m_trackBufferMap.get(trackID).decodeQueue.empty()) {
2126         DEBUG_LOG(LOGIDENTIFIER, &quot;enqueued all samples from track &quot;, trackID);
2127         m_private-&gt;allSamplesInTrackEnqueued(trackID);
2128     }
2129 }
2130 
2131 void SourceBuffer::trySignalAllSamplesEnqueued()
2132 {
2133     for (const AtomString&amp; trackID : m_trackBufferMap.keys())
2134         trySignalAllSamplesInTrackEnqueued(trackID);
2135 }
2136 
2137 void SourceBuffer::reenqueueMediaForTime(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID, const MediaTime&amp; time)
2138 {
2139     m_private-&gt;flush(trackID);
2140     trackBuffer.decodeQueue.clear();
2141 
<a name="31" id="anc31"></a><span class="line-added">2142     trackBuffer.highestEnqueuedPresentationTime = MediaTime::invalidTime();</span>
<span class="line-added">2143     trackBuffer.lastEnqueuedDecodeKey = {MediaTime::invalidTime(), MediaTime::invalidTime()};</span>
<span class="line-added">2144     trackBuffer.enqueueDiscontinuityBoundary = time + discontinuityTolerance;</span>
<span class="line-added">2145 </span>
2146     // Find the sample which contains the current presentation time.
2147     auto currentSamplePTSIterator = trackBuffer.samples.presentationOrder().findSampleContainingPresentationTime(time);
2148 
2149     if (currentSamplePTSIterator == trackBuffer.samples.presentationOrder().end())
2150         currentSamplePTSIterator = trackBuffer.samples.presentationOrder().findSampleStartingOnOrAfterPresentationTime(time);
2151 
2152     if (currentSamplePTSIterator == trackBuffer.samples.presentationOrder().end()
2153         || (currentSamplePTSIterator-&gt;first - time) &gt; MediaSource::currentTimeFudgeFactor())
2154         return;
2155 
2156     // Seach backward for the previous sync sample.
2157     DecodeOrderSampleMap::KeyType decodeKey(currentSamplePTSIterator-&gt;second-&gt;decodeTime(), currentSamplePTSIterator-&gt;second-&gt;presentationTime());
2158     auto currentSampleDTSIterator = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey(decodeKey);
2159     ASSERT(currentSampleDTSIterator != trackBuffer.samples.decodeOrder().end());
2160 
2161     auto reverseCurrentSampleIter = --DecodeOrderSampleMap::reverse_iterator(currentSampleDTSIterator);
2162     auto reverseLastSyncSampleIter = trackBuffer.samples.decodeOrder().findSyncSamplePriorToDecodeIterator(reverseCurrentSampleIter);
2163     if (reverseLastSyncSampleIter == trackBuffer.samples.decodeOrder().rend())
2164         return;
2165 
2166     // Fill the decode queue with the non-displaying samples.
2167     for (auto iter = reverseLastSyncSampleIter; iter != reverseCurrentSampleIter; --iter) {
2168         auto copy = iter-&gt;second-&gt;createNonDisplayingCopy();
2169         DecodeOrderSampleMap::KeyType decodeKey(copy-&gt;decodeTime(), copy-&gt;presentationTime());
2170         trackBuffer.decodeQueue.insert(DecodeOrderSampleMap::MapType::value_type(decodeKey, WTFMove(copy)));
2171     }
2172 
<a name="32" id="anc32"></a>












2173     // Fill the decode queue with the remaining samples.
2174     for (auto iter = currentSampleDTSIterator; iter != trackBuffer.samples.decodeOrder().end(); ++iter)
2175         trackBuffer.decodeQueue.insert(*iter);
2176     provideMediaData(trackBuffer, trackID);
2177 
2178     trackBuffer.needsReenqueueing = false;
2179 }
2180 
2181 
2182 void SourceBuffer::didDropSample()
2183 {
2184     if (!isRemoved())
2185         m_source-&gt;mediaElement()-&gt;incrementDroppedFrameCount();
2186 }
2187 
2188 void SourceBuffer::monitorBufferingRate()
2189 {
2190     MonotonicTime now = MonotonicTime::now();
2191     Seconds interval = now - m_timeOfBufferingMonitor;
2192     double rateSinceLastMonitor = m_bufferedSinceLastMonitor / interval.seconds();
2193 
2194     m_timeOfBufferingMonitor = now;
2195     m_bufferedSinceLastMonitor = 0;
2196 
2197     m_averageBufferRate += (interval.seconds() * ExponentialMovingAverageCoefficient) * (rateSinceLastMonitor - m_averageBufferRate);
2198 
2199     DEBUG_LOG(LOGIDENTIFIER, m_averageBufferRate);
2200 }
2201 
2202 void SourceBuffer::updateBufferedFromTrackBuffers()
2203 {
2204     // 3.1 Attributes, buffered
2205     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-buffered
2206 
2207     // 2. Let highest end time be the largest track buffer ranges end time across all the track buffers managed by this SourceBuffer object.
2208     MediaTime highestEndTime = MediaTime::negativeInfiniteTime();
2209     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
2210         if (!trackBuffer.buffered.length())
2211             continue;
2212         highestEndTime = std::max(highestEndTime, trackBuffer.buffered.maximumBufferedTime());
2213     }
2214 
2215     // NOTE: Short circuit the following if none of the TrackBuffers have buffered ranges to avoid generating
2216     // a single range of {0, 0}.
2217     if (highestEndTime.isNegativeInfinite()) {
2218         m_buffered-&gt;ranges() = PlatformTimeRanges();
2219         return;
2220     }
2221 
2222     // 3. Let intersection ranges equal a TimeRange object containing a single range from 0 to highest end time.
2223     PlatformTimeRanges intersectionRanges { MediaTime::zeroTime(), highestEndTime };
2224 
2225     // 4. For each audio and video track buffer managed by this SourceBuffer, run the following steps:
2226     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
2227         // 4.1 Let track ranges equal the track buffer ranges for the current track buffer.
2228         PlatformTimeRanges trackRanges = trackBuffer.buffered;
2229         if (!trackRanges.length())
2230             continue;
2231 
2232         // 4.2 If readyState is &quot;ended&quot;, then set the end time on the last range in track ranges to highest end time.
2233         if (m_source-&gt;isEnded())
2234             trackRanges.add(trackRanges.maximumBufferedTime(), highestEndTime);
2235 
2236         // 4.3 Let new intersection ranges equal the intersection between the intersection ranges and the track ranges.
2237         // 4.4 Replace the ranges in intersection ranges with the new intersection ranges.
2238         intersectionRanges.intersectWith(trackRanges);
2239     }
2240 
2241     // 5. If intersection ranges does not contain the exact same range information as the current value of this attribute,
2242     //    then update the current value of this attribute to intersection ranges.
2243     m_buffered-&gt;ranges() = intersectionRanges;
2244     setBufferedDirty(true);
2245 }
2246 
2247 bool SourceBuffer::canPlayThroughRange(PlatformTimeRanges&amp; ranges)
2248 {
2249     if (isRemoved())
2250         return false;
2251 
2252     monitorBufferingRate();
2253 
2254     // Assuming no fluctuations in the buffering rate, loading 1 second per second or greater
2255     // means indefinite playback. This could be improved by taking jitter into account.
2256     if (m_averageBufferRate &gt; 1)
2257         return true;
2258 
2259     // Add up all the time yet to be buffered.
2260     MediaTime currentTime = m_source-&gt;currentTime();
2261     MediaTime duration = m_source-&gt;duration();
2262 
2263     PlatformTimeRanges unbufferedRanges = ranges;
2264     unbufferedRanges.invert();
2265     unbufferedRanges.intersectWith(PlatformTimeRanges(currentTime, std::max(currentTime, duration)));
2266     MediaTime unbufferedTime = unbufferedRanges.totalDuration();
2267     if (!unbufferedTime.isValid())
2268         return true;
2269 
2270     MediaTime timeRemaining = duration - currentTime;
2271     return unbufferedTime.toDouble() / m_averageBufferRate &lt; timeRemaining.toDouble();
2272 }
2273 
2274 size_t SourceBuffer::extraMemoryCost() const
2275 {
2276     size_t extraMemoryCost = m_pendingAppendData.capacity();
2277     for (auto&amp; trackBuffer : m_trackBufferMap.values())
2278         extraMemoryCost += trackBuffer.samples.sizeInBytes();
2279 
2280     return extraMemoryCost;
2281 }
2282 
2283 void SourceBuffer::reportExtraMemoryAllocated()
2284 {
2285     size_t extraMemoryCost = this-&gt;extraMemoryCost();
2286     if (extraMemoryCost &lt;= m_reportedExtraMemoryCost)
2287         return;
2288 
2289     size_t extraMemoryCostDelta = extraMemoryCost - m_reportedExtraMemoryCost;
2290     m_reportedExtraMemoryCost = extraMemoryCost;
2291 
2292     JSC::JSLockHolder lock(scriptExecutionContext()-&gt;vm());
2293     // FIXME: Adopt reportExtraMemoryVisited, and switch to reportExtraMemoryAllocated.
2294     // https://bugs.webkit.org/show_bug.cgi?id=142595
2295     scriptExecutionContext()-&gt;vm().heap.deprecatedReportExtraMemory(extraMemoryCostDelta);
2296 }
2297 
2298 Vector&lt;String&gt; SourceBuffer::bufferedSamplesForTrackID(const AtomString&amp; trackID)
2299 {
2300     auto it = m_trackBufferMap.find(trackID);
2301     if (it == m_trackBufferMap.end())
2302         return Vector&lt;String&gt;();
2303 
2304     TrackBuffer&amp; trackBuffer = it-&gt;value;
2305     Vector&lt;String&gt; sampleDescriptions;
2306     for (auto&amp; pair : trackBuffer.samples.decodeOrder())
2307         sampleDescriptions.append(toString(*pair.second));
2308 
2309     return sampleDescriptions;
2310 }
2311 
2312 Vector&lt;String&gt; SourceBuffer::enqueuedSamplesForTrackID(const AtomString&amp; trackID)
2313 {
2314     return m_private-&gt;enqueuedSamplesForTrackID(trackID);
2315 }
2316 
2317 MediaTime SourceBuffer::minimumUpcomingPresentationTimeForTrackID(const AtomString&amp; trackID)
2318 {
2319     return m_private-&gt;minimumUpcomingPresentationTimeForTrackID(trackID);
2320 }
2321 
2322 void SourceBuffer::setMaximumQueueDepthForTrackID(const AtomString&amp; trackID, size_t maxQueueDepth)
2323 {
2324     m_private-&gt;setMaximumQueueDepthForTrackID(trackID, maxQueueDepth);
2325 }
2326 
2327 Document&amp; SourceBuffer::document() const
2328 {
2329     ASSERT(scriptExecutionContext());
2330     return downcast&lt;Document&gt;(*scriptExecutionContext());
2331 }
2332 
2333 ExceptionOr&lt;void&gt; SourceBuffer::setMode(AppendMode newMode)
2334 {
2335     // 3.1 Attributes - mode
2336     // http://www.w3.org/TR/media-source/#widl-SourceBuffer-mode
2337 
2338     // On setting, run the following steps:
2339 
2340     // 1. Let new mode equal the new value being assigned to this attribute.
2341     // 2. If generate timestamps flag equals true and new mode equals &quot;segments&quot;, then throw an InvalidAccessError exception and abort these steps.
2342     if (m_shouldGenerateTimestamps &amp;&amp; newMode == AppendMode::Segments)
2343         return Exception { InvalidAccessError };
2344 
2345     // 3. If this object has been removed from the sourceBuffers attribute of the parent media source, then throw an InvalidStateError exception and abort these steps.
2346     // 4. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
2347     if (isRemoved() || m_updating)
2348         return Exception { InvalidStateError };
2349 
2350     // 5. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
2351     if (m_source-&gt;isEnded()) {
2352         // 5.1. Set the readyState attribute of the parent media source to &quot;open&quot;
2353         // 5.2. Queue a task to fire a simple event named sourceopen at the parent media source.
2354         m_source-&gt;openIfInEndedState();
2355     }
2356 
2357     // 6. If the append state equals PARSING_MEDIA_SEGMENT, then throw an InvalidStateError and abort these steps.
2358     if (m_appendState == ParsingMediaSegment)
2359         return Exception { InvalidStateError };
2360 
2361     // 7. If the new mode equals &quot;sequence&quot;, then set the group start timestamp to the group end timestamp.
2362     if (newMode == AppendMode::Sequence)
2363         m_groupStartTimestamp = m_groupEndTimestamp;
2364 
2365     // 8. Update the attribute to new mode.
2366     m_mode = newMode;
2367 
2368     return { };
2369 }
2370 
2371 #if !RELEASE_LOG_DISABLED
2372 WTFLogChannel&amp; SourceBuffer::logChannel() const
2373 {
2374     return LogMediaSource;
2375 }
2376 #endif
2377 
2378 } // namespace WebCore
2379 
2380 #endif
<a name="33" id="anc33"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="33" type="hidden" />
</body>
</html>