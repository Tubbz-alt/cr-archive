<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGWorklist.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2013-2020 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGWorklist.h&quot;
 28 
 29 #include &quot;CodeBlock.h&quot;
 30 #include &quot;DFGSafepoint.h&quot;
 31 #include &quot;DeferGC.h&quot;
 32 #include &quot;JSCInlines.h&quot;
 33 #include &quot;ReleaseHeapAccessScope.h&quot;
 34 #include &lt;mutex&gt;
 35 
 36 namespace JSC { namespace DFG {
 37 
 38 #if ENABLE(DFG_JIT)
 39 
 40 class Worklist::ThreadBody : public AutomaticThread {
 41 public:
 42     ThreadBody(const AbstractLocker&amp; locker, Worklist&amp; worklist, ThreadData&amp; data, Box&lt;Lock&gt; lock, Ref&lt;AutomaticThreadCondition&gt;&amp;&amp; condition, int relativePriority)
 43         : AutomaticThread(locker, lock, WTFMove(condition))
 44         , m_worklist(worklist)
 45         , m_data(data)
 46         , m_relativePriority(relativePriority)
 47     {
 48     }
 49 
 50     const char* name() const override
 51     {
 52         return m_worklist.m_threadName.data();
 53     }
 54 
 55 protected:
 56     PollResult poll(const AbstractLocker&amp; locker) override
 57     {
 58         if (m_worklist.m_queue.isEmpty())
 59             return PollResult::Wait;
 60 
 61         m_plan = m_worklist.m_queue.takeFirst();
 62         if (!m_plan) {
 63             if (Options::verboseCompilationQueue()) {
 64                 m_worklist.dump(locker, WTF::dataFile());
 65                 dataLog(&quot;: Thread shutting down\n&quot;);
 66             }
 67             return PollResult::Stop;
 68         }
 69         RELEASE_ASSERT(m_plan-&gt;stage() == Plan::Preparing);
 70         m_worklist.m_numberOfActiveThreads++;
 71         return PollResult::Work;
 72     }
 73 
 74     class WorkScope;
 75     friend class WorkScope;
 76     class WorkScope {
 77     public:
 78         WorkScope(ThreadBody&amp; thread)
 79             : m_thread(thread)
 80         {
 81             RELEASE_ASSERT(m_thread.m_plan);
 82             RELEASE_ASSERT(m_thread.m_worklist.m_numberOfActiveThreads);
 83         }
 84 
 85         ~WorkScope()
 86         {
 87             LockHolder locker(*m_thread.m_worklist.m_lock);
 88             m_thread.m_plan = nullptr;
 89             m_thread.m_worklist.m_numberOfActiveThreads--;
 90         }
 91 
 92     private:
 93         ThreadBody&amp; m_thread;
 94     };
 95 
 96     WorkResult work() override
 97     {
 98         WorkScope workScope(*this);
 99 
100         LockHolder locker(m_data.m_rightToRun);
101         {
102             LockHolder locker(*m_worklist.m_lock);
103             if (m_plan-&gt;stage() == Plan::Cancelled)
104                 return WorkResult::Continue;
105             m_plan-&gt;notifyCompiling();
106         }
107 
108         dataLogLnIf(Options::verboseCompilationQueue(), m_worklist, &quot;: Compiling &quot;, m_plan-&gt;key(), &quot; asynchronously&quot;);
109 
110         // There&#39;s no way for the GC to be safepointing since we own rightToRun.
111         if (m_plan-&gt;vm()-&gt;heap.worldIsStopped()) {
112             dataLog(&quot;Heap is stopped but here we are! (1)\n&quot;);
113             RELEASE_ASSERT_NOT_REACHED();
114         }
115         m_plan-&gt;compileInThread(&amp;m_data);
116         if (m_plan-&gt;stage() != Plan::Cancelled) {
117             if (m_plan-&gt;vm()-&gt;heap.worldIsStopped()) {
118                 dataLog(&quot;Heap is stopped but here we are! (2)\n&quot;);
119                 RELEASE_ASSERT_NOT_REACHED();
120             }
121         }
122 
123         {
124             LockHolder locker(*m_worklist.m_lock);
125             if (m_plan-&gt;stage() == Plan::Cancelled)
126                 return WorkResult::Continue;
127 
128             m_plan-&gt;notifyReady();
129 
130             if (Options::verboseCompilationQueue()) {
131                 m_worklist.dump(locker, WTF::dataFile());
132                 dataLog(&quot;: Compiled &quot;, m_plan-&gt;key(), &quot; asynchronously\n&quot;);
133             }
134 
135             RELEASE_ASSERT(!m_plan-&gt;vm()-&gt;heap.worldIsStopped());
136             m_worklist.m_readyPlans.append(WTFMove(m_plan));
137             m_worklist.m_planCompiled.notifyAll();
138         }
139 
140         return WorkResult::Continue;
141     }
142 
143     void threadDidStart() override
144     {
145         dataLogLnIf(Options::verboseCompilationQueue(), m_worklist, &quot;: Thread started&quot;);
146 
147         if (m_relativePriority)
148             Thread::current().changePriority(m_relativePriority);
149 
150         m_compilationScope = makeUnique&lt;CompilationScope&gt;();
151     }
152 
153     void threadIsStopping(const AbstractLocker&amp;) override
154     {
155         // We&#39;re holding the Worklist::m_lock, so we should be careful not to deadlock.
156 
157         dataLogLnIf(Options::verboseCompilationQueue(), m_worklist, &quot;: Thread will stop&quot;);
158 
159         ASSERT(!m_plan);
160 
161         m_compilationScope = nullptr;
162         m_plan = nullptr;
163     }
164 
165 private:
166     Worklist&amp; m_worklist;
167     ThreadData&amp; m_data;
168     int m_relativePriority;
169     std::unique_ptr&lt;CompilationScope&gt; m_compilationScope;
170     RefPtr&lt;Plan&gt; m_plan;
171 };
172 
173 static CString createWorklistName(CString&amp;&amp; tierName)
174 {
175 #if OS(LINUX)
176     return toCString(WTFMove(tierName), &quot;Worker&quot;);
177 #else
178     return toCString(WTFMove(tierName), &quot; Worklist Worker Thread&quot;);
179 #endif
180 }
181 
182 Worklist::Worklist(CString&amp;&amp; tierName)
183     : m_threadName(createWorklistName(WTFMove(tierName)))
184     , m_planEnqueued(AutomaticThreadCondition::create())
185     , m_lock(Box&lt;Lock&gt;::create())
186 {
187 }
188 
189 Worklist::~Worklist()
190 {
191     {
192         LockHolder locker(*m_lock);
193         for (unsigned i = m_threads.size(); i--;)
194             m_queue.append(nullptr); // Use null plan to indicate that we want the thread to terminate.
195         m_planEnqueued-&gt;notifyAll(locker);
196     }
197     for (unsigned i = m_threads.size(); i--;)
198         m_threads[i]-&gt;m_thread-&gt;join();
199     ASSERT(!m_numberOfActiveThreads);
200 }
201 
202 void Worklist::finishCreation(unsigned numberOfThreads, int relativePriority)
203 {
204     RELEASE_ASSERT(numberOfThreads);
205     LockHolder locker(*m_lock);
206     for (unsigned i = numberOfThreads; i--;) {
207         createNewThread(locker, relativePriority);
208     }
209 }
210 
211 void Worklist::createNewThread(const AbstractLocker&amp; locker, int relativePriority)
212 {
213     std::unique_ptr&lt;ThreadData&gt; data = makeUnique&lt;ThreadData&gt;(this);
214     data-&gt;m_thread = adoptRef(new ThreadBody(locker, *this, *data, m_lock, m_planEnqueued.copyRef(), relativePriority));
215     m_threads.append(WTFMove(data));
216 }
217 
218 Ref&lt;Worklist&gt; Worklist::create(CString&amp;&amp; tierName, unsigned numberOfThreads, int relativePriority)
219 {
220     Ref&lt;Worklist&gt; result = adoptRef(*new Worklist(WTFMove(tierName)));
221     result-&gt;finishCreation(numberOfThreads, relativePriority);
222     return result;
223 }
224 
225 bool Worklist::isActiveForVM(VM&amp; vm) const
226 {
227     LockHolder locker(*m_lock);
228     PlanMap::const_iterator end = m_plans.end();
229     for (PlanMap::const_iterator iter = m_plans.begin(); iter != end; ++iter) {
230         if (iter-&gt;value-&gt;vm() == &amp;vm)
231             return true;
232     }
233     return false;
234 }
235 
236 void Worklist::enqueue(Ref&lt;Plan&gt;&amp;&amp; plan)
237 {
238     LockHolder locker(*m_lock);
239     if (Options::verboseCompilationQueue()) {
240         dump(locker, WTF::dataFile());
241         dataLog(&quot;: Enqueueing plan to optimize &quot;, plan-&gt;key(), &quot;\n&quot;);
242     }
243     ASSERT(m_plans.find(plan-&gt;key()) == m_plans.end());
244     m_plans.add(plan-&gt;key(), plan.copyRef());
245     m_queue.append(WTFMove(plan));
246     m_planEnqueued-&gt;notifyOne(locker);
247 }
248 
249 Worklist::State Worklist::compilationState(CompilationKey key)
250 {
251     LockHolder locker(*m_lock);
252     PlanMap::iterator iter = m_plans.find(key);
253     if (iter == m_plans.end())
254         return NotKnown;
255     return iter-&gt;value-&gt;stage() == Plan::Ready ? Compiled : Compiling;
256 }
257 
258 void Worklist::waitUntilAllPlansForVMAreReady(VM&amp; vm)
259 {
260     DeferGC deferGC(vm.heap);
261 
262     // While we are waiting for the compiler to finish, the collector might have already suspended
263     // the compiler and then it will be waiting for us to stop. That&#39;s a deadlock. We avoid that
264     // deadlock by relinquishing our heap access, so that the collector pretends that we are stopped
265     // even if we aren&#39;t.
266     ReleaseHeapAccessScope releaseHeapAccessScope(vm.heap);
267 
268     // Wait for all of the plans for the given VM to complete. The idea here
269     // is that we want all of the caller VM&#39;s plans to be done. We don&#39;t care
270     // about any other VM&#39;s plans, and we won&#39;t attempt to wait on those.
271     // After we release this lock, we know that although other VMs may still
272     // be adding plans, our VM will not be.
273 
274     LockHolder locker(*m_lock);
275 
276     if (Options::verboseCompilationQueue()) {
277         dump(locker, WTF::dataFile());
278         dataLog(&quot;: Waiting for all in VM to complete.\n&quot;);
279     }
280 
281     for (;;) {
282         bool allAreCompiled = true;
283         PlanMap::iterator end = m_plans.end();
284         for (PlanMap::iterator iter = m_plans.begin(); iter != end; ++iter) {
285             if (iter-&gt;value-&gt;vm() != &amp;vm)
286                 continue;
287             if (iter-&gt;value-&gt;stage() != Plan::Ready) {
288                 allAreCompiled = false;
289                 break;
290             }
291         }
292 
293         if (allAreCompiled)
294             break;
295 
296         m_planCompiled.wait(*m_lock);
297     }
298 }
299 
300 void Worklist::removeAllReadyPlansForVM(VM&amp; vm, Vector&lt;RefPtr&lt;Plan&gt;, 8&gt;&amp; myReadyPlans)
301 {
302     DeferGC deferGC(vm.heap);
303     LockHolder locker(*m_lock);
304     for (size_t i = 0; i &lt; m_readyPlans.size(); ++i) {
305         RefPtr&lt;Plan&gt; plan = m_readyPlans[i];
306         if (plan-&gt;vm() != &amp;vm)
307             continue;
308         if (plan-&gt;stage() != Plan::Ready)
309             continue;
310         myReadyPlans.append(plan);
311         m_readyPlans[i--] = m_readyPlans.last();
312         m_readyPlans.removeLast();
313         m_plans.remove(plan-&gt;key());
314     }
315 }
316 
317 void Worklist::removeAllReadyPlansForVM(VM&amp; vm)
318 {
319     Vector&lt;RefPtr&lt;Plan&gt;, 8&gt; myReadyPlans;
320     removeAllReadyPlansForVM(vm, myReadyPlans);
321 }
322 
323 Worklist::State Worklist::completeAllReadyPlansForVM(VM&amp; vm, CompilationKey requestedKey)
324 {
325     DeferGC deferGC(vm.heap);
326     Vector&lt;RefPtr&lt;Plan&gt;, 8&gt; myReadyPlans;
327 
328     removeAllReadyPlansForVM(vm, myReadyPlans);
329 
330     State resultingState = NotKnown;
331 
332     while (!myReadyPlans.isEmpty()) {
333         RefPtr&lt;Plan&gt; plan = myReadyPlans.takeLast();
334         CompilationKey currentKey = plan-&gt;key();
335 
336         dataLogLnIf(Options::verboseCompilationQueue(), *this, &quot;: Completing &quot;, currentKey);
337 
338         RELEASE_ASSERT(plan-&gt;stage() == Plan::Ready);
339 
340         plan-&gt;finalizeAndNotifyCallback();
341 
342         if (currentKey == requestedKey)
343             resultingState = Compiled;
344     }
345 
346     if (!!requestedKey &amp;&amp; resultingState == NotKnown) {
347         LockHolder locker(*m_lock);
348         if (m_plans.contains(requestedKey))
349             resultingState = Compiling;
350     }
351 
352     return resultingState;
353 }
354 
355 void Worklist::completeAllPlansForVM(VM&amp; vm)
356 {
357     DeferGC deferGC(vm.heap);
358     waitUntilAllPlansForVMAreReady(vm);
359     completeAllReadyPlansForVM(vm);
360 }
361 
362 void Worklist::suspendAllThreads()
363 {
364     m_suspensionLock.lock();
365     for (unsigned i = m_threads.size(); i--;)
366         m_threads[i]-&gt;m_rightToRun.lock();
367 }
368 
369 void Worklist::resumeAllThreads()
370 {
371     for (unsigned i = m_threads.size(); i--;)
372         m_threads[i]-&gt;m_rightToRun.unlock();
373     m_suspensionLock.unlock();
374 }
375 
376 void Worklist::visitWeakReferences(SlotVisitor&amp; visitor)
377 {
378     VM* vm = &amp;visitor.heap()-&gt;vm();
379     {
380         LockHolder locker(*m_lock);
381         for (PlanMap::iterator iter = m_plans.begin(); iter != m_plans.end(); ++iter) {
382             Plan* plan = iter-&gt;value.get();
383             if (plan-&gt;vm() != vm)
384                 continue;
385             plan-&gt;checkLivenessAndVisitChildren(visitor);
386         }
387     }
388     // This loop doesn&#39;t need locking because:
389     // (1) no new threads can be added to m_threads. Hence, it is immutable and needs no locks.
390     // (2) ThreadData::m_safepoint is protected by that thread&#39;s m_rightToRun which we must be
391     //     holding here because of a prior call to suspendAllThreads().
392     for (unsigned i = m_threads.size(); i--;) {
393         ThreadData* data = m_threads[i].get();
394         Safepoint* safepoint = data-&gt;m_safepoint;
395         if (safepoint &amp;&amp; safepoint-&gt;vm() == vm)
396             safepoint-&gt;checkLivenessAndVisitChildren(visitor);
397     }
398 }
399 
400 void Worklist::removeDeadPlans(VM&amp; vm)
401 {
402     {
403         LockHolder locker(*m_lock);
404         HashSet&lt;CompilationKey&gt; deadPlanKeys;
405         for (PlanMap::iterator iter = m_plans.begin(); iter != m_plans.end(); ++iter) {
406             Plan* plan = iter-&gt;value.get();
407             if (plan-&gt;vm() != &amp;vm)
408                 continue;
409             if (plan-&gt;isKnownToBeLiveDuringGC()) {
410                 plan-&gt;finalizeInGC();
411                 continue;
412             }
413             RELEASE_ASSERT(plan-&gt;stage() != Plan::Cancelled); // Should not be cancelled, yet.
414             ASSERT(!deadPlanKeys.contains(plan-&gt;key()));
415             deadPlanKeys.add(plan-&gt;key());
416         }
417         if (!deadPlanKeys.isEmpty()) {
418             for (HashSet&lt;CompilationKey&gt;::iterator iter = deadPlanKeys.begin(); iter != deadPlanKeys.end(); ++iter)
419                 m_plans.take(*iter)-&gt;cancel();
420             Deque&lt;RefPtr&lt;Plan&gt;&gt; newQueue;
421             while (!m_queue.isEmpty()) {
422                 RefPtr&lt;Plan&gt; plan = m_queue.takeFirst();
423                 if (plan-&gt;stage() != Plan::Cancelled)
424                     newQueue.append(plan);
425             }
426             m_queue.swap(newQueue);
427             for (unsigned i = 0; i &lt; m_readyPlans.size(); ++i) {
428                 if (m_readyPlans[i]-&gt;stage() != Plan::Cancelled)
429                     continue;
430                 m_readyPlans[i--] = m_readyPlans.last();
431                 m_readyPlans.removeLast();
432             }
433         }
434     }
435 
436     // No locking needed for this part, see comment in visitWeakReferences().
437     for (unsigned i = m_threads.size(); i--;) {
438         ThreadData* data = m_threads[i].get();
439         Safepoint* safepoint = data-&gt;m_safepoint;
440         if (!safepoint)
441             continue;
442         if (safepoint-&gt;vm() != &amp;vm)
443             continue;
444         if (safepoint-&gt;isKnownToBeLiveDuringGC())
445             continue;
446         safepoint-&gt;cancel();
447     }
448 }
449 
450 void Worklist::removeNonCompilingPlansForVM(VM&amp; vm)
451 {
452     LockHolder locker(*m_lock);
453     HashSet&lt;CompilationKey&gt; deadPlanKeys;
454     Vector&lt;RefPtr&lt;Plan&gt;&gt; deadPlans;
455     for (auto&amp; entry : m_plans) {
456         Plan* plan = entry.value.get();
457         if (plan-&gt;vm() != &amp;vm)
458             continue;
459         if (plan-&gt;stage() == Plan::Compiling)
460             continue;
461         deadPlanKeys.add(plan-&gt;key());
462         deadPlans.append(plan);
463     }
464     for (CompilationKey key : deadPlanKeys)
465         m_plans.remove(key);
466     Deque&lt;RefPtr&lt;Plan&gt;&gt; newQueue;
467     while (!m_queue.isEmpty()) {
468         RefPtr&lt;Plan&gt; plan = m_queue.takeFirst();
469         if (!deadPlanKeys.contains(plan-&gt;key()))
470             newQueue.append(WTFMove(plan));
471     }
472     m_queue = WTFMove(newQueue);
473     m_readyPlans.removeAllMatching(
474         [&amp;] (RefPtr&lt;Plan&gt;&amp; plan) -&gt; bool {
475             return deadPlanKeys.contains(plan-&gt;key());
476         });
477     for (auto&amp; plan : deadPlans)
478         plan-&gt;cancel();
479 }
480 
481 size_t Worklist::queueLength()
482 {
483     LockHolder locker(*m_lock);
484     return m_queue.size();
485 }
486 
487 void Worklist::dump(PrintStream&amp; out) const
488 {
489     LockHolder locker(*m_lock);
490     dump(locker, out);
491 }
492 
493 void Worklist::dump(const AbstractLocker&amp;, PrintStream&amp; out) const
494 {
495     out.print(
496         &quot;Worklist(&quot;, RawPointer(this), &quot;)[Queue Length = &quot;, m_queue.size(),
497         &quot;, Map Size = &quot;, m_plans.size(), &quot;, Num Ready = &quot;, m_readyPlans.size(),
498         &quot;, Num Active Threads = &quot;, m_numberOfActiveThreads, &quot;/&quot;, m_threads.size(), &quot;]&quot;);
499 }
500 
501 unsigned Worklist::setNumberOfThreads(unsigned numberOfThreads, int relativePriority)
502 {
503     LockHolder locker(m_suspensionLock);
504     auto currentNumberOfThreads = m_threads.size();
505     if (numberOfThreads &lt; currentNumberOfThreads) {
506         {
507             LockHolder locker(*m_lock);
508             for (unsigned i = currentNumberOfThreads; i-- &gt; numberOfThreads;) {
509                 if (m_threads[i]-&gt;m_thread-&gt;hasUnderlyingThread(locker)) {
510                     m_queue.append(nullptr);
511                     m_threads[i]-&gt;m_thread-&gt;notify(locker);
512                 }
513             }
514         }
515         for (unsigned i = currentNumberOfThreads; i-- &gt; numberOfThreads;) {
516             bool isStopped = false;
517             {
518                 LockHolder locker(*m_lock);
519                 isStopped = m_threads[i]-&gt;m_thread-&gt;tryStop(locker);
520             }
521             if (!isStopped)
522                 m_threads[i]-&gt;m_thread-&gt;join();
523             m_threads.remove(i);
524         }
525         m_threads.shrinkToFit();
526         ASSERT(m_numberOfActiveThreads &lt;= numberOfThreads);
527     } else if (numberOfThreads &gt; currentNumberOfThreads) {
528         LockHolder locker(*m_lock);
529         for (unsigned i = currentNumberOfThreads; i &lt; numberOfThreads; i++)
530             createNewThread(locker, relativePriority);
531     }
532     return currentNumberOfThreads;
533 }
534 
535 static Worklist* theGlobalDFGWorklist;
536 static unsigned numberOfDFGCompilerThreads;
537 static unsigned numberOfFTLCompilerThreads;
538 
539 static unsigned getNumberOfDFGCompilerThreads()
540 {
541     return numberOfDFGCompilerThreads ? numberOfDFGCompilerThreads : Options::numberOfDFGCompilerThreads();
542 }
543 
544 static unsigned getNumberOfFTLCompilerThreads()
545 {
546     return numberOfFTLCompilerThreads ? numberOfFTLCompilerThreads : Options::numberOfFTLCompilerThreads();
547 }
548 
549 unsigned setNumberOfDFGCompilerThreads(unsigned numberOfThreads)
550 {
551     auto previousNumberOfThreads = getNumberOfDFGCompilerThreads();
552     numberOfDFGCompilerThreads = numberOfThreads;
553     return previousNumberOfThreads;
554 }
555 
556 unsigned setNumberOfFTLCompilerThreads(unsigned numberOfThreads)
557 {
558     auto previousNumberOfThreads = getNumberOfFTLCompilerThreads();
559     numberOfFTLCompilerThreads = numberOfThreads;
560     return previousNumberOfThreads;
561 }
562 
563 Worklist&amp; ensureGlobalDFGWorklist()
564 {
565     static std::once_flag initializeGlobalWorklistOnceFlag;
566     std::call_once(initializeGlobalWorklistOnceFlag, [] {
567         Worklist* worklist = &amp;Worklist::create(&quot;DFG&quot;, getNumberOfDFGCompilerThreads(), Options::priorityDeltaOfDFGCompilerThreads()).leakRef();
568         WTF::storeStoreFence();
569         theGlobalDFGWorklist = worklist;
570     });
571     return *theGlobalDFGWorklist;
572 }
573 
574 Worklist* existingGlobalDFGWorklistOrNull()
575 {
576     return theGlobalDFGWorklist;
577 }
578 
579 static Worklist* theGlobalFTLWorklist;
580 
581 Worklist&amp; ensureGlobalFTLWorklist()
582 {
583     static std::once_flag initializeGlobalWorklistOnceFlag;
584     std::call_once(initializeGlobalWorklistOnceFlag, [] {
585         Worklist* worklist = &amp;Worklist::create(&quot;FTL&quot;, getNumberOfFTLCompilerThreads(), Options::priorityDeltaOfFTLCompilerThreads()).leakRef();
586         WTF::storeStoreFence();
587         theGlobalFTLWorklist = worklist;
588     });
589     return *theGlobalFTLWorklist;
590 }
591 
592 Worklist* existingGlobalFTLWorklistOrNull()
593 {
594     return theGlobalFTLWorklist;
595 }
596 
597 Worklist&amp; ensureGlobalWorklistFor(CompilationMode mode)
598 {
599     switch (mode) {
600     case InvalidCompilationMode:
601         RELEASE_ASSERT_NOT_REACHED();
602         return ensureGlobalDFGWorklist();
603     case DFGMode:
604         return ensureGlobalDFGWorklist();
605     case FTLMode:
606     case FTLForOSREntryMode:
607         return ensureGlobalFTLWorklist();
608     }
609     RELEASE_ASSERT_NOT_REACHED();
610     return ensureGlobalDFGWorklist();
611 }
612 
613 unsigned numberOfWorklists() { return 2; }
614 
615 Worklist&amp; ensureWorklistForIndex(unsigned index)
616 {
617     switch (index) {
618     case 0:
619         return ensureGlobalDFGWorklist();
620     case 1:
621         return ensureGlobalFTLWorklist();
622     default:
623         RELEASE_ASSERT_NOT_REACHED();
624         return ensureGlobalDFGWorklist();
625     }
626 }
627 
628 Worklist* existingWorklistForIndexOrNull(unsigned index)
629 {
630     switch (index) {
631     case 0:
632         return existingGlobalDFGWorklistOrNull();
633     case 1:
634         return existingGlobalFTLWorklistOrNull();
635     default:
636         RELEASE_ASSERT_NOT_REACHED();
637         return 0;
638     }
639 }
640 
641 Worklist&amp; existingWorklistForIndex(unsigned index)
642 {
643     Worklist* result = existingWorklistForIndexOrNull(index);
644     RELEASE_ASSERT(result);
645     return *result;
646 }
647 
648 void completeAllPlansForVM(VM&amp; vm)
649 {
650     for (unsigned i = DFG::numberOfWorklists(); i--;) {
651         if (DFG::Worklist* worklist = DFG::existingWorklistForIndexOrNull(i))
652             worklist-&gt;completeAllPlansForVM(vm);
653     }
654 }
655 
656 #else // ENABLE(DFG_JIT)
657 
658 void completeAllPlansForVM(VM&amp;)
659 {
660 }
661 
662 #endif // ENABLE(DFG_JIT)
663 
664 } } // namespace JSC::DFG
665 
666 
    </pre>
  </body>
</html>