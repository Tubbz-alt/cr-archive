<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="Gigacage.h.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.cdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (C) 2014-2018 Apple Inc. All rights reserved.</span>
   *
   * Redistribution and use in source and binary forms, with or without
   * modification, are permitted provided that the following conditions
   * are met:
   * 1. Redistributions of source code must retain the above copyright
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (C) 2014-2019 Apple Inc. All rights reserved.</span>
   *
   * Redistribution and use in source and binary forms, with or without
   * modification, are permitted provided that the following conditions
   * are met:
   * 1. Redistributions of source code must retain the above copyright
</pre>
<hr />
<pre>
<span class="line-old-header">*** 28,34 ***</span>
  #include &quot;AvailableMemory.h&quot;
  #include &quot;BulkDecommit.h&quot;
  #include &quot;BumpAllocator.h&quot;
  #include &quot;Chunk.h&quot;
  #include &quot;CryptoRandom.h&quot;
  #include &quot;Environment.h&quot;
  #include &quot;Gigacage.h&quot;
<span class="line-modified">! #include &quot;DebugHeap.h&quot;</span>
  #include &quot;PerProcess.h&quot;
  #include &quot;Scavenger.h&quot;
  #include &quot;SmallLine.h&quot;
  #include &quot;SmallPage.h&quot;
<span class="line-removed">- #include &quot;VMHeap.h&quot;</span>
  #include &quot;bmalloc.h&quot;
  #include &lt;thread&gt;
  #include &lt;vector&gt;
  
  namespace bmalloc {
  
<span class="line-modified">! Heap::Heap(HeapKind kind, std::lock_guard&lt;Mutex&gt;&amp;)</span>
<span class="line-modified">!     : m_kind(kind)</span>
<span class="line-removed">-     , m_vmPageSizePhysical(vmPageSizePhysical())</span>
  {
<span class="line-removed">-     RELEASE_BASSERT(vmPageSizePhysical() &gt;= smallPageSize);</span>
<span class="line-removed">-     RELEASE_BASSERT(vmPageSize() &gt;= vmPageSizePhysical());</span>
<span class="line-removed">- </span>
<span class="line-removed">-     initializeLineMetadata();</span>
<span class="line-removed">-     initializePageMetadata();</span>
<span class="line-removed">- </span>
      BASSERT(!Environment::get()-&gt;isDebugHeapEnabled());
  
      Gigacage::ensureGigacage();
  #if GIGACAGE_ENABLED
      if (usingGigacage()) {
<span class="line-new-header">--- 28,31 ---</span>
  #include &quot;AvailableMemory.h&quot;
  #include &quot;BulkDecommit.h&quot;
  #include &quot;BumpAllocator.h&quot;
  #include &quot;Chunk.h&quot;
  #include &quot;CryptoRandom.h&quot;
<span class="line-added">+ #include &quot;DebugHeap.h&quot;</span>
  #include &quot;Environment.h&quot;
  #include &quot;Gigacage.h&quot;
<span class="line-modified">! #include &quot;HeapConstants.h&quot;</span>
  #include &quot;PerProcess.h&quot;
  #include &quot;Scavenger.h&quot;
  #include &quot;SmallLine.h&quot;
  #include &quot;SmallPage.h&quot;
  #include &quot;bmalloc.h&quot;
  #include &lt;thread&gt;
  #include &lt;vector&gt;
  
<span class="line-added">+ #if BOS(DARWIN)</span>
<span class="line-added">+ #include &quot;Zone.h&quot;</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
  namespace bmalloc {
  
<span class="line-modified">! Heap::Heap(HeapKind kind, LockHolder&amp;)</span>
<span class="line-modified">!     : m_kind { kind }, m_constants { *HeapConstants::get() }</span>
  {
      BASSERT(!Environment::get()-&gt;isDebugHeapEnabled());
  
      Gigacage::ensureGigacage();
  #if GIGACAGE_ENABLED
      if (usingGigacage()) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 85,84 ***</span>
  size_t Heap::gigacageSize()
  {
      return Gigacage::size(gigacageKind(m_kind));
  }
  
<span class="line-modified">! void Heap::initializeLineMetadata()</span>
<span class="line-removed">- {</span>
<span class="line-removed">-     size_t sizeClassCount = bmalloc::sizeClass(smallLineSize);</span>
<span class="line-removed">-     size_t smallLineCount = m_vmPageSizePhysical / smallLineSize;</span>
<span class="line-removed">-     m_smallLineMetadata.grow(sizeClassCount * smallLineCount);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     for (size_t sizeClass = 0; sizeClass &lt; sizeClassCount; ++sizeClass) {</span>
<span class="line-removed">-         size_t size = objectSize(sizeClass);</span>
<span class="line-removed">-         LineMetadata* pageMetadata = &amp;m_smallLineMetadata[sizeClass * smallLineCount];</span>
<span class="line-removed">- </span>
<span class="line-removed">-         size_t object = 0;</span>
<span class="line-removed">-         size_t line = 0;</span>
<span class="line-removed">-         while (object &lt; m_vmPageSizePhysical) {</span>
<span class="line-removed">-             line = object / smallLineSize;</span>
<span class="line-removed">-             size_t leftover = object % smallLineSize;</span>
<span class="line-removed">- </span>
<span class="line-removed">-             size_t objectCount;</span>
<span class="line-removed">-             size_t remainder;</span>
<span class="line-removed">-             divideRoundingUp(smallLineSize - leftover, size, objectCount, remainder);</span>
<span class="line-removed">- </span>
<span class="line-removed">-             pageMetadata[line] = { static_cast&lt;unsigned char&gt;(leftover), static_cast&lt;unsigned char&gt;(objectCount) };</span>
<span class="line-removed">- </span>
<span class="line-removed">-             object += objectCount * size;</span>
<span class="line-removed">-         }</span>
<span class="line-removed">- </span>
<span class="line-removed">-         // Don&#39;t allow the last object in a page to escape the page.</span>
<span class="line-removed">-         if (object &gt; m_vmPageSizePhysical) {</span>
<span class="line-removed">-             BASSERT(pageMetadata[line].objectCount);</span>
<span class="line-removed">-             --pageMetadata[line].objectCount;</span>
<span class="line-removed">-         }</span>
<span class="line-removed">-     }</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
<span class="line-removed">- void Heap::initializePageMetadata()</span>
<span class="line-removed">- {</span>
<span class="line-removed">-     auto computePageSize = [&amp;](size_t sizeClass) {</span>
<span class="line-removed">-         size_t size = objectSize(sizeClass);</span>
<span class="line-removed">-         if (sizeClass &lt; bmalloc::sizeClass(smallLineSize))</span>
<span class="line-removed">-             return m_vmPageSizePhysical;</span>
<span class="line-removed">- </span>
<span class="line-removed">-         for (size_t pageSize = m_vmPageSizePhysical;</span>
<span class="line-removed">-             pageSize &lt; pageSizeMax;</span>
<span class="line-removed">-             pageSize += m_vmPageSizePhysical) {</span>
<span class="line-removed">-             RELEASE_BASSERT(pageSize &lt;= chunkSize / 2);</span>
<span class="line-removed">-             size_t waste = pageSize % size;</span>
<span class="line-removed">-             if (waste &lt;= pageSize / pageSizeWasteFactor)</span>
<span class="line-removed">-                 return pageSize;</span>
<span class="line-removed">-         }</span>
<span class="line-removed">- </span>
<span class="line-removed">-         return pageSizeMax;</span>
<span class="line-removed">-     };</span>
<span class="line-removed">- </span>
<span class="line-removed">-     for (size_t i = 0; i &lt; sizeClassCount; ++i)</span>
<span class="line-removed">-         m_pageClasses[i] = (computePageSize(i) - 1) / smallPageSize;</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
<span class="line-removed">- size_t Heap::freeableMemory(std::lock_guard&lt;Mutex&gt;&amp;)</span>
  {
      return m_freeableMemory;
  }
  
  size_t Heap::footprint()
  {
      return m_footprint;
  }
  
<span class="line-modified">! void Heap::markAllLargeAsEligibile(std::lock_guard&lt;Mutex&gt;&amp;)</span>
  {
      m_largeFree.markAllAsEligibile();
      m_hasPendingDecommits = false;
      m_condition.notify_all();
  }
  
<span class="line-modified">! void Heap::decommitLargeRange(std::lock_guard&lt;Mutex&gt;&amp;, LargeRange&amp; range, BulkDecommit&amp; decommitter)</span>
  {
      m_footprint -= range.totalPhysicalSize();
      m_freeableMemory -= range.totalPhysicalSize();
      decommitter.addLazy(range.begin(), range.size());
      m_hasPendingDecommits = true;
<span class="line-new-header">--- 82,28 ---</span>
  size_t Heap::gigacageSize()
  {
      return Gigacage::size(gigacageKind(m_kind));
  }
  
<span class="line-modified">! size_t Heap::freeableMemory(const LockHolder&amp;)</span>
  {
      return m_freeableMemory;
  }
  
  size_t Heap::footprint()
  {
      return m_footprint;
  }
  
<span class="line-modified">! void Heap::markAllLargeAsEligibile(const LockHolder&amp;)</span>
  {
      m_largeFree.markAllAsEligibile();
      m_hasPendingDecommits = false;
      m_condition.notify_all();
  }
  
<span class="line-modified">! void Heap::decommitLargeRange(const LockHolder&amp;, LargeRange&amp; range, BulkDecommit&amp; decommitter)</span>
  {
      m_footprint -= range.totalPhysicalSize();
      m_freeableMemory -= range.totalPhysicalSize();
      decommitter.addLazy(range.begin(), range.size());
      m_hasPendingDecommits = true;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 173,22 ***</span>
  #if ENABLE_PHYSICAL_PAGE_MAP
      m_physicalPageMap.decommit(range.begin(), range.size());
  #endif
  }
  
<span class="line-modified">! void Heap::scavenge(std::lock_guard&lt;Mutex&gt;&amp; lock, BulkDecommit&amp; decommitter, size_t&amp; deferredDecommits)</span>
  {
      for (auto&amp; list : m_freePages) {
          for (auto* chunk : list) {
              for (auto* page : chunk-&gt;freePages()) {
                  if (!page-&gt;hasPhysicalPages())
                      continue;
                  if (page-&gt;usedSinceLastScavenge()) {
                      page-&gt;clearUsedSinceLastScavenge();
                      deferredDecommits++;
                      continue;
                  }
  
                  size_t pageSize = bmalloc::pageSize(&amp;list - &amp;m_freePages[0]);
                  size_t decommitSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
                  m_freeableMemory -= decommitSize;
                  m_footprint -= decommitSize;
<span class="line-new-header">--- 114,28 ---</span>
  #if ENABLE_PHYSICAL_PAGE_MAP
      m_physicalPageMap.decommit(range.begin(), range.size());
  #endif
  }
  
<span class="line-modified">! #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">+ void Heap::scavenge(const LockHolder&amp; lock, BulkDecommit&amp; decommitter)</span>
<span class="line-added">+ #else</span>
<span class="line-added">+ void Heap::scavenge(const LockHolder&amp; lock, BulkDecommit&amp; decommitter, size_t&amp; deferredDecommits)</span>
<span class="line-added">+ #endif</span>
  {
      for (auto&amp; list : m_freePages) {
          for (auto* chunk : list) {
              for (auto* page : chunk-&gt;freePages()) {
                  if (!page-&gt;hasPhysicalPages())
                      continue;
<span class="line-added">+ #if !BUSE(PARTIAL_SCAVENGE)</span>
                  if (page-&gt;usedSinceLastScavenge()) {
                      page-&gt;clearUsedSinceLastScavenge();
                      deferredDecommits++;
                      continue;
                  }
<span class="line-added">+ #endif</span>
  
                  size_t pageSize = bmalloc::pageSize(&amp;list - &amp;m_freePages[0]);
                  size_t decommitSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
                  m_freeableMemory -= decommitSize;
                  m_footprint -= decommitSize;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 205,60 ***</span>
          while (!list.isEmpty())
              deallocateSmallChunk(list.pop(), &amp;list - &amp;m_chunkCache[0]);
      }
  
      for (LargeRange&amp; range : m_largeFree) {
          if (range.usedSinceLastScavenge()) {
              range.clearUsedSinceLastScavenge();
              deferredDecommits++;
              continue;
          }
          decommitLargeRange(lock, range, decommitter);
      }
  }
  
<span class="line-modified">! void Heap::deallocateLineCache(std::unique_lock&lt;Mutex&gt;&amp;, LineCache&amp; lineCache)</span>
  {
      for (auto&amp; list : lineCache) {
          while (!list.isEmpty()) {
              size_t sizeClass = &amp;list - &amp;lineCache[0];
              m_lineCache[sizeClass].push(list.popFront());
          }
      }
  }
  
<span class="line-modified">! void Heap::allocateSmallChunk(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t pageClass)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      size_t pageSize = bmalloc::pageSize(pageClass);
  
<span class="line-modified">!     Chunk* chunk = [&amp;]() {</span>
          if (!m_chunkCache[pageClass].isEmpty())
              return m_chunkCache[pageClass].pop();
  
<span class="line-modified">!         void* memory = allocateLarge(lock, chunkSize, chunkSize);</span>
  
          Chunk* chunk = new (memory) Chunk(pageSize);
  
          m_objectTypes.set(chunk, ObjectType::Small);
  
          forEachPage(chunk, pageSize, [&amp;](SmallPage* page) {
              page-&gt;setHasPhysicalPages(true);
              page-&gt;setUsedSinceLastScavenge();
              page-&gt;setHasFreeLines(lock, true);
              chunk-&gt;freePages().push(page);
          });
  
<span class="line-modified">!         m_freeableMemory += chunkSize;</span>
  
          m_scavenger-&gt;schedule(0);
  
          return chunk;
      }();
  
<span class="line-modified">!     m_freePages[pageClass].push(chunk);</span>
  }
  
  void Heap::deallocateSmallChunk(Chunk* chunk, size_t pageClass)
  {
      m_objectTypes.set(chunk, ObjectType::Large);
<span class="line-new-header">--- 152,98 ---</span>
          while (!list.isEmpty())
              deallocateSmallChunk(list.pop(), &amp;list - &amp;m_chunkCache[0]);
      }
  
      for (LargeRange&amp; range : m_largeFree) {
<span class="line-added">+ #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">+         m_highWatermark = std::min(m_highWatermark, static_cast&lt;void*&gt;(range.begin()));</span>
<span class="line-added">+ #else</span>
          if (range.usedSinceLastScavenge()) {
              range.clearUsedSinceLastScavenge();
              deferredDecommits++;
              continue;
          }
<span class="line-added">+ #endif</span>
          decommitLargeRange(lock, range, decommitter);
      }
<span class="line-added">+ </span>
<span class="line-added">+ #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">+     m_freeableMemory = 0;</span>
<span class="line-added">+ #endif</span>
  }
  
<span class="line-modified">! #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">+ void Heap::scavengeToHighWatermark(const LockHolder&amp; lock, BulkDecommit&amp; decommitter)</span>
<span class="line-added">+ {</span>
<span class="line-added">+     void* newHighWaterMark = nullptr;</span>
<span class="line-added">+     for (LargeRange&amp; range : m_largeFree) {</span>
<span class="line-added">+         if (range.begin() &lt;= m_highWatermark)</span>
<span class="line-added">+             newHighWaterMark = std::min(newHighWaterMark, static_cast&lt;void*&gt;(range.begin()));</span>
<span class="line-added">+         else</span>
<span class="line-added">+             decommitLargeRange(lock, range, decommitter);</span>
<span class="line-added">+     }</span>
<span class="line-added">+     m_highWatermark = newHighWaterMark;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+ void Heap::deallocateLineCache(UniqueLockHolder&amp;, LineCache&amp; lineCache)</span>
  {
      for (auto&amp; list : lineCache) {
          while (!list.isEmpty()) {
              size_t sizeClass = &amp;list - &amp;lineCache[0];
              m_lineCache[sizeClass].push(list.popFront());
          }
      }
  }
  
<span class="line-modified">! void Heap::allocateSmallChunk(UniqueLockHolder&amp; lock, size_t pageClass, FailureAction action)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      size_t pageSize = bmalloc::pageSize(pageClass);
  
<span class="line-modified">!     Chunk* chunk = [&amp;]() -&gt; Chunk* {</span>
          if (!m_chunkCache[pageClass].isEmpty())
              return m_chunkCache[pageClass].pop();
  
<span class="line-modified">!         void* memory = allocateLarge(lock, chunkSize, chunkSize, action);</span>
<span class="line-added">+         if (!memory) {</span>
<span class="line-added">+             BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">+             return nullptr;</span>
<span class="line-added">+         }</span>
  
          Chunk* chunk = new (memory) Chunk(pageSize);
  
          m_objectTypes.set(chunk, ObjectType::Small);
  
<span class="line-added">+         size_t accountedInFreeable = 0;</span>
          forEachPage(chunk, pageSize, [&amp;](SmallPage* page) {
              page-&gt;setHasPhysicalPages(true);
<span class="line-added">+ #if !BUSE(PARTIAL_SCAVENGE)</span>
              page-&gt;setUsedSinceLastScavenge();
<span class="line-added">+ #endif</span>
              page-&gt;setHasFreeLines(lock, true);
              chunk-&gt;freePages().push(page);
<span class="line-added">+             accountedInFreeable += pageSize;</span>
          });
  
<span class="line-modified">!         m_freeableMemory += accountedInFreeable;</span>
<span class="line-added">+ </span>
<span class="line-added">+         auto metadataSize = Chunk::metadataSize(pageSize);</span>
<span class="line-added">+         vmDeallocatePhysicalPagesSloppy(chunk-&gt;address(sizeof(Chunk)), metadataSize - sizeof(Chunk));</span>
<span class="line-added">+ </span>
<span class="line-added">+         auto decommitSize = chunkSize - metadataSize - accountedInFreeable;</span>
<span class="line-added">+         if (decommitSize &gt; 0)</span>
<span class="line-added">+             vmDeallocatePhysicalPagesSloppy(chunk-&gt;address(chunkSize - decommitSize), decommitSize);</span>
  
          m_scavenger-&gt;schedule(0);
  
          return chunk;
      }();
  
<span class="line-modified">!     if (chunk)</span>
<span class="line-added">+         m_freePages[pageClass].push(chunk);</span>
  }
  
  void Heap::deallocateSmallChunk(Chunk* chunk, size_t pageClass)
  {
      m_objectTypes.set(chunk, ObjectType::Large);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 283,11 ***</span>
  
      size_t startPhysicalSize = hasPhysicalPages ? size : 0;
      m_largeFree.add(LargeRange(chunk, size, startPhysicalSize, totalPhysicalSize));
  }
  
<span class="line-modified">! SmallPage* Heap::allocateSmallPage(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass, LineCache&amp; lineCache)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      if (!lineCache[sizeClass].isEmpty())
          return lineCache[sizeClass].popFront();
<span class="line-new-header">--- 268,11 ---</span>
  
      size_t startPhysicalSize = hasPhysicalPages ? size : 0;
      m_largeFree.add(LargeRange(chunk, size, startPhysicalSize, totalPhysicalSize));
  }
  
<span class="line-modified">! SmallPage* Heap::allocateSmallPage(UniqueLockHolder&amp; lock, size_t sizeClass, LineCache&amp; lineCache, FailureAction action)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      if (!lineCache[sizeClass].isEmpty())
          return lineCache[sizeClass].popFront();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 295,15 ***</span>
      if (!m_lineCache[sizeClass].isEmpty())
          return m_lineCache[sizeClass].popFront();
  
      m_scavenger-&gt;didStartGrowing();
  
<span class="line-modified">!     SmallPage* page = [&amp;]() {</span>
<span class="line-modified">!         size_t pageClass = m_pageClasses[sizeClass];</span>
  
          if (m_freePages[pageClass].isEmpty())
<span class="line-modified">!             allocateSmallChunk(lock, pageClass);</span>
  
          Chunk* chunk = m_freePages[pageClass].tail();
  
          chunk-&gt;ref();
  
<span class="line-new-header">--- 280,17 ---</span>
      if (!m_lineCache[sizeClass].isEmpty())
          return m_lineCache[sizeClass].popFront();
  
      m_scavenger-&gt;didStartGrowing();
  
<span class="line-modified">!     SmallPage* page = [&amp;]() -&gt; SmallPage* {</span>
<span class="line-modified">!         size_t pageClass = m_constants.pageClass(sizeClass);</span>
  
          if (m_freePages[pageClass].isEmpty())
<span class="line-modified">!             allocateSmallChunk(lock, pageClass, action);</span>
<span class="line-added">+         if (action == FailureAction::ReturnNull &amp;&amp; m_freePages[pageClass].isEmpty())</span>
<span class="line-added">+             return nullptr;</span>
  
          Chunk* chunk = m_freePages[pageClass].tail();
  
          chunk-&gt;ref();
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 322,20 ***</span>
              page-&gt;setHasPhysicalPages(true);
  #if ENABLE_PHYSICAL_PAGE_MAP
              m_physicalPageMap.commit(page-&gt;begin()-&gt;begin(), pageSize);
  #endif
          }
          page-&gt;setUsedSinceLastScavenge();
  
          return page;
      }();
  
      page-&gt;setSizeClass(sizeClass);
      return page;
  }
  
<span class="line-modified">! void Heap::deallocateSmallLine(std::unique_lock&lt;Mutex&gt;&amp; lock, Object object, LineCache&amp; lineCache)</span>
  {
      BASSERT(!object.line()-&gt;refCount(lock));
      SmallPage* page = object.page();
      page-&gt;deref(lock);
  
<span class="line-new-header">--- 309,26 ---</span>
              page-&gt;setHasPhysicalPages(true);
  #if ENABLE_PHYSICAL_PAGE_MAP
              m_physicalPageMap.commit(page-&gt;begin()-&gt;begin(), pageSize);
  #endif
          }
<span class="line-added">+ #if !BUSE(PARTIAL_SCAVENGE)</span>
          page-&gt;setUsedSinceLastScavenge();
<span class="line-added">+ #endif</span>
  
          return page;
      }();
<span class="line-added">+     if (!page) {</span>
<span class="line-added">+         BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">+         return nullptr;</span>
<span class="line-added">+     }</span>
  
      page-&gt;setSizeClass(sizeClass);
      return page;
  }
  
<span class="line-modified">! void Heap::deallocateSmallLine(UniqueLockHolder&amp; lock, Object object, LineCache&amp; lineCache)</span>
  {
      BASSERT(!object.line()-&gt;refCount(lock));
      SmallPage* page = object.page();
      page-&gt;deref(lock);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 345,12 ***</span>
      }
  
      if (page-&gt;refCount(lock))
          return;
  
<span class="line-modified">!     size_t sizeClass = page-&gt;sizeClass();</span>
<span class="line-removed">-     size_t pageClass = m_pageClasses[sizeClass];</span>
  
      m_freeableMemory += physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize(pageClass));
  
      List&lt;SmallPage&gt;::remove(page); // &#39;page&#39; may be in any thread&#39;s line cache.
  
<span class="line-new-header">--- 338,11 ---</span>
      }
  
      if (page-&gt;refCount(lock))
          return;
  
<span class="line-modified">!     size_t pageClass = m_constants.pageClass(page-&gt;sizeClass());</span>
  
      m_freeableMemory += physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize(pageClass));
  
      List&lt;SmallPage&gt;::remove(page); // &#39;page&#39; may be in any thread&#39;s line cache.
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 372,62 ***</span>
  
      m_scavenger-&gt;schedule(pageSize(pageClass));
  }
  
  void Heap::allocateSmallBumpRangesByMetadata(
<span class="line-modified">!     std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass,</span>
      BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">!     LineCache&amp; lineCache)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
<span class="line-modified">!     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache);</span>
      SmallLine* lines = page-&gt;begin();
      BASSERT(page-&gt;hasFreeLines(lock));
<span class="line-removed">-     size_t smallLineCount = m_vmPageSizePhysical / smallLineSize;</span>
<span class="line-removed">-     LineMetadata* pageMetadata = &amp;m_smallLineMetadata[sizeClass * smallLineCount];</span>
  
      auto findSmallBumpRange = [&amp;](size_t&amp; lineNumber) {
<span class="line-modified">!         for ( ; lineNumber &lt; smallLineCount; ++lineNumber) {</span>
              if (!lines[lineNumber].refCount(lock)) {
<span class="line-modified">!                 if (pageMetadata[lineNumber].objectCount)</span>
                      return true;
              }
          }
          return false;
      };
  
      auto allocateSmallBumpRange = [&amp;](size_t&amp; lineNumber) -&gt; BumpRange {
<span class="line-modified">!         char* begin = lines[lineNumber].begin() + pageMetadata[lineNumber].startOffset;</span>
          unsigned short objectCount = 0;
  
<span class="line-modified">!         for ( ; lineNumber &lt; smallLineCount; ++lineNumber) {</span>
              if (lines[lineNumber].refCount(lock))
                  break;
  
<span class="line-modified">!             if (!pageMetadata[lineNumber].objectCount)</span>
                  continue;
  
<span class="line-modified">!             objectCount += pageMetadata[lineNumber].objectCount;</span>
<span class="line-modified">!             lines[lineNumber].ref(lock, pageMetadata[lineNumber].objectCount);</span>
              page-&gt;ref(lock);
          }
          return { begin, objectCount };
      };
  
      size_t lineNumber = 0;
      for (;;) {
          if (!findSmallBumpRange(lineNumber)) {
              page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">!             BASSERT(allocator.canAllocate());</span>
              return;
          }
  
          // In a fragmented page, some free ranges might not fit in the cache.
          if (rangeCache.size() == rangeCache.capacity()) {
              lineCache[sizeClass].push(page);
<span class="line-modified">!             BASSERT(allocator.canAllocate());</span>
              return;
          }
  
          BumpRange bumpRange = allocateSmallBumpRange(lineNumber);
          if (allocator.canAllocate())
<span class="line-new-header">--- 364,66 ---</span>
  
      m_scavenger-&gt;schedule(pageSize(pageClass));
  }
  
  void Heap::allocateSmallBumpRangesByMetadata(
<span class="line-modified">!     UniqueLockHolder&amp; lock, size_t sizeClass,</span>
      BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">!     LineCache&amp; lineCache, FailureAction action)</span>
  {
<span class="line-added">+     BUNUSED(action);</span>
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
<span class="line-modified">!     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache, action);</span>
<span class="line-added">+     if (!page) {</span>
<span class="line-added">+         BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">+         return;</span>
<span class="line-added">+     }</span>
      SmallLine* lines = page-&gt;begin();
      BASSERT(page-&gt;hasFreeLines(lock));
  
      auto findSmallBumpRange = [&amp;](size_t&amp; lineNumber) {
<span class="line-modified">!         for ( ; lineNumber &lt; m_constants.smallLineCount(); ++lineNumber) {</span>
              if (!lines[lineNumber].refCount(lock)) {
<span class="line-modified">!                 if (m_constants.objectCount(sizeClass, lineNumber))</span>
                      return true;
              }
          }
          return false;
      };
  
      auto allocateSmallBumpRange = [&amp;](size_t&amp; lineNumber) -&gt; BumpRange {
<span class="line-modified">!         char* begin = lines[lineNumber].begin() + m_constants.startOffset(sizeClass, lineNumber);</span>
          unsigned short objectCount = 0;
  
<span class="line-modified">!         for ( ; lineNumber &lt; m_constants.smallLineCount(); ++lineNumber) {</span>
              if (lines[lineNumber].refCount(lock))
                  break;
  
<span class="line-modified">!             auto lineObjectCount = m_constants.objectCount(sizeClass, lineNumber);</span>
<span class="line-added">+             if (!lineObjectCount)</span>
                  continue;
  
<span class="line-modified">!             objectCount += lineObjectCount;</span>
<span class="line-modified">!             lines[lineNumber].ref(lock, lineObjectCount);</span>
              page-&gt;ref(lock);
          }
          return { begin, objectCount };
      };
  
      size_t lineNumber = 0;
      for (;;) {
          if (!findSmallBumpRange(lineNumber)) {
              page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">!             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
              return;
          }
  
          // In a fragmented page, some free ranges might not fit in the cache.
          if (rangeCache.size() == rangeCache.capacity()) {
              lineCache[sizeClass].push(page);
<span class="line-modified">!             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
              return;
          }
  
          BumpRange bumpRange = allocateSmallBumpRange(lineNumber);
          if (allocator.canAllocate())
</pre>
<hr />
<pre>
<span class="line-old-header">*** 436,18 ***</span>
              allocator.refill(bumpRange);
      }
  }
  
  void Heap::allocateSmallBumpRangesByObject(
<span class="line-modified">!     std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass,</span>
      BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">!     LineCache&amp; lineCache)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      size_t size = allocator.size();
<span class="line-modified">!     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache);</span>
      BASSERT(page-&gt;hasFreeLines(lock));
  
      auto findSmallBumpRange = [&amp;](Object&amp; it, Object&amp; end) {
          for ( ; it + size &lt;= end; it = it + size) {
              if (!it.line()-&gt;refCount(lock))
<span class="line-new-header">--- 432,23 ---</span>
              allocator.refill(bumpRange);
      }
  }
  
  void Heap::allocateSmallBumpRangesByObject(
<span class="line-modified">!     UniqueLockHolder&amp; lock, size_t sizeClass,</span>
      BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">!     LineCache&amp; lineCache, FailureAction action)</span>
  {
<span class="line-added">+     BUNUSED(action);</span>
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      size_t size = allocator.size();
<span class="line-modified">!     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache, action);</span>
<span class="line-added">+     if (!page) {</span>
<span class="line-added">+         BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">+         return;</span>
<span class="line-added">+     }</span>
      BASSERT(page-&gt;hasFreeLines(lock));
  
      auto findSmallBumpRange = [&amp;](Object&amp; it, Object&amp; end) {
          for ( ; it + size &lt;= end; it = it + size) {
              if (!it.line()-&gt;refCount(lock))
</pre>
<hr />
<pre>
<span class="line-old-header">*** 469,22 ***</span>
          }
          return { begin, objectCount };
      };
  
      Object it(page-&gt;begin()-&gt;begin());
<span class="line-modified">!     Object end(it + pageSize(m_pageClasses[sizeClass]));</span>
      for (;;) {
          if (!findSmallBumpRange(it, end)) {
              page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">!             BASSERT(allocator.canAllocate());</span>
              return;
          }
  
          // In a fragmented page, some free ranges might not fit in the cache.
          if (rangeCache.size() == rangeCache.capacity()) {
              lineCache[sizeClass].push(page);
<span class="line-modified">!             BASSERT(allocator.canAllocate());</span>
              return;
          }
  
          BumpRange bumpRange = allocateSmallBumpRange(it, end);
          if (allocator.canAllocate())
<span class="line-new-header">--- 470,22 ---</span>
          }
          return { begin, objectCount };
      };
  
      Object it(page-&gt;begin()-&gt;begin());
<span class="line-modified">!     Object end(it + pageSize(m_constants.pageClass(page-&gt;sizeClass())));</span>
      for (;;) {
          if (!findSmallBumpRange(it, end)) {
              page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">!             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
              return;
          }
  
          // In a fragmented page, some free ranges might not fit in the cache.
          if (rangeCache.size() == rangeCache.capacity()) {
              lineCache[sizeClass].push(page);
<span class="line-modified">!             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
              return;
          }
  
          BumpRange bumpRange = allocateSmallBumpRange(it, end);
          if (allocator.canAllocate())
</pre>
<hr />
<pre>
<span class="line-old-header">*** 492,11 ***</span>
          else
              allocator.refill(bumpRange);
      }
  }
  
<span class="line-modified">! LargeRange Heap::splitAndAllocate(std::unique_lock&lt;Mutex&gt;&amp;, LargeRange&amp; range, size_t alignment, size_t size)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      LargeRange prev;
      LargeRange next;
<span class="line-new-header">--- 493,11 ---</span>
          else
              allocator.refill(bumpRange);
      }
  }
  
<span class="line-modified">! LargeRange Heap::splitAndAllocate(UniqueLockHolder&amp;, LargeRange&amp; range, size_t alignment, size_t size)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      LargeRange prev;
      LargeRange next;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 540,96 ***</span>
  
      m_largeAllocated.set(range.begin(), range.size());
      return range;
  }
  
<span class="line-modified">! void* Heap::tryAllocateLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t alignment, size_t size)</span>
  {
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      BASSERT(isPowerOfTwo(alignment));
  
      m_scavenger-&gt;didStartGrowing();
  
      size_t roundedSize = size ? roundUpToMultipleOf(largeAlignment, size) : largeAlignment;
<span class="line-modified">!     if (roundedSize &lt; size) // Check for overflow</span>
<span class="line-removed">-         return nullptr;</span>
      size = roundedSize;
  
      size_t roundedAlignment = roundUpToMultipleOf&lt;largeAlignment&gt;(alignment);
<span class="line-modified">!     if (roundedAlignment &lt; alignment) // Check for overflow</span>
<span class="line-removed">-         return nullptr;</span>
      alignment = roundedAlignment;
  
      LargeRange range = m_largeFree.remove(alignment, size);
      if (!range) {
          if (m_hasPendingDecommits) {
              m_condition.wait(lock, [&amp;]() { return !m_hasPendingDecommits; });
              // Now we&#39;re guaranteed we&#39;re looking at all available memory.
<span class="line-modified">!             return tryAllocateLarge(lock, alignment, size);</span>
          }
  
<span class="line-modified">!         if (usingGigacage())</span>
<span class="line-removed">-             return nullptr;</span>
  
<span class="line-modified">!         range = VMHeap::get()-&gt;tryAllocateLargeChunk(alignment, size);</span>
<span class="line-modified">!         if (!range)</span>
<span class="line-removed">-             return nullptr;</span>
  
          m_largeFree.add(range);
          range = m_largeFree.remove(alignment, size);
      }
  
      m_freeableMemory -= range.totalPhysicalSize();
  
      void* result = splitAndAllocate(lock, range, alignment, size).begin();
      return result;
  }
  
<span class="line-modified">! void* Heap::allocateLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t alignment, size_t size)</span>
  {
<span class="line-modified">!     void* result = tryAllocateLarge(lock, alignment, size);</span>
<span class="line-modified">!     RELEASE_BASSERT(result);</span>
<span class="line-modified">!     return result;</span>
  }
  
<span class="line-modified">! bool Heap::isLarge(std::unique_lock&lt;Mutex&gt;&amp;, void* object)</span>
  {
      return m_objectTypes.get(Object(object).chunk()) == ObjectType::Large;
  }
  
<span class="line-modified">! size_t Heap::largeSize(std::unique_lock&lt;Mutex&gt;&amp;, void* object)</span>
  {
      return m_largeAllocated.get(object);
  }
  
<span class="line-modified">! void Heap::shrinkLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, const Range&amp; object, size_t newSize)</span>
  {
      BASSERT(object.size() &gt; newSize);
  
      size_t size = m_largeAllocated.remove(object.begin());
      LargeRange range = LargeRange(object, size, size);
      splitAndAllocate(lock, range, alignment, newSize);
  
      m_scavenger-&gt;schedule(size);
  }
  
<span class="line-modified">! void Heap::deallocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, void* object)</span>
  {
      size_t size = m_largeAllocated.remove(object);
      m_largeFree.add(LargeRange(object, size, size, size));
      m_freeableMemory += size;
      m_scavenger-&gt;schedule(size);
  }
  
  void Heap::externalCommit(void* ptr, size_t size)
  {
<span class="line-modified">!     std::unique_lock&lt;Mutex&gt; lock(Heap::mutex());</span>
      externalCommit(lock, ptr, size);
  }
  
<span class="line-modified">! void Heap::externalCommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t size)</span>
  {
      BUNUSED_PARAM(ptr);
  
      m_footprint += size;
  #if ENABLE_PHYSICAL_PAGE_MAP
<span class="line-new-header">--- 541,124 ---</span>
  
      m_largeAllocated.set(range.begin(), range.size());
      return range;
  }
  
<span class="line-modified">! void* Heap::allocateLarge(UniqueLockHolder&amp; lock, size_t alignment, size_t size, FailureAction action)</span>
  {
<span class="line-added">+ #define ASSERT_OR_RETURN_ON_FAILURE(cond) do { \</span>
<span class="line-added">+         if (action == FailureAction::Crash) \</span>
<span class="line-added">+             RELEASE_BASSERT(cond); \</span>
<span class="line-added">+         else if (!(cond)) \</span>
<span class="line-added">+             return nullptr; \</span>
<span class="line-added">+     } while (false)</span>
<span class="line-added">+ </span>
<span class="line-added">+ </span>
      RELEASE_BASSERT(isActiveHeapKind(m_kind));
  
      BASSERT(isPowerOfTwo(alignment));
  
      m_scavenger-&gt;didStartGrowing();
  
      size_t roundedSize = size ? roundUpToMultipleOf(largeAlignment, size) : largeAlignment;
<span class="line-modified">!     ASSERT_OR_RETURN_ON_FAILURE(roundedSize &gt;= size); // Check for overflow</span>
      size = roundedSize;
  
      size_t roundedAlignment = roundUpToMultipleOf&lt;largeAlignment&gt;(alignment);
<span class="line-modified">!     ASSERT_OR_RETURN_ON_FAILURE(roundedAlignment &gt;= alignment); // Check for overflow</span>
      alignment = roundedAlignment;
  
      LargeRange range = m_largeFree.remove(alignment, size);
      if (!range) {
          if (m_hasPendingDecommits) {
              m_condition.wait(lock, [&amp;]() { return !m_hasPendingDecommits; });
              // Now we&#39;re guaranteed we&#39;re looking at all available memory.
<span class="line-modified">!             return allocateLarge(lock, alignment, size, action);</span>
          }
  
<span class="line-modified">!         ASSERT_OR_RETURN_ON_FAILURE(!usingGigacage());</span>
  
<span class="line-modified">!         range = tryAllocateLargeChunk(alignment, size);</span>
<span class="line-modified">!         ASSERT_OR_RETURN_ON_FAILURE(range);</span>
  
          m_largeFree.add(range);
          range = m_largeFree.remove(alignment, size);
      }
  
      m_freeableMemory -= range.totalPhysicalSize();
  
      void* result = splitAndAllocate(lock, range, alignment, size).begin();
<span class="line-added">+ #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">+     m_highWatermark = std::max(m_highWatermark, result);</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+     ASSERT_OR_RETURN_ON_FAILURE(result);</span>
      return result;
<span class="line-added">+ </span>
<span class="line-added">+ #undef ASSERT_OR_RETURN_ON_FAILURE</span>
  }
  
<span class="line-modified">! LargeRange Heap::tryAllocateLargeChunk(size_t alignment, size_t size)</span>
  {
<span class="line-modified">!     // We allocate VM in aligned multiples to increase the chances that</span>
<span class="line-modified">!     // the OS will provide contiguous ranges that we can merge.</span>
<span class="line-modified">!     size_t roundedAlignment = roundUpToMultipleOf&lt;chunkSize&gt;(alignment);</span>
<span class="line-added">+     if (roundedAlignment &lt; alignment) // Check for overflow</span>
<span class="line-added">+         return LargeRange();</span>
<span class="line-added">+     alignment = roundedAlignment;</span>
<span class="line-added">+ </span>
<span class="line-added">+     size_t roundedSize = roundUpToMultipleOf&lt;chunkSize&gt;(size);</span>
<span class="line-added">+     if (roundedSize &lt; size) // Check for overflow</span>
<span class="line-added">+         return LargeRange();</span>
<span class="line-added">+     size = roundedSize;</span>
<span class="line-added">+ </span>
<span class="line-added">+     void* memory = tryVMAllocate(alignment, size);</span>
<span class="line-added">+     if (!memory)</span>
<span class="line-added">+         return LargeRange();</span>
<span class="line-added">+ </span>
<span class="line-added">+ #if BOS(DARWIN)</span>
<span class="line-added">+     PerProcess&lt;Zone&gt;::get()-&gt;addRange(Range(memory, size));</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+     return LargeRange(memory, size, 0, 0);</span>
  }
  
<span class="line-modified">! bool Heap::isLarge(UniqueLockHolder&amp;, void* object)</span>
  {
      return m_objectTypes.get(Object(object).chunk()) == ObjectType::Large;
  }
  
<span class="line-modified">! size_t Heap::largeSize(UniqueLockHolder&amp;, void* object)</span>
  {
      return m_largeAllocated.get(object);
  }
  
<span class="line-modified">! void Heap::shrinkLarge(UniqueLockHolder&amp; lock, const Range&amp; object, size_t newSize)</span>
  {
      BASSERT(object.size() &gt; newSize);
  
      size_t size = m_largeAllocated.remove(object.begin());
      LargeRange range = LargeRange(object, size, size);
      splitAndAllocate(lock, range, alignment, newSize);
  
      m_scavenger-&gt;schedule(size);
  }
  
<span class="line-modified">! void Heap::deallocateLarge(UniqueLockHolder&amp;, void* object)</span>
  {
      size_t size = m_largeAllocated.remove(object);
      m_largeFree.add(LargeRange(object, size, size, size));
      m_freeableMemory += size;
      m_scavenger-&gt;schedule(size);
  }
  
  void Heap::externalCommit(void* ptr, size_t size)
  {
<span class="line-modified">!     UniqueLockHolder lock(Heap::mutex());</span>
      externalCommit(lock, ptr, size);
  }
  
<span class="line-modified">! void Heap::externalCommit(UniqueLockHolder&amp;, void* ptr, size_t size)</span>
  {
      BUNUSED_PARAM(ptr);
  
      m_footprint += size;
  #if ENABLE_PHYSICAL_PAGE_MAP
</pre>
<hr />
<pre>
<span class="line-old-header">*** 637,15 ***</span>
  #endif
  }
  
  void Heap::externalDecommit(void* ptr, size_t size)
  {
<span class="line-modified">!     std::unique_lock&lt;Mutex&gt; lock(Heap::mutex());</span>
      externalDecommit(lock, ptr, size);
  }
  
<span class="line-modified">! void Heap::externalDecommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t size)</span>
  {
      BUNUSED_PARAM(ptr);
  
      m_footprint -= size;
  #if ENABLE_PHYSICAL_PAGE_MAP
<span class="line-new-header">--- 666,15 ---</span>
  #endif
  }
  
  void Heap::externalDecommit(void* ptr, size_t size)
  {
<span class="line-modified">!     UniqueLockHolder lock(Heap::mutex());</span>
      externalDecommit(lock, ptr, size);
  }
  
<span class="line-modified">! void Heap::externalDecommit(UniqueLockHolder&amp;, void* ptr, size_t size)</span>
  {
      BUNUSED_PARAM(ptr);
  
      m_footprint -= size;
  #if ENABLE_PHYSICAL_PAGE_MAP
</pre>
<center><a href="Gigacage.h.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>