<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/ftl/FTLOSRExitCompiler.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2013-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;FTLOSRExitCompiler.h&quot;
 28 
 29 #if ENABLE(FTL_JIT)
 30 
<a name="1" id="anc1"></a><span class="line-added"> 31 #include &quot;BytecodeStructs.h&quot;</span>
<span class="line-added"> 32 #include &quot;CheckpointOSRExitSideState.h&quot;</span>
 33 #include &quot;DFGOSRExitCompilerCommon.h&quot;
<a name="2" id="anc2"></a>
 34 #include &quot;FTLExitArgumentForOperand.h&quot;
 35 #include &quot;FTLJITCode.h&quot;
 36 #include &quot;FTLLocation.h&quot;
 37 #include &quot;FTLOSRExit.h&quot;
 38 #include &quot;FTLOperations.h&quot;
 39 #include &quot;FTLState.h&quot;
 40 #include &quot;FTLSaveRestore.h&quot;
<a name="3" id="anc3"></a><span class="line-added"> 41 #include &quot;JSCInlines.h&quot;</span>
 42 #include &quot;LinkBuffer.h&quot;
 43 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
 44 #include &quot;OperandsInlines.h&quot;
<a name="4" id="anc4"></a><span class="line-modified"> 45 #include &quot;ProbeContext.h&quot;</span>
 46 
 47 namespace JSC { namespace FTL {
 48 
 49 using namespace DFG;
 50 
 51 static void reboxAccordingToFormat(
 52     DataFormat format, AssemblyHelpers&amp; jit, GPRReg value, GPRReg scratch1, GPRReg scratch2)
 53 {
 54     switch (format) {
 55     case DataFormatInt32: {
 56         jit.zeroExtend32ToPtr(value, value);
<a name="5" id="anc5"></a><span class="line-modified"> 57         jit.or64(GPRInfo::numberTagRegister, value);</span>
 58         break;
 59     }
 60 
 61     case DataFormatInt52: {
 62         jit.rshift64(AssemblyHelpers::TrustedImm32(JSValue::int52ShiftAmount), value);
 63         jit.moveDoubleTo64(FPRInfo::fpRegT0, scratch2);
 64         jit.boxInt52(value, value, scratch1, FPRInfo::fpRegT0);
 65         jit.move64ToDouble(scratch2, FPRInfo::fpRegT0);
 66         break;
 67     }
 68 
 69     case DataFormatStrictInt52: {
 70         jit.moveDoubleTo64(FPRInfo::fpRegT0, scratch2);
 71         jit.boxInt52(value, value, scratch1, FPRInfo::fpRegT0);
 72         jit.move64ToDouble(scratch2, FPRInfo::fpRegT0);
 73         break;
 74     }
 75 
 76     case DataFormatBoolean: {
 77         jit.zeroExtend32ToPtr(value, value);
<a name="6" id="anc6"></a><span class="line-modified"> 78         jit.or32(MacroAssembler::TrustedImm32(JSValue::ValueFalse), value);</span>
 79         break;
 80     }
 81 
 82     case DataFormatJS: {
 83         // Done already!
 84         break;
 85     }
 86 
 87     case DataFormatDouble: {
 88         jit.moveDoubleTo64(FPRInfo::fpRegT0, scratch1);
 89         jit.move64ToDouble(value, FPRInfo::fpRegT0);
 90         jit.purifyNaN(FPRInfo::fpRegT0);
 91         jit.boxDouble(FPRInfo::fpRegT0, value);
 92         jit.move64ToDouble(scratch1, FPRInfo::fpRegT0);
 93         break;
 94     }
 95 
 96     default:
 97         RELEASE_ASSERT_NOT_REACHED();
 98         break;
 99     }
100 }
101 
102 static void compileRecovery(
103     CCallHelpers&amp; jit, const ExitValue&amp; value,
104     Vector&lt;B3::ValueRep&gt;&amp; valueReps,
105     char* registerScratch,
106     const HashMap&lt;ExitTimeObjectMaterialization*, EncodedJSValue*&gt;&amp; materializationToPointer)
107 {
108     switch (value.kind()) {
109     case ExitValueDead:
110         jit.move(MacroAssembler::TrustedImm64(JSValue::encode(jsUndefined())), GPRInfo::regT0);
111         break;
112 
113     case ExitValueConstant:
114         jit.move(MacroAssembler::TrustedImm64(JSValue::encode(value.constant())), GPRInfo::regT0);
115         break;
116 
117     case ExitValueArgument:
118         Location::forValueRep(valueReps[value.exitArgument().argument()]).restoreInto(
119             jit, registerScratch, GPRInfo::regT0);
120         break;
121 
122     case ExitValueInJSStack:
123     case ExitValueInJSStackAsInt32:
124     case ExitValueInJSStackAsInt52:
125     case ExitValueInJSStackAsDouble:
126         jit.load64(AssemblyHelpers::addressFor(value.virtualRegister()), GPRInfo::regT0);
127         break;
128 
129     case ExitValueMaterializeNewObject:
130         jit.loadPtr(materializationToPointer.get(value.objectMaterialization()), GPRInfo::regT0);
131         break;
132 
133     default:
134         RELEASE_ASSERT_NOT_REACHED();
135         break;
136     }
137 
138     reboxAccordingToFormat(
139         value.dataFormat(), jit, GPRInfo::regT0, GPRInfo::regT1, GPRInfo::regT2);
140 }
141 
<a name="7" id="anc7"></a><span class="line-modified">142 static void compileStub(VM&amp; vm, unsigned exitID, JITCode* jitCode, OSRExit&amp; exit, CodeBlock* codeBlock)</span>

143 {
144     // This code requires framePointerRegister is the same as callFrameRegister
145     static_assert(MacroAssembler::framePointerRegister == GPRInfo::callFrameRegister, &quot;MacroAssembler::framePointerRegister and GPRInfo::callFrameRegister must be the same&quot;);
146 
147     CCallHelpers jit(codeBlock);
148 
149     // The first thing we need to do is restablish our frame in the case of an exception.
150     if (exit.isGenericUnwindHandler()) {
<a name="8" id="anc8"></a><span class="line-modified">151         RELEASE_ASSERT(vm.callFrameForCatch); // The first time we hit this exit, like at all other times, this field should be non-null.</span>
<span class="line-modified">152         jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm.topEntryFrame);</span>
<span class="line-modified">153         jit.loadPtr(vm.addressOfCallFrameForCatch(), MacroAssembler::framePointerRegister);</span>
154         jit.addPtr(CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
155             MacroAssembler::framePointerRegister, CCallHelpers::stackPointerRegister);
156 
157         // Do a pushToSave because that&#39;s what the exit compiler below expects the stack
158         // to look like because that&#39;s the last thing the ExitThunkGenerator does. The code
159         // below doesn&#39;t actually use the value that was pushed, but it does rely on the
160         // general shape of the stack being as it is in the non-exception OSR case.
161         jit.pushToSaveImmediateWithoutTouchingRegisters(CCallHelpers::TrustedImm32(0xbadbeef));
162     }
163 
164     // We need scratch space to save all registers, to build up the JS stack, to deal with unwind
165     // fixup, pointers to all of the objects we materialize, and the elements inside those objects
166     // that we materialize.
167 
168     // Figure out how much space we need for those object allocations.
169     unsigned numMaterializations = 0;
170     size_t maxMaterializationNumArguments = 0;
171     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations) {
172         numMaterializations++;
173 
174         maxMaterializationNumArguments = std::max(
175             maxMaterializationNumArguments,
176             materialization-&gt;properties().size());
177     }
178 
<a name="9" id="anc9"></a><span class="line-modified">179     ScratchBuffer* scratchBuffer = vm.scratchBufferForSize(</span>
180         sizeof(EncodedJSValue) * (
181             exit.m_descriptor-&gt;m_values.size() + numMaterializations + maxMaterializationNumArguments) +
182         requiredScratchMemorySizeInBytes() +
183         codeBlock-&gt;calleeSaveRegisters()-&gt;size() * sizeof(uint64_t));
184     EncodedJSValue* scratch = scratchBuffer ? static_cast&lt;EncodedJSValue*&gt;(scratchBuffer-&gt;dataBuffer()) : 0;
185     EncodedJSValue* materializationPointers = scratch + exit.m_descriptor-&gt;m_values.size();
186     EncodedJSValue* materializationArguments = materializationPointers + numMaterializations;
187     char* registerScratch = bitwise_cast&lt;char*&gt;(materializationArguments + maxMaterializationNumArguments);
188     uint64_t* unwindScratch = bitwise_cast&lt;uint64_t*&gt;(registerScratch + requiredScratchMemorySizeInBytes());
189 
190     HashMap&lt;ExitTimeObjectMaterialization*, EncodedJSValue*&gt; materializationToPointer;
191     unsigned materializationCount = 0;
192     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations) {
193         materializationToPointer.add(
194             materialization, materializationPointers + materializationCount++);
195     }
196 
197     auto recoverValue = [&amp;] (const ExitValue&amp; value) {
198         compileRecovery(
199             jit, value,
200             exit.m_valueReps,
201             registerScratch, materializationToPointer);
202     };
203 
204     // Note that we come in here, the stack used to be as B3 left it except that someone called pushToSave().
205     // We don&#39;t care about the value they saved. But, we do appreciate the fact that they did it, because we use
206     // that slot for saveAllRegisters().
207 
208     saveAllRegisters(jit, registerScratch);
209 
210     if (validateDFGDoesGC) {
211         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
212         // code running that expects no GC. We need to set this before object
213         // materialization below.
214 
215         // Even though we set Heap::m_expectDoesGC in compileFTLOSRExit(), we also need
216         // to set it here because compileFTLOSRExit() is only called on the first time
217         // we exit from this site, but all subsequent exits will take this compiled
218         // ramp without calling compileFTLOSRExit() first.
<a name="10" id="anc10"></a><span class="line-modified">219         jit.store8(CCallHelpers::TrustedImm32(true), vm.heap.addressOfExpectDoesGC());</span>
220     }
221 
222     // Bring the stack back into a sane form and assert that it&#39;s sane.
223     jit.popToRestore(GPRInfo::regT0);
224     jit.checkStackPointerAlignment();
225 
<a name="11" id="anc11"></a><span class="line-modified">226     if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; jitCode-&gt;dfgCommon()-&gt;compilation)) {</span>
<span class="line-modified">227         Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;</span>
228         Profiler::Compilation* compilation = jitCode-&gt;dfgCommon()-&gt;compilation.get();
229 
230         Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
231             exitID, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
232             exit.m_kind, exit.m_kind == UncountableInvalidation);
233         jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
234     }
235 
236     // The remaining code assumes that SP/FP are in the same state that they were in the FTL&#39;s
237     // call frame.
238 
239     // Get the call frame and tag thingies.
240     // Restore the exiting function&#39;s callFrame value into a regT4
<a name="12" id="anc12"></a><span class="line-modified">241     jit.move(MacroAssembler::TrustedImm64(JSValue::NumberTag), GPRInfo::numberTagRegister);</span>
<span class="line-modified">242     jit.move(MacroAssembler::TrustedImm64(JSValue::NotCellMask), GPRInfo::notCellMaskRegister);</span>
243 
244     // Do some value profiling.
245     if (exit.m_descriptor-&gt;m_profileDataFormat != DataFormatNone) {
246         Location::forValueRep(exit.m_valueReps[0]).restoreInto(jit, registerScratch, GPRInfo::regT0);
247         reboxAccordingToFormat(
248             exit.m_descriptor-&gt;m_profileDataFormat, jit, GPRInfo::regT0, GPRInfo::regT1, GPRInfo::regT2);
249 
250         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
251             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<a name="13" id="anc13"></a><span class="line-modified">252             CodeBlock* codeBlock = jit.baselineCodeBlockFor(codeOrigin);</span>
<span class="line-added">253             if (ArrayProfile* arrayProfile = codeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex())) {</span>
<span class="line-added">254                 const Instruction* instruction = codeBlock-&gt;instructions().at(codeOrigin.bytecodeIndex()).ptr();</span>
<span class="line-added">255                 CCallHelpers::Jump skipProfile;</span>
<span class="line-added">256                 if (instruction-&gt;is&lt;OpGetById&gt;()) {</span>
<span class="line-added">257                     auto&amp; metadata = instruction-&gt;as&lt;OpGetById&gt;().metadata(codeBlock);</span>
<span class="line-added">258                     skipProfile = jit.branch8(CCallHelpers::NotEqual, CCallHelpers::AbsoluteAddress(&amp;metadata.m_modeMetadata.mode), CCallHelpers::TrustedImm32(static_cast&lt;uint8_t&gt;(GetByIdMode::ArrayLength)));</span>
<span class="line-added">259                 }</span>
<span class="line-added">260 </span>
261                 jit.load32(MacroAssembler::Address(GPRInfo::regT0, JSCell::structureIDOffset()), GPRInfo::regT1);
262                 jit.store32(GPRInfo::regT1, arrayProfile-&gt;addressOfLastSeenStructureID());
263 
264                 jit.load8(MacroAssembler::Address(GPRInfo::regT0, JSCell::typeInfoTypeOffset()), GPRInfo::regT2);
265                 jit.sub32(MacroAssembler::TrustedImm32(FirstTypedArrayType), GPRInfo::regT2);
266                 auto notTypedArray = jit.branch32(MacroAssembler::AboveOrEqual, GPRInfo::regT2, MacroAssembler::TrustedImm32(NumberOfTypedArrayTypesExcludingDataView));
267                 jit.move(MacroAssembler::TrustedImmPtr(typedArrayModes), GPRInfo::regT1);
268                 jit.load32(MacroAssembler::BaseIndex(GPRInfo::regT1, GPRInfo::regT2, MacroAssembler::TimesFour), GPRInfo::regT2);
269                 auto storeArrayModes = jit.jump();
270 
271                 notTypedArray.link(&amp;jit);
272                 jit.load8(MacroAssembler::Address(GPRInfo::regT0, JSCell::indexingTypeAndMiscOffset()), GPRInfo::regT1);
273                 jit.and32(MacroAssembler::TrustedImm32(IndexingModeMask), GPRInfo::regT1);
274                 jit.move(MacroAssembler::TrustedImm32(1), GPRInfo::regT2);
275                 jit.lshift32(GPRInfo::regT1, GPRInfo::regT2);
276                 storeArrayModes.link(&amp;jit);
277                 jit.or32(GPRInfo::regT2, MacroAssembler::AbsoluteAddress(arrayProfile-&gt;addressOfArrayModes()));
<a name="14" id="anc14"></a><span class="line-added">278 </span>
<span class="line-added">279                 if (skipProfile.isSet())</span>
<span class="line-added">280                     skipProfile.link(&amp;jit);</span>
281             }
282         }
283 
284         if (exit.m_descriptor-&gt;m_valueProfile)
285             exit.m_descriptor-&gt;m_valueProfile.emitReportValue(jit, JSValueRegs(GPRInfo::regT0));
286     }
287 
288     // Materialize all objects. Don&#39;t materialize an object until all
289     // of the objects it needs have been materialized. We break cycles
290     // by populating objects late - we only consider an object as
291     // needing another object if the later is needed for the
292     // allocation of the former.
293 
294     HashSet&lt;ExitTimeObjectMaterialization*&gt; toMaterialize;
295     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations)
296         toMaterialize.add(materialization);
297 
298     while (!toMaterialize.isEmpty()) {
299         unsigned previousToMaterializeSize = toMaterialize.size();
300 
301         Vector&lt;ExitTimeObjectMaterialization*&gt; worklist;
302         worklist.appendRange(toMaterialize.begin(), toMaterialize.end());
303         for (ExitTimeObjectMaterialization* materialization : worklist) {
304             // Check if we can do anything about this right now.
305             bool allGood = true;
306             for (ExitPropertyValue value : materialization-&gt;properties()) {
307                 if (!value.value().isObjectMaterialization())
308                     continue;
309                 if (!value.location().neededForMaterialization())
310                     continue;
311                 if (toMaterialize.contains(value.value().objectMaterialization())) {
312                     // Gotta skip this one, since it needs a
313                     // materialization that hasn&#39;t been materialized.
314                     allGood = false;
315                     break;
316                 }
317             }
318             if (!allGood)
319                 continue;
320 
321             // All systems go for materializing the object. First we
322             // recover the values of all of its fields and then we
323             // call a function to actually allocate the beast.
324             // We only recover the fields that are needed for the allocation.
325             for (unsigned propertyIndex = materialization-&gt;properties().size(); propertyIndex--;) {
326                 const ExitPropertyValue&amp; property = materialization-&gt;properties()[propertyIndex];
327                 if (!property.location().neededForMaterialization())
328                     continue;
329 
330                 recoverValue(property.value());
331                 jit.storePtr(GPRInfo::regT0, materializationArguments + propertyIndex);
332             }
333 
334             static_assert(FunctionTraits&lt;decltype(operationMaterializeObjectInOSR)&gt;::arity &lt; GPRInfo::numberOfArgumentRegisters, &quot;This call assumes that we don&#39;t pass arguments on the stack.&quot;);
335             jit.setupArguments&lt;decltype(operationMaterializeObjectInOSR)&gt;(
<a name="15" id="anc15"></a><span class="line-added">336                 CCallHelpers::TrustedImmPtr(codeBlock-&gt;globalObjectFor(materialization-&gt;origin())),</span>
337                 CCallHelpers::TrustedImmPtr(materialization),
338                 CCallHelpers::TrustedImmPtr(materializationArguments));
<a name="16" id="anc16"></a><span class="line-added">339             jit.prepareCallOperation(vm);</span>
340             jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationMaterializeObjectInOSR)), GPRInfo::nonArgGPR0);
341             jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
342             jit.storePtr(GPRInfo::returnValueGPR, materializationToPointer.get(materialization));
343 
344             // Let everyone know that we&#39;re done.
345             toMaterialize.remove(materialization);
346         }
347 
348         // We expect progress! This ensures that we crash rather than looping infinitely if there
349         // is something broken about this fixpoint. Or, this could happen if we ever violate the
350         // &quot;materializations form a DAG&quot; rule.
351         RELEASE_ASSERT(toMaterialize.size() &lt; previousToMaterializeSize);
352     }
353 
354     // Now that all the objects have been allocated, we populate them
355     // with the correct values. This time we can recover all the
356     // fields, including those that are only needed for the allocation.
357     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations) {
358         for (unsigned propertyIndex = materialization-&gt;properties().size(); propertyIndex--;) {
359             recoverValue(materialization-&gt;properties()[propertyIndex].value());
360             jit.storePtr(GPRInfo::regT0, materializationArguments + propertyIndex);
361         }
362 
363         static_assert(FunctionTraits&lt;decltype(operationPopulateObjectInOSR)&gt;::arity &lt; GPRInfo::numberOfArgumentRegisters, &quot;This call assumes that we don&#39;t pass arguments on the stack.&quot;);
364         jit.setupArguments&lt;decltype(operationPopulateObjectInOSR)&gt;(
<a name="17" id="anc17"></a><span class="line-added">365             CCallHelpers::TrustedImmPtr(codeBlock-&gt;globalObjectFor(materialization-&gt;origin())),</span>
366             CCallHelpers::TrustedImmPtr(materialization),
367             CCallHelpers::TrustedImmPtr(materializationToPointer.get(materialization)),
368             CCallHelpers::TrustedImmPtr(materializationArguments));
<a name="18" id="anc18"></a><span class="line-added">369         jit.prepareCallOperation(vm);</span>
370         jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationPopulateObjectInOSR)), GPRInfo::nonArgGPR0);
371         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
372     }
373 
374     // Save all state from wherever the exit data tells us it was, into the appropriate place in
375     // the scratch buffer. This also does the reboxing.
376 
377     for (unsigned index = exit.m_descriptor-&gt;m_values.size(); index--;) {
378         recoverValue(exit.m_descriptor-&gt;m_values[index]);
379         jit.store64(GPRInfo::regT0, scratch + index);
380     }
381 
382     // Henceforth we make it look like the exiting function was called through a register
383     // preservation wrapper. This implies that FP must be nudged down by a certain amount. Then
384     // we restore the various things according to either exit.m_descriptor-&gt;m_values or by copying from the
385     // old frame, and finally we save the various callee-save registers into where the
386     // restoration thunk would restore them from.
387 
388     // Before we start messing with the frame, we need to set aside any registers that the
389     // FTL code was preserving.
390     for (unsigned i = codeBlock-&gt;calleeSaveRegisters()-&gt;size(); i--;) {
391         RegisterAtOffset entry = codeBlock-&gt;calleeSaveRegisters()-&gt;at(i);
392         jit.load64(
393             MacroAssembler::Address(MacroAssembler::framePointerRegister, entry.offset()),
394             GPRInfo::regT0);
395         jit.store64(GPRInfo::regT0, unwindScratch + i);
396     }
397 
398     CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(exit.m_codeOrigin);
399 
400     // First set up SP so that our data doesn&#39;t get clobbered by signals.
401     unsigned conservativeStackDelta =
402         (exit.m_descriptor-&gt;m_values.numberOfLocals() + baselineCodeBlock-&gt;calleeSaveSpaceAsVirtualRegisters()) * sizeof(Register) +
403         maxFrameExtentForSlowPathCall;
404     conservativeStackDelta = WTF::roundUpToMultipleOf(
405         stackAlignmentBytes(), conservativeStackDelta);
406     jit.addPtr(
407         MacroAssembler::TrustedImm32(-conservativeStackDelta),
408         MacroAssembler::framePointerRegister, MacroAssembler::stackPointerRegister);
409     jit.checkStackPointerAlignment();
410 
411     RegisterSet allFTLCalleeSaves = RegisterSet::ftlCalleeSaveRegisters();
412     const RegisterAtOffsetList* baselineCalleeSaves = baselineCodeBlock-&gt;calleeSaveRegisters();
413     RegisterAtOffsetList* vmCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();
414     RegisterSet vmCalleeSavesToSkip = RegisterSet::stackRegisters();
415     if (exit.isExceptionHandler()) {
<a name="19" id="anc19"></a><span class="line-modified">416         jit.loadPtr(&amp;vm.topEntryFrame, GPRInfo::regT1);</span>
417         jit.addPtr(CCallHelpers::TrustedImm32(EntryFrame::calleeSaveRegistersBufferOffset()), GPRInfo::regT1);
418     }
419 
420     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
421         if (!allFTLCalleeSaves.get(reg)) {
422             if (exit.isExceptionHandler())
423                 RELEASE_ASSERT(!vmCalleeSaves-&gt;find(reg));
424             continue;
425         }
426         unsigned unwindIndex = codeBlock-&gt;calleeSaveRegisters()-&gt;indexOf(reg);
427         const RegisterAtOffset* baselineRegisterOffset = baselineCalleeSaves-&gt;find(reg);
428         RegisterAtOffset* vmCalleeSave = nullptr;
429         if (exit.isExceptionHandler())
430             vmCalleeSave = vmCalleeSaves-&gt;find(reg);
431 
432         if (reg.isGPR()) {
433             GPRReg regToLoad = baselineRegisterOffset ? GPRInfo::regT0 : reg.gpr();
434             RELEASE_ASSERT(regToLoad != GPRInfo::regT1);
435 
436             if (unwindIndex == UINT_MAX) {
437                 // The FTL compilation didn&#39;t preserve this register. This means that it also
438                 // didn&#39;t use the register. So its value at the beginning of OSR exit should be
439                 // preserved by the thunk. Luckily, we saved all registers into the register
440                 // scratch buffer, so we can restore them from there.
441                 jit.load64(registerScratch + offsetOfReg(reg), regToLoad);
442             } else {
443                 // The FTL compilation preserved the register. Its new value is therefore
444                 // irrelevant, but we can get the value that was preserved by using the unwind
445                 // data. We&#39;ve already copied all unwind-able preserved registers into the unwind
446                 // scratch buffer, so we can get it from there.
447                 jit.load64(unwindScratch + unwindIndex, regToLoad);
448             }
449 
450             if (baselineRegisterOffset)
451                 jit.store64(regToLoad, MacroAssembler::Address(MacroAssembler::framePointerRegister, baselineRegisterOffset-&gt;offset()));
452             if (vmCalleeSave &amp;&amp; !vmCalleeSavesToSkip.get(vmCalleeSave-&gt;reg()))
453                 jit.store64(regToLoad, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));
454         } else {
455             FPRReg fpRegToLoad = baselineRegisterOffset ? FPRInfo::fpRegT0 : reg.fpr();
456 
457             if (unwindIndex == UINT_MAX)
458                 jit.loadDouble(MacroAssembler::TrustedImmPtr(registerScratch + offsetOfReg(reg)), fpRegToLoad);
459             else
460                 jit.loadDouble(MacroAssembler::TrustedImmPtr(unwindScratch + unwindIndex), fpRegToLoad);
461 
462             if (baselineRegisterOffset)
463                 jit.storeDouble(fpRegToLoad, MacroAssembler::Address(MacroAssembler::framePointerRegister, baselineRegisterOffset-&gt;offset()));
464             if (vmCalleeSave &amp;&amp; !vmCalleeSavesToSkip.get(vmCalleeSave-&gt;reg()))
465                 jit.storeDouble(fpRegToLoad, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));
466         }
467     }
468 
469     if (exit.isExceptionHandler()) {
<a name="20" id="anc20"></a><span class="line-modified">470         RegisterAtOffset* vmCalleeSave = vmCalleeSaves-&gt;find(GPRInfo::numberTagRegister);</span>
<span class="line-modified">471         jit.store64(GPRInfo::numberTagRegister, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));</span>
472 
<a name="21" id="anc21"></a><span class="line-modified">473         vmCalleeSave = vmCalleeSaves-&gt;find(GPRInfo::notCellMaskRegister);</span>
<span class="line-modified">474         jit.store64(GPRInfo::notCellMaskRegister, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));</span>
475     }
476 
477     size_t baselineVirtualRegistersForCalleeSaves = baselineCodeBlock-&gt;calleeSaveSpaceAsVirtualRegisters();
478 
<a name="22" id="anc22"></a><span class="line-added">479     if (exit.m_codeOrigin.inlineStackContainsActiveCheckpoint()) {</span>
<span class="line-added">480         JSValue* tmpScratch = reinterpret_cast&lt;JSValue*&gt;(scratch + exit.m_descriptor-&gt;m_values.tmpIndex(0));</span>
<span class="line-added">481         VM* vmPtr = &amp;vm;</span>
<span class="line-added">482         jit.probe([=] (Probe::Context&amp; context) {</span>
<span class="line-added">483             auto addSideState = [&amp;] (CallFrame* frame, BytecodeIndex index, size_t tmpOffset) {</span>
<span class="line-added">484                 std::unique_ptr&lt;CheckpointOSRExitSideState&gt; sideState = WTF::makeUnique&lt;CheckpointOSRExitSideState&gt;();</span>
<span class="line-added">485 </span>
<span class="line-added">486                 sideState-&gt;bytecodeIndex = index;</span>
<span class="line-added">487                 for (size_t i = 0; i &lt; maxNumCheckpointTmps; ++i)</span>
<span class="line-added">488                     sideState-&gt;tmps[i] = tmpScratch[i + tmpOffset];</span>
<span class="line-added">489 </span>
<span class="line-added">490                 vmPtr-&gt;addCheckpointOSRSideState(frame, WTFMove(sideState));</span>
<span class="line-added">491             };</span>
<span class="line-added">492 </span>
<span class="line-added">493             const CodeOrigin* codeOrigin;</span>
<span class="line-added">494             CallFrame* callFrame = context.gpr&lt;CallFrame*&gt;(GPRInfo::callFrameRegister);</span>
<span class="line-added">495             for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {</span>
<span class="line-added">496                 BytecodeIndex callBytecodeIndex = codeOrigin-&gt;bytecodeIndex();</span>
<span class="line-added">497                 if (!callBytecodeIndex.checkpoint())</span>
<span class="line-added">498                     continue;</span>
<span class="line-added">499 </span>
<span class="line-added">500                 auto* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();</span>
<span class="line-added">501                 addSideState(reinterpret_cast&lt;CallFrame*&gt;(reinterpret_cast&lt;char*&gt;(callFrame) + inlineCallFrame-&gt;returnPCOffset() - sizeof(CPURegister)), callBytecodeIndex, inlineCallFrame-&gt;tmpOffset);</span>
<span class="line-added">502             }</span>
<span class="line-added">503 </span>
<span class="line-added">504             if (!codeOrigin)</span>
<span class="line-added">505                 return;</span>
<span class="line-added">506 </span>
<span class="line-added">507             if (BytecodeIndex bytecodeIndex = codeOrigin-&gt;bytecodeIndex(); bytecodeIndex.checkpoint())</span>
<span class="line-added">508                 addSideState(callFrame, bytecodeIndex, 0);</span>
<span class="line-added">509         });</span>
<span class="line-added">510     }</span>
<span class="line-added">511 </span>
512     // Now get state out of the scratch buffer and place it back into the stack. The values are
513     // already reboxed so we just move them.
514     for (unsigned index = exit.m_descriptor-&gt;m_values.size(); index--;) {
<a name="23" id="anc23"></a><span class="line-modified">515         Operand operand = exit.m_descriptor-&gt;m_values.operandForIndex(index);</span>
516 
<a name="24" id="anc24"></a><span class="line-modified">517         if (operand.isTmp())</span>
<span class="line-added">518             continue;</span>
<span class="line-added">519 </span>
<span class="line-added">520         if (operand.isLocal() &amp;&amp; operand.toLocal() &lt; static_cast&lt;int&gt;(baselineVirtualRegistersForCalleeSaves))</span>
521             continue;
522 
523         jit.load64(scratch + index, GPRInfo::regT0);
<a name="25" id="anc25"></a><span class="line-modified">524         jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand.virtualRegister()));</span>
525     }
526 
<a name="26" id="anc26"></a><span class="line-modified">527     handleExitCounts(vm, jit, exit);</span>
528     reifyInlinedCallFrames(jit, exit);
<a name="27" id="anc27"></a><span class="line-modified">529     adjustAndJumpToTarget(vm, jit, exit);</span>
530 
531     LinkBuffer patchBuffer(jit, codeBlock);
532     exit.m_code = FINALIZE_CODE_IF(
533         shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseFTLOSRExit(),
534         patchBuffer, OSRExitPtrTag,
535         &quot;FTL OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
536             exitID, toCString(exit.m_codeOrigin).data(),
537             exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
538             toCString(ignoringContext&lt;DumpContext&gt;(exit.m_descriptor-&gt;m_values)).data()
539         );
540 }
541 
<a name="28" id="anc28"></a><span class="line-modified">542 extern &quot;C&quot; JIT_OPERATION void* operationCompileFTLOSRExit(CallFrame* callFrame, unsigned exitID)</span>
543 {
544     if (shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseFTLOSRExit())
545         dataLog(&quot;Compiling OSR exit with exitID = &quot;, exitID, &quot;\n&quot;);
546 
<a name="29" id="anc29"></a><span class="line-modified">547     VM&amp; vm = callFrame-&gt;deprecatedVM();</span>
548 
549     if (validateDFGDoesGC) {
550         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
551         // code running that expects no GC.
552         vm.heap.setExpectDoesGC(true);
553     }
554 
555     if (vm.callFrameForCatch)
<a name="30" id="anc30"></a><span class="line-modified">556         RELEASE_ASSERT(vm.callFrameForCatch == callFrame);</span>
557 
<a name="31" id="anc31"></a><span class="line-modified">558     CodeBlock* codeBlock = callFrame-&gt;codeBlock();</span>
559 
560     ASSERT(codeBlock);
561     ASSERT(codeBlock-&gt;jitType() == JITType::FTLJIT);
562 
563     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
564     // really be profitable.
565     DeferGCForAWhile deferGC(vm.heap);
566 
567     JITCode* jitCode = codeBlock-&gt;jitCode()-&gt;ftl();
568     OSRExit&amp; exit = jitCode-&gt;osrExit[exitID];
569 
570     if (shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseFTLOSRExit()) {
571         dataLog(&quot;    Owning block: &quot;, pointerDump(codeBlock), &quot;\n&quot;);
572         dataLog(&quot;    Origin: &quot;, exit.m_codeOrigin, &quot;\n&quot;);
573         if (exit.m_codeOriginForExitProfile != exit.m_codeOrigin)
574             dataLog(&quot;    Origin for exit profile: &quot;, exit.m_codeOriginForExitProfile, &quot;\n&quot;);
<a name="32" id="anc32"></a><span class="line-modified">575         dataLog(&quot;    Current call site index: &quot;, callFrame-&gt;callSiteIndex().bits(), &quot;\n&quot;);</span>
576         dataLog(&quot;    Exit is exception handler: &quot;, exit.isExceptionHandler(), &quot;\n&quot;);
577         dataLog(&quot;    Is unwind handler: &quot;, exit.isGenericUnwindHandler(), &quot;\n&quot;);
578         dataLog(&quot;    Exit values: &quot;, exit.m_descriptor-&gt;m_values, &quot;\n&quot;);
579         dataLog(&quot;    Value reps: &quot;, listDump(exit.m_valueReps), &quot;\n&quot;);
580         if (!exit.m_descriptor-&gt;m_materializations.isEmpty()) {
581             dataLog(&quot;    Materializations:\n&quot;);
582             for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations)
583                 dataLog(&quot;        &quot;, pointerDump(materialization), &quot;\n&quot;);
584         }
585     }
586 
<a name="33" id="anc33"></a><span class="line-modified">587     compileStub(vm, exitID, jitCode, exit, codeBlock);</span>


588 
589     MacroAssembler::repatchJump(
590         exit.codeLocationForRepatch(codeBlock), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
591 
592     return exit.m_code.code().executableAddress();
593 }
594 
595 } } // namespace JSC::FTL
596 
597 #endif // ENABLE(FTL_JIT)
598 
<a name="34" id="anc34"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="34" type="hidden" />
</body>
</html>