<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/SlotVisitor.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2012-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
 14  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 15  * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
 17  * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 23  * THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;SlotVisitor.h&quot;
 28 
 29 #include &quot;BlockDirectoryInlines.h&quot;
 30 #include &quot;CPU.h&quot;
 31 #include &quot;ConservativeRoots.h&quot;
 32 #include &quot;GCSegmentedArrayInlines.h&quot;
 33 #include &quot;HeapAnalyzer.h&quot;
 34 #include &quot;HeapCellInlines.h&quot;
 35 #include &quot;HeapProfiler.h&quot;
 36 #include &quot;IntegrityInlines.h&quot;
 37 #include &quot;JSArray.h&quot;
 38 #include &quot;JSDestructibleObject.h&quot;
 39 #include &quot;JSObject.h&quot;
 40 #include &quot;JSString.h&quot;
 41 #include &quot;JSCInlines.h&quot;
 42 #include &quot;MarkedBlockInlines.h&quot;
 43 #include &quot;MarkingConstraintSolver.h&quot;
 44 #include &quot;SlotVisitorInlines.h&quot;
 45 #include &quot;StopIfNecessaryTimer.h&quot;
 46 #include &quot;SuperSampler.h&quot;
 47 #include &quot;VM.h&quot;
 48 #include &lt;wtf/ListDump.h&gt;
 49 #include &lt;wtf/Lock.h&gt;
 50 #include &lt;wtf/StdLibExtras.h&gt;
 51 
 52 namespace JSC {
 53 
 54 #if ENABLE(GC_VALIDATION)
 55 static void validate(JSCell* cell)
 56 {
 57     RELEASE_ASSERT(cell);
 58 
 59     if (!cell-&gt;structure()) {
 60         dataLogF(&quot;cell at %p has a null structure\n&quot; , cell);
 61         CRASH();
 62     }
 63 
 64     // Both the cell&#39;s structure, and the cell&#39;s structure&#39;s structure should be the Structure Structure.
 65     // I hate this sentence.
 66     VM&amp; vm = cell-&gt;vm();
 67     if (cell-&gt;structure()-&gt;structure()-&gt;JSCell::classInfo(vm) != cell-&gt;structure()-&gt;JSCell::classInfo(vm)) {
 68         const char* parentClassName = 0;
 69         const char* ourClassName = 0;
 70         if (cell-&gt;structure()-&gt;structure() &amp;&amp; cell-&gt;structure()-&gt;structure()-&gt;JSCell::classInfo(vm))
 71             parentClassName = cell-&gt;structure()-&gt;structure()-&gt;JSCell::classInfo(vm)-&gt;className;
 72         if (cell-&gt;structure()-&gt;JSCell::classInfo(vm))
 73             ourClassName = cell-&gt;structure()-&gt;JSCell::classInfo(vm)-&gt;className;
 74         dataLogF(&quot;parent structure (%p &lt;%s&gt;) of cell at %p doesn&#39;t match cell&#39;s structure (%p &lt;%s&gt;)\n&quot;,
 75             cell-&gt;structure()-&gt;structure(), parentClassName, cell, cell-&gt;structure(), ourClassName);
 76         CRASH();
 77     }
 78 
 79     // Make sure we can walk the ClassInfo chain
 80     const ClassInfo* info = cell-&gt;classInfo(vm);
 81     do { } while ((info = info-&gt;parentClass));
 82 }
 83 #endif
 84 
 85 SlotVisitor::SlotVisitor(Heap&amp; heap, CString codeName)
 86     : m_bytesVisited(0)
 87     , m_visitCount(0)
 88     , m_isInParallelMode(false)
 89     , m_markingVersion(MarkedSpace::initialVersion)
 90     , m_heap(heap)
 91     , m_codeName(codeName)
 92 #if ASSERT_ENABLED
 93     , m_isCheckingForDefaultMarkViolation(false)
 94     , m_isDraining(false)
 95 #endif
 96 {
 97 }
 98 
 99 SlotVisitor::~SlotVisitor()
100 {
101     clearMarkStacks();
102 }
103 
104 void SlotVisitor::didStartMarking()
105 {
106     auto scope = heap()-&gt;collectionScope();
107     if (scope) {
108         switch (*scope) {
109         case CollectionScope::Eden:
110             reset();
111             break;
112         case CollectionScope::Full:
113             m_extraMemorySize = 0;
114             break;
115         }
116     }
117 
118     if (HeapProfiler* heapProfiler = vm().heapProfiler())
119         m_heapAnalyzer = heapProfiler-&gt;activeHeapAnalyzer();
120 
121     m_markingVersion = heap()-&gt;objectSpace().markingVersion();
122 }
123 
124 void SlotVisitor::reset()
125 {
126     m_bytesVisited = 0;
127     m_visitCount = 0;
128     m_heapAnalyzer = nullptr;
129     RELEASE_ASSERT(!m_currentCell);
130 }
131 
132 void SlotVisitor::clearMarkStacks()
133 {
134     forEachMarkStack(
135         [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
136             stack.clear();
137             return IterationStatus::Continue;
138         });
139 }
140 
141 void SlotVisitor::append(const ConservativeRoots&amp; conservativeRoots)
142 {
143     HeapCell** roots = conservativeRoots.roots();
144     size_t size = conservativeRoots.size();
145     for (size_t i = 0; i &lt; size; ++i)
146         appendJSCellOrAuxiliary(roots[i]);
147 }
148 
149 void SlotVisitor::appendJSCellOrAuxiliary(HeapCell* heapCell)
150 {
151     if (!heapCell)
152         return;
153 
154     ASSERT(!m_isCheckingForDefaultMarkViolation);
155 
156     auto validateCell = [&amp;] (JSCell* jsCell) {
157         StructureID structureID = jsCell-&gt;structureID();
158 
159         auto die = [&amp;] (const char* text) {
160             WTF::dataFile().atomically(
161                 [&amp;] (PrintStream&amp; out) {
162                     out.print(text);
163                     out.print(&quot;GC type: &quot;, heap()-&gt;collectionScope(), &quot;\n&quot;);
164                     out.print(&quot;Object at: &quot;, RawPointer(jsCell), &quot;\n&quot;);
165 #if USE(JSVALUE64)
166                     out.print(&quot;Structure ID: &quot;, structureID, &quot; (0x&quot;, format(&quot;%x&quot;, structureID), &quot;)\n&quot;);
167                     out.print(&quot;Structure ID table size: &quot;, heap()-&gt;structureIDTable().size(), &quot;\n&quot;);
168 #else
169                     out.print(&quot;Structure: &quot;, RawPointer(structureID), &quot;\n&quot;);
170 #endif
171                     out.print(&quot;Object contents:&quot;);
172                     for (unsigned i = 0; i &lt; 2; ++i)
173                         out.print(&quot; &quot;, format(&quot;0x%016llx&quot;, bitwise_cast&lt;uint64_t*&gt;(jsCell)[i]));
174                     out.print(&quot;\n&quot;);
175                     CellContainer container = jsCell-&gt;cellContainer();
176                     out.print(&quot;Is marked: &quot;, container.isMarked(jsCell), &quot;\n&quot;);
177                     out.print(&quot;Is newly allocated: &quot;, container.isNewlyAllocated(jsCell), &quot;\n&quot;);
178                     if (container.isMarkedBlock()) {
179                         MarkedBlock&amp; block = container.markedBlock();
180                         out.print(&quot;Block: &quot;, RawPointer(&amp;block), &quot;\n&quot;);
181                         block.handle().dumpState(out);
182                         out.print(&quot;\n&quot;);
183                         out.print(&quot;Is marked raw: &quot;, block.isMarkedRaw(jsCell), &quot;\n&quot;);
184                         out.print(&quot;Marking version: &quot;, block.markingVersion(), &quot;\n&quot;);
185                         out.print(&quot;Heap marking version: &quot;, heap()-&gt;objectSpace().markingVersion(), &quot;\n&quot;);
186                         out.print(&quot;Is newly allocated raw: &quot;, block.isNewlyAllocated(jsCell), &quot;\n&quot;);
187                         out.print(&quot;Newly allocated version: &quot;, block.newlyAllocatedVersion(), &quot;\n&quot;);
188                         out.print(&quot;Heap newly allocated version: &quot;, heap()-&gt;objectSpace().newlyAllocatedVersion(), &quot;\n&quot;);
189                     }
190                     UNREACHABLE_FOR_PLATFORM();
191                 });
192         };
193 
194         // It&#39;s not OK for the structure to be null at any GC scan point. We must not GC while
195         // an object is not fully initialized.
196         if (!structureID)
197             die(&quot;GC scan found corrupt object: structureID is zero!\n&quot;);
198 
199         // It&#39;s not OK for the structure to be nuked at any GC scan point.
200         if (isNuked(structureID))
201             die(&quot;GC scan found object in bad state: structureID is nuked!\n&quot;);
202 
203 #if USE(JSVALUE64)
204         // This detects the worst of the badness.
205         if (!heap()-&gt;structureIDTable().isValid(structureID))
206             die(&quot;GC scan found corrupt object: structureID is invalid!\n&quot;);
207 #endif
208     };
209 
210     // In debug mode, we validate before marking since this makes it clearer what the problem
211     // was. It&#39;s also slower, so we don&#39;t do it normally.
212     if (ASSERT_ENABLED &amp;&amp; isJSCellKind(heapCell-&gt;cellKind()))
213         validateCell(static_cast&lt;JSCell*&gt;(heapCell));
214 
215     if (Heap::testAndSetMarked(m_markingVersion, heapCell))
216         return;
217 
218     switch (heapCell-&gt;cellKind()) {
219     case HeapCell::JSCell:
220     case HeapCell::JSCellWithInteriorPointers: {
221         // We have ample budget to perform validation here.
222 
223         JSCell* jsCell = static_cast&lt;JSCell*&gt;(heapCell);
224         validateCell(jsCell);
225         Integrity::auditCell(vm(), jsCell);
226 
227         jsCell-&gt;setCellState(CellState::PossiblyGrey);
228 
229         appendToMarkStack(jsCell);
230         return;
231     }
232 
233     case HeapCell::Auxiliary: {
234         noteLiveAuxiliaryCell(heapCell);
235         return;
236     } }
237 }
238 
239 void SlotVisitor::appendSlow(JSCell* cell, Dependency dependency)
240 {
241     if (UNLIKELY(m_heapAnalyzer))
242         m_heapAnalyzer-&gt;analyzeEdge(m_currentCell, cell, m_rootMarkReason);
243 
244     appendHiddenSlowImpl(cell, dependency);
245 }
246 
247 void SlotVisitor::appendHiddenSlow(JSCell* cell, Dependency dependency)
248 {
249     appendHiddenSlowImpl(cell, dependency);
250 }
251 
252 ALWAYS_INLINE void SlotVisitor::appendHiddenSlowImpl(JSCell* cell, Dependency dependency)
253 {
254     ASSERT(!m_isCheckingForDefaultMarkViolation);
255 
256 #if ENABLE(GC_VALIDATION)
257     validate(cell);
258 #endif
259 
260     if (cell-&gt;isPreciseAllocation())
261         setMarkedAndAppendToMarkStack(cell-&gt;preciseAllocation(), cell, dependency);
262     else
263         setMarkedAndAppendToMarkStack(cell-&gt;markedBlock(), cell, dependency);
264 }
265 
266 template&lt;typename ContainerType&gt;
267 ALWAYS_INLINE void SlotVisitor::setMarkedAndAppendToMarkStack(ContainerType&amp; container, JSCell* cell, Dependency dependency)
268 {
269     if (container.testAndSetMarked(cell, dependency))
270         return;
271 
272     ASSERT(cell-&gt;structure());
273 
274     // Indicate that the object is grey and that:
275     // In case of concurrent GC: it&#39;s the first time it is grey in this GC cycle.
276     // In case of eden collection: it&#39;s a new object that became grey rather than an old remembered object.
277     cell-&gt;setCellState(CellState::PossiblyGrey);
278 
279     appendToMarkStack(container, cell);
280 }
281 
282 void SlotVisitor::appendToMarkStack(JSCell* cell)
283 {
284     if (cell-&gt;isPreciseAllocation())
285         appendToMarkStack(cell-&gt;preciseAllocation(), cell);
286     else
287         appendToMarkStack(cell-&gt;markedBlock(), cell);
288 }
289 
290 template&lt;typename ContainerType&gt;
291 ALWAYS_INLINE void SlotVisitor::appendToMarkStack(ContainerType&amp; container, JSCell* cell)
292 {
293     ASSERT(m_heap.isMarked(cell));
294 #if CPU(X86_64)
295     if (UNLIKELY(Options::dumpZappedCellCrashData())) {
296         if (UNLIKELY(cell-&gt;isZapped()))
297             reportZappedCellAndCrash(cell);
298     }
299 #endif
300     ASSERT(!cell-&gt;isZapped());
301 
302     container.noteMarked();
303 
304     m_visitCount++;
305     m_bytesVisited += container.cellSize();
306 
307     m_collectorStack.append(cell);
308 }
309 
310 void SlotVisitor::markAuxiliary(const void* base)
311 {
312     HeapCell* cell = bitwise_cast&lt;HeapCell*&gt;(base);
313 
314     ASSERT(cell-&gt;heap() == heap());
315 
316     if (Heap::testAndSetMarked(m_markingVersion, cell))
317         return;
318 
319     noteLiveAuxiliaryCell(cell);
320 }
321 
322 void SlotVisitor::noteLiveAuxiliaryCell(HeapCell* cell)
323 {
324     // We get here once per GC under these circumstances:
325     //
326     // Eden collection: if the cell was allocated since the last collection and is live somehow.
327     //
328     // Full collection: if the cell is live somehow.
329 
330     CellContainer container = cell-&gt;cellContainer();
331 
332     container.assertValidCell(vm(), cell);
333     container.noteMarked();
334 
335     m_visitCount++;
336 
337     size_t cellSize = container.cellSize();
338     m_bytesVisited += cellSize;
339     m_nonCellVisitCount += cellSize;
340 }
341 
342 class SetCurrentCellScope {
343 public:
344     SetCurrentCellScope(SlotVisitor&amp; visitor, const JSCell* cell)
345         : m_visitor(visitor)
346     {
347         ASSERT(!m_visitor.m_currentCell);
348         m_visitor.m_currentCell = const_cast&lt;JSCell*&gt;(cell);
349     }
350 
351     ~SetCurrentCellScope()
352     {
353         ASSERT(m_visitor.m_currentCell);
354         m_visitor.m_currentCell = nullptr;
355     }
356 
357 private:
358     SlotVisitor&amp; m_visitor;
359 };
360 
361 ALWAYS_INLINE void SlotVisitor::visitChildren(const JSCell* cell)
362 {
363     ASSERT(m_heap.isMarked(cell));
364 
365     SetCurrentCellScope currentCellScope(*this, cell);
366 
367     if (false) {
368         dataLog(&quot;Visiting &quot;, RawPointer(cell));
369         if (!m_isFirstVisit)
370             dataLog(&quot; (subsequent)&quot;);
371         dataLog(&quot;\n&quot;);
372     }
373 
374     // Funny story: it&#39;s possible for the object to be black already, if we barrier the object at
375     // about the same time that it&#39;s marked. That&#39;s fine. It&#39;s a gnarly and super-rare race. It&#39;s
376     // not clear to me that it would be correct or profitable to bail here if the object is already
377     // black.
378 
379     cell-&gt;setCellState(CellState::PossiblyBlack);
380 
381     WTF::storeLoadFence();
382 
383     switch (cell-&gt;type()) {
384     case StringType:
385         JSString::visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
386         break;
387 
388     case FinalObjectType:
389         JSFinalObject::visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
390         break;
391 
392     case ArrayType:
393         JSArray::visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
394         break;
395 
396     default:
397         // FIXME: This could be so much better.
398         // https://bugs.webkit.org/show_bug.cgi?id=162462
399 #if CPU(X86_64)
400         if (UNLIKELY(Options::dumpZappedCellCrashData())) {
401             Structure* structure = cell-&gt;structure(vm());
402             if (LIKELY(structure)) {
403                 const MethodTable* methodTable = &amp;structure-&gt;classInfo()-&gt;methodTable;
404                 methodTable-&gt;visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
405                 break;
406             }
407             reportZappedCellAndCrash(const_cast&lt;JSCell*&gt;(cell));
408         }
409 #endif
410         cell-&gt;methodTable(vm())-&gt;visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
411         break;
412     }
413 
414     if (UNLIKELY(m_heapAnalyzer)) {
415         if (m_isFirstVisit)
416             m_heapAnalyzer-&gt;analyzeNode(const_cast&lt;JSCell*&gt;(cell));
417     }
418 }
419 
420 void SlotVisitor::visitAsConstraint(const JSCell* cell)
421 {
422     m_isFirstVisit = false;
423     visitChildren(cell);
424 }
425 
426 inline void SlotVisitor::propagateExternalMemoryVisitedIfNecessary()
427 {
428     if (m_isFirstVisit) {
429         if (m_extraMemorySize.hasOverflowed())
430             heap()-&gt;reportExtraMemoryVisited(std::numeric_limits&lt;size_t&gt;::max());
431         else if (m_extraMemorySize)
432             heap()-&gt;reportExtraMemoryVisited(m_extraMemorySize.unsafeGet());
433         m_extraMemorySize = 0;
434     }
435 }
436 
437 void SlotVisitor::donateKnownParallel(MarkStackArray&amp; from, MarkStackArray&amp; to)
438 {
439     // NOTE: Because we re-try often, we can afford to be conservative, and
440     // assume that donating is not profitable.
441 
442     // Avoid locking when a thread reaches a dead end in the object graph.
443     if (from.size() &lt; 2)
444         return;
445 
446     // If there&#39;s already some shared work queued up, be conservative and assume
447     // that donating more is not profitable.
448     if (to.size())
449         return;
450 
451     // If we&#39;re contending on the lock, be conservative and assume that another
452     // thread is already donating.
453     std::unique_lock&lt;Lock&gt; lock(m_heap.m_markingMutex, std::try_to_lock);
454     if (!lock.owns_lock())
455         return;
456 
457     // Otherwise, assume that a thread will go idle soon, and donate.
458     from.donateSomeCellsTo(to);
459 
460     m_heap.m_markingConditionVariable.notifyAll();
461 }
462 
463 void SlotVisitor::donateKnownParallel()
464 {
465     forEachMarkStack(
466         [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
467             donateKnownParallel(stack, correspondingGlobalStack(stack));
468             return IterationStatus::Continue;
469         });
470 }
471 
472 void SlotVisitor::updateMutatorIsStopped(const AbstractLocker&amp;)
473 {
474     m_mutatorIsStopped = (m_heap.worldIsStopped() &amp; m_canOptimizeForStoppedMutator);
475 }
476 
477 void SlotVisitor::updateMutatorIsStopped()
478 {
479     if (mutatorIsStoppedIsUpToDate())
480         return;
481     updateMutatorIsStopped(holdLock(m_rightToRun));
482 }
483 
484 bool SlotVisitor::hasAcknowledgedThatTheMutatorIsResumed() const
485 {
486     return !m_mutatorIsStopped;
487 }
488 
489 bool SlotVisitor::mutatorIsStoppedIsUpToDate() const
490 {
491     return m_mutatorIsStopped == (m_heap.worldIsStopped() &amp; m_canOptimizeForStoppedMutator);
492 }
493 
494 void SlotVisitor::optimizeForStoppedMutator()
495 {
496     m_canOptimizeForStoppedMutator = true;
497 }
498 
499 NEVER_INLINE void SlotVisitor::drain(MonotonicTime timeout)
500 {
501     if (!m_isInParallelMode) {
502         dataLog(&quot;FATAL: attempting to drain when not in parallel mode.\n&quot;);
503         RELEASE_ASSERT_NOT_REACHED();
504     }
505 
506     auto locker = holdLock(m_rightToRun);
507 
508     while (!hasElapsed(timeout)) {
509         updateMutatorIsStopped(locker);
510         IterationStatus status = forEachMarkStack(
511             [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
512                 if (stack.isEmpty())
513                     return IterationStatus::Continue;
514 
515                 stack.refill();
516 
517                 m_isFirstVisit = (&amp;stack == &amp;m_collectorStack);
518 
519                 for (unsigned countdown = Options::minimumNumberOfScansBetweenRebalance(); stack.canRemoveLast() &amp;&amp; countdown--;)
520                     visitChildren(stack.removeLast());
521                 return IterationStatus::Done;
522             });
523         propagateExternalMemoryVisitedIfNecessary();
524         if (status == IterationStatus::Continue)
525             break;
526 
527         m_rightToRun.safepoint();
528         donateKnownParallel();
529     }
530 }
531 
532 size_t SlotVisitor::performIncrementOfDraining(size_t bytesRequested)
533 {
534     RELEASE_ASSERT(m_isInParallelMode);
535 
536     size_t cellsRequested = bytesRequested / MarkedBlock::atomSize;
537     {
538         auto locker = holdLock(m_heap.m_markingMutex);
539         forEachMarkStack(
540             [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
541                 cellsRequested -= correspondingGlobalStack(stack).transferTo(stack, cellsRequested);
542                 return cellsRequested ? IterationStatus::Continue : IterationStatus::Done;
543             });
544     }
545 
546     size_t cellBytesVisited = 0;
547     m_nonCellVisitCount = 0;
548 
549     auto bytesVisited = [&amp;] () -&gt; size_t {
550         return cellBytesVisited + m_nonCellVisitCount;
551     };
552 
553     auto isDone = [&amp;] () -&gt; bool {
554         return bytesVisited() &gt;= bytesRequested;
555     };
556 
557     {
558         auto locker = holdLock(m_rightToRun);
559 
560         while (!isDone()) {
561             updateMutatorIsStopped(locker);
562             IterationStatus status = forEachMarkStack(
563                 [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
564                     if (stack.isEmpty() || isDone())
565                         return IterationStatus::Continue;
566 
567                     stack.refill();
568 
569                     m_isFirstVisit = (&amp;stack == &amp;m_collectorStack);
570 
571                     unsigned countdown = Options::minimumNumberOfScansBetweenRebalance();
572                     while (countdown &amp;&amp; stack.canRemoveLast() &amp;&amp; !isDone()) {
573                         const JSCell* cell = stack.removeLast();
574                         cellBytesVisited += cell-&gt;cellSize();
575                         visitChildren(cell);
576                         countdown--;
577                     }
578                     return IterationStatus::Done;
579                 });
580             propagateExternalMemoryVisitedIfNecessary();
581             if (status == IterationStatus::Continue)
582                 break;
583             m_rightToRun.safepoint();
584             donateKnownParallel();
585         }
586     }
587 
588     donateAll();
589 
590     return bytesVisited();
591 }
592 
593 bool SlotVisitor::didReachTermination()
594 {
595     LockHolder locker(m_heap.m_markingMutex);
596     return didReachTermination(locker);
597 }
598 
599 bool SlotVisitor::didReachTermination(const AbstractLocker&amp; locker)
600 {
601     return !m_heap.m_numberOfActiveParallelMarkers
602         &amp;&amp; !hasWork(locker);
603 }
604 
605 bool SlotVisitor::hasWork(const AbstractLocker&amp;)
606 {
607     return !isEmpty()
608         || !m_heap.m_sharedCollectorMarkStack-&gt;isEmpty()
609         || !m_heap.m_sharedMutatorMarkStack-&gt;isEmpty();
610 }
611 
612 NEVER_INLINE SlotVisitor::SharedDrainResult SlotVisitor::drainFromShared(SharedDrainMode sharedDrainMode, MonotonicTime timeout)
613 {
614     ASSERT(m_isInParallelMode);
615 
616     ASSERT(Options::numberOfGCMarkers());
617 
618     bool isActive = false;
619     while (true) {
620         RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; bonusTask;
621 
622         {
623             auto locker = holdLock(m_heap.m_markingMutex);
624             if (isActive)
625                 m_heap.m_numberOfActiveParallelMarkers--;
626             m_heap.m_numberOfWaitingParallelMarkers++;
627 
628             if (sharedDrainMode == MasterDrain) {
629                 while (true) {
630                     if (hasElapsed(timeout))
631                         return SharedDrainResult::TimedOut;
632 
633                     if (didReachTermination(locker)) {
634                         m_heap.m_markingConditionVariable.notifyAll();
635                         return SharedDrainResult::Done;
636                     }
637 
638                     if (hasWork(locker))
639                         break;
640 
641                     m_heap.m_markingConditionVariable.waitUntil(m_heap.m_markingMutex, timeout);
642                 }
643             } else {
644                 ASSERT(sharedDrainMode == SlaveDrain);
645 
646                 if (hasElapsed(timeout))
647                     return SharedDrainResult::TimedOut;
648 
649                 if (didReachTermination(locker)) {
650                     m_heap.m_markingConditionVariable.notifyAll();
651 
652                     // If we&#39;re in concurrent mode, then we know that the mutator will eventually do
653                     // the right thing because:
654                     // - It&#39;s possible that the collector has the conn. In that case, the collector will
655                     //   wake up from the notification above. This will happen if the app released heap
656                     //   access. Native apps can spend a lot of time with heap access released.
657                     // - It&#39;s possible that the mutator will allocate soon. Then it will check if we
658                     //   reached termination. This is the most likely outcome in programs that allocate
659                     //   a lot.
660                     // - WebCore never releases access. But WebCore has a runloop. The runloop will check
661                     //   if we reached termination.
662                     // So, this tells the runloop that it&#39;s got things to do.
663                     m_heap.m_stopIfNecessaryTimer-&gt;scheduleSoon();
664                 }
665 
666                 auto isReady = [&amp;] () -&gt; bool {
667                     return hasWork(locker)
668                         || m_heap.m_bonusVisitorTask
669                         || m_heap.m_parallelMarkersShouldExit;
670                 };
671 
672                 m_heap.m_markingConditionVariable.waitUntil(m_heap.m_markingMutex, timeout, isReady);
673 
674                 if (!hasWork(locker)
675                     &amp;&amp; m_heap.m_bonusVisitorTask)
676                     bonusTask = m_heap.m_bonusVisitorTask;
677 
678                 if (m_heap.m_parallelMarkersShouldExit)
679                     return SharedDrainResult::Done;
680             }
681 
682             if (!bonusTask &amp;&amp; isEmpty()) {
683                 forEachMarkStack(
684                     [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
685                         stack.stealSomeCellsFrom(
686                             correspondingGlobalStack(stack),
687                             m_heap.m_numberOfWaitingParallelMarkers);
688                         return IterationStatus::Continue;
689                     });
690             }
691 
692             m_heap.m_numberOfActiveParallelMarkers++;
693             m_heap.m_numberOfWaitingParallelMarkers--;
694         }
695 
696         if (bonusTask) {
697             bonusTask-&gt;run(*this);
698 
699             // The main thread could still be running, and may run for a while. Unless we clear the task
700             // ourselves, we will keep looping around trying to run the task.
701             {
702                 auto locker = holdLock(m_heap.m_markingMutex);
703                 if (m_heap.m_bonusVisitorTask == bonusTask)
704                     m_heap.m_bonusVisitorTask = nullptr;
705                 bonusTask = nullptr;
706                 m_heap.m_markingConditionVariable.notifyAll();
707             }
708         } else {
709             RELEASE_ASSERT(!isEmpty());
710             drain(timeout);
711         }
712 
713         isActive = true;
714     }
715 }
716 
717 SlotVisitor::SharedDrainResult SlotVisitor::drainInParallel(MonotonicTime timeout)
718 {
719     donateAndDrain(timeout);
720     return drainFromShared(MasterDrain, timeout);
721 }
722 
723 SlotVisitor::SharedDrainResult SlotVisitor::drainInParallelPassively(MonotonicTime timeout)
724 {
725     ASSERT(m_isInParallelMode);
726 
727     ASSERT(Options::numberOfGCMarkers());
728 
729     if (Options::numberOfGCMarkers() == 1
730         || (m_heap.m_worldState.load() &amp; Heap::mutatorWaitingBit)
731         || !m_heap.hasHeapAccess()
732         || m_heap.worldIsStopped()) {
733         // This is an optimization over drainInParallel() when we have a concurrent mutator but
734         // otherwise it is not profitable.
735         return drainInParallel(timeout);
736     }
737 
738     donateAll(holdLock(m_heap.m_markingMutex));
739     return waitForTermination(timeout);
740 }
741 
742 SlotVisitor::SharedDrainResult SlotVisitor::waitForTermination(MonotonicTime timeout)
743 {
744     auto locker = holdLock(m_heap.m_markingMutex);
745     for (;;) {
746         if (hasElapsed(timeout))
747             return SharedDrainResult::TimedOut;
748 
749         if (didReachTermination(locker)) {
750             m_heap.m_markingConditionVariable.notifyAll();
751             return SharedDrainResult::Done;
752         }
753 
754         m_heap.m_markingConditionVariable.waitUntil(m_heap.m_markingMutex, timeout);
755     }
756 }
757 
758 void SlotVisitor::donateAll()
759 {
760     if (isEmpty())
761         return;
762 
763     donateAll(holdLock(m_heap.m_markingMutex));
764 }
765 
766 void SlotVisitor::donateAll(const AbstractLocker&amp;)
767 {
768     forEachMarkStack(
769         [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
770             stack.transferTo(correspondingGlobalStack(stack));
771             return IterationStatus::Continue;
772         });
773 
774     m_heap.m_markingConditionVariable.notifyAll();
775 }
776 
777 void SlotVisitor::donate()
778 {
779     if (!m_isInParallelMode) {
780         dataLog(&quot;FATAL: Attempting to donate when not in parallel mode.\n&quot;);
781         RELEASE_ASSERT_NOT_REACHED();
782     }
783 
784     if (Options::numberOfGCMarkers() == 1)
785         return;
786 
787     donateKnownParallel();
788 }
789 
790 void SlotVisitor::donateAndDrain(MonotonicTime timeout)
791 {
792     donate();
793     drain(timeout);
794 }
795 
796 void SlotVisitor::didRace(const VisitRaceKey&amp; race)
797 {
798     dataLogLnIf(Options::verboseVisitRace(), toCString(&quot;GC visit race: &quot;, race));
799 
800     auto locker = holdLock(heap()-&gt;m_raceMarkStackLock);
801     JSCell* cell = race.cell();
802     cell-&gt;setCellState(CellState::PossiblyGrey);
803     heap()-&gt;m_raceMarkStack-&gt;append(cell);
804 }
805 
806 void SlotVisitor::dump(PrintStream&amp; out) const
807 {
808     out.print(&quot;Collector: [&quot;, pointerListDump(collectorMarkStack()), &quot;], Mutator: [&quot;, pointerListDump(mutatorMarkStack()), &quot;]&quot;);
809 }
810 
811 MarkStackArray&amp; SlotVisitor::correspondingGlobalStack(MarkStackArray&amp; stack)
812 {
813     if (&amp;stack == &amp;m_collectorStack)
814         return *m_heap.m_sharedCollectorMarkStack;
815     RELEASE_ASSERT(&amp;stack == &amp;m_mutatorStack);
816     return *m_heap.m_sharedMutatorMarkStack;
817 }
818 
819 void SlotVisitor::addParallelConstraintTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
820 {
821     RELEASE_ASSERT(m_currentSolver);
822     RELEASE_ASSERT(m_currentConstraint);
823     RELEASE_ASSERT(task);
824 
825     m_currentSolver-&gt;addParallelTask(task, *m_currentConstraint);
826 }
827 
828 #if CPU(X86_64)
829 NEVER_INLINE NO_RETURN_DUE_TO_CRASH NOT_TAIL_CALLED void SlotVisitor::reportZappedCellAndCrash(JSCell* cell)
830 {
831     MarkedBlock::Handle* foundBlockHandle = nullptr;
832     uint64_t* cellWords = reinterpret_cast_ptr&lt;uint64_t*&gt;(cell);
833 
834     uintptr_t cellAddress = bitwise_cast&lt;uintptr_t&gt;(cell);
835     uint64_t headerWord = cellWords[0];
836     uint64_t zapReasonAndMore = cellWords[1];
837     unsigned subspaceHash = 0;
838     size_t cellSize = 0;
839 
840     m_heap.objectSpace().forEachBlock([&amp;] (MarkedBlock::Handle* blockHandle) {
841         if (blockHandle-&gt;contains(cell)) {
842             foundBlockHandle = blockHandle;
843             return IterationStatus::Done;
844         }
845         return IterationStatus::Continue;
846     });
847 
848     uint64_t variousState = 0;
849     MarkedBlock* foundBlock = nullptr;
850     if (foundBlockHandle) {
851         foundBlock = &amp;foundBlockHandle-&gt;block();
852         subspaceHash = StringHasher::computeHash(foundBlockHandle-&gt;subspace()-&gt;name());
853         cellSize = foundBlockHandle-&gt;cellSize();
854 
855         variousState |= static_cast&lt;uint64_t&gt;(foundBlockHandle-&gt;isFreeListed()) &lt;&lt; 0;
856         variousState |= static_cast&lt;uint64_t&gt;(foundBlockHandle-&gt;isAllocated()) &lt;&lt; 1;
857         variousState |= static_cast&lt;uint64_t&gt;(foundBlockHandle-&gt;isEmpty()) &lt;&lt; 2;
858         variousState |= static_cast&lt;uint64_t&gt;(foundBlockHandle-&gt;needsDestruction()) &lt;&lt; 3;
859         variousState |= static_cast&lt;uint64_t&gt;(foundBlock-&gt;isNewlyAllocated(cell)) &lt;&lt; 4;
860 
861         ptrdiff_t cellOffset = cellAddress - reinterpret_cast&lt;uint64_t&gt;(foundBlockHandle-&gt;start());
862         bool cellIsProperlyAligned = !(cellOffset % cellSize);
863         variousState |= static_cast&lt;uint64_t&gt;(cellIsProperlyAligned) &lt;&lt; 5;
864     }
865 
866     CRASH_WITH_INFO(cellAddress, headerWord, zapReasonAndMore, subspaceHash, cellSize, foundBlock, variousState);
867 }
868 #endif // PLATFORM(MAC)
869 
870 } // namespace JSC
    </pre>
  </body>
</html>