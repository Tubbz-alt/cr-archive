<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/AssemblyHelpers.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(JIT)
  29 
  30 #include &quot;CodeBlock.h&quot;
  31 #include &quot;EntryFrame.h&quot;
  32 #include &quot;FPRInfo.h&quot;
  33 #include &quot;GPRInfo.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InlineCallFrame.h&quot;
  36 #include &quot;JITAllocator.h&quot;
  37 #include &quot;JITCode.h&quot;
<a name="1" id="anc1"></a><span class="line-added">  38 #include &quot;JSCellInlines.h&quot;</span>
  39 #include &quot;MacroAssembler.h&quot;
  40 #include &quot;MarkedSpace.h&quot;
  41 #include &quot;RegisterAtOffsetList.h&quot;
  42 #include &quot;RegisterSet.h&quot;
  43 #include &quot;StackAlignment.h&quot;
  44 #include &quot;TagRegistersMode.h&quot;
  45 #include &quot;TypeofType.h&quot;
  46 #include &quot;VM.h&quot;
  47 
  48 namespace JSC {
  49 
<a name="2" id="anc2"></a><span class="line-modified">  50 typedef void (*V_DebugOperation_EPP)(CallFrame*, void*, void*);</span>
  51 
  52 class AssemblyHelpers : public MacroAssembler {
  53 public:
  54     AssemblyHelpers(CodeBlock* codeBlock)
  55         : m_codeBlock(codeBlock)
  56         , m_baselineCodeBlock(codeBlock ? codeBlock-&gt;baselineAlternative() : 0)
  57     {
  58         if (m_codeBlock) {
  59             ASSERT(m_baselineCodeBlock);
  60             ASSERT(!m_baselineCodeBlock-&gt;alternative());
  61             ASSERT(m_baselineCodeBlock-&gt;jitType() == JITType::None || JITCode::isBaselineCode(m_baselineCodeBlock-&gt;jitType()));
  62         }
  63     }
  64 
  65     CodeBlock* codeBlock() { return m_codeBlock; }
  66     VM&amp; vm() { return m_codeBlock-&gt;vm(); }
  67     AssemblerType_T&amp; assembler() { return m_assembler; }
  68 
<a name="3" id="anc3"></a><span class="line-added">  69     void prepareCallOperation(VM&amp; vm)</span>
<span class="line-added">  70     {</span>
<span class="line-added">  71         UNUSED_PARAM(vm);</span>
<span class="line-added">  72 #if !USE(BUILTIN_FRAME_ADDRESS) || ASSERT_ENABLED</span>
<span class="line-added">  73         storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);</span>
<span class="line-added">  74 #endif</span>
<span class="line-added">  75     }</span>
<span class="line-added">  76 </span>
  77     void checkStackPointerAlignment()
  78     {
  79         // This check is both unneeded and harder to write correctly for ARM64
  80 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM64)
  81         Jump stackPointerAligned = branchTestPtr(Zero, stackPointerRegister, TrustedImm32(0xf));
  82         abortWithReason(AHStackPointerMisaligned);
  83         stackPointerAligned.link(this);
  84 #endif
  85     }
  86 
<a name="4" id="anc4"></a><span class="line-added">  87 #if USE(JSVALUE64)</span>
<span class="line-added">  88     void store64FromReg(Reg src, Address dst)</span>
<span class="line-added">  89     {</span>
<span class="line-added">  90         if (src.isFPR())</span>
<span class="line-added">  91             storeDouble(src.fpr(), dst);</span>
<span class="line-added">  92         else</span>
<span class="line-added">  93             store64(src.gpr(), dst);</span>
<span class="line-added">  94     }</span>
<span class="line-added">  95 #endif</span>
<span class="line-added">  96 </span>
<span class="line-added">  97     void store32FromReg(Reg src, Address dst)</span>
<span class="line-added">  98     {</span>
<span class="line-added">  99         if (src.isFPR())</span>
<span class="line-added"> 100             storeFloat(src.fpr(), dst);</span>
<span class="line-added"> 101         else</span>
<span class="line-added"> 102             store32(src.gpr(), dst);</span>
<span class="line-added"> 103     }</span>
<span class="line-added"> 104 </span>
<span class="line-added"> 105 #if USE(JSVALUE64)</span>
<span class="line-added"> 106     void load64ToReg(Address src, Reg dst)</span>
<span class="line-added"> 107     {</span>
<span class="line-added"> 108         if (dst.isFPR())</span>
<span class="line-added"> 109             loadDouble(src, dst.fpr());</span>
<span class="line-added"> 110         else</span>
<span class="line-added"> 111             load64(src, dst.gpr());</span>
<span class="line-added"> 112     }</span>
<span class="line-added"> 113 #endif</span>
<span class="line-added"> 114 </span>
<span class="line-added"> 115     void load32ToReg(Address src, Reg dst)</span>
<span class="line-added"> 116     {</span>
<span class="line-added"> 117         if (dst.isFPR())</span>
<span class="line-added"> 118             loadFloat(src, dst.fpr());</span>
<span class="line-added"> 119         else</span>
<span class="line-added"> 120             load32(src, dst.gpr());</span>
<span class="line-added"> 121     }</span>
<span class="line-added"> 122 </span>
 123     template&lt;typename T&gt;
 124     void storeCell(T cell, Address address)
 125     {
 126 #if USE(JSVALUE64)
 127         store64(cell, address);
 128 #else
 129         store32(cell, address.withOffset(PayloadOffset));
 130         store32(TrustedImm32(JSValue::CellTag), address.withOffset(TagOffset));
 131 #endif
 132     }
 133 
 134     void loadCell(Address address, GPRReg gpr)
 135     {
 136 #if USE(JSVALUE64)
 137         load64(address, gpr);
 138 #else
 139         load32(address.withOffset(PayloadOffset), gpr);
 140 #endif
 141     }
 142 
 143     void storeValue(JSValueRegs regs, Address address)
 144     {
 145 #if USE(JSVALUE64)
 146         store64(regs.gpr(), address);
 147 #else
 148         store32(regs.payloadGPR(), address.withOffset(PayloadOffset));
 149         store32(regs.tagGPR(), address.withOffset(TagOffset));
 150 #endif
 151     }
 152 
 153     void storeValue(JSValueRegs regs, BaseIndex address)
 154     {
 155 #if USE(JSVALUE64)
 156         store64(regs.gpr(), address);
 157 #else
 158         store32(regs.payloadGPR(), address.withOffset(PayloadOffset));
 159         store32(regs.tagGPR(), address.withOffset(TagOffset));
 160 #endif
 161     }
 162 
 163     void storeValue(JSValueRegs regs, void* address)
 164     {
 165 #if USE(JSVALUE64)
 166         store64(regs.gpr(), address);
 167 #else
 168         store32(regs.payloadGPR(), bitwise_cast&lt;void*&gt;(bitwise_cast&lt;uintptr_t&gt;(address) + PayloadOffset));
 169         store32(regs.tagGPR(), bitwise_cast&lt;void*&gt;(bitwise_cast&lt;uintptr_t&gt;(address) + TagOffset));
 170 #endif
 171     }
 172 
 173     void loadValue(Address address, JSValueRegs regs)
 174     {
 175 #if USE(JSVALUE64)
 176         load64(address, regs.gpr());
 177 #else
 178         if (address.base == regs.payloadGPR()) {
 179             load32(address.withOffset(TagOffset), regs.tagGPR());
 180             load32(address.withOffset(PayloadOffset), regs.payloadGPR());
 181         } else {
 182             load32(address.withOffset(PayloadOffset), regs.payloadGPR());
 183             load32(address.withOffset(TagOffset), regs.tagGPR());
 184         }
 185 #endif
 186     }
 187 
 188     void loadValue(BaseIndex address, JSValueRegs regs)
 189     {
 190 #if USE(JSVALUE64)
 191         load64(address, regs.gpr());
 192 #else
 193         if (address.base == regs.payloadGPR() || address.index == regs.payloadGPR()) {
 194             // We actually could handle the case where the registers are aliased to both
 195             // tag and payload, but we don&#39;t for now.
 196             RELEASE_ASSERT(address.base != regs.tagGPR());
 197             RELEASE_ASSERT(address.index != regs.tagGPR());
 198 
 199             load32(address.withOffset(TagOffset), regs.tagGPR());
 200             load32(address.withOffset(PayloadOffset), regs.payloadGPR());
 201         } else {
 202             load32(address.withOffset(PayloadOffset), regs.payloadGPR());
 203             load32(address.withOffset(TagOffset), regs.tagGPR());
 204         }
 205 #endif
 206     }
 207 
 208     void loadValue(void* address, JSValueRegs regs)
 209     {
 210 #if USE(JSVALUE64)
 211         load64(address, regs.gpr());
 212 #else
 213         move(TrustedImmPtr(address), regs.payloadGPR());
 214         loadValue(Address(regs.payloadGPR()), regs);
 215 #endif
 216     }
 217 
 218     // Note that this clobbers offset.
 219     void loadProperty(GPRReg object, GPRReg offset, JSValueRegs result);
 220 
 221     void moveValueRegs(JSValueRegs srcRegs, JSValueRegs destRegs)
 222     {
 223 #if USE(JSVALUE32_64)
 224         if (destRegs.tagGPR() == srcRegs.payloadGPR()) {
 225             if (destRegs.payloadGPR() == srcRegs.tagGPR()) {
 226                 swap(srcRegs.payloadGPR(), srcRegs.tagGPR());
 227                 return;
 228             }
 229             move(srcRegs.payloadGPR(), destRegs.payloadGPR());
 230             move(srcRegs.tagGPR(), destRegs.tagGPR());
 231             return;
 232         }
 233         move(srcRegs.tagGPR(), destRegs.tagGPR());
 234         move(srcRegs.payloadGPR(), destRegs.payloadGPR());
 235 #else
 236         move(srcRegs.gpr(), destRegs.gpr());
 237 #endif
 238     }
 239 
 240     void moveValue(JSValue value, JSValueRegs regs)
 241     {
 242 #if USE(JSVALUE64)
 243         move(Imm64(JSValue::encode(value)), regs.gpr());
 244 #else
 245         move(Imm32(value.tag()), regs.tagGPR());
 246         move(Imm32(value.payload()), regs.payloadGPR());
 247 #endif
 248     }
 249 
 250     void moveTrustedValue(JSValue value, JSValueRegs regs)
 251     {
 252 #if USE(JSVALUE64)
 253         move(TrustedImm64(JSValue::encode(value)), regs.gpr());
 254 #else
 255         move(TrustedImm32(value.tag()), regs.tagGPR());
 256         move(TrustedImm32(value.payload()), regs.payloadGPR());
 257 #endif
 258     }
 259 
 260     void storeTrustedValue(JSValue value, Address address)
 261     {
 262 #if USE(JSVALUE64)
 263         store64(TrustedImm64(JSValue::encode(value)), address);
 264 #else
 265         store32(TrustedImm32(value.tag()), address.withOffset(TagOffset));
 266         store32(TrustedImm32(value.payload()), address.withOffset(PayloadOffset));
 267 #endif
 268     }
 269 
 270     void storeTrustedValue(JSValue value, BaseIndex address)
 271     {
 272 #if USE(JSVALUE64)
 273         store64(TrustedImm64(JSValue::encode(value)), address);
 274 #else
 275         store32(TrustedImm32(value.tag()), address.withOffset(TagOffset));
 276         store32(TrustedImm32(value.payload()), address.withOffset(PayloadOffset));
 277 #endif
 278     }
 279 
 280     Address addressFor(const RegisterAtOffset&amp; entry)
 281     {
 282         return Address(GPRInfo::callFrameRegister, entry.offset());
 283     }
 284 
 285     void emitSave(const RegisterAtOffsetList&amp; list)
 286     {
 287         for (const RegisterAtOffset&amp; entry : list) {
 288             if (entry.reg().isGPR())
 289                 storePtr(entry.reg().gpr(), addressFor(entry));
 290             else
 291                 storeDouble(entry.reg().fpr(), addressFor(entry));
 292         }
 293     }
 294 
 295     void emitRestore(const RegisterAtOffsetList&amp; list)
 296     {
 297         for (const RegisterAtOffset&amp; entry : list) {
 298             if (entry.reg().isGPR())
 299                 loadPtr(addressFor(entry), entry.reg().gpr());
 300             else
 301                 loadDouble(addressFor(entry), entry.reg().fpr());
 302         }
 303     }
 304 
 305     void emitSaveCalleeSavesFor(CodeBlock* codeBlock)
 306     {
 307         ASSERT(codeBlock);
 308 
 309         const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();
 310         RegisterSet dontSaveRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 311         unsigned registerCount = calleeSaves-&gt;size();
 312 
 313         for (unsigned i = 0; i &lt; registerCount; i++) {
 314             RegisterAtOffset entry = calleeSaves-&gt;at(i);
 315             if (dontSaveRegisters.get(entry.reg()))
 316                 continue;
 317             storePtr(entry.reg().gpr(), Address(framePointerRegister, entry.offset()));
 318         }
 319     }
 320 
 321     enum RestoreTagRegisterMode { UseExistingTagRegisterContents, CopyBaselineCalleeSavedRegistersFromBaseFrame };
 322 
 323     void emitSaveOrCopyCalleeSavesFor(CodeBlock* codeBlock, VirtualRegister offsetVirtualRegister, RestoreTagRegisterMode tagRegisterMode, GPRReg temp)
 324     {
 325         ASSERT(codeBlock);
 326 
 327         const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();
 328         RegisterSet dontSaveRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 329         unsigned registerCount = calleeSaves-&gt;size();
 330 
 331 #if USE(JSVALUE64)
 332         RegisterSet baselineCalleeSaves = RegisterSet::llintBaselineCalleeSaveRegisters();
 333 #endif
 334 
 335         for (unsigned i = 0; i &lt; registerCount; i++) {
 336             RegisterAtOffset entry = calleeSaves-&gt;at(i);
 337             if (dontSaveRegisters.get(entry.reg()))
 338                 continue;
 339 
 340             GPRReg registerToWrite;
 341 
 342 #if USE(JSVALUE32_64)
 343             UNUSED_PARAM(tagRegisterMode);
 344             UNUSED_PARAM(temp);
 345 #else
 346             if (tagRegisterMode == CopyBaselineCalleeSavedRegistersFromBaseFrame &amp;&amp; baselineCalleeSaves.get(entry.reg())) {
 347                 registerToWrite = temp;
 348                 loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, entry.offset()), registerToWrite);
 349             } else
 350 #endif
 351                 registerToWrite = entry.reg().gpr();
 352 
 353             storePtr(registerToWrite, Address(framePointerRegister, offsetVirtualRegister.offsetInBytes() + entry.offset()));
 354         }
 355     }
 356 
 357     void emitRestoreCalleeSavesFor(CodeBlock* codeBlock)
 358     {
 359         ASSERT(codeBlock);
 360 
 361         const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();
 362         RegisterSet dontRestoreRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 363         unsigned registerCount = calleeSaves-&gt;size();
 364 
 365         for (unsigned i = 0; i &lt; registerCount; i++) {
 366             RegisterAtOffset entry = calleeSaves-&gt;at(i);
 367             if (dontRestoreRegisters.get(entry.reg()))
 368                 continue;
 369             loadPtr(Address(framePointerRegister, entry.offset()), entry.reg().gpr());
 370         }
 371     }
 372 
 373     void emitSaveCalleeSaves()
 374     {
 375         emitSaveCalleeSavesFor(codeBlock());
 376     }
 377 
 378     void emitSaveThenMaterializeTagRegisters()
 379     {
 380 #if USE(JSVALUE64)
 381 #if CPU(ARM64)
<a name="5" id="anc5"></a><span class="line-modified"> 382         pushPair(GPRInfo::numberTagRegister, GPRInfo::notCellMaskRegister);</span>
 383 #else
<a name="6" id="anc6"></a><span class="line-modified"> 384         push(GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 385         push(GPRInfo::notCellMaskRegister);</span>
 386 #endif
 387         emitMaterializeTagCheckRegisters();
 388 #endif
 389     }
 390 
 391     void emitRestoreCalleeSaves()
 392     {
 393         emitRestoreCalleeSavesFor(codeBlock());
 394     }
 395 
 396     void emitRestoreSavedTagRegisters()
 397     {
 398 #if USE(JSVALUE64)
 399 #if CPU(ARM64)
<a name="7" id="anc7"></a><span class="line-modified"> 400         popPair(GPRInfo::numberTagRegister, GPRInfo::notCellMaskRegister);</span>
 401 #else
<a name="8" id="anc8"></a><span class="line-modified"> 402         pop(GPRInfo::notCellMaskRegister);</span>
<span class="line-modified"> 403         pop(GPRInfo::numberTagRegister);</span>
 404 #endif
 405 #endif
 406     }
 407 
 408     // If you use this, be aware that vmGPR will get trashed.
 409     void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(GPRReg vmGPR)
 410     {
 411 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 412         loadPtr(Address(vmGPR, VM::topEntryFrameOffset()), vmGPR);
 413         copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(vmGPR);
 414 #else
 415         UNUSED_PARAM(vmGPR);
 416 #endif
 417     }
 418 
 419     void copyCalleeSavesToEntryFrameCalleeSavesBuffer(EntryFrame*&amp; topEntryFrame)
 420     {
 421 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 422         const TempRegisterSet&amp; usedRegisters = { RegisterSet::stubUnavailableRegisters() };
 423         GPRReg temp1 = usedRegisters.getFreeGPR(0);
 424         loadPtr(&amp;topEntryFrame, temp1);
 425         copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(temp1);
 426 #else
 427         UNUSED_PARAM(topEntryFrame);
 428 #endif
 429     }
 430 
 431     void copyCalleeSavesToEntryFrameCalleeSavesBuffer(GPRReg topEntryFrame)
 432     {
 433 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 434         copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(topEntryFrame);
 435 #else
 436         UNUSED_PARAM(topEntryFrame);
 437 #endif
 438     }
 439 
 440     void restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(EntryFrame*&amp;);
 441 
 442     void copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(EntryFrame*&amp; topEntryFrame, const TempRegisterSet&amp; usedRegisters = { RegisterSet::stubUnavailableRegisters() })
 443     {
 444 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 445         GPRReg temp1 = usedRegisters.getFreeGPR(0);
 446         GPRReg temp2 = usedRegisters.getFreeGPR(1);
 447         FPRReg fpTemp = usedRegisters.getFreeFPR();
 448         ASSERT(temp2 != InvalidGPRReg);
 449 
 450         ASSERT(codeBlock());
 451 
 452         // Copy saved calleeSaves on stack or unsaved calleeSaves in register to vm calleeSave buffer
 453         loadPtr(&amp;topEntryFrame, temp1);
 454         addPtr(TrustedImm32(EntryFrame::calleeSaveRegistersBufferOffset()), temp1);
 455 
 456         RegisterAtOffsetList* allCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();
 457         const RegisterAtOffsetList* currentCalleeSaves = codeBlock()-&gt;calleeSaveRegisters();
 458         RegisterSet dontCopyRegisters = RegisterSet::stackRegisters();
 459         unsigned registerCount = allCalleeSaves-&gt;size();
 460 
 461         for (unsigned i = 0; i &lt; registerCount; i++) {
 462             RegisterAtOffset entry = allCalleeSaves-&gt;at(i);
 463             if (dontCopyRegisters.get(entry.reg()))
 464                 continue;
 465             RegisterAtOffset* currentFrameEntry = currentCalleeSaves-&gt;find(entry.reg());
 466 
 467             if (entry.reg().isGPR()) {
 468                 GPRReg regToStore;
 469                 if (currentFrameEntry) {
 470                     // Load calleeSave from stack into temp register
 471                     regToStore = temp2;
 472                     loadPtr(Address(framePointerRegister, currentFrameEntry-&gt;offset()), regToStore);
 473                 } else
 474                     // Just store callee save directly
 475                     regToStore = entry.reg().gpr();
 476 
 477                 storePtr(regToStore, Address(temp1, entry.offset()));
 478             } else {
 479                 FPRReg fpRegToStore;
 480                 if (currentFrameEntry) {
 481                     // Load calleeSave from stack into temp register
 482                     fpRegToStore = fpTemp;
 483                     loadDouble(Address(framePointerRegister, currentFrameEntry-&gt;offset()), fpRegToStore);
 484                 } else
 485                     // Just store callee save directly
 486                     fpRegToStore = entry.reg().fpr();
 487 
 488                 storeDouble(fpRegToStore, Address(temp1, entry.offset()));
 489             }
 490         }
 491 #else
 492         UNUSED_PARAM(topEntryFrame);
 493         UNUSED_PARAM(usedRegisters);
 494 #endif
 495     }
 496 
 497     void emitMaterializeTagCheckRegisters()
 498     {
 499 #if USE(JSVALUE64)
<a name="9" id="anc9"></a><span class="line-modified"> 500         move(MacroAssembler::TrustedImm64(JSValue::NumberTag), GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 501         orPtr(MacroAssembler::TrustedImm32(JSValue::OtherTag), GPRInfo::numberTagRegister, GPRInfo::notCellMaskRegister);</span>
 502 #endif
 503     }
 504 
<a name="10" id="anc10"></a><span class="line-modified"> 505 #if CPU(X86_64)</span>
























 506     static constexpr size_t prologueStackPointerDelta()
 507     {
 508         // Prologue only saves the framePointerRegister
 509         return sizeof(void*);
 510     }
 511 
 512     void emitFunctionPrologue()
 513     {
 514         push(framePointerRegister);
 515         move(stackPointerRegister, framePointerRegister);
 516     }
 517 
 518     void emitFunctionEpilogueWithEmptyFrame()
 519     {
 520         pop(framePointerRegister);
 521     }
 522 
 523     void emitFunctionEpilogue()
 524     {
 525         move(framePointerRegister, stackPointerRegister);
 526         pop(framePointerRegister);
 527     }
 528 
 529     void preserveReturnAddressAfterCall(GPRReg reg)
 530     {
 531         pop(reg);
 532     }
 533 
 534     void restoreReturnAddressBeforeReturn(GPRReg reg)
 535     {
 536         push(reg);
 537     }
 538 
 539     void restoreReturnAddressBeforeReturn(Address address)
 540     {
 541         push(address);
 542     }
<a name="11" id="anc11"></a><span class="line-modified"> 543 #endif // CPU(X86_64)</span>
 544 
 545 #if CPU(ARM_THUMB2) || CPU(ARM64)
 546     static constexpr size_t prologueStackPointerDelta()
 547     {
 548         // Prologue saves the framePointerRegister and linkRegister
 549         return 2 * sizeof(void*);
 550     }
 551 
 552     void emitFunctionPrologue()
 553     {
 554         tagReturnAddress();
 555         pushPair(framePointerRegister, linkRegister);
 556         move(stackPointerRegister, framePointerRegister);
 557     }
 558 
 559     void emitFunctionEpilogueWithEmptyFrame()
 560     {
 561         popPair(framePointerRegister, linkRegister);
 562     }
 563 
 564     void emitFunctionEpilogue()
 565     {
 566         move(framePointerRegister, stackPointerRegister);
 567         emitFunctionEpilogueWithEmptyFrame();
 568     }
 569 
 570     ALWAYS_INLINE void preserveReturnAddressAfterCall(RegisterID reg)
 571     {
 572         move(linkRegister, reg);
 573     }
 574 
 575     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(RegisterID reg)
 576     {
 577         move(reg, linkRegister);
 578     }
 579 
 580     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(Address address)
 581     {
 582         loadPtr(address, linkRegister);
 583     }
 584 #endif
 585 
 586 #if CPU(MIPS)
 587     static constexpr size_t prologueStackPointerDelta()
 588     {
 589         // Prologue saves the framePointerRegister and returnAddressRegister
 590         return 2 * sizeof(void*);
 591     }
 592 
 593     void emitFunctionPrologue()
 594     {
 595         pushPair(framePointerRegister, returnAddressRegister);
 596         move(stackPointerRegister, framePointerRegister);
 597     }
 598 
 599     void emitFunctionEpilogueWithEmptyFrame()
 600     {
 601         popPair(framePointerRegister, returnAddressRegister);
 602     }
 603 
 604     void emitFunctionEpilogue()
 605     {
 606         move(framePointerRegister, stackPointerRegister);
 607         emitFunctionEpilogueWithEmptyFrame();
 608     }
 609 
 610     ALWAYS_INLINE void preserveReturnAddressAfterCall(RegisterID reg)
 611     {
 612         move(returnAddressRegister, reg);
 613     }
 614 
 615     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(RegisterID reg)
 616     {
 617         move(reg, returnAddressRegister);
 618     }
 619 
 620     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(Address address)
 621     {
 622         loadPtr(address, returnAddressRegister);
 623     }
 624 #endif
 625 
<a name="12" id="anc12"></a><span class="line-modified"> 626     void emitGetFromCallFrameHeaderPtr(VirtualRegister entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 627     {
<a name="13" id="anc13"></a><span class="line-modified"> 628         loadPtr(Address(from, entry.offset() * sizeof(Register)), to);</span>
 629     }
<a name="14" id="anc14"></a><span class="line-modified"> 630     void emitGetFromCallFrameHeader32(VirtualRegister entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 631     {
<a name="15" id="anc15"></a><span class="line-modified"> 632         load32(Address(from, entry.offset() * sizeof(Register)), to);</span>
 633     }
 634 #if USE(JSVALUE64)
<a name="16" id="anc16"></a><span class="line-modified"> 635     void emitGetFromCallFrameHeader64(VirtualRegister entry, GPRReg to, GPRReg from = GPRInfo::callFrameRegister)</span>
 636     {
<a name="17" id="anc17"></a><span class="line-modified"> 637         load64(Address(from, entry.offset() * sizeof(Register)), to);</span>
 638     }
 639 #endif // USE(JSVALUE64)
<a name="18" id="anc18"></a><span class="line-modified"> 640     void emitPutToCallFrameHeader(GPRReg from, VirtualRegister entry)</span>
 641     {
<a name="19" id="anc19"></a><span class="line-modified"> 642         storePtr(from, Address(GPRInfo::callFrameRegister, entry.offset() * sizeof(Register)));</span>
 643     }
 644 
<a name="20" id="anc20"></a><span class="line-modified"> 645     void emitPutToCallFrameHeader(void* value, VirtualRegister entry)</span>
 646     {
<a name="21" id="anc21"></a><span class="line-modified"> 647         storePtr(TrustedImmPtr(value), Address(GPRInfo::callFrameRegister, entry.offset() * sizeof(Register)));</span>
 648     }
 649 
 650     void emitGetCallerFrameFromCallFrameHeaderPtr(RegisterID to)
 651     {
 652         loadPtr(Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), to);
 653     }
 654     void emitPutCallerFrameToCallFrameHeader(RegisterID from)
 655     {
 656         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()));
 657     }
 658 
 659     void emitPutReturnPCToCallFrameHeader(RegisterID from)
 660     {
 661         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 662     }
 663     void emitPutReturnPCToCallFrameHeader(TrustedImmPtr from)
 664     {
 665         storePtr(from, Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 666     }
 667 
 668     // emitPutToCallFrameHeaderBeforePrologue() and related are used to access callee frame header
 669     // fields before the code from emitFunctionPrologue() has executed.
 670     // First, the access is via the stack pointer. Second, the address calculation must also take
 671     // into account that the stack pointer may not have been adjusted down for the return PC and/or
 672     // caller&#39;s frame pointer. On some platforms, the callee is responsible for pushing the
 673     // &quot;link register&quot; containing the return address in the function prologue.
 674 #if USE(JSVALUE64)
<a name="22" id="anc22"></a><span class="line-modified"> 675     void emitPutToCallFrameHeaderBeforePrologue(GPRReg from, VirtualRegister entry)</span>
 676     {
<a name="23" id="anc23"></a><span class="line-modified"> 677         storePtr(from, Address(stackPointerRegister, entry.offset() * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta()));</span>
 678     }
 679 #else
<a name="24" id="anc24"></a><span class="line-modified"> 680     void emitPutPayloadToCallFrameHeaderBeforePrologue(GPRReg from, VirtualRegister entry)</span>
 681     {
<a name="25" id="anc25"></a><span class="line-modified"> 682         storePtr(from, Address(stackPointerRegister, entry.offset() * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta() + OBJECT_OFFSETOF(EncodedValueDescriptor, asBits.payload)));</span>
 683     }
 684 
<a name="26" id="anc26"></a><span class="line-modified"> 685     void emitPutTagToCallFrameHeaderBeforePrologue(TrustedImm32 tag, VirtualRegister entry)</span>
 686     {
<a name="27" id="anc27"></a><span class="line-modified"> 687         storePtr(tag, Address(stackPointerRegister, entry.offset() * static_cast&lt;ptrdiff_t&gt;(sizeof(Register)) - prologueStackPointerDelta() + OBJECT_OFFSETOF(EncodedValueDescriptor, asBits.tag)));</span>
 688     }
 689 #endif
 690 
 691     JumpList branchIfNotEqual(JSValueRegs regs, JSValue value)
 692     {
 693 #if USE(JSVALUE64)
 694         return branch64(NotEqual, regs.gpr(), TrustedImm64(JSValue::encode(value)));
 695 #else
 696         JumpList result;
 697         result.append(branch32(NotEqual, regs.tagGPR(), TrustedImm32(value.tag())));
 698         if (value.isEmpty() || value.isUndefinedOrNull())
 699             return result; // These don&#39;t have anything interesting in the payload.
 700         result.append(branch32(NotEqual, regs.payloadGPR(), TrustedImm32(value.payload())));
 701         return result;
 702 #endif
 703     }
 704 
 705     Jump branchIfEqual(JSValueRegs regs, JSValue value)
 706     {
 707 #if USE(JSVALUE64)
 708         return branch64(Equal, regs.gpr(), TrustedImm64(JSValue::encode(value)));
 709 #else
 710         Jump notEqual;
 711         // These don&#39;t have anything interesting in the payload.
 712         if (!value.isEmpty() &amp;&amp; !value.isUndefinedOrNull())
 713             notEqual = branch32(NotEqual, regs.payloadGPR(), TrustedImm32(value.payload()));
 714         Jump result = branch32(Equal, regs.tagGPR(), TrustedImm32(value.tag()));
 715         if (notEqual.isSet())
 716             notEqual.link(this);
 717         return result;
 718 #endif
 719     }
 720 
 721     Jump branchIfNotCell(GPRReg reg, TagRegistersMode mode = HaveTagRegisters)
 722     {
 723 #if USE(JSVALUE64)
 724         if (mode == HaveTagRegisters)
<a name="28" id="anc28"></a><span class="line-modified"> 725             return branchTest64(NonZero, reg, GPRInfo::notCellMaskRegister);</span>
<span class="line-modified"> 726         return branchTest64(NonZero, reg, TrustedImm64(JSValue::NotCellMask));</span>
 727 #else
 728         UNUSED_PARAM(mode);
 729         return branch32(MacroAssembler::NotEqual, reg, TrustedImm32(JSValue::CellTag));
 730 #endif
 731     }
 732 
 733     Jump branchIfNotCell(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 734     {
 735 #if USE(JSVALUE64)
 736         return branchIfNotCell(regs.gpr(), mode);
 737 #else
 738         return branchIfNotCell(regs.tagGPR(), mode);
 739 #endif
 740     }
 741 
 742     Jump branchIfCell(GPRReg reg, TagRegistersMode mode = HaveTagRegisters)
 743     {
 744 #if USE(JSVALUE64)
 745         if (mode == HaveTagRegisters)
<a name="29" id="anc29"></a><span class="line-modified"> 746             return branchTest64(Zero, reg, GPRInfo::notCellMaskRegister);</span>
<span class="line-modified"> 747         return branchTest64(Zero, reg, TrustedImm64(JSValue::NotCellMask));</span>
 748 #else
 749         UNUSED_PARAM(mode);
 750         return branch32(MacroAssembler::Equal, reg, TrustedImm32(JSValue::CellTag));
 751 #endif
 752     }
 753     Jump branchIfCell(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 754     {
 755 #if USE(JSVALUE64)
 756         return branchIfCell(regs.gpr(), mode);
 757 #else
 758         return branchIfCell(regs.tagGPR(), mode);
 759 #endif
 760     }
 761 
 762     Jump branchIfOther(JSValueRegs regs, GPRReg tempGPR)
 763     {
 764 #if USE(JSVALUE64)
 765         move(regs.gpr(), tempGPR);
<a name="30" id="anc30"></a><span class="line-modified"> 766         and64(TrustedImm32(~JSValue::UndefinedTag), tempGPR);</span>
<span class="line-modified"> 767         return branch64(Equal, tempGPR, TrustedImm64(JSValue::ValueNull));</span>
 768 #else
 769         or32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 770         return branch32(Equal, tempGPR, TrustedImm32(JSValue::NullTag));
 771 #endif
 772     }
 773 
 774     Jump branchIfNotOther(JSValueRegs regs, GPRReg tempGPR)
 775     {
 776 #if USE(JSVALUE64)
 777         move(regs.gpr(), tempGPR);
<a name="31" id="anc31"></a><span class="line-modified"> 778         and64(TrustedImm32(~JSValue::UndefinedTag), tempGPR);</span>
<span class="line-modified"> 779         return branch64(NotEqual, tempGPR, TrustedImm64(JSValue::ValueNull));</span>
 780 #else
 781         or32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 782         return branch32(NotEqual, tempGPR, TrustedImm32(JSValue::NullTag));
 783 #endif
 784     }
 785 
 786     Jump branchIfInt32(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 787     {
 788 #if USE(JSVALUE64)
 789         if (mode == HaveTagRegisters)
<a name="32" id="anc32"></a><span class="line-modified"> 790             return branch64(AboveOrEqual, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 791         return branch64(AboveOrEqual, gpr, TrustedImm64(JSValue::NumberTag));</span>
 792 #else
 793         UNUSED_PARAM(mode);
 794         return branch32(Equal, gpr, TrustedImm32(JSValue::Int32Tag));
 795 #endif
 796     }
 797 
 798     Jump branchIfInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 799     {
 800 #if USE(JSVALUE64)
 801         return branchIfInt32(regs.gpr(), mode);
 802 #else
 803         return branchIfInt32(regs.tagGPR(), mode);
 804 #endif
 805     }
 806 
 807     Jump branchIfNotInt32(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 808     {
 809 #if USE(JSVALUE64)
 810         if (mode == HaveTagRegisters)
<a name="33" id="anc33"></a><span class="line-modified"> 811             return branch64(Below, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 812         return branch64(Below, gpr, TrustedImm64(JSValue::NumberTag));</span>
 813 #else
 814         UNUSED_PARAM(mode);
 815         return branch32(NotEqual, gpr, TrustedImm32(JSValue::Int32Tag));
 816 #endif
 817     }
 818 
 819     Jump branchIfNotInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 820     {
 821 #if USE(JSVALUE64)
 822         return branchIfNotInt32(regs.gpr(), mode);
 823 #else
 824         return branchIfNotInt32(regs.tagGPR(), mode);
 825 #endif
 826     }
 827 
 828     // Note that the tempGPR is not used in 64-bit mode.
 829     Jump branchIfNumber(JSValueRegs regs, GPRReg tempGPR, TagRegistersMode mode = HaveTagRegisters)
 830     {
 831 #if USE(JSVALUE64)
 832         UNUSED_PARAM(tempGPR);
 833         return branchIfNumber(regs.gpr(), mode);
 834 #else
 835         UNUSED_PARAM(mode);
 836         ASSERT(tempGPR != InvalidGPRReg);
 837         add32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 838         return branch32(Below, tempGPR, TrustedImm32(JSValue::LowestTag + 1));
 839 #endif
 840     }
 841 
 842 #if USE(JSVALUE64)
 843     Jump branchIfNumber(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 844     {
 845         if (mode == HaveTagRegisters)
<a name="34" id="anc34"></a><span class="line-modified"> 846             return branchTest64(NonZero, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 847         return branchTest64(NonZero, gpr, TrustedImm64(JSValue::NumberTag));</span>
 848     }
 849 #endif
 850 
 851     // Note that the tempGPR is not used in 64-bit mode.
 852     Jump branchIfNotNumber(JSValueRegs regs, GPRReg tempGPR, TagRegistersMode mode = HaveTagRegisters)
 853     {
 854 #if USE(JSVALUE64)
 855         UNUSED_PARAM(tempGPR);
 856         return branchIfNotNumber(regs.gpr(), mode);
 857 #else
 858         UNUSED_PARAM(mode);
 859         add32(TrustedImm32(1), regs.tagGPR(), tempGPR);
 860         return branch32(AboveOrEqual, tempGPR, TrustedImm32(JSValue::LowestTag + 1));
 861 #endif
 862     }
 863 
 864 #if USE(JSVALUE64)
 865     Jump branchIfNotNumber(GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
 866     {
 867         if (mode == HaveTagRegisters)
<a name="35" id="anc35"></a><span class="line-modified"> 868             return branchTest64(Zero, gpr, GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 869         return branchTest64(Zero, gpr, TrustedImm64(JSValue::NumberTag));</span>
 870     }
 871 #endif
 872 
 873     Jump branchIfNotDoubleKnownNotInt32(JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
 874     {
 875 #if USE(JSVALUE64)
 876         if (mode == HaveTagRegisters)
<a name="36" id="anc36"></a><span class="line-modified"> 877             return branchTest64(Zero, regs.gpr(), GPRInfo::numberTagRegister);</span>
<span class="line-modified"> 878         return branchTest64(Zero, regs.gpr(), TrustedImm64(JSValue::NumberTag));</span>
 879 #else
 880         UNUSED_PARAM(mode);
 881         return branch32(AboveOrEqual, regs.tagGPR(), TrustedImm32(JSValue::LowestTag));
 882 #endif
 883     }
 884 
 885     // Note that the tempGPR is not used in 32-bit mode.
 886     Jump branchIfBoolean(GPRReg gpr, GPRReg tempGPR)
 887     {
 888 #if USE(JSVALUE64)
 889         ASSERT(tempGPR != InvalidGPRReg);
 890         move(gpr, tempGPR);
<a name="37" id="anc37"></a><span class="line-modified"> 891         xor64(TrustedImm32(JSValue::ValueFalse), tempGPR);</span>
 892         return branchTest64(Zero, tempGPR, TrustedImm32(static_cast&lt;int32_t&gt;(~1)));
 893 #else
 894         UNUSED_PARAM(tempGPR);
 895         return branch32(Equal, gpr, TrustedImm32(JSValue::BooleanTag));
 896 #endif
 897     }
 898 
 899     // Note that the tempGPR is not used in 32-bit mode.
 900     Jump branchIfBoolean(JSValueRegs regs, GPRReg tempGPR)
 901     {
 902 #if USE(JSVALUE64)
 903         return branchIfBoolean(regs.gpr(), tempGPR);
 904 #else
 905         return branchIfBoolean(regs.tagGPR(), tempGPR);
 906 #endif
 907     }
 908 
 909     // Note that the tempGPR is not used in 32-bit mode.
 910     Jump branchIfNotBoolean(GPRReg gpr, GPRReg tempGPR)
 911     {
 912 #if USE(JSVALUE64)
 913         ASSERT(tempGPR != InvalidGPRReg);
 914         move(gpr, tempGPR);
<a name="38" id="anc38"></a><span class="line-modified"> 915         xor64(TrustedImm32(JSValue::ValueFalse), tempGPR);</span>
 916         return branchTest64(NonZero, tempGPR, TrustedImm32(static_cast&lt;int32_t&gt;(~1)));
 917 #else
 918         UNUSED_PARAM(tempGPR);
 919         return branch32(NotEqual, gpr, TrustedImm32(JSValue::BooleanTag));
 920 #endif
 921     }
 922 
 923     // Note that the tempGPR is not used in 32-bit mode.
 924     Jump branchIfNotBoolean(JSValueRegs regs, GPRReg tempGPR)
 925     {
 926 #if USE(JSVALUE64)
 927         return branchIfNotBoolean(regs.gpr(), tempGPR);
 928 #else
 929         return branchIfNotBoolean(regs.tagGPR(), tempGPR);
 930 #endif
 931     }
 932 
 933     Jump branchIfObject(GPRReg cellGPR)
 934     {
 935         return branch8(
 936             AboveOrEqual, Address(cellGPR, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType));
 937     }
 938 
 939     Jump branchIfNotObject(GPRReg cellGPR)
 940     {
 941         return branch8(
 942             Below, Address(cellGPR, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType));
 943     }
 944 
 945     Jump branchIfType(GPRReg cellGPR, JSType type)
 946     {
 947         return branch8(Equal, Address(cellGPR, JSCell::typeInfoTypeOffset()), TrustedImm32(type));
 948     }
 949 
 950     Jump branchIfNotType(GPRReg cellGPR, JSType type)
 951     {
 952         return branch8(NotEqual, Address(cellGPR, JSCell::typeInfoTypeOffset()), TrustedImm32(type));
 953     }
 954 
 955     Jump branchIfString(GPRReg cellGPR) { return branchIfType(cellGPR, StringType); }
 956     Jump branchIfNotString(GPRReg cellGPR) { return branchIfNotType(cellGPR, StringType); }
 957     Jump branchIfSymbol(GPRReg cellGPR) { return branchIfType(cellGPR, SymbolType); }
 958     Jump branchIfNotSymbol(GPRReg cellGPR) { return branchIfNotType(cellGPR, SymbolType); }
 959     Jump branchIfBigInt(GPRReg cellGPR) { return branchIfType(cellGPR, BigIntType); }
 960     Jump branchIfNotBigInt(GPRReg cellGPR) { return branchIfNotType(cellGPR, BigIntType); }
 961     Jump branchIfFunction(GPRReg cellGPR) { return branchIfType(cellGPR, JSFunctionType); }
 962     Jump branchIfNotFunction(GPRReg cellGPR) { return branchIfNotType(cellGPR, JSFunctionType); }
 963 
 964     Jump branchIfEmpty(GPRReg gpr)
 965     {
 966 #if USE(JSVALUE64)
 967         return branchTest64(Zero, gpr);
 968 #else
 969         return branch32(Equal, gpr, TrustedImm32(JSValue::EmptyValueTag));
 970 #endif
 971     }
 972 
 973     Jump branchIfEmpty(JSValueRegs regs)
 974     {
 975 #if USE(JSVALUE64)
 976         return branchIfEmpty(regs.gpr());
 977 #else
 978         return branchIfEmpty(regs.tagGPR());
 979 #endif
 980     }
 981 
 982     Jump branchIfNotEmpty(GPRReg gpr)
 983     {
 984 #if USE(JSVALUE64)
 985         return branchTest64(NonZero, gpr);
 986 #else
 987         return branch32(NotEqual, gpr, TrustedImm32(JSValue::EmptyValueTag));
 988 #endif
 989     }
 990 
 991     Jump branchIfNotEmpty(JSValueRegs regs)
 992     {
 993 #if USE(JSVALUE64)
 994         return branchIfNotEmpty(regs.gpr());
 995 #else
 996         return branchIfNotEmpty(regs.tagGPR());
 997 #endif
 998     }
 999 
1000     // Note that this function does not respect MasqueradesAsUndefined.
1001     Jump branchIfUndefined(GPRReg gpr)
1002     {
1003 #if USE(JSVALUE64)
1004         return branch64(Equal, gpr, TrustedImm64(JSValue::encode(jsUndefined())));
1005 #else
1006         return branch32(Equal, gpr, TrustedImm32(JSValue::UndefinedTag));
1007 #endif
1008     }
1009 
1010     // Note that this function does not respect MasqueradesAsUndefined.
1011     Jump branchIfUndefined(JSValueRegs regs)
1012     {
1013 #if USE(JSVALUE64)
1014         return branchIfUndefined(regs.gpr());
1015 #else
1016         return branchIfUndefined(regs.tagGPR());
1017 #endif
1018     }
1019 
1020     // Note that this function does not respect MasqueradesAsUndefined.
1021     Jump branchIfNotUndefined(GPRReg gpr)
1022     {
1023 #if USE(JSVALUE64)
1024         return branch64(NotEqual, gpr, TrustedImm64(JSValue::encode(jsUndefined())));
1025 #else
1026         return branch32(NotEqual, gpr, TrustedImm32(JSValue::UndefinedTag));
1027 #endif
1028     }
1029 
1030     // Note that this function does not respect MasqueradesAsUndefined.
1031     Jump branchIfNotUndefined(JSValueRegs regs)
1032     {
1033 #if USE(JSVALUE64)
1034         return branchIfNotUndefined(regs.gpr());
1035 #else
1036         return branchIfNotUndefined(regs.tagGPR());
1037 #endif
1038     }
1039 
1040     Jump branchIfNull(GPRReg gpr)
1041     {
1042 #if USE(JSVALUE64)
1043         return branch64(Equal, gpr, TrustedImm64(JSValue::encode(jsNull())));
1044 #else
1045         return branch32(Equal, gpr, TrustedImm32(JSValue::NullTag));
1046 #endif
1047     }
1048 
1049     Jump branchIfNull(JSValueRegs regs)
1050     {
1051 #if USE(JSVALUE64)
1052         return branchIfNull(regs.gpr());
1053 #else
1054         return branchIfNull(regs.tagGPR());
1055 #endif
1056     }
1057 
1058     Jump branchIfNotNull(GPRReg gpr)
1059     {
1060 #if USE(JSVALUE64)
1061         return branch64(NotEqual, gpr, TrustedImm64(JSValue::encode(jsNull())));
1062 #else
1063         return branch32(NotEqual, gpr, TrustedImm32(JSValue::NullTag));
1064 #endif
1065     }
1066 
1067     Jump branchIfNotNull(JSValueRegs regs)
1068     {
1069 #if USE(JSVALUE64)
1070         return branchIfNotNull(regs.gpr());
1071 #else
1072         return branchIfNotNull(regs.tagGPR());
1073 #endif
1074     }
1075 
1076     template&lt;typename T&gt;
1077     Jump branchStructure(RelationalCondition condition, T leftHandSide, Structure* structure)
1078     {
1079 #if USE(JSVALUE64)
1080         return branch32(condition, leftHandSide, TrustedImm32(structure-&gt;id()));
1081 #else
1082         return branchPtr(condition, leftHandSide, TrustedImmPtr(structure));
1083 #endif
1084     }
1085 
1086     Jump branchIfFastTypedArray(GPRReg baseGPR);
1087     Jump branchIfNotFastTypedArray(GPRReg baseGPR);
1088 
1089     Jump branchIfNaN(FPRReg fpr)
1090     {
1091         return branchDouble(DoubleNotEqualOrUnordered, fpr, fpr);
1092     }
1093 
1094     Jump branchIfNotNaN(FPRReg fpr)
1095     {
1096         return branchDouble(DoubleEqual, fpr, fpr);
1097     }
1098 
1099     Jump branchIfRopeStringImpl(GPRReg stringImplGPR)
1100     {
1101         return branchTestPtr(NonZero, stringImplGPR, TrustedImm32(JSString::isRopeInPointer));
1102     }
1103 
1104     Jump branchIfNotRopeStringImpl(GPRReg stringImplGPR)
1105     {
1106         return branchTestPtr(Zero, stringImplGPR, TrustedImm32(JSString::isRopeInPointer));
1107     }
1108 
1109     static Address addressForByteOffset(ptrdiff_t byteOffset)
1110     {
1111         return Address(GPRInfo::callFrameRegister, byteOffset);
1112     }
1113     static Address addressFor(VirtualRegister virtualRegister, GPRReg baseReg)
1114     {
1115         ASSERT(virtualRegister.isValid());
1116         return Address(baseReg, virtualRegister.offset() * sizeof(Register));
1117     }
1118     static Address addressFor(VirtualRegister virtualRegister)
1119     {
1120         // NB. It&#39;s tempting on some architectures to sometimes use an offset from the stack
1121         // register because for some offsets that will encode to a smaller instruction. But we
1122         // cannot do this. We use this in places where the stack pointer has been moved to some
1123         // unpredictable location.
1124         ASSERT(virtualRegister.isValid());
1125         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register));
1126     }
<a name="39" id="anc39"></a><span class="line-modified">1127     static Address addressFor(Operand operand)</span>
1128     {
<a name="40" id="anc40"></a><span class="line-modified">1129         ASSERT(!operand.isTmp());</span>
<span class="line-added">1130         return addressFor(operand.virtualRegister());</span>
1131     }
1132 
1133     static Address tagFor(VirtualRegister virtualRegister, GPRReg baseGPR)
1134     {
1135         ASSERT(virtualRegister.isValid());
1136         return Address(baseGPR, virtualRegister.offset() * sizeof(Register) + TagOffset);
1137     }
1138     static Address tagFor(VirtualRegister virtualRegister)
1139     {
1140         ASSERT(virtualRegister.isValid());
1141         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register) + TagOffset);
1142     }
<a name="41" id="anc41"></a><span class="line-modified">1143     static Address tagFor(Operand operand)</span>
1144     {
<a name="42" id="anc42"></a><span class="line-modified">1145         ASSERT(!operand.isTmp());</span>
<span class="line-added">1146         return tagFor(operand.virtualRegister());</span>
1147     }
1148 
1149     static Address payloadFor(VirtualRegister virtualRegister, GPRReg baseGPR)
1150     {
1151         ASSERT(virtualRegister.isValid());
1152         return Address(baseGPR, virtualRegister.offset() * sizeof(Register) + PayloadOffset);
1153     }
1154     static Address payloadFor(VirtualRegister virtualRegister)
1155     {
1156         ASSERT(virtualRegister.isValid());
1157         return Address(GPRInfo::callFrameRegister, virtualRegister.offset() * sizeof(Register) + PayloadOffset);
1158     }
<a name="43" id="anc43"></a><span class="line-modified">1159     static Address payloadFor(Operand operand)</span>
1160     {
<a name="44" id="anc44"></a><span class="line-modified">1161         ASSERT(!operand.isTmp());</span>
<span class="line-added">1162         return payloadFor(operand.virtualRegister());</span>
1163     }
1164 
1165     // Access to our fixed callee CallFrame.
<a name="45" id="anc45"></a><span class="line-modified">1166     static Address calleeFrameSlot(VirtualRegister slot)</span>
1167     {
<a name="46" id="anc46"></a><span class="line-modified">1168         ASSERT(slot.offset() &gt;= CallerFrameAndPC::sizeInRegisters);</span>
<span class="line-modified">1169         return Address(stackPointerRegister, sizeof(Register) * (slot - CallerFrameAndPC::sizeInRegisters).offset());</span>
1170     }
1171 
1172     // Access to our fixed callee CallFrame.
1173     static Address calleeArgumentSlot(int argument)
1174     {
<a name="47" id="anc47"></a><span class="line-modified">1175         return calleeFrameSlot(virtualRegisterForArgumentIncludingThis(argument));</span>
1176     }
1177 
<a name="48" id="anc48"></a><span class="line-modified">1178     static Address calleeFrameTagSlot(VirtualRegister slot)</span>
1179     {
1180         return calleeFrameSlot(slot).withOffset(TagOffset);
1181     }
1182 
<a name="49" id="anc49"></a><span class="line-modified">1183     static Address calleeFramePayloadSlot(VirtualRegister slot)</span>
1184     {
1185         return calleeFrameSlot(slot).withOffset(PayloadOffset);
1186     }
1187 
1188     static Address calleeArgumentTagSlot(int argument)
1189     {
1190         return calleeArgumentSlot(argument).withOffset(TagOffset);
1191     }
1192 
1193     static Address calleeArgumentPayloadSlot(int argument)
1194     {
1195         return calleeArgumentSlot(argument).withOffset(PayloadOffset);
1196     }
1197 
1198     static Address calleeFrameCallerFrame()
1199     {
<a name="50" id="anc50"></a><span class="line-modified">1200         return calleeFrameSlot(VirtualRegister(0)).withOffset(CallFrame::callerFrameOffset());</span>
1201     }
1202 
1203     static GPRReg selectScratchGPR(RegisterSet preserved)
1204     {
1205         GPRReg registers[] = {
1206             GPRInfo::regT0,
1207             GPRInfo::regT1,
1208             GPRInfo::regT2,
1209             GPRInfo::regT3,
1210             GPRInfo::regT4,
1211             GPRInfo::regT5,
1212         };
1213 
1214         for (GPRReg reg : registers) {
1215             if (!preserved.contains(reg))
1216                 return reg;
1217         }
1218         RELEASE_ASSERT_NOT_REACHED();
1219         return InvalidGPRReg;
1220     }
1221 
1222     template&lt;typename... Regs&gt;
1223     static GPRReg selectScratchGPR(Regs... args)
1224     {
1225         RegisterSet set;
1226         constructRegisterSet(set, args...);
1227         return selectScratchGPR(set);
1228     }
1229 
1230     static void constructRegisterSet(RegisterSet&amp;)
1231     {
1232     }
1233 
1234     template&lt;typename... Regs&gt;
1235     static void constructRegisterSet(RegisterSet&amp; set, JSValueRegs regs, Regs... args)
1236     {
1237         if (regs.tagGPR() != InvalidGPRReg)
1238             set.set(regs.tagGPR());
1239         if (regs.payloadGPR() != InvalidGPRReg)
1240             set.set(regs.payloadGPR());
1241         constructRegisterSet(set, args...);
1242     }
1243 
1244     template&lt;typename... Regs&gt;
1245     static void constructRegisterSet(RegisterSet&amp; set, GPRReg reg, Regs... args)
1246     {
1247         if (reg != InvalidGPRReg)
1248             set.set(reg);
1249         constructRegisterSet(set, args...);
1250     }
1251 
1252     // Add a debug call. This call has no effect on JIT code execution state.
1253     void debugCall(VM&amp;, V_DebugOperation_EPP function, void* argument);
1254 
1255     // These methods JIT generate dynamic, debug-only checks - akin to ASSERTs.
<a name="51" id="anc51"></a><span class="line-modified">1256 #if ASSERT_ENABLED</span>
1257     void jitAssertIsInt32(GPRReg);
1258     void jitAssertIsJSInt32(GPRReg);
1259     void jitAssertIsJSNumber(GPRReg);
1260     void jitAssertIsJSDouble(GPRReg);
1261     void jitAssertIsCell(GPRReg);
1262     void jitAssertHasValidCallFrame();
1263     void jitAssertIsNull(GPRReg);
1264     void jitAssertTagsInPlace();
1265     void jitAssertArgumentCountSane();
1266 #else
1267     void jitAssertIsInt32(GPRReg) { }
1268     void jitAssertIsJSInt32(GPRReg) { }
1269     void jitAssertIsJSNumber(GPRReg) { }
1270     void jitAssertIsJSDouble(GPRReg) { }
1271     void jitAssertIsCell(GPRReg) { }
1272     void jitAssertHasValidCallFrame() { }
1273     void jitAssertIsNull(GPRReg) { }
1274     void jitAssertTagsInPlace() { }
1275     void jitAssertArgumentCountSane() { }
1276 #endif
1277 
1278     void jitReleaseAssertNoException(VM&amp;);
1279 
1280     void incrementSuperSamplerCount();
1281     void decrementSuperSamplerCount();
1282 
1283     void purifyNaN(FPRReg);
1284 
1285     // These methods convert between doubles, and doubles boxed and JSValues.
1286 #if USE(JSVALUE64)
1287     GPRReg boxDouble(FPRReg fpr, GPRReg gpr, TagRegistersMode mode = HaveTagRegisters)
1288     {
1289         moveDoubleTo64(fpr, gpr);
1290         if (mode == DoNotHaveTagRegisters)
<a name="52" id="anc52"></a><span class="line-modified">1291             sub64(TrustedImm64(JSValue::NumberTag), gpr);</span>
1292         else {
<a name="53" id="anc53"></a><span class="line-modified">1293             sub64(GPRInfo::numberTagRegister, gpr);</span>
1294             jitAssertIsJSDouble(gpr);
1295         }
1296         return gpr;
1297     }
1298     FPRReg unboxDoubleWithoutAssertions(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
1299     {
<a name="54" id="anc54"></a><span class="line-modified">1300         add64(GPRInfo::numberTagRegister, gpr, resultGPR);</span>
1301         move64ToDouble(resultGPR, fpr);
1302         return fpr;
1303     }
1304     FPRReg unboxDouble(GPRReg gpr, GPRReg resultGPR, FPRReg fpr)
1305     {
1306         jitAssertIsJSDouble(gpr);
1307         return unboxDoubleWithoutAssertions(gpr, resultGPR, fpr);
1308     }
1309 
1310     void boxDouble(FPRReg fpr, JSValueRegs regs, TagRegistersMode mode = HaveTagRegisters)
1311     {
1312         boxDouble(fpr, regs.gpr(), mode);
1313     }
1314 
1315     void unboxDoubleNonDestructive(JSValueRegs regs, FPRReg destFPR, GPRReg resultGPR, FPRReg)
1316     {
1317         unboxDouble(regs.payloadGPR(), resultGPR, destFPR);
1318     }
1319 
1320     // Here are possible arrangements of source, target, scratch:
1321     // - source, target, scratch can all be separate registers.
1322     // - source and target can be the same but scratch is separate.
1323     // - target and scratch can be the same but source is separate.
1324     void boxInt52(GPRReg source, GPRReg target, GPRReg scratch, FPRReg fpScratch)
1325     {
1326         // Is it an int32?
1327         signExtend32ToPtr(source, scratch);
1328         Jump isInt32 = branch64(Equal, source, scratch);
1329 
1330         // Nope, it&#39;s not, but regT0 contains the int64 value.
1331         convertInt64ToDouble(source, fpScratch);
1332         boxDouble(fpScratch, target);
1333         Jump done = jump();
1334 
1335         isInt32.link(this);
1336         zeroExtend32ToPtr(source, target);
<a name="55" id="anc55"></a><span class="line-modified">1337         or64(GPRInfo::numberTagRegister, target);</span>
1338 
1339         done.link(this);
1340     }
1341 #endif
1342 
1343 #if USE(JSVALUE32_64)
1344     void boxDouble(FPRReg fpr, GPRReg tagGPR, GPRReg payloadGPR)
1345     {
1346         moveDoubleToInts(fpr, payloadGPR, tagGPR);
1347     }
1348     void unboxDouble(GPRReg tagGPR, GPRReg payloadGPR, FPRReg fpr, FPRReg scratchFPR)
1349     {
1350         moveIntsToDouble(payloadGPR, tagGPR, fpr, scratchFPR);
1351     }
1352 
1353     void boxDouble(FPRReg fpr, JSValueRegs regs)
1354     {
1355         boxDouble(fpr, regs.tagGPR(), regs.payloadGPR());
1356     }
1357     void unboxDouble(JSValueRegs regs, FPRReg fpr, FPRReg scratchFPR)
1358     {
1359         unboxDouble(regs.tagGPR(), regs.payloadGPR(), fpr, scratchFPR);
1360     }
1361 
1362     void unboxDoubleNonDestructive(const JSValueRegs regs, FPRReg destFPR, GPRReg, FPRReg scratchFPR)
1363     {
1364         unboxDouble(regs, destFPR, scratchFPR);
1365     }
1366 #endif
1367 
1368     void boxBooleanPayload(GPRReg boolGPR, GPRReg payloadGPR)
1369     {
1370 #if USE(JSVALUE64)
<a name="56" id="anc56"></a><span class="line-modified">1371         add32(TrustedImm32(JSValue::ValueFalse), boolGPR, payloadGPR);</span>
1372 #else
1373         move(boolGPR, payloadGPR);
1374 #endif
1375     }
1376 
1377     void boxBooleanPayload(bool value, GPRReg payloadGPR)
1378     {
1379 #if USE(JSVALUE64)
<a name="57" id="anc57"></a><span class="line-modified">1380         move(TrustedImm32(JSValue::ValueFalse + value), payloadGPR);</span>
1381 #else
1382         move(TrustedImm32(value), payloadGPR);
1383 #endif
1384     }
1385 
1386     void boxBoolean(GPRReg boolGPR, JSValueRegs boxedRegs)
1387     {
1388         boxBooleanPayload(boolGPR, boxedRegs.payloadGPR());
1389 #if USE(JSVALUE32_64)
1390         move(TrustedImm32(JSValue::BooleanTag), boxedRegs.tagGPR());
1391 #endif
1392     }
1393 
1394     void boxBoolean(bool value, JSValueRegs boxedRegs)
1395     {
1396         boxBooleanPayload(value, boxedRegs.payloadGPR());
1397 #if USE(JSVALUE32_64)
1398         move(TrustedImm32(JSValue::BooleanTag), boxedRegs.tagGPR());
1399 #endif
1400     }
1401 
1402     void boxInt32(GPRReg intGPR, JSValueRegs boxedRegs, TagRegistersMode mode = HaveTagRegisters)
1403     {
1404 #if USE(JSVALUE64)
1405         if (mode == DoNotHaveTagRegisters) {
1406             move(intGPR, boxedRegs.gpr());
<a name="58" id="anc58"></a><span class="line-modified">1407             or64(TrustedImm64(JSValue::NumberTag), boxedRegs.gpr());</span>
1408         } else
<a name="59" id="anc59"></a><span class="line-modified">1409             or64(GPRInfo::numberTagRegister, intGPR, boxedRegs.gpr());</span>
1410 #else
1411         UNUSED_PARAM(mode);
1412         move(intGPR, boxedRegs.payloadGPR());
1413         move(TrustedImm32(JSValue::Int32Tag), boxedRegs.tagGPR());
1414 #endif
1415     }
1416 
1417     void boxCell(GPRReg cellGPR, JSValueRegs boxedRegs)
1418     {
1419 #if USE(JSVALUE64)
1420         move(cellGPR, boxedRegs.gpr());
1421 #else
1422         move(cellGPR, boxedRegs.payloadGPR());
1423         move(TrustedImm32(JSValue::CellTag), boxedRegs.tagGPR());
1424 #endif
1425     }
1426 
1427     void callExceptionFuzz(VM&amp;);
1428 
1429     enum ExceptionCheckKind { NormalExceptionCheck, InvertedExceptionCheck };
1430     enum ExceptionJumpWidth { NormalJumpWidth, FarJumpWidth };
1431     JS_EXPORT_PRIVATE Jump emitExceptionCheck(
1432         VM&amp;, ExceptionCheckKind = NormalExceptionCheck, ExceptionJumpWidth = NormalJumpWidth);
1433     JS_EXPORT_PRIVATE Jump emitNonPatchableExceptionCheck(VM&amp;);
1434     Jump emitJumpIfException(VM&amp;);
1435 
1436 #if ENABLE(SAMPLING_COUNTERS)
1437     static void emitCount(MacroAssembler&amp; jit, AbstractSamplingCounter&amp; counter, int32_t increment = 1)
1438     {
1439         jit.add64(TrustedImm32(increment), AbsoluteAddress(counter.addressOfCounter()));
1440     }
1441     void emitCount(AbstractSamplingCounter&amp; counter, int32_t increment = 1)
1442     {
1443         add64(TrustedImm32(increment), AbsoluteAddress(counter.addressOfCounter()));
1444     }
1445 #endif
1446 
1447 #if ENABLE(SAMPLING_FLAGS)
1448     void setSamplingFlag(int32_t);
1449     void clearSamplingFlag(int32_t flag);
1450 #endif
1451 
1452     JSGlobalObject* globalObjectFor(CodeOrigin codeOrigin)
1453     {
1454         return codeBlock()-&gt;globalObjectFor(codeOrigin);
1455     }
1456 
1457     bool isStrictModeFor(CodeOrigin codeOrigin)
1458     {
1459         auto* inlineCallFrame = codeOrigin.inlineCallFrame();
1460         if (!inlineCallFrame)
1461             return codeBlock()-&gt;isStrictMode();
1462         return inlineCallFrame-&gt;isStrictMode();
1463     }
1464 
1465     ECMAMode ecmaModeFor(CodeOrigin codeOrigin)
1466     {
1467         return isStrictModeFor(codeOrigin) ? StrictMode : NotStrictMode;
1468     }
1469 
1470     ExecutableBase* executableFor(const CodeOrigin&amp; codeOrigin);
1471 
1472     CodeBlock* baselineCodeBlockFor(const CodeOrigin&amp; codeOrigin)
1473     {
1474         return baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock());
1475     }
1476 
1477     CodeBlock* baselineCodeBlockFor(InlineCallFrame* inlineCallFrame)
1478     {
1479         if (!inlineCallFrame)
1480             return baselineCodeBlock();
1481         return baselineCodeBlockForInlineCallFrame(inlineCallFrame);
1482     }
1483 
1484     CodeBlock* baselineCodeBlock()
1485     {
1486         return m_baselineCodeBlock;
1487     }
1488 
1489     static VirtualRegister argumentsStart(InlineCallFrame* inlineCallFrame)
1490     {
1491         if (!inlineCallFrame)
1492             return VirtualRegister(CallFrame::argumentOffset(0));
1493         if (inlineCallFrame-&gt;argumentsWithFixup.size() &lt;= 1)
1494             return virtualRegisterForLocal(0);
1495         ValueRecovery recovery = inlineCallFrame-&gt;argumentsWithFixup[1];
1496         RELEASE_ASSERT(recovery.technique() == DisplacedInJSStack);
1497         return recovery.virtualRegister();
1498     }
1499 
1500     static VirtualRegister argumentsStart(const CodeOrigin&amp; codeOrigin)
1501     {
1502         return argumentsStart(codeOrigin.inlineCallFrame());
1503     }
1504 
1505     static VirtualRegister argumentCount(InlineCallFrame* inlineCallFrame)
1506     {
1507         ASSERT(!inlineCallFrame || inlineCallFrame-&gt;isVarargs());
1508         if (!inlineCallFrame)
<a name="60" id="anc60"></a><span class="line-modified">1509             return CallFrameSlot::argumentCountIncludingThis;</span>
1510         return inlineCallFrame-&gt;argumentCountRegister;
1511     }
1512 
1513     static VirtualRegister argumentCount(const CodeOrigin&amp; codeOrigin)
1514     {
1515         return argumentCount(codeOrigin.inlineCallFrame());
1516     }
1517 
1518     void emitLoadStructure(VM&amp;, RegisterID source, RegisterID dest, RegisterID scratch);
1519 
1520     void emitStoreStructureWithTypeInfo(TrustedImmPtr structure, RegisterID dest, RegisterID)
1521     {
1522         emitStoreStructureWithTypeInfo(*this, structure, dest);
1523     }
1524 
1525     void emitStoreStructureWithTypeInfo(RegisterID structure, RegisterID dest, RegisterID scratch)
1526     {
1527 #if USE(JSVALUE64)
1528         load64(MacroAssembler::Address(structure, Structure::structureIDOffset()), scratch);
1529         store64(scratch, MacroAssembler::Address(dest, JSCell::structureIDOffset()));
1530 #else
1531         // Store all the info flags using a single 32-bit wide load and store.
1532         load32(MacroAssembler::Address(structure, Structure::indexingModeIncludingHistoryOffset()), scratch);
1533         store32(scratch, MacroAssembler::Address(dest, JSCell::indexingTypeAndMiscOffset()));
1534 
1535         // Store the StructureID
1536         storePtr(structure, MacroAssembler::Address(dest, JSCell::structureIDOffset()));
1537 #endif
1538     }
1539 
1540     static void emitStoreStructureWithTypeInfo(AssemblyHelpers&amp; jit, TrustedImmPtr structure, RegisterID dest);
1541 
1542     Jump barrierBranchWithoutFence(GPRReg cell)
1543     {
1544         return branch8(Above, Address(cell, JSCell::cellStateOffset()), TrustedImm32(blackThreshold));
1545     }
1546 
1547     Jump barrierBranchWithoutFence(JSCell* cell)
1548     {
1549         uint8_t* address = reinterpret_cast&lt;uint8_t*&gt;(cell) + JSCell::cellStateOffset();
1550         return branch8(Above, AbsoluteAddress(address), TrustedImm32(blackThreshold));
1551     }
1552 
1553     Jump barrierBranch(VM&amp; vm, GPRReg cell, GPRReg scratchGPR)
1554     {
1555         load8(Address(cell, JSCell::cellStateOffset()), scratchGPR);
1556         return branch32(Above, scratchGPR, AbsoluteAddress(vm.heap.addressOfBarrierThreshold()));
1557     }
1558 
1559     Jump barrierBranch(VM&amp; vm, JSCell* cell, GPRReg scratchGPR)
1560     {
1561         uint8_t* address = reinterpret_cast&lt;uint8_t*&gt;(cell) + JSCell::cellStateOffset();
1562         load8(address, scratchGPR);
1563         return branch32(Above, scratchGPR, AbsoluteAddress(vm.heap.addressOfBarrierThreshold()));
1564     }
1565 
1566     void barrierStoreLoadFence(VM&amp; vm)
1567     {
1568         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1569         memoryFence();
1570         ok.link(this);
1571     }
1572 
1573     void mutatorFence(VM&amp; vm)
1574     {
1575         if (isX86())
1576             return;
1577         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1578         storeFence();
1579         ok.link(this);
1580     }
1581 
1582     void cageWithoutUntagging(Gigacage::Kind kind, GPRReg storage)
1583     {
1584 #if GIGACAGE_ENABLED
1585         if (!Gigacage::isEnabled(kind))
1586             return;
1587 
1588 #if CPU(ARM64E)
1589         RegisterID tempReg = InvalidGPRReg;
1590         if (kind == Gigacage::Primitive) {
1591             tempReg = getCachedMemoryTempRegisterIDAndInvalidate();
1592             move(storage, tempReg);
1593             // Flip the registers since bitFieldInsert only inserts into the low bits.
1594             std::swap(storage, tempReg);
1595         }
1596 #endif
1597         andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1598         addPtr(TrustedImmPtr(Gigacage::basePtr(kind)), storage);
1599 #if CPU(ARM64E)
1600         if (kind == Gigacage::Primitive)
1601             bitFieldInsert64(storage, 0, 64 - numberOfPACBits, tempReg);
1602 #endif
1603 
1604 #else
1605         UNUSED_PARAM(kind);
1606         UNUSED_PARAM(storage);
1607 #endif
1608     }
1609 
1610     // length may be the same register as scratch.
1611     void cageConditionally(Gigacage::Kind kind, GPRReg storage, GPRReg length, GPRReg scratch)
1612     {
1613 #if GIGACAGE_ENABLED
1614         if (Gigacage::isEnabled(kind)) {
<a name="61" id="anc61"></a><span class="line-modified">1615             if (kind != Gigacage::Primitive || Gigacage::isDisablingPrimitiveGigacageForbidden())</span>
1616                 cageWithoutUntagging(kind, storage);
1617             else {
1618 #if CPU(ARM64E)
1619                 if (length == scratch)
1620                     scratch = getCachedMemoryTempRegisterIDAndInvalidate();
1621 #endif
<a name="62" id="anc62"></a><span class="line-modified">1622                 loadPtr(Gigacage::addressOfBasePtr(kind), scratch);</span>
1623                 Jump done = branchTest64(Zero, scratch);
1624 #if CPU(ARM64E)
1625                 GPRReg tempReg = getCachedDataTempRegisterIDAndInvalidate();
1626                 move(storage, tempReg);
1627                 ASSERT(LogicalImmediate::create64(Gigacage::mask(kind)).isValid());
1628                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), tempReg);
1629                 addPtr(scratch, tempReg);
1630                 bitFieldInsert64(tempReg, 0, 64 - numberOfPACBits, storage);
1631 #else
1632                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1633                 addPtr(scratch, storage);
1634 #endif // CPU(ARM64E)
1635                 done.link(this);
1636             }
1637         }
1638 #endif
1639 
1640 #if CPU(ARM64E)
1641         if (kind == Gigacage::Primitive)
1642             untagArrayPtr(length, storage);
1643 #endif
1644         UNUSED_PARAM(kind);
1645         UNUSED_PARAM(storage);
1646         UNUSED_PARAM(length);
1647         UNUSED_PARAM(scratch);
1648     }
1649 
1650     void emitComputeButterflyIndexingMask(GPRReg vectorLengthGPR, GPRReg scratchGPR, GPRReg resultGPR)
1651     {
1652         ASSERT(scratchGPR != resultGPR);
1653         Jump done;
<a name="63" id="anc63"></a>





1654         // If vectorLength == 0 then clz will return 32 on both ARM and x86. On 64-bit systems, we can then do a 64-bit right shift on a 32-bit -1 to get a 0 mask for zero vectorLength. On 32-bit ARM, shift masks with 0xff, which means it will still create a 0 mask.
1655         countLeadingZeros32(vectorLengthGPR, scratchGPR);
1656         move(TrustedImm32(-1), resultGPR);
1657         urshiftPtr(scratchGPR, resultGPR);
1658         if (done.isSet())
1659             done.link(this);
1660     }
1661 
1662     // If for whatever reason the butterfly is going to change vector length this function does NOT
1663     // update the indexing mask.
1664     void nukeStructureAndStoreButterfly(VM&amp; vm, GPRReg butterfly, GPRReg object)
1665     {
1666         if (isX86()) {
1667             or32(TrustedImm32(bitwise_cast&lt;int32_t&gt;(nukedStructureIDBit())), Address(object, JSCell::structureIDOffset()));
1668             storePtr(butterfly, Address(object, JSObject::butterflyOffset()));
1669             return;
1670         }
1671 
1672         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1673         or32(TrustedImm32(bitwise_cast&lt;int32_t&gt;(nukedStructureIDBit())), Address(object, JSCell::structureIDOffset()));
1674         storeFence();
1675         storePtr(butterfly, Address(object, JSObject::butterflyOffset()));
1676         storeFence();
1677         Jump done = jump();
1678         ok.link(this);
1679         storePtr(butterfly, Address(object, JSObject::butterflyOffset()));
1680         done.link(this);
1681     }
1682 
1683     Jump jumpIfMutatorFenceNotNeeded(VM&amp; vm)
1684     {
1685         return branchTest8(Zero, AbsoluteAddress(vm.heap.addressOfMutatorShouldBeFenced()));
1686     }
1687 
1688     void sanitizeStackInline(VM&amp;, GPRReg scratch);
1689 
1690     // Emits the branch structure for typeof. The code emitted by this doesn&#39;t fall through. The
1691     // functor is called at those points where we have pinpointed a type. One way to use this is to
1692     // have the functor emit the code to put the type string into an appropriate register and then
1693     // jump out. A secondary functor is used for the call trap and masquerades-as-undefined slow
1694     // case. It is passed the unlinked jump to the slow case.
1695     template&lt;typename Functor, typename SlowPathFunctor&gt;
1696     void emitTypeOf(
1697         JSValueRegs regs, GPRReg tempGPR, const Functor&amp; functor,
1698         const SlowPathFunctor&amp; slowPathFunctor)
1699     {
1700         // Implements the following branching structure:
1701         //
1702         // if (is cell) {
1703         //     if (is object) {
1704         //         if (is function) {
1705         //             return function;
1706         //         } else if (doesn&#39;t have call trap and doesn&#39;t masquerade as undefined) {
1707         //             return object
1708         //         } else {
1709         //             return slowPath();
1710         //         }
1711         //     } else if (is string) {
1712         //         return string
1713         //     } else if (is bigint) {
1714         //         return bigint
1715         //     } else {
1716         //         return symbol
1717         //     }
1718         // } else if (is number) {
1719         //     return number
1720         // } else if (is null) {
1721         //     return object
1722         // } else if (is boolean) {
1723         //     return boolean
1724         // } else {
1725         //     return undefined
1726         // }
1727         //
1728         // FIXME: typeof Symbol should be more frequently seen than BigInt.
1729         // We should change the order of type detection based on this frequency.
1730         // https://bugs.webkit.org/show_bug.cgi?id=192650
1731 
1732         Jump notCell = branchIfNotCell(regs);
1733 
1734         GPRReg cellGPR = regs.payloadGPR();
1735         Jump notObject = branchIfNotObject(cellGPR);
1736 
1737         Jump notFunction = branchIfNotFunction(cellGPR);
1738         functor(TypeofType::Function, false);
1739 
1740         notFunction.link(this);
1741         slowPathFunctor(
1742             branchTest8(
1743                 NonZero,
1744                 Address(cellGPR, JSCell::typeInfoFlagsOffset()),
1745                 TrustedImm32(MasqueradesAsUndefined | OverridesGetCallData)));
1746         functor(TypeofType::Object, false);
1747 
1748         notObject.link(this);
1749 
1750         Jump notString = branchIfNotString(cellGPR);
1751         functor(TypeofType::String, false);
1752 
1753         notString.link(this);
1754 
1755         Jump notBigInt = branchIfNotBigInt(cellGPR);
1756         functor(TypeofType::BigInt, false);
1757 
1758         notBigInt.link(this);
1759         functor(TypeofType::Symbol, false);
1760 
1761         notCell.link(this);
1762 
1763         Jump notNumber = branchIfNotNumber(regs, tempGPR);
1764         functor(TypeofType::Number, false);
1765         notNumber.link(this);
1766 
1767         JumpList notNull = branchIfNotEqual(regs, jsNull());
1768         functor(TypeofType::Object, false);
1769         notNull.link(this);
1770 
1771         Jump notBoolean = branchIfNotBoolean(regs, tempGPR);
1772         functor(TypeofType::Boolean, false);
1773         notBoolean.link(this);
1774 
1775         functor(TypeofType::Undefined, true);
1776     }
1777 
<a name="64" id="anc64"></a><span class="line-modified">1778     void emitDumbVirtualCall(VM&amp;, JSGlobalObject*, CallLinkInfo*);</span>
1779 
1780     void makeSpaceOnStackForCCall();
1781     void reclaimSpaceOnStackForCCall();
1782 
1783 #if USE(JSVALUE64)
1784     void emitRandomThunk(JSGlobalObject*, GPRReg scratch0, GPRReg scratch1, GPRReg scratch2, FPRReg result);
1785     void emitRandomThunk(VM&amp;, GPRReg scratch0, GPRReg scratch1, GPRReg scratch2, GPRReg scratch3, FPRReg result);
1786 #endif
1787 
1788     // Call this if you know that the value held in allocatorGPR is non-null. This DOES NOT mean
1789     // that allocator is non-null; allocator can be null as a signal that we don&#39;t know what the
1790     // value of allocatorGPR is. Additionally, if the allocator is not null, then there is no need
1791     // to populate allocatorGPR - this code will ignore the contents of allocatorGPR.
1792     void emitAllocateWithNonNullAllocator(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, GPRReg scratchGPR, JumpList&amp; slowPath);
1793 
1794     void emitAllocate(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, GPRReg scratchGPR, JumpList&amp; slowPath);
1795 
1796     template&lt;typename StructureType&gt;
1797     void emitAllocateJSCell(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure, GPRReg scratchGPR, JumpList&amp; slowPath)
1798     {
1799         emitAllocate(resultGPR, allocator, allocatorGPR, scratchGPR, slowPath);
1800         emitStoreStructureWithTypeInfo(structure, resultGPR, scratchGPR);
1801     }
1802 
1803     template&lt;typename StructureType, typename StorageType&gt;
1804     void emitAllocateJSObject(GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure, StorageType storage, GPRReg scratchGPR, JumpList&amp; slowPath)
1805     {
1806         emitAllocateJSCell(resultGPR, allocator, allocatorGPR, structure, scratchGPR, slowPath);
1807         storePtr(storage, Address(resultGPR, JSObject::butterflyOffset()));
1808     }
1809 
1810     template&lt;typename ClassType, typename StructureType, typename StorageType&gt;
1811     void emitAllocateJSObjectWithKnownSize(
1812         VM&amp; vm, GPRReg resultGPR, StructureType structure, StorageType storage, GPRReg scratchGPR1,
1813         GPRReg scratchGPR2, JumpList&amp; slowPath, size_t size)
1814     {
1815         Allocator allocator = allocatorForNonVirtualConcurrently&lt;ClassType&gt;(vm, size, AllocatorForMode::AllocatorIfExists);
1816         emitAllocateJSObject(resultGPR, JITAllocator::constant(allocator), scratchGPR1, structure, storage, scratchGPR2, slowPath);
1817     }
1818 
1819     template&lt;typename ClassType, typename StructureType, typename StorageType&gt;
1820     void emitAllocateJSObject(VM&amp; vm, GPRReg resultGPR, StructureType structure, StorageType storage, GPRReg scratchGPR1, GPRReg scratchGPR2, JumpList&amp; slowPath)
1821     {
1822         emitAllocateJSObjectWithKnownSize&lt;ClassType&gt;(vm, resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath, ClassType::allocationSize(0));
1823     }
1824 
1825     // allocationSize can be aliased with any of the other input GPRs. If it&#39;s not aliased then it
1826     // won&#39;t be clobbered.
1827     void emitAllocateVariableSized(GPRReg resultGPR, CompleteSubspace&amp; subspace, GPRReg allocationSize, GPRReg scratchGPR1, GPRReg scratchGPR2, JumpList&amp; slowPath);
1828 
1829     template&lt;typename ClassType, typename StructureType&gt;
1830     void emitAllocateVariableSizedCell(VM&amp; vm, GPRReg resultGPR, StructureType structure, GPRReg allocationSize, GPRReg scratchGPR1, GPRReg scratchGPR2, JumpList&amp; slowPath)
1831     {
1832         CompleteSubspace* subspace = subspaceForConcurrently&lt;ClassType&gt;(vm);
1833         RELEASE_ASSERT_WITH_MESSAGE(subspace, &quot;CompleteSubspace is always allocated&quot;);
1834         emitAllocateVariableSized(resultGPR, *subspace, allocationSize, scratchGPR1, scratchGPR2, slowPath);
1835         emitStoreStructureWithTypeInfo(structure, resultGPR, scratchGPR2);
1836     }
1837 
1838     template&lt;typename ClassType, typename StructureType&gt;
1839     void emitAllocateVariableSizedJSObject(VM&amp; vm, GPRReg resultGPR, StructureType structure, GPRReg allocationSize, GPRReg scratchGPR1, GPRReg scratchGPR2, JumpList&amp; slowPath)
1840     {
1841         emitAllocateVariableSizedCell&lt;ClassType&gt;(vm, resultGPR, structure, allocationSize, scratchGPR1, scratchGPR2, slowPath);
1842         storePtr(TrustedImmPtr(nullptr), Address(resultGPR, JSObject::butterflyOffset()));
1843     }
1844 
1845     JumpList branchIfValue(VM&amp;, JSValueRegs, GPRReg scratch, GPRReg scratchIfShouldCheckMasqueradesAsUndefined, FPRReg, FPRReg, bool shouldCheckMasqueradesAsUndefined, JSGlobalObject*, bool negateResult);
1846     JumpList branchIfTruthy(VM&amp; vm, JSValueRegs value, GPRReg scratch, GPRReg scratchIfShouldCheckMasqueradesAsUndefined, FPRReg scratchFPR0, FPRReg scratchFPR1, bool shouldCheckMasqueradesAsUndefined, JSGlobalObject* globalObject)
1847     {
1848         return branchIfValue(vm, value, scratch, scratchIfShouldCheckMasqueradesAsUndefined, scratchFPR0, scratchFPR1, shouldCheckMasqueradesAsUndefined, globalObject, false);
1849     }
1850     JumpList branchIfFalsey(VM&amp; vm, JSValueRegs value, GPRReg scratch, GPRReg scratchIfShouldCheckMasqueradesAsUndefined, FPRReg scratchFPR0, FPRReg scratchFPR1, bool shouldCheckMasqueradesAsUndefined, JSGlobalObject* globalObject)
1851     {
1852         return branchIfValue(vm, value, scratch, scratchIfShouldCheckMasqueradesAsUndefined, scratchFPR0, scratchFPR1, shouldCheckMasqueradesAsUndefined, globalObject, true);
1853     }
1854     void emitConvertValueToBoolean(VM&amp;, JSValueRegs, GPRReg result, GPRReg scratchIfShouldCheckMasqueradesAsUndefined, FPRReg, FPRReg, bool shouldCheckMasqueradesAsUndefined, JSGlobalObject*, bool negateResult = false);
1855 
1856     template&lt;typename ClassType&gt;
1857     void emitAllocateDestructibleObject(VM&amp; vm, GPRReg resultGPR, Structure* structure, GPRReg scratchGPR1, GPRReg scratchGPR2, JumpList&amp; slowPath)
1858     {
1859         auto butterfly = TrustedImmPtr(nullptr);
1860         emitAllocateJSObject&lt;ClassType&gt;(vm, resultGPR, TrustedImmPtr(structure), butterfly, scratchGPR1, scratchGPR2, slowPath);
1861         storePtr(TrustedImmPtr(structure-&gt;classInfo()), Address(resultGPR, JSDestructibleObject::classInfoOffset()));
1862     }
1863 
1864     void emitInitializeInlineStorage(GPRReg baseGPR, unsigned inlineCapacity)
1865     {
1866         for (unsigned i = 0; i &lt; inlineCapacity; ++i)
1867             storeTrustedValue(JSValue(), Address(baseGPR, JSObject::offsetOfInlineStorage() + i * sizeof(EncodedJSValue)));
1868     }
1869 
1870     void emitInitializeInlineStorage(GPRReg baseGPR, GPRReg inlineCapacity)
1871     {
1872         Jump empty = branchTest32(Zero, inlineCapacity);
1873         Label loop = label();
1874         sub32(TrustedImm32(1), inlineCapacity);
1875         storeTrustedValue(JSValue(), BaseIndex(baseGPR, inlineCapacity, TimesEight, JSObject::offsetOfInlineStorage()));
1876         branchTest32(NonZero, inlineCapacity).linkTo(loop, this);
1877         empty.link(this);
1878     }
1879 
1880     void emitInitializeOutOfLineStorage(GPRReg butterflyGPR, unsigned outOfLineCapacity)
1881     {
1882         for (unsigned i = 0; i &lt; outOfLineCapacity; ++i)
1883             storeTrustedValue(JSValue(), Address(butterflyGPR, -sizeof(IndexingHeader) - (i + 1) * sizeof(EncodedJSValue)));
1884     }
1885 
1886 #if USE(JSVALUE64)
1887     void wangsInt64Hash(GPRReg inputAndResult, GPRReg scratch);
1888 #endif
1889 
<a name="65" id="anc65"></a>




1890 #if ENABLE(WEBASSEMBLY)
1891     void loadWasmContextInstance(GPRReg dst);
1892     void storeWasmContextInstance(GPRReg src);
1893     static bool loadWasmContextInstanceNeedsMacroScratchRegister();
1894     static bool storeWasmContextInstanceNeedsMacroScratchRegister();
1895 #endif
1896 
1897 protected:
1898     void copyCalleeSavesToEntryFrameCalleeSavesBufferImpl(GPRReg calleeSavesBuffer);
1899 
1900     CodeBlock* m_codeBlock;
1901     CodeBlock* m_baselineCodeBlock;
1902 };
1903 
1904 } // namespace JSC
1905 
1906 #endif // ENABLE(JIT)
<a name="66" id="anc66"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="66" type="hidden" />
</body>
</html>