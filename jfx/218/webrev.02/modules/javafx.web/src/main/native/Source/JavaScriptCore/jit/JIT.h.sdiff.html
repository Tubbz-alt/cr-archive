<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="JIT.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITAddGenerator.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 51 
 52     enum OpcodeID : unsigned;
 53 
 54     class ArrayAllocationProfile;
 55     class CallLinkInfo;
 56     class CodeBlock;
 57     class FunctionExecutable;
 58     class JIT;
 59     class Identifier;
 60     class Interpreter;
 61     class BlockDirectory;
 62     class Register;
 63     class StructureChain;
 64     class StructureStubInfo;
 65 
 66     struct Instruction;
 67     struct OperandTypes;
 68     struct SimpleJumpTable;
 69     struct StringJumpTable;
 70 




 71     struct CallRecord {
 72         MacroAssembler::Call from;
<span class="line-modified"> 73         unsigned bytecodeOffset;</span>
 74         FunctionPtr&lt;OperationPtrTag&gt; callee;
 75 
 76         CallRecord()
 77         {
 78         }
 79 
<span class="line-modified"> 80         CallRecord(MacroAssembler::Call from, unsigned bytecodeOffset, FunctionPtr&lt;OperationPtrTag&gt; callee)</span>
 81             : from(from)
<span class="line-modified"> 82             , bytecodeOffset(bytecodeOffset)</span>
 83             , callee(callee)
 84         {
 85         }
 86     };
 87 
 88     struct JumpTable {
 89         MacroAssembler::Jump from;
 90         unsigned toBytecodeOffset;
 91 
 92         JumpTable(MacroAssembler::Jump f, unsigned t)
 93             : from(f)
 94             , toBytecodeOffset(t)
 95         {
 96         }
 97     };
 98 
 99     struct SlowCaseEntry {
100         MacroAssembler::Jump from;
<span class="line-modified">101         unsigned to;</span>
102 
<span class="line-modified">103         SlowCaseEntry(MacroAssembler::Jump f, unsigned t)</span>
104             : from(f)
105             , to(t)
106         {
107         }
108     };
109 
110     struct SwitchRecord {
111         enum Type {
112             Immediate,
113             Character,
114             String
115         };
116 
117         Type type;
118 
119         union {
120             SimpleJumpTable* simpleJumpTable;
121             StringJumpTable* stringJumpTable;
122         } jumpTable;
123 
<span class="line-modified">124         unsigned bytecodeOffset;</span>
125         unsigned defaultOffset;
126 
<span class="line-modified">127         SwitchRecord(SimpleJumpTable* jumpTable, unsigned bytecodeOffset, unsigned defaultOffset, Type type)</span>
128             : type(type)
<span class="line-modified">129             , bytecodeOffset(bytecodeOffset)</span>
130             , defaultOffset(defaultOffset)
131         {
132             this-&gt;jumpTable.simpleJumpTable = jumpTable;
133         }
134 
<span class="line-modified">135         SwitchRecord(StringJumpTable* jumpTable, unsigned bytecodeOffset, unsigned defaultOffset)</span>
136             : type(String)
<span class="line-modified">137             , bytecodeOffset(bytecodeOffset)</span>
138             , defaultOffset(defaultOffset)
139         {
140             this-&gt;jumpTable.stringJumpTable = jumpTable;
141         }
142     };
143 
144     struct ByValCompilationInfo {
145         ByValCompilationInfo() { }
146 
<span class="line-modified">147         ByValCompilationInfo(ByValInfo* byValInfo, unsigned bytecodeIndex, MacroAssembler::PatchableJump notIndexJump, MacroAssembler::PatchableJump badTypeJump, JITArrayMode arrayMode, ArrayProfile* arrayProfile, MacroAssembler::Label doneTarget, MacroAssembler::Label nextHotPathTarget)</span>
148             : byValInfo(byValInfo)
149             , bytecodeIndex(bytecodeIndex)
150             , notIndexJump(notIndexJump)
151             , badTypeJump(badTypeJump)
152             , arrayMode(arrayMode)
153             , arrayProfile(arrayProfile)
154             , doneTarget(doneTarget)
155             , nextHotPathTarget(nextHotPathTarget)
156         {
157         }
158 
159         ByValInfo* byValInfo;
<span class="line-modified">160         unsigned bytecodeIndex;</span>
161         MacroAssembler::PatchableJump notIndexJump;
162         MacroAssembler::PatchableJump badTypeJump;
163         JITArrayMode arrayMode;
164         ArrayProfile* arrayProfile;
165         MacroAssembler::Label doneTarget;
166         MacroAssembler::Label nextHotPathTarget;
167         MacroAssembler::Label slowPathTarget;
168         MacroAssembler::Call returnAddress;
169     };
170 
171     struct CallCompilationInfo {
172         MacroAssembler::DataLabelPtr hotPathBegin;
173         MacroAssembler::Call hotPathOther;
174         MacroAssembler::Call callReturnLocation;
175         CallLinkInfo* callLinkInfo;
176     };
177 
178     void ctiPatchCallByReturnAddress(ReturnAddressPtr, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction);
179 
180     class JIT_CLASS_ALIGNMENT JIT : private JSInterfaceJIT {
181         friend class JITSlowPathCall;
182         friend class JITStubCall;
183 
184         using MacroAssembler::Jump;
185         using MacroAssembler::JumpList;
186         using MacroAssembler::Label;
187 
<span class="line-modified">188         static const uintptr_t patchGetByIdDefaultStructure = unusedPointer;</span>
<span class="line-modified">189         static const int patchGetByIdDefaultOffset = 0;</span>
190         // Magic number - initial offset cannot be representable as a signed 8bit value, or the X86Assembler
191         // will compress the displacement, and we may not be able to fit a patched offset.
<span class="line-modified">192         static const int patchPutByIdDefaultOffset = 256;</span>
193 
194     public:
<span class="line-modified">195         JIT(VM&amp;, CodeBlock* = 0, unsigned loopOSREntryBytecodeOffset = 0);</span>
196         ~JIT();
197 
198         VM&amp; vm() { return *JSInterfaceJIT::vm(); }
199 
200         void compileWithoutLinking(JITCompilationEffort);
201         CompilationResult link();
202 
203         void doMainThreadPreparationBeforeCompile();
204 
<span class="line-modified">205         static CompilationResult compile(VM&amp; vm, CodeBlock* codeBlock, JITCompilationEffort effort, unsigned bytecodeOffset = 0)</span>
206         {
207             return JIT(vm, codeBlock, bytecodeOffset).privateCompile(effort);
208         }
209 
<span class="line-removed">210         static void compileGetByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)</span>
<span class="line-removed">211         {</span>
<span class="line-removed">212             JIT jit(vm, codeBlock);</span>
<span class="line-removed">213             jit.m_bytecodeOffset = byValInfo-&gt;bytecodeIndex;</span>
<span class="line-removed">214             jit.privateCompileGetByVal(locker, byValInfo, returnAddress, arrayMode);</span>
<span class="line-removed">215         }</span>
<span class="line-removed">216 </span>
<span class="line-removed">217         static void compileGetByValWithCachedId(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, const Identifier&amp; propertyName)</span>
<span class="line-removed">218         {</span>
<span class="line-removed">219             JIT jit(vm, codeBlock);</span>
<span class="line-removed">220             jit.m_bytecodeOffset = byValInfo-&gt;bytecodeIndex;</span>
<span class="line-removed">221             jit.privateCompileGetByValWithCachedId(byValInfo, returnAddress, propertyName);</span>
<span class="line-removed">222         }</span>
<span class="line-removed">223 </span>
224         static void compilePutByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
225         {
226             JIT jit(vm, codeBlock);
<span class="line-modified">227             jit.m_bytecodeOffset = byValInfo-&gt;bytecodeIndex;</span>
228             jit.privateCompilePutByVal&lt;OpPutByVal&gt;(locker, byValInfo, returnAddress, arrayMode);
229         }
230 
231         static void compileDirectPutByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
232         {
233             JIT jit(vm, codeBlock);
<span class="line-modified">234             jit.m_bytecodeOffset = byValInfo-&gt;bytecodeIndex;</span>
235             jit.privateCompilePutByVal&lt;OpPutByValDirect&gt;(locker, byValInfo, returnAddress, arrayMode);
236         }
237 
238         template&lt;typename Op&gt;
239         static void compilePutByValWithCachedId(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, PutKind putKind, const Identifier&amp; propertyName)
240         {
241             JIT jit(vm, codeBlock);
<span class="line-modified">242             jit.m_bytecodeOffset = byValInfo-&gt;bytecodeIndex;</span>
243             jit.privateCompilePutByValWithCachedId&lt;Op&gt;(byValInfo, returnAddress, putKind, propertyName);
244         }
245 
246         static void compileHasIndexedProperty(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
247         {
248             JIT jit(vm, codeBlock);
<span class="line-modified">249             jit.m_bytecodeOffset = byValInfo-&gt;bytecodeIndex;</span>
250             jit.privateCompileHasIndexedProperty(byValInfo, returnAddress, arrayMode);
251         }
252 
253         static unsigned frameRegisterCountFor(CodeBlock*);
254         static int stackPointerOffsetFor(CodeBlock*);
255 
256         JS_EXPORT_PRIVATE static HashMap&lt;CString, Seconds&gt; compileTimeStats();
257         JS_EXPORT_PRIVATE static Seconds totalCompileTime();
258 
259     private:
260         void privateCompileMainPass();
261         void privateCompileLinkPass();
262         void privateCompileSlowCases();
263         CompilationResult privateCompile(JITCompilationEffort);
264 
265         void privateCompileGetByVal(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
266         void privateCompileGetByValWithCachedId(ByValInfo*, ReturnAddressPtr, const Identifier&amp;);
267         template&lt;typename Op&gt;
268         void privateCompilePutByVal(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
269         template&lt;typename Op&gt;
270         void privateCompilePutByValWithCachedId(ByValInfo*, ReturnAddressPtr, PutKind, const Identifier&amp;);
271 
272         void privateCompileHasIndexedProperty(ByValInfo*, ReturnAddressPtr, JITArrayMode);
273 
274         void privateCompilePatchGetArrayLength(ReturnAddressPtr returnAddress);
275 
276         // Add a call out from JIT code, without an exception check.
277         Call appendCall(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
278         {
279             Call functionCall = call(OperationPtrTag);
<span class="line-modified">280             m_calls.append(CallRecord(functionCall, m_bytecodeOffset, function.retagged&lt;OperationPtrTag&gt;()));</span>
281             return functionCall;
282         }
283 
284 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
285         Call appendCallWithSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
286         {
287             Call functionCall = callWithSlowPathReturnType(OperationPtrTag);
<span class="line-modified">288             m_calls.append(CallRecord(functionCall, m_bytecodeOffset, function.retagged&lt;OperationPtrTag&gt;()));</span>
289             return functionCall;
290         }
291 #endif
292 
293         void exceptionCheck(Jump jumpToHandler)
294         {
295             m_exceptionChecks.append(jumpToHandler);
296         }
297 
298         void exceptionCheck()
299         {
300             m_exceptionChecks.append(emitExceptionCheck(vm()));
301         }
302 
303         void exceptionCheckWithCallFrameRollback()
304         {
305             m_exceptionChecksWithCallFrameRollback.append(emitExceptionCheck(vm()));
306         }
307 
308         void privateCompileExceptionHandlers();
</pre>
<hr />
<pre>
328         std::enable_if_t&lt;
329             Op::opcodeID == op_call_varargs || Op::opcodeID == op_construct_varargs
330             || Op::opcodeID == op_tail_call_varargs || Op::opcodeID == op_tail_call_forward_arguments
331         , void&gt; compileSetupFrame(const Op&amp;, CallLinkInfo*);
332 
333         template&lt;typename Op&gt;
334         bool compileTailCall(const Op&amp;, CallLinkInfo*, unsigned callLinkInfoIndex);
335         template&lt;typename Op&gt;
336         bool compileCallEval(const Op&amp;);
337         void compileCallEvalSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
338         template&lt;typename Op&gt;
339         void emitPutCallResult(const Op&amp;);
340 
341         enum class CompileOpStrictEqType { StrictEq, NStrictEq };
342         template&lt;typename Op&gt;
343         void compileOpStrictEq(const Instruction*, CompileOpStrictEqType);
344         template&lt;typename Op&gt;
345         void compileOpStrictEqJump(const Instruction*, CompileOpStrictEqType);
346         enum class CompileOpEqType { Eq, NEq };
347         void compileOpEqJumpSlow(Vector&lt;SlowCaseEntry&gt;::iterator&amp;, CompileOpEqType, int jumpTarget);
<span class="line-modified">348         bool isOperandConstantDouble(int src);</span>
<span class="line-removed">349 </span>
<span class="line-removed">350         void emitLoadDouble(int index, FPRegisterID value);</span>
<span class="line-removed">351         void emitLoadInt32ToDouble(int index, FPRegisterID value);</span>
352 
353         enum WriteBarrierMode { UnconditionalWriteBarrier, ShouldFilterBase, ShouldFilterValue, ShouldFilterBaseAndValue };
354         // value register in write barrier is used before any scratch registers
355         // so may safely be the same as either of the scratch registers.
<span class="line-modified">356         void emitWriteBarrier(unsigned owner, unsigned value, WriteBarrierMode);</span>
<span class="line-modified">357         void emitWriteBarrier(JSCell* owner, unsigned value, WriteBarrierMode);</span>
358         void emitWriteBarrier(JSCell* owner);
359 
360         // This assumes that the value to profile is in regT0 and that regT3 is available for
361         // scratch.
362         void emitValueProfilingSite(ValueProfile&amp;);
363         template&lt;typename Metadata&gt; void emitValueProfilingSite(Metadata&amp;);
364         void emitValueProfilingSiteIfProfiledOpcode(...);
365         template&lt;typename Op&gt;
366         std::enable_if_t&lt;std::is_same&lt;decltype(Op::Metadata::m_profile), ValueProfile&gt;::value, void&gt;
367         emitValueProfilingSiteIfProfiledOpcode(Op bytecode);
368 
369         void emitArrayProfilingSiteWithCell(RegisterID cell, RegisterID indexingType, ArrayProfile*);
370         void emitArrayProfileStoreToHoleSpecialCase(ArrayProfile*);
371         void emitArrayProfileOutOfBoundsSpecialCase(ArrayProfile*);
372 
373         JITArrayMode chooseArrayMode(ArrayProfile*);
374 
375         // Property is in regT1, base is in regT0. regT2 contains indexing type.
376         // Property is int-checked and zero extended. Base is cell checked.
377         // Structure is already profiled. Returns the slow cases. Fall-through
378         // case contains result in regT0, and it is not yet profiled.
379         JumpList emitInt32Load(const Instruction* instruction, PatchableJump&amp; badType) { return emitContiguousLoad(instruction, badType, Int32Shape); }
380         JumpList emitDoubleLoad(const Instruction*, PatchableJump&amp; badType);
381         JumpList emitContiguousLoad(const Instruction*, PatchableJump&amp; badType, IndexingType expectedShape = ContiguousShape);
382         JumpList emitArrayStorageLoad(const Instruction*, PatchableJump&amp; badType);
383         JumpList emitLoadForArrayMode(const Instruction*, JITArrayMode, PatchableJump&amp; badType);
384 
<span class="line-removed">385         JumpList emitInt32GetByVal(const Instruction* instruction, PatchableJump&amp; badType) { return emitContiguousGetByVal(instruction, badType, Int32Shape); }</span>
<span class="line-removed">386         JumpList emitDoubleGetByVal(const Instruction*, PatchableJump&amp; badType);</span>
<span class="line-removed">387         JumpList emitContiguousGetByVal(const Instruction*, PatchableJump&amp; badType, IndexingType expectedShape = ContiguousShape);</span>
<span class="line-removed">388         JumpList emitArrayStorageGetByVal(const Instruction*, PatchableJump&amp; badType);</span>
<span class="line-removed">389         JumpList emitDirectArgumentsGetByVal(const Instruction*, PatchableJump&amp; badType);</span>
<span class="line-removed">390         JumpList emitScopedArgumentsGetByVal(const Instruction*, PatchableJump&amp; badType);</span>
<span class="line-removed">391         JumpList emitIntTypedArrayGetByVal(const Instruction*, PatchableJump&amp; badType, TypedArrayType);</span>
<span class="line-removed">392         JumpList emitFloatTypedArrayGetByVal(const Instruction*, PatchableJump&amp; badType, TypedArrayType);</span>
<span class="line-removed">393 </span>
394         // Property is in regT1, base is in regT0. regT2 contains indecing type.
395         // The value to store is not yet loaded. Property is int-checked and
396         // zero-extended. Base is cell checked. Structure is already profiled.
397         // returns the slow cases.
398         template&lt;typename Op&gt;
399         JumpList emitInt32PutByVal(Op bytecode, PatchableJump&amp; badType)
400         {
401             return emitGenericContiguousPutByVal(bytecode, badType, Int32Shape);
402         }
403         template&lt;typename Op&gt;
404         JumpList emitDoublePutByVal(Op bytecode, PatchableJump&amp; badType)
405         {
406             return emitGenericContiguousPutByVal(bytecode, badType, DoubleShape);
407         }
408         template&lt;typename Op&gt;
409         JumpList emitContiguousPutByVal(Op bytecode, PatchableJump&amp; badType)
410         {
411             return emitGenericContiguousPutByVal(bytecode, badType);
412         }
413         template&lt;typename Op&gt;
414         JumpList emitGenericContiguousPutByVal(Op, PatchableJump&amp; badType, IndexingType indexingShape = ContiguousShape);
415         template&lt;typename Op&gt;
416         JumpList emitArrayStoragePutByVal(Op, PatchableJump&amp; badType);
417         template&lt;typename Op&gt;
418         JumpList emitIntTypedArrayPutByVal(Op, PatchableJump&amp; badType, TypedArrayType);
419         template&lt;typename Op&gt;
420         JumpList emitFloatTypedArrayPutByVal(Op, PatchableJump&amp; badType, TypedArrayType);
421 
422         // Identifier check helper for GetByVal and PutByVal.
423         void emitByValIdentifierCheck(ByValInfo*, RegisterID cell, RegisterID scratch, const Identifier&amp;, JumpList&amp; slowCases);
424 
<span class="line-removed">425         JITGetByIdGenerator emitGetByValWithCachedId(ByValInfo*, OpGetByVal, const Identifier&amp;, Jump&amp; fastDoneCase, Jump&amp; slowDoneCase, JumpList&amp; slowCases);</span>
426         template&lt;typename Op&gt;
427         JITPutByIdGenerator emitPutByValWithCachedId(ByValInfo*, Op, PutKind, const Identifier&amp;, JumpList&amp; doneCases, JumpList&amp; slowCases);
428 
429         enum FinalObjectMode { MayBeFinal, KnownNotFinal };
430 
<span class="line-modified">431         void emitGetVirtualRegister(int src, JSValueRegs dst);</span>
<span class="line-modified">432         void emitPutVirtualRegister(int dst, JSValueRegs src);</span>
433 
<span class="line-modified">434         int32_t getOperandConstantInt(int src);</span>
<span class="line-modified">435         double getOperandConstantDouble(int src);</span>
436 
437 #if USE(JSVALUE32_64)
<span class="line-modified">438         bool getOperandConstantInt(int op1, int op2, int&amp; op, int32_t&amp; constant);</span>
439 
<span class="line-modified">440         void emitLoadTag(int index, RegisterID tag);</span>
<span class="line-modified">441         void emitLoadPayload(int index, RegisterID payload);</span>

442 
443         void emitLoad(const JSValue&amp; v, RegisterID tag, RegisterID payload);
<span class="line-modified">444         void emitLoad(int index, RegisterID tag, RegisterID payload, RegisterID base = callFrameRegister);</span>
<span class="line-modified">445         void emitLoad2(int index1, RegisterID tag1, RegisterID payload1, int index2, RegisterID tag2, RegisterID payload2);</span>
446 
<span class="line-modified">447         void emitStore(int index, RegisterID tag, RegisterID payload, RegisterID base = callFrameRegister);</span>
<span class="line-modified">448         void emitStore(int index, const JSValue constant, RegisterID base = callFrameRegister);</span>
<span class="line-modified">449         void emitStoreInt32(int index, RegisterID payload, bool indexIsInt32 = false);</span>
<span class="line-modified">450         void emitStoreInt32(int index, TrustedImm32 payload, bool indexIsInt32 = false);</span>
<span class="line-modified">451         void emitStoreCell(int index, RegisterID payload, bool indexIsCell = false);</span>
<span class="line-modified">452         void emitStoreBool(int index, RegisterID payload, bool indexIsBool = false);</span>
<span class="line-modified">453         void emitStoreDouble(int index, FPRegisterID value);</span>
454 
<span class="line-modified">455         void emitJumpSlowCaseIfNotJSCell(int virtualRegisterIndex);</span>
<span class="line-modified">456         void emitJumpSlowCaseIfNotJSCell(int virtualRegisterIndex, RegisterID tag);</span>
457 
458         void compileGetByIdHotPath(const Identifier*);
459 
460         // Arithmetic opcode helpers
461         template &lt;typename Op&gt;
462         void emitBinaryDoubleOp(const Instruction *, OperandTypes, JumpList&amp; notInt32Op1, JumpList&amp; notInt32Op2, bool op1IsInRegisters = true, bool op2IsInRegisters = true);
463 
464 #else // USE(JSVALUE32_64)
<span class="line-removed">465         void emitGetVirtualRegister(int src, RegisterID dst);</span>
466         void emitGetVirtualRegister(VirtualRegister src, RegisterID dst);
<span class="line-removed">467         void emitGetVirtualRegisters(int src1, RegisterID dst1, int src2, RegisterID dst2);</span>
468         void emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2);
<span class="line-removed">469         void emitPutVirtualRegister(int dst, RegisterID from = regT0);</span>
470         void emitPutVirtualRegister(VirtualRegister dst, RegisterID from = regT0);
<span class="line-modified">471         void emitStoreCell(int dst, RegisterID payload, bool /* only used in JSValue32_64 */ = false)</span>
<span class="line-removed">472         {</span>
<span class="line-removed">473             emitPutVirtualRegister(dst, payload);</span>
<span class="line-removed">474         }</span>
<span class="line-removed">475         void emitStoreCell(VirtualRegister dst, RegisterID payload)</span>
476         {
477             emitPutVirtualRegister(dst, payload);
478         }
479 
480         Jump emitJumpIfBothJSCells(RegisterID, RegisterID, RegisterID);
481         void emitJumpSlowCaseIfJSCell(RegisterID);
482         void emitJumpSlowCaseIfNotJSCell(RegisterID);
<span class="line-modified">483         void emitJumpSlowCaseIfNotJSCell(RegisterID, int VReg);</span>
484         Jump emitJumpIfNotInt(RegisterID, RegisterID, RegisterID scratch);
485         PatchableJump emitPatchableJumpIfNotInt(RegisterID);
486         void emitJumpSlowCaseIfNotInt(RegisterID);
487         void emitJumpSlowCaseIfNotNumber(RegisterID);
488         void emitJumpSlowCaseIfNotInt(RegisterID, RegisterID, RegisterID scratch);
489 
<span class="line-modified">490         void compileGetByIdHotPath(int baseVReg, const Identifier*);</span>
491 
492 #endif // USE(JSVALUE32_64)
493 
494         template&lt;typename Op&gt;
495         void emit_compareAndJump(const Instruction*, RelationalCondition);

496         template&lt;typename Op&gt;
497         void emit_compareUnsigned(const Instruction*, RelationalCondition);

498         template&lt;typename Op&gt;
499         void emit_compareUnsignedAndJump(const Instruction*, RelationalCondition);

500         template&lt;typename Op&gt;
<span class="line-modified">501         void emit_compareAndJumpSlow(const Instruction*, DoubleCondition, size_t (JIT_OPERATION *operation)(ExecState*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);</span>

502 
503         void assertStackPointerOffset();
504 
505         void emit_op_add(const Instruction*);
506         void emit_op_bitand(const Instruction*);
507         void emit_op_bitor(const Instruction*);
508         void emit_op_bitxor(const Instruction*);
509         void emit_op_bitnot(const Instruction*);
510         void emit_op_call(const Instruction*);
511         void emit_op_tail_call(const Instruction*);
512         void emit_op_call_eval(const Instruction*);
513         void emit_op_call_varargs(const Instruction*);
514         void emit_op_tail_call_varargs(const Instruction*);
515         void emit_op_tail_call_forward_arguments(const Instruction*);
516         void emit_op_construct_varargs(const Instruction*);
517         void emit_op_catch(const Instruction*);
518         void emit_op_construct(const Instruction*);
519         void emit_op_create_this(const Instruction*);
520         void emit_op_to_this(const Instruction*);
521         void emit_op_get_argument(const Instruction*);
</pre>
<hr />
<pre>
558         void emit_op_jneq_null(const Instruction*);
559         void emit_op_jundefined_or_null(const Instruction*);
560         void emit_op_jnundefined_or_null(const Instruction*);
561         void emit_op_jneq_ptr(const Instruction*);
562         void emit_op_jless(const Instruction*);
563         void emit_op_jlesseq(const Instruction*);
564         void emit_op_jgreater(const Instruction*);
565         void emit_op_jgreatereq(const Instruction*);
566         void emit_op_jnless(const Instruction*);
567         void emit_op_jnlesseq(const Instruction*);
568         void emit_op_jngreater(const Instruction*);
569         void emit_op_jngreatereq(const Instruction*);
570         void emit_op_jeq(const Instruction*);
571         void emit_op_jneq(const Instruction*);
572         void emit_op_jstricteq(const Instruction*);
573         void emit_op_jnstricteq(const Instruction*);
574         void emit_op_jbelow(const Instruction*);
575         void emit_op_jbeloweq(const Instruction*);
576         void emit_op_jtrue(const Instruction*);
577         void emit_op_loop_hint(const Instruction*);

578         void emit_op_nop(const Instruction*);
579         void emit_op_super_sampler_begin(const Instruction*);
580         void emit_op_super_sampler_end(const Instruction*);
581         void emit_op_lshift(const Instruction*);
582         void emit_op_mod(const Instruction*);
583         void emit_op_mov(const Instruction*);
584         void emit_op_mul(const Instruction*);
585         void emit_op_negate(const Instruction*);
586         void emit_op_neq(const Instruction*);
587         void emit_op_neq_null(const Instruction*);
588         void emit_op_new_array(const Instruction*);
589         void emit_op_new_array_with_size(const Instruction*);
590         void emit_op_new_func(const Instruction*);
591         void emit_op_new_func_exp(const Instruction*);
592         void emit_op_new_generator_func(const Instruction*);
593         void emit_op_new_generator_func_exp(const Instruction*);
594         void emit_op_new_async_func(const Instruction*);
595         void emit_op_new_async_func_exp(const Instruction*);
596         void emit_op_new_async_generator_func(const Instruction*);
597         void emit_op_new_async_generator_func_exp(const Instruction*);
</pre>
<hr />
<pre>
607         void emit_op_put_by_id(const Instruction*);
608         template&lt;typename Op = OpPutByVal&gt;
609         void emit_op_put_by_val(const Instruction*);
610         void emit_op_put_by_val_direct(const Instruction*);
611         void emit_op_put_getter_by_id(const Instruction*);
612         void emit_op_put_setter_by_id(const Instruction*);
613         void emit_op_put_getter_setter_by_id(const Instruction*);
614         void emit_op_put_getter_by_val(const Instruction*);
615         void emit_op_put_setter_by_val(const Instruction*);
616         void emit_op_ret(const Instruction*);
617         void emit_op_rshift(const Instruction*);
618         void emit_op_set_function_name(const Instruction*);
619         void emit_op_stricteq(const Instruction*);
620         void emit_op_sub(const Instruction*);
621         void emit_op_switch_char(const Instruction*);
622         void emit_op_switch_imm(const Instruction*);
623         void emit_op_switch_string(const Instruction*);
624         void emit_op_tear_off_arguments(const Instruction*);
625         void emit_op_throw(const Instruction*);
626         void emit_op_to_number(const Instruction*);

627         void emit_op_to_string(const Instruction*);
628         void emit_op_to_object(const Instruction*);
629         void emit_op_to_primitive(const Instruction*);
630         void emit_op_unexpected_load(const Instruction*);
631         void emit_op_unsigned(const Instruction*);
632         void emit_op_urshift(const Instruction*);
633         void emit_op_has_structure_property(const Instruction*);
634         void emit_op_has_indexed_property(const Instruction*);
635         void emit_op_get_direct_pname(const Instruction*);
636         void emit_op_enumerator_structure_pname(const Instruction*);
637         void emit_op_enumerator_generic_pname(const Instruction*);


638         void emit_op_log_shadow_chicken_prologue(const Instruction*);
639         void emit_op_log_shadow_chicken_tail(const Instruction*);

640 
641         void emitSlow_op_add(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
642         void emitSlow_op_call(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
643         void emitSlow_op_tail_call(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
644         void emitSlow_op_call_eval(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
645         void emitSlow_op_call_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
646         void emitSlow_op_tail_call_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
647         void emitSlow_op_tail_call_forward_arguments(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
648         void emitSlow_op_construct_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
649         void emitSlow_op_construct(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
650         void emitSlow_op_eq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
651         void emitSlow_op_get_callee(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
652         void emitSlow_op_try_get_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
653         void emitSlow_op_get_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
654         void emitSlow_op_get_by_id_with_this(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
655         void emitSlow_op_get_by_id_direct(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
656         void emitSlow_op_get_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
657         void emitSlow_op_get_argument_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
658         void emitSlow_op_in_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
659         void emitSlow_op_instanceof(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
660         void emitSlow_op_instanceof_custom(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
661         void emitSlow_op_jless(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
662         void emitSlow_op_jlesseq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
663         void emitSlow_op_jgreater(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
664         void emitSlow_op_jgreatereq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
665         void emitSlow_op_jnless(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
666         void emitSlow_op_jnlesseq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
667         void emitSlow_op_jngreater(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
668         void emitSlow_op_jngreatereq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
669         void emitSlow_op_jeq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
670         void emitSlow_op_jneq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
671         void emitSlow_op_jstricteq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
672         void emitSlow_op_jnstricteq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
673         void emitSlow_op_jtrue(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
674         void emitSlow_op_loop_hint(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
<span class="line-modified">675         void emitSlow_op_enter(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);</span>
676         void emitSlow_op_mod(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
677         void emitSlow_op_mul(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
678         void emitSlow_op_negate(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
679         void emitSlow_op_neq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
680         void emitSlow_op_new_object(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
681         void emitSlow_op_put_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
682         void emitSlow_op_put_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
683         void emitSlow_op_sub(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
684         void emitSlow_op_has_indexed_property(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
685 
686         void emit_op_resolve_scope(const Instruction*);
687         void emit_op_get_from_scope(const Instruction*);
688         void emit_op_put_to_scope(const Instruction*);
689         void emit_op_get_from_arguments(const Instruction*);
690         void emit_op_put_to_arguments(const Instruction*);
691         void emitSlow_op_get_from_scope(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
692         void emitSlow_op_put_to_scope(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
693 
694         void emitSlowCaseCall(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, SlowPathFunction);
695 
696         void emitRightShift(const Instruction*, bool isUnsigned);
697         void emitRightShiftSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, bool isUnsigned);
698 
699         template&lt;typename Op&gt;
700         void emitNewFuncCommon(const Instruction*);
701         template&lt;typename Op&gt;
702         void emitNewFuncExprCommon(const Instruction*);
703         void emitVarInjectionCheck(bool needsVarInjectionChecks);
<span class="line-modified">704         void emitResolveClosure(int dst, int scope, bool needsVarInjectionChecks, unsigned depth);</span>
<span class="line-modified">705         void emitLoadWithStructureCheck(int scope, Structure** structureSlot);</span>
706 #if USE(JSVALUE64)
707         void emitGetVarFromPointer(JSValue* operand, GPRReg);
708         void emitGetVarFromIndirectPointer(JSValue** operand, GPRReg);
709 #else
710         void emitGetVarFromIndirectPointer(JSValue** operand, GPRReg tag, GPRReg payload);
711         void emitGetVarFromPointer(JSValue* operand, GPRReg tag, GPRReg payload);
712 #endif
<span class="line-modified">713         void emitGetClosureVar(int scope, uintptr_t operand);</span>
714         void emitNotifyWrite(WatchpointSet*);
715         void emitNotifyWrite(GPRReg pointerToSet);
<span class="line-modified">716         void emitPutGlobalVariable(JSValue* operand, int value, WatchpointSet*);</span>
<span class="line-modified">717         void emitPutGlobalVariableIndirect(JSValue** addressOfOperand, int value, WatchpointSet**);</span>
<span class="line-modified">718         void emitPutClosureVar(int scope, uintptr_t operand, int value, WatchpointSet*);</span>
719 
<span class="line-modified">720         void emitInitRegister(int dst);</span>
721 
<span class="line-modified">722         void emitPutIntToCallFrameHeader(RegisterID from, int entry);</span>
723 
<span class="line-modified">724         JSValue getConstantOperand(int src);</span>
<span class="line-modified">725         bool isOperandConstantInt(int src);</span>
<span class="line-modified">726         bool isOperandConstantChar(int src);</span>
727 
728         template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
729         void emitMathICFast(JITUnaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledFunction, NonProfiledFunction);
730         template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
731         void emitMathICFast(JITBinaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledFunction, NonProfiledFunction);
732 
733         template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
734         void emitMathICSlow(JITBinaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledRepatchFunction, ProfiledFunction, RepatchFunction);
735         template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
736         void emitMathICSlow(JITUnaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledRepatchFunction, ProfiledFunction, RepatchFunction);
737 
738         Jump getSlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
739         {
740             return iter++-&gt;from;
741         }
742         void linkSlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
743         {
744             if (iter-&gt;from.isSet())
745                 iter-&gt;from.link(this);
746             ++iter;
747         }
748         void linkDummySlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
749         {
750             ASSERT(!iter-&gt;from.isSet());
751             ++iter;
752         }
<span class="line-modified">753         void linkSlowCaseIfNotJSCell(Vector&lt;SlowCaseEntry&gt;::iterator&amp;, int virtualRegisterIndex);</span>
<span class="line-modified">754         void linkAllSlowCasesForBytecodeOffset(Vector&lt;SlowCaseEntry&gt;&amp; slowCases,</span>
<span class="line-modified">755             Vector&lt;SlowCaseEntry&gt;::iterator&amp;, unsigned bytecodeOffset);</span>
756 
757         void linkAllSlowCases(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
758         {
<span class="line-modified">759             linkAllSlowCasesForBytecodeOffset(m_slowCases, iter, m_bytecodeOffset);</span>






760         }
761 
762         MacroAssembler::Call appendCallWithExceptionCheck(const FunctionPtr&lt;CFunctionPtrTag&gt;);
763 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
764         MacroAssembler::Call appendCallWithExceptionCheckAndSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt;);
765 #endif
766         MacroAssembler::Call appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt;);
<span class="line-modified">767         MacroAssembler::Call appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr&lt;CFunctionPtrTag&gt;, int);</span>
768         template&lt;typename Metadata&gt;
<span class="line-modified">769         MacroAssembler::Call appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata&amp;, const FunctionPtr&lt;CFunctionPtrTag&gt;, int);</span>
770 
771         template&lt;typename OperationType, typename... Args&gt;
772         std::enable_if_t&lt;FunctionTraits&lt;OperationType&gt;::hasResult, MacroAssembler::Call&gt;
<span class="line-modified">773         callOperation(OperationType operation, int result, Args... args)</span>
774         {
775             setupArguments&lt;OperationType&gt;(args...);
776             return appendCallWithExceptionCheckSetJSValueResult(operation, result);
777         }
778 
779 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
780         template&lt;typename OperationType, typename... Args&gt;
781         std::enable_if_t&lt;std::is_same&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType, SlowPathReturnType&gt;::value, MacroAssembler::Call&gt;
782         callOperation(OperationType operation, Args... args)
783         {
784             setupArguments&lt;OperationType&gt;(args...);
785             return appendCallWithExceptionCheckAndSlowPathReturnType(operation);
786         }
787 
788         template&lt;typename Type&gt;
789         struct is64BitType {
790             static constexpr bool value = sizeof(Type) &lt;= 8;
791         };
792 
793         template&lt;&gt;
</pre>
<hr />
<pre>
797 
798         template&lt;typename OperationType, typename... Args&gt;
799         std::enable_if_t&lt;!std::is_same&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType, SlowPathReturnType&gt;::value, MacroAssembler::Call&gt;
800         callOperation(OperationType operation, Args... args)
801         {
802             static_assert(is64BitType&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType&gt;::value, &quot;Win64 cannot use standard call when return type is larger than 64 bits.&quot;);
803             setupArguments&lt;OperationType&gt;(args...);
804             return appendCallWithExceptionCheck(operation);
805         }
806 #else // OS(WINDOWS) &amp;&amp; CPU(X86_64)
807         template&lt;typename OperationType, typename... Args&gt;
808         MacroAssembler::Call callOperation(OperationType operation, Args... args)
809         {
810             setupArguments&lt;OperationType&gt;(args...);
811             return appendCallWithExceptionCheck(operation);
812         }
813 #endif // OS(WINDOWS) &amp;&amp; CPU(X86_64)
814 
815         template&lt;typename Metadata, typename OperationType, typename... Args&gt;
816         std::enable_if_t&lt;FunctionTraits&lt;OperationType&gt;::hasResult, MacroAssembler::Call&gt;
<span class="line-modified">817         callOperationWithProfile(Metadata&amp; metadata, OperationType operation, int result, Args... args)</span>
818         {
819             setupArguments&lt;OperationType&gt;(args...);
820             return appendCallWithExceptionCheckSetJSValueResultWithProfile(metadata, operation, result);
821         }
822 
823         template&lt;typename OperationType, typename... Args&gt;
824         MacroAssembler::Call callOperationWithResult(OperationType operation, JSValueRegs resultRegs, Args... args)
825         {
826             setupArguments&lt;OperationType&gt;(args...);
827             auto result = appendCallWithExceptionCheck(operation);
828             setupResults(resultRegs);
829             return result;
830         }
831 
832         template&lt;typename OperationType, typename... Args&gt;
833         MacroAssembler::Call callOperationNoExceptionCheck(OperationType operation, Args... args)
834         {
835             setupArguments&lt;OperationType&gt;(args...);
836             updateTopCallFrame();
837             return appendCall(operation);
</pre>
<hr />
<pre>
850         };
851 
852         template&lt;typename Op, typename SnippetGenerator&gt;
853         void emitBitBinaryOpFastPath(const Instruction* currentInstruction, ProfilingPolicy shouldEmitProfiling = ProfilingPolicy::NoProfiling);
854 
855         void emitRightShiftFastPath(const Instruction* currentInstruction, OpcodeID);
856 
857         template&lt;typename Op&gt;
858         void emitRightShiftFastPath(const Instruction* currentInstruction, JITRightShiftGenerator::ShiftType);
859 
860         void updateTopCallFrame();
861 
862         Call emitNakedCall(CodePtr&lt;NoPtrTag&gt; function = CodePtr&lt;NoPtrTag&gt;());
863         Call emitNakedTailCall(CodePtr&lt;NoPtrTag&gt; function = CodePtr&lt;NoPtrTag&gt;());
864 
865         // Loads the character value of a single character string into dst.
866         void emitLoadCharacterString(RegisterID src, RegisterID dst, JumpList&amp; failures);
867 
868         int jumpTarget(const Instruction*, int target);
869 






870 #ifndef NDEBUG
<span class="line-modified">871         void printBytecodeOperandTypes(int src1, int src2);</span>
872 #endif
873 
874 #if ENABLE(SAMPLING_FLAGS)
875         void setSamplingFlag(int32_t);
876         void clearSamplingFlag(int32_t);
877 #endif
878 
879 #if ENABLE(SAMPLING_COUNTERS)
880         void emitCount(AbstractSamplingCounter&amp;, int32_t = 1);
881 #endif
882 
883 #if ENABLE(OPCODE_SAMPLING)
884         void sampleInstruction(const Instruction*, bool = false);
885 #endif
886 
887 #if ENABLE(CODEBLOCK_SAMPLING)
888         void sampleCodeBlock(CodeBlock*);
889 #else
890         void sampleCodeBlock(CodeBlock*) {}
891 #endif
</pre>
<hr />
<pre>
893 #if ENABLE(DFG_JIT)
894         bool canBeOptimized() { return m_canBeOptimized; }
895         bool canBeOptimizedOrInlined() { return m_canBeOptimizedOrInlined; }
896         bool shouldEmitProfiling() { return m_shouldEmitProfiling; }
897 #else
898         bool canBeOptimized() { return false; }
899         bool canBeOptimizedOrInlined() { return false; }
900         // Enables use of value profiler with tiered compilation turned off,
901         // in which case all code gets profiled.
902         bool shouldEmitProfiling() { return false; }
903 #endif
904 
905         static bool reportCompileTimes();
906         static bool computeCompileTimes();
907 
908         // If you need to check a value from the metadata table and you need it to
909         // be consistent across the fast and slow path, then you want to use this.
910         // It will give the slow path the same value read by the fast path.
911         GetPutInfo copiedGetPutInfo(OpPutToScope);
912         template&lt;typename BinaryOp&gt;
<span class="line-modified">913         ArithProfile copiedArithProfile(BinaryOp);</span>
914 
915         Interpreter* m_interpreter;
916 
917         Vector&lt;CallRecord&gt; m_calls;
918         Vector&lt;Label&gt; m_labels;
919         Vector&lt;JITGetByIdGenerator&gt; m_getByIds;

920         Vector&lt;JITGetByIdWithThisGenerator&gt; m_getByIdsWithThis;
921         Vector&lt;JITPutByIdGenerator&gt; m_putByIds;
922         Vector&lt;JITInByIdGenerator&gt; m_inByIds;
923         Vector&lt;JITInstanceOfGenerator&gt; m_instanceOfs;
924         Vector&lt;ByValCompilationInfo&gt; m_byValCompilationInfo;
925         Vector&lt;CallCompilationInfo&gt; m_callCompilationInfo;
926         Vector&lt;JumpTable&gt; m_jmpTable;
927 
<span class="line-modified">928         unsigned m_bytecodeOffset;</span>
929         Vector&lt;SlowCaseEntry&gt; m_slowCases;
930         Vector&lt;SwitchRecord&gt; m_switches;
931 
932         HashMap&lt;unsigned, unsigned&gt; m_copiedGetPutInfos;
<span class="line-modified">933         HashMap&lt;uint64_t, ArithProfile&gt; m_copiedArithProfiles;</span>
934 
935         JumpList m_exceptionChecks;
936         JumpList m_exceptionChecksWithCallFrameRollback;
937         Label m_exceptionHandler;
938 
939         unsigned m_getByIdIndex { UINT_MAX };

940         unsigned m_getByIdWithThisIndex { UINT_MAX };
941         unsigned m_putByIdIndex { UINT_MAX };
942         unsigned m_inByIdIndex { UINT_MAX };
943         unsigned m_instanceOfIndex { UINT_MAX };
944         unsigned m_byValInstructionIndex { UINT_MAX };
945         unsigned m_callLinkInfoIndex { UINT_MAX };

946 
947         Label m_arityCheck;
948         std::unique_ptr&lt;LinkBuffer&gt; m_linkBuffer;
949 
950         std::unique_ptr&lt;JITDisassembler&gt; m_disassembler;
951         RefPtr&lt;Profiler::Compilation&gt; m_compilation;
952 
953         PCToCodeOriginMapBuilder m_pcToCodeOriginMapBuilder;
954 
955         HashMap&lt;const Instruction*, void*&gt; m_instructionToMathIC;
956         HashMap&lt;const Instruction*, MathICGenerationState&gt; m_instructionToMathICGenerationState;
957 
958         bool m_canBeOptimized;
959         bool m_canBeOptimizedOrInlined;
960         bool m_shouldEmitProfiling;
<span class="line-modified">961         bool m_shouldUseIndexMasking;</span>
<span class="line-removed">962         unsigned m_loopOSREntryBytecodeOffset { 0 };</span>
963     };
964 
965 } // namespace JSC
966 
967 
968 #endif // ENABLE(JIT)
</pre>
</td>
<td>
<hr />
<pre>
 51 
 52     enum OpcodeID : unsigned;
 53 
 54     class ArrayAllocationProfile;
 55     class CallLinkInfo;
 56     class CodeBlock;
 57     class FunctionExecutable;
 58     class JIT;
 59     class Identifier;
 60     class Interpreter;
 61     class BlockDirectory;
 62     class Register;
 63     class StructureChain;
 64     class StructureStubInfo;
 65 
 66     struct Instruction;
 67     struct OperandTypes;
 68     struct SimpleJumpTable;
 69     struct StringJumpTable;
 70 
<span class="line-added"> 71     struct OpPutByVal;</span>
<span class="line-added"> 72     struct OpPutByValDirect;</span>
<span class="line-added"> 73     struct OpPutToScope;</span>
<span class="line-added"> 74 </span>
 75     struct CallRecord {
 76         MacroAssembler::Call from;
<span class="line-modified"> 77         BytecodeIndex bytecodeIndex;</span>
 78         FunctionPtr&lt;OperationPtrTag&gt; callee;
 79 
 80         CallRecord()
 81         {
 82         }
 83 
<span class="line-modified"> 84         CallRecord(MacroAssembler::Call from, BytecodeIndex bytecodeIndex, FunctionPtr&lt;OperationPtrTag&gt; callee)</span>
 85             : from(from)
<span class="line-modified"> 86             , bytecodeIndex(bytecodeIndex)</span>
 87             , callee(callee)
 88         {
 89         }
 90     };
 91 
 92     struct JumpTable {
 93         MacroAssembler::Jump from;
 94         unsigned toBytecodeOffset;
 95 
 96         JumpTable(MacroAssembler::Jump f, unsigned t)
 97             : from(f)
 98             , toBytecodeOffset(t)
 99         {
100         }
101     };
102 
103     struct SlowCaseEntry {
104         MacroAssembler::Jump from;
<span class="line-modified">105         BytecodeIndex to;</span>
106 
<span class="line-modified">107         SlowCaseEntry(MacroAssembler::Jump f, BytecodeIndex t)</span>
108             : from(f)
109             , to(t)
110         {
111         }
112     };
113 
114     struct SwitchRecord {
115         enum Type {
116             Immediate,
117             Character,
118             String
119         };
120 
121         Type type;
122 
123         union {
124             SimpleJumpTable* simpleJumpTable;
125             StringJumpTable* stringJumpTable;
126         } jumpTable;
127 
<span class="line-modified">128         BytecodeIndex bytecodeIndex;</span>
129         unsigned defaultOffset;
130 
<span class="line-modified">131         SwitchRecord(SimpleJumpTable* jumpTable, BytecodeIndex bytecodeIndex, unsigned defaultOffset, Type type)</span>
132             : type(type)
<span class="line-modified">133             , bytecodeIndex(bytecodeIndex)</span>
134             , defaultOffset(defaultOffset)
135         {
136             this-&gt;jumpTable.simpleJumpTable = jumpTable;
137         }
138 
<span class="line-modified">139         SwitchRecord(StringJumpTable* jumpTable, BytecodeIndex bytecodeIndex, unsigned defaultOffset)</span>
140             : type(String)
<span class="line-modified">141             , bytecodeIndex(bytecodeIndex)</span>
142             , defaultOffset(defaultOffset)
143         {
144             this-&gt;jumpTable.stringJumpTable = jumpTable;
145         }
146     };
147 
148     struct ByValCompilationInfo {
149         ByValCompilationInfo() { }
150 
<span class="line-modified">151         ByValCompilationInfo(ByValInfo* byValInfo, BytecodeIndex bytecodeIndex, MacroAssembler::PatchableJump notIndexJump, MacroAssembler::PatchableJump badTypeJump, JITArrayMode arrayMode, ArrayProfile* arrayProfile, MacroAssembler::Label doneTarget, MacroAssembler::Label nextHotPathTarget)</span>
152             : byValInfo(byValInfo)
153             , bytecodeIndex(bytecodeIndex)
154             , notIndexJump(notIndexJump)
155             , badTypeJump(badTypeJump)
156             , arrayMode(arrayMode)
157             , arrayProfile(arrayProfile)
158             , doneTarget(doneTarget)
159             , nextHotPathTarget(nextHotPathTarget)
160         {
161         }
162 
163         ByValInfo* byValInfo;
<span class="line-modified">164         BytecodeIndex bytecodeIndex;</span>
165         MacroAssembler::PatchableJump notIndexJump;
166         MacroAssembler::PatchableJump badTypeJump;
167         JITArrayMode arrayMode;
168         ArrayProfile* arrayProfile;
169         MacroAssembler::Label doneTarget;
170         MacroAssembler::Label nextHotPathTarget;
171         MacroAssembler::Label slowPathTarget;
172         MacroAssembler::Call returnAddress;
173     };
174 
175     struct CallCompilationInfo {
176         MacroAssembler::DataLabelPtr hotPathBegin;
177         MacroAssembler::Call hotPathOther;
178         MacroAssembler::Call callReturnLocation;
179         CallLinkInfo* callLinkInfo;
180     };
181 
182     void ctiPatchCallByReturnAddress(ReturnAddressPtr, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction);
183 
184     class JIT_CLASS_ALIGNMENT JIT : private JSInterfaceJIT {
185         friend class JITSlowPathCall;
186         friend class JITStubCall;
187 
188         using MacroAssembler::Jump;
189         using MacroAssembler::JumpList;
190         using MacroAssembler::Label;
191 
<span class="line-modified">192         static constexpr uintptr_t patchGetByIdDefaultStructure = unusedPointer;</span>
<span class="line-modified">193         static constexpr int patchGetByIdDefaultOffset = 0;</span>
194         // Magic number - initial offset cannot be representable as a signed 8bit value, or the X86Assembler
195         // will compress the displacement, and we may not be able to fit a patched offset.
<span class="line-modified">196         static constexpr int patchPutByIdDefaultOffset = 256;</span>
197 
198     public:
<span class="line-modified">199         JIT(VM&amp;, CodeBlock* = nullptr, BytecodeIndex loopOSREntryBytecodeOffset = BytecodeIndex(0));</span>
200         ~JIT();
201 
202         VM&amp; vm() { return *JSInterfaceJIT::vm(); }
203 
204         void compileWithoutLinking(JITCompilationEffort);
205         CompilationResult link();
206 
207         void doMainThreadPreparationBeforeCompile();
208 
<span class="line-modified">209         static CompilationResult compile(VM&amp; vm, CodeBlock* codeBlock, JITCompilationEffort effort, BytecodeIndex bytecodeOffset = BytecodeIndex(0))</span>
210         {
211             return JIT(vm, codeBlock, bytecodeOffset).privateCompile(effort);
212         }
213 














214         static void compilePutByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
215         {
216             JIT jit(vm, codeBlock);
<span class="line-modified">217             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;</span>
218             jit.privateCompilePutByVal&lt;OpPutByVal&gt;(locker, byValInfo, returnAddress, arrayMode);
219         }
220 
221         static void compileDirectPutByVal(const ConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
222         {
223             JIT jit(vm, codeBlock);
<span class="line-modified">224             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;</span>
225             jit.privateCompilePutByVal&lt;OpPutByValDirect&gt;(locker, byValInfo, returnAddress, arrayMode);
226         }
227 
228         template&lt;typename Op&gt;
229         static void compilePutByValWithCachedId(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, PutKind putKind, const Identifier&amp; propertyName)
230         {
231             JIT jit(vm, codeBlock);
<span class="line-modified">232             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;</span>
233             jit.privateCompilePutByValWithCachedId&lt;Op&gt;(byValInfo, returnAddress, putKind, propertyName);
234         }
235 
236         static void compileHasIndexedProperty(VM&amp; vm, CodeBlock* codeBlock, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
237         {
238             JIT jit(vm, codeBlock);
<span class="line-modified">239             jit.m_bytecodeIndex = byValInfo-&gt;bytecodeIndex;</span>
240             jit.privateCompileHasIndexedProperty(byValInfo, returnAddress, arrayMode);
241         }
242 
243         static unsigned frameRegisterCountFor(CodeBlock*);
244         static int stackPointerOffsetFor(CodeBlock*);
245 
246         JS_EXPORT_PRIVATE static HashMap&lt;CString, Seconds&gt; compileTimeStats();
247         JS_EXPORT_PRIVATE static Seconds totalCompileTime();
248 
249     private:
250         void privateCompileMainPass();
251         void privateCompileLinkPass();
252         void privateCompileSlowCases();
253         CompilationResult privateCompile(JITCompilationEffort);
254 
255         void privateCompileGetByVal(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
256         void privateCompileGetByValWithCachedId(ByValInfo*, ReturnAddressPtr, const Identifier&amp;);
257         template&lt;typename Op&gt;
258         void privateCompilePutByVal(const ConcurrentJSLocker&amp;, ByValInfo*, ReturnAddressPtr, JITArrayMode);
259         template&lt;typename Op&gt;
260         void privateCompilePutByValWithCachedId(ByValInfo*, ReturnAddressPtr, PutKind, const Identifier&amp;);
261 
262         void privateCompileHasIndexedProperty(ByValInfo*, ReturnAddressPtr, JITArrayMode);
263 
264         void privateCompilePatchGetArrayLength(ReturnAddressPtr returnAddress);
265 
266         // Add a call out from JIT code, without an exception check.
267         Call appendCall(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
268         {
269             Call functionCall = call(OperationPtrTag);
<span class="line-modified">270             m_calls.append(CallRecord(functionCall, m_bytecodeIndex, function.retagged&lt;OperationPtrTag&gt;()));</span>
271             return functionCall;
272         }
273 
274 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
275         Call appendCallWithSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
276         {
277             Call functionCall = callWithSlowPathReturnType(OperationPtrTag);
<span class="line-modified">278             m_calls.append(CallRecord(functionCall, m_bytecodeIndex, function.retagged&lt;OperationPtrTag&gt;()));</span>
279             return functionCall;
280         }
281 #endif
282 
283         void exceptionCheck(Jump jumpToHandler)
284         {
285             m_exceptionChecks.append(jumpToHandler);
286         }
287 
288         void exceptionCheck()
289         {
290             m_exceptionChecks.append(emitExceptionCheck(vm()));
291         }
292 
293         void exceptionCheckWithCallFrameRollback()
294         {
295             m_exceptionChecksWithCallFrameRollback.append(emitExceptionCheck(vm()));
296         }
297 
298         void privateCompileExceptionHandlers();
</pre>
<hr />
<pre>
318         std::enable_if_t&lt;
319             Op::opcodeID == op_call_varargs || Op::opcodeID == op_construct_varargs
320             || Op::opcodeID == op_tail_call_varargs || Op::opcodeID == op_tail_call_forward_arguments
321         , void&gt; compileSetupFrame(const Op&amp;, CallLinkInfo*);
322 
323         template&lt;typename Op&gt;
324         bool compileTailCall(const Op&amp;, CallLinkInfo*, unsigned callLinkInfoIndex);
325         template&lt;typename Op&gt;
326         bool compileCallEval(const Op&amp;);
327         void compileCallEvalSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
328         template&lt;typename Op&gt;
329         void emitPutCallResult(const Op&amp;);
330 
331         enum class CompileOpStrictEqType { StrictEq, NStrictEq };
332         template&lt;typename Op&gt;
333         void compileOpStrictEq(const Instruction*, CompileOpStrictEqType);
334         template&lt;typename Op&gt;
335         void compileOpStrictEqJump(const Instruction*, CompileOpStrictEqType);
336         enum class CompileOpEqType { Eq, NEq };
337         void compileOpEqJumpSlow(Vector&lt;SlowCaseEntry&gt;::iterator&amp;, CompileOpEqType, int jumpTarget);
<span class="line-modified">338         bool isOperandConstantDouble(VirtualRegister);</span>



339 
340         enum WriteBarrierMode { UnconditionalWriteBarrier, ShouldFilterBase, ShouldFilterValue, ShouldFilterBaseAndValue };
341         // value register in write barrier is used before any scratch registers
342         // so may safely be the same as either of the scratch registers.
<span class="line-modified">343         void emitWriteBarrier(VirtualRegister owner, VirtualRegister value, WriteBarrierMode);</span>
<span class="line-modified">344         void emitWriteBarrier(JSCell* owner, VirtualRegister value, WriteBarrierMode);</span>
345         void emitWriteBarrier(JSCell* owner);
346 
347         // This assumes that the value to profile is in regT0 and that regT3 is available for
348         // scratch.
349         void emitValueProfilingSite(ValueProfile&amp;);
350         template&lt;typename Metadata&gt; void emitValueProfilingSite(Metadata&amp;);
351         void emitValueProfilingSiteIfProfiledOpcode(...);
352         template&lt;typename Op&gt;
353         std::enable_if_t&lt;std::is_same&lt;decltype(Op::Metadata::m_profile), ValueProfile&gt;::value, void&gt;
354         emitValueProfilingSiteIfProfiledOpcode(Op bytecode);
355 
356         void emitArrayProfilingSiteWithCell(RegisterID cell, RegisterID indexingType, ArrayProfile*);
357         void emitArrayProfileStoreToHoleSpecialCase(ArrayProfile*);
358         void emitArrayProfileOutOfBoundsSpecialCase(ArrayProfile*);
359 
360         JITArrayMode chooseArrayMode(ArrayProfile*);
361 
362         // Property is in regT1, base is in regT0. regT2 contains indexing type.
363         // Property is int-checked and zero extended. Base is cell checked.
364         // Structure is already profiled. Returns the slow cases. Fall-through
365         // case contains result in regT0, and it is not yet profiled.
366         JumpList emitInt32Load(const Instruction* instruction, PatchableJump&amp; badType) { return emitContiguousLoad(instruction, badType, Int32Shape); }
367         JumpList emitDoubleLoad(const Instruction*, PatchableJump&amp; badType);
368         JumpList emitContiguousLoad(const Instruction*, PatchableJump&amp; badType, IndexingType expectedShape = ContiguousShape);
369         JumpList emitArrayStorageLoad(const Instruction*, PatchableJump&amp; badType);
370         JumpList emitLoadForArrayMode(const Instruction*, JITArrayMode, PatchableJump&amp; badType);
371 









372         // Property is in regT1, base is in regT0. regT2 contains indecing type.
373         // The value to store is not yet loaded. Property is int-checked and
374         // zero-extended. Base is cell checked. Structure is already profiled.
375         // returns the slow cases.
376         template&lt;typename Op&gt;
377         JumpList emitInt32PutByVal(Op bytecode, PatchableJump&amp; badType)
378         {
379             return emitGenericContiguousPutByVal(bytecode, badType, Int32Shape);
380         }
381         template&lt;typename Op&gt;
382         JumpList emitDoublePutByVal(Op bytecode, PatchableJump&amp; badType)
383         {
384             return emitGenericContiguousPutByVal(bytecode, badType, DoubleShape);
385         }
386         template&lt;typename Op&gt;
387         JumpList emitContiguousPutByVal(Op bytecode, PatchableJump&amp; badType)
388         {
389             return emitGenericContiguousPutByVal(bytecode, badType);
390         }
391         template&lt;typename Op&gt;
392         JumpList emitGenericContiguousPutByVal(Op, PatchableJump&amp; badType, IndexingType indexingShape = ContiguousShape);
393         template&lt;typename Op&gt;
394         JumpList emitArrayStoragePutByVal(Op, PatchableJump&amp; badType);
395         template&lt;typename Op&gt;
396         JumpList emitIntTypedArrayPutByVal(Op, PatchableJump&amp; badType, TypedArrayType);
397         template&lt;typename Op&gt;
398         JumpList emitFloatTypedArrayPutByVal(Op, PatchableJump&amp; badType, TypedArrayType);
399 
400         // Identifier check helper for GetByVal and PutByVal.
401         void emitByValIdentifierCheck(ByValInfo*, RegisterID cell, RegisterID scratch, const Identifier&amp;, JumpList&amp; slowCases);
402 

403         template&lt;typename Op&gt;
404         JITPutByIdGenerator emitPutByValWithCachedId(ByValInfo*, Op, PutKind, const Identifier&amp;, JumpList&amp; doneCases, JumpList&amp; slowCases);
405 
406         enum FinalObjectMode { MayBeFinal, KnownNotFinal };
407 
<span class="line-modified">408         void emitGetVirtualRegister(VirtualRegister src, JSValueRegs dst);</span>
<span class="line-modified">409         void emitPutVirtualRegister(VirtualRegister dst, JSValueRegs src);</span>
410 
<span class="line-modified">411         int32_t getOperandConstantInt(VirtualRegister src);</span>
<span class="line-modified">412         double getOperandConstantDouble(VirtualRegister src);</span>
413 
414 #if USE(JSVALUE32_64)
<span class="line-modified">415         bool getOperandConstantInt(VirtualRegister op1, VirtualRegister op2, VirtualRegister&amp; op, int32_t&amp; constant);</span>
416 
<span class="line-modified">417         void emitLoadDouble(VirtualRegister, FPRegisterID value);</span>
<span class="line-modified">418         void emitLoadTag(VirtualRegister, RegisterID tag);</span>
<span class="line-added">419         void emitLoadPayload(VirtualRegister, RegisterID payload);</span>
420 
421         void emitLoad(const JSValue&amp; v, RegisterID tag, RegisterID payload);
<span class="line-modified">422         void emitLoad(VirtualRegister, RegisterID tag, RegisterID payload, RegisterID base = callFrameRegister);</span>
<span class="line-modified">423         void emitLoad2(VirtualRegister, RegisterID tag1, RegisterID payload1, VirtualRegister, RegisterID tag2, RegisterID payload2);</span>
424 
<span class="line-modified">425         void emitStore(VirtualRegister, RegisterID tag, RegisterID payload, RegisterID base = callFrameRegister);</span>
<span class="line-modified">426         void emitStore(VirtualRegister, const JSValue constant, RegisterID base = callFrameRegister);</span>
<span class="line-modified">427         void emitStoreInt32(VirtualRegister, RegisterID payload, bool indexIsInt32 = false);</span>
<span class="line-modified">428         void emitStoreInt32(VirtualRegister, TrustedImm32 payload, bool indexIsInt32 = false);</span>
<span class="line-modified">429         void emitStoreCell(VirtualRegister, RegisterID payload, bool indexIsCell = false);</span>
<span class="line-modified">430         void emitStoreBool(VirtualRegister, RegisterID payload, bool indexIsBool = false);</span>
<span class="line-modified">431         void emitStoreDouble(VirtualRegister, FPRegisterID value);</span>
432 
<span class="line-modified">433         void emitJumpSlowCaseIfNotJSCell(VirtualRegister);</span>
<span class="line-modified">434         void emitJumpSlowCaseIfNotJSCell(VirtualRegister, RegisterID tag);</span>
435 
436         void compileGetByIdHotPath(const Identifier*);
437 
438         // Arithmetic opcode helpers
439         template &lt;typename Op&gt;
440         void emitBinaryDoubleOp(const Instruction *, OperandTypes, JumpList&amp; notInt32Op1, JumpList&amp; notInt32Op2, bool op1IsInRegisters = true, bool op2IsInRegisters = true);
441 
442 #else // USE(JSVALUE32_64)

443         void emitGetVirtualRegister(VirtualRegister src, RegisterID dst);

444         void emitGetVirtualRegisters(VirtualRegister src1, RegisterID dst1, VirtualRegister src2, RegisterID dst2);

445         void emitPutVirtualRegister(VirtualRegister dst, RegisterID from = regT0);
<span class="line-modified">446         void emitStoreCell(VirtualRegister dst, RegisterID payload, bool /* only used in JSValue32_64 */ = false)</span>




447         {
448             emitPutVirtualRegister(dst, payload);
449         }
450 
451         Jump emitJumpIfBothJSCells(RegisterID, RegisterID, RegisterID);
452         void emitJumpSlowCaseIfJSCell(RegisterID);
453         void emitJumpSlowCaseIfNotJSCell(RegisterID);
<span class="line-modified">454         void emitJumpSlowCaseIfNotJSCell(RegisterID, VirtualRegister);</span>
455         Jump emitJumpIfNotInt(RegisterID, RegisterID, RegisterID scratch);
456         PatchableJump emitPatchableJumpIfNotInt(RegisterID);
457         void emitJumpSlowCaseIfNotInt(RegisterID);
458         void emitJumpSlowCaseIfNotNumber(RegisterID);
459         void emitJumpSlowCaseIfNotInt(RegisterID, RegisterID, RegisterID scratch);
460 
<span class="line-modified">461         void compileGetByIdHotPath(VirtualRegister baseReg, const Identifier*);</span>
462 
463 #endif // USE(JSVALUE32_64)
464 
465         template&lt;typename Op&gt;
466         void emit_compareAndJump(const Instruction*, RelationalCondition);
<span class="line-added">467         void emit_compareAndJumpImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, RelationalCondition);</span>
468         template&lt;typename Op&gt;
469         void emit_compareUnsigned(const Instruction*, RelationalCondition);
<span class="line-added">470         void emit_compareUnsignedImpl(VirtualRegister dst, VirtualRegister op1, VirtualRegister op2, RelationalCondition);</span>
471         template&lt;typename Op&gt;
472         void emit_compareUnsignedAndJump(const Instruction*, RelationalCondition);
<span class="line-added">473         void emit_compareUnsignedAndJumpImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, RelationalCondition);</span>
474         template&lt;typename Op&gt;
<span class="line-modified">475         void emit_compareAndJumpSlow(const Instruction*, DoubleCondition, size_t (JIT_OPERATION *operation)(JSGlobalObject*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);</span>
<span class="line-added">476         void emit_compareAndJumpSlowImpl(VirtualRegister op1, VirtualRegister op2, unsigned target, size_t instructionSize, DoubleCondition, size_t (JIT_OPERATION *operation)(JSGlobalObject*, EncodedJSValue, EncodedJSValue), bool invert, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);</span>
477 
478         void assertStackPointerOffset();
479 
480         void emit_op_add(const Instruction*);
481         void emit_op_bitand(const Instruction*);
482         void emit_op_bitor(const Instruction*);
483         void emit_op_bitxor(const Instruction*);
484         void emit_op_bitnot(const Instruction*);
485         void emit_op_call(const Instruction*);
486         void emit_op_tail_call(const Instruction*);
487         void emit_op_call_eval(const Instruction*);
488         void emit_op_call_varargs(const Instruction*);
489         void emit_op_tail_call_varargs(const Instruction*);
490         void emit_op_tail_call_forward_arguments(const Instruction*);
491         void emit_op_construct_varargs(const Instruction*);
492         void emit_op_catch(const Instruction*);
493         void emit_op_construct(const Instruction*);
494         void emit_op_create_this(const Instruction*);
495         void emit_op_to_this(const Instruction*);
496         void emit_op_get_argument(const Instruction*);
</pre>
<hr />
<pre>
533         void emit_op_jneq_null(const Instruction*);
534         void emit_op_jundefined_or_null(const Instruction*);
535         void emit_op_jnundefined_or_null(const Instruction*);
536         void emit_op_jneq_ptr(const Instruction*);
537         void emit_op_jless(const Instruction*);
538         void emit_op_jlesseq(const Instruction*);
539         void emit_op_jgreater(const Instruction*);
540         void emit_op_jgreatereq(const Instruction*);
541         void emit_op_jnless(const Instruction*);
542         void emit_op_jnlesseq(const Instruction*);
543         void emit_op_jngreater(const Instruction*);
544         void emit_op_jngreatereq(const Instruction*);
545         void emit_op_jeq(const Instruction*);
546         void emit_op_jneq(const Instruction*);
547         void emit_op_jstricteq(const Instruction*);
548         void emit_op_jnstricteq(const Instruction*);
549         void emit_op_jbelow(const Instruction*);
550         void emit_op_jbeloweq(const Instruction*);
551         void emit_op_jtrue(const Instruction*);
552         void emit_op_loop_hint(const Instruction*);
<span class="line-added">553         void emit_op_check_traps(const Instruction*);</span>
554         void emit_op_nop(const Instruction*);
555         void emit_op_super_sampler_begin(const Instruction*);
556         void emit_op_super_sampler_end(const Instruction*);
557         void emit_op_lshift(const Instruction*);
558         void emit_op_mod(const Instruction*);
559         void emit_op_mov(const Instruction*);
560         void emit_op_mul(const Instruction*);
561         void emit_op_negate(const Instruction*);
562         void emit_op_neq(const Instruction*);
563         void emit_op_neq_null(const Instruction*);
564         void emit_op_new_array(const Instruction*);
565         void emit_op_new_array_with_size(const Instruction*);
566         void emit_op_new_func(const Instruction*);
567         void emit_op_new_func_exp(const Instruction*);
568         void emit_op_new_generator_func(const Instruction*);
569         void emit_op_new_generator_func_exp(const Instruction*);
570         void emit_op_new_async_func(const Instruction*);
571         void emit_op_new_async_func_exp(const Instruction*);
572         void emit_op_new_async_generator_func(const Instruction*);
573         void emit_op_new_async_generator_func_exp(const Instruction*);
</pre>
<hr />
<pre>
583         void emit_op_put_by_id(const Instruction*);
584         template&lt;typename Op = OpPutByVal&gt;
585         void emit_op_put_by_val(const Instruction*);
586         void emit_op_put_by_val_direct(const Instruction*);
587         void emit_op_put_getter_by_id(const Instruction*);
588         void emit_op_put_setter_by_id(const Instruction*);
589         void emit_op_put_getter_setter_by_id(const Instruction*);
590         void emit_op_put_getter_by_val(const Instruction*);
591         void emit_op_put_setter_by_val(const Instruction*);
592         void emit_op_ret(const Instruction*);
593         void emit_op_rshift(const Instruction*);
594         void emit_op_set_function_name(const Instruction*);
595         void emit_op_stricteq(const Instruction*);
596         void emit_op_sub(const Instruction*);
597         void emit_op_switch_char(const Instruction*);
598         void emit_op_switch_imm(const Instruction*);
599         void emit_op_switch_string(const Instruction*);
600         void emit_op_tear_off_arguments(const Instruction*);
601         void emit_op_throw(const Instruction*);
602         void emit_op_to_number(const Instruction*);
<span class="line-added">603         void emit_op_to_numeric(const Instruction*);</span>
604         void emit_op_to_string(const Instruction*);
605         void emit_op_to_object(const Instruction*);
606         void emit_op_to_primitive(const Instruction*);
607         void emit_op_unexpected_load(const Instruction*);
608         void emit_op_unsigned(const Instruction*);
609         void emit_op_urshift(const Instruction*);
610         void emit_op_has_structure_property(const Instruction*);
611         void emit_op_has_indexed_property(const Instruction*);
612         void emit_op_get_direct_pname(const Instruction*);
613         void emit_op_enumerator_structure_pname(const Instruction*);
614         void emit_op_enumerator_generic_pname(const Instruction*);
<span class="line-added">615         void emit_op_get_internal_field(const Instruction*);</span>
<span class="line-added">616         void emit_op_put_internal_field(const Instruction*);</span>
617         void emit_op_log_shadow_chicken_prologue(const Instruction*);
618         void emit_op_log_shadow_chicken_tail(const Instruction*);
<span class="line-added">619         void emit_op_to_property_key(const Instruction*);</span>
620 
621         void emitSlow_op_add(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
622         void emitSlow_op_call(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
623         void emitSlow_op_tail_call(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
624         void emitSlow_op_call_eval(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
625         void emitSlow_op_call_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
626         void emitSlow_op_tail_call_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
627         void emitSlow_op_tail_call_forward_arguments(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
628         void emitSlow_op_construct_varargs(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
629         void emitSlow_op_construct(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
630         void emitSlow_op_eq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
631         void emitSlow_op_get_callee(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
632         void emitSlow_op_try_get_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
633         void emitSlow_op_get_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
634         void emitSlow_op_get_by_id_with_this(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
635         void emitSlow_op_get_by_id_direct(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
636         void emitSlow_op_get_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
637         void emitSlow_op_get_argument_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
638         void emitSlow_op_in_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
639         void emitSlow_op_instanceof(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
640         void emitSlow_op_instanceof_custom(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
641         void emitSlow_op_jless(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
642         void emitSlow_op_jlesseq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
643         void emitSlow_op_jgreater(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
644         void emitSlow_op_jgreatereq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
645         void emitSlow_op_jnless(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
646         void emitSlow_op_jnlesseq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
647         void emitSlow_op_jngreater(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
648         void emitSlow_op_jngreatereq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
649         void emitSlow_op_jeq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
650         void emitSlow_op_jneq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
651         void emitSlow_op_jstricteq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
652         void emitSlow_op_jnstricteq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
653         void emitSlow_op_jtrue(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
654         void emitSlow_op_loop_hint(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
<span class="line-modified">655         void emitSlow_op_check_traps(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);</span>
656         void emitSlow_op_mod(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
657         void emitSlow_op_mul(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
658         void emitSlow_op_negate(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
659         void emitSlow_op_neq(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
660         void emitSlow_op_new_object(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
661         void emitSlow_op_put_by_id(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
662         void emitSlow_op_put_by_val(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
663         void emitSlow_op_sub(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
664         void emitSlow_op_has_indexed_property(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
665 
666         void emit_op_resolve_scope(const Instruction*);
667         void emit_op_get_from_scope(const Instruction*);
668         void emit_op_put_to_scope(const Instruction*);
669         void emit_op_get_from_arguments(const Instruction*);
670         void emit_op_put_to_arguments(const Instruction*);
671         void emitSlow_op_get_from_scope(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
672         void emitSlow_op_put_to_scope(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;);
673 
674         void emitSlowCaseCall(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, SlowPathFunction);
675 
676         void emitRightShift(const Instruction*, bool isUnsigned);
677         void emitRightShiftSlowCase(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, bool isUnsigned);
678 
679         template&lt;typename Op&gt;
680         void emitNewFuncCommon(const Instruction*);
681         template&lt;typename Op&gt;
682         void emitNewFuncExprCommon(const Instruction*);
683         void emitVarInjectionCheck(bool needsVarInjectionChecks);
<span class="line-modified">684         void emitResolveClosure(VirtualRegister dst, VirtualRegister scope, bool needsVarInjectionChecks, unsigned depth);</span>
<span class="line-modified">685         void emitLoadWithStructureCheck(VirtualRegister scope, Structure** structureSlot);</span>
686 #if USE(JSVALUE64)
687         void emitGetVarFromPointer(JSValue* operand, GPRReg);
688         void emitGetVarFromIndirectPointer(JSValue** operand, GPRReg);
689 #else
690         void emitGetVarFromIndirectPointer(JSValue** operand, GPRReg tag, GPRReg payload);
691         void emitGetVarFromPointer(JSValue* operand, GPRReg tag, GPRReg payload);
692 #endif
<span class="line-modified">693         void emitGetClosureVar(VirtualRegister scope, uintptr_t operand);</span>
694         void emitNotifyWrite(WatchpointSet*);
695         void emitNotifyWrite(GPRReg pointerToSet);
<span class="line-modified">696         void emitPutGlobalVariable(JSValue* operand, VirtualRegister value, WatchpointSet*);</span>
<span class="line-modified">697         void emitPutGlobalVariableIndirect(JSValue** addressOfOperand, VirtualRegister value, WatchpointSet**);</span>
<span class="line-modified">698         void emitPutClosureVar(VirtualRegister scope, uintptr_t operand, VirtualRegister value, WatchpointSet*);</span>
699 
<span class="line-modified">700         void emitInitRegister(VirtualRegister);</span>
701 
<span class="line-modified">702         void emitPutIntToCallFrameHeader(RegisterID from, VirtualRegister);</span>
703 
<span class="line-modified">704         JSValue getConstantOperand(VirtualRegister);</span>
<span class="line-modified">705         bool isOperandConstantInt(VirtualRegister);</span>
<span class="line-modified">706         bool isOperandConstantChar(VirtualRegister);</span>
707 
708         template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
709         void emitMathICFast(JITUnaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledFunction, NonProfiledFunction);
710         template &lt;typename Op, typename Generator, typename ProfiledFunction, typename NonProfiledFunction&gt;
711         void emitMathICFast(JITBinaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledFunction, NonProfiledFunction);
712 
713         template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
714         void emitMathICSlow(JITBinaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledRepatchFunction, ProfiledFunction, RepatchFunction);
715         template &lt;typename Op, typename Generator, typename ProfiledRepatchFunction, typename ProfiledFunction, typename RepatchFunction&gt;
716         void emitMathICSlow(JITUnaryMathIC&lt;Generator&gt;*, const Instruction*, ProfiledRepatchFunction, ProfiledFunction, RepatchFunction);
717 
718         Jump getSlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
719         {
720             return iter++-&gt;from;
721         }
722         void linkSlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
723         {
724             if (iter-&gt;from.isSet())
725                 iter-&gt;from.link(this);
726             ++iter;
727         }
728         void linkDummySlowCase(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
729         {
730             ASSERT(!iter-&gt;from.isSet());
731             ++iter;
732         }
<span class="line-modified">733         void linkSlowCaseIfNotJSCell(Vector&lt;SlowCaseEntry&gt;::iterator&amp;, VirtualRegister);</span>
<span class="line-modified">734         void linkAllSlowCasesForBytecodeIndex(Vector&lt;SlowCaseEntry&gt;&amp; slowCases,</span>
<span class="line-modified">735             Vector&lt;SlowCaseEntry&gt;::iterator&amp;, BytecodeIndex bytecodeOffset);</span>
736 
737         void linkAllSlowCases(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
738         {
<span class="line-modified">739             linkAllSlowCasesForBytecodeIndex(m_slowCases, iter, m_bytecodeIndex);</span>
<span class="line-added">740         }</span>
<span class="line-added">741 </span>
<span class="line-added">742         bool hasAnySlowCases(Vector&lt;SlowCaseEntry&gt;&amp; slowCases, Vector&lt;SlowCaseEntry&gt;::iterator&amp;, BytecodeIndex bytecodeOffset);</span>
<span class="line-added">743         bool hasAnySlowCases(Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)</span>
<span class="line-added">744         {</span>
<span class="line-added">745             return hasAnySlowCases(m_slowCases, iter, m_bytecodeIndex);</span>
746         }
747 
748         MacroAssembler::Call appendCallWithExceptionCheck(const FunctionPtr&lt;CFunctionPtrTag&gt;);
749 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
750         MacroAssembler::Call appendCallWithExceptionCheckAndSlowPathReturnType(const FunctionPtr&lt;CFunctionPtrTag&gt;);
751 #endif
752         MacroAssembler::Call appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt;);
<span class="line-modified">753         MacroAssembler::Call appendCallWithExceptionCheckSetJSValueResult(const FunctionPtr&lt;CFunctionPtrTag&gt;, VirtualRegister result);</span>
754         template&lt;typename Metadata&gt;
<span class="line-modified">755         MacroAssembler::Call appendCallWithExceptionCheckSetJSValueResultWithProfile(Metadata&amp;, const FunctionPtr&lt;CFunctionPtrTag&gt;, VirtualRegister result);</span>
756 
757         template&lt;typename OperationType, typename... Args&gt;
758         std::enable_if_t&lt;FunctionTraits&lt;OperationType&gt;::hasResult, MacroAssembler::Call&gt;
<span class="line-modified">759         callOperation(OperationType operation, VirtualRegister result, Args... args)</span>
760         {
761             setupArguments&lt;OperationType&gt;(args...);
762             return appendCallWithExceptionCheckSetJSValueResult(operation, result);
763         }
764 
765 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
766         template&lt;typename OperationType, typename... Args&gt;
767         std::enable_if_t&lt;std::is_same&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType, SlowPathReturnType&gt;::value, MacroAssembler::Call&gt;
768         callOperation(OperationType operation, Args... args)
769         {
770             setupArguments&lt;OperationType&gt;(args...);
771             return appendCallWithExceptionCheckAndSlowPathReturnType(operation);
772         }
773 
774         template&lt;typename Type&gt;
775         struct is64BitType {
776             static constexpr bool value = sizeof(Type) &lt;= 8;
777         };
778 
779         template&lt;&gt;
</pre>
<hr />
<pre>
783 
784         template&lt;typename OperationType, typename... Args&gt;
785         std::enable_if_t&lt;!std::is_same&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType, SlowPathReturnType&gt;::value, MacroAssembler::Call&gt;
786         callOperation(OperationType operation, Args... args)
787         {
788             static_assert(is64BitType&lt;typename FunctionTraits&lt;OperationType&gt;::ResultType&gt;::value, &quot;Win64 cannot use standard call when return type is larger than 64 bits.&quot;);
789             setupArguments&lt;OperationType&gt;(args...);
790             return appendCallWithExceptionCheck(operation);
791         }
792 #else // OS(WINDOWS) &amp;&amp; CPU(X86_64)
793         template&lt;typename OperationType, typename... Args&gt;
794         MacroAssembler::Call callOperation(OperationType operation, Args... args)
795         {
796             setupArguments&lt;OperationType&gt;(args...);
797             return appendCallWithExceptionCheck(operation);
798         }
799 #endif // OS(WINDOWS) &amp;&amp; CPU(X86_64)
800 
801         template&lt;typename Metadata, typename OperationType, typename... Args&gt;
802         std::enable_if_t&lt;FunctionTraits&lt;OperationType&gt;::hasResult, MacroAssembler::Call&gt;
<span class="line-modified">803         callOperationWithProfile(Metadata&amp; metadata, OperationType operation, VirtualRegister result, Args... args)</span>
804         {
805             setupArguments&lt;OperationType&gt;(args...);
806             return appendCallWithExceptionCheckSetJSValueResultWithProfile(metadata, operation, result);
807         }
808 
809         template&lt;typename OperationType, typename... Args&gt;
810         MacroAssembler::Call callOperationWithResult(OperationType operation, JSValueRegs resultRegs, Args... args)
811         {
812             setupArguments&lt;OperationType&gt;(args...);
813             auto result = appendCallWithExceptionCheck(operation);
814             setupResults(resultRegs);
815             return result;
816         }
817 
818         template&lt;typename OperationType, typename... Args&gt;
819         MacroAssembler::Call callOperationNoExceptionCheck(OperationType operation, Args... args)
820         {
821             setupArguments&lt;OperationType&gt;(args...);
822             updateTopCallFrame();
823             return appendCall(operation);
</pre>
<hr />
<pre>
836         };
837 
838         template&lt;typename Op, typename SnippetGenerator&gt;
839         void emitBitBinaryOpFastPath(const Instruction* currentInstruction, ProfilingPolicy shouldEmitProfiling = ProfilingPolicy::NoProfiling);
840 
841         void emitRightShiftFastPath(const Instruction* currentInstruction, OpcodeID);
842 
843         template&lt;typename Op&gt;
844         void emitRightShiftFastPath(const Instruction* currentInstruction, JITRightShiftGenerator::ShiftType);
845 
846         void updateTopCallFrame();
847 
848         Call emitNakedCall(CodePtr&lt;NoPtrTag&gt; function = CodePtr&lt;NoPtrTag&gt;());
849         Call emitNakedTailCall(CodePtr&lt;NoPtrTag&gt; function = CodePtr&lt;NoPtrTag&gt;());
850 
851         // Loads the character value of a single character string into dst.
852         void emitLoadCharacterString(RegisterID src, RegisterID dst, JumpList&amp; failures);
853 
854         int jumpTarget(const Instruction*, int target);
855 
<span class="line-added">856 #if ENABLE(DFG_JIT)</span>
<span class="line-added">857         void emitEnterOptimizationCheck();</span>
<span class="line-added">858 #else</span>
<span class="line-added">859         void emitEnterOptimizationCheck() { }</span>
<span class="line-added">860 #endif</span>
<span class="line-added">861 </span>
862 #ifndef NDEBUG
<span class="line-modified">863         void printBytecodeOperandTypes(VirtualRegister src1, VirtualRegister src2);</span>
864 #endif
865 
866 #if ENABLE(SAMPLING_FLAGS)
867         void setSamplingFlag(int32_t);
868         void clearSamplingFlag(int32_t);
869 #endif
870 
871 #if ENABLE(SAMPLING_COUNTERS)
872         void emitCount(AbstractSamplingCounter&amp;, int32_t = 1);
873 #endif
874 
875 #if ENABLE(OPCODE_SAMPLING)
876         void sampleInstruction(const Instruction*, bool = false);
877 #endif
878 
879 #if ENABLE(CODEBLOCK_SAMPLING)
880         void sampleCodeBlock(CodeBlock*);
881 #else
882         void sampleCodeBlock(CodeBlock*) {}
883 #endif
</pre>
<hr />
<pre>
885 #if ENABLE(DFG_JIT)
886         bool canBeOptimized() { return m_canBeOptimized; }
887         bool canBeOptimizedOrInlined() { return m_canBeOptimizedOrInlined; }
888         bool shouldEmitProfiling() { return m_shouldEmitProfiling; }
889 #else
890         bool canBeOptimized() { return false; }
891         bool canBeOptimizedOrInlined() { return false; }
892         // Enables use of value profiler with tiered compilation turned off,
893         // in which case all code gets profiled.
894         bool shouldEmitProfiling() { return false; }
895 #endif
896 
897         static bool reportCompileTimes();
898         static bool computeCompileTimes();
899 
900         // If you need to check a value from the metadata table and you need it to
901         // be consistent across the fast and slow path, then you want to use this.
902         // It will give the slow path the same value read by the fast path.
903         GetPutInfo copiedGetPutInfo(OpPutToScope);
904         template&lt;typename BinaryOp&gt;
<span class="line-modified">905         BinaryArithProfile copiedArithProfile(BinaryOp);</span>
906 
907         Interpreter* m_interpreter;
908 
909         Vector&lt;CallRecord&gt; m_calls;
910         Vector&lt;Label&gt; m_labels;
911         Vector&lt;JITGetByIdGenerator&gt; m_getByIds;
<span class="line-added">912         Vector&lt;JITGetByValGenerator&gt; m_getByVals;</span>
913         Vector&lt;JITGetByIdWithThisGenerator&gt; m_getByIdsWithThis;
914         Vector&lt;JITPutByIdGenerator&gt; m_putByIds;
915         Vector&lt;JITInByIdGenerator&gt; m_inByIds;
916         Vector&lt;JITInstanceOfGenerator&gt; m_instanceOfs;
917         Vector&lt;ByValCompilationInfo&gt; m_byValCompilationInfo;
918         Vector&lt;CallCompilationInfo&gt; m_callCompilationInfo;
919         Vector&lt;JumpTable&gt; m_jmpTable;
920 
<span class="line-modified">921         BytecodeIndex m_bytecodeIndex;</span>
922         Vector&lt;SlowCaseEntry&gt; m_slowCases;
923         Vector&lt;SwitchRecord&gt; m_switches;
924 
925         HashMap&lt;unsigned, unsigned&gt; m_copiedGetPutInfos;
<span class="line-modified">926         HashMap&lt;uint64_t, BinaryArithProfile&gt; m_copiedArithProfiles;</span>
927 
928         JumpList m_exceptionChecks;
929         JumpList m_exceptionChecksWithCallFrameRollback;
930         Label m_exceptionHandler;
931 
932         unsigned m_getByIdIndex { UINT_MAX };
<span class="line-added">933         unsigned m_getByValIndex { UINT_MAX };</span>
934         unsigned m_getByIdWithThisIndex { UINT_MAX };
935         unsigned m_putByIdIndex { UINT_MAX };
936         unsigned m_inByIdIndex { UINT_MAX };
937         unsigned m_instanceOfIndex { UINT_MAX };
938         unsigned m_byValInstructionIndex { UINT_MAX };
939         unsigned m_callLinkInfoIndex { UINT_MAX };
<span class="line-added">940         unsigned m_bytecodeCountHavingSlowCase { 0 };</span>
941 
942         Label m_arityCheck;
943         std::unique_ptr&lt;LinkBuffer&gt; m_linkBuffer;
944 
945         std::unique_ptr&lt;JITDisassembler&gt; m_disassembler;
946         RefPtr&lt;Profiler::Compilation&gt; m_compilation;
947 
948         PCToCodeOriginMapBuilder m_pcToCodeOriginMapBuilder;
949 
950         HashMap&lt;const Instruction*, void*&gt; m_instructionToMathIC;
951         HashMap&lt;const Instruction*, MathICGenerationState&gt; m_instructionToMathICGenerationState;
952 
953         bool m_canBeOptimized;
954         bool m_canBeOptimizedOrInlined;
955         bool m_shouldEmitProfiling;
<span class="line-modified">956         BytecodeIndex m_loopOSREntryBytecodeIndex;</span>

957     };
958 
959 } // namespace JSC
960 
961 
962 #endif // ENABLE(JIT)
</pre>
</td>
</tr>
</table>
<center><a href="JIT.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITAddGenerator.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>