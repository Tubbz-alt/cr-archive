diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2014-2018 Apple Inc. All rights reserved.
+ * Copyright (C) 2014-2020 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -28,10 +28,11 @@
 
 #if ENABLE(JIT)
 
 #include "BinarySwitch.h"
 #include "CCallHelpers.h"
+#include "CacheableIdentifierInlines.h"
 #include "CodeBlock.h"
 #include "FullCodeOrigin.h"
 #include "Heap.h"
 #include "JITOperations.h"
 #include "JSCInlines.h"
@@ -43,23 +44,25 @@
 #include <wtf/ListDump.h>
 
 namespace JSC {
 
 namespace PolymorphicAccessInternal {
-static const bool verbose = false;
+static constexpr bool verbose = false;
 }
 
+DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(PolymorphicAccess);
+
 void AccessGenerationResult::dump(PrintStream& out) const
 {
     out.print(m_kind);
     if (m_code)
         out.print(":", m_code);
 }
 
-Watchpoint* AccessGenerationState::addWatchpoint(const ObjectPropertyCondition& condition)
+void AccessGenerationState::installWatchpoint(const ObjectPropertyCondition& condition)
 {
-    return WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
+    WatchpointsOnStructureStubInfo::ensureReferenceAndInstallWatchpoint(
         watchpoints, jit->codeBlock(), stubInfo, condition);
 }
 
 void AccessGenerationState::restoreScratch()
 {
@@ -208,27 +211,27 @@
         jit->addLinkTask(
             [=] (LinkBuffer& linkBuffer) {
                 linkBuffer.link(jumpToOSRExitExceptionHandler, originalHandler.nativeCode);
             });
     } else {
-        jit->setupArguments<decltype(lookupExceptionHandler)>(CCallHelpers::TrustedImmPtr(&m_vm), GPRInfo::callFrameRegister);
+        jit->setupArguments<decltype(operationLookupExceptionHandler)>(CCallHelpers::TrustedImmPtr(&m_vm));
+        jit->prepareCallOperation(m_vm);
         CCallHelpers::Call lookupExceptionHandlerCall = jit->call(OperationPtrTag);
         jit->addLinkTask(
             [=] (LinkBuffer& linkBuffer) {
-                linkBuffer.link(lookupExceptionHandlerCall, FunctionPtr<OperationPtrTag>(lookupExceptionHandler));
+                linkBuffer.link(lookupExceptionHandlerCall, FunctionPtr<OperationPtrTag>(operationLookupExceptionHandler));
             });
         jit->jumpToExceptionHandler(m_vm);
     }
 }
 
-
 PolymorphicAccess::PolymorphicAccess() { }
 PolymorphicAccess::~PolymorphicAccess() { }
 
 AccessGenerationResult PolymorphicAccess::addCases(
     const GCSafeConcurrentJSLocker& locker, VM& vm, CodeBlock* codeBlock, StructureStubInfo& stubInfo,
-    const Identifier& ident, Vector<std::unique_ptr<AccessCase>, 2> originalCasesToAdd)
+    Vector<std::unique_ptr<AccessCase>, 2> originalCasesToAdd)
 {
     SuperSamplerScope superSamplerScope(false);
 
     // This method will add the originalCasesToAdd to the list one at a time while preserving the
     // invariants:
@@ -307,27 +310,26 @@
     }
 
     // Now add things to the new list. Note that at this point, we will still have old cases that
     // may be replaced by the new ones. That's fine. We will sort that out when we regenerate.
     for (auto& caseToAdd : casesToAdd) {
-        commit(locker, vm, m_watchpoints, codeBlock, stubInfo, ident, *caseToAdd);
+        commit(locker, vm, m_watchpoints, codeBlock, stubInfo, *caseToAdd);
         m_list.append(WTFMove(caseToAdd));
     }
 
     if (PolymorphicAccessInternal::verbose)
         dataLog("After addCases: m_list: ", listDump(m_list), "\n");
 
     return AccessGenerationResult::Buffered;
 }
 
 AccessGenerationResult PolymorphicAccess::addCase(
-    const GCSafeConcurrentJSLocker& locker, VM& vm, CodeBlock* codeBlock, StructureStubInfo& stubInfo,
-    const Identifier& ident, std::unique_ptr<AccessCase> newAccess)
+    const GCSafeConcurrentJSLocker& locker, VM& vm, CodeBlock* codeBlock, StructureStubInfo& stubInfo, std::unique_ptr<AccessCase> newAccess)
 {
     Vector<std::unique_ptr<AccessCase>, 2> newAccesses;
     newAccesses.append(WTFMove(newAccess));
-    return addCases(locker, vm, codeBlock, stubInfo, ident, WTFMove(newAccesses));
+    return addCases(locker, vm, codeBlock, stubInfo, WTFMove(newAccesses));
 }
 
 bool PolymorphicAccess::visitWeak(VM& vm) const
 {
     for (unsigned i = 0; i < size(); ++i) {
@@ -349,10 +351,16 @@
     for (unsigned i = 0; i < size(); ++i)
         result &= at(i).propagateTransitions(visitor);
     return result;
 }
 
+void PolymorphicAccess::visitAggregate(SlotVisitor& visitor)
+{
+    for (unsigned i = 0; i < size(); ++i)
+        at(i).visitAggregate(visitor);
+}
+
 void PolymorphicAccess::dump(PrintStream& out) const
 {
     out.print(RawPointer(this), ":[");
     CommaPrinter comma;
     for (auto& entry : m_list)
@@ -360,63 +368,44 @@
     out.print("]");
 }
 
 void PolymorphicAccess::commit(
     const GCSafeConcurrentJSLocker&, VM& vm, std::unique_ptr<WatchpointsOnStructureStubInfo>& watchpoints, CodeBlock* codeBlock,
-    StructureStubInfo& stubInfo, const Identifier& ident, AccessCase& accessCase)
+    StructureStubInfo& stubInfo, AccessCase& accessCase)
 {
     // NOTE: We currently assume that this is relatively rare. It mainly arises for accesses to
     // properties on DOM nodes. For sure we cache many DOM node accesses, but even in
     // Real Pages (TM), we appear to spend most of our time caching accesses to properties on
     // vanilla objects or exotic objects from within JSC (like Arguments, those are super popular).
     // Those common kinds of JSC object accesses don't hit this case.
 
-    for (WatchpointSet* set : accessCase.commit(vm, ident)) {
+    for (WatchpointSet* set : accessCase.commit(vm)) {
         Watchpoint* watchpoint =
             WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
-                watchpoints, codeBlock, &stubInfo, ObjectPropertyCondition());
+                watchpoints, codeBlock, &stubInfo);
 
         set->add(watchpoint);
     }
 }
 
 AccessGenerationResult PolymorphicAccess::regenerate(
-    const GCSafeConcurrentJSLocker& locker, VM& vm, CodeBlock* codeBlock, StructureStubInfo& stubInfo, const Identifier& ident)
+    const GCSafeConcurrentJSLocker& locker, VM& vm, CodeBlock* codeBlock, StructureStubInfo& stubInfo)
 {
     SuperSamplerScope superSamplerScope(false);
 
     if (PolymorphicAccessInternal::verbose)
         dataLog("Regenerate with m_list: ", listDump(m_list), "\n");
 
     AccessGenerationState state(vm, codeBlock->globalObject());
 
     state.access = this;
     state.stubInfo = &stubInfo;
-    state.ident = &ident;
 
-    state.baseGPR = stubInfo.baseGPR();
-    state.thisGPR = stubInfo.patch.thisGPR;
+    state.baseGPR = stubInfo.baseGPR;
+    state.u.thisGPR = stubInfo.regs.thisGPR;
     state.valueRegs = stubInfo.valueRegs();
 
-    ScratchRegisterAllocator allocator(stubInfo.patch.usedRegisters);
-    state.allocator = &allocator;
-    allocator.lock(state.baseGPR);
-    if (state.thisGPR != InvalidGPRReg)
-        allocator.lock(state.thisGPR);
-    allocator.lock(state.valueRegs);
-#if USE(JSVALUE32_64)
-    allocator.lock(stubInfo.patch.baseTagGPR);
-#endif
-
-    state.scratchGPR = allocator.allocateScratchGPR();
-
-    CCallHelpers jit(codeBlock);
-    state.jit = &jit;
-
-    state.preservedReusedRegisterState =
-        allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
-
     // Regenerating is our opportunity to figure out what our list of cases should look like. We
     // do this here. The newly produced 'cases' list may be smaller than m_list. We don't edit
     // m_list in-place because we may still fail, in which case we want the PolymorphicAccess object
     // to be unmutated. For sure, we want it to hang onto any data structures that may be referenced
     // from the code of the current stub (aka previous).
@@ -461,19 +450,46 @@
         if (isGenerated)
             m_list[dstIndex++] = WTFMove(someCase);
     }
     m_list.resize(dstIndex);
 
+    ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+    state.allocator = &allocator;
+    allocator.lock(state.baseGPR);
+    if (state.u.thisGPR != InvalidGPRReg)
+        allocator.lock(state.u.thisGPR);
+    allocator.lock(state.valueRegs);
+#if USE(JSVALUE32_64)
+    allocator.lock(stubInfo.baseTagGPR);
+    if (stubInfo.v.thisTagGPR != InvalidGPRReg)
+        allocator.lock(stubInfo.v.thisTagGPR);
+#endif
+
+    state.scratchGPR = allocator.allocateScratchGPR();
+
+    for (auto& accessCase : cases) {
+        if (accessCase->needsScratchFPR()) {
+            state.scratchFPR = allocator.allocateScratchFPR();
+            break;
+        }
+    }
+
+    CCallHelpers jit(codeBlock);
+    state.jit = &jit;
+
+    state.preservedReusedRegisterState =
+        allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
+
     bool generatedFinalCode = false;
 
     // If the resulting set of cases is so big that we would stop caching and this is InstanceOf,
     // then we want to generate the generic InstanceOf and then stop.
     if (cases.size() >= Options::maxAccessVariantListSize()
         && stubInfo.accessType == AccessType::InstanceOf) {
         while (!cases.isEmpty())
             m_list.append(cases.takeLast());
-        cases.append(AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric));
+        cases.append(AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric, nullptr));
         generatedFinalCode = true;
     }
 
     if (PolymorphicAccessInternal::verbose)
         dataLog("Optimized cases: ", listDump(cases), "\n");
@@ -481,13 +497,25 @@
     // At this point we're convinced that 'cases' contains the cases that we want to JIT now and we
     // won't change that set anymore.
 
     bool allGuardedByStructureCheck = true;
     bool hasJSGetterSetterCall = false;
+    bool needsInt32PropertyCheck = false;
+    bool needsStringPropertyCheck = false;
+    bool needsSymbolPropertyCheck = false;
     for (auto& newCase : cases) {
-        commit(locker, vm, state.watchpoints, codeBlock, stubInfo, ident, *newCase);
-        allGuardedByStructureCheck &= newCase->guardedByStructureCheck();
+        if (!stubInfo.hasConstantIdentifier) {
+            if (newCase->requiresIdentifierNameMatch()) {
+                if (newCase->uid()->isSymbol())
+                    needsSymbolPropertyCheck = true;
+                else
+                    needsStringPropertyCheck = true;
+            } else if (newCase->requiresInt32PropertyCheck())
+                needsInt32PropertyCheck = true;
+        }
+        commit(locker, vm, state.watchpoints, codeBlock, stubInfo, *newCase);
+        allGuardedByStructureCheck &= newCase->guardedByStructureCheck(stubInfo);
         if (newCase->type() == AccessCase::Getter || newCase->type() == AccessCase::Setter)
             hasJSGetterSetterCall = true;
     }
 
     if (cases.isEmpty()) {
@@ -496,18 +524,104 @@
     } else if (!allGuardedByStructureCheck || cases.size() == 1) {
         // If there are any proxies in the list, we cannot just use a binary switch over the structure.
         // We need to resort to a cascade. A cascade also happens to be optimal if we only have just
         // one case.
         CCallHelpers::JumpList fallThrough;
+        if (needsInt32PropertyCheck || needsStringPropertyCheck || needsSymbolPropertyCheck) {
+            if (needsInt32PropertyCheck) {
+                CCallHelpers::Jump notInt32;
+
+                if (!stubInfo.propertyIsInt32) {
+#if USE(JSVALUE64)
+                    notInt32 = jit.branchIfNotInt32(state.u.propertyGPR);
+#else
+                    notInt32 = jit.branchIfNotInt32(state.stubInfo->v.propertyTagGPR);
+#endif
+                }
+                for (unsigned i = cases.size(); i--;) {
+                    fallThrough.link(&jit);
+                    fallThrough.clear();
+                    if (cases[i]->requiresInt32PropertyCheck())
+                        cases[i]->generateWithGuard(state, fallThrough);
+                }
+
+                if (needsStringPropertyCheck || needsSymbolPropertyCheck) {
+                    if (notInt32.isSet())
+                        notInt32.link(&jit);
+                    fallThrough.link(&jit);
+                    fallThrough.clear();
+                } else {
+                    if (notInt32.isSet())
+                        state.failAndRepatch.append(notInt32);
+                }
+            }
+
+            if (needsStringPropertyCheck) {
+                CCallHelpers::JumpList notString;
+                GPRReg propertyGPR = state.u.propertyGPR;
+                if (!stubInfo.propertyIsString) {
+#if USE(JSVALUE32_64)
+                    GPRReg propertyTagGPR = state.stubInfo->v.propertyTagGPR;
+                    notString.append(jit.branchIfNotCell(propertyTagGPR));
+#else
+                    notString.append(jit.branchIfNotCell(propertyGPR));
+#endif
+                    notString.append(jit.branchIfNotString(propertyGPR));
+                }
+
+                jit.loadPtr(MacroAssembler::Address(propertyGPR, JSString::offsetOfValue()), state.scratchGPR);
+
+                state.failAndRepatch.append(jit.branchIfRopeStringImpl(state.scratchGPR));
+
+                for (unsigned i = cases.size(); i--;) {
+                    fallThrough.link(&jit);
+                    fallThrough.clear();
+                    if (cases[i]->requiresIdentifierNameMatch() && !cases[i]->uid()->isSymbol())
+                        cases[i]->generateWithGuard(state, fallThrough);
+                }
+
+                if (needsSymbolPropertyCheck) {
+                    notString.link(&jit);
+                    fallThrough.link(&jit);
+                    fallThrough.clear();
+                } else
+                    state.failAndRepatch.append(notString);
+            }
+
+            if (needsSymbolPropertyCheck) {
+                CCallHelpers::JumpList notSymbol;
+                if (!stubInfo.propertyIsSymbol) {
+                    GPRReg propertyGPR = state.u.propertyGPR;
+#if USE(JSVALUE32_64)
+                    GPRReg propertyTagGPR = state.stubInfo->v.propertyTagGPR;
+                    notSymbol.append(jit.branchIfNotCell(propertyTagGPR));
+#else
+                    notSymbol.append(jit.branchIfNotCell(propertyGPR));
+#endif
+                    notSymbol.append(jit.branchIfNotSymbol(propertyGPR));
+                }
+
+                for (unsigned i = cases.size(); i--;) {
+                    fallThrough.link(&jit);
+                    fallThrough.clear();
+                    if (cases[i]->requiresIdentifierNameMatch() && cases[i]->uid()->isSymbol())
+                        cases[i]->generateWithGuard(state, fallThrough);
+                }
 
-        // Cascade through the list, preferring newer entries.
-        for (unsigned i = cases.size(); i--;) {
-            fallThrough.link(&jit);
-            fallThrough.clear();
-            cases[i]->generateWithGuard(state, fallThrough);
+                state.failAndRepatch.append(notSymbol);
+            }
+        } else {
+            // Cascade through the list, preferring newer entries.
+            for (unsigned i = cases.size(); i--;) {
+                fallThrough.link(&jit);
+                fallThrough.clear();
+                cases[i]->generateWithGuard(state, fallThrough);
+            }
         }
+
         state.failAndRepatch.append(fallThrough);
+
     } else {
         jit.load32(
             CCallHelpers::Address(state.baseGPR, JSCell::structureIDOffset()),
             state.scratchGPR);
 
@@ -593,15 +707,15 @@
         if (PolymorphicAccessInternal::verbose)
             dataLog("Did fail to allocate.\n");
         return AccessGenerationResult::GaveUp;
     }
 
-    CodeLocationLabel<JSInternalPtrTag> successLabel = stubInfo.doneLocation();
+    CodeLocationLabel<JSInternalPtrTag> successLabel = stubInfo.doneLocation;
 
     linkBuffer.link(state.success, successLabel);
 
-    linkBuffer.link(failure, stubInfo.slowPathStartLocation());
+    linkBuffer.link(failure, stubInfo.slowPathStartLocation);
 
     if (PolymorphicAccessInternal::verbose)
         dataLog(FullCodeOrigin(codeBlock, stubInfo.codeOrigin), ": Generating polymorphic access stub for ", listDump(cases), "\n");
 
     MacroAssemblerCodeRef<JITStubRoutinePtrTag> code = FINALIZE_CODE_FOR(
@@ -609,20 +723,23 @@
         "%s", toCString("Access stub for ", *codeBlock, " ", stubInfo.codeOrigin, " with return point ", successLabel, ": ", listDump(cases)).data());
 
     bool doesCalls = false;
     Vector<JSCell*> cellsToMark;
     for (auto& entry : cases)
-        doesCalls |= entry->doesCalls(&cellsToMark);
+        doesCalls |= entry->doesCalls(vm, &cellsToMark);
 
-    m_stubRoutine = createJITStubRoutine(code, vm, codeBlock, doesCalls, cellsToMark, codeBlockThatOwnsExceptionHandlers, callSiteIndexForExceptionHandling);
+    m_stubRoutine = createJITStubRoutine(code, vm, codeBlock, doesCalls, cellsToMark, WTFMove(state.m_callLinkInfos), codeBlockThatOwnsExceptionHandlers, callSiteIndexForExceptionHandling);
     m_watchpoints = WTFMove(state.watchpoints);
-    if (!state.weakReferences.isEmpty())
+    if (!state.weakReferences.isEmpty()) {
+        state.weakReferences.shrinkToFit();
         m_weakReferences = makeUnique<Vector<WriteBarrier<JSCell>>>(WTFMove(state.weakReferences));
+    }
     if (PolymorphicAccessInternal::verbose)
         dataLog("Returning: ", code.code(), "\n");
 
     m_list = WTFMove(cases);
+    m_list.shrinkToFit();
 
     AccessGenerationResult::Kind resultKind;
     if (m_list.size() >= Options::maxAccessVariantListSize() || generatedFinalCode)
         resultKind = AccessGenerationResult::GeneratedFinalCode;
     else
@@ -736,10 +853,58 @@
         out.print("InstanceOfMiss");
         return;
     case AccessCase::InstanceOfGeneric:
         out.print("InstanceOfGeneric");
         return;
+    case AccessCase::IndexedInt32Load:
+        out.print("IndexedInt32Load");
+        return;
+    case AccessCase::IndexedDoubleLoad:
+        out.print("IndexedDoubleLoad");
+        return;
+    case AccessCase::IndexedContiguousLoad:
+        out.print("IndexedContiguousLoad");
+        return;
+    case AccessCase::IndexedArrayStorageLoad:
+        out.print("IndexedArrayStorageLoad");
+        return;
+    case AccessCase::IndexedScopedArgumentsLoad:
+        out.print("IndexedScopedArgumentsLoad");
+        return;
+    case AccessCase::IndexedDirectArgumentsLoad:
+        out.print("IndexedDirectArgumentsLoad");
+        return;
+    case AccessCase::IndexedTypedArrayInt8Load:
+        out.print("IndexedTypedArrayInt8Load");
+        return;
+    case AccessCase::IndexedTypedArrayUint8Load:
+        out.print("IndexedTypedArrayUint8Load");
+        return;
+    case AccessCase::IndexedTypedArrayUint8ClampedLoad:
+        out.print("IndexedTypedArrayUint8ClampedLoad");
+        return;
+    case AccessCase::IndexedTypedArrayInt16Load:
+        out.print("IndexedTypedArrayInt16Load");
+        return;
+    case AccessCase::IndexedTypedArrayUint16Load:
+        out.print("IndexedTypedArrayUint16Load");
+        return;
+    case AccessCase::IndexedTypedArrayInt32Load:
+        out.print("IndexedTypedArrayInt32Load");
+        return;
+    case AccessCase::IndexedTypedArrayUint32Load:
+        out.print("IndexedTypedArrayUint32Load");
+        return;
+    case AccessCase::IndexedTypedArrayFloat32Load:
+        out.print("IndexedTypedArrayFloat32Load");
+        return;
+    case AccessCase::IndexedTypedArrayFloat64Load:
+        out.print("IndexedTypedArrayFloat64Load");
+        return;
+    case AccessCase::IndexedStringLoad:
+        out.print("IndexedStringLoad");
+        return;
     }
 
     RELEASE_ASSERT_NOT_REACHED();
 }
 
