diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/AccessCase.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/AccessCase.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/AccessCase.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/AccessCase.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2017-2019 Apple Inc. All rights reserved.
+ * Copyright (C) 2017-2020 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -27,10 +27,11 @@
 #include "AccessCase.h"
 
 #if ENABLE(JIT)
 
 #include "CCallHelpers.h"
+#include "CacheableIdentifierInlines.h"
 #include "CallLinkInfo.h"
 #include "DOMJITGetterSetter.h"
 #include "DirectArguments.h"
 #include "GetterSetter.h"
 #include "GetterSetterAccessCase.h"
@@ -49,23 +50,27 @@
 #include "ThunkGenerators.h"
 
 namespace JSC {
 
 namespace AccessCaseInternal {
-static const bool verbose = false;
+static constexpr bool verbose = false;
 }
 
-AccessCase::AccessCase(VM& vm, JSCell* owner, AccessType type, PropertyOffset offset, Structure* structure, const ObjectPropertyConditionSet& conditionSet, std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain)
+DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(AccessCase);
+
+AccessCase::AccessCase(VM& vm, JSCell* owner, AccessType type, CacheableIdentifier identifier, PropertyOffset offset, Structure* structure, const ObjectPropertyConditionSet& conditionSet, std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain)
     : m_type(type)
     , m_offset(offset)
     , m_polyProtoAccessChain(WTFMove(prototypeAccessChain))
+    , m_identifier(identifier)
 {
     m_structure.setMayBeNull(vm, owner, structure);
     m_conditionSet = conditionSet;
+    RELEASE_ASSERT(m_conditionSet.isValid());
 }
 
-std::unique_ptr<AccessCase> AccessCase::create(VM& vm, JSCell* owner, AccessType type, PropertyOffset offset, Structure* structure, const ObjectPropertyConditionSet& conditionSet, std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain)
+std::unique_ptr<AccessCase> AccessCase::create(VM& vm, JSCell* owner, AccessType type, CacheableIdentifier identifier, PropertyOffset offset, Structure* structure, const ObjectPropertyConditionSet& conditionSet, std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain)
 {
     switch (type) {
     case InHit:
     case InMiss:
         break;
@@ -74,21 +79,37 @@
     case DirectArgumentsLength:
     case ScopedArgumentsLength:
     case ModuleNamespaceLoad:
     case Replace:
     case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
         RELEASE_ASSERT(!prototypeAccessChain);
         break;
     default:
         RELEASE_ASSERT_NOT_REACHED();
     };
 
-    return std::unique_ptr<AccessCase>(new AccessCase(vm, owner, type, offset, structure, conditionSet, WTFMove(prototypeAccessChain)));
+    return std::unique_ptr<AccessCase>(new AccessCase(vm, owner, type, identifier, offset, structure, conditionSet, WTFMove(prototypeAccessChain)));
 }
 
 std::unique_ptr<AccessCase> AccessCase::create(
-    VM& vm, JSCell* owner, PropertyOffset offset, Structure* oldStructure, Structure* newStructure,
+    VM& vm, JSCell* owner, CacheableIdentifier identifier, PropertyOffset offset, Structure* oldStructure, Structure* newStructure,
     const ObjectPropertyConditionSet& conditionSet, std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain)
 {
     RELEASE_ASSERT(oldStructure == newStructure->previousID());
 
     // Skip optimizing the case where we need a realloc, if we don't have
@@ -97,35 +118,44 @@
         && oldStructure->outOfLineCapacity() != newStructure->outOfLineCapacity()
         && oldStructure->outOfLineCapacity()) {
         return nullptr;
     }
 
-    return std::unique_ptr<AccessCase>(new AccessCase(vm, owner, Transition, offset, newStructure, conditionSet, WTFMove(prototypeAccessChain)));
+    return std::unique_ptr<AccessCase>(new AccessCase(vm, owner, Transition, identifier, offset, newStructure, conditionSet, WTFMove(prototypeAccessChain)));
 }
 
 AccessCase::~AccessCase()
 {
 }
 
 std::unique_ptr<AccessCase> AccessCase::fromStructureStubInfo(
-    VM& vm, JSCell* owner, StructureStubInfo& stubInfo)
+    VM& vm, JSCell* owner, CacheableIdentifier identifier, StructureStubInfo& stubInfo)
 {
-    switch (stubInfo.cacheType) {
+    switch (stubInfo.cacheType()) {
     case CacheType::GetByIdSelf:
-        return ProxyableAccessCase::create(vm, owner, Load, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
+        RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
+        return ProxyableAccessCase::create(vm, owner, Load, identifier, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
 
     case CacheType::PutByIdReplace:
-        return AccessCase::create(vm, owner, Replace, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
+        RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
+        ASSERT(!identifier.isCell());
+        return AccessCase::create(vm, owner, Replace, identifier, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
 
     case CacheType::InByIdSelf:
-        return AccessCase::create(vm, owner, InHit, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
+        RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
+        ASSERT(!identifier.isCell());
+        return AccessCase::create(vm, owner, InHit, identifier, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
 
     case CacheType::ArrayLength:
-        return AccessCase::create(vm, owner, AccessCase::ArrayLength);
+        RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
+        ASSERT(!identifier.isCell());
+        return AccessCase::create(vm, owner, AccessCase::ArrayLength, identifier);
 
     case CacheType::StringLength:
-        return AccessCase::create(vm, owner, AccessCase::StringLength);
+        RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
+        ASSERT(!identifier.isCell());
+        return AccessCase::create(vm, owner, AccessCase::StringLength, identifier);
 
     default:
         return nullptr;
     }
 }
@@ -145,25 +175,25 @@
     std::unique_ptr<AccessCase> result(new AccessCase(*this));
     result->resetState();
     return result;
 }
 
-Vector<WatchpointSet*, 2> AccessCase::commit(VM& vm, const Identifier& ident)
+Vector<WatchpointSet*, 2> AccessCase::commit(VM& vm)
 {
     // It's fine to commit something that is already committed. That arises when we switch to using
     // newly allocated watchpoints. When it happens, it's not efficient - but we think that's OK
     // because most AccessCases have no extra watchpoints anyway.
     RELEASE_ASSERT(m_state == Primordial || m_state == Committed);
 
     Vector<WatchpointSet*, 2> result;
     Structure* structure = this->structure();
 
-    if (!ident.isNull()) {
+    if (m_identifier) {
         if ((structure && structure->needImpurePropertyWatchpoint())
             || m_conditionSet.needImpurePropertyWatchpoint()
-            || (m_polyProtoAccessChain && m_polyProtoAccessChain->needImpurePropertyWatchpoint()))
-            result.append(vm.ensureWatchpointSetForImpureProperty(ident));
+            || (m_polyProtoAccessChain && m_polyProtoAccessChain->needImpurePropertyWatchpoint(vm)))
+            result.append(vm.ensureWatchpointSetForImpureProperty(m_identifier.uid()));
     }
 
     if (additionalSet())
         result.append(additionalSet());
 
@@ -178,11 +208,18 @@
     m_state = Committed;
 
     return result;
 }
 
-bool AccessCase::guardedByStructureCheck() const
+bool AccessCase::guardedByStructureCheck(const StructureStubInfo& stubInfo) const
+{
+    if (!stubInfo.hasConstantIdentifier)
+        return false;
+    return guardedByStructureCheckSkippingConstantIdentifierCheck();
+}
+
+bool AccessCase::guardedByStructureCheckSkippingConstantIdentifierCheck() const
 {
     if (viaProxy())
         return false;
 
     if (m_polyProtoAccessChain)
@@ -195,85 +232,402 @@
     case ScopedArgumentsLength:
     case ModuleNamespaceLoad:
     case InstanceOfHit:
     case InstanceOfMiss:
     case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
         return false;
     default:
         return true;
     }
 }
 
-bool AccessCase::doesCalls(Vector<JSCell*>* cellsToMark) const
+bool AccessCase::requiresIdentifierNameMatch() const
 {
-    switch (type()) {
+    switch (m_type) {
+    case Load:
+    // We don't currently have a by_val for these puts, but we do care about the identifier.
+    case Transition:
+    case Replace:
+    case Miss:
+    case GetGetter:
     case Getter:
     case Setter:
     case CustomValueGetter:
     case CustomAccessorGetter:
     case CustomValueSetter:
     case CustomAccessorSetter:
+    case IntrinsicGetter:
+    case InHit:
+    case InMiss:
+    case ArrayLength:
+    case StringLength:
+    case DirectArgumentsLength:
+    case ScopedArgumentsLength:
+    case ModuleNamespaceLoad:
         return true;
+    case InstanceOfHit:
+    case InstanceOfMiss:
+    case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
+        return false;
+    }
+    RELEASE_ASSERT_NOT_REACHED();
+}
+
+bool AccessCase::requiresInt32PropertyCheck() const
+{
+    switch (m_type) {
+    case Load:
     case Transition:
-        if (newStructure()->outOfLineCapacity() != structure()->outOfLineCapacity()
-            && structure()->couldHaveIndexingHeader()) {
-            if (cellsToMark)
-                cellsToMark->append(newStructure());
-            return true;
-        }
+    case Replace:
+    case Miss:
+    case GetGetter:
+    case Getter:
+    case Setter:
+    case CustomValueGetter:
+    case CustomAccessorGetter:
+    case CustomValueSetter:
+    case CustomAccessorSetter:
+    case IntrinsicGetter:
+    case InHit:
+    case InMiss:
+    case ArrayLength:
+    case StringLength:
+    case DirectArgumentsLength:
+    case ScopedArgumentsLength:
+    case ModuleNamespaceLoad:
+    case InstanceOfHit:
+    case InstanceOfMiss:
+    case InstanceOfGeneric:
         return false;
-    default:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
+        return true;
+    }
+    RELEASE_ASSERT_NOT_REACHED();
+}
+
+bool AccessCase::needsScratchFPR() const
+{
+    switch (m_type) {
+    case Load:
+    case Transition:
+    case Replace:
+    case Miss:
+    case GetGetter:
+    case Getter:
+    case Setter:
+    case CustomValueGetter:
+    case CustomAccessorGetter:
+    case CustomValueSetter:
+    case CustomAccessorSetter:
+    case IntrinsicGetter:
+    case InHit:
+    case InMiss:
+    case ArrayLength:
+    case StringLength:
+    case DirectArgumentsLength:
+    case ScopedArgumentsLength:
+    case ModuleNamespaceLoad:
+    case InstanceOfHit:
+    case InstanceOfMiss:
+    case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedStringLoad:
         return false;
+    case IndexedDoubleLoad:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedTypedArrayUint32Load:
+        return true;
+    }
+    RELEASE_ASSERT_NOT_REACHED();
+}
+
+template<typename Functor>
+void AccessCase::forEachDependentCell(VM& vm, const Functor& functor) const
+{
+    m_conditionSet.forEachDependentCell(functor);
+    if (m_structure)
+        functor(m_structure.get());
+    if (m_polyProtoAccessChain) {
+        for (StructureID structureID : m_polyProtoAccessChain->chain())
+            functor(vm.getStructure(structureID));
+    }
+
+    switch (type()) {
+    case Getter:
+    case Setter: {
+        auto& accessor = this->as<GetterSetterAccessCase>();
+        if (accessor.callLinkInfo())
+            accessor.callLinkInfo()->forEachDependentCell(functor);
+        break;
+    }
+    case CustomValueGetter:
+    case CustomValueSetter: {
+        auto& accessor = this->as<GetterSetterAccessCase>();
+        if (accessor.customSlotBase())
+            functor(accessor.customSlotBase());
+        break;
+    }
+    case IntrinsicGetter: {
+        auto& intrinsic = this->as<IntrinsicGetterAccessCase>();
+        if (intrinsic.intrinsicFunction())
+            functor(intrinsic.intrinsicFunction());
+        break;
+    }
+    case ModuleNamespaceLoad: {
+        auto& accessCase = this->as<ModuleNamespaceAccessCase>();
+        if (accessCase.moduleNamespaceObject())
+            functor(accessCase.moduleNamespaceObject());
+        if (accessCase.moduleEnvironment())
+            functor(accessCase.moduleEnvironment());
+        break;
+    }
+    case InstanceOfHit:
+    case InstanceOfMiss:
+        if (as<InstanceOfAccessCase>().prototype())
+            functor(as<InstanceOfAccessCase>().prototype());
+        break;
+    case CustomAccessorGetter:
+    case CustomAccessorSetter:
+    case Load:
+    case Transition:
+    case Replace:
+    case Miss:
+    case GetGetter:
+    case InHit:
+    case InMiss:
+    case ArrayLength:
+    case StringLength:
+    case DirectArgumentsLength:
+    case ScopedArgumentsLength:
+    case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
+        break;
     }
 }
 
+bool AccessCase::doesCalls(VM& vm, Vector<JSCell*>* cellsToMarkIfDoesCalls) const
+{
+    bool doesCalls = false;
+    switch (type()) {
+    case Transition:
+        doesCalls = newStructure()->outOfLineCapacity() != structure()->outOfLineCapacity() && structure()->couldHaveIndexingHeader();
+        break;
+    case Getter:
+    case Setter:
+    case CustomValueGetter:
+    case CustomAccessorGetter:
+    case CustomValueSetter:
+    case CustomAccessorSetter:
+        doesCalls = true;
+        break;
+    case Load:
+    case Replace:
+    case Miss:
+    case GetGetter:
+    case IntrinsicGetter:
+    case InHit:
+    case InMiss:
+    case ArrayLength:
+    case StringLength:
+    case DirectArgumentsLength:
+    case ScopedArgumentsLength:
+    case ModuleNamespaceLoad:
+    case InstanceOfHit:
+    case InstanceOfMiss:
+    case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
+        doesCalls = false;
+        break;
+    }
+
+    if (doesCalls && cellsToMarkIfDoesCalls) {
+        forEachDependentCell(vm, [&](JSCell* cell) {
+            cellsToMarkIfDoesCalls->append(cell);
+        });
+    }
+    return doesCalls;
+}
+
 bool AccessCase::couldStillSucceed() const
 {
-    return m_conditionSet.structuresEnsureValidityAssumingImpurePropertyWatchpoint();
+    for (const ObjectPropertyCondition& condition : m_conditionSet) {
+        if (condition.condition().kind() == PropertyCondition::Equivalence) {
+            if (!condition.isWatchableAssumingImpurePropertyWatchpoint(PropertyCondition::WatchabilityEffort::EnsureWatchability))
+                return false;
+        } else {
+            if (!condition.structureEnsuresValidityAssumingImpurePropertyWatchpoint())
+                return false;
+        }
+    }
+    return true;
 }
 
 bool AccessCase::canReplace(const AccessCase& other) const
 {
     // This puts in a good effort to try to figure out if 'other' is made superfluous by '*this'.
     // It's fine for this to return false if it's in doubt.
     //
     // Note that if A->guardedByStructureCheck() && B->guardedByStructureCheck() then
     // A->canReplace(B) == B->canReplace(A).
 
+    if (m_identifier != other.m_identifier)
+        return false;
+
     switch (type()) {
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
     case ArrayLength:
     case StringLength:
     case DirectArgumentsLength:
     case ScopedArgumentsLength:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
         return other.type() == type();
+
     case ModuleNamespaceLoad: {
         if (other.type() != type())
             return false;
         auto& thisCase = this->as<ModuleNamespaceAccessCase>();
         auto& otherCase = this->as<ModuleNamespaceAccessCase>();
         return thisCase.moduleNamespaceObject() == otherCase.moduleNamespaceObject();
     }
+
     case InstanceOfHit:
     case InstanceOfMiss: {
         if (other.type() != type())
             return false;
 
         if (this->as<InstanceOfAccessCase>().prototype() != other.as<InstanceOfAccessCase>().prototype())
             return false;
 
         return structure() == other.structure();
     }
+
     case InstanceOfGeneric:
         switch (other.type()) {
         case InstanceOfGeneric:
         case InstanceOfHit:
         case InstanceOfMiss:
             return true;
         default:
             return false;
         }
-    default:
+
+    case Load:
+    case Transition:
+    case Replace:
+    case Miss:
+    case GetGetter:
+    case Getter:
+    case Setter:
+    case CustomValueGetter:
+    case CustomAccessorGetter:
+    case CustomValueSetter:
+    case CustomAccessorSetter:
+    case IntrinsicGetter:
+    case InHit:
+    case InMiss:
         if (other.type() != type())
             return false;
 
         if (m_polyProtoAccessChain) {
             if (!other.m_polyProtoAccessChain)
@@ -283,29 +637,29 @@
             // this to check the base structure.
             return structure() == other.structure()
                 && *m_polyProtoAccessChain == *other.m_polyProtoAccessChain;
         }
 
-        if (!guardedByStructureCheck() || !other.guardedByStructureCheck())
+        if (!guardedByStructureCheckSkippingConstantIdentifierCheck() || !other.guardedByStructureCheckSkippingConstantIdentifierCheck())
             return false;
 
         return structure() == other.structure();
     }
+    RELEASE_ASSERT_NOT_REACHED();
 }
 
 void AccessCase::dump(PrintStream& out) const
 {
     out.print("\n", m_type, ":(");
 
     CommaPrinter comma;
 
     out.print(comma, m_state);
 
+    out.print(comma, "ident = '", m_identifier, "'");
     if (isValidOffset(m_offset))
         out.print(comma, "offset = ", m_offset);
-    if (!m_conditionSet.isEmpty())
-        out.print(comma, "conditions = ", m_conditionSet);
 
     if (m_polyProtoAccessChain) {
         out.print(comma, "prototype access chain = ");
         m_polyProtoAccessChain->dump(structure(), out);
     } else {
@@ -313,60 +667,42 @@
             out.print(comma, "structure = ", pointerDump(structure()), " -> ", pointerDump(newStructure()));
         else if (m_structure)
             out.print(comma, "structure = ", pointerDump(m_structure.get()));
     }
 
+    if (!m_conditionSet.isEmpty())
+        out.print(comma, "conditions = ", m_conditionSet);
+
     dumpImpl(out, comma);
     out.print(")");
 }
 
 bool AccessCase::visitWeak(VM& vm) const
 {
-    if (m_structure && !vm.heap.isMarked(m_structure.get()))
-        return false;
-    if (m_polyProtoAccessChain) {
-        for (Structure* structure : m_polyProtoAccessChain->chain()) {
-            if (!vm.heap.isMarked(structure))
-                return false;
-        }
-    }
-    if (!m_conditionSet.areStillLive(vm))
-        return false;
     if (isAccessor()) {
         auto& accessor = this->as<GetterSetterAccessCase>();
         if (accessor.callLinkInfo())
             accessor.callLinkInfo()->visitWeak(vm);
-        if (accessor.customSlotBase() && !vm.heap.isMarked(accessor.customSlotBase()))
-            return false;
-    } else if (type() == IntrinsicGetter) {
-        auto& intrinsic = this->as<IntrinsicGetterAccessCase>();
-        if (intrinsic.intrinsicFunction() && !vm.heap.isMarked(intrinsic.intrinsicFunction()))
-            return false;
-    } else if (type() == ModuleNamespaceLoad) {
-        auto& accessCase = this->as<ModuleNamespaceAccessCase>();
-        if (accessCase.moduleNamespaceObject() && !vm.heap.isMarked(accessCase.moduleNamespaceObject()))
-            return false;
-        if (accessCase.moduleEnvironment() && !vm.heap.isMarked(accessCase.moduleEnvironment()))
-            return false;
-    } else if (type() == InstanceOfHit || type() == InstanceOfMiss) {
-        if (as<InstanceOfAccessCase>().prototype() && !vm.heap.isMarked(as<InstanceOfAccessCase>().prototype()))
-            return false;
     }
 
-    return true;
+    bool isValid = true;
+    forEachDependentCell(vm, [&](JSCell* cell) {
+        isValid &= vm.heap.isMarked(cell);
+    });
+    return isValid;
 }
 
 bool AccessCase::propagateTransitions(SlotVisitor& visitor) const
 {
     bool result = true;
 
     if (m_structure)
         result &= m_structure->markIfCheap(visitor);
 
     if (m_polyProtoAccessChain) {
-        for (Structure* structure : m_polyProtoAccessChain->chain())
-            result &= structure->markIfCheap(visitor);
+        for (StructureID structureID : m_polyProtoAccessChain->chain())
+            result &= visitor.vm().getStructure(structureID)->markIfCheap(visitor);
     }
 
     switch (m_type) {
     case Transition:
         if (visitor.vm().heap.isMarked(m_structure->previousID()))
@@ -379,33 +715,49 @@
     }
 
     return result;
 }
 
+void AccessCase::visitAggregate(SlotVisitor& visitor) const
+{
+    m_identifier.visitAggregate(visitor);
+}
+
 void AccessCase::generateWithGuard(
     AccessGenerationState& state, CCallHelpers::JumpList& fallThrough)
 {
     SuperSamplerScope superSamplerScope(false);
 
+    checkConsistency(*state.stubInfo);
+
     RELEASE_ASSERT(m_state == Committed);
     m_state = Generated;
 
     CCallHelpers& jit = *state.jit;
     StructureStubInfo& stubInfo = *state.stubInfo;
     VM& vm = state.m_vm;
     JSValueRegs valueRegs = state.valueRegs;
     GPRReg baseGPR = state.baseGPR;
-    GPRReg thisGPR = state.thisGPR != InvalidGPRReg ? state.thisGPR : baseGPR;
     GPRReg scratchGPR = state.scratchGPR;
 
-    UNUSED_PARAM(vm);
+    if (requiresIdentifierNameMatch() && !stubInfo.hasConstantIdentifier) {
+        RELEASE_ASSERT(m_identifier);
+        GPRReg propertyGPR = state.u.propertyGPR;
+        // non-rope string check done inside polymorphic access.
+
+        if (uid()->isSymbol())
+            jit.loadPtr(MacroAssembler::Address(propertyGPR, Symbol::offsetOfSymbolImpl()), scratchGPR);
+        else
+            jit.loadPtr(MacroAssembler::Address(propertyGPR, JSString::offsetOfValue()), scratchGPR);
+        fallThrough.append(jit.branchPtr(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImmPtr(uid())));
+    }
 
     auto emitDefaultGuard = [&] () {
         if (m_polyProtoAccessChain) {
             GPRReg baseForAccessGPR = state.scratchGPR;
             jit.move(state.baseGPR, baseForAccessGPR);
-            m_polyProtoAccessChain->forEach(structure(), [&] (Structure* structure, bool atEnd) {
+            m_polyProtoAccessChain->forEach(vm, structure(), [&] (Structure* structure, bool atEnd) {
                 fallThrough.append(
                     jit.branchStructure(
                         CCallHelpers::NotEqual,
                         CCallHelpers::Address(baseForAccessGPR, JSCell::structureIDOffset()),
                         structure));
@@ -415,11 +767,11 @@
                         // Transitions must do this because they need to verify there isn't a setter in the chain.
                         // Miss/InMiss need to do this to ensure there isn't a new item at the end of the chain that
                         // has the property.
 #if USE(JSVALUE64)
                         jit.load64(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset)), baseForAccessGPR);
-                        fallThrough.append(jit.branch64(CCallHelpers::NotEqual, baseForAccessGPR, CCallHelpers::TrustedImm64(ValueNull)));
+                        fallThrough.append(jit.branch64(CCallHelpers::NotEqual, baseForAccessGPR, CCallHelpers::TrustedImm64(JSValue::ValueNull)));
 #else
                         jit.load32(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset) + PayloadOffset), baseForAccessGPR);
                         fallThrough.append(jit.branchTestPtr(CCallHelpers::NonZero, baseForAccessGPR));
 #endif
                     }
@@ -430,11 +782,11 @@
                         jit.move(CCallHelpers::TrustedImmPtr(asObject(prototype)), baseForAccessGPR);
                     } else {
                         RELEASE_ASSERT(structure->isObject()); // Primitives must have a stored prototype. We use prototypeForLookup for them.
 #if USE(JSVALUE64)
                         jit.load64(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset)), baseForAccessGPR);
-                        fallThrough.append(jit.branch64(CCallHelpers::Equal, baseForAccessGPR, CCallHelpers::TrustedImm64(ValueNull)));
+                        fallThrough.append(jit.branch64(CCallHelpers::Equal, baseForAccessGPR, CCallHelpers::TrustedImm64(JSValue::ValueNull)));
 #else
                         jit.load32(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset) + PayloadOffset), baseForAccessGPR);
                         fallThrough.append(jit.branchTestPtr(CCallHelpers::Zero, baseForAccessGPR));
 #endif
                     }
@@ -504,19 +856,16 @@
     case ScopedArgumentsLength: {
         ASSERT(!viaProxy());
         fallThrough.append(
             jit.branchIfNotType(baseGPR, ScopedArgumentsType));
 
-        jit.loadPtr(
-            CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfStorage()),
-            scratchGPR);
         fallThrough.append(
             jit.branchTest8(
                 CCallHelpers::NonZero,
-                CCallHelpers::Address(scratchGPR, ScopedArguments::offsetOfOverrodeThingsInStorage())));
+                CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfOverrodeThings())));
         jit.load32(
-            CCallHelpers::Address(scratchGPR, ScopedArguments::offsetOfTotalLengthInStorage()),
+            CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfTotalLength()),
             valueRegs.payloadGPR());
         jit.boxInt32(valueRegs.payloadGPR(), valueRegs);
         state.succeed();
         return;
     }
@@ -524,35 +873,375 @@
     case ModuleNamespaceLoad: {
         this->as<ModuleNamespaceAccessCase>().emit(state, fallThrough);
         return;
     }
 
+    case IndexedScopedArgumentsLoad: {
+        // This code is written such that the result could alias with the base or the property.
+        GPRReg propertyGPR = state.u.propertyGPR;
+
+        jit.load8(CCallHelpers::Address(baseGPR, JSCell::typeInfoTypeOffset()), scratchGPR);
+        fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(ScopedArgumentsType)));
+
+        ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+        allocator.lock(stubInfo.baseRegs());
+        allocator.lock(valueRegs);
+        allocator.lock(stubInfo.propertyRegs());
+        allocator.lock(scratchGPR);
+
+        GPRReg scratch2GPR = allocator.allocateScratchGPR();
+        GPRReg scratch3GPR = allocator.allocateScratchGPR();
+
+        ScratchRegisterAllocator::PreservedState preservedState = allocator.preserveReusedRegistersByPushing(
+            jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
+
+        CCallHelpers::JumpList failAndIgnore;
+
+        failAndIgnore.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfTotalLength())));
+
+        jit.loadPtr(CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfTable()), scratchGPR);
+        jit.load32(CCallHelpers::Address(scratchGPR, ScopedArgumentsTable::offsetOfLength()), scratch2GPR);
+        auto overflowCase = jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratch2GPR);
+
+        jit.loadPtr(CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfScope()), scratch2GPR);
+        jit.loadPtr(CCallHelpers::Address(scratchGPR, ScopedArgumentsTable::offsetOfArguments()), scratchGPR);
+        jit.zeroExtend32ToPtr(propertyGPR, scratch3GPR);
+        jit.load32(CCallHelpers::BaseIndex(scratchGPR, scratch3GPR, CCallHelpers::TimesFour), scratchGPR);
+        failAndIgnore.append(jit.branch32(CCallHelpers::Equal, scratchGPR, CCallHelpers::TrustedImm32(ScopeOffset::invalidOffset)));
+        jit.loadValue(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesEight, JSLexicalEnvironment::offsetOfVariables()), valueRegs);
+        auto done = jit.jump();
+
+        overflowCase.link(&jit);
+        jit.sub32(propertyGPR, scratch2GPR);
+        jit.neg32(scratch2GPR);
+        jit.loadPtr(CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfStorage()), scratch3GPR);
+#if USE(JSVALUE64)
+        jit.loadValue(CCallHelpers::BaseIndex(scratch3GPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratchGPR));
+        failAndIgnore.append(jit.branchIfEmpty(scratchGPR));
+        jit.move(scratchGPR, valueRegs.payloadGPR());
+#else
+        jit.loadValue(CCallHelpers::BaseIndex(scratch3GPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratch2GPR, scratchGPR));
+        failAndIgnore.append(jit.branchIfEmpty(scratch2GPR));
+        jit.move(scratchGPR, valueRegs.payloadGPR());
+        jit.move(scratch2GPR, valueRegs.tagGPR());
+#endif
+
+        done.link(&jit);
+
+        allocator.restoreReusedRegistersByPopping(jit, preservedState);
+        state.succeed();
+
+        if (allocator.didReuseRegisters()) {
+            failAndIgnore.link(&jit);
+            allocator.restoreReusedRegistersByPopping(jit, preservedState);
+            state.failAndIgnore.append(jit.jump());
+        } else
+            state.failAndIgnore.append(failAndIgnore);
+
+        return;
+    }
+
+    case IndexedDirectArgumentsLoad: {
+        // This code is written such that the result could alias with the base or the property.
+        GPRReg propertyGPR = state.u.propertyGPR;
+        jit.load8(CCallHelpers::Address(baseGPR, JSCell::typeInfoTypeOffset()), scratchGPR);
+        fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(DirectArgumentsType)));
+
+        jit.load32(CCallHelpers::Address(baseGPR, DirectArguments::offsetOfLength()), scratchGPR);
+        state.failAndRepatch.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratchGPR));
+        state.failAndRepatch.append(jit.branchTestPtr(CCallHelpers::NonZero, CCallHelpers::Address(baseGPR, DirectArguments::offsetOfMappedArguments())));
+        jit.zeroExtend32ToPtr(propertyGPR, scratchGPR);
+        jit.loadValue(CCallHelpers::BaseIndex(baseGPR, scratchGPR, CCallHelpers::TimesEight, DirectArguments::storageOffset()), valueRegs);
+        state.succeed();
+        return;
+    }
+
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load: {
+        // This code is written such that the result could alias with the base or the property.
+
+        TypedArrayType type = toTypedArrayType(m_type);
+
+        GPRReg propertyGPR = state.u.propertyGPR;
+
+
+        jit.load8(CCallHelpers::Address(baseGPR, JSCell::typeInfoTypeOffset()), scratchGPR);
+        fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(typeForTypedArrayType(type))));
+
+        jit.load32(CCallHelpers::Address(baseGPR, JSArrayBufferView::offsetOfLength()), scratchGPR);
+        state.failAndRepatch.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratchGPR));
+
+        ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+        allocator.lock(stubInfo.baseRegs());
+        allocator.lock(valueRegs);
+        allocator.lock(stubInfo.propertyRegs());
+        allocator.lock(scratchGPR);
+        GPRReg scratch2GPR = allocator.allocateScratchGPR();
+
+        ScratchRegisterAllocator::PreservedState preservedState = allocator.preserveReusedRegistersByPushing(
+            jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
+
+        jit.loadPtr(CCallHelpers::Address(baseGPR, JSArrayBufferView::offsetOfVector()), scratch2GPR);
+        jit.cageConditionally(Gigacage::Primitive, scratch2GPR, scratchGPR, scratchGPR);
+
+        jit.signExtend32ToPtr(propertyGPR, scratchGPR);
+        if (isInt(type)) {
+            switch (elementSize(type)) {
+            case 1:
+                if (JSC::isSigned(type))
+                    jit.load8SignedExtendTo32(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesOne), valueRegs.payloadGPR());
+                else
+                    jit.load8(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesOne), valueRegs.payloadGPR());
+                break;
+            case 2:
+                if (JSC::isSigned(type))
+                    jit.load16SignedExtendTo32(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesTwo), valueRegs.payloadGPR());
+                else
+                    jit.load16(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesTwo), valueRegs.payloadGPR());
+                break;
+            case 4:
+                jit.load32(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesFour), valueRegs.payloadGPR());
+                break;
+            default:
+                CRASH();
+            }
+
+            CCallHelpers::Jump done;
+            if (type == TypeUint32) {
+                RELEASE_ASSERT(state.scratchFPR != InvalidFPRReg);
+                auto canBeInt = jit.branch32(CCallHelpers::GreaterThanOrEqual, valueRegs.payloadGPR(), CCallHelpers::TrustedImm32(0));
+
+                jit.convertInt32ToDouble(valueRegs.payloadGPR(), state.scratchFPR);
+                jit.addDouble(CCallHelpers::AbsoluteAddress(&CCallHelpers::twoToThe32), state.scratchFPR);
+                jit.boxDouble(state.scratchFPR, valueRegs);
+                done = jit.jump();
+                canBeInt.link(&jit);
+            }
+
+            jit.boxInt32(valueRegs.payloadGPR(), valueRegs);
+            if (done.isSet())
+                done.link(&jit);
+        } else {
+            ASSERT(isFloat(type));
+            RELEASE_ASSERT(state.scratchFPR != InvalidFPRReg);
+            switch (elementSize(type)) {
+            case 4:
+                jit.loadFloat(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesFour), state.scratchFPR);
+                jit.convertFloatToDouble(state.scratchFPR, state.scratchFPR);
+                break;
+            case 8: {
+                jit.loadDouble(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesEight), state.scratchFPR);
+                break;
+            }
+            default:
+                CRASH();
+            }
+
+            jit.purifyNaN(state.scratchFPR);
+            jit.boxDouble(state.scratchFPR, valueRegs);
+        }
+
+        allocator.restoreReusedRegistersByPopping(jit, preservedState);
+        state.succeed();
+
+        return;
+    }
+
+    case IndexedStringLoad: {
+        // This code is written such that the result could alias with the base or the property.
+        GPRReg propertyGPR = state.u.propertyGPR;
+
+        fallThrough.append(jit.branchIfNotString(baseGPR));
+
+        ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+        allocator.lock(stubInfo.baseRegs());
+        allocator.lock(valueRegs);
+        allocator.lock(stubInfo.propertyRegs());
+        allocator.lock(scratchGPR);
+        GPRReg scratch2GPR = allocator.allocateScratchGPR();
+
+        CCallHelpers::JumpList failAndIgnore;
+
+        ScratchRegisterAllocator::PreservedState preservedState = allocator.preserveReusedRegistersByPushing(
+            jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
+
+        jit.loadPtr(CCallHelpers::Address(baseGPR, JSString::offsetOfValue()), scratch2GPR);
+        failAndIgnore.append(jit.branchIfRopeStringImpl(scratch2GPR));
+        jit.load32(CCallHelpers::Address(scratch2GPR, StringImpl::lengthMemoryOffset()), scratchGPR);
+
+        failAndIgnore.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratchGPR));
+
+        jit.load32(CCallHelpers::Address(scratch2GPR, StringImpl::flagsOffset()), scratchGPR);
+        jit.loadPtr(CCallHelpers::Address(scratch2GPR, StringImpl::dataOffset()), scratch2GPR);
+        auto is16Bit = jit.branchTest32(CCallHelpers::Zero, scratchGPR, CCallHelpers::TrustedImm32(StringImpl::flagIs8Bit()));
+        jit.zeroExtend32ToPtr(propertyGPR, scratchGPR);
+        jit.load8(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesOne, 0), scratch2GPR);
+        auto is8BitLoadDone = jit.jump();
+        is16Bit.link(&jit);
+        jit.zeroExtend32ToPtr(propertyGPR, scratchGPR);
+        jit.load16(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesTwo, 0), scratch2GPR);
+        is8BitLoadDone.link(&jit);
+
+        failAndIgnore.append(jit.branch32(CCallHelpers::Above, scratch2GPR, CCallHelpers::TrustedImm32(maxSingleCharacterString)));
+        jit.move(CCallHelpers::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), scratchGPR);
+        jit.loadPtr(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::ScalePtr, 0), valueRegs.payloadGPR());
+        jit.boxCell(valueRegs.payloadGPR(), valueRegs);
+        allocator.restoreReusedRegistersByPopping(jit, preservedState);
+        state.succeed();
+
+        if (allocator.didReuseRegisters()) {
+            failAndIgnore.link(&jit);
+            allocator.restoreReusedRegistersByPopping(jit, preservedState);
+            state.failAndIgnore.append(jit.jump());
+        } else
+            state.failAndIgnore.append(failAndIgnore);
+
+        return;
+    }
+
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad: {
+        // This code is written such that the result could alias with the base or the property.
+        GPRReg propertyGPR = state.u.propertyGPR;
+
+        // int32 check done in polymorphic access.
+        jit.load8(CCallHelpers::Address(baseGPR, JSCell::indexingTypeAndMiscOffset()), scratchGPR);
+        jit.and32(CCallHelpers::TrustedImm32(IndexingShapeMask), scratchGPR);
+
+        CCallHelpers::Jump isOutOfBounds;
+        CCallHelpers::Jump isEmpty;
+
+        ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+        allocator.lock(stubInfo.baseRegs());
+        allocator.lock(valueRegs);
+        allocator.lock(stubInfo.propertyRegs());
+        allocator.lock(scratchGPR);
+        GPRReg scratch2GPR = allocator.allocateScratchGPR();
+#if USE(JSVALUE32_64)
+        GPRReg scratch3GPR = allocator.allocateScratchGPR();
+#endif
+        ScratchRegisterAllocator::PreservedState preservedState;
+
+        CCallHelpers::JumpList failAndIgnore;
+        auto preserveReusedRegisters = [&] {
+            preservedState = allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
+        };
+
+        if (m_type == IndexedArrayStorageLoad) {
+            jit.add32(CCallHelpers::TrustedImm32(-ArrayStorageShape), scratchGPR, scratchGPR);
+            fallThrough.append(jit.branch32(CCallHelpers::Above, scratchGPR, CCallHelpers::TrustedImm32(SlowPutArrayStorageShape - ArrayStorageShape)));
+
+            preserveReusedRegisters();
+
+            jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
+            isOutOfBounds = jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, CCallHelpers::Address(scratchGPR, ArrayStorage::vectorLengthOffset()));
+
+            jit.zeroExtend32ToPtr(propertyGPR, scratch2GPR);
+#if USE(JSVALUE64)
+            jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight, ArrayStorage::vectorOffset()), JSValueRegs(scratchGPR));
+            isEmpty = jit.branchIfEmpty(scratchGPR);
+            jit.move(scratchGPR, valueRegs.payloadGPR());
+#else
+            jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight, ArrayStorage::vectorOffset()), JSValueRegs(scratch3GPR, scratchGPR));
+            isEmpty = jit.branchIfEmpty(scratch3GPR);
+            jit.move(scratchGPR, valueRegs.payloadGPR());
+            jit.move(scratch3GPR, valueRegs.tagGPR());
+#endif
+        } else {
+            IndexingType expectedShape;
+            switch (m_type) {
+            case IndexedInt32Load:
+                expectedShape = Int32Shape;
+                break;
+            case IndexedDoubleLoad:
+                expectedShape = DoubleShape;
+                break;
+            case IndexedContiguousLoad:
+                expectedShape = ContiguousShape;
+                break;
+            default:
+                RELEASE_ASSERT_NOT_REACHED();
+                break;
+            }
+
+            fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(expectedShape)));
+
+            preserveReusedRegisters();
+
+            jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
+            isOutOfBounds = jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, CCallHelpers::Address(scratchGPR, Butterfly::offsetOfPublicLength()));
+            jit.zeroExtend32ToPtr(propertyGPR, scratch2GPR);
+            if (m_type == IndexedDoubleLoad) {
+                RELEASE_ASSERT(state.scratchFPR != InvalidFPRReg);
+                jit.loadDouble(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight), state.scratchFPR);
+                isEmpty = jit.branchIfNaN(state.scratchFPR);
+                jit.boxDouble(state.scratchFPR, valueRegs);
+            } else {
+#if USE(JSVALUE64)
+                jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratchGPR));
+                isEmpty = jit.branchIfEmpty(scratchGPR);
+                jit.move(scratchGPR, valueRegs.payloadGPR());
+#else
+                jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratch3GPR, scratchGPR));
+                isEmpty = jit.branchIfEmpty(scratch3GPR);
+                jit.move(scratchGPR, valueRegs.payloadGPR());
+                jit.move(scratch3GPR, valueRegs.tagGPR());
+#endif
+            }
+        }
+
+        allocator.restoreReusedRegistersByPopping(jit, preservedState);
+        state.succeed();
+
+        if (allocator.didReuseRegisters()) {
+            isOutOfBounds.link(&jit);
+            isEmpty.link(&jit);
+            allocator.restoreReusedRegistersByPopping(jit, preservedState);
+            state.failAndIgnore.append(jit.jump());
+        } else {
+            state.failAndIgnore.append(isOutOfBounds);
+            state.failAndIgnore.append(isEmpty);
+        }
+
+        return;
+    }
+
     case InstanceOfHit:
     case InstanceOfMiss:
         emitDefaultGuard();
 
         fallThrough.append(
             jit.branchPtr(
-                CCallHelpers::NotEqual, thisGPR,
+                CCallHelpers::NotEqual, state.u.prototypeGPR,
                 CCallHelpers::TrustedImmPtr(as<InstanceOfAccessCase>().prototype())));
         break;
 
     case InstanceOfGeneric: {
-        // Legend: value = `base instanceof this`.
+        GPRReg prototypeGPR = state.u.prototypeGPR;
+        // Legend: value = `base instanceof prototypeGPR`.
 
         GPRReg valueGPR = valueRegs.payloadGPR();
 
-        ScratchRegisterAllocator allocator(stubInfo.patch.usedRegisters);
-        allocator.lock(baseGPR);
-        allocator.lock(valueGPR);
-        allocator.lock(thisGPR);
+        ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+        allocator.lock(stubInfo.baseRegs());
+        allocator.lock(valueRegs);
+        allocator.lock(stubInfo.propertyRegs());
         allocator.lock(scratchGPR);
 
         GPRReg scratch2GPR = allocator.allocateScratchGPR();
 
         if (!state.stubInfo->prototypeIsKnownObject)
-            state.failAndIgnore.append(jit.branchIfNotObject(thisGPR));
+            state.failAndIgnore.append(jit.branchIfNotObject(prototypeGPR));
 
         ScratchRegisterAllocator::PreservedState preservedState =
             allocator.preserveReusedRegistersByPushing(
                 jit,
                 ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
@@ -586,11 +1275,11 @@
             scratch2GPR);
         hasMonoProto.link(&jit);
 #endif
         jit.move(scratch2GPR, valueGPR);
 
-        CCallHelpers::Jump isInstance = jit.branchPtr(CCallHelpers::Equal, valueGPR, thisGPR);
+        CCallHelpers::Jump isInstance = jit.branchPtr(CCallHelpers::Equal, valueGPR, prototypeGPR);
 
 #if USE(JSVALUE64)
         jit.branchIfCell(JSValueRegs(valueGPR)).linkTo(loop, &jit);
 #else
         jit.branchTestPtr(CCallHelpers::NonZero, valueGPR).linkTo(loop, &jit);
@@ -623,12 +1312,15 @@
 }
 
 void AccessCase::generate(AccessGenerationState& state)
 {
     RELEASE_ASSERT(m_state == Committed);
+    RELEASE_ASSERT(state.stubInfo->hasConstantIdentifier);
     m_state = Generated;
 
+    checkConsistency(*state.stubInfo);
+
     generateImpl(state);
 }
 
 void AccessCase::generateImpl(AccessGenerationState& state)
 {
@@ -640,38 +1332,37 @@
 
     CCallHelpers& jit = *state.jit;
     VM& vm = state.m_vm;
     CodeBlock* codeBlock = jit.codeBlock();
     StructureStubInfo& stubInfo = *state.stubInfo;
-    const Identifier& ident = *state.ident;
     JSValueRegs valueRegs = state.valueRegs;
     GPRReg baseGPR = state.baseGPR;
-    GPRReg thisGPR = state.thisGPR != InvalidGPRReg ? state.thisGPR : baseGPR;
+    GPRReg thisGPR = stubInfo.thisValueIsInThisGPR() ? state.u.thisGPR : baseGPR;
     GPRReg scratchGPR = state.scratchGPR;
 
-    ASSERT(m_conditionSet.structuresEnsureValidityAssumingImpurePropertyWatchpoint());
-
     for (const ObjectPropertyCondition& condition : m_conditionSet) {
         RELEASE_ASSERT(!m_polyProtoAccessChain);
 
-        Structure* structure = condition.object()->structure(vm);
-
-        if (condition.isWatchableAssumingImpurePropertyWatchpoint()) {
-            structure->addTransitionWatchpoint(state.addWatchpoint(condition));
+        if (condition.isWatchableAssumingImpurePropertyWatchpoint(PropertyCondition::WatchabilityEffort::EnsureWatchability)) {
+            state.installWatchpoint(condition);
             continue;
         }
 
-        if (!condition.structureEnsuresValidityAssumingImpurePropertyWatchpoint(structure)) {
+        // For now, we only allow equivalence when it's watchable.
+        RELEASE_ASSERT(condition.condition().kind() != PropertyCondition::Equivalence);
+
+        if (!condition.structureEnsuresValidityAssumingImpurePropertyWatchpoint()) {
             // The reason why this cannot happen is that we require that PolymorphicAccess calls
             // AccessCase::generate() only after it has verified that
             // AccessCase::couldStillSucceed() returned true.
 
             dataLog("This condition is no longer met: ", condition, "\n");
             RELEASE_ASSERT_NOT_REACHED();
         }
 
         // We will emit code that has a weak reference that isn't otherwise listed anywhere.
+        Structure* structure = condition.object()->structure(vm);
         state.weakReferences.append(WriteBarrier<JSCell>(vm, codeBlock, structure));
 
         jit.move(CCallHelpers::TrustedImmPtr(condition.object()), scratchGPR);
         state.failAndRepatch.append(
             jit.branchStructure(
@@ -827,11 +1518,11 @@
             state.restoreLiveRegistersFromStackForCall(spillState, dontRestore);
         };
 
         jit.store32(
             CCallHelpers::TrustedImm32(state.callSiteIndexForExceptionHandlingOrOriginal().bits()),
-            CCallHelpers::tagFor(static_cast<VirtualRegister>(CallFrameSlot::argumentCount)));
+            CCallHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));
 
         if (m_type == Getter || m_type == Setter) {
             auto& access = this->as<GetterSetterAccessCase>();
             ASSERT(baseGPR != loadedValueGPR);
             ASSERT(m_type != Setter || valueRegsPayloadGPR != loadedValueGPR);
@@ -851,24 +1542,24 @@
             // shrink it after.
 
             state.setSpillStateForJSGetterSetter(spillState);
 
             RELEASE_ASSERT(!access.callLinkInfo());
-            access.m_callLinkInfo = makeUnique<CallLinkInfo>();
+            CallLinkInfo* callLinkInfo = state.m_callLinkInfos.add();
+            access.m_callLinkInfo = callLinkInfo;
 
             // FIXME: If we generated a polymorphic call stub that jumped back to the getter
             // stub, which then jumped back to the main code, then we'd have a reachability
             // situation that the GC doesn't know about. The GC would ensure that the polymorphic
             // call stub stayed alive, and it would ensure that the main code stayed alive, but
             // it wouldn't know that the getter stub was alive. Ideally JIT stub routines would
             // be GC objects, and then we'd be able to say that the polymorphic call stub has a
             // reference to the getter stub.
             // https://bugs.webkit.org/show_bug.cgi?id=148914
-            access.callLinkInfo()->disallowStubs();
+            callLinkInfo->disallowStubs();
 
-            access.callLinkInfo()->setUpCall(
-                CallLinkInfo::Call, stubInfo.codeOrigin, loadedValueGPR);
+            callLinkInfo->setUpCall(CallLinkInfo::Call, stubInfo.codeOrigin, loadedValueGPR);
 
             CCallHelpers::JumpList done;
 
             // There is a "this" argument.
             unsigned numberOfParameters = 1;
@@ -888,11 +1579,12 @@
             }
 
             CCallHelpers::Jump returnUndefined = jit.branchTestPtr(
                 CCallHelpers::Zero, loadedValueGPR);
 
-            unsigned numberOfRegsForCall = CallFrame::headerSizeInRegisters + numberOfParameters;
+            unsigned numberOfRegsForCall = CallFrame::headerSizeInRegisters + roundArgumentCountToAlignFrame(numberOfParameters);
+            ASSERT(!(numberOfRegsForCall % stackAlignmentRegisters()));
             unsigned numberOfBytesForCall = numberOfRegsForCall * sizeof(Register) - sizeof(CallerFrameAndPC);
 
             unsigned alignedNumberOfBytesForCall =
             WTF::roundUpToMultipleOf(stackAlignmentBytes(), numberOfBytesForCall);
 
@@ -904,24 +1596,24 @@
                 CCallHelpers::stackPointerRegister,
                 -static_cast<ptrdiff_t>(sizeof(CallerFrameAndPC)));
 
             jit.store32(
                 CCallHelpers::TrustedImm32(numberOfParameters),
-                calleeFrame.withOffset(CallFrameSlot::argumentCount * sizeof(Register) + PayloadOffset));
+                calleeFrame.withOffset(CallFrameSlot::argumentCountIncludingThis * sizeof(Register) + PayloadOffset));
 
             jit.storeCell(
                 loadedValueGPR, calleeFrame.withOffset(CallFrameSlot::callee * sizeof(Register)));
 
             jit.storeCell(
                 thisGPR,
-                calleeFrame.withOffset(virtualRegisterForArgument(0).offset() * sizeof(Register)));
+                calleeFrame.withOffset(virtualRegisterForArgumentIncludingThis(0).offset() * sizeof(Register)));
 
             if (m_type == Setter) {
                 jit.storeValue(
                     valueRegs,
                     calleeFrame.withOffset(
-                        virtualRegisterForArgument(1).offset() * sizeof(Register)));
+                        virtualRegisterForArgumentIncludingThis(1).offset() * sizeof(Register)));
             }
 
             CCallHelpers::Jump slowCase = jit.branchPtrWithPatch(
                 CCallHelpers::NotEqual, loadedValueGPR, addressOfLinkFunctionCheck,
                 CCallHelpers::TrustedImmPtr(nullptr));
@@ -929,17 +1621,20 @@
             fastPathCall = jit.nearCall();
             if (m_type == Getter)
                 jit.setupResults(valueRegs);
             done.append(jit.jump());
 
+            // FIXME: Revisit JSGlobalObject.
+            // https://bugs.webkit.org/show_bug.cgi?id=203204
             slowCase.link(&jit);
             jit.move(loadedValueGPR, GPRInfo::regT0);
 #if USE(JSVALUE32_64)
             // We *always* know that the getter/setter, if non-null, is a cell.
             jit.move(CCallHelpers::TrustedImm32(JSValue::CellTag), GPRInfo::regT1);
 #endif
             jit.move(CCallHelpers::TrustedImmPtr(access.callLinkInfo()), GPRInfo::regT2);
+            jit.move(CCallHelpers::TrustedImmPtr(state.m_globalObject), GPRInfo::regT3);
             slowPathCall = jit.nearCall();
             if (m_type == Getter)
                 jit.setupResults(valueRegs);
             done.append(jit.jump());
 
@@ -973,22 +1668,27 @@
             jit.makeSpaceOnStackForCCall();
 
             // Check if it is a super access
             GPRReg baseForCustomGetGPR = baseGPR != thisGPR ? thisGPR : baseForGetGPR;
 
-            // getter: EncodedJSValue (*GetValueFunc)(ExecState*, EncodedJSValue thisValue, PropertyName);
-            // setter: void (*PutValueFunc)(ExecState*, EncodedJSValue thisObject, EncodedJSValue value);
+            // getter: EncodedJSValue (*GetValueFunc)(JSGlobalObject*, EncodedJSValue thisValue, PropertyName);
+            // setter: void (*PutValueFunc)(JSGlobalObject*, EncodedJSValue thisObject, EncodedJSValue value);
             // Custom values are passed the slotBase (the property holder), custom accessors are passed the thisVaule (reciever).
             // FIXME: Remove this differences in custom values and custom accessors.
             // https://bugs.webkit.org/show_bug.cgi?id=158014
             GPRReg baseForCustom = m_type == CustomValueGetter || m_type == CustomValueSetter ? baseForAccessGPR : baseForCustomGetGPR;
+            // FIXME: Revisit JSGlobalObject.
+            // https://bugs.webkit.org/show_bug.cgi?id=203204
             if (m_type == CustomValueGetter || m_type == CustomAccessorGetter) {
+                RELEASE_ASSERT(m_identifier);
                 jit.setupArguments<PropertySlot::GetValueFunc>(
+                    CCallHelpers::TrustedImmPtr(codeBlock->globalObject()),
                     CCallHelpers::CellValue(baseForCustom),
-                    CCallHelpers::TrustedImmPtr(ident.impl()));
+                    CCallHelpers::TrustedImmPtr(uid()));
             } else {
                 jit.setupArguments<PutPropertySlot::PutValueFunc>(
+                    CCallHelpers::TrustedImmPtr(codeBlock->globalObject()),
                     CCallHelpers::CellValue(baseForCustom),
                     valueRegs);
             }
             jit.storePtr(GPRInfo::callFrameRegister, &vm.topCallFrame);
 
@@ -1042,15 +1742,12 @@
         // exactly when this would make calls.
         bool allocating = newStructure()->outOfLineCapacity() != structure()->outOfLineCapacity();
         bool reallocating = allocating && structure()->outOfLineCapacity();
         bool allocatingInline = allocating && !structure()->couldHaveIndexingHeader();
 
-        ScratchRegisterAllocator allocator(stubInfo.patch.usedRegisters);
-        allocator.lock(baseGPR);
-#if USE(JSVALUE32_64)
-        allocator.lock(stubInfo.patch.baseTagGPR);
-#endif
+        ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
+        allocator.lock(stubInfo.baseRegs());
         allocator.lock(valueRegs);
         allocator.lock(scratchGPR);
 
         GPRReg scratchGPR2 = InvalidGPRReg;
         GPRReg scratchGPR3 = InvalidGPRReg;
@@ -1111,28 +1808,29 @@
                 AccessGenerationState::SpillState spillState = state.preserveLiveRegistersToStackForCall(extraRegistersToPreserve);
 
                 jit.store32(
                     CCallHelpers::TrustedImm32(
                         state.callSiteIndexForExceptionHandlingOrOriginal().bits()),
-                    CCallHelpers::tagFor(static_cast<VirtualRegister>(CallFrameSlot::argumentCount)));
+                    CCallHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));
 
                 jit.makeSpaceOnStackForCCall();
 
                 if (!reallocating) {
-                    jit.setupArguments<decltype(operationReallocateButterflyToHavePropertyStorageWithInitialCapacity)>(baseGPR);
+                    jit.setupArguments<decltype(operationReallocateButterflyToHavePropertyStorageWithInitialCapacity)>(CCallHelpers::TrustedImmPtr(&vm), baseGPR);
+                    jit.prepareCallOperation(vm);
 
                     CCallHelpers::Call operationCall = jit.call(OperationPtrTag);
                     jit.addLinkTask([=] (LinkBuffer& linkBuffer) {
                         linkBuffer.link(
                             operationCall,
                             FunctionPtr<OperationPtrTag>(operationReallocateButterflyToHavePropertyStorageWithInitialCapacity));
                     });
                 } else {
                     // Handle the case where we are reallocating (i.e. the old structure/butterfly
                     // already had out-of-line property storage).
-                    jit.setupArguments<decltype(operationReallocateButterflyToGrowPropertyStorage)>(
-                        baseGPR, CCallHelpers::TrustedImm32(newSize / sizeof(JSValue)));
+                    jit.setupArguments<decltype(operationReallocateButterflyToGrowPropertyStorage)>(CCallHelpers::TrustedImmPtr(&vm), baseGPR, CCallHelpers::TrustedImm32(newSize / sizeof(JSValue)));
+                    jit.prepareCallOperation(vm);
 
                     CCallHelpers::Call operationCall = jit.call(OperationPtrTag);
                     jit.addLinkTask([=] (LinkBuffer& linkBuffer) {
                         linkBuffer.link(
                             operationCall,
@@ -1243,17 +1941,71 @@
 
     case DirectArgumentsLength:
     case ScopedArgumentsLength:
     case ModuleNamespaceLoad:
     case InstanceOfGeneric:
+    case IndexedInt32Load:
+    case IndexedDoubleLoad:
+    case IndexedContiguousLoad:
+    case IndexedArrayStorageLoad:
+    case IndexedScopedArgumentsLoad:
+    case IndexedDirectArgumentsLoad:
+    case IndexedTypedArrayInt8Load:
+    case IndexedTypedArrayUint8Load:
+    case IndexedTypedArrayUint8ClampedLoad:
+    case IndexedTypedArrayInt16Load:
+    case IndexedTypedArrayUint16Load:
+    case IndexedTypedArrayInt32Load:
+    case IndexedTypedArrayUint32Load:
+    case IndexedTypedArrayFloat32Load:
+    case IndexedTypedArrayFloat64Load:
+    case IndexedStringLoad:
         // These need to be handled by generateWithGuard(), since the guard is part of the
         // algorithm. We can be sure that nobody will call generate() directly for these since they
         // are not guarded by structure checks.
         RELEASE_ASSERT_NOT_REACHED();
     }
 
     RELEASE_ASSERT_NOT_REACHED();
 }
 
+TypedArrayType AccessCase::toTypedArrayType(AccessType accessType)
+{
+    switch (accessType) {
+    case IndexedTypedArrayInt8Load:
+        return TypeInt8;
+    case IndexedTypedArrayUint8Load:
+        return TypeUint8;
+    case IndexedTypedArrayUint8ClampedLoad:
+        return TypeUint8Clamped;
+    case IndexedTypedArrayInt16Load:
+        return TypeInt16;
+    case IndexedTypedArrayUint16Load:
+        return TypeUint16;
+    case IndexedTypedArrayInt32Load:
+        return TypeInt32;
+    case IndexedTypedArrayUint32Load:
+        return TypeUint32;
+    case IndexedTypedArrayFloat32Load:
+        return TypeFloat32;
+    case IndexedTypedArrayFloat64Load:
+        return TypeFloat64;
+    default:
+        RELEASE_ASSERT_NOT_REACHED();
+    }
+}
+
+#if ASSERT_ENABLED
+void AccessCase::checkConsistency(StructureStubInfo& stubInfo)
+{
+    RELEASE_ASSERT(!(requiresInt32PropertyCheck() && requiresIdentifierNameMatch()));
+
+    if (stubInfo.hasConstantIdentifier) {
+        RELEASE_ASSERT(!requiresInt32PropertyCheck());
+        RELEASE_ASSERT(requiresIdentifierNameMatch());
+    }
+}
+#endif // ASSERT_ENABLED
+
 } // namespace JSC
 
 #endif
