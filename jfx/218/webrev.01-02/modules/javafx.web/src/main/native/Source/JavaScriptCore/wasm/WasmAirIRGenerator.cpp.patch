diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmAirIRGenerator.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmAirIRGenerator.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmAirIRGenerator.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmAirIRGenerator.cpp
@@ -28,10 +28,11 @@
 
 #if ENABLE(WEBASSEMBLY)
 
 #include "AirCode.h"
 #include "AirGenerate.h"
+#include "AirHelpers.h"
 #include "AirOpcodeUtils.h"
 #include "AirValidate.h"
 #include "AllowMacroScratchRegisterUsageIf.h"
 #include "B3CCallValue.h"
 #include "B3CheckSpecial.h"
@@ -65,20 +66,23 @@
 namespace JSC { namespace Wasm {
 
 using namespace B3::Air;
 
 struct ConstrainedTmp {
+    ConstrainedTmp() = default;
     ConstrainedTmp(Tmp tmp)
         : ConstrainedTmp(tmp, tmp.isReg() ? B3::ValueRep::reg(tmp.reg()) : B3::ValueRep::SomeRegister)
     { }
 
     ConstrainedTmp(Tmp tmp, B3::ValueRep rep)
         : tmp(tmp)
         , rep(rep)
     {
     }
 
+    explicit operator bool() const { return !!tmp; }
+
     Tmp tmp;
     B3::ValueRep rep;
 };
 
 class TypedTmp {
@@ -112,40 +116,47 @@
     operator Tmp() const { return tmp(); }
     operator Arg() const { return Arg(tmp()); }
     Tmp tmp() const { return m_tmp; }
     Type type() const { return m_type; }
 
+    void dump(PrintStream& out) const
+    {
+        out.print("(", m_tmp, ", ", m_type, ")");
+    }
+
 private:
 
     Tmp m_tmp;
     Type m_type;
 };
 
 class AirIRGenerator {
 public:
+    using ExpressionType = TypedTmp;
+    using ResultList = Vector<ExpressionType, 8>;
+
     struct ControlData {
-        ControlData(B3::Origin origin, Type returnType, TypedTmp resultTmp, BlockType type, BasicBlock* continuation, BasicBlock* special = nullptr)
-            : blockType(type)
+        ControlData(B3::Origin origin, BlockSignature result, ResultList resultTmps, BlockType type, BasicBlock* continuation, BasicBlock* special = nullptr)
+            : controlBlockType(type)
             , continuation(continuation)
             , special(special)
-            , returnType(returnType)
+            , results(resultTmps)
+            , returnType(result)
         {
-            UNUSED_PARAM(origin); // FIXME: Use origin.
-            if (resultTmp) {
-                ASSERT(returnType != Type::Void);
-                result.append(resultTmp);
-            } else
-                ASSERT(returnType == Type::Void);
+            UNUSED_PARAM(origin);
         }
 
         ControlData()
         {
         }
 
+        static bool isIf(const ControlData& control) { return control.blockType() == BlockType::If; }
+        static bool isTopLevel(const ControlData& control) { return control.blockType() == BlockType::TopLevel; }
+
         void dump(PrintStream& out) const
         {
-            switch (type()) {
+            switch (blockType()) {
             case BlockType::If:
                 out.print("If:       ");
                 break;
             case BlockType::Block:
                 out.print("Block:    ");
@@ -160,65 +171,75 @@
             out.print("Continuation: ", *continuation, ", Special: ");
             if (special)
                 out.print(*special);
             else
                 out.print("None");
-        }
-
-        BlockType type() const { return blockType; }
 
-        Type signature() const { return returnType; }
+            CommaPrinter comma(", ", " Result Tmps: [");
+            for (const auto& tmp : results)
+                out.print(comma, tmp);
+            if (comma.didPrint())
+                out.print("]");
+        }
 
-        bool hasNonVoidSignature() const { return result.size(); }
+        BlockType blockType() const { return controlBlockType; }
+        BlockSignature signature() const { return returnType; }
 
         BasicBlock* targetBlockForBranch()
         {
-            if (type() == BlockType::Loop)
+            if (blockType() == BlockType::Loop)
                 return special;
             return continuation;
         }
 
         void convertIfToBlock()
         {
-            ASSERT(type() == BlockType::If);
-            blockType = BlockType::Block;
+            ASSERT(blockType() == BlockType::If);
+            controlBlockType = BlockType::Block;
             special = nullptr;
         }
 
-        using ResultList = Vector<TypedTmp, 1>;
+        SignatureArgCount branchTargetArity() const
+        {
+            if (blockType() == BlockType::Loop)
+                return returnType->argumentCount();
+            return returnType->returnCount();
+        }
 
-        ResultList resultForBranch() const
+        Type branchTargetType(unsigned i) const
         {
-            if (type() == BlockType::Loop)
-                return ResultList();
-            return result;
+            ASSERT(i < branchTargetArity());
+            if (blockType() == BlockType::Loop)
+                return returnType->argument(i);
+            return returnType->returnType(i);
         }
 
     private:
         friend class AirIRGenerator;
-        BlockType blockType;
+        BlockType controlBlockType;
         BasicBlock* continuation;
         BasicBlock* special;
-        ResultList result;
-        Type returnType;
+        ResultList results;
+        BlockSignature returnType;
     };
 
-    using ExpressionType = TypedTmp;
     using ControlType = ControlData;
-    using ExpressionList = Vector<ExpressionType, 1>;
-    using Stack = ExpressionList;
-    using ResultList = ControlData::ResultList;
-    using ControlEntry = FunctionParser<AirIRGenerator>::ControlEntry;
 
-    static ExpressionType emptyExpression() { return { }; };
-    Stack createStack() { return Stack(); }
+    using ControlEntry = FunctionParser<AirIRGenerator>::ControlEntry;
+    using ControlStack = FunctionParser<AirIRGenerator>::ControlStack;
+    using Stack = FunctionParser<AirIRGenerator>::Stack;
+    using TypedExpression = FunctionParser<AirIRGenerator>::TypedExpression;
 
     using ErrorType = String;
     using UnexpectedResult = Unexpected<ErrorType>;
     using Result = Expected<std::unique_ptr<InternalFunction>, ErrorType>;
     using PartialResult = Expected<void, ErrorType>;
 
+    static_assert(std::is_same_v<ResultList, FunctionParser<AirIRGenerator>::ResultList>);
+
+    static ExpressionType emptyExpression() { return { }; };
+
     template <typename ...Args>
     NEVER_INLINE UnexpectedResult WARN_UNUSED_RETURN fail(Args... args) const
     {
         using namespace FailureHelper; // See ADL comment in WasmParser.h.
         return UnexpectedResult(makeString("WebAssembly.Module failed compiling: "_s, makeString(args)...));
@@ -227,27 +248,28 @@
 #define WASM_COMPILE_FAIL_IF(condition, ...) do { \
         if (UNLIKELY(condition))                  \
             return fail(__VA_ARGS__);             \
     } while (0)
 
-    AirIRGenerator(const ModuleInformation&, B3::Procedure&, InternalFunction*, Vector<UnlinkedWasmToWasmCall>&, MemoryMode, unsigned functionIndex, TierUpCount*, ThrowWasmException, const Signature&);
+    AirIRGenerator(const ModuleInformation&, B3::Procedure&, InternalFunction*, Vector<UnlinkedWasmToWasmCall>&, MemoryMode, unsigned functionIndex, TierUpCount*, const Signature&);
 
     PartialResult WARN_UNUSED_RETURN addArguments(const Signature&);
     PartialResult WARN_UNUSED_RETURN addLocal(Type, uint32_t);
     ExpressionType addConstant(Type, uint64_t);
     ExpressionType addConstant(BasicBlock*, Type, uint64_t);
+    ExpressionType addBottom(BasicBlock*, Type);
 
     // References
-    PartialResult WARN_UNUSED_RETURN addRefIsNull(ExpressionType& value, ExpressionType& result);
+    PartialResult WARN_UNUSED_RETURN addRefIsNull(ExpressionType value, ExpressionType& result);
     PartialResult WARN_UNUSED_RETURN addRefFunc(uint32_t index, ExpressionType& result);
 
     // Tables
-    PartialResult WARN_UNUSED_RETURN addTableGet(unsigned, ExpressionType& index, ExpressionType& result);
-    PartialResult WARN_UNUSED_RETURN addTableSet(unsigned, ExpressionType& index, ExpressionType& value);
+    PartialResult WARN_UNUSED_RETURN addTableGet(unsigned, ExpressionType index, ExpressionType& result);
+    PartialResult WARN_UNUSED_RETURN addTableSet(unsigned, ExpressionType index, ExpressionType value);
     PartialResult WARN_UNUSED_RETURN addTableSize(unsigned, ExpressionType& result);
-    PartialResult WARN_UNUSED_RETURN addTableGrow(unsigned, ExpressionType& fill, ExpressionType& delta, ExpressionType& result);
-    PartialResult WARN_UNUSED_RETURN addTableFill(unsigned, ExpressionType& offset, ExpressionType& fill, ExpressionType& count);
+    PartialResult WARN_UNUSED_RETURN addTableGrow(unsigned, ExpressionType fill, ExpressionType delta, ExpressionType& result);
+    PartialResult WARN_UNUSED_RETURN addTableFill(unsigned, ExpressionType offset, ExpressionType fill, ExpressionType count);
 
     // Locals
     PartialResult WARN_UNUSED_RETURN getLocal(uint32_t index, ExpressionType& result);
     PartialResult WARN_UNUSED_RETURN setLocal(uint32_t index, ExpressionType value);
 
@@ -267,66 +289,56 @@
     template<OpType>
     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType left, ExpressionType right, ExpressionType& result);
     PartialResult WARN_UNUSED_RETURN addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType& result);
 
     // Control flow
-    ControlData WARN_UNUSED_RETURN addTopLevel(Type signature);
-    ControlData WARN_UNUSED_RETURN addBlock(Type signature);
-    ControlData WARN_UNUSED_RETURN addLoop(Type signature, const Stack&, uint32_t loopIndex);
-    PartialResult WARN_UNUSED_RETURN addIf(ExpressionType condition, Type signature, ControlData& result);
+    ControlData WARN_UNUSED_RETURN addTopLevel(BlockSignature);
+    PartialResult WARN_UNUSED_RETURN addBlock(BlockSignature, Stack& enclosingStack, ControlType& newBlock, Stack& newStack);
+    PartialResult WARN_UNUSED_RETURN addLoop(BlockSignature, Stack& enclosingStack, ControlType& block, Stack& newStack, uint32_t loopIndex);
+    PartialResult WARN_UNUSED_RETURN addIf(ExpressionType condition, BlockSignature, Stack& enclosingStack, ControlType& result, Stack& newStack);
     PartialResult WARN_UNUSED_RETURN addElse(ControlData&, const Stack&);
     PartialResult WARN_UNUSED_RETURN addElseToUnreachable(ControlData&);
 
-    PartialResult WARN_UNUSED_RETURN addReturn(const ControlData&, const ExpressionList& returnValues);
+    PartialResult WARN_UNUSED_RETURN addReturn(const ControlData&, const Stack& returnValues);
     PartialResult WARN_UNUSED_RETURN addBranch(ControlData&, ExpressionType condition, const Stack& returnValues);
     PartialResult WARN_UNUSED_RETURN addSwitch(ExpressionType condition, const Vector<ControlData*>& targets, ControlData& defaultTargets, const Stack& expressionStack);
     PartialResult WARN_UNUSED_RETURN endBlock(ControlEntry&, Stack& expressionStack);
-    PartialResult WARN_UNUSED_RETURN addEndToUnreachable(ControlEntry&);
+    PartialResult WARN_UNUSED_RETURN addEndToUnreachable(ControlEntry&, const Stack& expressionStack = { });
+
+    PartialResult WARN_UNUSED_RETURN endTopLevel(BlockSignature, const Stack&) { return { }; }
 
     // Calls
-    PartialResult WARN_UNUSED_RETURN addCall(uint32_t calleeIndex, const Signature&, Vector<ExpressionType>& args, ExpressionType& result);
-    PartialResult WARN_UNUSED_RETURN addCallIndirect(unsigned tableIndex, const Signature&, Vector<ExpressionType>& args, ExpressionType& result);
+    PartialResult WARN_UNUSED_RETURN addCall(uint32_t calleeIndex, const Signature&, Vector<ExpressionType>& args, ResultList& results);
+    PartialResult WARN_UNUSED_RETURN addCallIndirect(unsigned tableIndex, const Signature&, Vector<ExpressionType>& args, ResultList& results);
     PartialResult WARN_UNUSED_RETURN addUnreachable();
+    B3::PatchpointValue* WARN_UNUSED_RETURN emitCallPatchpoint(BasicBlock*, const Signature&, const ResultList& results, const Vector<TypedTmp>& args, Vector<ConstrainedTmp>&& extraArgs = { });
 
     PartialResult addShift(Type, B3::Air::Opcode, ExpressionType value, ExpressionType shift, ExpressionType& result);
     PartialResult addIntegerSub(B3::Air::Opcode, ExpressionType lhs, ExpressionType rhs, ExpressionType& result);
     PartialResult addFloatingPointAbs(B3::Air::Opcode, ExpressionType value, ExpressionType& result);
     PartialResult addFloatingPointBinOp(Type, B3::Air::Opcode, ExpressionType lhs, ExpressionType rhs, ExpressionType& result);
 
-    void dump(const Vector<ControlEntry>& controlStack, const Stack* expressionStack);
+    void dump(const ControlStack&, const Stack* expressionStack);
     void setParser(FunctionParser<AirIRGenerator>* parser) { m_parser = parser; };
-
-    static Vector<Tmp> toTmpVector(const Vector<TypedTmp>& vector)
-    {
-        Vector<Tmp> result;
-        for (const auto& item : vector)
-            result.append(item.tmp());
-        return result;
-    }
-
-    ALWAYS_INLINE void didKill(const ExpressionType& typedTmp)
-    {
-        Tmp tmp = typedTmp.tmp();
-        if (!tmp)
-            return;
-        if (tmp.isGP())
-            m_freeGPs.append(tmp);
-        else
-            m_freeFPs.append(tmp);
-    }
+    void didFinishParsingLocals() { }
+    void didPopValueFromStack() { }
 
     const Bag<B3::PatchpointValue*>& patchpoints() const
     {
         return m_patchpoints;
     }
 
 private:
+    B3::Type toB3ResultType(BlockSignature returnType);
     ALWAYS_INLINE void validateInst(Inst& inst)
     {
-        if (!ASSERT_DISABLED) {
+        if (ASSERT_ENABLED) {
             if (!inst.isValidForm()) {
-                dataLogLn(inst);
+                dataLogLn("Inst validation failed:");
+                dataLogLn(inst, "\n");
+                if (inst.origin)
+                    dataLogLn(deepDump(inst.origin), "\n");
                 CRASH();
             }
         }
     }
 
@@ -398,10 +410,18 @@
         default:
             RELEASE_ASSERT_NOT_REACHED();
         }
     }
 
+    ResultList tmpsForSignature(BlockSignature signature)
+    {
+        ResultList result(signature->returnCount());
+        for (unsigned i = 0; i < signature->returnCount(); ++i)
+            result[i] = tmpForType(signature->returnType(i));
+        return result;
+    }
+
     B3::PatchpointValue* addPatchpoint(B3::Type type)
     {
         auto* result = m_proc.add<B3::PatchpointValue>(type, B3::Origin());
         if (UNLIKELY(shouldDumpIRAtEachPhase(B3::AirMode)))
             m_patchpoints.add(result);
@@ -415,43 +435,64 @@
     }
 
     template <typename ...Args>
     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result, Args... theArgs)
     {
-        emitPatchpoint(basicBlock, patch, result, Vector<ConstrainedTmp, sizeof...(Args)>::from(theArgs...));
+        emitPatchpoint(basicBlock, patch, Vector<Tmp, 8> { result }, Vector<ConstrainedTmp, sizeof...(Args)>::from(theArgs...));
     }
 
     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result)
     {
-        emitPatchpoint(basicBlock, patch, result, Vector<ConstrainedTmp>());
+        emitPatchpoint(basicBlock, patch, Vector<Tmp, 8> { result }, Vector<ConstrainedTmp>());
     }
 
-    template <size_t inlineSize>
-    void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result, Vector<ConstrainedTmp, inlineSize>&& args)
+    template <typename ResultTmpType, size_t inlineSize>
+    void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, const Vector<ResultTmpType, 8>&  results, Vector<ConstrainedTmp, inlineSize>&& args)
     {
         if (!m_patchpointSpecial)
             m_patchpointSpecial = static_cast<B3::PatchpointSpecial*>(m_code.addSpecial(makeUnique<B3::PatchpointSpecial>()));
 
+        auto toTmp = [&] (ResultTmpType tmp) {
+            if constexpr (std::is_same_v<ResultTmpType, Tmp>)
+                return tmp;
+            else
+                return tmp.tmp();
+        };
+
         Inst inst(Patch, patch, Arg::special(m_patchpointSpecial));
-        Inst resultMov;
-        if (result) {
-            ASSERT(patch->type() != B3::Void);
-            switch (patch->resultConstraints[0].kind()) {
-            case B3::ValueRep::Register:
-                inst.args.append(Tmp(patch->resultConstraints[0].reg()));
-                resultMov = Inst(result.isGP() ? Move : MoveDouble, nullptr, Tmp(patch->resultConstraints[0].reg()), result);
-                break;
-            case B3::ValueRep::SomeRegister:
-                inst.args.append(result);
-                break;
-            default:
-                RELEASE_ASSERT_NOT_REACHED();
+        Vector<Inst, 1> resultMovs;
+        switch (patch->type().kind()) {
+        case B3::Void:
+            break;
+        default: {
+            ASSERT(results.size());
+            for (unsigned i = 0; i < results.size(); ++i) {
+                switch (patch->resultConstraints[i].kind()) {
+                case B3::ValueRep::StackArgument: {
+                    Arg arg = Arg::callArg(patch->resultConstraints[i].offsetFromSP());
+                    inst.args.append(arg);
+                    resultMovs.append(Inst(B3::Air::moveForType(m_proc.typeAtOffset(patch->type(), i)), nullptr, arg, toTmp(results[i])));
+                    break;
+                }
+                case B3::ValueRep::Register: {
+                    inst.args.append(Tmp(patch->resultConstraints[i].reg()));
+                    resultMovs.append(Inst(B3::Air::relaxedMoveForType(m_proc.typeAtOffset(patch->type(), i)), nullptr, Tmp(patch->resultConstraints[i].reg()), toTmp(results[i])));
+                    break;
+                }
+                case B3::ValueRep::SomeRegister: {
+                    inst.args.append(toTmp(results[i]));
+                    break;
+                }
+                default:
+                    RELEASE_ASSERT_NOT_REACHED();
+                }
             }
-        } else
-            ASSERT(patch->type() == B3::Void);
+        }
+        }
 
-        for (ConstrainedTmp& tmp : args) {
+        for (unsigned i = 0; i < args.size(); ++i) {
+            ConstrainedTmp& tmp = args[i];
             // FIXME: This is less than ideal to create dummy values just to satisfy Air's
             // validation. We should abstrcat Patch enough so ValueRep's don't need to be
             // backed by Values.
             // https://bugs.webkit.org/show_bug.cgi?id=194040
             B3::Value* dummyValue = m_proc.addConstant(B3::Origin(), tmp.tmp.isGP() ? B3::Int64 : B3::Double, 0);
@@ -465,32 +506,36 @@
                 patch->earlyClobbered().clear(tmp.rep.reg());
                 append(basicBlock, tmp.tmp.isGP() ? Move : MoveDouble, tmp.tmp, tmp.rep.reg());
                 inst.args.append(Tmp(tmp.rep.reg()));
                 break;
             case B3::ValueRep::StackArgument: {
-                auto arg = Arg::callArg(tmp.rep.offsetFromSP());
+                ASSERT(!patch->effects.terminal);
+                Arg arg = Arg::callArg(tmp.rep.offsetFromSP());
                 append(basicBlock, tmp.tmp.isGP() ? Move : MoveDouble, tmp.tmp, arg);
+                ASSERT(arg.canRepresent(patch->child(i)->type()));
                 inst.args.append(arg);
                 break;
             }
             default:
                 RELEASE_ASSERT_NOT_REACHED();
             }
         }
 
-        if (patch->resultConstraints[0].isReg())
-            patch->lateClobbered().clear(patch->resultConstraints[0].reg());
+        for (auto valueRep : patch->resultConstraints) {
+            if (valueRep.isReg())
+                patch->lateClobbered().clear(valueRep.reg());
+        }
         for (unsigned i = patch->numGPScratchRegisters; i--;)
             inst.args.append(g64().tmp());
         for (unsigned i = patch->numFPScratchRegisters; i--;)
             inst.args.append(f64().tmp());
 
         validateInst(inst);
         basicBlock->append(WTFMove(inst));
-        if (resultMov) {
-            validateInst(resultMov);
-            basicBlock->append(WTFMove(resultMov));
+        for (Inst result : resultMovs) {
+            validateInst(result);
+            basicBlock->append(WTFMove(result));
         }
     }
 
     template <typename Branch, typename Generator>
     void emitCheck(const Branch& makeBranch, const Generator& generator)
@@ -589,19 +634,19 @@
         }
     }
 
     void emitThrowException(CCallHelpers&, ExceptionType);
 
-    void emitEntryTierUpCheck(int32_t incrementCount, B3::Origin);
-    void emitLoopTierUpCheck(int32_t incrementCount, const Stack&, uint32_t, uint32_t, B3::Origin);
+    void emitEntryTierUpCheck();
+    void emitLoopTierUpCheck(uint32_t loopIndex, const Stack& enclosingStack);
 
     void emitWriteBarrierForJSWrapper();
     ExpressionType emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOp);
     ExpressionType emitLoadOp(LoadOpType, ExpressionType pointer, uint32_t offset);
     void emitStoreOp(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 
-    void unify(const ExpressionType& dst, const ExpressionType& source);
+    void unify(const ExpressionType dst, const ExpressionType source);
     void unifyValuesWithBlock(const Stack& resultStack, const ResultList& stack);
 
     template <typename IntType>
     void emitChecksForModOrDiv(bool isSignedDiv, ExpressionType left, ExpressionType right);
 
@@ -646,10 +691,11 @@
     bool m_makesCalls { false };
 
     Vector<Tmp, 8> m_freeGPs;
     Vector<Tmp, 8> m_freeFPs;
 
+    HashMap<BlockSignature, B3::Type> m_tupleMap;
     // This is only filled if we are dumping IR.
     Bag<B3::PatchpointValue*> m_patchpoints;
 
     TypedTmp m_instanceValue; // Always use the accessor below to ensure the instance value is materialized when used.
     bool m_usesInstanceValue { false };
@@ -706,11 +752,11 @@
         jit.move(param[0].gpr(), wasmContextInstanceGPR);
     });
     emitPatchpoint(block, patchpoint, Tmp(), instance);
 }
 
-AirIRGenerator::AirIRGenerator(const ModuleInformation& info, B3::Procedure& procedure, InternalFunction* compilation, Vector<UnlinkedWasmToWasmCall>& unlinkedWasmToWasmCalls, MemoryMode mode, unsigned functionIndex, TierUpCount* tierUp, ThrowWasmException throwWasmException, const Signature& signature)
+AirIRGenerator::AirIRGenerator(const ModuleInformation& info, B3::Procedure& procedure, InternalFunction* compilation, Vector<UnlinkedWasmToWasmCall>& unlinkedWasmToWasmCalls, MemoryMode mode, unsigned functionIndex, TierUpCount* tierUp, const Signature& signature)
     : m_info(info)
     , m_mode(mode)
     , m_functionIndex(functionIndex)
     , m_tierUp(tierUp)
     , m_proc(procedure)
@@ -734,13 +780,10 @@
     if (mode != MemoryMode::Signaling) {
         m_memorySizeGPR = pinnedRegs.sizeRegister;
         m_code.pinRegister(m_memorySizeGPR);
     }
 
-    if (throwWasmException)
-        Thunks::singleton().setThrowWasmException(throwWasmException);
-
     if (info.memory) {
         switch (m_mode) {
         case MemoryMode::BoundsChecking:
             break;
         case MemoryMode::Signaling:
@@ -754,18 +797,18 @@
         }
     }
 
     m_code.setNumEntrypoints(1);
 
-    GPRReg contextInstance = Context::useFastTLS() ? wasmCallingConventionAir().prologueScratch(1) : m_wasmContextInstanceGPR;
+    GPRReg contextInstance = Context::useFastTLS() ? wasmCallingConvention().prologueScratchGPRs[1] : m_wasmContextInstanceGPR;
 
     Ref<B3::Air::PrologueGenerator> prologueGenerator = createSharedTask<B3::Air::PrologueGeneratorFunction>([=] (CCallHelpers& jit, B3::Air::Code& code) {
         AllowMacroScratchRegisterUsage allowScratch(jit);
         code.emitDefaultPrologue(jit);
 
         {
-            GPRReg calleeGPR = wasmCallingConventionAir().prologueScratch(0);
+            GPRReg calleeGPR = wasmCallingConvention().prologueScratchGPRs[0];
             auto moveLocation = jit.moveWithPatch(MacroAssembler::TrustedImmPtr(nullptr), calleeGPR);
             jit.addLinkTask([compilation, moveLocation] (LinkBuffer& linkBuffer) {
                 compilation->calleeMoveLocation = linkBuffer.locationOf<WasmEntryPtrTag>(moveLocation);
             });
             jit.emitPutToCallFrameHeader(calleeGPR, CallFrameSlot::callee);
@@ -784,20 +827,20 @@
                 // 2. Try to speed things up by skipping stack checks.
                 minimumParentCheckSize,
                 // This allows us to elide stack checks in the Wasm -> Embedder call IC stub. Since these will
                 // spill all arguments to the stack, we ensure that a stack check here covers the
                 // stack that such a stub would use.
-                (Checked<uint32_t>(m_maxNumJSCallArguments) * sizeof(Register) + jscCallingConvention().headerSizeInBytes()).unsafeGet()
+                (Checked<uint32_t>(m_maxNumJSCallArguments) * sizeof(Register) + jsCallingConvention().headerSizeInBytes).unsafeGet()
             ));
             const int32_t checkSize = m_makesCalls ? (wasmFrameSize + extraFrameSize).unsafeGet() : wasmFrameSize.unsafeGet();
             bool needUnderflowCheck = static_cast<unsigned>(checkSize) > Options::reservedZoneSize();
             bool needsOverflowCheck = m_makesCalls || wasmFrameSize >= minimumParentCheckSize || needUnderflowCheck;
 
             // This allows leaf functions to not do stack checks if their frame size is within
             // certain limits since their caller would have already done the check.
             if (needsOverflowCheck) {
-                GPRReg scratch = wasmCallingConventionAir().prologueScratch(0);
+                GPRReg scratch = wasmCallingConvention().prologueScratchGPRs[0];
 
                 if (Context::useFastTLS())
                     jit.loadWasmContextInstance(contextInstance);
 
                 jit.addPtr(CCallHelpers::TrustedImm32(-checkSize), GPRInfo::callFrameRegister, scratch);
@@ -829,11 +872,15 @@
     for (unsigned i = 0; i < signature.argumentCount(); ++i) {
         Type type = signature.argument(i);
         m_locals[i] = tmpForType(type);
     }
 
-    wasmCallingConventionAir().loadArguments(signature, [&] (const Arg& arg, unsigned i) {
+    CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(signature, CallRole::Callee);
+
+    for (unsigned i = 0; i < wasmCallInfo.params.size(); ++i) {
+        B3::ValueRep location = wasmCallInfo.params[i];
+        Arg arg = location.isReg() ? Arg(Tmp(location.reg())) : Arg::addr(Tmp(GPRInfo::callFrameRegister), location.offsetFromFP());
         switch (signature.argument(i)) {
         case Type::I32:
             append(Move32, arg, m_locals[i]);
             break;
         case Type::I64:
@@ -848,13 +895,30 @@
             append(MoveDouble, arg, m_locals[i]);
             break;
         default:
             RELEASE_ASSERT_NOT_REACHED();
         }
-    });
+    }
 
-    emitEntryTierUpCheck(TierUpCount::functionEntryIncrement(), B3::Origin());
+    emitEntryTierUpCheck();
+}
+
+B3::Type AirIRGenerator::toB3ResultType(BlockSignature returnType)
+{
+    if (returnType->returnsVoid())
+        return B3::Void;
+
+    if (returnType->returnCount() == 1)
+        return toB3Type(returnType->returnType(0));
+
+    auto result = m_tupleMap.ensure(returnType, [&] {
+        Vector<B3::Type> result;
+        for (unsigned i = 0; i < returnType->returnCount(); ++i)
+            result.append(toB3Type(returnType->returnType(i)));
+        return m_proc.addTuple(WTFMove(result));
+    });
+    return result.iterator->value;
 }
 
 void AirIRGenerator::restoreWebAssemblyGlobalState(RestoreCachedStackLimit restoreCachedStackLimit, const MemoryInformation& memory, TypedTmp instance, BasicBlock* block)
 {
     restoreWasmContextInstance(block, instance);
@@ -975,17 +1039,23 @@
     }
 
     return result;
 }
 
+auto AirIRGenerator::addBottom(BasicBlock* block, Type type) -> ExpressionType
+{
+    append(block, B3::Air::Oops);
+    return addConstant(type, 0);
+}
+
 auto AirIRGenerator::addArguments(const Signature& signature) -> PartialResult
 {
     RELEASE_ASSERT(m_locals.size() == signature.argumentCount()); // We handle arguments in the prologue
     return { };
 }
 
-auto AirIRGenerator::addRefIsNull(ExpressionType& value, ExpressionType& result) -> PartialResult
+auto AirIRGenerator::addRefIsNull(ExpressionType value, ExpressionType& result) -> PartialResult
 {
     ASSERT(value.tmp());
     result = tmpForType(Type::I32);
     auto tmp = g64();
 
@@ -997,41 +1067,41 @@
 
 auto AirIRGenerator::addRefFunc(uint32_t index, ExpressionType& result) -> PartialResult
 {
     // FIXME: Emit this inline <https://bugs.webkit.org/show_bug.cgi?id=198506>.
     result = tmpForType(Type::Funcref);
-    emitCCall(&doWasmRefFunc, result, instanceValue(), addConstant(Type::I32, index));
+    emitCCall(&operationWasmRefFunc, result, instanceValue(), addConstant(Type::I32, index));
 
     return { };
 }
 
-auto AirIRGenerator::addTableGet(unsigned tableIndex, ExpressionType& index, ExpressionType& result) -> PartialResult
+auto AirIRGenerator::addTableGet(unsigned tableIndex, ExpressionType index, ExpressionType& result) -> PartialResult
 {
     // FIXME: Emit this inline <https://bugs.webkit.org/show_bug.cgi?id=198506>.
     ASSERT(index.tmp());
     ASSERT(index.type() == Type::I32);
     result = tmpForType(m_info.tables[tableIndex].wasmType());
 
-    emitCCall(&getWasmTableElement, result, instanceValue(), addConstant(Type::I32, tableIndex), index);
+    emitCCall(&operationGetWasmTableElement, result, instanceValue(), addConstant(Type::I32, tableIndex), index);
     emitCheck([&] {
         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), result, result);
     }, [=] (CCallHelpers& jit, const B3::StackmapGenerationParams&) {
         this->emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
     });
 
     return { };
 }
 
-auto AirIRGenerator::addTableSet(unsigned tableIndex, ExpressionType& index, ExpressionType& value) -> PartialResult
+auto AirIRGenerator::addTableSet(unsigned tableIndex, ExpressionType index, ExpressionType value) -> PartialResult
 {
     // FIXME: Emit this inline <https://bugs.webkit.org/show_bug.cgi?id=198506>.
     ASSERT(index.tmp());
     ASSERT(index.type() == Type::I32);
     ASSERT(value.tmp());
 
     auto shouldThrow = g32();
-    emitCCall(&setWasmTableElement, shouldThrow, instanceValue(), addConstant(Type::I32, tableIndex), index, value);
+    emitCCall(&operationSetWasmTableElement, shouldThrow, instanceValue(), addConstant(Type::I32, tableIndex), index, value);
 
     emitCheck([&] {
         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), shouldThrow, shouldThrow);
     }, [=] (CCallHelpers& jit, const B3::StackmapGenerationParams&) {
         this->emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
@@ -1043,43 +1113,39 @@
 auto AirIRGenerator::addTableSize(unsigned tableIndex, ExpressionType& result) -> PartialResult
 {
     // FIXME: Emit this inline <https://bugs.webkit.org/show_bug.cgi?id=198506>.
     result = tmpForType(Type::I32);
 
-    int32_t (*doSize)(Instance*, unsigned) = [] (Instance* instance, unsigned tableIndex) -> int32_t {
-        return instance->table(tableIndex)->length();
-    };
-
-    emitCCall(doSize, result, instanceValue(), addConstant(Type::I32, tableIndex));
+    emitCCall(&operationGetWasmTableSize, result, instanceValue(), addConstant(Type::I32, tableIndex));
 
     return { };
 }
 
-auto AirIRGenerator::addTableGrow(unsigned tableIndex, ExpressionType& fill, ExpressionType& delta, ExpressionType& result) -> PartialResult
+auto AirIRGenerator::addTableGrow(unsigned tableIndex, ExpressionType fill, ExpressionType delta, ExpressionType& result) -> PartialResult
 {
     ASSERT(fill.tmp());
     ASSERT(isSubtype(fill.type(), m_info.tables[tableIndex].wasmType()));
     ASSERT(delta.tmp());
     ASSERT(delta.type() == Type::I32);
     result = tmpForType(Type::I32);
 
-    emitCCall(&doWasmTableGrow, result, instanceValue(), addConstant(Type::I32, tableIndex), fill, delta);
+    emitCCall(&operationWasmTableGrow, result, instanceValue(), addConstant(Type::I32, tableIndex), fill, delta);
 
     return { };
 }
 
-auto AirIRGenerator::addTableFill(unsigned tableIndex, ExpressionType& offset, ExpressionType& fill, ExpressionType& count) -> PartialResult
+auto AirIRGenerator::addTableFill(unsigned tableIndex, ExpressionType offset, ExpressionType fill, ExpressionType count) -> PartialResult
 {
     ASSERT(fill.tmp());
     ASSERT(isSubtype(fill.type(), m_info.tables[tableIndex].wasmType()));
     ASSERT(offset.tmp());
     ASSERT(offset.type() == Type::I32);
     ASSERT(count.tmp());
     ASSERT(count.type() == Type::I32);
 
     auto result = tmpForType(Type::I32);
-    emitCCall(&doWasmTableFill, result, instanceValue(), addConstant(Type::I32, tableIndex), offset, fill, count);
+    emitCCall(&operationWasmTableFill, result, instanceValue(), addConstant(Type::I32, tableIndex), offset, fill, count);
 
     emitCheck([&] {
         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), result, result);
     }, [=] (CCallHelpers& jit, const B3::StackmapGenerationParams&) {
         this->emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
@@ -1107,33 +1173,12 @@
     return { };
 }
 
 auto AirIRGenerator::addGrowMemory(ExpressionType delta, ExpressionType& result) -> PartialResult
 {
-    int32_t (*growMemory)(void*, Instance*, int32_t) = [] (void* callFrame, Instance* instance, int32_t delta) -> int32_t {
-        instance->storeTopCallFrame(callFrame);
-
-        if (delta < 0)
-            return -1;
-
-        auto grown = instance->memory()->grow(PageCount(delta));
-        if (!grown) {
-            switch (grown.error()) {
-            case Memory::GrowFailReason::InvalidDelta:
-            case Memory::GrowFailReason::InvalidGrowSize:
-            case Memory::GrowFailReason::WouldExceedMaximum:
-            case Memory::GrowFailReason::OutOfMemory:
-                return -1;
-            }
-            RELEASE_ASSERT_NOT_REACHED();
-        }
-
-        return grown.value().pageCount();
-    };
-
     result = g32();
-    emitCCall(growMemory, result, TypedTmp { Tmp(GPRInfo::callFrameRegister), Type::I64 }, instanceValue(), delta);
+    emitCCall(&operationGrowMemory, result, TypedTmp { Tmp(GPRInfo::callFrameRegister), Type::I64 }, instanceValue(), delta);
     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::No, m_info.memory, instanceValue(), m_currentBlock);
 
     return { };
 }
 
@@ -1162,27 +1207,44 @@
     return { };
 }
 
 auto AirIRGenerator::getGlobal(uint32_t index, ExpressionType& result) -> PartialResult
 {
-    Type type = m_info.globals[index].type;
+    const Wasm::GlobalInformation& global = m_info.globals[index];
+    Type type = global.type;
 
     result = tmpForType(type);
 
     auto temp = g64();
 
     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfGlobals(), B3::Width64));
     append(Move, Arg::addr(instanceValue(), Instance::offsetOfGlobals()), temp);
 
     int32_t offset = safeCast<int32_t>(index * sizeof(Register));
-    if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
-        append(moveOpForValueType(type), Arg::addr(temp, offset), result);
-    else {
-        auto temp2 = g64();
-        append(Move, Arg::bigImm(offset), temp2);
-        append(Add64, temp2, temp, temp);
+    switch (global.bindingMode) {
+    case Wasm::GlobalInformation::BindingMode::EmbeddedInInstance:
+        if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
+            append(moveOpForValueType(type), Arg::addr(temp, offset), result);
+        else {
+            auto temp2 = g64();
+            append(Move, Arg::bigImm(offset), temp2);
+            append(Add64, temp2, temp, temp);
+            append(moveOpForValueType(type), Arg::addr(temp), result);
+        }
+        break;
+    case Wasm::GlobalInformation::BindingMode::Portable:
+        ASSERT(global.mutability == Wasm::GlobalInformation::Mutability::Mutable);
+        if (Arg::isValidAddrForm(offset, B3::Width64))
+            append(Move, Arg::addr(temp, offset), temp);
+        else {
+            auto temp2 = g64();
+            append(Move, Arg::bigImm(offset), temp2);
+            append(Add64, temp2, temp, temp);
+            append(Move, Arg::addr(temp), temp);
+        }
         append(moveOpForValueType(type), Arg::addr(temp), result);
+        break;
     }
     return { };
 }
 
 auto AirIRGenerator::setGlobal(uint32_t index, ExpressionType value) -> PartialResult
@@ -1190,24 +1252,84 @@
     auto temp = g64();
 
     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfGlobals(), B3::Width64));
     append(Move, Arg::addr(instanceValue(), Instance::offsetOfGlobals()), temp);
 
-    Type type = m_info.globals[index].type;
+    const Wasm::GlobalInformation& global = m_info.globals[index];
+    Type type = global.type;
 
     int32_t offset = safeCast<int32_t>(index * sizeof(Register));
-    if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
-        append(moveOpForValueType(type), value, Arg::addr(temp, offset));
-    else {
-        auto temp2 = g64();
-        append(Move, Arg::bigImm(offset), temp2);
-        append(Add64, temp2, temp, temp);
+    switch (global.bindingMode) {
+    case Wasm::GlobalInformation::BindingMode::EmbeddedInInstance:
+        if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
+            append(moveOpForValueType(type), value, Arg::addr(temp, offset));
+        else {
+            auto temp2 = g64();
+            append(Move, Arg::bigImm(offset), temp2);
+            append(Add64, temp2, temp, temp);
+            append(moveOpForValueType(type), value, Arg::addr(temp));
+        }
+        if (isSubtype(type, Anyref))
+            emitWriteBarrierForJSWrapper();
+        break;
+    case Wasm::GlobalInformation::BindingMode::Portable:
+        ASSERT(global.mutability == Wasm::GlobalInformation::Mutability::Mutable);
+        if (Arg::isValidAddrForm(offset, B3::Width64))
+            append(Move, Arg::addr(temp, offset), temp);
+        else {
+            auto temp2 = g64();
+            append(Move, Arg::bigImm(offset), temp2);
+            append(Add64, temp2, temp, temp);
+            append(Move, Arg::addr(temp), temp);
+        }
         append(moveOpForValueType(type), value, Arg::addr(temp));
-    }
+        // We emit a write-barrier onto JSWebAssemblyGlobal, not JSWebAssemblyInstance.
+        if (isSubtype(type, Anyref)) {
+            auto cell = g64();
+            auto vm = g64();
+            auto cellState = g32();
+            auto threshold = g32();
+
+            BasicBlock* fenceCheckPath = m_code.addBlock();
+            BasicBlock* fencePath = m_code.addBlock();
+            BasicBlock* doSlowPath = m_code.addBlock();
+            BasicBlock* continuation = m_code.addBlock();
+
+            append(Move, Arg::addr(instanceValue(), Instance::offsetOfOwner()), cell);
+            append(Move, Arg::addr(cell, JSWebAssemblyInstance::offsetOfVM()), vm);
+
+            append(Move, Arg::addr(temp, Wasm::Global::offsetOfOwner() - Wasm::Global::offsetOfValue()), cell);
+            append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
+            append(Move32, Arg::addr(vm, VM::offsetOfHeapBarrierThreshold()), threshold);
+
+            append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, threshold);
+            m_currentBlock->setSuccessors(continuation, fenceCheckPath);
+            m_currentBlock = fenceCheckPath;
+
+            append(Load8, Arg::addr(vm, VM::offsetOfHeapMutatorShouldBeFenced()), threshold);
+            append(BranchTest32, Arg::resCond(MacroAssembler::Zero), threshold, threshold);
+            m_currentBlock->setSuccessors(doSlowPath, fencePath);
+            m_currentBlock = fencePath;
+
+            auto* doFence = addPatchpoint(B3::Void);
+            doFence->setGenerator([] (CCallHelpers& jit, const B3::StackmapGenerationParams&) {
+                jit.memoryFence();
+            });
+            emitPatchpoint(doFence, Tmp());
+
+            append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
+            append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, Arg::imm(blackThreshold));
+            m_currentBlock->setSuccessors(continuation, doSlowPath);
+            m_currentBlock = doSlowPath;
 
-    if (isSubtype(type, Anyref))
-        emitWriteBarrierForJSWrapper();
+            emitCCall(&operationWasmWriteBarrierSlowPath, TypedTmp(), cell, vm);
+            append(Jump);
+            m_currentBlock->setSuccessors(continuation);
+            m_currentBlock = continuation;
+        }
+        break;
+    }
 
     return { };
 }
 
 inline void AirIRGenerator::emitWriteBarrierForJSWrapper()
@@ -1245,16 +1367,11 @@
     append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
     append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, Arg::imm(blackThreshold));
     m_currentBlock->setSuccessors(continuation, doSlowPath);
     m_currentBlock = doSlowPath;
 
-    void (*writeBarrier)(JSWebAssemblyInstance*, VM*) = [] (JSWebAssemblyInstance* cell, VM* vm) -> void {
-        ASSERT(cell);
-        ASSERT(vm);
-        vm->heap.writeBarrierSlowPath(cell);
-    };
-    emitCCall(writeBarrier, TypedTmp(), cell, vm);
+    emitCCall(&operationWasmWriteBarrierSlowPath, TypedTmp(), cell, vm);
     append(Jump);
     m_currentBlock->setSuccessors(continuation);
     m_currentBlock = continuation;
 }
 
@@ -1601,14 +1718,12 @@
     m_currentBlock = continuation;
 
     return { };
 }
 
-void AirIRGenerator::emitEntryTierUpCheck(int32_t incrementCount, B3::Origin origin)
+void AirIRGenerator::emitEntryTierUpCheck()
 {
-    UNUSED_PARAM(origin);
-
     if (!m_tierUp)
         return;
 
     auto countdownPtr = g64();
 
@@ -1622,11 +1737,11 @@
     patch->clobber(RegisterSet::macroScratchRegisters());
 
     patch->setGenerator([=] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
         AllowMacroScratchRegisterUsage allowScratch(jit);
 
-        CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(incrementCount), CCallHelpers::Address(params[0].gpr()));
+        CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(TierUpCount::functionEntryIncrement()), CCallHelpers::Address(params[0].gpr()));
         CCallHelpers::Label tierUpResume = jit.label();
 
         params.addLatePath([=] (CCallHelpers& jit) {
             tierUp.link(&jit);
 
@@ -1648,13 +1763,14 @@
     });
 
     emitPatchpoint(patch, Tmp(), countdownPtr);
 }
 
-void AirIRGenerator::emitLoopTierUpCheck(int32_t incrementCount, const Stack& expressionStack, uint32_t loopIndex, uint32_t outerLoopIndex, B3::Origin origin)
+void AirIRGenerator::emitLoopTierUpCheck(uint32_t loopIndex, const Stack& enclosingStack)
 {
-    UNUSED_PARAM(origin);
+    uint32_t outerLoopIndex = this->outerLoopIndex();
+    m_outerLoops.append(loopIndex);
 
     if (!m_tierUp)
         return;
 
     ASSERT(m_tierUp->osrEntryTriggers().size() == loopIndex);
@@ -1678,143 +1794,161 @@
     patch->clobberLate(clobberLate);
 
     Vector<ConstrainedTmp> patchArgs;
     patchArgs.append(countdownPtr);
 
-    Vector<B3::Type> types;
-    for (auto& local : m_locals) {
+    for (auto& local : m_locals)
         patchArgs.append(ConstrainedTmp(local, B3::ValueRep::ColdAny));
-        types.append(toB3Type(local.type()));
-    }
-    for (auto& expression : expressionStack) {
-        patchArgs.append(ConstrainedTmp(expression, B3::ValueRep::ColdAny));
-        types.append(toB3Type(expression.type()));
+    for (unsigned controlIndex = 0; controlIndex < m_parser->controlStack().size(); ++controlIndex) {
+        Stack& expressionStack = m_parser->controlStack()[controlIndex].enclosedExpressionStack;
+        for (TypedExpression value : expressionStack)
+            patchArgs.append(ConstrainedTmp(value.value(), B3::ValueRep::ColdAny));
     }
+    for (TypedExpression value : enclosingStack)
+        patchArgs.append(ConstrainedTmp(value.value(), B3::ValueRep::ColdAny));
 
     TierUpCount::TriggerReason* forceEntryTrigger = &(m_tierUp->osrEntryTriggers().last());
     static_assert(!static_cast<uint8_t>(TierUpCount::TriggerReason::DontTrigger), "the JIT code assumes non-zero means 'enter'");
     static_assert(sizeof(TierUpCount::TriggerReason) == 1, "branchTest8 assumes this size");
     patch->setGenerator([=] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
         AllowMacroScratchRegisterUsage allowScratch(jit);
         CCallHelpers::Jump forceOSREntry = jit.branchTest8(CCallHelpers::NonZero, CCallHelpers::AbsoluteAddress(forceEntryTrigger));
-        CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(incrementCount), CCallHelpers::Address(params[0].gpr()));
+        CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(TierUpCount::loopIncrement()), CCallHelpers::Address(params[0].gpr()));
         MacroAssembler::Label tierUpResume = jit.label();
 
         OSREntryData& osrEntryData = m_tierUp->addOSREntryData(m_functionIndex, loopIndex);
-        for (unsigned index = 0; index < types.size(); ++index)
-            osrEntryData.values().constructAndAppend(params[index + 1], types[index]);
+        // First argument is the countdown location.
+        for (unsigned index = 1; index < params.value()->numChildren(); ++index)
+            osrEntryData.values().constructAndAppend(params[index], params.value()->child(index)->type());
         OSREntryData* osrEntryDataPtr = &osrEntryData;
 
         params.addLatePath([=] (CCallHelpers& jit) {
             AllowMacroScratchRegisterUsage allowScratch(jit);
             forceOSREntry.link(&jit);
             tierUp.link(&jit);
 
-            jit.probe(triggerOSREntryNow, osrEntryDataPtr);
+            jit.probe(operationWasmTriggerOSREntryNow, osrEntryDataPtr);
             jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::argumentGPR0).linkTo(tierUpResume, &jit);
             jit.farJump(GPRInfo::argumentGPR1, WasmEntryPtrTag);
         });
     });
 
-    emitPatchpoint(patch, Tmp(), WTFMove(patchArgs));
+    emitPatchpoint(m_currentBlock, patch, ResultList { }, WTFMove(patchArgs));
 }
 
-AirIRGenerator::ControlData AirIRGenerator::addLoop(Type signature, const Stack& expressionStack, uint32_t loopIndex)
+AirIRGenerator::ControlData AirIRGenerator::addTopLevel(BlockSignature signature)
+{
+    return ControlData(B3::Origin(), signature, tmpsForSignature(signature), BlockType::TopLevel, m_code.addBlock());
+}
+
+auto AirIRGenerator::addLoop(BlockSignature signature, Stack& enclosingStack, ControlType& block, Stack& newStack, uint32_t loopIndex) -> PartialResult
 {
     BasicBlock* body = m_code.addBlock();
     BasicBlock* continuation = m_code.addBlock();
 
+    splitStack(signature, enclosingStack, newStack);
+    ResultList results;
+    results.reserveInitialCapacity(newStack.size());
+    for (auto item : newStack)
+        results.uncheckedAppend(item);
+    block = ControlData(origin(), signature, WTFMove(results), BlockType::Loop, continuation, body);
+
     append(Jump);
     m_currentBlock->setSuccessors(body);
 
-    uint32_t outerLoopIndex = this->outerLoopIndex();
-    m_outerLoops.append(loopIndex);
     m_currentBlock = body;
-    emitLoopTierUpCheck(TierUpCount::loopIncrement(), expressionStack, loopIndex, outerLoopIndex, origin());
-
-    return ControlData(origin(), signature, tmpForType(signature), BlockType::Loop, continuation, body);
-}
+    emitLoopTierUpCheck(loopIndex, enclosingStack);
 
-AirIRGenerator::ControlData AirIRGenerator::addTopLevel(Type signature)
-{
-    return ControlData(B3::Origin(), signature, tmpForType(signature), BlockType::TopLevel, m_code.addBlock());
+    return { };
 }
 
-AirIRGenerator::ControlData AirIRGenerator::addBlock(Type signature)
+auto AirIRGenerator::addBlock(BlockSignature signature, Stack& enclosingStack, ControlType& newBlock, Stack& newStack) -> PartialResult
 {
-    return ControlData(origin(), signature, tmpForType(signature), BlockType::Block, m_code.addBlock());
+    splitStack(signature, enclosingStack, newStack);
+    newBlock = ControlData(origin(), signature, tmpsForSignature(signature), BlockType::Block, m_code.addBlock());
+    return { };
 }
 
-auto AirIRGenerator::addIf(ExpressionType condition, Type signature, ControlType& result) -> PartialResult
+auto AirIRGenerator::addIf(ExpressionType condition, BlockSignature signature, Stack& enclosingStack, ControlType& result, Stack& newStack) -> PartialResult
 {
     BasicBlock* taken = m_code.addBlock();
     BasicBlock* notTaken = m_code.addBlock();
     BasicBlock* continuation = m_code.addBlock();
 
     // Wasm bools are i32.
     append(BranchTest32, Arg::resCond(MacroAssembler::NonZero), condition, condition);
     m_currentBlock->setSuccessors(taken, notTaken);
 
     m_currentBlock = taken;
-    result = ControlData(origin(), signature, tmpForType(signature), BlockType::If, continuation, notTaken);
+    splitStack(signature, enclosingStack, newStack);
+    result = ControlData(origin(), signature, tmpsForSignature(signature), BlockType::If, continuation, notTaken);
     return { };
 }
 
 auto AirIRGenerator::addElse(ControlData& data, const Stack& currentStack) -> PartialResult
 {
-    unifyValuesWithBlock(currentStack, data.result);
+    unifyValuesWithBlock(currentStack, data.results);
     append(Jump);
     m_currentBlock->setSuccessors(data.continuation);
     return addElseToUnreachable(data);
 }
 
 auto AirIRGenerator::addElseToUnreachable(ControlData& data) -> PartialResult
 {
-    ASSERT(data.type() == BlockType::If);
+    ASSERT(data.blockType() == BlockType::If);
     m_currentBlock = data.special;
     data.convertIfToBlock();
     return { };
 }
 
-auto AirIRGenerator::addReturn(const ControlData& data, const ExpressionList& returnValues) -> PartialResult
+auto AirIRGenerator::addReturn(const ControlData& data, const Stack& returnValues) -> PartialResult
 {
-    ASSERT(returnValues.size() <= 1);
-    if (returnValues.size()) {
-        Tmp returnValueGPR = Tmp(GPRInfo::returnValueGPR);
-        Tmp returnValueFPR = Tmp(FPRInfo::returnValueFPR);
-        switch (data.signature()) {
-        case Type::I32:
-            append(Move32, returnValues[0], returnValueGPR);
-            append(Ret32, returnValueGPR);
-            break;
-        case Type::I64:
-        case Type::Anyref:
-        case Type::Funcref:
-            append(Move, returnValues[0], returnValueGPR);
-            append(Ret64, returnValueGPR);
-            break;
-        case Type::F32:
-            append(MoveFloat, returnValues[0], returnValueFPR);
-            append(RetFloat, returnValueFPR);
-            break;
-        case Type::F64:
-            append(MoveDouble, returnValues[0], returnValueFPR);
-            append(RetFloat, returnValueFPR);
-            break;
-        default:
-            RELEASE_ASSERT_NOT_REACHED();
-        }
-    } else
+    CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(*data.signature(), CallRole::Callee);
+    if (!wasmCallInfo.results.size()) {
         append(RetVoid);
+        return { };
+    }
+
+    B3::PatchpointValue* patch = addPatchpoint(B3::Void);
+    patch->setGenerator([] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
+        auto calleeSaves = params.code().calleeSaveRegisterAtOffsetList();
+
+        for (RegisterAtOffset calleeSave : calleeSaves)
+            jit.load64ToReg(CCallHelpers::Address(GPRInfo::callFrameRegister, calleeSave.offset()), calleeSave.reg());
+
+        jit.emitFunctionEpilogue();
+        jit.ret();
+    });
+    patch->effects.terminal = true;
+
+    ASSERT(returnValues.size() >= wasmCallInfo.results.size());
+    unsigned offset = returnValues.size() - wasmCallInfo.results.size();
+    Vector<ConstrainedTmp, 8> returnConstraints;
+    for (unsigned i = 0; i < wasmCallInfo.results.size(); ++i) {
+        B3::ValueRep rep = wasmCallInfo.results[i];
+        TypedTmp tmp = returnValues[offset + i];
+
+        if (rep.isStack()) {
+            append(moveForType(toB3Type(tmp.type())), tmp, Arg::addr(Tmp(GPRInfo::callFrameRegister), rep.offsetFromFP()));
+            continue;
+        }
+
+        ASSERT(rep.isReg());
+        if (data.signature()->returnType(i) == I32)
+            append(Move32, tmp, tmp);
+        returnConstraints.append(ConstrainedTmp(tmp, wasmCallInfo.results[i]));
+    }
+
+    emitPatchpoint(m_currentBlock, patch, ResultList { }, WTFMove(returnConstraints));
     return { };
 }
 
 // NOTE: All branches in Wasm are on 32-bit ints
 
 auto AirIRGenerator::addBranch(ControlData& data, ExpressionType condition, const Stack& returnValues) -> PartialResult
 {
-    unifyValuesWithBlock(returnValues, data.resultForBranch());
+    unifyValuesWithBlock(returnValues, data.results);
 
     BasicBlock* target = data.targetBlockForBranch();
     if (condition) {
         BasicBlock* continuation = m_code.addBlock();
         append(BranchTest32, Arg::resCond(MacroAssembler::NonZero), condition, condition);
@@ -1831,14 +1965,14 @@
 auto AirIRGenerator::addSwitch(ExpressionType condition, const Vector<ControlData*>& targets, ControlData& defaultTarget, const Stack& expressionStack) -> PartialResult
 {
     auto& successors = m_currentBlock->successors();
     ASSERT(successors.isEmpty());
     for (const auto& target : targets) {
-        unifyValuesWithBlock(expressionStack, target->resultForBranch());
+        unifyValuesWithBlock(expressionStack, target->results);
         successors.append(target->targetBlockForBranch());
     }
-    unifyValuesWithBlock(expressionStack, defaultTarget.resultForBranch());
+    unifyValuesWithBlock(expressionStack, defaultTarget.results);
     successors.append(defaultTarget.targetBlockForBranch());
 
     ASSERT(condition.type() == Type::I32);
 
     // FIXME: We should consider dynamically switching between a jump table
@@ -1893,50 +2027,84 @@
 
 auto AirIRGenerator::endBlock(ControlEntry& entry, Stack& expressionStack) -> PartialResult
 {
     ControlData& data = entry.controlData;
 
-    unifyValuesWithBlock(expressionStack, data.result);
+    if (data.blockType() != BlockType::Loop)
+        unifyValuesWithBlock(expressionStack, data.results);
     append(Jump);
     m_currentBlock->setSuccessors(data.continuation);
 
-    return addEndToUnreachable(entry);
+    return addEndToUnreachable(entry, expressionStack);
 }
 
 
-auto AirIRGenerator::addEndToUnreachable(ControlEntry& entry) -> PartialResult
+auto AirIRGenerator::addEndToUnreachable(ControlEntry& entry, const Stack& expressionStack) -> PartialResult
 {
     ControlData& data = entry.controlData;
     m_currentBlock = data.continuation;
 
-    if (data.type() == BlockType::If) {
+    if (data.blockType() == BlockType::If) {
         append(data.special, Jump);
         data.special->setSuccessors(m_currentBlock);
     }
 
-    if (data.type() == BlockType::Loop)
+    if (data.blockType() == BlockType::Loop) {
         m_outerLoops.removeLast();
-
-    for (const auto& result : data.result)
-        entry.enclosedExpressionStack.append(result);
+        for (unsigned i = 0; i < data.signature()->returnCount(); ++i) {
+            if (i < expressionStack.size())
+                entry.enclosedExpressionStack.append(expressionStack[i]);
+            else {
+                Type type = data.signature()->returnType(i);
+                entry.enclosedExpressionStack.constructAndAppend(type, addBottom(m_currentBlock, type));
+            }
+        }
+    } else {
+        for (unsigned i = 0; i < data.signature()->returnCount(); ++i)
+            entry.enclosedExpressionStack.constructAndAppend(data.signature()->returnType(i), data.results[i]);
+    }
 
     // TopLevel does not have any code after this so we need to make sure we emit a return here.
-    if (data.type() == BlockType::TopLevel)
+    if (data.blockType() == BlockType::TopLevel)
         return addReturn(data, entry.enclosedExpressionStack);
 
     return { };
 }
 
-auto AirIRGenerator::addCall(uint32_t functionIndex, const Signature& signature, Vector<ExpressionType>& args, ExpressionType& result) -> PartialResult
+B3::PatchpointValue* AirIRGenerator::emitCallPatchpoint(BasicBlock* block, const Signature& signature, const ResultList& results, const Vector<TypedTmp>& args, Vector<ConstrainedTmp>&& patchArgs)
+{
+    auto* patchpoint = addPatchpoint(toB3ResultType(&signature));
+    patchpoint->effects.writesPinned = true;
+    patchpoint->effects.readsPinned = true;
+    patchpoint->clobberEarly(RegisterSet::macroScratchRegisters());
+    patchpoint->clobberLate(RegisterSet::volatileRegistersForJSCall());
+
+    CallInformation locations = wasmCallingConvention().callInformationFor(signature);
+    m_code.requestCallArgAreaSizeInBytes(WTF::roundUpToMultipleOf(stackAlignmentBytes(), locations.headerAndArgumentStackSizeInBytes));
+
+    size_t offset = patchArgs.size();
+    Checked<size_t> newSize = checkedSum<size_t>(patchArgs.size(), args.size());
+    RELEASE_ASSERT(!newSize.hasOverflowed());
+
+    patchArgs.grow(newSize.unsafeGet());
+    for (unsigned i = 0; i < args.size(); ++i)
+        patchArgs[i + offset] = ConstrainedTmp(args[i], locations.params[i]);
+
+    if (patchpoint->type() != B3::Void)
+        patchpoint->resultConstraints = WTFMove(locations.results);
+    emitPatchpoint(block, patchpoint, results, WTFMove(patchArgs));
+    return patchpoint;
+}
+
+auto AirIRGenerator::addCall(uint32_t functionIndex, const Signature& signature, Vector<ExpressionType>& args, ResultList& results) -> PartialResult
 {
     ASSERT(signature.argumentCount() == args.size());
 
     m_makesCalls = true;
 
-    Type returnType = signature.returnType();
-    if (returnType != Type::Void)
-        result = tmpForType(returnType);
+    for (unsigned i = 0; i < signature.returnCount(); ++i)
+        results.append(tmpForType(signature.returnType(i)));
 
     Vector<UnlinkedWasmToWasmCall>* unlinkedWasmToWasmCalls = &m_unlinkedWasmToWasmCalls;
 
     if (m_info.isImportedFunctionFromFunctionIndexSpace(functionIndex)) {
         m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast<uint32_t>(args.size()));
@@ -1958,95 +2126,71 @@
 
         append(BranchTest64, Arg::resCond(MacroAssembler::NonZero), targetInstance, targetInstance);
         m_currentBlock->setSuccessors(isWasmBlock, isEmbedderBlock);
 
         {
-            auto* patchpoint = addPatchpoint(toB3Type(returnType));
-            patchpoint->effects.writesPinned = true;
-            patchpoint->effects.readsPinned = true;
+            auto* patchpoint = emitCallPatchpoint(isWasmBlock, signature, results, args);
             // We need to clobber all potential pinned registers since we might be leaving the instance.
             // We pessimistically assume we could be calling to something that is bounds checking.
             // FIXME: We shouldn't have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
             patchpoint->clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
 
-            Vector<ConstrainedTmp> patchArgs;
-            wasmCallingConventionAir().setupCall(m_code, returnType, patchpoint, toTmpVector(args), [&] (Tmp tmp, B3::ValueRep rep) {
-                patchArgs.append({ tmp, rep });
-            });
-
             patchpoint->setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers& jit, const B3::StackmapGenerationParams&) {
                 AllowMacroScratchRegisterUsage allowScratch(jit);
                 CCallHelpers::Call call = jit.threadSafePatchableNearCall();
                 jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer& linkBuffer) {
                     unlinkedWasmToWasmCalls->append({ linkBuffer.locationOfNearCall<WasmEntryPtrTag>(call), functionIndex });
                 });
             });
 
-            emitPatchpoint(isWasmBlock, patchpoint, result, WTFMove(patchArgs));
             append(isWasmBlock, Jump);
             isWasmBlock->setSuccessors(continuation);
         }
 
         {
             auto jumpDestination = g64();
             append(isEmbedderBlock, Move, Arg::bigImm(Instance::offsetOfWasmToEmbedderStub(functionIndex)), jumpDestination);
             append(isEmbedderBlock, Add64, instanceValue(), jumpDestination);
             append(isEmbedderBlock, Move, Arg::addr(jumpDestination), jumpDestination);
 
-            auto* patchpoint = addPatchpoint(toB3Type(returnType));
-            patchpoint->effects.writesPinned = true;
-            patchpoint->effects.readsPinned = true;
+            Vector<ConstrainedTmp> jumpArgs;
+            jumpArgs.append({ jumpDestination, B3::ValueRep::SomeRegister });
+            auto* patchpoint = emitCallPatchpoint(isEmbedderBlock, signature, results, args, WTFMove(jumpArgs));
             // We need to clobber all potential pinned registers since we might be leaving the instance.
             // We pessimistically assume we could be calling to something that is bounds checking.
             // FIXME: We shouldn't have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
             patchpoint->clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
-
-            Vector<ConstrainedTmp> patchArgs;
-            patchArgs.append(jumpDestination);
-
-            wasmCallingConventionAir().setupCall(m_code, returnType, patchpoint, toTmpVector(args), [&] (Tmp tmp, B3::ValueRep rep) {
-                patchArgs.append({ tmp, rep });
-            });
-
-            patchpoint->setGenerator([returnType] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
+            patchpoint->setGenerator([] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
                 AllowMacroScratchRegisterUsage allowScratch(jit);
-                jit.call(params[returnType == Void ? 0 : 1].gpr(), WasmEntryPtrTag);
+                jit.call(params[params.proc().resultCount(params.value()->type())].gpr(), WasmEntryPtrTag);
             });
 
-            emitPatchpoint(isEmbedderBlock, patchpoint, result, WTFMove(patchArgs));
             append(isEmbedderBlock, Jump);
             isEmbedderBlock->setSuccessors(continuation);
         }
 
         m_currentBlock = continuation;
         // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
         restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, currentInstance, continuation);
     } else {
-        auto* patchpoint = addPatchpoint(toB3Type(returnType));
-        patchpoint->effects.writesPinned = true;
-        patchpoint->effects.readsPinned = true;
-
-        Vector<ConstrainedTmp> patchArgs;
-        wasmCallingConventionAir().setupCall(m_code, returnType, patchpoint, toTmpVector(args), [&] (Tmp tmp, B3::ValueRep rep) {
-            patchArgs.append({ tmp, rep });
-        });
-
+        auto* patchpoint = emitCallPatchpoint(m_currentBlock, signature, results, args);
+        // We need to clobber the size register since the LLInt always bounds checks
+        if (m_mode == MemoryMode::Signaling)
+            patchpoint->clobberLate(RegisterSet { PinnedRegisterInfo::get().sizeRegister });
         patchpoint->setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers& jit, const B3::StackmapGenerationParams&) {
             AllowMacroScratchRegisterUsage allowScratch(jit);
             CCallHelpers::Call call = jit.threadSafePatchableNearCall();
             jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer& linkBuffer) {
                 unlinkedWasmToWasmCalls->append({ linkBuffer.locationOfNearCall<WasmEntryPtrTag>(call), functionIndex });
             });
         });
-
-        emitPatchpoint(m_currentBlock, patchpoint, result, WTFMove(patchArgs));
     }
 
     return { };
 }
 
-auto AirIRGenerator::addCallIndirect(unsigned tableIndex, const Signature& signature, Vector<ExpressionType>& args, ExpressionType& result) -> PartialResult
+auto AirIRGenerator::addCallIndirect(unsigned tableIndex, const Signature& signature, Vector<ExpressionType>& args, ResultList& results) -> PartialResult
 {
     ExpressionType calleeIndex = args.takeLast();
     ASSERT(signature.argumentCount() == args.size());
     ASSERT(m_info.tableCount() > tableIndex);
     ASSERT(m_info.tables[tableIndex].type() == TableElementType::Funcref);
@@ -2171,43 +2315,38 @@
         m_currentBlock = continuation;
     }
 
     append(Move, Arg::addr(calleeCode), calleeCode);
 
-    Type returnType = signature.returnType();
-    if (returnType != Type::Void)
-        result = tmpForType(returnType);
+    Vector<ConstrainedTmp> extraArgs;
+    extraArgs.append(calleeCode);
+
+    for (unsigned i = 0; i < signature.returnCount(); ++i)
+        results.append(tmpForType(signature.returnType(i)));
+
+    auto* patchpoint = emitCallPatchpoint(m_currentBlock, signature, results, args, WTFMove(extraArgs));
 
-    auto* patch = addPatchpoint(toB3Type(returnType));
-    patch->effects.writesPinned = true;
-    patch->effects.readsPinned = true;
     // We need to clobber all potential pinned registers since we might be leaving the instance.
     // We pessimistically assume we're always calling something that is bounds checking so
     // because the wasm->wasm thunk unconditionally overrides the size registers.
     // FIXME: We should not have to do this, but the wasm->wasm stub assumes it can
     // use all the pinned registers as scratch: https://bugs.webkit.org/show_bug.cgi?id=172181
-    patch->clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
 
-    Vector<ConstrainedTmp> emitArgs;
-    emitArgs.append(calleeCode);
-    wasmCallingConventionAir().setupCall(m_code, returnType, patch, toTmpVector(args), [&] (Tmp tmp, B3::ValueRep rep) {
-        emitArgs.append({ tmp, rep });
-    });
-    patch->setGenerator([=] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
+    patchpoint->clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
+
+    patchpoint->setGenerator([=] (CCallHelpers& jit, const B3::StackmapGenerationParams& params) {
         AllowMacroScratchRegisterUsage allowScratch(jit);
-        jit.call(params[returnType == Void ? 0 : 1].gpr(), WasmEntryPtrTag);
+        jit.call(params[params.proc().resultCount(params.value()->type())].gpr(), WasmEntryPtrTag);
     });
 
-    emitPatchpoint(m_currentBlock, patch, result, WTFMove(emitArgs));
-
     // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, currentInstance, m_currentBlock);
 
     return { };
 }
 
-void AirIRGenerator::unify(const ExpressionType& dst, const ExpressionType& source)
+void AirIRGenerator::unify(const ExpressionType dst, const ExpressionType source)
 {
     ASSERT(isSubtype(source.type(), dst.type()));
     append(moveOpForValueType(dst.type()), source, dst);
 }
 
@@ -2217,21 +2356,40 @@
 
     for (size_t i = 0; i < result.size(); ++i)
         unify(result[result.size() - 1 - i], resultStack[resultStack.size() - 1 - i]);
 }
 
-void AirIRGenerator::dump(const Vector<ControlEntry>&, const Stack*)
+static void dumpExpressionStack(const CommaPrinter& comma, const AirIRGenerator::Stack& expressionStack)
+{
+    dataLog(comma, "ExpressionStack:");
+    for (const auto& expression : expressionStack)
+        dataLog(comma, expression.value());
+}
+
+void AirIRGenerator::dump(const ControlStack& controlStack, const Stack* stack)
 {
+    dataLogLn("Processing Graph:");
+    dataLog(m_code);
+    dataLogLn("With current block:", *m_currentBlock);
+    dataLogLn("Control stack:");
+    for (size_t i = controlStack.size(); i--;) {
+        dataLog("  ", controlStack[i].controlData, ": ");
+        CommaPrinter comma(", ", "");
+        dumpExpressionStack(comma, *stack);
+        stack = &controlStack[i].enclosedExpressionStack;
+        dataLogLn();
+    }
+    dataLogLn("\n");
 }
 
 auto AirIRGenerator::origin() -> B3::Origin
 {
     // FIXME: We should implement a way to give Inst's an origin.
     return B3::Origin();
 }
 
-Expected<std::unique_ptr<InternalFunction>, String> parseAndCompileAir(CompilationContext& compilationContext, const uint8_t* functionStart, size_t functionLength, const Signature& signature, Vector<UnlinkedWasmToWasmCall>& unlinkedWasmToWasmCalls, const ModuleInformation& info, MemoryMode mode, uint32_t functionIndex, TierUpCount* tierUp, ThrowWasmException throwWasmException)
+Expected<std::unique_ptr<InternalFunction>, String> parseAndCompileAir(CompilationContext& compilationContext, const FunctionData& function, const Signature& signature, Vector<UnlinkedWasmToWasmCall>& unlinkedWasmToWasmCalls, const ModuleInformation& info, MemoryMode mode, uint32_t functionIndex, TierUpCount* tierUp)
 {
     auto result = makeUnique<InternalFunction>();
 
     compilationContext.embedderEntrypointJIT = makeUnique<CCallHelpers>();
     compilationContext.wasmEntrypointJIT = makeUnique<CCallHelpers>();
@@ -2250,12 +2408,12 @@
     // optLevel=1.
     procedure.setNeedsUsedRegisters(false);
 
     procedure.setOptLevel(Options::webAssemblyBBQAirOptimizationLevel());
 
-    AirIRGenerator irGenerator(info, procedure, result.get(), unlinkedWasmToWasmCalls, mode, functionIndex, tierUp, throwWasmException, signature);
-    FunctionParser<AirIRGenerator> parser(irGenerator, functionStart, functionLength, signature, info);
+    AirIRGenerator irGenerator(info, procedure, result.get(), unlinkedWasmToWasmCalls, mode, functionIndex, tierUp, signature);
+    FunctionParser<AirIRGenerator> parser(irGenerator, function.data.data(), function.data.size(), signature, info);
     WASM_FAIL_IF_HELPER_FAILS(parser.parse());
 
 
     for (BasicBlock* block : code) {
         for (size_t i = 0; i < block->numSuccessors(); ++i)
@@ -2342,11 +2500,11 @@
         }
 
         return;
     }
 
-#if CPU(X86) || CPU(X86_64)
+#if CPU(X86_64)
     Tmp eax(X86Registers::eax);
     Tmp edx(X86Registers::edx);
 
     if (isSigned) {
         B3::Air::Opcode convertToDoubleWord;
@@ -2534,12 +2692,11 @@
         emitPatchpoint(patchpoint, result, arg);
         return { };
     }
 #endif
 
-    uint32_t (*popcount)(int32_t) = [] (int32_t value) -> uint32_t { return __builtin_popcount(value); };
-    emitCCall(popcount, result, arg);
+    emitCCall(&operationPopcount32, result, arg);
     return { };
 }
 
 template<>
 auto AirIRGenerator::addOp<OpType::I64Popcnt>(ExpressionType arg, ExpressionType& result) -> PartialResult
@@ -2556,12 +2713,11 @@
         emitPatchpoint(patchpoint, result, arg);
         return { };
     }
 #endif
 
-    uint64_t (*popcount)(int64_t) = [] (int64_t value) -> uint64_t { return __builtin_popcountll(value); };
-    emitCCall(popcount, result, arg);
+    emitCCall(&operationPopcount64, result, arg);
     return { };
 }
 
 template<>
 auto AirIRGenerator::addOp<F64ConvertUI64>(ExpressionType arg, ExpressionType& result) -> PartialResult
@@ -2841,11 +2997,11 @@
         }
         jit.truncateDoubleToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
     });
 
     result = g64();
-    emitPatchpoint(m_currentBlock, patchpoint, result, WTFMove(args));
+    emitPatchpoint(m_currentBlock, patchpoint, Vector<TypedTmp, 8> { result }, WTFMove(args));
     return { };
 }
 
 template<>
 auto AirIRGenerator::addOp<OpType::I64TruncSF32>(ExpressionType arg, ExpressionType& result) -> PartialResult
@@ -2898,11 +3054,11 @@
         signBitConstant = addConstant(Type::F32, bitwise_cast<uint32_t>(static_cast<float>(std::numeric_limits<uint64_t>::max() - std::numeric_limits<int64_t>::max())));
 
     auto* patchpoint = addPatchpoint(B3::Int64);
     patchpoint->effects = B3::Effects::none();
     patchpoint->clobber(RegisterSet::macroScratchRegisters());
-    Vector<ConstrainedTmp> args;
+    Vector<ConstrainedTmp, 2> args;
     args.append(arg);
     if (isX86()) {
         args.append(signBitConstant);
         patchpoint->numFPScratchRegisters = 1;
     }
@@ -2916,11 +3072,11 @@
         }
         jit.truncateFloatToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
     });
 
     result = g64();
-    emitPatchpoint(m_currentBlock, patchpoint, result, WTFMove(args));
+    emitPatchpoint(m_currentBlock, patchpoint, Vector<TypedTmp, 8> { result }, WTFMove(args));
 
     return { };
 }
 
 auto AirIRGenerator::addShift(Type type, B3::Air::Opcode op, ExpressionType value, ExpressionType shift, ExpressionType& result) -> PartialResult
