<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;AirAllocateRegistersAndStackAndGenerateCode.h&quot;
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;AirArgInlines.h&quot;
 32 #include &quot;AirBlockInsertionSet.h&quot;
 33 #include &quot;AirCode.h&quot;
 34 #include &quot;AirHandleCalleeSaves.h&quot;
 35 #include &quot;AirLowerStackArgs.h&quot;
 36 #include &quot;AirStackAllocation.h&quot;
 37 #include &quot;AirTmpMap.h&quot;
 38 #include &quot;CCallHelpers.h&quot;
 39 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
 40 
 41 namespace JSC { namespace B3 { namespace Air {
 42 
 43 GenerateAndAllocateRegisters::GenerateAndAllocateRegisters(Code&amp; code)
 44     : m_code(code)
 45     , m_map(code)
 46 { }
 47 
 48 ALWAYS_INLINE void GenerateAndAllocateRegisters::checkConsistency()
 49 {
 50     // This isn&#39;t exactly the right option for this but adding a new one for just this seems silly.
 51     if (Options::validateGraph() || Options::validateGraphAtEachPhase()) {
 52         m_code.forEachTmp([&amp;] (Tmp tmp) {
 53             Reg reg = m_map[tmp].reg;
 54             if (!reg)
 55                 return;
 56 
 57             ASSERT(!m_availableRegs[tmp.bank()].contains(reg));
 58             ASSERT(m_currentAllocation-&gt;at(reg) == tmp);
 59         });
 60 
 61         for (Reg reg : RegisterSet::allRegisters()) {
 62             if (isDisallowedRegister(reg))
 63                 continue;
 64 
 65             Tmp tmp = m_currentAllocation-&gt;at(reg);
 66             if (!tmp) {
 67                 ASSERT(m_availableRegs[bankForReg(reg)].contains(reg));
 68                 continue;
 69             }
 70 
 71             ASSERT(!m_availableRegs[tmp.bank()].contains(reg));
 72             ASSERT(m_map[tmp].reg == reg);
 73         }
 74     }
 75 }
 76 
 77 void GenerateAndAllocateRegisters::buildLiveRanges(UnifiedTmpLiveness&amp; liveness)
 78 {
 79     m_liveRangeEnd = TmpMap&lt;size_t&gt;(m_code, 0);
 80 
 81     m_globalInstIndex = 0;
 82     for (BasicBlock* block : m_code) {
 83         for (Tmp tmp : liveness.liveAtHead(block)) {
 84             if (!tmp.isReg())
 85                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 86         }
 87         ++m_globalInstIndex;
 88         for (Inst&amp; inst : *block) {
 89             inst.forEachTmpFast([&amp;] (Tmp tmp) {
 90                 if (!tmp.isReg())
 91                     m_liveRangeEnd[tmp] = m_globalInstIndex;
 92             });
 93             ++m_globalInstIndex;
 94         }
 95         for (Tmp tmp : liveness.liveAtTail(block)) {
 96             if (!tmp.isReg())
 97                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 98         }
 99         ++m_globalInstIndex;
100     }
101 }
102 
103 void GenerateAndAllocateRegisters::insertBlocksForFlushAfterTerminalPatchpoints()
104 {
105     BlockInsertionSet blockInsertionSet(m_code);
106     for (BasicBlock* block : m_code) {
107         Inst&amp; inst = block-&gt;last();
108         if (inst.kind.opcode != Patch)
109             continue;
110 
111         HashMap&lt;Tmp, Arg*&gt; needToDef;
112 
113         inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
114             if (!arg.isTmp())
115                 return;
116             Tmp tmp = arg.tmp();
117             if (Arg::isAnyDef(role) &amp;&amp; !tmp.isReg())
118                 needToDef.add(tmp, &amp;arg);
119         });
120 
121         if (needToDef.isEmpty())
122             continue;
123 
124         for (FrequentedBlock&amp; frequentedSuccessor : block-&gt;successors()) {
125             BasicBlock* successor = frequentedSuccessor.block();
126             BasicBlock* newBlock = blockInsertionSet.insertBefore(successor, successor-&gt;frequency());
127             newBlock-&gt;appendInst(Inst(Jump, inst.origin));
128             newBlock-&gt;setSuccessors(successor);
129             newBlock-&gt;addPredecessor(block);
130             frequentedSuccessor.block() = newBlock;
131             successor-&gt;replacePredecessor(block, newBlock);
132 
133             m_blocksAfterTerminalPatchForSpilling.add(newBlock, PatchSpillData { CCallHelpers::Jump(), CCallHelpers::Label(), needToDef });
134         }
135     }
136 
137     blockInsertionSet.execute();
138 }
139 
140 static ALWAYS_INLINE CCallHelpers::Address callFrameAddr(CCallHelpers&amp; jit, intptr_t offsetFromFP)
141 {
142     if (isX86()) {
143         ASSERT(Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP).isValidForm(Width64));
144         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
145     }
146 
147     ASSERT(pinnedExtendedOffsetAddrRegister());
148     auto addr = Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP);
149     if (addr.isValidForm(Width64))
150         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
151     GPRReg reg = *pinnedExtendedOffsetAddrRegister();
152     jit.move(CCallHelpers::TrustedImmPtr(offsetFromFP), reg);
153     jit.add64(GPRInfo::callFrameRegister, reg);
154     return CCallHelpers::Address(reg);
155 }
156 
157 ALWAYS_INLINE void GenerateAndAllocateRegisters::release(Tmp tmp, Reg reg)
158 {
159     ASSERT(reg);
160     ASSERT(m_currentAllocation-&gt;at(reg) == tmp);
161     m_currentAllocation-&gt;at(reg) = Tmp();
162     ASSERT(!m_availableRegs[tmp.bank()].contains(reg));
163     m_availableRegs[tmp.bank()].set(reg);
164     ASSERT(m_map[tmp].reg == reg);
165     m_map[tmp].reg = Reg();
166 }
167 
168 
169 ALWAYS_INLINE void GenerateAndAllocateRegisters::flush(Tmp tmp, Reg reg)
170 {
171     ASSERT(tmp);
172     intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
173     if (tmp.isGP())
174         m_jit-&gt;store64(reg.gpr(), callFrameAddr(*m_jit, offset));
175     else
176         m_jit-&gt;storeDouble(reg.fpr(), callFrameAddr(*m_jit, offset));
177 }
178 
179 ALWAYS_INLINE void GenerateAndAllocateRegisters::spill(Tmp tmp, Reg reg)
180 {
181     ASSERT(reg);
182     ASSERT(m_map[tmp].reg == reg);
183     flush(tmp, reg);
184     release(tmp, reg);
185 }
186 
187 ALWAYS_INLINE void GenerateAndAllocateRegisters::alloc(Tmp tmp, Reg reg, bool isDef)
188 {
189     if (Tmp occupyingTmp = m_currentAllocation-&gt;at(reg))
190         spill(occupyingTmp, reg);
191     else {
192         ASSERT(!m_currentAllocation-&gt;at(reg));
193         ASSERT(m_availableRegs[tmp.bank()].get(reg));
194     }
195 
196     m_map[tmp].reg = reg;
197     m_availableRegs[tmp.bank()].clear(reg);
198     m_currentAllocation-&gt;at(reg) = tmp;
199 
200     if (!isDef) {
201         intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
202         if (tmp.bank() == GP)
203             m_jit-&gt;load64(callFrameAddr(*m_jit, offset), reg.gpr());
204         else
205             m_jit-&gt;loadDouble(callFrameAddr(*m_jit, offset), reg.fpr());
206     }
207 }
208 
209 ALWAYS_INLINE void GenerateAndAllocateRegisters::freeDeadTmpsIfNeeded()
210 {
211     if (m_didAlreadyFreeDeadSlots)
212         return;
213 
214     m_didAlreadyFreeDeadSlots = true;
215     for (size_t i = 0; i &lt; m_currentAllocation-&gt;size(); ++i) {
216         Tmp tmp = m_currentAllocation-&gt;at(i);
217         if (!tmp)
218             continue;
219         if (tmp.isReg())
220             continue;
221         if (m_liveRangeEnd[tmp] &gt;= m_globalInstIndex)
222             continue;
223 
224         release(tmp, Reg::fromIndex(i));
225     }
226 }
227 
228 ALWAYS_INLINE bool GenerateAndAllocateRegisters::assignTmp(Tmp&amp; tmp, Bank bank, bool isDef)
229 {
230     ASSERT(!tmp.isReg());
231     if (Reg reg = m_map[tmp].reg) {
232         ASSERT(!m_namedDefdRegs.contains(reg));
233         tmp = Tmp(reg);
234         m_namedUsedRegs.set(reg);
235         ASSERT(!m_availableRegs[bank].get(reg));
236         return true;
237     }
238 
239     if (!m_availableRegs[bank].numberOfSetRegisters())
240         freeDeadTmpsIfNeeded();
241 
242     if (m_availableRegs[bank].numberOfSetRegisters()) {
243         // We first take an available register.
244         for (Reg reg : m_registers[bank]) {
245             if (m_namedUsedRegs.contains(reg) || m_namedDefdRegs.contains(reg))
246                 continue;
247             if (!m_availableRegs[bank].contains(reg))
248                 continue;
249             m_namedUsedRegs.set(reg); // At this point, it doesn&#39;t matter if we add it to the m_namedUsedRegs or m_namedDefdRegs. We just need to mark that we can&#39;t use it again.
250             alloc(tmp, reg, isDef);
251             tmp = Tmp(reg);
252             return true;
253         }
254 
255         RELEASE_ASSERT_NOT_REACHED();
256     }
257 
258     // Nothing was available, let&#39;s make some room.
259     for (Reg reg : m_registers[bank]) {
260         if (m_namedUsedRegs.contains(reg) || m_namedDefdRegs.contains(reg))
261             continue;
262 
263         m_namedUsedRegs.set(reg);
264 
265         alloc(tmp, reg, isDef);
266         tmp = Tmp(reg);
267         return true;
268     }
269 
270     // This can happen if we have a #WarmAnys &gt; #Available registers
271     return false;
272 }
273 
274 ALWAYS_INLINE bool GenerateAndAllocateRegisters::isDisallowedRegister(Reg reg)
275 {
276     return !m_allowedRegisters.get(reg);
277 }
278 
279 void GenerateAndAllocateRegisters::prepareForGeneration()
280 {
281     // We pessimistically assume we use all callee saves.
282     handleCalleeSaves(m_code, RegisterSet::calleeSaveRegisters());
283     allocateEscapedStackSlots(m_code);
284 
285     insertBlocksForFlushAfterTerminalPatchpoints();
286 
287 #if ASSERT_ENABLED
288     m_code.forEachTmp([&amp;] (Tmp tmp) {
289         ASSERT(!tmp.isReg());
290         m_allTmps[tmp.bank()].append(tmp);
291     });
292 #endif
293 
294     m_liveness = makeUnique&lt;UnifiedTmpLiveness&gt;(m_code);
295 
296     {
297         buildLiveRanges(*m_liveness);
298 
299         Vector&lt;StackSlot*, 16&gt; freeSlots;
300         Vector&lt;StackSlot*, 4&gt; toFree;
301         m_globalInstIndex = 0;
302         for (BasicBlock* block : m_code) {
303             auto assignStackSlotToTmp = [&amp;] (Tmp tmp) {
304                 if (tmp.isReg())
305                     return;
306 
307                 TmpData&amp; data = m_map[tmp];
308                 if (data.spillSlot) {
309                     if (m_liveRangeEnd[tmp] == m_globalInstIndex)
310                         toFree.append(data.spillSlot);
311                     return;
312                 }
313 
314                 if (freeSlots.size())
315                     data.spillSlot = freeSlots.takeLast();
316                 else
317                     data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);
318                 data.reg = Reg();
319             };
320 
321             auto flushToFreeList = [&amp;] {
322                 for (auto* stackSlot : toFree)
323                     freeSlots.append(stackSlot);
324                 toFree.clear();
325             };
326 
327             for (Tmp tmp : m_liveness-&gt;liveAtHead(block))
328                 assignStackSlotToTmp(tmp);
329             flushToFreeList();
330 
331             ++m_globalInstIndex;
332 
333             for (Inst&amp; inst : *block) {
334                 Vector&lt;Tmp, 4&gt; seenTmps;
335                 inst.forEachTmpFast([&amp;] (Tmp tmp) {
336                     if (seenTmps.contains(tmp))
337                         return;
338                     seenTmps.append(tmp);
339                     assignStackSlotToTmp(tmp);
340                 });
341 
342                 flushToFreeList();
343                 ++m_globalInstIndex;
344             }
345 
346             for (Tmp tmp : m_liveness-&gt;liveAtTail(block))
347                 assignStackSlotToTmp(tmp);
348             flushToFreeList();
349 
350             ++m_globalInstIndex;
351         }
352     }
353 
354     m_allowedRegisters = RegisterSet();
355 
356     forEachBank([&amp;] (Bank bank) {
357         m_registers[bank] = m_code.regsInPriorityOrder(bank);
358 
359         for (Reg reg : m_registers[bank]) {
360             m_allowedRegisters.set(reg);
361             TmpData&amp; data = m_map[Tmp(reg)];
362             data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);
363             data.reg = Reg();
364         }
365     });
366 
367     {
368         unsigned nextIndex = 0;
369         for (StackSlot* slot : m_code.stackSlots()) {
370             if (slot-&gt;isLocked())
371                 continue;
372             intptr_t offset = -static_cast&lt;intptr_t&gt;(m_code.frameSize()) - static_cast&lt;intptr_t&gt;(nextIndex) * 8 - 8;
373             ++nextIndex;
374             slot-&gt;setOffsetFromFP(offset);
375         }
376     }
377 
378     updateFrameSizeBasedOnStackSlots(m_code);
379     m_code.setStackIsAllocated(true);
380 
381     lowerStackArgs(m_code);
382 
383 #if ASSERT_ENABLED
384     // Verify none of these passes add any tmps.
385     forEachBank([&amp;] (Bank bank) {
386         ASSERT(m_allTmps[bank].size() == m_code.numTmps(bank));
387     });
388 
389     {
390         // Verify that lowerStackArgs didn&#39;t change Tmp liveness at the boundaries for the Tmps and Registers we model.
391         UnifiedTmpLiveness liveness(m_code);
392         for (BasicBlock* block : m_code) {
393             auto assertLivenessAreEqual = [&amp;] (auto a, auto b) {
394                 HashSet&lt;Tmp&gt; livenessA;
395                 HashSet&lt;Tmp&gt; livenessB;
396                 for (Tmp tmp : a) {
397                     if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
398                         continue;
399                     livenessA.add(tmp);
400                 }
401                 for (Tmp tmp : b) {
402                     if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
403                         continue;
404                     livenessB.add(tmp);
405                 }
406 
407                 ASSERT(livenessA == livenessB);
408             };
409 
410             assertLivenessAreEqual(m_liveness-&gt;liveAtHead(block), liveness.liveAtHead(block));
411             assertLivenessAreEqual(m_liveness-&gt;liveAtTail(block), liveness.liveAtTail(block));
412         }
413     }
414 #endif
415 }
416 
417 void GenerateAndAllocateRegisters::generate(CCallHelpers&amp; jit)
418 {
419     m_jit = &amp;jit;
420 
421     TimingScope timingScope(&quot;Air::generateAndAllocateRegisters&quot;);
422 
423     DisallowMacroScratchRegisterUsage disallowScratch(*m_jit);
424 
425     buildLiveRanges(*m_liveness);
426 
427     IndexMap&lt;BasicBlock*, IndexMap&lt;Reg, Tmp&gt;&gt; currentAllocationMap(m_code.size());
428     {
429         IndexMap&lt;Reg, Tmp&gt; defaultCurrentAllocation(Reg::maxIndex() + 1);
430         for (BasicBlock* block : m_code)
431             currentAllocationMap[block] = defaultCurrentAllocation;
432 
433         // The only things live that are in registers at the root blocks are
434         // the explicitly named registers that are live.
435 
436         for (unsigned i = m_code.numEntrypoints(); i--;) {
437             BasicBlock* entrypoint = m_code.entrypoint(i).block();
438             for (Tmp tmp : m_liveness-&gt;liveAtHead(entrypoint)) {
439                 if (tmp.isReg())
440                     currentAllocationMap[entrypoint][tmp.reg()] = tmp;
441             }
442         }
443     }
444 
445     // And now, we generate code.
446     GenerationContext context;
447     context.code = &amp;m_code;
448     context.blockLabels.resize(m_code.size());
449     for (BasicBlock* block : m_code)
450         context.blockLabels[block] = Box&lt;CCallHelpers::Label&gt;::create();
451     IndexMap&lt;BasicBlock*, CCallHelpers::JumpList&gt; blockJumps(m_code.size());
452 
453     auto link = [&amp;] (CCallHelpers::Jump jump, BasicBlock* target) {
454         if (context.blockLabels[target]-&gt;isSet()) {
455             jump.linkTo(*context.blockLabels[target], m_jit);
456             return;
457         }
458 
459         blockJumps[target].append(jump);
460     };
461 
462     Disassembler* disassembler = m_code.disassembler();
463 
464     m_globalInstIndex = 0;
465 
466     for (BasicBlock* block : m_code) {
467         context.currentBlock = block;
468         context.indexInBlock = UINT_MAX;
469         blockJumps[block].link(m_jit);
470         CCallHelpers::Label label = m_jit-&gt;label();
471         *context.blockLabels[block] = label;
472 
473         if (disassembler)
474             disassembler-&gt;startBlock(block, *m_jit);
475 
476         if (Optional&lt;unsigned&gt; entrypointIndex = m_code.entrypointIndex(block)) {
477             ASSERT(m_code.isEntrypoint(block));
478             if (disassembler)
479                 disassembler-&gt;startEntrypoint(*m_jit);
480 
481             m_code.prologueGeneratorForEntrypoint(*entrypointIndex)-&gt;run(*m_jit, m_code);
482 
483             if (disassembler)
484                 disassembler-&gt;endEntrypoint(*m_jit);
485         } else
486             ASSERT(!m_code.isEntrypoint(block));
487 
488         auto startLabel = m_jit-&gt;labelIgnoringWatchpoints();
489 
490         {
491             auto iter = m_blocksAfterTerminalPatchForSpilling.find(block);
492             if (iter != m_blocksAfterTerminalPatchForSpilling.end()) {
493                 auto&amp; data = iter-&gt;value;
494                 data.jump = m_jit-&gt;jump();
495                 data.continueLabel = m_jit-&gt;label();
496             }
497         }
498 
499         forEachBank([&amp;] (Bank bank) {
500 #if ASSERT_ENABLED
501             // By default, everything is spilled at block boundaries. We do this after we process each block
502             // so we don&#39;t have to walk all Tmps, since #Tmps &gt;&gt; #Available regs. Instead, we walk the register file at
503             // each block boundary and clear entries in this map.
504             for (Tmp tmp : m_allTmps[bank])
505                 ASSERT(m_map[tmp].reg == Reg());
506 #endif
507 
508             RegisterSet availableRegisters;
509             for (Reg reg : m_registers[bank])
510                 availableRegisters.set(reg);
511             m_availableRegs[bank] = WTFMove(availableRegisters);
512         });
513 
514         IndexMap&lt;Reg, Tmp&gt;&amp; currentAllocation = currentAllocationMap[block];
515         m_currentAllocation = &amp;currentAllocation;
516 
517         for (unsigned i = 0; i &lt; currentAllocation.size(); ++i) {
518             Tmp tmp = currentAllocation[i];
519             if (!tmp)
520                 continue;
521             Reg reg = Reg::fromIndex(i);
522             m_map[tmp].reg = reg;
523             m_availableRegs[tmp.bank()].clear(reg);
524         }
525 
526         ++m_globalInstIndex;
527 
528         bool isReplayingSameInst = false;
529         for (size_t instIndex = 0; instIndex &lt; block-&gt;size(); ++instIndex) {
530             checkConsistency();
531 
532             if (instIndex &amp;&amp; !isReplayingSameInst)
533                 startLabel = m_jit-&gt;labelIgnoringWatchpoints();
534 
535             context.indexInBlock = instIndex;
536 
537             Inst&amp; inst = block-&gt;at(instIndex);
538 
539             m_didAlreadyFreeDeadSlots = false;
540 
541             m_namedUsedRegs = RegisterSet();
542             m_namedDefdRegs = RegisterSet();
543 
544             bool needsToGenerate = ([&amp;] () -&gt; bool {
545                 // FIXME: We should consider trying to figure out if we can also elide Mov32s
546                 if (!(inst.kind.opcode == Move || inst.kind.opcode == MoveDouble))
547                     return true;
548 
549                 ASSERT(inst.args.size() &gt;= 2);
550                 Arg source = inst.args[0];
551                 Arg dest = inst.args[1];
552                 if (!source.isTmp() || !dest.isTmp())
553                     return true;
554 
555                 // FIXME: We don&#39;t track where the last use of a reg is globally so we don&#39;t know where we can elide them.
556                 ASSERT(source.isReg() || m_liveRangeEnd[source.tmp()] &gt;= m_globalInstIndex);
557                 if (source.isReg() || m_liveRangeEnd[source.tmp()] != m_globalInstIndex)
558                     return true;
559 
560                 // If we are doing a self move at the end of the temps liveness we can trivially elide the move.
561                 if (source == dest)
562                     return false;
563 
564                 Reg sourceReg = m_map[source.tmp()].reg;
565                 // If the value is not already materialized into a register we may still move it into one so let the normal generation code run.
566                 if (!sourceReg)
567                     return true;
568 
569                 ASSERT(m_currentAllocation-&gt;at(sourceReg) == source.tmp());
570 
571                 if (dest.isReg() &amp;&amp; dest.reg() != sourceReg)
572                     return true;
573 
574                 if (Reg oldReg = m_map[dest.tmp()].reg)
575                     release(dest.tmp(), oldReg);
576 
577                 m_map[dest.tmp()].reg = sourceReg;
578                 m_currentAllocation-&gt;at(sourceReg) = dest.tmp();
579                 m_map[source.tmp()].reg = Reg();
580                 return false;
581             })();
582             checkConsistency();
583 
584             inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
585                 if (!arg.isTmp())
586                     return;
587 
588                 Tmp tmp = arg.tmp();
589                 if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
590                     return;
591 
592                 if (tmp.isReg()) {
593                     if (Arg::isAnyUse(role))
594                         m_namedUsedRegs.set(tmp.reg());
595                     if (Arg::isAnyDef(role))
596                         m_namedDefdRegs.set(tmp.reg());
597                 }
598 
599                 // We convert any cold uses that are already in the stack to just point to
600                 // the canonical stack location.
601                 if (!Arg::isColdUse(role))
602                     return;
603 
604                 if (!inst.admitsStack(arg))
605                     return;
606 
607                 auto&amp; entry = m_map[tmp];
608                 if (!entry.reg) {
609                     // We&#39;re a cold use, and our current location is already on the stack. Just use that.
610                     arg = Arg::addr(Tmp(GPRInfo::callFrameRegister), entry.spillSlot-&gt;offsetFromFP());
611                 }
612             });
613 
614             RegisterSet clobberedRegisters;
615             {
616                 Inst* nextInst = block-&gt;get(instIndex + 1);
617                 if (inst.kind.opcode == Patch || (nextInst &amp;&amp; nextInst-&gt;kind.opcode == Patch)) {
618                     if (inst.kind.opcode == Patch)
619                         clobberedRegisters.merge(inst.extraClobberedRegs());
620                     if (nextInst &amp;&amp; nextInst-&gt;kind.opcode == Patch)
621                         clobberedRegisters.merge(nextInst-&gt;extraEarlyClobberedRegs());
622 
623                     clobberedRegisters.filter(m_allowedRegisters);
624                     clobberedRegisters.exclude(m_namedDefdRegs);
625 
626                     m_namedDefdRegs.merge(clobberedRegisters);
627                 }
628             }
629 
630             auto allocNamed = [&amp;] (const RegisterSet&amp; named, bool isDef) {
631                 for (Reg reg : named) {
632                     if (Tmp occupyingTmp = currentAllocation[reg]) {
633                         if (occupyingTmp == Tmp(reg))
634                             continue;
635                     }
636 
637                     freeDeadTmpsIfNeeded(); // We don&#39;t want to spill a dead tmp.
638                     alloc(Tmp(reg), reg, isDef);
639                 }
640             };
641 
642             allocNamed(m_namedUsedRegs, false); // Must come before the defd registers since we may use and def the same register.
643             allocNamed(m_namedDefdRegs, true);
644 
645             if (needsToGenerate) {
646                 auto tryAllocate = [&amp;] {
647                     Vector&lt;Tmp*, 8&gt; usesToAlloc;
648                     Vector&lt;Tmp*, 8&gt; defsToAlloc;
649 
650                     inst.forEachTmp([&amp;] (Tmp&amp; tmp, Arg::Role role, Bank, Width) {
651                         if (tmp.isReg())
652                             return;
653 
654                         // We treat Use+Def as a use.
655                         if (Arg::isAnyUse(role))
656                             usesToAlloc.append(&amp;tmp);
657                         else if (Arg::isAnyDef(role))
658                             defsToAlloc.append(&amp;tmp);
659                     });
660 
661                     auto tryAllocateTmps = [&amp;] (auto&amp; vector, bool isDef) {
662                         bool success = true;
663                         for (Tmp* tmp : vector)
664                             success &amp;= assignTmp(*tmp, tmp-&gt;bank(), isDef);
665                         return success;
666                     };
667 
668                     // We first handle uses, then defs. We want to be able to tell the register allocator
669                     // which tmps need to be loaded from memory into their assigned register. Those such
670                     // tmps are uses. Defs don&#39;t need to be reloaded since we&#39;re defining them. However,
671                     // some tmps may both be used and defd. So we handle uses first since forEachTmp could
672                     // walk uses/defs in any order.
673                     bool success = true;
674                     success &amp;= tryAllocateTmps(usesToAlloc, false);
675                     success &amp;= tryAllocateTmps(defsToAlloc, true);
676                     return success;
677                 };
678 
679                 // We first allocate trying to give any Tmp a register. If that makes us exhaust the
680                 // available registers, we convert anything that accepts stack to be a stack addr
681                 // instead. This can happen for programs Insts that take in many args, but most
682                 // args can just be stack values.
683                 bool success = tryAllocate();
684                 if (!success) {
685                     RELEASE_ASSERT(!isReplayingSameInst); // We should only need to do the below at most once per inst.
686 
687                     // We need to capture the register state before we start spilling things
688                     // since we may have multiple arguments that are the same register.
689                     IndexMap&lt;Reg, Tmp&gt; allocationSnapshot = currentAllocation;
690 
691                     // We rewind this Inst to be in its previous state, however, if any arg admits stack,
692                     // we move to providing that arg in stack form. This will allow us to fully allocate
693                     // this inst when we rewind.
694                     inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role, Bank, Width) {
695                         if (!arg.isTmp())
696                             return;
697 
698                         Tmp tmp = arg.tmp();
699                         if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
700                             return;
701 
702                         if (tmp.isReg()) {
703                             Tmp originalTmp = allocationSnapshot[tmp.reg()];
704                             if (originalTmp.isReg()) {
705                                 ASSERT(tmp.reg() == originalTmp.reg());
706                                 // This means this Inst referred to this reg directly. We leave these as is.
707                                 return;
708                             }
709                             tmp = originalTmp;
710                         }
711 
712                         if (!inst.admitsStack(arg)) {
713                             arg = tmp;
714                             return;
715                         }
716 
717                         auto&amp; entry = m_map[tmp];
718                         if (Reg reg = entry.reg)
719                             spill(tmp, reg);
720 
721                         arg = Arg::addr(Tmp(GPRInfo::callFrameRegister), entry.spillSlot-&gt;offsetFromFP());
722                     });
723 
724                     --instIndex;
725                     isReplayingSameInst = true;
726                     continue;
727                 }
728 
729                 isReplayingSameInst = false;
730             }
731 
732             if (m_code.needsUsedRegisters() &amp;&amp; inst.kind.opcode == Patch) {
733                 freeDeadTmpsIfNeeded();
734                 RegisterSet registerSet;
735                 for (size_t i = 0; i &lt; currentAllocation.size(); ++i) {
736                     if (currentAllocation[i])
737                         registerSet.set(Reg::fromIndex(i));
738                 }
739                 inst.reportUsedRegisters(registerSet);
740             }
741 
742             if (inst.isTerminal() &amp;&amp; block-&gt;numSuccessors()) {
743                 // By default, we spill everything between block boundaries. However, we have a small
744                 // heuristic to pass along register state. We should eventually make this better.
745                 // What we do now is if we have a successor with a single predecessor (us), and we
746                 // haven&#39;t yet generated code for it, we give it our register state. If all our successors
747                 // can take on our register state, we don&#39;t flush at the end of this block.
748 
749                 bool everySuccessorGetsOurRegisterState = true;
750                 for (unsigned i = 0; i &lt; block-&gt;numSuccessors(); ++i) {
751                     BasicBlock* successor = block-&gt;successorBlock(i);
752                     if (successor-&gt;numPredecessors() == 1 &amp;&amp; !context.blockLabels[successor]-&gt;isSet())
753                         currentAllocationMap[successor] = currentAllocation;
754                     else
755                         everySuccessorGetsOurRegisterState = false;
756                 }
757                 if (!everySuccessorGetsOurRegisterState) {
758                     for (Tmp tmp : m_liveness-&gt;liveAtTail(block)) {
759                         if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
760                             continue;
761                         if (Reg reg = m_map[tmp].reg)
762                             flush(tmp, reg);
763                     }
764                 }
765             }
766 
767             if (!inst.isTerminal()) {
768                 CCallHelpers::Jump jump;
769                 if (needsToGenerate)
770                     jump = inst.generate(*m_jit, context);
771                 ASSERT_UNUSED(jump, !jump.isSet());
772 
773                 for (Reg reg : clobberedRegisters) {
774                     Tmp tmp(reg);
775                     ASSERT(currentAllocation[reg] == tmp);
776                     m_availableRegs[tmp.bank()].set(reg);
777                     m_currentAllocation-&gt;at(reg) = Tmp();
778                     m_map[tmp].reg = Reg();
779                 }
780             } else {
781                 ASSERT(needsToGenerate);
782                 if (inst.kind.opcode == Jump &amp;&amp; block-&gt;successorBlock(0) == m_code.findNextBlock(block))
783                     needsToGenerate = false;
784 
785                 if (isReturn(inst.kind.opcode)) {
786                     needsToGenerate = false;
787 
788                     // We currently don&#39;t represent the full epilogue in Air, so we need to
789                     // have this override.
790                     if (m_code.frameSize()) {
791                         m_jit-&gt;emitRestore(m_code.calleeSaveRegisterAtOffsetList());
792                         m_jit-&gt;emitFunctionEpilogue();
793                     } else
794                         m_jit-&gt;emitFunctionEpilogueWithEmptyFrame();
795                     m_jit-&gt;ret();
796                 }
797 
798                 if (needsToGenerate) {
799                     CCallHelpers::Jump jump = inst.generate(*m_jit, context);
800 
801                     // The jump won&#39;t be set for patchpoints. It won&#39;t be set for Oops because then it won&#39;t have
802                     // any successors.
803                     if (jump.isSet()) {
804                         switch (block-&gt;numSuccessors()) {
805                         case 1:
806                             link(jump, block-&gt;successorBlock(0));
807                             break;
808                         case 2:
809                             link(jump, block-&gt;successorBlock(0));
810                             if (block-&gt;successorBlock(1) != m_code.findNextBlock(block))
811                                 link(m_jit-&gt;jump(), block-&gt;successorBlock(1));
812                             break;
813                         default:
814                             RELEASE_ASSERT_NOT_REACHED();
815                             break;
816                         }
817                     }
818                 }
819             }
820 
821             auto endLabel = m_jit-&gt;labelIgnoringWatchpoints();
822             if (disassembler)
823                 disassembler-&gt;addInst(&amp;inst, startLabel, endLabel);
824 
825             ++m_globalInstIndex;
826         }
827 
828         // Registers usually get spilled at block boundaries. We do it this way since we don&#39;t
829         // want to iterate the entire TmpMap, since usually #Tmps &gt;&gt; #Regs. We may not actually spill
830         // all registers, but at the top of this loop we handle that case by pre-populating register
831         // state. Here, we just clear this map. After this loop, this map should contain only
832         // null entries.
833         for (size_t i = 0; i &lt; currentAllocation.size(); ++i) {
834             if (Tmp tmp = currentAllocation[i])
835                 m_map[tmp].reg = Reg();
836         }
837 
838         ++m_globalInstIndex;
839     }
840 
841     for (auto&amp; entry : m_blocksAfterTerminalPatchForSpilling) {
842         entry.value.jump.linkTo(m_jit-&gt;label(), m_jit);
843         const HashMap&lt;Tmp, Arg*&gt;&amp; spills = entry.value.defdTmps;
844         for (auto&amp; entry : spills) {
845             Arg* arg = entry.value;
846             if (!arg-&gt;isTmp())
847                 continue;
848             Tmp originalTmp = entry.key;
849             Tmp currentTmp = arg-&gt;tmp();
850             ASSERT_WITH_MESSAGE(currentTmp.isReg(), &quot;We already did register allocation so we should have assigned this Tmp to a register.&quot;);
851             flush(originalTmp, currentTmp.reg());
852         }
853         m_jit-&gt;jump().linkTo(entry.value.continueLabel, m_jit);
854     }
855 
856     context.currentBlock = nullptr;
857     context.indexInBlock = UINT_MAX;
858 
859     Vector&lt;CCallHelpers::Label&gt; entrypointLabels(m_code.numEntrypoints());
860     for (unsigned i = m_code.numEntrypoints(); i--;)
861         entrypointLabels[i] = *context.blockLabels[m_code.entrypoint(i).block()];
862     m_code.setEntrypointLabels(WTFMove(entrypointLabels));
863 
864     if (disassembler)
865         disassembler-&gt;startLatePath(*m_jit);
866 
867     // FIXME: Make late paths have Origins: https://bugs.webkit.org/show_bug.cgi?id=153689
868     for (auto&amp; latePath : context.latePaths)
869         latePath-&gt;run(*m_jit, context);
870 
871     if (disassembler)
872         disassembler-&gt;endLatePath(*m_jit);
873 }
874 
875 } } } // namespace JSC::B3::Air
876 
877 #endif // ENABLE(B3_JIT)
    </pre>
  </body>
</html>