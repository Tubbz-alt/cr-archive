<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmOperations.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;WasmOperations.h&quot;
 28 
 29 #if ENABLE(WEBASSEMBLY)
 30 
 31 #include &quot;ButterflyInlines.h&quot;
 32 #include &quot;FrameTracers.h&quot;
 33 #include &quot;IteratorOperations.h&quot;
 34 #include &quot;JITExceptions.h&quot;
 35 #include &quot;JSCJSValueInlines.h&quot;
 36 #include &quot;JSGlobalObjectInlines.h&quot;
 37 #include &quot;JSWebAssemblyHelpers.h&quot;
 38 #include &quot;JSWebAssemblyInstance.h&quot;
 39 #include &quot;JSWebAssemblyRuntimeError.h&quot;
 40 #include &quot;ProbeContext.h&quot;
 41 #include &quot;WasmCallee.h&quot;
 42 #include &quot;WasmCallingConvention.h&quot;
 43 #include &quot;WasmContextInlines.h&quot;
 44 #include &quot;WasmInstance.h&quot;
 45 #include &quot;WasmMemory.h&quot;
 46 #include &quot;WasmNameSection.h&quot;
 47 #include &quot;WasmOMGForOSREntryPlan.h&quot;
 48 #include &quot;WasmOMGPlan.h&quot;
 49 #include &quot;WasmOSREntryData.h&quot;
 50 #include &quot;WasmSignatureInlines.h&quot;
 51 #include &quot;WasmWorklist.h&quot;
 52 #include &lt;wtf/DataLog.h&gt;
 53 #include &lt;wtf/Locker.h&gt;
 54 #include &lt;wtf/MonotonicTime.h&gt;
 55 #include &lt;wtf/StdLibExtras.h&gt;
 56 
 57 IGNORE_WARNINGS_BEGIN(&quot;frame-address&quot;)
 58 
 59 namespace JSC { namespace Wasm {
 60 
 61 void JIT_OPERATION operationWasmThrowBadI64(JSWebAssemblyInstance* instance)
 62 {
 63     VM&amp; vm = instance-&gt;vm();
 64     CallFrame* callFrame = DECLARE_CALL_FRAME(vm);
 65     JITOperationPrologueCallFrameTracer tracer(vm, callFrame);
 66 
 67     {
 68         auto throwScope = DECLARE_THROW_SCOPE(vm);
 69         JSGlobalObject* globalObject = instance-&gt;globalObject();
 70         auto* error = ErrorInstance::create(globalObject, vm, globalObject-&gt;errorStructure(ErrorType::TypeError), &quot;i64 not allowed as return type or argument to an imported function&quot;_s);
 71         throwException(globalObject, throwScope, error);
 72     }
 73 
 74     genericUnwind(vm, callFrame);
 75     ASSERT(!!vm.callFrameForCatch);
 76 }
 77 
 78 static bool shouldTriggerOMGCompile(TierUpCount&amp; tierUp, OMGCallee* replacement, uint32_t functionIndex)
 79 {
 80     if (!replacement &amp;&amp; !tierUp.checkIfOptimizationThresholdReached()) {
 81         dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile counter = &quot;, tierUp, &quot; for &quot;, functionIndex);
 82         dataLogLnIf(Options::verboseOSR(), &quot;Choosing not to OMG-optimize &quot;, functionIndex, &quot; yet.&quot;);
 83         return false;
 84     }
 85     return true;
 86 }
 87 
 88 static void triggerOMGReplacementCompile(TierUpCount&amp; tierUp, OMGCallee* replacement, Instance* instance, Wasm::CodeBlock&amp; codeBlock, uint32_t functionIndex)
 89 {
 90     if (replacement) {
 91         tierUp.optimizeSoon(functionIndex);
 92         return;
 93     }
 94 
 95     bool compile = false;
 96     {
 97         auto locker = holdLock(tierUp.getLock());
 98         switch (tierUp.m_compilationStatusForOMG) {
 99         case TierUpCount::CompilationStatus::StartCompilation:
100             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
101             return;
102         case TierUpCount::CompilationStatus::NotCompiled:
103             compile = true;
104             tierUp.m_compilationStatusForOMG = TierUpCount::CompilationStatus::StartCompilation;
105             break;
106         default:
107             break;
108         }
109     }
110 
111     if (compile) {
112         dataLogLnIf(Options::verboseOSR(), &quot;triggerOMGReplacement for &quot;, functionIndex);
113         // We need to compile the code.
114         Ref&lt;Plan&gt; plan = adoptRef(*new OMGPlan(instance-&gt;context(), Ref&lt;Wasm::Module&gt;(instance-&gt;module()), functionIndex, codeBlock.mode(), Plan::dontFinalize()));
115         ensureWorklist().enqueue(plan.copyRef());
116         if (UNLIKELY(!Options::useConcurrentJIT()))
117             plan-&gt;waitForCompletion();
118         else
119             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
120     }
121 }
122 
123 SUPPRESS_ASAN
124 static void doOSREntry(Instance* instance, Probe::Context&amp; context, BBQCallee&amp; callee, OMGForOSREntryCallee&amp; osrEntryCallee, OSREntryData&amp; osrEntryData)
125 {
126     auto returnWithoutOSREntry = [&amp;] {
127         context.gpr(GPRInfo::argumentGPR0) = 0;
128     };
129 
130     RELEASE_ASSERT(osrEntryCallee.osrEntryScratchBufferSize() == osrEntryData.values().size());
131 
132     uint64_t* buffer = instance-&gt;context()-&gt;scratchBufferForSize(osrEntryCallee.osrEntryScratchBufferSize());
133     if (!buffer)
134         return returnWithoutOSREntry();
135 
136     dataLogLnIf(Options::verboseOSR(), osrEntryData.functionIndex(), &quot;:OMG OSR entry: got entry callee &quot;, RawPointer(&amp;osrEntryCallee));
137 
138     // 1. Place required values in scratch buffer.
139     for (unsigned index = 0; index &lt; osrEntryData.values().size(); ++index) {
140         const OSREntryValue&amp; value = osrEntryData.values()[index];
141         dataLogLnIf(Options::verboseOSR(), &quot;OMG OSR entry values[&quot;, index, &quot;] &quot;, value.type(), &quot; &quot;, value);
142         if (value.isGPR()) {
143             switch (value.type().kind()) {
144             case B3::Float:
145             case B3::Double:
146                 RELEASE_ASSERT_NOT_REACHED();
147             default:
148                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = context.gpr(value.gpr());
149             }
150         } else if (value.isFPR()) {
151             switch (value.type().kind()) {
152             case B3::Float:
153             case B3::Double:
154                 *bitwise_cast&lt;double*&gt;(buffer + index) = context.fpr(value.fpr());
155                 break;
156             default:
157                 RELEASE_ASSERT_NOT_REACHED();
158             }
159         } else if (value.isConstant()) {
160             switch (value.type().kind()) {
161             case B3::Float:
162                 *bitwise_cast&lt;float*&gt;(buffer + index) = value.floatValue();
163                 break;
164             case B3::Double:
165                 *bitwise_cast&lt;double*&gt;(buffer + index) = value.doubleValue();
166                 break;
167             default:
168                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = value.value();
169             }
170         } else if (value.isStack()) {
171             switch (value.type().kind()) {
172             case B3::Float:
173                 *bitwise_cast&lt;float*&gt;(buffer + index) = *bitwise_cast&lt;float*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + value.offsetFromFP());
174                 break;
175             case B3::Double:
176                 *bitwise_cast&lt;double*&gt;(buffer + index) = *bitwise_cast&lt;double*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + value.offsetFromFP());
177                 break;
178             default:
179                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = *bitwise_cast&lt;uint64_t*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + value.offsetFromFP());
180                 break;
181             }
182         } else if (value.isStackArgument()) {
183             switch (value.type().kind()) {
184             case B3::Float:
185                 *bitwise_cast&lt;float*&gt;(buffer + index) = *bitwise_cast&lt;float*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.sp()) + value.offsetFromSP());
186                 break;
187             case B3::Double:
188                 *bitwise_cast&lt;double*&gt;(buffer + index) = *bitwise_cast&lt;double*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.sp()) + value.offsetFromSP());
189                 break;
190             default:
191                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = *bitwise_cast&lt;uint64_t*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.sp()) + value.offsetFromSP());
192                 break;
193             }
194         } else
195             RELEASE_ASSERT_NOT_REACHED();
196     }
197 
198     // 2. Restore callee saves.
199     RegisterSet dontRestoreRegisters = RegisterSet::stackRegisters();
200     for (const RegisterAtOffset&amp; entry : *callee.calleeSaveRegisters()) {
201         if (dontRestoreRegisters.get(entry.reg()))
202             continue;
203         if (entry.reg().isGPR())
204             context.gpr(entry.reg().gpr()) = *bitwise_cast&lt;UCPURegister*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + entry.offset());
205         else
206             context.fpr(entry.reg().fpr()) = *bitwise_cast&lt;double*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + entry.offset());
207     }
208 
209     // 3. Function epilogue, like a tail-call.
210     UCPURegister* framePointer = bitwise_cast&lt;UCPURegister*&gt;(context.fp());
211 #if CPU(X86_64)
212     // move(framePointerRegister, stackPointerRegister);
213     // pop(framePointerRegister);
214     context.fp() = bitwise_cast&lt;UCPURegister*&gt;(*framePointer);
215     context.sp() = framePointer + 1;
216     static_assert(AssemblyHelpers::prologueStackPointerDelta() == sizeof(void*) * 1);
217 #elif CPU(ARM64E) || CPU(ARM64)
218     // move(framePointerRegister, stackPointerRegister);
219     // popPair(framePointerRegister, linkRegister);
220     context.fp() = bitwise_cast&lt;UCPURegister*&gt;(*framePointer);
221     context.gpr(ARM64Registers::lr) = bitwise_cast&lt;UCPURegister&gt;(*(framePointer + 1));
222     context.sp() = framePointer + 2;
223     static_assert(AssemblyHelpers::prologueStackPointerDelta() == sizeof(void*) * 2);
224 #if CPU(ARM64E)
225     // LR needs to be untagged since OSR entry function prologue will tag it with SP. This is similar to tail-call.
226     context.gpr(ARM64Registers::lr) = bitwise_cast&lt;UCPURegister&gt;(untagCodePtr(context.gpr&lt;void*&gt;(ARM64Registers::lr), bitwise_cast&lt;PtrTag&gt;(context.sp())));
227 #endif
228 #else
229 #error Unsupported architecture.
230 #endif
231     // 4. Configure argument registers to jump to OSR entry from the caller of this runtime function.
232     context.gpr(GPRInfo::argumentGPR0) = bitwise_cast&lt;UCPURegister&gt;(buffer);
233     context.gpr(GPRInfo::argumentGPR1) = bitwise_cast&lt;UCPURegister&gt;(osrEntryCallee.entrypoint().executableAddress&lt;&gt;());
234 }
235 
236 void JIT_OPERATION operationWasmTriggerOSREntryNow(Probe::Context&amp; context)
237 {
238     OSREntryData&amp; osrEntryData = *context.arg&lt;OSREntryData*&gt;();
239     uint32_t functionIndex = osrEntryData.functionIndex();
240     uint32_t loopIndex = osrEntryData.loopIndex();
241     Instance* instance = Wasm::Context::tryLoadInstanceFromTLS();
242     if (!instance)
243         instance = context.gpr&lt;Instance*&gt;(Wasm::PinnedRegisterInfo::get().wasmContextInstancePointer);
244 
245     auto returnWithoutOSREntry = [&amp;] {
246         context.gpr(GPRInfo::argumentGPR0) = 0;
247     };
248 
249     Wasm::CodeBlock&amp; codeBlock = *instance-&gt;codeBlock();
250     ASSERT(instance-&gt;memory()-&gt;mode() == codeBlock.mode());
251 
252     uint32_t functionIndexInSpace = functionIndex + codeBlock.functionImportCount();
253     ASSERT(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace).compilationMode() == Wasm::CompilationMode::BBQMode);
254     BBQCallee&amp; callee = static_cast&lt;BBQCallee&amp;&gt;(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace));
255     TierUpCount&amp; tierUp = *callee.tierUpCount();
256     dataLogLnIf(Options::verboseOSR(), &quot;Consider OMGForOSREntryPlan for [&quot;, functionIndex, &quot;] loopIndex#&quot;, loopIndex, &quot; with executeCounter = &quot;, tierUp, &quot; &quot;, RawPointer(callee.replacement()));
257 
258     if (!Options::useWebAssemblyOSR()) {
259         if (shouldTriggerOMGCompile(tierUp, callee.replacement(), functionIndex))
260             triggerOMGReplacementCompile(tierUp, callee.replacement(), instance, codeBlock, functionIndex);
261 
262         // We already have an OMG replacement.
263         if (callee.replacement()) {
264             // No OSR entry points. Just defer indefinitely.
265             if (tierUp.osrEntryTriggers().isEmpty()) {
266                 tierUp.dontOptimizeAnytimeSoon(functionIndex);
267                 return;
268             }
269 
270             // Found one OSR entry point. Since we do not have a way to jettison Wasm::Callee right now, this means that tierUp function is now meaningless.
271             // Not call it as much as possible.
272             if (callee.osrEntryCallee()) {
273                 tierUp.dontOptimizeAnytimeSoon(functionIndex);
274                 return;
275             }
276         }
277         return returnWithoutOSREntry();
278     }
279 
280     TierUpCount::CompilationStatus compilationStatus = TierUpCount::CompilationStatus::NotCompiled;
281     {
282         auto locker = holdLock(tierUp.getLock());
283         compilationStatus = tierUp.m_compilationStatusForOMGForOSREntry;
284     }
285 
286     bool triggeredSlowPathToStartCompilation = false;
287     switch (tierUp.osrEntryTriggers()[loopIndex]) {
288     case TierUpCount::TriggerReason::DontTrigger:
289         // The trigger isn&#39;t set, we entered because the counter reached its
290         // threshold.
291         break;
292     case TierUpCount::TriggerReason::CompilationDone:
293         // The trigger was set because compilation completed. Don&#39;t unset it
294         // so that further BBQ executions OSR enter as well.
295         break;
296     case TierUpCount::TriggerReason::StartCompilation: {
297         // We were asked to enter as soon as possible and start compiling an
298         // entry for the current loopIndex. Unset this trigger so we
299         // don&#39;t continually enter.
300         auto locker = holdLock(tierUp.getLock());
301         TierUpCount::TriggerReason reason = tierUp.osrEntryTriggers()[loopIndex];
302         if (reason == TierUpCount::TriggerReason::StartCompilation) {
303             tierUp.osrEntryTriggers()[loopIndex] = TierUpCount::TriggerReason::DontTrigger;
304             triggeredSlowPathToStartCompilation = true;
305         }
306         break;
307     }
308     }
309 
310     if (compilationStatus == TierUpCount::CompilationStatus::StartCompilation) {
311         dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile still compiling for &quot;, functionIndex);
312         tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
313         return returnWithoutOSREntry();
314     }
315 
316     if (OMGForOSREntryCallee* osrEntryCallee = callee.osrEntryCallee()) {
317         if (osrEntryCallee-&gt;loopIndex() == loopIndex)
318             return doOSREntry(instance, context, callee, *osrEntryCallee, osrEntryData);
319     }
320 
321     if (!shouldTriggerOMGCompile(tierUp, callee.replacement(), functionIndex) &amp;&amp; !triggeredSlowPathToStartCompilation)
322         return returnWithoutOSREntry();
323 
324     if (!triggeredSlowPathToStartCompilation) {
325         triggerOMGReplacementCompile(tierUp, callee.replacement(), instance, codeBlock, functionIndex);
326 
327         if (!callee.replacement())
328             return returnWithoutOSREntry();
329     }
330 
331     if (OMGForOSREntryCallee* osrEntryCallee = callee.osrEntryCallee()) {
332         if (osrEntryCallee-&gt;loopIndex() == loopIndex)
333             return doOSREntry(instance, context, callee, *osrEntryCallee, osrEntryData);
334         tierUp.dontOptimizeAnytimeSoon(functionIndex);
335         return returnWithoutOSREntry();
336     }
337 
338     // Instead of triggering OSR entry compilation in inner loop, try outer loop&#39;s trigger immediately effective (setting TriggerReason::StartCompilation) and
339     // let outer loop attempt to compile.
340     if (!triggeredSlowPathToStartCompilation) {
341         // An inner loop didn&#39;t specifically ask for us to kick off a compilation. This means the counter
342         // crossed its threshold. We either fall through and kick off a compile for originBytecodeIndex,
343         // or we flag an outer loop to immediately try to compile itself. If there are outer loops,
344         // we first try to make them compile themselves. But we will eventually fall back to compiling
345         // a progressively inner loop if it takes too long for control to reach an outer loop.
346 
347         auto tryTriggerOuterLoopToCompile = [&amp;] {
348             // We start with the outermost loop and make our way inwards (hence why we iterate the vector in reverse).
349             // Our policy is that we will trigger an outer loop to compile immediately when program control reaches it.
350             // If program control is taking too long to reach that outer loop, we progressively move inwards, meaning,
351             // we&#39;ll eventually trigger some loop that is executing to compile. We start with trying to compile outer
352             // loops since we believe outer loop compilations reveal the best opportunities for optimizing code.
353             uint32_t currentLoopIndex = tierUp.outerLoops()[loopIndex];
354             auto locker = holdLock(tierUp.getLock());
355 
356             // We already started OMGForOSREntryPlan.
357             if (callee.didStartCompilingOSREntryCallee())
358                 return false;
359 
360             while (currentLoopIndex != UINT32_MAX) {
361                 if (tierUp.osrEntryTriggers()[currentLoopIndex] == TierUpCount::TriggerReason::StartCompilation) {
362                     // This means that we already asked this loop to compile. If we&#39;ve reached here, it
363                     // means program control has not yet reached that loop. So it&#39;s taking too long to compile.
364                     // So we move on to asking the inner loop of this loop to compile itself.
365                     currentLoopIndex = tierUp.outerLoops()[currentLoopIndex];
366                     continue;
367                 }
368 
369                 // This is where we ask the outer to loop to immediately compile itself if program
370                 // control reaches it.
371                 dataLogLnIf(Options::verboseOSR(), &quot;Inner-loop loopIndex#&quot;, loopIndex, &quot; in &quot;, functionIndex, &quot; setting parent loop loopIndex#&quot;, currentLoopIndex, &quot;&#39;s trigger and backing off.&quot;);
372                 tierUp.osrEntryTriggers()[currentLoopIndex] = TierUpCount::TriggerReason::StartCompilation;
373                 return true;
374             }
375             return false;
376         };
377 
378         if (tryTriggerOuterLoopToCompile()) {
379             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
380             return returnWithoutOSREntry();
381         }
382     }
383 
384     bool startOSREntryCompilation = false;
385     {
386         auto locker = holdLock(tierUp.getLock());
387         if (tierUp.m_compilationStatusForOMGForOSREntry == TierUpCount::CompilationStatus::NotCompiled) {
388             tierUp.m_compilationStatusForOMGForOSREntry = TierUpCount::CompilationStatus::StartCompilation;
389             startOSREntryCompilation = true;
390             // Currently, we do not have a way to jettison wasm code. This means that once we decide to compile OSR entry code for a particular loopIndex,
391             // we cannot throw the compiled code so long as Wasm module is live. We immediately disable all the triggers.
392             for (auto&amp; trigger : tierUp.osrEntryTriggers())
393                 trigger = TierUpCount::TriggerReason::DontTrigger;
394         }
395     }
396 
397     if (startOSREntryCompilation) {
398         dataLogLnIf(Options::verboseOSR(), &quot;triggerOMGOSR for &quot;, functionIndex);
399         Ref&lt;Plan&gt; plan = adoptRef(*new OMGForOSREntryPlan(instance-&gt;context(), Ref&lt;Wasm::Module&gt;(instance-&gt;module()), Ref&lt;Wasm::BBQCallee&gt;(callee), functionIndex, loopIndex, codeBlock.mode(), Plan::dontFinalize()));
400         ensureWorklist().enqueue(plan.copyRef());
401         if (UNLIKELY(!Options::useConcurrentJIT()))
402             plan-&gt;waitForCompletion();
403         else
404             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
405     }
406 
407     OMGForOSREntryCallee* osrEntryCallee = callee.osrEntryCallee();
408     if (!osrEntryCallee) {
409         tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
410         return returnWithoutOSREntry();
411     }
412 
413     if (osrEntryCallee-&gt;loopIndex() == loopIndex)
414         return doOSREntry(instance, context, callee, *osrEntryCallee, osrEntryData);
415 
416     tierUp.dontOptimizeAnytimeSoon(functionIndex);
417     return returnWithoutOSREntry();
418 }
419 
420 void JIT_OPERATION operationWasmTriggerTierUpNow(Instance* instance, uint32_t functionIndex)
421 {
422     Wasm::CodeBlock&amp; codeBlock = *instance-&gt;codeBlock();
423     ASSERT(instance-&gt;memory()-&gt;mode() == codeBlock.mode());
424 
425     uint32_t functionIndexInSpace = functionIndex + codeBlock.functionImportCount();
426     ASSERT(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace).compilationMode() == Wasm::CompilationMode::BBQMode);
427     BBQCallee&amp; callee = static_cast&lt;BBQCallee&amp;&gt;(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace));
428     TierUpCount&amp; tierUp = *callee.tierUpCount();
429     dataLogLnIf(Options::verboseOSR(), &quot;Consider OMGPlan for [&quot;, functionIndex, &quot;] with executeCounter = &quot;, tierUp, &quot; &quot;, RawPointer(callee.replacement()));
430 
431     if (shouldTriggerOMGCompile(tierUp, callee.replacement(), functionIndex))
432         triggerOMGReplacementCompile(tierUp, callee.replacement(), instance, codeBlock, functionIndex);
433 
434     // We already have an OMG replacement.
435     if (callee.replacement()) {
436         // No OSR entry points. Just defer indefinitely.
437         if (tierUp.osrEntryTriggers().isEmpty()) {
438             dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile replacement in place, delaying indefinitely for &quot;, functionIndex);
439             tierUp.dontOptimizeAnytimeSoon(functionIndex);
440             return;
441         }
442 
443         // Found one OSR entry point. Since we do not have a way to jettison Wasm::Callee right now, this means that tierUp function is now meaningless.
444         // Not call it as much as possible.
445         if (callee.osrEntryCallee()) {
446             dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile trigger in place, delaying indefinitely for &quot;, functionIndex);
447             tierUp.dontOptimizeAnytimeSoon(functionIndex);
448             return;
449         }
450     }
451 }
452 
453 void JIT_OPERATION operationWasmUnwind(CallFrame* callFrame)
454 {
455     // FIXME: Consider passing JSWebAssemblyInstance* instead.
456     // https://bugs.webkit.org/show_bug.cgi?id=203206
457     VM&amp; vm = callFrame-&gt;deprecatedVM();
458     NativeCallFrameTracer tracer(vm, callFrame);
459     genericUnwind(vm, callFrame);
460     ASSERT(!!vm.callFrameForCatch);
461 }
462 
463 double JIT_OPERATION operationConvertToF64(CallFrame* callFrame, JSValue v)
464 {
465     // FIXME: Consider passing JSWebAssemblyInstance* instead.
466     // https://bugs.webkit.org/show_bug.cgi?id=203206
467     VM&amp; vm = callFrame-&gt;deprecatedVM();
468     NativeCallFrameTracer tracer(vm, callFrame);
469     return v.toNumber(callFrame-&gt;lexicalGlobalObject(vm));
470 }
471 
472 int32_t JIT_OPERATION operationConvertToI32(CallFrame* callFrame, JSValue v)
473 {
474     // FIXME: Consider passing JSWebAssemblyInstance* instead.
475     // https://bugs.webkit.org/show_bug.cgi?id=203206
476     VM&amp; vm = callFrame-&gt;deprecatedVM();
477     NativeCallFrameTracer tracer(vm, callFrame);
478     return v.toInt32(callFrame-&gt;lexicalGlobalObject(vm));
479 }
480 
481 float JIT_OPERATION operationConvertToF32(CallFrame* callFrame, JSValue v)
482 {
483     // FIXME: Consider passing JSWebAssemblyInstance* instead.
484     // https://bugs.webkit.org/show_bug.cgi?id=203206
485     VM&amp; vm = callFrame-&gt;deprecatedVM();
486     NativeCallFrameTracer tracer(vm, callFrame);
487     return static_cast&lt;float&gt;(v.toNumber(callFrame-&gt;lexicalGlobalObject(vm)));
488 }
489 
490 void JIT_OPERATION operationIterateResults(CallFrame* callFrame, Instance* instance, const Signature* signature, JSValue result, uint64_t* registerResults, uint64_t* calleeFramePointer)
491 {
492     // FIXME: Consider passing JSWebAssemblyInstance* instead.
493     // https://bugs.webkit.org/show_bug.cgi?id=203206
494     JSWebAssemblyInstance* jsInstance = instance-&gt;owner&lt;JSWebAssemblyInstance&gt;();
495     JSGlobalObject* globalObject = jsInstance-&gt;globalObject();
496     VM&amp; vm = globalObject-&gt;vm();
497     NativeCallFrameTracer(vm, callFrame);
498     auto scope = DECLARE_THROW_SCOPE(vm);
499 
500     auto wasmCallInfo = wasmCallingConvention().callInformationFor(*signature, CallRole::Callee);
501     RegisterAtOffsetList registerResultOffsets = wasmCallInfo.computeResultsOffsetList();
502 
503     unsigned itemsInserted = 0;
504     forEachInIterable(globalObject, result, [&amp;] (VM&amp; vm, JSGlobalObject* globalObject, JSValue value) -&gt; void {
505         auto scope = DECLARE_THROW_SCOPE(vm);
506         if (itemsInserted &lt; signature-&gt;returnCount()) {
507             uint64_t unboxedValue;
508             switch (signature-&gt;returnType(itemsInserted)) {
509             case I32:
510                 unboxedValue = value.toInt32(globalObject);
511                 break;
512             case F32:
513                 unboxedValue = bitwise_cast&lt;uint32_t&gt;(value.toFloat(globalObject));
514                 break;
515             case F64:
516                 unboxedValue = bitwise_cast&lt;uint64_t&gt;(value.toNumber(globalObject));
517                 break;
518             case Funcref:
519                 if (!value.isFunction(vm)) {
520                     throwTypeError(globalObject, scope, &quot;Funcref value is not a function&quot;_s);
521                     return;
522                 }
523                 FALLTHROUGH;
524             case Anyref:
525                 unboxedValue = bitwise_cast&lt;uint64_t&gt;(value);
526                 RELEASE_ASSERT(Options::useWebAssemblyReferences());
527                 break;
528             default:
529                 RELEASE_ASSERT_NOT_REACHED();
530             }
531 
532             RETURN_IF_EXCEPTION(scope, void());
533             auto rep = wasmCallInfo.results[itemsInserted];
534             if (rep.isReg())
535                 registerResults[registerResultOffsets.find(rep.reg())-&gt;offset() / sizeof(uint64_t)] = unboxedValue;
536             else
537                 calleeFramePointer[rep.offsetFromFP() / sizeof(uint64_t)] = unboxedValue;
538         }
539         itemsInserted++;
540     });
541     RETURN_IF_EXCEPTION(scope, void());
542     if (itemsInserted != signature-&gt;returnCount())
543         throwVMTypeError(globalObject, scope, &quot;Incorrect number of values returned to Wasm from JS&quot;);
544 }
545 
546 // FIXME: It would be much easier to inline this when we have a global GC, which could probably mean we could avoid
547 // spilling the results onto the stack.
548 // Saved result registers should be placed on the stack just above the last stack result.
549 JSArray* JIT_OPERATION operationAllocateResultsArray(CallFrame* callFrame, Wasm::Instance* instance, const Signature* signature, IndexingType indexingType, JSValue* stackPointerFromCallee)
550 {
551     JSWebAssemblyInstance* jsInstance = instance-&gt;owner&lt;JSWebAssemblyInstance&gt;();
552     VM&amp; vm = jsInstance-&gt;vm();
553     NativeCallFrameTracer tracer(vm, callFrame);
554 
555     JSGlobalObject* globalObject = jsInstance-&gt;globalObject();
556     ObjectInitializationScope initializationScope(globalObject-&gt;vm());
557     JSArray* result = JSArray::tryCreateUninitializedRestricted(initializationScope, nullptr, globalObject-&gt;arrayStructureForIndexingTypeDuringAllocation(indexingType), signature-&gt;returnCount());
558 
559     // FIXME: Handle allocation failure...
560     RELEASE_ASSERT(result);
561 
562     auto wasmCallInfo = wasmCallingConvention().callInformationFor(*signature);
563     RegisterAtOffsetList registerResults = wasmCallInfo.computeResultsOffsetList();
564 
565     static_assert(sizeof(JSValue) == sizeof(CPURegister), &quot;The code below relies on this.&quot;);
566     for (unsigned i = 0; i &lt; signature-&gt;returnCount(); ++i) {
567         B3::ValueRep rep = wasmCallInfo.results[i];
568         JSValue value;
569         if (rep.isReg())
570             value = stackPointerFromCallee[(registerResults.find(rep.reg())-&gt;offset() + wasmCallInfo.headerAndArgumentStackSizeInBytes) / sizeof(JSValue)];
571         else
572             value = stackPointerFromCallee[rep.offsetFromSP() / sizeof(JSValue)];
573         result-&gt;initializeIndex(initializationScope, i, value);
574     }
575 
576     ASSERT(result-&gt;indexingType() == indexingType);
577     return result;
578 }
579 
580 void JIT_OPERATION operationWasmWriteBarrierSlowPath(JSCell* cell, VM* vmPointer)
581 {
582     ASSERT(cell);
583     ASSERT(vmPointer);
584     VM&amp; vm = *vmPointer;
585     vm.heap.writeBarrierSlowPath(cell);
586 }
587 
588 uint32_t JIT_OPERATION operationPopcount32(int32_t value)
589 {
590     return __builtin_popcount(value);
591 }
592 
593 uint64_t JIT_OPERATION operationPopcount64(int64_t value)
594 {
595     return __builtin_popcountll(value);
596 }
597 
598 int32_t JIT_OPERATION operationGrowMemory(void* callFrame, Instance* instance, int32_t delta)
599 {
600     instance-&gt;storeTopCallFrame(callFrame);
601 
602     if (delta &lt; 0)
603         return -1;
604 
605     auto grown = instance-&gt;memory()-&gt;grow(PageCount(delta));
606     if (!grown) {
607         switch (grown.error()) {
608         case Memory::GrowFailReason::InvalidDelta:
609         case Memory::GrowFailReason::InvalidGrowSize:
610         case Memory::GrowFailReason::WouldExceedMaximum:
611         case Memory::GrowFailReason::OutOfMemory:
612             return -1;
613         }
614         RELEASE_ASSERT_NOT_REACHED();
615     }
616 
617     return grown.value().pageCount();
618 }
619 
620 EncodedJSValue JIT_OPERATION operationGetWasmTableElement(Instance* instance, unsigned tableIndex, int32_t signedIndex)
621 {
622     ASSERT(tableIndex &lt; instance-&gt;module().moduleInformation().tableCount());
623     if (signedIndex &lt; 0)
624         return 0;
625 
626     uint32_t index = signedIndex;
627     if (index &gt;= instance-&gt;table(tableIndex)-&gt;length())
628         return 0;
629 
630     return JSValue::encode(instance-&gt;table(tableIndex)-&gt;get(index));
631 }
632 
633 static bool setWasmTableElement(Instance* instance, unsigned tableIndex, int32_t signedIndex, EncodedJSValue encValue)
634 {
635     ASSERT(tableIndex &lt; instance-&gt;module().moduleInformation().tableCount());
636     if (signedIndex &lt; 0)
637         return false;
638 
639     uint32_t index = signedIndex;
640     if (index &gt;= instance-&gt;table(tableIndex)-&gt;length())
641         return false;
642 
643     JSValue value = JSValue::decode(encValue);
644     if (instance-&gt;table(tableIndex)-&gt;type() == Wasm::TableElementType::Anyref)
645         instance-&gt;table(tableIndex)-&gt;set(index, value);
646     else if (instance-&gt;table(tableIndex)-&gt;type() == Wasm::TableElementType::Funcref) {
647         WebAssemblyFunction* wasmFunction;
648         WebAssemblyWrapperFunction* wasmWrapperFunction;
649 
650         if (isWebAssemblyHostFunction(instance-&gt;owner&lt;JSObject&gt;()-&gt;vm(), value, wasmFunction, wasmWrapperFunction)) {
651             ASSERT(!!wasmFunction || !!wasmWrapperFunction);
652             if (wasmFunction)
653                 instance-&gt;table(tableIndex)-&gt;asFuncrefTable()-&gt;setFunction(index, jsCast&lt;JSObject*&gt;(value), wasmFunction-&gt;importableFunction(), &amp;wasmFunction-&gt;instance()-&gt;instance());
654             else
655                 instance-&gt;table(tableIndex)-&gt;asFuncrefTable()-&gt;setFunction(index, jsCast&lt;JSObject*&gt;(value), wasmWrapperFunction-&gt;importableFunction(), &amp;wasmWrapperFunction-&gt;instance()-&gt;instance());
656         } else if (value.isNull())
657             instance-&gt;table(tableIndex)-&gt;clear(index);
658         else
659             ASSERT_NOT_REACHED();
660     } else
661         ASSERT_NOT_REACHED();
662 
663     return true;
664 }
665 
666 bool JIT_OPERATION operationSetWasmTableElement(Instance* instance, unsigned tableIndex, int32_t signedIndex, EncodedJSValue encValue)
667 {
668     return setWasmTableElement(instance, tableIndex, signedIndex, encValue);
669 }
670 
671 int32_t JIT_OPERATION operationWasmTableGrow(Instance* instance, unsigned tableIndex, EncodedJSValue fill, int32_t delta)
672 {
673     ASSERT(tableIndex &lt; instance-&gt;module().moduleInformation().tableCount());
674     auto oldSize = instance-&gt;table(tableIndex)-&gt;length();
675     if (delta &lt; 0)
676         return oldSize;
677     auto newSize = instance-&gt;table(tableIndex)-&gt;grow(delta);
678     if (!newSize || *newSize == oldSize)
679         return -1;
680 
681     for (unsigned i = oldSize; i &lt; instance-&gt;table(tableIndex)-&gt;length(); ++i)
682         setWasmTableElement(instance, tableIndex, i, fill);
683 
684     return oldSize;
685 }
686 
687 bool JIT_OPERATION operationWasmTableFill(Instance* instance, unsigned tableIndex, int32_t unsafeOffset, EncodedJSValue fill, int32_t unsafeCount)
688 {
689     ASSERT(tableIndex &lt; instance-&gt;module().moduleInformation().tableCount());
690     if (unsafeOffset &lt; 0 || unsafeCount &lt; 0)
691         return false;
692 
693     unsigned offset = unsafeOffset;
694     unsigned count = unsafeCount;
695 
696     if (offset &gt;= instance-&gt;table(tableIndex)-&gt;length() || offset + count &gt; instance-&gt;table(tableIndex)-&gt;length())
697         return false;
698 
699     for (unsigned j = 0; j &lt; count; ++j)
700         setWasmTableElement(instance, tableIndex, offset + j, fill);
701 
702     return true;
703 }
704 
705 EncodedJSValue JIT_OPERATION operationWasmRefFunc(Instance* instance, uint32_t index)
706 {
707     JSValue value = instance-&gt;getFunctionWrapper(index);
708     ASSERT(value.isFunction(instance-&gt;owner&lt;JSObject&gt;()-&gt;vm()));
709     return JSValue::encode(value);
710 }
711 
712 int32_t JIT_OPERATION operationGetWasmTableSize(Instance* instance, unsigned tableIndex)
713 {
714     return instance-&gt;table(tableIndex)-&gt;length();
715 }
716 
717 void* JIT_OPERATION operationWasmToJSException(CallFrame* callFrame, Wasm::ExceptionType type, Instance* wasmInstance)
718 {
719     wasmInstance-&gt;storeTopCallFrame(callFrame);
720     JSWebAssemblyInstance* instance = wasmInstance-&gt;owner&lt;JSWebAssemblyInstance&gt;();
721     JSGlobalObject* globalObject = instance-&gt;globalObject();
722 
723     // Do not retrieve VM&amp; from CallFrame since CallFrame&#39;s callee is not a JSCell.
724     VM&amp; vm = globalObject-&gt;vm();
725 
726     {
727         auto throwScope = DECLARE_THROW_SCOPE(vm);
728 
729         JSObject* error;
730         if (type == ExceptionType::StackOverflow)
731             error = createStackOverflowError(globalObject);
732         else
733             error = JSWebAssemblyRuntimeError::create(globalObject, vm, globalObject-&gt;webAssemblyRuntimeErrorStructure(), Wasm::errorMessageForExceptionType(type));
734         throwException(globalObject, throwScope, error);
735     }
736 
737     genericUnwind(vm, callFrame);
738     ASSERT(!!vm.callFrameForCatch);
739     ASSERT(!!vm.targetMachinePCForThrow);
740     // FIXME: We could make this better:
741     // This is a total hack, but the llint (both op_catch and handleUncaughtException)
742     // require a cell in the callee field to load the VM. (The baseline JIT does not require
743     // this since it is compiled with a constant VM pointer.) We could make the calling convention
744     // for exceptions first load callFrameForCatch info call frame register before jumping
745     // to the exception handler. If we did this, we could remove this terrible hack.
746     // https://bugs.webkit.org/show_bug.cgi?id=170440
747     bitwise_cast&lt;uint64_t*&gt;(callFrame)[static_cast&lt;int&gt;(CallFrameSlot::callee)] = bitwise_cast&lt;uint64_t&gt;(instance-&gt;module());
748     return vm.targetMachinePCForThrow;
749 }
750 
751 } } // namespace JSC::Wasm
752 
753 IGNORE_WARNINGS_END
754 
755 #endif // ENABLE(WEBASSEMBLY)
    </pre>
  </body>
</html>