<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/WTF/wtf/ParkingLot.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2015-2016 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &lt;wtf/ParkingLot.h&gt;
 28 
 29 #include &lt;condition_variable&gt;
 30 #include &lt;mutex&gt;
 31 #include &lt;thread&gt;
 32 #include &lt;wtf/DataLog.h&gt;
 33 #include &lt;wtf/HashFunctions.h&gt;
 34 #include &lt;wtf/StringPrintStream.h&gt;
 35 #include &lt;wtf/ThreadSpecific.h&gt;
 36 #include &lt;wtf/Threading.h&gt;
 37 #include &lt;wtf/Vector.h&gt;
 38 #include &lt;wtf/WeakRandom.h&gt;
 39 #include &lt;wtf/WordLock.h&gt;
 40 
 41 namespace WTF {
 42 
 43 namespace {
 44 
 45 const bool verbose = false;
 46 
 47 struct ThreadData : public ThreadSafeRefCounted&lt;ThreadData&gt; {
 48     WTF_MAKE_FAST_ALLOCATED;
 49 public:
 50 
 51     ThreadData();
 52     ~ThreadData();
 53 
 54     Ref&lt;Thread&gt; thread;
 55 
 56     Mutex parkingLock;
 57     ThreadCondition parkingCondition;
 58 
 59     const void* address { nullptr };
 60 
 61     ThreadData* nextInQueue { nullptr };
 62 
 63     intptr_t token { 0 };
 64 };
 65 
 66 enum class DequeueResult {
 67     Ignore,
 68     RemoveAndContinue,
 69     RemoveAndStop
 70 };
 71 
 72 struct Bucket {
 73     WTF_MAKE_FAST_ALLOCATED;
 74 public:
 75     Bucket()
 76         : random(static_cast&lt;unsigned&gt;(bitwise_cast&lt;intptr_t&gt;(this))) // Cannot use default seed since that recurses into Lock.
 77     {
 78     }
 79 
 80     void enqueue(ThreadData* data)
 81     {
 82         if (verbose)
 83             dataLog(toString(Thread::current(), &quot;: enqueueing &quot;, RawPointer(data), &quot; with address = &quot;, RawPointer(data-&gt;address), &quot; onto &quot;, RawPointer(this), &quot;\n&quot;));
 84         ASSERT(data-&gt;address);
 85         ASSERT(!data-&gt;nextInQueue);
 86 
 87         if (queueTail) {
 88             queueTail-&gt;nextInQueue = data;
 89             queueTail = data;
 90             return;
 91         }
 92 
 93         queueHead = data;
 94         queueTail = data;
 95     }
 96 
 97     template&lt;typename Functor&gt;
 98     void genericDequeue(const Functor&amp; functor)
 99     {
100         if (verbose)
101             dataLog(toString(Thread::current(), &quot;: dequeueing from bucket at &quot;, RawPointer(this), &quot;\n&quot;));
102 
103         if (!queueHead) {
104             if (verbose)
105                 dataLog(toString(Thread::current(), &quot;: empty.\n&quot;));
106             return;
107         }
108 
109         // This loop is a very clever abomination. The induction variables are the pointer to the
110         // pointer to the current node, and the pointer to the previous node. This gives us everything
111         // we need to both proceed forward to the next node, and to remove nodes while maintaining the
112         // queueHead/queueTail and all of the nextInQueue links. For example, when we are at the head
113         // element, then removal means rewiring queueHead, and if it was also equal to queueTail, then
114         // we&#39;d want queueTail to be set to nullptr. This works because:
115         //
116         //     currentPtr == &amp;queueHead
117         //     previous == nullptr
118         //
119         // We remove by setting *currentPtr = (*currentPtr)-&gt;nextInQueue, i.e. changing the pointer
120         // that used to point to this node to instead point to this node&#39;s successor. Another example:
121         // if we were at the second node in the queue, then we&#39;d have:
122         //
123         //     currentPtr == &amp;queueHead-&gt;nextInQueue
124         //     previous == queueHead
125         //
126         // If this node is not equal to queueTail, then removing it simply means making
127         // queueHead-&gt;nextInQueue point to queueHead-&gt;nextInQueue-&gt;nextInQueue (which the algorithm
128         // achieves by mutating *currentPtr). If this node is equal to queueTail, then we want to set
129         // queueTail to previous, which in this case is queueHead - thus making the queue look like a
130         // proper one-element queue with queueHead == queueTail.
131         bool shouldContinue = true;
132         ThreadData** currentPtr = &amp;queueHead;
133         ThreadData* previous = nullptr;
134 
135         MonotonicTime time = MonotonicTime::now();
136         bool timeToBeFair = false;
137         if (time &gt; nextFairTime)
138             timeToBeFair = true;
139 
140         bool didDequeue = false;
141 
142         while (shouldContinue) {
143             ThreadData* current = *currentPtr;
144             if (verbose)
145                 dataLog(toString(Thread::current(), &quot;: got thread &quot;, RawPointer(current), &quot;\n&quot;));
146             if (!current)
147                 break;
148             DequeueResult result = functor(current, timeToBeFair);
149             switch (result) {
150             case DequeueResult::Ignore:
151                 if (verbose)
152                     dataLog(toString(Thread::current(), &quot;: currentPtr = &quot;, RawPointer(currentPtr), &quot;, *currentPtr = &quot;, RawPointer(*currentPtr), &quot;\n&quot;));
153                 previous = current;
154                 currentPtr = &amp;(*currentPtr)-&gt;nextInQueue;
155                 break;
156             case DequeueResult::RemoveAndStop:
157                 shouldContinue = false;
158                 FALLTHROUGH;
159             case DequeueResult::RemoveAndContinue:
160                 if (verbose)
161                     dataLog(toString(Thread::current(), &quot;: dequeueing &quot;, RawPointer(current), &quot; from &quot;, RawPointer(this), &quot;\n&quot;));
162                 if (current == queueTail)
163                     queueTail = previous;
164                 didDequeue = true;
165                 *currentPtr = current-&gt;nextInQueue;
166                 current-&gt;nextInQueue = nullptr;
167                 break;
168             }
169         }
170 
171         if (timeToBeFair &amp;&amp; didDequeue)
172             nextFairTime = time + Seconds::fromMilliseconds(random.get());
173 
174         ASSERT(!!queueHead == !!queueTail);
175     }
176 
177     ThreadData* dequeue()
178     {
179         ThreadData* result = nullptr;
180         genericDequeue(
181             [&amp;] (ThreadData* element, bool) -&gt; DequeueResult {
182                 result = element;
183                 return DequeueResult::RemoveAndStop;
184             });
185         return result;
186     }
187 
188     ThreadData* queueHead { nullptr };
189     ThreadData* queueTail { nullptr };
190 
191     // This lock protects the entire bucket. Thou shall not make changes to Bucket without holding
192     // this lock.
193     WordLock lock;
194 
195     MonotonicTime nextFairTime;
196 
197     WeakRandom random;
198 
199     // Put some distane between buckets in memory. This is one of several mitigations against false
200     // sharing.
201     char padding[64];
202 };
203 
204 struct Hashtable;
205 
206 // We track all allocated hashtables so that hashtable resizing doesn&#39;t anger leak detectors.
207 Vector&lt;Hashtable*&gt;* hashtables;
208 WordLock hashtablesLock;
209 
210 struct Hashtable {
211     unsigned size;
212     Atomic&lt;Bucket*&gt; data[1];
213 
214     static Hashtable* create(unsigned size)
215     {
216         ASSERT(size &gt;= 1);
217 
218         Hashtable* result = static_cast&lt;Hashtable*&gt;(
219             fastZeroedMalloc(sizeof(Hashtable) + sizeof(Atomic&lt;Bucket*&gt;) * (size - 1)));
220         result-&gt;size = size;
221 
222         {
223             // This is not fast and it&#39;s not data-access parallel, but that&#39;s fine, because
224             // hashtable resizing is guaranteed to be rare and it will never happen in steady
225             // state.
226             WordLockHolder locker(hashtablesLock);
227             if (!hashtables)
228                 hashtables = new Vector&lt;Hashtable*&gt;();
229             hashtables-&gt;append(result);
230         }
231 
232         return result;
233     }
234 
235     static void destroy(Hashtable* hashtable)
236     {
237         {
238             // This is not fast, but that&#39;s OK. See comment in create().
239             WordLockHolder locker(hashtablesLock);
240             hashtables-&gt;removeFirst(hashtable);
241         }
242 
243         fastFree(hashtable);
244     }
245 };
246 
247 Atomic&lt;Hashtable*&gt; hashtable;
248 Atomic&lt;unsigned&gt; numThreads;
249 
250 // With 64 bytes of padding per bucket, assuming a hashtable is fully populated with buckets, the
251 // memory usage per thread will still be less than 1KB.
252 const unsigned maxLoadFactor = 3;
253 
254 const unsigned growthFactor = 2;
255 
256 unsigned hashAddress(const void* address)
257 {
258     return WTF::PtrHash&lt;const void*&gt;::hash(address);
259 }
260 
261 Hashtable* ensureHashtable()
262 {
263     for (;;) {
264         Hashtable* currentHashtable = hashtable.load();
265 
266         if (currentHashtable)
267             return currentHashtable;
268 
269         if (!currentHashtable) {
270             currentHashtable = Hashtable::create(maxLoadFactor);
271             if (hashtable.compareExchangeWeak(nullptr, currentHashtable)) {
272                 if (verbose)
273                     dataLog(toString(Thread::current(), &quot;: created initial hashtable &quot;, RawPointer(currentHashtable), &quot;\n&quot;));
274                 return currentHashtable;
275             }
276 
277             Hashtable::destroy(currentHashtable);
278         }
279     }
280 }
281 
282 // Locks the hashtable. This reloops in case of rehashing, so the current hashtable may be different
283 // after this returns than when you called it. Guarantees that there is a hashtable. This is pretty
284 // slow and not scalable, so it&#39;s only used during thread creation and for debugging/testing.
285 Vector&lt;Bucket*&gt; lockHashtable()
286 {
287     for (;;) {
288         Hashtable* currentHashtable = ensureHashtable();
289 
290         ASSERT(currentHashtable);
291 
292         // Now find all of the buckets. This makes sure that the hashtable is full of buckets so that
293         // we can lock all of the buckets, not just the ones that are materialized.
294         Vector&lt;Bucket*&gt; buckets;
295         for (unsigned i = currentHashtable-&gt;size; i--;) {
296             Atomic&lt;Bucket*&gt;&amp; bucketPointer = currentHashtable-&gt;data[i];
297 
298             for (;;) {
299                 Bucket* bucket = bucketPointer.load();
300 
301                 if (!bucket) {
302                     bucket = new Bucket();
303                     if (!bucketPointer.compareExchangeWeak(nullptr, bucket)) {
304                         delete bucket;
305                         continue;
306                     }
307                 }
308 
309                 buckets.append(bucket);
310                 break;
311             }
312         }
313 
314         // Now lock the buckets in the right order.
315         std::sort(buckets.begin(), buckets.end());
316         for (Bucket* bucket : buckets)
317             bucket-&gt;lock.lock();
318 
319         // If the hashtable didn&#39;t change (wasn&#39;t rehashed) while we were locking it, then we own it
320         // now.
321         if (hashtable.load() == currentHashtable)
322             return buckets;
323 
324         // The hashtable rehashed. Unlock everything and try again.
325         for (Bucket* bucket : buckets)
326             bucket-&gt;lock.unlock();
327     }
328 }
329 
330 void unlockHashtable(const Vector&lt;Bucket*&gt;&amp; buckets)
331 {
332     for (Bucket* bucket : buckets)
333         bucket-&gt;lock.unlock();
334 }
335 
336 // Rehash the hashtable to handle numThreads threads.
337 void ensureHashtableSize(unsigned numThreads)
338 {
339     // We try to ensure that the size of the hashtable used for thread queues is always large enough
340     // to avoid collisions. So, since we started a new thread, we may need to increase the size of the
341     // hashtable. This does just that. Note that we never free the old spine, since we never lock
342     // around spine accesses (i.e. the &quot;hashtable&quot; global variable).
343 
344     // First do a fast check to see if rehashing is needed.
345     Hashtable* oldHashtable = hashtable.load();
346     if (oldHashtable &amp;&amp; static_cast&lt;double&gt;(oldHashtable-&gt;size) / static_cast&lt;double&gt;(numThreads) &gt;= maxLoadFactor) {
347         if (verbose)
348             dataLog(toString(Thread::current(), &quot;: no need to rehash because &quot;, oldHashtable-&gt;size, &quot; / &quot;, numThreads, &quot; &gt;= &quot;, maxLoadFactor, &quot;\n&quot;));
349         return;
350     }
351 
352     // Seems like we *might* have to rehash, so lock the hashtable and try again.
353     Vector&lt;Bucket*&gt; bucketsToUnlock = lockHashtable();
354 
355     // Check again, since the hashtable could have rehashed while we were locking it. Also,
356     // lockHashtable() creates an initial hashtable for us.
357     oldHashtable = hashtable.load();
358     RELEASE_ASSERT(oldHashtable);
359     if (static_cast&lt;double&gt;(oldHashtable-&gt;size) / static_cast&lt;double&gt;(numThreads) &gt;= maxLoadFactor) {
360         if (verbose)
361             dataLog(toString(Thread::current(), &quot;: after locking, no need to rehash because &quot;, oldHashtable-&gt;size, &quot; / &quot;, numThreads, &quot; &gt;= &quot;, maxLoadFactor, &quot;\n&quot;));
362         unlockHashtable(bucketsToUnlock);
363         return;
364     }
365 
366     Vector&lt;Bucket*&gt; reusableBuckets = bucketsToUnlock;
367 
368     // OK, now we resize. First we gather all thread datas from the old hashtable. These thread datas
369     // are placed into the vector in queue order.
370     Vector&lt;ThreadData*&gt; threadDatas;
371     for (Bucket* bucket : reusableBuckets) {
372         while (ThreadData* threadData = bucket-&gt;dequeue())
373             threadDatas.append(threadData);
374     }
375 
376     unsigned newSize = numThreads * growthFactor * maxLoadFactor;
377     RELEASE_ASSERT(newSize &gt; oldHashtable-&gt;size);
378 
379     Hashtable* newHashtable = Hashtable::create(newSize);
380     if (verbose)
381         dataLog(toString(Thread::current(), &quot;: created new hashtable: &quot;, RawPointer(newHashtable), &quot;\n&quot;));
382     for (ThreadData* threadData : threadDatas) {
383         if (verbose)
384             dataLog(toString(Thread::current(), &quot;: rehashing thread data &quot;, RawPointer(threadData), &quot; with address = &quot;, RawPointer(threadData-&gt;address), &quot;\n&quot;));
385         unsigned hash = hashAddress(threadData-&gt;address);
386         unsigned index = hash % newHashtable-&gt;size;
387         if (verbose)
388             dataLog(toString(Thread::current(), &quot;: index = &quot;, index, &quot;\n&quot;));
389         Bucket* bucket = newHashtable-&gt;data[index].load();
390         if (!bucket) {
391             if (reusableBuckets.isEmpty())
392                 bucket = new Bucket();
393             else
394                 bucket = reusableBuckets.takeLast();
395             newHashtable-&gt;data[index].store(bucket);
396         }
397 
398         bucket-&gt;enqueue(threadData);
399     }
400 
401     // At this point there may be some buckets left unreused. This could easily happen if the
402     // number of enqueued threads right now is low but the high watermark of the number of threads
403     // enqueued was high. We place these buckets into the hashtable basically at random, just to
404     // make sure we don&#39;t leak them.
405     for (unsigned i = 0; i &lt; newHashtable-&gt;size &amp;&amp; !reusableBuckets.isEmpty(); ++i) {
406         Atomic&lt;Bucket*&gt;&amp; bucketPtr = newHashtable-&gt;data[i];
407         if (bucketPtr.load())
408             continue;
409         bucketPtr.store(reusableBuckets.takeLast());
410     }
411 
412     // Since we increased the size of the hashtable, we should have exhausted our preallocated
413     // buckets by now.
414     ASSERT(reusableBuckets.isEmpty());
415 
416     // OK, right now the old hashtable is locked up and the new hashtable is ready to rock and
417     // roll. After we install the new hashtable, we can release all bucket locks.
418 
419     bool result = hashtable.compareExchangeStrong(oldHashtable, newHashtable) == oldHashtable;
420     RELEASE_ASSERT(result);
421 
422     unlockHashtable(bucketsToUnlock);
423 }
424 
425 ThreadData::ThreadData()
426     : thread(Thread::current())
427 {
428     unsigned currentNumThreads;
429     for (;;) {
430         unsigned oldNumThreads = numThreads.load();
431         currentNumThreads = oldNumThreads + 1;
432         if (numThreads.compareExchangeWeak(oldNumThreads, currentNumThreads))
433             break;
434     }
435 
436     ensureHashtableSize(currentNumThreads);
437 }
438 
439 ThreadData::~ThreadData()
440 {
441     for (;;) {
442         unsigned oldNumThreads = numThreads.load();
443         if (numThreads.compareExchangeWeak(oldNumThreads, oldNumThreads - 1))
444             break;
445     }
446 }
447 
448 ThreadData* myThreadData()
449 {
450     static ThreadSpecific&lt;RefPtr&lt;ThreadData&gt;, CanBeGCThread::True&gt;* threadData;
451     static std::once_flag initializeOnce;
452     std::call_once(
453         initializeOnce,
454         [] {
455             threadData = new ThreadSpecific&lt;RefPtr&lt;ThreadData&gt;, CanBeGCThread::True&gt;();
456         });
457 
458     RefPtr&lt;ThreadData&gt;&amp; result = **threadData;
459 
460     if (!result)
461         result = adoptRef(new ThreadData());
462 
463     return result.get();
464 }
465 
466 template&lt;typename Functor&gt;
467 bool enqueue(const void* address, const Functor&amp; functor)
468 {
469     unsigned hash = hashAddress(address);
470 
471     for (;;) {
472         Hashtable* myHashtable = ensureHashtable();
473         unsigned index = hash % myHashtable-&gt;size;
474         Atomic&lt;Bucket*&gt;&amp; bucketPointer = myHashtable-&gt;data[index];
475         Bucket* bucket;
476         for (;;) {
477             bucket = bucketPointer.load();
478             if (!bucket) {
479                 bucket = new Bucket();
480                 if (!bucketPointer.compareExchangeWeak(nullptr, bucket)) {
481                     delete bucket;
482                     continue;
483                 }
484             }
485             break;
486         }
487         if (verbose)
488             dataLog(toString(Thread::current(), &quot;: enqueueing onto bucket &quot;, RawPointer(bucket), &quot; with index &quot;, index, &quot; for address &quot;, RawPointer(address), &quot; with hash &quot;, hash, &quot;\n&quot;));
489         bucket-&gt;lock.lock();
490 
491         // At this point the hashtable could have rehashed under us.
492         if (hashtable.load() != myHashtable) {
493             bucket-&gt;lock.unlock();
494             continue;
495         }
496 
497         ThreadData* threadData = functor();
498         bool result;
499         if (threadData) {
500             if (verbose)
501                 dataLog(toString(Thread::current(), &quot;: proceeding to enqueue &quot;, RawPointer(threadData), &quot;\n&quot;));
502             bucket-&gt;enqueue(threadData);
503             result = true;
504         } else
505             result = false;
506         bucket-&gt;lock.unlock();
507         return result;
508     }
509 }
510 
511 enum class BucketMode {
512     EnsureNonEmpty,
513     IgnoreEmpty
514 };
515 
516 template&lt;typename DequeueFunctor, typename FinishFunctor&gt;
517 bool dequeue(
518     const void* address, BucketMode bucketMode, const DequeueFunctor&amp; dequeueFunctor,
519     const FinishFunctor&amp; finishFunctor)
520 {
521     unsigned hash = hashAddress(address);
522 
523     for (;;) {
524         Hashtable* myHashtable = ensureHashtable();
525         unsigned index = hash % myHashtable-&gt;size;
526         Atomic&lt;Bucket*&gt;&amp; bucketPointer = myHashtable-&gt;data[index];
527         Bucket* bucket = bucketPointer.load();
528         if (!bucket) {
529             if (bucketMode == BucketMode::IgnoreEmpty)
530                 return false;
531 
532             for (;;) {
533                 bucket = bucketPointer.load();
534                 if (!bucket) {
535                     bucket = new Bucket();
536                     if (!bucketPointer.compareExchangeWeak(nullptr, bucket)) {
537                         delete bucket;
538                         continue;
539                     }
540                 }
541                 break;
542             }
543         }
544 
545         bucket-&gt;lock.lock();
546 
547         // At this point the hashtable could have rehashed under us.
548         if (hashtable.load() != myHashtable) {
549             bucket-&gt;lock.unlock();
550             continue;
551         }
552 
553         bucket-&gt;genericDequeue(dequeueFunctor);
554         bool result = !!bucket-&gt;queueHead;
555         finishFunctor(result);
556         bucket-&gt;lock.unlock();
557         return result;
558     }
559 }
560 
561 } // anonymous namespace
562 
563 NEVER_INLINE ParkingLot::ParkResult ParkingLot::parkConditionallyImpl(
564     const void* address,
565     const ScopedLambda&lt;bool()&gt;&amp; validation,
566     const ScopedLambda&lt;void()&gt;&amp; beforeSleep,
567     const TimeWithDynamicClockType&amp; timeout)
568 {
569     if (verbose)
570         dataLog(toString(Thread::current(), &quot;: parking.\n&quot;));
571 
572     ThreadData* me = myThreadData();
573     me-&gt;token = 0;
574 
575     // Guard against someone calling parkConditionally() recursively from beforeSleep().
576     RELEASE_ASSERT(!me-&gt;address);
577 
578     bool enqueueResult = enqueue(
579         address,
580         [&amp;] () -&gt; ThreadData* {
581             if (!validation())
582                 return nullptr;
583 
584             me-&gt;address = address;
585             return me;
586         });
587 
588     if (!enqueueResult)
589         return ParkResult();
590 
591     beforeSleep();
592 
593     bool didGetDequeued;
594     {
595         MutexLocker locker(me-&gt;parkingLock);
596         while (me-&gt;address &amp;&amp; timeout.nowWithSameClock() &lt; timeout) {
597             me-&gt;parkingCondition.timedWait(
598                 me-&gt;parkingLock, timeout.approximateWallTime());
599 
600             // It&#39;s possible for the OS to decide not to wait. If it does that then it will also
601             // decide not to release the lock. If there&#39;s a bug in the time math, then this could
602             // result in a deadlock. Flashing the lock means that at worst it&#39;s just a CPU-eating
603             // spin.
604             me-&gt;parkingLock.unlock();
605             me-&gt;parkingLock.lock();
606         }
607         ASSERT(!me-&gt;address || me-&gt;address == address);
608         didGetDequeued = !me-&gt;address;
609     }
610 
611     if (didGetDequeued) {
612         // Great! We actually got dequeued rather than the timeout expiring.
613         ParkResult result;
614         result.wasUnparked = true;
615         result.token = me-&gt;token;
616         return result;
617     }
618 
619     // Have to remove ourselves from the queue since we timed out and nobody has dequeued us yet.
620 
621     bool didDequeue = false;
622     dequeue(
623         address, BucketMode::IgnoreEmpty,
624         [&amp;] (ThreadData* element, bool) {
625             if (element == me) {
626                 didDequeue = true;
627                 return DequeueResult::RemoveAndStop;
628             }
629             return DequeueResult::Ignore;
630         },
631         [] (bool) { });
632 
633     // If didDequeue is true, then we dequeued ourselves. This means that we were not unparked.
634     // If didDequeue is false, then someone unparked us.
635 
636     RELEASE_ASSERT(!me-&gt;nextInQueue);
637 
638     // Make sure that no matter what, me-&gt;address is null after this point.
639     {
640         MutexLocker locker(me-&gt;parkingLock);
641         if (!didDequeue) {
642             // If we did not dequeue ourselves, then someone else did. They will set our address to
643             // null. We don&#39;t want to proceed until they do this, because otherwise, they may set
644             // our address to null in some distant future when we&#39;re already trying to wait for
645             // other things.
646             while (me-&gt;address)
647                 me-&gt;parkingCondition.wait(me-&gt;parkingLock);
648         }
649         me-&gt;address = nullptr;
650     }
651 
652     ParkResult result;
653     result.wasUnparked = !didDequeue;
654     if (!didDequeue) {
655         // If we were unparked then there should be a token.
656         result.token = me-&gt;token;
657     }
658     return result;
659 }
660 
661 NEVER_INLINE ParkingLot::UnparkResult ParkingLot::unparkOne(const void* address)
662 {
663     if (verbose)
664         dataLog(toString(Thread::current(), &quot;: unparking one.\n&quot;));
665 
666     UnparkResult result;
667 
668     RefPtr&lt;ThreadData&gt; threadData;
669     result.mayHaveMoreThreads = dequeue(
670         address,
671         // Why is this here?
672         // FIXME: It seems like this could be IgnoreEmpty, but I switched this to EnsureNonEmpty
673         // without explanation in r199760. We need it to use EnsureNonEmpty if we need to perform
674         // some operation while holding the bucket lock, which usually goes into the finish func.
675         // But if that operation is a no-op, then it&#39;s not clear why we need this.
676         BucketMode::EnsureNonEmpty,
677         [&amp;] (ThreadData* element, bool) {
678             if (element-&gt;address != address)
679                 return DequeueResult::Ignore;
680             threadData = element;
681             result.didUnparkThread = true;
682             return DequeueResult::RemoveAndStop;
683         },
684         [] (bool) { });
685 
686     if (!threadData) {
687         ASSERT(!result.didUnparkThread);
688         result.mayHaveMoreThreads = false;
689         return result;
690     }
691 
692     ASSERT(threadData-&gt;address);
693 
694     {
695         MutexLocker locker(threadData-&gt;parkingLock);
696         threadData-&gt;address = nullptr;
697         threadData-&gt;token = 0;
698     }
699     threadData-&gt;parkingCondition.signal();
700 
701     return result;
702 }
703 
704 NEVER_INLINE void ParkingLot::unparkOneImpl(
705     const void* address,
706     const ScopedLambda&lt;intptr_t(ParkingLot::UnparkResult)&gt;&amp; callback)
707 {
708     if (verbose)
709         dataLog(toString(Thread::current(), &quot;: unparking one the hard way.\n&quot;));
710 
711     RefPtr&lt;ThreadData&gt; threadData;
712     bool timeToBeFair = false;
713     dequeue(
714         address,
715         BucketMode::EnsureNonEmpty,
716         [&amp;] (ThreadData* element, bool passedTimeToBeFair) {
717             if (element-&gt;address != address)
718                 return DequeueResult::Ignore;
719             threadData = element;
720             timeToBeFair = passedTimeToBeFair;
721             return DequeueResult::RemoveAndStop;
722         },
723         [&amp;] (bool mayHaveMoreThreads) {
724             UnparkResult result;
725             result.didUnparkThread = !!threadData;
726             result.mayHaveMoreThreads = result.didUnparkThread &amp;&amp; mayHaveMoreThreads;
727             if (timeToBeFair)
728                 RELEASE_ASSERT(threadData);
729             result.timeToBeFair = timeToBeFair;
730             intptr_t token = callback(result);
731             if (threadData)
732                 threadData-&gt;token = token;
733         });
734 
735     if (!threadData)
736         return;
737 
738     ASSERT(threadData-&gt;address);
739 
740     {
741         MutexLocker locker(threadData-&gt;parkingLock);
742         threadData-&gt;address = nullptr;
743     }
744     // At this point, the threadData may die. Good thing we have a RefPtr&lt;&gt; on it.
745     threadData-&gt;parkingCondition.signal();
746 }
747 
748 NEVER_INLINE unsigned ParkingLot::unparkCount(const void* address, unsigned count)
749 {
750     if (!count)
751         return 0;
752 
753     if (verbose)
754         dataLog(toString(Thread::current(), &quot;: unparking count = &quot;, count, &quot; from &quot;, RawPointer(address), &quot;.\n&quot;));
755 
756     Vector&lt;RefPtr&lt;ThreadData&gt;, 8&gt; threadDatas;
757     dequeue(
758         address,
759         // FIXME: It seems like this ought to be EnsureNonEmpty if we follow what unparkOne() does,
760         // but that seems wrong.
761         BucketMode::IgnoreEmpty,
762         [&amp;] (ThreadData* element, bool) {
763             if (verbose)
764                 dataLog(toString(Thread::current(), &quot;: Observing element with address = &quot;, RawPointer(element-&gt;address), &quot;\n&quot;));
765             if (element-&gt;address != address)
766                 return DequeueResult::Ignore;
767             threadDatas.append(element);
768             if (threadDatas.size() == count)
769                 return DequeueResult::RemoveAndStop;
770             return DequeueResult::RemoveAndContinue;
771         },
772         [] (bool) { });
773 
774     for (RefPtr&lt;ThreadData&gt;&amp; threadData : threadDatas) {
775         if (verbose)
776             dataLog(toString(Thread::current(), &quot;: unparking &quot;, RawPointer(threadData.get()), &quot; with address &quot;, RawPointer(threadData-&gt;address), &quot;\n&quot;));
777         ASSERT(threadData-&gt;address);
778         {
779             MutexLocker locker(threadData-&gt;parkingLock);
780             threadData-&gt;address = nullptr;
781         }
782         threadData-&gt;parkingCondition.signal();
783     }
784 
785     if (verbose)
786         dataLog(toString(Thread::current(), &quot;: done unparking.\n&quot;));
787 
788     return threadDatas.size();
789 }
790 
791 NEVER_INLINE void ParkingLot::unparkAll(const void* address)
792 {
793     unparkCount(address, UINT_MAX);
794 }
795 
796 NEVER_INLINE void ParkingLot::forEachImpl(const ScopedLambda&lt;void(Thread&amp;, const void*)&gt;&amp; callback)
797 {
798     Vector&lt;Bucket*&gt; bucketsToUnlock = lockHashtable();
799 
800     Hashtable* currentHashtable = hashtable.load();
801     for (unsigned i = currentHashtable-&gt;size; i--;) {
802         Bucket* bucket = currentHashtable-&gt;data[i].load();
803         if (!bucket)
804             continue;
805         for (ThreadData* currentThreadData = bucket-&gt;queueHead; currentThreadData; currentThreadData = currentThreadData-&gt;nextInQueue)
806             callback(currentThreadData-&gt;thread.get(), currentThreadData-&gt;address);
807     }
808 
809     unlockHashtable(bucketsToUnlock);
810 }
811 
812 } // namespace WTF
813 
    </pre>
  </body>
</html>