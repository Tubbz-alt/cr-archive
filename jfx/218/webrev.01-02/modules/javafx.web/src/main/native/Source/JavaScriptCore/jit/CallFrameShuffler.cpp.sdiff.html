<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/CallFrameShuffler.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CallFrameShuffleData.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CallFrameShuffler.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/CallFrameShuffler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 44         + roundArgumentCountToAlignFrame(data.args.size()))
 45     , m_frameDelta(m_alignedNewFrameSize - m_alignedOldFrameSize)
 46     , m_lockedRegisters(RegisterSet::allRegisters())
 47     , m_numPassedArgs(data.numPassedArgs)
 48 {
 49     // We are allowed all the usual registers...
 50     for (unsigned i = GPRInfo::numberOfRegisters; i--; )
 51         m_lockedRegisters.clear(GPRInfo::toRegister(i));
 52     for (unsigned i = FPRInfo::numberOfRegisters; i--; )
 53         m_lockedRegisters.clear(FPRInfo::toRegister(i));
 54 
 55 #if USE(JSVALUE64)
 56     // ... as well as the runtime registers on 64-bit architectures.
 57     // However do not use these registers on 32-bit architectures since
 58     // saving and restoring callee-saved registers in CallFrameShuffler isn&#39;t supported
 59     // on 32-bit architectures yet.
 60     m_lockedRegisters.exclude(RegisterSet::vmCalleeSaveRegisters());
 61 #endif
 62 
 63     ASSERT(!data.callee.isInJSStack() || data.callee.virtualRegister().isLocal());
<span class="line-modified"> 64     addNew(VirtualRegister(CallFrameSlot::callee), data.callee);</span>
 65 
 66     for (size_t i = 0; i &lt; data.args.size(); ++i) {
 67         ASSERT(!data.args[i].isInJSStack() || data.args[i].virtualRegister().isLocal());
<span class="line-modified"> 68         addNew(virtualRegisterForArgument(i), data.args[i]);</span>
 69     }
 70 
 71 #if USE(JSVALUE64)
 72     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
 73         if (!data.registers[reg].isSet())
 74             continue;
 75 
 76         if (reg.isGPR())
 77             addNew(JSValueRegs(reg.gpr()), data.registers[reg]);
 78         else
 79             addNew(reg.fpr(), data.registers[reg]);
 80     }
 81 
<span class="line-modified"> 82     m_tagTypeNumber = data.tagTypeNumber;</span>
<span class="line-modified"> 83     if (m_tagTypeNumber != InvalidGPRReg)</span>
<span class="line-modified"> 84         lockGPR(m_tagTypeNumber);</span>
 85 #endif
 86 }
 87 
 88 void CallFrameShuffler::dump(PrintStream&amp; out) const
 89 {
 90     static const char* delimiter             = &quot; +-------------------------------+ &quot;;
 91     static const char* dangerDelimiter       = &quot; X-------------------------------X &quot;;
 92     static const char* dangerBoundsDelimiter = &quot; XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX &quot;;
 93     static const char* emptySpace            = &quot;                                   &quot;;
 94     out.print(&quot;          &quot;);
 95     out.print(&quot;           Old frame               &quot;);
 96     out.print(&quot;           New frame               &quot;);
 97     out.print(&quot;\n&quot;);
 98     int totalSize = m_alignedOldFrameSize + std::max(numLocals(), m_alignedNewFrameSize) + 3;
 99     for (int i = 0; i &lt; totalSize; ++i) {
100         VirtualRegister old { m_alignedOldFrameSize - i - 1 };
101         VirtualRegister newReg { old + m_frameDelta };
102 
103         if (!isValidOld(old) &amp;&amp; old != firstOld() - 1
104             &amp;&amp; !isValidNew(newReg) &amp;&amp; newReg != firstNew() - 1)
</pre>
<hr />
<pre>
135                     out.printf(&quot; X      %18s       X &quot;, str.data());
136                 else
137                     out.printf(&quot; |      %18s       | &quot;, str.data());
138             } else if (isValidNew(newReg) &amp;&amp; isDangerNew(newReg))
139                 out.printf(&quot; X%30s X &quot;, &quot;&quot;);
140             else
141                 out.printf(&quot; |%30s | &quot;, &quot;&quot;);
142         } else
143             out.print(emptySpace);
144         if (isValidNew(newReg)) {
145             const char d = isDangerNew(newReg) ? &#39;X&#39; : &#39;|&#39;;
146             auto str = toCString(newReg);
147             if (getNew(newReg)) {
148                 if (getNew(newReg)-&gt;recovery().isConstant())
149                     out.printf(&quot; %c%8s &lt;-           constant %c &quot;, d, str.data(), d);
150                 else {
151                     auto recoveryStr = toCString(getNew(newReg)-&gt;recovery());
152                     out.printf(&quot; %c%8s &lt;- %18s %c &quot;, d, str.data(),
153                         recoveryStr.data(), d);
154                 }
<span class="line-modified">155             } else if (newReg == VirtualRegister { CallFrameSlot::argumentCount })</span>
156                 out.printf(&quot; %c%8s &lt;- %18zu %c &quot;, d, str.data(), argCount(), d);
157             else
158                 out.printf(&quot; %c%30s %c &quot;, d, &quot;&quot;, d);
159         } else
160             out.print(emptySpace);
161         if (newReg == firstNew() - m_newFrameOffset &amp;&amp; !isSlowPath())
162             out.print(&quot; &lt;-- new sp before jump (current &quot;, m_newFrameBase, &quot;) &quot;);
163         if (newReg == firstNew())
164             out.print(&quot; &lt;-- new fp after prologue&quot;);
165         out.print(&quot;\n&quot;);
166     }
167     out.print(&quot;          &quot;);
168     out.print(&quot;        Live registers             &quot;);
169     out.print(&quot;        Wanted registers           &quot;);
170     out.print(&quot;\n&quot;);
171     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
172         CachedRecovery* oldCachedRecovery { m_registers[reg] };
173         CachedRecovery* newCachedRecovery { m_newRegisters[reg] };
174         if (!oldCachedRecovery &amp;&amp; !newCachedRecovery)
175             continue;
</pre>
<hr />
<pre>
199     }
200     out.print(&quot;  Locked registers: &quot;);
201     bool firstLocked { true };
202     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
203         if (m_lockedRegisters.get(reg)) {
204             out.print(firstLocked ? &quot;&quot; : &quot;, &quot;, reg);
205             firstLocked = false;
206         }
207     }
208     out.print(&quot;\n&quot;);
209 
210     if (isSlowPath())
211         out.print(&quot;  Using fp-relative addressing for slow path call\n&quot;);
212     else
213         out.print(&quot;  Using sp-relative addressing for jump (using &quot;, m_newFrameBase, &quot; as new sp)\n&quot;);
214     if (m_oldFrameOffset)
215         out.print(&quot;   Old frame offset is &quot;, m_oldFrameOffset, &quot;\n&quot;);
216     if (m_newFrameOffset)
217         out.print(&quot;   New frame offset is &quot;, m_newFrameOffset, &quot;\n&quot;);
218 #if USE(JSVALUE64)
<span class="line-modified">219     if (m_tagTypeNumber != InvalidGPRReg)</span>
<span class="line-modified">220         out.print(&quot;   TagTypeNumber is currently in &quot;, m_tagTypeNumber, &quot;\n&quot;);</span>
221 #endif
222 }
223 
224 CachedRecovery* CallFrameShuffler::getCachedRecovery(ValueRecovery recovery)
225 {
226     ASSERT(!recovery.isConstant());
227     if (recovery.isInGPR())
228         return m_registers[recovery.gpr()];
229     if (recovery.isInFPR())
230         return m_registers[recovery.fpr()];
231 #if USE(JSVALUE32_64)
232     if (recovery.technique() == InPair) {
233         ASSERT(m_registers[recovery.tagGPR()] == m_registers[recovery.payloadGPR()]);
234         return m_registers[recovery.payloadGPR()];
235     }
236 #endif
237     ASSERT(recovery.isInJSStack());
238     return getOld(recovery.virtualRegister());
239 }
240 
</pre>
<hr />
<pre>
275     // We must have enough slots to be able to fit the whole callee&#39;s
276     // frame for the slow path - unless we are in the FTL. In that
277     // case, we are allowed to extend the frame *once*, since we are
278     // guaranteed to have enough available space for that.
279     if (spillSlot &gt;= newAsOld(firstNew()) || !spillSlot.isLocal()) {
280         RELEASE_ASSERT(!m_didExtendFrame);
281         extendFrameIfNeeded();
282         spill(cachedRecovery);
283         return;
284     }
285 
286     if (verbose)
287         dataLog(&quot;   * Spilling &quot;, cachedRecovery.recovery(), &quot; into &quot;, spillSlot, &quot;\n&quot;);
288     auto format = emitStore(cachedRecovery, addressForOld(spillSlot));
289     ASSERT(format != DataFormatNone);
290     updateRecovery(cachedRecovery, ValueRecovery::displacedInJSStack(spillSlot, format));
291 }
292 
293 void CallFrameShuffler::emitDeltaCheck()
294 {
<span class="line-modified">295     if (ASSERT_DISABLED)</span>
296         return;
297 
298     GPRReg scratchGPR { getFreeGPR() };
299     if (scratchGPR != InvalidGPRReg) {
300         if (verbose)
301             dataLog(&quot;  Using &quot;, scratchGPR, &quot; for the fp-sp delta check\n&quot;);
302         m_jit.move(MacroAssembler::stackPointerRegister, scratchGPR);
303         m_jit.subPtr(GPRInfo::callFrameRegister, scratchGPR);
304         MacroAssembler::Jump ok = m_jit.branch32(
305             MacroAssembler::Equal, scratchGPR,
306             MacroAssembler::TrustedImm32(-numLocals() * sizeof(Register)));
307         m_jit.abortWithReason(JITUnexpectedCallFrameSize);
308         ok.link(&amp;m_jit);
309     } else if (verbose)
310         dataLog(&quot;  Skipping the fp-sp delta check since there is too much pressure&quot;);
311 }
312 
313 void CallFrameShuffler::extendFrameIfNeeded()
314 {
315     ASSERT(!m_didExtendFrame);
</pre>
<hr />
<pre>
360 
361     if (verbose)
362         dataLog(*this);
363 
364     prepareAny();
365 
366     if (verbose)
367         dataLog(&quot;Ready for slow path call!\n&quot;);
368 }
369 
370 void CallFrameShuffler::prepareForTailCall()
371 {
372     ASSERT(isUndecided());
373     emitDeltaCheck();
374 
375     // We&#39;ll use sp-based indexing so that we can load the
376     // caller&#39;s frame pointer into the fpr immediately
377     m_oldFrameBase = MacroAssembler::stackPointerRegister;
378     m_oldFrameOffset = numLocals();
379     m_newFrameBase = acquireGPR();
<span class="line-modified">380 #if CPU(X86)</span>
<span class="line-removed">381     // We load the frame pointer manually, but we need to ask the</span>
<span class="line-removed">382     // algorithm to move the return PC for us (it&#39;d probably</span>
<span class="line-removed">383     // require a write to the danger zone). Since it&#39;d be awkward</span>
<span class="line-removed">384     // to ask for half a value move, we ask that the whole thing</span>
<span class="line-removed">385     // be moved for us.</span>
<span class="line-removed">386     addNew(VirtualRegister { 0 },</span>
<span class="line-removed">387         ValueRecovery::displacedInJSStack(VirtualRegister(0), DataFormatJS));</span>
<span class="line-removed">388 </span>
<span class="line-removed">389     // sp will point to head0 and we will move it up half a slot</span>
<span class="line-removed">390     // manually</span>
<span class="line-removed">391     m_newFrameOffset = 0;</span>
<span class="line-removed">392 #elif CPU(ARM_THUMB2) || CPU(MIPS)</span>
393     // We load the frame pointer and link register
394     // manually. We could ask the algorithm to load them for us,
395     // and it would allow us to use the link register as an extra
396     // temporary - but it&#39;d mean that the frame pointer can also
397     // be used as an extra temporary, so we keep the link register
398     // locked instead.
399 
400     // sp will point to head1 since the callee&#39;s prologue pushes
401     // the call frame and link register.
402     m_newFrameOffset = -1;
403 #elif CPU(ARM64)
404     // We load the frame pointer and link register manually. We
405     // could ask the algorithm to load the link register for us
406     // (which would allow for its use as an extra temporary), but
407     // since its not in GPRInfo, we can&#39;t do it.
408 
409     // sp will point to head2 since the callee&#39;s prologue pushes the
410     // call frame and link register
411     m_newFrameOffset = -2;
412 #elif CPU(X86_64)
413     // We load the frame pointer manually, but we ask the
414     // algorithm to move the return PC for us (it&#39;d probably
415     // require a write in the danger zone)
416     addNew(VirtualRegister { 1 },
417         ValueRecovery::displacedInJSStack(VirtualRegister(1), DataFormatJS));
418 
419     // sp will point to head1 since the callee&#39;s prologue pushes
420     // the call frame register
421     m_newFrameOffset = -1;
422 #else
423     UNREACHABLE_FOR_PLATFORM();
424 #endif
425 
426     if (verbose)
427         dataLog(&quot;  Emitting code for computing the new frame base\n&quot;);
428 
429     // We compute the new frame base by first computing the top of the
430     // old frame (taking into account an argument count higher than
431     // the number of parameters), then substracting to it the aligned
432     // new frame size (adjusted).
<span class="line-modified">433     m_jit.load32(MacroAssembler::Address(GPRInfo::callFrameRegister, CallFrameSlot::argumentCount * static_cast&lt;int&gt;(sizeof(Register)) + PayloadOffset), m_newFrameBase);</span>
434     MacroAssembler::Jump argumentCountOK =
435         m_jit.branch32(MacroAssembler::BelowOrEqual, m_newFrameBase,
436             MacroAssembler::TrustedImm32(m_jit.codeBlock()-&gt;numParameters()));
437     m_jit.add32(MacroAssembler::TrustedImm32(stackAlignmentRegisters() - 1 + CallFrame::headerSizeInRegisters), m_newFrameBase);
438     m_jit.and32(MacroAssembler::TrustedImm32(-stackAlignmentRegisters()), m_newFrameBase);
439     m_jit.mul32(MacroAssembler::TrustedImm32(sizeof(Register)), m_newFrameBase, m_newFrameBase);
440     MacroAssembler::Jump done = m_jit.jump();
441     argumentCountOK.link(&amp;m_jit);
442     m_jit.move(
443         MacroAssembler::TrustedImm32(m_alignedOldFrameSize * sizeof(Register)),
444         m_newFrameBase);
445     done.link(&amp;m_jit);
446 
447     m_jit.addPtr(GPRInfo::callFrameRegister, m_newFrameBase);
448     m_jit.subPtr(
449         MacroAssembler::TrustedImm32(
450             (m_alignedNewFrameSize + m_newFrameOffset) * sizeof(Register)),
451         m_newFrameBase);
452 
453     // We load the link register manually for architectures that have one
</pre>
<hr />
<pre>
459     m_jit.untagPtr(MacroAssembler::framePointerRegister, MacroAssembler::linkRegister);
460     m_jit.subPtr(MacroAssembler::TrustedImm32(sizeof(CallerFrameAndPC)), MacroAssembler::framePointerRegister);
461 #endif
462 
463 #elif CPU(MIPS)
464     m_jit.loadPtr(MacroAssembler::Address(MacroAssembler::framePointerRegister, sizeof(void*)),
465         MacroAssembler::returnAddressRegister);
466 #endif
467 
468     // We want the frame pointer to always point to a valid frame, and
469     // we are going to trash the current one. Let&#39;s make it point to
470     // our caller&#39;s frame, since that&#39;s what we want to end up with.
471     m_jit.loadPtr(MacroAssembler::Address(MacroAssembler::framePointerRegister),
472         MacroAssembler::framePointerRegister);
473 
474     if (verbose)
475         dataLog(&quot;Preparing frame for tail call:\n&quot;, *this);
476 
477     prepareAny();
478 
<span class="line-removed">479 #if CPU(X86)</span>
<span class="line-removed">480     if (verbose)</span>
<span class="line-removed">481         dataLog(&quot;  Simulating pop of the call frame register\n&quot;);</span>
<span class="line-removed">482     m_jit.addPtr(MacroAssembler::TrustedImm32(sizeof(void*)), MacroAssembler::stackPointerRegister);</span>
<span class="line-removed">483 #endif</span>
<span class="line-removed">484 </span>
485     if (verbose)
486         dataLog(&quot;Ready for tail call!\n&quot;);
487 }
488 
489 bool CallFrameShuffler::tryWrites(CachedRecovery&amp; cachedRecovery)
490 {
491     ASSERT(m_newFrameBase != InvalidGPRReg);
492 
493     // If the value is already set up correctly, we don&#39;t have
494     // anything to do.
495     if (isSlowPath() &amp;&amp; cachedRecovery.recovery().isInJSStack()
496         &amp;&amp; cachedRecovery.targets().size() == 1
497         &amp;&amp; newAsOld(cachedRecovery.targets()[0]) == cachedRecovery.recovery().virtualRegister()) {
498         cachedRecovery.clearTargets();
499         if (!cachedRecovery.wantedJSValueRegs() &amp;&amp; cachedRecovery.wantedFPR() == InvalidFPRReg)
500             clearCachedRecovery(cachedRecovery.recovery());
501         return true;
502     }
503 
504     if (!canLoadAndBox(cachedRecovery))
</pre>
<hr />
<pre>
699 
700     // At this point, we must have enough registers available for
701     // handling 1). None of the loads can fail because we have been
702     // eagerly freeing up registers in all the previous phases - so
703     // the only values that are in registers at this point must have
704     // wanted registers.
705     if (verbose)
706         dataLog(&quot;  Danger zone is clear, performing remaining writes.\n&quot;);
707     for (VirtualRegister reg = firstNew(); reg &lt;= lastNew(); reg += 1) {
708         CachedRecovery* cachedRecovery { getNew(reg) };
709         if (!cachedRecovery)
710             continue;
711 
712         emitLoad(*cachedRecovery);
713         emitBox(*cachedRecovery);
714         bool writesOK = tryWrites(*cachedRecovery);
715         ASSERT_UNUSED(writesOK, writesOK);
716     }
717 
718 #if USE(JSVALUE64)
<span class="line-modified">719     if (m_tagTypeNumber != InvalidGPRReg &amp;&amp; m_newRegisters[m_tagTypeNumber])</span>
<span class="line-modified">720         releaseGPR(m_tagTypeNumber);</span>
721 #endif
722 
723     // Handle 2) by loading all registers. We don&#39;t have to do any
724     // writes, since they have been taken care of above.
725     if (verbose)
726         dataLog(&quot;  Loading wanted registers into registers\n&quot;);
727     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
728         CachedRecovery* cachedRecovery { m_newRegisters[reg] };
729         if (!cachedRecovery)
730             continue;
731 
732         emitLoad(*cachedRecovery);
733         emitBox(*cachedRecovery);
734         ASSERT(cachedRecovery-&gt;targets().isEmpty());
735     }
736 
737 #if USE(JSVALUE64)
<span class="line-modified">738     if (m_tagTypeNumber != InvalidGPRReg)</span>
<span class="line-modified">739         releaseGPR(m_tagTypeNumber);</span>
740 #endif
741 
742     // At this point, we have read everything we cared about from the
743     // stack, and written everything we had to to the stack.
744     if (verbose)
745         dataLog(&quot;  Callee frame is fully set up\n&quot;);
<span class="line-modified">746     if (!ASSERT_DISABLED) {</span>
747         for (VirtualRegister reg = firstNew(); reg &lt;= lastNew(); reg += 1)
748             ASSERT_UNUSED(reg, !getNew(reg));
749 
750         for (CachedRecovery* cachedRecovery : m_cachedRecoveries) {
751             ASSERT_UNUSED(cachedRecovery, cachedRecovery-&gt;targets().isEmpty());
752             ASSERT(!cachedRecovery-&gt;recovery().isInJSStack());
753         }
754     }
755 
756     // We need to handle 4) first because it implies releasing
757     // m_newFrameBase, which could be a wanted register.
758     if (verbose)
<span class="line-modified">759         dataLog(&quot;   * Storing the argument count into &quot;, VirtualRegister { CallFrameSlot::argumentCount }, &quot;\n&quot;);</span>
760     m_jit.store32(MacroAssembler::TrustedImm32(0),
<span class="line-modified">761         addressForNew(VirtualRegister { CallFrameSlot::argumentCount }).withOffset(TagOffset));</span>
762     RELEASE_ASSERT(m_numPassedArgs != UINT_MAX);
763     m_jit.store32(MacroAssembler::TrustedImm32(m_numPassedArgs),
<span class="line-modified">764         addressForNew(VirtualRegister { CallFrameSlot::argumentCount }).withOffset(PayloadOffset));</span>
765 
766     if (!isSlowPath()) {
767         ASSERT(m_newFrameBase != MacroAssembler::stackPointerRegister);
768         if (verbose)
769             dataLog(&quot;  Releasing the new frame base pointer\n&quot;);
770         m_jit.move(m_newFrameBase, MacroAssembler::stackPointerRegister);
771         releaseGPR(m_newFrameBase);
772     }
773 
774     // Finally we handle 3)
775     if (verbose)
776         dataLog(&quot;  Ensuring wanted registers are in the right register\n&quot;);
777     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
778         CachedRecovery* cachedRecovery { m_newRegisters[reg] };
779         if (!cachedRecovery)
780             continue;
781 
782         emitDisplace(*cachedRecovery);
783     }
784 }
</pre>
</td>
<td>
<hr />
<pre>
 44         + roundArgumentCountToAlignFrame(data.args.size()))
 45     , m_frameDelta(m_alignedNewFrameSize - m_alignedOldFrameSize)
 46     , m_lockedRegisters(RegisterSet::allRegisters())
 47     , m_numPassedArgs(data.numPassedArgs)
 48 {
 49     // We are allowed all the usual registers...
 50     for (unsigned i = GPRInfo::numberOfRegisters; i--; )
 51         m_lockedRegisters.clear(GPRInfo::toRegister(i));
 52     for (unsigned i = FPRInfo::numberOfRegisters; i--; )
 53         m_lockedRegisters.clear(FPRInfo::toRegister(i));
 54 
 55 #if USE(JSVALUE64)
 56     // ... as well as the runtime registers on 64-bit architectures.
 57     // However do not use these registers on 32-bit architectures since
 58     // saving and restoring callee-saved registers in CallFrameShuffler isn&#39;t supported
 59     // on 32-bit architectures yet.
 60     m_lockedRegisters.exclude(RegisterSet::vmCalleeSaveRegisters());
 61 #endif
 62 
 63     ASSERT(!data.callee.isInJSStack() || data.callee.virtualRegister().isLocal());
<span class="line-modified"> 64     addNew(CallFrameSlot::callee, data.callee);</span>
 65 
 66     for (size_t i = 0; i &lt; data.args.size(); ++i) {
 67         ASSERT(!data.args[i].isInJSStack() || data.args[i].virtualRegister().isLocal());
<span class="line-modified"> 68         addNew(virtualRegisterForArgumentIncludingThis(i), data.args[i]);</span>
 69     }
 70 
 71 #if USE(JSVALUE64)
 72     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
 73         if (!data.registers[reg].isSet())
 74             continue;
 75 
 76         if (reg.isGPR())
 77             addNew(JSValueRegs(reg.gpr()), data.registers[reg]);
 78         else
 79             addNew(reg.fpr(), data.registers[reg]);
 80     }
 81 
<span class="line-modified"> 82     m_numberTagRegister = data.numberTagRegister;</span>
<span class="line-modified"> 83     if (m_numberTagRegister != InvalidGPRReg)</span>
<span class="line-modified"> 84         lockGPR(m_numberTagRegister);</span>
 85 #endif
 86 }
 87 
 88 void CallFrameShuffler::dump(PrintStream&amp; out) const
 89 {
 90     static const char* delimiter             = &quot; +-------------------------------+ &quot;;
 91     static const char* dangerDelimiter       = &quot; X-------------------------------X &quot;;
 92     static const char* dangerBoundsDelimiter = &quot; XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX &quot;;
 93     static const char* emptySpace            = &quot;                                   &quot;;
 94     out.print(&quot;          &quot;);
 95     out.print(&quot;           Old frame               &quot;);
 96     out.print(&quot;           New frame               &quot;);
 97     out.print(&quot;\n&quot;);
 98     int totalSize = m_alignedOldFrameSize + std::max(numLocals(), m_alignedNewFrameSize) + 3;
 99     for (int i = 0; i &lt; totalSize; ++i) {
100         VirtualRegister old { m_alignedOldFrameSize - i - 1 };
101         VirtualRegister newReg { old + m_frameDelta };
102 
103         if (!isValidOld(old) &amp;&amp; old != firstOld() - 1
104             &amp;&amp; !isValidNew(newReg) &amp;&amp; newReg != firstNew() - 1)
</pre>
<hr />
<pre>
135                     out.printf(&quot; X      %18s       X &quot;, str.data());
136                 else
137                     out.printf(&quot; |      %18s       | &quot;, str.data());
138             } else if (isValidNew(newReg) &amp;&amp; isDangerNew(newReg))
139                 out.printf(&quot; X%30s X &quot;, &quot;&quot;);
140             else
141                 out.printf(&quot; |%30s | &quot;, &quot;&quot;);
142         } else
143             out.print(emptySpace);
144         if (isValidNew(newReg)) {
145             const char d = isDangerNew(newReg) ? &#39;X&#39; : &#39;|&#39;;
146             auto str = toCString(newReg);
147             if (getNew(newReg)) {
148                 if (getNew(newReg)-&gt;recovery().isConstant())
149                     out.printf(&quot; %c%8s &lt;-           constant %c &quot;, d, str.data(), d);
150                 else {
151                     auto recoveryStr = toCString(getNew(newReg)-&gt;recovery());
152                     out.printf(&quot; %c%8s &lt;- %18s %c &quot;, d, str.data(),
153                         recoveryStr.data(), d);
154                 }
<span class="line-modified">155             } else if (newReg == VirtualRegister { CallFrameSlot::argumentCountIncludingThis })</span>
156                 out.printf(&quot; %c%8s &lt;- %18zu %c &quot;, d, str.data(), argCount(), d);
157             else
158                 out.printf(&quot; %c%30s %c &quot;, d, &quot;&quot;, d);
159         } else
160             out.print(emptySpace);
161         if (newReg == firstNew() - m_newFrameOffset &amp;&amp; !isSlowPath())
162             out.print(&quot; &lt;-- new sp before jump (current &quot;, m_newFrameBase, &quot;) &quot;);
163         if (newReg == firstNew())
164             out.print(&quot; &lt;-- new fp after prologue&quot;);
165         out.print(&quot;\n&quot;);
166     }
167     out.print(&quot;          &quot;);
168     out.print(&quot;        Live registers             &quot;);
169     out.print(&quot;        Wanted registers           &quot;);
170     out.print(&quot;\n&quot;);
171     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
172         CachedRecovery* oldCachedRecovery { m_registers[reg] };
173         CachedRecovery* newCachedRecovery { m_newRegisters[reg] };
174         if (!oldCachedRecovery &amp;&amp; !newCachedRecovery)
175             continue;
</pre>
<hr />
<pre>
199     }
200     out.print(&quot;  Locked registers: &quot;);
201     bool firstLocked { true };
202     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
203         if (m_lockedRegisters.get(reg)) {
204             out.print(firstLocked ? &quot;&quot; : &quot;, &quot;, reg);
205             firstLocked = false;
206         }
207     }
208     out.print(&quot;\n&quot;);
209 
210     if (isSlowPath())
211         out.print(&quot;  Using fp-relative addressing for slow path call\n&quot;);
212     else
213         out.print(&quot;  Using sp-relative addressing for jump (using &quot;, m_newFrameBase, &quot; as new sp)\n&quot;);
214     if (m_oldFrameOffset)
215         out.print(&quot;   Old frame offset is &quot;, m_oldFrameOffset, &quot;\n&quot;);
216     if (m_newFrameOffset)
217         out.print(&quot;   New frame offset is &quot;, m_newFrameOffset, &quot;\n&quot;);
218 #if USE(JSVALUE64)
<span class="line-modified">219     if (m_numberTagRegister != InvalidGPRReg)</span>
<span class="line-modified">220         out.print(&quot;   NumberTag is currently in &quot;, m_numberTagRegister, &quot;\n&quot;);</span>
221 #endif
222 }
223 
224 CachedRecovery* CallFrameShuffler::getCachedRecovery(ValueRecovery recovery)
225 {
226     ASSERT(!recovery.isConstant());
227     if (recovery.isInGPR())
228         return m_registers[recovery.gpr()];
229     if (recovery.isInFPR())
230         return m_registers[recovery.fpr()];
231 #if USE(JSVALUE32_64)
232     if (recovery.technique() == InPair) {
233         ASSERT(m_registers[recovery.tagGPR()] == m_registers[recovery.payloadGPR()]);
234         return m_registers[recovery.payloadGPR()];
235     }
236 #endif
237     ASSERT(recovery.isInJSStack());
238     return getOld(recovery.virtualRegister());
239 }
240 
</pre>
<hr />
<pre>
275     // We must have enough slots to be able to fit the whole callee&#39;s
276     // frame for the slow path - unless we are in the FTL. In that
277     // case, we are allowed to extend the frame *once*, since we are
278     // guaranteed to have enough available space for that.
279     if (spillSlot &gt;= newAsOld(firstNew()) || !spillSlot.isLocal()) {
280         RELEASE_ASSERT(!m_didExtendFrame);
281         extendFrameIfNeeded();
282         spill(cachedRecovery);
283         return;
284     }
285 
286     if (verbose)
287         dataLog(&quot;   * Spilling &quot;, cachedRecovery.recovery(), &quot; into &quot;, spillSlot, &quot;\n&quot;);
288     auto format = emitStore(cachedRecovery, addressForOld(spillSlot));
289     ASSERT(format != DataFormatNone);
290     updateRecovery(cachedRecovery, ValueRecovery::displacedInJSStack(spillSlot, format));
291 }
292 
293 void CallFrameShuffler::emitDeltaCheck()
294 {
<span class="line-modified">295     if (!ASSERT_ENABLED)</span>
296         return;
297 
298     GPRReg scratchGPR { getFreeGPR() };
299     if (scratchGPR != InvalidGPRReg) {
300         if (verbose)
301             dataLog(&quot;  Using &quot;, scratchGPR, &quot; for the fp-sp delta check\n&quot;);
302         m_jit.move(MacroAssembler::stackPointerRegister, scratchGPR);
303         m_jit.subPtr(GPRInfo::callFrameRegister, scratchGPR);
304         MacroAssembler::Jump ok = m_jit.branch32(
305             MacroAssembler::Equal, scratchGPR,
306             MacroAssembler::TrustedImm32(-numLocals() * sizeof(Register)));
307         m_jit.abortWithReason(JITUnexpectedCallFrameSize);
308         ok.link(&amp;m_jit);
309     } else if (verbose)
310         dataLog(&quot;  Skipping the fp-sp delta check since there is too much pressure&quot;);
311 }
312 
313 void CallFrameShuffler::extendFrameIfNeeded()
314 {
315     ASSERT(!m_didExtendFrame);
</pre>
<hr />
<pre>
360 
361     if (verbose)
362         dataLog(*this);
363 
364     prepareAny();
365 
366     if (verbose)
367         dataLog(&quot;Ready for slow path call!\n&quot;);
368 }
369 
370 void CallFrameShuffler::prepareForTailCall()
371 {
372     ASSERT(isUndecided());
373     emitDeltaCheck();
374 
375     // We&#39;ll use sp-based indexing so that we can load the
376     // caller&#39;s frame pointer into the fpr immediately
377     m_oldFrameBase = MacroAssembler::stackPointerRegister;
378     m_oldFrameOffset = numLocals();
379     m_newFrameBase = acquireGPR();
<span class="line-modified">380 #if CPU(ARM_THUMB2) || CPU(MIPS)</span>












381     // We load the frame pointer and link register
382     // manually. We could ask the algorithm to load them for us,
383     // and it would allow us to use the link register as an extra
384     // temporary - but it&#39;d mean that the frame pointer can also
385     // be used as an extra temporary, so we keep the link register
386     // locked instead.
387 
388     // sp will point to head1 since the callee&#39;s prologue pushes
389     // the call frame and link register.
390     m_newFrameOffset = -1;
391 #elif CPU(ARM64)
392     // We load the frame pointer and link register manually. We
393     // could ask the algorithm to load the link register for us
394     // (which would allow for its use as an extra temporary), but
395     // since its not in GPRInfo, we can&#39;t do it.
396 
397     // sp will point to head2 since the callee&#39;s prologue pushes the
398     // call frame and link register
399     m_newFrameOffset = -2;
400 #elif CPU(X86_64)
401     // We load the frame pointer manually, but we ask the
402     // algorithm to move the return PC for us (it&#39;d probably
403     // require a write in the danger zone)
404     addNew(VirtualRegister { 1 },
405         ValueRecovery::displacedInJSStack(VirtualRegister(1), DataFormatJS));
406 
407     // sp will point to head1 since the callee&#39;s prologue pushes
408     // the call frame register
409     m_newFrameOffset = -1;
410 #else
411     UNREACHABLE_FOR_PLATFORM();
412 #endif
413 
414     if (verbose)
415         dataLog(&quot;  Emitting code for computing the new frame base\n&quot;);
416 
417     // We compute the new frame base by first computing the top of the
418     // old frame (taking into account an argument count higher than
419     // the number of parameters), then substracting to it the aligned
420     // new frame size (adjusted).
<span class="line-modified">421     m_jit.load32(MacroAssembler::Address(GPRInfo::callFrameRegister, CallFrameSlot::argumentCountIncludingThis * static_cast&lt;int&gt;(sizeof(Register)) + PayloadOffset), m_newFrameBase);</span>
422     MacroAssembler::Jump argumentCountOK =
423         m_jit.branch32(MacroAssembler::BelowOrEqual, m_newFrameBase,
424             MacroAssembler::TrustedImm32(m_jit.codeBlock()-&gt;numParameters()));
425     m_jit.add32(MacroAssembler::TrustedImm32(stackAlignmentRegisters() - 1 + CallFrame::headerSizeInRegisters), m_newFrameBase);
426     m_jit.and32(MacroAssembler::TrustedImm32(-stackAlignmentRegisters()), m_newFrameBase);
427     m_jit.mul32(MacroAssembler::TrustedImm32(sizeof(Register)), m_newFrameBase, m_newFrameBase);
428     MacroAssembler::Jump done = m_jit.jump();
429     argumentCountOK.link(&amp;m_jit);
430     m_jit.move(
431         MacroAssembler::TrustedImm32(m_alignedOldFrameSize * sizeof(Register)),
432         m_newFrameBase);
433     done.link(&amp;m_jit);
434 
435     m_jit.addPtr(GPRInfo::callFrameRegister, m_newFrameBase);
436     m_jit.subPtr(
437         MacroAssembler::TrustedImm32(
438             (m_alignedNewFrameSize + m_newFrameOffset) * sizeof(Register)),
439         m_newFrameBase);
440 
441     // We load the link register manually for architectures that have one
</pre>
<hr />
<pre>
447     m_jit.untagPtr(MacroAssembler::framePointerRegister, MacroAssembler::linkRegister);
448     m_jit.subPtr(MacroAssembler::TrustedImm32(sizeof(CallerFrameAndPC)), MacroAssembler::framePointerRegister);
449 #endif
450 
451 #elif CPU(MIPS)
452     m_jit.loadPtr(MacroAssembler::Address(MacroAssembler::framePointerRegister, sizeof(void*)),
453         MacroAssembler::returnAddressRegister);
454 #endif
455 
456     // We want the frame pointer to always point to a valid frame, and
457     // we are going to trash the current one. Let&#39;s make it point to
458     // our caller&#39;s frame, since that&#39;s what we want to end up with.
459     m_jit.loadPtr(MacroAssembler::Address(MacroAssembler::framePointerRegister),
460         MacroAssembler::framePointerRegister);
461 
462     if (verbose)
463         dataLog(&quot;Preparing frame for tail call:\n&quot;, *this);
464 
465     prepareAny();
466 






467     if (verbose)
468         dataLog(&quot;Ready for tail call!\n&quot;);
469 }
470 
471 bool CallFrameShuffler::tryWrites(CachedRecovery&amp; cachedRecovery)
472 {
473     ASSERT(m_newFrameBase != InvalidGPRReg);
474 
475     // If the value is already set up correctly, we don&#39;t have
476     // anything to do.
477     if (isSlowPath() &amp;&amp; cachedRecovery.recovery().isInJSStack()
478         &amp;&amp; cachedRecovery.targets().size() == 1
479         &amp;&amp; newAsOld(cachedRecovery.targets()[0]) == cachedRecovery.recovery().virtualRegister()) {
480         cachedRecovery.clearTargets();
481         if (!cachedRecovery.wantedJSValueRegs() &amp;&amp; cachedRecovery.wantedFPR() == InvalidFPRReg)
482             clearCachedRecovery(cachedRecovery.recovery());
483         return true;
484     }
485 
486     if (!canLoadAndBox(cachedRecovery))
</pre>
<hr />
<pre>
681 
682     // At this point, we must have enough registers available for
683     // handling 1). None of the loads can fail because we have been
684     // eagerly freeing up registers in all the previous phases - so
685     // the only values that are in registers at this point must have
686     // wanted registers.
687     if (verbose)
688         dataLog(&quot;  Danger zone is clear, performing remaining writes.\n&quot;);
689     for (VirtualRegister reg = firstNew(); reg &lt;= lastNew(); reg += 1) {
690         CachedRecovery* cachedRecovery { getNew(reg) };
691         if (!cachedRecovery)
692             continue;
693 
694         emitLoad(*cachedRecovery);
695         emitBox(*cachedRecovery);
696         bool writesOK = tryWrites(*cachedRecovery);
697         ASSERT_UNUSED(writesOK, writesOK);
698     }
699 
700 #if USE(JSVALUE64)
<span class="line-modified">701     if (m_numberTagRegister != InvalidGPRReg &amp;&amp; m_newRegisters[m_numberTagRegister])</span>
<span class="line-modified">702         releaseGPR(m_numberTagRegister);</span>
703 #endif
704 
705     // Handle 2) by loading all registers. We don&#39;t have to do any
706     // writes, since they have been taken care of above.
707     if (verbose)
708         dataLog(&quot;  Loading wanted registers into registers\n&quot;);
709     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
710         CachedRecovery* cachedRecovery { m_newRegisters[reg] };
711         if (!cachedRecovery)
712             continue;
713 
714         emitLoad(*cachedRecovery);
715         emitBox(*cachedRecovery);
716         ASSERT(cachedRecovery-&gt;targets().isEmpty());
717     }
718 
719 #if USE(JSVALUE64)
<span class="line-modified">720     if (m_numberTagRegister != InvalidGPRReg)</span>
<span class="line-modified">721         releaseGPR(m_numberTagRegister);</span>
722 #endif
723 
724     // At this point, we have read everything we cared about from the
725     // stack, and written everything we had to to the stack.
726     if (verbose)
727         dataLog(&quot;  Callee frame is fully set up\n&quot;);
<span class="line-modified">728     if (ASSERT_ENABLED) {</span>
729         for (VirtualRegister reg = firstNew(); reg &lt;= lastNew(); reg += 1)
730             ASSERT_UNUSED(reg, !getNew(reg));
731 
732         for (CachedRecovery* cachedRecovery : m_cachedRecoveries) {
733             ASSERT_UNUSED(cachedRecovery, cachedRecovery-&gt;targets().isEmpty());
734             ASSERT(!cachedRecovery-&gt;recovery().isInJSStack());
735         }
736     }
737 
738     // We need to handle 4) first because it implies releasing
739     // m_newFrameBase, which could be a wanted register.
740     if (verbose)
<span class="line-modified">741         dataLog(&quot;   * Storing the argument count into &quot;, VirtualRegister { CallFrameSlot::argumentCountIncludingThis }, &quot;\n&quot;);</span>
742     m_jit.store32(MacroAssembler::TrustedImm32(0),
<span class="line-modified">743         addressForNew(VirtualRegister { CallFrameSlot::argumentCountIncludingThis }).withOffset(TagOffset));</span>
744     RELEASE_ASSERT(m_numPassedArgs != UINT_MAX);
745     m_jit.store32(MacroAssembler::TrustedImm32(m_numPassedArgs),
<span class="line-modified">746         addressForNew(VirtualRegister { CallFrameSlot::argumentCountIncludingThis }).withOffset(PayloadOffset));</span>
747 
748     if (!isSlowPath()) {
749         ASSERT(m_newFrameBase != MacroAssembler::stackPointerRegister);
750         if (verbose)
751             dataLog(&quot;  Releasing the new frame base pointer\n&quot;);
752         m_jit.move(m_newFrameBase, MacroAssembler::stackPointerRegister);
753         releaseGPR(m_newFrameBase);
754     }
755 
756     // Finally we handle 3)
757     if (verbose)
758         dataLog(&quot;  Ensuring wanted registers are in the right register\n&quot;);
759     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
760         CachedRecovery* cachedRecovery { m_newRegisters[reg] };
761         if (!cachedRecovery)
762             continue;
763 
764         emitDisplace(*cachedRecovery);
765     }
766 }
</pre>
</td>
</tr>
</table>
<center><a href="CallFrameShuffleData.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CallFrameShuffler.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>