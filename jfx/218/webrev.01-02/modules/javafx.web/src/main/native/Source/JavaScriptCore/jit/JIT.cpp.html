<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 
  28 #if ENABLE(JIT)
  29 
  30 #include &quot;JIT.h&quot;
  31 
  32 #include &quot;BytecodeGraph.h&quot;
  33 #include &quot;BytecodeLivenessAnalysis.h&quot;
  34 #include &quot;CodeBlock.h&quot;
  35 #include &quot;CodeBlockWithJITType.h&quot;
  36 #include &quot;DFGCapabilities.h&quot;
  37 #include &quot;InterpreterInlines.h&quot;
  38 #include &quot;JITInlines.h&quot;
  39 #include &quot;JITOperations.h&quot;
  40 #include &quot;JSArray.h&quot;
  41 #include &quot;JSCInlines.h&quot;
  42 #include &quot;JSFunction.h&quot;
  43 #include &quot;LinkBuffer.h&quot;
  44 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  45 #include &quot;ModuleProgramCodeBlock.h&quot;
  46 #include &quot;PCToCodeOriginMap.h&quot;
  47 #include &quot;ProbeContext.h&quot;
  48 #include &quot;ProfilerDatabase.h&quot;
  49 #include &quot;ProgramCodeBlock.h&quot;
  50 #include &quot;ResultType.h&quot;
  51 #include &quot;SlowPathCall.h&quot;
  52 #include &quot;StackAlignment.h&quot;
  53 #include &quot;ThunkGenerators.h&quot;
  54 #include &quot;TypeProfilerLog.h&quot;
  55 #include &lt;wtf/CryptographicallyRandomNumber.h&gt;
  56 #include &lt;wtf/GraphNodeWorklist.h&gt;
  57 #include &lt;wtf/SimpleStats.h&gt;
  58 
  59 namespace JSC {
  60 namespace JITInternal {
  61 static constexpr const bool verbose = false;
  62 }
  63 
  64 Seconds totalBaselineCompileTime;
  65 Seconds totalDFGCompileTime;
  66 Seconds totalFTLCompileTime;
  67 Seconds totalFTLDFGCompileTime;
  68 Seconds totalFTLB3CompileTime;
  69 
  70 void ctiPatchCallByReturnAddress(ReturnAddressPtr returnAddress, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  71 {
  72     MacroAssembler::repatchCall(
  73         CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
  74         newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  75 }
  76 
  77 JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, BytecodeIndex loopOSREntryBytecodeIndex)
  78     : JSInterfaceJIT(&amp;vm, codeBlock)
  79     , m_interpreter(vm.interpreter)
  80     , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
  81     , m_pcToCodeOriginMapBuilder(vm)
  82     , m_canBeOptimized(false)
  83     , m_shouldEmitProfiling(false)
  84     , m_loopOSREntryBytecodeIndex(loopOSREntryBytecodeIndex)
  85 {
  86 }
  87 
  88 JIT::~JIT()
  89 {
  90 }
  91 
  92 #if ENABLE(DFG_JIT)
  93 void JIT::emitEnterOptimizationCheck()
  94 {
  95     if (!canBeOptimized())
  96         return;
  97 
  98     JumpList skipOptimize;
  99 
 100     skipOptimize.append(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
 101     ASSERT(!m_bytecodeIndex.offset());
 102 
 103     copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 104 
 105     callOperation(operationOptimize, &amp;vm(), m_bytecodeIndex.asBits());
 106     skipOptimize.append(branchTestPtr(Zero, returnValueGPR));
 107     farJump(returnValueGPR, GPRInfo::callFrameRegister);
 108     skipOptimize.link(this);
 109 }
 110 #endif
 111 
 112 void JIT::emitNotifyWrite(WatchpointSet* set)
 113 {
 114     if (!set || set-&gt;state() == IsInvalidated) {
 115         addSlowCase(Jump());
 116         return;
 117     }
 118 
 119     addSlowCase(branch8(NotEqual, AbsoluteAddress(set-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 120 }
 121 
 122 void JIT::emitNotifyWrite(GPRReg pointerToSet)
 123 {
 124     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 125 }
 126 
 127 void JIT::assertStackPointerOffset()
 128 {
 129     if (!ASSERT_ENABLED)
 130         return;
 131 
 132     addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
 133     Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
 134     breakpoint();
 135     ok.link(this);
 136 }
 137 
 138 #define NEXT_OPCODE(name) \
 139     m_bytecodeIndex = BytecodeIndex(m_bytecodeIndex.offset() + currentInstruction-&gt;size()); \
 140     break;
 141 
 142 #define NEXT_OPCODE_IN_MAIN(name) \
 143     if (previousSlowCasesSize != m_slowCases.size()) \
 144         ++m_bytecodeCountHavingSlowCase; \
 145     NEXT_OPCODE(name)
 146 
 147 #define DEFINE_SLOW_OP(name) \
 148     case op_##name: { \
 149         if (m_bytecodeIndex &gt;= startBytecodeIndex) { \
 150             JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
 151             slowPathCall.call(); \
 152         } \
 153         NEXT_OPCODE_IN_MAIN(op_##name); \
 154     }
 155 
 156 #define DEFINE_OP(name) \
 157     case name: { \
 158         if (m_bytecodeIndex &gt;= startBytecodeIndex) { \
 159             emit_##name(currentInstruction); \
 160         } \
 161         NEXT_OPCODE_IN_MAIN(name); \
 162     }
 163 
 164 #define DEFINE_SLOWCASE_OP(name) \
 165     case name: { \
 166         emitSlow_##name(currentInstruction, iter); \
 167         NEXT_OPCODE(name); \
 168     }
 169 
 170 #define DEFINE_SLOWCASE_SLOW_OP(name) \
 171     case op_##name: { \
 172         emitSlowCaseCall(currentInstruction, iter, slow_path_##name); \
 173         NEXT_OPCODE(op_##name); \
 174     }
 175 
 176 void JIT::emitSlowCaseCall(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, SlowPathFunction stub)
 177 {
 178     linkAllSlowCases(iter);
 179 
 180     JITSlowPathCall slowPathCall(this, currentInstruction, stub);
 181     slowPathCall.call();
 182 }
 183 
 184 void JIT::privateCompileMainPass()
 185 {
 186     if (JITInternal::verbose)
 187         dataLog(&quot;Compiling &quot;, *m_codeBlock, &quot;\n&quot;);
 188 
 189     jitAssertTagsInPlace();
 190     jitAssertArgumentCountSane();
 191 
 192     auto&amp; instructions = m_codeBlock-&gt;instructions();
 193     unsigned instructionCount = m_codeBlock-&gt;instructions().size();
 194 
 195     m_callLinkInfoIndex = 0;
 196 
 197     VM&amp; vm = m_codeBlock-&gt;vm();
 198     BytecodeIndex startBytecodeIndex(0);
 199     if (m_loopOSREntryBytecodeIndex &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {
 200         // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
 201         // This optimization would be invalid otherwise. When the LLInt determines it wants to
 202         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
 203         // was executing at when it kicked off our compilation. We only need to compile code for
 204         // anything reachable from that bytecode offset.
 205 
 206         // We only bother building the bytecode graph if it could save time and executable
 207         // memory. We pick an arbitrary offset where we deem this is profitable.
 208         if (m_loopOSREntryBytecodeIndex.offset() &gt;= 200) {
 209             // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
 210             // Instead, we just find the minimum bytecode offset that is reachable, and
 211             // compile code from that bytecode offset onwards.
 212 
 213             BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
 214             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeIndex.offset());
 215             RELEASE_ASSERT(block);
 216 
 217             GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
 218             startBytecodeIndex = BytecodeIndex();
 219             worklist.push(block);
 220 
 221             while (BytecodeBasicBlock* block = worklist.pop()) {
 222                 startBytecodeIndex = BytecodeIndex(std::min(startBytecodeIndex.offset(), block-&gt;leaderOffset()));
 223                 for (unsigned successorIndex : block-&gt;successors())
 224                     worklist.push(&amp;graph[successorIndex]);
 225 
 226                 // Also add catch blocks for bytecodes that throw.
 227                 if (m_codeBlock-&gt;numberOfExceptionHandlers()) {
 228                     for (unsigned bytecodeOffset = block-&gt;leaderOffset(); bytecodeOffset &lt; block-&gt;leaderOffset() + block-&gt;totalLength();) {
 229                         auto instruction = instructions.at(bytecodeOffset);
 230                         if (auto* handler = m_codeBlock-&gt;handlerForBytecodeIndex(BytecodeIndex(bytecodeOffset)))
 231                             worklist.push(graph.findBasicBlockWithLeaderOffset(handler-&gt;target));
 232 
 233                         bytecodeOffset += instruction-&gt;size();
 234                     }
 235                 }
 236             }
 237         }
 238     }
 239 
 240     m_bytecodeCountHavingSlowCase = 0;
 241     for (m_bytecodeIndex = BytecodeIndex(0); m_bytecodeIndex.offset() &lt; instructionCount; ) {
 242         unsigned previousSlowCasesSize = m_slowCases.size();
 243         if (m_bytecodeIndex == startBytecodeIndex &amp;&amp; startBytecodeIndex.offset() &gt; 0) {
 244             // We&#39;ve proven all bytecode instructions up until here are unreachable.
 245             // Let&#39;s ensure that by crashing if it&#39;s ever hit.
 246             breakpoint();
 247         }
 248 
 249         if (m_disassembler)
 250             m_disassembler-&gt;setForBytecodeMainPath(m_bytecodeIndex.offset(), label());
 251         const Instruction* currentInstruction = instructions.at(m_bytecodeIndex).ptr();
 252         ASSERT(currentInstruction-&gt;size());
 253 
 254         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));
 255 
 256 #if ENABLE(OPCODE_SAMPLING)
 257         if (m_bytecodeIndex &gt; 0) // Avoid the overhead of sampling op_enter twice.
 258             sampleInstruction(currentInstruction);
 259 #endif
 260 
 261         m_labels[m_bytecodeIndex.offset()] = label();
 262 
 263         if (JITInternal::verbose)
 264             dataLogLn(&quot;Old JIT emitting code for &quot;, m_bytecodeIndex, &quot; at offset &quot;, (long)debugOffset());
 265 
 266         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
 267 
 268         if (UNLIKELY(m_compilation)) {
 269             add64(
 270                 TrustedImm32(1),
 271                 AbsoluteAddress(m_compilation-&gt;executionCounterFor(Profiler::OriginStack(Profiler::Origin(
 272                     m_compilation-&gt;bytecodes(), m_bytecodeIndex)))-&gt;address()));
 273         }
 274 
 275         if (Options::eagerlyUpdateTopCallFrame())
 276             updateTopCallFrame();
 277 
 278         unsigned bytecodeOffset = m_bytecodeIndex.offset();
 279 #if ENABLE(MASM_PROBE)
 280         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 281             CodeBlock* codeBlock = m_codeBlock;
 282             probe([=] (Probe::Context&amp; ctx) {
 283                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 284             });
 285         }
 286 #endif
 287 
 288         switch (opcodeID) {
 289         DEFINE_SLOW_OP(in_by_val)
 290         DEFINE_SLOW_OP(less)
 291         DEFINE_SLOW_OP(lesseq)
 292         DEFINE_SLOW_OP(greater)
 293         DEFINE_SLOW_OP(greatereq)
 294         DEFINE_SLOW_OP(is_function)
 295         DEFINE_SLOW_OP(is_object_or_null)
 296         DEFINE_SLOW_OP(typeof)
 297         DEFINE_SLOW_OP(strcat)
 298         DEFINE_SLOW_OP(push_with_scope)
 299         DEFINE_SLOW_OP(create_lexical_environment)
 300         DEFINE_SLOW_OP(get_by_val_with_this)
 301         DEFINE_SLOW_OP(put_by_id_with_this)
 302         DEFINE_SLOW_OP(put_by_val_with_this)
 303         DEFINE_SLOW_OP(resolve_scope_for_hoisting_func_decl_in_eval)
 304         DEFINE_SLOW_OP(define_data_property)
 305         DEFINE_SLOW_OP(define_accessor_property)
 306         DEFINE_SLOW_OP(unreachable)
 307         DEFINE_SLOW_OP(throw_static_error)
 308         DEFINE_SLOW_OP(new_array_with_spread)
 309         DEFINE_SLOW_OP(new_array_buffer)
 310         DEFINE_SLOW_OP(spread)
 311         DEFINE_SLOW_OP(get_enumerable_length)
 312         DEFINE_SLOW_OP(has_generic_property)
 313         DEFINE_SLOW_OP(get_property_enumerator)
 314         DEFINE_SLOW_OP(to_index_string)
 315         DEFINE_SLOW_OP(create_direct_arguments)
 316         DEFINE_SLOW_OP(create_scoped_arguments)
 317         DEFINE_SLOW_OP(create_cloned_arguments)
 318         DEFINE_SLOW_OP(create_arguments_butterfly)
 319         DEFINE_SLOW_OP(create_rest)
 320         DEFINE_SLOW_OP(create_promise)
 321         DEFINE_SLOW_OP(new_promise)
 322         DEFINE_SLOW_OP(create_generator)
 323         DEFINE_SLOW_OP(create_async_generator)
 324         DEFINE_SLOW_OP(new_generator)
 325         DEFINE_SLOW_OP(pow)
 326 
 327         DEFINE_OP(op_add)
 328         DEFINE_OP(op_bitnot)
 329         DEFINE_OP(op_bitand)
 330         DEFINE_OP(op_bitor)
 331         DEFINE_OP(op_bitxor)
 332         DEFINE_OP(op_call)
 333         DEFINE_OP(op_tail_call)
 334         DEFINE_OP(op_call_eval)
 335         DEFINE_OP(op_call_varargs)
 336         DEFINE_OP(op_tail_call_varargs)
 337         DEFINE_OP(op_tail_call_forward_arguments)
 338         DEFINE_OP(op_construct_varargs)
 339         DEFINE_OP(op_catch)
 340         DEFINE_OP(op_construct)
 341         DEFINE_OP(op_create_this)
 342         DEFINE_OP(op_to_this)
 343         DEFINE_OP(op_get_argument)
 344         DEFINE_OP(op_argument_count)
 345         DEFINE_OP(op_get_rest_length)
 346         DEFINE_OP(op_check_tdz)
 347         DEFINE_OP(op_identity_with_profile)
 348         DEFINE_OP(op_debug)
 349         DEFINE_OP(op_del_by_id)
 350         DEFINE_OP(op_del_by_val)
 351         DEFINE_OP(op_div)
 352         DEFINE_OP(op_end)
 353         DEFINE_OP(op_enter)
 354         DEFINE_OP(op_get_scope)
 355         DEFINE_OP(op_eq)
 356         DEFINE_OP(op_eq_null)
 357         DEFINE_OP(op_below)
 358         DEFINE_OP(op_beloweq)
 359         DEFINE_OP(op_try_get_by_id)
 360         DEFINE_OP(op_in_by_id)
 361         DEFINE_OP(op_get_by_id)
 362         DEFINE_OP(op_get_by_id_with_this)
 363         DEFINE_OP(op_get_by_id_direct)
 364         DEFINE_OP(op_get_by_val)
 365         DEFINE_OP(op_overrides_has_instance)
 366         DEFINE_OP(op_instanceof)
 367         DEFINE_OP(op_instanceof_custom)
 368         DEFINE_OP(op_is_empty)
 369         DEFINE_OP(op_is_undefined)
 370         DEFINE_OP(op_is_undefined_or_null)
 371         DEFINE_OP(op_is_boolean)
 372         DEFINE_OP(op_is_number)
 373         DEFINE_OP(op_is_object)
 374         DEFINE_OP(op_is_cell_with_type)
 375         DEFINE_OP(op_jeq_null)
 376         DEFINE_OP(op_jfalse)
 377         DEFINE_OP(op_jmp)
 378         DEFINE_OP(op_jneq_null)
 379         DEFINE_OP(op_jundefined_or_null)
 380         DEFINE_OP(op_jnundefined_or_null)
 381         DEFINE_OP(op_jneq_ptr)
 382         DEFINE_OP(op_jless)
 383         DEFINE_OP(op_jlesseq)
 384         DEFINE_OP(op_jgreater)
 385         DEFINE_OP(op_jgreatereq)
 386         DEFINE_OP(op_jnless)
 387         DEFINE_OP(op_jnlesseq)
 388         DEFINE_OP(op_jngreater)
 389         DEFINE_OP(op_jngreatereq)
 390         DEFINE_OP(op_jeq)
 391         DEFINE_OP(op_jneq)
 392         DEFINE_OP(op_jstricteq)
 393         DEFINE_OP(op_jnstricteq)
 394         DEFINE_OP(op_jbelow)
 395         DEFINE_OP(op_jbeloweq)
 396         DEFINE_OP(op_jtrue)
 397         DEFINE_OP(op_loop_hint)
 398         DEFINE_OP(op_check_traps)
 399         DEFINE_OP(op_nop)
 400         DEFINE_OP(op_super_sampler_begin)
 401         DEFINE_OP(op_super_sampler_end)
 402         DEFINE_OP(op_lshift)
 403         DEFINE_OP(op_mod)
 404         DEFINE_OP(op_mov)
 405         DEFINE_OP(op_mul)
 406         DEFINE_OP(op_negate)
 407         DEFINE_OP(op_neq)
 408         DEFINE_OP(op_neq_null)
 409         DEFINE_OP(op_new_array)
 410         DEFINE_OP(op_new_array_with_size)
 411         DEFINE_OP(op_new_func)
 412         DEFINE_OP(op_new_func_exp)
 413         DEFINE_OP(op_new_generator_func)
 414         DEFINE_OP(op_new_generator_func_exp)
 415         DEFINE_OP(op_new_async_func)
 416         DEFINE_OP(op_new_async_func_exp)
 417         DEFINE_OP(op_new_async_generator_func)
 418         DEFINE_OP(op_new_async_generator_func_exp)
 419         DEFINE_OP(op_new_object)
 420         DEFINE_OP(op_new_regexp)
 421         DEFINE_OP(op_not)
 422         DEFINE_OP(op_nstricteq)
 423         DEFINE_OP(op_dec)
 424         DEFINE_OP(op_inc)
 425         DEFINE_OP(op_profile_type)
 426         DEFINE_OP(op_profile_control_flow)
 427         DEFINE_OP(op_get_parent_scope)
 428         DEFINE_OP(op_put_by_id)
 429         DEFINE_OP(op_put_by_val_direct)
 430         DEFINE_OP(op_put_by_val)
 431         DEFINE_OP(op_put_getter_by_id)
 432         DEFINE_OP(op_put_setter_by_id)
 433         DEFINE_OP(op_put_getter_setter_by_id)
 434         DEFINE_OP(op_put_getter_by_val)
 435         DEFINE_OP(op_put_setter_by_val)
 436         DEFINE_OP(op_to_property_key)
 437 
 438         DEFINE_OP(op_get_internal_field)
 439         DEFINE_OP(op_put_internal_field)
 440 
 441         DEFINE_OP(op_ret)
 442         DEFINE_OP(op_rshift)
 443         DEFINE_OP(op_unsigned)
 444         DEFINE_OP(op_urshift)
 445         DEFINE_OP(op_set_function_name)
 446         DEFINE_OP(op_stricteq)
 447         DEFINE_OP(op_sub)
 448         DEFINE_OP(op_switch_char)
 449         DEFINE_OP(op_switch_imm)
 450         DEFINE_OP(op_switch_string)
 451         DEFINE_OP(op_throw)
 452         DEFINE_OP(op_to_number)
 453         DEFINE_OP(op_to_numeric)
 454         DEFINE_OP(op_to_string)
 455         DEFINE_OP(op_to_object)
 456         DEFINE_OP(op_to_primitive)
 457 
 458         DEFINE_OP(op_resolve_scope)
 459         DEFINE_OP(op_get_from_scope)
 460         DEFINE_OP(op_put_to_scope)
 461         DEFINE_OP(op_get_from_arguments)
 462         DEFINE_OP(op_put_to_arguments)
 463 
 464         DEFINE_OP(op_has_structure_property)
 465         DEFINE_OP(op_has_indexed_property)
 466         DEFINE_OP(op_get_direct_pname)
 467         DEFINE_OP(op_enumerator_structure_pname)
 468         DEFINE_OP(op_enumerator_generic_pname)
 469 
 470         DEFINE_OP(op_log_shadow_chicken_prologue)
 471         DEFINE_OP(op_log_shadow_chicken_tail)
 472         default:
 473             RELEASE_ASSERT_NOT_REACHED();
 474         }
 475 
 476         if (JITInternal::verbose)
 477             dataLog(&quot;At &quot;, bytecodeOffset, &quot;: &quot;, m_slowCases.size(), &quot;\n&quot;);
 478     }
 479 
 480     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 481 
 482 #ifndef NDEBUG
 483     // Reset this, in order to guard its use with ASSERTs.
 484     m_bytecodeIndex = BytecodeIndex();
 485 #endif
 486 }
 487 
 488 void JIT::privateCompileLinkPass()
 489 {
 490     unsigned jmpTableCount = m_jmpTable.size();
 491     for (unsigned i = 0; i &lt; jmpTableCount; ++i)
 492         m_jmpTable[i].from.linkTo(m_labels[m_jmpTable[i].toBytecodeOffset], this);
 493     m_jmpTable.clear();
 494 }
 495 
 496 void JIT::privateCompileSlowCases()
 497 {
 498     m_getByIdIndex = 0;
 499     m_getByValIndex = 0;
 500     m_getByIdWithThisIndex = 0;
 501     m_putByIdIndex = 0;
 502     m_inByIdIndex = 0;
 503     m_instanceOfIndex = 0;
 504     m_byValInstructionIndex = 0;
 505     m_callLinkInfoIndex = 0;
 506 
 507     RefCountedArray&lt;RareCaseProfile&gt; rareCaseProfiles;
 508     if (shouldEmitProfiling())
 509         rareCaseProfiles = RefCountedArray&lt;RareCaseProfile&gt;(m_bytecodeCountHavingSlowCase);
 510 
 511     unsigned bytecodeCountHavingSlowCase = 0;
 512     for (Vector&lt;SlowCaseEntry&gt;::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
 513         m_bytecodeIndex = iter-&gt;to;
 514 
 515         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));
 516 
 517         BytecodeIndex firstTo = m_bytecodeIndex;
 518 
 519         const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(m_bytecodeIndex).ptr();
 520 
 521         RareCaseProfile* rareCaseProfile = nullptr;
 522         if (shouldEmitProfiling())
 523             rareCaseProfile = &amp;rareCaseProfiles.at(bytecodeCountHavingSlowCase);
 524 
 525         if (JITInternal::verbose)
 526             dataLogLn(&quot;Old JIT emitting slow code for &quot;, m_bytecodeIndex, &quot; at offset &quot;, (long)debugOffset());
 527 
 528         if (m_disassembler)
 529             m_disassembler-&gt;setForBytecodeSlowPath(m_bytecodeIndex.offset(), label());
 530 
 531 #if ENABLE(MASM_PROBE)
 532         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 533             OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
 534             unsigned bytecodeOffset = m_bytecodeIndex.offset();
 535             CodeBlock* codeBlock = m_codeBlock;
 536             probe([=] (Probe::Context&amp; ctx) {
 537                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] SLOW &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 538             });
 539         }
 540 #endif
 541 
 542         switch (currentInstruction-&gt;opcodeID()) {
 543         DEFINE_SLOWCASE_OP(op_add)
 544         DEFINE_SLOWCASE_OP(op_call)
 545         DEFINE_SLOWCASE_OP(op_tail_call)
 546         DEFINE_SLOWCASE_OP(op_call_eval)
 547         DEFINE_SLOWCASE_OP(op_call_varargs)
 548         DEFINE_SLOWCASE_OP(op_tail_call_varargs)
 549         DEFINE_SLOWCASE_OP(op_tail_call_forward_arguments)
 550         DEFINE_SLOWCASE_OP(op_construct_varargs)
 551         DEFINE_SLOWCASE_OP(op_construct)
 552         DEFINE_SLOWCASE_OP(op_eq)
 553         DEFINE_SLOWCASE_OP(op_try_get_by_id)
 554         DEFINE_SLOWCASE_OP(op_in_by_id)
 555         DEFINE_SLOWCASE_OP(op_get_by_id)
 556         DEFINE_SLOWCASE_OP(op_get_by_id_with_this)
 557         DEFINE_SLOWCASE_OP(op_get_by_id_direct)
 558         DEFINE_SLOWCASE_OP(op_get_by_val)
 559         DEFINE_SLOWCASE_OP(op_instanceof)
 560         DEFINE_SLOWCASE_OP(op_instanceof_custom)
 561         DEFINE_SLOWCASE_OP(op_jless)
 562         DEFINE_SLOWCASE_OP(op_jlesseq)
 563         DEFINE_SLOWCASE_OP(op_jgreater)
 564         DEFINE_SLOWCASE_OP(op_jgreatereq)
 565         DEFINE_SLOWCASE_OP(op_jnless)
 566         DEFINE_SLOWCASE_OP(op_jnlesseq)
 567         DEFINE_SLOWCASE_OP(op_jngreater)
 568         DEFINE_SLOWCASE_OP(op_jngreatereq)
 569         DEFINE_SLOWCASE_OP(op_jeq)
 570         DEFINE_SLOWCASE_OP(op_jneq)
 571         DEFINE_SLOWCASE_OP(op_jstricteq)
 572         DEFINE_SLOWCASE_OP(op_jnstricteq)
 573         DEFINE_SLOWCASE_OP(op_loop_hint)
 574         DEFINE_SLOWCASE_OP(op_check_traps)
 575         DEFINE_SLOWCASE_OP(op_mod)
 576         DEFINE_SLOWCASE_OP(op_mul)
 577         DEFINE_SLOWCASE_OP(op_negate)
 578         DEFINE_SLOWCASE_OP(op_neq)
 579         DEFINE_SLOWCASE_OP(op_new_object)
 580         DEFINE_SLOWCASE_OP(op_put_by_id)
 581         case op_put_by_val_direct:
 582         DEFINE_SLOWCASE_OP(op_put_by_val)
 583         DEFINE_SLOWCASE_OP(op_sub)
 584         DEFINE_SLOWCASE_OP(op_has_indexed_property)
 585         DEFINE_SLOWCASE_OP(op_get_from_scope)
 586         DEFINE_SLOWCASE_OP(op_put_to_scope)
 587 
 588         DEFINE_SLOWCASE_SLOW_OP(unsigned)
 589         DEFINE_SLOWCASE_SLOW_OP(inc)
 590         DEFINE_SLOWCASE_SLOW_OP(dec)
 591         DEFINE_SLOWCASE_SLOW_OP(bitnot)
 592         DEFINE_SLOWCASE_SLOW_OP(bitand)
 593         DEFINE_SLOWCASE_SLOW_OP(bitor)
 594         DEFINE_SLOWCASE_SLOW_OP(bitxor)
 595         DEFINE_SLOWCASE_SLOW_OP(lshift)
 596         DEFINE_SLOWCASE_SLOW_OP(rshift)
 597         DEFINE_SLOWCASE_SLOW_OP(urshift)
 598         DEFINE_SLOWCASE_SLOW_OP(div)
 599         DEFINE_SLOWCASE_SLOW_OP(create_this)
 600         DEFINE_SLOWCASE_SLOW_OP(create_promise)
 601         DEFINE_SLOWCASE_SLOW_OP(create_generator)
 602         DEFINE_SLOWCASE_SLOW_OP(create_async_generator)
 603         DEFINE_SLOWCASE_SLOW_OP(to_this)
 604         DEFINE_SLOWCASE_SLOW_OP(to_primitive)
 605         DEFINE_SLOWCASE_SLOW_OP(to_number)
 606         DEFINE_SLOWCASE_SLOW_OP(to_numeric)
 607         DEFINE_SLOWCASE_SLOW_OP(to_string)
 608         DEFINE_SLOWCASE_SLOW_OP(to_object)
 609         DEFINE_SLOWCASE_SLOW_OP(not)
 610         DEFINE_SLOWCASE_SLOW_OP(stricteq)
 611         DEFINE_SLOWCASE_SLOW_OP(nstricteq)
 612         DEFINE_SLOWCASE_SLOW_OP(get_direct_pname)
 613         DEFINE_SLOWCASE_SLOW_OP(has_structure_property)
 614         DEFINE_SLOWCASE_SLOW_OP(resolve_scope)
 615         DEFINE_SLOWCASE_SLOW_OP(check_tdz)
 616         DEFINE_SLOWCASE_SLOW_OP(to_property_key)
 617 
 618         default:
 619             RELEASE_ASSERT_NOT_REACHED();
 620         }
 621 
 622         if (JITInternal::verbose)
 623             dataLog(&quot;At &quot;, firstTo, &quot; slow: &quot;, iter - m_slowCases.begin(), &quot;\n&quot;);
 624 
 625         RELEASE_ASSERT_WITH_MESSAGE(iter == m_slowCases.end() || firstTo != iter-&gt;to, &quot;Not enough jumps linked in slow case codegen.&quot;);
 626         RELEASE_ASSERT_WITH_MESSAGE(firstTo == (iter - 1)-&gt;to, &quot;Too many jumps linked in slow case codegen.&quot;);
 627 
 628         if (shouldEmitProfiling())
 629             add32(TrustedImm32(1), AbsoluteAddress(&amp;rareCaseProfile-&gt;m_counter));
 630 
 631         emitJumpSlowToHot(jump(), 0);
 632         ++bytecodeCountHavingSlowCase;
 633     }
 634 
 635     RELEASE_ASSERT(bytecodeCountHavingSlowCase == m_bytecodeCountHavingSlowCase);
 636     RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
 637     RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
 638     RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
 639     RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
 640     RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
 641     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 642 
 643     if (shouldEmitProfiling())
 644         m_codeBlock-&gt;setRareCaseProfiles(WTFMove(rareCaseProfiles));
 645 
 646 #ifndef NDEBUG
 647     // Reset this, in order to guard its use with ASSERTs.
 648     m_bytecodeIndex = BytecodeIndex();
 649 #endif
 650 }
 651 
 652 void JIT::compileWithoutLinking(JITCompilationEffort effort)
 653 {
 654     MonotonicTime before { };
 655     if (UNLIKELY(computeCompileTimes()))
 656         before = MonotonicTime::now();
 657 
 658     DFG::CapabilityLevel level = m_codeBlock-&gt;capabilityLevel();
 659     switch (level) {
 660     case DFG::CannotCompile:
 661         m_canBeOptimized = false;
 662         m_canBeOptimizedOrInlined = false;
 663         m_shouldEmitProfiling = false;
 664         break;
 665     case DFG::CanCompile:
 666     case DFG::CanCompileAndInline:
 667         m_canBeOptimized = true;
 668         m_canBeOptimizedOrInlined = true;
 669         m_shouldEmitProfiling = true;
 670         break;
 671     default:
 672         RELEASE_ASSERT_NOT_REACHED();
 673         break;
 674     }
 675 
 676     switch (m_codeBlock-&gt;codeType()) {
 677     case GlobalCode:
 678     case ModuleCode:
 679     case EvalCode:
 680         m_codeBlock-&gt;m_shouldAlwaysBeInlined = false;
 681         break;
 682     case FunctionCode:
 683         // We could have already set it to false because we detected an uninlineable call.
 684         // Don&#39;t override that observation.
 685         m_codeBlock-&gt;m_shouldAlwaysBeInlined &amp;= canInline(level) &amp;&amp; DFG::mightInlineFunction(m_codeBlock);
 686         break;
 687     }
 688 
 689     if (UNLIKELY(Options::dumpDisassembly() || (m_vm-&gt;m_perBytecodeProfiler &amp;&amp; Options::disassembleBaselineForProfiler())))
 690         m_disassembler = makeUnique&lt;JITDisassembler&gt;(m_codeBlock);
 691     if (UNLIKELY(m_vm-&gt;m_perBytecodeProfiler)) {
 692         m_compilation = adoptRef(
 693             new Profiler::Compilation(
 694                 m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
 695                 Profiler::Baseline));
 696         m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
 697     }
 698 
 699     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(BytecodeIndex(0)));
 700 
 701     Label entryLabel(this);
 702     if (m_disassembler)
 703         m_disassembler-&gt;setStartOfCode(entryLabel);
 704 
 705     // Just add a little bit of randomness to the codegen
 706     if (random() &amp; 1)
 707         nop();
 708 
 709     emitFunctionPrologue();
 710     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 711 
 712     Label beginLabel(this);
 713 
 714     sampleCodeBlock(m_codeBlock);
 715 #if ENABLE(OPCODE_SAMPLING)
 716     sampleInstruction(m_codeBlock-&gt;instructions().begin());
 717 #endif
 718 
 719     int frameTopOffset = stackPointerOffsetFor(m_codeBlock) * sizeof(Register);
 720     unsigned maxFrameSize = -frameTopOffset;
 721     addPtr(TrustedImm32(frameTopOffset), callFrameRegister, regT1);
 722     JumpList stackOverflow;
 723     if (UNLIKELY(maxFrameSize &gt; Options::reservedZoneSize()))
 724         stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
 725     stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm-&gt;addressOfSoftStackLimit()), regT1));
 726 
 727     move(regT1, stackPointerRegister);
 728     checkStackPointerAlignment();
 729 
 730     emitSaveCalleeSaves();
 731     emitMaterializeTagCheckRegisters();
 732 
 733     if (m_codeBlock-&gt;codeType() == FunctionCode) {
 734         ASSERT(!m_bytecodeIndex);
 735         if (shouldEmitProfiling()) {
 736             for (int argument = 0; argument &lt; m_codeBlock-&gt;numParameters(); ++argument) {
 737                 // If this is a constructor, then we want to put in a dummy profiling site (to
 738                 // keep things consistent) but we don&#39;t actually want to record the dummy value.
 739                 if (m_codeBlock-&gt;isConstructor() &amp;&amp; !argument)
 740                     continue;
 741                 int offset = CallFrame::argumentOffsetIncludingThis(argument) * static_cast&lt;int&gt;(sizeof(Register));
 742 #if USE(JSVALUE64)
 743                 load64(Address(callFrameRegister, offset), regT0);
 744 #elif USE(JSVALUE32_64)
 745                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.payload)), regT0);
 746                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.tag)), regT1);
 747 #endif
 748                 emitValueProfilingSite(m_codeBlock-&gt;valueProfileForArgument(argument));
 749             }
 750         }
 751     }
 752 
 753     RELEASE_ASSERT(!JITCode::isJIT(m_codeBlock-&gt;jitType()));
 754 
 755     privateCompileMainPass();
 756     privateCompileLinkPass();
 757     privateCompileSlowCases();
 758 
 759     if (m_disassembler)
 760         m_disassembler-&gt;setEndOfSlowPath(label());
 761     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 762 
 763     stackOverflow.link(this);
 764     m_bytecodeIndex = BytecodeIndex(0);
 765     if (maxFrameExtentForSlowPathCall)
 766         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
 767     callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
 768 
 769     // If the number of parameters is 1, we never require arity fixup.
 770     bool requiresArityFixup = m_codeBlock-&gt;m_numParameters != 1;
 771     if (m_codeBlock-&gt;codeType() == FunctionCode &amp;&amp; requiresArityFixup) {
 772         m_arityCheck = label();
 773         store8(TrustedImm32(0), &amp;m_codeBlock-&gt;m_shouldAlwaysBeInlined);
 774         emitFunctionPrologue();
 775         emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 776 
 777         load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT1);
 778         branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock-&gt;m_numParameters)).linkTo(beginLabel, this);
 779 
 780         m_bytecodeIndex = BytecodeIndex(0);
 781 
 782         if (maxFrameExtentForSlowPathCall)
 783             addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
 784         callOperationWithCallFrameRollbackOnException(m_codeBlock-&gt;isConstructor() ? operationConstructArityCheck : operationCallArityCheck, m_codeBlock-&gt;globalObject());
 785         if (maxFrameExtentForSlowPathCall)
 786             addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
 787         branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
 788         move(returnValueGPR, GPRInfo::argumentGPR0);
 789         emitNakedCall(m_vm-&gt;getCTIStub(arityFixupGenerator).retaggedCode&lt;NoPtrTag&gt;());
 790 
 791 #if ASSERT_ENABLED
 792         m_bytecodeIndex = BytecodeIndex(); // Reset this, in order to guard its use with ASSERTs.
 793 #endif
 794 
 795         jump(beginLabel);
 796     } else
 797         m_arityCheck = entryLabel; // Never require arity fixup.
 798 
 799     ASSERT(m_jmpTable.isEmpty());
 800 
 801     privateCompileExceptionHandlers();
 802 
 803     if (m_disassembler)
 804         m_disassembler-&gt;setEndOfCode(label());
 805     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 806 
 807     m_linkBuffer = std::unique_ptr&lt;LinkBuffer&gt;(new LinkBuffer(*this, m_codeBlock, effort));
 808 
 809     MonotonicTime after { };
 810     if (UNLIKELY(computeCompileTimes())) {
 811         after = MonotonicTime::now();
 812 
 813         if (Options::reportTotalCompileTimes())
 814             totalBaselineCompileTime += after - before;
 815     }
 816     if (UNLIKELY(reportCompileTimes())) {
 817         CString codeBlockName = toCString(*m_codeBlock);
 818 
 819         dataLog(&quot;Optimized &quot;, codeBlockName, &quot; with Baseline JIT into &quot;, m_linkBuffer-&gt;size(), &quot; bytes in &quot;, (after - before).milliseconds(), &quot; ms.\n&quot;);
 820     }
 821 }
 822 
 823 CompilationResult JIT::link()
 824 {
 825     LinkBuffer&amp; patchBuffer = *m_linkBuffer;
 826 
 827     if (patchBuffer.didFailToAllocate())
 828         return CompilationFailed;
 829 
 830     // Translate vPC offsets into addresses in JIT generated code, for switch tables.
 831     for (auto&amp; record : m_switches) {
 832         unsigned bytecodeOffset = record.bytecodeIndex.offset();
 833 
 834         if (record.type != SwitchRecord::String) {
 835             ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
 836             ASSERT(record.jumpTable.simpleJumpTable-&gt;branchOffsets.size() == record.jumpTable.simpleJumpTable-&gt;ctiOffsets.size());
 837 
 838             auto* simpleJumpTable = record.jumpTable.simpleJumpTable;
 839             simpleJumpTable-&gt;ctiDefault = patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
 840 
 841             for (unsigned j = 0; j &lt; record.jumpTable.simpleJumpTable-&gt;branchOffsets.size(); ++j) {
 842                 unsigned offset = record.jumpTable.simpleJumpTable-&gt;branchOffsets[j];
 843                 simpleJumpTable-&gt;ctiOffsets[j] = offset
 844                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 845                     : simpleJumpTable-&gt;ctiDefault;
 846             }
 847         } else {
 848             ASSERT(record.type == SwitchRecord::String);
 849 
 850             auto* stringJumpTable = record.jumpTable.stringJumpTable;
 851             stringJumpTable-&gt;ctiDefault =
 852                 patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
 853 
 854             for (auto&amp; location : stringJumpTable-&gt;offsetTable.values()) {
 855                 unsigned offset = location.branchOffset;
 856                 location.ctiOffset = offset
 857                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 858                     : stringJumpTable-&gt;ctiDefault;
 859             }
 860         }
 861     }
 862 
 863     for (size_t i = 0; i &lt; m_codeBlock-&gt;numberOfExceptionHandlers(); ++i) {
 864         HandlerInfo&amp; handler = m_codeBlock-&gt;exceptionHandler(i);
 865         // FIXME: &lt;rdar://problem/39433318&gt;.
 866         handler.nativeCode = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_labels[handler.target]);
 867     }
 868 
 869     for (auto&amp; record : m_calls) {
 870         if (record.callee)
 871             patchBuffer.link(record.from, record.callee);
 872     }
 873 
 874     finalizeInlineCaches(m_getByIds, patchBuffer);
 875     finalizeInlineCaches(m_getByVals, patchBuffer);
 876     finalizeInlineCaches(m_getByIdsWithThis, patchBuffer);
 877     finalizeInlineCaches(m_putByIds, patchBuffer);
 878     finalizeInlineCaches(m_inByIds, patchBuffer);
 879     finalizeInlineCaches(m_instanceOfs, patchBuffer);
 880 
 881     if (m_byValCompilationInfo.size()) {
 882         CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt; exceptionHandler = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_exceptionHandler);
 883 
 884         for (const auto&amp; byValCompilationInfo : m_byValCompilationInfo) {
 885             PatchableJump patchableNotIndexJump = byValCompilationInfo.notIndexJump;
 886             auto notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;();
 887             if (Jump(patchableNotIndexJump).isSet())
 888                 notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(patchableNotIndexJump));
 889             auto badTypeJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.badTypeJump));
 890             auto doneTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.doneTarget));
 891             auto nextHotPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.nextHotPathTarget));
 892             auto slowPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.slowPathTarget));
 893 
 894             *byValCompilationInfo.byValInfo = ByValInfo(
 895                 byValCompilationInfo.bytecodeIndex,
 896                 notIndexJump,
 897                 badTypeJump,
 898                 exceptionHandler,
 899                 byValCompilationInfo.arrayMode,
 900                 byValCompilationInfo.arrayProfile,
 901                 doneTarget,
 902                 nextHotPathTarget,
 903                 slowPathTarget);
 904         }
 905     }
 906 
 907     for (auto&amp; compilationInfo : m_callCompilationInfo) {
 908         CallLinkInfo&amp; info = *compilationInfo.callLinkInfo;
 909         info.setCallLocations(
 910             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.callReturnLocation)),
 911             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathBegin)),
 912             patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathOther));
 913     }
 914 
 915     {
 916         JITCodeMapBuilder jitCodeMapBuilder;
 917         for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {
 918             if (m_labels[bytecodeOffset].isSet())
 919                 jitCodeMapBuilder.append(BytecodeIndex(bytecodeOffset), patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));
 920         }
 921         m_codeBlock-&gt;setJITCodeMap(jitCodeMapBuilder.finalize());
 922     }
 923 
 924     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
 925 
 926     if (UNLIKELY(Options::dumpDisassembly())) {
 927         m_disassembler-&gt;dump(patchBuffer);
 928         patchBuffer.didAlreadyDisassemble();
 929     }
 930     if (UNLIKELY(m_compilation)) {
 931         if (Options::disassembleBaselineForProfiler())
 932             m_disassembler-&gt;reportToProfiler(m_compilation.get(), patchBuffer);
 933         m_vm-&gt;m_perBytecodeProfiler-&gt;addCompilation(m_codeBlock, *m_compilation);
 934     }
 935 
 936     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
 937         m_codeBlock-&gt;setPCToCodeOriginMap(makeUnique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), patchBuffer));
 938 
 939     CodeRef&lt;JSEntryPtrTag&gt; result = FINALIZE_CODE(
 940         patchBuffer, JSEntryPtrTag,
 941         &quot;Baseline JIT code for %s&quot;, toCString(CodeBlockWithJITType(m_codeBlock, JITType::BaselineJIT)).data());
 942 
 943     m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
 944         static_cast&lt;double&gt;(result.size()) /
 945         static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));
 946 
 947     {
 948         ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
 949         m_codeBlock-&gt;shrinkToFit(locker, CodeBlock::ShrinkMode::LateShrink);
 950     }
 951     m_codeBlock-&gt;setJITCode(
 952         adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));
 953 
 954     if (JITInternal::verbose)
 955         dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
 956 
 957     return CompilationSuccessful;
 958 }
 959 
 960 CompilationResult JIT::privateCompile(JITCompilationEffort effort)
 961 {
 962     doMainThreadPreparationBeforeCompile();
 963     compileWithoutLinking(effort);
 964     return link();
 965 }
 966 
 967 void JIT::privateCompileExceptionHandlers()
 968 {
 969     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
 970         m_exceptionChecksWithCallFrameRollback.link(this);
 971 
 972         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 973 
 974         // operationLookupExceptionHandlerFromCallerFrame is passed one argument, the VM*.
 975         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
 976         prepareCallOperation(vm());
 977         m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandlerFromCallerFrame)));
 978         jumpToExceptionHandler(vm());
 979     }
 980 
 981     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
 982         m_exceptionHandler = label();
 983         m_exceptionChecks.link(this);
 984 
 985         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 986 
 987         // operationLookupExceptionHandler is passed one argument, the VM*.
 988         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
 989         prepareCallOperation(vm());
 990         m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandler)));
 991         jumpToExceptionHandler(vm());
 992     }
 993 }
 994 
 995 void JIT::doMainThreadPreparationBeforeCompile()
 996 {
 997     // This ensures that we have the most up to date type information when performing typecheck optimizations for op_profile_type.
 998     if (m_vm-&gt;typeProfiler())
 999         m_vm-&gt;typeProfilerLog()-&gt;processLogEntries(*m_vm, &quot;Preparing for JIT compilation.&quot;_s);
1000 }
1001 
1002 unsigned JIT::frameRegisterCountFor(CodeBlock* codeBlock)
1003 {
1004     ASSERT(static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals()) == WTF::roundUpToMultipleOf(stackAlignmentRegisters(), static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals())));
1005 
1006     return roundLocalRegisterCountForFramePointerOffset(codeBlock-&gt;numCalleeLocals() + maxFrameExtentForSlowPathCallInRegisters);
1007 }
1008 
1009 int JIT::stackPointerOffsetFor(CodeBlock* codeBlock)
1010 {
1011     return virtualRegisterForLocal(frameRegisterCountFor(codeBlock) - 1).offset();
1012 }
1013 
1014 bool JIT::reportCompileTimes()
1015 {
1016     return Options::reportCompileTimes() || Options::reportBaselineCompileTimes();
1017 }
1018 
1019 bool JIT::computeCompileTimes()
1020 {
1021     return reportCompileTimes() || Options::reportTotalCompileTimes();
1022 }
1023 
1024 HashMap&lt;CString, Seconds&gt; JIT::compileTimeStats()
1025 {
1026     HashMap&lt;CString, Seconds&gt; result;
1027     if (Options::reportTotalCompileTimes()) {
1028         result.add(&quot;Total Compile Time&quot;, totalBaselineCompileTime + totalDFGCompileTime + totalFTLCompileTime);
1029         result.add(&quot;Baseline Compile Time&quot;, totalBaselineCompileTime);
1030 #if ENABLE(DFG_JIT)
1031         result.add(&quot;DFG Compile Time&quot;, totalDFGCompileTime);
1032 #if ENABLE(FTL_JIT)
1033         result.add(&quot;FTL Compile Time&quot;, totalFTLCompileTime);
1034         result.add(&quot;FTL (DFG) Compile Time&quot;, totalFTLDFGCompileTime);
1035         result.add(&quot;FTL (B3) Compile Time&quot;, totalFTLB3CompileTime);
1036 #endif // ENABLE(FTL_JIT)
1037 #endif // ENABLE(DFG_JIT)
1038     }
1039     return result;
1040 }
1041 
1042 Seconds JIT::totalCompileTime()
1043 {
1044     return totalBaselineCompileTime + totalDFGCompileTime + totalFTLCompileTime;
1045 }
1046 
1047 } // namespace JSC
1048 
1049 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>