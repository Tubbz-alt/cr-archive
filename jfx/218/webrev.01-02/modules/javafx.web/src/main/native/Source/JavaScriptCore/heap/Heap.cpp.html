<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.
   3  *  Copyright (C) 2007 Eric Seidel &lt;eric@webkit.org&gt;
   4  *
   5  *  This library is free software; you can redistribute it and/or
   6  *  modify it under the terms of the GNU Lesser General Public
   7  *  License as published by the Free Software Foundation; either
   8  *  version 2 of the License, or (at your option) any later version.
   9  *
  10  *  This library is distributed in the hope that it will be useful,
  11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
  12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  13  *  Lesser General Public License for more details.
  14  *
  15  *  You should have received a copy of the GNU Lesser General Public
  16  *  License along with this library; if not, write to the Free Software
  17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
  18  *
  19  */
  20 
  21 #include &quot;config.h&quot;
  22 #include &quot;Heap.h&quot;
  23 
  24 #include &quot;BlockDirectoryInlines.h&quot;
  25 #include &quot;BuiltinExecutables.h&quot;
  26 #include &quot;CodeBlock.h&quot;
  27 #include &quot;CodeBlockSetInlines.h&quot;
  28 #include &quot;CollectingScope.h&quot;
  29 #include &quot;ConservativeRoots.h&quot;
  30 #include &quot;DFGWorklistInlines.h&quot;
  31 #include &quot;EdenGCActivityCallback.h&quot;
  32 #include &quot;Exception.h&quot;
  33 #include &quot;FullGCActivityCallback.h&quot;
  34 #include &quot;FunctionExecutableInlines.h&quot;
  35 #include &quot;GCActivityCallback.h&quot;
  36 #include &quot;GCIncomingRefCountedSetInlines.h&quot;
  37 #include &quot;GCSegmentedArrayInlines.h&quot;
  38 #include &quot;GCTypeMap.h&quot;
  39 #include &quot;HasOwnPropertyCache.h&quot;
  40 #include &quot;HeapHelperPool.h&quot;
  41 #include &quot;HeapIterationScope.h&quot;
  42 #include &quot;HeapProfiler.h&quot;
  43 #include &quot;HeapSnapshot.h&quot;
  44 #include &quot;HeapVerifier.h&quot;
  45 #include &quot;IncrementalSweeper.h&quot;
  46 #include &quot;InferredValueInlines.h&quot;
  47 #include &quot;Interpreter.h&quot;
  48 #include &quot;IsoCellSetInlines.h&quot;
  49 #include &quot;JITStubRoutineSet.h&quot;
  50 #include &quot;JITWorklist.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSGlobalObject.h&quot;
  53 #include &quot;JSLock.h&quot;
  54 #include &quot;JSVirtualMachineInternal.h&quot;
  55 #include &quot;JSWeakMap.h&quot;
  56 #include &quot;JSWeakObjectRef.h&quot;
  57 #include &quot;JSWeakSet.h&quot;
  58 #include &quot;JSWebAssemblyCodeBlock.h&quot;
  59 #include &quot;MachineStackMarker.h&quot;
  60 #include &quot;MarkStackMergingConstraint.h&quot;
  61 #include &quot;MarkedSpaceInlines.h&quot;
  62 #include &quot;MarkingConstraintSet.h&quot;
  63 #include &quot;PreventCollectionScope.h&quot;
  64 #include &quot;SamplingProfiler.h&quot;
  65 #include &quot;ShadowChicken.h&quot;
  66 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  67 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  68 #include &quot;StopIfNecessaryTimer.h&quot;
  69 #include &quot;SubspaceInlines.h&quot;
  70 #include &quot;SuperSampler.h&quot;
  71 #include &quot;SweepingScope.h&quot;
  72 #include &quot;SymbolTableInlines.h&quot;
  73 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  74 #include &quot;TypeProfiler.h&quot;
  75 #include &quot;TypeProfilerLog.h&quot;
  76 #include &quot;UnlinkedCodeBlock.h&quot;
  77 #include &quot;VM.h&quot;
  78 #include &quot;VisitCounter.h&quot;
  79 #include &quot;WasmMemory.h&quot;
  80 #include &quot;WeakMapImplInlines.h&quot;
  81 #include &quot;WeakSetInlines.h&quot;
  82 #include &lt;algorithm&gt;
  83 #include &lt;wtf/CryptographicallyRandomNumber.h&gt;
  84 #include &lt;wtf/ListDump.h&gt;
  85 #include &lt;wtf/MainThread.h&gt;
  86 #include &lt;wtf/ParallelVectorIterator.h&gt;
  87 #include &lt;wtf/ProcessID.h&gt;
  88 #include &lt;wtf/RAMSize.h&gt;
  89 #include &lt;wtf/SimpleStats.h&gt;
  90 #include &lt;wtf/Threading.h&gt;
  91 
  92 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)
  93 #include &lt;bmalloc/bmalloc.h&gt;
  94 #endif
  95 
  96 #if USE(FOUNDATION)
  97 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  98 #endif
  99 
 100 #ifdef JSC_GLIB_API_ENABLED
 101 #include &quot;JSCGLibWrapperObject.h&quot;
 102 #endif
 103 
 104 namespace JSC {
 105 
 106 namespace {
 107 
 108 bool verboseStop = false;
 109 
 110 double maxPauseMS(double thisPauseMS)
 111 {
 112     static double maxPauseMS;
 113     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 114     return maxPauseMS;
 115 }
 116 
 117 size_t minHeapSize(HeapType heapType, size_t ramSize)
 118 {
 119     if (heapType == LargeHeap) {
 120         double result = std::min(
 121             static_cast&lt;double&gt;(Options::largeHeapSize()),
 122             ramSize * Options::smallHeapRAMFraction());
 123         return static_cast&lt;size_t&gt;(result);
 124     }
 125     return Options::smallHeapSize();
 126 }
 127 
 128 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 129 {
 130     if (VM::isInMiniMode())
 131         return Options::miniVMHeapGrowthFactor() * heapSize;
 132 
 133 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)
 134     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 135     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 136         return Options::smallHeapGrowthFactor() * heapSize;
 137     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 138         return Options::mediumHeapGrowthFactor() * heapSize;
 139 #else
 140     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 141         return Options::smallHeapGrowthFactor() * heapSize;
 142     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 143         return Options::mediumHeapGrowthFactor() * heapSize;
 144 #endif
 145     return Options::largeHeapGrowthFactor() * heapSize;
 146 }
 147 
 148 bool isValidSharedInstanceThreadState(VM&amp; vm)
 149 {
 150     return vm.currentThreadIsHoldingAPILock();
 151 }
 152 
 153 bool isValidThreadState(VM&amp; vm)
 154 {
 155     if (vm.atomStringTable() != Thread::current().atomStringTable())
 156         return false;
 157 
 158     if (vm.isSharedInstance() &amp;&amp; !isValidSharedInstanceThreadState(vm))
 159         return false;
 160 
 161     return true;
 162 }
 163 
 164 void recordType(VM&amp; vm, TypeCountSet&amp; set, JSCell* cell)
 165 {
 166     const char* typeName = &quot;[unknown]&quot;;
 167     const ClassInfo* info = cell-&gt;classInfo(vm);
 168     if (info &amp;&amp; info-&gt;className)
 169         typeName = info-&gt;className;
 170     set.add(typeName);
 171 }
 172 
 173 bool measurePhaseTiming()
 174 {
 175     return false;
 176 }
 177 
 178 HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;&amp; timingStats()
 179 {
 180     static HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;* result;
 181     static std::once_flag once;
 182     std::call_once(
 183         once,
 184         [] {
 185             result = new HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;();
 186         });
 187     return *result;
 188 }
 189 
 190 SimpleStats&amp; timingStats(const char* name, CollectionScope scope)
 191 {
 192     return timingStats().add(name, GCTypeMap&lt;SimpleStats&gt;()).iterator-&gt;value[scope];
 193 }
 194 
 195 class TimingScope {
 196 public:
 197     TimingScope(Optional&lt;CollectionScope&gt; scope, const char* name)
 198         : m_scope(scope)
 199         , m_name(name)
 200     {
 201         if (measurePhaseTiming())
 202             m_before = MonotonicTime::now();
 203     }
 204 
 205     TimingScope(Heap&amp; heap, const char* name)
 206         : TimingScope(heap.collectionScope(), name)
 207     {
 208     }
 209 
 210     void setScope(Optional&lt;CollectionScope&gt; scope)
 211     {
 212         m_scope = scope;
 213     }
 214 
 215     void setScope(Heap&amp; heap)
 216     {
 217         setScope(heap.collectionScope());
 218     }
 219 
 220     ~TimingScope()
 221     {
 222         if (measurePhaseTiming()) {
 223             MonotonicTime after = MonotonicTime::now();
 224             Seconds timing = after - m_before;
 225             SimpleStats&amp; stats = timingStats(m_name, *m_scope);
 226             stats.add(timing.milliseconds());
 227             dataLog(&quot;[GC:&quot;, *m_scope, &quot;] &quot;, m_name, &quot; took: &quot;, timing.milliseconds(), &quot;ms (average &quot;, stats.mean(), &quot;ms).\n&quot;);
 228         }
 229     }
 230 private:
 231     Optional&lt;CollectionScope&gt; m_scope;
 232     MonotonicTime m_before;
 233     const char* m_name;
 234 };
 235 
 236 } // anonymous namespace
 237 
 238 class Heap::HeapThread : public AutomaticThread {
 239 public:
 240     HeapThread(const AbstractLocker&amp; locker, Heap&amp; heap)
 241         : AutomaticThread(locker, heap.m_threadLock, heap.m_threadCondition.copyRef())
 242         , m_heap(heap)
 243     {
 244     }
 245 
 246     const char* name() const override
 247     {
 248         return &quot;JSC Heap Collector Thread&quot;;
 249     }
 250 
 251 protected:
 252     PollResult poll(const AbstractLocker&amp; locker) override
 253     {
 254         if (m_heap.m_threadShouldStop) {
 255             m_heap.notifyThreadStopping(locker);
 256             return PollResult::Stop;
 257         }
 258         if (m_heap.shouldCollectInCollectorThread(locker)) {
 259             m_heap.m_collectorThreadIsRunning = true;
 260             return PollResult::Work;
 261         }
 262         m_heap.m_collectorThreadIsRunning = false;
 263         return PollResult::Wait;
 264     }
 265 
 266     WorkResult work() override
 267     {
 268         m_heap.collectInCollectorThread();
 269         return WorkResult::Continue;
 270     }
 271 
 272     void threadDidStart() override
 273     {
 274         Thread::registerGCThread(GCThreadType::Main);
 275     }
 276 
 277     void threadIsStopping(const AbstractLocker&amp;) override
 278     {
 279         m_heap.m_collectorThreadIsRunning = false;
 280     }
 281 
 282 private:
 283     Heap&amp; m_heap;
 284 };
 285 
 286 Heap::Heap(VM&amp; vm, HeapType heapType)
 287     : m_heapType(heapType)
 288     , m_ramSize(Options::forceRAMSize() ? Options::forceRAMSize() : ramSize())
 289     , m_minBytesPerCycle(minHeapSize(m_heapType, m_ramSize))
 290     , m_maxEdenSize(m_minBytesPerCycle)
 291     , m_maxHeapSize(m_minBytesPerCycle)
 292     , m_objectSpace(this)
 293     , m_machineThreads(makeUnique&lt;MachineThreads&gt;())
 294     , m_collectorSlotVisitor(makeUnique&lt;SlotVisitor&gt;(*this, &quot;C&quot;))
 295     , m_mutatorSlotVisitor(makeUnique&lt;SlotVisitor&gt;(*this, &quot;M&quot;))
 296     , m_mutatorMarkStack(makeUnique&lt;MarkStackArray&gt;())
 297     , m_raceMarkStack(makeUnique&lt;MarkStackArray&gt;())
 298     , m_constraintSet(makeUnique&lt;MarkingConstraintSet&gt;(*this))
 299     , m_handleSet(vm)
 300     , m_codeBlocks(makeUnique&lt;CodeBlockSet&gt;())
 301     , m_jitStubRoutines(makeUnique&lt;JITStubRoutineSet&gt;())
 302     , m_vm(vm)
 303     // We seed with 10ms so that GCActivityCallback::didAllocate doesn&#39;t continuously
 304     // schedule the timer if we&#39;ve never done a collection.
 305     , m_fullActivityCallback(GCActivityCallback::tryCreateFullTimer(this))
 306     , m_edenActivityCallback(GCActivityCallback::tryCreateEdenTimer(this))
 307     , m_sweeper(adoptRef(*new IncrementalSweeper(this)))
 308     , m_stopIfNecessaryTimer(adoptRef(*new StopIfNecessaryTimer(vm)))
 309     , m_sharedCollectorMarkStack(makeUnique&lt;MarkStackArray&gt;())
 310     , m_sharedMutatorMarkStack(makeUnique&lt;MarkStackArray&gt;())
 311     , m_helperClient(&amp;heapHelperPool())
 312     , m_threadLock(Box&lt;Lock&gt;::create())
 313     , m_threadCondition(AutomaticThreadCondition::create())
 314 {
 315     m_worldState.store(0);
 316 
 317     for (unsigned i = 0, numberOfParallelThreads = heapHelperPool().numberOfThreads(); i &lt; numberOfParallelThreads; ++i) {
 318         std::unique_ptr&lt;SlotVisitor&gt; visitor = makeUnique&lt;SlotVisitor&gt;(*this, toCString(&quot;P&quot;, i + 1));
 319         if (Options::optimizeParallelSlotVisitorsForStoppedMutator())
 320             visitor-&gt;optimizeForStoppedMutator();
 321         m_availableParallelSlotVisitors.append(visitor.get());
 322         m_parallelSlotVisitors.append(WTFMove(visitor));
 323     }
 324 
 325     if (Options::useConcurrentGC()) {
 326         if (Options::useStochasticMutatorScheduler())
 327             m_scheduler = makeUnique&lt;StochasticSpaceTimeMutatorScheduler&gt;(*this);
 328         else
 329             m_scheduler = makeUnique&lt;SpaceTimeMutatorScheduler&gt;(*this);
 330     } else {
 331         // We simulate turning off concurrent GC by making the scheduler say that the world
 332         // should always be stopped when the collector is running.
 333         m_scheduler = makeUnique&lt;SynchronousStopTheWorldMutatorScheduler&gt;();
 334     }
 335 
 336     if (Options::verifyHeap())
 337         m_verifier = makeUnique&lt;HeapVerifier&gt;(this, Options::numberOfGCCyclesToRecordForVerification());
 338 
 339     m_collectorSlotVisitor-&gt;optimizeForStoppedMutator();
 340 
 341     // When memory is critical, allow allocating 25% of the amount above the critical threshold before collecting.
 342     size_t memoryAboveCriticalThreshold = static_cast&lt;size_t&gt;(static_cast&lt;double&gt;(m_ramSize) * (1.0 - Options::criticalGCMemoryThreshold()));
 343     m_maxEdenSizeWhenCritical = memoryAboveCriticalThreshold / 4;
 344 
 345     LockHolder locker(*m_threadLock);
 346     m_thread = adoptRef(new HeapThread(locker, *this));
 347 }
 348 
 349 Heap::~Heap()
 350 {
 351     // Scribble m_worldState to make it clear that the heap has already been destroyed if we crash in checkConn
 352     m_worldState.store(0xbadbeeffu);
 353 
 354     forEachSlotVisitor(
 355         [&amp;] (SlotVisitor&amp; visitor) {
 356             visitor.clearMarkStacks();
 357         });
 358     m_mutatorMarkStack-&gt;clear();
 359     m_raceMarkStack-&gt;clear();
 360 
 361     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 362         WeakBlock::destroy(*this, block);
 363 }
 364 
 365 bool Heap::isPagedOut(MonotonicTime deadline)
 366 {
 367     return m_objectSpace.isPagedOut(deadline);
 368 }
 369 
 370 void Heap::dumpHeapStatisticsAtVMDestruction()
 371 {
 372     unsigned counter = 0;
 373     m_objectSpace.forEachBlock([&amp;] (MarkedBlock::Handle* block) {
 374         unsigned live = 0;
 375         block-&gt;forEachCell([&amp;] (size_t, HeapCell* cell, HeapCell::Kind) {
 376             if (cell-&gt;isLive())
 377                 live++;
 378             return IterationStatus::Continue;
 379         });
 380         dataLogLn(&quot;[&quot;, counter++, &quot;] &quot;, block-&gt;cellSize(), &quot;, &quot;, live, &quot; / &quot;, block-&gt;cellsPerBlock(), &quot; &quot;, static_cast&lt;double&gt;(live) / block-&gt;cellsPerBlock() * 100, &quot;% &quot;, block-&gt;attributes(), &quot; &quot;, block-&gt;subspace()-&gt;name());
 381         block-&gt;forEachCell([&amp;] (size_t, HeapCell* heapCell, HeapCell::Kind kind) {
 382             if (heapCell-&gt;isLive() &amp;&amp; kind == HeapCell::Kind::JSCell) {
 383                 auto* cell = static_cast&lt;JSCell*&gt;(heapCell);
 384                 if (cell-&gt;isObject())
 385                     dataLogLn(&quot;    &quot;, JSValue((JSObject*)cell));
 386                 else
 387                     dataLogLn(&quot;    &quot;, *cell);
 388             }
 389             return IterationStatus::Continue;
 390         });
 391     });
 392 }
 393 
 394 // The VM is being destroyed and the collector will never run again.
 395 // Run all pending finalizers now because we won&#39;t get another chance.
 396 void Heap::lastChanceToFinalize()
 397 {
 398     MonotonicTime before;
 399     if (UNLIKELY(Options::logGC())) {
 400         before = MonotonicTime::now();
 401         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 402     }
 403 
 404     m_isShuttingDown = true;
 405 
 406     RELEASE_ASSERT(!m_vm.entryScope);
 407     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 408 
 409     if (m_collectContinuouslyThread) {
 410         {
 411             LockHolder locker(m_collectContinuouslyLock);
 412             m_shouldStopCollectingContinuously = true;
 413             m_collectContinuouslyCondition.notifyOne();
 414         }
 415         m_collectContinuouslyThread-&gt;waitForCompletion();
 416     }
 417 
 418     dataLogIf(Options::logGC(), &quot;1&quot;);
 419 
 420     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 421     // going to call into anything that starts collections. Still, this makes the algorithm more
 422     // obviously sound.
 423     m_isSafeToCollect = false;
 424 
 425     dataLogIf(Options::logGC(), &quot;2&quot;);
 426 
 427     bool isCollecting;
 428     {
 429         auto locker = holdLock(*m_threadLock);
 430         RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 431         isCollecting = m_lastServedTicket &lt; m_lastGrantedTicket;
 432     }
 433     if (isCollecting) {
 434         dataLogIf(Options::logGC(), &quot;...]\n&quot;);
 435 
 436         // Wait for the current collection to finish.
 437         waitForCollector(
 438             [&amp;] (const AbstractLocker&amp;) -&gt; bool {
 439                 RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 440                 return m_lastServedTicket == m_lastGrantedTicket;
 441             });
 442 
 443         dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 444     }
 445     dataLogIf(Options::logGC(), &quot;3&quot;);
 446 
 447     RELEASE_ASSERT(m_requests.isEmpty());
 448     RELEASE_ASSERT(m_lastServedTicket == m_lastGrantedTicket);
 449 
 450     // Carefully bring the thread down.
 451     bool stopped = false;
 452     {
 453         LockHolder locker(*m_threadLock);
 454         stopped = m_thread-&gt;tryStop(locker);
 455         m_threadShouldStop = true;
 456         if (!stopped)
 457             m_threadCondition-&gt;notifyOne(locker);
 458     }
 459 
 460     dataLogIf(Options::logGC(), &quot;4&quot;);
 461 
 462     if (!stopped)
 463         m_thread-&gt;join();
 464 
 465     dataLogIf(Options::logGC(), &quot;5 &quot;);
 466 
 467     if (UNLIKELY(Options::dumpHeapStatisticsAtVMDestruction()))
 468         dumpHeapStatisticsAtVMDestruction();
 469 
 470     m_arrayBuffers.lastChanceToFinalize();
 471     m_objectSpace.stopAllocatingForGood();
 472     m_objectSpace.lastChanceToFinalize();
 473     releaseDelayedReleasedObjects();
 474 
 475     sweepAllLogicallyEmptyWeakBlocks();
 476 
 477     m_objectSpace.freeMemory();
 478 
 479     dataLogIf(Options::logGC(), (MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
 480 }
 481 
 482 void Heap::releaseDelayedReleasedObjects()
 483 {
 484 #if USE(FOUNDATION) || defined(JSC_GLIB_API_ENABLED)
 485     // We need to guard against the case that releasing an object can create more objects due to the
 486     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 487     // back here and could try to recursively release objects. We guard that with a recursive entry
 488     // count. Only the initial call will release objects, recursive calls simple return and let the
 489     // the initial call to the function take care of any objects created during release time.
 490     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 491     // and use a temp Vector for the actual releasing.
 492     if (!m_delayedReleaseRecursionCount++) {
 493         while (!m_delayedReleaseObjects.isEmpty()) {
 494             ASSERT(m_vm.currentThreadIsHoldingAPILock());
 495 
 496             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 497 
 498             {
 499                 // We need to drop locks before calling out to arbitrary code.
 500                 JSLock::DropAllLocks dropAllLocks(m_vm);
 501 
 502 #if USE(FOUNDATION)
 503                 void* context = objc_autoreleasePoolPush();
 504 #endif
 505                 objectsToRelease.clear();
 506 #if USE(FOUNDATION)
 507                 objc_autoreleasePoolPop(context);
 508 #endif
 509             }
 510         }
 511     }
 512     m_delayedReleaseRecursionCount--;
 513 #endif
 514 }
 515 
 516 void Heap::reportExtraMemoryAllocatedSlowCase(size_t size)
 517 {
 518     didAllocate(size);
 519     collectIfNecessaryOrDefer();
 520 }
 521 
 522 void Heap::deprecatedReportExtraMemorySlowCase(size_t size)
 523 {
 524     // FIXME: Change this to use SaturatedArithmetic when available.
 525     // https://bugs.webkit.org/show_bug.cgi?id=170411
 526     Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = m_deprecatedExtraMemorySize;
 527     checkedNewSize += size;
 528     m_deprecatedExtraMemorySize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
 529     reportExtraMemoryAllocatedSlowCase(size);
 530 }
 531 
 532 bool Heap::overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType)
 533 {
 534 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)
 535     if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_percentAvailableMemoryCachedCallCount &gt;= 100) {
 536         m_overCriticalMemoryThreshold = bmalloc::api::percentAvailableMemoryInUse() &gt; Options::criticalGCMemoryThreshold();
 537         m_percentAvailableMemoryCachedCallCount = 0;
 538     }
 539 
 540     return m_overCriticalMemoryThreshold;
 541 #else
 542     UNUSED_PARAM(memoryThresholdCallType);
 543     return false;
 544 #endif
 545 }
 546 
 547 void Heap::reportAbandonedObjectGraph()
 548 {
 549     // Our clients don&#39;t know exactly how much memory they
 550     // are abandoning so we just guess for them.
 551     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 552 
 553     // We want to accelerate the next collection. Because memory has just
 554     // been abandoned, the next collection has the potential to
 555     // be more profitable. Since allocation is the trigger for collection,
 556     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 557     if (m_fullActivityCallback) {
 558         m_fullActivityCallback-&gt;didAllocate(*this,
 559             m_sizeAfterLastCollect - m_sizeAfterLastFullCollect + m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
 560     }
 561     m_bytesAbandonedSinceLastFullCollect += abandonedBytes;
 562 }
 563 
 564 void Heap::protect(JSValue k)
 565 {
 566     ASSERT(k);
 567     ASSERT(m_vm.currentThreadIsHoldingAPILock());
 568 
 569     if (!k.isCell())
 570         return;
 571 
 572     m_protectedValues.add(k.asCell());
 573 }
 574 
 575 bool Heap::unprotect(JSValue k)
 576 {
 577     ASSERT(k);
 578     ASSERT(m_vm.currentThreadIsHoldingAPILock());
 579 
 580     if (!k.isCell())
 581         return false;
 582 
 583     return m_protectedValues.remove(k.asCell());
 584 }
 585 
 586 void Heap::addReference(JSCell* cell, ArrayBuffer* buffer)
 587 {
 588     if (m_arrayBuffers.addReference(cell, buffer)) {
 589         collectIfNecessaryOrDefer();
 590         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 591     }
 592 }
 593 
 594 template&lt;typename CellType, typename CellSet&gt;
 595 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 596 {
 597     cellSet.forEachMarkedCell(
 598         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 599             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(vm());
 600         });
 601 }
 602 
 603 void Heap::finalizeUnconditionalFinalizers()
 604 {
 605     vm().builtinExecutables()-&gt;finalizeUnconditionally();
 606     finalizeMarkedUnconditionalFinalizers&lt;FunctionExecutable&gt;(vm().functionExecutableSpace.space);
 607     finalizeMarkedUnconditionalFinalizers&lt;SymbolTable&gt;(vm().symbolTableSpace);
 608     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm().executableToCodeBlockEdgesWithFinalizers); // We run this before CodeBlock&#39;s unconditional finalizer since CodeBlock looks at the owner executable&#39;s installed CodeBlock in its finalizeUnconditionally.
 609     vm().forEachCodeBlockSpace(
 610         [&amp;] (auto&amp; space) {
 611             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 612         });
 613     finalizeMarkedUnconditionalFinalizers&lt;StructureRareData&gt;(vm().structureRareDataSpace);
 614     finalizeMarkedUnconditionalFinalizers&lt;UnlinkedFunctionExecutable&gt;(vm().unlinkedFunctionExecutableSpace.set);
 615     if (vm().m_weakSetSpace)
 616         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm().m_weakSetSpace);
 617     if (vm().m_weakMapSpace)
 618         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm().m_weakMapSpace);
 619     if (vm().m_weakObjectRefSpace)
 620         finalizeMarkedUnconditionalFinalizers&lt;JSWeakObjectRef&gt;(*vm().m_weakObjectRefSpace);
 621     if (vm().m_errorInstanceSpace)
 622         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm().m_errorInstanceSpace);
 623 
 624 #if ENABLE(WEBASSEMBLY)
 625     if (vm().m_webAssemblyCodeBlockSpace)
 626         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm().m_webAssemblyCodeBlockSpace);
 627 #endif
 628 }
 629 
 630 void Heap::willStartIterating()
 631 {
 632     m_objectSpace.willStartIterating();
 633 }
 634 
 635 void Heap::didFinishIterating()
 636 {
 637     m_objectSpace.didFinishIterating();
 638 }
 639 
 640 void Heap::completeAllJITPlans()
 641 {
 642     if (!VM::canUseJIT())
 643         return;
 644 #if ENABLE(JIT)
 645     JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm);
 646 #endif // ENABLE(JIT)
 647     DFG::completeAllPlansForVM(m_vm);
 648 }
 649 
 650 template&lt;typename Func&gt;
 651 void Heap::iterateExecutingAndCompilingCodeBlocks(const Func&amp; func)
 652 {
 653     m_codeBlocks-&gt;iterateCurrentlyExecuting(func);
 654     if (VM::canUseJIT())
 655         DFG::iterateCodeBlocksForGC(m_vm, func);
 656 }
 657 
 658 template&lt;typename Func&gt;
 659 void Heap::iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp; func)
 660 {
 661     Vector&lt;CodeBlock*, 256&gt; codeBlocks;
 662     iterateExecutingAndCompilingCodeBlocks(
 663         [&amp;] (CodeBlock* codeBlock) {
 664             codeBlocks.append(codeBlock);
 665         });
 666     for (CodeBlock* codeBlock : codeBlocks)
 667         func(codeBlock);
 668 }
 669 
 670 void Heap::assertMarkStacksEmpty()
 671 {
 672     bool ok = true;
 673 
 674     if (!m_sharedCollectorMarkStack-&gt;isEmpty()) {
 675         dataLog(&quot;FATAL: Shared collector mark stack not empty! It has &quot;, m_sharedCollectorMarkStack-&gt;size(), &quot; elements.\n&quot;);
 676         ok = false;
 677     }
 678 
 679     if (!m_sharedMutatorMarkStack-&gt;isEmpty()) {
 680         dataLog(&quot;FATAL: Shared mutator mark stack not empty! It has &quot;, m_sharedMutatorMarkStack-&gt;size(), &quot; elements.\n&quot;);
 681         ok = false;
 682     }
 683 
 684     forEachSlotVisitor(
 685         [&amp;] (SlotVisitor&amp; visitor) {
 686             if (visitor.isEmpty())
 687                 return;
 688 
 689             dataLog(&quot;FATAL: Visitor &quot;, RawPointer(&amp;visitor), &quot; is not empty!\n&quot;);
 690             ok = false;
 691         });
 692 
 693     RELEASE_ASSERT(ok);
 694 }
 695 
 696 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 697 {
 698     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 699 }
 700 
 701 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 702 {
 703 #if ENABLE(C_LOOP)
 704     m_vm.interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);
 705 #else
 706     UNUSED_PARAM(roots);
 707 #endif
 708 }
 709 
 710 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 711 {
 712 #if ENABLE(DFG_JIT)
 713     if (!VM::canUseJIT())
 714         return;
 715     m_vm.gatherScratchBufferRoots(roots);
 716     m_vm.scanSideState(roots);
 717 #else
 718     UNUSED_PARAM(roots);
 719 #endif
 720 }
 721 
 722 void Heap::beginMarking()
 723 {
 724     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 725     m_jitStubRoutines-&gt;clearMarks();
 726     m_objectSpace.beginMarking();
 727     setMutatorShouldBeFenced(true);
 728 }
 729 
 730 void Heap::removeDeadCompilerWorklistEntries()
 731 {
 732 #if ENABLE(DFG_JIT)
 733     if (!VM::canUseJIT())
 734         return;
 735     for (unsigned i = DFG::numberOfWorklists(); i--;)
 736         DFG::existingWorklistForIndex(i).removeDeadPlans(m_vm);
 737 #endif
 738 }
 739 
 740 bool Heap::isAnalyzingHeap() const
 741 {
 742     HeapProfiler* heapProfiler = m_vm.heapProfiler();
 743     if (UNLIKELY(heapProfiler))
 744         return heapProfiler-&gt;activeHeapAnalyzer();
 745     return false;
 746 }
 747 
 748 struct GatherExtraHeapData : MarkedBlock::CountFunctor {
 749     GatherExtraHeapData(VM&amp; vm, HeapAnalyzer&amp; analyzer)
 750         : m_vm(vm)
 751         , m_analyzer(analyzer)
 752     {
 753     }
 754 
 755     IterationStatus operator()(HeapCell* heapCell, HeapCell::Kind kind) const
 756     {
 757         if (isJSCellKind(kind)) {
 758             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 759             cell-&gt;methodTable(m_vm)-&gt;analyzeHeap(cell, m_analyzer);
 760         }
 761         return IterationStatus::Continue;
 762     }
 763 
 764     VM&amp; m_vm;
 765     HeapAnalyzer&amp; m_analyzer;
 766 };
 767 
 768 void Heap::gatherExtraHeapData(HeapProfiler&amp; heapProfiler)
 769 {
 770     if (auto* analyzer = heapProfiler.activeHeapAnalyzer()) {
 771         HeapIterationScope heapIterationScope(*this);
 772         GatherExtraHeapData functor(m_vm, *analyzer);
 773         m_objectSpace.forEachLiveCell(heapIterationScope, functor);
 774     }
 775 }
 776 
 777 struct RemoveDeadHeapSnapshotNodes : MarkedBlock::CountFunctor {
 778     RemoveDeadHeapSnapshotNodes(HeapSnapshot&amp; snapshot)
 779         : m_snapshot(snapshot)
 780     {
 781     }
 782 
 783     IterationStatus operator()(HeapCell* cell, HeapCell::Kind kind) const
 784     {
 785         if (isJSCellKind(kind))
 786             m_snapshot.sweepCell(static_cast&lt;JSCell*&gt;(cell));
 787         return IterationStatus::Continue;
 788     }
 789 
 790     HeapSnapshot&amp; m_snapshot;
 791 };
 792 
 793 void Heap::removeDeadHeapSnapshotNodes(HeapProfiler&amp; heapProfiler)
 794 {
 795     if (HeapSnapshot* snapshot = heapProfiler.mostRecentSnapshot()) {
 796         HeapIterationScope heapIterationScope(*this);
 797         RemoveDeadHeapSnapshotNodes functor(*snapshot);
 798         m_objectSpace.forEachDeadCell(heapIterationScope, functor);
 799         snapshot-&gt;shrinkToFit();
 800     }
 801 }
 802 
 803 void Heap::updateObjectCounts()
 804 {
 805     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full)
 806         m_totalBytesVisited = 0;
 807 
 808     m_totalBytesVisitedThisCycle = bytesVisited();
 809 
 810     m_totalBytesVisited += m_totalBytesVisitedThisCycle;
 811 }
 812 
 813 void Heap::endMarking()
 814 {
 815     forEachSlotVisitor(
 816         [&amp;] (SlotVisitor&amp; visitor) {
 817             visitor.reset();
 818         });
 819 
 820     assertMarkStacksEmpty();
 821 
 822     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
 823 
 824     m_objectSpace.endMarking();
 825     setMutatorShouldBeFenced(Options::forceFencedBarrier());
 826 }
 827 
 828 size_t Heap::objectCount()
 829 {
 830     return m_objectSpace.objectCount();
 831 }
 832 
 833 size_t Heap::extraMemorySize()
 834 {
 835     // FIXME: Change this to use SaturatedArithmetic when available.
 836     // https://bugs.webkit.org/show_bug.cgi?id=170411
 837     Checked&lt;size_t, RecordOverflow&gt; checkedTotal = m_extraMemorySize;
 838     checkedTotal += m_deprecatedExtraMemorySize;
 839     checkedTotal += m_arrayBuffers.size();
 840     size_t total = UNLIKELY(checkedTotal.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedTotal.unsafeGet();
 841 
 842     ASSERT(m_objectSpace.capacity() &gt;= m_objectSpace.size());
 843     return std::min(total, std::numeric_limits&lt;size_t&gt;::max() - m_objectSpace.capacity());
 844 }
 845 
 846 size_t Heap::size()
 847 {
 848     return m_objectSpace.size() + extraMemorySize();
 849 }
 850 
 851 size_t Heap::capacity()
 852 {
 853     return m_objectSpace.capacity() + extraMemorySize();
 854 }
 855 
 856 size_t Heap::protectedGlobalObjectCount()
 857 {
 858     size_t result = 0;
 859     forEachProtectedCell(
 860         [&amp;] (JSCell* cell) {
 861             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 862                 result++;
 863         });
 864     return result;
 865 }
 866 
 867 size_t Heap::globalObjectCount()
 868 {
 869     HeapIterationScope iterationScope(*this);
 870     size_t result = 0;
 871     m_objectSpace.forEachLiveCell(
 872         iterationScope,
 873         [&amp;] (HeapCell* heapCell, HeapCell::Kind kind) -&gt; IterationStatus {
 874             if (!isJSCellKind(kind))
 875                 return IterationStatus::Continue;
 876             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 877             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 878                 result++;
 879             return IterationStatus::Continue;
 880         });
 881     return result;
 882 }
 883 
 884 size_t Heap::protectedObjectCount()
 885 {
 886     size_t result = 0;
 887     forEachProtectedCell(
 888         [&amp;] (JSCell*) {
 889             result++;
 890         });
 891     return result;
 892 }
 893 
 894 std::unique_ptr&lt;TypeCountSet&gt; Heap::protectedObjectTypeCounts()
 895 {
 896     std::unique_ptr&lt;TypeCountSet&gt; result = makeUnique&lt;TypeCountSet&gt;();
 897     forEachProtectedCell(
 898         [&amp;] (JSCell* cell) {
 899             recordType(vm(), *result, cell);
 900         });
 901     return result;
 902 }
 903 
 904 std::unique_ptr&lt;TypeCountSet&gt; Heap::objectTypeCounts()
 905 {
 906     std::unique_ptr&lt;TypeCountSet&gt; result = makeUnique&lt;TypeCountSet&gt;();
 907     HeapIterationScope iterationScope(*this);
 908     m_objectSpace.forEachLiveCell(
 909         iterationScope,
 910         [&amp;] (HeapCell* cell, HeapCell::Kind kind) -&gt; IterationStatus {
 911             if (isJSCellKind(kind))
 912                 recordType(vm(), *result, static_cast&lt;JSCell*&gt;(cell));
 913             return IterationStatus::Continue;
 914         });
 915     return result;
 916 }
 917 
 918 void Heap::deleteAllCodeBlocks(DeleteAllCodeEffort effort)
 919 {
 920     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 921         return;
 922 
 923     VM&amp; vm = m_vm;
 924     PreventCollectionScope preventCollectionScope(*this);
 925 
 926     // If JavaScript is running, it&#39;s not safe to delete all JavaScript code, since
 927     // we&#39;ll end up returning to deleted code.
 928     RELEASE_ASSERT(!vm.entryScope);
 929     RELEASE_ASSERT(!m_collectionScope);
 930 
 931     completeAllJITPlans();
 932 
 933     vm.forEachScriptExecutableSpace(
 934         [&amp;] (auto&amp; spaceAndSet) {
 935             HeapIterationScope heapIterationScope(*this);
 936             auto&amp; set = spaceAndSet.set;
 937             set.forEachLiveCell(
 938                 [&amp;] (HeapCell* cell, HeapCell::Kind) {
 939                     ScriptExecutable* executable = static_cast&lt;ScriptExecutable*&gt;(cell);
 940                     executable-&gt;clearCode(set);
 941                 });
 942         });
 943 
 944 #if ENABLE(WEBASSEMBLY)
 945     {
 946         // We must ensure that we clear the JS call ICs from Wasm. Otherwise, Wasm will
 947         // have no idea that we cleared the code from all of the Executables in the
 948         // VM. This could leave Wasm in an inconsistent state where it has an IC that
 949         // points into a CodeBlock that could be dead. The IC will still succeed because
 950         // it uses a callee check, but then it will call into dead code.
 951         HeapIterationScope heapIterationScope(*this);
 952         if (vm.m_webAssemblyCodeBlockSpace) {
 953             vm.m_webAssemblyCodeBlockSpace-&gt;forEachLiveCell([&amp;] (HeapCell* cell, HeapCell::Kind kind) {
 954                 ASSERT_UNUSED(kind, kind == HeapCell::JSCell);
 955                 JSWebAssemblyCodeBlock* codeBlock = static_cast&lt;JSWebAssemblyCodeBlock*&gt;(cell);
 956                 codeBlock-&gt;clearJSCallICs(vm);
 957             });
 958         }
 959     }
 960 #endif
 961 }
 962 
 963 void Heap::deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort effort)
 964 {
 965     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 966         return;
 967 
 968     VM&amp; vm = m_vm;
 969     PreventCollectionScope preventCollectionScope(*this);
 970 
 971     RELEASE_ASSERT(!m_collectionScope);
 972 
 973     HeapIterationScope heapIterationScope(*this);
 974     vm.unlinkedFunctionExecutableSpace.set.forEachLiveCell(
 975         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 976             UnlinkedFunctionExecutable* executable = static_cast&lt;UnlinkedFunctionExecutable*&gt;(cell);
 977             executable-&gt;clearCode(vm);
 978         });
 979 }
 980 
 981 void Heap::deleteUnmarkedCompiledCode()
 982 {
 983     vm().forEachScriptExecutableSpace([] (auto&amp; space) { space.space.sweep(); });
 984     vm().forEachCodeBlockSpace([] (auto&amp; space) { space.space.sweep(); }); // Sweeping must occur before deleting stubs, otherwise the stubs might still think they&#39;re alive as they get deleted.
 985     m_jitStubRoutines-&gt;deleteUnmarkedJettisonedStubRoutines();
 986 }
 987 
 988 void Heap::addToRememberedSet(const JSCell* constCell)
 989 {
 990     JSCell* cell = const_cast&lt;JSCell*&gt;(constCell);
 991     ASSERT(cell);
 992     ASSERT(!Options::useConcurrentJIT() || !isCompilationThread());
 993     m_barriersExecuted++;
 994     if (m_mutatorShouldBeFenced) {
 995         WTF::loadLoadFence();
 996         if (!isMarked(cell)) {
 997             // During a full collection a store into an unmarked object that had surivived past
 998             // collections will manifest as a store to an unmarked PossiblyBlack object. If the
 999             // object gets marked at some time after this then it will go down the normal marking
1000             // path. So, we don&#39;t have to remember this object. We could return here. But we go
1001             // further and attempt to re-white the object.
1002 
1003             RELEASE_ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full);
1004 
1005             if (cell-&gt;atomicCompareExchangeCellStateStrong(CellState::PossiblyBlack, CellState::DefinitelyWhite) == CellState::PossiblyBlack) {
1006                 // Now we protect against this race:
1007                 //
1008                 //     1) Object starts out black + unmarked.
1009                 //     --&gt; We do isMarked here.
1010                 //     2) Object is marked and greyed.
1011                 //     3) Object is scanned and blacked.
1012                 //     --&gt; We do atomicCompareExchangeCellStateStrong here.
1013                 //
1014                 // In this case we would have made the object white again, even though it should
1015                 // be black. This check lets us correct our mistake. This relies on the fact that
1016                 // isMarked converges monotonically to true.
1017                 if (isMarked(cell)) {
1018                     // It&#39;s difficult to work out whether the object should be grey or black at
1019                     // this point. We say black conservatively.
1020                     cell-&gt;setCellState(CellState::PossiblyBlack);
1021                 }
1022 
1023                 // Either way, we can return. Most likely, the object was not marked, and so the
1024                 // object is now labeled white. This means that future barrier executions will not
1025                 // fire. In the unlikely event that the object had become marked, we can still
1026                 // return anyway, since we proved that the object was not marked at the time that
1027                 // we executed this slow path.
1028             }
1029 
1030             return;
1031         }
1032     } else
1033         ASSERT(isMarked(cell));
1034     // It could be that the object was *just* marked. This means that the collector may set the
1035     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
1036     // race with the collector here. If we win then this is accurate because the object _will_
1037     // get scanned again. If we lose then someone else will barrier the object again. That would
1038     // be unfortunate but not the end of the world.
1039     cell-&gt;setCellState(CellState::PossiblyGrey);
1040     m_mutatorMarkStack-&gt;append(cell);
1041 }
1042 
1043 void Heap::sweepSynchronously()
1044 {
1045     MonotonicTime before { };
1046     if (UNLIKELY(Options::logGC())) {
1047         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
1048         before = MonotonicTime::now();
1049     }
1050     m_objectSpace.sweepBlocks();
1051     m_objectSpace.shrink();
1052     if (UNLIKELY(Options::logGC())) {
1053         MonotonicTime after = MonotonicTime::now();
1054         dataLog(&quot;=&gt; &quot;, capacity() / 1024, &quot;kb, &quot;, (after - before).milliseconds(), &quot;ms&quot;);
1055     }
1056 }
1057 
1058 void Heap::collect(Synchronousness synchronousness, GCRequest request)
1059 {
1060     switch (synchronousness) {
1061     case Async:
1062         collectAsync(request);
1063         return;
1064     case Sync:
1065         collectSync(request);
1066         return;
1067     }
1068     RELEASE_ASSERT_NOT_REACHED();
1069 }
1070 
1071 void Heap::collectNow(Synchronousness synchronousness, GCRequest request)
1072 {
1073     if (validateDFGDoesGC)
1074         RELEASE_ASSERT(expectDoesGC());
1075 
1076     switch (synchronousness) {
1077     case Async: {
1078         collectAsync(request);
1079         stopIfNecessary();
1080         return;
1081     }
1082 
1083     case Sync: {
1084         collectSync(request);
1085 
1086         DeferGCForAWhile deferGC(*this);
1087         if (UNLIKELY(Options::useImmortalObjects()))
1088             sweeper().stopSweeping();
1089 
1090         bool alreadySweptInCollectSync = shouldSweepSynchronously();
1091         if (!alreadySweptInCollectSync) {
1092             dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;);
1093             sweepSynchronously();
1094             dataLogIf(Options::logGC(), &quot;]\n&quot;);
1095         }
1096         m_objectSpace.assertNoUnswept();
1097 
1098         sweepAllLogicallyEmptyWeakBlocks();
1099         return;
1100     } }
1101     RELEASE_ASSERT_NOT_REACHED();
1102 }
1103 
1104 void Heap::collectAsync(GCRequest request)
1105 {
1106     if (validateDFGDoesGC)
1107         RELEASE_ASSERT(expectDoesGC());
1108 
1109     if (!m_isSafeToCollect)
1110         return;
1111 
1112     bool alreadyRequested = false;
1113     {
1114         LockHolder locker(*m_threadLock);
1115         for (const GCRequest&amp; previousRequest : m_requests) {
1116             if (request.subsumedBy(previousRequest)) {
1117                 alreadyRequested = true;
1118                 break;
1119             }
1120         }
1121     }
1122     if (alreadyRequested)
1123         return;
1124 
1125     requestCollection(request);
1126 }
1127 
1128 void Heap::collectSync(GCRequest request)
1129 {
1130     if (validateDFGDoesGC)
1131         RELEASE_ASSERT(expectDoesGC());
1132 
1133     if (!m_isSafeToCollect)
1134         return;
1135 
1136     waitForCollection(requestCollection(request));
1137 }
1138 
1139 bool Heap::shouldCollectInCollectorThread(const AbstractLocker&amp;)
1140 {
1141     RELEASE_ASSERT(m_requests.isEmpty() == (m_lastServedTicket == m_lastGrantedTicket));
1142     RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
1143 
1144     if (false)
1145         dataLog(&quot;Mutator has the conn = &quot;, !!(m_worldState.load() &amp; mutatorHasConnBit), &quot;\n&quot;);
1146 
1147     return !m_requests.isEmpty() &amp;&amp; !(m_worldState.load() &amp; mutatorHasConnBit);
1148 }
1149 
1150 void Heap::collectInCollectorThread()
1151 {
1152     for (;;) {
1153         RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Collector, nullptr);
1154         switch (result) {
1155         case RunCurrentPhaseResult::Finished:
1156             return;
1157         case RunCurrentPhaseResult::Continue:
1158             break;
1159         case RunCurrentPhaseResult::NeedCurrentThreadState:
1160             RELEASE_ASSERT_NOT_REACHED();
1161             break;
1162         }
1163     }
1164 }
1165 
1166 ALWAYS_INLINE int asInt(CollectorPhase phase)
1167 {
1168     return static_cast&lt;int&gt;(phase);
1169 }
1170 
1171 void Heap::checkConn(GCConductor conn)
1172 {
1173     unsigned worldState = m_worldState.load();
1174     switch (conn) {
1175     case GCConductor::Mutator:
1176         RELEASE_ASSERT(worldState &amp; mutatorHasConnBit, worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm().id(), VM::numberOfIDs(), vm().isEntered());
1177         return;
1178     case GCConductor::Collector:
1179         RELEASE_ASSERT(!(worldState &amp; mutatorHasConnBit), worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm().id(), VM::numberOfIDs(), vm().isEntered());
1180         return;
1181     }
1182     RELEASE_ASSERT_NOT_REACHED();
1183 }
1184 
1185 auto Heap::runCurrentPhase(GCConductor conn, CurrentThreadState* currentThreadState) -&gt; RunCurrentPhaseResult
1186 {
1187     checkConn(conn);
1188     m_currentThreadState = currentThreadState;
1189     m_currentThread = &amp;Thread::current();
1190 
1191     if (conn == GCConductor::Mutator)
1192         sanitizeStackForVM(vm());
1193 
1194     // If the collector transfers the conn to the mutator, it leaves us in between phases.
1195     if (!finishChangingPhase(conn)) {
1196         // A mischevious mutator could repeatedly relinquish the conn back to us. We try to avoid doing
1197         // this, but it&#39;s probably not the end of the world if it did happen.
1198         if (false)
1199             dataLog(&quot;Conn bounce-back.\n&quot;);
1200         return RunCurrentPhaseResult::Finished;
1201     }
1202 
1203     bool result = false;
1204     switch (m_currentPhase) {
1205     case CollectorPhase::NotRunning:
1206         result = runNotRunningPhase(conn);
1207         break;
1208 
1209     case CollectorPhase::Begin:
1210         result = runBeginPhase(conn);
1211         break;
1212 
1213     case CollectorPhase::Fixpoint:
1214         if (!currentThreadState &amp;&amp; conn == GCConductor::Mutator)
1215             return RunCurrentPhaseResult::NeedCurrentThreadState;
1216 
1217         result = runFixpointPhase(conn);
1218         break;
1219 
1220     case CollectorPhase::Concurrent:
1221         result = runConcurrentPhase(conn);
1222         break;
1223 
1224     case CollectorPhase::Reloop:
1225         result = runReloopPhase(conn);
1226         break;
1227 
1228     case CollectorPhase::End:
1229         result = runEndPhase(conn);
1230         break;
1231     }
1232 
1233     return result ? RunCurrentPhaseResult::Continue : RunCurrentPhaseResult::Finished;
1234 }
1235 
1236 NEVER_INLINE bool Heap::runNotRunningPhase(GCConductor conn)
1237 {
1238     // Check m_requests since the mutator calls this to poll what&#39;s going on.
1239     {
1240         auto locker = holdLock(*m_threadLock);
1241         if (m_requests.isEmpty())
1242             return false;
1243     }
1244 
1245     return changePhase(conn, CollectorPhase::Begin);
1246 }
1247 
1248 NEVER_INLINE bool Heap::runBeginPhase(GCConductor conn)
1249 {
1250     m_currentGCStartTime = MonotonicTime::now();
1251 
1252     {
1253         LockHolder locker(*m_threadLock);
1254         RELEASE_ASSERT(!m_requests.isEmpty());
1255         m_currentRequest = m_requests.first();
1256     }
1257 
1258     dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: START &quot;, gcConductorShortName(conn), &quot; &quot;, capacity() / 1024, &quot;kb &quot;);
1259 
1260     m_beforeGC = MonotonicTime::now();
1261 
1262     if (!Options::seedOfVMRandomForFuzzer())
1263         vm().random().setSeed(cryptographicallyRandomNumber());
1264 
1265     if (m_collectionScope) {
1266         dataLogLn(&quot;Collection scope already set during GC: &quot;, *m_collectionScope);
1267         RELEASE_ASSERT_NOT_REACHED();
1268     }
1269 
1270     willStartCollection();
1271 
1272     if (UNLIKELY(m_verifier)) {
1273         // Verify that live objects from the last GC cycle haven&#39;t been corrupted by
1274         // mutators before we begin this new GC cycle.
1275         m_verifier-&gt;verify(HeapVerifier::Phase::BeforeGC);
1276 
1277         m_verifier-&gt;startGC();
1278         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::BeforeMarking);
1279     }
1280 
1281     prepareForMarking();
1282 
1283     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
1284         m_opaqueRoots.clear();
1285         m_collectorSlotVisitor-&gt;clearMarkStacks();
1286         m_mutatorMarkStack-&gt;clear();
1287     }
1288 
1289     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
1290 
1291     beginMarking();
1292 
1293     forEachSlotVisitor(
1294         [&amp;] (SlotVisitor&amp; visitor) {
1295             visitor.didStartMarking();
1296         });
1297 
1298     m_parallelMarkersShouldExit = false;
1299 
1300     m_helperClient.setFunction(
1301         [this] () {
1302             SlotVisitor* slotVisitor;
1303             {
1304                 LockHolder locker(m_parallelSlotVisitorLock);
1305                 RELEASE_ASSERT_WITH_MESSAGE(!m_availableParallelSlotVisitors.isEmpty(), &quot;Parallel SlotVisitors are allocated apriori&quot;);
1306                 slotVisitor = m_availableParallelSlotVisitors.takeLast();
1307             }
1308 
1309             Thread::registerGCThread(GCThreadType::Helper);
1310 
1311             {
1312                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1313                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1314             }
1315 
1316             {
1317                 LockHolder locker(m_parallelSlotVisitorLock);
1318                 m_availableParallelSlotVisitors.append(slotVisitor);
1319             }
1320         });
1321 
1322     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1323 
1324     m_constraintSet-&gt;didStartMarking();
1325 
1326     m_scheduler-&gt;beginCollection();
1327     if (UNLIKELY(Options::logGC()))
1328         m_scheduler-&gt;log();
1329 
1330     // After this, we will almost certainly fall through all of the &quot;slotVisitor.isEmpty()&quot;
1331     // checks because bootstrap would have put things into the visitor. So, we should fall
1332     // through to draining.
1333 
1334     if (!slotVisitor.didReachTermination()) {
1335         dataLog(&quot;Fatal: SlotVisitor should think that GC should terminate before constraint solving, but it does not think this.\n&quot;);
1336         dataLog(&quot;slotVisitor.isEmpty(): &quot;, slotVisitor.isEmpty(), &quot;\n&quot;);
1337         dataLog(&quot;slotVisitor.collectorMarkStack().isEmpty(): &quot;, slotVisitor.collectorMarkStack().isEmpty(), &quot;\n&quot;);
1338         dataLog(&quot;slotVisitor.mutatorMarkStack().isEmpty(): &quot;, slotVisitor.mutatorMarkStack().isEmpty(), &quot;\n&quot;);
1339         dataLog(&quot;m_numberOfActiveParallelMarkers: &quot;, m_numberOfActiveParallelMarkers, &quot;\n&quot;);
1340         dataLog(&quot;m_sharedCollectorMarkStack-&gt;isEmpty(): &quot;, m_sharedCollectorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1341         dataLog(&quot;m_sharedMutatorMarkStack-&gt;isEmpty(): &quot;, m_sharedMutatorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1342         dataLog(&quot;slotVisitor.didReachTermination(): &quot;, slotVisitor.didReachTermination(), &quot;\n&quot;);
1343         RELEASE_ASSERT_NOT_REACHED();
1344     }
1345 
1346     return changePhase(conn, CollectorPhase::Fixpoint);
1347 }
1348 
1349 NEVER_INLINE bool Heap::runFixpointPhase(GCConductor conn)
1350 {
1351     RELEASE_ASSERT(conn == GCConductor::Collector || m_currentThreadState);
1352 
1353     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1354 
1355     if (UNLIKELY(Options::logGC())) {
1356         HashMap&lt;const char*, size_t&gt; visitMap;
1357         forEachSlotVisitor(
1358             [&amp;] (SlotVisitor&amp; slotVisitor) {
1359                 visitMap.add(slotVisitor.codeName(), slotVisitor.bytesVisited() / 1024);
1360             });
1361 
1362         auto perVisitorDump = sortedMapDump(
1363             visitMap,
1364             [] (const char* a, const char* b) -&gt; bool {
1365                 return strcmp(a, b) &lt; 0;
1366             },
1367             &quot;:&quot;, &quot; &quot;);
1368 
1369         dataLog(&quot;v=&quot;, bytesVisited() / 1024, &quot;kb (&quot;, perVisitorDump, &quot;) o=&quot;, m_opaqueRoots.size(), &quot; b=&quot;, m_barriersExecuted, &quot; &quot;);
1370     }
1371 
1372     if (slotVisitor.didReachTermination()) {
1373         m_opaqueRoots.deleteOldTables();
1374 
1375         m_scheduler-&gt;didReachTermination();
1376 
1377         assertMarkStacksEmpty();
1378 
1379         // FIXME: Take m_mutatorDidRun into account when scheduling constraints. Most likely,
1380         // we don&#39;t have to execute root constraints again unless the mutator did run. At a
1381         // minimum, we could use this for work estimates - but it&#39;s probably more than just an
1382         // estimate.
1383         // https://bugs.webkit.org/show_bug.cgi?id=166828
1384 
1385         // Wondering what this does? Look at Heap::addCoreConstraints(). The DOM and others can also
1386         // add their own using Heap::addMarkingConstraint().
1387         bool converged = m_constraintSet-&gt;executeConvergence(slotVisitor);
1388 
1389         // FIXME: The slotVisitor.isEmpty() check is most likely not needed.
1390         // https://bugs.webkit.org/show_bug.cgi?id=180310
1391         if (converged &amp;&amp; slotVisitor.isEmpty()) {
1392             assertMarkStacksEmpty();
1393             return changePhase(conn, CollectorPhase::End);
1394         }
1395 
1396         m_scheduler-&gt;didExecuteConstraints();
1397     }
1398 
1399     dataLogIf(Options::logGC(), slotVisitor.collectorMarkStack().size(), &quot;+&quot;, m_mutatorMarkStack-&gt;size() + slotVisitor.mutatorMarkStack().size(), &quot; &quot;);
1400 
1401     {
1402         ParallelModeEnabler enabler(slotVisitor);
1403         slotVisitor.drainInParallel(m_scheduler-&gt;timeToResume());
1404     }
1405 
1406     m_scheduler-&gt;synchronousDrainingDidStall();
1407 
1408     // This is kinda tricky. The termination check looks at:
1409     //
1410     // - Whether the marking threads are active. If they are not, this means that the marking threads&#39;
1411     //   SlotVisitors are empty.
1412     // - Whether the collector&#39;s slot visitor is empty.
1413     // - Whether the shared mark stacks are empty.
1414     //
1415     // This doesn&#39;t have to check the mutator SlotVisitor because that one becomes empty after every GC
1416     // work increment, so it must be empty now.
1417     if (slotVisitor.didReachTermination())
1418         return true; // This is like relooping to the top if runFixpointPhase().
1419 
1420     if (!m_scheduler-&gt;shouldResume())
1421         return true;
1422 
1423     m_scheduler-&gt;willResume();
1424 
1425     if (UNLIKELY(Options::logGC())) {
1426         double thisPauseMS = (MonotonicTime::now() - m_stopTime).milliseconds();
1427         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;)...]\n&quot;);
1428     }
1429 
1430     // Forgive the mutator for its past failures to keep up.
1431     // FIXME: Figure out if moving this to different places results in perf changes.
1432     m_incrementBalance = 0;
1433 
1434     return changePhase(conn, CollectorPhase::Concurrent);
1435 }
1436 
1437 NEVER_INLINE bool Heap::runConcurrentPhase(GCConductor conn)
1438 {
1439     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1440 
1441     switch (conn) {
1442     case GCConductor::Mutator: {
1443         // When the mutator has the conn, we poll runConcurrentPhase() on every time someone says
1444         // stopIfNecessary(), so on every allocation slow path. When that happens we poll if it&#39;s time
1445         // to stop and do some work.
1446         if (slotVisitor.didReachTermination()
1447             || m_scheduler-&gt;shouldStop())
1448             return changePhase(conn, CollectorPhase::Reloop);
1449 
1450         // We could be coming from a collector phase that stuffed our SlotVisitor, so make sure we donate
1451         // everything. This is super cheap if the SlotVisitor is already empty.
1452         slotVisitor.donateAll();
1453         return false;
1454     }
1455     case GCConductor::Collector: {
1456         {
1457             ParallelModeEnabler enabler(slotVisitor);
1458             slotVisitor.drainInParallelPassively(m_scheduler-&gt;timeToStop());
1459         }
1460         return changePhase(conn, CollectorPhase::Reloop);
1461     } }
1462 
1463     RELEASE_ASSERT_NOT_REACHED();
1464     return false;
1465 }
1466 
1467 NEVER_INLINE bool Heap::runReloopPhase(GCConductor conn)
1468 {
1469     dataLogIf(Options::logGC(), &quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;, gcConductorShortName(conn), &quot; &quot;);
1470 
1471     m_scheduler-&gt;didStop();
1472 
1473     if (UNLIKELY(Options::logGC()))
1474         m_scheduler-&gt;log();
1475 
1476     return changePhase(conn, CollectorPhase::Fixpoint);
1477 }
1478 
1479 NEVER_INLINE bool Heap::runEndPhase(GCConductor conn)
1480 {
1481     m_scheduler-&gt;endCollection();
1482 
1483     {
1484         auto locker = holdLock(m_markingMutex);
1485         m_parallelMarkersShouldExit = true;
1486         m_markingConditionVariable.notifyAll();
1487     }
1488     m_helperClient.finish();
1489 
1490     iterateExecutingAndCompilingCodeBlocks(
1491         [&amp;] (CodeBlock* codeBlock) {
1492             writeBarrier(codeBlock);
1493         });
1494 
1495     updateObjectCounts();
1496     endMarking();
1497 
1498     if (UNLIKELY(m_verifier)) {
1499         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1500         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1501     }
1502 
1503     if (vm().typeProfiler())
1504         vm().typeProfiler()-&gt;invalidateTypeSetCache(vm());
1505 
1506     m_structureIDTable.flushOldTables();
1507 
1508     reapWeakHandles();
1509     pruneStaleEntriesFromWeakGCMaps();
1510     sweepArrayBuffers();
1511     snapshotUnswept();
1512     finalizeUnconditionalFinalizers(); // We rely on these unconditional finalizers running before clearCurrentlyExecuting since CodeBlock&#39;s finalizer relies on querying currently executing.
1513     removeDeadCompilerWorklistEntries();
1514     notifyIncrementalSweeper();
1515 
1516     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1517         [&amp;] (CodeBlock* codeBlock) {
1518             writeBarrier(codeBlock);
1519         });
1520     m_codeBlocks-&gt;clearCurrentlyExecuting();
1521 
1522     m_objectSpace.prepareForAllocation();
1523     updateAllocationLimits();
1524 
1525     if (UNLIKELY(m_verifier)) {
1526         m_verifier-&gt;trimDeadCells();
1527         m_verifier-&gt;verify(HeapVerifier::Phase::AfterGC);
1528     }
1529 
1530     didFinishCollection();
1531 
1532     if (m_currentRequest.didFinishEndPhase)
1533         m_currentRequest.didFinishEndPhase-&gt;run();
1534 
1535     if (false) {
1536         dataLog(&quot;Heap state after GC:\n&quot;);
1537         m_objectSpace.dumpBits();
1538     }
1539 
1540     if (UNLIKELY(Options::logGC())) {
1541         double thisPauseMS = (m_afterGC - m_stopTime).milliseconds();
1542         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;), cycle &quot;, (m_afterGC - m_beforeGC).milliseconds(), &quot;ms END]\n&quot;);
1543     }
1544 
1545     {
1546         auto locker = holdLock(*m_threadLock);
1547         m_requests.removeFirst();
1548         m_lastServedTicket++;
1549         clearMutatorWaiting();
1550     }
1551     ParkingLot::unparkAll(&amp;m_worldState);
1552 
1553     dataLogLnIf(Options::logGC(), &quot;GC END!&quot;);
1554 
1555     setNeedFinalize();
1556 
1557     m_lastGCStartTime = m_currentGCStartTime;
1558     m_lastGCEndTime = MonotonicTime::now();
1559     m_totalGCTime += m_lastGCEndTime - m_lastGCStartTime;
1560 
1561     return changePhase(conn, CollectorPhase::NotRunning);
1562 }
1563 
1564 bool Heap::changePhase(GCConductor conn, CollectorPhase nextPhase)
1565 {
1566     checkConn(conn);
1567 
1568     m_lastPhase = m_currentPhase;
1569     m_nextPhase = nextPhase;
1570 
1571     return finishChangingPhase(conn);
1572 }
1573 
1574 NEVER_INLINE bool Heap::finishChangingPhase(GCConductor conn)
1575 {
1576     checkConn(conn);
1577 
1578     if (m_nextPhase == m_currentPhase)
1579         return true;
1580 
1581     if (false)
1582         dataLog(conn, &quot;: Going to phase: &quot;, m_nextPhase, &quot; (from &quot;, m_currentPhase, &quot;)\n&quot;);
1583 
1584     m_phaseVersion++;
1585 
1586     bool suspendedBefore = worldShouldBeSuspended(m_currentPhase);
1587     bool suspendedAfter = worldShouldBeSuspended(m_nextPhase);
1588 
1589     if (suspendedBefore != suspendedAfter) {
1590         if (suspendedBefore) {
1591             RELEASE_ASSERT(!suspendedAfter);
1592 
1593             resumeThePeriphery();
1594             if (conn == GCConductor::Collector)
1595                 resumeTheMutator();
1596             else
1597                 handleNeedFinalize();
1598         } else {
1599             RELEASE_ASSERT(!suspendedBefore);
1600             RELEASE_ASSERT(suspendedAfter);
1601 
1602             if (conn == GCConductor::Collector) {
1603                 waitWhileNeedFinalize();
1604                 if (!stopTheMutator()) {
1605                     if (false)
1606                         dataLog(&quot;Returning false.\n&quot;);
1607                     return false;
1608                 }
1609             } else {
1610                 sanitizeStackForVM(m_vm);
1611                 handleNeedFinalize();
1612             }
1613             stopThePeriphery(conn);
1614         }
1615     }
1616 
1617     m_currentPhase = m_nextPhase;
1618     return true;
1619 }
1620 
1621 void Heap::stopThePeriphery(GCConductor conn)
1622 {
1623     if (m_worldIsStopped) {
1624         dataLog(&quot;FATAL: world already stopped.\n&quot;);
1625         RELEASE_ASSERT_NOT_REACHED();
1626     }
1627 
1628     if (m_mutatorDidRun)
1629         m_mutatorExecutionVersion++;
1630 
1631     m_mutatorDidRun = false;
1632 
1633     suspendCompilerThreads();
1634     m_worldIsStopped = true;
1635 
1636     forEachSlotVisitor(
1637         [&amp;] (SlotVisitor&amp; slotVisitor) {
1638             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1639         });
1640 
1641 #if ENABLE(JIT)
1642     if (VM::canUseJIT()) {
1643         DeferGCForAWhile awhile(*this);
1644         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm)
1645             &amp;&amp; conn == GCConductor::Collector)
1646             setGCDidJIT();
1647     }
1648 #endif // ENABLE(JIT)
1649     UNUSED_PARAM(conn);
1650 
1651     if (auto* shadowChicken = vm().shadowChicken())
1652         shadowChicken-&gt;update(vm(), vm().topCallFrame);
1653 
1654     m_objectSpace.stopAllocating();
1655 
1656     m_stopTime = MonotonicTime::now();
1657 }
1658 
1659 NEVER_INLINE void Heap::resumeThePeriphery()
1660 {
1661     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1662     // collection cycle or this is just a concurrent phase within a collection cycle:
1663     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1664     //   last active block.
1665     // - During collection cycle: it reinstates the last active block.
1666     m_objectSpace.resumeAllocating();
1667 
1668     m_barriersExecuted = 0;
1669 
1670     if (!m_worldIsStopped) {
1671         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
1672         RELEASE_ASSERT_NOT_REACHED();
1673     }
1674     m_worldIsStopped = false;
1675 
1676     // FIXME: This could be vastly improved: we want to grab the locks in the order in which they
1677     // become available. We basically want a lockAny() method that will lock whatever lock is available
1678     // and tell you which one it locked. That would require teaching ParkingLot how to park on multiple
1679     // queues at once, which is totally achievable - it would just require memory allocation, which is
1680     // suboptimal but not a disaster. Alternatively, we could replace the SlotVisitor rightToRun lock
1681     // with a DLG-style handshake mechanism, but that seems not as general.
1682     Vector&lt;SlotVisitor*, 8&gt; slotVisitorsToUpdate;
1683 
1684     forEachSlotVisitor(
1685         [&amp;] (SlotVisitor&amp; slotVisitor) {
1686             slotVisitorsToUpdate.append(&amp;slotVisitor);
1687         });
1688 
1689     for (unsigned countdown = 40; !slotVisitorsToUpdate.isEmpty() &amp;&amp; countdown--;) {
1690         for (unsigned index = 0; index &lt; slotVisitorsToUpdate.size(); ++index) {
1691             SlotVisitor&amp; slotVisitor = *slotVisitorsToUpdate[index];
1692             bool remove = false;
1693             if (slotVisitor.hasAcknowledgedThatTheMutatorIsResumed())
1694                 remove = true;
1695             else if (auto locker = tryHoldLock(slotVisitor.rightToRun())) {
1696                 slotVisitor.updateMutatorIsStopped(locker);
1697                 remove = true;
1698             }
1699             if (remove) {
1700                 slotVisitorsToUpdate[index--] = slotVisitorsToUpdate.last();
1701                 slotVisitorsToUpdate.takeLast();
1702             }
1703         }
1704         Thread::yield();
1705     }
1706 
1707     for (SlotVisitor* slotVisitor : slotVisitorsToUpdate)
1708         slotVisitor-&gt;updateMutatorIsStopped();
1709 
1710     resumeCompilerThreads();
1711 }
1712 
1713 bool Heap::stopTheMutator()
1714 {
1715     for (;;) {
1716         unsigned oldState = m_worldState.load();
1717         if (oldState &amp; stoppedBit) {
1718             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1719             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1720             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1721             return true;
1722         }
1723 
1724         if (oldState &amp; mutatorHasConnBit) {
1725             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1726             RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1727             return false;
1728         }
1729 
1730         if (!(oldState &amp; hasAccessBit)) {
1731             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1732             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1733             // We can stop the world instantly.
1734             if (m_worldState.compareExchangeWeak(oldState, oldState | stoppedBit))
1735                 return true;
1736             continue;
1737         }
1738 
1739         // Transfer the conn to the mutator and bail.
1740         RELEASE_ASSERT(oldState &amp; hasAccessBit);
1741         RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1742         unsigned newState = (oldState | mutatorHasConnBit) &amp; ~mutatorWaitingBit;
1743         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1744             if (false)
1745                 dataLog(&quot;Handed off the conn.\n&quot;);
1746             m_stopIfNecessaryTimer-&gt;scheduleSoon();
1747             ParkingLot::unparkAll(&amp;m_worldState);
1748             return false;
1749         }
1750     }
1751 }
1752 
1753 NEVER_INLINE void Heap::resumeTheMutator()
1754 {
1755     if (false)
1756         dataLog(&quot;Resuming the mutator.\n&quot;);
1757     for (;;) {
1758         unsigned oldState = m_worldState.load();
1759         if (!!(oldState &amp; hasAccessBit) != !(oldState &amp; stoppedBit)) {
1760             dataLog(&quot;Fatal: hasAccess = &quot;, !!(oldState &amp; hasAccessBit), &quot;, stopped = &quot;, !!(oldState &amp; stoppedBit), &quot;\n&quot;);
1761             RELEASE_ASSERT_NOT_REACHED();
1762         }
1763         if (oldState &amp; mutatorHasConnBit) {
1764             dataLog(&quot;Fatal: mutator has the conn.\n&quot;);
1765             RELEASE_ASSERT_NOT_REACHED();
1766         }
1767 
1768         if (!(oldState &amp; stoppedBit)) {
1769             if (false)
1770                 dataLog(&quot;Returning because not stopped.\n&quot;);
1771             return;
1772         }
1773 
1774         if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~stoppedBit)) {
1775             if (false)
1776                 dataLog(&quot;CASing and returning.\n&quot;);
1777             ParkingLot::unparkAll(&amp;m_worldState);
1778             return;
1779         }
1780     }
1781 }
1782 
1783 void Heap::stopIfNecessarySlow()
1784 {
1785     if (validateDFGDoesGC)
1786         RELEASE_ASSERT(expectDoesGC());
1787 
1788     while (stopIfNecessarySlow(m_worldState.load())) { }
1789 
1790     RELEASE_ASSERT(m_worldState.load() &amp; hasAccessBit);
1791     RELEASE_ASSERT(!(m_worldState.load() &amp; stoppedBit));
1792 
1793     handleGCDidJIT();
1794     handleNeedFinalize();
1795     m_mutatorDidRun = true;
1796 }
1797 
1798 bool Heap::stopIfNecessarySlow(unsigned oldState)
1799 {
1800     if (validateDFGDoesGC)
1801         RELEASE_ASSERT(expectDoesGC());
1802 
1803     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1804     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1805 
1806     // It&#39;s possible for us to wake up with finalization already requested but the world not yet
1807     // resumed. If that happens, we can&#39;t run finalization yet.
1808     if (handleNeedFinalize(oldState))
1809         return true;
1810 
1811     // FIXME: When entering the concurrent phase, we could arrange for this branch not to fire, and then
1812     // have the SlotVisitor do things to the m_worldState to make this branch fire again. That would
1813     // prevent us from polling this so much. Ideally, stopIfNecessary would ignore the mutatorHasConnBit
1814     // and there would be some other bit indicating whether we were in some GC phase other than the
1815     // NotRunning or Concurrent ones.
1816     if (oldState &amp; mutatorHasConnBit)
1817         collectInMutatorThread();
1818 
1819     return false;
1820 }
1821 
1822 NEVER_INLINE void Heap::collectInMutatorThread()
1823 {
1824     CollectingScope collectingScope(*this);
1825     for (;;) {
1826         RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Mutator, nullptr);
1827         switch (result) {
1828         case RunCurrentPhaseResult::Finished:
1829             return;
1830         case RunCurrentPhaseResult::Continue:
1831             break;
1832         case RunCurrentPhaseResult::NeedCurrentThreadState:
1833             sanitizeStackForVM(m_vm);
1834             auto lambda = [&amp;] (CurrentThreadState&amp; state) {
1835                 for (;;) {
1836                     RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Mutator, &amp;state);
1837                     switch (result) {
1838                     case RunCurrentPhaseResult::Finished:
1839                         return;
1840                     case RunCurrentPhaseResult::Continue:
1841                         break;
1842                     case RunCurrentPhaseResult::NeedCurrentThreadState:
1843                         RELEASE_ASSERT_NOT_REACHED();
1844                         break;
1845                     }
1846                 }
1847             };
1848             callWithCurrentThreadState(scopedLambda&lt;void(CurrentThreadState&amp;)&gt;(WTFMove(lambda)));
1849             return;
1850         }
1851     }
1852 }
1853 
1854 template&lt;typename Func&gt;
1855 void Heap::waitForCollector(const Func&amp; func)
1856 {
1857     for (;;) {
1858         bool done;
1859         {
1860             LockHolder locker(*m_threadLock);
1861             done = func(locker);
1862             if (!done) {
1863                 setMutatorWaiting();
1864 
1865                 // At this point, the collector knows that we intend to wait, and he will clear the
1866                 // waiting bit and then unparkAll when the GC cycle finishes. Clearing the bit
1867                 // prevents us from parking except if there is also stop-the-world. Unparking after
1868                 // clearing means that if the clearing happens after we park, then we will unpark.
1869             }
1870         }
1871 
1872         // If we&#39;re in a stop-the-world scenario, we need to wait for that even if done is true.
1873         unsigned oldState = m_worldState.load();
1874         if (stopIfNecessarySlow(oldState))
1875             continue;
1876 
1877         // FIXME: We wouldn&#39;t need this if stopIfNecessarySlow() had a mode where it knew to just
1878         // do the collection.
1879         relinquishConn();
1880 
1881         if (done) {
1882             clearMutatorWaiting(); // Clean up just in case.
1883             return;
1884         }
1885 
1886         // If mutatorWaitingBit is still set then we want to wait.
1887         ParkingLot::compareAndPark(&amp;m_worldState, oldState | mutatorWaitingBit);
1888     }
1889 }
1890 
1891 void Heap::acquireAccessSlow()
1892 {
1893     for (;;) {
1894         unsigned oldState = m_worldState.load();
1895         RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1896 
1897         if (oldState &amp; stoppedBit) {
1898             if (verboseStop) {
1899                 dataLog(&quot;Stopping in acquireAccess!\n&quot;);
1900                 WTFReportBacktrace();
1901             }
1902             // Wait until we&#39;re not stopped anymore.
1903             ParkingLot::compareAndPark(&amp;m_worldState, oldState);
1904             continue;
1905         }
1906 
1907         RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1908         unsigned newState = oldState | hasAccessBit;
1909         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1910             handleGCDidJIT();
1911             handleNeedFinalize();
1912             m_mutatorDidRun = true;
1913             stopIfNecessary();
1914             return;
1915         }
1916     }
1917 }
1918 
1919 void Heap::releaseAccessSlow()
1920 {
1921     for (;;) {
1922         unsigned oldState = m_worldState.load();
1923         if (!(oldState &amp; hasAccessBit)) {
1924             dataLog(&quot;FATAL: Attempting to release access but the mutator does not have access.\n&quot;);
1925             RELEASE_ASSERT_NOT_REACHED();
1926         }
1927         if (oldState &amp; stoppedBit) {
1928             dataLog(&quot;FATAL: Attempting to release access but the mutator is stopped.\n&quot;);
1929             RELEASE_ASSERT_NOT_REACHED();
1930         }
1931 
1932         if (handleNeedFinalize(oldState))
1933             continue;
1934 
1935         unsigned newState = oldState &amp; ~(hasAccessBit | mutatorHasConnBit);
1936 
1937         if ((oldState &amp; mutatorHasConnBit)
1938             &amp;&amp; m_nextPhase != m_currentPhase) {
1939             // This means that the collector thread had given us the conn so that we would do something
1940             // for it. Stop ourselves as we release access. This ensures that acquireAccess blocks. In
1941             // the meantime, since we&#39;re handing the conn over, the collector will be awoken and it is
1942             // sure to have work to do.
1943             newState |= stoppedBit;
1944         }
1945 
1946         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1947             if (oldState &amp; mutatorHasConnBit)
1948                 finishRelinquishingConn();
1949             return;
1950         }
1951     }
1952 }
1953 
1954 bool Heap::relinquishConn(unsigned oldState)
1955 {
1956     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1957     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1958 
1959     if (!(oldState &amp; mutatorHasConnBit))
1960         return false; // Done.
1961 
1962     if (m_threadShouldStop)
1963         return false;
1964 
1965     if (!m_worldState.compareExchangeWeak(oldState, oldState &amp; ~mutatorHasConnBit))
1966         return true; // Loop around.
1967 
1968     finishRelinquishingConn();
1969     return true;
1970 }
1971 
1972 void Heap::finishRelinquishingConn()
1973 {
1974     if (false)
1975         dataLog(&quot;Relinquished the conn.\n&quot;);
1976 
1977     sanitizeStackForVM(m_vm);
1978 
1979     auto locker = holdLock(*m_threadLock);
1980     if (!m_requests.isEmpty())
1981         m_threadCondition-&gt;notifyOne(locker);
1982     ParkingLot::unparkAll(&amp;m_worldState);
1983 }
1984 
1985 void Heap::relinquishConn()
1986 {
1987     while (relinquishConn(m_worldState.load())) { }
1988 }
1989 
1990 bool Heap::handleGCDidJIT(unsigned oldState)
1991 {
1992     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1993     if (!(oldState &amp; gcDidJITBit))
1994         return false;
1995     if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~gcDidJITBit)) {
1996         WTF::crossModifyingCodeFence();
1997         return true;
1998     }
1999     return true;
2000 }
2001 
2002 NEVER_INLINE bool Heap::handleNeedFinalize(unsigned oldState)
2003 {
2004     RELEASE_ASSERT(oldState &amp; hasAccessBit);
2005     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
2006 
2007     if (!(oldState &amp; needFinalizeBit))
2008         return false;
2009     if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~needFinalizeBit)) {
2010         finalize();
2011         // Wake up anyone waiting for us to finalize. Note that they may have woken up already, in
2012         // which case they would be waiting for us to release heap access.
2013         ParkingLot::unparkAll(&amp;m_worldState);
2014         return true;
2015     }
2016     return true;
2017 }
2018 
2019 void Heap::handleGCDidJIT()
2020 {
2021     while (handleGCDidJIT(m_worldState.load())) { }
2022 }
2023 
2024 void Heap::handleNeedFinalize()
2025 {
2026     while (handleNeedFinalize(m_worldState.load())) { }
2027 }
2028 
2029 void Heap::setGCDidJIT()
2030 {
2031     m_worldState.transaction(
2032         [&amp;] (unsigned&amp; state) -&gt; bool {
2033             RELEASE_ASSERT(state &amp; stoppedBit);
2034             state |= gcDidJITBit;
2035             return true;
2036         });
2037 }
2038 
2039 void Heap::setNeedFinalize()
2040 {
2041     m_worldState.exchangeOr(needFinalizeBit);
2042     ParkingLot::unparkAll(&amp;m_worldState);
2043     m_stopIfNecessaryTimer-&gt;scheduleSoon();
2044 }
2045 
2046 void Heap::waitWhileNeedFinalize()
2047 {
2048     for (;;) {
2049         unsigned oldState = m_worldState.load();
2050         if (!(oldState &amp; needFinalizeBit)) {
2051             // This means that either there was no finalize request or the main thread will finalize
2052             // with heap access, so a subsequent call to stopTheWorld() will return only when
2053             // finalize finishes.
2054             return;
2055         }
2056         ParkingLot::compareAndPark(&amp;m_worldState, oldState);
2057     }
2058 }
2059 
2060 void Heap::setMutatorWaiting()
2061 {
2062     m_worldState.exchangeOr(mutatorWaitingBit);
2063 }
2064 
2065 void Heap::clearMutatorWaiting()
2066 {
2067     m_worldState.exchangeAnd(~mutatorWaitingBit);
2068 }
2069 
2070 void Heap::notifyThreadStopping(const AbstractLocker&amp;)
2071 {
2072     m_threadIsStopping = true;
2073     clearMutatorWaiting();
2074     ParkingLot::unparkAll(&amp;m_worldState);
2075 }
2076 
2077 void Heap::finalize()
2078 {
2079     MonotonicTime before;
2080     if (UNLIKELY(Options::logGC())) {
2081         before = MonotonicTime::now();
2082         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2083     }
2084 
2085     {
2086         SweepingScope sweepingScope(*this);
2087         deleteUnmarkedCompiledCode();
2088         deleteSourceProviderCaches();
2089         sweepInFinalize();
2090     }
2091 
2092     if (HasOwnPropertyCache* cache = vm().hasOwnPropertyCache())
2093         cache-&gt;clear();
2094 
2095     immutableButterflyToStringCache.clear();
2096 
2097     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
2098         callback.run(vm());
2099 
2100     if (shouldSweepSynchronously())
2101         sweepSynchronously();
2102 
2103     if (UNLIKELY(Options::logGC())) {
2104         MonotonicTime after = MonotonicTime::now();
2105         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2106     }
2107 }
2108 
2109 Heap::Ticket Heap::requestCollection(GCRequest request)
2110 {
2111     stopIfNecessary();
2112 
2113     ASSERT(vm().currentThreadIsHoldingAPILock());
2114     RELEASE_ASSERT(vm().atomStringTable() == Thread::current().atomStringTable());
2115 
2116     LockHolder locker(*m_threadLock);
2117     // We may be able to steal the conn. That only works if the collector is definitely not running
2118     // right now. This is an optimization that prevents the collector thread from ever starting in most
2119     // cases.
2120     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2121     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; !m_collectorThreadIsRunning) {
2122         if (false)
2123             dataLog(&quot;Taking the conn.\n&quot;);
2124         m_worldState.exchangeOr(mutatorHasConnBit);
2125     }
2126 
2127     m_requests.append(request);
2128     m_lastGrantedTicket++;
2129     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2130         m_threadCondition-&gt;notifyOne(locker);
2131     return m_lastGrantedTicket;
2132 }
2133 
2134 void Heap::waitForCollection(Ticket ticket)
2135 {
2136     waitForCollector(
2137         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2138             return m_lastServedTicket &gt;= ticket;
2139         });
2140 }
2141 
2142 void Heap::sweepInFinalize()
2143 {
2144     m_objectSpace.sweepPreciseAllocations();
2145 #if ENABLE(WEBASSEMBLY)
2146     // We hold onto a lot of memory, so it makes a lot of sense to be swept eagerly.
2147     if (vm().m_webAssemblyMemorySpace)
2148         vm().m_webAssemblyMemorySpace-&gt;sweep();
2149 #endif
2150 }
2151 
2152 void Heap::suspendCompilerThreads()
2153 {
2154 #if ENABLE(DFG_JIT)
2155     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2156     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2157     // the worklists use AutomaticThreads anyway.
2158     if (!VM::canUseJIT())
2159         return;
2160     for (unsigned i = DFG::numberOfWorklists(); i--;)
2161         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2162 #endif
2163 }
2164 
2165 void Heap::willStartCollection()
2166 {
2167     dataLogIf(Options::logGC(), &quot;=&gt; &quot;);
2168 
2169     if (shouldDoFullCollection()) {
2170         m_collectionScope = CollectionScope::Full;
2171         m_shouldDoFullCollection = false;
2172         dataLogIf(Options::logGC(), &quot;FullCollection, &quot;);
2173     } else {
2174         m_collectionScope = CollectionScope::Eden;
2175         dataLogIf(Options::logGC(), &quot;EdenCollection, &quot;);
2176     }
2177     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2178         m_sizeBeforeLastFullCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2179         m_extraMemorySize = 0;
2180         m_deprecatedExtraMemorySize = 0;
2181 #if ENABLE(RESOURCE_USAGE)
2182         m_externalMemorySize = 0;
2183 #endif
2184 
2185         if (m_fullActivityCallback)
2186             m_fullActivityCallback-&gt;willCollect();
2187     } else {
2188         ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Eden);
2189         m_sizeBeforeLastEdenCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2190     }
2191 
2192     if (m_edenActivityCallback)
2193         m_edenActivityCallback-&gt;willCollect();
2194 
2195     for (auto* observer : m_observers)
2196         observer-&gt;willGarbageCollect();
2197 }
2198 
2199 void Heap::prepareForMarking()
2200 {
2201     m_objectSpace.prepareForMarking();
2202 }
2203 
2204 void Heap::reapWeakHandles()
2205 {
2206     m_objectSpace.reapWeakSets();
2207 }
2208 
2209 void Heap::pruneStaleEntriesFromWeakGCMaps()
2210 {
2211     if (!m_collectionScope || m_collectionScope.value() != CollectionScope::Full)
2212         return;
2213     for (WeakGCMapBase* weakGCMap : m_weakGCMaps)
2214         weakGCMap-&gt;pruneStaleEntries();
2215 }
2216 
2217 void Heap::sweepArrayBuffers()
2218 {
2219     m_arrayBuffers.sweep(vm());
2220 }
2221 
2222 void Heap::snapshotUnswept()
2223 {
2224     TimingScope timingScope(*this, &quot;Heap::snapshotUnswept&quot;);
2225     m_objectSpace.snapshotUnswept();
2226 }
2227 
2228 void Heap::deleteSourceProviderCaches()
2229 {
2230     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
2231         m_vm.clearSourceProviderCaches();
2232 }
2233 
2234 void Heap::notifyIncrementalSweeper()
2235 {
2236     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2237         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2238             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2239     }
2240 
2241     m_sweeper-&gt;startSweeping(*this);
2242 }
2243 
2244 void Heap::updateAllocationLimits()
2245 {
2246     static constexpr bool verbose = false;
2247 
2248     if (verbose) {
2249         dataLog(&quot;\n&quot;);
2250         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2251     }
2252 
2253     // Calculate our current heap size threshold for the purpose of figuring out when we should
2254     // run another collection. This isn&#39;t the same as either size() or capacity(), though it should
2255     // be somewhere between the two. The key is to match the size calculations involved calls to
2256     // didAllocate(), while never dangerously underestimating capacity(). In extreme cases of
2257     // fragmentation, we may have size() much smaller than capacity().
2258     size_t currentHeapSize = 0;
2259 
2260     // For marked space, we use the total number of bytes visited. This matches the logic for
2261     // BlockDirectory&#39;s calls to didAllocate(), which effectively accounts for the total size of
2262     // objects allocated rather than blocks used. This will underestimate capacity(), and in case
2263     // of fragmentation, this may be substantial. Fortunately, marked space rarely fragments because
2264     // cells usually have a narrow range of sizes. So, the underestimation is probably OK.
2265     currentHeapSize += m_totalBytesVisited;
2266     if (verbose)
2267         dataLog(&quot;totalBytesVisited = &quot;, m_totalBytesVisited, &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2268 
2269     // It&#39;s up to the user to ensure that extraMemorySize() ends up corresponding to allocation-time
2270     // extra memory reporting.
2271     currentHeapSize += extraMemorySize();
2272     if (ASSERT_ENABLED) {
2273         Checked&lt;size_t, RecordOverflow&gt; checkedCurrentHeapSize = m_totalBytesVisited;
2274         checkedCurrentHeapSize += extraMemorySize();
2275         ASSERT(!checkedCurrentHeapSize.hasOverflowed() &amp;&amp; checkedCurrentHeapSize.unsafeGet() == currentHeapSize);
2276     }
2277 
2278     if (verbose)
2279         dataLog(&quot;extraMemorySize() = &quot;, extraMemorySize(), &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2280 
2281     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2282         // To avoid pathological GC churn in very small and very large heaps, we set
2283         // the new allocation limit based on the current size of the heap, with a
2284         // fixed minimum.
2285         m_maxHeapSize = std::max(minHeapSize(m_heapType, m_ramSize), proportionalHeapSize(currentHeapSize, m_ramSize));
2286         if (verbose)
2287             dataLog(&quot;Full: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2288         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2289         if (verbose)
2290             dataLog(&quot;Full: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2291         m_sizeAfterLastFullCollect = currentHeapSize;
2292         if (verbose)
2293             dataLog(&quot;Full: sizeAfterLastFullCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2294         m_bytesAbandonedSinceLastFullCollect = 0;
2295         if (verbose)
2296             dataLog(&quot;Full: bytesAbandonedSinceLastFullCollect = &quot;, 0, &quot;\n&quot;);
2297     } else {
2298         ASSERT(currentHeapSize &gt;= m_sizeAfterLastCollect);
2299         // Theoretically, we shouldn&#39;t ever scan more memory than the heap size we planned to have.
2300         // But we are sloppy, so we have to defend against the overflow.
2301         m_maxEdenSize = currentHeapSize &gt; m_maxHeapSize ? 0 : m_maxHeapSize - currentHeapSize;
2302         if (verbose)
2303             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2304         m_sizeAfterLastEdenCollect = currentHeapSize;
2305         if (verbose)
2306             dataLog(&quot;Eden: sizeAfterLastEdenCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2307         double edenToOldGenerationRatio = (double)m_maxEdenSize / (double)m_maxHeapSize;
2308         double minEdenToOldGenerationRatio = 1.0 / 3.0;
2309         if (edenToOldGenerationRatio &lt; minEdenToOldGenerationRatio)
2310             m_shouldDoFullCollection = true;
2311         // This seems suspect at first, but what it does is ensure that the nursery size is fixed.
2312         m_maxHeapSize += currentHeapSize - m_sizeAfterLastCollect;
2313         if (verbose)
2314             dataLog(&quot;Eden: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2315         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2316         if (verbose)
2317             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2318         if (m_fullActivityCallback) {
2319             ASSERT(currentHeapSize &gt;= m_sizeAfterLastFullCollect);
2320             m_fullActivityCallback-&gt;didAllocate(*this, currentHeapSize - m_sizeAfterLastFullCollect);
2321         }
2322     }
2323 
2324 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)
2325     // Get critical memory threshold for next cycle.
2326     overCriticalMemoryThreshold(MemoryThresholdCallType::Direct);
2327 #endif
2328 
2329     m_sizeAfterLastCollect = currentHeapSize;
2330     if (verbose)
2331         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2332     m_bytesAllocatedThisCycle = 0;
2333 
2334     dataLogIf(Options::logGC(), &quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);
2335 }
2336 
2337 void Heap::didFinishCollection()
2338 {
2339     m_afterGC = MonotonicTime::now();
2340     CollectionScope scope = *m_collectionScope;
2341     if (scope == CollectionScope::Full)
2342         m_lastFullGCLength = m_afterGC - m_beforeGC;
2343     else
2344         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2345 
2346 #if ENABLE(RESOURCE_USAGE)
2347     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2348 #endif
2349 
2350     if (HeapProfiler* heapProfiler = m_vm.heapProfiler()) {
2351         gatherExtraHeapData(*heapProfiler);
2352         removeDeadHeapSnapshotNodes(*heapProfiler);
2353     }
2354 
2355     if (UNLIKELY(m_verifier))
2356         m_verifier-&gt;endGC();
2357 
2358     RELEASE_ASSERT(m_collectionScope);
2359     m_lastCollectionScope = m_collectionScope;
2360     m_collectionScope = WTF::nullopt;
2361 
2362     for (auto* observer : m_observers)
2363         observer-&gt;didGarbageCollect(scope);
2364 }
2365 
2366 void Heap::resumeCompilerThreads()
2367 {
2368 #if ENABLE(DFG_JIT)
2369     if (!VM::canUseJIT())
2370         return;
2371     for (unsigned i = DFG::numberOfWorklists(); i--;)
2372         DFG::existingWorklistForIndex(i).resumeAllThreads();
2373 #endif
2374 }
2375 
2376 GCActivityCallback* Heap::fullActivityCallback()
2377 {
2378     return m_fullActivityCallback.get();
2379 }
2380 
2381 GCActivityCallback* Heap::edenActivityCallback()
2382 {
2383     return m_edenActivityCallback.get();
2384 }
2385 
2386 IncrementalSweeper&amp; Heap::sweeper()
2387 {
2388     return m_sweeper.get();
2389 }
2390 
2391 void Heap::setGarbageCollectionTimerEnabled(bool enable)
2392 {
2393     if (m_fullActivityCallback)
2394         m_fullActivityCallback-&gt;setEnabled(enable);
2395     if (m_edenActivityCallback)
2396         m_edenActivityCallback-&gt;setEnabled(enable);
2397 }
2398 
2399 void Heap::didAllocate(size_t bytes)
2400 {
2401     if (m_edenActivityCallback)
2402         m_edenActivityCallback-&gt;didAllocate(*this, m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
2403     m_bytesAllocatedThisCycle += bytes;
2404     performIncrement(bytes);
2405 }
2406 
2407 bool Heap::isValidAllocation(size_t)
2408 {
2409     if (!isValidThreadState(m_vm))
2410         return false;
2411 
2412     if (isCurrentThreadBusy())
2413         return false;
2414 
2415     return true;
2416 }
2417 
2418 void Heap::addFinalizer(JSCell* cell, CFinalizer finalizer)
2419 {
2420     WeakSet::allocate(cell, &amp;m_cFinalizerOwner, bitwise_cast&lt;void*&gt;(finalizer)); // Balanced by CFinalizerOwner::finalize().
2421 }
2422 
2423 void Heap::addFinalizer(JSCell* cell, LambdaFinalizer function)
2424 {
2425     WeakSet::allocate(cell, &amp;m_lambdaFinalizerOwner, function.leakImpl()); // Balanced by LambdaFinalizerOwner::finalize().
2426 }
2427 
2428 void Heap::CFinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)
2429 {
2430     HandleSlot slot = handle.slot();
2431     CFinalizer finalizer = bitwise_cast&lt;CFinalizer&gt;(context);
2432     finalizer(slot-&gt;asCell());
2433     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
2434 }
2435 
2436 void Heap::LambdaFinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)
2437 {
2438     LambdaFinalizer::Impl* impl = bitwise_cast&lt;LambdaFinalizer::Impl*&gt;(context);
2439     LambdaFinalizer finalizer(impl);
2440     HandleSlot slot = handle.slot();
2441     finalizer(slot-&gt;asCell());
2442     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
2443 }
2444 
2445 void Heap::collectNowFullIfNotDoneRecently(Synchronousness synchronousness)
2446 {
2447     if (!m_fullActivityCallback) {
2448         collectNow(synchronousness, CollectionScope::Full);
2449         return;
2450     }
2451 
2452     if (m_fullActivityCallback-&gt;didGCRecently()) {
2453         // A synchronous GC was already requested recently so we merely accelerate next collection.
2454         reportAbandonedObjectGraph();
2455         return;
2456     }
2457 
2458     m_fullActivityCallback-&gt;setDidGCRecently();
2459     collectNow(synchronousness, CollectionScope::Full);
2460 }
2461 
2462 bool Heap::useGenerationalGC()
2463 {
2464     return Options::useGenerationalGC() &amp;&amp; !VM::isInMiniMode();
2465 }
2466 
2467 bool Heap::shouldSweepSynchronously()
2468 {
2469     return Options::sweepSynchronously() || VM::isInMiniMode();
2470 }
2471 
2472 bool Heap::shouldDoFullCollection()
2473 {
2474     if (!useGenerationalGC())
2475         return true;
2476 
2477     if (!m_currentRequest.scope)
2478         return m_shouldDoFullCollection || overCriticalMemoryThreshold();
2479     return *m_currentRequest.scope == CollectionScope::Full;
2480 }
2481 
2482 void Heap::addLogicallyEmptyWeakBlock(WeakBlock* block)
2483 {
2484     m_logicallyEmptyWeakBlocks.append(block);
2485 }
2486 
2487 void Heap::sweepAllLogicallyEmptyWeakBlocks()
2488 {
2489     if (m_logicallyEmptyWeakBlocks.isEmpty())
2490         return;
2491 
2492     m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2493     while (sweepNextLogicallyEmptyWeakBlock()) { }
2494 }
2495 
2496 bool Heap::sweepNextLogicallyEmptyWeakBlock()
2497 {
2498     if (m_indexOfNextLogicallyEmptyWeakBlockToSweep == WTF::notFound)
2499         return false;
2500 
2501     WeakBlock* block = m_logicallyEmptyWeakBlocks[m_indexOfNextLogicallyEmptyWeakBlockToSweep];
2502 
2503     block-&gt;sweep();
2504     if (block-&gt;isEmpty()) {
2505         std::swap(m_logicallyEmptyWeakBlocks[m_indexOfNextLogicallyEmptyWeakBlockToSweep], m_logicallyEmptyWeakBlocks.last());
2506         m_logicallyEmptyWeakBlocks.removeLast();
2507         WeakBlock::destroy(*this, block);
2508     } else
2509         m_indexOfNextLogicallyEmptyWeakBlockToSweep++;
2510 
2511     if (m_indexOfNextLogicallyEmptyWeakBlockToSweep &gt;= m_logicallyEmptyWeakBlocks.size()) {
2512         m_indexOfNextLogicallyEmptyWeakBlockToSweep = WTF::notFound;
2513         return false;
2514     }
2515 
2516     return true;
2517 }
2518 
2519 size_t Heap::visitCount()
2520 {
2521     size_t result = 0;
2522     forEachSlotVisitor(
2523         [&amp;] (SlotVisitor&amp; visitor) {
2524             result += visitor.visitCount();
2525         });
2526     return result;
2527 }
2528 
2529 size_t Heap::bytesVisited()
2530 {
2531     size_t result = 0;
2532     forEachSlotVisitor(
2533         [&amp;] (SlotVisitor&amp; visitor) {
2534             result += visitor.bytesVisited();
2535         });
2536     return result;
2537 }
2538 
2539 void Heap::forEachCodeBlockImpl(const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2540 {
2541     // We don&#39;t know the full set of CodeBlocks until compilation has terminated.
2542     completeAllJITPlans();
2543 
2544     return m_codeBlocks-&gt;iterate(func);
2545 }
2546 
2547 void Heap::forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; locker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2548 {
2549     return m_codeBlocks-&gt;iterate(locker, func);
2550 }
2551 
2552 void Heap::writeBarrierSlowPath(const JSCell* from)
2553 {
2554     if (UNLIKELY(mutatorShouldBeFenced())) {
2555         // In this case, the barrierThreshold is the tautological threshold, so from could still be
2556         // not black. But we can&#39;t know for sure until we fire off a fence.
2557         WTF::storeLoadFence();
2558         if (from-&gt;cellState() != CellState::PossiblyBlack)
2559             return;
2560     }
2561 
2562     addToRememberedSet(from);
2563 }
2564 
2565 bool Heap::isCurrentThreadBusy()
2566 {
2567     return Thread::mayBeGCThread() || mutatorState() != MutatorState::Running;
2568 }
2569 
2570 void Heap::reportExtraMemoryVisited(size_t size)
2571 {
2572     size_t* counter = &amp;m_extraMemorySize;
2573 
2574     for (;;) {
2575         size_t oldSize = *counter;
2576         // FIXME: Change this to use SaturatedArithmetic when available.
2577         // https://bugs.webkit.org/show_bug.cgi?id=170411
2578         Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = oldSize;
2579         checkedNewSize += size;
2580         size_t newSize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
2581         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, newSize))
2582             return;
2583     }
2584 }
2585 
2586 #if ENABLE(RESOURCE_USAGE)
2587 void Heap::reportExternalMemoryVisited(size_t size)
2588 {
2589     size_t* counter = &amp;m_externalMemorySize;
2590 
2591     for (;;) {
2592         size_t oldSize = *counter;
2593         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, oldSize + size))
2594             return;
2595     }
2596 }
2597 #endif
2598 
2599 void Heap::collectIfNecessaryOrDefer(GCDeferralContext* deferralContext)
2600 {
2601     ASSERT(deferralContext || isDeferred() || !DisallowGC::isInEffectOnCurrentThread());
2602     if (validateDFGDoesGC)
2603         RELEASE_ASSERT(expectDoesGC());
2604 
2605     if (!m_isSafeToCollect)
2606         return;
2607 
2608     switch (mutatorState()) {
2609     case MutatorState::Running:
2610     case MutatorState::Allocating:
2611         break;
2612     case MutatorState::Sweeping:
2613     case MutatorState::Collecting:
2614         return;
2615     }
2616     if (!Options::useGC())
2617         return;
2618 
2619     if (mayNeedToStop()) {
2620         if (deferralContext)
2621             deferralContext-&gt;m_shouldGC = true;
2622         else if (isDeferred())
2623             m_didDeferGCWork = true;
2624         else
2625             stopIfNecessary();
2626     }
2627 
2628     if (UNLIKELY(Options::gcMaxHeapSize())) {
2629         if (m_bytesAllocatedThisCycle &lt;= Options::gcMaxHeapSize())
2630             return;
2631     } else {
2632         size_t bytesAllowedThisCycle = m_maxEdenSize;
2633 
2634 #if USE(BMALLOC_MEMORY_FOOTPRINT_API)
2635         if (overCriticalMemoryThreshold())
2636             bytesAllowedThisCycle = std::min(m_maxEdenSizeWhenCritical, bytesAllowedThisCycle);
2637 #endif
2638 
2639         if (m_bytesAllocatedThisCycle &lt;= bytesAllowedThisCycle)
2640             return;
2641     }
2642 
2643     if (deferralContext)
2644         deferralContext-&gt;m_shouldGC = true;
2645     else if (isDeferred())
2646         m_didDeferGCWork = true;
2647     else {
2648         collectAsync();
2649         stopIfNecessary(); // This will immediately start the collection if we have the conn.
2650     }
2651 }
2652 
2653 void Heap::decrementDeferralDepthAndGCIfNeededSlow()
2654 {
2655     // Can&#39;t do anything if we&#39;re still deferred.
2656     if (m_deferralDepth)
2657         return;
2658 
2659     ASSERT(!isDeferred());
2660 
2661     m_didDeferGCWork = false;
2662     // FIXME: Bring back something like the DeferGCProbability mode.
2663     // https://bugs.webkit.org/show_bug.cgi?id=166627
2664     collectIfNecessaryOrDefer();
2665 }
2666 
2667 void Heap::registerWeakGCMap(WeakGCMapBase* weakGCMap)
2668 {
2669     m_weakGCMaps.add(weakGCMap);
2670 }
2671 
2672 void Heap::unregisterWeakGCMap(WeakGCMapBase* weakGCMap)
2673 {
2674     m_weakGCMaps.remove(weakGCMap);
2675 }
2676 
2677 void Heap::didAllocateBlock(size_t capacity)
2678 {
2679 #if ENABLE(RESOURCE_USAGE)
2680     m_blockBytesAllocated += capacity;
2681 #else
2682     UNUSED_PARAM(capacity);
2683 #endif
2684 }
2685 
2686 void Heap::didFreeBlock(size_t capacity)
2687 {
2688 #if ENABLE(RESOURCE_USAGE)
2689     m_blockBytesAllocated -= capacity;
2690 #else
2691     UNUSED_PARAM(capacity);
2692 #endif
2693 }
2694 
2695 void Heap::addCoreConstraints()
2696 {
2697     m_constraintSet-&gt;add(
2698         &quot;Cs&quot;, &quot;Conservative Scan&quot;,
2699         [this, lastVersion = static_cast&lt;uint64_t&gt;(0)] (SlotVisitor&amp; slotVisitor) mutable {
2700             bool shouldNotProduceWork = lastVersion == m_phaseVersion;
2701             if (shouldNotProduceWork)
2702                 return;
2703 
2704             TimingScope preConvergenceTimingScope(*this, &quot;Constraint: conservative scan&quot;);
2705             m_objectSpace.prepareForConservativeScan();
2706             m_jitStubRoutines-&gt;prepareForConservativeScan();
2707 
2708             {
2709                 ConservativeRoots conservativeRoots(*this);
2710                 SuperSamplerScope superSamplerScope(false);
2711 
2712                 gatherStackRoots(conservativeRoots);
2713                 gatherJSStackRoots(conservativeRoots);
2714                 gatherScratchBufferRoots(conservativeRoots);
2715 
2716                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2717                 slotVisitor.append(conservativeRoots);
2718             }
2719             if (VM::canUseJIT()) {
2720                 // JITStubRoutines must be visited after scanning ConservativeRoots since JITStubRoutines depend on the hook executed during gathering ConservativeRoots.
2721                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::JITStubRoutines);
2722                 m_jitStubRoutines-&gt;traceMarkedStubRoutines(slotVisitor);
2723             }
2724 
2725             lastVersion = m_phaseVersion;
2726         },
2727         ConstraintVolatility::GreyedByExecution);
2728 
2729     m_constraintSet-&gt;add(
2730         &quot;Msr&quot;, &quot;Misc Small Roots&quot;,
2731         [this] (SlotVisitor&amp; slotVisitor) {
2732 
2733 #if JSC_OBJC_API_ENABLED
2734             scanExternalRememberedSet(m_vm, slotVisitor);
2735 #endif
2736             if (m_vm.smallStrings.needsToBeVisited(*m_collectionScope)) {
2737                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongReferences);
2738                 m_vm.smallStrings.visitStrongReferences(slotVisitor);
2739             }
2740 
2741             {
2742                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ProtectedValues);
2743                 for (auto&amp; pair : m_protectedValues)
2744                     slotVisitor.appendUnbarriered(pair.key);
2745             }
2746 
2747             if (m_markListSet &amp;&amp; m_markListSet-&gt;size()) {
2748                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2749                 MarkedArgumentBuffer::markLists(slotVisitor, *m_markListSet);
2750             }
2751 
2752             {
2753                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::VMExceptions);
2754                 slotVisitor.appendUnbarriered(m_vm.exception());
2755                 slotVisitor.appendUnbarriered(m_vm.lastException());
2756             }
2757         },
2758         ConstraintVolatility::GreyedByExecution);
2759 
2760     m_constraintSet-&gt;add(
2761         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2762         [this] (SlotVisitor&amp; slotVisitor) {
2763             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2764             m_handleSet.visitStrongHandles(slotVisitor);
2765         },
2766         ConstraintVolatility::GreyedByExecution);
2767 
2768     m_constraintSet-&gt;add(
2769         &quot;D&quot;, &quot;Debugger&quot;,
2770         [this] (SlotVisitor&amp; slotVisitor) {
2771             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2772 
2773 #if ENABLE(SAMPLING_PROFILER)
2774             if (SamplingProfiler* samplingProfiler = m_vm.samplingProfiler()) {
2775                 auto locker = holdLock(samplingProfiler-&gt;getLock());
2776                 samplingProfiler-&gt;processUnverifiedStackTraces(locker);
2777                 samplingProfiler-&gt;visit(slotVisitor);
2778                 if (Options::logGC() == GCLogging::Verbose)
2779                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2780             }
2781 #endif // ENABLE(SAMPLING_PROFILER)
2782 
2783             if (m_vm.typeProfiler())
2784                 m_vm.typeProfilerLog()-&gt;visit(slotVisitor);
2785 
2786             if (auto* shadowChicken = m_vm.shadowChicken())
2787                 shadowChicken-&gt;visitChildren(slotVisitor);
2788         },
2789         ConstraintVolatility::GreyedByExecution);
2790 
2791     m_constraintSet-&gt;add(
2792         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2793         [this] (SlotVisitor&amp; slotVisitor) {
2794             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2795             m_objectSpace.visitWeakSets(slotVisitor);
2796         },
2797         ConstraintVolatility::GreyedByMarking);
2798 
2799     m_constraintSet-&gt;add(
2800         &quot;O&quot;, &quot;Output&quot;,
2801         [] (SlotVisitor&amp; slotVisitor) {
2802             VM&amp; vm = slotVisitor.vm();
2803 
2804             auto callOutputConstraint = [] (SlotVisitor&amp; slotVisitor, HeapCell* heapCell, HeapCell::Kind) {
2805                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Output);
2806                 VM&amp; vm = slotVisitor.vm();
2807                 JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
2808                 cell-&gt;methodTable(vm)-&gt;visitOutputConstraints(cell, slotVisitor);
2809             };
2810 
2811             auto add = [&amp;] (auto&amp; set) {
2812                 slotVisitor.addParallelConstraintTask(set.forEachMarkedCellInParallel(callOutputConstraint));
2813             };
2814 
2815             add(vm.executableToCodeBlockEdgesWithConstraints);
2816             if (vm.m_weakMapSpace)
2817                 add(*vm.m_weakMapSpace);
2818         },
2819         ConstraintVolatility::GreyedByMarking,
2820         ConstraintParallelism::Parallel);
2821 
2822 #if ENABLE(DFG_JIT)
2823     if (VM::canUseJIT()) {
2824         m_constraintSet-&gt;add(
2825             &quot;Dw&quot;, &quot;DFG Worklists&quot;,
2826             [this] (SlotVisitor&amp; slotVisitor) {
2827                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::DFGWorkLists);
2828 
2829                 for (unsigned i = DFG::numberOfWorklists(); i--;)
2830                     DFG::existingWorklistForIndex(i).visitWeakReferences(slotVisitor);
2831 
2832                 // FIXME: This is almost certainly unnecessary.
2833                 // https://bugs.webkit.org/show_bug.cgi?id=166829
2834                 DFG::iterateCodeBlocksForGC(
2835                     m_vm,
2836                     [&amp;] (CodeBlock* codeBlock) {
2837                         slotVisitor.appendUnbarriered(codeBlock);
2838                     });
2839 
2840                 if (Options::logGC() == GCLogging::Verbose)
2841                     dataLog(&quot;DFG Worklists:\n&quot;, slotVisitor);
2842             },
2843             ConstraintVolatility::GreyedByMarking);
2844     }
2845 #endif
2846 
2847     m_constraintSet-&gt;add(
2848         &quot;Cb&quot;, &quot;CodeBlocks&quot;,
2849         [this] (SlotVisitor&amp; slotVisitor) {
2850             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::CodeBlocks);
2851             iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(
2852                 [&amp;] (CodeBlock* codeBlock) {
2853                     // Visit the CodeBlock as a constraint only if it&#39;s black.
2854                     if (isMarked(codeBlock)
2855                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2856                         slotVisitor.visitAsConstraint(codeBlock);
2857                 });
2858         },
2859         ConstraintVolatility::SeldomGreyed);
2860 
2861     m_constraintSet-&gt;add(makeUnique&lt;MarkStackMergingConstraint&gt;(*this));
2862 }
2863 
2864 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2865 {
2866     PreventCollectionScope preventCollectionScope(*this);
2867     m_constraintSet-&gt;add(WTFMove(constraint));
2868 }
2869 
2870 void Heap::notifyIsSafeToCollect()
2871 {
2872     MonotonicTime before;
2873     if (UNLIKELY(Options::logGC())) {
2874         before = MonotonicTime::now();
2875         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2876     }
2877 
2878     addCoreConstraints();
2879 
2880     m_isSafeToCollect = true;
2881 
2882     if (Options::collectContinuously()) {
2883         m_collectContinuouslyThread = Thread::create(
2884             &quot;JSC DEBUG Continuous GC&quot;,
2885             [this] () {
2886                 MonotonicTime initialTime = MonotonicTime::now();
2887                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2888                 while (!m_shouldStopCollectingContinuously) {
2889                     {
2890                         LockHolder locker(*m_threadLock);
2891                         if (m_requests.isEmpty()) {
2892                             m_requests.append(WTF::nullopt);
2893                             m_lastGrantedTicket++;
2894                             m_threadCondition-&gt;notifyOne(locker);
2895                         }
2896                     }
2897 
2898                     {
2899                         LockHolder locker(m_collectContinuouslyLock);
2900                         Seconds elapsed = MonotonicTime::now() - initialTime;
2901                         Seconds elapsedInPeriod = elapsed % period;
2902                         MonotonicTime timeToWakeUp =
2903                             initialTime + elapsed - elapsedInPeriod + period;
2904                         while (!hasElapsed(timeToWakeUp) &amp;&amp; !m_shouldStopCollectingContinuously) {
2905                             m_collectContinuouslyCondition.waitUntil(
2906                                 m_collectContinuouslyLock, timeToWakeUp);
2907                         }
2908                     }
2909                 }
2910             });
2911     }
2912 
2913     dataLogIf(Options::logGC(), (MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
2914 }
2915 
2916 void Heap::preventCollection()
2917 {
2918     if (!m_isSafeToCollect)
2919         return;
2920 
2921     // This prevents the collectContinuously thread from starting a collection.
2922     m_collectContinuouslyLock.lock();
2923 
2924     // Wait for all collections to finish.
2925     waitForCollector(
2926         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2927             ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2928             return m_lastServedTicket == m_lastGrantedTicket;
2929         });
2930 
2931     // Now a collection can only start if this thread starts it.
2932     RELEASE_ASSERT(!m_collectionScope);
2933 }
2934 
2935 void Heap::allowCollection()
2936 {
2937     if (!m_isSafeToCollect)
2938         return;
2939 
2940     m_collectContinuouslyLock.unlock();
2941 }
2942 
2943 void Heap::setMutatorShouldBeFenced(bool value)
2944 {
2945     m_mutatorShouldBeFenced = value;
2946     m_barrierThreshold = value ? tautologicalThreshold : blackThreshold;
2947 }
2948 
2949 void Heap::performIncrement(size_t bytes)
2950 {
2951     if (!m_objectSpace.isMarking())
2952         return;
2953 
2954     if (isDeferred())
2955         return;
2956 
2957     m_incrementBalance += bytes * Options::gcIncrementScale();
2958 
2959     // Save ourselves from crazy. Since this is an optimization, it&#39;s OK to go back to any consistent
2960     // state when the double goes wild.
2961     if (std::isnan(m_incrementBalance) || std::isinf(m_incrementBalance))
2962         m_incrementBalance = 0;
2963 
2964     if (m_incrementBalance &lt; static_cast&lt;double&gt;(Options::gcIncrementBytes()))
2965         return;
2966 
2967     double targetBytes = m_incrementBalance;
2968     if (targetBytes &lt;= 0)
2969         return;
2970     targetBytes = std::min(targetBytes, Options::gcIncrementMaxBytes());
2971 
2972     SlotVisitor&amp; slotVisitor = *m_mutatorSlotVisitor;
2973     ParallelModeEnabler parallelModeEnabler(slotVisitor);
2974     size_t bytesVisited = slotVisitor.performIncrementOfDraining(static_cast&lt;size_t&gt;(targetBytes));
2975     // incrementBalance may go negative here because it&#39;ll remember how many bytes we overshot.
2976     m_incrementBalance -= bytesVisited;
2977 }
2978 
2979 void Heap::addHeapFinalizerCallback(const HeapFinalizerCallback&amp; callback)
2980 {
2981     m_heapFinalizerCallbacks.append(callback);
2982 }
2983 
2984 void Heap::removeHeapFinalizerCallback(const HeapFinalizerCallback&amp; callback)
2985 {
2986     m_heapFinalizerCallbacks.removeFirst(callback);
2987 }
2988 
2989 void Heap::setBonusVisitorTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
2990 {
2991     auto locker = holdLock(m_markingMutex);
2992     m_bonusVisitorTask = task;
2993     m_markingConditionVariable.notifyAll();
2994 }
2995 
2996 void Heap::runTaskInParallel(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
2997 {
2998     unsigned initialRefCount = task-&gt;refCount();
2999     setBonusVisitorTask(task);
3000     task-&gt;run(*m_collectorSlotVisitor);
3001     setBonusVisitorTask(nullptr);
3002     // The constraint solver expects return of this function to imply termination of the task in all
3003     // threads. This ensures that property.
3004     {
3005         auto locker = holdLock(m_markingMutex);
3006         while (task-&gt;refCount() &gt; initialRefCount)
3007             m_markingConditionVariable.wait(m_markingMutex);
3008     }
3009 }
3010 
3011 } // namespace JSC
    </pre>
  </body>
</html>