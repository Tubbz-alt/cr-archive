<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirCode.h</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2015-2020 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #if ENABLE(B3_JIT)
 29 
 30 #include &quot;AirArg.h&quot;
 31 #include &quot;AirBasicBlock.h&quot;
 32 #include &quot;AirDisassembler.h&quot;
 33 #include &quot;AirSpecial.h&quot;
 34 #include &quot;AirStackSlot.h&quot;
 35 #include &quot;AirTmp.h&quot;
 36 #include &quot;B3SparseCollection.h&quot;
 37 #include &quot;GPRInfo.h&quot;
 38 #include &quot;MacroAssembler.h&quot;
 39 #include &quot;RegisterAtOffsetList.h&quot;
 40 #include &quot;StackAlignment.h&quot;
 41 #include &lt;wtf/HashSet.h&gt;
 42 #include &lt;wtf/IndexMap.h&gt;
 43 #include &lt;wtf/WeakRandom.h&gt;
 44 
 45 namespace JSC {
 46 
 47 class CCallHelpers;
 48 
 49 namespace B3 {
 50 
 51 class Procedure;
 52 
 53 #if !ASSERT_ENABLED
 54 IGNORE_RETURN_TYPE_WARNINGS_BEGIN
 55 #endif
 56 
 57 namespace Air {
 58 
 59 class GenerateAndAllocateRegisters;
 60 class BlockInsertionSet;
 61 class CCallSpecial;
 62 class CFG;
 63 class Code;
 64 class Disassembler;
 65 
 66 typedef void WasmBoundsCheckGeneratorFunction(CCallHelpers&amp;, GPRReg);
 67 typedef SharedTask&lt;WasmBoundsCheckGeneratorFunction&gt; WasmBoundsCheckGenerator;
 68 
 69 typedef void PrologueGeneratorFunction(CCallHelpers&amp;, Code&amp;);
 70 typedef SharedTask&lt;PrologueGeneratorFunction&gt; PrologueGenerator;
 71 
 72 extern const char* const tierName;
 73 
 74 // This is an IR that is very close to the bare metal. It requires about 40x more bytes than the
 75 // generated machine code - for example if you&#39;re generating 1MB of machine code, you need about
 76 // 40MB of Air.
 77 
 78 class Code {
 79     WTF_MAKE_NONCOPYABLE(Code);
 80     WTF_MAKE_FAST_ALLOCATED;
 81 public:
 82     ~Code();
 83 
 84     Procedure&amp; proc() { return m_proc; }
 85 
 86     const Vector&lt;Reg&gt;&amp; regsInPriorityOrder(Bank bank) const
 87     {
 88         switch (bank) {
 89         case GP:
 90             return m_gpRegsInPriorityOrder;
 91         case FP:
 92             return m_fpRegsInPriorityOrder;
 93         }
 94         ASSERT_NOT_REACHED();
 95     }
 96 
 97     // This is the set of registers that Air is allowed to emit code to mutate. It&#39;s derived from
 98     // regsInPriorityOrder. Any registers not in this set are said to be &quot;pinned&quot;.
 99     const RegisterSet&amp; mutableRegs() const { return m_mutableRegs; }
100 
101     bool isPinned(Reg reg) const { return !mutableRegs().get(reg); }
102     void pinRegister(Reg);
103 
104     void setOptLevel(unsigned optLevel) { m_optLevel = optLevel; }
105     unsigned optLevel() const { return m_optLevel; }
106 
107     bool needsUsedRegisters() const;
108 
109     JS_EXPORT_PRIVATE BasicBlock* addBlock(double frequency = 1);
110 
111     // Note that you can rely on stack slots always getting indices that are larger than the index
112     // of any prior stack slot. In fact, all stack slots you create in the future will have an index
113     // that is &gt;= stackSlots().size().
114     JS_EXPORT_PRIVATE StackSlot* addStackSlot(
115         unsigned byteSize, StackSlotKind, B3::StackSlot* = nullptr);
116     StackSlot* addStackSlot(B3::StackSlot*);
117 
118     JS_EXPORT_PRIVATE Special* addSpecial(std::unique_ptr&lt;Special&gt;);
119 
120     // This is the special you need to make a C call!
121     CCallSpecial* cCallSpecial();
122 
123     Tmp newTmp(Bank bank)
124     {
125         switch (bank) {
126         case GP:
127             return Tmp::gpTmpForIndex(m_numGPTmps++);
128         case FP:
129             return Tmp::fpTmpForIndex(m_numFPTmps++);
130         }
131         ASSERT_NOT_REACHED();
132     }
133 
134     unsigned numTmps(Bank bank)
135     {
136         switch (bank) {
137         case GP:
138             return m_numGPTmps;
139         case FP:
140             return m_numFPTmps;
141         }
142         ASSERT_NOT_REACHED();
143     }
144 
145     template&lt;typename Func&gt;
146     void forEachTmp(const Func&amp; func)
147     {
148         for (unsigned bankIndex = 0; bankIndex &lt; numBanks; ++bankIndex) {
149             Bank bank = static_cast&lt;Bank&gt;(bankIndex);
150             unsigned numTmps = this-&gt;numTmps(bank);
151             for (unsigned i = 0; i &lt; numTmps; ++i)
152                 func(Tmp::tmpForIndex(bank, i));
153         }
154     }
155 
156     unsigned callArgAreaSizeInBytes() const { return m_callArgAreaSize; }
157 
158     // You can call this before code generation to force a minimum call arg area size.
159     void requestCallArgAreaSizeInBytes(unsigned size)
160     {
161         m_callArgAreaSize = std::max(
162             m_callArgAreaSize,
163             static_cast&lt;unsigned&gt;(WTF::roundUpToMultipleOf(stackAlignmentBytes(), size)));
164     }
165 
166     unsigned frameSize() const { return m_frameSize; }
167 
168     // Only phases that do stack allocation are allowed to set this. Currently, only
169     // Air::allocateStack() does this.
170     void setFrameSize(unsigned frameSize)
171     {
172         m_frameSize = frameSize;
173     }
174 
175     // Note that this is not the same thing as proc().numEntrypoints(). This value here may be zero
176     // until we lower EntrySwitch.
177     unsigned numEntrypoints() const { return m_entrypoints.size(); }
178     const Vector&lt;FrequentedBlock&gt;&amp; entrypoints() const { return m_entrypoints; }
179     const FrequentedBlock&amp; entrypoint(unsigned index) const { return m_entrypoints[index]; }
180     bool isEntrypoint(BasicBlock*) const;
181     // Note: It is only valid to call this function after LowerEntrySwitch.
182     Optional&lt;unsigned&gt; entrypointIndex(BasicBlock*) const;
183 
184     // Note: We allow this to be called even before we set m_entrypoints just for convenience to users of this API.
185     // However, if you call this before setNumEntrypoints, setNumEntrypoints will overwrite this value.
186     void setPrologueForEntrypoint(unsigned entrypointIndex, Ref&lt;PrologueGenerator&gt;&amp;&amp; generator)
187     {
188         m_prologueGenerators[entrypointIndex] = WTFMove(generator);
189     }
190     const Ref&lt;PrologueGenerator&gt;&amp; prologueGeneratorForEntrypoint(unsigned entrypointIndex)
191     {
192         return m_prologueGenerators[entrypointIndex];
193     }
194 
195     void setNumEntrypoints(unsigned);
196 
197     // This is used by lowerEntrySwitch().
198     template&lt;typename Vector&gt;
199     void setEntrypoints(Vector&amp;&amp; vector)
200     {
201         m_entrypoints = std::forward&lt;Vector&gt;(vector);
202         RELEASE_ASSERT(m_entrypoints.size() == m_prologueGenerators.size());
203     }
204 
205     MacroAssembler::Label entrypointLabel(unsigned index) const
206     {
207         return m_entrypointLabels[index];
208     }
209 
210     // This is used by generate().
211     template&lt;typename Vector&gt;
212     void setEntrypointLabels(Vector&amp;&amp; vector)
213     {
214         m_entrypointLabels = std::forward&lt;Vector&gt;(vector);
215         RELEASE_ASSERT(m_entrypointLabels.size() == m_prologueGenerators.size());
216     }
217 
218     void setStackIsAllocated(bool value)
219     {
220         m_stackIsAllocated = value;
221     }
222 
223     bool stackIsAllocated() const { return m_stackIsAllocated; }
224 
225     // This sets the callee save registers.
226     void setCalleeSaveRegisterAtOffsetList(RegisterAtOffsetList&amp;&amp;, StackSlot*);
227 
228     // This returns the correctly offset list of callee save registers.
229     RegisterAtOffsetList calleeSaveRegisterAtOffsetList() const;
230 
231     // This just tells you what the callee saves are.
232     RegisterSet calleeSaveRegisters() const { return m_calleeSaveRegisters; }
233 
234     // Recomputes predecessors and deletes unreachable blocks.
235     JS_EXPORT_PRIVATE void resetReachability();
236 
237     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
238 
239     unsigned size() const { return m_blocks.size(); }
240     BasicBlock* at(unsigned index) const { return m_blocks[index].get(); }
241     BasicBlock* operator[](unsigned index) const { return at(index); }
242 
243     // This is used by phases that optimize the block list. You shouldn&#39;t use this unless you really know
244     // what you&#39;re doing.
245     Vector&lt;std::unique_ptr&lt;BasicBlock&gt;&gt;&amp; blockList() { return m_blocks; }
246 
247     // Finds the smallest index&#39; such that at(index&#39;) != null and index&#39; &gt;= index.
248     JS_EXPORT_PRIVATE unsigned findFirstBlockIndex(unsigned index) const;
249 
250     // Finds the smallest index&#39; such that at(index&#39;) != null and index&#39; &gt; index.
251     unsigned findNextBlockIndex(unsigned index) const;
252 
253     BasicBlock* findNextBlock(BasicBlock*) const;
254 
255     class iterator {
256     public:
257         iterator()
258             : m_code(nullptr)
259             , m_index(0)
260         {
261         }
262 
263         iterator(const Code&amp; code, unsigned index)
264             : m_code(&amp;code)
265             , m_index(m_code-&gt;findFirstBlockIndex(index))
266         {
267         }
268 
269         BasicBlock* operator*()
270         {
271             return m_code-&gt;at(m_index);
272         }
273 
274         iterator&amp; operator++()
275         {
276             m_index = m_code-&gt;findFirstBlockIndex(m_index + 1);
277             return *this;
278         }
279 
280         bool operator==(const iterator&amp; other) const
281         {
282             return m_index == other.m_index;
283         }
284 
285         bool operator!=(const iterator&amp; other) const
286         {
287             return !(*this == other);
288         }
289 
290     private:
291         const Code* m_code;
292         unsigned m_index;
293     };
294 
295     iterator begin() const { return iterator(*this, 0); }
296     iterator end() const { return iterator(*this, size()); }
297 
298     const SparseCollection&lt;StackSlot&gt;&amp; stackSlots() const { return m_stackSlots; }
299     SparseCollection&lt;StackSlot&gt;&amp; stackSlots() { return m_stackSlots; }
300 
301     const SparseCollection&lt;Special&gt;&amp; specials() const { return m_specials; }
302     SparseCollection&lt;Special&gt;&amp; specials() { return m_specials; }
303 
304     template&lt;typename Callback&gt;
305     void forAllTmps(const Callback&amp; callback) const
306     {
307         for (unsigned i = m_numGPTmps; i--;)
308             callback(Tmp::gpTmpForIndex(i));
309         for (unsigned i = m_numFPTmps; i--;)
310             callback(Tmp::fpTmpForIndex(i));
311     }
312 
313     void addFastTmp(Tmp);
314     bool isFastTmp(Tmp tmp) const { return m_fastTmps.contains(tmp); }
315 
316     CFG&amp; cfg() const { return *m_cfg; }
317 
318     void* addDataSection(size_t);
319 
320     // The name has to be a string literal, since we don&#39;t do any memory management for the string.
321     void setLastPhaseName(const char* name)
322     {
323         m_lastPhaseName = name;
324     }
325 
326     const char* lastPhaseName() const { return m_lastPhaseName; }
327 
328     void setWasmBoundsCheckGenerator(RefPtr&lt;WasmBoundsCheckGenerator&gt; generator)
329     {
330         m_wasmBoundsCheckGenerator = generator;
331     }
332 
333     RefPtr&lt;WasmBoundsCheckGenerator&gt; wasmBoundsCheckGenerator() const { return m_wasmBoundsCheckGenerator; }
334 
335     // This is a hash of the code. You can use this if you want to put code into a hashtable, but
336     // it&#39;s mainly for validating the results from JSAir.
337     unsigned jsHash() const;
338 
339     void setDisassembler(std::unique_ptr&lt;Disassembler&gt;&amp;&amp; disassembler) { m_disassembler = WTFMove(disassembler); }
340     Disassembler* disassembler() { return m_disassembler.get(); }
341 
342     RegisterSet mutableGPRs();
343     RegisterSet mutableFPRs();
344     RegisterSet pinnedRegisters() const { return m_pinnedRegs; }
345 
346     WeakRandom&amp; weakRandom() { return m_weakRandom; }
347 
348     void emitDefaultPrologue(CCallHelpers&amp;);
349 
350     std::unique_ptr&lt;GenerateAndAllocateRegisters&gt; m_generateAndAllocateRegisters;
351 
352 private:
353     friend class ::JSC::B3::Procedure;
354     friend class BlockInsertionSet;
355 
356     Code(Procedure&amp;);
357 
358     void setRegsInPriorityOrder(Bank, const Vector&lt;Reg&gt;&amp;);
359 
360     Vector&lt;Reg&gt;&amp; regsInPriorityOrderImpl(Bank bank)
361     {
362         switch (bank) {
363         case GP:
364             return m_gpRegsInPriorityOrder;
365         case FP:
366             return m_fpRegsInPriorityOrder;
367         }
368         ASSERT_NOT_REACHED();
369     }
370 
371     WeakRandom m_weakRandom;
372     Procedure&amp; m_proc; // Some meta-data, like byproducts, is stored in the Procedure.
373     Vector&lt;Reg&gt; m_gpRegsInPriorityOrder;
374     Vector&lt;Reg&gt; m_fpRegsInPriorityOrder;
375     RegisterSet m_mutableRegs;
376     RegisterSet m_pinnedRegs;
377     SparseCollection&lt;StackSlot&gt; m_stackSlots;
378     Vector&lt;std::unique_ptr&lt;BasicBlock&gt;&gt; m_blocks;
379     SparseCollection&lt;Special&gt; m_specials;
380     std::unique_ptr&lt;CFG&gt; m_cfg;
381     HashSet&lt;Tmp&gt; m_fastTmps;
382     CCallSpecial* m_cCallSpecial { nullptr };
383     unsigned m_numGPTmps { 0 };
384     unsigned m_numFPTmps { 0 };
385     unsigned m_frameSize { 0 };
386     unsigned m_callArgAreaSize { 0 };
387     bool m_stackIsAllocated { false };
388     RegisterAtOffsetList m_uncorrectedCalleeSaveRegisterAtOffsetList;
389     RegisterSet m_calleeSaveRegisters;
390     StackSlot* m_calleeSaveStackSlot { nullptr };
391     Vector&lt;FrequentedBlock&gt; m_entrypoints; // This is empty until after lowerEntrySwitch().
392     Vector&lt;MacroAssembler::Label&gt; m_entrypointLabels; // This is empty until code generation.
393     Vector&lt;Ref&lt;PrologueGenerator&gt;, 1&gt; m_prologueGenerators;
394     RefPtr&lt;WasmBoundsCheckGenerator&gt; m_wasmBoundsCheckGenerator;
395     const char* m_lastPhaseName;
396     std::unique_ptr&lt;Disassembler&gt; m_disassembler;
397     unsigned m_optLevel { defaultOptLevel() };
398     Ref&lt;PrologueGenerator&gt; m_defaultPrologueGenerator;
399 };
400 
401 } } } // namespace JSC::B3::Air
402 
403 #if !ASSERT_ENABLED
404 IGNORE_RETURN_TYPE_WARNINGS_END
405 #endif
406 
407 #endif // ENABLE(B3_JIT)
    </pre>
  </body>
</html>