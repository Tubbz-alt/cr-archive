<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/IsoHeapImplInlines.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2017-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #pragma once
 27 
 28 #include &quot;IsoHeapImpl.h&quot;
 29 #include &quot;IsoTLSDeallocatorEntry.h&quot;
 30 #include &quot;IsoSharedHeapInlines.h&quot;
 31 #include &quot;IsoSharedPageInlines.h&quot;
 32 
 33 namespace bmalloc {
 34 
 35 template&lt;typename Config&gt;
 36 IsoHeapImpl&lt;Config&gt;::IsoHeapImpl()
 37     : IsoHeapImplBase((*PerProcess&lt;IsoTLSEntryHolder&lt;IsoTLSDeallocatorEntry&lt;Config&gt;&gt;&gt;::get())-&gt;lock)
 38     , m_inlineDirectory(*this)
 39     , m_allocator(*this)
 40 {
 41 }
 42 
 43 template&lt;typename Config&gt;
 44 EligibilityResult&lt;Config&gt; IsoHeapImpl&lt;Config&gt;::takeFirstEligible(const LockHolder&amp; locker)
 45 {
 46     if (m_isInlineDirectoryEligibleOrDecommitted) {
 47         EligibilityResult&lt;Config&gt; result = m_inlineDirectory.takeFirstEligible(locker);
 48         if (result.kind == EligibilityKind::Full)
 49             m_isInlineDirectoryEligibleOrDecommitted = false;
 50         else
 51             return result;
 52     }
 53 
 54     {
 55         auto* cursor = m_firstEligibleOrDecommitedDirectory.get();
 56         if (!cursor) {
 57             // If nothing is eligible, it can only be because we have no directories. It wouldn&#39;t be the end
 58             // of the world if we broke this invariant. It would only mean that didBecomeEligibleOrDecommited() would need
 59             // a null check.
 60             RELEASE_BASSERT(!m_headDirectory.get());
 61             RELEASE_BASSERT(!m_tailDirectory.get());
 62         } else {
 63             auto* originalCursor = cursor;
 64             BUNUSED(originalCursor);
 65             for (; cursor; cursor = cursor-&gt;next) {
 66                 EligibilityResult&lt;Config&gt; result = cursor-&gt;payload.takeFirstEligible(locker);
 67                 // While iterating, m_firstEligibleOrDecommitedDirectory is never changed. We are holding a lock,
 68                 // and IsoDirectory::takeFirstEligible must not populate a new eligibile / decommitted pages.
 69                 BASSERT(m_firstEligibleOrDecommitedDirectory.get() == originalCursor);
 70                 if (result.kind != EligibilityKind::Full) {
 71                     m_directoryHighWatermark = std::max(m_directoryHighWatermark, cursor-&gt;index());
 72                     m_firstEligibleOrDecommitedDirectory = cursor;
 73                     return result;
 74                 }
 75             }
 76             m_firstEligibleOrDecommitedDirectory = nullptr;
 77         }
 78     }
 79 
 80     auto* newDirectory = new IsoDirectoryPage&lt;Config&gt;(*this, m_nextDirectoryPageIndex++);
 81     if (m_headDirectory.get()) {
 82         m_tailDirectory-&gt;next = newDirectory;
 83         m_tailDirectory = newDirectory;
 84     } else {
 85         RELEASE_BASSERT(!m_tailDirectory.get());
 86         m_headDirectory = newDirectory;
 87         m_tailDirectory = newDirectory;
 88     }
 89     m_directoryHighWatermark = newDirectory-&gt;index();
 90     m_firstEligibleOrDecommitedDirectory = newDirectory;
 91     EligibilityResult&lt;Config&gt; result = newDirectory-&gt;payload.takeFirstEligible(locker);
 92     RELEASE_BASSERT(result.kind != EligibilityKind::Full);
 93     return result;
 94 }
 95 
 96 template&lt;typename Config&gt;
 97 void IsoHeapImpl&lt;Config&gt;::didBecomeEligibleOrDecommited(const LockHolder&amp;, IsoDirectory&lt;Config, numPagesInInlineDirectory&gt;* directory)
 98 {
 99     RELEASE_BASSERT(directory == &amp;m_inlineDirectory);
100     m_isInlineDirectoryEligibleOrDecommitted = true;
101 }
102 
103 template&lt;typename Config&gt;
104 void IsoHeapImpl&lt;Config&gt;::didBecomeEligibleOrDecommited(const LockHolder&amp;, IsoDirectory&lt;Config, IsoDirectoryPage&lt;Config&gt;::numPages&gt;* directory)
105 {
106     RELEASE_BASSERT(m_firstEligibleOrDecommitedDirectory);
107     auto* directoryPage = IsoDirectoryPage&lt;Config&gt;::pageFor(directory);
108     if (directoryPage-&gt;index() &lt; m_firstEligibleOrDecommitedDirectory-&gt;index())
109         m_firstEligibleOrDecommitedDirectory = directoryPage;
110 }
111 
112 template&lt;typename Config&gt;
113 void IsoHeapImpl&lt;Config&gt;::scavenge(Vector&lt;DeferredDecommit&gt;&amp; decommits)
114 {
115     LockHolder locker(this-&gt;lock);
116     forEachDirectory(
117         locker,
118         [&amp;] (auto&amp; directory) {
119             directory.scavenge(locker, decommits);
120         });
121     m_directoryHighWatermark = 0;
122 }
123 
124 #if BUSE(PARTIAL_SCAVENGE)
125 template&lt;typename Config&gt;
126 void IsoHeapImpl&lt;Config&gt;::scavengeToHighWatermark(Vector&lt;DeferredDecommit&gt;&amp; decommits)
127 {
128     LockHolder locker(this-&gt;lock);
129     if (!m_directoryHighWatermark)
130         m_inlineDirectory.scavengeToHighWatermark(locker, decommits);
131     for (IsoDirectoryPage&lt;Config&gt;* page = m_headDirectory.get(); page; page = page-&gt;next) {
132         if (page-&gt;index() &gt;= m_directoryHighWatermark)
133             page-&gt;payload.scavengeToHighWatermark(locker, decommits);
134     }
135     m_directoryHighWatermark = 0;
136 }
137 #endif
138 
139 inline size_t IsoHeapImplBase::freeableMemory()
140 {
141     return m_freeableMemory;
142 }
143 
144 template&lt;typename Config&gt;
145 unsigned IsoHeapImpl&lt;Config&gt;::allocatorOffset()
146 {
147     return m_allocator-&gt;offset();
148 }
149 
150 template&lt;typename Config&gt;
151 unsigned IsoHeapImpl&lt;Config&gt;::deallocatorOffset()
152 {
153     return (*PerProcess&lt;IsoTLSEntryHolder&lt;IsoTLSDeallocatorEntry&lt;Config&gt;&gt;&gt;::get())-&gt;offset();
154 }
155 
156 template&lt;typename Config&gt;
157 unsigned IsoHeapImpl&lt;Config&gt;::numLiveObjects()
158 {
159     LockHolder locker(this-&gt;lock);
160     unsigned result = 0;
161     forEachLiveObject(
162         locker,
163         [&amp;] (void*) {
164             result++;
165         });
166     return result;
167 }
168 
169 template&lt;typename Config&gt;
170 unsigned IsoHeapImpl&lt;Config&gt;::numCommittedPages()
171 {
172     LockHolder locker(this-&gt;lock);
173     unsigned result = 0;
174     forEachCommittedPage(
175         locker,
176         [&amp;] (IsoPage&lt;Config&gt;&amp;) {
177             result++;
178         });
179     return result;
180 }
181 
182 template&lt;typename Config&gt;
183 template&lt;typename Func&gt;
184 void IsoHeapImpl&lt;Config&gt;::forEachDirectory(const LockHolder&amp;, const Func&amp; func)
185 {
186     func(m_inlineDirectory);
187     for (IsoDirectoryPage&lt;Config&gt;* page = m_headDirectory.get(); page; page = page-&gt;next)
188         func(page-&gt;payload);
189 }
190 
191 template&lt;typename Config&gt;
192 template&lt;typename Func&gt;
193 void IsoHeapImpl&lt;Config&gt;::forEachCommittedPage(const LockHolder&amp; locker, const Func&amp; func)
194 {
195     forEachDirectory(
196         locker,
197         [&amp;] (auto&amp; directory) {
198             directory.forEachCommittedPage(locker, func);
199         });
200 }
201 
202 template&lt;typename Config&gt;
203 template&lt;typename Func&gt;
204 void IsoHeapImpl&lt;Config&gt;::forEachLiveObject(const LockHolder&amp; locker, const Func&amp; func)
205 {
206     forEachCommittedPage(
207         locker,
208         [&amp;] (IsoPage&lt;Config&gt;&amp; page) {
209             page.forEachLiveObject(locker, func);
210         });
211     for (unsigned index = 0; index &lt; maxAllocationFromShared; ++index) {
212         void* pointer = m_sharedCells[index].get();
213         if (pointer &amp;&amp; !(m_availableShared &amp; (1U &lt;&lt; index)))
214             func(pointer);
215     }
216 }
217 
218 inline size_t IsoHeapImplBase::footprint()
219 {
220 #if ENABLE_PHYSICAL_PAGE_MAP
221     RELEASE_BASSERT(m_footprint == m_physicalPageMap.footprint());
222 #endif
223     return m_footprint;
224 }
225 
226 inline void IsoHeapImplBase::didCommit(void* ptr, size_t bytes)
227 {
228     BUNUSED_PARAM(ptr);
229     m_footprint += bytes;
230 #if ENABLE_PHYSICAL_PAGE_MAP
231     m_physicalPageMap.commit(ptr, bytes);
232 #endif
233 }
234 
235 inline void IsoHeapImplBase::didDecommit(void* ptr, size_t bytes)
236 {
237     BUNUSED_PARAM(ptr);
238     m_footprint -= bytes;
239 #if ENABLE_PHYSICAL_PAGE_MAP
240     m_physicalPageMap.decommit(ptr, bytes);
241 #endif
242 }
243 
244 inline void IsoHeapImplBase::isNowFreeable(void* ptr, size_t bytes)
245 {
246     BUNUSED_PARAM(ptr);
247     m_freeableMemory += bytes;
248 }
249 
250 inline void IsoHeapImplBase::isNoLongerFreeable(void* ptr, size_t bytes)
251 {
252     BUNUSED_PARAM(ptr);
253     m_freeableMemory -= bytes;
254 }
255 
256 template&lt;typename Config&gt;
257 AllocationMode IsoHeapImpl&lt;Config&gt;::updateAllocationMode()
258 {
259     auto getNewAllocationMode = [&amp;] {
260         // Exhaust shared free cells, which means we should start activating the fast allocation mode for this type.
261         if (!m_availableShared) {
262             m_lastSlowPathTime = std::chrono::steady_clock::now();
263             return AllocationMode::Fast;
264         }
265 
266         switch (m_allocationMode) {
267         case AllocationMode::Shared:
268             // Currently in the shared allocation mode. Until we exhaust shared free cells, continue using the shared allocation mode.
269             // But if we allocate so many shared cells within very short period, we should use the fast allocation mode instead.
270             // This avoids the following pathological case.
271             //
272             //     for (int i = 0; i &lt; 1e6; ++i) {
273             //         auto* ptr = allocate();
274             //         ...
275             //         free(ptr);
276             //     }
277             if (m_numberOfAllocationsFromSharedInOneCycle &lt;= IsoPage&lt;Config&gt;::numObjects)
278                 return AllocationMode::Shared;
279             BFALLTHROUGH;
280 
281         case AllocationMode::Fast: {
282             // The allocation pattern may change. We should check the allocation rate and decide which mode is more appropriate.
283             // If we don&#39;t go to the allocation slow path during ~1 seconds, we think the allocation becomes quiescent state.
284             auto now = std::chrono::steady_clock::now();
285             if ((now - m_lastSlowPathTime) &lt; std::chrono::seconds(1)) {
286                 m_lastSlowPathTime = now;
287                 return AllocationMode::Fast;
288             }
289 
290             m_numberOfAllocationsFromSharedInOneCycle = 0;
291             m_lastSlowPathTime = now;
292             return AllocationMode::Shared;
293         }
294 
295         case AllocationMode::Init:
296             m_lastSlowPathTime = std::chrono::steady_clock::now();
297             return AllocationMode::Shared;
298         }
299 
300         return AllocationMode::Shared;
301     };
302     AllocationMode allocationMode = getNewAllocationMode();
303     m_allocationMode = allocationMode;
304     return allocationMode;
305 }
306 
307 template&lt;typename Config&gt;
308 void* IsoHeapImpl&lt;Config&gt;::allocateFromShared(const LockHolder&amp;, bool abortOnFailure)
309 {
310     static constexpr bool verbose = false;
311 
312     unsigned indexPlusOne = __builtin_ffs(m_availableShared);
313     BASSERT(indexPlusOne);
314     unsigned index = indexPlusOne - 1;
315     void* result = m_sharedCells[index].get();
316     if (result) {
317         if (verbose)
318             fprintf(stderr, &quot;%p: allocated %p from shared again of size %u\n&quot;, this, result, Config::objectSize);
319     } else {
320         constexpr unsigned objectSizeWithHeapImplPointer = Config::objectSize + sizeof(uint8_t);
321         result = IsoSharedHeap::get()-&gt;allocateNew&lt;objectSizeWithHeapImplPointer&gt;(abortOnFailure);
322         if (!result)
323             return nullptr;
324         if (verbose)
325             fprintf(stderr, &quot;%p: allocated %p from shared of size %u\n&quot;, this, result, Config::objectSize);
326         BASSERT(index &lt; IsoHeapImplBase::maxAllocationFromShared);
327         *indexSlotFor&lt;Config&gt;(result) = index;
328         m_sharedCells[index] = bitwise_cast&lt;uint8_t*&gt;(result);
329     }
330     BASSERT(result);
331     m_availableShared &amp;= ~(1U &lt;&lt; index);
332     ++m_numberOfAllocationsFromSharedInOneCycle;
333     return result;
334 }
335 
336 } // namespace bmalloc
337 
    </pre>
  </body>
</html>