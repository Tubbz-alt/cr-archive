<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="B3Kind.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="B3MemoryValue.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;B3LowerToAir.h&quot;
  28 
  29 #if ENABLE(B3_JIT)
  30 
  31 #include &quot;AirBlockInsertionSet.h&quot;
  32 #include &quot;AirCCallSpecial.h&quot;
  33 #include &quot;AirCode.h&quot;

  34 #include &quot;AirInsertionSet.h&quot;
  35 #include &quot;AirInstInlines.h&quot;
  36 #include &quot;AirPrintSpecial.h&quot;
  37 #include &quot;AirStackSlot.h&quot;
  38 #include &quot;B3ArgumentRegValue.h&quot;
  39 #include &quot;B3AtomicValue.h&quot;
  40 #include &quot;B3BasicBlockInlines.h&quot;
  41 #include &quot;B3BlockWorklist.h&quot;
  42 #include &quot;B3CCallValue.h&quot;
  43 #include &quot;B3CheckSpecial.h&quot;
  44 #include &quot;B3Commutativity.h&quot;
  45 #include &quot;B3Dominators.h&quot;
  46 #include &quot;B3ExtractValue.h&quot;
  47 #include &quot;B3FenceValue.h&quot;
  48 #include &quot;B3MemoryValueInlines.h&quot;
  49 #include &quot;B3PatchpointSpecial.h&quot;
  50 #include &quot;B3PatchpointValue.h&quot;
  51 #include &quot;B3PhaseScope.h&quot;
  52 #include &quot;B3PhiChildren.h&quot;
  53 #include &quot;B3Procedure.h&quot;
  54 #include &quot;B3SlotBaseValue.h&quot;
  55 #include &quot;B3StackSlot.h&quot;
  56 #include &quot;B3UpsilonValue.h&quot;
  57 #include &quot;B3UseCounts.h&quot;
  58 #include &quot;B3ValueInlines.h&quot;
  59 #include &quot;B3Variable.h&quot;
  60 #include &quot;B3VariableValue.h&quot;
  61 #include &quot;B3WasmAddressValue.h&quot;
  62 #include &lt;wtf/IndexMap.h&gt;
  63 #include &lt;wtf/IndexSet.h&gt;
  64 #include &lt;wtf/ListDump.h&gt;
  65 
<span class="line-modified">  66 #if ASSERT_DISABLED</span>
  67 IGNORE_RETURN_TYPE_WARNINGS_BEGIN
  68 #endif
  69 
  70 namespace JSC { namespace B3 {
  71 
  72 namespace {
  73 
  74 namespace B3LowerToAirInternal {
<span class="line-modified">  75 static const bool verbose = false;</span>
  76 }
  77 
  78 using Arg = Air::Arg;
  79 using Inst = Air::Inst;
  80 using Code = Air::Code;
  81 using Tmp = Air::Tmp;
  82 



  83 // FIXME: We wouldn&#39;t need this if Air supported Width modifiers in Air::Kind.
  84 // https://bugs.webkit.org/show_bug.cgi?id=169247
  85 #define OPCODE_FOR_WIDTH(opcode, width) ( \
  86     (width) == Width8 ? Air::opcode ## 8 : \
  87     (width) == Width16 ? Air::opcode ## 16 :    \
  88     (width) == Width32 ? Air::opcode ## 32 :    \
  89     Air::opcode ## 64)
  90 #define OPCODE_FOR_CANONICAL_WIDTH(opcode, width) ( \
  91     (width) == Width64 ? Air::opcode ## 64 : Air::opcode ## 32)
  92 
  93 class LowerToAir {
  94 public:
  95     LowerToAir(Procedure&amp; procedure)
  96         : m_valueToTmp(procedure.values().size())
  97         , m_phiToTmp(procedure.values().size())
  98         , m_blockToBlock(procedure.size())
  99         , m_useCounts(procedure)
 100         , m_phiChildren(procedure)
 101         , m_dominators(procedure.dominators())
 102         , m_procedure(procedure)
</pre>
<hr />
<pre>
 138 
 139                 m_phiToTmp[value] = m_code.newTmp(value-&gt;resultBank());
 140                 if (B3LowerToAirInternal::verbose)
 141                     dataLog(&quot;Phi tmp for &quot;, *value, &quot;: &quot;, m_phiToTmp[value], &quot;\n&quot;);
 142                 break;
 143             }
 144             case Get:
 145             case Patchpoint: {
 146                 if (value-&gt;type().isTuple())
 147                     ensureTupleTmps(value, m_tupleValueToTmps);
 148                 break;
 149             }
 150             default:
 151                 break;
 152             }
 153         }
 154 
 155         for (B3::StackSlot* stack : m_procedure.stackSlots())
 156             m_stackToStack.add(stack, m_code.addStackSlot(stack));
 157         for (Variable* variable : m_procedure.variables()) {
<span class="line-modified"> 158             auto addResult = m_variableToTmps.add(variable, Vector&lt;Tmp, 1&gt;(m_procedure.returnCount(variable-&gt;type())));</span>
 159             ASSERT(addResult.isNewEntry);
<span class="line-modified"> 160             for (unsigned i = 0; i &lt; m_procedure.returnCount(variable-&gt;type()); ++i)</span>
<span class="line-modified"> 161                 addResult.iterator-&gt;value[i] = tmpForType(variable-&gt;type().isNumeric() ? variable-&gt;type() : m_procedure.extractFromTuple(variable-&gt;type(), i));</span>
 162         }
 163 
 164         // Figure out which blocks are not rare.
 165         m_fastWorklist.push(m_procedure[0]);
 166         while (B3::BasicBlock* block = m_fastWorklist.pop()) {
 167             for (B3::FrequentedBlock&amp; successor : block-&gt;successors()) {
 168                 if (!successor.isRare())
 169                     m_fastWorklist.push(successor.block());
 170             }
 171         }
 172 
 173         m_procedure.resetValueOwners(); // Used by crossesInterference().
 174 
 175         // Lower defs before uses on a global level. This is a good heuristic to lock down a
 176         // hoisted address expression before we duplicate it back into the loop.
 177         for (B3::BasicBlock* block : m_procedure.blocksInPreOrder()) {
 178             m_block = block;
 179 
 180             m_isRare = !m_fastWorklist.saw(block);
 181 
</pre>
<hr />
<pre>
 514         // to signed since that&#39;s what all of our APIs want.
 515         int64_t bigScale = static_cast&lt;uint64_t&gt;(1) &lt;&lt; static_cast&lt;uint64_t&gt;(logScale);
 516         if (!isRepresentableAs&lt;int32_t&gt;(bigScale))
 517             return WTF::nullopt;
 518         unsigned scale = static_cast&lt;int32_t&gt;(bigScale);
 519         if (!Arg::isValidIndexForm(scale, offset, width))
 520             return WTF::nullopt;
 521         return scale;
 522     }
 523 
 524     // This turns the given operand into an address.
 525     template&lt;typename Int, typename = Value::IsLegalOffset&lt;Int&gt;&gt;
 526     Arg effectiveAddr(Value* address, Int offset, Width width)
 527     {
 528         ASSERT(Arg::isValidAddrForm(offset, width));
 529 
 530         auto fallback = [&amp;] () -&gt; Arg {
 531             return Arg::addr(tmp(address), offset);
 532         };
 533 
<span class="line-modified"> 534         static const unsigned lotsOfUses = 10; // This is arbitrary and we should tune it eventually.</span>
 535 
 536         // Only match if the address value isn&#39;t used in some large number of places.
 537         if (m_useCounts.numUses(address) &gt; lotsOfUses)
 538             return fallback();
 539 
 540         switch (address-&gt;opcode()) {
 541         case Add: {
 542             Value* left = address-&gt;child(0);
 543             Value* right = address-&gt;child(1);
 544 
 545             auto tryIndex = [&amp;] (Value* index, Value* base) -&gt; Arg {
 546                 Optional&lt;unsigned&gt; scale = scaleForShl(index, offset, width);
 547                 if (!scale)
 548                     return Arg();
 549                 if (m_locked.contains(index-&gt;child(0)) || m_locked.contains(base))
 550                     return Arg();
 551                 return Arg::index(tmp(base), tmp(index-&gt;child(0)), *scale, offset);
 552             };
 553 
 554             if (Arg result = tryIndex(left, right))
</pre>
<hr />
<pre>
1156             RELEASE_ASSERT(memory-&gt;accessBank() == GP);
1157 
1158             if (isX86()) {
1159                 kind = OPCODE_FOR_WIDTH(Xchg, memory-&gt;accessWidth());
1160                 kind.effects = true;
1161                 Tmp swapTmp = m_code.newTmp(GP);
1162                 append(relaxedMoveForType(memory-&gt;accessType()), tmp(memory-&gt;child(0)), swapTmp);
1163                 append(kind, swapTmp, dest);
1164                 return;
1165             }
1166 
1167             kind = OPCODE_FOR_WIDTH(StoreRel, memory-&gt;accessWidth());
1168         } else
1169             kind = storeOpcode(memory-&gt;accessWidth(), memory-&gt;accessBank());
1170 
1171         kind.effects |= memory-&gt;traps();
1172 
1173         append(createStore(kind, memory-&gt;child(0), dest));
1174     }
1175 
<span class="line-removed">1176     Air::Opcode moveForType(Type type)</span>
<span class="line-removed">1177     {</span>
<span class="line-removed">1178         using namespace Air;</span>
<span class="line-removed">1179         switch (type.kind()) {</span>
<span class="line-removed">1180         case Int32:</span>
<span class="line-removed">1181             return Move32;</span>
<span class="line-removed">1182         case Int64:</span>
<span class="line-removed">1183             RELEASE_ASSERT(is64Bit());</span>
<span class="line-removed">1184             return Move;</span>
<span class="line-removed">1185         case Float:</span>
<span class="line-removed">1186             return MoveFloat;</span>
<span class="line-removed">1187         case Double:</span>
<span class="line-removed">1188             return MoveDouble;</span>
<span class="line-removed">1189         case Void:</span>
<span class="line-removed">1190         case Tuple:</span>
<span class="line-removed">1191             break;</span>
<span class="line-removed">1192         }</span>
<span class="line-removed">1193         RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">1194         return Air::Oops;</span>
<span class="line-removed">1195     }</span>
<span class="line-removed">1196 </span>
<span class="line-removed">1197     Air::Opcode relaxedMoveForType(Type type)</span>
<span class="line-removed">1198     {</span>
<span class="line-removed">1199         using namespace Air;</span>
<span class="line-removed">1200         switch (type.kind()) {</span>
<span class="line-removed">1201         case Int32:</span>
<span class="line-removed">1202         case Int64:</span>
<span class="line-removed">1203             // For Int32, we could return Move or Move32. It&#39;s a trade-off.</span>
<span class="line-removed">1204             //</span>
<span class="line-removed">1205             // Move32: Using Move32 guarantees that we use the narrower move, but in cases where the</span>
<span class="line-removed">1206             //     register allocator can&#39;t prove that the variables involved are 32-bit, this will</span>
<span class="line-removed">1207             //     disable coalescing.</span>
<span class="line-removed">1208             //</span>
<span class="line-removed">1209             // Move: Using Move guarantees that the register allocator can coalesce normally, but in</span>
<span class="line-removed">1210             //     cases where it can&#39;t prove that the variables are 32-bit and it doesn&#39;t coalesce,</span>
<span class="line-removed">1211             //     this will force us to use a full 64-bit Move instead of the slightly cheaper</span>
<span class="line-removed">1212             //     32-bit Move32.</span>
<span class="line-removed">1213             //</span>
<span class="line-removed">1214             // Coalescing is a lot more profitable than turning Move into Move32. So, it&#39;s better to</span>
<span class="line-removed">1215             // use Move here because in cases where the register allocator cannot prove that</span>
<span class="line-removed">1216             // everything is 32-bit, we still get coalescing.</span>
<span class="line-removed">1217             return Move;</span>
<span class="line-removed">1218         case Float:</span>
<span class="line-removed">1219             // MoveFloat is always coalescable and we never convert MoveDouble to MoveFloat, so we</span>
<span class="line-removed">1220             // should use MoveFloat when we know that the temporaries involved are 32-bit.</span>
<span class="line-removed">1221             return MoveFloat;</span>
<span class="line-removed">1222         case Double:</span>
<span class="line-removed">1223             return MoveDouble;</span>
<span class="line-removed">1224         case Void:</span>
<span class="line-removed">1225         case Tuple:</span>
<span class="line-removed">1226             break;</span>
<span class="line-removed">1227         }</span>
<span class="line-removed">1228         RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">1229         return Air::Oops;</span>
<span class="line-removed">1230     }</span>
<span class="line-removed">1231 </span>
1232 #if ENABLE(MASM_PROBE)
1233     template&lt;typename... Arguments&gt;
1234     void print(Arguments&amp;&amp;... arguments)
1235     {
1236         Value* origin = m_value;
1237         print(origin, std::forward&lt;Arguments&gt;(arguments)...);
1238     }
1239 
1240     template&lt;typename... Arguments&gt;
1241     void print(Value* origin, Arguments&amp;&amp;... arguments)
1242     {
1243         auto printList = Printer::makePrintRecordList(arguments...);
1244         auto printSpecial = static_cast&lt;Air::PrintSpecial*&gt;(m_code.addSpecial(makeUnique&lt;Air::PrintSpecial&gt;(printList)));
1245         Inst inst(Air::Patch, origin, Arg::special(printSpecial));
1246         Printer::appendAirArgs(inst, std::forward&lt;Arguments&gt;(arguments)...);
1247         append(WTFMove(inst));
1248     }
1249 #endif // ENABLE(MASM_PROBE)
1250 
1251     template&lt;typename... Arguments&gt;
</pre>
<hr />
<pre>
2323         //     StoreCond
2324         //     Xor $1, %result    &lt;--- only if !invert
2325         //     Jump
2326         //   Successors: #done
2327         // Block #fail:
2328         //     Move $invert, %result
2329         //     Jump
2330         //   Successors: #done
2331         // Block #done:
2332 
2333         Air::BasicBlock* reloopBlock = newBlock();
2334         Air::BasicBlock* storeBlock = newBlock();
2335         Air::BasicBlock* successBlock = nullptr;
2336         if (!isBranch &amp;&amp; isStrong)
2337             successBlock = newBlock();
2338         Air::BasicBlock* failBlock = nullptr;
2339         if (!isBranch) {
2340             failBlock = newBlock();
2341             failure = failBlock;
2342         }
<span class="line-modified">2343         Air::BasicBlock* strongFailBlock;</span>
2344         if (isStrong &amp;&amp; hasFence)
2345             strongFailBlock = newBlock();
2346         Air::FrequentedBlock comparisonFail = failure;
2347         Air::FrequentedBlock weakFail;
2348         if (isStrong) {
2349             if (hasFence)
2350                 comparisonFail = strongFailBlock;
2351             weakFail = reloopBlock;
2352         } else
2353             weakFail = failure;
2354         Air::BasicBlock* beginBlock;
2355         Air::BasicBlock* doneBlock;
2356         splitBlock(beginBlock, doneBlock);
2357 
2358         append(Air::Jump);
2359         beginBlock-&gt;setSuccessors(reloopBlock);
2360 
2361         reloopBlock-&gt;append(trappingInst(m_value, loadLinkOpcode(width, atomic-&gt;hasFence()), m_value, address, valueResultTmp));
2362         reloopBlock-&gt;append(OPCODE_FOR_CANONICAL_WIDTH(Branch, width), m_value, Arg::relCond(MacroAssembler::NotEqual), valueResultTmp, expectedValueTmp);
2363         reloopBlock-&gt;setSuccessors(comparisonFail, storeBlock);
</pre>
<hr />
<pre>
2732             return;
2733         }
2734 
2735         case UMod: {
2736             RELEASE_ASSERT(isX86());
2737             appendX86UDiv(UMod);
2738             return;
2739         }
2740 
2741         case BitAnd: {
2742             if (m_value-&gt;child(1)-&gt;isInt(0xff)) {
2743                 appendUnOp&lt;ZeroExtend8To32, ZeroExtend8To32&gt;(m_value-&gt;child(0));
2744                 return;
2745             }
2746 
2747             if (m_value-&gt;child(1)-&gt;isInt(0xffff)) {
2748                 appendUnOp&lt;ZeroExtend16To32, ZeroExtend16To32&gt;(m_value-&gt;child(0));
2749                 return;
2750             }
2751 
<span class="line-modified">2752             if (m_value-&gt;child(1)-&gt;isInt(0xffffffff)) {</span>
2753                 appendUnOp&lt;Move32, Move32&gt;(m_value-&gt;child(0));
2754                 return;
2755             }
2756 
2757             appendBinOp&lt;And32, And64, AndDouble, AndFloat, Commutative&gt;(
2758                 m_value-&gt;child(0), m_value-&gt;child(1));
2759             return;
2760         }
2761 
2762         case BitOr: {
2763             appendBinOp&lt;Or32, Or64, OrDouble, OrFloat, Commutative&gt;(
2764                 m_value-&gt;child(0), m_value-&gt;child(1));
2765             return;
2766         }
2767 
2768         case BitXor: {
2769             // FIXME: If canBeInternal(child), we should generate this using the comparison path.
2770             // https://bugs.webkit.org/show_bug.cgi?id=152367
2771 
2772             if (m_value-&gt;child(1)-&gt;isInt(-1)) {
</pre>
<hr />
<pre>
3723     Code&amp; m_code;
3724 
3725     Air::BlockInsertionSet m_blockInsertionSet;
3726 
3727     Tmp m_eax;
3728     Tmp m_ecx;
3729     Tmp m_edx;
3730 };
3731 
3732 } // anonymous namespace
3733 
3734 void lowerToAir(Procedure&amp; procedure)
3735 {
3736     PhaseScope phaseScope(procedure, &quot;lowerToAir&quot;);
3737     LowerToAir lowerToAir(procedure);
3738     lowerToAir.run();
3739 }
3740 
3741 } } // namespace JSC::B3
3742 
<span class="line-modified">3743 #if ASSERT_DISABLED</span>
3744 IGNORE_RETURN_TYPE_WARNINGS_END
3745 #endif
3746 
3747 #endif // ENABLE(B3_JIT)
</pre>
</td>
<td>
<hr />
<pre>
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;B3LowerToAir.h&quot;
  28 
  29 #if ENABLE(B3_JIT)
  30 
  31 #include &quot;AirBlockInsertionSet.h&quot;
  32 #include &quot;AirCCallSpecial.h&quot;
  33 #include &quot;AirCode.h&quot;
<span class="line-added">  34 #include &quot;AirHelpers.h&quot;</span>
  35 #include &quot;AirInsertionSet.h&quot;
  36 #include &quot;AirInstInlines.h&quot;
  37 #include &quot;AirPrintSpecial.h&quot;
  38 #include &quot;AirStackSlot.h&quot;
  39 #include &quot;B3ArgumentRegValue.h&quot;
  40 #include &quot;B3AtomicValue.h&quot;
  41 #include &quot;B3BasicBlockInlines.h&quot;
  42 #include &quot;B3BlockWorklist.h&quot;
  43 #include &quot;B3CCallValue.h&quot;
  44 #include &quot;B3CheckSpecial.h&quot;
  45 #include &quot;B3Commutativity.h&quot;
  46 #include &quot;B3Dominators.h&quot;
  47 #include &quot;B3ExtractValue.h&quot;
  48 #include &quot;B3FenceValue.h&quot;
  49 #include &quot;B3MemoryValueInlines.h&quot;
  50 #include &quot;B3PatchpointSpecial.h&quot;
  51 #include &quot;B3PatchpointValue.h&quot;
  52 #include &quot;B3PhaseScope.h&quot;
  53 #include &quot;B3PhiChildren.h&quot;
  54 #include &quot;B3Procedure.h&quot;
  55 #include &quot;B3SlotBaseValue.h&quot;
  56 #include &quot;B3StackSlot.h&quot;
  57 #include &quot;B3UpsilonValue.h&quot;
  58 #include &quot;B3UseCounts.h&quot;
  59 #include &quot;B3ValueInlines.h&quot;
  60 #include &quot;B3Variable.h&quot;
  61 #include &quot;B3VariableValue.h&quot;
  62 #include &quot;B3WasmAddressValue.h&quot;
  63 #include &lt;wtf/IndexMap.h&gt;
  64 #include &lt;wtf/IndexSet.h&gt;
  65 #include &lt;wtf/ListDump.h&gt;
  66 
<span class="line-modified">  67 #if !ASSERT_ENABLED</span>
  68 IGNORE_RETURN_TYPE_WARNINGS_BEGIN
  69 #endif
  70 
  71 namespace JSC { namespace B3 {
  72 
  73 namespace {
  74 
  75 namespace B3LowerToAirInternal {
<span class="line-modified">  76 static constexpr bool verbose = false;</span>
  77 }
  78 
  79 using Arg = Air::Arg;
  80 using Inst = Air::Inst;
  81 using Code = Air::Code;
  82 using Tmp = Air::Tmp;
  83 
<span class="line-added">  84 using Air::moveForType;</span>
<span class="line-added">  85 using Air::relaxedMoveForType;</span>
<span class="line-added">  86 </span>
  87 // FIXME: We wouldn&#39;t need this if Air supported Width modifiers in Air::Kind.
  88 // https://bugs.webkit.org/show_bug.cgi?id=169247
  89 #define OPCODE_FOR_WIDTH(opcode, width) ( \
  90     (width) == Width8 ? Air::opcode ## 8 : \
  91     (width) == Width16 ? Air::opcode ## 16 :    \
  92     (width) == Width32 ? Air::opcode ## 32 :    \
  93     Air::opcode ## 64)
  94 #define OPCODE_FOR_CANONICAL_WIDTH(opcode, width) ( \
  95     (width) == Width64 ? Air::opcode ## 64 : Air::opcode ## 32)
  96 
  97 class LowerToAir {
  98 public:
  99     LowerToAir(Procedure&amp; procedure)
 100         : m_valueToTmp(procedure.values().size())
 101         , m_phiToTmp(procedure.values().size())
 102         , m_blockToBlock(procedure.size())
 103         , m_useCounts(procedure)
 104         , m_phiChildren(procedure)
 105         , m_dominators(procedure.dominators())
 106         , m_procedure(procedure)
</pre>
<hr />
<pre>
 142 
 143                 m_phiToTmp[value] = m_code.newTmp(value-&gt;resultBank());
 144                 if (B3LowerToAirInternal::verbose)
 145                     dataLog(&quot;Phi tmp for &quot;, *value, &quot;: &quot;, m_phiToTmp[value], &quot;\n&quot;);
 146                 break;
 147             }
 148             case Get:
 149             case Patchpoint: {
 150                 if (value-&gt;type().isTuple())
 151                     ensureTupleTmps(value, m_tupleValueToTmps);
 152                 break;
 153             }
 154             default:
 155                 break;
 156             }
 157         }
 158 
 159         for (B3::StackSlot* stack : m_procedure.stackSlots())
 160             m_stackToStack.add(stack, m_code.addStackSlot(stack));
 161         for (Variable* variable : m_procedure.variables()) {
<span class="line-modified"> 162             auto addResult = m_variableToTmps.add(variable, Vector&lt;Tmp, 1&gt;(m_procedure.resultCount(variable-&gt;type())));</span>
 163             ASSERT(addResult.isNewEntry);
<span class="line-modified"> 164             for (unsigned i = 0; i &lt; m_procedure.resultCount(variable-&gt;type()); ++i)</span>
<span class="line-modified"> 165                 addResult.iterator-&gt;value[i] = tmpForType(m_procedure.typeAtOffset(variable-&gt;type(), i));</span>
 166         }
 167 
 168         // Figure out which blocks are not rare.
 169         m_fastWorklist.push(m_procedure[0]);
 170         while (B3::BasicBlock* block = m_fastWorklist.pop()) {
 171             for (B3::FrequentedBlock&amp; successor : block-&gt;successors()) {
 172                 if (!successor.isRare())
 173                     m_fastWorklist.push(successor.block());
 174             }
 175         }
 176 
 177         m_procedure.resetValueOwners(); // Used by crossesInterference().
 178 
 179         // Lower defs before uses on a global level. This is a good heuristic to lock down a
 180         // hoisted address expression before we duplicate it back into the loop.
 181         for (B3::BasicBlock* block : m_procedure.blocksInPreOrder()) {
 182             m_block = block;
 183 
 184             m_isRare = !m_fastWorklist.saw(block);
 185 
</pre>
<hr />
<pre>
 518         // to signed since that&#39;s what all of our APIs want.
 519         int64_t bigScale = static_cast&lt;uint64_t&gt;(1) &lt;&lt; static_cast&lt;uint64_t&gt;(logScale);
 520         if (!isRepresentableAs&lt;int32_t&gt;(bigScale))
 521             return WTF::nullopt;
 522         unsigned scale = static_cast&lt;int32_t&gt;(bigScale);
 523         if (!Arg::isValidIndexForm(scale, offset, width))
 524             return WTF::nullopt;
 525         return scale;
 526     }
 527 
 528     // This turns the given operand into an address.
 529     template&lt;typename Int, typename = Value::IsLegalOffset&lt;Int&gt;&gt;
 530     Arg effectiveAddr(Value* address, Int offset, Width width)
 531     {
 532         ASSERT(Arg::isValidAddrForm(offset, width));
 533 
 534         auto fallback = [&amp;] () -&gt; Arg {
 535             return Arg::addr(tmp(address), offset);
 536         };
 537 
<span class="line-modified"> 538         static constexpr unsigned lotsOfUses = 10; // This is arbitrary and we should tune it eventually.</span>
 539 
 540         // Only match if the address value isn&#39;t used in some large number of places.
 541         if (m_useCounts.numUses(address) &gt; lotsOfUses)
 542             return fallback();
 543 
 544         switch (address-&gt;opcode()) {
 545         case Add: {
 546             Value* left = address-&gt;child(0);
 547             Value* right = address-&gt;child(1);
 548 
 549             auto tryIndex = [&amp;] (Value* index, Value* base) -&gt; Arg {
 550                 Optional&lt;unsigned&gt; scale = scaleForShl(index, offset, width);
 551                 if (!scale)
 552                     return Arg();
 553                 if (m_locked.contains(index-&gt;child(0)) || m_locked.contains(base))
 554                     return Arg();
 555                 return Arg::index(tmp(base), tmp(index-&gt;child(0)), *scale, offset);
 556             };
 557 
 558             if (Arg result = tryIndex(left, right))
</pre>
<hr />
<pre>
1160             RELEASE_ASSERT(memory-&gt;accessBank() == GP);
1161 
1162             if (isX86()) {
1163                 kind = OPCODE_FOR_WIDTH(Xchg, memory-&gt;accessWidth());
1164                 kind.effects = true;
1165                 Tmp swapTmp = m_code.newTmp(GP);
1166                 append(relaxedMoveForType(memory-&gt;accessType()), tmp(memory-&gt;child(0)), swapTmp);
1167                 append(kind, swapTmp, dest);
1168                 return;
1169             }
1170 
1171             kind = OPCODE_FOR_WIDTH(StoreRel, memory-&gt;accessWidth());
1172         } else
1173             kind = storeOpcode(memory-&gt;accessWidth(), memory-&gt;accessBank());
1174 
1175         kind.effects |= memory-&gt;traps();
1176 
1177         append(createStore(kind, memory-&gt;child(0), dest));
1178     }
1179 
























































1180 #if ENABLE(MASM_PROBE)
1181     template&lt;typename... Arguments&gt;
1182     void print(Arguments&amp;&amp;... arguments)
1183     {
1184         Value* origin = m_value;
1185         print(origin, std::forward&lt;Arguments&gt;(arguments)...);
1186     }
1187 
1188     template&lt;typename... Arguments&gt;
1189     void print(Value* origin, Arguments&amp;&amp;... arguments)
1190     {
1191         auto printList = Printer::makePrintRecordList(arguments...);
1192         auto printSpecial = static_cast&lt;Air::PrintSpecial*&gt;(m_code.addSpecial(makeUnique&lt;Air::PrintSpecial&gt;(printList)));
1193         Inst inst(Air::Patch, origin, Arg::special(printSpecial));
1194         Printer::appendAirArgs(inst, std::forward&lt;Arguments&gt;(arguments)...);
1195         append(WTFMove(inst));
1196     }
1197 #endif // ENABLE(MASM_PROBE)
1198 
1199     template&lt;typename... Arguments&gt;
</pre>
<hr />
<pre>
2271         //     StoreCond
2272         //     Xor $1, %result    &lt;--- only if !invert
2273         //     Jump
2274         //   Successors: #done
2275         // Block #fail:
2276         //     Move $invert, %result
2277         //     Jump
2278         //   Successors: #done
2279         // Block #done:
2280 
2281         Air::BasicBlock* reloopBlock = newBlock();
2282         Air::BasicBlock* storeBlock = newBlock();
2283         Air::BasicBlock* successBlock = nullptr;
2284         if (!isBranch &amp;&amp; isStrong)
2285             successBlock = newBlock();
2286         Air::BasicBlock* failBlock = nullptr;
2287         if (!isBranch) {
2288             failBlock = newBlock();
2289             failure = failBlock;
2290         }
<span class="line-modified">2291         Air::BasicBlock* strongFailBlock = nullptr;</span>
2292         if (isStrong &amp;&amp; hasFence)
2293             strongFailBlock = newBlock();
2294         Air::FrequentedBlock comparisonFail = failure;
2295         Air::FrequentedBlock weakFail;
2296         if (isStrong) {
2297             if (hasFence)
2298                 comparisonFail = strongFailBlock;
2299             weakFail = reloopBlock;
2300         } else
2301             weakFail = failure;
2302         Air::BasicBlock* beginBlock;
2303         Air::BasicBlock* doneBlock;
2304         splitBlock(beginBlock, doneBlock);
2305 
2306         append(Air::Jump);
2307         beginBlock-&gt;setSuccessors(reloopBlock);
2308 
2309         reloopBlock-&gt;append(trappingInst(m_value, loadLinkOpcode(width, atomic-&gt;hasFence()), m_value, address, valueResultTmp));
2310         reloopBlock-&gt;append(OPCODE_FOR_CANONICAL_WIDTH(Branch, width), m_value, Arg::relCond(MacroAssembler::NotEqual), valueResultTmp, expectedValueTmp);
2311         reloopBlock-&gt;setSuccessors(comparisonFail, storeBlock);
</pre>
<hr />
<pre>
2680             return;
2681         }
2682 
2683         case UMod: {
2684             RELEASE_ASSERT(isX86());
2685             appendX86UDiv(UMod);
2686             return;
2687         }
2688 
2689         case BitAnd: {
2690             if (m_value-&gt;child(1)-&gt;isInt(0xff)) {
2691                 appendUnOp&lt;ZeroExtend8To32, ZeroExtend8To32&gt;(m_value-&gt;child(0));
2692                 return;
2693             }
2694 
2695             if (m_value-&gt;child(1)-&gt;isInt(0xffff)) {
2696                 appendUnOp&lt;ZeroExtend16To32, ZeroExtend16To32&gt;(m_value-&gt;child(0));
2697                 return;
2698             }
2699 
<span class="line-modified">2700             if (m_value-&gt;child(1)-&gt;isInt64(0xffffffff) || m_value-&gt;child(1)-&gt;isInt32(0xffffffff)) {</span>
2701                 appendUnOp&lt;Move32, Move32&gt;(m_value-&gt;child(0));
2702                 return;
2703             }
2704 
2705             appendBinOp&lt;And32, And64, AndDouble, AndFloat, Commutative&gt;(
2706                 m_value-&gt;child(0), m_value-&gt;child(1));
2707             return;
2708         }
2709 
2710         case BitOr: {
2711             appendBinOp&lt;Or32, Or64, OrDouble, OrFloat, Commutative&gt;(
2712                 m_value-&gt;child(0), m_value-&gt;child(1));
2713             return;
2714         }
2715 
2716         case BitXor: {
2717             // FIXME: If canBeInternal(child), we should generate this using the comparison path.
2718             // https://bugs.webkit.org/show_bug.cgi?id=152367
2719 
2720             if (m_value-&gt;child(1)-&gt;isInt(-1)) {
</pre>
<hr />
<pre>
3671     Code&amp; m_code;
3672 
3673     Air::BlockInsertionSet m_blockInsertionSet;
3674 
3675     Tmp m_eax;
3676     Tmp m_ecx;
3677     Tmp m_edx;
3678 };
3679 
3680 } // anonymous namespace
3681 
3682 void lowerToAir(Procedure&amp; procedure)
3683 {
3684     PhaseScope phaseScope(procedure, &quot;lowerToAir&quot;);
3685     LowerToAir lowerToAir(procedure);
3686     lowerToAir.run();
3687 }
3688 
3689 } } // namespace JSC::B3
3690 
<span class="line-modified">3691 #if !ASSERT_ENABLED</span>
3692 IGNORE_RETURN_TYPE_WARNINGS_END
3693 #endif
3694 
3695 #endif // ENABLE(B3_JIT)
</pre>
</td>
</tr>
</table>
<center><a href="B3Kind.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="B3MemoryValue.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>