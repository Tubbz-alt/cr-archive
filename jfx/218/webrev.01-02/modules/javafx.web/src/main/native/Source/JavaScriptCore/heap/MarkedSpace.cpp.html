<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  *  Copyright (C) 2003-2018 Apple Inc. All rights reserved.
  3  *  Copyright (C) 2007 Eric Seidel &lt;eric@webkit.org&gt;
  4  *
  5  *  This library is free software; you can redistribute it and/or
  6  *  modify it under the terms of the GNU Lesser General Public
  7  *  License as published by the Free Software Foundation; either
  8  *  version 2 of the License, or (at your option) any later version.
  9  *
 10  *  This library is distributed in the hope that it will be useful,
 11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 13  *  Lesser General Public License for more details.
 14  *
 15  *  You should have received a copy of the GNU Lesser General Public
 16  *  License along with this library; if not, write to the Free Software
 17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 18  *
 19  */
 20 
 21 #include &quot;config.h&quot;
 22 #include &quot;MarkedSpace.h&quot;
 23 
 24 #include &quot;BlockDirectoryInlines.h&quot;
 25 #include &quot;FunctionCodeBlock.h&quot;
 26 #include &quot;IncrementalSweeper.h&quot;
 27 #include &quot;JSObject.h&quot;
 28 #include &quot;JSCInlines.h&quot;
 29 #include &quot;MarkedBlockInlines.h&quot;
 30 #include &quot;MarkedSpaceInlines.h&quot;
 31 #include &lt;wtf/ListDump.h&gt;
 32 
 33 namespace JSC {
 34 
 35 std::array&lt;size_t, MarkedSpace::numSizeClasses&gt; MarkedSpace::s_sizeClassForSizeStep;
 36 
 37 namespace {
 38 
 39 const Vector&lt;size_t&gt;&amp; sizeClasses()
 40 {
 41     static Vector&lt;size_t&gt;* result;
 42     static std::once_flag once;
 43     std::call_once(
 44         once,
 45         [] {
 46             result = new Vector&lt;size_t&gt;();
 47 
 48             if (UNLIKELY(Options::dumpSizeClasses())) {
 49                 dataLog(&quot;Block size: &quot;, MarkedBlock::blockSize, &quot;\n&quot;);
 50                 dataLog(&quot;Footer size: &quot;, sizeof(MarkedBlock::Footer), &quot;\n&quot;);
 51             }
 52 
 53             auto add = [&amp;] (size_t sizeClass) {
 54                 sizeClass = WTF::roundUpToMultipleOf&lt;MarkedBlock::atomSize&gt;(sizeClass);
 55                 dataLogLnIf(Options::dumpSizeClasses(), &quot;Adding JSC MarkedSpace size class: &quot;, sizeClass);
 56                 // Perform some validation as we go.
 57                 RELEASE_ASSERT(!(sizeClass % MarkedSpace::sizeStep));
 58                 if (result-&gt;isEmpty())
 59                     RELEASE_ASSERT(sizeClass == MarkedSpace::sizeStep);
 60                 result-&gt;append(sizeClass);
 61             };
 62 
 63             // This is a definition of the size classes in our GC. It must define all of the
 64             // size classes from sizeStep up to largeCutoff.
 65 
 66             // Have very precise size classes for the small stuff. This is a loop to make it easy to reduce
 67             // atomSize.
 68             for (size_t size = MarkedSpace::sizeStep; size &lt; MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep)
 69                 add(size);
 70 
 71             // We want to make sure that the remaining size classes minimize internal fragmentation (i.e.
 72             // the wasted space at the tail end of a MarkedBlock) while proceeding roughly in an exponential
 73             // way starting at just above the precise size classes to four cells per block.
 74 
 75             dataLogLnIf(Options::dumpSizeClasses(), &quot;    Marked block payload size: &quot;, static_cast&lt;size_t&gt;(MarkedSpace::blockPayload));
 76 
 77             for (unsigned i = 0; ; ++i) {
 78                 double approximateSize = MarkedSpace::preciseCutoff * pow(Options::sizeClassProgression(), i);
 79                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Next size class as a double: &quot;, approximateSize);
 80 
 81                 size_t approximateSizeInBytes = static_cast&lt;size_t&gt;(approximateSize);
 82                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Next size class as bytes: &quot;, approximateSizeInBytes);
 83 
 84                 // Make sure that the computer did the math correctly.
 85                 RELEASE_ASSERT(approximateSizeInBytes &gt;= MarkedSpace::preciseCutoff);
 86 
 87                 if (approximateSizeInBytes &gt; MarkedSpace::largeCutoff)
 88                     break;
 89 
 90                 size_t sizeClass =
 91                     WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(approximateSizeInBytes);
 92                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Size class: &quot;, sizeClass);
 93 
 94                 // Optimize the size class so that there isn&#39;t any slop at the end of the block&#39;s
 95                 // payload.
 96                 unsigned cellsPerBlock = MarkedSpace::blockPayload / sizeClass;
 97                 size_t possiblyBetterSizeClass = (MarkedSpace::blockPayload / cellsPerBlock) &amp; ~(MarkedSpace::sizeStep - 1);
 98                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Possibly better size class: &quot;, possiblyBetterSizeClass);
 99 
100                 // The size class we just came up with is better than the other one if it reduces
101                 // total wastage assuming we only allocate cells of that size.
102                 size_t originalWastage = MarkedSpace::blockPayload - cellsPerBlock * sizeClass;
103                 size_t newWastage = (possiblyBetterSizeClass - sizeClass) * cellsPerBlock;
104                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Original wastage: &quot;, originalWastage, &quot;, new wastage: &quot;, newWastage);
105 
106                 size_t betterSizeClass;
107                 if (newWastage &gt; originalWastage)
108                     betterSizeClass = sizeClass;
109                 else
110                     betterSizeClass = possiblyBetterSizeClass;
111 
112                 dataLogLnIf(Options::dumpSizeClasses(), &quot;    Choosing size class: &quot;, betterSizeClass);
113 
114                 if (betterSizeClass == result-&gt;last()) {
115                     // Defense for when expStep is small.
116                     continue;
117                 }
118 
119                 // This is usually how we get out of the loop.
120                 if (betterSizeClass &gt; MarkedSpace::largeCutoff
121                     || betterSizeClass &gt; Options::preciseAllocationCutoff())
122                     break;
123 
124                 add(betterSizeClass);
125             }
126 
127             // Manually inject size classes for objects we know will be allocated in high volume.
128             // FIXME: All of these things should have IsoSubspaces.
129             // https://bugs.webkit.org/show_bug.cgi?id=179876
130             add(256);
131 
132             {
133                 // Sort and deduplicate.
134                 std::sort(result-&gt;begin(), result-&gt;end());
135                 auto it = std::unique(result-&gt;begin(), result-&gt;end());
136                 result-&gt;shrinkCapacity(it - result-&gt;begin());
137             }
138 
139             dataLogLnIf(Options::dumpSizeClasses(), &quot;JSC Heap MarkedSpace size class dump: &quot;, listDump(*result));
140 
141             // We have an optimiation in MarkedSpace::optimalSizeFor() that assumes things about
142             // the size class table. This checks our results against that function&#39;s assumptions.
143             for (size_t size = MarkedSpace::sizeStep, i = 0; size &lt;= MarkedSpace::preciseCutoff; size += MarkedSpace::sizeStep, i++)
144                 RELEASE_ASSERT(result-&gt;at(i) == size);
145         });
146     return *result;
147 }
148 
149 template&lt;typename TableType, typename SizeClassCons, typename DefaultCons&gt;
150 void buildSizeClassTable(TableType&amp; table, const SizeClassCons&amp; cons, const DefaultCons&amp; defaultCons)
151 {
152     size_t nextIndex = 0;
153     for (size_t sizeClass : sizeClasses()) {
154         auto entry = cons(sizeClass);
155         size_t index = MarkedSpace::sizeClassToIndex(sizeClass);
156         for (size_t i = nextIndex; i &lt;= index; ++i)
157             table[i] = entry;
158         nextIndex = index + 1;
159     }
160     ASSERT(MarkedSpace::sizeClassToIndex(MarkedSpace::largeCutoff - 1) &lt; MarkedSpace::numSizeClasses);
161     for (size_t i = nextIndex; i &lt; MarkedSpace::numSizeClasses; ++i)
162         table[i] = defaultCons(MarkedSpace::indexToSizeClass(i));
163 }
164 
165 } // anonymous namespace
166 
167 void MarkedSpace::initializeSizeClassForStepSize()
168 {
169     static std::once_flag flag;
170     std::call_once(
171         flag,
172         [] {
173             buildSizeClassTable(
174                 s_sizeClassForSizeStep,
175                 [&amp;] (size_t sizeClass) -&gt; size_t {
176                     return sizeClass;
177                 },
178                 [&amp;] (size_t sizeClass) -&gt; size_t {
179                     return sizeClass;
180                 });
181         });
182 }
183 
184 MarkedSpace::MarkedSpace(Heap* heap)
185 {
186     ASSERT_UNUSED(heap, heap == &amp;this-&gt;heap());
187     initializeSizeClassForStepSize();
188 }
189 
190 MarkedSpace::~MarkedSpace()
191 {
192     ASSERT(!m_blocks.set().size());
193 }
194 
195 void MarkedSpace::freeMemory()
196 {
197     forEachBlock(
198         [&amp;] (MarkedBlock::Handle* block) {
199             freeBlock(block);
200         });
201     for (PreciseAllocation* allocation : m_preciseAllocations)
202         allocation-&gt;destroy();
203     forEachSubspace([&amp;](Subspace&amp; subspace) {
204         if (subspace.isIsoSubspace())
205             static_cast&lt;IsoSubspace&amp;&gt;(subspace).destroyLowerTierFreeList();
206         return IterationStatus::Continue;
207     });
208 }
209 
210 void MarkedSpace::lastChanceToFinalize()
211 {
212     forEachDirectory(
213         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
214             directory.lastChanceToFinalize();
215             return IterationStatus::Continue;
216         });
217     for (PreciseAllocation* allocation : m_preciseAllocations)
218         allocation-&gt;lastChanceToFinalize();
219     // We do not call lastChanceToFinalize for lower-tier swept cells since we need nothing to do.
220 }
221 
222 void MarkedSpace::sweepBlocks()
223 {
224     heap().sweeper().stopSweeping();
225     forEachDirectory(
226         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
227             directory.sweep();
228             return IterationStatus::Continue;
229         });
230 }
231 
232 void MarkedSpace::sweepPreciseAllocations()
233 {
234     RELEASE_ASSERT(m_preciseAllocationsNurseryOffset == m_preciseAllocations.size());
235     unsigned srcIndex = m_preciseAllocationsNurseryOffsetForSweep;
236     unsigned dstIndex = srcIndex;
237     while (srcIndex &lt; m_preciseAllocations.size()) {
238         PreciseAllocation* allocation = m_preciseAllocations[srcIndex++];
239         allocation-&gt;sweep();
240         if (allocation-&gt;isEmpty()) {
241             if (auto* set = preciseAllocationSet())
242                 set-&gt;remove(allocation-&gt;cell());
243             if (allocation-&gt;isLowerTier())
244                 static_cast&lt;IsoSubspace*&gt;(allocation-&gt;subspace())-&gt;sweepLowerTierCell(allocation);
245             else {
246                 m_capacity -= allocation-&gt;cellSize();
247                 allocation-&gt;destroy();
248             }
249             continue;
250         }
251         allocation-&gt;setIndexInSpace(dstIndex);
252         m_preciseAllocations[dstIndex++] = allocation;
253     }
254     m_preciseAllocations.shrink(dstIndex);
255     m_preciseAllocationsNurseryOffset = m_preciseAllocations.size();
256 }
257 
258 void MarkedSpace::prepareForAllocation()
259 {
260     ASSERT(!Thread::mayBeGCThread() || heap().worldIsStopped());
261     for (Subspace* subspace : m_subspaces)
262         subspace-&gt;prepareForAllocation();
263 
264     m_activeWeakSets.takeFrom(m_newActiveWeakSets);
265 
266     if (heap().collectionScope() == CollectionScope::Eden)
267         m_preciseAllocationsNurseryOffsetForSweep = m_preciseAllocationsNurseryOffset;
268     else
269         m_preciseAllocationsNurseryOffsetForSweep = 0;
270     m_preciseAllocationsNurseryOffset = m_preciseAllocations.size();
271 }
272 
273 void MarkedSpace::enablePreciseAllocationTracking()
274 {
275     m_preciseAllocationSet = makeUnique&lt;HashSet&lt;HeapCell*&gt;&gt;();
276     for (auto* allocation : m_preciseAllocations)
277         m_preciseAllocationSet-&gt;add(allocation-&gt;cell());
278 }
279 
280 void MarkedSpace::visitWeakSets(SlotVisitor&amp; visitor)
281 {
282     auto visit = [&amp;] (WeakSet* weakSet) {
283         weakSet-&gt;visit(visitor);
284     };
285 
286     m_newActiveWeakSets.forEach(visit);
287 
288     if (heap().collectionScope() == CollectionScope::Full)
289         m_activeWeakSets.forEach(visit);
290 }
291 
292 void MarkedSpace::reapWeakSets()
293 {
294     auto visit = [&amp;] (WeakSet* weakSet) {
295         weakSet-&gt;reap();
296     };
297 
298     m_newActiveWeakSets.forEach(visit);
299 
300     if (heap().collectionScope() == CollectionScope::Full)
301         m_activeWeakSets.forEach(visit);
302 }
303 
304 void MarkedSpace::stopAllocating()
305 {
306     ASSERT(!isIterating());
307     forEachDirectory(
308         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
309             directory.stopAllocating();
310             return IterationStatus::Continue;
311         });
312 }
313 
314 void MarkedSpace::stopAllocatingForGood()
315 {
316     ASSERT(!isIterating());
317     forEachDirectory(
318         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
319             directory.stopAllocatingForGood();
320             return IterationStatus::Continue;
321         });
322 }
323 
324 void MarkedSpace::prepareForConservativeScan()
325 {
326     m_preciseAllocationsForThisCollectionBegin = m_preciseAllocations.begin() + m_preciseAllocationsOffsetForThisCollection;
327     m_preciseAllocationsForThisCollectionSize = m_preciseAllocations.size() - m_preciseAllocationsOffsetForThisCollection;
328     m_preciseAllocationsForThisCollectionEnd = m_preciseAllocations.end();
329     RELEASE_ASSERT(m_preciseAllocationsForThisCollectionEnd == m_preciseAllocationsForThisCollectionBegin + m_preciseAllocationsForThisCollectionSize);
330 
331     std::sort(
332         m_preciseAllocationsForThisCollectionBegin, m_preciseAllocationsForThisCollectionEnd,
333         [&amp;] (PreciseAllocation* a, PreciseAllocation* b) {
334             return a &lt; b;
335         });
336     unsigned index = m_preciseAllocationsOffsetForThisCollection;
337     for (auto* start = m_preciseAllocationsForThisCollectionBegin; start != m_preciseAllocationsForThisCollectionEnd; ++start, ++index) {
338         (*start)-&gt;setIndexInSpace(index);
339         ASSERT(m_preciseAllocations[index] == *start);
340         ASSERT(m_preciseAllocations[index]-&gt;indexInSpace() == index);
341     }
342 }
343 
344 void MarkedSpace::prepareForMarking()
345 {
346     if (heap().collectionScope() == CollectionScope::Eden)
347         m_preciseAllocationsOffsetForThisCollection = m_preciseAllocationsNurseryOffset;
348     else
349         m_preciseAllocationsOffsetForThisCollection = 0;
350 }
351 
352 void MarkedSpace::resumeAllocating()
353 {
354     forEachDirectory(
355         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
356             directory.resumeAllocating();
357             return IterationStatus::Continue;
358         });
359     // Nothing to do for PreciseAllocations.
360 }
361 
362 bool MarkedSpace::isPagedOut(MonotonicTime deadline)
363 {
364     bool result = false;
365     forEachDirectory(
366         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
367             if (directory.isPagedOut(deadline)) {
368                 result = true;
369                 return IterationStatus::Done;
370             }
371             return IterationStatus::Continue;
372         });
373     // FIXME: Consider taking PreciseAllocations into account here.
374     return result;
375 }
376 
377 void MarkedSpace::freeBlock(MarkedBlock::Handle* block)
378 {
379     block-&gt;directory()-&gt;removeBlock(block);
380     m_capacity -= MarkedBlock::blockSize;
381     m_blocks.remove(&amp;block-&gt;block());
382     delete block;
383 }
384 
385 void MarkedSpace::freeOrShrinkBlock(MarkedBlock::Handle* block)
386 {
387     if (!block-&gt;isEmpty()) {
388         block-&gt;shrink();
389         return;
390     }
391 
392     freeBlock(block);
393 }
394 
395 void MarkedSpace::shrink()
396 {
397     forEachDirectory(
398         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
399             directory.shrink();
400             return IterationStatus::Continue;
401         });
402 }
403 
404 void MarkedSpace::beginMarking()
405 {
406     if (heap().collectionScope() == CollectionScope::Full) {
407         forEachDirectory(
408             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
409                 directory.beginMarkingForFullCollection();
410                 return IterationStatus::Continue;
411             });
412 
413         if (UNLIKELY(nextVersion(m_markingVersion) == initialVersion)) {
414             forEachBlock(
415                 [&amp;] (MarkedBlock::Handle* handle) {
416                     handle-&gt;block().resetMarks();
417                 });
418         }
419 
420         m_markingVersion = nextVersion(m_markingVersion);
421 
422         for (PreciseAllocation* allocation : m_preciseAllocations)
423             allocation-&gt;flip();
424     }
425 
426     if (ASSERT_ENABLED) {
427         forEachBlock(
428             [&amp;] (MarkedBlock::Handle* block) {
429                 if (block-&gt;areMarksStale())
430                     return;
431                 ASSERT(!block-&gt;isFreeListed());
432             });
433     }
434 
435     m_isMarking = true;
436 }
437 
438 void MarkedSpace::endMarking()
439 {
440     if (UNLIKELY(nextVersion(m_newlyAllocatedVersion) == initialVersion)) {
441         forEachBlock(
442             [&amp;] (MarkedBlock::Handle* handle) {
443                 handle-&gt;block().resetAllocated();
444             });
445     }
446 
447     m_newlyAllocatedVersion = nextVersion(m_newlyAllocatedVersion);
448 
449     for (unsigned i = m_preciseAllocationsOffsetForThisCollection; i &lt; m_preciseAllocations.size(); ++i)
450         m_preciseAllocations[i]-&gt;clearNewlyAllocated();
451 
452     if (ASSERT_ENABLED) {
453         for (PreciseAllocation* allocation : m_preciseAllocations)
454             ASSERT_UNUSED(allocation, !allocation-&gt;isNewlyAllocated());
455     }
456 
457     forEachDirectory(
458         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
459             directory.endMarking();
460             return IterationStatus::Continue;
461         });
462 
463     m_isMarking = false;
464 }
465 
466 void MarkedSpace::willStartIterating()
467 {
468     ASSERT(!isIterating());
469     stopAllocating();
470     m_isIterating = true;
471 }
472 
473 void MarkedSpace::didFinishIterating()
474 {
475     ASSERT(isIterating());
476     resumeAllocating();
477     m_isIterating = false;
478 }
479 
480 size_t MarkedSpace::objectCount()
481 {
482     size_t result = 0;
483     forEachBlock(
484         [&amp;] (MarkedBlock::Handle* block) {
485             result += block-&gt;markCount();
486         });
487     for (PreciseAllocation* allocation : m_preciseAllocations) {
488         if (allocation-&gt;isMarked())
489             result++;
490     }
491     return result;
492 }
493 
494 size_t MarkedSpace::size()
495 {
496     size_t result = 0;
497     forEachBlock(
498         [&amp;] (MarkedBlock::Handle* block) {
499             result += block-&gt;markCount() * block-&gt;cellSize();
500         });
501     for (PreciseAllocation* allocation : m_preciseAllocations) {
502         if (allocation-&gt;isMarked())
503             result += allocation-&gt;cellSize();
504     }
505     return result;
506 }
507 
508 size_t MarkedSpace::capacity()
509 {
510     return m_capacity;
511 }
512 
513 void MarkedSpace::addActiveWeakSet(WeakSet* weakSet)
514 {
515     // We conservatively assume that the WeakSet should belong in the new set. In fact, some weak
516     // sets might contain new weak handles even though they are tied to old objects. This slightly
517     // increases the amount of scanning that an eden collection would have to do, but the effect
518     // ought to be small.
519     m_newActiveWeakSets.append(weakSet);
520 }
521 
522 void MarkedSpace::didAddBlock(MarkedBlock::Handle* block)
523 {
524     // WARNING: This function is called before block is fully initialized. The block will not know
525     // its cellSize() or attributes(). The latter implies that you can&#39;t ask things like
526     // needsDestruction().
527     m_capacity += MarkedBlock::blockSize;
528     m_blocks.add(&amp;block-&gt;block());
529 }
530 
531 void MarkedSpace::didAllocateInBlock(MarkedBlock::Handle* block)
532 {
533     if (block-&gt;weakSet().isOnList()) {
534         block-&gt;weakSet().remove();
535         m_newActiveWeakSets.append(&amp;block-&gt;weakSet());
536     }
537 }
538 
539 void MarkedSpace::snapshotUnswept()
540 {
541     if (heap().collectionScope() == CollectionScope::Eden) {
542         forEachDirectory(
543             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
544                 directory.snapshotUnsweptForEdenCollection();
545                 return IterationStatus::Continue;
546             });
547     } else {
548         forEachDirectory(
549             [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
550                 directory.snapshotUnsweptForFullCollection();
551                 return IterationStatus::Continue;
552             });
553     }
554 }
555 
556 void MarkedSpace::assertNoUnswept()
557 {
558     if (!ASSERT_ENABLED)
559         return;
560     forEachDirectory(
561         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
562             directory.assertNoUnswept();
563             return IterationStatus::Continue;
564         });
565 }
566 
567 void MarkedSpace::dumpBits(PrintStream&amp; out)
568 {
569     forEachDirectory(
570         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
571             out.print(&quot;Bits for &quot;, directory, &quot;:\n&quot;);
572             directory.dumpBits(out);
573             return IterationStatus::Continue;
574         });
575 }
576 
577 void MarkedSpace::addBlockDirectory(const AbstractLocker&amp;, BlockDirectory* directory)
578 {
579     directory-&gt;setNextDirectory(nullptr);
580 
581     WTF::storeStoreFence();
582 
583     m_directories.append(std::mem_fn(&amp;BlockDirectory::setNextDirectory), directory);
584 }
585 
586 } // namespace JSC
    </pre>
  </body>
</html>