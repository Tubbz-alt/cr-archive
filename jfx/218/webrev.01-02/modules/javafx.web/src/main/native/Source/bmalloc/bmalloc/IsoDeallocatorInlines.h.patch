diff a/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/IsoDeallocatorInlines.h b/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/IsoDeallocatorInlines.h
--- a/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/IsoDeallocatorInlines.h
+++ b/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/IsoDeallocatorInlines.h
@@ -57,11 +57,11 @@
     // The batching delays the reclamation of the shared cells, which can make allocator mistakenly think that "we exhaust shared
     // cells because this is allocated a lot". Since the number of shared cells are limited, this immediate deallocation path
     // should be rarely taken. If we see frequent malloc-and-free pattern, we tier up the allocator from shared mode to fast mode.
     IsoPageBase* page = IsoPageBase::pageFor(ptr);
     if (page->isShared()) {
-        std::lock_guard<Mutex> locker(*m_lock);
+        LockHolder locker(*m_lock);
         static_cast<IsoSharedPage*>(page)->free<Config>(locker, handle, ptr);
         return;
     }
 
     if (m_objectLog.size() == m_objectLog.capacity())
@@ -71,14 +71,14 @@
 }
 
 template<typename Config>
 BNO_INLINE void IsoDeallocator<Config>::scavenge()
 {
-    std::lock_guard<Mutex> locker(*m_lock);
+    LockHolder locker(*m_lock);
 
     for (void* ptr : m_objectLog)
-        IsoPage<Config>::pageFor(ptr)->free(ptr);
+        IsoPage<Config>::pageFor(ptr)->free(locker, ptr);
     m_objectLog.clear();
 }
 
 } // namespace bmalloc
 
