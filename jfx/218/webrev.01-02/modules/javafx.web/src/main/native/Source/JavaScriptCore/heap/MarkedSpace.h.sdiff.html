<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="MarkedSpace.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedSpaceInlines.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedSpace.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;BlockDirectory.h&quot;
 25 #include &quot;IterationStatus.h&quot;
<span class="line-removed"> 26 #include &quot;LargeAllocation.h&quot;</span>
 27 #include &quot;MarkedBlock.h&quot;
 28 #include &quot;MarkedBlockSet.h&quot;

 29 #include &lt;array&gt;
 30 #include &lt;wtf/Bag.h&gt;
 31 #include &lt;wtf/HashSet.h&gt;
 32 #include &lt;wtf/Noncopyable.h&gt;
 33 #include &lt;wtf/RetainPtr.h&gt;
 34 #include &lt;wtf/SentinelLinkedList.h&gt;
 35 #include &lt;wtf/SinglyLinkedListWithTail.h&gt;
 36 #include &lt;wtf/Vector.h&gt;
 37 
 38 namespace JSC {
 39 
 40 class CompleteSubspace;
 41 class Heap;

 42 class HeapIterationScope;

 43 class LLIntOffsetsExtractor;
 44 class Subspace;
 45 class WeakSet;
 46 
 47 typedef uint32_t HeapVersion;
 48 
 49 class MarkedSpace {
 50     WTF_MAKE_NONCOPYABLE(MarkedSpace);
 51 public:
 52     // sizeStep is really a synonym for atomSize; it&#39;s no accident that they are the same.
 53     static constexpr size_t sizeStep = MarkedBlock::atomSize;
 54 
 55     // Sizes up to this amount get a size class for each size step.
 56     static constexpr size_t preciseCutoff = 80;
 57 
 58     // The amount of available payload in a block is the block&#39;s size minus the footer.
 59     static constexpr size_t blockPayload = MarkedBlock::payloadSize;
 60 
 61     // The largest cell we&#39;re willing to allocate in a MarkedBlock the &quot;normal way&quot; (i.e. using size
 62     // classes, rather than a large allocation) is half the size of the payload, rounded down. This
</pre>
<hr />
<pre>
 76         if (version == nullVersion)
 77             version = initialVersion;
 78         return version;
 79     }
 80 
 81     static size_t sizeClassToIndex(size_t size)
 82     {
 83         return (size + sizeStep - 1) / sizeStep;
 84     }
 85 
 86     static size_t indexToSizeClass(size_t index)
 87     {
 88         size_t result = index * sizeStep;
 89         ASSERT(sizeClassToIndex(result) == index);
 90         return result;
 91     }
 92 
 93     MarkedSpace(Heap*);
 94     ~MarkedSpace();
 95 
<span class="line-modified"> 96     Heap* heap() const { return m_heap; }</span>
 97 
 98     void lastChanceToFinalize(); // Must call stopAllocatingForGood first.
 99     void freeMemory();
100 
101     static size_t optimalSizeFor(size_t);
102 
103     void prepareForAllocation();
104 
105     void visitWeakSets(SlotVisitor&amp;);
106     void reapWeakSets();
107 
108     MarkedBlockSet&amp; blocks() { return m_blocks; }
109 
110     void willStartIterating();
111     bool isIterating() const { return m_isIterating; }
112     void didFinishIterating();
113 
114     void stopAllocating();
115     void stopAllocatingForGood();
116     void resumeAllocating(); // If we just stopped allocation but we didn&#39;t do a collection, we need to resume allocation.
</pre>
<hr />
<pre>
121 
122     typedef HashSet&lt;MarkedBlock*&gt;::iterator BlockIterator;
123 
124     template&lt;typename Functor&gt; void forEachLiveCell(HeapIterationScope&amp;, const Functor&amp;);
125     template&lt;typename Functor&gt; void forEachDeadCell(HeapIterationScope&amp;, const Functor&amp;);
126     template&lt;typename Functor&gt; void forEachBlock(const Functor&amp;);
127     template&lt;typename Functor&gt; void forEachSubspace(const Functor&amp;);
128 
129     void shrink();
130     void freeBlock(MarkedBlock::Handle*);
131     void freeOrShrinkBlock(MarkedBlock::Handle*);
132 
133     void didAddBlock(MarkedBlock::Handle*);
134     void didConsumeFreeList(MarkedBlock::Handle*);
135     void didAllocateInBlock(MarkedBlock::Handle*);
136 
137     void beginMarking();
138     void endMarking();
139     void snapshotUnswept();
140     void clearNewlyAllocated();
<span class="line-modified">141     void sweep();</span>
<span class="line-modified">142     void sweepLargeAllocations();</span>
143     void assertNoUnswept();
144     size_t objectCount();
145     size_t size();
146     size_t capacity();
147 
148     bool isPagedOut(MonotonicTime deadline);
149 
150     HeapVersion markingVersion() const { return m_markingVersion; }
151     HeapVersion newlyAllocatedVersion() const { return m_newlyAllocatedVersion; }
152 
<span class="line-modified">153     const Vector&lt;LargeAllocation*&gt;&amp; largeAllocations() const { return m_largeAllocations; }</span>
<span class="line-modified">154     unsigned largeAllocationsNurseryOffset() const { return m_largeAllocationsNurseryOffset; }</span>
<span class="line-modified">155     unsigned largeAllocationsOffsetForThisCollection() const { return m_largeAllocationsOffsetForThisCollection; }</span>



156 
157     // These are cached pointers and offsets for quickly searching the large allocations that are
158     // relevant to this collection.
<span class="line-modified">159     LargeAllocation** largeAllocationsForThisCollectionBegin() const { return m_largeAllocationsForThisCollectionBegin; }</span>
<span class="line-modified">160     LargeAllocation** largeAllocationsForThisCollectionEnd() const { return m_largeAllocationsForThisCollectionEnd; }</span>
<span class="line-modified">161     unsigned largeAllocationsForThisCollectionSize() const { return m_largeAllocationsForThisCollectionSize; }</span>
162 
163     BlockDirectory* firstDirectory() const { return m_directories.first(); }
164 
165     Lock&amp; directoryLock() { return m_directoryLock; }
166     void addBlockDirectory(const AbstractLocker&amp;, BlockDirectory*);
167 
168     // When this is true it means that we have flipped but the mark bits haven&#39;t converged yet.
169     bool isMarking() const { return m_isMarking; }
170 
171     WeakSet* activeWeakSetsBegin() { return m_activeWeakSets.begin(); }
172     WeakSet* activeWeakSetsEnd() { return m_activeWeakSets.end(); }
173     WeakSet* newActiveWeakSetsBegin() { return m_newActiveWeakSets.begin(); }
174     WeakSet* newActiveWeakSetsEnd() { return m_newActiveWeakSets.end(); }
175 
176     void dumpBits(PrintStream&amp; = WTF::dataFile());
177 
178     JS_EXPORT_PRIVATE static std::array&lt;size_t, numSizeClasses&gt; s_sizeClassForSizeStep;
179 
180 private:
181     friend class CompleteSubspace;
182     friend class LLIntOffsetsExtractor;
183     friend class JIT;
184     friend class WeakSet;
185     friend class Subspace;

186 
187     // Use this version when calling from within the GC where we know that the directories
188     // have already been stopped.
189     template&lt;typename Functor&gt; void forEachLiveCell(const Functor&amp;);
190 
191     static void initializeSizeClassForStepSize();
192 
193     void initializeSubspace(Subspace&amp;);
194 
195     template&lt;typename Functor&gt; inline void forEachDirectory(const Functor&amp;);
196 
197     void addActiveWeakSet(WeakSet*);
198 
199     Vector&lt;Subspace*&gt; m_subspaces;
200 
<span class="line-modified">201     Vector&lt;LargeAllocation*&gt; m_largeAllocations;</span>
<span class="line-modified">202     unsigned m_largeAllocationsNurseryOffset { 0 };</span>
<span class="line-modified">203     unsigned m_largeAllocationsOffsetForThisCollection { 0 };</span>
<span class="line-modified">204     unsigned m_largeAllocationsNurseryOffsetForSweep { 0 };</span>
<span class="line-modified">205     unsigned m_largeAllocationsForThisCollectionSize { 0 };</span>
<span class="line-modified">206     LargeAllocation** m_largeAllocationsForThisCollectionBegin { nullptr };</span>
<span class="line-modified">207     LargeAllocation** m_largeAllocationsForThisCollectionEnd { nullptr };</span>

208 
<span class="line-removed">209     Heap* m_heap;</span>
210     size_t m_capacity { 0 };
211     HeapVersion m_markingVersion { initialVersion };
212     HeapVersion m_newlyAllocatedVersion { initialVersion };
213     bool m_isIterating { false };
214     bool m_isMarking { false };
215     Lock m_directoryLock;
216     MarkedBlockSet m_blocks;
217 
218     SentinelLinkedList&lt;WeakSet, BasicRawSentinelNode&lt;WeakSet&gt;&gt; m_activeWeakSets;
219     SentinelLinkedList&lt;WeakSet, BasicRawSentinelNode&lt;WeakSet&gt;&gt; m_newActiveWeakSets;
220 
221     SinglyLinkedListWithTail&lt;BlockDirectory&gt; m_directories;
222 
223     friend class HeapVerifier;
224 };
225 
226 template &lt;typename Functor&gt; inline void MarkedSpace::forEachBlock(const Functor&amp; functor)
227 {
228     forEachDirectory(
229         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
</pre>
</td>
<td>
<hr />
<pre>
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;BlockDirectory.h&quot;
 25 #include &quot;IterationStatus.h&quot;

 26 #include &quot;MarkedBlock.h&quot;
 27 #include &quot;MarkedBlockSet.h&quot;
<span class="line-added"> 28 #include &quot;PreciseAllocation.h&quot;</span>
 29 #include &lt;array&gt;
 30 #include &lt;wtf/Bag.h&gt;
 31 #include &lt;wtf/HashSet.h&gt;
 32 #include &lt;wtf/Noncopyable.h&gt;
 33 #include &lt;wtf/RetainPtr.h&gt;
 34 #include &lt;wtf/SentinelLinkedList.h&gt;
 35 #include &lt;wtf/SinglyLinkedListWithTail.h&gt;
 36 #include &lt;wtf/Vector.h&gt;
 37 
 38 namespace JSC {
 39 
 40 class CompleteSubspace;
 41 class Heap;
<span class="line-added"> 42 class HeapCell;</span>
 43 class HeapIterationScope;
<span class="line-added"> 44 class IsoSubspace;</span>
 45 class LLIntOffsetsExtractor;
 46 class Subspace;
 47 class WeakSet;
 48 
 49 typedef uint32_t HeapVersion;
 50 
 51 class MarkedSpace {
 52     WTF_MAKE_NONCOPYABLE(MarkedSpace);
 53 public:
 54     // sizeStep is really a synonym for atomSize; it&#39;s no accident that they are the same.
 55     static constexpr size_t sizeStep = MarkedBlock::atomSize;
 56 
 57     // Sizes up to this amount get a size class for each size step.
 58     static constexpr size_t preciseCutoff = 80;
 59 
 60     // The amount of available payload in a block is the block&#39;s size minus the footer.
 61     static constexpr size_t blockPayload = MarkedBlock::payloadSize;
 62 
 63     // The largest cell we&#39;re willing to allocate in a MarkedBlock the &quot;normal way&quot; (i.e. using size
 64     // classes, rather than a large allocation) is half the size of the payload, rounded down. This
</pre>
<hr />
<pre>
 78         if (version == nullVersion)
 79             version = initialVersion;
 80         return version;
 81     }
 82 
 83     static size_t sizeClassToIndex(size_t size)
 84     {
 85         return (size + sizeStep - 1) / sizeStep;
 86     }
 87 
 88     static size_t indexToSizeClass(size_t index)
 89     {
 90         size_t result = index * sizeStep;
 91         ASSERT(sizeClassToIndex(result) == index);
 92         return result;
 93     }
 94 
 95     MarkedSpace(Heap*);
 96     ~MarkedSpace();
 97 
<span class="line-modified"> 98     Heap&amp; heap() const;</span>
 99 
100     void lastChanceToFinalize(); // Must call stopAllocatingForGood first.
101     void freeMemory();
102 
103     static size_t optimalSizeFor(size_t);
104 
105     void prepareForAllocation();
106 
107     void visitWeakSets(SlotVisitor&amp;);
108     void reapWeakSets();
109 
110     MarkedBlockSet&amp; blocks() { return m_blocks; }
111 
112     void willStartIterating();
113     bool isIterating() const { return m_isIterating; }
114     void didFinishIterating();
115 
116     void stopAllocating();
117     void stopAllocatingForGood();
118     void resumeAllocating(); // If we just stopped allocation but we didn&#39;t do a collection, we need to resume allocation.
</pre>
<hr />
<pre>
123 
124     typedef HashSet&lt;MarkedBlock*&gt;::iterator BlockIterator;
125 
126     template&lt;typename Functor&gt; void forEachLiveCell(HeapIterationScope&amp;, const Functor&amp;);
127     template&lt;typename Functor&gt; void forEachDeadCell(HeapIterationScope&amp;, const Functor&amp;);
128     template&lt;typename Functor&gt; void forEachBlock(const Functor&amp;);
129     template&lt;typename Functor&gt; void forEachSubspace(const Functor&amp;);
130 
131     void shrink();
132     void freeBlock(MarkedBlock::Handle*);
133     void freeOrShrinkBlock(MarkedBlock::Handle*);
134 
135     void didAddBlock(MarkedBlock::Handle*);
136     void didConsumeFreeList(MarkedBlock::Handle*);
137     void didAllocateInBlock(MarkedBlock::Handle*);
138 
139     void beginMarking();
140     void endMarking();
141     void snapshotUnswept();
142     void clearNewlyAllocated();
<span class="line-modified">143     void sweepBlocks();</span>
<span class="line-modified">144     void sweepPreciseAllocations();</span>
145     void assertNoUnswept();
146     size_t objectCount();
147     size_t size();
148     size_t capacity();
149 
150     bool isPagedOut(MonotonicTime deadline);
151 
152     HeapVersion markingVersion() const { return m_markingVersion; }
153     HeapVersion newlyAllocatedVersion() const { return m_newlyAllocatedVersion; }
154 
<span class="line-modified">155     const Vector&lt;PreciseAllocation*&gt;&amp; preciseAllocations() const { return m_preciseAllocations; }</span>
<span class="line-modified">156     unsigned preciseAllocationsNurseryOffset() const { return m_preciseAllocationsNurseryOffset; }</span>
<span class="line-modified">157     unsigned preciseAllocationsOffsetForThisCollection() const { return m_preciseAllocationsOffsetForThisCollection; }</span>
<span class="line-added">158     HashSet&lt;HeapCell*&gt;* preciseAllocationSet() const { return m_preciseAllocationSet.get(); }</span>
<span class="line-added">159 </span>
<span class="line-added">160     void enablePreciseAllocationTracking();</span>
161 
162     // These are cached pointers and offsets for quickly searching the large allocations that are
163     // relevant to this collection.
<span class="line-modified">164     PreciseAllocation** preciseAllocationsForThisCollectionBegin() const { return m_preciseAllocationsForThisCollectionBegin; }</span>
<span class="line-modified">165     PreciseAllocation** preciseAllocationsForThisCollectionEnd() const { return m_preciseAllocationsForThisCollectionEnd; }</span>
<span class="line-modified">166     unsigned preciseAllocationsForThisCollectionSize() const { return m_preciseAllocationsForThisCollectionSize; }</span>
167 
168     BlockDirectory* firstDirectory() const { return m_directories.first(); }
169 
170     Lock&amp; directoryLock() { return m_directoryLock; }
171     void addBlockDirectory(const AbstractLocker&amp;, BlockDirectory*);
172 
173     // When this is true it means that we have flipped but the mark bits haven&#39;t converged yet.
174     bool isMarking() const { return m_isMarking; }
175 
176     WeakSet* activeWeakSetsBegin() { return m_activeWeakSets.begin(); }
177     WeakSet* activeWeakSetsEnd() { return m_activeWeakSets.end(); }
178     WeakSet* newActiveWeakSetsBegin() { return m_newActiveWeakSets.begin(); }
179     WeakSet* newActiveWeakSetsEnd() { return m_newActiveWeakSets.end(); }
180 
181     void dumpBits(PrintStream&amp; = WTF::dataFile());
182 
183     JS_EXPORT_PRIVATE static std::array&lt;size_t, numSizeClasses&gt; s_sizeClassForSizeStep;
184 
185 private:
186     friend class CompleteSubspace;
187     friend class LLIntOffsetsExtractor;
188     friend class JIT;
189     friend class WeakSet;
190     friend class Subspace;
<span class="line-added">191     friend class IsoSubspace;</span>
192 
193     // Use this version when calling from within the GC where we know that the directories
194     // have already been stopped.
195     template&lt;typename Functor&gt; void forEachLiveCell(const Functor&amp;);
196 
197     static void initializeSizeClassForStepSize();
198 
199     void initializeSubspace(Subspace&amp;);
200 
201     template&lt;typename Functor&gt; inline void forEachDirectory(const Functor&amp;);
202 
203     void addActiveWeakSet(WeakSet*);
204 
205     Vector&lt;Subspace*&gt; m_subspaces;
206 
<span class="line-modified">207     std::unique_ptr&lt;HashSet&lt;HeapCell*&gt;&gt; m_preciseAllocationSet;</span>
<span class="line-modified">208     Vector&lt;PreciseAllocation*&gt; m_preciseAllocations;</span>
<span class="line-modified">209     unsigned m_preciseAllocationsNurseryOffset { 0 };</span>
<span class="line-modified">210     unsigned m_preciseAllocationsOffsetForThisCollection { 0 };</span>
<span class="line-modified">211     unsigned m_preciseAllocationsNurseryOffsetForSweep { 0 };</span>
<span class="line-modified">212     unsigned m_preciseAllocationsForThisCollectionSize { 0 };</span>
<span class="line-modified">213     PreciseAllocation** m_preciseAllocationsForThisCollectionBegin { nullptr };</span>
<span class="line-added">214     PreciseAllocation** m_preciseAllocationsForThisCollectionEnd { nullptr };</span>
215 

216     size_t m_capacity { 0 };
217     HeapVersion m_markingVersion { initialVersion };
218     HeapVersion m_newlyAllocatedVersion { initialVersion };
219     bool m_isIterating { false };
220     bool m_isMarking { false };
221     Lock m_directoryLock;
222     MarkedBlockSet m_blocks;
223 
224     SentinelLinkedList&lt;WeakSet, BasicRawSentinelNode&lt;WeakSet&gt;&gt; m_activeWeakSets;
225     SentinelLinkedList&lt;WeakSet, BasicRawSentinelNode&lt;WeakSet&gt;&gt; m_newActiveWeakSets;
226 
227     SinglyLinkedListWithTail&lt;BlockDirectory&gt; m_directories;
228 
229     friend class HeapVerifier;
230 };
231 
232 template &lt;typename Functor&gt; inline void MarkedSpace::forEachBlock(const Functor&amp; functor)
233 {
234     forEachDirectory(
235         [&amp;] (BlockDirectory&amp; directory) -&gt; IterationStatus {
</pre>
</td>
</tr>
</table>
<center><a href="MarkedSpace.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedSpaceInlines.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>