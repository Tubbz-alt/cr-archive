<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="ICStats.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JIT.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  57 #include &lt;wtf/SimpleStats.h&gt;
  58 
  59 namespace JSC {
  60 namespace JITInternal {
  61 static constexpr const bool verbose = false;
  62 }
  63 
  64 Seconds totalBaselineCompileTime;
  65 Seconds totalDFGCompileTime;
  66 Seconds totalFTLCompileTime;
  67 Seconds totalFTLDFGCompileTime;
  68 Seconds totalFTLB3CompileTime;
  69 
  70 void ctiPatchCallByReturnAddress(ReturnAddressPtr returnAddress, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  71 {
  72     MacroAssembler::repatchCall(
  73         CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
  74         newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  75 }
  76 
<span class="line-modified">  77 JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, unsigned loopOSREntryBytecodeOffset)</span>
  78     : JSInterfaceJIT(&amp;vm, codeBlock)
  79     , m_interpreter(vm.interpreter)
  80     , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
<span class="line-removed">  81     , m_bytecodeOffset(std::numeric_limits&lt;unsigned&gt;::max())</span>
  82     , m_pcToCodeOriginMapBuilder(vm)
  83     , m_canBeOptimized(false)
  84     , m_shouldEmitProfiling(false)
<span class="line-modified">  85     , m_shouldUseIndexMasking(Options::enableSpectreMitigations())</span>
<span class="line-removed">  86     , m_loopOSREntryBytecodeOffset(loopOSREntryBytecodeOffset)</span>
  87 {
  88 }
  89 
  90 JIT::~JIT()
  91 {
  92 }
  93 




















  94 void JIT::emitNotifyWrite(WatchpointSet* set)
  95 {
  96     if (!set || set-&gt;state() == IsInvalidated) {
  97         addSlowCase(Jump());
  98         return;
  99     }
 100 
 101     addSlowCase(branch8(NotEqual, AbsoluteAddress(set-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 102 }
 103 
 104 void JIT::emitNotifyWrite(GPRReg pointerToSet)
 105 {
 106     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 107 }
 108 
 109 void JIT::assertStackPointerOffset()
 110 {
<span class="line-modified"> 111     if (ASSERT_DISABLED)</span>
 112         return;
 113 
 114     addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
 115     Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
 116     breakpoint();
 117     ok.link(this);
 118 }
 119 
 120 #define NEXT_OPCODE(name) \
<span class="line-modified"> 121     m_bytecodeOffset += currentInstruction-&gt;size(); \</span>
 122     break;
 123 





 124 #define DEFINE_SLOW_OP(name) \
 125     case op_##name: { \
<span class="line-modified"> 126         if (m_bytecodeOffset &gt;= startBytecodeOffset) { \</span>
 127             JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
 128             slowPathCall.call(); \
 129         } \
<span class="line-modified"> 130         NEXT_OPCODE(op_##name); \</span>
 131     }
 132 
 133 #define DEFINE_OP(name) \
 134     case name: { \
<span class="line-modified"> 135         if (m_bytecodeOffset &gt;= startBytecodeOffset) { \</span>
 136             emit_##name(currentInstruction); \
 137         } \
<span class="line-modified"> 138         NEXT_OPCODE(name); \</span>
 139     }
 140 
 141 #define DEFINE_SLOWCASE_OP(name) \
 142     case name: { \
 143         emitSlow_##name(currentInstruction, iter); \
 144         NEXT_OPCODE(name); \
 145     }
 146 
 147 #define DEFINE_SLOWCASE_SLOW_OP(name) \
 148     case op_##name: { \
 149         emitSlowCaseCall(currentInstruction, iter, slow_path_##name); \
 150         NEXT_OPCODE(op_##name); \
 151     }
 152 
 153 void JIT::emitSlowCaseCall(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, SlowPathFunction stub)
 154 {
 155     linkAllSlowCases(iter);
 156 
 157     JITSlowPathCall slowPathCall(this, currentInstruction, stub);
 158     slowPathCall.call();
 159 }
 160 
 161 void JIT::privateCompileMainPass()
 162 {
 163     if (JITInternal::verbose)
 164         dataLog(&quot;Compiling &quot;, *m_codeBlock, &quot;\n&quot;);
 165 
 166     jitAssertTagsInPlace();
 167     jitAssertArgumentCountSane();
 168 
 169     auto&amp; instructions = m_codeBlock-&gt;instructions();
 170     unsigned instructionCount = m_codeBlock-&gt;instructions().size();
 171 
 172     m_callLinkInfoIndex = 0;
 173 
 174     VM&amp; vm = m_codeBlock-&gt;vm();
<span class="line-modified"> 175     unsigned startBytecodeOffset = 0;</span>
<span class="line-modified"> 176     if (m_loopOSREntryBytecodeOffset &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {</span>
 177         // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
 178         // This optimization would be invalid otherwise. When the LLInt determines it wants to
 179         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
 180         // was executing at when it kicked off our compilation. We only need to compile code for
 181         // anything reachable from that bytecode offset.
 182 
 183         // We only bother building the bytecode graph if it could save time and executable
 184         // memory. We pick an arbitrary offset where we deem this is profitable.
<span class="line-modified"> 185         if (m_loopOSREntryBytecodeOffset &gt;= 200) {</span>
 186             // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
 187             // Instead, we just find the minimum bytecode offset that is reachable, and
 188             // compile code from that bytecode offset onwards.
 189 
 190             BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
<span class="line-modified"> 191             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeOffset);</span>
 192             RELEASE_ASSERT(block);
 193 
 194             GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
<span class="line-modified"> 195             startBytecodeOffset = UINT_MAX;</span>
 196             worklist.push(block);
 197 
 198             while (BytecodeBasicBlock* block = worklist.pop()) {
<span class="line-modified"> 199                 startBytecodeOffset = std::min(startBytecodeOffset, block-&gt;leaderOffset());</span>
<span class="line-modified"> 200                 worklist.pushAll(block-&gt;successors());</span>

 201 
 202                 // Also add catch blocks for bytecodes that throw.
 203                 if (m_codeBlock-&gt;numberOfExceptionHandlers()) {
 204                     for (unsigned bytecodeOffset = block-&gt;leaderOffset(); bytecodeOffset &lt; block-&gt;leaderOffset() + block-&gt;totalLength();) {
 205                         auto instruction = instructions.at(bytecodeOffset);
<span class="line-modified"> 206                         if (auto* handler = m_codeBlock-&gt;handlerForBytecodeOffset(bytecodeOffset))</span>
 207                             worklist.push(graph.findBasicBlockWithLeaderOffset(handler-&gt;target));
 208 
 209                         bytecodeOffset += instruction-&gt;size();
 210                     }
 211                 }
 212             }
 213         }
 214     }
 215 
<span class="line-modified"> 216     for (m_bytecodeOffset = 0; m_bytecodeOffset &lt; instructionCount; ) {</span>
<span class="line-modified"> 217         if (m_bytecodeOffset == startBytecodeOffset &amp;&amp; startBytecodeOffset &gt; 0) {</span>


 218             // We&#39;ve proven all bytecode instructions up until here are unreachable.
 219             // Let&#39;s ensure that by crashing if it&#39;s ever hit.
 220             breakpoint();
 221         }
 222 
 223         if (m_disassembler)
<span class="line-modified"> 224             m_disassembler-&gt;setForBytecodeMainPath(m_bytecodeOffset, label());</span>
<span class="line-modified"> 225         const Instruction* currentInstruction = instructions.at(m_bytecodeOffset).ptr();</span>
<span class="line-modified"> 226         ASSERT_WITH_MESSAGE(currentInstruction-&gt;size(), &quot;privateCompileMainPass gone bad @ %d&quot;, m_bytecodeOffset);</span>
 227 
<span class="line-modified"> 228         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));</span>
 229 
 230 #if ENABLE(OPCODE_SAMPLING)
<span class="line-modified"> 231         if (m_bytecodeOffset &gt; 0) // Avoid the overhead of sampling op_enter twice.</span>
 232             sampleInstruction(currentInstruction);
 233 #endif
 234 
<span class="line-modified"> 235         m_labels[m_bytecodeOffset] = label();</span>
 236 
 237         if (JITInternal::verbose)
<span class="line-modified"> 238             dataLogF(&quot;Old JIT emitting code for bc#%u at offset 0x%lx.\n&quot;, m_bytecodeOffset, (long)debugOffset());</span>
 239 
 240         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
 241 
 242         if (UNLIKELY(m_compilation)) {
 243             add64(
 244                 TrustedImm32(1),
 245                 AbsoluteAddress(m_compilation-&gt;executionCounterFor(Profiler::OriginStack(Profiler::Origin(
<span class="line-modified"> 246                     m_compilation-&gt;bytecodes(), m_bytecodeOffset)))-&gt;address()));</span>
 247         }
 248 
 249         if (Options::eagerlyUpdateTopCallFrame())
 250             updateTopCallFrame();
 251 
<span class="line-modified"> 252         unsigned bytecodeOffset = m_bytecodeOffset;</span>
 253 #if ENABLE(MASM_PROBE)
 254         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 255             CodeBlock* codeBlock = m_codeBlock;
 256             probe([=] (Probe::Context&amp; ctx) {
 257                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 258             });
 259         }
 260 #endif
 261 
 262         switch (opcodeID) {
 263         DEFINE_SLOW_OP(in_by_val)
 264         DEFINE_SLOW_OP(less)
 265         DEFINE_SLOW_OP(lesseq)
 266         DEFINE_SLOW_OP(greater)
 267         DEFINE_SLOW_OP(greatereq)
 268         DEFINE_SLOW_OP(is_function)
 269         DEFINE_SLOW_OP(is_object_or_null)
 270         DEFINE_SLOW_OP(typeof)
 271         DEFINE_SLOW_OP(strcat)
 272         DEFINE_SLOW_OP(push_with_scope)
 273         DEFINE_SLOW_OP(create_lexical_environment)
 274         DEFINE_SLOW_OP(get_by_val_with_this)
 275         DEFINE_SLOW_OP(put_by_id_with_this)
 276         DEFINE_SLOW_OP(put_by_val_with_this)
 277         DEFINE_SLOW_OP(resolve_scope_for_hoisting_func_decl_in_eval)
 278         DEFINE_SLOW_OP(define_data_property)
 279         DEFINE_SLOW_OP(define_accessor_property)
 280         DEFINE_SLOW_OP(unreachable)
 281         DEFINE_SLOW_OP(throw_static_error)
 282         DEFINE_SLOW_OP(new_array_with_spread)
 283         DEFINE_SLOW_OP(new_array_buffer)
 284         DEFINE_SLOW_OP(spread)
 285         DEFINE_SLOW_OP(get_enumerable_length)
 286         DEFINE_SLOW_OP(has_generic_property)
 287         DEFINE_SLOW_OP(get_property_enumerator)
 288         DEFINE_SLOW_OP(to_index_string)
 289         DEFINE_SLOW_OP(create_direct_arguments)
 290         DEFINE_SLOW_OP(create_scoped_arguments)
 291         DEFINE_SLOW_OP(create_cloned_arguments)

 292         DEFINE_SLOW_OP(create_rest)





 293         DEFINE_SLOW_OP(pow)
 294 
 295         DEFINE_OP(op_add)
 296         DEFINE_OP(op_bitnot)
 297         DEFINE_OP(op_bitand)
 298         DEFINE_OP(op_bitor)
 299         DEFINE_OP(op_bitxor)
 300         DEFINE_OP(op_call)
 301         DEFINE_OP(op_tail_call)
 302         DEFINE_OP(op_call_eval)
 303         DEFINE_OP(op_call_varargs)
 304         DEFINE_OP(op_tail_call_varargs)
 305         DEFINE_OP(op_tail_call_forward_arguments)
 306         DEFINE_OP(op_construct_varargs)
 307         DEFINE_OP(op_catch)
 308         DEFINE_OP(op_construct)
 309         DEFINE_OP(op_create_this)
 310         DEFINE_OP(op_to_this)
 311         DEFINE_OP(op_get_argument)
 312         DEFINE_OP(op_argument_count)
</pre>
<hr />
<pre>
 346         DEFINE_OP(op_jneq_null)
 347         DEFINE_OP(op_jundefined_or_null)
 348         DEFINE_OP(op_jnundefined_or_null)
 349         DEFINE_OP(op_jneq_ptr)
 350         DEFINE_OP(op_jless)
 351         DEFINE_OP(op_jlesseq)
 352         DEFINE_OP(op_jgreater)
 353         DEFINE_OP(op_jgreatereq)
 354         DEFINE_OP(op_jnless)
 355         DEFINE_OP(op_jnlesseq)
 356         DEFINE_OP(op_jngreater)
 357         DEFINE_OP(op_jngreatereq)
 358         DEFINE_OP(op_jeq)
 359         DEFINE_OP(op_jneq)
 360         DEFINE_OP(op_jstricteq)
 361         DEFINE_OP(op_jnstricteq)
 362         DEFINE_OP(op_jbelow)
 363         DEFINE_OP(op_jbeloweq)
 364         DEFINE_OP(op_jtrue)
 365         DEFINE_OP(op_loop_hint)

 366         DEFINE_OP(op_nop)
 367         DEFINE_OP(op_super_sampler_begin)
 368         DEFINE_OP(op_super_sampler_end)
 369         DEFINE_OP(op_lshift)
 370         DEFINE_OP(op_mod)
 371         DEFINE_OP(op_mov)
 372         DEFINE_OP(op_mul)
 373         DEFINE_OP(op_negate)
 374         DEFINE_OP(op_neq)
 375         DEFINE_OP(op_neq_null)
 376         DEFINE_OP(op_new_array)
 377         DEFINE_OP(op_new_array_with_size)
 378         DEFINE_OP(op_new_func)
 379         DEFINE_OP(op_new_func_exp)
 380         DEFINE_OP(op_new_generator_func)
 381         DEFINE_OP(op_new_generator_func_exp)
 382         DEFINE_OP(op_new_async_func)
 383         DEFINE_OP(op_new_async_func_exp)
 384         DEFINE_OP(op_new_async_generator_func)
 385         DEFINE_OP(op_new_async_generator_func_exp)
 386         DEFINE_OP(op_new_object)
 387         DEFINE_OP(op_new_regexp)
 388         DEFINE_OP(op_not)
 389         DEFINE_OP(op_nstricteq)
 390         DEFINE_OP(op_dec)
 391         DEFINE_OP(op_inc)
 392         DEFINE_OP(op_profile_type)
 393         DEFINE_OP(op_profile_control_flow)
 394         DEFINE_OP(op_get_parent_scope)
 395         DEFINE_OP(op_put_by_id)
 396         DEFINE_OP(op_put_by_val_direct)
 397         DEFINE_OP(op_put_by_val)
 398         DEFINE_OP(op_put_getter_by_id)
 399         DEFINE_OP(op_put_setter_by_id)
 400         DEFINE_OP(op_put_getter_setter_by_id)
 401         DEFINE_OP(op_put_getter_by_val)
 402         DEFINE_OP(op_put_setter_by_val)




 403 
 404         DEFINE_OP(op_ret)
 405         DEFINE_OP(op_rshift)
 406         DEFINE_OP(op_unsigned)
 407         DEFINE_OP(op_urshift)
 408         DEFINE_OP(op_set_function_name)
 409         DEFINE_OP(op_stricteq)
 410         DEFINE_OP(op_sub)
 411         DEFINE_OP(op_switch_char)
 412         DEFINE_OP(op_switch_imm)
 413         DEFINE_OP(op_switch_string)
 414         DEFINE_OP(op_throw)
 415         DEFINE_OP(op_to_number)

 416         DEFINE_OP(op_to_string)
 417         DEFINE_OP(op_to_object)
 418         DEFINE_OP(op_to_primitive)
 419 
 420         DEFINE_OP(op_resolve_scope)
 421         DEFINE_OP(op_get_from_scope)
 422         DEFINE_OP(op_put_to_scope)
 423         DEFINE_OP(op_get_from_arguments)
 424         DEFINE_OP(op_put_to_arguments)
 425 
 426         DEFINE_OP(op_has_structure_property)
 427         DEFINE_OP(op_has_indexed_property)
 428         DEFINE_OP(op_get_direct_pname)
 429         DEFINE_OP(op_enumerator_structure_pname)
 430         DEFINE_OP(op_enumerator_generic_pname)
 431 
 432         DEFINE_OP(op_log_shadow_chicken_prologue)
 433         DEFINE_OP(op_log_shadow_chicken_tail)
 434         default:
 435             RELEASE_ASSERT_NOT_REACHED();
 436         }
 437 
 438         if (JITInternal::verbose)
 439             dataLog(&quot;At &quot;, bytecodeOffset, &quot;: &quot;, m_slowCases.size(), &quot;\n&quot;);
 440     }
 441 
 442     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 443 
 444 #ifndef NDEBUG
 445     // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified"> 446     m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max();</span>
 447 #endif
 448 }
 449 
 450 void JIT::privateCompileLinkPass()
 451 {
 452     unsigned jmpTableCount = m_jmpTable.size();
 453     for (unsigned i = 0; i &lt; jmpTableCount; ++i)
 454         m_jmpTable[i].from.linkTo(m_labels[m_jmpTable[i].toBytecodeOffset], this);
 455     m_jmpTable.clear();
 456 }
 457 
 458 void JIT::privateCompileSlowCases()
 459 {
 460     m_getByIdIndex = 0;

 461     m_getByIdWithThisIndex = 0;
 462     m_putByIdIndex = 0;
 463     m_inByIdIndex = 0;
 464     m_instanceOfIndex = 0;
 465     m_byValInstructionIndex = 0;
 466     m_callLinkInfoIndex = 0;
 467 





 468     for (Vector&lt;SlowCaseEntry&gt;::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
<span class="line-modified"> 469         m_bytecodeOffset = iter-&gt;to;</span>
 470 
<span class="line-modified"> 471         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));</span>
 472 
<span class="line-modified"> 473         unsigned firstTo = m_bytecodeOffset;</span>
 474 
<span class="line-modified"> 475         const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(m_bytecodeOffset).ptr();</span>
 476 
<span class="line-modified"> 477         RareCaseProfile* rareCaseProfile = 0;</span>
 478         if (shouldEmitProfiling())
<span class="line-modified"> 479             rareCaseProfile = m_codeBlock-&gt;addRareCaseProfile(m_bytecodeOffset);</span>
 480 
 481         if (JITInternal::verbose)
<span class="line-modified"> 482             dataLogF(&quot;Old JIT emitting slow code for bc#%u at offset 0x%lx.\n&quot;, m_bytecodeOffset, (long)debugOffset());</span>
 483 
 484         if (m_disassembler)
<span class="line-modified"> 485             m_disassembler-&gt;setForBytecodeSlowPath(m_bytecodeOffset, label());</span>
 486 
 487 #if ENABLE(MASM_PROBE)
 488         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 489             OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
<span class="line-modified"> 490             unsigned bytecodeOffset = m_bytecodeOffset;</span>
 491             CodeBlock* codeBlock = m_codeBlock;
 492             probe([=] (Probe::Context&amp; ctx) {
 493                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] SLOW &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 494             });
 495         }
 496 #endif
 497 
 498         switch (currentInstruction-&gt;opcodeID()) {
 499         DEFINE_SLOWCASE_OP(op_add)
 500         DEFINE_SLOWCASE_OP(op_call)
 501         DEFINE_SLOWCASE_OP(op_tail_call)
 502         DEFINE_SLOWCASE_OP(op_call_eval)
 503         DEFINE_SLOWCASE_OP(op_call_varargs)
 504         DEFINE_SLOWCASE_OP(op_tail_call_varargs)
 505         DEFINE_SLOWCASE_OP(op_tail_call_forward_arguments)
 506         DEFINE_SLOWCASE_OP(op_construct_varargs)
 507         DEFINE_SLOWCASE_OP(op_construct)
 508         DEFINE_SLOWCASE_OP(op_eq)
 509         DEFINE_SLOWCASE_OP(op_try_get_by_id)
 510         DEFINE_SLOWCASE_OP(op_in_by_id)
 511         DEFINE_SLOWCASE_OP(op_get_by_id)
 512         DEFINE_SLOWCASE_OP(op_get_by_id_with_this)
 513         DEFINE_SLOWCASE_OP(op_get_by_id_direct)
 514         DEFINE_SLOWCASE_OP(op_get_by_val)
 515         DEFINE_SLOWCASE_OP(op_instanceof)
 516         DEFINE_SLOWCASE_OP(op_instanceof_custom)
 517         DEFINE_SLOWCASE_OP(op_jless)
 518         DEFINE_SLOWCASE_OP(op_jlesseq)
 519         DEFINE_SLOWCASE_OP(op_jgreater)
 520         DEFINE_SLOWCASE_OP(op_jgreatereq)
 521         DEFINE_SLOWCASE_OP(op_jnless)
 522         DEFINE_SLOWCASE_OP(op_jnlesseq)
 523         DEFINE_SLOWCASE_OP(op_jngreater)
 524         DEFINE_SLOWCASE_OP(op_jngreatereq)
 525         DEFINE_SLOWCASE_OP(op_jeq)
 526         DEFINE_SLOWCASE_OP(op_jneq)
 527         DEFINE_SLOWCASE_OP(op_jstricteq)
 528         DEFINE_SLOWCASE_OP(op_jnstricteq)
 529         DEFINE_SLOWCASE_OP(op_loop_hint)
<span class="line-modified"> 530         DEFINE_SLOWCASE_OP(op_enter)</span>
 531         DEFINE_SLOWCASE_OP(op_mod)
 532         DEFINE_SLOWCASE_OP(op_mul)
 533         DEFINE_SLOWCASE_OP(op_negate)
 534         DEFINE_SLOWCASE_OP(op_neq)
 535         DEFINE_SLOWCASE_OP(op_new_object)
 536         DEFINE_SLOWCASE_OP(op_put_by_id)
 537         case op_put_by_val_direct:
 538         DEFINE_SLOWCASE_OP(op_put_by_val)
 539         DEFINE_SLOWCASE_OP(op_sub)
 540         DEFINE_SLOWCASE_OP(op_has_indexed_property)
 541         DEFINE_SLOWCASE_OP(op_get_from_scope)
 542         DEFINE_SLOWCASE_OP(op_put_to_scope)
 543 
 544         DEFINE_SLOWCASE_SLOW_OP(unsigned)
 545         DEFINE_SLOWCASE_SLOW_OP(inc)
 546         DEFINE_SLOWCASE_SLOW_OP(dec)
 547         DEFINE_SLOWCASE_SLOW_OP(bitnot)
 548         DEFINE_SLOWCASE_SLOW_OP(bitand)
 549         DEFINE_SLOWCASE_SLOW_OP(bitor)
 550         DEFINE_SLOWCASE_SLOW_OP(bitxor)
 551         DEFINE_SLOWCASE_SLOW_OP(lshift)
 552         DEFINE_SLOWCASE_SLOW_OP(rshift)
 553         DEFINE_SLOWCASE_SLOW_OP(urshift)
 554         DEFINE_SLOWCASE_SLOW_OP(div)
 555         DEFINE_SLOWCASE_SLOW_OP(create_this)



 556         DEFINE_SLOWCASE_SLOW_OP(to_this)
 557         DEFINE_SLOWCASE_SLOW_OP(to_primitive)
 558         DEFINE_SLOWCASE_SLOW_OP(to_number)

 559         DEFINE_SLOWCASE_SLOW_OP(to_string)
 560         DEFINE_SLOWCASE_SLOW_OP(to_object)
 561         DEFINE_SLOWCASE_SLOW_OP(not)
 562         DEFINE_SLOWCASE_SLOW_OP(stricteq)
 563         DEFINE_SLOWCASE_SLOW_OP(nstricteq)
 564         DEFINE_SLOWCASE_SLOW_OP(get_direct_pname)
 565         DEFINE_SLOWCASE_SLOW_OP(has_structure_property)
 566         DEFINE_SLOWCASE_SLOW_OP(resolve_scope)
 567         DEFINE_SLOWCASE_SLOW_OP(check_tdz)

 568 
 569         default:
 570             RELEASE_ASSERT_NOT_REACHED();
 571         }
 572 
 573         if (JITInternal::verbose)
 574             dataLog(&quot;At &quot;, firstTo, &quot; slow: &quot;, iter - m_slowCases.begin(), &quot;\n&quot;);
 575 
 576         RELEASE_ASSERT_WITH_MESSAGE(iter == m_slowCases.end() || firstTo != iter-&gt;to, &quot;Not enough jumps linked in slow case codegen.&quot;);
 577         RELEASE_ASSERT_WITH_MESSAGE(firstTo == (iter - 1)-&gt;to, &quot;Too many jumps linked in slow case codegen.&quot;);
 578 
 579         if (shouldEmitProfiling())
 580             add32(TrustedImm32(1), AbsoluteAddress(&amp;rareCaseProfile-&gt;m_counter));
 581 
 582         emitJumpSlowToHot(jump(), 0);

 583     }
 584 

 585     RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
 586     RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
 587     RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
 588     RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
 589     RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
 590     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 591 



 592 #ifndef NDEBUG
 593     // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified"> 594     m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max();</span>
 595 #endif
 596 }
 597 
 598 void JIT::compileWithoutLinking(JITCompilationEffort effort)
 599 {
 600     MonotonicTime before { };
 601     if (UNLIKELY(computeCompileTimes()))
 602         before = MonotonicTime::now();
 603 
 604     DFG::CapabilityLevel level = m_codeBlock-&gt;capabilityLevel();
 605     switch (level) {
 606     case DFG::CannotCompile:
 607         m_canBeOptimized = false;
 608         m_canBeOptimizedOrInlined = false;
 609         m_shouldEmitProfiling = false;
 610         break;
 611     case DFG::CanCompile:
 612     case DFG::CanCompileAndInline:
 613         m_canBeOptimized = true;
 614         m_canBeOptimizedOrInlined = true;
</pre>
<hr />
<pre>
 625     case EvalCode:
 626         m_codeBlock-&gt;m_shouldAlwaysBeInlined = false;
 627         break;
 628     case FunctionCode:
 629         // We could have already set it to false because we detected an uninlineable call.
 630         // Don&#39;t override that observation.
 631         m_codeBlock-&gt;m_shouldAlwaysBeInlined &amp;= canInline(level) &amp;&amp; DFG::mightInlineFunction(m_codeBlock);
 632         break;
 633     }
 634 
 635     if (UNLIKELY(Options::dumpDisassembly() || (m_vm-&gt;m_perBytecodeProfiler &amp;&amp; Options::disassembleBaselineForProfiler())))
 636         m_disassembler = makeUnique&lt;JITDisassembler&gt;(m_codeBlock);
 637     if (UNLIKELY(m_vm-&gt;m_perBytecodeProfiler)) {
 638         m_compilation = adoptRef(
 639             new Profiler::Compilation(
 640                 m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
 641                 Profiler::Baseline));
 642         m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
 643     }
 644 
<span class="line-modified"> 645     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(0, nullptr));</span>
 646 
 647     Label entryLabel(this);
 648     if (m_disassembler)
 649         m_disassembler-&gt;setStartOfCode(entryLabel);
 650 
 651     // Just add a little bit of randomness to the codegen
 652     if (random() &amp; 1)
 653         nop();
 654 
 655     emitFunctionPrologue();
 656     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 657 
 658     Label beginLabel(this);
 659 
 660     sampleCodeBlock(m_codeBlock);
 661 #if ENABLE(OPCODE_SAMPLING)
 662     sampleInstruction(m_codeBlock-&gt;instructions().begin());
 663 #endif
 664 
 665     int frameTopOffset = stackPointerOffsetFor(m_codeBlock) * sizeof(Register);
 666     unsigned maxFrameSize = -frameTopOffset;
 667     addPtr(TrustedImm32(frameTopOffset), callFrameRegister, regT1);
 668     JumpList stackOverflow;
 669     if (UNLIKELY(maxFrameSize &gt; Options::reservedZoneSize()))
 670         stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
 671     stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm-&gt;addressOfSoftStackLimit()), regT1));
 672 
 673     move(regT1, stackPointerRegister);
 674     checkStackPointerAlignment();
<span class="line-removed"> 675     if (Options::zeroStackFrame())</span>
<span class="line-removed"> 676         clearStackFrame(callFrameRegister, stackPointerRegister, regT0, maxFrameSize);</span>
 677 
 678     emitSaveCalleeSaves();
 679     emitMaterializeTagCheckRegisters();
 680 
 681     if (m_codeBlock-&gt;codeType() == FunctionCode) {
<span class="line-modified"> 682         ASSERT(m_bytecodeOffset == std::numeric_limits&lt;unsigned&gt;::max());</span>
 683         if (shouldEmitProfiling()) {
 684             for (int argument = 0; argument &lt; m_codeBlock-&gt;numParameters(); ++argument) {
 685                 // If this is a constructor, then we want to put in a dummy profiling site (to
 686                 // keep things consistent) but we don&#39;t actually want to record the dummy value.
 687                 if (m_codeBlock-&gt;isConstructor() &amp;&amp; !argument)
 688                     continue;
 689                 int offset = CallFrame::argumentOffsetIncludingThis(argument) * static_cast&lt;int&gt;(sizeof(Register));
 690 #if USE(JSVALUE64)
 691                 load64(Address(callFrameRegister, offset), regT0);
 692 #elif USE(JSVALUE32_64)
 693                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.payload)), regT0);
 694                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.tag)), regT1);
 695 #endif
 696                 emitValueProfilingSite(m_codeBlock-&gt;valueProfileForArgument(argument));
 697             }
 698         }
 699     }
 700 
 701     RELEASE_ASSERT(!JITCode::isJIT(m_codeBlock-&gt;jitType()));
 702 
 703     privateCompileMainPass();
 704     privateCompileLinkPass();
 705     privateCompileSlowCases();
 706 
 707     if (m_disassembler)
 708         m_disassembler-&gt;setEndOfSlowPath(label());
 709     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 710 
 711     stackOverflow.link(this);
<span class="line-modified"> 712     m_bytecodeOffset = 0;</span>
 713     if (maxFrameExtentForSlowPathCall)
 714         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
 715     callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
 716 
 717     // If the number of parameters is 1, we never require arity fixup.
 718     bool requiresArityFixup = m_codeBlock-&gt;m_numParameters != 1;
 719     if (m_codeBlock-&gt;codeType() == FunctionCode &amp;&amp; requiresArityFixup) {
 720         m_arityCheck = label();
 721         store8(TrustedImm32(0), &amp;m_codeBlock-&gt;m_shouldAlwaysBeInlined);
 722         emitFunctionPrologue();
 723         emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 724 
<span class="line-modified"> 725         load32(payloadFor(CallFrameSlot::argumentCount), regT1);</span>
 726         branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock-&gt;m_numParameters)).linkTo(beginLabel, this);
 727 
<span class="line-modified"> 728         m_bytecodeOffset = 0;</span>
 729 
 730         if (maxFrameExtentForSlowPathCall)
 731             addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
<span class="line-modified"> 732         callOperationWithCallFrameRollbackOnException(m_codeBlock-&gt;isConstructor() ? operationConstructArityCheck : operationCallArityCheck);</span>
 733         if (maxFrameExtentForSlowPathCall)
 734             addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
 735         branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
 736         move(returnValueGPR, GPRInfo::argumentGPR0);
 737         emitNakedCall(m_vm-&gt;getCTIStub(arityFixupGenerator).retaggedCode&lt;NoPtrTag&gt;());
 738 
<span class="line-modified"> 739 #if !ASSERT_DISABLED</span>
<span class="line-modified"> 740         m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max(); // Reset this, in order to guard its use with ASSERTs.</span>
 741 #endif
 742 
 743         jump(beginLabel);
 744     } else
 745         m_arityCheck = entryLabel; // Never require arity fixup.
 746 
 747     ASSERT(m_jmpTable.isEmpty());
 748 
 749     privateCompileExceptionHandlers();
 750 
 751     if (m_disassembler)
 752         m_disassembler-&gt;setEndOfCode(label());
 753     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 754 
 755     m_linkBuffer = std::unique_ptr&lt;LinkBuffer&gt;(new LinkBuffer(*this, m_codeBlock, effort));
 756 
 757     MonotonicTime after { };
 758     if (UNLIKELY(computeCompileTimes())) {
 759         after = MonotonicTime::now();
 760 
 761         if (Options::reportTotalCompileTimes())
 762             totalBaselineCompileTime += after - before;
 763     }
 764     if (UNLIKELY(reportCompileTimes())) {
 765         CString codeBlockName = toCString(*m_codeBlock);
 766 
 767         dataLog(&quot;Optimized &quot;, codeBlockName, &quot; with Baseline JIT into &quot;, m_linkBuffer-&gt;size(), &quot; bytes in &quot;, (after - before).milliseconds(), &quot; ms.\n&quot;);
 768     }
 769 }
 770 
 771 CompilationResult JIT::link()
 772 {
 773     LinkBuffer&amp; patchBuffer = *m_linkBuffer;
 774 
 775     if (patchBuffer.didFailToAllocate())
 776         return CompilationFailed;
 777 
 778     // Translate vPC offsets into addresses in JIT generated code, for switch tables.
 779     for (auto&amp; record : m_switches) {
<span class="line-modified"> 780         unsigned bytecodeOffset = record.bytecodeOffset;</span>
 781 
 782         if (record.type != SwitchRecord::String) {
 783             ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
 784             ASSERT(record.jumpTable.simpleJumpTable-&gt;branchOffsets.size() == record.jumpTable.simpleJumpTable-&gt;ctiOffsets.size());
 785 
 786             auto* simpleJumpTable = record.jumpTable.simpleJumpTable;
 787             simpleJumpTable-&gt;ctiDefault = patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
 788 
 789             for (unsigned j = 0; j &lt; record.jumpTable.simpleJumpTable-&gt;branchOffsets.size(); ++j) {
 790                 unsigned offset = record.jumpTable.simpleJumpTable-&gt;branchOffsets[j];
 791                 simpleJumpTable-&gt;ctiOffsets[j] = offset
 792                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 793                     : simpleJumpTable-&gt;ctiDefault;
 794             }
 795         } else {
 796             ASSERT(record.type == SwitchRecord::String);
 797 
 798             auto* stringJumpTable = record.jumpTable.stringJumpTable;
 799             stringJumpTable-&gt;ctiDefault =
 800                 patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
</pre>
<hr />
<pre>
 803                 unsigned offset = location.branchOffset;
 804                 location.ctiOffset = offset
 805                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 806                     : stringJumpTable-&gt;ctiDefault;
 807             }
 808         }
 809     }
 810 
 811     for (size_t i = 0; i &lt; m_codeBlock-&gt;numberOfExceptionHandlers(); ++i) {
 812         HandlerInfo&amp; handler = m_codeBlock-&gt;exceptionHandler(i);
 813         // FIXME: &lt;rdar://problem/39433318&gt;.
 814         handler.nativeCode = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_labels[handler.target]);
 815     }
 816 
 817     for (auto&amp; record : m_calls) {
 818         if (record.callee)
 819             patchBuffer.link(record.from, record.callee);
 820     }
 821 
 822     finalizeInlineCaches(m_getByIds, patchBuffer);

 823     finalizeInlineCaches(m_getByIdsWithThis, patchBuffer);
 824     finalizeInlineCaches(m_putByIds, patchBuffer);
 825     finalizeInlineCaches(m_inByIds, patchBuffer);
 826     finalizeInlineCaches(m_instanceOfs, patchBuffer);
 827 
 828     if (m_byValCompilationInfo.size()) {
 829         CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt; exceptionHandler = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_exceptionHandler);
 830 
 831         for (const auto&amp; byValCompilationInfo : m_byValCompilationInfo) {
 832             PatchableJump patchableNotIndexJump = byValCompilationInfo.notIndexJump;
 833             auto notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;();
 834             if (Jump(patchableNotIndexJump).isSet())
 835                 notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(patchableNotIndexJump));
 836             auto badTypeJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.badTypeJump));
 837             auto doneTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.doneTarget));
 838             auto nextHotPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.nextHotPathTarget));
 839             auto slowPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.slowPathTarget));
 840 
 841             *byValCompilationInfo.byValInfo = ByValInfo(
 842                 byValCompilationInfo.bytecodeIndex,
 843                 notIndexJump,
 844                 badTypeJump,
 845                 exceptionHandler,
 846                 byValCompilationInfo.arrayMode,
 847                 byValCompilationInfo.arrayProfile,
 848                 doneTarget,
 849                 nextHotPathTarget,
 850                 slowPathTarget);
 851         }
 852     }
 853 
 854     for (auto&amp; compilationInfo : m_callCompilationInfo) {
 855         CallLinkInfo&amp; info = *compilationInfo.callLinkInfo;
 856         info.setCallLocations(
 857             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.callReturnLocation)),
 858             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathBegin)),
 859             patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathOther));
 860     }
 861 
<span class="line-modified"> 862     JITCodeMap jitCodeMap;</span>
<span class="line-modified"> 863     for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {</span>
<span class="line-modified"> 864         if (m_labels[bytecodeOffset].isSet())</span>
<span class="line-modified"> 865             jitCodeMap.append(bytecodeOffset, patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));</span>



 866     }
<span class="line-removed"> 867     jitCodeMap.finish();</span>
<span class="line-removed"> 868     m_codeBlock-&gt;setJITCodeMap(WTFMove(jitCodeMap));</span>
 869 
 870     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
 871 
<span class="line-modified"> 872     if (Options::dumpDisassembly()) {</span>
 873         m_disassembler-&gt;dump(patchBuffer);
 874         patchBuffer.didAlreadyDisassemble();
 875     }
 876     if (UNLIKELY(m_compilation)) {
 877         if (Options::disassembleBaselineForProfiler())
 878             m_disassembler-&gt;reportToProfiler(m_compilation.get(), patchBuffer);
 879         m_vm-&gt;m_perBytecodeProfiler-&gt;addCompilation(m_codeBlock, *m_compilation);
 880     }
 881 
 882     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
 883         m_codeBlock-&gt;setPCToCodeOriginMap(makeUnique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), patchBuffer));
 884 
 885     CodeRef&lt;JSEntryPtrTag&gt; result = FINALIZE_CODE(
 886         patchBuffer, JSEntryPtrTag,
 887         &quot;Baseline JIT code for %s&quot;, toCString(CodeBlockWithJITType(m_codeBlock, JITType::BaselineJIT)).data());
 888 
 889     m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
 890         static_cast&lt;double&gt;(result.size()) /
 891         static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));
 892 
<span class="line-modified"> 893     m_codeBlock-&gt;shrinkToFit(CodeBlock::LateShrink);</span>



 894     m_codeBlock-&gt;setJITCode(
 895         adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));
 896 
 897     if (JITInternal::verbose)
 898         dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
 899 
 900     return CompilationSuccessful;
 901 }
 902 
 903 CompilationResult JIT::privateCompile(JITCompilationEffort effort)
 904 {
 905     doMainThreadPreparationBeforeCompile();
 906     compileWithoutLinking(effort);
 907     return link();
 908 }
 909 
 910 void JIT::privateCompileExceptionHandlers()
 911 {
 912     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
 913         m_exceptionChecksWithCallFrameRollback.link(this);
 914 
 915         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 916 
<span class="line-modified"> 917         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).</span>
<span class="line-removed"> 918 </span>
 919         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified"> 920         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);</span>
<span class="line-modified"> 921 </span>
<span class="line-removed"> 922 #if CPU(X86)</span>
<span class="line-removed"> 923         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!</span>
<span class="line-removed"> 924         poke(GPRInfo::argumentGPR0);</span>
<span class="line-removed"> 925         poke(GPRInfo::argumentGPR1, 1);</span>
<span class="line-removed"> 926 #endif</span>
<span class="line-removed"> 927         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));</span>
 928         jumpToExceptionHandler(vm());
 929     }
 930 
 931     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
 932         m_exceptionHandler = label();
 933         m_exceptionChecks.link(this);
 934 
 935         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 936 
<span class="line-modified"> 937         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).</span>
 938         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified"> 939         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);</span>
<span class="line-modified"> 940 </span>
<span class="line-removed"> 941 #if CPU(X86)</span>
<span class="line-removed"> 942         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!</span>
<span class="line-removed"> 943         poke(GPRInfo::argumentGPR0);</span>
<span class="line-removed"> 944         poke(GPRInfo::argumentGPR1, 1);</span>
<span class="line-removed"> 945 #endif</span>
<span class="line-removed"> 946         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));</span>
 947         jumpToExceptionHandler(vm());
 948     }
 949 }
 950 
 951 void JIT::doMainThreadPreparationBeforeCompile()
 952 {
 953     // This ensures that we have the most up to date type information when performing typecheck optimizations for op_profile_type.
 954     if (m_vm-&gt;typeProfiler())
 955         m_vm-&gt;typeProfilerLog()-&gt;processLogEntries(*m_vm, &quot;Preparing for JIT compilation.&quot;_s);
 956 }
 957 
 958 unsigned JIT::frameRegisterCountFor(CodeBlock* codeBlock)
 959 {
 960     ASSERT(static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals()) == WTF::roundUpToMultipleOf(stackAlignmentRegisters(), static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals())));
 961 
 962     return roundLocalRegisterCountForFramePointerOffset(codeBlock-&gt;numCalleeLocals() + maxFrameExtentForSlowPathCallInRegisters);
 963 }
 964 
 965 int JIT::stackPointerOffsetFor(CodeBlock* codeBlock)
 966 {
</pre>
</td>
<td>
<hr />
<pre>
  57 #include &lt;wtf/SimpleStats.h&gt;
  58 
  59 namespace JSC {
  60 namespace JITInternal {
  61 static constexpr const bool verbose = false;
  62 }
  63 
  64 Seconds totalBaselineCompileTime;
  65 Seconds totalDFGCompileTime;
  66 Seconds totalFTLCompileTime;
  67 Seconds totalFTLDFGCompileTime;
  68 Seconds totalFTLB3CompileTime;
  69 
  70 void ctiPatchCallByReturnAddress(ReturnAddressPtr returnAddress, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  71 {
  72     MacroAssembler::repatchCall(
  73         CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
  74         newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  75 }
  76 
<span class="line-modified">  77 JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, BytecodeIndex loopOSREntryBytecodeIndex)</span>
  78     : JSInterfaceJIT(&amp;vm, codeBlock)
  79     , m_interpreter(vm.interpreter)
  80     , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)

  81     , m_pcToCodeOriginMapBuilder(vm)
  82     , m_canBeOptimized(false)
  83     , m_shouldEmitProfiling(false)
<span class="line-modified">  84     , m_loopOSREntryBytecodeIndex(loopOSREntryBytecodeIndex)</span>

  85 {
  86 }
  87 
  88 JIT::~JIT()
  89 {
  90 }
  91 
<span class="line-added">  92 #if ENABLE(DFG_JIT)</span>
<span class="line-added">  93 void JIT::emitEnterOptimizationCheck()</span>
<span class="line-added">  94 {</span>
<span class="line-added">  95     if (!canBeOptimized())</span>
<span class="line-added">  96         return;</span>
<span class="line-added">  97 </span>
<span class="line-added">  98     JumpList skipOptimize;</span>
<span class="line-added">  99 </span>
<span class="line-added"> 100     skipOptimize.append(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));</span>
<span class="line-added"> 101     ASSERT(!m_bytecodeIndex.offset());</span>
<span class="line-added"> 102 </span>
<span class="line-added"> 103     copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
<span class="line-added"> 104 </span>
<span class="line-added"> 105     callOperation(operationOptimize, &amp;vm(), m_bytecodeIndex.asBits());</span>
<span class="line-added"> 106     skipOptimize.append(branchTestPtr(Zero, returnValueGPR));</span>
<span class="line-added"> 107     farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-added"> 108     skipOptimize.link(this);</span>
<span class="line-added"> 109 }</span>
<span class="line-added"> 110 #endif</span>
<span class="line-added"> 111 </span>
 112 void JIT::emitNotifyWrite(WatchpointSet* set)
 113 {
 114     if (!set || set-&gt;state() == IsInvalidated) {
 115         addSlowCase(Jump());
 116         return;
 117     }
 118 
 119     addSlowCase(branch8(NotEqual, AbsoluteAddress(set-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 120 }
 121 
 122 void JIT::emitNotifyWrite(GPRReg pointerToSet)
 123 {
 124     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 125 }
 126 
 127 void JIT::assertStackPointerOffset()
 128 {
<span class="line-modified"> 129     if (!ASSERT_ENABLED)</span>
 130         return;
 131 
 132     addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
 133     Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
 134     breakpoint();
 135     ok.link(this);
 136 }
 137 
 138 #define NEXT_OPCODE(name) \
<span class="line-modified"> 139     m_bytecodeIndex = BytecodeIndex(m_bytecodeIndex.offset() + currentInstruction-&gt;size()); \</span>
 140     break;
 141 
<span class="line-added"> 142 #define NEXT_OPCODE_IN_MAIN(name) \</span>
<span class="line-added"> 143     if (previousSlowCasesSize != m_slowCases.size()) \</span>
<span class="line-added"> 144         ++m_bytecodeCountHavingSlowCase; \</span>
<span class="line-added"> 145     NEXT_OPCODE(name)</span>
<span class="line-added"> 146 </span>
 147 #define DEFINE_SLOW_OP(name) \
 148     case op_##name: { \
<span class="line-modified"> 149         if (m_bytecodeIndex &gt;= startBytecodeIndex) { \</span>
 150             JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
 151             slowPathCall.call(); \
 152         } \
<span class="line-modified"> 153         NEXT_OPCODE_IN_MAIN(op_##name); \</span>
 154     }
 155 
 156 #define DEFINE_OP(name) \
 157     case name: { \
<span class="line-modified"> 158         if (m_bytecodeIndex &gt;= startBytecodeIndex) { \</span>
 159             emit_##name(currentInstruction); \
 160         } \
<span class="line-modified"> 161         NEXT_OPCODE_IN_MAIN(name); \</span>
 162     }
 163 
 164 #define DEFINE_SLOWCASE_OP(name) \
 165     case name: { \
 166         emitSlow_##name(currentInstruction, iter); \
 167         NEXT_OPCODE(name); \
 168     }
 169 
 170 #define DEFINE_SLOWCASE_SLOW_OP(name) \
 171     case op_##name: { \
 172         emitSlowCaseCall(currentInstruction, iter, slow_path_##name); \
 173         NEXT_OPCODE(op_##name); \
 174     }
 175 
 176 void JIT::emitSlowCaseCall(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, SlowPathFunction stub)
 177 {
 178     linkAllSlowCases(iter);
 179 
 180     JITSlowPathCall slowPathCall(this, currentInstruction, stub);
 181     slowPathCall.call();
 182 }
 183 
 184 void JIT::privateCompileMainPass()
 185 {
 186     if (JITInternal::verbose)
 187         dataLog(&quot;Compiling &quot;, *m_codeBlock, &quot;\n&quot;);
 188 
 189     jitAssertTagsInPlace();
 190     jitAssertArgumentCountSane();
 191 
 192     auto&amp; instructions = m_codeBlock-&gt;instructions();
 193     unsigned instructionCount = m_codeBlock-&gt;instructions().size();
 194 
 195     m_callLinkInfoIndex = 0;
 196 
 197     VM&amp; vm = m_codeBlock-&gt;vm();
<span class="line-modified"> 198     BytecodeIndex startBytecodeIndex(0);</span>
<span class="line-modified"> 199     if (m_loopOSREntryBytecodeIndex &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {</span>
 200         // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
 201         // This optimization would be invalid otherwise. When the LLInt determines it wants to
 202         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
 203         // was executing at when it kicked off our compilation. We only need to compile code for
 204         // anything reachable from that bytecode offset.
 205 
 206         // We only bother building the bytecode graph if it could save time and executable
 207         // memory. We pick an arbitrary offset where we deem this is profitable.
<span class="line-modified"> 208         if (m_loopOSREntryBytecodeIndex.offset() &gt;= 200) {</span>
 209             // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
 210             // Instead, we just find the minimum bytecode offset that is reachable, and
 211             // compile code from that bytecode offset onwards.
 212 
 213             BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
<span class="line-modified"> 214             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeIndex.offset());</span>
 215             RELEASE_ASSERT(block);
 216 
 217             GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
<span class="line-modified"> 218             startBytecodeIndex = BytecodeIndex();</span>
 219             worklist.push(block);
 220 
 221             while (BytecodeBasicBlock* block = worklist.pop()) {
<span class="line-modified"> 222                 startBytecodeIndex = BytecodeIndex(std::min(startBytecodeIndex.offset(), block-&gt;leaderOffset()));</span>
<span class="line-modified"> 223                 for (unsigned successorIndex : block-&gt;successors())</span>
<span class="line-added"> 224                     worklist.push(&amp;graph[successorIndex]);</span>
 225 
 226                 // Also add catch blocks for bytecodes that throw.
 227                 if (m_codeBlock-&gt;numberOfExceptionHandlers()) {
 228                     for (unsigned bytecodeOffset = block-&gt;leaderOffset(); bytecodeOffset &lt; block-&gt;leaderOffset() + block-&gt;totalLength();) {
 229                         auto instruction = instructions.at(bytecodeOffset);
<span class="line-modified"> 230                         if (auto* handler = m_codeBlock-&gt;handlerForBytecodeIndex(BytecodeIndex(bytecodeOffset)))</span>
 231                             worklist.push(graph.findBasicBlockWithLeaderOffset(handler-&gt;target));
 232 
 233                         bytecodeOffset += instruction-&gt;size();
 234                     }
 235                 }
 236             }
 237         }
 238     }
 239 
<span class="line-modified"> 240     m_bytecodeCountHavingSlowCase = 0;</span>
<span class="line-modified"> 241     for (m_bytecodeIndex = BytecodeIndex(0); m_bytecodeIndex.offset() &lt; instructionCount; ) {</span>
<span class="line-added"> 242         unsigned previousSlowCasesSize = m_slowCases.size();</span>
<span class="line-added"> 243         if (m_bytecodeIndex == startBytecodeIndex &amp;&amp; startBytecodeIndex.offset() &gt; 0) {</span>
 244             // We&#39;ve proven all bytecode instructions up until here are unreachable.
 245             // Let&#39;s ensure that by crashing if it&#39;s ever hit.
 246             breakpoint();
 247         }
 248 
 249         if (m_disassembler)
<span class="line-modified"> 250             m_disassembler-&gt;setForBytecodeMainPath(m_bytecodeIndex.offset(), label());</span>
<span class="line-modified"> 251         const Instruction* currentInstruction = instructions.at(m_bytecodeIndex).ptr();</span>
<span class="line-modified"> 252         ASSERT(currentInstruction-&gt;size());</span>
 253 
<span class="line-modified"> 254         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));</span>
 255 
 256 #if ENABLE(OPCODE_SAMPLING)
<span class="line-modified"> 257         if (m_bytecodeIndex &gt; 0) // Avoid the overhead of sampling op_enter twice.</span>
 258             sampleInstruction(currentInstruction);
 259 #endif
 260 
<span class="line-modified"> 261         m_labels[m_bytecodeIndex.offset()] = label();</span>
 262 
 263         if (JITInternal::verbose)
<span class="line-modified"> 264             dataLogLn(&quot;Old JIT emitting code for &quot;, m_bytecodeIndex, &quot; at offset &quot;, (long)debugOffset());</span>
 265 
 266         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
 267 
 268         if (UNLIKELY(m_compilation)) {
 269             add64(
 270                 TrustedImm32(1),
 271                 AbsoluteAddress(m_compilation-&gt;executionCounterFor(Profiler::OriginStack(Profiler::Origin(
<span class="line-modified"> 272                     m_compilation-&gt;bytecodes(), m_bytecodeIndex)))-&gt;address()));</span>
 273         }
 274 
 275         if (Options::eagerlyUpdateTopCallFrame())
 276             updateTopCallFrame();
 277 
<span class="line-modified"> 278         unsigned bytecodeOffset = m_bytecodeIndex.offset();</span>
 279 #if ENABLE(MASM_PROBE)
 280         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 281             CodeBlock* codeBlock = m_codeBlock;
 282             probe([=] (Probe::Context&amp; ctx) {
 283                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 284             });
 285         }
 286 #endif
 287 
 288         switch (opcodeID) {
 289         DEFINE_SLOW_OP(in_by_val)
 290         DEFINE_SLOW_OP(less)
 291         DEFINE_SLOW_OP(lesseq)
 292         DEFINE_SLOW_OP(greater)
 293         DEFINE_SLOW_OP(greatereq)
 294         DEFINE_SLOW_OP(is_function)
 295         DEFINE_SLOW_OP(is_object_or_null)
 296         DEFINE_SLOW_OP(typeof)
 297         DEFINE_SLOW_OP(strcat)
 298         DEFINE_SLOW_OP(push_with_scope)
 299         DEFINE_SLOW_OP(create_lexical_environment)
 300         DEFINE_SLOW_OP(get_by_val_with_this)
 301         DEFINE_SLOW_OP(put_by_id_with_this)
 302         DEFINE_SLOW_OP(put_by_val_with_this)
 303         DEFINE_SLOW_OP(resolve_scope_for_hoisting_func_decl_in_eval)
 304         DEFINE_SLOW_OP(define_data_property)
 305         DEFINE_SLOW_OP(define_accessor_property)
 306         DEFINE_SLOW_OP(unreachable)
 307         DEFINE_SLOW_OP(throw_static_error)
 308         DEFINE_SLOW_OP(new_array_with_spread)
 309         DEFINE_SLOW_OP(new_array_buffer)
 310         DEFINE_SLOW_OP(spread)
 311         DEFINE_SLOW_OP(get_enumerable_length)
 312         DEFINE_SLOW_OP(has_generic_property)
 313         DEFINE_SLOW_OP(get_property_enumerator)
 314         DEFINE_SLOW_OP(to_index_string)
 315         DEFINE_SLOW_OP(create_direct_arguments)
 316         DEFINE_SLOW_OP(create_scoped_arguments)
 317         DEFINE_SLOW_OP(create_cloned_arguments)
<span class="line-added"> 318         DEFINE_SLOW_OP(create_arguments_butterfly)</span>
 319         DEFINE_SLOW_OP(create_rest)
<span class="line-added"> 320         DEFINE_SLOW_OP(create_promise)</span>
<span class="line-added"> 321         DEFINE_SLOW_OP(new_promise)</span>
<span class="line-added"> 322         DEFINE_SLOW_OP(create_generator)</span>
<span class="line-added"> 323         DEFINE_SLOW_OP(create_async_generator)</span>
<span class="line-added"> 324         DEFINE_SLOW_OP(new_generator)</span>
 325         DEFINE_SLOW_OP(pow)
 326 
 327         DEFINE_OP(op_add)
 328         DEFINE_OP(op_bitnot)
 329         DEFINE_OP(op_bitand)
 330         DEFINE_OP(op_bitor)
 331         DEFINE_OP(op_bitxor)
 332         DEFINE_OP(op_call)
 333         DEFINE_OP(op_tail_call)
 334         DEFINE_OP(op_call_eval)
 335         DEFINE_OP(op_call_varargs)
 336         DEFINE_OP(op_tail_call_varargs)
 337         DEFINE_OP(op_tail_call_forward_arguments)
 338         DEFINE_OP(op_construct_varargs)
 339         DEFINE_OP(op_catch)
 340         DEFINE_OP(op_construct)
 341         DEFINE_OP(op_create_this)
 342         DEFINE_OP(op_to_this)
 343         DEFINE_OP(op_get_argument)
 344         DEFINE_OP(op_argument_count)
</pre>
<hr />
<pre>
 378         DEFINE_OP(op_jneq_null)
 379         DEFINE_OP(op_jundefined_or_null)
 380         DEFINE_OP(op_jnundefined_or_null)
 381         DEFINE_OP(op_jneq_ptr)
 382         DEFINE_OP(op_jless)
 383         DEFINE_OP(op_jlesseq)
 384         DEFINE_OP(op_jgreater)
 385         DEFINE_OP(op_jgreatereq)
 386         DEFINE_OP(op_jnless)
 387         DEFINE_OP(op_jnlesseq)
 388         DEFINE_OP(op_jngreater)
 389         DEFINE_OP(op_jngreatereq)
 390         DEFINE_OP(op_jeq)
 391         DEFINE_OP(op_jneq)
 392         DEFINE_OP(op_jstricteq)
 393         DEFINE_OP(op_jnstricteq)
 394         DEFINE_OP(op_jbelow)
 395         DEFINE_OP(op_jbeloweq)
 396         DEFINE_OP(op_jtrue)
 397         DEFINE_OP(op_loop_hint)
<span class="line-added"> 398         DEFINE_OP(op_check_traps)</span>
 399         DEFINE_OP(op_nop)
 400         DEFINE_OP(op_super_sampler_begin)
 401         DEFINE_OP(op_super_sampler_end)
 402         DEFINE_OP(op_lshift)
 403         DEFINE_OP(op_mod)
 404         DEFINE_OP(op_mov)
 405         DEFINE_OP(op_mul)
 406         DEFINE_OP(op_negate)
 407         DEFINE_OP(op_neq)
 408         DEFINE_OP(op_neq_null)
 409         DEFINE_OP(op_new_array)
 410         DEFINE_OP(op_new_array_with_size)
 411         DEFINE_OP(op_new_func)
 412         DEFINE_OP(op_new_func_exp)
 413         DEFINE_OP(op_new_generator_func)
 414         DEFINE_OP(op_new_generator_func_exp)
 415         DEFINE_OP(op_new_async_func)
 416         DEFINE_OP(op_new_async_func_exp)
 417         DEFINE_OP(op_new_async_generator_func)
 418         DEFINE_OP(op_new_async_generator_func_exp)
 419         DEFINE_OP(op_new_object)
 420         DEFINE_OP(op_new_regexp)
 421         DEFINE_OP(op_not)
 422         DEFINE_OP(op_nstricteq)
 423         DEFINE_OP(op_dec)
 424         DEFINE_OP(op_inc)
 425         DEFINE_OP(op_profile_type)
 426         DEFINE_OP(op_profile_control_flow)
 427         DEFINE_OP(op_get_parent_scope)
 428         DEFINE_OP(op_put_by_id)
 429         DEFINE_OP(op_put_by_val_direct)
 430         DEFINE_OP(op_put_by_val)
 431         DEFINE_OP(op_put_getter_by_id)
 432         DEFINE_OP(op_put_setter_by_id)
 433         DEFINE_OP(op_put_getter_setter_by_id)
 434         DEFINE_OP(op_put_getter_by_val)
 435         DEFINE_OP(op_put_setter_by_val)
<span class="line-added"> 436         DEFINE_OP(op_to_property_key)</span>
<span class="line-added"> 437 </span>
<span class="line-added"> 438         DEFINE_OP(op_get_internal_field)</span>
<span class="line-added"> 439         DEFINE_OP(op_put_internal_field)</span>
 440 
 441         DEFINE_OP(op_ret)
 442         DEFINE_OP(op_rshift)
 443         DEFINE_OP(op_unsigned)
 444         DEFINE_OP(op_urshift)
 445         DEFINE_OP(op_set_function_name)
 446         DEFINE_OP(op_stricteq)
 447         DEFINE_OP(op_sub)
 448         DEFINE_OP(op_switch_char)
 449         DEFINE_OP(op_switch_imm)
 450         DEFINE_OP(op_switch_string)
 451         DEFINE_OP(op_throw)
 452         DEFINE_OP(op_to_number)
<span class="line-added"> 453         DEFINE_OP(op_to_numeric)</span>
 454         DEFINE_OP(op_to_string)
 455         DEFINE_OP(op_to_object)
 456         DEFINE_OP(op_to_primitive)
 457 
 458         DEFINE_OP(op_resolve_scope)
 459         DEFINE_OP(op_get_from_scope)
 460         DEFINE_OP(op_put_to_scope)
 461         DEFINE_OP(op_get_from_arguments)
 462         DEFINE_OP(op_put_to_arguments)
 463 
 464         DEFINE_OP(op_has_structure_property)
 465         DEFINE_OP(op_has_indexed_property)
 466         DEFINE_OP(op_get_direct_pname)
 467         DEFINE_OP(op_enumerator_structure_pname)
 468         DEFINE_OP(op_enumerator_generic_pname)
 469 
 470         DEFINE_OP(op_log_shadow_chicken_prologue)
 471         DEFINE_OP(op_log_shadow_chicken_tail)
 472         default:
 473             RELEASE_ASSERT_NOT_REACHED();
 474         }
 475 
 476         if (JITInternal::verbose)
 477             dataLog(&quot;At &quot;, bytecodeOffset, &quot;: &quot;, m_slowCases.size(), &quot;\n&quot;);
 478     }
 479 
 480     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 481 
 482 #ifndef NDEBUG
 483     // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified"> 484     m_bytecodeIndex = BytecodeIndex();</span>
 485 #endif
 486 }
 487 
 488 void JIT::privateCompileLinkPass()
 489 {
 490     unsigned jmpTableCount = m_jmpTable.size();
 491     for (unsigned i = 0; i &lt; jmpTableCount; ++i)
 492         m_jmpTable[i].from.linkTo(m_labels[m_jmpTable[i].toBytecodeOffset], this);
 493     m_jmpTable.clear();
 494 }
 495 
 496 void JIT::privateCompileSlowCases()
 497 {
 498     m_getByIdIndex = 0;
<span class="line-added"> 499     m_getByValIndex = 0;</span>
 500     m_getByIdWithThisIndex = 0;
 501     m_putByIdIndex = 0;
 502     m_inByIdIndex = 0;
 503     m_instanceOfIndex = 0;
 504     m_byValInstructionIndex = 0;
 505     m_callLinkInfoIndex = 0;
 506 
<span class="line-added"> 507     RefCountedArray&lt;RareCaseProfile&gt; rareCaseProfiles;</span>
<span class="line-added"> 508     if (shouldEmitProfiling())</span>
<span class="line-added"> 509         rareCaseProfiles = RefCountedArray&lt;RareCaseProfile&gt;(m_bytecodeCountHavingSlowCase);</span>
<span class="line-added"> 510 </span>
<span class="line-added"> 511     unsigned bytecodeCountHavingSlowCase = 0;</span>
 512     for (Vector&lt;SlowCaseEntry&gt;::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
<span class="line-modified"> 513         m_bytecodeIndex = iter-&gt;to;</span>
 514 
<span class="line-modified"> 515         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeIndex));</span>
 516 
<span class="line-modified"> 517         BytecodeIndex firstTo = m_bytecodeIndex;</span>
 518 
<span class="line-modified"> 519         const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(m_bytecodeIndex).ptr();</span>
 520 
<span class="line-modified"> 521         RareCaseProfile* rareCaseProfile = nullptr;</span>
 522         if (shouldEmitProfiling())
<span class="line-modified"> 523             rareCaseProfile = &amp;rareCaseProfiles.at(bytecodeCountHavingSlowCase);</span>
 524 
 525         if (JITInternal::verbose)
<span class="line-modified"> 526             dataLogLn(&quot;Old JIT emitting slow code for &quot;, m_bytecodeIndex, &quot; at offset &quot;, (long)debugOffset());</span>
 527 
 528         if (m_disassembler)
<span class="line-modified"> 529             m_disassembler-&gt;setForBytecodeSlowPath(m_bytecodeIndex.offset(), label());</span>
 530 
 531 #if ENABLE(MASM_PROBE)
 532         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 533             OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
<span class="line-modified"> 534             unsigned bytecodeOffset = m_bytecodeIndex.offset();</span>
 535             CodeBlock* codeBlock = m_codeBlock;
 536             probe([=] (Probe::Context&amp; ctx) {
 537                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] SLOW &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 538             });
 539         }
 540 #endif
 541 
 542         switch (currentInstruction-&gt;opcodeID()) {
 543         DEFINE_SLOWCASE_OP(op_add)
 544         DEFINE_SLOWCASE_OP(op_call)
 545         DEFINE_SLOWCASE_OP(op_tail_call)
 546         DEFINE_SLOWCASE_OP(op_call_eval)
 547         DEFINE_SLOWCASE_OP(op_call_varargs)
 548         DEFINE_SLOWCASE_OP(op_tail_call_varargs)
 549         DEFINE_SLOWCASE_OP(op_tail_call_forward_arguments)
 550         DEFINE_SLOWCASE_OP(op_construct_varargs)
 551         DEFINE_SLOWCASE_OP(op_construct)
 552         DEFINE_SLOWCASE_OP(op_eq)
 553         DEFINE_SLOWCASE_OP(op_try_get_by_id)
 554         DEFINE_SLOWCASE_OP(op_in_by_id)
 555         DEFINE_SLOWCASE_OP(op_get_by_id)
 556         DEFINE_SLOWCASE_OP(op_get_by_id_with_this)
 557         DEFINE_SLOWCASE_OP(op_get_by_id_direct)
 558         DEFINE_SLOWCASE_OP(op_get_by_val)
 559         DEFINE_SLOWCASE_OP(op_instanceof)
 560         DEFINE_SLOWCASE_OP(op_instanceof_custom)
 561         DEFINE_SLOWCASE_OP(op_jless)
 562         DEFINE_SLOWCASE_OP(op_jlesseq)
 563         DEFINE_SLOWCASE_OP(op_jgreater)
 564         DEFINE_SLOWCASE_OP(op_jgreatereq)
 565         DEFINE_SLOWCASE_OP(op_jnless)
 566         DEFINE_SLOWCASE_OP(op_jnlesseq)
 567         DEFINE_SLOWCASE_OP(op_jngreater)
 568         DEFINE_SLOWCASE_OP(op_jngreatereq)
 569         DEFINE_SLOWCASE_OP(op_jeq)
 570         DEFINE_SLOWCASE_OP(op_jneq)
 571         DEFINE_SLOWCASE_OP(op_jstricteq)
 572         DEFINE_SLOWCASE_OP(op_jnstricteq)
 573         DEFINE_SLOWCASE_OP(op_loop_hint)
<span class="line-modified"> 574         DEFINE_SLOWCASE_OP(op_check_traps)</span>
 575         DEFINE_SLOWCASE_OP(op_mod)
 576         DEFINE_SLOWCASE_OP(op_mul)
 577         DEFINE_SLOWCASE_OP(op_negate)
 578         DEFINE_SLOWCASE_OP(op_neq)
 579         DEFINE_SLOWCASE_OP(op_new_object)
 580         DEFINE_SLOWCASE_OP(op_put_by_id)
 581         case op_put_by_val_direct:
 582         DEFINE_SLOWCASE_OP(op_put_by_val)
 583         DEFINE_SLOWCASE_OP(op_sub)
 584         DEFINE_SLOWCASE_OP(op_has_indexed_property)
 585         DEFINE_SLOWCASE_OP(op_get_from_scope)
 586         DEFINE_SLOWCASE_OP(op_put_to_scope)
 587 
 588         DEFINE_SLOWCASE_SLOW_OP(unsigned)
 589         DEFINE_SLOWCASE_SLOW_OP(inc)
 590         DEFINE_SLOWCASE_SLOW_OP(dec)
 591         DEFINE_SLOWCASE_SLOW_OP(bitnot)
 592         DEFINE_SLOWCASE_SLOW_OP(bitand)
 593         DEFINE_SLOWCASE_SLOW_OP(bitor)
 594         DEFINE_SLOWCASE_SLOW_OP(bitxor)
 595         DEFINE_SLOWCASE_SLOW_OP(lshift)
 596         DEFINE_SLOWCASE_SLOW_OP(rshift)
 597         DEFINE_SLOWCASE_SLOW_OP(urshift)
 598         DEFINE_SLOWCASE_SLOW_OP(div)
 599         DEFINE_SLOWCASE_SLOW_OP(create_this)
<span class="line-added"> 600         DEFINE_SLOWCASE_SLOW_OP(create_promise)</span>
<span class="line-added"> 601         DEFINE_SLOWCASE_SLOW_OP(create_generator)</span>
<span class="line-added"> 602         DEFINE_SLOWCASE_SLOW_OP(create_async_generator)</span>
 603         DEFINE_SLOWCASE_SLOW_OP(to_this)
 604         DEFINE_SLOWCASE_SLOW_OP(to_primitive)
 605         DEFINE_SLOWCASE_SLOW_OP(to_number)
<span class="line-added"> 606         DEFINE_SLOWCASE_SLOW_OP(to_numeric)</span>
 607         DEFINE_SLOWCASE_SLOW_OP(to_string)
 608         DEFINE_SLOWCASE_SLOW_OP(to_object)
 609         DEFINE_SLOWCASE_SLOW_OP(not)
 610         DEFINE_SLOWCASE_SLOW_OP(stricteq)
 611         DEFINE_SLOWCASE_SLOW_OP(nstricteq)
 612         DEFINE_SLOWCASE_SLOW_OP(get_direct_pname)
 613         DEFINE_SLOWCASE_SLOW_OP(has_structure_property)
 614         DEFINE_SLOWCASE_SLOW_OP(resolve_scope)
 615         DEFINE_SLOWCASE_SLOW_OP(check_tdz)
<span class="line-added"> 616         DEFINE_SLOWCASE_SLOW_OP(to_property_key)</span>
 617 
 618         default:
 619             RELEASE_ASSERT_NOT_REACHED();
 620         }
 621 
 622         if (JITInternal::verbose)
 623             dataLog(&quot;At &quot;, firstTo, &quot; slow: &quot;, iter - m_slowCases.begin(), &quot;\n&quot;);
 624 
 625         RELEASE_ASSERT_WITH_MESSAGE(iter == m_slowCases.end() || firstTo != iter-&gt;to, &quot;Not enough jumps linked in slow case codegen.&quot;);
 626         RELEASE_ASSERT_WITH_MESSAGE(firstTo == (iter - 1)-&gt;to, &quot;Too many jumps linked in slow case codegen.&quot;);
 627 
 628         if (shouldEmitProfiling())
 629             add32(TrustedImm32(1), AbsoluteAddress(&amp;rareCaseProfile-&gt;m_counter));
 630 
 631         emitJumpSlowToHot(jump(), 0);
<span class="line-added"> 632         ++bytecodeCountHavingSlowCase;</span>
 633     }
 634 
<span class="line-added"> 635     RELEASE_ASSERT(bytecodeCountHavingSlowCase == m_bytecodeCountHavingSlowCase);</span>
 636     RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
 637     RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
 638     RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
 639     RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
 640     RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
 641     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 642 
<span class="line-added"> 643     if (shouldEmitProfiling())</span>
<span class="line-added"> 644         m_codeBlock-&gt;setRareCaseProfiles(WTFMove(rareCaseProfiles));</span>
<span class="line-added"> 645 </span>
 646 #ifndef NDEBUG
 647     // Reset this, in order to guard its use with ASSERTs.
<span class="line-modified"> 648     m_bytecodeIndex = BytecodeIndex();</span>
 649 #endif
 650 }
 651 
 652 void JIT::compileWithoutLinking(JITCompilationEffort effort)
 653 {
 654     MonotonicTime before { };
 655     if (UNLIKELY(computeCompileTimes()))
 656         before = MonotonicTime::now();
 657 
 658     DFG::CapabilityLevel level = m_codeBlock-&gt;capabilityLevel();
 659     switch (level) {
 660     case DFG::CannotCompile:
 661         m_canBeOptimized = false;
 662         m_canBeOptimizedOrInlined = false;
 663         m_shouldEmitProfiling = false;
 664         break;
 665     case DFG::CanCompile:
 666     case DFG::CanCompileAndInline:
 667         m_canBeOptimized = true;
 668         m_canBeOptimizedOrInlined = true;
</pre>
<hr />
<pre>
 679     case EvalCode:
 680         m_codeBlock-&gt;m_shouldAlwaysBeInlined = false;
 681         break;
 682     case FunctionCode:
 683         // We could have already set it to false because we detected an uninlineable call.
 684         // Don&#39;t override that observation.
 685         m_codeBlock-&gt;m_shouldAlwaysBeInlined &amp;= canInline(level) &amp;&amp; DFG::mightInlineFunction(m_codeBlock);
 686         break;
 687     }
 688 
 689     if (UNLIKELY(Options::dumpDisassembly() || (m_vm-&gt;m_perBytecodeProfiler &amp;&amp; Options::disassembleBaselineForProfiler())))
 690         m_disassembler = makeUnique&lt;JITDisassembler&gt;(m_codeBlock);
 691     if (UNLIKELY(m_vm-&gt;m_perBytecodeProfiler)) {
 692         m_compilation = adoptRef(
 693             new Profiler::Compilation(
 694                 m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
 695                 Profiler::Baseline));
 696         m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
 697     }
 698 
<span class="line-modified"> 699     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(BytecodeIndex(0)));</span>
 700 
 701     Label entryLabel(this);
 702     if (m_disassembler)
 703         m_disassembler-&gt;setStartOfCode(entryLabel);
 704 
 705     // Just add a little bit of randomness to the codegen
 706     if (random() &amp; 1)
 707         nop();
 708 
 709     emitFunctionPrologue();
 710     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 711 
 712     Label beginLabel(this);
 713 
 714     sampleCodeBlock(m_codeBlock);
 715 #if ENABLE(OPCODE_SAMPLING)
 716     sampleInstruction(m_codeBlock-&gt;instructions().begin());
 717 #endif
 718 
 719     int frameTopOffset = stackPointerOffsetFor(m_codeBlock) * sizeof(Register);
 720     unsigned maxFrameSize = -frameTopOffset;
 721     addPtr(TrustedImm32(frameTopOffset), callFrameRegister, regT1);
 722     JumpList stackOverflow;
 723     if (UNLIKELY(maxFrameSize &gt; Options::reservedZoneSize()))
 724         stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
 725     stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm-&gt;addressOfSoftStackLimit()), regT1));
 726 
 727     move(regT1, stackPointerRegister);
 728     checkStackPointerAlignment();


 729 
 730     emitSaveCalleeSaves();
 731     emitMaterializeTagCheckRegisters();
 732 
 733     if (m_codeBlock-&gt;codeType() == FunctionCode) {
<span class="line-modified"> 734         ASSERT(!m_bytecodeIndex);</span>
 735         if (shouldEmitProfiling()) {
 736             for (int argument = 0; argument &lt; m_codeBlock-&gt;numParameters(); ++argument) {
 737                 // If this is a constructor, then we want to put in a dummy profiling site (to
 738                 // keep things consistent) but we don&#39;t actually want to record the dummy value.
 739                 if (m_codeBlock-&gt;isConstructor() &amp;&amp; !argument)
 740                     continue;
 741                 int offset = CallFrame::argumentOffsetIncludingThis(argument) * static_cast&lt;int&gt;(sizeof(Register));
 742 #if USE(JSVALUE64)
 743                 load64(Address(callFrameRegister, offset), regT0);
 744 #elif USE(JSVALUE32_64)
 745                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.payload)), regT0);
 746                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.tag)), regT1);
 747 #endif
 748                 emitValueProfilingSite(m_codeBlock-&gt;valueProfileForArgument(argument));
 749             }
 750         }
 751     }
 752 
 753     RELEASE_ASSERT(!JITCode::isJIT(m_codeBlock-&gt;jitType()));
 754 
 755     privateCompileMainPass();
 756     privateCompileLinkPass();
 757     privateCompileSlowCases();
 758 
 759     if (m_disassembler)
 760         m_disassembler-&gt;setEndOfSlowPath(label());
 761     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 762 
 763     stackOverflow.link(this);
<span class="line-modified"> 764     m_bytecodeIndex = BytecodeIndex(0);</span>
 765     if (maxFrameExtentForSlowPathCall)
 766         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
 767     callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
 768 
 769     // If the number of parameters is 1, we never require arity fixup.
 770     bool requiresArityFixup = m_codeBlock-&gt;m_numParameters != 1;
 771     if (m_codeBlock-&gt;codeType() == FunctionCode &amp;&amp; requiresArityFixup) {
 772         m_arityCheck = label();
 773         store8(TrustedImm32(0), &amp;m_codeBlock-&gt;m_shouldAlwaysBeInlined);
 774         emitFunctionPrologue();
 775         emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 776 
<span class="line-modified"> 777         load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT1);</span>
 778         branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock-&gt;m_numParameters)).linkTo(beginLabel, this);
 779 
<span class="line-modified"> 780         m_bytecodeIndex = BytecodeIndex(0);</span>
 781 
 782         if (maxFrameExtentForSlowPathCall)
 783             addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
<span class="line-modified"> 784         callOperationWithCallFrameRollbackOnException(m_codeBlock-&gt;isConstructor() ? operationConstructArityCheck : operationCallArityCheck, m_codeBlock-&gt;globalObject());</span>
 785         if (maxFrameExtentForSlowPathCall)
 786             addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
 787         branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
 788         move(returnValueGPR, GPRInfo::argumentGPR0);
 789         emitNakedCall(m_vm-&gt;getCTIStub(arityFixupGenerator).retaggedCode&lt;NoPtrTag&gt;());
 790 
<span class="line-modified"> 791 #if ASSERT_ENABLED</span>
<span class="line-modified"> 792         m_bytecodeIndex = BytecodeIndex(); // Reset this, in order to guard its use with ASSERTs.</span>
 793 #endif
 794 
 795         jump(beginLabel);
 796     } else
 797         m_arityCheck = entryLabel; // Never require arity fixup.
 798 
 799     ASSERT(m_jmpTable.isEmpty());
 800 
 801     privateCompileExceptionHandlers();
 802 
 803     if (m_disassembler)
 804         m_disassembler-&gt;setEndOfCode(label());
 805     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 806 
 807     m_linkBuffer = std::unique_ptr&lt;LinkBuffer&gt;(new LinkBuffer(*this, m_codeBlock, effort));
 808 
 809     MonotonicTime after { };
 810     if (UNLIKELY(computeCompileTimes())) {
 811         after = MonotonicTime::now();
 812 
 813         if (Options::reportTotalCompileTimes())
 814             totalBaselineCompileTime += after - before;
 815     }
 816     if (UNLIKELY(reportCompileTimes())) {
 817         CString codeBlockName = toCString(*m_codeBlock);
 818 
 819         dataLog(&quot;Optimized &quot;, codeBlockName, &quot; with Baseline JIT into &quot;, m_linkBuffer-&gt;size(), &quot; bytes in &quot;, (after - before).milliseconds(), &quot; ms.\n&quot;);
 820     }
 821 }
 822 
 823 CompilationResult JIT::link()
 824 {
 825     LinkBuffer&amp; patchBuffer = *m_linkBuffer;
 826 
 827     if (patchBuffer.didFailToAllocate())
 828         return CompilationFailed;
 829 
 830     // Translate vPC offsets into addresses in JIT generated code, for switch tables.
 831     for (auto&amp; record : m_switches) {
<span class="line-modified"> 832         unsigned bytecodeOffset = record.bytecodeIndex.offset();</span>
 833 
 834         if (record.type != SwitchRecord::String) {
 835             ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
 836             ASSERT(record.jumpTable.simpleJumpTable-&gt;branchOffsets.size() == record.jumpTable.simpleJumpTable-&gt;ctiOffsets.size());
 837 
 838             auto* simpleJumpTable = record.jumpTable.simpleJumpTable;
 839             simpleJumpTable-&gt;ctiDefault = patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
 840 
 841             for (unsigned j = 0; j &lt; record.jumpTable.simpleJumpTable-&gt;branchOffsets.size(); ++j) {
 842                 unsigned offset = record.jumpTable.simpleJumpTable-&gt;branchOffsets[j];
 843                 simpleJumpTable-&gt;ctiOffsets[j] = offset
 844                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 845                     : simpleJumpTable-&gt;ctiDefault;
 846             }
 847         } else {
 848             ASSERT(record.type == SwitchRecord::String);
 849 
 850             auto* stringJumpTable = record.jumpTable.stringJumpTable;
 851             stringJumpTable-&gt;ctiDefault =
 852                 patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
</pre>
<hr />
<pre>
 855                 unsigned offset = location.branchOffset;
 856                 location.ctiOffset = offset
 857                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 858                     : stringJumpTable-&gt;ctiDefault;
 859             }
 860         }
 861     }
 862 
 863     for (size_t i = 0; i &lt; m_codeBlock-&gt;numberOfExceptionHandlers(); ++i) {
 864         HandlerInfo&amp; handler = m_codeBlock-&gt;exceptionHandler(i);
 865         // FIXME: &lt;rdar://problem/39433318&gt;.
 866         handler.nativeCode = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_labels[handler.target]);
 867     }
 868 
 869     for (auto&amp; record : m_calls) {
 870         if (record.callee)
 871             patchBuffer.link(record.from, record.callee);
 872     }
 873 
 874     finalizeInlineCaches(m_getByIds, patchBuffer);
<span class="line-added"> 875     finalizeInlineCaches(m_getByVals, patchBuffer);</span>
 876     finalizeInlineCaches(m_getByIdsWithThis, patchBuffer);
 877     finalizeInlineCaches(m_putByIds, patchBuffer);
 878     finalizeInlineCaches(m_inByIds, patchBuffer);
 879     finalizeInlineCaches(m_instanceOfs, patchBuffer);
 880 
 881     if (m_byValCompilationInfo.size()) {
 882         CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt; exceptionHandler = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_exceptionHandler);
 883 
 884         for (const auto&amp; byValCompilationInfo : m_byValCompilationInfo) {
 885             PatchableJump patchableNotIndexJump = byValCompilationInfo.notIndexJump;
 886             auto notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;();
 887             if (Jump(patchableNotIndexJump).isSet())
 888                 notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(patchableNotIndexJump));
 889             auto badTypeJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.badTypeJump));
 890             auto doneTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.doneTarget));
 891             auto nextHotPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.nextHotPathTarget));
 892             auto slowPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.slowPathTarget));
 893 
 894             *byValCompilationInfo.byValInfo = ByValInfo(
 895                 byValCompilationInfo.bytecodeIndex,
 896                 notIndexJump,
 897                 badTypeJump,
 898                 exceptionHandler,
 899                 byValCompilationInfo.arrayMode,
 900                 byValCompilationInfo.arrayProfile,
 901                 doneTarget,
 902                 nextHotPathTarget,
 903                 slowPathTarget);
 904         }
 905     }
 906 
 907     for (auto&amp; compilationInfo : m_callCompilationInfo) {
 908         CallLinkInfo&amp; info = *compilationInfo.callLinkInfo;
 909         info.setCallLocations(
 910             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.callReturnLocation)),
 911             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathBegin)),
 912             patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathOther));
 913     }
 914 
<span class="line-modified"> 915     {</span>
<span class="line-modified"> 916         JITCodeMapBuilder jitCodeMapBuilder;</span>
<span class="line-modified"> 917         for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {</span>
<span class="line-modified"> 918             if (m_labels[bytecodeOffset].isSet())</span>
<span class="line-added"> 919                 jitCodeMapBuilder.append(BytecodeIndex(bytecodeOffset), patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));</span>
<span class="line-added"> 920         }</span>
<span class="line-added"> 921         m_codeBlock-&gt;setJITCodeMap(jitCodeMapBuilder.finalize());</span>
 922     }


 923 
 924     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
 925 
<span class="line-modified"> 926     if (UNLIKELY(Options::dumpDisassembly())) {</span>
 927         m_disassembler-&gt;dump(patchBuffer);
 928         patchBuffer.didAlreadyDisassemble();
 929     }
 930     if (UNLIKELY(m_compilation)) {
 931         if (Options::disassembleBaselineForProfiler())
 932             m_disassembler-&gt;reportToProfiler(m_compilation.get(), patchBuffer);
 933         m_vm-&gt;m_perBytecodeProfiler-&gt;addCompilation(m_codeBlock, *m_compilation);
 934     }
 935 
 936     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
 937         m_codeBlock-&gt;setPCToCodeOriginMap(makeUnique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), patchBuffer));
 938 
 939     CodeRef&lt;JSEntryPtrTag&gt; result = FINALIZE_CODE(
 940         patchBuffer, JSEntryPtrTag,
 941         &quot;Baseline JIT code for %s&quot;, toCString(CodeBlockWithJITType(m_codeBlock, JITType::BaselineJIT)).data());
 942 
 943     m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
 944         static_cast&lt;double&gt;(result.size()) /
 945         static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));
 946 
<span class="line-modified"> 947     {</span>
<span class="line-added"> 948         ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);</span>
<span class="line-added"> 949         m_codeBlock-&gt;shrinkToFit(locker, CodeBlock::ShrinkMode::LateShrink);</span>
<span class="line-added"> 950     }</span>
 951     m_codeBlock-&gt;setJITCode(
 952         adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));
 953 
 954     if (JITInternal::verbose)
 955         dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
 956 
 957     return CompilationSuccessful;
 958 }
 959 
 960 CompilationResult JIT::privateCompile(JITCompilationEffort effort)
 961 {
 962     doMainThreadPreparationBeforeCompile();
 963     compileWithoutLinking(effort);
 964     return link();
 965 }
 966 
 967 void JIT::privateCompileExceptionHandlers()
 968 {
 969     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
 970         m_exceptionChecksWithCallFrameRollback.link(this);
 971 
 972         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 973 
<span class="line-modified"> 974         // operationLookupExceptionHandlerFromCallerFrame is passed one argument, the VM*.</span>

 975         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified"> 976         prepareCallOperation(vm());</span>
<span class="line-modified"> 977         m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandlerFromCallerFrame)));</span>






 978         jumpToExceptionHandler(vm());
 979     }
 980 
 981     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
 982         m_exceptionHandler = label();
 983         m_exceptionChecks.link(this);
 984 
 985         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 986 
<span class="line-modified"> 987         // operationLookupExceptionHandler is passed one argument, the VM*.</span>
 988         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);
<span class="line-modified"> 989         prepareCallOperation(vm());</span>
<span class="line-modified"> 990         m_calls.append(CallRecord(call(OperationPtrTag), BytecodeIndex(), FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandler)));</span>






 991         jumpToExceptionHandler(vm());
 992     }
 993 }
 994 
 995 void JIT::doMainThreadPreparationBeforeCompile()
 996 {
 997     // This ensures that we have the most up to date type information when performing typecheck optimizations for op_profile_type.
 998     if (m_vm-&gt;typeProfiler())
 999         m_vm-&gt;typeProfilerLog()-&gt;processLogEntries(*m_vm, &quot;Preparing for JIT compilation.&quot;_s);
1000 }
1001 
1002 unsigned JIT::frameRegisterCountFor(CodeBlock* codeBlock)
1003 {
1004     ASSERT(static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals()) == WTF::roundUpToMultipleOf(stackAlignmentRegisters(), static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals())));
1005 
1006     return roundLocalRegisterCountForFramePointerOffset(codeBlock-&gt;numCalleeLocals() + maxFrameExtentForSlowPathCallInRegisters);
1007 }
1008 
1009 int JIT::stackPointerOffsetFor(CodeBlock* codeBlock)
1010 {
</pre>
</td>
</tr>
</table>
<center><a href="ICStats.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JIT.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>