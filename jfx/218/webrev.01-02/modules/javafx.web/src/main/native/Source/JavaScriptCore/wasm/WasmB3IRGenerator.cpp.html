<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmB3IRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2016-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;WasmB3IRGenerator.h&quot;
  28 
  29 #if ENABLE(WEBASSEMBLY)
  30 
  31 #include &quot;AllowMacroScratchRegisterUsageIf.h&quot;
  32 #include &quot;B3BasicBlockInlines.h&quot;
  33 #include &quot;B3CCallValue.h&quot;
  34 #include &quot;B3Compile.h&quot;
  35 #include &quot;B3ConstPtrValue.h&quot;
  36 #include &quot;B3FixSSA.h&quot;
  37 #include &quot;B3Generate.h&quot;
  38 #include &quot;B3InsertionSet.h&quot;
  39 #include &quot;B3SlotBaseValue.h&quot;
  40 #include &quot;B3StackmapGenerationParams.h&quot;
  41 #include &quot;B3SwitchValue.h&quot;
  42 #include &quot;B3UpsilonValue.h&quot;
  43 #include &quot;B3Validate.h&quot;
  44 #include &quot;B3ValueInlines.h&quot;
  45 #include &quot;B3ValueKey.h&quot;
  46 #include &quot;B3Variable.h&quot;
  47 #include &quot;B3VariableValue.h&quot;
  48 #include &quot;B3WasmAddressValue.h&quot;
  49 #include &quot;B3WasmBoundsCheckValue.h&quot;
  50 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSWebAssemblyInstance.h&quot;
  53 #include &quot;ScratchRegisterAllocator.h&quot;
  54 #include &quot;VirtualRegister.h&quot;
  55 #include &quot;WasmCallingConvention.h&quot;
  56 #include &quot;WasmContextInlines.h&quot;
  57 #include &quot;WasmExceptionType.h&quot;
  58 #include &quot;WasmFunctionParser.h&quot;
  59 #include &quot;WasmInstance.h&quot;
  60 #include &quot;WasmMemory.h&quot;
  61 #include &quot;WasmOMGPlan.h&quot;
  62 #include &quot;WasmOSREntryData.h&quot;
  63 #include &quot;WasmOpcodeOrigin.h&quot;
  64 #include &quot;WasmOperations.h&quot;
  65 #include &quot;WasmSignatureInlines.h&quot;
  66 #include &quot;WasmThunks.h&quot;
  67 #include &lt;limits&gt;
  68 #include &lt;wtf/Optional.h&gt;
  69 #include &lt;wtf/StdLibExtras.h&gt;
  70 
  71 void dumpProcedure(void* ptr)
  72 {
  73     JSC::B3::Procedure* proc = static_cast&lt;JSC::B3::Procedure*&gt;(ptr);
  74     proc-&gt;dump(WTF::dataFile());
  75 }
  76 
  77 namespace JSC { namespace Wasm {
  78 
  79 using namespace B3;
  80 
  81 namespace {
  82 namespace WasmB3IRGeneratorInternal {
  83 static constexpr bool verbose = false;
  84 }
  85 }
  86 
  87 class B3IRGenerator {
  88 public:
  89     using ExpressionType = Value*;
  90     using ResultList = Vector&lt;ExpressionType, 8&gt;;
  91 
  92     struct ControlData {
  93         ControlData(Procedure&amp; proc, Origin origin, BlockSignature signature, BlockType type, BasicBlock* continuation, BasicBlock* special = nullptr)
  94             : controlBlockType(type)
  95             , m_signature(signature)
  96             , continuation(continuation)
  97             , special(special)
  98         {
  99             if (type == BlockType::Loop) {
 100                 for (unsigned i = 0; i &lt; signature-&gt;argumentCount(); ++i)
 101                     phis.append(proc.add&lt;Value&gt;(Phi, toB3Type(signature-&gt;argument(i)), origin));
 102             } else {
 103                 for (unsigned i = 0; i &lt; signature-&gt;returnCount(); ++i)
 104                     phis.append(proc.add&lt;Value&gt;(Phi, toB3Type(signature-&gt;returnType(i)), origin));
 105             }
 106         }
 107 
 108         ControlData()
 109         {
 110         }
 111 
 112         static bool isIf(const ControlData&amp; control) { return control.blockType() == BlockType::If; }
 113         static bool isTopLevel(const ControlData&amp; control) { return control.blockType() == BlockType::TopLevel; }
 114 
 115         void dump(PrintStream&amp; out) const
 116         {
 117             switch (blockType()) {
 118             case BlockType::If:
 119                 out.print(&quot;If:       &quot;);
 120                 break;
 121             case BlockType::Block:
 122                 out.print(&quot;Block:    &quot;);
 123                 break;
 124             case BlockType::Loop:
 125                 out.print(&quot;Loop:     &quot;);
 126                 break;
 127             case BlockType::TopLevel:
 128                 out.print(&quot;TopLevel: &quot;);
 129                 break;
 130             }
 131             out.print(&quot;Continuation: &quot;, *continuation, &quot;, Special: &quot;);
 132             if (special)
 133                 out.print(*special);
 134             else
 135                 out.print(&quot;None&quot;);
 136         }
 137 
 138         BlockType blockType() const { return controlBlockType; }
 139 
 140         BlockSignature signature() const { return m_signature; }
 141 
 142         bool hasNonVoidresult() const { return m_signature-&gt;returnsVoid(); }
 143 
 144         BasicBlock* targetBlockForBranch()
 145         {
 146             if (blockType() == BlockType::Loop)
 147                 return special;
 148             return continuation;
 149         }
 150 
 151         void convertIfToBlock()
 152         {
 153             ASSERT(blockType() == BlockType::If);
 154             controlBlockType = BlockType::Block;
 155             special = nullptr;
 156         }
 157 
 158         SignatureArgCount branchTargetArity() const
 159         {
 160             if (blockType() == BlockType::Loop)
 161                 return m_signature-&gt;argumentCount();
 162             return m_signature-&gt;returnCount();
 163         }
 164 
 165         Type branchTargetType(unsigned i) const
 166         {
 167             ASSERT(i &lt; branchTargetArity());
 168             if (blockType() == BlockType::Loop)
 169                 return m_signature-&gt;argument(i);
 170             return m_signature-&gt;returnType(i);
 171         }
 172 
 173     private:
 174         friend class B3IRGenerator;
 175         BlockType controlBlockType;
 176         BlockSignature m_signature;
 177         BasicBlock* continuation;
 178         BasicBlock* special;
 179         ResultList phis;
 180     };
 181 
 182     using ControlType = ControlData;
 183     using ExpressionList = Vector&lt;ExpressionType, 1&gt;;
 184 
 185     using ControlEntry = FunctionParser&lt;B3IRGenerator&gt;::ControlEntry;
 186     using ControlStack = FunctionParser&lt;B3IRGenerator&gt;::ControlStack;
 187     using Stack = FunctionParser&lt;B3IRGenerator&gt;::Stack;
 188     using TypedExpression = FunctionParser&lt;B3IRGenerator&gt;::TypedExpression;
 189 
 190     static_assert(std::is_same_v&lt;ResultList, FunctionParser&lt;B3IRGenerator&gt;::ResultList&gt;);
 191 
 192     typedef String ErrorType;
 193     typedef Unexpected&lt;ErrorType&gt; UnexpectedResult;
 194     typedef Expected&lt;std::unique_ptr&lt;InternalFunction&gt;, ErrorType&gt; Result;
 195     typedef Expected&lt;void, ErrorType&gt; PartialResult;
 196 
 197     static ExpressionType emptyExpression() { return nullptr; };
 198 
 199     template &lt;typename ...Args&gt;
 200     NEVER_INLINE UnexpectedResult WARN_UNUSED_RETURN fail(Args... args) const
 201     {
 202         using namespace FailureHelper; // See ADL comment in WasmParser.h.
 203         return UnexpectedResult(makeString(&quot;WebAssembly.Module failed compiling: &quot;_s, makeString(args)...));
 204     }
 205 #define WASM_COMPILE_FAIL_IF(condition, ...) do { \
 206         if (UNLIKELY(condition))                  \
 207             return fail(__VA_ARGS__);             \
 208     } while (0)
 209 
 210     B3IRGenerator(const ModuleInformation&amp;, Procedure&amp;, InternalFunction*, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp;, unsigned&amp; osrEntryScratchBufferSize, MemoryMode, CompilationMode, unsigned functionIndex, unsigned loopIndexForOSREntry, TierUpCount*);
 211 
 212     PartialResult WARN_UNUSED_RETURN addArguments(const Signature&amp;);
 213     PartialResult WARN_UNUSED_RETURN addLocal(Type, uint32_t);
 214     ExpressionType addConstant(Type, uint64_t);
 215 
 216     // References
 217     PartialResult WARN_UNUSED_RETURN addRefIsNull(ExpressionType value, ExpressionType&amp; result);
 218     PartialResult WARN_UNUSED_RETURN addRefFunc(uint32_t index, ExpressionType&amp; result);
 219 
 220     // Tables
 221     PartialResult WARN_UNUSED_RETURN addTableGet(unsigned, ExpressionType index, ExpressionType&amp; result);
 222     PartialResult WARN_UNUSED_RETURN addTableSet(unsigned, ExpressionType index, ExpressionType value);
 223     PartialResult WARN_UNUSED_RETURN addTableSize(unsigned, ExpressionType&amp; result);
 224     PartialResult WARN_UNUSED_RETURN addTableGrow(unsigned, ExpressionType fill, ExpressionType delta, ExpressionType&amp; result);
 225     PartialResult WARN_UNUSED_RETURN addTableFill(unsigned, ExpressionType offset, ExpressionType fill, ExpressionType count);
 226     // Locals
 227     PartialResult WARN_UNUSED_RETURN getLocal(uint32_t index, ExpressionType&amp; result);
 228     PartialResult WARN_UNUSED_RETURN setLocal(uint32_t index, ExpressionType value);
 229 
 230     // Globals
 231     PartialResult WARN_UNUSED_RETURN getGlobal(uint32_t index, ExpressionType&amp; result);
 232     PartialResult WARN_UNUSED_RETURN setGlobal(uint32_t index, ExpressionType value);
 233 
 234     // Memory
 235     PartialResult WARN_UNUSED_RETURN load(LoadOpType, ExpressionType pointer, ExpressionType&amp; result, uint32_t offset);
 236     PartialResult WARN_UNUSED_RETURN store(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 237     PartialResult WARN_UNUSED_RETURN addGrowMemory(ExpressionType delta, ExpressionType&amp; result);
 238     PartialResult WARN_UNUSED_RETURN addCurrentMemory(ExpressionType&amp; result);
 239 
 240     // Basic operators
 241     template&lt;OpType&gt;
 242     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType arg, ExpressionType&amp; result);
 243     template&lt;OpType&gt;
 244     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType left, ExpressionType right, ExpressionType&amp; result);
 245     PartialResult WARN_UNUSED_RETURN addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType&amp; result);
 246 
 247 
 248     // Control flow
 249     ControlData WARN_UNUSED_RETURN addTopLevel(BlockSignature);
 250     PartialResult WARN_UNUSED_RETURN addBlock(BlockSignature, Stack&amp; enclosingStack, ControlType&amp; newBlock, Stack&amp; newStack);
 251     PartialResult WARN_UNUSED_RETURN addLoop(BlockSignature, Stack&amp; enclosingStack, ControlType&amp; block, Stack&amp; newStack, uint32_t loopIndex);
 252     PartialResult WARN_UNUSED_RETURN addIf(ExpressionType condition, BlockSignature, Stack&amp; enclosingStack, ControlType&amp; result, Stack&amp; newStack);
 253     PartialResult WARN_UNUSED_RETURN addElse(ControlData&amp;, const Stack&amp;);
 254     PartialResult WARN_UNUSED_RETURN addElseToUnreachable(ControlData&amp;);
 255 
 256     PartialResult WARN_UNUSED_RETURN addReturn(const ControlData&amp;, const Stack&amp; returnValues);
 257     PartialResult WARN_UNUSED_RETURN addBranch(ControlData&amp;, ExpressionType condition, const Stack&amp; returnValues);
 258     PartialResult WARN_UNUSED_RETURN addSwitch(ExpressionType condition, const Vector&lt;ControlData*&gt;&amp; targets, ControlData&amp; defaultTargets, const Stack&amp; expressionStack);
 259     PartialResult WARN_UNUSED_RETURN endBlock(ControlEntry&amp;, Stack&amp; expressionStack);
 260     PartialResult WARN_UNUSED_RETURN addEndToUnreachable(ControlEntry&amp;, const Stack&amp; = { });
 261 
 262     PartialResult WARN_UNUSED_RETURN endTopLevel(BlockSignature, const Stack&amp;) { return { }; }
 263 
 264     // Calls
 265     PartialResult WARN_UNUSED_RETURN addCall(uint32_t calleeIndex, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results);
 266     PartialResult WARN_UNUSED_RETURN addCallIndirect(unsigned tableIndex, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results);
 267     PartialResult WARN_UNUSED_RETURN addUnreachable();
 268     B3::Value* createCallPatchpoint(BasicBlock*, Origin, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, const ScopedLambda&lt;void(PatchpointValue*)&gt;&amp; patchpointFunctor);
 269 
 270     void dump(const ControlStack&amp;, const Stack* expressionStack);
 271     void setParser(FunctionParser&lt;B3IRGenerator&gt;* parser) { m_parser = parser; };
 272     void didFinishParsingLocals() { }
 273     void didPopValueFromStack() { }
 274 
 275     Value* constant(B3::Type, uint64_t bits, Optional&lt;Origin&gt; = WTF::nullopt);
 276     Value* framePointer();
 277     void insertConstants();
 278 
 279     B3::Type toB3ResultType(BlockSignature);
 280 
 281 private:
 282     void emitExceptionCheck(CCallHelpers&amp;, ExceptionType);
 283 
 284     void emitEntryTierUpCheck();
 285     void emitLoopTierUpCheck(uint32_t loopIndex, const Stack&amp; enclosingStack);
 286 
 287     void emitWriteBarrierForJSWrapper();
 288     ExpressionType emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOp);
 289     B3::Kind memoryKind(B3::Opcode memoryOp);
 290     ExpressionType emitLoadOp(LoadOpType, ExpressionType pointer, uint32_t offset);
 291     void emitStoreOp(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 292 
 293     void unify(const ExpressionType phi, const ExpressionType source);
 294     void unifyValuesWithBlock(const Stack&amp; resultStack, const ResultList&amp; stack);
 295 
 296     void emitChecksForModOrDiv(B3::Opcode, ExpressionType left, ExpressionType right);
 297 
 298     int32_t WARN_UNUSED_RETURN fixupPointerPlusOffset(ExpressionType&amp;, uint32_t);
 299 
 300     void restoreWasmContextInstance(Procedure&amp;, BasicBlock*, Value*);
 301     enum class RestoreCachedStackLimit { No, Yes };
 302     void restoreWebAssemblyGlobalState(RestoreCachedStackLimit, const MemoryInformation&amp;, Value* instance, Procedure&amp;, BasicBlock*);
 303 
 304     Origin origin();
 305 
 306     uint32_t outerLoopIndex() const
 307     {
 308         if (m_outerLoops.isEmpty())
 309             return UINT32_MAX;
 310         return m_outerLoops.last();
 311     }
 312 
 313     FunctionParser&lt;B3IRGenerator&gt;* m_parser { nullptr };
 314     const ModuleInformation&amp; m_info;
 315     const MemoryMode m_mode { MemoryMode::BoundsChecking };
 316     const CompilationMode m_compilationMode { CompilationMode::BBQMode };
 317     const unsigned m_functionIndex { UINT_MAX };
 318     const unsigned m_loopIndexForOSREntry { UINT_MAX };
 319     TierUpCount* m_tierUp { nullptr };
 320 
 321     Procedure&amp; m_proc;
 322     BasicBlock* m_rootBlock { nullptr };
 323     BasicBlock* m_currentBlock { nullptr };
 324     Vector&lt;uint32_t&gt; m_outerLoops;
 325     Vector&lt;Variable*&gt; m_locals;
 326     Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; m_unlinkedWasmToWasmCalls; // List each call site and the function index whose address it should be patched with.
 327     unsigned&amp; m_osrEntryScratchBufferSize;
 328     HashMap&lt;ValueKey, Value*&gt; m_constantPool;
 329     HashMap&lt;BlockSignature, B3::Type&gt; m_tupleMap;
 330     InsertionSet m_constantInsertionValues;
 331     Value* m_framePointer { nullptr };
 332     GPRReg m_memoryBaseGPR { InvalidGPRReg };
 333     GPRReg m_memorySizeGPR { InvalidGPRReg };
 334     GPRReg m_wasmContextInstanceGPR { InvalidGPRReg };
 335     bool m_makesCalls { false };
 336 
 337     Value* m_instanceValue { nullptr }; // Always use the accessor below to ensure the instance value is materialized when used.
 338     bool m_usesInstanceValue { false };
 339     Value* instanceValue()
 340     {
 341         m_usesInstanceValue = true;
 342         return m_instanceValue;
 343     }
 344 
 345     uint32_t m_maxNumJSCallArguments { 0 };
 346     unsigned m_numImportFunctions;
 347 };
 348 
 349 // Memory accesses in WebAssembly have unsigned 32-bit offsets, whereas they have signed 32-bit offsets in B3.
 350 int32_t B3IRGenerator::fixupPointerPlusOffset(ExpressionType&amp; ptr, uint32_t offset)
 351 {
 352     if (static_cast&lt;uint64_t&gt;(offset) &gt; static_cast&lt;uint64_t&gt;(std::numeric_limits&lt;int32_t&gt;::max())) {
 353         ptr = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Add, origin(), ptr, m_currentBlock-&gt;appendNew&lt;Const64Value&gt;(m_proc, origin(), offset));
 354         return 0;
 355     }
 356     return offset;
 357 }
 358 
 359 void B3IRGenerator::restoreWasmContextInstance(Procedure&amp; proc, BasicBlock* block, Value* arg)
 360 {
 361     if (Context::useFastTLS()) {
 362         PatchpointValue* patchpoint = block-&gt;appendNew&lt;PatchpointValue&gt;(proc, B3::Void, Origin());
 363         if (CCallHelpers::storeWasmContextInstanceNeedsMacroScratchRegister())
 364             patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
 365         patchpoint-&gt;append(ConstrainedValue(arg, ValueRep::SomeRegister));
 366         patchpoint-&gt;setGenerator(
 367             [=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
 368                 AllowMacroScratchRegisterUsageIf allowScratch(jit, CCallHelpers::storeWasmContextInstanceNeedsMacroScratchRegister());
 369                 jit.storeWasmContextInstance(params[0].gpr());
 370             });
 371         return;
 372     }
 373 
 374     // FIXME: Because WasmToWasm call clobbers wasmContextInstance register and does not restore it, we need to restore it in the caller side.
 375     // This prevents us from using ArgumentReg to this (logically) immutable pinned register.
 376     PatchpointValue* patchpoint = block-&gt;appendNew&lt;PatchpointValue&gt;(proc, B3::Void, Origin());
 377     Effects effects = Effects::none();
 378     effects.writesPinned = true;
 379     effects.reads = B3::HeapRange::top();
 380     patchpoint-&gt;effects = effects;
 381     patchpoint-&gt;clobberLate(RegisterSet(m_wasmContextInstanceGPR));
 382     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
 383     GPRReg wasmContextInstanceGPR = m_wasmContextInstanceGPR;
 384     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; param) {
 385         jit.move(param[0].gpr(), wasmContextInstanceGPR);
 386     });
 387 }
 388 
 389 B3IRGenerator::B3IRGenerator(const ModuleInformation&amp; info, Procedure&amp; procedure, InternalFunction* compilation, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; unlinkedWasmToWasmCalls, unsigned&amp; osrEntryScratchBufferSize, MemoryMode mode, CompilationMode compilationMode, unsigned functionIndex, unsigned loopIndexForOSREntry, TierUpCount* tierUp)
 390     : m_info(info)
 391     , m_mode(mode)
 392     , m_compilationMode(compilationMode)
 393     , m_functionIndex(functionIndex)
 394     , m_loopIndexForOSREntry(loopIndexForOSREntry)
 395     , m_tierUp(tierUp)
 396     , m_proc(procedure)
 397     , m_unlinkedWasmToWasmCalls(unlinkedWasmToWasmCalls)
 398     , m_osrEntryScratchBufferSize(osrEntryScratchBufferSize)
 399     , m_constantInsertionValues(m_proc)
 400     , m_numImportFunctions(info.importFunctionCount())
 401 {
 402     m_rootBlock = m_proc.addBlock();
 403     m_currentBlock = m_rootBlock;
 404 
 405     // FIXME we don&#39;t really need to pin registers here if there&#39;s no memory. It makes wasm -&gt; wasm thunks simpler for now. https://bugs.webkit.org/show_bug.cgi?id=166623
 406     const PinnedRegisterInfo&amp; pinnedRegs = PinnedRegisterInfo::get();
 407 
 408     m_memoryBaseGPR = pinnedRegs.baseMemoryPointer;
 409     m_proc.pinRegister(m_memoryBaseGPR);
 410 
 411     m_wasmContextInstanceGPR = pinnedRegs.wasmContextInstancePointer;
 412     if (!Context::useFastTLS())
 413         m_proc.pinRegister(m_wasmContextInstanceGPR);
 414 
 415     if (mode != MemoryMode::Signaling) {
 416         m_memorySizeGPR = pinnedRegs.sizeRegister;
 417         m_proc.pinRegister(m_memorySizeGPR);
 418     }
 419 
 420     if (info.memory) {
 421         m_proc.setWasmBoundsCheckGenerator([=] (CCallHelpers&amp; jit, GPRReg pinnedGPR) {
 422             AllowMacroScratchRegisterUsage allowScratch(jit);
 423             switch (m_mode) {
 424             case MemoryMode::BoundsChecking:
 425                 ASSERT_UNUSED(pinnedGPR, m_memorySizeGPR == pinnedGPR);
 426                 break;
 427             case MemoryMode::Signaling:
 428                 ASSERT_UNUSED(pinnedGPR, InvalidGPRReg == pinnedGPR);
 429                 break;
 430             }
 431             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsMemoryAccess);
 432         });
 433 
 434         switch (m_mode) {
 435         case MemoryMode::BoundsChecking:
 436             break;
 437         case MemoryMode::Signaling:
 438             // Most memory accesses in signaling mode don&#39;t do an explicit
 439             // exception check because they can rely on fault handling to detect
 440             // out-of-bounds accesses. FaultSignalHandler nonetheless needs the
 441             // thunk to exist so that it can jump to that thunk.
 442             if (UNLIKELY(!Thunks::singleton().stub(throwExceptionFromWasmThunkGenerator)))
 443                 CRASH();
 444             break;
 445         }
 446     }
 447 
 448     {
 449         auto* calleeMoveLocation = &amp;compilation-&gt;calleeMoveLocation;
 450         static_assert(CallFrameSlot::codeBlock * sizeof(Register) &lt; WasmCallingConvention::headerSizeInBytes, &quot;We rely on this here for now.&quot;);
 451         static_assert(CallFrameSlot::callee * sizeof(Register) &lt; WasmCallingConvention::headerSizeInBytes, &quot;We rely on this here for now.&quot;);
 452         B3::PatchpointValue* getCalleePatchpoint = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, B3::Int64, Origin());
 453         getCalleePatchpoint-&gt;resultConstraints = { B3::ValueRep::SomeRegister };
 454         getCalleePatchpoint-&gt;effects = B3::Effects::none();
 455         getCalleePatchpoint-&gt;setGenerator(
 456             [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 457                 GPRReg result = params[0].gpr();
 458                 MacroAssembler::DataLabelPtr moveLocation = jit.moveWithPatch(MacroAssembler::TrustedImmPtr(nullptr), result);
 459                 jit.addLinkTask([calleeMoveLocation, moveLocation] (LinkBuffer&amp; linkBuffer) {
 460                     *calleeMoveLocation = linkBuffer.locationOf&lt;WasmEntryPtrTag&gt;(moveLocation);
 461                 });
 462             });
 463 
 464         B3::Value* offsetOfCallee = m_currentBlock-&gt;appendNew&lt;B3::Const64Value&gt;(m_proc, Origin(), CallFrameSlot::callee * sizeof(Register));
 465         m_currentBlock-&gt;appendNew&lt;B3::MemoryValue&gt;(m_proc, B3::Store, Origin(),
 466             getCalleePatchpoint,
 467             m_currentBlock-&gt;appendNew&lt;B3::Value&gt;(m_proc, B3::Add, Origin(), framePointer(), offsetOfCallee));
 468 
 469         // FIXME: We shouldn&#39;t have to store zero into the CodeBlock* spot in the call frame,
 470         // but there are places that interpret non-null CodeBlock slot to mean a valid CodeBlock.
 471         // When doing unwinding, we&#39;ll need to verify that the entire runtime is OK with a non-null
 472         // CodeBlock not implying that the CodeBlock is valid.
 473         // https://bugs.webkit.org/show_bug.cgi?id=165321
 474         B3::Value* offsetOfCodeBlock = m_currentBlock-&gt;appendNew&lt;B3::Const64Value&gt;(m_proc, Origin(), CallFrameSlot::codeBlock * sizeof(Register));
 475         m_currentBlock-&gt;appendNew&lt;B3::MemoryValue&gt;(m_proc, B3::Store, Origin(),
 476             m_currentBlock-&gt;appendNew&lt;B3::Const64Value&gt;(m_proc, Origin(), 0),
 477             m_currentBlock-&gt;appendNew&lt;B3::Value&gt;(m_proc, B3::Add, Origin(), framePointer(), offsetOfCodeBlock));
 478     }
 479 
 480     {
 481         B3::PatchpointValue* stackOverflowCheck = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, pointerType(), Origin());
 482         m_instanceValue = stackOverflowCheck;
 483         stackOverflowCheck-&gt;appendSomeRegister(framePointer());
 484         stackOverflowCheck-&gt;clobber(RegisterSet::macroScratchRegisters());
 485         if (!Context::useFastTLS()) {
 486             // FIXME: Because WasmToWasm call clobbers wasmContextInstance register and does not restore it, we need to restore it in the caller side.
 487             // This prevents us from using ArgumentReg to this (logically) immutable pinned register.
 488             stackOverflowCheck-&gt;effects.writesPinned = false;
 489             stackOverflowCheck-&gt;effects.readsPinned = true;
 490             stackOverflowCheck-&gt;resultConstraints = { ValueRep::reg(m_wasmContextInstanceGPR) };
 491         }
 492         stackOverflowCheck-&gt;numGPScratchRegisters = 2;
 493         stackOverflowCheck-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 494             const Checked&lt;int32_t&gt; wasmFrameSize = params.proc().frameSize();
 495             const unsigned minimumParentCheckSize = WTF::roundUpToMultipleOf(stackAlignmentBytes(), 1024);
 496             const unsigned extraFrameSize = WTF::roundUpToMultipleOf(stackAlignmentBytes(), std::max&lt;uint32_t&gt;(
 497                 // This allows us to elide stack checks for functions that are terminal nodes in the call
 498                 // tree, (e.g they don&#39;t make any calls) and have a small enough frame size. This works by
 499                 // having any such terminal node have its parent caller include some extra size in its
 500                 // own check for it. The goal here is twofold:
 501                 // 1. Emit less code.
 502                 // 2. Try to speed things up by skipping stack checks.
 503                 minimumParentCheckSize,
 504                 // This allows us to elide stack checks in the Wasm -&gt; Embedder call IC stub. Since these will
 505                 // spill all arguments to the stack, we ensure that a stack check here covers the
 506                 // stack that such a stub would use.
 507                 (Checked&lt;uint32_t&gt;(m_maxNumJSCallArguments) * sizeof(Register) + JSCallingConvention::headerSizeInBytes).unsafeGet()
 508             ));
 509             const int32_t checkSize = m_makesCalls ? (wasmFrameSize + extraFrameSize).unsafeGet() : wasmFrameSize.unsafeGet();
 510             bool needUnderflowCheck = static_cast&lt;unsigned&gt;(checkSize) &gt; Options::reservedZoneSize();
 511             bool needsOverflowCheck = m_makesCalls || wasmFrameSize &gt;= minimumParentCheckSize || needUnderflowCheck;
 512 
 513             GPRReg contextInstance = Context::useFastTLS() ? params[0].gpr() : m_wasmContextInstanceGPR;
 514 
 515             // This allows leaf functions to not do stack checks if their frame size is within
 516             // certain limits since their caller would have already done the check.
 517             if (needsOverflowCheck) {
 518                 AllowMacroScratchRegisterUsage allowScratch(jit);
 519                 GPRReg fp = params[1].gpr();
 520                 GPRReg scratch1 = params.gpScratch(0);
 521                 GPRReg scratch2 = params.gpScratch(1);
 522 
 523                 if (Context::useFastTLS())
 524                     jit.loadWasmContextInstance(contextInstance);
 525 
 526                 jit.loadPtr(CCallHelpers::Address(contextInstance, Instance::offsetOfCachedStackLimit()), scratch2);
 527                 jit.addPtr(CCallHelpers::TrustedImm32(-checkSize), fp, scratch1);
 528                 MacroAssembler::JumpList overflow;
 529                 if (UNLIKELY(needUnderflowCheck))
 530                     overflow.append(jit.branchPtr(CCallHelpers::Above, scratch1, fp));
 531                 overflow.append(jit.branchPtr(CCallHelpers::Below, scratch1, scratch2));
 532                 jit.addLinkTask([overflow] (LinkBuffer&amp; linkBuffer) {
 533                     linkBuffer.link(overflow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(throwStackOverflowFromWasmThunkGenerator).code()));
 534                 });
 535             } else if (m_usesInstanceValue &amp;&amp; Context::useFastTLS()) {
 536                 // No overflow check is needed, but the instance values still needs to be correct.
 537                 AllowMacroScratchRegisterUsageIf allowScratch(jit, CCallHelpers::loadWasmContextInstanceNeedsMacroScratchRegister());
 538                 jit.loadWasmContextInstance(contextInstance);
 539             } else {
 540                 // We said we&#39;d return a pointer. We don&#39;t actually need to because it isn&#39;t used, but the patchpoint conservatively said it had effects (potential stack check) which prevent it from getting removed.
 541             }
 542         });
 543     }
 544 
 545     emitEntryTierUpCheck();
 546 
 547     if (m_compilationMode == CompilationMode::OMGForOSREntryMode)
 548         m_currentBlock = m_proc.addBlock();
 549 }
 550 
 551 void B3IRGenerator::restoreWebAssemblyGlobalState(RestoreCachedStackLimit restoreCachedStackLimit, const MemoryInformation&amp; memory, Value* instance, Procedure&amp; proc, BasicBlock* block)
 552 {
 553     restoreWasmContextInstance(proc, block, instance);
 554 
 555     if (restoreCachedStackLimit == RestoreCachedStackLimit::Yes) {
 556         // The Instance caches the stack limit, but also knows where its canonical location is.
 557         Value* pointerToActualStackLimit = block-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfPointerToActualStackLimit()));
 558         Value* actualStackLimit = block-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), pointerToActualStackLimit);
 559         block-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Store, origin(), actualStackLimit, instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfCachedStackLimit()));
 560     }
 561 
 562     if (!!memory) {
 563         const PinnedRegisterInfo* pinnedRegs = &amp;PinnedRegisterInfo::get();
 564         RegisterSet clobbers;
 565         clobbers.set(pinnedRegs-&gt;baseMemoryPointer);
 566         clobbers.set(pinnedRegs-&gt;sizeRegister);
 567         if (!isARM64())
 568             clobbers.set(RegisterSet::macroScratchRegisters());
 569 
 570         B3::PatchpointValue* patchpoint = block-&gt;appendNew&lt;B3::PatchpointValue&gt;(proc, B3::Void, origin());
 571         Effects effects = Effects::none();
 572         effects.writesPinned = true;
 573         effects.reads = B3::HeapRange::top();
 574         patchpoint-&gt;effects = effects;
 575         patchpoint-&gt;clobber(clobbers);
 576         patchpoint-&gt;numGPScratchRegisters = Gigacage::isEnabled(Gigacage::Primitive) ? 1 : 0;
 577 
 578         patchpoint-&gt;append(instance, ValueRep::SomeRegister);
 579         patchpoint-&gt;setGenerator([pinnedRegs] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 580             AllowMacroScratchRegisterUsage allowScratch(jit);
 581             GPRReg baseMemory = pinnedRegs-&gt;baseMemoryPointer;
 582             GPRReg scratchOrSize = Gigacage::isEnabled(Gigacage::Primitive) ? params.gpScratch(0) : pinnedRegs-&gt;sizeRegister;
 583 
 584             jit.loadPtr(CCallHelpers::Address(params[0].gpr(), Instance::offsetOfCachedMemorySize()), pinnedRegs-&gt;sizeRegister);
 585             jit.loadPtr(CCallHelpers::Address(params[0].gpr(), Instance::offsetOfCachedMemory()), baseMemory);
 586 
 587             jit.cageConditionally(Gigacage::Primitive, baseMemory, pinnedRegs-&gt;sizeRegister, scratchOrSize);
 588         });
 589     }
 590 }
 591 
 592 void B3IRGenerator::emitExceptionCheck(CCallHelpers&amp; jit, ExceptionType type)
 593 {
 594     jit.move(CCallHelpers::TrustedImm32(static_cast&lt;uint32_t&gt;(type)), GPRInfo::argumentGPR1);
 595     auto jumpToExceptionStub = jit.jump();
 596 
 597     jit.addLinkTask([jumpToExceptionStub] (LinkBuffer&amp; linkBuffer) {
 598         linkBuffer.link(jumpToExceptionStub, CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(throwExceptionFromWasmThunkGenerator).code()));
 599     });
 600 }
 601 
 602 Value* B3IRGenerator::constant(B3::Type type, uint64_t bits, Optional&lt;Origin&gt; maybeOrigin)
 603 {
 604     auto result = m_constantPool.ensure(ValueKey(opcodeForConstant(type), type, static_cast&lt;int64_t&gt;(bits)), [&amp;] {
 605         Value* result = m_proc.addConstant(maybeOrigin ? *maybeOrigin : origin(), type, bits);
 606         m_constantInsertionValues.insertValue(0, result);
 607         return result;
 608     });
 609     return result.iterator-&gt;value;
 610 }
 611 
 612 Value* B3IRGenerator::framePointer()
 613 {
 614     if (!m_framePointer) {
 615         m_framePointer = m_proc.add&lt;B3::Value&gt;(B3::FramePointer, Origin());
 616         ASSERT(m_framePointer);
 617         m_constantInsertionValues.insertValue(0, m_framePointer);
 618     }
 619     return m_framePointer;
 620 }
 621 
 622 void B3IRGenerator::insertConstants()
 623 {
 624     m_constantInsertionValues.execute(m_proc.at(0));
 625 }
 626 
 627 B3::Type B3IRGenerator::toB3ResultType(BlockSignature returnType)
 628 {
 629     if (returnType-&gt;returnsVoid())
 630         return B3::Void;
 631 
 632     if (returnType-&gt;returnCount() == 1)
 633         return toB3Type(returnType-&gt;returnType(0));
 634 
 635     auto result = m_tupleMap.ensure(returnType, [&amp;] {
 636         Vector&lt;B3::Type&gt; result;
 637         for (unsigned i = 0; i &lt; returnType-&gt;returnCount(); ++i)
 638             result.append(toB3Type(returnType-&gt;returnType(i)));
 639         return m_proc.addTuple(WTFMove(result));
 640     });
 641     return result.iterator-&gt;value;
 642 }
 643 
 644 auto B3IRGenerator::addLocal(Type type, uint32_t count) -&gt; PartialResult
 645 {
 646     size_t newSize = m_locals.size() + count;
 647     ASSERT(!(CheckedUint32(count) + m_locals.size()).hasOverflowed());
 648     ASSERT(newSize &lt;= maxFunctionLocals);
 649     WASM_COMPILE_FAIL_IF(!m_locals.tryReserveCapacity(newSize), &quot;can&#39;t allocate memory for &quot;, newSize, &quot; locals&quot;);
 650 
 651     for (uint32_t i = 0; i &lt; count; ++i) {
 652         Variable* local = m_proc.addVariable(toB3Type(type));
 653         m_locals.uncheckedAppend(local);
 654         auto val = isSubtype(type, Anyref) ? JSValue::encode(jsNull()) : 0;
 655         m_currentBlock-&gt;appendNew&lt;VariableValue&gt;(m_proc, Set, Origin(), local, constant(toB3Type(type), val, Origin()));
 656     }
 657     return { };
 658 }
 659 
 660 auto B3IRGenerator::addArguments(const Signature&amp; signature) -&gt; PartialResult
 661 {
 662     ASSERT(!m_locals.size());
 663     WASM_COMPILE_FAIL_IF(!m_locals.tryReserveCapacity(signature.argumentCount()), &quot;can&#39;t allocate memory for &quot;, signature.argumentCount(), &quot; arguments&quot;);
 664 
 665     m_locals.grow(signature.argumentCount());
 666     CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(signature, CallRole::Callee);
 667 
 668     for (size_t i = 0; i &lt; signature.argumentCount(); ++i) {
 669         B3::Type type = toB3Type(signature.argument(i));
 670         B3::Value* argument;
 671         auto rep = wasmCallInfo.params[i];
 672         if (rep.isReg()) {
 673             argument = m_currentBlock-&gt;appendNew&lt;B3::ArgumentRegValue&gt;(m_proc, Origin(), rep.reg());
 674             if (type == B3::Int32 || type == B3::Float)
 675                 argument = m_currentBlock-&gt;appendNew&lt;B3::Value&gt;(m_proc, B3::Trunc, Origin(), argument);
 676         } else {
 677             ASSERT(rep.isStack());
 678             B3::Value* address = m_currentBlock-&gt;appendNew&lt;B3::Value&gt;(m_proc, B3::Add, Origin(), framePointer(),
 679                 m_currentBlock-&gt;appendNew&lt;B3::Const64Value&gt;(m_proc, Origin(), rep.offsetFromFP()));
 680             argument = m_currentBlock-&gt;appendNew&lt;B3::MemoryValue&gt;(m_proc, B3::Load, type, Origin(), address);
 681         }
 682 
 683         Variable* argumentVariable = m_proc.addVariable(argument-&gt;type());
 684         m_locals[i] = argumentVariable;
 685         m_currentBlock-&gt;appendNew&lt;VariableValue&gt;(m_proc, Set, Origin(), argumentVariable, argument);
 686     }
 687 
 688     return { };
 689 }
 690 
 691 auto B3IRGenerator::addRefIsNull(ExpressionType value, ExpressionType&amp; result) -&gt; PartialResult
 692 {
 693     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, B3::Equal, origin(), value, m_currentBlock-&gt;appendNew&lt;Const64Value&gt;(m_proc, origin(), JSValue::encode(jsNull())));
 694     return { };
 695 }
 696 
 697 auto B3IRGenerator::addTableGet(unsigned tableIndex, ExpressionType index, ExpressionType&amp; result) -&gt; PartialResult
 698 {
 699     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
 700     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, toB3Type(Anyref), origin(),
 701         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationGetWasmTableElement, B3CCallPtrTag)),
 702         instanceValue(), m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), tableIndex), index);
 703 
 704     {
 705         CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
 706             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), result, m_currentBlock-&gt;appendNew&lt;Const64Value&gt;(m_proc, origin(), 0)));
 707 
 708         check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
 709             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTableAccess);
 710         });
 711     }
 712 
 713     return { };
 714 }
 715 
 716 auto B3IRGenerator::addTableSet(unsigned tableIndex, ExpressionType index, ExpressionType value) -&gt; PartialResult
 717 {
 718     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
 719     auto shouldThrow = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, B3::Int32, origin(),
 720         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationSetWasmTableElement, B3CCallPtrTag)),
 721         instanceValue(), m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), tableIndex), index, value);
 722 
 723     {
 724         CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
 725             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), shouldThrow, m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), 0)));
 726 
 727         check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
 728             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTableAccess);
 729         });
 730     }
 731 
 732     return { };
 733 }
 734 
 735 auto B3IRGenerator::addRefFunc(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
 736 {
 737     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
 738 
 739     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, B3::Int64, origin(),
 740         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationWasmRefFunc, B3CCallPtrTag)),
 741         instanceValue(), addConstant(Type::I32, index));
 742 
 743     return { };
 744 }
 745 
 746 auto B3IRGenerator::addTableSize(unsigned tableIndex, ExpressionType&amp; result) -&gt; PartialResult
 747 {
 748     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
 749     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, toB3Type(I32), origin(),
 750         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationGetWasmTableSize, B3CCallPtrTag)),
 751         instanceValue(), m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), tableIndex));
 752 
 753     return { };
 754 }
 755 
 756 auto B3IRGenerator::addTableGrow(unsigned tableIndex, ExpressionType fill, ExpressionType delta, ExpressionType&amp; result) -&gt; PartialResult
 757 {
 758     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, toB3Type(I32), origin(),
 759         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationWasmTableGrow, B3CCallPtrTag)),
 760         instanceValue(), m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), tableIndex), fill, delta);
 761 
 762     return { };
 763 }
 764 
 765 auto B3IRGenerator::addTableFill(unsigned tableIndex, ExpressionType offset, ExpressionType fill, ExpressionType count) -&gt; PartialResult
 766 {
 767     auto result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, toB3Type(I32), origin(),
 768         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationWasmTableFill, B3CCallPtrTag)),
 769         instanceValue(), m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), tableIndex), offset, fill, count);
 770 
 771     {
 772         CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
 773             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), result, m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), 0)));
 774 
 775         check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
 776             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTableAccess);
 777         });
 778     }
 779 
 780     return { };
 781 }
 782 
 783 auto B3IRGenerator::getLocal(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
 784 {
 785     ASSERT(m_locals[index]);
 786     result = m_currentBlock-&gt;appendNew&lt;VariableValue&gt;(m_proc, B3::Get, origin(), m_locals[index]);
 787     return { };
 788 }
 789 
 790 auto B3IRGenerator::addUnreachable() -&gt; PartialResult
 791 {
 792     B3::PatchpointValue* unreachable = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, B3::Void, origin());
 793     unreachable-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
 794         this-&gt;emitExceptionCheck(jit, ExceptionType::Unreachable);
 795     });
 796     unreachable-&gt;effects.terminal = true;
 797     return { };
 798 }
 799 
 800 auto B3IRGenerator::addGrowMemory(ExpressionType delta, ExpressionType&amp; result) -&gt; PartialResult
 801 {
 802     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, Int32, origin(),
 803         m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationGrowMemory, B3CCallPtrTag)),
 804         framePointer(), instanceValue(), delta);
 805 
 806     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::No, m_info.memory, instanceValue(), m_proc, m_currentBlock);
 807 
 808     return { };
 809 }
 810 
 811 auto B3IRGenerator::addCurrentMemory(ExpressionType&amp; result) -&gt; PartialResult
 812 {
 813     static_assert(sizeof(decltype(static_cast&lt;Memory*&gt;(nullptr)-&gt;size())) == sizeof(uint64_t), &quot;codegen relies on this size&quot;);
 814     Value* size = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, Int64, origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfCachedMemorySize()));
 815 
 816     constexpr uint32_t shiftValue = 16;
 817     static_assert(PageCount::pageSize == 1ull &lt;&lt; shiftValue, &quot;This must hold for the code below to be correct.&quot;);
 818     Value* numPages = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, ZShr, origin(),
 819         size, m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), shiftValue));
 820 
 821     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Trunc, origin(), numPages);
 822 
 823     return { };
 824 }
 825 
 826 auto B3IRGenerator::setLocal(uint32_t index, ExpressionType value) -&gt; PartialResult
 827 {
 828     ASSERT(m_locals[index]);
 829     m_currentBlock-&gt;appendNew&lt;VariableValue&gt;(m_proc, B3::Set, origin(), m_locals[index], value);
 830     return { };
 831 }
 832 
 833 auto B3IRGenerator::getGlobal(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
 834 {
 835     const Wasm::GlobalInformation&amp; global = m_info.globals[index];
 836     Value* globalsArray = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfGlobals()));
 837     switch (global.bindingMode) {
 838     case Wasm::GlobalInformation::BindingMode::EmbeddedInInstance:
 839         result = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, toB3Type(global.type), origin(), globalsArray, safeCast&lt;int32_t&gt;(index * sizeof(Register)));
 840         break;
 841     case Wasm::GlobalInformation::BindingMode::Portable: {
 842         ASSERT(global.mutability == Wasm::GlobalInformation::Mutability::Mutable);
 843         Value* pointer = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, B3::Int64, origin(), globalsArray, safeCast&lt;int32_t&gt;(index * sizeof(Register)));
 844         result = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, toB3Type(global.type), origin(), pointer);
 845         break;
 846     }
 847     }
 848     return { };
 849 }
 850 
 851 auto B3IRGenerator::setGlobal(uint32_t index, ExpressionType value) -&gt; PartialResult
 852 {
 853     const Wasm::GlobalInformation&amp; global = m_info.globals[index];
 854     ASSERT(toB3Type(global.type) == value-&gt;type());
 855     Value* globalsArray = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfGlobals()));
 856     switch (global.bindingMode) {
 857     case Wasm::GlobalInformation::BindingMode::EmbeddedInInstance:
 858         m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Store, origin(), value, globalsArray, safeCast&lt;int32_t&gt;(index * sizeof(Register)));
 859         if (isSubtype(global.type, Anyref))
 860             emitWriteBarrierForJSWrapper();
 861         break;
 862     case Wasm::GlobalInformation::BindingMode::Portable: {
 863         ASSERT(global.mutability == Wasm::GlobalInformation::Mutability::Mutable);
 864         Value* pointer = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, B3::Int64, origin(), globalsArray, safeCast&lt;int32_t&gt;(index * sizeof(Register)));
 865         m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Store, origin(), value, pointer);
 866         // We emit a write-barrier onto JSWebAssemblyGlobal, not JSWebAssemblyInstance.
 867         if (isSubtype(global.type, Anyref)) {
 868             Value* instance = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfOwner()));
 869             Value* cell = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), pointer, Wasm::Global::offsetOfOwner() - Wasm::Global::offsetOfValue());
 870             Value* cellState = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load8Z, Int32, origin(), cell, safeCast&lt;int32_t&gt;(JSCell::cellStateOffset()));
 871             Value* vm = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instance, safeCast&lt;int32_t&gt;(JSWebAssemblyInstance::offsetOfVM()));
 872             Value* threshold = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, Int32, origin(), vm, safeCast&lt;int32_t&gt;(VM::offsetOfHeapBarrierThreshold()));
 873 
 874             BasicBlock* fenceCheckPath = m_proc.addBlock();
 875             BasicBlock* fencePath = m_proc.addBlock();
 876             BasicBlock* doSlowPath = m_proc.addBlock();
 877             BasicBlock* continuation = m_proc.addBlock();
 878 
 879             m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
 880                 m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Above, origin(), cellState, threshold),
 881                 FrequentedBlock(continuation), FrequentedBlock(fenceCheckPath, FrequencyClass::Rare));
 882             fenceCheckPath-&gt;addPredecessor(m_currentBlock);
 883             continuation-&gt;addPredecessor(m_currentBlock);
 884             m_currentBlock = fenceCheckPath;
 885 
 886             Value* shouldFence = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load8Z, Int32, origin(), vm, safeCast&lt;int32_t&gt;(VM::offsetOfHeapMutatorShouldBeFenced()));
 887             m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
 888                 shouldFence,
 889                 FrequentedBlock(fencePath), FrequentedBlock(doSlowPath));
 890             fencePath-&gt;addPredecessor(m_currentBlock);
 891             doSlowPath-&gt;addPredecessor(m_currentBlock);
 892             m_currentBlock = fencePath;
 893 
 894             B3::PatchpointValue* doFence = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, B3::Void, origin());
 895             doFence-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
 896                 jit.memoryFence();
 897             });
 898 
 899             Value* cellStateLoadAfterFence = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load8Z, Int32, origin(), cell, safeCast&lt;int32_t&gt;(JSCell::cellStateOffset()));
 900             m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
 901                 m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Above, origin(), cellStateLoadAfterFence, m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), blackThreshold)),
 902                 FrequentedBlock(continuation), FrequentedBlock(doSlowPath, FrequencyClass::Rare));
 903             doSlowPath-&gt;addPredecessor(m_currentBlock);
 904             continuation-&gt;addPredecessor(m_currentBlock);
 905             m_currentBlock = doSlowPath;
 906 
 907             Value* writeBarrierAddress = m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationWasmWriteBarrierSlowPath, B3CCallPtrTag));
 908             m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, B3::Void, origin(), writeBarrierAddress, cell, vm);
 909             m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), continuation);
 910 
 911             continuation-&gt;addPredecessor(m_currentBlock);
 912             m_currentBlock = continuation;
 913         }
 914         break;
 915     }
 916     }
 917     return { };
 918 }
 919 
 920 inline void B3IRGenerator::emitWriteBarrierForJSWrapper()
 921 {
 922     Value* cell = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfOwner()));
 923     Value* cellState = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load8Z, Int32, origin(), cell, safeCast&lt;int32_t&gt;(JSCell::cellStateOffset()));
 924     Value* vm = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), cell, safeCast&lt;int32_t&gt;(JSWebAssemblyInstance::offsetOfVM()));
 925     Value* threshold = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, Int32, origin(), vm, safeCast&lt;int32_t&gt;(VM::offsetOfHeapBarrierThreshold()));
 926 
 927     BasicBlock* fenceCheckPath = m_proc.addBlock();
 928     BasicBlock* fencePath = m_proc.addBlock();
 929     BasicBlock* doSlowPath = m_proc.addBlock();
 930     BasicBlock* continuation = m_proc.addBlock();
 931 
 932     m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
 933         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Above, origin(), cellState, threshold),
 934         FrequentedBlock(continuation), FrequentedBlock(fenceCheckPath, FrequencyClass::Rare));
 935     fenceCheckPath-&gt;addPredecessor(m_currentBlock);
 936     continuation-&gt;addPredecessor(m_currentBlock);
 937     m_currentBlock = fenceCheckPath;
 938 
 939     Value* shouldFence = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load8Z, Int32, origin(), vm, safeCast&lt;int32_t&gt;(VM::offsetOfHeapMutatorShouldBeFenced()));
 940     m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
 941         shouldFence,
 942         FrequentedBlock(fencePath), FrequentedBlock(doSlowPath));
 943     fencePath-&gt;addPredecessor(m_currentBlock);
 944     doSlowPath-&gt;addPredecessor(m_currentBlock);
 945     m_currentBlock = fencePath;
 946 
 947     B3::PatchpointValue* doFence = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, B3::Void, origin());
 948     doFence-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
 949         jit.memoryFence();
 950     });
 951 
 952     Value* cellStateLoadAfterFence = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load8Z, Int32, origin(), cell, safeCast&lt;int32_t&gt;(JSCell::cellStateOffset()));
 953     m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
 954         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Above, origin(), cellStateLoadAfterFence, m_currentBlock-&gt;appendNew&lt;Const32Value&gt;(m_proc, origin(), blackThreshold)),
 955         FrequentedBlock(continuation), FrequentedBlock(doSlowPath, FrequencyClass::Rare));
 956     doSlowPath-&gt;addPredecessor(m_currentBlock);
 957     continuation-&gt;addPredecessor(m_currentBlock);
 958     m_currentBlock = doSlowPath;
 959 
 960     Value* writeBarrierAddress = m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationWasmWriteBarrierSlowPath, B3CCallPtrTag));
 961     m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, B3::Void, origin(), writeBarrierAddress, cell, vm);
 962     m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), continuation);
 963 
 964     continuation-&gt;addPredecessor(m_currentBlock);
 965     m_currentBlock = continuation;
 966 }
 967 
 968 inline Value* B3IRGenerator::emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOperation)
 969 {
 970     ASSERT(m_memoryBaseGPR);
 971 
 972     switch (m_mode) {
 973     case MemoryMode::BoundsChecking: {
 974         // We&#39;re not using signal handling at all, we must therefore check that no memory access exceeds the current memory size.
 975         ASSERT(m_memorySizeGPR);
 976         ASSERT(sizeOfOperation + offset &gt; offset);
 977         m_currentBlock-&gt;appendNew&lt;WasmBoundsCheckValue&gt;(m_proc, origin(), m_memorySizeGPR, pointer, sizeOfOperation + offset - 1);
 978         break;
 979     }
 980 
 981     case MemoryMode::Signaling: {
 982         // We&#39;ve virtually mapped 4GiB+redzone for this memory. Only the user-allocated pages are addressable, contiguously in range [0, current],
 983         // and everything above is mapped PROT_NONE. We don&#39;t need to perform any explicit bounds check in the 4GiB range because WebAssembly register
 984         // memory accesses are 32-bit. However WebAssembly register + offset accesses perform the addition in 64-bit which can push an access above
 985         // the 32-bit limit (the offset is unsigned 32-bit). The redzone will catch most small offsets, and we&#39;ll explicitly bounds check any
 986         // register + large offset access. We don&#39;t think this will be generated frequently.
 987         //
 988         // We could check that register + large offset doesn&#39;t exceed 4GiB+redzone since that&#39;s technically the limit we need to avoid overflowing the
 989         // PROT_NONE region, but it&#39;s better if we use a smaller immediate because it can codegens better. We know that anything equal to or greater
 990         // than the declared &#39;maximum&#39; will trap, so we can compare against that number. If there was no declared &#39;maximum&#39; then we still know that
 991         // any access equal to or greater than 4GiB will trap, no need to add the redzone.
 992         if (offset &gt;= Memory::fastMappedRedzoneBytes()) {
 993             size_t maximum = m_info.memory.maximum() ? m_info.memory.maximum().bytes() : std::numeric_limits&lt;uint32_t&gt;::max();
 994             m_currentBlock-&gt;appendNew&lt;WasmBoundsCheckValue&gt;(m_proc, origin(), pointer, sizeOfOperation + offset - 1, maximum);
 995         }
 996         break;
 997     }
 998     }
 999 
1000     pointer = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, ZExt32, origin(), pointer);
1001     return m_currentBlock-&gt;appendNew&lt;WasmAddressValue&gt;(m_proc, origin(), pointer, m_memoryBaseGPR);
1002 }
1003 
1004 inline uint32_t sizeOfLoadOp(LoadOpType op)
1005 {
1006     switch (op) {
1007     case LoadOpType::I32Load8S:
1008     case LoadOpType::I32Load8U:
1009     case LoadOpType::I64Load8S:
1010     case LoadOpType::I64Load8U:
1011         return 1;
1012     case LoadOpType::I32Load16S:
1013     case LoadOpType::I64Load16S:
1014     case LoadOpType::I32Load16U:
1015     case LoadOpType::I64Load16U:
1016         return 2;
1017     case LoadOpType::I32Load:
1018     case LoadOpType::I64Load32S:
1019     case LoadOpType::I64Load32U:
1020     case LoadOpType::F32Load:
1021         return 4;
1022     case LoadOpType::I64Load:
1023     case LoadOpType::F64Load:
1024         return 8;
1025     }
1026     RELEASE_ASSERT_NOT_REACHED();
1027 }
1028 
1029 inline B3::Kind B3IRGenerator::memoryKind(B3::Opcode memoryOp)
1030 {
1031     if (m_mode == MemoryMode::Signaling)
1032         return trapping(memoryOp);
1033     return memoryOp;
1034 }
1035 
1036 inline Value* B3IRGenerator::emitLoadOp(LoadOpType op, ExpressionType pointer, uint32_t uoffset)
1037 {
1038     int32_t offset = fixupPointerPlusOffset(pointer, uoffset);
1039 
1040     switch (op) {
1041     case LoadOpType::I32Load8S: {
1042         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load8S), origin(), pointer, offset);
1043     }
1044 
1045     case LoadOpType::I64Load8S: {
1046         Value* value = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load8S), origin(), pointer, offset);
1047         return m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, SExt32, origin(), value);
1048     }
1049 
1050     case LoadOpType::I32Load8U: {
1051         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load8Z), origin(), pointer, offset);
1052     }
1053 
1054     case LoadOpType::I64Load8U: {
1055         Value* value = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load8Z), origin(), pointer, offset);
1056         return m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, ZExt32, origin(), value);
1057     }
1058 
1059     case LoadOpType::I32Load16S: {
1060         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load16S), origin(), pointer, offset);
1061     }
1062 
1063     case LoadOpType::I64Load16S: {
1064         Value* value = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load16S), origin(), pointer, offset);
1065         return m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, SExt32, origin(), value);
1066     }
1067 
1068     case LoadOpType::I32Load16U: {
1069         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load16Z), origin(), pointer, offset);
1070     }
1071 
1072     case LoadOpType::I64Load16U: {
1073         Value* value = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load16Z), origin(), pointer, offset);
1074         return m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, ZExt32, origin(), value);
1075     }
1076 
1077     case LoadOpType::I32Load: {
1078         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load), Int32, origin(), pointer, offset);
1079     }
1080 
1081     case LoadOpType::I64Load32U: {
1082         Value* value = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load), Int32, origin(), pointer, offset);
1083         return m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, ZExt32, origin(), value);
1084     }
1085 
1086     case LoadOpType::I64Load32S: {
1087         Value* value = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load), Int32, origin(), pointer, offset);
1088         return m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, SExt32, origin(), value);
1089     }
1090 
1091     case LoadOpType::I64Load: {
1092         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load), Int64, origin(), pointer, offset);
1093     }
1094 
1095     case LoadOpType::F32Load: {
1096         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load), Float, origin(), pointer, offset);
1097     }
1098 
1099     case LoadOpType::F64Load: {
1100         return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Load), Double, origin(), pointer, offset);
1101     }
1102     }
1103     RELEASE_ASSERT_NOT_REACHED();
1104 }
1105 
1106 auto B3IRGenerator::load(LoadOpType op, ExpressionType pointer, ExpressionType&amp; result, uint32_t offset) -&gt; PartialResult
1107 {
1108     ASSERT(pointer-&gt;type() == Int32);
1109 
1110     if (UNLIKELY(sumOverflows&lt;uint32_t&gt;(offset, sizeOfLoadOp(op)))) {
1111         // FIXME: Even though this is provably out of bounds, it&#39;s not a validation error, so we have to handle it
1112         // as a runtime exception. However, this may change: https://bugs.webkit.org/show_bug.cgi?id=166435
1113         B3::PatchpointValue* throwException = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, B3::Void, origin());
1114         throwException-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1115             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsMemoryAccess);
1116         });
1117 
1118         switch (op) {
1119         case LoadOpType::I32Load8S:
1120         case LoadOpType::I32Load16S:
1121         case LoadOpType::I32Load:
1122         case LoadOpType::I32Load16U:
1123         case LoadOpType::I32Load8U:
1124             result = constant(Int32, 0);
1125             break;
1126         case LoadOpType::I64Load8S:
1127         case LoadOpType::I64Load8U:
1128         case LoadOpType::I64Load16S:
1129         case LoadOpType::I64Load32U:
1130         case LoadOpType::I64Load32S:
1131         case LoadOpType::I64Load:
1132         case LoadOpType::I64Load16U:
1133             result = constant(Int64, 0);
1134             break;
1135         case LoadOpType::F32Load:
1136             result = constant(Float, 0);
1137             break;
1138         case LoadOpType::F64Load:
1139             result = constant(Double, 0);
1140             break;
1141         }
1142 
1143     } else
1144         result = emitLoadOp(op, emitCheckAndPreparePointer(pointer, offset, sizeOfLoadOp(op)), offset);
1145 
1146     return { };
1147 }
1148 
1149 inline uint32_t sizeOfStoreOp(StoreOpType op)
1150 {
1151     switch (op) {
1152     case StoreOpType::I32Store8:
1153     case StoreOpType::I64Store8:
1154         return 1;
1155     case StoreOpType::I32Store16:
1156     case StoreOpType::I64Store16:
1157         return 2;
1158     case StoreOpType::I32Store:
1159     case StoreOpType::I64Store32:
1160     case StoreOpType::F32Store:
1161         return 4;
1162     case StoreOpType::I64Store:
1163     case StoreOpType::F64Store:
1164         return 8;
1165     }
1166     RELEASE_ASSERT_NOT_REACHED();
1167 }
1168 
1169 
1170 inline void B3IRGenerator::emitStoreOp(StoreOpType op, ExpressionType pointer, ExpressionType value, uint32_t uoffset)
1171 {
1172     int32_t offset = fixupPointerPlusOffset(pointer, uoffset);
1173 
1174     switch (op) {
1175     case StoreOpType::I64Store8:
1176         value = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Trunc, origin(), value);
1177         FALLTHROUGH;
1178 
1179     case StoreOpType::I32Store8:
1180         m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Store8), origin(), value, pointer, offset);
1181         return;
1182 
1183     case StoreOpType::I64Store16:
1184         value = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Trunc, origin(), value);
1185         FALLTHROUGH;
1186 
1187     case StoreOpType::I32Store16:
1188         m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Store16), origin(), value, pointer, offset);
1189         return;
1190 
1191     case StoreOpType::I64Store32:
1192         value = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Trunc, origin(), value);
1193         FALLTHROUGH;
1194 
1195     case StoreOpType::I64Store:
1196     case StoreOpType::I32Store:
1197     case StoreOpType::F32Store:
1198     case StoreOpType::F64Store:
1199         m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, memoryKind(Store), origin(), value, pointer, offset);
1200         return;
1201     }
1202     RELEASE_ASSERT_NOT_REACHED();
1203 }
1204 
1205 auto B3IRGenerator::store(StoreOpType op, ExpressionType pointer, ExpressionType value, uint32_t offset) -&gt; PartialResult
1206 {
1207     ASSERT(pointer-&gt;type() == Int32);
1208 
1209     if (UNLIKELY(sumOverflows&lt;uint32_t&gt;(offset, sizeOfStoreOp(op)))) {
1210         // FIXME: Even though this is provably out of bounds, it&#39;s not a validation error, so we have to handle it
1211         // as a runtime exception. However, this may change: https://bugs.webkit.org/show_bug.cgi?id=166435
1212         B3::PatchpointValue* throwException = m_currentBlock-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, B3::Void, origin());
1213         throwException-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1214             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsMemoryAccess);
1215         });
1216     } else
1217         emitStoreOp(op, emitCheckAndPreparePointer(pointer, offset, sizeOfStoreOp(op)), value, offset);
1218 
1219     return { };
1220 }
1221 
1222 auto B3IRGenerator::addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType&amp; result) -&gt; PartialResult
1223 {
1224     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, B3::Select, origin(), condition, nonZero, zero);
1225     return { };
1226 }
1227 
1228 B3IRGenerator::ExpressionType B3IRGenerator::addConstant(Type type, uint64_t value)
1229 {
1230 
1231     return constant(toB3Type(type), value);
1232 }
1233 
1234 void B3IRGenerator::emitEntryTierUpCheck()
1235 {
1236     if (!m_tierUp)
1237         return;
1238 
1239     ASSERT(m_tierUp);
1240     Value* countDownLocation = constant(pointerType(), reinterpret_cast&lt;uint64_t&gt;(&amp;m_tierUp-&gt;m_counter), Origin());
1241 
1242     PatchpointValue* patch = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, B3::Void, Origin());
1243     Effects effects = Effects::none();
1244     // FIXME: we should have a more precise heap range for the tier up count.
1245     effects.reads = B3::HeapRange::top();
1246     effects.writes = B3::HeapRange::top();
1247     patch-&gt;effects = effects;
1248     patch-&gt;clobber(RegisterSet::macroScratchRegisters());
1249 
1250     patch-&gt;append(countDownLocation, ValueRep::SomeRegister);
1251     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
1252         AllowMacroScratchRegisterUsage allowScratch(jit);
1253         CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(TierUpCount::functionEntryIncrement()), CCallHelpers::Address(params[0].gpr()));
1254         CCallHelpers::Label tierUpResume = jit.label();
1255 
1256         params.addLatePath([=] (CCallHelpers&amp; jit) {
1257             tierUp.link(&amp;jit);
1258 
1259             const unsigned extraPaddingBytes = 0;
1260             RegisterSet registersToSpill = { };
1261             registersToSpill.add(GPRInfo::argumentGPR1);
1262             unsigned numberOfStackBytesUsedForRegisterPreservation = ScratchRegisterAllocator::preserveRegistersToStackForCall(jit, registersToSpill, extraPaddingBytes);
1263 
1264             jit.move(MacroAssembler::TrustedImm32(m_functionIndex), GPRInfo::argumentGPR1);
1265             MacroAssembler::Call call = jit.nearCall();
1266 
1267             ScratchRegisterAllocator::restoreRegistersFromStackForCall(jit, registersToSpill, RegisterSet(), numberOfStackBytesUsedForRegisterPreservation, extraPaddingBytes);
1268             jit.jump(tierUpResume);
1269 
1270             jit.addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
1271                 MacroAssembler::repatchNearCall(linkBuffer.locationOfNearCall&lt;NoPtrTag&gt;(call), CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(triggerOMGEntryTierUpThunkGenerator).code()));
1272             });
1273         });
1274     });
1275 }
1276 
1277 void B3IRGenerator::emitLoopTierUpCheck(uint32_t loopIndex, const Stack&amp; enclosingStack)
1278 {
1279     uint32_t outerLoopIndex = this-&gt;outerLoopIndex();
1280     m_outerLoops.append(loopIndex);
1281 
1282     if (!m_tierUp)
1283         return;
1284 
1285     Origin origin = this-&gt;origin();
1286     ASSERT(m_tierUp-&gt;osrEntryTriggers().size() == loopIndex);
1287     m_tierUp-&gt;osrEntryTriggers().append(TierUpCount::TriggerReason::DontTrigger);
1288     m_tierUp-&gt;outerLoops().append(outerLoopIndex);
1289 
1290     Value* countDownLocation = constant(pointerType(), reinterpret_cast&lt;uint64_t&gt;(&amp;m_tierUp-&gt;m_counter), origin);
1291 
1292     Vector&lt;ExpressionType&gt; stackmap;
1293     for (auto&amp; local : m_locals) {
1294         Value* result = m_currentBlock-&gt;appendNew&lt;VariableValue&gt;(m_proc, B3::Get, origin, local);
1295         stackmap.append(result);
1296     }
1297     for (unsigned controlIndex = 0; controlIndex &lt; m_parser-&gt;controlStack().size(); ++controlIndex) {
1298         auto&amp; expressionStack = m_parser-&gt;controlStack()[controlIndex].enclosedExpressionStack;
1299         for (TypedExpression value : expressionStack)
1300             stackmap.append(value);
1301     }
1302     for (TypedExpression value : enclosingStack)
1303         stackmap.append(value);
1304 
1305     PatchpointValue* patch = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, B3::Void, origin);
1306     Effects effects = Effects::none();
1307     // FIXME: we should have a more precise heap range for the tier up count.
1308     effects.reads = B3::HeapRange::top();
1309     effects.writes = B3::HeapRange::top();
1310     effects.exitsSideways = true;
1311     patch-&gt;effects = effects;
1312 
1313     patch-&gt;clobber(RegisterSet::macroScratchRegisters());
1314     RegisterSet clobberLate;
1315     clobberLate.add(GPRInfo::argumentGPR0);
1316     patch-&gt;clobberLate(clobberLate);
1317 
1318     patch-&gt;append(countDownLocation, ValueRep::SomeRegister);
1319     patch-&gt;appendVectorWithRep(stackmap, ValueRep::ColdAny);
1320 
1321     TierUpCount::TriggerReason* forceEntryTrigger = &amp;(m_tierUp-&gt;osrEntryTriggers().last());
1322     static_assert(!static_cast&lt;uint8_t&gt;(TierUpCount::TriggerReason::DontTrigger), &quot;the JIT code assumes non-zero means &#39;enter&#39;&quot;);
1323     static_assert(sizeof(TierUpCount::TriggerReason) == 1, &quot;branchTest8 assumes this size&quot;);
1324     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
1325         AllowMacroScratchRegisterUsage allowScratch(jit);
1326         CCallHelpers::Jump forceOSREntry = jit.branchTest8(CCallHelpers::NonZero, CCallHelpers::AbsoluteAddress(forceEntryTrigger));
1327         CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(TierUpCount::loopIncrement()), CCallHelpers::Address(params[0].gpr()));
1328         MacroAssembler::Label tierUpResume = jit.label();
1329 
1330         OSREntryData&amp; osrEntryData = m_tierUp-&gt;addOSREntryData(m_functionIndex, loopIndex);
1331         // First argument is the countdown location.
1332         for (unsigned i = 1; i &lt; params.value()-&gt;numChildren(); ++i)
1333             osrEntryData.values().constructAndAppend(params[i], params.value()-&gt;child(i)-&gt;type());
1334         OSREntryData* osrEntryDataPtr = &amp;osrEntryData;
1335 
1336         params.addLatePath([=] (CCallHelpers&amp; jit) {
1337             AllowMacroScratchRegisterUsage allowScratch(jit);
1338             forceOSREntry.link(&amp;jit);
1339             tierUp.link(&amp;jit);
1340 
1341             jit.probe(operationWasmTriggerOSREntryNow, osrEntryDataPtr);
1342             jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::argumentGPR0).linkTo(tierUpResume, &amp;jit);
1343             jit.farJump(GPRInfo::argumentGPR1, WasmEntryPtrTag);
1344         });
1345     });
1346 }
1347 
1348 auto B3IRGenerator::addLoop(BlockSignature signature, Stack&amp; enclosingStack, ControlType&amp; block, Stack&amp; newStack, uint32_t loopIndex) -&gt; PartialResult
1349 {
1350     BasicBlock* body = m_proc.addBlock();
1351     BasicBlock* continuation = m_proc.addBlock();
1352 
1353     block = ControlData(m_proc, origin(), signature, BlockType::Loop, continuation, body);
1354 
1355     ExpressionList args;
1356     {
1357         unsigned offset = enclosingStack.size() - signature-&gt;argumentCount();
1358         for (unsigned i = 0; i &lt; signature-&gt;argumentCount(); ++i) {
1359             TypedExpression value = enclosingStack.at(offset + i);
1360             auto* upsilon = m_currentBlock-&gt;appendNew&lt;UpsilonValue&gt;(m_proc, origin(), value);
1361             Value* phi = block.phis[i];
1362             body-&gt;append(phi);
1363             upsilon-&gt;setPhi(phi);
1364             newStack.constructAndAppend(value.type(), phi);
1365         }
1366         enclosingStack.shrink(offset);
1367     }
1368 
1369     m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), body);
1370     if (loopIndex == m_loopIndexForOSREntry) {
1371         dataLogLnIf(WasmB3IRGeneratorInternal::verbose, &quot;Setting up for OSR entry&quot;);
1372 
1373         m_currentBlock = m_rootBlock;
1374         Value* pointer = m_rootBlock-&gt;appendNew&lt;ArgumentRegValue&gt;(m_proc, Origin(), GPRInfo::argumentGPR0);
1375 
1376         unsigned indexInBuffer = 0;
1377         auto loadFromScratchBuffer = [&amp;] (B3::Type type) {
1378             size_t offset = sizeof(uint64_t) * indexInBuffer++;
1379             RELEASE_ASSERT(type.isNumeric());
1380             return m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, type, origin(), pointer, offset);
1381         };
1382 
1383         for (auto&amp; local : m_locals)
1384             m_currentBlock-&gt;appendNew&lt;VariableValue&gt;(m_proc, Set, Origin(), local, loadFromScratchBuffer(local-&gt;type()));
1385 
1386         auto connectControlEntry = [&amp;](const ControlData&amp; data, Stack&amp; expressionStack) {
1387             // For each stack entry enclosed by this loop we need to replace the value with a phi so we can fill it on OSR entry.
1388             BasicBlock* sourceBlock = nullptr;
1389             unsigned blockIndex = 0;
1390             B3::InsertionSet insertionSet(m_proc);
1391             for (unsigned i = 0; i &lt; expressionStack.size(); i++) {
1392                 TypedExpression value = expressionStack[i];
1393                 if (value-&gt;isConstant()) {
1394                     ++indexInBuffer;
1395                     continue;
1396                 }
1397 
1398                 if (value-&gt;owner != sourceBlock) {
1399                     if (sourceBlock)
1400                         insertionSet.execute(sourceBlock);
1401                     ASSERT(insertionSet.isEmpty());
1402                     dataLogLnIf(WasmB3IRGeneratorInternal::verbose &amp;&amp; sourceBlock, &quot;Executed insertion set into: &quot;, *sourceBlock);
1403                     blockIndex = 0;
1404                     sourceBlock = value-&gt;owner;
1405                 }
1406 
1407                 while (sourceBlock-&gt;at(blockIndex++) != value)
1408                     ASSERT(blockIndex &lt; sourceBlock-&gt;size());
1409                 ASSERT(sourceBlock-&gt;at(blockIndex - 1) == value);
1410 
1411                 auto* phi = data.continuation-&gt;appendNew&lt;Value&gt;(m_proc, Phi,  value-&gt;type(), value-&gt;origin());
1412                 expressionStack[i] = TypedExpression { value.type(), phi };
1413                 m_currentBlock-&gt;appendNew&lt;UpsilonValue&gt;(m_proc, value-&gt;origin(), loadFromScratchBuffer(value-&gt;type()), phi);
1414 
1415                 auto* sourceUpsilon = m_proc.add&lt;UpsilonValue&gt;(value-&gt;origin(), value, phi);
1416                 insertionSet.insertValue(blockIndex, sourceUpsilon);
1417             }
1418             if (sourceBlock)
1419                 insertionSet.execute(sourceBlock);
1420         };
1421 
1422         for (unsigned controlIndex = 0; controlIndex &lt; m_parser-&gt;controlStack().size(); ++controlIndex) {
1423             auto&amp; data = m_parser-&gt;controlStack()[controlIndex].controlData;
1424             auto&amp; expressionStack = m_parser-&gt;controlStack()[controlIndex].enclosedExpressionStack;
1425             connectControlEntry(data, expressionStack);
1426         }
1427         connectControlEntry(block, enclosingStack);
1428 
1429         m_osrEntryScratchBufferSize = indexInBuffer;
1430         m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), body);
1431         body-&gt;addPredecessor(m_currentBlock);
1432     }
1433 
1434     m_currentBlock = body;
1435     emitLoopTierUpCheck(loopIndex, enclosingStack);
1436     return { };
1437 }
1438 
1439 B3IRGenerator::ControlData B3IRGenerator::addTopLevel(BlockSignature signature)
1440 {
1441     return ControlData(m_proc, Origin(), signature, BlockType::TopLevel, m_proc.addBlock());
1442 }
1443 
1444 auto B3IRGenerator::addBlock(BlockSignature signature, Stack&amp; enclosingStack, ControlType&amp; newBlock, Stack&amp; newStack) -&gt; PartialResult
1445 {
1446     BasicBlock* continuation = m_proc.addBlock();
1447 
1448     splitStack(signature, enclosingStack, newStack);
1449     newBlock = ControlData(m_proc, origin(), signature, BlockType::Block, continuation);
1450     return { };
1451 }
1452 
1453 auto B3IRGenerator::addIf(ExpressionType condition, BlockSignature signature, Stack&amp; enclosingStack, ControlType&amp; result, Stack&amp; newStack) -&gt; PartialResult
1454 {
1455     // FIXME: This needs to do some kind of stack passing.
1456 
1457     BasicBlock* taken = m_proc.addBlock();
1458     BasicBlock* notTaken = m_proc.addBlock();
1459     BasicBlock* continuation = m_proc.addBlock();
1460 
1461     m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, B3::Branch, origin(), condition);
1462     m_currentBlock-&gt;setSuccessors(FrequentedBlock(taken), FrequentedBlock(notTaken));
1463     taken-&gt;addPredecessor(m_currentBlock);
1464     notTaken-&gt;addPredecessor(m_currentBlock);
1465 
1466     m_currentBlock = taken;
1467     splitStack(signature, enclosingStack, newStack);
1468     result = ControlData(m_proc, origin(), signature, BlockType::If, continuation, notTaken);
1469     return { };
1470 }
1471 
1472 auto B3IRGenerator::addElse(ControlData&amp; data, const Stack&amp; currentStack) -&gt; PartialResult
1473 {
1474     unifyValuesWithBlock(currentStack, data.phis);
1475     m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), data.continuation);
1476     return addElseToUnreachable(data);
1477 }
1478 
1479 auto B3IRGenerator::addElseToUnreachable(ControlData&amp; data) -&gt; PartialResult
1480 {
1481     ASSERT(data.blockType() == BlockType::If);
1482     m_currentBlock = data.special;
1483     data.convertIfToBlock();
1484     return { };
1485 }
1486 
1487 auto B3IRGenerator::addReturn(const ControlData&amp;, const Stack&amp; returnValues) -&gt; PartialResult
1488 {
1489     CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(m_parser-&gt;signature(), CallRole::Callee);
1490 
1491     PatchpointValue* patch = m_proc.add&lt;PatchpointValue&gt;(B3::Void, origin());
1492     patch-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1493         auto calleeSaves = params.code().calleeSaveRegisterAtOffsetList();
1494 
1495         for (RegisterAtOffset calleeSave : calleeSaves)
1496             jit.load64ToReg(CCallHelpers::Address(GPRInfo::callFrameRegister, calleeSave.offset()), calleeSave.reg());
1497 
1498         jit.emitFunctionEpilogue();
1499         jit.ret();
1500     });
1501     patch-&gt;effects.terminal = true;
1502 
1503     RELEASE_ASSERT(returnValues.size() &gt;= wasmCallInfo.results.size());
1504     unsigned offset = returnValues.size() - wasmCallInfo.results.size();
1505     for (unsigned i = 0; i &lt; wasmCallInfo.results.size(); ++i) {
1506         B3::ValueRep rep = wasmCallInfo.results[i];
1507         if (rep.isStack()) {
1508             B3::Value* address = m_currentBlock-&gt;appendNew&lt;B3::Value&gt;(m_proc, B3::Add, Origin(), framePointer(), constant(pointerType(), rep.offsetFromFP()));
1509             m_currentBlock-&gt;appendNew&lt;B3::MemoryValue&gt;(m_proc, B3::Store, Origin(), returnValues[offset + i], address);
1510         } else {
1511             ASSERT(rep.isReg());
1512             patch-&gt;append(returnValues[offset + i], rep);
1513         }
1514     }
1515 
1516     m_currentBlock-&gt;append(patch);
1517     return { };
1518 }
1519 
1520 auto B3IRGenerator::addBranch(ControlData&amp; data, ExpressionType condition, const Stack&amp; returnValues) -&gt; PartialResult
1521 {
1522     unifyValuesWithBlock(returnValues, data.phis);
1523 
1524     BasicBlock* target = data.targetBlockForBranch();
1525     if (condition) {
1526         BasicBlock* continuation = m_proc.addBlock();
1527         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, B3::Branch, origin(), condition);
1528         m_currentBlock-&gt;setSuccessors(FrequentedBlock(target), FrequentedBlock(continuation));
1529         target-&gt;addPredecessor(m_currentBlock);
1530         continuation-&gt;addPredecessor(m_currentBlock);
1531         m_currentBlock = continuation;
1532     } else {
1533         m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), FrequentedBlock(target));
1534         target-&gt;addPredecessor(m_currentBlock);
1535     }
1536 
1537     return { };
1538 }
1539 
1540 auto B3IRGenerator::addSwitch(ExpressionType condition, const Vector&lt;ControlData*&gt;&amp; targets, ControlData&amp; defaultTarget, const Stack&amp; expressionStack) -&gt; PartialResult
1541 {
1542     for (size_t i = 0; i &lt; targets.size(); ++i)
1543         unifyValuesWithBlock(expressionStack, targets[i]-&gt;phis);
1544     unifyValuesWithBlock(expressionStack, defaultTarget.phis);
1545 
1546     SwitchValue* switchValue = m_currentBlock-&gt;appendNew&lt;SwitchValue&gt;(m_proc, origin(), condition);
1547     switchValue-&gt;setFallThrough(FrequentedBlock(defaultTarget.targetBlockForBranch()));
1548     for (size_t i = 0; i &lt; targets.size(); ++i)
1549         switchValue-&gt;appendCase(SwitchCase(i, FrequentedBlock(targets[i]-&gt;targetBlockForBranch())));
1550 
1551     return { };
1552 }
1553 
1554 auto B3IRGenerator::endBlock(ControlEntry&amp; entry, Stack&amp; expressionStack) -&gt; PartialResult
1555 {
1556     ControlData&amp; data = entry.controlData;
1557 
1558     ASSERT(expressionStack.size() == data.signature()-&gt;returnCount());
1559     if (data.blockType() != BlockType::Loop)
1560         unifyValuesWithBlock(expressionStack, data.phis);
1561 
1562     m_currentBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), data.continuation);
1563     data.continuation-&gt;addPredecessor(m_currentBlock);
1564 
1565     return addEndToUnreachable(entry, expressionStack);
1566 }
1567 
1568 auto B3IRGenerator::addEndToUnreachable(ControlEntry&amp; entry, const Stack&amp; expressionStack) -&gt; PartialResult
1569 {
1570     ControlData&amp; data = entry.controlData;
1571     m_currentBlock = data.continuation;
1572 
1573     if (data.blockType() == BlockType::If) {
1574         data.special-&gt;appendNewControlValue(m_proc, Jump, origin(), m_currentBlock);
1575         m_currentBlock-&gt;addPredecessor(data.special);
1576     }
1577 
1578     if (data.blockType() != BlockType::Loop) {
1579         for (unsigned i = 0; i &lt; data.signature()-&gt;returnCount(); ++i) {
1580             Value* result = data.phis[i];
1581             m_currentBlock-&gt;append(result);
1582             entry.enclosedExpressionStack.constructAndAppend(data.signature()-&gt;returnType(i), result);
1583         }
1584     } else {
1585         m_outerLoops.removeLast();
1586         for (unsigned i = 0; i &lt; data.signature()-&gt;returnCount(); ++i) {
1587             if (i &lt; expressionStack.size())
1588                 entry.enclosedExpressionStack.append(expressionStack[i]);
1589             else {
1590                 Type returnType = data.signature()-&gt;returnType(i);
1591                 entry.enclosedExpressionStack.constructAndAppend(returnType, constant(toB3Type(returnType), 0xbbadbeef));
1592             }
1593         }
1594     }
1595 
1596     // TopLevel does not have any code after this so we need to make sure we emit a return here.
1597     if (data.blockType() == BlockType::TopLevel)
1598         return addReturn(entry.controlData, entry.enclosedExpressionStack);
1599 
1600     return { };
1601 }
1602 
1603 
1604 B3::Value* B3IRGenerator::createCallPatchpoint(BasicBlock* block, Origin origin, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, const ScopedLambda&lt;void(PatchpointValue*)&gt;&amp; patchpointFunctor)
1605 {
1606     Vector&lt;B3::ConstrainedValue&gt; constrainedArguments;
1607     CallInformation wasmCallInfo = wasmCallingConvention().callInformationFor(signature);
1608     for (unsigned i = 0; i &lt; args.size(); ++i)
1609         constrainedArguments.append(B3::ConstrainedValue(args[i], wasmCallInfo.params[i]));
1610 
1611     m_proc.requestCallArgAreaSizeInBytes(WTF::roundUpToMultipleOf(stackAlignmentBytes(), wasmCallInfo.headerAndArgumentStackSizeInBytes));
1612 
1613     B3::Type returnType = toB3ResultType(&amp;signature);
1614     B3::PatchpointValue* patchpoint = block-&gt;appendNew&lt;B3::PatchpointValue&gt;(m_proc, returnType, origin);
1615     patchpoint-&gt;clobberEarly(RegisterSet::macroScratchRegisters());
1616     patchpoint-&gt;clobberLate(RegisterSet::volatileRegistersForJSCall());
1617     patchpointFunctor(patchpoint);
1618     patchpoint-&gt;appendVector(constrainedArguments);
1619 
1620     if (returnType != B3::Void)
1621         patchpoint-&gt;resultConstraints = WTFMove(wasmCallInfo.results);
1622     return patchpoint;
1623 }
1624 
1625 auto B3IRGenerator::addCall(uint32_t functionIndex, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results) -&gt; PartialResult
1626 {
1627     ASSERT(signature.argumentCount() == args.size());
1628 
1629     m_makesCalls = true;
1630     B3::Type returnType = toB3ResultType(&amp;signature);
1631 
1632     auto fillResults = [&amp;] (Value* callResult) {
1633         ASSERT(returnType == callResult-&gt;type());
1634 
1635         switch (returnType.kind()) {
1636         case B3::Void: {
1637             break;
1638         }
1639         case B3::Tuple: {
1640             const Vector&lt;B3::Type&gt;&amp; tuple = m_proc.tupleForType(returnType);
1641             ASSERT(signature.returnCount() == tuple.size());
1642             for (unsigned i = 0; i &lt; signature.returnCount(); ++i)
1643                 results.append(m_currentBlock-&gt;appendNew&lt;ExtractValue&gt;(m_proc, origin(), tuple[i], callResult, i));
1644             break;
1645         }
1646         default: {
1647             results.append(callResult);
1648             break;
1649         }
1650         }
1651     };
1652 
1653     Vector&lt;UnlinkedWasmToWasmCall&gt;* unlinkedWasmToWasmCalls = &amp;m_unlinkedWasmToWasmCalls;
1654 
1655     if (m_info.isImportedFunctionFromFunctionIndexSpace(functionIndex)) {
1656         m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast&lt;uint32_t&gt;(args.size()));
1657 
1658         // FIXME: imports can be linked here, instead of generating a patchpoint, because all import stubs are generated before B3 compilation starts. https://bugs.webkit.org/show_bug.cgi?id=166462
1659         Value* targetInstance = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfTargetInstance(functionIndex)));
1660         // The target instance is 0 unless the call is wasm-&gt;wasm.
1661         Value* isWasmCall = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, NotEqual, origin(), targetInstance, m_currentBlock-&gt;appendNew&lt;Const64Value&gt;(m_proc, origin(), 0));
1662 
1663         BasicBlock* isWasmBlock = m_proc.addBlock();
1664         BasicBlock* isEmbedderBlock = m_proc.addBlock();
1665         BasicBlock* continuation = m_proc.addBlock();
1666         m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(), isWasmCall, FrequentedBlock(isWasmBlock), FrequentedBlock(isEmbedderBlock));
1667 
1668         Value* wasmCallResult = createCallPatchpoint(isWasmBlock, origin(), signature, args,
1669             scopedLambdaRef&lt;void(PatchpointValue*)&gt;([=] (PatchpointValue* patchpoint) -&gt; void {
1670                 patchpoint-&gt;effects.writesPinned = true;
1671                 patchpoint-&gt;effects.readsPinned = true;
1672                 // We need to clobber all potential pinned registers since we might be leaving the instance.
1673                 // We pessimistically assume we could be calling to something that is bounds checking.
1674                 // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
1675                 patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
1676                 patchpoint-&gt;setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1677                     AllowMacroScratchRegisterUsage allowScratch(jit);
1678                     CCallHelpers::Call call = jit.threadSafePatchableNearCall();
1679                     jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer&amp; linkBuffer) {
1680                         unlinkedWasmToWasmCalls-&gt;append({ linkBuffer.locationOfNearCall&lt;WasmEntryPtrTag&gt;(call), functionIndex });
1681                     });
1682                 });
1683             }));
1684         UpsilonValue* wasmCallResultUpsilon = returnType == B3::Void ? nullptr : isWasmBlock-&gt;appendNew&lt;UpsilonValue&gt;(m_proc, origin(), wasmCallResult);
1685         isWasmBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), continuation);
1686 
1687         // FIXME: Let&#39;s remove this indirection by creating a PIC friendly IC
1688         // for calls out to the embedder. This shouldn&#39;t be that hard to do. We could probably
1689         // implement the IC to be over Context*.
1690         // https://bugs.webkit.org/show_bug.cgi?id=170375
1691         Value* jumpDestination = isEmbedderBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc,
1692             Load, pointerType(), origin(), instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfWasmToEmbedderStub(functionIndex)));
1693 
1694         Value* embedderCallResult = createCallPatchpoint(isEmbedderBlock, origin(), signature, args,
1695             scopedLambdaRef&lt;void(PatchpointValue*)&gt;([=] (PatchpointValue* patchpoint) -&gt; void {
1696                 patchpoint-&gt;effects.writesPinned = true;
1697                 patchpoint-&gt;effects.readsPinned = true;
1698                 patchpoint-&gt;append(jumpDestination, ValueRep::SomeRegister);
1699                 // We need to clobber all potential pinned registers since we might be leaving the instance.
1700                 // We pessimistically assume we could be calling to something that is bounds checking.
1701                 // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
1702                 patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
1703                 patchpoint-&gt;setGenerator([returnType] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1704                     AllowMacroScratchRegisterUsage allowScratch(jit);
1705                     jit.call(params[params.proc().resultCount(returnType)].gpr(), WasmEntryPtrTag);
1706                 });
1707             }));
1708         UpsilonValue* embedderCallResultUpsilon = returnType == B3::Void ? nullptr : isEmbedderBlock-&gt;appendNew&lt;UpsilonValue&gt;(m_proc, origin(), embedderCallResult);
1709         isEmbedderBlock-&gt;appendNewControlValue(m_proc, Jump, origin(), continuation);
1710 
1711         m_currentBlock = continuation;
1712 
1713         if (returnType != B3::Void) {
1714             Value* phi = continuation-&gt;appendNew&lt;Value&gt;(m_proc, Phi, returnType, origin());
1715             wasmCallResultUpsilon-&gt;setPhi(phi);
1716             embedderCallResultUpsilon-&gt;setPhi(phi);
1717             fillResults(phi);
1718         }
1719 
1720         // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
1721         restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, instanceValue(), m_proc, continuation);
1722     } else {
1723 
1724         Value* patch = createCallPatchpoint(m_currentBlock, origin(), signature, args,
1725             scopedLambdaRef&lt;void(PatchpointValue*)&gt;([=] (PatchpointValue* patchpoint) -&gt; void {
1726                 patchpoint-&gt;effects.writesPinned = true;
1727                 patchpoint-&gt;effects.readsPinned = true;
1728 
1729                 // We need to clobber the size register since the LLInt always bounds checks
1730                 if (m_mode == MemoryMode::Signaling)
1731                     patchpoint-&gt;clobberLate(RegisterSet { PinnedRegisterInfo::get().sizeRegister });
1732                 patchpoint-&gt;setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1733                     AllowMacroScratchRegisterUsage allowScratch(jit);
1734                     CCallHelpers::Call call = jit.threadSafePatchableNearCall();
1735                     jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer&amp; linkBuffer) {
1736                         unlinkedWasmToWasmCalls-&gt;append({ linkBuffer.locationOfNearCall&lt;WasmEntryPtrTag&gt;(call), functionIndex });
1737                     });
1738                 });
1739             }));
1740         fillResults(patch);
1741     }
1742 
1743     return { };
1744 }
1745 
1746 auto B3IRGenerator::addCallIndirect(unsigned tableIndex, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, ResultList&amp; results) -&gt; PartialResult
1747 {
1748     ExpressionType calleeIndex = args.takeLast();
1749     ASSERT(signature.argumentCount() == args.size());
1750 
1751     m_makesCalls = true;
1752     // Note: call indirect can call either WebAssemblyFunction or WebAssemblyWrapperFunction. Because
1753     // WebAssemblyWrapperFunction is like calling into the embedder, we conservatively assume all call indirects
1754     // can be to the embedder for our stack check calculation.
1755     m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast&lt;uint32_t&gt;(args.size()));
1756 
1757     ExpressionType callableFunctionBuffer;
1758     ExpressionType instancesBuffer;
1759     ExpressionType callableFunctionBufferLength;
1760     {
1761         ExpressionType table = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(),
1762             instanceValue(), safeCast&lt;int32_t&gt;(Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex)));
1763         callableFunctionBuffer = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(),
1764             table, safeCast&lt;int32_t&gt;(FuncRefTable::offsetOfFunctions()));
1765         instancesBuffer = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(),
1766             table, safeCast&lt;int32_t&gt;(FuncRefTable::offsetOfInstances()));
1767         callableFunctionBufferLength = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, Int32, origin(),
1768             table, safeCast&lt;int32_t&gt;(Table::offsetOfLength()));
1769     }
1770 
1771     // Check the index we are looking for is valid.
1772     {
1773         CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
1774             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, AboveEqual, origin(), calleeIndex, callableFunctionBufferLength));
1775 
1776         check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1777             this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsCallIndirect);
1778         });
1779     }
1780 
1781     calleeIndex = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, ZExt32, origin(), calleeIndex);
1782 
1783     ExpressionType callableFunction;
1784     {
1785         // Compute the offset in the table index space we are looking for.
1786         ExpressionType offset = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Mul, origin(),
1787             calleeIndex, constant(pointerType(), sizeof(WasmToWasmImportableFunction)));
1788         callableFunction = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Add, origin(), callableFunctionBuffer, offset);
1789 
1790         // Check that the WasmToWasmImportableFunction is initialized. We trap if it isn&#39;t. An &quot;invalid&quot; SignatureIndex indicates it&#39;s not initialized.
1791         // FIXME: when we have trap handlers, we can just let the call fail because Signature::invalidIndex is 0. https://bugs.webkit.org/show_bug.cgi?id=177210
1792         static_assert(sizeof(WasmToWasmImportableFunction::signatureIndex) == sizeof(uint64_t), &quot;Load codegen assumes i64&quot;);
1793         ExpressionType calleeSignatureIndex = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, Int64, origin(), callableFunction, safeCast&lt;int32_t&gt;(WasmToWasmImportableFunction::offsetOfSignatureIndex()));
1794         {
1795             CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
1796                 m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(),
1797                     calleeSignatureIndex,
1798                     m_currentBlock-&gt;appendNew&lt;Const64Value&gt;(m_proc, origin(), Signature::invalidIndex)));
1799 
1800             check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1801                 this-&gt;emitExceptionCheck(jit, ExceptionType::NullTableEntry);
1802             });
1803         }
1804 
1805         // Check the signature matches the value we expect.
1806         {
1807             ExpressionType expectedSignatureIndex = m_currentBlock-&gt;appendNew&lt;Const64Value&gt;(m_proc, origin(), SignatureInformation::get(signature));
1808             CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
1809                 m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, NotEqual, origin(), calleeSignatureIndex, expectedSignatureIndex));
1810 
1811             check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1812                 this-&gt;emitExceptionCheck(jit, ExceptionType::BadSignature);
1813             });
1814         }
1815     }
1816 
1817     // Do a context switch if needed.
1818     {
1819         Value* offset = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Mul, origin(),
1820             calleeIndex, constant(pointerType(), sizeof(Instance*)));
1821         Value* newContextInstance = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(),
1822             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Add, origin(), instancesBuffer, offset));
1823 
1824         BasicBlock* continuation = m_proc.addBlock();
1825         BasicBlock* doContextSwitch = m_proc.addBlock();
1826 
1827         Value* isSameContextInstance = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(),
1828             newContextInstance, instanceValue());
1829         m_currentBlock-&gt;appendNewControlValue(m_proc, B3::Branch, origin(),
1830             isSameContextInstance, FrequentedBlock(continuation), FrequentedBlock(doContextSwitch));
1831 
1832         PatchpointValue* patchpoint = doContextSwitch-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, B3::Void, origin());
1833         patchpoint-&gt;effects.writesPinned = true;
1834         // We pessimistically assume we&#39;re calling something with BoundsChecking memory.
1835         // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
1836         patchpoint-&gt;clobber(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
1837         patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
1838         patchpoint-&gt;append(newContextInstance, ValueRep::SomeRegister);
1839         patchpoint-&gt;append(instanceValue(), ValueRep::SomeRegister);
1840         patchpoint-&gt;numGPScratchRegisters = Gigacage::isEnabled(Gigacage::Primitive) ? 1 : 0;
1841 
1842         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1843             AllowMacroScratchRegisterUsage allowScratch(jit);
1844             GPRReg newContextInstance = params[0].gpr();
1845             GPRReg oldContextInstance = params[1].gpr();
1846             const PinnedRegisterInfo&amp; pinnedRegs = PinnedRegisterInfo::get();
1847             GPRReg baseMemory = pinnedRegs.baseMemoryPointer;
1848             ASSERT(newContextInstance != baseMemory);
1849             jit.loadPtr(CCallHelpers::Address(oldContextInstance, Instance::offsetOfCachedStackLimit()), baseMemory);
1850             jit.storePtr(baseMemory, CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedStackLimit()));
1851             jit.storeWasmContextInstance(newContextInstance);
1852             ASSERT(pinnedRegs.sizeRegister != baseMemory);
1853             // FIXME: We should support more than one memory size register
1854             //   see: https://bugs.webkit.org/show_bug.cgi?id=162952
1855             ASSERT(pinnedRegs.sizeRegister != newContextInstance);
1856             GPRReg scratchOrSize = Gigacage::isEnabled(Gigacage::Primitive) ? params.gpScratch(0) : pinnedRegs.sizeRegister;
1857 
1858             jit.loadPtr(CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedMemorySize()), pinnedRegs.sizeRegister); // Memory size.
1859             jit.loadPtr(CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedMemory()), baseMemory); // Memory::void*.
1860 
1861             jit.cageConditionally(Gigacage::Primitive, baseMemory, pinnedRegs.sizeRegister, scratchOrSize);
1862         });
1863         doContextSwitch-&gt;appendNewControlValue(m_proc, Jump, origin(), continuation);
1864 
1865         m_currentBlock = continuation;
1866     }
1867 
1868     ExpressionType calleeCode = m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(),
1869         m_currentBlock-&gt;appendNew&lt;MemoryValue&gt;(m_proc, Load, pointerType(), origin(), callableFunction,
1870             safeCast&lt;int32_t&gt;(WasmToWasmImportableFunction::offsetOfEntrypointLoadLocation())));
1871 
1872     B3::Type returnType = toB3ResultType(&amp;signature);
1873     ExpressionType callResult = createCallPatchpoint(m_currentBlock, origin(), signature, args,
1874         scopedLambdaRef&lt;void(PatchpointValue*)&gt;([=] (PatchpointValue* patchpoint) -&gt; void {
1875             patchpoint-&gt;effects.writesPinned = true;
1876             patchpoint-&gt;effects.readsPinned = true;
1877             // We need to clobber all potential pinned registers since we might be leaving the instance.
1878             // We pessimistically assume we&#39;re always calling something that is bounds checking so
1879             // because the wasm-&gt;wasm thunk unconditionally overrides the size registers.
1880             // FIXME: We should not have to do this, but the wasm-&gt;wasm stub assumes it can
1881             // use all the pinned registers as scratch: https://bugs.webkit.org/show_bug.cgi?id=172181
1882             patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
1883 
1884             patchpoint-&gt;append(calleeCode, ValueRep::SomeRegister);
1885             patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1886                 AllowMacroScratchRegisterUsage allowScratch(jit);
1887                 jit.call(params[params.proc().resultCount(returnType)].gpr(), WasmEntryPtrTag);
1888             });
1889         }));
1890 
1891     switch (returnType.kind()) {
1892     case B3::Void: {
1893         break;
1894     }
1895     case B3::Tuple: {
1896         const Vector&lt;B3::Type&gt;&amp; tuple = m_proc.tupleForType(returnType);
1897         for (unsigned i = 0; i &lt; signature.returnCount(); ++i)
1898             results.append(m_currentBlock-&gt;appendNew&lt;ExtractValue&gt;(m_proc, origin(), tuple[i], callResult, i));
1899         break;
1900     }
1901     default: {
1902         results.append(callResult);
1903         break;
1904     }
1905     }
1906 
1907     // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
1908     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, instanceValue(), m_proc, m_currentBlock);
1909 
1910     return { };
1911 }
1912 
1913 void B3IRGenerator::unify(const ExpressionType phi, const ExpressionType source)
1914 {
1915     m_currentBlock-&gt;appendNew&lt;UpsilonValue&gt;(m_proc, origin(), source, phi);
1916 }
1917 
1918 void B3IRGenerator::unifyValuesWithBlock(const Stack&amp; resultStack, const ResultList&amp; result)
1919 {
1920     ASSERT(result.size() &lt;= resultStack.size());
1921 
1922     for (size_t i = 0; i &lt; result.size(); ++i)
1923         unify(result[result.size() - 1 - i], resultStack.at(resultStack.size() - 1 - i));
1924 }
1925 
1926 static void dumpExpressionStack(const CommaPrinter&amp; comma, const B3IRGenerator::Stack&amp; expressionStack)
1927 {
1928     dataLog(comma, &quot;ExpressionStack:&quot;);
1929     for (const auto&amp; expression : expressionStack)
1930         dataLog(comma, *expression);
1931 }
1932 
1933 void B3IRGenerator::dump(const ControlStack&amp; controlStack, const Stack* expressionStack)
1934 {
1935     dataLogLn(&quot;Constants:&quot;);
1936     for (const auto&amp; constant : m_constantPool)
1937         dataLogLn(deepDump(m_proc, constant.value));
1938 
1939     dataLogLn(&quot;Processing Graph:&quot;);
1940     dataLog(m_proc);
1941     dataLogLn(&quot;With current block:&quot;, *m_currentBlock);
1942     dataLogLn(&quot;Control stack:&quot;);
1943     ASSERT(controlStack.size());
1944     for (size_t i = controlStack.size(); i--;) {
1945         dataLog(&quot;  &quot;, controlStack[i].controlData, &quot;: &quot;);
1946         CommaPrinter comma(&quot;, &quot;, &quot;&quot;);
1947         dumpExpressionStack(comma, *expressionStack);
1948         expressionStack = &amp;controlStack[i].enclosedExpressionStack;
1949         dataLogLn();
1950     }
1951     dataLogLn();
1952 }
1953 
1954 auto B3IRGenerator::origin() -&gt; Origin
1955 {
1956     OpcodeOrigin origin(m_parser-&gt;currentOpcode(), m_parser-&gt;currentOpcodeStartingOffset());
1957     ASSERT(isValidOpType(static_cast&lt;uint8_t&gt;(origin.opcode())));
1958     return bitwise_cast&lt;Origin&gt;(origin);
1959 }
1960 
1961 Expected&lt;std::unique_ptr&lt;InternalFunction&gt;, String&gt; parseAndCompile(CompilationContext&amp; compilationContext, const FunctionData&amp; function, const Signature&amp; signature, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; unlinkedWasmToWasmCalls, unsigned&amp; osrEntryScratchBufferSize, const ModuleInformation&amp; info, MemoryMode mode, CompilationMode compilationMode, uint32_t functionIndex, uint32_t loopIndexForOSREntry, TierUpCount* tierUp)
1962 {
1963     auto result = makeUnique&lt;InternalFunction&gt;();
1964 
1965     compilationContext.embedderEntrypointJIT = makeUnique&lt;CCallHelpers&gt;();
1966     compilationContext.wasmEntrypointJIT = makeUnique&lt;CCallHelpers&gt;();
1967 
1968     Procedure procedure;
1969 
1970     procedure.setOriginPrinter([] (PrintStream&amp; out, Origin origin) {
1971         if (origin.data())
1972             out.print(&quot;Wasm: &quot;, bitwise_cast&lt;OpcodeOrigin&gt;(origin));
1973     });
1974 
1975     // This means we cannot use either StackmapGenerationParams::usedRegisters() or
1976     // StackmapGenerationParams::unavailableRegisters(). In exchange for this concession, we
1977     // don&#39;t strictly need to run Air::reportUsedRegisters(), which saves a bit of CPU time at
1978     // optLevel=1.
1979     procedure.setNeedsUsedRegisters(false);
1980 
1981     procedure.setOptLevel(compilationMode == CompilationMode::BBQMode
1982         ? Options::webAssemblyBBQB3OptimizationLevel()
1983         : Options::webAssemblyOMGOptimizationLevel());
1984 
1985     B3IRGenerator irGenerator(info, procedure, result.get(), unlinkedWasmToWasmCalls, osrEntryScratchBufferSize, mode, compilationMode, functionIndex, loopIndexForOSREntry, tierUp);
1986     FunctionParser&lt;B3IRGenerator&gt; parser(irGenerator, function.data.data(), function.data.size(), signature, info);
1987     WASM_FAIL_IF_HELPER_FAILS(parser.parse());
1988 
1989     irGenerator.insertConstants();
1990 
1991     procedure.resetReachability();
1992     if (ASSERT_ENABLED)
1993         validate(procedure, &quot;After parsing:\n&quot;);
1994 
1995     dataLogIf(WasmB3IRGeneratorInternal::verbose, &quot;Pre SSA: &quot;, procedure);
1996     fixSSA(procedure);
1997     dataLogIf(WasmB3IRGeneratorInternal::verbose, &quot;Post SSA: &quot;, procedure);
1998 
1999     {
2000         B3::prepareForGeneration(procedure);
2001         B3::generate(procedure, *compilationContext.wasmEntrypointJIT);
2002         compilationContext.wasmEntrypointByproducts = procedure.releaseByproducts();
2003         result-&gt;entrypoint.calleeSaveRegisters = procedure.calleeSaveRegisterAtOffsetList();
2004     }
2005 
2006     return result;
2007 }
2008 
2009 // Custom wasm ops. These are the ones too messy to do in wasm.json.
2010 
2011 void B3IRGenerator::emitChecksForModOrDiv(B3::Opcode operation, ExpressionType left, ExpressionType right)
2012 {
2013     ASSERT(operation == Div || operation == Mod || operation == UDiv || operation == UMod);
2014     const B3::Type type = left-&gt;type();
2015 
2016     {
2017         CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
2018             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), right, constant(type, 0)));
2019 
2020         check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2021             this-&gt;emitExceptionCheck(jit, ExceptionType::DivisionByZero);
2022         });
2023     }
2024 
2025     if (operation == Div) {
2026         int64_t min = type == Int32 ? std::numeric_limits&lt;int32_t&gt;::min() : std::numeric_limits&lt;int64_t&gt;::min();
2027 
2028         CheckValue* check = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(),
2029             m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2030                 m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), left, constant(type, min)),
2031                 m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), right, constant(type, -1))));
2032 
2033         check-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2034             this-&gt;emitExceptionCheck(jit, ExceptionType::IntegerOverflow);
2035         });
2036     }
2037 }
2038 
2039 template&lt;&gt;
2040 auto B3IRGenerator::addOp&lt;OpType::I32DivS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2041 {
2042     const B3::Opcode op = Div;
2043     emitChecksForModOrDiv(op, left, right);
2044     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, op, origin(), left, right);
2045     return { };
2046 }
2047 
2048 template&lt;&gt;
2049 auto B3IRGenerator::addOp&lt;OpType::I32RemS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2050 {
2051     const B3::Opcode op = Mod;
2052     emitChecksForModOrDiv(op, left, right);
2053     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, chill(op), origin(), left, right);
2054     return { };
2055 }
2056 
2057 template&lt;&gt;
2058 auto B3IRGenerator::addOp&lt;OpType::I32DivU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2059 {
2060     const B3::Opcode op = UDiv;
2061     emitChecksForModOrDiv(op, left, right);
2062     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, op, origin(), left, right);
2063     return { };
2064 }
2065 
2066 template&lt;&gt;
2067 auto B3IRGenerator::addOp&lt;OpType::I32RemU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2068 {
2069     const B3::Opcode op = UMod;
2070     emitChecksForModOrDiv(op, left, right);
2071     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, op, origin(), left, right);
2072     return { };
2073 }
2074 
2075 template&lt;&gt;
2076 auto B3IRGenerator::addOp&lt;OpType::I64DivS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2077 {
2078     const B3::Opcode op = Div;
2079     emitChecksForModOrDiv(op, left, right);
2080     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, op, origin(), left, right);
2081     return { };
2082 }
2083 
2084 template&lt;&gt;
2085 auto B3IRGenerator::addOp&lt;OpType::I64RemS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2086 {
2087     const B3::Opcode op = Mod;
2088     emitChecksForModOrDiv(op, left, right);
2089     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, chill(op), origin(), left, right);
2090     return { };
2091 }
2092 
2093 template&lt;&gt;
2094 auto B3IRGenerator::addOp&lt;OpType::I64DivU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2095 {
2096     const B3::Opcode op = UDiv;
2097     emitChecksForModOrDiv(op, left, right);
2098     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, op, origin(), left, right);
2099     return { };
2100 }
2101 
2102 template&lt;&gt;
2103 auto B3IRGenerator::addOp&lt;OpType::I64RemU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2104 {
2105     const B3::Opcode op = UMod;
2106     emitChecksForModOrDiv(op, left, right);
2107     result = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, op, origin(), left, right);
2108     return { };
2109 }
2110 
2111 template&lt;&gt;
2112 auto B3IRGenerator::addOp&lt;OpType::I32Ctz&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2113 {
2114     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int32, origin());
2115     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2116     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2117         jit.countTrailingZeros32(params[1].gpr(), params[0].gpr());
2118     });
2119     patchpoint-&gt;effects = Effects::none();
2120     result = patchpoint;
2121     return { };
2122 }
2123 
2124 template&lt;&gt;
2125 auto B3IRGenerator::addOp&lt;OpType::I64Ctz&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2126 {
2127     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int64, origin());
2128     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2129     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2130         jit.countTrailingZeros64(params[1].gpr(), params[0].gpr());
2131     });
2132     patchpoint-&gt;effects = Effects::none();
2133     result = patchpoint;
2134     return { };
2135 }
2136 
2137 template&lt;&gt;
2138 auto B3IRGenerator::addOp&lt;OpType::I32Popcnt&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2139 {
2140 #if CPU(X86_64)
2141     if (MacroAssembler::supportsCountPopulation()) {
2142         PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int32, origin());
2143         patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2144         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2145             jit.countPopulation32(params[1].gpr(), params[0].gpr());
2146         });
2147         patchpoint-&gt;effects = Effects::none();
2148         result = patchpoint;
2149         return { };
2150     }
2151 #endif
2152 
2153     Value* funcAddress = m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(&amp;operationPopcount32, B3CCallPtrTag));
2154     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, Int32, origin(), Effects::none(), funcAddress, arg);
2155     return { };
2156 }
2157 
2158 template&lt;&gt;
2159 auto B3IRGenerator::addOp&lt;OpType::I64Popcnt&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2160 {
2161 #if CPU(X86_64)
2162     if (MacroAssembler::supportsCountPopulation()) {
2163         PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int64, origin());
2164         patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2165         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2166             jit.countPopulation64(params[1].gpr(), params[0].gpr());
2167         });
2168         patchpoint-&gt;effects = Effects::none();
2169         result = patchpoint;
2170         return { };
2171     }
2172 #endif
2173 
2174     Value* funcAddress = m_currentBlock-&gt;appendNew&lt;ConstPtrValue&gt;(m_proc, origin(), tagCFunctionPtr&lt;void*&gt;(operationPopcount64, B3CCallPtrTag));
2175     result = m_currentBlock-&gt;appendNew&lt;CCallValue&gt;(m_proc, Int64, origin(), Effects::none(), funcAddress, arg);
2176     return { };
2177 }
2178 
2179 template&lt;&gt;
2180 auto B3IRGenerator::addOp&lt;F64ConvertUI64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2181 {
2182     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Double, origin());
2183     if (isX86())
2184         patchpoint-&gt;numGPScratchRegisters = 1;
2185     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2186     patchpoint-&gt;append(ConstrainedValue(arg, ValueRep::SomeRegister));
2187     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2188         AllowMacroScratchRegisterUsage allowScratch(jit);
2189 #if CPU(X86_64)
2190         jit.convertUInt64ToDouble(params[1].gpr(), params[0].fpr(), params.gpScratch(0));
2191 #else
2192         jit.convertUInt64ToDouble(params[1].gpr(), params[0].fpr());
2193 #endif
2194     });
2195     patchpoint-&gt;effects = Effects::none();
2196     result = patchpoint;
2197     return { };
2198 }
2199 
2200 template&lt;&gt;
2201 auto B3IRGenerator::addOp&lt;OpType::F32ConvertUI64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2202 {
2203     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Float, origin());
2204     if (isX86())
2205         patchpoint-&gt;numGPScratchRegisters = 1;
2206     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2207     patchpoint-&gt;append(ConstrainedValue(arg, ValueRep::SomeRegister));
2208     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2209         AllowMacroScratchRegisterUsage allowScratch(jit);
2210 #if CPU(X86_64)
2211         jit.convertUInt64ToFloat(params[1].gpr(), params[0].fpr(), params.gpScratch(0));
2212 #else
2213         jit.convertUInt64ToFloat(params[1].gpr(), params[0].fpr());
2214 #endif
2215     });
2216     patchpoint-&gt;effects = Effects::none();
2217     result = patchpoint;
2218     return { };
2219 }
2220 
2221 template&lt;&gt;
2222 auto B3IRGenerator::addOp&lt;OpType::F64Nearest&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2223 {
2224     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Double, origin());
2225     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2226     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2227         jit.roundTowardNearestIntDouble(params[1].fpr(), params[0].fpr());
2228     });
2229     patchpoint-&gt;effects = Effects::none();
2230     result = patchpoint;
2231     return { };
2232 }
2233 
2234 template&lt;&gt;
2235 auto B3IRGenerator::addOp&lt;OpType::F32Nearest&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2236 {
2237     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Float, origin());
2238     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2239     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2240         jit.roundTowardNearestIntFloat(params[1].fpr(), params[0].fpr());
2241     });
2242     patchpoint-&gt;effects = Effects::none();
2243     result = patchpoint;
2244     return { };
2245 }
2246 
2247 template&lt;&gt;
2248 auto B3IRGenerator::addOp&lt;OpType::F64Trunc&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2249 {
2250     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Double, origin());
2251     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2252     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2253         jit.roundTowardZeroDouble(params[1].fpr(), params[0].fpr());
2254     });
2255     patchpoint-&gt;effects = Effects::none();
2256     result = patchpoint;
2257     return { };
2258 }
2259 
2260 template&lt;&gt;
2261 auto B3IRGenerator::addOp&lt;OpType::F32Trunc&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2262 {
2263     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Float, origin());
2264     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2265     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2266         jit.roundTowardZeroFloat(params[1].fpr(), params[0].fpr());
2267     });
2268     patchpoint-&gt;effects = Effects::none();
2269     result = patchpoint;
2270     return { };
2271 }
2272 
2273 template&lt;&gt;
2274 auto B3IRGenerator::addOp&lt;OpType::I32TruncSF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2275 {
2276     Value* max = constant(Double, bitwise_cast&lt;uint64_t&gt;(-static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2277     Value* min = constant(Double, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2278     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2279         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2280         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterEqual, origin(), arg, min));
2281     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2282     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2283     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2284         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2285     });
2286     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int32, origin());
2287     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2288     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2289         jit.truncateDoubleToInt32(params[1].fpr(), params[0].gpr());
2290     });
2291     patchpoint-&gt;effects = Effects::none();
2292     result = patchpoint;
2293     return { };
2294 }
2295 
2296 template&lt;&gt;
2297 auto B3IRGenerator::addOp&lt;OpType::I32TruncSF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2298 {
2299     Value* max = constant(Float, bitwise_cast&lt;uint32_t&gt;(-static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2300     Value* min = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2301     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2302         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2303         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterEqual, origin(), arg, min));
2304     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2305     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2306     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2307         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2308     });
2309     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int32, origin());
2310     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2311     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2312         jit.truncateFloatToInt32(params[1].fpr(), params[0].gpr());
2313     });
2314     patchpoint-&gt;effects = Effects::none();
2315     result = patchpoint;
2316     return { };
2317 }
2318 
2319 
2320 template&lt;&gt;
2321 auto B3IRGenerator::addOp&lt;OpType::I32TruncUF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2322 {
2323     Value* max = constant(Double, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min()) * -2.0));
2324     Value* min = constant(Double, bitwise_cast&lt;uint64_t&gt;(-1.0));
2325     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2326         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2327         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterThan, origin(), arg, min));
2328     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2329     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2330     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2331         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2332     });
2333     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int32, origin());
2334     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2335     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2336         jit.truncateDoubleToUint32(params[1].fpr(), params[0].gpr());
2337     });
2338     patchpoint-&gt;effects = Effects::none();
2339     result = patchpoint;
2340     return { };
2341 }
2342 
2343 template&lt;&gt;
2344 auto B3IRGenerator::addOp&lt;OpType::I32TruncUF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2345 {
2346     Value* max = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min()) * static_cast&lt;float&gt;(-2.0)));
2347     Value* min = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-1.0)));
2348     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2349         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2350         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterThan, origin(), arg, min));
2351     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2352     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2353     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2354         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2355     });
2356     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int32, origin());
2357     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2358     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2359         jit.truncateFloatToUint32(params[1].fpr(), params[0].gpr());
2360     });
2361     patchpoint-&gt;effects = Effects::none();
2362     result = patchpoint;
2363     return { };
2364 }
2365 
2366 template&lt;&gt;
2367 auto B3IRGenerator::addOp&lt;OpType::I64TruncSF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2368 {
2369     Value* max = constant(Double, bitwise_cast&lt;uint64_t&gt;(-static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2370     Value* min = constant(Double, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2371     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2372         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2373         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterEqual, origin(), arg, min));
2374     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2375     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2376     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2377         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2378     });
2379     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int64, origin());
2380     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2381     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2382         jit.truncateDoubleToInt64(params[1].fpr(), params[0].gpr());
2383     });
2384     patchpoint-&gt;effects = Effects::none();
2385     result = patchpoint;
2386     return { };
2387 }
2388 
2389 template&lt;&gt;
2390 auto B3IRGenerator::addOp&lt;OpType::I64TruncUF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2391 {
2392     Value* max = constant(Double, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min()) * -2.0));
2393     Value* min = constant(Double, bitwise_cast&lt;uint64_t&gt;(-1.0));
2394     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2395         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2396         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterThan, origin(), arg, min));
2397     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2398     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2399     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2400         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2401     });
2402 
2403     Value* signBitConstant;
2404     if (isX86()) {
2405         // Since x86 doesn&#39;t have an instruction to convert floating points to unsigned integers, we at least try to do the smart thing if
2406         // the numbers are would be positive anyway as a signed integer. Since we cannot materialize constants into fprs we have b3 do it
2407         // so we can pool them if needed.
2408         signBitConstant = constant(Double, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;uint64_t&gt;::max() - std::numeric_limits&lt;int64_t&gt;::max())));
2409     }
2410     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int64, origin());
2411     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2412     if (isX86()) {
2413         patchpoint-&gt;append(signBitConstant, ValueRep::SomeRegister);
2414         patchpoint-&gt;numFPScratchRegisters = 1;
2415     }
2416     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2417     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2418         AllowMacroScratchRegisterUsage allowScratch(jit);
2419         FPRReg scratch = InvalidFPRReg;
2420         FPRReg constant = InvalidFPRReg;
2421         if (isX86()) {
2422             scratch = params.fpScratch(0);
2423             constant = params[2].fpr();
2424         }
2425         jit.truncateDoubleToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
2426     });
2427     patchpoint-&gt;effects = Effects::none();
2428     result = patchpoint;
2429     return { };
2430 }
2431 
2432 template&lt;&gt;
2433 auto B3IRGenerator::addOp&lt;OpType::I64TruncSF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2434 {
2435     Value* max = constant(Float, bitwise_cast&lt;uint32_t&gt;(-static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2436     Value* min = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2437     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2438         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2439         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterEqual, origin(), arg, min));
2440     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2441     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2442     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2443         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2444     });
2445     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int64, origin());
2446     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2447     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2448         jit.truncateFloatToInt64(params[1].fpr(), params[0].gpr());
2449     });
2450     patchpoint-&gt;effects = Effects::none();
2451     result = patchpoint;
2452     return { };
2453 }
2454 
2455 template&lt;&gt;
2456 auto B3IRGenerator::addOp&lt;OpType::I64TruncUF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2457 {
2458     Value* max = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min()) * static_cast&lt;float&gt;(-2.0)));
2459     Value* min = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-1.0)));
2460     Value* outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, BitAnd, origin(),
2461         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, LessThan, origin(), arg, max),
2462         m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, GreaterThan, origin(), arg, min));
2463     outOfBounds = m_currentBlock-&gt;appendNew&lt;Value&gt;(m_proc, Equal, origin(), outOfBounds, constant(Int32, 0));
2464     CheckValue* trap = m_currentBlock-&gt;appendNew&lt;CheckValue&gt;(m_proc, Check, origin(), outOfBounds);
2465     trap-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp;) {
2466         this-&gt;emitExceptionCheck(jit, ExceptionType::OutOfBoundsTrunc);
2467     });
2468 
2469     Value* signBitConstant;
2470     if (isX86()) {
2471         // Since x86 doesn&#39;t have an instruction to convert floating points to unsigned integers, we at least try to do the smart thing if
2472         // the numbers would be positive anyway as a signed integer. Since we cannot materialize constants into fprs we have b3 do it
2473         // so we can pool them if needed.
2474         signBitConstant = constant(Float, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;uint64_t&gt;::max() - std::numeric_limits&lt;int64_t&gt;::max())));
2475     }
2476     PatchpointValue* patchpoint = m_currentBlock-&gt;appendNew&lt;PatchpointValue&gt;(m_proc, Int64, origin());
2477     patchpoint-&gt;append(arg, ValueRep::SomeRegister);
2478     if (isX86()) {
2479         patchpoint-&gt;append(signBitConstant, ValueRep::SomeRegister);
2480         patchpoint-&gt;numFPScratchRegisters = 1;
2481     }
2482     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2483     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const StackmapGenerationParams&amp; params) {
2484         AllowMacroScratchRegisterUsage allowScratch(jit);
2485         FPRReg scratch = InvalidFPRReg;
2486         FPRReg constant = InvalidFPRReg;
2487         if (isX86()) {
2488             scratch = params.fpScratch(0);
2489             constant = params[2].fpr();
2490         }
2491         jit.truncateFloatToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
2492     });
2493     patchpoint-&gt;effects = Effects::none();
2494     result = patchpoint;
2495     return { };
2496 }
2497 
2498 } } // namespace JSC::Wasm
2499 
2500 #include &quot;WasmB3IRGeneratorInlines.h&quot;
2501 
2502 #endif // ENABLE(WEBASSEMBLY)
    </pre>
  </body>
</html>