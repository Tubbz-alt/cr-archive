<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExitCompilerCommon.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGOSRExitBase.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExitCompilerCommon.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExitCompilerCommon.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGOSRExitCompilerCommon.h&quot;
 28 
 29 #if ENABLE(DFG_JIT)
 30 


 31 #include &quot;DFGJITCode.h&quot;
 32 #include &quot;DFGOperations.h&quot;
 33 #include &quot;JIT.h&quot;
 34 #include &quot;JSCJSValueInlines.h&quot;
 35 #include &quot;JSCInlines.h&quot;



 36 #include &quot;StructureStubInfo.h&quot;
 37 
 38 namespace JSC { namespace DFG {
 39 
<span class="line-modified"> 40 void handleExitCounts(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)</span>
 41 {
 42     if (!exitKindMayJettison(exit.m_kind)) {
 43         // FIXME: We may want to notice that we&#39;re frequently exiting
 44         // at an op_catch that we didn&#39;t compile an entrypoint for, and
 45         // then trigger a reoptimization of this CodeBlock:
 46         // https://bugs.webkit.org/show_bug.cgi?id=175842
 47         return;
 48     }
 49 
 50     jit.add32(AssemblyHelpers::TrustedImm32(1), AssemblyHelpers::AbsoluteAddress(&amp;exit.m_count));
 51 
 52     jit.move(AssemblyHelpers::TrustedImmPtr(jit.codeBlock()), GPRInfo::regT3);
 53 
 54     AssemblyHelpers::Jump tooFewFails;
 55 
 56     jit.load32(AssemblyHelpers::Address(GPRInfo::regT3, CodeBlock::offsetOfOSRExitCounter()), GPRInfo::regT2);
 57     jit.add32(AssemblyHelpers::TrustedImm32(1), GPRInfo::regT2);
 58     jit.store32(GPRInfo::regT2, AssemblyHelpers::Address(GPRInfo::regT3, CodeBlock::offsetOfOSRExitCounter()));
 59 
 60     jit.move(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), GPRInfo::regT0);
</pre>
<hr />
<pre>
 84     jit.move(
 85         AssemblyHelpers::TrustedImm32(jit.codeBlock()-&gt;exitCountThresholdForReoptimization()),
 86         GPRInfo::regT1);
 87 
 88     if (!loopThreshold.empty()) {
 89         AssemblyHelpers::Jump done = jit.jump();
 90 
 91         loopThreshold.link(&amp;jit);
 92         jit.move(
 93             AssemblyHelpers::TrustedImm32(
 94                 jit.codeBlock()-&gt;exitCountThresholdForReoptimizationFromLoop()),
 95             GPRInfo::regT1);
 96 
 97         done.link(&amp;jit);
 98     }
 99 
100     tooFewFails = jit.branch32(AssemblyHelpers::BelowOrEqual, GPRInfo::regT2, GPRInfo::regT1);
101 
102     reoptimizeNow.link(&amp;jit);
103 
<span class="line-modified">104     jit.setupArguments&lt;decltype(triggerReoptimizationNow)&gt;(GPRInfo::regT0, GPRInfo::regT3, AssemblyHelpers::TrustedImmPtr(&amp;exit));</span>
<span class="line-modified">105     jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(triggerReoptimizationNow)), GPRInfo::nonArgGPR0);</span>

106     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
107     AssemblyHelpers::Jump doneAdjusting = jit.jump();
108 
109     tooFewFails.link(&amp;jit);
110 
111     // Adjust the execution counter such that the target is to only optimize after a while.
112     int32_t activeThreshold =
113         jit.baselineCodeBlock()-&gt;adjustedCounterValue(
114             Options::thresholdForOptimizeAfterLongWarmUp());
115     int32_t targetValue = applyMemoryUsageHeuristicsAndConvertToInt(
116         activeThreshold, jit.baselineCodeBlock());
117     int32_t clippedValue;
118     switch (jit.codeBlock()-&gt;jitType()) {
119     case JITType::DFGJIT:
120         clippedValue = BaselineExecutionCounter::clippedThreshold(jit.codeBlock()-&gt;globalObject(), targetValue);
121         break;
122     case JITType::FTLJIT:
123         clippedValue = UpperTierExecutionCounter::clippedThreshold(jit.codeBlock()-&gt;globalObject(), targetValue);
124         break;
125     default:
126         RELEASE_ASSERT_NOT_REACHED();
127 #if COMPILER_QUIRK(CONSIDERS_UNREACHABLE_CODE)
128         clippedValue = 0; // Make some compilers, and mhahnenberg, happy.
129 #endif
130         break;
131     }
132     jit.store32(AssemblyHelpers::TrustedImm32(-clippedValue), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecuteCounter()));
133     jit.store32(AssemblyHelpers::TrustedImm32(activeThreshold), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionActiveThreshold()));
134     jit.store32(AssemblyHelpers::TrustedImm32(formattedTotalExecutionCount(clippedValue)), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionTotalCount()));
135 
136     doneAdjusting.link(&amp;jit);
137 }
138 





































































































139 void reifyInlinedCallFrames(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
140 {
141     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
142     // in presence of inlined tail calls.
143     // https://bugs.webkit.org/show_bug.cgi?id=147511
<span class="line-modified">144     ASSERT(jit.baselineCodeBlock()-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-modified">145     jit.storePtr(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), AssemblyHelpers::addressFor((VirtualRegister)CallFrameSlot::codeBlock));</span>
146 
147     const CodeOrigin* codeOrigin;
148     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {
149         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();
150         CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(*codeOrigin);
151         InlineCallFrame::Kind trueCallerCallKind;
152         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
153         GPRReg callerFrameGPR = GPRInfo::callFrameRegister;
154 


155         if (!trueCaller) {
156             ASSERT(inlineCallFrame-&gt;isTail());
157             jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
158 #if CPU(ARM64E)
159             jit.addPtr(AssemblyHelpers::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, GPRInfo::regT2);
160             jit.untagPtr(GPRInfo::regT2, GPRInfo::regT3);
161             jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
162             jit.tagPtr(GPRInfo::regT2, GPRInfo::regT3);
163 #endif
164             jit.storePtr(GPRInfo::regT3, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
165             jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), GPRInfo::regT3);
166             callerFrameGPR = GPRInfo::regT3;
167         } else {
168             CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);
<span class="line-modified">169             unsigned callBytecodeIndex = trueCaller-&gt;bytecodeIndex();</span>
<span class="line-modified">170             void* jumpTarget = nullptr;</span>
<span class="line-removed">171 </span>
<span class="line-removed">172             switch (trueCallerCallKind) {</span>
<span class="line-removed">173             case InlineCallFrame::Call:</span>
<span class="line-removed">174             case InlineCallFrame::Construct:</span>
<span class="line-removed">175             case InlineCallFrame::CallVarargs:</span>
<span class="line-removed">176             case InlineCallFrame::ConstructVarargs:</span>
<span class="line-removed">177             case InlineCallFrame::TailCall:</span>
<span class="line-removed">178             case InlineCallFrame::TailCallVarargs: {</span>
<span class="line-removed">179                 CallLinkInfo* callLinkInfo =</span>
<span class="line-removed">180                     baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);</span>
<span class="line-removed">181                 RELEASE_ASSERT(callLinkInfo);</span>
<span class="line-removed">182 </span>
<span class="line-removed">183                 jumpTarget = callLinkInfo-&gt;callReturnLocation().untaggedExecutableAddress();</span>
<span class="line-removed">184                 break;</span>
<span class="line-removed">185             }</span>
<span class="line-removed">186 </span>
<span class="line-removed">187             case InlineCallFrame::GetterCall:</span>
<span class="line-removed">188             case InlineCallFrame::SetterCall: {</span>
<span class="line-removed">189                 StructureStubInfo* stubInfo =</span>
<span class="line-removed">190                     baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));</span>
<span class="line-removed">191                 RELEASE_ASSERT(stubInfo);</span>
<span class="line-removed">192 </span>
<span class="line-removed">193                 jumpTarget = stubInfo-&gt;doneLocation().untaggedExecutableAddress();</span>
<span class="line-removed">194                 break;</span>
<span class="line-removed">195             }</span>
<span class="line-removed">196 </span>
<span class="line-removed">197             default:</span>
<span class="line-removed">198                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">199             }</span>
200 
201             if (trueCaller-&gt;inlineCallFrame()) {
202                 jit.addPtr(
203                     AssemblyHelpers::TrustedImm32(trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue)),
204                     GPRInfo::callFrameRegister,
205                     GPRInfo::regT3);
206                 callerFrameGPR = GPRInfo::regT3;
207             }
208 
209 #if CPU(ARM64E)
210             jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
<span class="line-modified">211             jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget), GPRInfo::nonArgGPR0);</span>
212             jit.tagPtr(GPRInfo::regT2, GPRInfo::nonArgGPR0);
213             jit.storePtr(GPRInfo::nonArgGPR0, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
214 #else
<span class="line-modified">215             jit.storePtr(AssemblyHelpers::TrustedImmPtr(jumpTarget), AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));</span>
216 #endif
217         }
218 
219         jit.storePtr(AssemblyHelpers::TrustedImmPtr(baselineCodeBlock), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock)));
220 
221         // Restore the inline call frame&#39;s callee save registers.
222         // If this inlined frame is a tail call that will return back to the original caller, we need to
223         // copy the prior contents of the tag registers already saved for the outer frame to this frame.
224         jit.emitSaveOrCopyCalleeSavesFor(
225             baselineCodeBlock,
226             static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset),
227             trueCaller ? AssemblyHelpers::UseExistingTagRegisterContents : AssemblyHelpers::CopyBaselineCalleeSavedRegistersFromBaseFrame,
228             GPRInfo::regT2);
229 






230         if (!inlineCallFrame-&gt;isVarargs())
<span class="line-modified">231             jit.store32(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis), AssemblyHelpers::payloadFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount)));</span>
<span class="line-removed">232 #if USE(JSVALUE64)</span>
233         jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));
<span class="line-modified">234         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
<span class="line-modified">235         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount)));</span>

236         if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">237             jit.store64(AssemblyHelpers::TrustedImm64(JSValue::encode(JSValue(inlineCallFrame-&gt;calleeConstant()))), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
238 #else // USE(JSVALUE64) // so this is the 32-bit part
<span class="line-modified">239         jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));</span>
<span class="line-removed">240         const Instruction* instruction = baselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
<span class="line-removed">241         uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed">242         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount)));</span>
<span class="line-removed">243         jit.store32(AssemblyHelpers::TrustedImm32(JSValue::CellTag), AssemblyHelpers::tagFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
244         if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">245             jit.storePtr(AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeConstant()), AssemblyHelpers::payloadFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
246 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
247     }
248 
249     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
250     if (codeOrigin) {
<span class="line-modified">251 #if USE(JSVALUE64)</span>
<span class="line-modified">252         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
<span class="line-removed">253 #else</span>
<span class="line-removed">254         const Instruction* instruction = jit.baselineCodeBlock()-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
<span class="line-removed">255         uint32_t locationBits = CallSiteIndex(instruction).bits();</span>
<span class="line-removed">256 #endif</span>
<span class="line-removed">257         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor((VirtualRegister)(CallFrameSlot::argumentCount)));</span>
258     }
259 }
260 
<span class="line-modified">261 static void osrWriteBarrier(CCallHelpers&amp; jit, GPRReg owner, GPRReg scratch)</span>
262 {
263     AssemblyHelpers::Jump ownerIsRememberedOrInEden = jit.barrierBranchWithoutFence(owner);
264 
<span class="line-modified">265     // We need these extra slots because setupArgumentsWithExecState will use poke on x86.</span>
<span class="line-modified">266 #if CPU(X86)</span>
<span class="line-removed">267     jit.subPtr(MacroAssembler::TrustedImm32(sizeof(void*) * 4), MacroAssembler::stackPointerRegister);</span>
<span class="line-removed">268 #endif</span>
<span class="line-removed">269 </span>
<span class="line-removed">270     jit.setupArguments&lt;decltype(operationOSRWriteBarrier)&gt;(owner);</span>
271     jit.move(MacroAssembler::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationOSRWriteBarrier)), scratch);
272     jit.call(scratch, OperationPtrTag);
273 
<span class="line-removed">274 #if CPU(X86)</span>
<span class="line-removed">275     jit.addPtr(MacroAssembler::TrustedImm32(sizeof(void*) * 4), MacroAssembler::stackPointerRegister);</span>
<span class="line-removed">276 #endif</span>
<span class="line-removed">277 </span>
278     ownerIsRememberedOrInEden.link(&amp;jit);
279 }
280 
281 void adjustAndJumpToTarget(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
282 {
283     jit.memoryFence();
284 
285     jit.move(
286         AssemblyHelpers::TrustedImmPtr(
287             jit.codeBlock()-&gt;baselineAlternative()), GPRInfo::argumentGPR1);
<span class="line-modified">288     osrWriteBarrier(jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
289 
290     // We barrier all inlined frames -- and not just the current inline stack --
291     // because we don&#39;t know which inlined function owns the value profile that
292     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
293     // a and b are inlined, we might exit inside b due to a bad value loaded
294     // from a.
295     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns
296     // the value profile.
297     InlineCallFrameSet* inlineCallFrames = jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();
298     if (inlineCallFrames) {
299         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames) {
300             jit.move(
301                 AssemblyHelpers::TrustedImmPtr(
302                     inlineCallFrame-&gt;baselineCodeBlock.get()), GPRInfo::argumentGPR1);
<span class="line-modified">303             osrWriteBarrier(jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
304         }
305     }
306 
307     auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();
308     if (exitInlineCallFrame)
309         jit.addPtr(AssemblyHelpers::TrustedImm32(exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue)), GPRInfo::callFrameRegister);
310 
311     CodeBlock* codeBlockForExit = jit.baselineCodeBlockFor(exit.m_codeOrigin);
312     ASSERT(codeBlockForExit == codeBlockForExit-&gt;baselineVersion());
<span class="line-modified">313     ASSERT(codeBlockForExit-&gt;jitType() == JITType::BaselineJIT);</span>
<span class="line-modified">314     CodeLocationLabel&lt;JSEntryPtrTag&gt; codeLocation = codeBlockForExit-&gt;jitCodeMap().find(exit.m_codeOrigin.bytecodeIndex());</span>
<span class="line-modified">315     ASSERT(codeLocation);</span>



































316 
<span class="line-removed">317     void* jumpTarget = codeLocation.retagged&lt;OSRExitPtrTag&gt;().executableAddress();</span>
318     jit.addPtr(AssemblyHelpers::TrustedImm32(JIT::stackPointerOffsetFor(codeBlockForExit) * sizeof(Register)), GPRInfo::callFrameRegister, AssemblyHelpers::stackPointerRegister);
319     if (exit.isExceptionHandler()) {
320         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
321         jit.storePtr(GPRInfo::callFrameRegister, vm.addressOfCallFrameForCatch());
322     }
323 
324     jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget), GPRInfo::regT2);
325     jit.farJump(GPRInfo::regT2, OSRExitPtrTag);
326 }
327 
328 } } // namespace JSC::DFG
329 
330 #endif // ENABLE(DFG_JIT)
331 
</pre>
</td>
<td>
<hr />
<pre>
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGOSRExitCompilerCommon.h&quot;
 28 
 29 #if ENABLE(DFG_JIT)
 30 
<span class="line-added"> 31 #include &quot;Bytecodes.h&quot;</span>
<span class="line-added"> 32 #include &quot;CheckpointOSRExitSideState.h&quot;</span>
 33 #include &quot;DFGJITCode.h&quot;
 34 #include &quot;DFGOperations.h&quot;
 35 #include &quot;JIT.h&quot;
 36 #include &quot;JSCJSValueInlines.h&quot;
 37 #include &quot;JSCInlines.h&quot;
<span class="line-added"> 38 #include &quot;LLIntData.h&quot;</span>
<span class="line-added"> 39 #include &quot;LLIntThunks.h&quot;</span>
<span class="line-added"> 40 #include &quot;ProbeContext.h&quot;</span>
 41 #include &quot;StructureStubInfo.h&quot;
 42 
 43 namespace JSC { namespace DFG {
 44 
<span class="line-modified"> 45 void handleExitCounts(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)</span>
 46 {
 47     if (!exitKindMayJettison(exit.m_kind)) {
 48         // FIXME: We may want to notice that we&#39;re frequently exiting
 49         // at an op_catch that we didn&#39;t compile an entrypoint for, and
 50         // then trigger a reoptimization of this CodeBlock:
 51         // https://bugs.webkit.org/show_bug.cgi?id=175842
 52         return;
 53     }
 54 
 55     jit.add32(AssemblyHelpers::TrustedImm32(1), AssemblyHelpers::AbsoluteAddress(&amp;exit.m_count));
 56 
 57     jit.move(AssemblyHelpers::TrustedImmPtr(jit.codeBlock()), GPRInfo::regT3);
 58 
 59     AssemblyHelpers::Jump tooFewFails;
 60 
 61     jit.load32(AssemblyHelpers::Address(GPRInfo::regT3, CodeBlock::offsetOfOSRExitCounter()), GPRInfo::regT2);
 62     jit.add32(AssemblyHelpers::TrustedImm32(1), GPRInfo::regT2);
 63     jit.store32(GPRInfo::regT2, AssemblyHelpers::Address(GPRInfo::regT3, CodeBlock::offsetOfOSRExitCounter()));
 64 
 65     jit.move(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), GPRInfo::regT0);
</pre>
<hr />
<pre>
 89     jit.move(
 90         AssemblyHelpers::TrustedImm32(jit.codeBlock()-&gt;exitCountThresholdForReoptimization()),
 91         GPRInfo::regT1);
 92 
 93     if (!loopThreshold.empty()) {
 94         AssemblyHelpers::Jump done = jit.jump();
 95 
 96         loopThreshold.link(&amp;jit);
 97         jit.move(
 98             AssemblyHelpers::TrustedImm32(
 99                 jit.codeBlock()-&gt;exitCountThresholdForReoptimizationFromLoop()),
100             GPRInfo::regT1);
101 
102         done.link(&amp;jit);
103     }
104 
105     tooFewFails = jit.branch32(AssemblyHelpers::BelowOrEqual, GPRInfo::regT2, GPRInfo::regT1);
106 
107     reoptimizeNow.link(&amp;jit);
108 
<span class="line-modified">109     jit.setupArguments&lt;decltype(operationTriggerReoptimizationNow)&gt;(GPRInfo::regT0, GPRInfo::regT3, AssemblyHelpers::TrustedImmPtr(&amp;exit));</span>
<span class="line-modified">110     jit.prepareCallOperation(vm);</span>
<span class="line-added">111     jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationTriggerReoptimizationNow)), GPRInfo::nonArgGPR0);</span>
112     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
113     AssemblyHelpers::Jump doneAdjusting = jit.jump();
114 
115     tooFewFails.link(&amp;jit);
116 
117     // Adjust the execution counter such that the target is to only optimize after a while.
118     int32_t activeThreshold =
119         jit.baselineCodeBlock()-&gt;adjustedCounterValue(
120             Options::thresholdForOptimizeAfterLongWarmUp());
121     int32_t targetValue = applyMemoryUsageHeuristicsAndConvertToInt(
122         activeThreshold, jit.baselineCodeBlock());
123     int32_t clippedValue;
124     switch (jit.codeBlock()-&gt;jitType()) {
125     case JITType::DFGJIT:
126         clippedValue = BaselineExecutionCounter::clippedThreshold(jit.codeBlock()-&gt;globalObject(), targetValue);
127         break;
128     case JITType::FTLJIT:
129         clippedValue = UpperTierExecutionCounter::clippedThreshold(jit.codeBlock()-&gt;globalObject(), targetValue);
130         break;
131     default:
132         RELEASE_ASSERT_NOT_REACHED();
133 #if COMPILER_QUIRK(CONSIDERS_UNREACHABLE_CODE)
134         clippedValue = 0; // Make some compilers, and mhahnenberg, happy.
135 #endif
136         break;
137     }
138     jit.store32(AssemblyHelpers::TrustedImm32(-clippedValue), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecuteCounter()));
139     jit.store32(AssemblyHelpers::TrustedImm32(activeThreshold), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionActiveThreshold()));
140     jit.store32(AssemblyHelpers::TrustedImm32(formattedTotalExecutionCount(clippedValue)), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionTotalCount()));
141 
142     doneAdjusting.link(&amp;jit);
143 }
144 
<span class="line-added">145 MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; callerReturnPC(CodeBlock* baselineCodeBlockForCaller, BytecodeIndex callBytecodeIndex, InlineCallFrame::Kind trueCallerCallKind, bool&amp; callerIsLLInt)</span>
<span class="line-added">146 {</span>
<span class="line-added">147     callerIsLLInt = Options::forceOSRExitToLLInt() || baselineCodeBlockForCaller-&gt;jitType() == JITType::InterpreterThunk;</span>
<span class="line-added">148 </span>
<span class="line-added">149     if (callBytecodeIndex.checkpoint()) {</span>
<span class="line-added">150         if (!callerIsLLInt)</span>
<span class="line-added">151             baselineCodeBlockForCaller-&gt;m_hasLinkedOSRExit = true;</span>
<span class="line-added">152         return LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_from_inlined_call_trampoline);</span>
<span class="line-added">153     }</span>
<span class="line-added">154 </span>
<span class="line-added">155     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jumpTarget;</span>
<span class="line-added">156 </span>
<span class="line-added">157     if (callerIsLLInt) {</span>
<span class="line-added">158         const Instruction&amp; callInstruction = *baselineCodeBlockForCaller-&gt;instructions().at(callBytecodeIndex).ptr();</span>
<span class="line-added">159 #define LLINT_RETURN_LOCATION(name) (callInstruction.isWide16() ? LLInt::getWide16CodePtr&lt;JSEntryPtrTag&gt;(name##_return_location) : (callInstruction.isWide32() ? LLInt::getWide32CodePtr&lt;JSEntryPtrTag&gt;(name##_return_location) : LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(name##_return_location)))</span>
<span class="line-added">160 </span>
<span class="line-added">161         switch (trueCallerCallKind) {</span>
<span class="line-added">162         case InlineCallFrame::Call:</span>
<span class="line-added">163             jumpTarget = LLINT_RETURN_LOCATION(op_call);</span>
<span class="line-added">164             break;</span>
<span class="line-added">165         case InlineCallFrame::Construct:</span>
<span class="line-added">166             jumpTarget = LLINT_RETURN_LOCATION(op_construct);</span>
<span class="line-added">167             break;</span>
<span class="line-added">168         case InlineCallFrame::CallVarargs:</span>
<span class="line-added">169             jumpTarget = LLINT_RETURN_LOCATION(op_call_varargs_slow);</span>
<span class="line-added">170             break;</span>
<span class="line-added">171         case InlineCallFrame::ConstructVarargs:</span>
<span class="line-added">172             jumpTarget = LLINT_RETURN_LOCATION(op_construct_varargs_slow);</span>
<span class="line-added">173             break;</span>
<span class="line-added">174         case InlineCallFrame::GetterCall: {</span>
<span class="line-added">175             if (callInstruction.opcodeID() == op_get_by_id)</span>
<span class="line-added">176                 jumpTarget = LLINT_RETURN_LOCATION(op_get_by_id);</span>
<span class="line-added">177             else if (callInstruction.opcodeID() == op_get_by_val)</span>
<span class="line-added">178                 jumpTarget = LLINT_RETURN_LOCATION(op_get_by_val);</span>
<span class="line-added">179             else</span>
<span class="line-added">180                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">181             break;</span>
<span class="line-added">182         }</span>
<span class="line-added">183         case InlineCallFrame::SetterCall: {</span>
<span class="line-added">184             if (callInstruction.opcodeID() == op_put_by_id)</span>
<span class="line-added">185                 jumpTarget = LLINT_RETURN_LOCATION(op_put_by_id);</span>
<span class="line-added">186             else if (callInstruction.opcodeID() == op_put_by_val)</span>
<span class="line-added">187                 jumpTarget = LLINT_RETURN_LOCATION(op_put_by_val);</span>
<span class="line-added">188             else</span>
<span class="line-added">189                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">190             break;</span>
<span class="line-added">191         }</span>
<span class="line-added">192         default:</span>
<span class="line-added">193             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">194         }</span>
<span class="line-added">195 </span>
<span class="line-added">196 #undef LLINT_RETURN_LOCATION</span>
<span class="line-added">197 </span>
<span class="line-added">198     } else {</span>
<span class="line-added">199         baselineCodeBlockForCaller-&gt;m_hasLinkedOSRExit = true;</span>
<span class="line-added">200 </span>
<span class="line-added">201         switch (trueCallerCallKind) {</span>
<span class="line-added">202         case InlineCallFrame::Call:</span>
<span class="line-added">203         case InlineCallFrame::Construct:</span>
<span class="line-added">204         case InlineCallFrame::CallVarargs:</span>
<span class="line-added">205         case InlineCallFrame::ConstructVarargs: {</span>
<span class="line-added">206             CallLinkInfo* callLinkInfo =</span>
<span class="line-added">207                 baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);</span>
<span class="line-added">208             RELEASE_ASSERT(callLinkInfo);</span>
<span class="line-added">209 </span>
<span class="line-added">210             jumpTarget = callLinkInfo-&gt;callReturnLocation().retagged&lt;JSEntryPtrTag&gt;();</span>
<span class="line-added">211             break;</span>
<span class="line-added">212         }</span>
<span class="line-added">213 </span>
<span class="line-added">214         case InlineCallFrame::GetterCall:</span>
<span class="line-added">215         case InlineCallFrame::SetterCall: {</span>
<span class="line-added">216             StructureStubInfo* stubInfo =</span>
<span class="line-added">217                 baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));</span>
<span class="line-added">218             RELEASE_ASSERT(stubInfo);</span>
<span class="line-added">219 </span>
<span class="line-added">220             jumpTarget = stubInfo-&gt;doneLocation.retagged&lt;JSEntryPtrTag&gt;();</span>
<span class="line-added">221             break;</span>
<span class="line-added">222         }</span>
<span class="line-added">223 </span>
<span class="line-added">224         default:</span>
<span class="line-added">225             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">226         }</span>
<span class="line-added">227     }</span>
<span class="line-added">228 </span>
<span class="line-added">229     return jumpTarget;</span>
<span class="line-added">230 }</span>
<span class="line-added">231 </span>
<span class="line-added">232 CCallHelpers::Address calleeSaveSlot(InlineCallFrame* inlineCallFrame, CodeBlock* baselineCodeBlock, GPRReg calleeSave)</span>
<span class="line-added">233 {</span>
<span class="line-added">234     const RegisterAtOffsetList* calleeSaves = baselineCodeBlock-&gt;calleeSaveRegisters();</span>
<span class="line-added">235     for (unsigned i = 0; i &lt; calleeSaves-&gt;size(); i++) {</span>
<span class="line-added">236         RegisterAtOffset entry = calleeSaves-&gt;at(i);</span>
<span class="line-added">237         if (entry.reg() != calleeSave)</span>
<span class="line-added">238             continue;</span>
<span class="line-added">239         return CCallHelpers::Address(CCallHelpers::framePointerRegister, static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset).offsetInBytes() + entry.offset());</span>
<span class="line-added">240     }</span>
<span class="line-added">241 </span>
<span class="line-added">242     RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">243     return CCallHelpers::Address(CCallHelpers::framePointerRegister);</span>
<span class="line-added">244 }</span>
<span class="line-added">245 </span>
246 void reifyInlinedCallFrames(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
247 {
248     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
249     // in presence of inlined tail calls.
250     // https://bugs.webkit.org/show_bug.cgi?id=147511
<span class="line-modified">251     ASSERT(JITCode::isBaselineCode(jit.baselineCodeBlock()-&gt;jitType()));</span>
<span class="line-modified">252     jit.storePtr(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), AssemblyHelpers::addressFor(CallFrameSlot::codeBlock));</span>
253 
254     const CodeOrigin* codeOrigin;
255     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {
256         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();
257         CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(*codeOrigin);
258         InlineCallFrame::Kind trueCallerCallKind;
259         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
260         GPRReg callerFrameGPR = GPRInfo::callFrameRegister;
261 
<span class="line-added">262         bool callerIsLLInt = false;</span>
<span class="line-added">263 </span>
264         if (!trueCaller) {
265             ASSERT(inlineCallFrame-&gt;isTail());
266             jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
267 #if CPU(ARM64E)
268             jit.addPtr(AssemblyHelpers::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, GPRInfo::regT2);
269             jit.untagPtr(GPRInfo::regT2, GPRInfo::regT3);
270             jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
271             jit.tagPtr(GPRInfo::regT2, GPRInfo::regT3);
272 #endif
273             jit.storePtr(GPRInfo::regT3, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
274             jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), GPRInfo::regT3);
275             callerFrameGPR = GPRInfo::regT3;
276         } else {
277             CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);
<span class="line-modified">278             auto callBytecodeIndex = trueCaller-&gt;bytecodeIndex();</span>
<span class="line-modified">279             MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jumpTarget = callerReturnPC(baselineCodeBlockForCaller, callBytecodeIndex, trueCallerCallKind, callerIsLLInt);</span>





























280 
281             if (trueCaller-&gt;inlineCallFrame()) {
282                 jit.addPtr(
283                     AssemblyHelpers::TrustedImm32(trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue)),
284                     GPRInfo::callFrameRegister,
285                     GPRInfo::regT3);
286                 callerFrameGPR = GPRInfo::regT3;
287             }
288 
289 #if CPU(ARM64E)
290             jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
<span class="line-modified">291             jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget.untaggedExecutableAddress()), GPRInfo::nonArgGPR0);</span>
292             jit.tagPtr(GPRInfo::regT2, GPRInfo::nonArgGPR0);
293             jit.storePtr(GPRInfo::nonArgGPR0, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
294 #else
<span class="line-modified">295             jit.storePtr(AssemblyHelpers::TrustedImmPtr(jumpTarget.untaggedExecutableAddress()), AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));</span>
296 #endif
297         }
298 
299         jit.storePtr(AssemblyHelpers::TrustedImmPtr(baselineCodeBlock), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock)));
300 
301         // Restore the inline call frame&#39;s callee save registers.
302         // If this inlined frame is a tail call that will return back to the original caller, we need to
303         // copy the prior contents of the tag registers already saved for the outer frame to this frame.
304         jit.emitSaveOrCopyCalleeSavesFor(
305             baselineCodeBlock,
306             static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset),
307             trueCaller ? AssemblyHelpers::UseExistingTagRegisterContents : AssemblyHelpers::CopyBaselineCalleeSavedRegistersFromBaseFrame,
308             GPRInfo::regT2);
309 
<span class="line-added">310         if (callerIsLLInt) {</span>
<span class="line-added">311             CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);</span>
<span class="line-added">312             jit.storePtr(CCallHelpers::TrustedImmPtr(baselineCodeBlockForCaller-&gt;metadataTable()), calleeSaveSlot(inlineCallFrame, baselineCodeBlock, LLInt::Registers::metadataTableGPR));</span>
<span class="line-added">313             jit.storePtr(CCallHelpers::TrustedImmPtr(baselineCodeBlockForCaller-&gt;instructionsRawPointer()), calleeSaveSlot(inlineCallFrame, baselineCodeBlock, LLInt::Registers::pbGPR));</span>
<span class="line-added">314         }</span>
<span class="line-added">315 </span>
316         if (!inlineCallFrame-&gt;isVarargs())
<span class="line-modified">317             jit.store32(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis), AssemblyHelpers::payloadFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis)));</span>

318         jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));
<span class="line-modified">319         uint32_t locationBits = CallSiteIndex(baselineCodeBlock-&gt;bytecodeIndexForExit(codeOrigin-&gt;bytecodeIndex())).bits();</span>
<span class="line-modified">320         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis)));</span>
<span class="line-added">321 #if USE(JSVALUE64)</span>
322         if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">323             jit.store64(AssemblyHelpers::TrustedImm64(JSValue::encode(JSValue(inlineCallFrame-&gt;calleeConstant()))), AssemblyHelpers::addressFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
324 #else // USE(JSVALUE64) // so this is the 32-bit part
<span class="line-modified">325         jit.store32(AssemblyHelpers::TrustedImm32(JSValue::CellTag), AssemblyHelpers::tagFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>




326         if (!inlineCallFrame-&gt;isClosureCall)
<span class="line-modified">327             jit.storePtr(AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeConstant()), AssemblyHelpers::payloadFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));</span>
328 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
329     }
330 
331     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
332     if (codeOrigin) {
<span class="line-modified">333         uint32_t locationBits = CallSiteIndex(BytecodeIndex(codeOrigin-&gt;bytecodeIndex().offset())).bits();</span>
<span class="line-modified">334         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));</span>





335     }
336 }
337 
<span class="line-modified">338 static void osrWriteBarrier(VM&amp; vm, CCallHelpers&amp; jit, GPRReg owner, GPRReg scratch)</span>
339 {
340     AssemblyHelpers::Jump ownerIsRememberedOrInEden = jit.barrierBranchWithoutFence(owner);
341 
<span class="line-modified">342     jit.setupArguments&lt;decltype(operationOSRWriteBarrier)&gt;(&amp;vm, owner);</span>
<span class="line-modified">343     jit.prepareCallOperation(vm);</span>




344     jit.move(MacroAssembler::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationOSRWriteBarrier)), scratch);
345     jit.call(scratch, OperationPtrTag);
346 




347     ownerIsRememberedOrInEden.link(&amp;jit);
348 }
349 
350 void adjustAndJumpToTarget(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
351 {
352     jit.memoryFence();
353 
354     jit.move(
355         AssemblyHelpers::TrustedImmPtr(
356             jit.codeBlock()-&gt;baselineAlternative()), GPRInfo::argumentGPR1);
<span class="line-modified">357     osrWriteBarrier(vm, jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
358 
359     // We barrier all inlined frames -- and not just the current inline stack --
360     // because we don&#39;t know which inlined function owns the value profile that
361     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
362     // a and b are inlined, we might exit inside b due to a bad value loaded
363     // from a.
364     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns
365     // the value profile.
366     InlineCallFrameSet* inlineCallFrames = jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();
367     if (inlineCallFrames) {
368         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames) {
369             jit.move(
370                 AssemblyHelpers::TrustedImmPtr(
371                     inlineCallFrame-&gt;baselineCodeBlock.get()), GPRInfo::argumentGPR1);
<span class="line-modified">372             osrWriteBarrier(vm, jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);</span>
373         }
374     }
375 
376     auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();
377     if (exitInlineCallFrame)
378         jit.addPtr(AssemblyHelpers::TrustedImm32(exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue)), GPRInfo::callFrameRegister);
379 
380     CodeBlock* codeBlockForExit = jit.baselineCodeBlockFor(exit.m_codeOrigin);
381     ASSERT(codeBlockForExit == codeBlockForExit-&gt;baselineVersion());
<span class="line-modified">382     ASSERT(JITCode::isBaselineCode(codeBlockForExit-&gt;jitType()));</span>
<span class="line-modified">383 </span>
<span class="line-modified">384     void* jumpTarget;</span>
<span class="line-added">385     bool exitToLLInt = Options::forceOSRExitToLLInt() || codeBlockForExit-&gt;jitType() == JITType::InterpreterThunk;</span>
<span class="line-added">386     if (exitToLLInt) {</span>
<span class="line-added">387         auto bytecodeIndex = exit.m_codeOrigin.bytecodeIndex();</span>
<span class="line-added">388         const Instruction&amp; currentInstruction = *codeBlockForExit-&gt;instructions().at(bytecodeIndex).ptr();</span>
<span class="line-added">389         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; destination;</span>
<span class="line-added">390         if (bytecodeIndex.checkpoint())</span>
<span class="line-added">391             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_trampoline);</span>
<span class="line-added">392         else</span>
<span class="line-added">393             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(currentInstruction);</span>
<span class="line-added">394 </span>
<span class="line-added">395         if (exit.isExceptionHandler()) {</span>
<span class="line-added">396             jit.move(CCallHelpers::TrustedImmPtr(&amp;currentInstruction), GPRInfo::regT2);</span>
<span class="line-added">397             jit.storePtr(GPRInfo::regT2, &amp;vm.targetInterpreterPCForThrow);</span>
<span class="line-added">398         }</span>
<span class="line-added">399 </span>
<span class="line-added">400         jit.move(CCallHelpers::TrustedImmPtr(codeBlockForExit-&gt;metadataTable()), LLInt::Registers::metadataTableGPR);</span>
<span class="line-added">401         jit.move(CCallHelpers::TrustedImmPtr(codeBlockForExit-&gt;instructionsRawPointer()), LLInt::Registers::pbGPR);</span>
<span class="line-added">402         jit.move(CCallHelpers::TrustedImm32(bytecodeIndex.offset()), LLInt::Registers::pcGPR);</span>
<span class="line-added">403         jumpTarget = destination.retagged&lt;OSRExitPtrTag&gt;().executableAddress();</span>
<span class="line-added">404     } else {</span>
<span class="line-added">405         codeBlockForExit-&gt;m_hasLinkedOSRExit = true;</span>
<span class="line-added">406 </span>
<span class="line-added">407         BytecodeIndex exitIndex = exit.m_codeOrigin.bytecodeIndex();</span>
<span class="line-added">408         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; destination;</span>
<span class="line-added">409         if (exitIndex.checkpoint())</span>
<span class="line-added">410             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_trampoline);</span>
<span class="line-added">411         else {</span>
<span class="line-added">412             ASSERT(codeBlockForExit-&gt;bytecodeIndexForExit(exitIndex) == exitIndex);</span>
<span class="line-added">413             destination = codeBlockForExit-&gt;jitCodeMap().find(exitIndex);</span>
<span class="line-added">414         }</span>
<span class="line-added">415 </span>
<span class="line-added">416         ASSERT(destination);</span>
<span class="line-added">417 </span>
<span class="line-added">418         jumpTarget = destination.retagged&lt;OSRExitPtrTag&gt;().executableAddress();</span>
<span class="line-added">419     }</span>
420 

421     jit.addPtr(AssemblyHelpers::TrustedImm32(JIT::stackPointerOffsetFor(codeBlockForExit) * sizeof(Register)), GPRInfo::callFrameRegister, AssemblyHelpers::stackPointerRegister);
422     if (exit.isExceptionHandler()) {
423         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
424         jit.storePtr(GPRInfo::callFrameRegister, vm.addressOfCallFrameForCatch());
425     }
426 
427     jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget), GPRInfo::regT2);
428     jit.farJump(GPRInfo::regT2, OSRExitPtrTag);
429 }
430 
431 } } // namespace JSC::DFG
432 
433 #endif // ENABLE(DFG_JIT)
434 
</pre>
</td>
</tr>
</table>
<center><a href="DFGOSRExitBase.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExitCompilerCommon.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>