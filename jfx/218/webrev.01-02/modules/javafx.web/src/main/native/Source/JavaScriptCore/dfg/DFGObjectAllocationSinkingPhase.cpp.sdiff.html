<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGObjectAllocationSinkingPhase.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGOSRExitCompilerCommon.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOpInfo.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGObjectAllocationSinkingPhase.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2015-2018 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGObjectAllocationSinkingPhase.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;DFGBlockMapInlines.h&quot;
  32 #include &quot;DFGClobbersExitState.h&quot;
  33 #include &quot;DFGCombinedLiveness.h&quot;
  34 #include &quot;DFGGraph.h&quot;
  35 #include &quot;DFGInsertionSet.h&quot;
  36 #include &quot;DFGLazyNode.h&quot;
  37 #include &quot;DFGLivenessAnalysisPhase.h&quot;
  38 #include &quot;DFGOSRAvailabilityAnalysisPhase.h&quot;
  39 #include &quot;DFGPhase.h&quot;
  40 #include &quot;DFGPromotedHeapLocation.h&quot;
  41 #include &quot;DFGSSACalculator.h&quot;
  42 #include &quot;DFGValidate.h&quot;

  43 #include &quot;JSCInlines.h&quot;
  44 #include &lt;wtf/StdList.h&gt;
  45 
  46 namespace JSC { namespace DFG {
  47 
  48 namespace {
  49 
  50 namespace DFGObjectAllocationSinkingPhaseInternal {
<span class="line-modified">  51 static const bool verbose = false;</span>
  52 }
  53 
  54 // In order to sink object cycles, we use a points-to analysis coupled
  55 // with an escape analysis. This analysis is actually similar to an
  56 // abstract interpreter focused on local allocations and ignoring
  57 // everything else.
  58 //
  59 // We represent the local heap using two mappings:
  60 //
  61 // - A set of the local allocations present in the function, where
  62 //   each of those have a further mapping from
  63 //   PromotedLocationDescriptor to local allocations they must point
  64 //   to.
  65 //
  66 // - A &quot;pointer&quot; mapping from nodes to local allocations, if they must
  67 //   be equal to said local allocation and are currently live. This
  68 //   can be because the node is the actual node that created the
  69 //   allocation, or any other node that must currently point to it -
  70 //   we don&#39;t make a difference.
  71 //
</pre>
<hr />
<pre>
 123 //
 124 // Block #3
 125 //  -: Return(@0)
 126 //
 127 // we should think of the heaps at tail of blocks #1 and #2 as being
 128 // exactly the same, even though one has @0.x pointing to @1 and the
 129 // other has @0.x pointing to @2, because in essence this should not
 130 // be different from the graph where we hoisted @1 and @2 into a
 131 // single allocation in block #0. We currently will not handle this
 132 // case, because we merge allocations based on the node they are
 133 // coming from, but this is only a technicality for the sake of
 134 // simplicity that shouldn&#39;t hide the deeper idea outlined here.
 135 
 136 class Allocation {
 137 public:
 138     // We use Escaped as a special allocation kind because when we
 139     // decide to sink an allocation, we still need to keep track of it
 140     // once it is escaped if it still has pointers to it in order to
 141     // replace any use of those pointers by the corresponding
 142     // materialization
<span class="line-modified"> 143     enum class Kind { Escaped, Object, Activation, Function, GeneratorFunction, AsyncFunction, AsyncGeneratorFunction, RegExpObject };</span>
 144 
 145     using Fields = HashMap&lt;PromotedLocationDescriptor, Node*&gt;;
 146 
 147     explicit Allocation(Node* identifier = nullptr, Kind kind = Kind::Escaped)
 148         : m_identifier(identifier)
 149         , m_kind(kind)
 150     {
 151     }
 152 
 153 
 154     const Fields&amp; fields() const
 155     {
 156         return m_fields;
 157     }
 158 
 159     Fields&amp; fields()
 160     {
 161         return m_fields;
 162     }
 163 
</pre>
<hr />
<pre>
 228     bool isEscapedAllocation() const
 229     {
 230         return kind() == Kind::Escaped;
 231     }
 232 
 233     bool isObjectAllocation() const
 234     {
 235         return m_kind == Kind::Object;
 236     }
 237 
 238     bool isActivationAllocation() const
 239     {
 240         return m_kind == Kind::Activation;
 241     }
 242 
 243     bool isFunctionAllocation() const
 244     {
 245         return m_kind == Kind::Function || m_kind == Kind::GeneratorFunction || m_kind == Kind::AsyncFunction;
 246     }
 247 





 248     bool isRegExpObjectAllocation() const
 249     {
 250         return m_kind == Kind::RegExpObject;
 251     }
 252 
 253     bool operator==(const Allocation&amp; other) const
 254     {
 255         return m_identifier == other.m_identifier
 256             &amp;&amp; m_kind == other.m_kind
 257             &amp;&amp; m_fields == other.m_fields
 258             &amp;&amp; m_structures == other.m_structures;
 259     }
 260 
 261     bool operator!=(const Allocation&amp; other) const
 262     {
 263         return !(*this == other);
 264     }
 265 
 266     void dump(PrintStream&amp; out) const
 267     {
</pre>
<hr />
<pre>
 274         case Kind::Escaped:
 275             out.print(&quot;Escaped&quot;);
 276             break;
 277 
 278         case Kind::Object:
 279             out.print(&quot;Object&quot;);
 280             break;
 281 
 282         case Kind::Function:
 283             out.print(&quot;Function&quot;);
 284             break;
 285 
 286         case Kind::GeneratorFunction:
 287             out.print(&quot;GeneratorFunction&quot;);
 288             break;
 289 
 290         case Kind::AsyncFunction:
 291             out.print(&quot;AsyncFunction&quot;);
 292             break;
 293 




 294         case Kind::AsyncGeneratorFunction:
 295             out.print(&quot;AsyncGeneratorFunction&quot;);
 296             break;
 297 
 298         case Kind::Activation:
 299             out.print(&quot;Activation&quot;);
 300             break;
 301 
 302         case Kind::RegExpObject:
 303             out.print(&quot;RegExpObject&quot;);
 304             break;
 305         }
 306         out.print(&quot;Allocation(&quot;);
 307         if (!m_structures.isEmpty())
 308             out.print(inContext(m_structures.toStructureSet(), context));
 309         if (!m_fields.isEmpty()) {
 310             if (!m_structures.isEmpty())
 311                 out.print(&quot;, &quot;);
 312             out.print(mapDump(m_fields, &quot; =&gt; #&quot;, &quot;, &quot;));
 313         }
</pre>
<hr />
<pre>
 375     }
 376 
 377     void newPointer(Node* node, Node* identifier)
 378     {
 379         ASSERT(!m_allocations.contains(node) &amp;&amp; !m_pointers.contains(node));
 380         ASSERT(isAllocation(identifier));
 381         m_pointers.add(node, identifier);
 382     }
 383 
 384     // follow solves the points-to problem. Given a live node, which
 385     // may be either an allocation itself or a heap read (e.g. a
 386     // GetByOffset node), it returns the corresponding allocation
 387     // node, if there is one. If the argument node is neither an
 388     // allocation or a heap read, or may point to different nodes,
 389     // nullptr will be returned. Note that a node that points to
 390     // different nodes can never point to an unescaped local
 391     // allocation.
 392     Node* follow(Node* node) const
 393     {
 394         auto iter = m_pointers.find(node);
<span class="line-modified"> 395         ASSERT(iter == m_pointers.end() || m_allocations.contains(iter-&gt;value));</span>
 396         return iter == m_pointers.end() ? nullptr : iter-&gt;value;
 397     }
 398 
 399     Node* follow(PromotedHeapLocation location) const
 400     {
 401         const Allocation&amp; base = m_allocations.find(location.base())-&gt;value;
 402         auto iter = base.fields().find(location.descriptor());
<span class="line-removed"> 403 </span>
 404         if (iter == base.fields().end())
 405             return nullptr;
 406 
 407         return iter-&gt;value;
 408     }
 409 
 410     // onlyLocalAllocation find the corresponding allocation metadata
 411     // for any live node. onlyLocalAllocation(node) is essentially
 412     // getAllocation(follow(node)), with appropriate null handling.
 413     Allocation* onlyLocalAllocation(Node* node)
 414     {
 415         Node* identifier = follow(node);
 416         if (!identifier)
 417             return nullptr;
 418 
 419         return &amp;getAllocation(identifier);
 420     }
 421 
 422     Allocation* onlyLocalAllocation(PromotedHeapLocation location)
 423     {
 424         Node* identifier = follow(location);
 425         if (!identifier)
 426             return nullptr;
 427 
 428         return &amp;getAllocation(identifier);
 429     }
 430 






 431     // This allows us to store the escapees only when necessary. If
 432     // set, the current escapees can be retrieved at any time using
 433     // takeEscapees(), which will clear the cached set of escapees;
 434     // otherwise the heap won&#39;t remember escaping allocations.
 435     void setWantEscapees()
 436     {
 437         m_wantEscapees = true;
 438     }
 439 
 440     HashMap&lt;Node*, Allocation&gt; takeEscapees()
 441     {
 442         return WTFMove(m_escapees);
 443     }
 444 
 445     void escape(Node* node)
 446     {
 447         Node* identifier = follow(node);
 448         if (!identifier)
 449             return;
 450 
</pre>
<hr />
<pre>
 472 
 473             // If we have it and they don&#39;t, it died for them but we
 474             // are keeping it alive from another field somewhere.
 475             // There is nothing to do - we will be escaped
 476             // automatically when we handle that other field.
 477             // This will also happen for allocation that we have and
 478             // they don&#39;t, and all of those will get pruned.
 479             if (allocationIter == other.m_allocations.end())
 480                 continue;
 481 
 482             if (allocationEntry.value.kind() != allocationIter-&gt;value.kind()) {
 483                 toEscape.addVoid(allocationEntry.key);
 484                 for (const auto&amp; fieldEntry : allocationIter-&gt;value.fields())
 485                     toEscape.addVoid(fieldEntry.value);
 486             } else {
 487                 mergePointerSets(allocationEntry.value.fields(), allocationIter-&gt;value.fields(), toEscape);
 488                 allocationEntry.value.mergeStructures(allocationIter-&gt;value.structures());
 489             }
 490         }
 491 
<span class="line-modified"> 492         mergePointerSets(m_pointers, other.m_pointers, toEscape);</span>































































 493 
 494         for (Node* identifier : toEscape)
 495             escapeAllocation(identifier);
 496 
<span class="line-modified"> 497         if (!ASSERT_DISABLED) {</span>
 498             for (const auto&amp; entry : m_allocations)
 499                 ASSERT_UNUSED(entry, entry.value.isEscapedAllocation() || other.m_allocations.contains(entry.key));
 500         }
 501 
 502         // If there is no remaining pointer to an allocation, we can
 503         // remove it. This should only happen for escaped allocations,
 504         // because we only merge liveness-pruned heaps in the first
 505         // place.
 506         prune();
 507 
 508         assertIsValid();
 509     }
 510 
 511     void pruneByLiveness(const NodeSet&amp; live)
 512     {
 513         m_pointers.removeIf(
 514             [&amp;] (const auto&amp; entry) {
 515                 return !live.contains(entry.key);
 516             });
 517         prune();
 518     }
 519 
 520     void assertIsValid() const
 521     {
<span class="line-modified"> 522         if (ASSERT_DISABLED)</span>
 523             return;
 524 
 525         // Pointers should point to an actual allocation
 526         for (const auto&amp; entry : m_pointers) {
<span class="line-modified"> 527             ASSERT_UNUSED(entry, entry.value);</span>
<span class="line-modified"> 528             ASSERT(m_allocations.contains(entry.value));</span>
 529         }
 530 
 531         for (const auto&amp; allocationEntry : m_allocations) {
 532             // Fields should point to an actual allocation
 533             for (const auto&amp; fieldEntry : allocationEntry.value.fields()) {
 534                 ASSERT_UNUSED(fieldEntry, fieldEntry.value);
 535                 ASSERT(m_allocations.contains(fieldEntry.value));
 536             }
 537         }
 538     }
 539 
 540     bool operator==(const LocalHeap&amp; other) const
 541     {
 542         assertIsValid();
 543         other.assertIsValid();
 544         return m_allocations == other.m_allocations
 545             &amp;&amp; m_pointers == other.m_pointers;
 546     }
 547 
 548     bool operator!=(const LocalHeap&amp; other) const
 549     {
 550         return !(*this == other);
 551     }
 552 
 553     const HashMap&lt;Node*, Allocation&gt;&amp; allocations() const
 554     {
 555         return m_allocations;
 556     }
 557 
<span class="line-removed"> 558     const HashMap&lt;Node*, Node*&gt;&amp; pointers() const</span>
<span class="line-removed"> 559     {</span>
<span class="line-removed"> 560         return m_pointers;</span>
<span class="line-removed"> 561     }</span>
<span class="line-removed"> 562 </span>
 563     void dump(PrintStream&amp; out) const
 564     {
 565         out.print(&quot;  Allocations:\n&quot;);
 566         for (const auto&amp; entry : m_allocations)
 567             out.print(&quot;    #&quot;, entry.key, &quot;: &quot;, entry.value, &quot;\n&quot;);
 568         out.print(&quot;  Pointers:\n&quot;);
<span class="line-modified"> 569         for (const auto&amp; entry : m_pointers)</span>
<span class="line-modified"> 570             out.print(&quot;    &quot;, entry.key, &quot; =&gt; #&quot;, entry.value, &quot;\n&quot;);</span>





 571     }
 572 
 573     bool reached() const
 574     {
 575         return m_reached;
 576     }
 577 
 578     void setReached()
 579     {
 580         m_reached = true;
 581     }
 582 
 583 private:
 584     // When we merge two heaps, we escape all fields of allocations,
 585     // unless they point to the same thing in both heaps.
 586     // The reason for this is that it allows us not to do extra work
 587     // for diamond graphs where we would otherwise have to check
 588     // whether we have a single definition or not, which would be
 589     // cumbersome.
 590     //
</pre>
<hr />
<pre>
 644 
 645     void escapeAllocation(Node* identifier)
 646     {
 647         Allocation&amp; allocation = getAllocation(identifier);
 648         if (allocation.isEscapedAllocation())
 649             return;
 650 
 651         Allocation unescaped = WTFMove(allocation);
 652         allocation = Allocation(unescaped.identifier(), Allocation::Kind::Escaped);
 653 
 654         for (const auto&amp; entry : unescaped.fields())
 655             escapeAllocation(entry.value);
 656 
 657         if (m_wantEscapees)
 658             m_escapees.add(unescaped.identifier(), WTFMove(unescaped));
 659     }
 660 
 661     void prune()
 662     {
 663         NodeSet reachable;
<span class="line-modified"> 664         for (const auto&amp; entry : m_pointers)</span>
<span class="line-modified"> 665             reachable.addVoid(entry.value);</span>


 666 
 667         // Repeatedly mark as reachable allocations in fields of other
 668         // reachable allocations
 669         {
 670             Vector&lt;Node*&gt; worklist;
 671             worklist.appendRange(reachable.begin(), reachable.end());
 672 
 673             while (!worklist.isEmpty()) {
 674                 Node* identifier = worklist.takeLast();
 675                 Allocation&amp; allocation = m_allocations.find(identifier)-&gt;value;
 676                 for (const auto&amp; entry : allocation.fields()) {
 677                     if (reachable.add(entry.value).isNewEntry)
 678                         worklist.append(entry.value);
 679                 }
 680             }
 681         }
 682 
 683         // Remove unreachable allocations
 684         m_allocations.removeIf(
 685             [&amp;] (const auto&amp; entry) {
</pre>
<hr />
<pre>
 715 
 716         if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
 717             dataLog(&quot;Graph after elimination:\n&quot;);
 718             m_graph.dump();
 719         }
 720 
 721         return true;
 722     }
 723 
 724 private:
 725     bool performSinking()
 726     {
 727         m_graph.computeRefCounts();
 728         m_graph.initializeNodeOwners();
 729         m_graph.ensureSSADominators();
 730         performLivenessAnalysis(m_graph);
 731         performOSRAvailabilityAnalysis(m_graph);
 732         m_combinedLiveness = CombinedLiveness(m_graph);
 733 
 734         CString graphBeforeSinking;
<span class="line-modified"> 735         if (Options::verboseValidationFailure() &amp;&amp; Options::validateGraphAtEachPhase()) {</span>
 736             StringPrintStream out;
 737             m_graph.dump(out);
 738             graphBeforeSinking = out.toCString();
 739         }
 740 
 741         if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
 742             dataLog(&quot;Graph before elimination:\n&quot;);
 743             m_graph.dump();
 744         }
 745 
 746         performAnalysis();
 747 
 748         if (!determineSinkCandidates())
 749             return false;
 750 
 751         if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
 752             for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
 753                 dataLog(&quot;Heap at head of &quot;, *block, &quot;: \n&quot;, m_heapAtHead[block]);
 754                 dataLog(&quot;Heap at tail of &quot;, *block, &quot;: \n&quot;, m_heapAtTail[block]);
 755             }
</pre>
<hr />
<pre>
 783                         node,
 784                         [] (PromotedHeapLocation, LazyNode) { },
 785                         [&amp;] (PromotedHeapLocation) -&gt; Node* {
 786                             return nullptr;
 787                         });
 788                 }
 789 
 790                 if (m_heap == m_heapAtTail[block])
 791                     continue;
 792 
 793                 m_heapAtTail[block] = m_heap;
 794                 changed = true;
 795 
 796                 m_heap.assertIsValid();
 797 
 798                 // We keep only pointers that are live, and only
 799                 // allocations that are either live, pointed to by a
 800                 // live pointer, or (recursively) stored in a field of
 801                 // a live allocation.
 802                 //
<span class="line-modified"> 803                 // This means we can accidentaly leak non-dominating</span>
 804                 // nodes into the successor. However, due to the
 805                 // non-dominance property, we are guaranteed that the
 806                 // successor has at least one predecessor that is not
 807                 // dominated either: this means any reference to a
 808                 // non-dominating allocation in the successor will
 809                 // trigger an escape and get pruned during the merge.
 810                 m_heap.pruneByLiveness(m_combinedLiveness.liveAtTail[block]);
 811 
<span class="line-modified"> 812                 for (BasicBlock* successorBlock : block-&gt;successors())</span>






 813                     m_heapAtHead[successorBlock].merge(m_heap);



 814             }
 815         } while (changed);
 816     }
 817 













 818     template&lt;typename WriteFunctor, typename ResolveFunctor&gt;
 819     void handleNode(
 820         Node* node,
 821         const WriteFunctor&amp; heapWrite,
 822         const ResolveFunctor&amp; heapResolve)
 823     {
 824         m_heap.assertIsValid();
 825         ASSERT(m_heap.takeEscapees().isEmpty());
 826 
 827         Allocation* target = nullptr;
 828         HashMap&lt;PromotedLocationDescriptor, LazyNode&gt; writes;
 829         PromotedLocationDescriptor exactRead;
 830 
 831         switch (node-&gt;op()) {
 832         case NewObject:
 833             target = &amp;m_heap.newAllocation(node, Allocation::Kind::Object);
 834             target-&gt;setStructures(node-&gt;structure());
 835             writes.add(
 836                 StructurePLoc, LazyNode(m_graph.freeze(node-&gt;structure().get())));
 837             break;
</pre>
<hr />
<pre>
 842         case NewAsyncFunction: {
 843             if (isStillValid(node-&gt;castOperand&lt;FunctionExecutable*&gt;())) {
 844                 m_heap.escape(node-&gt;child1().node());
 845                 break;
 846             }
 847 
 848             if (node-&gt;op() == NewGeneratorFunction)
 849                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::GeneratorFunction);
 850             else if (node-&gt;op() == NewAsyncFunction)
 851                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::AsyncFunction);
 852             else if (node-&gt;op() == NewAsyncGeneratorFunction)
 853                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::AsyncGeneratorFunction);
 854             else
 855                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::Function);
 856 
 857             writes.add(FunctionExecutablePLoc, LazyNode(node-&gt;cellOperand()));
 858             writes.add(FunctionActivationPLoc, LazyNode(node-&gt;child1().node()));
 859             break;
 860         }
 861 





 862         case NewRegexp: {
 863             target = &amp;m_heap.newAllocation(node, Allocation::Kind::RegExpObject);
 864 
 865             writes.add(RegExpObjectRegExpPLoc, LazyNode(node-&gt;cellOperand()));
 866             writes.add(RegExpObjectLastIndexPLoc, LazyNode(node-&gt;child1().node()));
 867             break;
 868         }
 869 
 870         case CreateActivation: {
 871             if (isStillValid(node-&gt;castOperand&lt;SymbolTable*&gt;())) {
 872                 m_heap.escape(node-&gt;child1().node());
 873                 break;
 874             }
 875             target = &amp;m_heap.newAllocation(node, Allocation::Kind::Activation);
 876             writes.add(ActivationSymbolTablePLoc, LazyNode(node-&gt;cellOperand()));
 877             writes.add(ActivationScopePLoc, LazyNode(node-&gt;child1().node()));
 878             {
 879                 SymbolTable* symbolTable = node-&gt;castOperand&lt;SymbolTable*&gt;();
 880                 LazyNode initialValue(m_graph.freeze(node-&gt;initializationValueForActivation()));
 881                 for (unsigned offset = 0; offset &lt; symbolTable-&gt;scopeSize(); ++offset) {
</pre>
<hr />
<pre>
1052         case GetRegExpObjectLastIndex:
1053             target = m_heap.onlyLocalAllocation(node-&gt;child1().node());
1054             if (target &amp;&amp; target-&gt;isRegExpObjectAllocation())
1055                 exactRead = RegExpObjectLastIndexPLoc;
1056             else
1057                 m_heap.escape(node-&gt;child1().node());
1058             break;
1059 
1060         case SetRegExpObjectLastIndex:
1061             target = m_heap.onlyLocalAllocation(node-&gt;child1().node());
1062             if (target &amp;&amp; target-&gt;isRegExpObjectAllocation()) {
1063                 writes.add(
1064                     PromotedLocationDescriptor(RegExpObjectLastIndexPLoc),
1065                     LazyNode(node-&gt;child2().node()));
1066             } else {
1067                 m_heap.escape(node-&gt;child1().node());
1068                 m_heap.escape(node-&gt;child2().node());
1069             }
1070             break;
1071 




















1072         case Check:
1073         case CheckVarargs:
1074             m_graph.doToChildren(
1075                 node,
1076                 [&amp;] (Edge edge) {
1077                     if (edge.willNotHaveCheck())
1078                         return;
1079 
1080                     if (alreadyChecked(edge.useKind(), SpecObject))
1081                         return;
1082 
1083                     m_heap.escape(edge.node());
1084                 });
1085             break;
1086 
1087         case MovHint:
1088         case PutHint:
1089             // Handled by OSR availability analysis
1090             break;
1091 
1092         case FilterCallLinkStatus:
<span class="line-modified">1093         case FilterGetByIdStatus:</span>
1094         case FilterPutByIdStatus:
1095         case FilterInByIdStatus:
1096             break;
1097 
1098         default:
1099             m_graph.doToChildren(
1100                 node,
1101                 [&amp;] (Edge edge) {
1102                     m_heap.escape(edge.node());
1103                 });
1104             break;
1105         }
1106 
1107         if (exactRead) {
1108             ASSERT(target);
1109             ASSERT(writes.isEmpty());
1110             if (Node* value = heapResolve(PromotedHeapLocation(target-&gt;identifier(), exactRead))) {
1111                 ASSERT(!value-&gt;replacement());
1112                 node-&gt;replaceWith(m_graph, value);
1113             }
</pre>
<hr />
<pre>
1544             case Allocation::Kind::GeneratorFunction:
1545                 nodeType = NewGeneratorFunction;
1546                 break;
1547             case Allocation::Kind::AsyncGeneratorFunction:
1548                 nodeType = NewAsyncGeneratorFunction;
1549                 break;
1550             case Allocation::Kind::AsyncFunction:
1551                 nodeType = NewAsyncFunction;
1552                 break;
1553             default:
1554                 nodeType = NewFunction;
1555             }
1556 
1557             return m_graph.addNode(
1558                 allocation.identifier()-&gt;prediction(), nodeType,
1559                 where-&gt;origin.withSemantic(
1560                     allocation.identifier()-&gt;origin.semantic),
1561                 OpInfo(executable));
1562         }
1563 









1564         case Allocation::Kind::Activation: {
1565             ObjectMaterializationData* data = m_graph.m_objectMaterializationData.add();
1566             FrozenValue* symbolTable = allocation.identifier()-&gt;cellOperand();
1567 
1568             return m_graph.addNode(
1569                 allocation.identifier()-&gt;prediction(), Node::VarArg, MaterializeCreateActivation,
1570                 where-&gt;origin.withSemantic(
1571                     allocation.identifier()-&gt;origin.semantic),
1572                 OpInfo(symbolTable), OpInfo(data), 0, 0);
1573         }
1574 
1575         case Allocation::Kind::RegExpObject: {
1576             FrozenValue* regExp = allocation.identifier()-&gt;cellOperand();
1577             return m_graph.addNode(
1578                 allocation.identifier()-&gt;prediction(), NewRegexp,
1579                 where-&gt;origin.withSemantic(
1580                     allocation.identifier()-&gt;origin.semantic),
1581                 OpInfo(regExp));
1582         }
1583 
</pre>
<hr />
<pre>
1798                 if (m_sinkCandidates.contains(location.base())) {
1799                     m_insertionSet.insert(
1800                         0,
1801                         location.createHint(
1802                             m_graph, block-&gt;at(0)-&gt;origin.withInvalidExit(), phiDef-&gt;value()));
1803                 }
1804             }
1805 
1806             for (SSACalculator::Def* phiDef : m_allocationSSA.phisForBlock(block)) {
1807                 SSACalculator::Variable* variable = phiDef-&gt;variable();
1808                 m_insertionSet.insert(0, phiDef-&gt;value());
1809 
1810                 Node* identifier = indexToNode[variable-&gt;index()];
1811                 m_escapeeToMaterialization.add(identifier, phiDef-&gt;value());
1812                 bool canExit = false;
1813                 insertOSRHintsForUpdate(
1814                     0, block-&gt;at(0)-&gt;origin, canExit,
1815                     availabilityCalculator.m_availability, identifier, phiDef-&gt;value());
1816 
1817                 for (PromotedHeapLocation location : hintsForPhi[variable-&gt;index()]) {
<span class="line-modified">1818                     if (m_heap.onlyLocalAllocation(location.base())) {</span>
1819                         m_insertionSet.insert(0,
1820                             location.createHint(m_graph, block-&gt;at(0)-&gt;origin.withInvalidExit(), phiDef-&gt;value()));
1821                         m_localMapping.set(location, phiDef-&gt;value());
1822                     }
1823                 }
1824             }
1825 
1826             if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
1827                 dataLog(&quot;Local mapping at &quot;, pointerDump(block), &quot;: &quot;, mapDump(m_localMapping), &quot;\n&quot;);
1828                 dataLog(&quot;Local materializations at &quot;, pointerDump(block), &quot;: &quot;, mapDump(m_escapeeToMaterialization), &quot;\n&quot;);
1829             }
1830 
1831             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
1832                 Node* node = block-&gt;at(nodeIndex);
1833                 bool canExit = true;
1834                 bool nextCanExit = node-&gt;origin.exitOK;
1835                 for (PromotedHeapLocation location : m_locationsForAllocation.get(node)) {
1836                     if (location.kind() != NamedPropertyPLoc)
1837                         continue;
1838 
</pre>
<hr />
<pre>
1931                     case NewObject:
1932                         node-&gt;convertToPhantomNewObject();
1933                         break;
1934 
1935                     case NewFunction:
1936                         node-&gt;convertToPhantomNewFunction();
1937                         break;
1938 
1939                     case NewGeneratorFunction:
1940                         node-&gt;convertToPhantomNewGeneratorFunction();
1941                         break;
1942 
1943                     case NewAsyncGeneratorFunction:
1944                         node-&gt;convertToPhantomNewAsyncGeneratorFunction();
1945                         break;
1946 
1947                     case NewAsyncFunction:
1948                         node-&gt;convertToPhantomNewAsyncFunction();
1949                         break;
1950 




1951                     case CreateActivation:
1952                         node-&gt;convertToPhantomCreateActivation();
1953                         break;
1954 
1955                     case NewRegexp:
1956                         node-&gt;convertToPhantomNewRegexp();
1957                         break;
1958 
1959                     default:
1960                         node-&gt;remove(m_graph);
1961                         break;
1962                     }
1963                 }
1964 
1965                 m_graph.doToChildren(
1966                     node,
1967                     [&amp;] (Edge&amp; edge) {
1968                         edge.setNode(resolve(block, edge.node()));
1969                     });
1970             }
</pre>
<hr />
<pre>
2090         // In order to do this, we say that we need to insert an
2091         // update hint for any availability whose node resolve()s to
2092         // the materialization.
2093         for (auto entry : availability.m_heap) {
2094             if (!entry.value.hasNode())
2095                 continue;
2096             if (m_heap.follow(entry.value.node()) != escapee)
2097                 continue;
2098 
2099             m_insertionSet.insert(
2100                 nodeIndex,
2101                 entry.key.createHint(m_graph, origin.takeValidExit(canExit), materialization));
2102         }
2103 
2104         for (unsigned i = availability.m_locals.size(); i--;) {
2105             if (!availability.m_locals[i].hasNode())
2106                 continue;
2107             if (m_heap.follow(availability.m_locals[i].node()) != escapee)
2108                 continue;
2109 
<span class="line-modified">2110             int operand = availability.m_locals.operandForIndex(i);</span>
2111             m_insertionSet.insertNode(
2112                 nodeIndex, SpecNone, MovHint, origin.takeValidExit(canExit), OpInfo(operand),
2113                 materialization-&gt;defaultEdge());
2114         }
2115     }
2116 
2117     void populateMaterialization(BasicBlock* block, Node* node, Node* escapee)
2118     {
2119         Allocation&amp; allocation = m_heap.getAllocation(escapee);
2120         switch (node-&gt;op()) {
2121         case MaterializeNewObject: {
2122             ObjectMaterializationData&amp; data = node-&gt;objectMaterializationData();
2123             unsigned firstChild = m_graph.m_varArgChildren.size();
2124 
2125             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);
2126 
2127             PromotedHeapLocation structure(StructurePLoc, allocation.identifier());
2128             ASSERT(locations.contains(structure));
2129 
2130             m_graph.m_varArgChildren.append(Edge(resolve(block, structure), KnownCellUse));
</pre>
<hr />
<pre>
2207             break;
2208         }
2209 
2210         case NewFunction:
2211         case NewGeneratorFunction:
2212         case NewAsyncGeneratorFunction:
2213         case NewAsyncFunction: {
2214             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);
2215             ASSERT(locations.size() == 2);
2216 
2217             PromotedHeapLocation executable(FunctionExecutablePLoc, allocation.identifier());
2218             ASSERT_UNUSED(executable, locations.contains(executable));
2219 
2220             PromotedHeapLocation activation(FunctionActivationPLoc, allocation.identifier());
2221             ASSERT(locations.contains(activation));
2222 
2223             node-&gt;child1() = Edge(resolve(block, activation), KnownCellUse);
2224             break;
2225         }
2226 








































2227         case NewRegexp: {
2228             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);
2229             ASSERT(locations.size() == 2);
2230 
2231             PromotedHeapLocation regExp(RegExpObjectRegExpPLoc, allocation.identifier());
2232             ASSERT_UNUSED(regExp, locations.contains(regExp));
2233 
2234             PromotedHeapLocation lastIndex(RegExpObjectLastIndexPLoc, allocation.identifier());
2235             ASSERT(locations.contains(lastIndex));
2236             Node* value = resolve(block, lastIndex);
2237             if (m_sinkCandidates.contains(value))
2238                 node-&gt;child1() = Edge(m_bottom);
2239             else
2240                 node-&gt;child1() = Edge(value);
2241             break;
2242         }
2243 
2244         default:
2245             DFG_CRASH(m_graph, node, &quot;Bad materialize op&quot;);
2246         }
</pre>
<hr />
<pre>
2338                     PutByIdVariant::replace(currentSet, currentOffset));
2339             }
2340 
2341             return m_graph.addNode(
2342                 MultiPutByOffset,
2343                 origin.takeValidExit(canExit),
2344                 OpInfo(data),
2345                 Edge(base, KnownCellUse),
2346                 value-&gt;defaultEdge());
2347         }
2348 
2349         case ClosureVarPLoc: {
2350             return m_graph.addNode(
2351                 PutClosureVar,
2352                 origin.takeValidExit(canExit),
2353                 OpInfo(location.info()),
2354                 Edge(base, KnownCellUse),
2355                 value-&gt;defaultEdge());
2356         }
2357 









2358         case RegExpObjectLastIndexPLoc: {
2359             return m_graph.addNode(
2360                 SetRegExpObjectLastIndex,
2361                 origin.takeValidExit(canExit),
2362                 OpInfo(true),
2363                 Edge(base, KnownCellUse),
2364                 value-&gt;defaultEdge());
2365         }
2366 
2367         default:
2368             DFG_CRASH(m_graph, base, &quot;Bad location kind&quot;);
2369             break;
2370         }
2371 
2372         RELEASE_ASSERT_NOT_REACHED();
2373     }
2374 
2375     void removeICStatusFilters()
2376     {
2377         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
2378             for (Node* node : *block) {
2379                 switch (node-&gt;op()) {
2380                 case FilterCallLinkStatus:
<span class="line-modified">2381                 case FilterGetByIdStatus:</span>
2382                 case FilterPutByIdStatus:
2383                 case FilterInByIdStatus:
2384                     if (node-&gt;child1()-&gt;isPhantomAllocation())
2385                         node-&gt;removeWithoutChecks();
2386                     break;
2387                 default:
2388                     break;
2389                 }
2390             }
2391         }
2392     }
2393 
2394     // This is a great way of asking value-&gt;isStillValid() without having to worry about getting
2395     // different answers. It turns out that this analysis works OK regardless of what this
2396     // returns but breaks badly if this changes its mind for any particular InferredValue. This
2397     // method protects us from that.
2398     bool isStillValid(SymbolTable* value)
2399     {
2400         return m_validInferredValues.add(value, value-&gt;singleton().isStillValid()).iterator-&gt;value;
2401     }
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2015-2020 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGObjectAllocationSinkingPhase.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;DFGBlockMapInlines.h&quot;
  32 #include &quot;DFGClobbersExitState.h&quot;
  33 #include &quot;DFGCombinedLiveness.h&quot;
  34 #include &quot;DFGGraph.h&quot;
  35 #include &quot;DFGInsertionSet.h&quot;
  36 #include &quot;DFGLazyNode.h&quot;
  37 #include &quot;DFGLivenessAnalysisPhase.h&quot;
  38 #include &quot;DFGOSRAvailabilityAnalysisPhase.h&quot;
  39 #include &quot;DFGPhase.h&quot;
  40 #include &quot;DFGPromotedHeapLocation.h&quot;
  41 #include &quot;DFGSSACalculator.h&quot;
  42 #include &quot;DFGValidate.h&quot;
<span class="line-added">  43 #include &quot;JSArrayIterator.h&quot;</span>
  44 #include &quot;JSCInlines.h&quot;
  45 #include &lt;wtf/StdList.h&gt;
  46 
  47 namespace JSC { namespace DFG {
  48 
  49 namespace {
  50 
  51 namespace DFGObjectAllocationSinkingPhaseInternal {
<span class="line-modified">  52 static constexpr bool verbose = false;</span>
  53 }
  54 
  55 // In order to sink object cycles, we use a points-to analysis coupled
  56 // with an escape analysis. This analysis is actually similar to an
  57 // abstract interpreter focused on local allocations and ignoring
  58 // everything else.
  59 //
  60 // We represent the local heap using two mappings:
  61 //
  62 // - A set of the local allocations present in the function, where
  63 //   each of those have a further mapping from
  64 //   PromotedLocationDescriptor to local allocations they must point
  65 //   to.
  66 //
  67 // - A &quot;pointer&quot; mapping from nodes to local allocations, if they must
  68 //   be equal to said local allocation and are currently live. This
  69 //   can be because the node is the actual node that created the
  70 //   allocation, or any other node that must currently point to it -
  71 //   we don&#39;t make a difference.
  72 //
</pre>
<hr />
<pre>
 124 //
 125 // Block #3
 126 //  -: Return(@0)
 127 //
 128 // we should think of the heaps at tail of blocks #1 and #2 as being
 129 // exactly the same, even though one has @0.x pointing to @1 and the
 130 // other has @0.x pointing to @2, because in essence this should not
 131 // be different from the graph where we hoisted @1 and @2 into a
 132 // single allocation in block #0. We currently will not handle this
 133 // case, because we merge allocations based on the node they are
 134 // coming from, but this is only a technicality for the sake of
 135 // simplicity that shouldn&#39;t hide the deeper idea outlined here.
 136 
 137 class Allocation {
 138 public:
 139     // We use Escaped as a special allocation kind because when we
 140     // decide to sink an allocation, we still need to keep track of it
 141     // once it is escaped if it still has pointers to it in order to
 142     // replace any use of those pointers by the corresponding
 143     // materialization
<span class="line-modified"> 144     enum class Kind { Escaped, Object, Activation, Function, GeneratorFunction, AsyncFunction, AsyncGeneratorFunction, InternalFieldObject, RegExpObject };</span>
 145 
 146     using Fields = HashMap&lt;PromotedLocationDescriptor, Node*&gt;;
 147 
 148     explicit Allocation(Node* identifier = nullptr, Kind kind = Kind::Escaped)
 149         : m_identifier(identifier)
 150         , m_kind(kind)
 151     {
 152     }
 153 
 154 
 155     const Fields&amp; fields() const
 156     {
 157         return m_fields;
 158     }
 159 
 160     Fields&amp; fields()
 161     {
 162         return m_fields;
 163     }
 164 
</pre>
<hr />
<pre>
 229     bool isEscapedAllocation() const
 230     {
 231         return kind() == Kind::Escaped;
 232     }
 233 
 234     bool isObjectAllocation() const
 235     {
 236         return m_kind == Kind::Object;
 237     }
 238 
 239     bool isActivationAllocation() const
 240     {
 241         return m_kind == Kind::Activation;
 242     }
 243 
 244     bool isFunctionAllocation() const
 245     {
 246         return m_kind == Kind::Function || m_kind == Kind::GeneratorFunction || m_kind == Kind::AsyncFunction;
 247     }
 248 
<span class="line-added"> 249     bool isInternalFieldObjectAllocation() const</span>
<span class="line-added"> 250     {</span>
<span class="line-added"> 251         return m_kind == Kind::InternalFieldObject;</span>
<span class="line-added"> 252     }</span>
<span class="line-added"> 253 </span>
 254     bool isRegExpObjectAllocation() const
 255     {
 256         return m_kind == Kind::RegExpObject;
 257     }
 258 
 259     bool operator==(const Allocation&amp; other) const
 260     {
 261         return m_identifier == other.m_identifier
 262             &amp;&amp; m_kind == other.m_kind
 263             &amp;&amp; m_fields == other.m_fields
 264             &amp;&amp; m_structures == other.m_structures;
 265     }
 266 
 267     bool operator!=(const Allocation&amp; other) const
 268     {
 269         return !(*this == other);
 270     }
 271 
 272     void dump(PrintStream&amp; out) const
 273     {
</pre>
<hr />
<pre>
 280         case Kind::Escaped:
 281             out.print(&quot;Escaped&quot;);
 282             break;
 283 
 284         case Kind::Object:
 285             out.print(&quot;Object&quot;);
 286             break;
 287 
 288         case Kind::Function:
 289             out.print(&quot;Function&quot;);
 290             break;
 291 
 292         case Kind::GeneratorFunction:
 293             out.print(&quot;GeneratorFunction&quot;);
 294             break;
 295 
 296         case Kind::AsyncFunction:
 297             out.print(&quot;AsyncFunction&quot;);
 298             break;
 299 
<span class="line-added"> 300         case Kind::InternalFieldObject:</span>
<span class="line-added"> 301             out.print(&quot;InternalFieldObject&quot;);</span>
<span class="line-added"> 302             break;</span>
<span class="line-added"> 303 </span>
 304         case Kind::AsyncGeneratorFunction:
 305             out.print(&quot;AsyncGeneratorFunction&quot;);
 306             break;
 307 
 308         case Kind::Activation:
 309             out.print(&quot;Activation&quot;);
 310             break;
 311 
 312         case Kind::RegExpObject:
 313             out.print(&quot;RegExpObject&quot;);
 314             break;
 315         }
 316         out.print(&quot;Allocation(&quot;);
 317         if (!m_structures.isEmpty())
 318             out.print(inContext(m_structures.toStructureSet(), context));
 319         if (!m_fields.isEmpty()) {
 320             if (!m_structures.isEmpty())
 321                 out.print(&quot;, &quot;);
 322             out.print(mapDump(m_fields, &quot; =&gt; #&quot;, &quot;, &quot;));
 323         }
</pre>
<hr />
<pre>
 385     }
 386 
 387     void newPointer(Node* node, Node* identifier)
 388     {
 389         ASSERT(!m_allocations.contains(node) &amp;&amp; !m_pointers.contains(node));
 390         ASSERT(isAllocation(identifier));
 391         m_pointers.add(node, identifier);
 392     }
 393 
 394     // follow solves the points-to problem. Given a live node, which
 395     // may be either an allocation itself or a heap read (e.g. a
 396     // GetByOffset node), it returns the corresponding allocation
 397     // node, if there is one. If the argument node is neither an
 398     // allocation or a heap read, or may point to different nodes,
 399     // nullptr will be returned. Note that a node that points to
 400     // different nodes can never point to an unescaped local
 401     // allocation.
 402     Node* follow(Node* node) const
 403     {
 404         auto iter = m_pointers.find(node);
<span class="line-modified"> 405         ASSERT(iter == m_pointers.end() || (!iter-&gt;value || m_allocations.contains(iter-&gt;value)));</span>
 406         return iter == m_pointers.end() ? nullptr : iter-&gt;value;
 407     }
 408 
 409     Node* follow(PromotedHeapLocation location) const
 410     {
 411         const Allocation&amp; base = m_allocations.find(location.base())-&gt;value;
 412         auto iter = base.fields().find(location.descriptor());

 413         if (iter == base.fields().end())
 414             return nullptr;
 415 
 416         return iter-&gt;value;
 417     }
 418 
 419     // onlyLocalAllocation find the corresponding allocation metadata
 420     // for any live node. onlyLocalAllocation(node) is essentially
 421     // getAllocation(follow(node)), with appropriate null handling.
 422     Allocation* onlyLocalAllocation(Node* node)
 423     {
 424         Node* identifier = follow(node);
 425         if (!identifier)
 426             return nullptr;
 427 
 428         return &amp;getAllocation(identifier);
 429     }
 430 
 431     Allocation* onlyLocalAllocation(PromotedHeapLocation location)
 432     {
 433         Node* identifier = follow(location);
 434         if (!identifier)
 435             return nullptr;
 436 
 437         return &amp;getAllocation(identifier);
 438     }
 439 
<span class="line-added"> 440     bool isUnescapedAllocation(Node* identifier) const</span>
<span class="line-added"> 441     {</span>
<span class="line-added"> 442         auto iter = m_allocations.find(identifier);</span>
<span class="line-added"> 443         return iter != m_allocations.end() &amp;&amp; !iter-&gt;value.isEscapedAllocation();</span>
<span class="line-added"> 444     }</span>
<span class="line-added"> 445 </span>
 446     // This allows us to store the escapees only when necessary. If
 447     // set, the current escapees can be retrieved at any time using
 448     // takeEscapees(), which will clear the cached set of escapees;
 449     // otherwise the heap won&#39;t remember escaping allocations.
 450     void setWantEscapees()
 451     {
 452         m_wantEscapees = true;
 453     }
 454 
 455     HashMap&lt;Node*, Allocation&gt; takeEscapees()
 456     {
 457         return WTFMove(m_escapees);
 458     }
 459 
 460     void escape(Node* node)
 461     {
 462         Node* identifier = follow(node);
 463         if (!identifier)
 464             return;
 465 
</pre>
<hr />
<pre>
 487 
 488             // If we have it and they don&#39;t, it died for them but we
 489             // are keeping it alive from another field somewhere.
 490             // There is nothing to do - we will be escaped
 491             // automatically when we handle that other field.
 492             // This will also happen for allocation that we have and
 493             // they don&#39;t, and all of those will get pruned.
 494             if (allocationIter == other.m_allocations.end())
 495                 continue;
 496 
 497             if (allocationEntry.value.kind() != allocationIter-&gt;value.kind()) {
 498                 toEscape.addVoid(allocationEntry.key);
 499                 for (const auto&amp; fieldEntry : allocationIter-&gt;value.fields())
 500                     toEscape.addVoid(fieldEntry.value);
 501             } else {
 502                 mergePointerSets(allocationEntry.value.fields(), allocationIter-&gt;value.fields(), toEscape);
 503                 allocationEntry.value.mergeStructures(allocationIter-&gt;value.structures());
 504             }
 505         }
 506 
<span class="line-modified"> 507         {</span>
<span class="line-added"> 508             // This works because we won&#39;t collect all pointers until all of our predecessors</span>
<span class="line-added"> 509             // merge their pointer sets with ours. That allows us to see the full state of the</span>
<span class="line-added"> 510             // world during our fixpoint analysis. Once we have the full set of pointers, we</span>
<span class="line-added"> 511             // only mark pointers to TOP, so we will eventually converge.</span>
<span class="line-added"> 512             for (auto entry : other.m_pointers) {</span>
<span class="line-added"> 513                 auto addResult = m_pointers.add(entry.key, entry.value);</span>
<span class="line-added"> 514                 if (addResult.iterator-&gt;value != entry.value) {</span>
<span class="line-added"> 515                     if (addResult.iterator-&gt;value) {</span>
<span class="line-added"> 516                         toEscape.addVoid(addResult.iterator-&gt;value);</span>
<span class="line-added"> 517                         addResult.iterator-&gt;value = nullptr;</span>
<span class="line-added"> 518                     }</span>
<span class="line-added"> 519                     if (entry.value)</span>
<span class="line-added"> 520                         toEscape.addVoid(entry.value);</span>
<span class="line-added"> 521                 }</span>
<span class="line-added"> 522             }</span>
<span class="line-added"> 523             // This allows us to rule out pointers for graphs like this:</span>
<span class="line-added"> 524             // bb#0</span>
<span class="line-added"> 525             // branch #1, #2</span>
<span class="line-added"> 526             // #1:</span>
<span class="line-added"> 527             // x = pointer A</span>
<span class="line-added"> 528             // jump #3</span>
<span class="line-added"> 529             // #2:</span>
<span class="line-added"> 530             // y = pointer B</span>
<span class="line-added"> 531             // jump #3</span>
<span class="line-added"> 532             // #3:</span>
<span class="line-added"> 533             // ...</span>
<span class="line-added"> 534             //</span>
<span class="line-added"> 535             // When we merge state at #3, we&#39;ll very likely prune away the x and y pointer,</span>
<span class="line-added"> 536             // since they&#39;re not live. But if they do happen to make it to this merge function, when</span>
<span class="line-added"> 537             // #3 merges with #2 and #1, it&#39;ll eventually rule out x and y as not existing</span>
<span class="line-added"> 538             // in the other, and therefore not existing in #3, which is the desired behavior.</span>
<span class="line-added"> 539             //</span>
<span class="line-added"> 540             // This also is necessary for a graph like this:</span>
<span class="line-added"> 541             // #0</span>
<span class="line-added"> 542             // o = {}</span>
<span class="line-added"> 543             // o2 = {}</span>
<span class="line-added"> 544             // jump #1</span>
<span class="line-added"> 545             //</span>
<span class="line-added"> 546             // #1</span>
<span class="line-added"> 547             // o.f = o2</span>
<span class="line-added"> 548             // effects()</span>
<span class="line-added"> 549             // x = o.f</span>
<span class="line-added"> 550             // escape(o)</span>
<span class="line-added"> 551             // branch #2, #1</span>
<span class="line-added"> 552             //</span>
<span class="line-added"> 553             // #2</span>
<span class="line-added"> 554             // x cannot be o2 here, it has to be TOP</span>
<span class="line-added"> 555             // ...</span>
<span class="line-added"> 556             //</span>
<span class="line-added"> 557             // On the first fixpoint iteration, we might think that x is o2 at the head</span>
<span class="line-added"> 558             // of #2. However, when we fixpoint our analysis, we determine that o gets</span>
<span class="line-added"> 559             // escaped. This means that when we fixpoint, x will eventually not be a pointer.</span>
<span class="line-added"> 560             // When we merge again here, we&#39;ll notice and mark o2 as escaped.</span>
<span class="line-added"> 561             for (auto&amp; entry : m_pointers) {</span>
<span class="line-added"> 562                 if (!other.m_pointers.contains(entry.key)) {</span>
<span class="line-added"> 563                     if (entry.value) {</span>
<span class="line-added"> 564                         toEscape.addVoid(entry.value);</span>
<span class="line-added"> 565                         entry.value = nullptr;</span>
<span class="line-added"> 566                         ASSERT(!m_pointers.find(entry.key)-&gt;value);</span>
<span class="line-added"> 567                     }</span>
<span class="line-added"> 568                 }</span>
<span class="line-added"> 569             }</span>
<span class="line-added"> 570         }</span>
 571 
 572         for (Node* identifier : toEscape)
 573             escapeAllocation(identifier);
 574 
<span class="line-modified"> 575         if (ASSERT_ENABLED) {</span>
 576             for (const auto&amp; entry : m_allocations)
 577                 ASSERT_UNUSED(entry, entry.value.isEscapedAllocation() || other.m_allocations.contains(entry.key));
 578         }
 579 
 580         // If there is no remaining pointer to an allocation, we can
 581         // remove it. This should only happen for escaped allocations,
 582         // because we only merge liveness-pruned heaps in the first
 583         // place.
 584         prune();
 585 
 586         assertIsValid();
 587     }
 588 
 589     void pruneByLiveness(const NodeSet&amp; live)
 590     {
 591         m_pointers.removeIf(
 592             [&amp;] (const auto&amp; entry) {
 593                 return !live.contains(entry.key);
 594             });
 595         prune();
 596     }
 597 
 598     void assertIsValid() const
 599     {
<span class="line-modified"> 600         if (!ASSERT_ENABLED)</span>
 601             return;
 602 
 603         // Pointers should point to an actual allocation
 604         for (const auto&amp; entry : m_pointers) {
<span class="line-modified"> 605             if (entry.value)</span>
<span class="line-modified"> 606                 ASSERT(m_allocations.contains(entry.value));</span>
 607         }
 608 
 609         for (const auto&amp; allocationEntry : m_allocations) {
 610             // Fields should point to an actual allocation
 611             for (const auto&amp; fieldEntry : allocationEntry.value.fields()) {
 612                 ASSERT_UNUSED(fieldEntry, fieldEntry.value);
 613                 ASSERT(m_allocations.contains(fieldEntry.value));
 614             }
 615         }
 616     }
 617 
 618     bool operator==(const LocalHeap&amp; other) const
 619     {
 620         assertIsValid();
 621         other.assertIsValid();
 622         return m_allocations == other.m_allocations
 623             &amp;&amp; m_pointers == other.m_pointers;
 624     }
 625 
 626     bool operator!=(const LocalHeap&amp; other) const
 627     {
 628         return !(*this == other);
 629     }
 630 
 631     const HashMap&lt;Node*, Allocation&gt;&amp; allocations() const
 632     {
 633         return m_allocations;
 634     }
 635 





 636     void dump(PrintStream&amp; out) const
 637     {
 638         out.print(&quot;  Allocations:\n&quot;);
 639         for (const auto&amp; entry : m_allocations)
 640             out.print(&quot;    #&quot;, entry.key, &quot;: &quot;, entry.value, &quot;\n&quot;);
 641         out.print(&quot;  Pointers:\n&quot;);
<span class="line-modified"> 642         for (const auto&amp; entry : m_pointers) {</span>
<span class="line-modified"> 643             out.print(&quot;    &quot;, entry.key, &quot; =&gt; #&quot;);</span>
<span class="line-added"> 644             if (entry.value)</span>
<span class="line-added"> 645                 out.print(entry.value, &quot;\n&quot;);</span>
<span class="line-added"> 646             else</span>
<span class="line-added"> 647                 out.print(&quot;TOP\n&quot;);</span>
<span class="line-added"> 648         }</span>
 649     }
 650 
 651     bool reached() const
 652     {
 653         return m_reached;
 654     }
 655 
 656     void setReached()
 657     {
 658         m_reached = true;
 659     }
 660 
 661 private:
 662     // When we merge two heaps, we escape all fields of allocations,
 663     // unless they point to the same thing in both heaps.
 664     // The reason for this is that it allows us not to do extra work
 665     // for diamond graphs where we would otherwise have to check
 666     // whether we have a single definition or not, which would be
 667     // cumbersome.
 668     //
</pre>
<hr />
<pre>
 722 
 723     void escapeAllocation(Node* identifier)
 724     {
 725         Allocation&amp; allocation = getAllocation(identifier);
 726         if (allocation.isEscapedAllocation())
 727             return;
 728 
 729         Allocation unescaped = WTFMove(allocation);
 730         allocation = Allocation(unescaped.identifier(), Allocation::Kind::Escaped);
 731 
 732         for (const auto&amp; entry : unescaped.fields())
 733             escapeAllocation(entry.value);
 734 
 735         if (m_wantEscapees)
 736             m_escapees.add(unescaped.identifier(), WTFMove(unescaped));
 737     }
 738 
 739     void prune()
 740     {
 741         NodeSet reachable;
<span class="line-modified"> 742         for (const auto&amp; entry : m_pointers) {</span>
<span class="line-modified"> 743             if (entry.value)</span>
<span class="line-added"> 744                 reachable.addVoid(entry.value);</span>
<span class="line-added"> 745         }</span>
 746 
 747         // Repeatedly mark as reachable allocations in fields of other
 748         // reachable allocations
 749         {
 750             Vector&lt;Node*&gt; worklist;
 751             worklist.appendRange(reachable.begin(), reachable.end());
 752 
 753             while (!worklist.isEmpty()) {
 754                 Node* identifier = worklist.takeLast();
 755                 Allocation&amp; allocation = m_allocations.find(identifier)-&gt;value;
 756                 for (const auto&amp; entry : allocation.fields()) {
 757                     if (reachable.add(entry.value).isNewEntry)
 758                         worklist.append(entry.value);
 759                 }
 760             }
 761         }
 762 
 763         // Remove unreachable allocations
 764         m_allocations.removeIf(
 765             [&amp;] (const auto&amp; entry) {
</pre>
<hr />
<pre>
 795 
 796         if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
 797             dataLog(&quot;Graph after elimination:\n&quot;);
 798             m_graph.dump();
 799         }
 800 
 801         return true;
 802     }
 803 
 804 private:
 805     bool performSinking()
 806     {
 807         m_graph.computeRefCounts();
 808         m_graph.initializeNodeOwners();
 809         m_graph.ensureSSADominators();
 810         performLivenessAnalysis(m_graph);
 811         performOSRAvailabilityAnalysis(m_graph);
 812         m_combinedLiveness = CombinedLiveness(m_graph);
 813 
 814         CString graphBeforeSinking;
<span class="line-modified"> 815         if (UNLIKELY(Options::verboseValidationFailure() &amp;&amp; Options::validateGraphAtEachPhase())) {</span>
 816             StringPrintStream out;
 817             m_graph.dump(out);
 818             graphBeforeSinking = out.toCString();
 819         }
 820 
 821         if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
 822             dataLog(&quot;Graph before elimination:\n&quot;);
 823             m_graph.dump();
 824         }
 825 
 826         performAnalysis();
 827 
 828         if (!determineSinkCandidates())
 829             return false;
 830 
 831         if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
 832             for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
 833                 dataLog(&quot;Heap at head of &quot;, *block, &quot;: \n&quot;, m_heapAtHead[block]);
 834                 dataLog(&quot;Heap at tail of &quot;, *block, &quot;: \n&quot;, m_heapAtTail[block]);
 835             }
</pre>
<hr />
<pre>
 863                         node,
 864                         [] (PromotedHeapLocation, LazyNode) { },
 865                         [&amp;] (PromotedHeapLocation) -&gt; Node* {
 866                             return nullptr;
 867                         });
 868                 }
 869 
 870                 if (m_heap == m_heapAtTail[block])
 871                     continue;
 872 
 873                 m_heapAtTail[block] = m_heap;
 874                 changed = true;
 875 
 876                 m_heap.assertIsValid();
 877 
 878                 // We keep only pointers that are live, and only
 879                 // allocations that are either live, pointed to by a
 880                 // live pointer, or (recursively) stored in a field of
 881                 // a live allocation.
 882                 //
<span class="line-modified"> 883                 // This means we can accidentally leak non-dominating</span>
 884                 // nodes into the successor. However, due to the
 885                 // non-dominance property, we are guaranteed that the
 886                 // successor has at least one predecessor that is not
 887                 // dominated either: this means any reference to a
 888                 // non-dominating allocation in the successor will
 889                 // trigger an escape and get pruned during the merge.
 890                 m_heap.pruneByLiveness(m_combinedLiveness.liveAtTail[block]);
 891 
<span class="line-modified"> 892                 for (BasicBlock* successorBlock : block-&gt;successors()) {</span>
<span class="line-added"> 893                     // FIXME: Maybe we should:</span>
<span class="line-added"> 894                     // 1. Store the liveness pruned heap as part of m_heapAtTail</span>
<span class="line-added"> 895                     // 2. Move this code above where we make block merge with</span>
<span class="line-added"> 896                     // its predecessors before walking the block forward.</span>
<span class="line-added"> 897                     // https://bugs.webkit.org/show_bug.cgi?id=206041</span>
<span class="line-added"> 898                     LocalHeap heap = m_heapAtHead[successorBlock];</span>
 899                     m_heapAtHead[successorBlock].merge(m_heap);
<span class="line-added"> 900                     if (heap != m_heapAtHead[successorBlock])</span>
<span class="line-added"> 901                         changed = true;</span>
<span class="line-added"> 902                 }</span>
 903             }
 904         } while (changed);
 905     }
 906 
<span class="line-added"> 907     template&lt;typename InternalFieldClass&gt;</span>
<span class="line-added"> 908     Allocation* handleInternalFieldClass(Node* node, HashMap&lt;PromotedLocationDescriptor, LazyNode&gt;&amp; writes)</span>
<span class="line-added"> 909     {</span>
<span class="line-added"> 910         Allocation* result = &amp;m_heap.newAllocation(node, Allocation::Kind::InternalFieldObject);</span>
<span class="line-added"> 911         writes.add(StructurePLoc, LazyNode(m_graph.freeze(node-&gt;structure().get())));</span>
<span class="line-added"> 912         auto initialValues = InternalFieldClass::initialValues();</span>
<span class="line-added"> 913         static_assert(initialValues.size() == InternalFieldClass::numberOfInternalFields);</span>
<span class="line-added"> 914         for (unsigned index = 0; index &lt; initialValues.size(); ++index)</span>
<span class="line-added"> 915             writes.add(PromotedLocationDescriptor(InternalFieldObjectPLoc, index), LazyNode(m_graph.freeze(initialValues[index])));</span>
<span class="line-added"> 916 </span>
<span class="line-added"> 917         return result;</span>
<span class="line-added"> 918     }</span>
<span class="line-added"> 919 </span>
 920     template&lt;typename WriteFunctor, typename ResolveFunctor&gt;
 921     void handleNode(
 922         Node* node,
 923         const WriteFunctor&amp; heapWrite,
 924         const ResolveFunctor&amp; heapResolve)
 925     {
 926         m_heap.assertIsValid();
 927         ASSERT(m_heap.takeEscapees().isEmpty());
 928 
 929         Allocation* target = nullptr;
 930         HashMap&lt;PromotedLocationDescriptor, LazyNode&gt; writes;
 931         PromotedLocationDescriptor exactRead;
 932 
 933         switch (node-&gt;op()) {
 934         case NewObject:
 935             target = &amp;m_heap.newAllocation(node, Allocation::Kind::Object);
 936             target-&gt;setStructures(node-&gt;structure());
 937             writes.add(
 938                 StructurePLoc, LazyNode(m_graph.freeze(node-&gt;structure().get())));
 939             break;
</pre>
<hr />
<pre>
 944         case NewAsyncFunction: {
 945             if (isStillValid(node-&gt;castOperand&lt;FunctionExecutable*&gt;())) {
 946                 m_heap.escape(node-&gt;child1().node());
 947                 break;
 948             }
 949 
 950             if (node-&gt;op() == NewGeneratorFunction)
 951                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::GeneratorFunction);
 952             else if (node-&gt;op() == NewAsyncFunction)
 953                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::AsyncFunction);
 954             else if (node-&gt;op() == NewAsyncGeneratorFunction)
 955                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::AsyncGeneratorFunction);
 956             else
 957                 target = &amp;m_heap.newAllocation(node, Allocation::Kind::Function);
 958 
 959             writes.add(FunctionExecutablePLoc, LazyNode(node-&gt;cellOperand()));
 960             writes.add(FunctionActivationPLoc, LazyNode(node-&gt;child1().node()));
 961             break;
 962         }
 963 
<span class="line-added"> 964         case NewArrayIterator: {</span>
<span class="line-added"> 965             target = handleInternalFieldClass&lt;JSArrayIterator&gt;(node, writes);</span>
<span class="line-added"> 966             break;</span>
<span class="line-added"> 967         }</span>
<span class="line-added"> 968 </span>
 969         case NewRegexp: {
 970             target = &amp;m_heap.newAllocation(node, Allocation::Kind::RegExpObject);
 971 
 972             writes.add(RegExpObjectRegExpPLoc, LazyNode(node-&gt;cellOperand()));
 973             writes.add(RegExpObjectLastIndexPLoc, LazyNode(node-&gt;child1().node()));
 974             break;
 975         }
 976 
 977         case CreateActivation: {
 978             if (isStillValid(node-&gt;castOperand&lt;SymbolTable*&gt;())) {
 979                 m_heap.escape(node-&gt;child1().node());
 980                 break;
 981             }
 982             target = &amp;m_heap.newAllocation(node, Allocation::Kind::Activation);
 983             writes.add(ActivationSymbolTablePLoc, LazyNode(node-&gt;cellOperand()));
 984             writes.add(ActivationScopePLoc, LazyNode(node-&gt;child1().node()));
 985             {
 986                 SymbolTable* symbolTable = node-&gt;castOperand&lt;SymbolTable*&gt;();
 987                 LazyNode initialValue(m_graph.freeze(node-&gt;initializationValueForActivation()));
 988                 for (unsigned offset = 0; offset &lt; symbolTable-&gt;scopeSize(); ++offset) {
</pre>
<hr />
<pre>
1159         case GetRegExpObjectLastIndex:
1160             target = m_heap.onlyLocalAllocation(node-&gt;child1().node());
1161             if (target &amp;&amp; target-&gt;isRegExpObjectAllocation())
1162                 exactRead = RegExpObjectLastIndexPLoc;
1163             else
1164                 m_heap.escape(node-&gt;child1().node());
1165             break;
1166 
1167         case SetRegExpObjectLastIndex:
1168             target = m_heap.onlyLocalAllocation(node-&gt;child1().node());
1169             if (target &amp;&amp; target-&gt;isRegExpObjectAllocation()) {
1170                 writes.add(
1171                     PromotedLocationDescriptor(RegExpObjectLastIndexPLoc),
1172                     LazyNode(node-&gt;child2().node()));
1173             } else {
1174                 m_heap.escape(node-&gt;child1().node());
1175                 m_heap.escape(node-&gt;child2().node());
1176             }
1177             break;
1178 
<span class="line-added">1179         case GetInternalField: {</span>
<span class="line-added">1180             target = m_heap.onlyLocalAllocation(node-&gt;child1().node());</span>
<span class="line-added">1181             if (target &amp;&amp; target-&gt;isInternalFieldObjectAllocation())</span>
<span class="line-added">1182                 exactRead = PromotedLocationDescriptor(InternalFieldObjectPLoc, node-&gt;internalFieldIndex());</span>
<span class="line-added">1183             else</span>
<span class="line-added">1184                 m_heap.escape(node-&gt;child1().node());</span>
<span class="line-added">1185             break;</span>
<span class="line-added">1186         }</span>
<span class="line-added">1187 </span>
<span class="line-added">1188         case PutInternalField: {</span>
<span class="line-added">1189             target = m_heap.onlyLocalAllocation(node-&gt;child1().node());</span>
<span class="line-added">1190             if (target &amp;&amp; target-&gt;isInternalFieldObjectAllocation())</span>
<span class="line-added">1191                 writes.add(PromotedLocationDescriptor(InternalFieldObjectPLoc, node-&gt;internalFieldIndex()), LazyNode(node-&gt;child2().node()));</span>
<span class="line-added">1192             else {</span>
<span class="line-added">1193                 m_heap.escape(node-&gt;child1().node());</span>
<span class="line-added">1194                 m_heap.escape(node-&gt;child2().node());</span>
<span class="line-added">1195             }</span>
<span class="line-added">1196             break;</span>
<span class="line-added">1197         }</span>
<span class="line-added">1198 </span>
1199         case Check:
1200         case CheckVarargs:
1201             m_graph.doToChildren(
1202                 node,
1203                 [&amp;] (Edge edge) {
1204                     if (edge.willNotHaveCheck())
1205                         return;
1206 
1207                     if (alreadyChecked(edge.useKind(), SpecObject))
1208                         return;
1209 
1210                     m_heap.escape(edge.node());
1211                 });
1212             break;
1213 
1214         case MovHint:
1215         case PutHint:
1216             // Handled by OSR availability analysis
1217             break;
1218 
1219         case FilterCallLinkStatus:
<span class="line-modified">1220         case FilterGetByStatus:</span>
1221         case FilterPutByIdStatus:
1222         case FilterInByIdStatus:
1223             break;
1224 
1225         default:
1226             m_graph.doToChildren(
1227                 node,
1228                 [&amp;] (Edge edge) {
1229                     m_heap.escape(edge.node());
1230                 });
1231             break;
1232         }
1233 
1234         if (exactRead) {
1235             ASSERT(target);
1236             ASSERT(writes.isEmpty());
1237             if (Node* value = heapResolve(PromotedHeapLocation(target-&gt;identifier(), exactRead))) {
1238                 ASSERT(!value-&gt;replacement());
1239                 node-&gt;replaceWith(m_graph, value);
1240             }
</pre>
<hr />
<pre>
1671             case Allocation::Kind::GeneratorFunction:
1672                 nodeType = NewGeneratorFunction;
1673                 break;
1674             case Allocation::Kind::AsyncGeneratorFunction:
1675                 nodeType = NewAsyncGeneratorFunction;
1676                 break;
1677             case Allocation::Kind::AsyncFunction:
1678                 nodeType = NewAsyncFunction;
1679                 break;
1680             default:
1681                 nodeType = NewFunction;
1682             }
1683 
1684             return m_graph.addNode(
1685                 allocation.identifier()-&gt;prediction(), nodeType,
1686                 where-&gt;origin.withSemantic(
1687                     allocation.identifier()-&gt;origin.semantic),
1688                 OpInfo(executable));
1689         }
1690 
<span class="line-added">1691         case Allocation::Kind::InternalFieldObject: {</span>
<span class="line-added">1692             ObjectMaterializationData* data = m_graph.m_objectMaterializationData.add();</span>
<span class="line-added">1693             return m_graph.addNode(</span>
<span class="line-added">1694                 allocation.identifier()-&gt;prediction(), Node::VarArg, MaterializeNewInternalFieldObject,</span>
<span class="line-added">1695                 where-&gt;origin.withSemantic(</span>
<span class="line-added">1696                     allocation.identifier()-&gt;origin.semantic),</span>
<span class="line-added">1697                 OpInfo(allocation.identifier()-&gt;structure()), OpInfo(data), 0, 0);</span>
<span class="line-added">1698         }</span>
<span class="line-added">1699 </span>
1700         case Allocation::Kind::Activation: {
1701             ObjectMaterializationData* data = m_graph.m_objectMaterializationData.add();
1702             FrozenValue* symbolTable = allocation.identifier()-&gt;cellOperand();
1703 
1704             return m_graph.addNode(
1705                 allocation.identifier()-&gt;prediction(), Node::VarArg, MaterializeCreateActivation,
1706                 where-&gt;origin.withSemantic(
1707                     allocation.identifier()-&gt;origin.semantic),
1708                 OpInfo(symbolTable), OpInfo(data), 0, 0);
1709         }
1710 
1711         case Allocation::Kind::RegExpObject: {
1712             FrozenValue* regExp = allocation.identifier()-&gt;cellOperand();
1713             return m_graph.addNode(
1714                 allocation.identifier()-&gt;prediction(), NewRegexp,
1715                 where-&gt;origin.withSemantic(
1716                     allocation.identifier()-&gt;origin.semantic),
1717                 OpInfo(regExp));
1718         }
1719 
</pre>
<hr />
<pre>
1934                 if (m_sinkCandidates.contains(location.base())) {
1935                     m_insertionSet.insert(
1936                         0,
1937                         location.createHint(
1938                             m_graph, block-&gt;at(0)-&gt;origin.withInvalidExit(), phiDef-&gt;value()));
1939                 }
1940             }
1941 
1942             for (SSACalculator::Def* phiDef : m_allocationSSA.phisForBlock(block)) {
1943                 SSACalculator::Variable* variable = phiDef-&gt;variable();
1944                 m_insertionSet.insert(0, phiDef-&gt;value());
1945 
1946                 Node* identifier = indexToNode[variable-&gt;index()];
1947                 m_escapeeToMaterialization.add(identifier, phiDef-&gt;value());
1948                 bool canExit = false;
1949                 insertOSRHintsForUpdate(
1950                     0, block-&gt;at(0)-&gt;origin, canExit,
1951                     availabilityCalculator.m_availability, identifier, phiDef-&gt;value());
1952 
1953                 for (PromotedHeapLocation location : hintsForPhi[variable-&gt;index()]) {
<span class="line-modified">1954                     if (m_heap.isUnescapedAllocation(location.base())) {</span>
1955                         m_insertionSet.insert(0,
1956                             location.createHint(m_graph, block-&gt;at(0)-&gt;origin.withInvalidExit(), phiDef-&gt;value()));
1957                         m_localMapping.set(location, phiDef-&gt;value());
1958                     }
1959                 }
1960             }
1961 
1962             if (DFGObjectAllocationSinkingPhaseInternal::verbose) {
1963                 dataLog(&quot;Local mapping at &quot;, pointerDump(block), &quot;: &quot;, mapDump(m_localMapping), &quot;\n&quot;);
1964                 dataLog(&quot;Local materializations at &quot;, pointerDump(block), &quot;: &quot;, mapDump(m_escapeeToMaterialization), &quot;\n&quot;);
1965             }
1966 
1967             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
1968                 Node* node = block-&gt;at(nodeIndex);
1969                 bool canExit = true;
1970                 bool nextCanExit = node-&gt;origin.exitOK;
1971                 for (PromotedHeapLocation location : m_locationsForAllocation.get(node)) {
1972                     if (location.kind() != NamedPropertyPLoc)
1973                         continue;
1974 
</pre>
<hr />
<pre>
2067                     case NewObject:
2068                         node-&gt;convertToPhantomNewObject();
2069                         break;
2070 
2071                     case NewFunction:
2072                         node-&gt;convertToPhantomNewFunction();
2073                         break;
2074 
2075                     case NewGeneratorFunction:
2076                         node-&gt;convertToPhantomNewGeneratorFunction();
2077                         break;
2078 
2079                     case NewAsyncGeneratorFunction:
2080                         node-&gt;convertToPhantomNewAsyncGeneratorFunction();
2081                         break;
2082 
2083                     case NewAsyncFunction:
2084                         node-&gt;convertToPhantomNewAsyncFunction();
2085                         break;
2086 
<span class="line-added">2087                     case NewArrayIterator:</span>
<span class="line-added">2088                         node-&gt;convertToPhantomNewArrayIterator();</span>
<span class="line-added">2089                         break;</span>
<span class="line-added">2090 </span>
2091                     case CreateActivation:
2092                         node-&gt;convertToPhantomCreateActivation();
2093                         break;
2094 
2095                     case NewRegexp:
2096                         node-&gt;convertToPhantomNewRegexp();
2097                         break;
2098 
2099                     default:
2100                         node-&gt;remove(m_graph);
2101                         break;
2102                     }
2103                 }
2104 
2105                 m_graph.doToChildren(
2106                     node,
2107                     [&amp;] (Edge&amp; edge) {
2108                         edge.setNode(resolve(block, edge.node()));
2109                     });
2110             }
</pre>
<hr />
<pre>
2230         // In order to do this, we say that we need to insert an
2231         // update hint for any availability whose node resolve()s to
2232         // the materialization.
2233         for (auto entry : availability.m_heap) {
2234             if (!entry.value.hasNode())
2235                 continue;
2236             if (m_heap.follow(entry.value.node()) != escapee)
2237                 continue;
2238 
2239             m_insertionSet.insert(
2240                 nodeIndex,
2241                 entry.key.createHint(m_graph, origin.takeValidExit(canExit), materialization));
2242         }
2243 
2244         for (unsigned i = availability.m_locals.size(); i--;) {
2245             if (!availability.m_locals[i].hasNode())
2246                 continue;
2247             if (m_heap.follow(availability.m_locals[i].node()) != escapee)
2248                 continue;
2249 
<span class="line-modified">2250             Operand operand = availability.m_locals.operandForIndex(i);</span>
2251             m_insertionSet.insertNode(
2252                 nodeIndex, SpecNone, MovHint, origin.takeValidExit(canExit), OpInfo(operand),
2253                 materialization-&gt;defaultEdge());
2254         }
2255     }
2256 
2257     void populateMaterialization(BasicBlock* block, Node* node, Node* escapee)
2258     {
2259         Allocation&amp; allocation = m_heap.getAllocation(escapee);
2260         switch (node-&gt;op()) {
2261         case MaterializeNewObject: {
2262             ObjectMaterializationData&amp; data = node-&gt;objectMaterializationData();
2263             unsigned firstChild = m_graph.m_varArgChildren.size();
2264 
2265             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);
2266 
2267             PromotedHeapLocation structure(StructurePLoc, allocation.identifier());
2268             ASSERT(locations.contains(structure));
2269 
2270             m_graph.m_varArgChildren.append(Edge(resolve(block, structure), KnownCellUse));
</pre>
<hr />
<pre>
2347             break;
2348         }
2349 
2350         case NewFunction:
2351         case NewGeneratorFunction:
2352         case NewAsyncGeneratorFunction:
2353         case NewAsyncFunction: {
2354             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);
2355             ASSERT(locations.size() == 2);
2356 
2357             PromotedHeapLocation executable(FunctionExecutablePLoc, allocation.identifier());
2358             ASSERT_UNUSED(executable, locations.contains(executable));
2359 
2360             PromotedHeapLocation activation(FunctionActivationPLoc, allocation.identifier());
2361             ASSERT(locations.contains(activation));
2362 
2363             node-&gt;child1() = Edge(resolve(block, activation), KnownCellUse);
2364             break;
2365         }
2366 
<span class="line-added">2367         case MaterializeNewInternalFieldObject: {</span>
<span class="line-added">2368             ObjectMaterializationData&amp; data = node-&gt;objectMaterializationData();</span>
<span class="line-added">2369 </span>
<span class="line-added">2370             unsigned firstChild = m_graph.m_varArgChildren.size();</span>
<span class="line-added">2371 </span>
<span class="line-added">2372             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);</span>
<span class="line-added">2373 </span>
<span class="line-added">2374             PromotedHeapLocation structure(StructurePLoc, allocation.identifier());</span>
<span class="line-added">2375             ASSERT(locations.contains(structure));</span>
<span class="line-added">2376             m_graph.m_varArgChildren.append(Edge(resolve(block, structure), KnownCellUse));</span>
<span class="line-added">2377 </span>
<span class="line-added">2378             for (PromotedHeapLocation location : locations) {</span>
<span class="line-added">2379                 switch (location.kind()) {</span>
<span class="line-added">2380                 case StructurePLoc: {</span>
<span class="line-added">2381                     ASSERT(location == structure);</span>
<span class="line-added">2382                     break;</span>
<span class="line-added">2383                 }</span>
<span class="line-added">2384 </span>
<span class="line-added">2385                 case InternalFieldObjectPLoc: {</span>
<span class="line-added">2386                     ASSERT(location.base() == allocation.identifier());</span>
<span class="line-added">2387                     data.m_properties.append(location.descriptor());</span>
<span class="line-added">2388                     Node* value = resolve(block, location);</span>
<span class="line-added">2389                     if (m_sinkCandidates.contains(value))</span>
<span class="line-added">2390                         m_graph.m_varArgChildren.append(m_bottom);</span>
<span class="line-added">2391                     else</span>
<span class="line-added">2392                         m_graph.m_varArgChildren.append(value);</span>
<span class="line-added">2393                     break;</span>
<span class="line-added">2394                 }</span>
<span class="line-added">2395 </span>
<span class="line-added">2396                 default:</span>
<span class="line-added">2397                     DFG_CRASH(m_graph, node, &quot;Bad location kind&quot;);</span>
<span class="line-added">2398                 }</span>
<span class="line-added">2399             }</span>
<span class="line-added">2400 </span>
<span class="line-added">2401             node-&gt;children = AdjacencyList(</span>
<span class="line-added">2402                 AdjacencyList::Variable,</span>
<span class="line-added">2403                 firstChild, m_graph.m_varArgChildren.size() - firstChild);</span>
<span class="line-added">2404             break;</span>
<span class="line-added">2405         }</span>
<span class="line-added">2406 </span>
2407         case NewRegexp: {
2408             Vector&lt;PromotedHeapLocation&gt; locations = m_locationsForAllocation.get(escapee);
2409             ASSERT(locations.size() == 2);
2410 
2411             PromotedHeapLocation regExp(RegExpObjectRegExpPLoc, allocation.identifier());
2412             ASSERT_UNUSED(regExp, locations.contains(regExp));
2413 
2414             PromotedHeapLocation lastIndex(RegExpObjectLastIndexPLoc, allocation.identifier());
2415             ASSERT(locations.contains(lastIndex));
2416             Node* value = resolve(block, lastIndex);
2417             if (m_sinkCandidates.contains(value))
2418                 node-&gt;child1() = Edge(m_bottom);
2419             else
2420                 node-&gt;child1() = Edge(value);
2421             break;
2422         }
2423 
2424         default:
2425             DFG_CRASH(m_graph, node, &quot;Bad materialize op&quot;);
2426         }
</pre>
<hr />
<pre>
2518                     PutByIdVariant::replace(currentSet, currentOffset));
2519             }
2520 
2521             return m_graph.addNode(
2522                 MultiPutByOffset,
2523                 origin.takeValidExit(canExit),
2524                 OpInfo(data),
2525                 Edge(base, KnownCellUse),
2526                 value-&gt;defaultEdge());
2527         }
2528 
2529         case ClosureVarPLoc: {
2530             return m_graph.addNode(
2531                 PutClosureVar,
2532                 origin.takeValidExit(canExit),
2533                 OpInfo(location.info()),
2534                 Edge(base, KnownCellUse),
2535                 value-&gt;defaultEdge());
2536         }
2537 
<span class="line-added">2538         case InternalFieldObjectPLoc: {</span>
<span class="line-added">2539             return m_graph.addNode(</span>
<span class="line-added">2540                 PutInternalField,</span>
<span class="line-added">2541                 origin.takeValidExit(canExit),</span>
<span class="line-added">2542                 OpInfo(location.info()),</span>
<span class="line-added">2543                 Edge(base, KnownCellUse),</span>
<span class="line-added">2544                 value-&gt;defaultEdge());</span>
<span class="line-added">2545         }</span>
<span class="line-added">2546 </span>
2547         case RegExpObjectLastIndexPLoc: {
2548             return m_graph.addNode(
2549                 SetRegExpObjectLastIndex,
2550                 origin.takeValidExit(canExit),
2551                 OpInfo(true),
2552                 Edge(base, KnownCellUse),
2553                 value-&gt;defaultEdge());
2554         }
2555 
2556         default:
2557             DFG_CRASH(m_graph, base, &quot;Bad location kind&quot;);
2558             break;
2559         }
2560 
2561         RELEASE_ASSERT_NOT_REACHED();
2562     }
2563 
2564     void removeICStatusFilters()
2565     {
2566         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
2567             for (Node* node : *block) {
2568                 switch (node-&gt;op()) {
2569                 case FilterCallLinkStatus:
<span class="line-modified">2570                 case FilterGetByStatus:</span>
2571                 case FilterPutByIdStatus:
2572                 case FilterInByIdStatus:
2573                     if (node-&gt;child1()-&gt;isPhantomAllocation())
2574                         node-&gt;removeWithoutChecks();
2575                     break;
2576                 default:
2577                     break;
2578                 }
2579             }
2580         }
2581     }
2582 
2583     // This is a great way of asking value-&gt;isStillValid() without having to worry about getting
2584     // different answers. It turns out that this analysis works OK regardless of what this
2585     // returns but breaks badly if this changes its mind for any particular InferredValue. This
2586     // method protects us from that.
2587     bool isStillValid(SymbolTable* value)
2588     {
2589         return m_validInferredValues.add(value, value-&gt;singleton().isStillValid()).iterator-&gt;value;
2590     }
</pre>
</td>
</tr>
</table>
<center><a href="DFGOSRExitCompilerCommon.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOpInfo.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>