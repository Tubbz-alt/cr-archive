<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGGraph.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGGraph.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGHeapLocation.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGGraph.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 50,11 ***</span>
  }
  
  namespace JSC {
  
  class CodeBlock;
<span class="line-modified">! class ExecState;</span>
  
  namespace DFG {
  
  class BackwardsCFG;
  class BackwardsDominators;
<span class="line-new-header">--- 50,11 ---</span>
  }
  
  namespace JSC {
  
  class CodeBlock;
<span class="line-modified">! class CallFrame;</span>
  
  namespace DFG {
  
  class BackwardsCFG;
  class BackwardsDominators;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 439,11 ***</span>
      }
  
      JSObject* globalThisObjectFor(CodeOrigin codeOrigin)
      {
          JSGlobalObject* object = globalObjectFor(codeOrigin);
<span class="line-modified">!         return jsCast&lt;JSObject*&gt;(object-&gt;methodTable(m_vm)-&gt;toThis(object, object-&gt;globalExec(), NotStrictMode));</span>
      }
  
      ScriptExecutable* executableFor(InlineCallFrame* inlineCallFrame)
      {
          if (!inlineCallFrame)
<span class="line-new-header">--- 439,11 ---</span>
      }
  
      JSObject* globalThisObjectFor(CodeOrigin codeOrigin)
      {
          JSGlobalObject* object = globalObjectFor(codeOrigin);
<span class="line-modified">!         return jsCast&lt;JSObject*&gt;(object-&gt;methodTable(m_vm)-&gt;toThis(object, object, NotStrictMode));</span>
      }
  
      ScriptExecutable* executableFor(InlineCallFrame* inlineCallFrame)
      {
          if (!inlineCallFrame)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 838,51 ***</span>
      FullBytecodeLiveness&amp; livenessFor(InlineCallFrame*);
  
      // Quickly query if a single local is live at the given point. This is faster than calling
      // forAllLiveInBytecode() if you will only query one local. But, if you want to know all of the
      // locals live, then calling this for each local is much slower than forAllLiveInBytecode().
<span class="line-modified">!     bool isLiveInBytecode(VirtualRegister, CodeOrigin);</span>
  
<span class="line-modified">!     // Quickly get all of the non-argument locals live at the given point. This doesn&#39;t give you</span>
      // any arguments because those are all presumed live. You can call forAllLiveInBytecode() to
      // also get the arguments. This is much faster than calling isLiveInBytecode() for each local.
      template&lt;typename Functor&gt;
<span class="line-modified">!     void forAllLocalsLiveInBytecode(CodeOrigin codeOrigin, const Functor&amp; functor)</span>
      {
          // Support for not redundantly reporting arguments. Necessary because in case of a varargs
          // call, only the callee knows that arguments are live while in the case of a non-varargs
          // call, both callee and caller will see the variables live.
          VirtualRegister exclusionStart;
          VirtualRegister exclusionEnd;
  
          CodeOrigin* codeOriginPtr = &amp;codeOrigin;
  
          for (;;) {
              InlineCallFrame* inlineCallFrame = codeOriginPtr-&gt;inlineCallFrame();
              VirtualRegister stackOffset(inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
  
              if (inlineCallFrame) {
                  if (inlineCallFrame-&gt;isClosureCall)
                      functor(stackOffset + CallFrameSlot::callee);
                  if (inlineCallFrame-&gt;isVarargs())
<span class="line-modified">!                     functor(stackOffset + CallFrameSlot::argumentCount);</span>
              }
  
              CodeBlock* codeBlock = baselineCodeBlockFor(inlineCallFrame);
              FullBytecodeLiveness&amp; fullLiveness = livenessFor(codeBlock);
<span class="line-modified">!             const FastBitVector&amp; liveness = fullLiveness.getLiveness(codeOriginPtr-&gt;bytecodeIndex());</span>
              for (unsigned relativeLocal = codeBlock-&gt;numCalleeLocals(); relativeLocal--;) {
                  VirtualRegister reg = stackOffset + virtualRegisterForLocal(relativeLocal);
  
                  // Don&#39;t report if our callee already reported.
                  if (reg &gt;= exclusionStart &amp;&amp; reg &lt; exclusionEnd)
                      continue;
  
<span class="line-modified">!                 if (liveness[relativeLocal])</span>
                      functor(reg);
              }
  
              if (!inlineCallFrame)
                  break;
  
              // Arguments are always live. This would be redundant if it wasn&#39;t for our
              // op_call_varargs inlining. See the comment above.
<span class="line-new-header">--- 838,60 ---</span>
      FullBytecodeLiveness&amp; livenessFor(InlineCallFrame*);
  
      // Quickly query if a single local is live at the given point. This is faster than calling
      // forAllLiveInBytecode() if you will only query one local. But, if you want to know all of the
      // locals live, then calling this for each local is much slower than forAllLiveInBytecode().
<span class="line-modified">!     bool isLiveInBytecode(Operand, CodeOrigin);</span>
  
<span class="line-modified">!     // Quickly get all of the non-argument locals and tmps live at the given point. This doesn&#39;t give you</span>
      // any arguments because those are all presumed live. You can call forAllLiveInBytecode() to
      // also get the arguments. This is much faster than calling isLiveInBytecode() for each local.
      template&lt;typename Functor&gt;
<span class="line-modified">!     void forAllLocalsAndTmpsLiveInBytecode(CodeOrigin codeOrigin, const Functor&amp; functor)</span>
      {
          // Support for not redundantly reporting arguments. Necessary because in case of a varargs
          // call, only the callee knows that arguments are live while in the case of a non-varargs
          // call, both callee and caller will see the variables live.
          VirtualRegister exclusionStart;
          VirtualRegister exclusionEnd;
  
          CodeOrigin* codeOriginPtr = &amp;codeOrigin;
  
<span class="line-added">+         bool isCallerOrigin = false;</span>
          for (;;) {
              InlineCallFrame* inlineCallFrame = codeOriginPtr-&gt;inlineCallFrame();
              VirtualRegister stackOffset(inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
  
              if (inlineCallFrame) {
                  if (inlineCallFrame-&gt;isClosureCall)
                      functor(stackOffset + CallFrameSlot::callee);
                  if (inlineCallFrame-&gt;isVarargs())
<span class="line-modified">!                     functor(stackOffset + CallFrameSlot::argumentCountIncludingThis);</span>
              }
  
              CodeBlock* codeBlock = baselineCodeBlockFor(inlineCallFrame);
              FullBytecodeLiveness&amp; fullLiveness = livenessFor(codeBlock);
<span class="line-modified">!             const auto&amp; livenessAtBytecode = fullLiveness.getLiveness(codeOriginPtr-&gt;bytecodeIndex(), appropriateLivenessCalculationPoint(*codeOriginPtr, isCallerOrigin));</span>
              for (unsigned relativeLocal = codeBlock-&gt;numCalleeLocals(); relativeLocal--;) {
                  VirtualRegister reg = stackOffset + virtualRegisterForLocal(relativeLocal);
  
                  // Don&#39;t report if our callee already reported.
                  if (reg &gt;= exclusionStart &amp;&amp; reg &lt; exclusionEnd)
                      continue;
  
<span class="line-modified">!                 if (livenessAtBytecode[relativeLocal])</span>
                      functor(reg);
              }
  
<span class="line-added">+             if (codeOriginPtr-&gt;bytecodeIndex().checkpoint()) {</span>
<span class="line-added">+                 ASSERT(codeBlock-&gt;numTmps());</span>
<span class="line-added">+                 auto liveTmps = tmpLivenessForCheckpoint(*codeBlock, codeOriginPtr-&gt;bytecodeIndex());</span>
<span class="line-added">+                 liveTmps.forEachSetBit([&amp;] (size_t tmp) {</span>
<span class="line-added">+                     functor(remapOperand(inlineCallFrame, Operand::tmp(tmp)));</span>
<span class="line-added">+                 });</span>
<span class="line-added">+             }</span>
<span class="line-added">+ </span>
              if (!inlineCallFrame)
                  break;
  
              // Arguments are always live. This would be redundant if it wasn&#39;t for our
              // op_call_varargs inlining. See the comment above.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 897,27 ***</span>
                  functor(reg);
  
              // We need to handle tail callers because we may decide to exit to the
              // the return bytecode following the tail call.
              codeOriginPtr = &amp;inlineCallFrame-&gt;directCaller;
          }
      }
  
<span class="line-modified">!     // Get a BitVector of all of the non-argument locals live right now. This is mostly useful if</span>
      // you want to compare two sets of live locals from two different CodeOrigins.
<span class="line-modified">!     BitVector localsLiveInBytecode(CodeOrigin);</span>
  
<span class="line-modified">!     // Tells you all of the arguments and locals live at the given CodeOrigin. This is a small</span>
<span class="line-modified">!     // extension to forAllLocalsLiveInBytecode(), since all arguments are always presumed live.</span>
      template&lt;typename Functor&gt;
      void forAllLiveInBytecode(CodeOrigin codeOrigin, const Functor&amp; functor)
      {
<span class="line-modified">!         forAllLocalsLiveInBytecode(codeOrigin, functor);</span>
  
          // Report all arguments as being live.
          for (unsigned argument = block(0)-&gt;variablesAtHead.numberOfArguments(); argument--;)
<span class="line-modified">!             functor(virtualRegisterForArgument(argument));</span>
      }
  
      BytecodeKills&amp; killsFor(CodeBlock*);
      BytecodeKills&amp; killsFor(InlineCallFrame*);
  
<span class="line-new-header">--- 906,58 ---</span>
                  functor(reg);
  
              // We need to handle tail callers because we may decide to exit to the
              // the return bytecode following the tail call.
              codeOriginPtr = &amp;inlineCallFrame-&gt;directCaller;
<span class="line-added">+             isCallerOrigin = true;</span>
          }
      }
  
<span class="line-modified">!     // Get a BitVector of all of the locals and tmps live right now. This is mostly useful if</span>
      // you want to compare two sets of live locals from two different CodeOrigins.
<span class="line-modified">!     BitVector localsAndTmpsLiveInBytecode(CodeOrigin);</span>
<span class="line-added">+ </span>
<span class="line-added">+     LivenessCalculationPoint appropriateLivenessCalculationPoint(CodeOrigin origin, bool isCallerOrigin)</span>
<span class="line-added">+     {</span>
<span class="line-added">+         if (isCallerOrigin) {</span>
<span class="line-added">+             // We do not need to keep used registers of call bytecodes live when terminating in inlined function,</span>
<span class="line-added">+             // except for inlining invoked by non call bytecodes including getter/setter calls.</span>
<span class="line-added">+             BytecodeIndex bytecodeIndex = origin.bytecodeIndex();</span>
<span class="line-added">+             InlineCallFrame* inlineCallFrame = origin.inlineCallFrame();</span>
<span class="line-added">+             CodeBlock* codeBlock = baselineCodeBlockFor(inlineCallFrame);</span>
<span class="line-added">+             auto instruction = codeBlock-&gt;instructions().at(bytecodeIndex.offset());</span>
<span class="line-added">+             switch (instruction-&gt;opcodeID()) {</span>
<span class="line-added">+             case op_call_varargs:</span>
<span class="line-added">+             case op_tail_call_varargs:</span>
<span class="line-added">+             case op_construct_varargs:</span>
<span class="line-added">+                 // When inlining varargs call, uses include array used for varargs. But when we are in inlined function,</span>
<span class="line-added">+                 // the content of this is already read and flushed to the stack. So, at this point, we no longer need to</span>
<span class="line-added">+                 // keep these use registers. We can use the liveness at LivenessCalculationPoint::AfterUse point.</span>
<span class="line-added">+                 // This is important to kill arguments allocations in DFG (not in FTL) when calling a function in a</span>
<span class="line-added">+                 // `func.apply(undefined, arguments)` manner.</span>
<span class="line-added">+                 return LivenessCalculationPoint::AfterUse;</span>
<span class="line-added">+             default:</span>
<span class="line-added">+                 // We could list up the other bytecodes here, like, `op_call`, `op_get_by_id` (getter inlining). But we don&#39;t do that.</span>
<span class="line-added">+                 // To list up bytecodes here, we must ensure that these bytecodes never use `uses` registers after inlining. So we cannot</span>
<span class="line-added">+                 // return LivenessCalculationPoint::AfterUse blindly if isCallerOrigin = true. And since excluding liveness in the other</span>
<span class="line-added">+                 // bytecodes does not offer practical benefit, we do not try it.</span>
<span class="line-added">+                 break;</span>
<span class="line-added">+             }</span>
<span class="line-added">+         }</span>
<span class="line-added">+         return LivenessCalculationPoint::BeforeUse;</span>
<span class="line-added">+     }</span>
  
<span class="line-modified">!     // Tells you all of the operands live at the given CodeOrigin. This is a small</span>
<span class="line-modified">!     // extension to forAllLocalsOrTmpsLiveInBytecode(), since all arguments are always presumed live.</span>
      template&lt;typename Functor&gt;
      void forAllLiveInBytecode(CodeOrigin codeOrigin, const Functor&amp; functor)
      {
<span class="line-modified">!         forAllLocalsAndTmpsLiveInBytecode(codeOrigin, functor);</span>
  
          // Report all arguments as being live.
          for (unsigned argument = block(0)-&gt;variablesAtHead.numberOfArguments(); argument--;)
<span class="line-modified">!             functor(virtualRegisterForArgumentIncludingThis(argument));</span>
      }
  
      BytecodeKills&amp; killsFor(CodeBlock*);
      BytecodeKills&amp; killsFor(InlineCallFrame*);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1080,10 ***</span>
<span class="line-new-header">--- 1120,11 ---</span>
      std::unique_ptr&lt;SSACFG&gt; m_ssaCFG;
      std::unique_ptr&lt;CPSCFG&gt; m_cpsCFG;
      std::unique_ptr&lt;BackwardsCFG&gt; m_backwardsCFG;
      std::unique_ptr&lt;BackwardsDominators&gt; m_backwardsDominators;
      std::unique_ptr&lt;ControlEquivalenceAnalysis&gt; m_controlEquivalenceAnalysis;
<span class="line-added">+     unsigned m_tmps;</span>
      unsigned m_localVars;
      unsigned m_nextMachineLocal;
      unsigned m_parameterSlots;
  
      // This is the number of logical entrypoints that we&#39;re compiling. This is only used
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1092,11 ***</span>
      // the CFG. In SSA, m_roots.size() == 1 even if we&#39;re compiling more than one entrypoint.
      unsigned m_numberOfEntrypoints { UINT_MAX };
  
      // This maps an entrypoint index to a particular op_catch bytecode offset. By convention,
      // it&#39;ll never have zero as a key because we use zero to mean the op_enter entrypoint.
<span class="line-modified">!     HashMap&lt;unsigned, unsigned&gt; m_entrypointIndexToCatchBytecodeOffset;</span>
  
      HashSet&lt;String&gt; m_localStrings;
      HashMap&lt;const StringImpl*, String&gt; m_copiedStrings;
  
  #if USE(JSVALUE32_64)
<span class="line-new-header">--- 1133,11 ---</span>
      // the CFG. In SSA, m_roots.size() == 1 even if we&#39;re compiling more than one entrypoint.
      unsigned m_numberOfEntrypoints { UINT_MAX };
  
      // This maps an entrypoint index to a particular op_catch bytecode offset. By convention,
      // it&#39;ll never have zero as a key because we use zero to mean the op_enter entrypoint.
<span class="line-modified">!     HashMap&lt;unsigned, BytecodeIndex&gt; m_entrypointIndexToCatchBytecodeIndex;</span>
  
      HashSet&lt;String&gt; m_localStrings;
      HashMap&lt;const StringImpl*, String&gt; m_copiedStrings;
  
  #if USE(JSVALUE32_64)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1119,10 ***</span>
<span class="line-new-header">--- 1160,12 ---</span>
      Bag&lt;EntrySwitchData&gt; m_entrySwitchData;
  
      RegisteredStructure stringStructure;
      RegisteredStructure symbolStructure;
  
<span class="line-added">+     HashSet&lt;Node*&gt; m_slowGetByVal;</span>
<span class="line-added">+ </span>
  private:
      bool isStringPrototypeMethodSane(JSGlobalObject*, UniquedStringImpl*);
  
      void handleSuccessor(Vector&lt;BasicBlock*, 16&gt;&amp; worklist, BasicBlock*, BasicBlock* successor);
  
</pre>
<center><a href="DFGGraph.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGHeapLocation.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>