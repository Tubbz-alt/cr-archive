diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
+ * Copyright (C) 2011-2020 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -28,10 +28,11 @@
 
 #if ENABLE(JIT)
 
 #include "BinarySwitch.h"
 #include "CCallHelpers.h"
+#include "CacheableIdentifierInlines.h"
 #include "CallFrameShuffler.h"
 #include "DFGOperations.h"
 #include "DFGSpeculativeJIT.h"
 #include "DOMJITGetterSetter.h"
 #include "DirectArguments.h"
@@ -62,11 +63,10 @@
 #include "StructureStubClearingWatchpoint.h"
 #include "StructureStubInfo.h"
 #include "SuperSampler.h"
 #include "ThunkGenerators.h"
 #include "WebAssemblyFunction.h"
-#include "WebAssemblyToJSCallee.h"
 #include <wtf/CommaPrinter.h>
 #include <wtf/ListDump.h>
 #include <wtf/StringPrintStream.h>
 
 namespace JSC {
@@ -93,11 +93,11 @@
         FTL::Thunks& thunks = *vm.ftlThunks;
         FunctionPtr<OperationPtrTag> target = MacroAssembler::readCallTarget<OperationPtrTag>(call);
         auto slowPathThunk = MacroAssemblerCodePtr<JITThunkPtrTag>::createFromExecutableAddress(target.retaggedExecutableAddress<JITThunkPtrTag>());
         FTL::SlowPathCallKey key = thunks.keyForSlowPathCallThunk(slowPathThunk);
         key = key.withCallTarget(newCalleeFunction);
-        MacroAssembler::repatchCall(call, FunctionPtr<OperationPtrTag>(thunks.getSlowPathCallThunk(key).retaggedCode<OperationPtrTag>()));
+        MacroAssembler::repatchCall(call, FunctionPtr<OperationPtrTag>(thunks.getSlowPathCallThunk(vm, key).retaggedCode<OperationPtrTag>()));
         return;
     }
 #else // ENABLE(FTL_JIT)
     UNUSED_PARAM(codeBlock);
 #endif // ENABLE(FTL_JIT)
@@ -130,11 +130,11 @@
         return GiveUpOnCache;
 
     return AttemptToCache;
 }
 
-static bool forceICFailure(ExecState*)
+static bool forceICFailure(JSGlobalObject*)
 {
     return Options::forceICFailure();
 }
 
 ALWAYS_INLINE static void fireWatchpointsAndClearStubIfNeeded(VM& vm, StructureStubInfo& stubInfo, CodeBlock* codeBlock, AccessGenerationResult& result)
@@ -143,104 +143,103 @@
         result.fireWatchpoints(vm);
         stubInfo.reset(codeBlock);
     }
 }
 
-inline FunctionPtr<CFunctionPtrTag> appropriateOptimizingGetByIdFunction(GetByIDKind kind)
+inline FunctionPtr<CFunctionPtrTag> appropriateOptimizingGetByFunction(GetByKind kind)
 {
     switch (kind) {
-    case GetByIDKind::Normal:
+    case GetByKind::Normal:
         return operationGetByIdOptimize;
-    case GetByIDKind::WithThis:
+    case GetByKind::WithThis:
         return operationGetByIdWithThisOptimize;
-    case GetByIDKind::Try:
+    case GetByKind::Try:
         return operationTryGetByIdOptimize;
-    case GetByIDKind::Direct:
+    case GetByKind::Direct:
         return operationGetByIdDirectOptimize;
+    case GetByKind::NormalByVal:
+        return operationGetByValOptimize;
     }
-    ASSERT_NOT_REACHED();
-    return operationGetById;
+    RELEASE_ASSERT_NOT_REACHED();
 }
 
-inline FunctionPtr<CFunctionPtrTag> appropriateGetByIdFunction(GetByIDKind kind)
+inline FunctionPtr<CFunctionPtrTag> appropriateGetByFunction(GetByKind kind)
 {
     switch (kind) {
-    case GetByIDKind::Normal:
+    case GetByKind::Normal:
         return operationGetById;
-    case GetByIDKind::WithThis:
+    case GetByKind::WithThis:
         return operationGetByIdWithThis;
-    case GetByIDKind::Try:
+    case GetByKind::Try:
         return operationTryGetById;
-    case GetByIDKind::Direct:
+    case GetByKind::Direct:
         return operationGetByIdDirect;
+    case GetByKind::NormalByVal:
+        return operationGetByValGeneric;
     }
-    ASSERT_NOT_REACHED();
-    return operationGetById;
+    RELEASE_ASSERT_NOT_REACHED();
 }
 
-static InlineCacheAction tryCacheGetByID(ExecState* exec, JSValue baseValue, const Identifier& propertyName, const PropertySlot& slot, StructureStubInfo& stubInfo, GetByIDKind kind)
+static InlineCacheAction tryCacheGetBy(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, CacheableIdentifier propertyName, const PropertySlot& slot, StructureStubInfo& stubInfo, GetByKind kind)
 {
-    VM& vm = exec->vm();
+    VM& vm = globalObject->vm();
     AccessGenerationResult result;
 
     {
-        GCSafeConcurrentJSLocker locker(exec->codeBlock()->m_lock, exec->vm().heap);
+        GCSafeConcurrentJSLocker locker(codeBlock->m_lock, globalObject->vm().heap);
 
-        if (forceICFailure(exec))
+        if (forceICFailure(globalObject))
             return GiveUpOnCache;
 
         // FIXME: Cache property access for immediates.
         if (!baseValue.isCell())
             return GiveUpOnCache;
         JSCell* baseCell = baseValue.asCell();
 
-        CodeBlock* codeBlock = exec->codeBlock();
-
         std::unique_ptr<AccessCase> newCase;
 
         if (propertyName == vm.propertyNames->length) {
             if (isJSArray(baseCell)) {
-                if (stubInfo.cacheType == CacheType::Unset
+                if (stubInfo.cacheType() == CacheType::Unset
                     && slot.slotBase() == baseCell
                     && InlineAccess::isCacheableArrayLength(stubInfo, jsCast<JSArray*>(baseCell))) {
 
                     bool generatedCodeInline = InlineAccess::generateArrayLength(stubInfo, jsCast<JSArray*>(baseCell));
                     if (generatedCodeInline) {
-                        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
+                        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
                         stubInfo.initArrayLength();
                         return RetryCacheLater;
                     }
                 }
 
-                newCase = AccessCase::create(vm, codeBlock, AccessCase::ArrayLength);
+                newCase = AccessCase::create(vm, codeBlock, AccessCase::ArrayLength, propertyName);
             } else if (isJSString(baseCell)) {
-                if (stubInfo.cacheType == CacheType::Unset && InlineAccess::isCacheableStringLength(stubInfo)) {
+                if (stubInfo.cacheType() == CacheType::Unset && InlineAccess::isCacheableStringLength(stubInfo)) {
                     bool generatedCodeInline = InlineAccess::generateStringLength(stubInfo);
                     if (generatedCodeInline) {
-                        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
+                        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
                         stubInfo.initStringLength();
                         return RetryCacheLater;
                     }
                 }
 
-                newCase = AccessCase::create(vm, codeBlock, AccessCase::StringLength);
-            }
-            else if (DirectArguments* arguments = jsDynamicCast<DirectArguments*>(vm, baseCell)) {
+                newCase = AccessCase::create(vm, codeBlock, AccessCase::StringLength, propertyName);
+            } else if (DirectArguments* arguments = jsDynamicCast<DirectArguments*>(vm, baseCell)) {
                 // If there were overrides, then we can handle this as a normal property load! Guarding
                 // this with such a check enables us to add an IC case for that load if needed.
                 if (!arguments->overrodeThings())
-                    newCase = AccessCase::create(vm, codeBlock, AccessCase::DirectArgumentsLength);
+                    newCase = AccessCase::create(vm, codeBlock, AccessCase::DirectArgumentsLength, propertyName);
             } else if (ScopedArguments* arguments = jsDynamicCast<ScopedArguments*>(vm, baseCell)) {
                 // Ditto.
                 if (!arguments->overrodeThings())
-                    newCase = AccessCase::create(vm, codeBlock, AccessCase::ScopedArgumentsLength);
+                    newCase = AccessCase::create(vm, codeBlock, AccessCase::ScopedArgumentsLength, propertyName);
             }
         }
 
         if (!propertyName.isSymbol() && baseCell->inherits<JSModuleNamespaceObject>(vm) && !slot.isUnset()) {
             if (auto moduleNamespaceSlot = slot.moduleNamespaceSlot())
-                newCase = ModuleNamespaceAccessCase::create(vm, codeBlock, jsCast<JSModuleNamespaceObject*>(baseCell), moduleNamespaceSlot->environment, ScopeOffset(moduleNamespaceSlot->scopeOffset));
+                newCase = ModuleNamespaceAccessCase::create(vm, codeBlock, propertyName, jsCast<JSModuleNamespaceObject*>(baseCell), moduleNamespaceSlot->environment, ScopeOffset(moduleNamespaceSlot->scopeOffset));
         }
 
         if (!newCase) {
             if (!slot.isCacheable() && !slot.isUnset())
                 return GiveUpOnCache;
@@ -259,156 +258,266 @@
             InlineCacheAction action = actionForCell(vm, baseCell);
             if (action != AttemptToCache)
                 return action;
 
             // Optimize self access.
-            if (stubInfo.cacheType == CacheType::Unset
+            if (stubInfo.cacheType() == CacheType::Unset
                 && slot.isCacheableValue()
                 && slot.slotBase() == baseValue
                 && !slot.watchpointSet()
                 && !structure->needImpurePropertyWatchpoint()
                 && !loadTargetFromProxy) {
 
                 bool generatedCodeInline = InlineAccess::generateSelfPropertyAccess(stubInfo, structure, slot.cachedOffset());
                 if (generatedCodeInline) {
-                    LOG_IC((ICEvent::GetByIdSelfPatch, structure->classInfo(), propertyName, slot.slotBase() == baseValue));
+                    LOG_IC((ICEvent::GetBySelfPatch, structure->classInfo(), Identifier::fromUid(vm, propertyName.uid()), slot.slotBase() == baseValue));
                     structure->startWatchingPropertyForReplacements(vm, slot.cachedOffset());
-                    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
-                    stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset());
+                    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
+                    stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset(), propertyName);
                     return RetryCacheLater;
                 }
             }
 
             std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain;
 
             PropertyOffset offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 
+            if (slot.isCustom() && slot.slotBase() == baseValue) {
+                // To cache self customs, we must disallow dictionaries because we
+                // need to be informed if the custom goes away since we cache the
+                // constant function pointer.
+
+                if (!prepareChainForCaching(globalObject, slot.slotBase(), slot.slotBase()))
+                    return GiveUpOnCache;
+            }
+
             if (slot.isUnset() || slot.slotBase() != baseValue) {
                 if (structure->typeInfo().prohibitsPropertyCaching())
                     return GiveUpOnCache;
 
                 if (structure->isDictionary()) {
                     if (structure->hasBeenFlattenedBefore())
                         return GiveUpOnCache;
                     structure->flattenDictionaryStructure(vm, jsCast<JSObject*>(baseCell));
+                    return RetryCacheLater; // We may have changed property offsets.
                 }
 
                 if (slot.isUnset() && structure->typeInfo().getOwnPropertySlotIsImpureForPropertyAbsence())
                     return GiveUpOnCache;
 
-                // If a kind is GetByIDKind::Direct, we do not need to investigate prototype chains further.
+                // If a kind is GetByKind::Direct, we do not need to investigate prototype chains further.
                 // Cacheability just depends on the head structure.
-                if (kind != GetByIDKind::Direct) {
-                    bool usesPolyProto;
-                    prototypeAccessChain = PolyProtoAccessChain::create(exec->lexicalGlobalObject(), baseCell, slot, usesPolyProto);
-                    if (!prototypeAccessChain) {
-                        // It's invalid to access this prototype property.
+                if (kind != GetByKind::Direct) {
+                    auto cacheStatus = prepareChainForCaching(globalObject, baseCell, slot);
+                    if (!cacheStatus)
                         return GiveUpOnCache;
+
+                    if (cacheStatus->flattenedDictionary) {
+                        // Property offsets may have changed due to flattening. We'll cache later.
+                        return RetryCacheLater;
                     }
 
-                    if (!usesPolyProto) {
+                    if (cacheStatus->usesPolyProto) {
+                        prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, slot);
+                        if (!prototypeAccessChain)
+                            return GiveUpOnCache;
+                        RELEASE_ASSERT(slot.isCacheableCustom() || prototypeAccessChain->slotBaseStructure(vm, structure)->get(vm, propertyName.uid()) == offset);
+                    } else {
                         // We use ObjectPropertyConditionSet instead for faster accesses.
                         prototypeAccessChain = nullptr;
 
                         // FIXME: Maybe this `if` should be inside generateConditionsForPropertyBlah.
                         // https://bugs.webkit.org/show_bug.cgi?id=185215
                         if (slot.isUnset()) {
                             conditionSet = generateConditionsForPropertyMiss(
-                                vm, codeBlock, exec, structure, propertyName.impl());
+                                vm, codeBlock, globalObject, structure, propertyName.uid());
                         } else if (!slot.isCacheableCustom()) {
                             conditionSet = generateConditionsForPrototypePropertyHit(
-                                vm, codeBlock, exec, structure, slot.slotBase(),
-                                propertyName.impl());
+                                vm, codeBlock, globalObject, structure, slot.slotBase(),
+                                propertyName.uid());
+                            RELEASE_ASSERT(!conditionSet.isValid() || conditionSet.slotBaseCondition().offset() == offset);
                         } else {
                             conditionSet = generateConditionsForPrototypePropertyHitCustom(
-                                vm, codeBlock, exec, structure, slot.slotBase(),
-                                propertyName.impl());
+                                vm, codeBlock, globalObject, structure, slot.slotBase(),
+                                propertyName.uid(), slot.attributes());
                         }
 
                         if (!conditionSet.isValid())
                             return GiveUpOnCache;
                     }
                 }
-
-                offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
             }
 
             JSFunction* getter = nullptr;
             if (slot.isCacheableGetter())
                 getter = jsDynamicCast<JSFunction*>(vm, slot.getterSetter()->getter());
 
             Optional<DOMAttributeAnnotation> domAttribute;
             if (slot.isCacheableCustom() && slot.domAttribute())
                 domAttribute = slot.domAttribute();
 
-            if (kind == GetByIDKind::Try) {
+            if (kind == GetByKind::Try) {
                 AccessCase::AccessType type;
                 if (slot.isCacheableValue())
                     type = AccessCase::Load;
                 else if (slot.isUnset())
                     type = AccessCase::Miss;
                 else if (slot.isCacheableGetter())
                     type = AccessCase::GetGetter;
                 else
                     RELEASE_ASSERT_NOT_REACHED();
 
-                newCase = ProxyableAccessCase::create(vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
+                newCase = ProxyableAccessCase::create(vm, codeBlock, type, propertyName, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
             } else if (!loadTargetFromProxy && getter && IntrinsicGetterAccessCase::canEmitIntrinsicGetter(getter, structure))
-                newCase = IntrinsicGetterAccessCase::create(vm, codeBlock, slot.cachedOffset(), structure, conditionSet, getter, WTFMove(prototypeAccessChain));
+                newCase = IntrinsicGetterAccessCase::create(vm, codeBlock, propertyName, slot.cachedOffset(), structure, conditionSet, getter, WTFMove(prototypeAccessChain));
             else {
                 if (slot.isCacheableValue() || slot.isUnset()) {
                     newCase = ProxyableAccessCase::create(vm, codeBlock, slot.isUnset() ? AccessCase::Miss : AccessCase::Load,
-                        offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
+                        propertyName, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
                 } else {
                     AccessCase::AccessType type;
                     if (slot.isCacheableGetter())
                         type = AccessCase::Getter;
                     else if (slot.attributes() & PropertyAttribute::CustomAccessor)
                         type = AccessCase::CustomAccessorGetter;
                     else
                         type = AccessCase::CustomValueGetter;
 
-                    if (kind == GetByIDKind::WithThis && type == AccessCase::CustomAccessorGetter && domAttribute)
+                    if (kind == GetByKind::WithThis && type == AccessCase::CustomAccessorGetter && domAttribute)
                         return GiveUpOnCache;
 
                     newCase = GetterSetterAccessCase::create(
-                        vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy,
+                        vm, codeBlock, type, propertyName, offset, structure, conditionSet, loadTargetFromProxy,
                         slot.watchpointSet(), slot.isCacheableCustom() ? slot.customGetter() : nullptr,
                         slot.isCacheableCustom() && slot.slotBase() != baseValue ? slot.slotBase() : nullptr,
                         domAttribute, WTFMove(prototypeAccessChain));
                 }
             }
         }
 
-        LOG_IC((ICEvent::GetByIdAddAccessCase, baseValue.classInfoOrNull(vm), propertyName, slot.slotBase() == baseValue));
+        LOG_IC((ICEvent::GetByAddAccessCase, baseValue.classInfoOrNull(vm), Identifier::fromUid(vm, propertyName.uid()), slot.slotBase() == baseValue));
 
         result = stubInfo.addAccessCase(locker, codeBlock, propertyName, WTFMove(newCase));
 
         if (result.generatedSomeCode()) {
-            LOG_IC((ICEvent::GetByIdReplaceWithJump, baseValue.classInfoOrNull(vm), propertyName, slot.slotBase() == baseValue));
+            LOG_IC((ICEvent::GetByReplaceWithJump, baseValue.classInfoOrNull(vm), Identifier::fromUid(vm, propertyName.uid()), slot.slotBase() == baseValue));
 
             RELEASE_ASSERT(result.code());
             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel<JITStubRoutinePtrTag>(result.code()));
         }
     }
 
-    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec->codeBlock(), result);
+    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 
     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 }
 
-void repatchGetByID(ExecState* exec, JSValue baseValue, const Identifier& propertyName, const PropertySlot& slot, StructureStubInfo& stubInfo, GetByIDKind kind)
+void repatchGetBy(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, CacheableIdentifier propertyName, const PropertySlot& slot, StructureStubInfo& stubInfo, GetByKind kind)
 {
     SuperSamplerScope superSamplerScope(false);
 
-    if (tryCacheGetByID(exec, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache) {
-        CodeBlock* codeBlock = exec->codeBlock();
-        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGetByIdFunction(kind));
+    if (tryCacheGetBy(globalObject, codeBlock, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache)
+        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateGetByFunction(kind));
+}
+
+
+static InlineCacheAction tryCacheArrayGetByVal(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, JSValue index, StructureStubInfo& stubInfo)
+{
+    if (!baseValue.isCell())
+        return GiveUpOnCache;
+
+    if (!index.isInt32())
+        return RetryCacheLater;
+
+    VM& vm = globalObject->vm();
+    AccessGenerationResult result;
+
+    {
+        GCSafeConcurrentJSLocker locker(codeBlock->m_lock, globalObject->vm().heap);
+
+        JSCell* base = baseValue.asCell();
+
+        AccessCase::AccessType accessType;
+        if (base->type() == DirectArgumentsType)
+            accessType = AccessCase::IndexedDirectArgumentsLoad;
+        else if (base->type() == ScopedArgumentsType)
+            accessType = AccessCase::IndexedScopedArgumentsLoad;
+        else if (base->type() == StringType)
+            accessType = AccessCase::IndexedStringLoad;
+        else if (isTypedView(base->classInfo(vm)->typedArrayStorageType)) {
+            switch (base->classInfo(vm)->typedArrayStorageType) {
+            case TypeInt8:
+                accessType = AccessCase::IndexedTypedArrayInt8Load;
+                break;
+            case TypeUint8:
+                accessType = AccessCase::IndexedTypedArrayUint8Load;
+                break;
+            case TypeUint8Clamped:
+                accessType = AccessCase::IndexedTypedArrayUint8ClampedLoad;
+                break;
+            case TypeInt16:
+                accessType = AccessCase::IndexedTypedArrayInt16Load;
+                break;
+            case TypeUint16:
+                accessType = AccessCase::IndexedTypedArrayUint16Load;
+                break;
+            case TypeInt32:
+                accessType = AccessCase::IndexedTypedArrayInt32Load;
+                break;
+            case TypeUint32:
+                accessType = AccessCase::IndexedTypedArrayUint32Load;
+                break;
+            case TypeFloat32:
+                accessType = AccessCase::IndexedTypedArrayFloat32Load;
+                break;
+            case TypeFloat64:
+                accessType = AccessCase::IndexedTypedArrayFloat64Load;
+                break;
+            default:
+                RELEASE_ASSERT_NOT_REACHED();
+            }
+        } else {
+            IndexingType indexingShape = base->indexingType() & IndexingShapeMask;
+            switch (indexingShape) {
+            case Int32Shape:
+                accessType = AccessCase::IndexedInt32Load;
+                break;
+            case DoubleShape:
+                accessType = AccessCase::IndexedDoubleLoad;
+                break;
+            case ContiguousShape:
+                accessType = AccessCase::IndexedContiguousLoad;
+                break;
+            case ArrayStorageShape:
+                accessType = AccessCase::IndexedArrayStorageLoad;
+                break;
+            default:
+                return GiveUpOnCache;
+            }
+        }
+
+        result = stubInfo.addAccessCase(locker, codeBlock, nullptr, AccessCase::create(vm, codeBlock, accessType, nullptr));
+
+        if (result.generatedSomeCode()) {
+            LOG_IC((ICEvent::GetByReplaceWithJump, baseValue.classInfoOrNull(vm), Identifier()));
+
+            RELEASE_ASSERT(result.code());
+            InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel<JITStubRoutinePtrTag>(result.code()));
+        }
     }
+
+    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
+    return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
+}
+
+void repatchArrayGetByVal(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue base, JSValue index, StructureStubInfo& stubInfo)
+{
+    if (tryCacheArrayGetByVal(globalObject, codeBlock, base, index, stubInfo) == GiveUpOnCache)
+        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationGetByValGeneric);
 }
 
-static V_JITOperation_ESsiJJI appropriateGenericPutByIdFunction(const PutPropertySlot &slot, PutKind putKind)
+static V_JITOperation_GSsiJJI appropriateGenericPutByIdFunction(const PutPropertySlot &slot, PutKind putKind)
 {
     if (slot.isStrictMode()) {
         if (putKind == Direct)
             return operationPutByIdDirectStrict;
         return operationPutByIdStrict;
@@ -416,11 +525,11 @@
     if (putKind == Direct)
         return operationPutByIdDirectNonStrict;
     return operationPutByIdNonStrict;
 }
 
-static V_JITOperation_ESsiJJI appropriateOptimizingPutByIdFunction(const PutPropertySlot &slot, PutKind putKind)
+static V_JITOperation_GSsiJJI appropriateOptimizingPutByIdFunction(const PutPropertySlot &slot, PutKind putKind)
 {
     if (slot.isStrictMode()) {
         if (putKind == Direct)
             return operationPutByIdDirectStrictOptimize;
         return operationPutByIdStrictOptimize;
@@ -428,35 +537,33 @@
     if (putKind == Direct)
         return operationPutByIdDirectNonStrictOptimize;
     return operationPutByIdNonStrictOptimize;
 }
 
-static InlineCacheAction tryCachePutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier& ident, const PutPropertySlot& slot, StructureStubInfo& stubInfo, PutKind putKind)
+static InlineCacheAction tryCachePutByID(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, Structure* oldStructure, const Identifier& ident, const PutPropertySlot& slot, StructureStubInfo& stubInfo, PutKind putKind)
 {
-    VM& vm = exec->vm();
+    VM& vm = globalObject->vm();
     AccessGenerationResult result;
     {
-        GCSafeConcurrentJSLocker locker(exec->codeBlock()->m_lock, exec->vm().heap);
+        GCSafeConcurrentJSLocker locker(codeBlock->m_lock, globalObject->vm().heap);
 
-        if (forceICFailure(exec))
+        if (forceICFailure(globalObject))
             return GiveUpOnCache;
 
-        CodeBlock* codeBlock = exec->codeBlock();
-
         if (!baseValue.isCell())
             return GiveUpOnCache;
 
         if (!slot.isCacheablePut() && !slot.isCacheableCustom() && !slot.isCacheableSetter())
             return GiveUpOnCache;
 
         // FIXME: We should try to do something smarter here...
-        if (isCopyOnWrite(structure->indexingMode()))
+        if (isCopyOnWrite(oldStructure->indexingMode()))
             return GiveUpOnCache;
         // We can't end up storing to a CoW on the prototype since it shouldn't own properties.
         ASSERT(!isCopyOnWrite(slot.base()->indexingMode()));
 
-        if (!structure->propertyAccessesAreCacheable())
+        if (!oldStructure->propertyAccessesAreCacheable())
             return GiveUpOnCache;
 
         std::unique_ptr<AccessCase> newCase;
         JSCell* baseCell = baseValue.asCell();
 
@@ -466,242 +573,257 @@
                 // when we transition then store to an existing property. This is common among
                 // paths that reify lazy properties. If we reify a lazy property and forget
                 // to disable caching, we may come down this path. The Replace IC does not
                 // know how to model these types of structure transitions (or any structure
                 // transition for that matter).
-                RELEASE_ASSERT(baseValue.asCell()->structure(vm) == structure);
+                RELEASE_ASSERT(baseValue.asCell()->structure(vm) == oldStructure);
 
-                structure->didCachePropertyReplacement(vm, slot.cachedOffset());
+                oldStructure->didCachePropertyReplacement(vm, slot.cachedOffset());
 
-                if (stubInfo.cacheType == CacheType::Unset
+                if (stubInfo.cacheType() == CacheType::Unset
                     && InlineAccess::canGenerateSelfPropertyReplace(stubInfo, slot.cachedOffset())
-                    && !structure->needImpurePropertyWatchpoint()) {
+                    && !oldStructure->needImpurePropertyWatchpoint()) {
 
-                    bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, structure, slot.cachedOffset());
+                    bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, oldStructure, slot.cachedOffset());
                     if (generatedCodeInline) {
-                        LOG_IC((ICEvent::PutByIdSelfPatch, structure->classInfo(), ident, slot.base() == baseValue));
-                        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingPutByIdFunction(slot, putKind));
-                        stubInfo.initPutByIdReplace(codeBlock, structure, slot.cachedOffset());
+                        LOG_IC((ICEvent::PutByIdSelfPatch, oldStructure->classInfo(), ident, slot.base() == baseValue));
+                        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingPutByIdFunction(slot, putKind));
+                        stubInfo.initPutByIdReplace(codeBlock, oldStructure, slot.cachedOffset());
                         return RetryCacheLater;
                     }
                 }
 
-                newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, slot.cachedOffset(), structure);
+                newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, ident, slot.cachedOffset(), oldStructure);
             } else {
                 ASSERT(slot.type() == PutPropertySlot::NewProperty);
 
-                if (!structure->isObject())
+                if (!oldStructure->isObject())
                     return GiveUpOnCache;
 
-                if (structure->isDictionary()) {
-                    if (structure->hasBeenFlattenedBefore())
-                        return GiveUpOnCache;
-                    structure->flattenDictionaryStructure(vm, jsCast<JSObject*>(baseValue));
-                }
+                // If the old structure is dictionary, it means that this is one-on-one between an object and a structure.
+                // If this is NewProperty operation, generating IC for this does not offer any benefit because this transition never happens again.
+                if (oldStructure->isDictionary())
+                    return RetryCacheLater;
 
                 PropertyOffset offset;
-                Structure* newStructure =
-                    Structure::addPropertyTransitionToExistingStructureConcurrently(
-                        structure, ident.impl(), 0, offset);
+                Structure* newStructure = Structure::addPropertyTransitionToExistingStructureConcurrently(oldStructure, ident.impl(), static_cast<unsigned>(PropertyAttribute::None), offset);
                 if (!newStructure || !newStructure->propertyAccessesAreCacheable())
                     return GiveUpOnCache;
 
-                ASSERT(newStructure->previousID() == structure);
+                // If JSObject::put is overridden by UserObject, UserObject::put performs side-effect on JSObject::put, and it neglects to mark the PutPropertySlot as non-cachaeble,
+                // then arbitrary structure transitions can happen during the put operation, and this generates wrong transition information here as if oldStructure -> newStructure.
+                // In reality, the transition is oldStructure -> something unknown structures -> baseValue's structure.
+                // To guard against the embedder's potentially incorrect UserObject::put implementation, we should check for this condition and if found, and give up on caching the put.
+                ASSERT(baseValue.asCell()->structure(vm) == newStructure);
+                if (baseValue.asCell()->structure(vm) != newStructure)
+                    return GiveUpOnCache;
+
+                ASSERT(newStructure->previousID() == oldStructure);
                 ASSERT(!newStructure->isDictionary());
                 ASSERT(newStructure->isObject());
 
                 std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain;
                 ObjectPropertyConditionSet conditionSet;
                 if (putKind == NotDirect) {
-                    bool usesPolyProto;
-                    prototypeAccessChain = PolyProtoAccessChain::create(exec->lexicalGlobalObject(), baseCell, nullptr, usesPolyProto);
-                    if (!prototypeAccessChain) {
-                        // It's invalid to access this prototype property.
+                    auto cacheStatus = prepareChainForCaching(globalObject, baseCell, nullptr);
+                    if (!cacheStatus)
                         return GiveUpOnCache;
-                    }
 
-                    if (!usesPolyProto) {
+                    if (cacheStatus->usesPolyProto) {
+                        prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, nullptr);
+                        if (!prototypeAccessChain)
+                            return GiveUpOnCache;
+                    } else {
                         prototypeAccessChain = nullptr;
-                        conditionSet =
-                            generateConditionsForPropertySetterMiss(
-                                vm, codeBlock, exec, newStructure, ident.impl());
+                        conditionSet = generateConditionsForPropertySetterMiss(
+                            vm, codeBlock, globalObject, newStructure, ident.impl());
                         if (!conditionSet.isValid())
                             return GiveUpOnCache;
                     }
-
                 }
 
-                newCase = AccessCase::create(vm, codeBlock, offset, structure, newStructure, conditionSet, WTFMove(prototypeAccessChain));
+                newCase = AccessCase::create(vm, codeBlock, ident, offset, oldStructure, newStructure, conditionSet, WTFMove(prototypeAccessChain));
             }
         } else if (slot.isCacheableCustom() || slot.isCacheableSetter()) {
             if (slot.isCacheableCustom()) {
                 ObjectPropertyConditionSet conditionSet;
                 std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain;
 
-                if (slot.base() != baseValue) {
-                    bool usesPolyProto;
-                    prototypeAccessChain = PolyProtoAccessChain::create(exec->lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
-                    if (!prototypeAccessChain) {
-                        // It's invalid to access this prototype property.
-                        return GiveUpOnCache;
-                    }
+                // We need to do this even if we're a self custom, since we must disallow dictionaries
+                // because we need to be informed if the custom goes away since we cache the constant
+                // function pointer.
+                auto cacheStatus = prepareChainForCaching(globalObject, baseCell, slot.base());
+                if (!cacheStatus)
+                    return GiveUpOnCache;
 
-                    if (!usesPolyProto) {
+                if (slot.base() != baseValue) {
+                    if (cacheStatus->usesPolyProto) {
+                        prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, slot.base());
+                        if (!prototypeAccessChain)
+                            return GiveUpOnCache;
+                    } else {
                         prototypeAccessChain = nullptr;
-                        conditionSet =
-                            generateConditionsForPrototypePropertyHitCustom(
-                                vm, codeBlock, exec, structure, slot.base(), ident.impl());
+                        conditionSet = generateConditionsForPrototypePropertyHitCustom(
+                            vm, codeBlock, globalObject, oldStructure, slot.base(), ident.impl(), static_cast<unsigned>(PropertyAttribute::None));
                         if (!conditionSet.isValid())
                             return GiveUpOnCache;
                     }
                 }
 
                 newCase = GetterSetterAccessCase::create(
-                    vm, codeBlock, slot.isCustomAccessor() ? AccessCase::CustomAccessorSetter : AccessCase::CustomValueSetter, structure, invalidOffset,
-                    conditionSet, WTFMove(prototypeAccessChain), slot.customSetter(), slot.base() != baseValue ? slot.base() : nullptr);
+                    vm, codeBlock, slot.isCustomAccessor() ? AccessCase::CustomAccessorSetter : AccessCase::CustomValueSetter, oldStructure, ident,
+                    invalidOffset, conditionSet, WTFMove(prototypeAccessChain), slot.customSetter(), slot.base() != baseValue ? slot.base() : nullptr);
             } else {
                 ObjectPropertyConditionSet conditionSet;
                 std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain;
                 PropertyOffset offset = slot.cachedOffset();
 
                 if (slot.base() != baseValue) {
-                    bool usesPolyProto;
-                    prototypeAccessChain = PolyProtoAccessChain::create(exec->lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
-                    if (!prototypeAccessChain) {
-                        // It's invalid to access this prototype property.
+                    auto cacheStatus = prepareChainForCaching(globalObject, baseCell, slot.base());
+                    if (!cacheStatus)
                         return GiveUpOnCache;
-                    }
+                    if (cacheStatus->flattenedDictionary)
+                        return RetryCacheLater;
 
-                    if (!usesPolyProto) {
+                    if (cacheStatus->usesPolyProto) {
+                        prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, slot.base());
+                        if (!prototypeAccessChain)
+                            return GiveUpOnCache;
+                        offset = prototypeAccessChain->slotBaseStructure(vm, baseCell->structure(vm))->get(vm, ident.impl());
+                    } else {
                         prototypeAccessChain = nullptr;
-                        conditionSet =
-                            generateConditionsForPrototypePropertyHit(
-                                vm, codeBlock, exec, structure, slot.base(), ident.impl());
+                        conditionSet = generateConditionsForPrototypePropertyHit(
+                            vm, codeBlock, globalObject, oldStructure, slot.base(), ident.impl());
                         if (!conditionSet.isValid())
                             return GiveUpOnCache;
 
                         if (!(conditionSet.slotBaseCondition().attributes() & PropertyAttribute::Accessor))
                             return GiveUpOnCache;
 
                         offset = conditionSet.slotBaseCondition().offset();
                     }
-
                 }
 
                 newCase = GetterSetterAccessCase::create(
-                    vm, codeBlock, AccessCase::Setter, structure, offset, conditionSet, WTFMove(prototypeAccessChain));
+                    vm, codeBlock, AccessCase::Setter, oldStructure, ident, offset, conditionSet, WTFMove(prototypeAccessChain));
             }
         }
 
-        LOG_IC((ICEvent::PutByIdAddAccessCase, structure->classInfo(), ident, slot.base() == baseValue));
+        LOG_IC((ICEvent::PutByIdAddAccessCase, oldStructure->classInfo(), ident, slot.base() == baseValue));
 
         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 
         if (result.generatedSomeCode()) {
-            LOG_IC((ICEvent::PutByIdReplaceWithJump, structure->classInfo(), ident, slot.base() == baseValue));
+            LOG_IC((ICEvent::PutByIdReplaceWithJump, oldStructure->classInfo(), ident, slot.base() == baseValue));
 
             RELEASE_ASSERT(result.code());
 
             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel<JITStubRoutinePtrTag>(result.code()));
         }
     }
 
-    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec->codeBlock(), result);
+    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 
     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 }
 
-void repatchPutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier& propertyName, const PutPropertySlot& slot, StructureStubInfo& stubInfo, PutKind putKind)
+void repatchPutByID(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, Structure* oldStructure, const Identifier& propertyName, const PutPropertySlot& slot, StructureStubInfo& stubInfo, PutKind putKind)
 {
     SuperSamplerScope superSamplerScope(false);
 
-    if (tryCachePutByID(exec, baseValue, structure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache) {
-        CodeBlock* codeBlock = exec->codeBlock();
-        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGenericPutByIdFunction(slot, putKind));
-    }
+    if (tryCachePutByID(globalObject, codeBlock, baseValue, oldStructure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache)
+        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateGenericPutByIdFunction(slot, putKind));
 }
 
 static InlineCacheAction tryCacheInByID(
-    ExecState* exec, JSObject* base, const Identifier& ident,
+    JSGlobalObject* globalObject, CodeBlock* codeBlock, JSObject* base, const Identifier& ident,
     bool wasFound, const PropertySlot& slot, StructureStubInfo& stubInfo)
 {
-    VM& vm = exec->vm();
+    VM& vm = globalObject->vm();
     AccessGenerationResult result;
 
     {
-        GCSafeConcurrentJSLocker locker(exec->codeBlock()->m_lock, vm.heap);
-        if (forceICFailure(exec))
+        GCSafeConcurrentJSLocker locker(codeBlock->m_lock, vm.heap);
+        if (forceICFailure(globalObject))
             return GiveUpOnCache;
 
         if (!base->structure(vm)->propertyAccessesAreCacheable() || (!wasFound && !base->structure(vm)->propertyAccessesAreCacheableForAbsence()))
             return GiveUpOnCache;
 
         if (wasFound) {
             if (!slot.isCacheable())
                 return GiveUpOnCache;
         }
 
-        CodeBlock* codeBlock = exec->codeBlock();
         Structure* structure = base->structure(vm);
 
         std::unique_ptr<PolyProtoAccessChain> prototypeAccessChain;
         ObjectPropertyConditionSet conditionSet;
         if (wasFound) {
             InlineCacheAction action = actionForCell(vm, base);
             if (action != AttemptToCache)
                 return action;
 
             // Optimize self access.
-            if (stubInfo.cacheType == CacheType::Unset
+            if (stubInfo.cacheType() == CacheType::Unset
                 && slot.isCacheableValue()
                 && slot.slotBase() == base
                 && !slot.watchpointSet()
                 && !structure->needImpurePropertyWatchpoint()) {
                 bool generatedCodeInline = InlineAccess::generateSelfInAccess(stubInfo, structure);
                 if (generatedCodeInline) {
                     LOG_IC((ICEvent::InByIdSelfPatch, structure->classInfo(), ident, slot.slotBase() == base));
                     structure->startWatchingPropertyForReplacements(vm, slot.cachedOffset());
-                    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
+                    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInByIdOptimize);
                     stubInfo.initInByIdSelf(codeBlock, structure, slot.cachedOffset());
                     return RetryCacheLater;
                 }
             }
 
             if (slot.slotBase() != base) {
-                bool usesPolyProto;
-                prototypeAccessChain = PolyProtoAccessChain::create(exec->lexicalGlobalObject(), base, slot, usesPolyProto);
-                if (!prototypeAccessChain) {
-                    // It's invalid to access this prototype property.
+                auto cacheStatus = prepareChainForCaching(globalObject, base, slot);
+                if (!cacheStatus)
                     return GiveUpOnCache;
-                }
-                if (!usesPolyProto) {
+                if (cacheStatus->flattenedDictionary)
+                    return RetryCacheLater;
+
+                if (cacheStatus->usesPolyProto) {
+                    prototypeAccessChain = PolyProtoAccessChain::create(globalObject, base, slot);
+                    if (!prototypeAccessChain)
+                        return GiveUpOnCache;
+                    RELEASE_ASSERT(slot.isCacheableCustom() || prototypeAccessChain->slotBaseStructure(vm, structure)->get(vm, ident.impl()) == slot.cachedOffset());
+                } else {
                     prototypeAccessChain = nullptr;
                     conditionSet = generateConditionsForPrototypePropertyHit(
-                        vm, codeBlock, exec, structure, slot.slotBase(), ident.impl());
+                        vm, codeBlock, globalObject, structure, slot.slotBase(), ident.impl());
+                    if (!conditionSet.isValid())
+                        return GiveUpOnCache;
+                    RELEASE_ASSERT(slot.isCacheableCustom() || conditionSet.slotBaseCondition().offset() == slot.cachedOffset());
                 }
             }
         } else {
-            bool usesPolyProto;
-            prototypeAccessChain = PolyProtoAccessChain::create(exec->lexicalGlobalObject(), base, slot, usesPolyProto);
-            if (!prototypeAccessChain) {
-                // It's invalid to access this prototype property.
+            auto cacheStatus = prepareChainForCaching(globalObject, base, nullptr);
+            if (!cacheStatus)
                 return GiveUpOnCache;
-            }
 
-            if (!usesPolyProto) {
+            if (cacheStatus->usesPolyProto) {
+                prototypeAccessChain = PolyProtoAccessChain::create(globalObject, base, slot);
+                if (!prototypeAccessChain)
+                    return GiveUpOnCache;
+            } else {
                 prototypeAccessChain = nullptr;
                 conditionSet = generateConditionsForPropertyMiss(
-                    vm, codeBlock, exec, structure, ident.impl());
+                    vm, codeBlock, globalObject, structure, ident.impl());
+                if (!conditionSet.isValid())
+                    return GiveUpOnCache;
             }
         }
-        if (!conditionSet.isValid())
-            return GiveUpOnCache;
 
         LOG_IC((ICEvent::InAddAccessCase, structure->classInfo(), ident, slot.slotBase() == base));
 
         std::unique_ptr<AccessCase> newCase = AccessCase::create(
-            vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
+            vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, ident, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
 
         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 
         if (result.generatedSomeCode()) {
             LOG_IC((ICEvent::InReplaceWithJump, structure->classInfo(), ident, slot.slotBase() == base));
@@ -709,36 +831,33 @@
             RELEASE_ASSERT(result.code());
             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel<JITStubRoutinePtrTag>(result.code()));
         }
     }
 
-    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec->codeBlock(), result);
+    fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 
     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 }
 
-void repatchInByID(ExecState* exec, JSObject* baseObject, const Identifier& propertyName, bool wasFound, const PropertySlot& slot, StructureStubInfo& stubInfo)
+void repatchInByID(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSObject* baseObject, const Identifier& propertyName, bool wasFound, const PropertySlot& slot, StructureStubInfo& stubInfo)
 {
     SuperSamplerScope superSamplerScope(false);
 
-    if (tryCacheInByID(exec, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache) {
-        CodeBlock* codeBlock = exec->codeBlock();
-        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInById);
-    }
+    if (tryCacheInByID(globalObject, codeBlock, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache)
+        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInById);
 }
 
 static InlineCacheAction tryCacheInstanceOf(
-    ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo& stubInfo,
+    JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue valueValue, JSValue prototypeValue, StructureStubInfo& stubInfo,
     bool wasFound)
 {
-    VM& vm = exec->vm();
-    CodeBlock* codeBlock = exec->codeBlock();
+    VM& vm = globalObject->vm();
     AccessGenerationResult result;
 
     RELEASE_ASSERT(valueValue.isCell()); // shouldConsiderCaching rejects non-cells.
 
-    if (forceICFailure(exec))
+    if (forceICFailure(globalObject))
         return GiveUpOnCache;
 
     {
         GCSafeConcurrentJSLocker locker(codeBlock->m_lock, vm.heap);
 
@@ -752,13 +871,13 @@
                     vm, codeBlock, AccessCase::InstanceOfMiss, structure, ObjectPropertyConditionSet(),
                     prototype);
             } else if (structure->prototypeQueriesAreCacheable()) {
                 // FIXME: Teach this to do poly proto.
                 // https://bugs.webkit.org/show_bug.cgi?id=185663
-
+                prepareChainForCaching(globalObject, value, wasFound ? prototype : nullptr);
                 ObjectPropertyConditionSet conditionSet = generateConditionsForInstanceOf(
-                    vm, codeBlock, exec, structure, prototype, wasFound);
+                    vm, codeBlock, globalObject, structure, prototype, wasFound);
 
                 if (conditionSet.isValid()) {
                     newCase = InstanceOfAccessCase::create(
                         vm, codeBlock,
                         wasFound ? AccessCase::InstanceOfHit : AccessCase::InstanceOfMiss,
@@ -766,15 +885,15 @@
                 }
             }
         }
 
         if (!newCase)
-            newCase = AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric);
+            newCase = AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric, Identifier());
 
         LOG_IC((ICEvent::InstanceOfAddAccessCase, structure->classInfo(), Identifier()));
 
-        result = stubInfo.addAccessCase(locker, codeBlock, Identifier(), WTFMove(newCase));
+        result = stubInfo.addAccessCase(locker, codeBlock, nullptr, WTFMove(newCase));
 
         if (result.generatedSomeCode()) {
             LOG_IC((ICEvent::InstanceOfReplaceWithJump, structure->classInfo(), Identifier()));
 
             RELEASE_ASSERT(result.code());
@@ -789,16 +908,16 @@
 
     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 }
 
 void repatchInstanceOf(
-    ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo& stubInfo,
+    JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue valueValue, JSValue prototypeValue, StructureStubInfo& stubInfo,
     bool wasFound)
 {
     SuperSamplerScope superSamplerScope(false);
-    if (tryCacheInstanceOf(exec, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
-        ftlThunkAwareRepatchCall(exec->codeBlock(), stubInfo.slowPathCallLocation(), operationInstanceOfGeneric);
+    if (tryCacheInstanceOf(globalObject, codeBlock, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
+        ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInstanceOfGeneric);
 }
 
 static void linkSlowFor(VM&, CallLinkInfo& callLinkInfo, MacroAssemblerCodeRef<JITStubRoutinePtrTag> codeRef)
 {
     MacroAssembler::repatchNearCall(callLinkInfo.callReturnLocation(), CodeLocationLabel<JITStubRoutinePtrTag>(codeRef.code()));
@@ -811,41 +930,40 @@
 
 static void linkSlowFor(VM& vm, CallLinkInfo& callLinkInfo)
 {
     MacroAssemblerCodeRef<JITStubRoutinePtrTag> virtualThunk = virtualThunkFor(vm, callLinkInfo);
     linkSlowFor(vm, callLinkInfo, virtualThunk);
-    callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
+    callLinkInfo.setSlowStub(GCAwareJITStubRoutine::create(virtualThunk, vm));
 }
 
 static JSCell* webAssemblyOwner(JSCell* callee)
 {
 #if ENABLE(WEBASSEMBLY)
     // Each WebAssembly.Instance shares the stubs from their WebAssembly.Module, which are therefore the appropriate owner.
-    return jsCast<WebAssemblyToJSCallee*>(callee)->module();
+    return jsCast<JSWebAssemblyModule*>(callee);
 #else
     UNUSED_PARAM(callee);
     RELEASE_ASSERT_NOT_REACHED();
     return nullptr;
 #endif // ENABLE(WEBASSEMBLY)
 }
 
 void linkFor(
-    ExecState* exec, CallLinkInfo& callLinkInfo, CodeBlock* calleeCodeBlock,
+    VM& vm, CallFrame* callFrame, CallLinkInfo& callLinkInfo, CodeBlock* calleeCodeBlock,
     JSObject* callee, MacroAssemblerCodePtr<JSEntryPtrTag> codePtr)
 {
     ASSERT(!callLinkInfo.stub());
 
-    CallFrame* callerFrame = exec->callerFrame();
+    CallFrame* callerFrame = callFrame->callerFrame();
     // Our caller must have a cell for a callee. When calling
     // this from Wasm, we ensure the callee is a cell.
     ASSERT(callerFrame->callee().isCell());
 
-    VM& vm = callerFrame->vm();
     CodeBlock* callerCodeBlock = callerFrame->codeBlock();
 
     // WebAssembly -> JS stubs don't have a valid CodeBlock.
-    JSCell* owner = isWebAssemblyToJSCallee(callerFrame->callee().asCell()) ? webAssemblyOwner(callerFrame->callee().asCell()) : callerCodeBlock;
+    JSCell* owner = isWebAssemblyModule(callerFrame->callee().asCell()) ? webAssemblyOwner(callerFrame->callee().asCell()) : callerCodeBlock;
     ASSERT(owner);
 
     ASSERT(!callLinkInfo.isLinked());
     callLinkInfo.setCallee(vm, owner, callee);
     MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), callee);
@@ -865,16 +983,16 @@
 
     linkSlowFor(vm, callLinkInfo);
 }
 
 void linkDirectFor(
-    ExecState* exec, CallLinkInfo& callLinkInfo, CodeBlock* calleeCodeBlock,
+    CallFrame* callFrame, CallLinkInfo& callLinkInfo, CodeBlock* calleeCodeBlock,
     MacroAssemblerCodePtr<JSEntryPtrTag> codePtr)
 {
     ASSERT(!callLinkInfo.stub());
 
-    CodeBlock* callerCodeBlock = exec->codeBlock();
+    CodeBlock* callerCodeBlock = callFrame->codeBlock();
 
     VM& vm = callerCodeBlock->vm();
 
     ASSERT(!callLinkInfo.isLinked());
     callLinkInfo.setCodeBlock(vm, callerCodeBlock, jsCast<FunctionCodeBlock*>(calleeCodeBlock));
@@ -884,17 +1002,16 @@
     if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
         MacroAssembler::repatchJumpToNop(callLinkInfo.patchableJump());
     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel<JSEntryPtrTag>(codePtr));
 
     if (calleeCodeBlock)
-        calleeCodeBlock->linkIncomingCall(exec, &callLinkInfo);
+        calleeCodeBlock->linkIncomingCall(callFrame, &callLinkInfo);
 }
 
-void linkSlowFor(
-    ExecState* exec, CallLinkInfo& callLinkInfo)
+void linkSlowFor(CallFrame* callFrame, CallLinkInfo& callLinkInfo)
 {
-    CodeBlock* callerCodeBlock = exec->callerFrame()->codeBlock();
+    CodeBlock* callerCodeBlock = callFrame->callerFrame()->codeBlock();
     VM& vm = callerCodeBlock->vm();
 
     linkSlowFor(vm, callLinkInfo);
 }
 
@@ -925,61 +1042,58 @@
         callLinkInfo.remove();
 }
 
 void unlinkFor(VM& vm, CallLinkInfo& callLinkInfo)
 {
-    if (Options::dumpDisassembly())
-        dataLog("Unlinking call at ", callLinkInfo.hotPathOther(), "\n");
+    dataLogLnIf(Options::dumpDisassembly(), "Unlinking call at ", callLinkInfo.hotPathOther());
 
     revertCall(vm, callLinkInfo, vm.getCTIStub(linkCallThunkGenerator).retagged<JITStubRoutinePtrTag>());
 }
 
-static void linkVirtualFor(ExecState* exec, CallLinkInfo& callLinkInfo)
+static void linkVirtualFor(VM& vm, CallFrame* callFrame, CallLinkInfo& callLinkInfo)
 {
-    CallFrame* callerFrame = exec->callerFrame();
-    VM& vm = callerFrame->vm();
+    CallFrame* callerFrame = callFrame->callerFrame();
     CodeBlock* callerCodeBlock = callerFrame->codeBlock();
 
-    if (shouldDumpDisassemblyFor(callerCodeBlock))
-        dataLog("Linking virtual call at ", FullCodeOrigin(callerCodeBlock, callerFrame->codeOrigin()), "\n");
+    dataLogLnIf(shouldDumpDisassemblyFor(callerCodeBlock),
+        "Linking virtual call at ", FullCodeOrigin(callerCodeBlock, callerFrame->codeOrigin()));
 
     MacroAssemblerCodeRef<JITStubRoutinePtrTag> virtualThunk = virtualThunkFor(vm, callLinkInfo);
     revertCall(vm, callLinkInfo, virtualThunk);
-    callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
+    callLinkInfo.setSlowStub(GCAwareJITStubRoutine::create(virtualThunk, vm));
     callLinkInfo.setClearedByVirtual();
 }
 
 namespace {
 struct CallToCodePtr {
     CCallHelpers::Call call;
     MacroAssemblerCodePtr<JSEntryPtrTag> codePtr;
 };
 } // annonymous namespace
 
-void linkPolymorphicCall(
-    ExecState* exec, CallLinkInfo& callLinkInfo, CallVariant newVariant)
+void linkPolymorphicCall(JSGlobalObject* globalObject, CallFrame* callFrame, CallLinkInfo& callLinkInfo, CallVariant newVariant)
 {
     RELEASE_ASSERT(callLinkInfo.allowStubs());
 
-    CallFrame* callerFrame = exec->callerFrame();
-    VM& vm = callerFrame->vm();
+    CallFrame* callerFrame = callFrame->callerFrame();
+    VM& vm = globalObject->vm();
 
     // During execution of linkPolymorphicCall, we strongly assume that we never do GC.
     // GC jettisons CodeBlocks, changes CallLinkInfo etc. and breaks assumption done before and after this call.
     DeferGCForAWhile deferGCForAWhile(vm.heap);
 
     if (!newVariant) {
-        linkVirtualFor(exec, callLinkInfo);
+        linkVirtualFor(vm, callFrame, callLinkInfo);
         return;
     }
 
     // Our caller must be have a cell for a callee. When calling
     // this from Wasm, we ensure the callee is a cell.
     ASSERT(callerFrame->callee().isCell());
 
     CodeBlock* callerCodeBlock = callerFrame->codeBlock();
-    bool isWebAssembly = isWebAssemblyToJSCallee(callerFrame->callee().asCell());
+    bool isWebAssembly = isWebAssemblyModule(callerFrame->callee().asCell());
 
     // WebAssembly -> JS stubs don't have a valid CodeBlock.
     JSCell* owner = isWebAssembly ? webAssemblyOwner(callerFrame->callee().asCell()) : callerCodeBlock;
     ASSERT(owner);
 
@@ -1015,12 +1129,12 @@
         if (variant.executable() && !variant.executable()->isHostFunction()) {
             ExecutableBase* executable = variant.executable();
             codeBlock = jsCast<FunctionExecutable*>(executable)->codeBlockForCall();
             // If we cannot handle a callee, either because we don't have a CodeBlock or because arity mismatch,
             // assume that it's better for this whole thing to be a virtual call.
-            if (!codeBlock || exec->argumentCountIncludingThis() < static_cast<size_t>(codeBlock->numParameters()) || callLinkInfo.isVarargs()) {
-                linkVirtualFor(exec, callLinkInfo);
+            if (!codeBlock || callFrame->argumentCountIncludingThis() < static_cast<size_t>(codeBlock->numParameters()) || callLinkInfo.isVarargs()) {
+                linkVirtualFor(vm, callFrame, callLinkInfo);
                 return;
             }
         }
 
         int64_t newCaseValue = 0;
@@ -1035,11 +1149,11 @@
                 newCaseValue = bitwise_cast<intptr_t>(function);
             else
                 newCaseValue = bitwise_cast<intptr_t>(variant.internalFunction());
         }
 
-        if (!ASSERT_DISABLED) {
+        if (ASSERT_ENABLED) {
             if (caseValues.contains(newCaseValue)) {
                 dataLog("ERROR: Attempt to add duplicate case value.\n");
                 dataLog("Existing case values: ");
                 CommaPrinter comma;
                 for (auto& value : caseValues)
@@ -1065,11 +1179,11 @@
     else
         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSize();
 
     // We use list.size() instead of callCases.size() because we respect CallVariant size for now.
     if (list.size() > maxPolymorphicCallVariantListSize) {
-        linkVirtualFor(exec, callLinkInfo);
+        linkVirtualFor(vm, callFrame, callLinkInfo);
         return;
     }
 
     Vector<CallToCodePtr> calls(callCases.size());
     UniqueArray<uint32_t> fastCounts;
@@ -1131,13 +1245,14 @@
         // We would have already checked that the callee is a cell.
 #endif
         // FIXME: We could add a fast path for InternalFunction with closure call.
         slowPath.append(stubJit.branchIfNotFunction(calleeGPR));
 
-        stubJit.loadPtr(
-            CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutable()),
-            comparisonValueGPR);
+        stubJit.loadPtr(CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutableOrRareData()), comparisonValueGPR);
+        auto hasExecutable = stubJit.branchTestPtr(CCallHelpers::Zero, comparisonValueGPR, CCallHelpers::TrustedImm32(JSFunction::rareDataTag));
+        stubJit.loadPtr(CCallHelpers::Address(comparisonValueGPR, FunctionRareData::offsetOfExecutable() - JSFunction::rareDataTag), comparisonValueGPR);
+        hasExecutable.link(&stubJit);
     }
 
     BinarySwitch binarySwitch(comparisonValueGPR, caseValues, BinarySwitch::IntPtr);
     CCallHelpers::JumpList done;
     while (binarySwitch.advance(stubJit)) {
@@ -1191,19 +1306,20 @@
         stubJit.move(calleeGPR, GPRInfo::regT0);
 #if USE(JSVALUE32_64)
         stubJit.move(CCallHelpers::TrustedImm32(JSValue::CellTag), GPRInfo::regT1);
 #endif
     }
+    stubJit.move(CCallHelpers::TrustedImmPtr(globalObject), GPRInfo::regT3);
     stubJit.move(CCallHelpers::TrustedImmPtr(&callLinkInfo), GPRInfo::regT2);
     stubJit.move(CCallHelpers::TrustedImmPtr(callLinkInfo.callReturnLocation().untaggedExecutableAddress()), GPRInfo::regT4);
 
     stubJit.restoreReturnAddressBeforeReturn(GPRInfo::regT4);
     AssemblyHelpers::Jump slow = stubJit.jump();
 
     LinkBuffer patchBuffer(stubJit, owner, JITCompilationCanFail);
     if (patchBuffer.didFailToAllocate()) {
-        linkVirtualFor(exec, callLinkInfo);
+        linkVirtualFor(vm, callFrame, callLinkInfo);
         return;
     }
 
     RELEASE_ASSERT(callCases.size() == calls.size());
     for (CallToCodePtr callToCodePtr : calls) {
@@ -1227,11 +1343,11 @@
         FINALIZE_CODE_FOR(
             callerCodeBlock, patchBuffer, JITStubRoutinePtrTag,
             "Polymorphic call stub for %s, return point %p, targets %s",
                 isWebAssembly ? "WebAssembly" : toCString(*callerCodeBlock).data(), callLinkInfo.callReturnLocation().labelAtOffset(0).executableAddress(),
                 toCString(listDump(callCases)).data()),
-        vm, owner, exec->callerFrame(), callLinkInfo, callCases,
+        vm, owner, callFrame->callerFrame(), callLinkInfo, callCases,
         WTFMove(fastCounts)));
 
     MacroAssembler::replaceWithJump(
         MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
         CodeLocationLabel<JITStubRoutinePtrTag>(stubRoutine->code().code()));
@@ -1248,20 +1364,20 @@
     // stub.
     if (callLinkInfo.isOnList())
         callLinkInfo.remove();
 }
 
-void resetGetByID(CodeBlock* codeBlock, StructureStubInfo& stubInfo, GetByIDKind kind)
+void resetGetBy(CodeBlock* codeBlock, StructureStubInfo& stubInfo, GetByKind kind)
 {
-    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
-    InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
+    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
+    InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation);
 }
 
 void resetPutByID(CodeBlock* codeBlock, StructureStubInfo& stubInfo)
 {
-    V_JITOperation_ESsiJJI unoptimizedFunction = reinterpret_cast<V_JITOperation_ESsiJJI>(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation()).executableAddress());
-    V_JITOperation_ESsiJJI optimizedFunction;
+    V_JITOperation_GSsiJJI unoptimizedFunction = reinterpret_cast<V_JITOperation_GSsiJJI>(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation).executableAddress());
+    V_JITOperation_GSsiJJI optimizedFunction;
     if (unoptimizedFunction == operationPutByIdStrict || unoptimizedFunction == operationPutByIdStrictOptimize)
         optimizedFunction = operationPutByIdStrictOptimize;
     else if (unoptimizedFunction == operationPutByIdNonStrict || unoptimizedFunction == operationPutByIdNonStrictOptimize)
         optimizedFunction = operationPutByIdNonStrictOptimize;
     else if (unoptimizedFunction == operationPutByIdDirectStrict || unoptimizedFunction == operationPutByIdDirectStrictOptimize)
@@ -1269,23 +1385,23 @@
     else {
         ASSERT(unoptimizedFunction == operationPutByIdDirectNonStrict || unoptimizedFunction == operationPutByIdDirectNonStrictOptimize);
         optimizedFunction = operationPutByIdDirectNonStrictOptimize;
     }
 
-    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), optimizedFunction);
-    InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
+    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, optimizedFunction);
+    InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation);
 }
 
 static void resetPatchableJump(StructureStubInfo& stubInfo)
 {
-    MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation());
+    MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation);
 }
 
 void resetInByID(CodeBlock* codeBlock, StructureStubInfo& stubInfo)
 {
-    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
-    InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
+    ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInByIdOptimize);
+    InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation);
 }
 
 void resetInstanceOf(StructureStubInfo& stubInfo)
 {
     resetPatchableJump(stubInfo);
