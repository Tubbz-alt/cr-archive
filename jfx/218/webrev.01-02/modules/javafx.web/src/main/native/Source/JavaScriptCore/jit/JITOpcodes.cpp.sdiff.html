<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="JITNegGenerator.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITOpcodes32_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  12  *    documentation and/or other materials provided with the distribution.
  13  *
  14  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  15  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  16  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  17  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  18  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  19  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  20  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  21  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  22  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  23  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  24  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  25  */
  26 
  27 #include &quot;config.h&quot;
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;BasicBlockLocation.h&quot;
<span class="line-modified">  32 #include &quot;BytecodeStructs.h&quot;</span>
  33 #include &quot;Exception.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSCast.h&quot;
  39 #include &quot;JSFunction.h&quot;
  40 #include &quot;JSPropertyNameEnumerator.h&quot;
  41 #include &quot;LinkBuffer.h&quot;
  42 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  43 #include &quot;OpcodeInlines.h&quot;
  44 #include &quot;SlowPathCall.h&quot;
  45 #include &quot;SuperSampler.h&quot;
  46 #include &quot;ThunkGenerators.h&quot;
  47 #include &quot;TypeLocation.h&quot;
  48 #include &quot;TypeProfilerLog.h&quot;
  49 #include &quot;VirtualRegister.h&quot;
  50 #include &quot;Watchdog.h&quot;
  51 
  52 namespace JSC {
  53 
  54 #if USE(JSVALUE64)
  55 
  56 void JIT::emit_op_mov(const Instruction* currentInstruction)
  57 {
  58     auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
<span class="line-modified">  59     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">  60     int src = bytecode.m_src.offset();</span>
  61 
<span class="line-modified">  62     if (m_codeBlock-&gt;isConstantRegisterIndex(src)) {</span>
  63         JSValue value = m_codeBlock-&gt;getConstant(src);
  64         if (!value.isNumber())
  65             store64(TrustedImm64(JSValue::encode(value)), addressFor(dst));
  66         else
  67             store64(Imm64(JSValue::encode(value)), addressFor(dst));
  68         return;
  69     }
  70 
  71     load64(addressFor(src), regT0);
  72     store64(regT0, addressFor(dst));
  73 }
  74 
  75 
  76 void JIT::emit_op_end(const Instruction* currentInstruction)
  77 {
  78     auto bytecode = currentInstruction-&gt;as&lt;OpEnd&gt;();
  79     RELEASE_ASSERT(returnValueGPR != callFrameRegister);
<span class="line-modified">  80     emitGetVirtualRegister(bytecode.m_value.offset(), returnValueGPR);</span>
  81     emitRestoreCalleeSaves();
  82     emitFunctionEpilogue();
  83     ret();
  84 }
  85 
  86 void JIT::emit_op_jmp(const Instruction* currentInstruction)
  87 {
  88     auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
  89     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
  90     addJump(jump(), target);
  91 }
  92 
  93 void JIT::emit_op_new_object(const Instruction* currentInstruction)
  94 {
  95     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
  96     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  97     Structure* structure = metadata.m_objectAllocationProfile.structure();
  98     size_t allocationSize = JSFinalObject::allocationSize(structure-&gt;inlineCapacity());
  99     Allocator allocator = allocatorForNonVirtualConcurrently&lt;JSFinalObject&gt;(*m_vm, allocationSize, AllocatorForMode::AllocatorIfExists);
 100 
 101     RegisterID resultReg = regT0;
 102     RegisterID allocatorReg = regT1;
 103     RegisterID scratchReg = regT2;
 104 
 105     if (!allocator)
 106         addSlowCase(jump());
 107     else {
 108         JumpList slowCases;
 109         auto butterfly = TrustedImmPtr(nullptr);
 110         emitAllocateJSObject(resultReg, JITAllocator::constant(allocator), allocatorReg, TrustedImmPtr(structure), butterfly, scratchReg, slowCases);
 111         emitInitializeInlineStorage(resultReg, structure-&gt;inlineCapacity());
 112         addSlowCase(slowCases);
<span class="line-modified"> 113         emitPutVirtualRegister(bytecode.m_dst.offset());</span>
 114     }
 115 }
 116 
 117 void JIT::emitSlow_op_new_object(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 118 {
 119     linkAllSlowCases(iter);
 120 
 121     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
 122     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 123     int dst = bytecode.m_dst.offset();</span>
 124     Structure* structure = metadata.m_objectAllocationProfile.structure();
<span class="line-modified"> 125     callOperation(operationNewObject, structure);</span>
 126     emitStoreCell(dst, returnValueGPR);
 127 }
 128 
 129 void JIT::emit_op_overrides_has_instance(const Instruction* currentInstruction)
 130 {
 131     auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
<span class="line-modified"> 132     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 133     int constructor = bytecode.m_constructor.offset();</span>
<span class="line-modified"> 134     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();</span>
 135 
 136     emitGetVirtualRegister(hasInstanceValue, regT0);
 137 
 138     // We don&#39;t jump if we know what Symbol.hasInstance would do.
 139     Jump customhasInstanceValue = branchPtr(NotEqual, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()-&gt;functionProtoHasInstanceSymbolFunction()));
 140 
 141     emitGetVirtualRegister(constructor, regT0);
 142 
 143     // Check that constructor &#39;ImplementsDefaultHasInstance&#39; i.e. the object is not a C-API user nor a bound function.
 144     test8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(ImplementsDefaultHasInstance), regT0);
 145     boxBoolean(regT0, JSValueRegs { regT0 });
 146     Jump done = jump();
 147 
 148     customhasInstanceValue.link(this);
<span class="line-modified"> 149     move(TrustedImm32(ValueTrue), regT0);</span>
 150 
 151     done.link(this);
 152     emitPutVirtualRegister(dst);
 153 }
 154 
 155 void JIT::emit_op_instanceof(const Instruction* currentInstruction)
 156 {
 157     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
<span class="line-modified"> 158     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 159     int value = bytecode.m_value.offset();</span>
<span class="line-modified"> 160     int proto = bytecode.m_prototype.offset();</span>
 161 
 162     // Load the operands (baseVal, proto, and value respectively) into registers.
 163     // We use regT0 for baseVal since we will be done with this first, and we can then use it for the result.
 164     emitGetVirtualRegister(value, regT2);
 165     emitGetVirtualRegister(proto, regT1);
 166 
 167     // Check that proto are cells. baseVal must be a cell - this is checked by the get_by_id for Symbol.hasInstance.
 168     emitJumpSlowCaseIfNotJSCell(regT2, value);
 169     emitJumpSlowCaseIfNotJSCell(regT1, proto);
 170 
 171     JITInstanceOfGenerator gen(
<span class="line-modified"> 172         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset),</span>
 173         RegisterSet::stubUnavailableRegisters(),
 174         regT0, // result
 175         regT2, // value
 176         regT1, // proto
 177         regT3, regT4); // scratch
 178     gen.generateFastPath(*this);
 179     m_instanceOfs.append(gen);
 180 
 181     emitPutVirtualRegister(dst);
 182 }
 183 
 184 void JIT::emitSlow_op_instanceof(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 185 {
 186     linkAllSlowCases(iter);
 187 
 188     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
<span class="line-modified"> 189     int resultVReg = bytecode.m_dst.offset();</span>
 190 
 191     JITInstanceOfGenerator&amp; gen = m_instanceOfs[m_instanceOfIndex++];
 192 
 193     Label coldPathBegin = label();
<span class="line-modified"> 194     Call call = callOperation(operationInstanceOfOptimize, resultVReg, gen.stubInfo(), regT2, regT1);</span>
 195     gen.reportSlowPathCall(coldPathBegin, call);
 196 }
 197 
 198 void JIT::emit_op_instanceof_custom(const Instruction*)
 199 {
 200     // This always goes to slow path since we expect it to be rare.
 201     addSlowCase(jump());
 202 }
 203 
 204 void JIT::emit_op_is_empty(const Instruction* currentInstruction)
 205 {
 206     auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
<span class="line-modified"> 207     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 208     int value = bytecode.m_operand.offset();</span>
 209 
 210     emitGetVirtualRegister(value, regT0);
 211     compare64(Equal, regT0, TrustedImm32(JSValue::encode(JSValue())), regT0);
 212 
 213     boxBoolean(regT0, JSValueRegs { regT0 });
 214     emitPutVirtualRegister(dst);
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
<span class="line-modified"> 220     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 221     int value = bytecode.m_operand.offset();</span>
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
<span class="line-modified"> 226     compare64(Equal, regT0, TrustedImm32(ValueUndefined), regT0);</span>
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
 235     emitLoadStructure(vm(), regT0, regT1, regT2);
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
<span class="line-modified"> 249     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 250     int value = bytecode.m_operand.offset();</span>
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
<span class="line-modified"> 254     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-modified"> 255     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);</span>
 256 
 257     boxBoolean(regT0, JSValueRegs { regT0 });
 258     emitPutVirtualRegister(dst);
 259 }
 260 
 261 void JIT::emit_op_is_boolean(const Instruction* currentInstruction)
 262 {
 263     auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
<span class="line-modified"> 264     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 265     int value = bytecode.m_operand.offset();</span>
 266 
 267     emitGetVirtualRegister(value, regT0);
<span class="line-modified"> 268     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);</span>
 269     test64(Zero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1)), regT0);
 270     boxBoolean(regT0, JSValueRegs { regT0 });
 271     emitPutVirtualRegister(dst);
 272 }
 273 
 274 void JIT::emit_op_is_number(const Instruction* currentInstruction)
 275 {
 276     auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
<span class="line-modified"> 277     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 278     int value = bytecode.m_operand.offset();</span>
 279 
 280     emitGetVirtualRegister(value, regT0);
<span class="line-modified"> 281     test64(NonZero, regT0, tagTypeNumberRegister, regT0);</span>
 282     boxBoolean(regT0, JSValueRegs { regT0 });
 283     emitPutVirtualRegister(dst);
 284 }
 285 
 286 void JIT::emit_op_is_cell_with_type(const Instruction* currentInstruction)
 287 {
 288     auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
<span class="line-modified"> 289     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 290     int value = bytecode.m_operand.offset();</span>
 291     int type = bytecode.m_type;
 292 
 293     emitGetVirtualRegister(value, regT0);
 294     Jump isNotCell = branchIfNotCell(regT0);
 295 
 296     compare8(Equal, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(type), regT0);
 297     boxBoolean(regT0, JSValueRegs { regT0 });
 298     Jump done = jump();
 299 
 300     isNotCell.link(this);
<span class="line-modified"> 301     move(TrustedImm32(ValueFalse), regT0);</span>
 302 
 303     done.link(this);
 304     emitPutVirtualRegister(dst);
 305 }
 306 
 307 void JIT::emit_op_is_object(const Instruction* currentInstruction)
 308 {
 309     auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
<span class="line-modified"> 310     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 311     int value = bytecode.m_operand.offset();</span>
 312 
 313     emitGetVirtualRegister(value, regT0);
 314     Jump isNotCell = branchIfNotCell(regT0);
 315 
 316     compare8(AboveOrEqual, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType), regT0);
 317     boxBoolean(regT0, JSValueRegs { regT0 });
 318     Jump done = jump();
 319 
 320     isNotCell.link(this);
<span class="line-modified"> 321     move(TrustedImm32(ValueFalse), regT0);</span>
 322 
 323     done.link(this);
 324     emitPutVirtualRegister(dst);
 325 }
 326 
 327 void JIT::emit_op_ret(const Instruction* currentInstruction)
 328 {
 329     ASSERT(callFrameRegister != regT1);
 330     ASSERT(regT1 != returnValueGPR);
 331     ASSERT(returnValueGPR != callFrameRegister);
 332 
 333     // Return the result in %eax.
 334     auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
<span class="line-modified"> 335     emitGetVirtualRegister(bytecode.m_value.offset(), returnValueGPR);</span>
 336 
 337     checkStackPointerAlignment();
 338     emitRestoreCalleeSaves();
 339     emitFunctionEpilogue();
 340     ret();
 341 }
 342 
 343 void JIT::emit_op_to_primitive(const Instruction* currentInstruction)
 344 {
 345     auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
<span class="line-modified"> 346     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 347     int src = bytecode.m_src.offset();</span>
 348 
 349     emitGetVirtualRegister(src, regT0);
 350 
 351     Jump isImm = branchIfNotCell(regT0);
 352     addSlowCase(branchIfObject(regT0));
 353     isImm.link(this);
 354 
 355     if (dst != src)
 356         emitPutVirtualRegister(dst);
 357 
 358 }
 359 

















 360 void JIT::emit_op_set_function_name(const Instruction* currentInstruction)
 361 {
 362     auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
<span class="line-modified"> 363     emitGetVirtualRegister(bytecode.m_function.offset(), regT0);</span>
<span class="line-modified"> 364     emitGetVirtualRegister(bytecode.m_name.offset(), regT1);</span>
<span class="line-modified"> 365     callOperation(operationSetFunctionName, regT0, regT1);</span>
 366 }
 367 
 368 void JIT::emit_op_not(const Instruction* currentInstruction)
 369 {
 370     auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
<span class="line-modified"> 371     emitGetVirtualRegister(bytecode.m_operand.offset(), regT0);</span>
 372 
 373     // Invert against JSValue(false); if the value was tagged as a boolean, then all bits will be
 374     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 375     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
<span class="line-modified"> 376     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);</span>
 377     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
<span class="line-modified"> 378     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueTrue)), regT0);</span>
 379 
<span class="line-modified"> 380     emitPutVirtualRegister(bytecode.m_dst.offset());</span>
 381 }
 382 
 383 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 384 {
 385     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 386     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 387 
 388     GPRReg value = regT0;
 389     GPRReg scratch1 = regT1;
 390     GPRReg scratch2 = regT2;
 391     bool shouldCheckMasqueradesAsUndefined = true;
 392 
<span class="line-modified"> 393     emitGetVirtualRegister(bytecode.m_condition.offset(), value);</span>
 394     addJump(branchIfFalsey(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 395 }
 396 
 397 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 398 {
 399     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
<span class="line-modified"> 400     int src = bytecode.m_value.offset();</span>
 401     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 402 
 403     emitGetVirtualRegister(src, regT0);
 404     Jump isImmediate = branchIfNotCell(regT0);
 405 
 406     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 407     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 408     emitLoadStructure(vm(), regT0, regT2, regT1);
 409     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 410     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 411     Jump masqueradesGlobalObjectIsForeign = jump();
 412 
 413     // Now handle the immediate cases - undefined &amp; null
 414     isImmediate.link(this);
<span class="line-modified"> 415     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
 416     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 417 
 418     isNotMasqueradesAsUndefined.link(this);
 419     masqueradesGlobalObjectIsForeign.link(this);
 420 };
 421 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 422 {
 423     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
<span class="line-modified"> 424     int src = bytecode.m_value.offset();</span>
 425     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 426 
 427     emitGetVirtualRegister(src, regT0);
 428     Jump isImmediate = branchIfNotCell(regT0);
 429 
 430     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 431     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
 432     emitLoadStructure(vm(), regT0, regT2, regT1);
 433     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 434     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 435     Jump wasNotImmediate = jump();
 436 
 437     // Now handle the immediate cases - undefined &amp; null
 438     isImmediate.link(this);
<span class="line-modified"> 439     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
 440     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 441 
 442     wasNotImmediate.link(this);
 443 }
 444 
 445 void JIT::emit_op_jundefined_or_null(const Instruction* currentInstruction)
 446 {
 447     auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();
<span class="line-modified"> 448     int value = bytecode.m_value.offset();</span>
 449     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 450 
 451     emitGetVirtualRegister(value, regT0);
 452 
<span class="line-modified"> 453     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
 454     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 455 }
 456 
 457 void JIT::emit_op_jnundefined_or_null(const Instruction* currentInstruction)
 458 {
 459     auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();
<span class="line-modified"> 460     int value = bytecode.m_value.offset();</span>
 461     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 462 
 463     emitGetVirtualRegister(value, regT0);
 464 
<span class="line-modified"> 465     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
 466     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 467 }
 468 
 469 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 470 {
 471     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 472     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 473     int src = bytecode.m_value.offset();</span>
<span class="line-modified"> 474     Special::Pointer ptr = bytecode.m_specialPointer;</span>

 475     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 476 
 477     emitGetVirtualRegister(src, regT0);
<span class="line-modified"> 478     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(actualPointerFor(m_codeBlock, ptr)));</span>
 479     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 480     addJump(jump(), target);
 481     equal.link(this);
 482 }
 483 
 484 void JIT::emit_op_eq(const Instruction* currentInstruction)
 485 {
 486     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
<span class="line-modified"> 487     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);</span>
 488     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 489     compare32(Equal, regT1, regT0, regT0);
 490     boxBoolean(regT0, JSValueRegs { regT0 });
<span class="line-modified"> 491     emitPutVirtualRegister(bytecode.m_dst.offset());</span>
 492 }
 493 
 494 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 495 {
 496     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 497     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 498     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);</span>
 499     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 500     addJump(branch32(Equal, regT0, regT1), target);
 501 }
 502 
 503 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 504 {
 505     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 506     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 507 
 508     GPRReg value = regT0;
 509     GPRReg scratch1 = regT1;
 510     GPRReg scratch2 = regT2;
 511     bool shouldCheckMasqueradesAsUndefined = true;
<span class="line-modified"> 512     emitGetVirtualRegister(bytecode.m_condition.offset(), value);</span>
 513     addJump(branchIfTruthy(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 514 }
 515 
 516 void JIT::emit_op_neq(const Instruction* currentInstruction)
 517 {
 518     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
<span class="line-modified"> 519     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);</span>
 520     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 521     compare32(NotEqual, regT1, regT0, regT0);
 522     boxBoolean(regT0, JSValueRegs { regT0 });
 523 
<span class="line-modified"> 524     emitPutVirtualRegister(bytecode.m_dst.offset());</span>
 525 }
 526 
 527 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 528 {
 529     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 530     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 531     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);</span>
 532     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 533     addJump(branch32(NotEqual, regT0, regT1), target);
 534 }
 535 
 536 void JIT::emit_op_throw(const Instruction* currentInstruction)
 537 {
 538     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 539     ASSERT(regT0 == returnValueGPR);
 540     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
<span class="line-modified"> 541     emitGetVirtualRegister(bytecode.m_value.offset(), regT0);</span>
<span class="line-modified"> 542     callOperationNoExceptionCheck(operationThrow, regT0);</span>
 543     jumpToExceptionHandler(vm());
 544 }
 545 
 546 template&lt;typename Op&gt;
 547 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 548 {
 549     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
<span class="line-modified"> 550     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 551     int src1 = bytecode.m_lhs.offset();</span>
<span class="line-modified"> 552     int src2 = bytecode.m_rhs.offset();</span>
 553 
 554     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 555 
 556     // Jump slow if both are cells (to cover strings).
 557     move(regT0, regT2);
 558     or64(regT1, regT2);
 559     addSlowCase(branchIfCell(regT2));
 560 
 561     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 562     // if it&#39;s a double.
 563     Jump leftOK = branchIfInt32(regT0);
 564     addSlowCase(branchIfNumber(regT0));
 565     leftOK.link(this);
 566     Jump rightOK = branchIfInt32(regT1);
 567     addSlowCase(branchIfNumber(regT1));
 568     rightOK.link(this);
 569 
 570     if (type == CompileOpStrictEqType::StrictEq)
 571         compare64(Equal, regT1, regT0, regT0);
 572     else
</pre>
<hr />
<pre>
 574     boxBoolean(regT0, JSValueRegs { regT0 });
 575 
 576     emitPutVirtualRegister(dst);
 577 }
 578 
 579 void JIT::emit_op_stricteq(const Instruction* currentInstruction)
 580 {
 581     compileOpStrictEq&lt;OpStricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 582 }
 583 
 584 void JIT::emit_op_nstricteq(const Instruction* currentInstruction)
 585 {
 586     compileOpStrictEq&lt;OpNstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 587 }
 588 
 589 template&lt;typename Op&gt;
 590 void JIT::compileOpStrictEqJump(const Instruction* currentInstruction, CompileOpStrictEqType type)
 591 {
 592     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 593     int target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 594     int src1 = bytecode.m_lhs.offset();</span>
<span class="line-modified"> 595     int src2 = bytecode.m_rhs.offset();</span>
 596 
 597     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 598 
 599     // Jump slow if both are cells (to cover strings).
 600     move(regT0, regT2);
 601     or64(regT1, regT2);
 602     addSlowCase(branchIfCell(regT2));
 603 
 604     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 605     // if it&#39;s a double.
 606     Jump leftOK = branchIfInt32(regT0);
 607     addSlowCase(branchIfNumber(regT0));
 608     leftOK.link(this);
 609     Jump rightOK = branchIfInt32(regT1);
 610     addSlowCase(branchIfNumber(regT1));
 611     rightOK.link(this);
 612 
 613     if (type == CompileOpStrictEqType::StrictEq)
 614         addJump(branch64(Equal, regT1, regT0), target);
 615     else
 616         addJump(branch64(NotEqual, regT1, regT0), target);
 617 }
 618 
 619 void JIT::emit_op_jstricteq(const Instruction* currentInstruction)
 620 {
 621     compileOpStrictEqJump&lt;OpJstricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 622 }
 623 
 624 void JIT::emit_op_jnstricteq(const Instruction* currentInstruction)
 625 {
 626     compileOpStrictEqJump&lt;OpJnstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 627 }
 628 
 629 void JIT::emitSlow_op_jstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 630 {
 631     linkAllSlowCases(iter);
 632 
 633     auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
 634     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 635     callOperation(operationCompareStrictEq, regT0, regT1);</span>
 636     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 637 }
 638 
 639 void JIT::emitSlow_op_jnstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 640 {
 641     linkAllSlowCases(iter);
 642 
 643     auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
 644     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 645     callOperation(operationCompareStrictEq, regT0, regT1);</span>
 646     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 647 }
 648 
 649 void JIT::emit_op_to_number(const Instruction* currentInstruction)
 650 {
 651     auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
<span class="line-modified"> 652     int dstVReg = bytecode.m_dst.offset();</span>
<span class="line-modified"> 653     int srcVReg = bytecode.m_operand.offset();</span>














 654     emitGetVirtualRegister(srcVReg, regT0);
 655 





 656     addSlowCase(branchIfNotNumber(regT0));

 657 
 658     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 659     if (srcVReg != dstVReg)
 660         emitPutVirtualRegister(dstVReg);
 661 }
 662 
 663 void JIT::emit_op_to_string(const Instruction* currentInstruction)
 664 {
 665     auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
<span class="line-modified"> 666     int srcVReg = bytecode.m_operand.offset();</span>
 667     emitGetVirtualRegister(srcVReg, regT0);
 668 
 669     addSlowCase(branchIfNotCell(regT0));
 670     addSlowCase(branchIfNotString(regT0));
 671 
<span class="line-modified"> 672     emitPutVirtualRegister(bytecode.m_dst.offset());</span>
 673 }
 674 
 675 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 676 {
 677     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
<span class="line-modified"> 678     int dstVReg = bytecode.m_dst.offset();</span>
<span class="line-modified"> 679     int srcVReg = bytecode.m_operand.offset();</span>
 680     emitGetVirtualRegister(srcVReg, regT0);
 681 
 682     addSlowCase(branchIfNotCell(regT0));
 683     addSlowCase(branchIfNotObject(regT0));
 684 
 685     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 686     if (srcVReg != dstVReg)
 687         emitPutVirtualRegister(dstVReg);
 688 }
 689 
 690 void JIT::emit_op_catch(const Instruction* currentInstruction)
 691 {
 692     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 693 
 694     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 695 
 696     move(TrustedImmPtr(m_vm), regT3);
 697     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 698     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 699 
 700     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 701 
<span class="line-modified"> 702     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler);</span>
 703     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
 704     jumpToExceptionHandler(vm());
 705     isCatchableException.link(this);
 706 
 707     move(TrustedImmPtr(m_vm), regT3);
 708     load64(Address(regT3, VM::exceptionOffset()), regT0);
 709     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
<span class="line-modified"> 710     emitPutVirtualRegister(bytecode.m_exception.offset());</span>
 711 
 712     load64(Address(regT0, Exception::valueOffset()), regT0);
<span class="line-modified"> 713     emitPutVirtualRegister(bytecode.m_thrownValue.offset());</span>
 714 
 715 #if ENABLE(DFG_JIT)
 716     // FIXME: consider inline caching the process of doing OSR entry, including
 717     // argument type proofs, storing locals to the buffer, etc
 718     // https://bugs.webkit.org/show_bug.cgi?id=175598
 719 
 720     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 721     ValueProfileAndOperandBuffer* buffer = metadata.m_buffer;</span>
 722     if (buffer || !shouldEmitProfiling())
<span class="line-modified"> 723         callOperation(operationTryOSREnterAtCatch, m_bytecodeOffset);</span>
 724     else
<span class="line-modified"> 725         callOperation(operationTryOSREnterAtCatchAndValueProfile, m_bytecodeOffset);</span>
 726     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 727     emitRestoreCalleeSaves();
 728     farJump(returnValueGPR, ExceptionHandlerPtrTag);
 729     skipOSREntry.link(this);
 730     if (buffer &amp;&amp; shouldEmitProfiling()) {
<span class="line-modified"> 731         buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {</span>
 732             JSValueRegs regs(regT0);
 733             emitGetVirtualRegister(profile.m_operand, regs);
 734             emitValueProfilingSite(static_cast&lt;ValueProfile&amp;&gt;(profile));
 735         });
 736     }
 737 #endif // ENABLE(DFG_JIT)
 738 }
 739 
 740 void JIT::emit_op_identity_with_profile(const Instruction*)
 741 {
 742     // We don&#39;t need to do anything here...
 743 }
 744 
 745 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 746 {
 747     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
<span class="line-modified"> 748     int currentScope = bytecode.m_scope.offset();</span>
 749     emitGetVirtualRegister(currentScope, regT0);
 750     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
<span class="line-modified"> 751     emitStoreCell(bytecode.m_dst.offset(), regT0);</span>
 752 }
 753 
 754 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 755 {
 756     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 757     size_t tableIndex = bytecode.m_tableIndex;
 758     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
<span class="line-modified"> 759     unsigned scrutinee = bytecode.m_scrutinee.offset();</span>
 760 
 761     // create jump table for switch destinations, track this switch statement.
 762     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
<span class="line-modified"> 763     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Immediate));</span>
 764     jumpTable-&gt;ensureCTITable();
 765 
 766     emitGetVirtualRegister(scrutinee, regT0);
<span class="line-modified"> 767     callOperation(operationSwitchImmWithUnknownKeyType, regT0, tableIndex);</span>
 768     farJump(returnValueGPR, JSSwitchPtrTag);
 769 }
 770 
 771 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 772 {
 773     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 774     size_t tableIndex = bytecode.m_tableIndex;
 775     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
<span class="line-modified"> 776     unsigned scrutinee = bytecode.m_scrutinee.offset();</span>
 777 
 778     // create jump table for switch destinations, track this switch statement.
 779     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
<span class="line-modified"> 780     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Character));</span>
 781     jumpTable-&gt;ensureCTITable();
 782 
 783     emitGetVirtualRegister(scrutinee, regT0);
<span class="line-modified"> 784     callOperation(operationSwitchCharWithUnknownKeyType, regT0, tableIndex);</span>
 785     farJump(returnValueGPR, JSSwitchPtrTag);
 786 }
 787 
 788 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 789 {
 790     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 791     size_t tableIndex = bytecode.m_tableIndex;
 792     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
<span class="line-modified"> 793     unsigned scrutinee = bytecode.m_scrutinee.offset();</span>
 794 
 795     // create jump table for switch destinations, track this switch statement.
 796     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
<span class="line-modified"> 797     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset));</span>
 798 
 799     emitGetVirtualRegister(scrutinee, regT0);
<span class="line-modified"> 800     callOperation(operationSwitchStringWithUnknownKeyType, regT0, tableIndex);</span>
 801     farJump(returnValueGPR, JSSwitchPtrTag);
 802 }
 803 
 804 void JIT::emit_op_debug(const Instruction* currentInstruction)
 805 {
 806     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 807     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 808     Jump noDebuggerRequests = branchTest32(Zero, regT0);
<span class="line-modified"> 809     callOperation(operationDebug, static_cast&lt;int&gt;(bytecode.m_debugHookType));</span>
 810     noDebuggerRequests.link(this);
 811 }
 812 
 813 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 814 {
 815     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
<span class="line-modified"> 816     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 817     int src1 = bytecode.m_operand.offset();</span>
 818 
 819     emitGetVirtualRegister(src1, regT0);
 820     Jump isImmediate = branchIfNotCell(regT0);
 821 
 822     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 823     move(TrustedImm32(0), regT0);
 824     Jump wasNotMasqueradesAsUndefined = jump();
 825 
 826     isMasqueradesAsUndefined.link(this);
 827     emitLoadStructure(vm(), regT0, regT2, regT1);
 828     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 829     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 830     comparePtr(Equal, regT0, regT2, regT0);
 831     Jump wasNotImmediate = jump();
 832 
 833     isImmediate.link(this);
 834 
<span class="line-modified"> 835     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-modified"> 836     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);</span>
 837 
 838     wasNotImmediate.link(this);
 839     wasNotMasqueradesAsUndefined.link(this);
 840 
 841     boxBoolean(regT0, JSValueRegs { regT0 });
 842     emitPutVirtualRegister(dst);
 843 
 844 }
 845 
 846 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 847 {
 848     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
<span class="line-modified"> 849     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 850     int src1 = bytecode.m_operand.offset();</span>
 851 
 852     emitGetVirtualRegister(src1, regT0);
 853     Jump isImmediate = branchIfNotCell(regT0);
 854 
 855     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 856     move(TrustedImm32(1), regT0);
 857     Jump wasNotMasqueradesAsUndefined = jump();
 858 
 859     isMasqueradesAsUndefined.link(this);
 860     emitLoadStructure(vm(), regT0, regT2, regT1);
 861     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 862     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 863     comparePtr(NotEqual, regT0, regT2, regT0);
 864     Jump wasNotImmediate = jump();
 865 
 866     isImmediate.link(this);
 867 
<span class="line-modified"> 868     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-modified"> 869     compare64(NotEqual, regT0, TrustedImm32(ValueNull), regT0);</span>
 870 
 871     wasNotImmediate.link(this);
 872     wasNotMasqueradesAsUndefined.link(this);
 873 
 874     boxBoolean(regT0, JSValueRegs { regT0 });
 875     emitPutVirtualRegister(dst);
 876 }
 877 














 878 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 879 {
 880     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
<span class="line-modified"> 881     int dst = bytecode.m_dst.offset();</span>
 882     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 883     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 884     emitStoreCell(dst, regT0);
 885 }
 886 
 887 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 888 {
 889     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 890     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 891     StructureID* cachedStructureID = &amp;metadata.m_cachedStructureID;
<span class="line-modified"> 892     emitGetVirtualRegister(bytecode.m_srcDst.offset(), regT1);</span>
 893 
 894     emitJumpSlowCaseIfNotJSCell(regT1);
 895 
 896     addSlowCase(branchIfNotType(regT1, FinalObjectType));
 897     load32(cachedStructureID, regT2);
 898     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 899 }
 900 
 901 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 902 {
 903     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 904     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 905     int callee = bytecode.m_callee.offset();</span>
 906     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 907     RegisterID calleeReg = regT0;
 908     RegisterID rareDataReg = regT4;
 909     RegisterID resultReg = regT0;
 910     RegisterID allocatorReg = regT1;
 911     RegisterID structureReg = regT2;
 912     RegisterID cachedFunctionReg = regT4;
 913     RegisterID scratchReg = regT3;
 914 
 915     emitGetVirtualRegister(callee, calleeReg);
 916     addSlowCase(branchIfNotFunction(calleeReg));
<span class="line-modified"> 917     loadPtr(Address(calleeReg, JSFunction::offsetOfRareData()), rareDataReg);</span>
<span class="line-modified"> 918     addSlowCase(branchTestPtr(Zero, rareDataReg));</span>
<span class="line-modified"> 919     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfAllocator()), allocatorReg);</span>
<span class="line-modified"> 920     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfStructure()), structureReg);</span>
 921 
 922     loadPtr(cachedFunction, cachedFunctionReg);
 923     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 924     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 925     hasSeenMultipleCallees.link(this);
 926 
 927     JumpList slowCases;
 928     auto butterfly = TrustedImmPtr(nullptr);
 929     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
 930     load8(Address(structureReg, Structure::inlineCapacityOffset()), scratchReg);
 931     emitInitializeInlineStorage(resultReg, scratchReg);
 932     addSlowCase(slowCases);
<span class="line-modified"> 933     emitPutVirtualRegister(bytecode.m_dst.offset());</span>
 934 }
 935 
 936 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 937 {
 938     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
<span class="line-modified"> 939     emitGetVirtualRegister(bytecode.m_targetVirtualRegister.offset(), regT0);</span>
 940     addSlowCase(branchIfEmpty(regT0));
 941 }
 942 
 943 
 944 // Slow cases
 945 
 946 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 947 {
 948     linkAllSlowCases(iter);
 949 
 950     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
<span class="line-modified"> 951     callOperation(operationCompareEq, regT0, regT1);</span>
 952     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
<span class="line-modified"> 953     emitPutVirtualRegister(bytecode.m_dst.offset(), returnValueGPR);</span>
 954 }
 955 
 956 void JIT::emitSlow_op_neq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 957 {
 958     linkAllSlowCases(iter);
 959 
 960     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
<span class="line-modified"> 961     callOperation(operationCompareEq, regT0, regT1);</span>
 962     xor32(TrustedImm32(0x1), regT0);
 963     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
<span class="line-modified"> 964     emitPutVirtualRegister(bytecode.m_dst.offset(), returnValueGPR);</span>
 965 }
 966 
 967 void JIT::emitSlow_op_jeq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 968 {
 969     linkAllSlowCases(iter);
 970 
 971     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 972     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 973     callOperation(operationCompareEq, regT0, regT1);</span>
 974     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 975 }
 976 
 977 void JIT::emitSlow_op_jneq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 978 {
 979     linkAllSlowCases(iter);
 980 
 981     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 982     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 983     callOperation(operationCompareEq, regT0, regT1);</span>
 984     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 985 }
 986 
 987 void JIT::emitSlow_op_instanceof_custom(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 988 {
 989     linkAllSlowCases(iter);
 990 
 991     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
<span class="line-modified"> 992     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified"> 993     int value = bytecode.m_value.offset();</span>
<span class="line-modified"> 994     int constructor = bytecode.m_constructor.offset();</span>
<span class="line-modified"> 995     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();</span>
 996 
 997     emitGetVirtualRegister(value, regT0);
 998     emitGetVirtualRegister(constructor, regT1);
 999     emitGetVirtualRegister(hasInstanceValue, regT2);
<span class="line-modified">1000     callOperation(operationInstanceOfCustom, regT0, regT1, regT2);</span>
1001     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1002     emitPutVirtualRegister(dst, returnValueGPR);
1003 }
1004 
1005 #endif // USE(JSVALUE64)
1006 
1007 void JIT::emit_op_loop_hint(const Instruction*)
1008 {
<span class="line-removed">1009     // Check traps.</span>
<span class="line-removed">1010     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
<span class="line-removed">1011 #if ENABLE(DFG_JIT)</span>
1012     // Emit the JIT optimization check:
1013     if (canBeOptimized()) {
1014         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1015             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1016     }
<span class="line-removed">1017 #endif</span>
1018 }
1019 
1020 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1021 {
<span class="line-removed">1022     linkSlowCase(iter);</span>
<span class="line-removed">1023     callOperation(operationHandleTraps);</span>
1024 #if ENABLE(DFG_JIT)
1025     // Emit the slow path for the JIT optimization check:
1026     if (canBeOptimized()) {
<span class="line-modified">1027         emitJumpSlowToHot(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForLoop()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())), currentInstruction-&gt;size());</span>
<span class="line-removed">1028         linkSlowCase(iter);</span>
1029 
1030         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
1031 
<span class="line-modified">1032         callOperation(operationOptimize, m_bytecodeOffset);</span>
<span class="line-modified">1033         emitJumpSlowToHot(branchTestPtr(Zero, returnValueGPR), currentInstruction-&gt;size());</span>
<span class="line-modified">1034         if (!ASSERT_DISABLED) {</span>
1035             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1036             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1037             ok.link(this);
1038         }
1039         farJump(returnValueGPR, GPRInfo::callFrameRegister);



1040     }
1041 #else
1042     UNUSED_PARAM(currentInstruction);

1043 #endif
1044 }
1045 





1046 void JIT::emit_op_nop(const Instruction*)
1047 {
1048 }
1049 
1050 void JIT::emit_op_super_sampler_begin(const Instruction*)
1051 {
1052     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1053 }
1054 
1055 void JIT::emit_op_super_sampler_end(const Instruction*)
1056 {
1057     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1058 }
1059 
<span class="line-modified">1060 void JIT::emit_op_enter(const Instruction*)</span>
1061 {
<span class="line-modified">1062     // Even though JIT doesn&#39;t use them, we initialize our constant</span>
<span class="line-removed">1063     // registers to zap stale pointers, to avoid unnecessarily prolonging</span>
<span class="line-removed">1064     // object lifetime and increasing GC pressure.</span>
<span class="line-removed">1065     size_t count = m_codeBlock-&gt;numVars();</span>
<span class="line-removed">1066     for (size_t i = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); i &lt; count; ++i)</span>
<span class="line-removed">1067         emitInitRegister(virtualRegisterForLocal(i).offset());</span>
<span class="line-removed">1068 </span>
<span class="line-removed">1069     emitWriteBarrier(m_codeBlock);</span>
<span class="line-removed">1070 </span>
<span class="line-removed">1071     // Check traps.</span>
<span class="line-removed">1072     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
<span class="line-removed">1073 </span>
<span class="line-removed">1074 #if ENABLE(DFG_JIT)</span>
<span class="line-removed">1075     if (canBeOptimized())</span>
<span class="line-removed">1076         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));</span>
<span class="line-removed">1077 #endif</span>
<span class="line-removed">1078 }</span>
<span class="line-removed">1079 </span>
<span class="line-removed">1080 void JIT::emitSlow_op_enter(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)</span>
<span class="line-removed">1081 {</span>
<span class="line-removed">1082     linkSlowCase(iter);</span>
<span class="line-removed">1083     callOperation(operationHandleTraps);</span>
<span class="line-removed">1084 #if ENABLE(DFG_JIT)</span>
<span class="line-removed">1085     if (canBeOptimized()) {</span>
<span class="line-removed">1086         emitJumpSlowToHot(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())), currentInstruction-&gt;size());</span>
<span class="line-removed">1087         linkSlowCase(iter);</span>
<span class="line-removed">1088 </span>
<span class="line-removed">1089         ASSERT(!m_bytecodeOffset);</span>
<span class="line-removed">1090 </span>
<span class="line-removed">1091         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
1092 
<span class="line-modified">1093         callOperation(operationOptimize, m_bytecodeOffset);</span>
<span class="line-removed">1094         emitJumpSlowToHot(branchTestPtr(Zero, returnValueGPR), currentInstruction-&gt;size());</span>
<span class="line-removed">1095         farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-removed">1096     }</span>
<span class="line-removed">1097 #else</span>
<span class="line-removed">1098     UNUSED_PARAM(currentInstruction);</span>
<span class="line-removed">1099 #endif</span>
1100 }
1101 
1102 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1103 {
1104     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
<span class="line-modified">1105     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1106     int regexp = bytecode.m_regexp.offset();</span>
<span class="line-modified">1107     callOperation(operationNewRegexp, jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));</span>
1108     emitStoreCell(dst, returnValueGPR);
1109 }
1110 
1111 template&lt;typename Op&gt;
1112 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1113 {
1114     Jump lazyJump;
1115     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
<span class="line-modified">1116     int dst = bytecode.m_dst.offset();</span>
1117 
1118 #if USE(JSVALUE64)
<span class="line-modified">1119     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);</span>
1120 #else
<span class="line-modified">1121     emitLoadPayload(bytecode.m_scope.offset(), regT0);</span>
1122 #endif
1123     FunctionExecutable* funcExec = m_codeBlock-&gt;functionDecl(bytecode.m_functionDecl);
1124 
1125     OpcodeID opcodeID = Op::opcodeID;
1126     if (opcodeID == op_new_func)
<span class="line-modified">1127         callOperation(operationNewFunction, dst, regT0, funcExec);</span>
1128     else if (opcodeID == op_new_generator_func)
<span class="line-modified">1129         callOperation(operationNewGeneratorFunction, dst, regT0, funcExec);</span>
1130     else if (opcodeID == op_new_async_func)
<span class="line-modified">1131         callOperation(operationNewAsyncFunction, dst, regT0, funcExec);</span>
1132     else {
1133         ASSERT(opcodeID == op_new_async_generator_func);
<span class="line-modified">1134         callOperation(operationNewAsyncGeneratorFunction, dst, regT0, funcExec);</span>
1135     }
1136 }
1137 
1138 void JIT::emit_op_new_func(const Instruction* currentInstruction)
1139 {
1140     emitNewFuncCommon&lt;OpNewFunc&gt;(currentInstruction);
1141 }
1142 
1143 void JIT::emit_op_new_generator_func(const Instruction* currentInstruction)
1144 {
1145     emitNewFuncCommon&lt;OpNewGeneratorFunc&gt;(currentInstruction);
1146 }
1147 
1148 void JIT::emit_op_new_async_generator_func(const Instruction* currentInstruction)
1149 {
1150     emitNewFuncCommon&lt;OpNewAsyncGeneratorFunc&gt;(currentInstruction);
1151 }
1152 
1153 void JIT::emit_op_new_async_func(const Instruction* currentInstruction)
1154 {
1155     emitNewFuncCommon&lt;OpNewAsyncFunc&gt;(currentInstruction);
1156 }
1157 
1158 template&lt;typename Op&gt;
1159 void JIT::emitNewFuncExprCommon(const Instruction* currentInstruction)
1160 {
1161     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
<span class="line-modified">1162     int dst = bytecode.m_dst.offset();</span>
1163 #if USE(JSVALUE64)
<span class="line-modified">1164     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);</span>
1165 #else
<span class="line-modified">1166     emitLoadPayload(bytecode.m_scope.offset(), regT0);</span>
1167 #endif
1168 
1169     FunctionExecutable* function = m_codeBlock-&gt;functionExpr(bytecode.m_functionDecl);
1170     OpcodeID opcodeID = Op::opcodeID;
1171 
1172     if (opcodeID == op_new_func_exp)
<span class="line-modified">1173         callOperation(operationNewFunction, dst, regT0, function);</span>
1174     else if (opcodeID == op_new_generator_func_exp)
<span class="line-modified">1175         callOperation(operationNewGeneratorFunction, dst, regT0, function);</span>
1176     else if (opcodeID == op_new_async_func_exp)
<span class="line-modified">1177         callOperation(operationNewAsyncFunction, dst, regT0, function);</span>
1178     else {
1179         ASSERT(opcodeID == op_new_async_generator_func_exp);
<span class="line-modified">1180         callOperation(operationNewAsyncGeneratorFunction, dst, regT0, function);</span>
1181     }
1182 }
1183 
1184 void JIT::emit_op_new_func_exp(const Instruction* currentInstruction)
1185 {
1186     emitNewFuncExprCommon&lt;OpNewFuncExp&gt;(currentInstruction);
1187 }
1188 
1189 void JIT::emit_op_new_generator_func_exp(const Instruction* currentInstruction)
1190 {
1191     emitNewFuncExprCommon&lt;OpNewGeneratorFuncExp&gt;(currentInstruction);
1192 }
1193 
1194 void JIT::emit_op_new_async_func_exp(const Instruction* currentInstruction)
1195 {
1196     emitNewFuncExprCommon&lt;OpNewAsyncFuncExp&gt;(currentInstruction);
1197 }
1198 
1199 void JIT::emit_op_new_async_generator_func_exp(const Instruction* currentInstruction)
1200 {
1201     emitNewFuncExprCommon&lt;OpNewAsyncGeneratorFuncExp&gt;(currentInstruction);
1202 }
1203 
1204 void JIT::emit_op_new_array(const Instruction* currentInstruction)
1205 {
1206     auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
1207     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified">1208     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1209     int valuesIndex = bytecode.m_argv.offset();</span>
1210     int size = bytecode.m_argc;
<span class="line-modified">1211     addPtr(TrustedImm32(valuesIndex * sizeof(Register)), callFrameRegister, regT0);</span>
<span class="line-modified">1212     callOperation(operationNewArrayWithProfile, dst,</span>
1213         &amp;metadata.m_arrayAllocationProfile, regT0, size);
1214 }
1215 
1216 void JIT::emit_op_new_array_with_size(const Instruction* currentInstruction)
1217 {
1218     auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
1219     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified">1220     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1221     int sizeIndex = bytecode.m_length.offset();</span>
1222 #if USE(JSVALUE64)
1223     emitGetVirtualRegister(sizeIndex, regT0);
<span class="line-modified">1224     callOperation(operationNewArrayWithSizeAndProfile, dst,</span>
1225         &amp;metadata.m_arrayAllocationProfile, regT0);
1226 #else
1227     emitLoad(sizeIndex, regT1, regT0);
<span class="line-modified">1228     callOperation(operationNewArrayWithSizeAndProfile, dst,</span>
1229         &amp;metadata.m_arrayAllocationProfile, JSValueRegs(regT1, regT0));
1230 #endif
1231 }
1232 
1233 #if USE(JSVALUE64)
1234 void JIT::emit_op_has_structure_property(const Instruction* currentInstruction)
1235 {
1236     auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
<span class="line-modified">1237     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1238     int base = bytecode.m_base.offset();</span>
<span class="line-modified">1239     int enumerator = bytecode.m_enumerator.offset();</span>
1240 
1241     emitGetVirtualRegister(base, regT0);
1242     emitGetVirtualRegister(enumerator, regT1);
1243     emitJumpSlowCaseIfNotJSCell(regT0, base);
1244 
1245     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1246     addSlowCase(branch32(NotEqual, regT0, Address(regT1, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1247 
1248     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1249     emitPutVirtualRegister(dst);
1250 }
1251 
1252 void JIT::privateCompileHasIndexedProperty(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1253 {
1254     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1255 
1256     PatchableJump badType;
1257 
1258     // FIXME: Add support for other types like TypedArrays and Arguments.
1259     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
</pre>
<hr />
<pre>
1263 
1264     LinkBuffer patchBuffer(*this, m_codeBlock);
1265 
1266     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1267     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1268 
1269     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1270 
1271     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1272         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1273         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1274 
1275     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1276     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1277 }
1278 
1279 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1280 {
1281     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1282     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified">1283     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1284     int base = bytecode.m_base.offset();</span>
<span class="line-modified">1285     int property = bytecode.m_property.offset();</span>
1286     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1287     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1288 
1289     emitGetVirtualRegisters(base, regT0, property, regT1);
1290 
1291     emitJumpSlowCaseIfNotInt(regT1);
1292 
1293     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1294     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1295     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1296     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1297     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1298     // extending since it makes it easier to re-tag the value in the slow case.
1299     zeroExtend32ToPtr(regT1, regT1);
1300 
1301     emitJumpSlowCaseIfNotJSCell(regT0, base);
1302     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1303     and32(TrustedImm32(IndexingShapeMask), regT2);
1304 
1305     JITArrayMode mode = chooseArrayMode(profile);
1306     PatchableJump badType;
1307 
1308     // FIXME: Add support for other types like TypedArrays and Arguments.
1309     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1310     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1311 
1312     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1313 
1314     addSlowCase(badType);
1315     addSlowCase(slowCases);
1316 
1317     Label done = label();
1318 
1319     emitPutVirtualRegister(dst);
1320 
1321     Label nextHotPath = label();
1322 
<span class="line-modified">1323     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeOffset, PatchableJump(), badType, mode, profile, done, nextHotPath));</span>
1324 }
1325 
1326 void JIT::emitSlow_op_has_indexed_property(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1327 {
1328     linkAllSlowCases(iter);
1329 
1330     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
<span class="line-modified">1331     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1332     int base = bytecode.m_base.offset();</span>
<span class="line-modified">1333     int property = bytecode.m_property.offset();</span>
1334     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
1335 
1336     Label slowPath = label();
1337 
1338     emitGetVirtualRegister(base, regT0);
1339     emitGetVirtualRegister(property, regT1);
<span class="line-modified">1340     Call call = callOperation(operationHasIndexedPropertyDefault, dst, regT0, regT1, byValInfo);</span>
1341 
1342     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
1343     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
1344     m_byValInstructionIndex++;
1345 }
1346 
1347 void JIT::emit_op_get_direct_pname(const Instruction* currentInstruction)
1348 {
1349     auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
<span class="line-modified">1350     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1351     int base = bytecode.m_base.offset();</span>
<span class="line-modified">1352     int index = bytecode.m_index.offset();</span>
<span class="line-modified">1353     int enumerator = bytecode.m_enumerator.offset();</span>
1354 
1355     // Check that base is a cell
1356     emitGetVirtualRegister(base, regT0);
1357     emitJumpSlowCaseIfNotJSCell(regT0, base);
1358 
1359     // Check the structure
1360     emitGetVirtualRegister(enumerator, regT2);
1361     load32(Address(regT0, JSCell::structureIDOffset()), regT1);
1362     addSlowCase(branch32(NotEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1363 
1364     // Compute the offset
1365     emitGetVirtualRegister(index, regT1);
1366     // If index is less than the enumerator&#39;s cached inline storage, then it&#39;s an inline access
1367     Jump outOfLineAccess = branch32(AboveOrEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()));
1368     addPtr(TrustedImm32(JSObject::offsetOfInlineStorage()), regT0);
1369     signExtend32ToPtr(regT1, regT1);
1370     load64(BaseIndex(regT0, regT1, TimesEight), regT0);
1371 
1372     Jump done = jump();
1373 
1374     // Otherwise it&#39;s out of line
1375     outOfLineAccess.link(this);
1376     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
1377     sub32(Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()), regT1);
1378     neg32(regT1);
1379     signExtend32ToPtr(regT1, regT1);
1380     int32_t offsetOfFirstProperty = static_cast&lt;int32_t&gt;(offsetInButterfly(firstOutOfLineOffset)) * sizeof(EncodedJSValue);
1381     load64(BaseIndex(regT0, regT1, TimesEight, offsetOfFirstProperty), regT0);
1382 
1383     done.link(this);
1384     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1385     emitPutVirtualRegister(dst, regT0);
1386 }
1387 
1388 void JIT::emit_op_enumerator_structure_pname(const Instruction* currentInstruction)
1389 {
1390     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
<span class="line-modified">1391     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1392     int enumerator = bytecode.m_enumerator.offset();</span>
<span class="line-modified">1393     int index = bytecode.m_index.offset();</span>
1394 
1395     emitGetVirtualRegister(index, regT0);
1396     emitGetVirtualRegister(enumerator, regT1);
1397     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endStructurePropertyIndexOffset()));
1398 
1399     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1400 
1401     Jump done = jump();
1402     inBounds.link(this);
1403 
1404     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1405     signExtend32ToPtr(regT0, regT0);
1406     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1407 
1408     done.link(this);
1409     emitPutVirtualRegister(dst);
1410 }
1411 
1412 void JIT::emit_op_enumerator_generic_pname(const Instruction* currentInstruction)
1413 {
1414     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
<span class="line-modified">1415     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1416     int enumerator = bytecode.m_enumerator.offset();</span>
<span class="line-modified">1417     int index = bytecode.m_index.offset();</span>
1418 
1419     emitGetVirtualRegister(index, regT0);
1420     emitGetVirtualRegister(enumerator, regT1);
1421     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endGenericPropertyIndexOffset()));
1422 
1423     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1424 
1425     Jump done = jump();
1426     inBounds.link(this);
1427 
1428     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1429     signExtend32ToPtr(regT0, regT0);
1430     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1431 
1432     done.link(this);
1433     emitPutVirtualRegister(dst);
1434 }
1435 
1436 void JIT::emit_op_profile_type(const Instruction* currentInstruction)
1437 {
1438     auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
1439     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1440     TypeLocation* cachedTypeLocation = metadata.m_typeLocation;
<span class="line-modified">1441     int valueToProfile = bytecode.m_targetVirtualRegister.offset();</span>
1442 
1443     emitGetVirtualRegister(valueToProfile, regT0);
1444 
1445     JumpList jumpToEnd;
1446 
1447     jumpToEnd.append(branchIfEmpty(regT0));
1448 
1449     // Compile in a predictive type check, if possible, to see if we can skip writing to the log.
1450     // These typechecks are inlined to match those of the 64-bit JSValue type checks.
1451     if (cachedTypeLocation-&gt;m_lastSeenType == TypeUndefined)
1452         jumpToEnd.append(branchIfUndefined(regT0));
1453     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNull)
1454         jumpToEnd.append(branchIfNull(regT0));
1455     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeBoolean)
1456         jumpToEnd.append(branchIfBoolean(regT0, regT1));
1457     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeAnyInt)
1458         jumpToEnd.append(branchIfInt32(regT0));
1459     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNumber)
1460         jumpToEnd.append(branchIfNumber(regT0));
1461     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeString) {
</pre>
<hr />
<pre>
1474     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::valueOffset()));
1475 
1476     // Store the structureID of the cell if T0 is a cell, otherwise, store 0 on the log entry.
1477     Jump notCell = branchIfNotCell(regT0);
1478     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1479     store32(regT0, Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1480     Jump skipIsCell = jump();
1481     notCell.link(this);
1482     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1483     skipIsCell.link(this);
1484 
1485     // Store the typeLocation on the log entry.
1486     move(TrustedImmPtr(cachedTypeLocation), regT0);
1487     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1488 
1489     // Increment the current log entry.
1490     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1491     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1492     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1493     // Clear the log if we&#39;re at the end of the log.
<span class="line-modified">1494     callOperation(operationProcessTypeProfilerLog);</span>
1495     skipClearLog.link(this);
1496 
1497     jumpToEnd.link(this);
1498 }
1499 
1500 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1501 {
1502     RELEASE_ASSERT(vm().shadowChicken());
1503     updateTopCallFrame();
1504     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1505     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1506     GPRReg shadowPacketReg = regT0;
1507     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1508     GPRReg scratch2Reg = regT2;
1509     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
<span class="line-modified">1510     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);</span>
1511     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1512 }
1513 
1514 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1515 {
1516     RELEASE_ASSERT(vm().shadowChicken());
1517     updateTopCallFrame();
1518     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1519     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1520     GPRReg shadowPacketReg = regT0;
1521     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1522     GPRReg scratch2Reg = regT2;
1523     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
<span class="line-modified">1524     emitGetVirtualRegister(bytecode.m_thisValue.offset(), regT2);</span>
<span class="line-modified">1525     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);</span>
<span class="line-modified">1526     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeOffset));</span>
1527 }
1528 
1529 #endif // USE(JSVALUE64)
1530 
1531 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1532 {
1533     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1534     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1535     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1536 #if USE(JSVALUE64)
1537     basicBlockLocation-&gt;emitExecuteCode(*this);
1538 #else
1539     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1540 #endif
1541 }
1542 
1543 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
1544 {
1545     auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
<span class="line-modified">1546     int dst = bytecode.m_dst.offset();</span>
<span class="line-modified">1547     load32(payloadFor(CallFrameSlot::argumentCount), regT0);</span>
1548     sub32(TrustedImm32(1), regT0);
1549     JSValueRegs result = JSValueRegs::withTwoAvailableRegs(regT0, regT1);
1550     boxInt32(regT0, result);
1551     emitPutVirtualRegister(dst, result);
1552 }
1553 
1554 void JIT::emit_op_get_rest_length(const Instruction* currentInstruction)
1555 {
1556     auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
<span class="line-modified">1557     int dst = bytecode.m_dst.offset();</span>
1558     unsigned numParamsToSkip = bytecode.m_numParametersToSkip;
<span class="line-modified">1559     load32(payloadFor(CallFrameSlot::argumentCount), regT0);</span>
1560     sub32(TrustedImm32(1), regT0);
1561     Jump zeroLength = branch32(LessThanOrEqual, regT0, Imm32(numParamsToSkip));
1562     sub32(Imm32(numParamsToSkip), regT0);
1563 #if USE(JSVALUE64)
1564     boxInt32(regT0, JSValueRegs(regT0));
1565 #endif
1566     Jump done = jump();
1567 
1568     zeroLength.link(this);
1569 #if USE(JSVALUE64)
1570     move(TrustedImm64(JSValue::encode(jsNumber(0))), regT0);
1571 #else
1572     move(TrustedImm32(0), regT0);
1573 #endif
1574 
1575     done.link(this);
1576 #if USE(JSVALUE64)
1577     emitPutVirtualRegister(dst, regT0);
1578 #else
1579     move(TrustedImm32(JSValue::Int32Tag), regT1);
1580     emitPutVirtualRegister(dst, JSValueRegs(regT1, regT0));
1581 #endif
1582 }
1583 
1584 void JIT::emit_op_get_argument(const Instruction* currentInstruction)
1585 {
1586     auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
<span class="line-modified">1587     int dst = bytecode.m_dst.offset();</span>
1588     int index = bytecode.m_index;
1589 #if USE(JSVALUE64)
1590     JSValueRegs resultRegs(regT0);
1591 #else
1592     JSValueRegs resultRegs(regT1, regT0);
1593 #endif
1594 
<span class="line-modified">1595     load32(payloadFor(CallFrameSlot::argumentCount), regT2);</span>
1596     Jump argumentOutOfBounds = branch32(LessThanOrEqual, regT2, TrustedImm32(index));
<span class="line-modified">1597     loadValue(addressFor(CallFrameSlot::thisArgument + index), resultRegs);</span>
1598     Jump done = jump();
1599 
1600     argumentOutOfBounds.link(this);
1601     moveValue(jsUndefined(), resultRegs);
1602 
1603     done.link(this);
1604     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1605     emitPutVirtualRegister(dst, resultRegs);
1606 }
1607 
1608 } // namespace JSC
1609 
1610 #endif // ENABLE(JIT)
</pre>
</td>
<td>
<hr />
<pre>
  12  *    documentation and/or other materials provided with the distribution.
  13  *
  14  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  15  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  16  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  17  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  18  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  19  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  20  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  21  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  22  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  23  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  24  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  25  */
  26 
  27 #include &quot;config.h&quot;
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;BasicBlockLocation.h&quot;
<span class="line-modified">  32 #include &quot;BytecodeGenerator.h&quot;</span>
  33 #include &quot;Exception.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSCast.h&quot;
  39 #include &quot;JSFunction.h&quot;
  40 #include &quot;JSPropertyNameEnumerator.h&quot;
  41 #include &quot;LinkBuffer.h&quot;
  42 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  43 #include &quot;OpcodeInlines.h&quot;
  44 #include &quot;SlowPathCall.h&quot;
  45 #include &quot;SuperSampler.h&quot;
  46 #include &quot;ThunkGenerators.h&quot;
  47 #include &quot;TypeLocation.h&quot;
  48 #include &quot;TypeProfilerLog.h&quot;
  49 #include &quot;VirtualRegister.h&quot;
  50 #include &quot;Watchdog.h&quot;
  51 
  52 namespace JSC {
  53 
  54 #if USE(JSVALUE64)
  55 
  56 void JIT::emit_op_mov(const Instruction* currentInstruction)
  57 {
  58     auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
<span class="line-modified">  59     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">  60     VirtualRegister src = bytecode.m_src;</span>
  61 
<span class="line-modified">  62     if (src.isConstant()) {</span>
  63         JSValue value = m_codeBlock-&gt;getConstant(src);
  64         if (!value.isNumber())
  65             store64(TrustedImm64(JSValue::encode(value)), addressFor(dst));
  66         else
  67             store64(Imm64(JSValue::encode(value)), addressFor(dst));
  68         return;
  69     }
  70 
  71     load64(addressFor(src), regT0);
  72     store64(regT0, addressFor(dst));
  73 }
  74 
  75 
  76 void JIT::emit_op_end(const Instruction* currentInstruction)
  77 {
  78     auto bytecode = currentInstruction-&gt;as&lt;OpEnd&gt;();
  79     RELEASE_ASSERT(returnValueGPR != callFrameRegister);
<span class="line-modified">  80     emitGetVirtualRegister(bytecode.m_value, returnValueGPR);</span>
  81     emitRestoreCalleeSaves();
  82     emitFunctionEpilogue();
  83     ret();
  84 }
  85 
  86 void JIT::emit_op_jmp(const Instruction* currentInstruction)
  87 {
  88     auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
  89     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
  90     addJump(jump(), target);
  91 }
  92 
  93 void JIT::emit_op_new_object(const Instruction* currentInstruction)
  94 {
  95     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
  96     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  97     Structure* structure = metadata.m_objectAllocationProfile.structure();
  98     size_t allocationSize = JSFinalObject::allocationSize(structure-&gt;inlineCapacity());
  99     Allocator allocator = allocatorForNonVirtualConcurrently&lt;JSFinalObject&gt;(*m_vm, allocationSize, AllocatorForMode::AllocatorIfExists);
 100 
 101     RegisterID resultReg = regT0;
 102     RegisterID allocatorReg = regT1;
 103     RegisterID scratchReg = regT2;
 104 
 105     if (!allocator)
 106         addSlowCase(jump());
 107     else {
 108         JumpList slowCases;
 109         auto butterfly = TrustedImmPtr(nullptr);
 110         emitAllocateJSObject(resultReg, JITAllocator::constant(allocator), allocatorReg, TrustedImmPtr(structure), butterfly, scratchReg, slowCases);
 111         emitInitializeInlineStorage(resultReg, structure-&gt;inlineCapacity());
 112         addSlowCase(slowCases);
<span class="line-modified"> 113         emitPutVirtualRegister(bytecode.m_dst);</span>
 114     }
 115 }
 116 
 117 void JIT::emitSlow_op_new_object(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 118 {
 119     linkAllSlowCases(iter);
 120 
 121     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
 122     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 123     VirtualRegister dst = bytecode.m_dst;</span>
 124     Structure* structure = metadata.m_objectAllocationProfile.structure();
<span class="line-modified"> 125     callOperation(operationNewObject, TrustedImmPtr(&amp;vm()), structure);</span>
 126     emitStoreCell(dst, returnValueGPR);
 127 }
 128 
 129 void JIT::emit_op_overrides_has_instance(const Instruction* currentInstruction)
 130 {
 131     auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
<span class="line-modified"> 132     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 133     VirtualRegister constructor = bytecode.m_constructor;</span>
<span class="line-modified"> 134     VirtualRegister hasInstanceValue = bytecode.m_hasInstanceValue;</span>
 135 
 136     emitGetVirtualRegister(hasInstanceValue, regT0);
 137 
 138     // We don&#39;t jump if we know what Symbol.hasInstance would do.
 139     Jump customhasInstanceValue = branchPtr(NotEqual, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()-&gt;functionProtoHasInstanceSymbolFunction()));
 140 
 141     emitGetVirtualRegister(constructor, regT0);
 142 
 143     // Check that constructor &#39;ImplementsDefaultHasInstance&#39; i.e. the object is not a C-API user nor a bound function.
 144     test8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(ImplementsDefaultHasInstance), regT0);
 145     boxBoolean(regT0, JSValueRegs { regT0 });
 146     Jump done = jump();
 147 
 148     customhasInstanceValue.link(this);
<span class="line-modified"> 149     move(TrustedImm32(JSValue::ValueTrue), regT0);</span>
 150 
 151     done.link(this);
 152     emitPutVirtualRegister(dst);
 153 }
 154 
 155 void JIT::emit_op_instanceof(const Instruction* currentInstruction)
 156 {
 157     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
<span class="line-modified"> 158     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 159     VirtualRegister value = bytecode.m_value;</span>
<span class="line-modified"> 160     VirtualRegister proto = bytecode.m_prototype;</span>
 161 
 162     // Load the operands (baseVal, proto, and value respectively) into registers.
 163     // We use regT0 for baseVal since we will be done with this first, and we can then use it for the result.
 164     emitGetVirtualRegister(value, regT2);
 165     emitGetVirtualRegister(proto, regT1);
 166 
 167     // Check that proto are cells. baseVal must be a cell - this is checked by the get_by_id for Symbol.hasInstance.
 168     emitJumpSlowCaseIfNotJSCell(regT2, value);
 169     emitJumpSlowCaseIfNotJSCell(regT1, proto);
 170 
 171     JITInstanceOfGenerator gen(
<span class="line-modified"> 172         m_codeBlock, CodeOrigin(m_bytecodeIndex), CallSiteIndex(m_bytecodeIndex),</span>
 173         RegisterSet::stubUnavailableRegisters(),
 174         regT0, // result
 175         regT2, // value
 176         regT1, // proto
 177         regT3, regT4); // scratch
 178     gen.generateFastPath(*this);
 179     m_instanceOfs.append(gen);
 180 
 181     emitPutVirtualRegister(dst);
 182 }
 183 
 184 void JIT::emitSlow_op_instanceof(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 185 {
 186     linkAllSlowCases(iter);
 187 
 188     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
<span class="line-modified"> 189     VirtualRegister resultVReg = bytecode.m_dst;</span>
 190 
 191     JITInstanceOfGenerator&amp; gen = m_instanceOfs[m_instanceOfIndex++];
 192 
 193     Label coldPathBegin = label();
<span class="line-modified"> 194     Call call = callOperation(operationInstanceOfOptimize, resultVReg, TrustedImmPtr(m_codeBlock-&gt;globalObject()), gen.stubInfo(), regT2, regT1);</span>
 195     gen.reportSlowPathCall(coldPathBegin, call);
 196 }
 197 
 198 void JIT::emit_op_instanceof_custom(const Instruction*)
 199 {
 200     // This always goes to slow path since we expect it to be rare.
 201     addSlowCase(jump());
 202 }
 203 
 204 void JIT::emit_op_is_empty(const Instruction* currentInstruction)
 205 {
 206     auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
<span class="line-modified"> 207     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 208     VirtualRegister value = bytecode.m_operand;</span>
 209 
 210     emitGetVirtualRegister(value, regT0);
 211     compare64(Equal, regT0, TrustedImm32(JSValue::encode(JSValue())), regT0);
 212 
 213     boxBoolean(regT0, JSValueRegs { regT0 });
 214     emitPutVirtualRegister(dst);
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
<span class="line-modified"> 220     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 221     VirtualRegister value = bytecode.m_operand;</span>
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
<span class="line-modified"> 226     compare64(Equal, regT0, TrustedImm32(JSValue::ValueUndefined), regT0);</span>
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
 235     emitLoadStructure(vm(), regT0, regT1, regT2);
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
<span class="line-modified"> 249     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 250     VirtualRegister value = bytecode.m_operand;</span>
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
<span class="line-modified"> 254     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
<span class="line-modified"> 255     compare64(Equal, regT0, TrustedImm32(JSValue::ValueNull), regT0);</span>
 256 
 257     boxBoolean(regT0, JSValueRegs { regT0 });
 258     emitPutVirtualRegister(dst);
 259 }
 260 
 261 void JIT::emit_op_is_boolean(const Instruction* currentInstruction)
 262 {
 263     auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
<span class="line-modified"> 264     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 265     VirtualRegister value = bytecode.m_operand;</span>
 266 
 267     emitGetVirtualRegister(value, regT0);
<span class="line-modified"> 268     xor64(TrustedImm32(JSValue::ValueFalse), regT0);</span>
 269     test64(Zero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1)), regT0);
 270     boxBoolean(regT0, JSValueRegs { regT0 });
 271     emitPutVirtualRegister(dst);
 272 }
 273 
 274 void JIT::emit_op_is_number(const Instruction* currentInstruction)
 275 {
 276     auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
<span class="line-modified"> 277     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 278     VirtualRegister value = bytecode.m_operand;</span>
 279 
 280     emitGetVirtualRegister(value, regT0);
<span class="line-modified"> 281     test64(NonZero, regT0, numberTagRegister, regT0);</span>
 282     boxBoolean(regT0, JSValueRegs { regT0 });
 283     emitPutVirtualRegister(dst);
 284 }
 285 
 286 void JIT::emit_op_is_cell_with_type(const Instruction* currentInstruction)
 287 {
 288     auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
<span class="line-modified"> 289     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 290     VirtualRegister value = bytecode.m_operand;</span>
 291     int type = bytecode.m_type;
 292 
 293     emitGetVirtualRegister(value, regT0);
 294     Jump isNotCell = branchIfNotCell(regT0);
 295 
 296     compare8(Equal, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(type), regT0);
 297     boxBoolean(regT0, JSValueRegs { regT0 });
 298     Jump done = jump();
 299 
 300     isNotCell.link(this);
<span class="line-modified"> 301     move(TrustedImm32(JSValue::ValueFalse), regT0);</span>
 302 
 303     done.link(this);
 304     emitPutVirtualRegister(dst);
 305 }
 306 
 307 void JIT::emit_op_is_object(const Instruction* currentInstruction)
 308 {
 309     auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
<span class="line-modified"> 310     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 311     VirtualRegister value = bytecode.m_operand;</span>
 312 
 313     emitGetVirtualRegister(value, regT0);
 314     Jump isNotCell = branchIfNotCell(regT0);
 315 
 316     compare8(AboveOrEqual, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType), regT0);
 317     boxBoolean(regT0, JSValueRegs { regT0 });
 318     Jump done = jump();
 319 
 320     isNotCell.link(this);
<span class="line-modified"> 321     move(TrustedImm32(JSValue::ValueFalse), regT0);</span>
 322 
 323     done.link(this);
 324     emitPutVirtualRegister(dst);
 325 }
 326 
 327 void JIT::emit_op_ret(const Instruction* currentInstruction)
 328 {
 329     ASSERT(callFrameRegister != regT1);
 330     ASSERT(regT1 != returnValueGPR);
 331     ASSERT(returnValueGPR != callFrameRegister);
 332 
 333     // Return the result in %eax.
 334     auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
<span class="line-modified"> 335     emitGetVirtualRegister(bytecode.m_value, returnValueGPR);</span>
 336 
 337     checkStackPointerAlignment();
 338     emitRestoreCalleeSaves();
 339     emitFunctionEpilogue();
 340     ret();
 341 }
 342 
 343 void JIT::emit_op_to_primitive(const Instruction* currentInstruction)
 344 {
 345     auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
<span class="line-modified"> 346     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 347     VirtualRegister src = bytecode.m_src;</span>
 348 
 349     emitGetVirtualRegister(src, regT0);
 350 
 351     Jump isImm = branchIfNotCell(regT0);
 352     addSlowCase(branchIfObject(regT0));
 353     isImm.link(this);
 354 
 355     if (dst != src)
 356         emitPutVirtualRegister(dst);
 357 
 358 }
 359 
<span class="line-added"> 360 void JIT::emit_op_to_property_key(const Instruction* currentInstruction)</span>
<span class="line-added"> 361 {</span>
<span class="line-added"> 362     auto bytecode = currentInstruction-&gt;as&lt;OpToPropertyKey&gt;();</span>
<span class="line-added"> 363     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-added"> 364     VirtualRegister src = bytecode.m_src;</span>
<span class="line-added"> 365 </span>
<span class="line-added"> 366     emitGetVirtualRegister(src, regT0);</span>
<span class="line-added"> 367 </span>
<span class="line-added"> 368     addSlowCase(branchIfNotCell(regT0));</span>
<span class="line-added"> 369     Jump done = branchIfSymbol(regT0);</span>
<span class="line-added"> 370     addSlowCase(branchIfNotString(regT0));</span>
<span class="line-added"> 371 </span>
<span class="line-added"> 372     done.link(this);</span>
<span class="line-added"> 373     if (src != dst)</span>
<span class="line-added"> 374         emitPutVirtualRegister(dst);</span>
<span class="line-added"> 375 }</span>
<span class="line-added"> 376 </span>
 377 void JIT::emit_op_set_function_name(const Instruction* currentInstruction)
 378 {
 379     auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
<span class="line-modified"> 380     emitGetVirtualRegister(bytecode.m_function, regT0);</span>
<span class="line-modified"> 381     emitGetVirtualRegister(bytecode.m_name, regT1);</span>
<span class="line-modified"> 382     callOperation(operationSetFunctionName, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
 383 }
 384 
 385 void JIT::emit_op_not(const Instruction* currentInstruction)
 386 {
 387     auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
<span class="line-modified"> 388     emitGetVirtualRegister(bytecode.m_operand, regT0);</span>
 389 
 390     // Invert against JSValue(false); if the value was tagged as a boolean, then all bits will be
 391     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 392     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
<span class="line-modified"> 393     xor64(TrustedImm32(JSValue::ValueFalse), regT0);</span>
 394     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
<span class="line-modified"> 395     xor64(TrustedImm32(JSValue::ValueTrue), regT0);</span>
 396 
<span class="line-modified"> 397     emitPutVirtualRegister(bytecode.m_dst);</span>
 398 }
 399 
 400 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 401 {
 402     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 403     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 404 
 405     GPRReg value = regT0;
 406     GPRReg scratch1 = regT1;
 407     GPRReg scratch2 = regT2;
 408     bool shouldCheckMasqueradesAsUndefined = true;
 409 
<span class="line-modified"> 410     emitGetVirtualRegister(bytecode.m_condition, value);</span>
 411     addJump(branchIfFalsey(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 412 }
 413 
 414 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 415 {
 416     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
<span class="line-modified"> 417     VirtualRegister src = bytecode.m_value;</span>
 418     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 419 
 420     emitGetVirtualRegister(src, regT0);
 421     Jump isImmediate = branchIfNotCell(regT0);
 422 
 423     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 424     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 425     emitLoadStructure(vm(), regT0, regT2, regT1);
 426     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 427     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 428     Jump masqueradesGlobalObjectIsForeign = jump();
 429 
 430     // Now handle the immediate cases - undefined &amp; null
 431     isImmediate.link(this);
<span class="line-modified"> 432     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
 433     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 434 
 435     isNotMasqueradesAsUndefined.link(this);
 436     masqueradesGlobalObjectIsForeign.link(this);
 437 };
 438 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 439 {
 440     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
<span class="line-modified"> 441     VirtualRegister src = bytecode.m_value;</span>
 442     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 443 
 444     emitGetVirtualRegister(src, regT0);
 445     Jump isImmediate = branchIfNotCell(regT0);
 446 
 447     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 448     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
 449     emitLoadStructure(vm(), regT0, regT2, regT1);
 450     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 451     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 452     Jump wasNotImmediate = jump();
 453 
 454     // Now handle the immediate cases - undefined &amp; null
 455     isImmediate.link(this);
<span class="line-modified"> 456     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
 457     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 458 
 459     wasNotImmediate.link(this);
 460 }
 461 
 462 void JIT::emit_op_jundefined_or_null(const Instruction* currentInstruction)
 463 {
 464     auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();
<span class="line-modified"> 465     VirtualRegister value = bytecode.m_value;</span>
 466     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 467 
 468     emitGetVirtualRegister(value, regT0);
 469 
<span class="line-modified"> 470     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
 471     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 472 }
 473 
 474 void JIT::emit_op_jnundefined_or_null(const Instruction* currentInstruction)
 475 {
 476     auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();
<span class="line-modified"> 477     VirtualRegister value = bytecode.m_value;</span>
 478     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 479 
 480     emitGetVirtualRegister(value, regT0);
 481 
<span class="line-modified"> 482     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
 483     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 484 }
 485 
 486 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 487 {
 488     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 489     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 490     VirtualRegister src = bytecode.m_value;</span>
<span class="line-modified"> 491     JSValue specialPointer = getConstantOperand(bytecode.m_specialPointer);</span>
<span class="line-added"> 492     ASSERT(specialPointer.isCell());</span>
 493     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 494 
 495     emitGetVirtualRegister(src, regT0);
<span class="line-modified"> 496     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(specialPointer.asCell()));</span>
 497     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 498     addJump(jump(), target);
 499     equal.link(this);
 500 }
 501 
 502 void JIT::emit_op_eq(const Instruction* currentInstruction)
 503 {
 504     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
<span class="line-modified"> 505     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);</span>
 506     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 507     compare32(Equal, regT1, regT0, regT0);
 508     boxBoolean(regT0, JSValueRegs { regT0 });
<span class="line-modified"> 509     emitPutVirtualRegister(bytecode.m_dst);</span>
 510 }
 511 
 512 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 513 {
 514     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 515     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 516     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);</span>
 517     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 518     addJump(branch32(Equal, regT0, regT1), target);
 519 }
 520 
 521 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 522 {
 523     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 524     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 525 
 526     GPRReg value = regT0;
 527     GPRReg scratch1 = regT1;
 528     GPRReg scratch2 = regT2;
 529     bool shouldCheckMasqueradesAsUndefined = true;
<span class="line-modified"> 530     emitGetVirtualRegister(bytecode.m_condition, value);</span>
 531     addJump(branchIfTruthy(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 532 }
 533 
 534 void JIT::emit_op_neq(const Instruction* currentInstruction)
 535 {
 536     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
<span class="line-modified"> 537     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);</span>
 538     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 539     compare32(NotEqual, regT1, regT0, regT0);
 540     boxBoolean(regT0, JSValueRegs { regT0 });
 541 
<span class="line-modified"> 542     emitPutVirtualRegister(bytecode.m_dst);</span>
 543 }
 544 
 545 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 546 {
 547     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 548     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 549     emitGetVirtualRegisters(bytecode.m_lhs, regT0, bytecode.m_rhs, regT1);</span>
 550     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 551     addJump(branch32(NotEqual, regT0, regT1), target);
 552 }
 553 
 554 void JIT::emit_op_throw(const Instruction* currentInstruction)
 555 {
 556     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 557     ASSERT(regT0 == returnValueGPR);
 558     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
<span class="line-modified"> 559     emitGetVirtualRegister(bytecode.m_value, regT0);</span>
<span class="line-modified"> 560     callOperationNoExceptionCheck(operationThrow, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);</span>
 561     jumpToExceptionHandler(vm());
 562 }
 563 
 564 template&lt;typename Op&gt;
 565 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 566 {
 567     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
<span class="line-modified"> 568     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 569     VirtualRegister src1 = bytecode.m_lhs;</span>
<span class="line-modified"> 570     VirtualRegister src2 = bytecode.m_rhs;</span>
 571 
 572     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 573 
 574     // Jump slow if both are cells (to cover strings).
 575     move(regT0, regT2);
 576     or64(regT1, regT2);
 577     addSlowCase(branchIfCell(regT2));
 578 
 579     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 580     // if it&#39;s a double.
 581     Jump leftOK = branchIfInt32(regT0);
 582     addSlowCase(branchIfNumber(regT0));
 583     leftOK.link(this);
 584     Jump rightOK = branchIfInt32(regT1);
 585     addSlowCase(branchIfNumber(regT1));
 586     rightOK.link(this);
 587 
 588     if (type == CompileOpStrictEqType::StrictEq)
 589         compare64(Equal, regT1, regT0, regT0);
 590     else
</pre>
<hr />
<pre>
 592     boxBoolean(regT0, JSValueRegs { regT0 });
 593 
 594     emitPutVirtualRegister(dst);
 595 }
 596 
 597 void JIT::emit_op_stricteq(const Instruction* currentInstruction)
 598 {
 599     compileOpStrictEq&lt;OpStricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 600 }
 601 
 602 void JIT::emit_op_nstricteq(const Instruction* currentInstruction)
 603 {
 604     compileOpStrictEq&lt;OpNstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 605 }
 606 
 607 template&lt;typename Op&gt;
 608 void JIT::compileOpStrictEqJump(const Instruction* currentInstruction, CompileOpStrictEqType type)
 609 {
 610     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 611     int target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 612     VirtualRegister src1 = bytecode.m_lhs;</span>
<span class="line-modified"> 613     VirtualRegister src2 = bytecode.m_rhs;</span>
 614 
 615     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 616 
 617     // Jump slow if both are cells (to cover strings).
 618     move(regT0, regT2);
 619     or64(regT1, regT2);
 620     addSlowCase(branchIfCell(regT2));
 621 
 622     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 623     // if it&#39;s a double.
 624     Jump leftOK = branchIfInt32(regT0);
 625     addSlowCase(branchIfNumber(regT0));
 626     leftOK.link(this);
 627     Jump rightOK = branchIfInt32(regT1);
 628     addSlowCase(branchIfNumber(regT1));
 629     rightOK.link(this);
 630 
 631     if (type == CompileOpStrictEqType::StrictEq)
 632         addJump(branch64(Equal, regT1, regT0), target);
 633     else
 634         addJump(branch64(NotEqual, regT1, regT0), target);
 635 }
 636 
 637 void JIT::emit_op_jstricteq(const Instruction* currentInstruction)
 638 {
 639     compileOpStrictEqJump&lt;OpJstricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 640 }
 641 
 642 void JIT::emit_op_jnstricteq(const Instruction* currentInstruction)
 643 {
 644     compileOpStrictEqJump&lt;OpJnstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 645 }
 646 
 647 void JIT::emitSlow_op_jstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 648 {
 649     linkAllSlowCases(iter);
 650 
 651     auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
 652     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 653     callOperation(operationCompareStrictEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
 654     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 655 }
 656 
 657 void JIT::emitSlow_op_jnstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 658 {
 659     linkAllSlowCases(iter);
 660 
 661     auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
 662     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified"> 663     callOperation(operationCompareStrictEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
 664     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 665 }
 666 
 667 void JIT::emit_op_to_number(const Instruction* currentInstruction)
 668 {
 669     auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
<span class="line-modified"> 670     VirtualRegister dstVReg = bytecode.m_dst;</span>
<span class="line-modified"> 671     VirtualRegister srcVReg = bytecode.m_operand;</span>
<span class="line-added"> 672     emitGetVirtualRegister(srcVReg, regT0);</span>
<span class="line-added"> 673 </span>
<span class="line-added"> 674     addSlowCase(branchIfNotNumber(regT0));</span>
<span class="line-added"> 675 </span>
<span class="line-added"> 676     emitValueProfilingSite(bytecode.metadata(m_codeBlock));</span>
<span class="line-added"> 677     if (srcVReg != dstVReg)</span>
<span class="line-added"> 678         emitPutVirtualRegister(dstVReg);</span>
<span class="line-added"> 679 }</span>
<span class="line-added"> 680 </span>
<span class="line-added"> 681 void JIT::emit_op_to_numeric(const Instruction* currentInstruction)</span>
<span class="line-added"> 682 {</span>
<span class="line-added"> 683     auto bytecode = currentInstruction-&gt;as&lt;OpToNumeric&gt;();</span>
<span class="line-added"> 684     VirtualRegister dstVReg = bytecode.m_dst;</span>
<span class="line-added"> 685     VirtualRegister srcVReg = bytecode.m_operand;</span>
 686     emitGetVirtualRegister(srcVReg, regT0);
 687 
<span class="line-added"> 688     Jump isNotCell = branchIfNotCell(regT0);</span>
<span class="line-added"> 689     addSlowCase(branchIfNotBigInt(regT0));</span>
<span class="line-added"> 690     Jump isBigInt = jump();</span>
<span class="line-added"> 691 </span>
<span class="line-added"> 692     isNotCell.link(this);</span>
 693     addSlowCase(branchIfNotNumber(regT0));
<span class="line-added"> 694     isBigInt.link(this);</span>
 695 
 696     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 697     if (srcVReg != dstVReg)
 698         emitPutVirtualRegister(dstVReg);
 699 }
 700 
 701 void JIT::emit_op_to_string(const Instruction* currentInstruction)
 702 {
 703     auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
<span class="line-modified"> 704     VirtualRegister srcVReg = bytecode.m_operand;</span>
 705     emitGetVirtualRegister(srcVReg, regT0);
 706 
 707     addSlowCase(branchIfNotCell(regT0));
 708     addSlowCase(branchIfNotString(regT0));
 709 
<span class="line-modified"> 710     emitPutVirtualRegister(bytecode.m_dst);</span>
 711 }
 712 
 713 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 714 {
 715     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
<span class="line-modified"> 716     VirtualRegister dstVReg = bytecode.m_dst;</span>
<span class="line-modified"> 717     VirtualRegister srcVReg = bytecode.m_operand;</span>
 718     emitGetVirtualRegister(srcVReg, regT0);
 719 
 720     addSlowCase(branchIfNotCell(regT0));
 721     addSlowCase(branchIfNotObject(regT0));
 722 
 723     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 724     if (srcVReg != dstVReg)
 725         emitPutVirtualRegister(dstVReg);
 726 }
 727 
 728 void JIT::emit_op_catch(const Instruction* currentInstruction)
 729 {
 730     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 731 
 732     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
 733 
 734     move(TrustedImmPtr(m_vm), regT3);
 735     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 736     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 737 
 738     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 739 
<span class="line-modified"> 740     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler, TrustedImmPtr(&amp;vm()));</span>
 741     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
 742     jumpToExceptionHandler(vm());
 743     isCatchableException.link(this);
 744 
 745     move(TrustedImmPtr(m_vm), regT3);
 746     load64(Address(regT3, VM::exceptionOffset()), regT0);
 747     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
<span class="line-modified"> 748     emitPutVirtualRegister(bytecode.m_exception);</span>
 749 
 750     load64(Address(regT0, Exception::valueOffset()), regT0);
<span class="line-modified"> 751     emitPutVirtualRegister(bytecode.m_thrownValue);</span>
 752 
 753 #if ENABLE(DFG_JIT)
 754     // FIXME: consider inline caching the process of doing OSR entry, including
 755     // argument type proofs, storing locals to the buffer, etc
 756     // https://bugs.webkit.org/show_bug.cgi?id=175598
 757 
 758     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 759     ValueProfileAndVirtualRegisterBuffer* buffer = metadata.m_buffer;</span>
 760     if (buffer || !shouldEmitProfiling())
<span class="line-modified"> 761         callOperation(operationTryOSREnterAtCatch, &amp;vm(), m_bytecodeIndex.asBits());</span>
 762     else
<span class="line-modified"> 763         callOperation(operationTryOSREnterAtCatchAndValueProfile, &amp;vm(), m_bytecodeIndex.asBits());</span>
 764     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 765     emitRestoreCalleeSaves();
 766     farJump(returnValueGPR, ExceptionHandlerPtrTag);
 767     skipOSREntry.link(this);
 768     if (buffer &amp;&amp; shouldEmitProfiling()) {
<span class="line-modified"> 769         buffer-&gt;forEach([&amp;] (ValueProfileAndVirtualRegister&amp; profile) {</span>
 770             JSValueRegs regs(regT0);
 771             emitGetVirtualRegister(profile.m_operand, regs);
 772             emitValueProfilingSite(static_cast&lt;ValueProfile&amp;&gt;(profile));
 773         });
 774     }
 775 #endif // ENABLE(DFG_JIT)
 776 }
 777 
 778 void JIT::emit_op_identity_with_profile(const Instruction*)
 779 {
 780     // We don&#39;t need to do anything here...
 781 }
 782 
 783 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 784 {
 785     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
<span class="line-modified"> 786     VirtualRegister currentScope = bytecode.m_scope;</span>
 787     emitGetVirtualRegister(currentScope, regT0);
 788     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
<span class="line-modified"> 789     emitStoreCell(bytecode.m_dst, regT0);</span>
 790 }
 791 
 792 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 793 {
 794     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 795     size_t tableIndex = bytecode.m_tableIndex;
 796     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
<span class="line-modified"> 797     VirtualRegister scrutinee = bytecode.m_scrutinee;</span>
 798 
 799     // create jump table for switch destinations, track this switch statement.
 800     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
<span class="line-modified"> 801     m_switches.append(SwitchRecord(jumpTable, m_bytecodeIndex, defaultOffset, SwitchRecord::Immediate));</span>
 802     jumpTable-&gt;ensureCTITable();
 803 
 804     emitGetVirtualRegister(scrutinee, regT0);
<span class="line-modified"> 805     callOperation(operationSwitchImmWithUnknownKeyType, TrustedImmPtr(&amp;vm()), regT0, tableIndex);</span>
 806     farJump(returnValueGPR, JSSwitchPtrTag);
 807 }
 808 
 809 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 810 {
 811     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 812     size_t tableIndex = bytecode.m_tableIndex;
 813     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
<span class="line-modified"> 814     VirtualRegister scrutinee = bytecode.m_scrutinee;</span>
 815 
 816     // create jump table for switch destinations, track this switch statement.
 817     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
<span class="line-modified"> 818     m_switches.append(SwitchRecord(jumpTable, m_bytecodeIndex, defaultOffset, SwitchRecord::Character));</span>
 819     jumpTable-&gt;ensureCTITable();
 820 
 821     emitGetVirtualRegister(scrutinee, regT0);
<span class="line-modified"> 822     callOperation(operationSwitchCharWithUnknownKeyType, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, tableIndex);</span>
 823     farJump(returnValueGPR, JSSwitchPtrTag);
 824 }
 825 
 826 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 827 {
 828     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 829     size_t tableIndex = bytecode.m_tableIndex;
 830     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
<span class="line-modified"> 831     VirtualRegister scrutinee = bytecode.m_scrutinee;</span>
 832 
 833     // create jump table for switch destinations, track this switch statement.
 834     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
<span class="line-modified"> 835     m_switches.append(SwitchRecord(jumpTable, m_bytecodeIndex, defaultOffset));</span>
 836 
 837     emitGetVirtualRegister(scrutinee, regT0);
<span class="line-modified"> 838     callOperation(operationSwitchStringWithUnknownKeyType, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, tableIndex);</span>
 839     farJump(returnValueGPR, JSSwitchPtrTag);
 840 }
 841 
 842 void JIT::emit_op_debug(const Instruction* currentInstruction)
 843 {
 844     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 845     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 846     Jump noDebuggerRequests = branchTest32(Zero, regT0);
<span class="line-modified"> 847     callOperation(operationDebug, &amp;vm(), static_cast&lt;int&gt;(bytecode.m_debugHookType));</span>
 848     noDebuggerRequests.link(this);
 849 }
 850 
 851 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 852 {
 853     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
<span class="line-modified"> 854     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 855     VirtualRegister src1 = bytecode.m_operand;</span>
 856 
 857     emitGetVirtualRegister(src1, regT0);
 858     Jump isImmediate = branchIfNotCell(regT0);
 859 
 860     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 861     move(TrustedImm32(0), regT0);
 862     Jump wasNotMasqueradesAsUndefined = jump();
 863 
 864     isMasqueradesAsUndefined.link(this);
 865     emitLoadStructure(vm(), regT0, regT2, regT1);
 866     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 867     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 868     comparePtr(Equal, regT0, regT2, regT0);
 869     Jump wasNotImmediate = jump();
 870 
 871     isImmediate.link(this);
 872 
<span class="line-modified"> 873     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
<span class="line-modified"> 874     compare64(Equal, regT0, TrustedImm32(JSValue::ValueNull), regT0);</span>
 875 
 876     wasNotImmediate.link(this);
 877     wasNotMasqueradesAsUndefined.link(this);
 878 
 879     boxBoolean(regT0, JSValueRegs { regT0 });
 880     emitPutVirtualRegister(dst);
 881 
 882 }
 883 
 884 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 885 {
 886     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
<span class="line-modified"> 887     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified"> 888     VirtualRegister src1 = bytecode.m_operand;</span>
 889 
 890     emitGetVirtualRegister(src1, regT0);
 891     Jump isImmediate = branchIfNotCell(regT0);
 892 
 893     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 894     move(TrustedImm32(1), regT0);
 895     Jump wasNotMasqueradesAsUndefined = jump();
 896 
 897     isMasqueradesAsUndefined.link(this);
 898     emitLoadStructure(vm(), regT0, regT2, regT1);
 899     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 900     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 901     comparePtr(NotEqual, regT0, regT2, regT0);
 902     Jump wasNotImmediate = jump();
 903 
 904     isImmediate.link(this);
 905 
<span class="line-modified"> 906     and64(TrustedImm32(~JSValue::UndefinedTag), regT0);</span>
<span class="line-modified"> 907     compare64(NotEqual, regT0, TrustedImm32(JSValue::ValueNull), regT0);</span>
 908 
 909     wasNotImmediate.link(this);
 910     wasNotMasqueradesAsUndefined.link(this);
 911 
 912     boxBoolean(regT0, JSValueRegs { regT0 });
 913     emitPutVirtualRegister(dst);
 914 }
 915 
<span class="line-added"> 916 void JIT::emit_op_enter(const Instruction*)</span>
<span class="line-added"> 917 {</span>
<span class="line-added"> 918     // Even though CTI doesn&#39;t use them, we initialize our constant</span>
<span class="line-added"> 919     // registers to zap stale pointers, to avoid unnecessarily prolonging</span>
<span class="line-added"> 920     // object lifetime and increasing GC pressure.</span>
<span class="line-added"> 921     size_t count = m_codeBlock-&gt;numVars();</span>
<span class="line-added"> 922     for (size_t j = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); j &lt; count; ++j)</span>
<span class="line-added"> 923         emitInitRegister(virtualRegisterForLocal(j));</span>
<span class="line-added"> 924 </span>
<span class="line-added"> 925     emitWriteBarrier(m_codeBlock);</span>
<span class="line-added"> 926 </span>
<span class="line-added"> 927     emitEnterOptimizationCheck();</span>
<span class="line-added"> 928 }</span>
<span class="line-added"> 929 </span>
 930 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 931 {
 932     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
<span class="line-modified"> 933     VirtualRegister dst = bytecode.m_dst;</span>
 934     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 935     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 936     emitStoreCell(dst, regT0);
 937 }
 938 
 939 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 940 {
 941     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 942     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 943     StructureID* cachedStructureID = &amp;metadata.m_cachedStructureID;
<span class="line-modified"> 944     emitGetVirtualRegister(bytecode.m_srcDst, regT1);</span>
 945 
 946     emitJumpSlowCaseIfNotJSCell(regT1);
 947 
 948     addSlowCase(branchIfNotType(regT1, FinalObjectType));
 949     load32(cachedStructureID, regT2);
 950     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 951 }
 952 
 953 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 954 {
 955     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 956     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 957     VirtualRegister callee = bytecode.m_callee;</span>
 958     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 959     RegisterID calleeReg = regT0;
 960     RegisterID rareDataReg = regT4;
 961     RegisterID resultReg = regT0;
 962     RegisterID allocatorReg = regT1;
 963     RegisterID structureReg = regT2;
 964     RegisterID cachedFunctionReg = regT4;
 965     RegisterID scratchReg = regT3;
 966 
 967     emitGetVirtualRegister(callee, calleeReg);
 968     addSlowCase(branchIfNotFunction(calleeReg));
<span class="line-modified"> 969     loadPtr(Address(calleeReg, JSFunction::offsetOfExecutableOrRareData()), rareDataReg);</span>
<span class="line-modified"> 970     addSlowCase(branchTestPtr(Zero, rareDataReg, TrustedImm32(JSFunction::rareDataTag)));</span>
<span class="line-modified"> 971     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfAllocator() - JSFunction::rareDataTag), allocatorReg);</span>
<span class="line-modified"> 972     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfStructure() - JSFunction::rareDataTag), structureReg);</span>
 973 
 974     loadPtr(cachedFunction, cachedFunctionReg);
 975     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 976     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 977     hasSeenMultipleCallees.link(this);
 978 
 979     JumpList slowCases;
 980     auto butterfly = TrustedImmPtr(nullptr);
 981     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
 982     load8(Address(structureReg, Structure::inlineCapacityOffset()), scratchReg);
 983     emitInitializeInlineStorage(resultReg, scratchReg);
 984     addSlowCase(slowCases);
<span class="line-modified"> 985     emitPutVirtualRegister(bytecode.m_dst);</span>
 986 }
 987 
 988 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 989 {
 990     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
<span class="line-modified"> 991     emitGetVirtualRegister(bytecode.m_targetVirtualRegister, regT0);</span>
 992     addSlowCase(branchIfEmpty(regT0));
 993 }
 994 
 995 
 996 // Slow cases
 997 
 998 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 999 {
1000     linkAllSlowCases(iter);
1001 
1002     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
<span class="line-modified">1003     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
1004     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
<span class="line-modified">1005     emitPutVirtualRegister(bytecode.m_dst, returnValueGPR);</span>
1006 }
1007 
1008 void JIT::emitSlow_op_neq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1009 {
1010     linkAllSlowCases(iter);
1011 
1012     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
<span class="line-modified">1013     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
1014     xor32(TrustedImm32(0x1), regT0);
1015     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
<span class="line-modified">1016     emitPutVirtualRegister(bytecode.m_dst, returnValueGPR);</span>
1017 }
1018 
1019 void JIT::emitSlow_op_jeq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1020 {
1021     linkAllSlowCases(iter);
1022 
1023     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
1024     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified">1025     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
1026     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
1027 }
1028 
1029 void JIT::emitSlow_op_jneq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1030 {
1031     linkAllSlowCases(iter);
1032 
1033     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
1034     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
<span class="line-modified">1035     callOperation(operationCompareEq, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1);</span>
1036     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
1037 }
1038 
1039 void JIT::emitSlow_op_instanceof_custom(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1040 {
1041     linkAllSlowCases(iter);
1042 
1043     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
<span class="line-modified">1044     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1045     VirtualRegister value = bytecode.m_value;</span>
<span class="line-modified">1046     VirtualRegister constructor = bytecode.m_constructor;</span>
<span class="line-modified">1047     VirtualRegister hasInstanceValue = bytecode.m_hasInstanceValue;</span>
1048 
1049     emitGetVirtualRegister(value, regT0);
1050     emitGetVirtualRegister(constructor, regT1);
1051     emitGetVirtualRegister(hasInstanceValue, regT2);
<span class="line-modified">1052     callOperation(operationInstanceOfCustom, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, regT2);</span>
1053     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1054     emitPutVirtualRegister(dst, returnValueGPR);
1055 }
1056 
1057 #endif // USE(JSVALUE64)
1058 
1059 void JIT::emit_op_loop_hint(const Instruction*)
1060 {



1061     // Emit the JIT optimization check:
1062     if (canBeOptimized()) {
1063         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1064             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1065     }

1066 }
1067 
1068 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1069 {


1070 #if ENABLE(DFG_JIT)
1071     // Emit the slow path for the JIT optimization check:
1072     if (canBeOptimized()) {
<span class="line-modified">1073         linkAllSlowCases(iter);</span>

1074 
1075         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);
1076 
<span class="line-modified">1077         callOperation(operationOptimize, &amp;vm(), m_bytecodeIndex.asBits());</span>
<span class="line-modified">1078         Jump noOptimizedEntry = branchTestPtr(Zero, returnValueGPR);</span>
<span class="line-modified">1079         if (ASSERT_ENABLED) {</span>
1080             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1081             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1082             ok.link(this);
1083         }
1084         farJump(returnValueGPR, GPRInfo::callFrameRegister);
<span class="line-added">1085         noOptimizedEntry.link(this);</span>
<span class="line-added">1086 </span>
<span class="line-added">1087         emitJumpSlowToHot(jump(), currentInstruction-&gt;size());</span>
1088     }
1089 #else
1090     UNUSED_PARAM(currentInstruction);
<span class="line-added">1091     UNUSED_PARAM(iter);</span>
1092 #endif
1093 }
1094 
<span class="line-added">1095 void JIT::emit_op_check_traps(const Instruction*)</span>
<span class="line-added">1096 {</span>
<span class="line-added">1097     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
<span class="line-added">1098 }</span>
<span class="line-added">1099 </span>
1100 void JIT::emit_op_nop(const Instruction*)
1101 {
1102 }
1103 
1104 void JIT::emit_op_super_sampler_begin(const Instruction*)
1105 {
1106     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1107 }
1108 
1109 void JIT::emit_op_super_sampler_end(const Instruction*)
1110 {
1111     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1112 }
1113 
<span class="line-modified">1114 void JIT::emitSlow_op_check_traps(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)</span>
1115 {
<span class="line-modified">1116     linkAllSlowCases(iter);</span>





























1117 
<span class="line-modified">1118     callOperation(operationHandleTraps, TrustedImmPtr(m_codeBlock-&gt;globalObject()));</span>






1119 }
1120 
1121 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1122 {
1123     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
<span class="line-modified">1124     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1125     VirtualRegister regexp = bytecode.m_regexp;</span>
<span class="line-modified">1126     callOperation(operationNewRegexp, TrustedImmPtr(m_codeBlock-&gt;globalObject()), jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));</span>
1127     emitStoreCell(dst, returnValueGPR);
1128 }
1129 
1130 template&lt;typename Op&gt;
1131 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1132 {
1133     Jump lazyJump;
1134     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
<span class="line-modified">1135     VirtualRegister dst = bytecode.m_dst;</span>
1136 
1137 #if USE(JSVALUE64)
<span class="line-modified">1138     emitGetVirtualRegister(bytecode.m_scope, regT0);</span>
1139 #else
<span class="line-modified">1140     emitLoadPayload(bytecode.m_scope, regT0);</span>
1141 #endif
1142     FunctionExecutable* funcExec = m_codeBlock-&gt;functionDecl(bytecode.m_functionDecl);
1143 
1144     OpcodeID opcodeID = Op::opcodeID;
1145     if (opcodeID == op_new_func)
<span class="line-modified">1146         callOperation(operationNewFunction, dst, &amp;vm(), regT0, funcExec);</span>
1147     else if (opcodeID == op_new_generator_func)
<span class="line-modified">1148         callOperation(operationNewGeneratorFunction, dst, &amp;vm(), regT0, funcExec);</span>
1149     else if (opcodeID == op_new_async_func)
<span class="line-modified">1150         callOperation(operationNewAsyncFunction, dst, &amp;vm(), regT0, funcExec);</span>
1151     else {
1152         ASSERT(opcodeID == op_new_async_generator_func);
<span class="line-modified">1153         callOperation(operationNewAsyncGeneratorFunction, dst, &amp;vm(), regT0, funcExec);</span>
1154     }
1155 }
1156 
1157 void JIT::emit_op_new_func(const Instruction* currentInstruction)
1158 {
1159     emitNewFuncCommon&lt;OpNewFunc&gt;(currentInstruction);
1160 }
1161 
1162 void JIT::emit_op_new_generator_func(const Instruction* currentInstruction)
1163 {
1164     emitNewFuncCommon&lt;OpNewGeneratorFunc&gt;(currentInstruction);
1165 }
1166 
1167 void JIT::emit_op_new_async_generator_func(const Instruction* currentInstruction)
1168 {
1169     emitNewFuncCommon&lt;OpNewAsyncGeneratorFunc&gt;(currentInstruction);
1170 }
1171 
1172 void JIT::emit_op_new_async_func(const Instruction* currentInstruction)
1173 {
1174     emitNewFuncCommon&lt;OpNewAsyncFunc&gt;(currentInstruction);
1175 }
1176 
1177 template&lt;typename Op&gt;
1178 void JIT::emitNewFuncExprCommon(const Instruction* currentInstruction)
1179 {
1180     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
<span class="line-modified">1181     VirtualRegister dst = bytecode.m_dst;</span>
1182 #if USE(JSVALUE64)
<span class="line-modified">1183     emitGetVirtualRegister(bytecode.m_scope, regT0);</span>
1184 #else
<span class="line-modified">1185     emitLoadPayload(bytecode.m_scope, regT0);</span>
1186 #endif
1187 
1188     FunctionExecutable* function = m_codeBlock-&gt;functionExpr(bytecode.m_functionDecl);
1189     OpcodeID opcodeID = Op::opcodeID;
1190 
1191     if (opcodeID == op_new_func_exp)
<span class="line-modified">1192         callOperation(operationNewFunction, dst, &amp;vm(), regT0, function);</span>
1193     else if (opcodeID == op_new_generator_func_exp)
<span class="line-modified">1194         callOperation(operationNewGeneratorFunction, dst, &amp;vm(), regT0, function);</span>
1195     else if (opcodeID == op_new_async_func_exp)
<span class="line-modified">1196         callOperation(operationNewAsyncFunction, dst, &amp;vm(), regT0, function);</span>
1197     else {
1198         ASSERT(opcodeID == op_new_async_generator_func_exp);
<span class="line-modified">1199         callOperation(operationNewAsyncGeneratorFunction, dst, &amp;vm(), regT0, function);</span>
1200     }
1201 }
1202 
1203 void JIT::emit_op_new_func_exp(const Instruction* currentInstruction)
1204 {
1205     emitNewFuncExprCommon&lt;OpNewFuncExp&gt;(currentInstruction);
1206 }
1207 
1208 void JIT::emit_op_new_generator_func_exp(const Instruction* currentInstruction)
1209 {
1210     emitNewFuncExprCommon&lt;OpNewGeneratorFuncExp&gt;(currentInstruction);
1211 }
1212 
1213 void JIT::emit_op_new_async_func_exp(const Instruction* currentInstruction)
1214 {
1215     emitNewFuncExprCommon&lt;OpNewAsyncFuncExp&gt;(currentInstruction);
1216 }
1217 
1218 void JIT::emit_op_new_async_generator_func_exp(const Instruction* currentInstruction)
1219 {
1220     emitNewFuncExprCommon&lt;OpNewAsyncGeneratorFuncExp&gt;(currentInstruction);
1221 }
1222 
1223 void JIT::emit_op_new_array(const Instruction* currentInstruction)
1224 {
1225     auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
1226     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified">1227     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1228     VirtualRegister valuesStart = bytecode.m_argv;</span>
1229     int size = bytecode.m_argc;
<span class="line-modified">1230     addPtr(TrustedImm32(valuesStart.offset() * sizeof(Register)), callFrameRegister, regT0);</span>
<span class="line-modified">1231     callOperation(operationNewArrayWithProfile, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()),</span>
1232         &amp;metadata.m_arrayAllocationProfile, regT0, size);
1233 }
1234 
1235 void JIT::emit_op_new_array_with_size(const Instruction* currentInstruction)
1236 {
1237     auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
1238     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified">1239     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1240     VirtualRegister sizeIndex = bytecode.m_length;</span>
1241 #if USE(JSVALUE64)
1242     emitGetVirtualRegister(sizeIndex, regT0);
<span class="line-modified">1243     callOperation(operationNewArrayWithSizeAndProfile, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()),</span>
1244         &amp;metadata.m_arrayAllocationProfile, regT0);
1245 #else
1246     emitLoad(sizeIndex, regT1, regT0);
<span class="line-modified">1247     callOperation(operationNewArrayWithSizeAndProfile, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()),</span>
1248         &amp;metadata.m_arrayAllocationProfile, JSValueRegs(regT1, regT0));
1249 #endif
1250 }
1251 
1252 #if USE(JSVALUE64)
1253 void JIT::emit_op_has_structure_property(const Instruction* currentInstruction)
1254 {
1255     auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
<span class="line-modified">1256     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1257     VirtualRegister base = bytecode.m_base;</span>
<span class="line-modified">1258     VirtualRegister enumerator = bytecode.m_enumerator;</span>
1259 
1260     emitGetVirtualRegister(base, regT0);
1261     emitGetVirtualRegister(enumerator, regT1);
1262     emitJumpSlowCaseIfNotJSCell(regT0, base);
1263 
1264     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1265     addSlowCase(branch32(NotEqual, regT0, Address(regT1, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1266 
1267     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1268     emitPutVirtualRegister(dst);
1269 }
1270 
1271 void JIT::privateCompileHasIndexedProperty(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1272 {
1273     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1274 
1275     PatchableJump badType;
1276 
1277     // FIXME: Add support for other types like TypedArrays and Arguments.
1278     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
</pre>
<hr />
<pre>
1282 
1283     LinkBuffer patchBuffer(*this, m_codeBlock);
1284 
1285     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1286     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1287 
1288     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1289 
1290     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1291         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1292         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1293 
1294     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1295     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1296 }
1297 
1298 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1299 {
1300     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1301     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified">1302     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1303     VirtualRegister base = bytecode.m_base;</span>
<span class="line-modified">1304     VirtualRegister property = bytecode.m_property;</span>
1305     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1306     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1307 
1308     emitGetVirtualRegisters(base, regT0, property, regT1);
1309 
1310     emitJumpSlowCaseIfNotInt(regT1);
1311 
1312     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1313     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1314     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1315     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1316     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1317     // extending since it makes it easier to re-tag the value in the slow case.
1318     zeroExtend32ToPtr(regT1, regT1);
1319 
1320     emitJumpSlowCaseIfNotJSCell(regT0, base);
1321     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1322     and32(TrustedImm32(IndexingShapeMask), regT2);
1323 
1324     JITArrayMode mode = chooseArrayMode(profile);
1325     PatchableJump badType;
1326 
1327     // FIXME: Add support for other types like TypedArrays and Arguments.
1328     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1329     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1330 
1331     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1332 
1333     addSlowCase(badType);
1334     addSlowCase(slowCases);
1335 
1336     Label done = label();
1337 
1338     emitPutVirtualRegister(dst);
1339 
1340     Label nextHotPath = label();
1341 
<span class="line-modified">1342     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeIndex, PatchableJump(), badType, mode, profile, done, nextHotPath));</span>
1343 }
1344 
1345 void JIT::emitSlow_op_has_indexed_property(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1346 {
1347     linkAllSlowCases(iter);
1348 
1349     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
<span class="line-modified">1350     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1351     VirtualRegister base = bytecode.m_base;</span>
<span class="line-modified">1352     VirtualRegister property = bytecode.m_property;</span>
1353     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
1354 
1355     Label slowPath = label();
1356 
1357     emitGetVirtualRegister(base, regT0);
1358     emitGetVirtualRegister(property, regT1);
<span class="line-modified">1359     Call call = callOperation(operationHasIndexedPropertyDefault, dst, TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0, regT1, byValInfo);</span>
1360 
1361     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
1362     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
1363     m_byValInstructionIndex++;
1364 }
1365 
1366 void JIT::emit_op_get_direct_pname(const Instruction* currentInstruction)
1367 {
1368     auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
<span class="line-modified">1369     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1370     VirtualRegister base = bytecode.m_base;</span>
<span class="line-modified">1371     VirtualRegister index = bytecode.m_index;</span>
<span class="line-modified">1372     VirtualRegister enumerator = bytecode.m_enumerator;</span>
1373 
1374     // Check that base is a cell
1375     emitGetVirtualRegister(base, regT0);
1376     emitJumpSlowCaseIfNotJSCell(regT0, base);
1377 
1378     // Check the structure
1379     emitGetVirtualRegister(enumerator, regT2);
1380     load32(Address(regT0, JSCell::structureIDOffset()), regT1);
1381     addSlowCase(branch32(NotEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1382 
1383     // Compute the offset
1384     emitGetVirtualRegister(index, regT1);
1385     // If index is less than the enumerator&#39;s cached inline storage, then it&#39;s an inline access
1386     Jump outOfLineAccess = branch32(AboveOrEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()));
1387     addPtr(TrustedImm32(JSObject::offsetOfInlineStorage()), regT0);
1388     signExtend32ToPtr(regT1, regT1);
1389     load64(BaseIndex(regT0, regT1, TimesEight), regT0);
1390 
1391     Jump done = jump();
1392 
1393     // Otherwise it&#39;s out of line
1394     outOfLineAccess.link(this);
1395     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
1396     sub32(Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()), regT1);
1397     neg32(regT1);
1398     signExtend32ToPtr(regT1, regT1);
1399     int32_t offsetOfFirstProperty = static_cast&lt;int32_t&gt;(offsetInButterfly(firstOutOfLineOffset)) * sizeof(EncodedJSValue);
1400     load64(BaseIndex(regT0, regT1, TimesEight, offsetOfFirstProperty), regT0);
1401 
1402     done.link(this);
1403     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1404     emitPutVirtualRegister(dst, regT0);
1405 }
1406 
1407 void JIT::emit_op_enumerator_structure_pname(const Instruction* currentInstruction)
1408 {
1409     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
<span class="line-modified">1410     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1411     VirtualRegister enumerator = bytecode.m_enumerator;</span>
<span class="line-modified">1412     VirtualRegister index = bytecode.m_index;</span>
1413 
1414     emitGetVirtualRegister(index, regT0);
1415     emitGetVirtualRegister(enumerator, regT1);
1416     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endStructurePropertyIndexOffset()));
1417 
1418     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1419 
1420     Jump done = jump();
1421     inBounds.link(this);
1422 
1423     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1424     signExtend32ToPtr(regT0, regT0);
1425     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1426 
1427     done.link(this);
1428     emitPutVirtualRegister(dst);
1429 }
1430 
1431 void JIT::emit_op_enumerator_generic_pname(const Instruction* currentInstruction)
1432 {
1433     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
<span class="line-modified">1434     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1435     VirtualRegister enumerator = bytecode.m_enumerator;</span>
<span class="line-modified">1436     VirtualRegister index = bytecode.m_index;</span>
1437 
1438     emitGetVirtualRegister(index, regT0);
1439     emitGetVirtualRegister(enumerator, regT1);
1440     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endGenericPropertyIndexOffset()));
1441 
1442     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1443 
1444     Jump done = jump();
1445     inBounds.link(this);
1446 
1447     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1448     signExtend32ToPtr(regT0, regT0);
1449     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1450 
1451     done.link(this);
1452     emitPutVirtualRegister(dst);
1453 }
1454 
1455 void JIT::emit_op_profile_type(const Instruction* currentInstruction)
1456 {
1457     auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
1458     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1459     TypeLocation* cachedTypeLocation = metadata.m_typeLocation;
<span class="line-modified">1460     VirtualRegister valueToProfile = bytecode.m_targetVirtualRegister;</span>
1461 
1462     emitGetVirtualRegister(valueToProfile, regT0);
1463 
1464     JumpList jumpToEnd;
1465 
1466     jumpToEnd.append(branchIfEmpty(regT0));
1467 
1468     // Compile in a predictive type check, if possible, to see if we can skip writing to the log.
1469     // These typechecks are inlined to match those of the 64-bit JSValue type checks.
1470     if (cachedTypeLocation-&gt;m_lastSeenType == TypeUndefined)
1471         jumpToEnd.append(branchIfUndefined(regT0));
1472     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNull)
1473         jumpToEnd.append(branchIfNull(regT0));
1474     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeBoolean)
1475         jumpToEnd.append(branchIfBoolean(regT0, regT1));
1476     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeAnyInt)
1477         jumpToEnd.append(branchIfInt32(regT0));
1478     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNumber)
1479         jumpToEnd.append(branchIfNumber(regT0));
1480     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeString) {
</pre>
<hr />
<pre>
1493     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::valueOffset()));
1494 
1495     // Store the structureID of the cell if T0 is a cell, otherwise, store 0 on the log entry.
1496     Jump notCell = branchIfNotCell(regT0);
1497     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1498     store32(regT0, Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1499     Jump skipIsCell = jump();
1500     notCell.link(this);
1501     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1502     skipIsCell.link(this);
1503 
1504     // Store the typeLocation on the log entry.
1505     move(TrustedImmPtr(cachedTypeLocation), regT0);
1506     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1507 
1508     // Increment the current log entry.
1509     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1510     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1511     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1512     // Clear the log if we&#39;re at the end of the log.
<span class="line-modified">1513     callOperation(operationProcessTypeProfilerLog, &amp;vm());</span>
1514     skipClearLog.link(this);
1515 
1516     jumpToEnd.link(this);
1517 }
1518 
1519 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1520 {
1521     RELEASE_ASSERT(vm().shadowChicken());
1522     updateTopCallFrame();
1523     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1524     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1525     GPRReg shadowPacketReg = regT0;
1526     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1527     GPRReg scratch2Reg = regT2;
1528     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
<span class="line-modified">1529     emitGetVirtualRegister(bytecode.m_scope, regT3);</span>
1530     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1531 }
1532 
1533 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1534 {
1535     RELEASE_ASSERT(vm().shadowChicken());
1536     updateTopCallFrame();
1537     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1538     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1539     GPRReg shadowPacketReg = regT0;
1540     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1541     GPRReg scratch2Reg = regT2;
1542     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
<span class="line-modified">1543     emitGetVirtualRegister(bytecode.m_thisValue, regT2);</span>
<span class="line-modified">1544     emitGetVirtualRegister(bytecode.m_scope, regT3);</span>
<span class="line-modified">1545     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeIndex));</span>
1546 }
1547 
1548 #endif // USE(JSVALUE64)
1549 
1550 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1551 {
1552     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1553     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1554     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1555 #if USE(JSVALUE64)
1556     basicBlockLocation-&gt;emitExecuteCode(*this);
1557 #else
1558     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1559 #endif
1560 }
1561 
1562 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
1563 {
1564     auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
<span class="line-modified">1565     VirtualRegister dst = bytecode.m_dst;</span>
<span class="line-modified">1566     load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT0);</span>
1567     sub32(TrustedImm32(1), regT0);
1568     JSValueRegs result = JSValueRegs::withTwoAvailableRegs(regT0, regT1);
1569     boxInt32(regT0, result);
1570     emitPutVirtualRegister(dst, result);
1571 }
1572 
1573 void JIT::emit_op_get_rest_length(const Instruction* currentInstruction)
1574 {
1575     auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
<span class="line-modified">1576     VirtualRegister dst = bytecode.m_dst;</span>
1577     unsigned numParamsToSkip = bytecode.m_numParametersToSkip;
<span class="line-modified">1578     load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT0);</span>
1579     sub32(TrustedImm32(1), regT0);
1580     Jump zeroLength = branch32(LessThanOrEqual, regT0, Imm32(numParamsToSkip));
1581     sub32(Imm32(numParamsToSkip), regT0);
1582 #if USE(JSVALUE64)
1583     boxInt32(regT0, JSValueRegs(regT0));
1584 #endif
1585     Jump done = jump();
1586 
1587     zeroLength.link(this);
1588 #if USE(JSVALUE64)
1589     move(TrustedImm64(JSValue::encode(jsNumber(0))), regT0);
1590 #else
1591     move(TrustedImm32(0), regT0);
1592 #endif
1593 
1594     done.link(this);
1595 #if USE(JSVALUE64)
1596     emitPutVirtualRegister(dst, regT0);
1597 #else
1598     move(TrustedImm32(JSValue::Int32Tag), regT1);
1599     emitPutVirtualRegister(dst, JSValueRegs(regT1, regT0));
1600 #endif
1601 }
1602 
1603 void JIT::emit_op_get_argument(const Instruction* currentInstruction)
1604 {
1605     auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
<span class="line-modified">1606     VirtualRegister dst = bytecode.m_dst;</span>
1607     int index = bytecode.m_index;
1608 #if USE(JSVALUE64)
1609     JSValueRegs resultRegs(regT0);
1610 #else
1611     JSValueRegs resultRegs(regT1, regT0);
1612 #endif
1613 
<span class="line-modified">1614     load32(payloadFor(CallFrameSlot::argumentCountIncludingThis), regT2);</span>
1615     Jump argumentOutOfBounds = branch32(LessThanOrEqual, regT2, TrustedImm32(index));
<span class="line-modified">1616     loadValue(addressFor(VirtualRegister(CallFrameSlot::thisArgument + index)), resultRegs);</span>
1617     Jump done = jump();
1618 
1619     argumentOutOfBounds.link(this);
1620     moveValue(jsUndefined(), resultRegs);
1621 
1622     done.link(this);
1623     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1624     emitPutVirtualRegister(dst, resultRegs);
1625 }
1626 
1627 } // namespace JSC
1628 
1629 #endif // ENABLE(JIT)
</pre>
</td>
</tr>
</table>
<center><a href="JITNegGenerator.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITOpcodes32_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>