diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp
@@ -78,19 +78,20 @@
 #include "VisitCounter.h"
 #include "WasmMemory.h"
 #include "WeakMapImplInlines.h"
 #include "WeakSetInlines.h"
 #include <algorithm>
+#include <wtf/CryptographicallyRandomNumber.h>
 #include <wtf/ListDump.h>
 #include <wtf/MainThread.h>
 #include <wtf/ParallelVectorIterator.h>
 #include <wtf/ProcessID.h>
 #include <wtf/RAMSize.h>
 #include <wtf/SimpleStats.h>
 #include <wtf/Threading.h>
 
-#if PLATFORM(IOS_FAMILY)
+#if USE(BMALLOC_MEMORY_FOOTPRINT_API)
 #include <bmalloc/bmalloc.h>
 #endif
 
 #if USE(FOUNDATION)
 #include <wtf/spi/cocoa/objcSPI.h>
@@ -127,11 +128,11 @@
 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 {
     if (VM::isInMiniMode())
         return Options::miniVMHeapGrowthFactor() * heapSize;
 
-#if PLATFORM(IOS_FAMILY)
+#if USE(BMALLOC_MEMORY_FOOTPRINT_API)
     size_t memoryFootprint = bmalloc::api::memoryFootprint();
     if (memoryFootprint < ramSize * Options::smallHeapRAMFraction())
         return Options::smallHeapGrowthFactor() * heapSize;
     if (memoryFootprint < ramSize * Options::mediumHeapRAMFraction())
         return Options::mediumHeapGrowthFactor() * heapSize;
@@ -369,17 +370,17 @@
 void Heap::dumpHeapStatisticsAtVMDestruction()
 {
     unsigned counter = 0;
     m_objectSpace.forEachBlock([&] (MarkedBlock::Handle* block) {
         unsigned live = 0;
-        block->forEachCell([&] (HeapCell* cell, HeapCell::Kind) {
+        block->forEachCell([&] (size_t, HeapCell* cell, HeapCell::Kind) {
             if (cell->isLive())
                 live++;
             return IterationStatus::Continue;
         });
         dataLogLn("[", counter++, "] ", block->cellSize(), ", ", live, " / ", block->cellsPerBlock(), " ", static_cast<double>(live) / block->cellsPerBlock() * 100, "% ", block->attributes(), " ", block->subspace()->name());
-        block->forEachCell([&] (HeapCell* heapCell, HeapCell::Kind kind) {
+        block->forEachCell([&] (size_t, HeapCell* heapCell, HeapCell::Kind kind) {
             if (heapCell->isLive() && kind == HeapCell::Kind::JSCell) {
                 auto* cell = static_cast<JSCell*>(heapCell);
                 if (cell->isObject())
                     dataLogLn("    ", JSValue((JSObject*)cell));
                 else
@@ -393,11 +394,11 @@
 // The VM is being destroyed and the collector will never run again.
 // Run all pending finalizers now because we won't get another chance.
 void Heap::lastChanceToFinalize()
 {
     MonotonicTime before;
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         before = MonotonicTime::now();
         dataLog("[GC<", RawPointer(this), ">: shutdown ");
     }
 
     m_isShuttingDown = true;
@@ -412,43 +413,38 @@
             m_collectContinuouslyCondition.notifyOne();
         }
         m_collectContinuouslyThread->waitForCompletion();
     }
 
-    if (Options::logGC())
-        dataLog("1");
+    dataLogIf(Options::logGC(), "1");
 
     // Prevent new collections from being started. This is probably not even necessary, since we're not
     // going to call into anything that starts collections. Still, this makes the algorithm more
     // obviously sound.
     m_isSafeToCollect = false;
 
-    if (Options::logGC())
-        dataLog("2");
+    dataLogIf(Options::logGC(), "2");
 
     bool isCollecting;
     {
         auto locker = holdLock(*m_threadLock);
         RELEASE_ASSERT(m_lastServedTicket <= m_lastGrantedTicket);
         isCollecting = m_lastServedTicket < m_lastGrantedTicket;
     }
     if (isCollecting) {
-        if (Options::logGC())
-            dataLog("...]\n");
+        dataLogIf(Options::logGC(), "...]\n");
 
         // Wait for the current collection to finish.
         waitForCollector(
             [&] (const AbstractLocker&) -> bool {
                 RELEASE_ASSERT(m_lastServedTicket <= m_lastGrantedTicket);
                 return m_lastServedTicket == m_lastGrantedTicket;
             });
 
-        if (Options::logGC())
-            dataLog("[GC<", RawPointer(this), ">: shutdown ");
+        dataLogIf(Options::logGC(), "[GC<", RawPointer(this), ">: shutdown ");
     }
-    if (Options::logGC())
-        dataLog("3");
+    dataLogIf(Options::logGC(), "3");
 
     RELEASE_ASSERT(m_requests.isEmpty());
     RELEASE_ASSERT(m_lastServedTicket == m_lastGrantedTicket);
 
     // Carefully bring the thread down.
@@ -459,18 +455,16 @@
         m_threadShouldStop = true;
         if (!stopped)
             m_threadCondition->notifyOne(locker);
     }
 
-    if (Options::logGC())
-        dataLog("4");
+    dataLogIf(Options::logGC(), "4");
 
     if (!stopped)
         m_thread->join();
 
-    if (Options::logGC())
-        dataLog("5 ");
+    dataLogIf(Options::logGC(), "5 ");
 
     if (UNLIKELY(Options::dumpHeapStatisticsAtVMDestruction()))
         dumpHeapStatisticsAtVMDestruction();
 
     m_arrayBuffers.lastChanceToFinalize();
@@ -480,12 +474,11 @@
 
     sweepAllLogicallyEmptyWeakBlocks();
 
     m_objectSpace.freeMemory();
 
-    if (Options::logGC())
-        dataLog((MonotonicTime::now() - before).milliseconds(), "ms]\n");
+    dataLogIf(Options::logGC(), (MonotonicTime::now() - before).milliseconds(), "ms]\n");
 }
 
 void Heap::releaseDelayedReleasedObjects()
 {
 #if USE(FOUNDATION) || defined(JSC_GLIB_API_ENABLED)
@@ -536,14 +529,14 @@
     reportExtraMemoryAllocatedSlowCase(size);
 }
 
 bool Heap::overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType)
 {
-#if PLATFORM(IOS_FAMILY)
-    if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_precentAvailableMemoryCachedCallCount >= 100) {
+#if USE(BMALLOC_MEMORY_FOOTPRINT_API)
+    if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_percentAvailableMemoryCachedCallCount >= 100) {
         m_overCriticalMemoryThreshold = bmalloc::api::percentAvailableMemoryInUse() > Options::criticalGCMemoryThreshold();
-        m_precentAvailableMemoryCachedCallCount = 0;
+        m_percentAvailableMemoryCachedCallCount = 0;
     }
 
     return m_overCriticalMemoryThreshold;
 #else
     UNUSED_PARAM(memoryThresholdCallType);
@@ -610,15 +603,15 @@
 void Heap::finalizeUnconditionalFinalizers()
 {
     vm().builtinExecutables()->finalizeUnconditionally();
     finalizeMarkedUnconditionalFinalizers<FunctionExecutable>(vm().functionExecutableSpace.space);
     finalizeMarkedUnconditionalFinalizers<SymbolTable>(vm().symbolTableSpace);
+    finalizeMarkedUnconditionalFinalizers<ExecutableToCodeBlockEdge>(vm().executableToCodeBlockEdgesWithFinalizers); // We run this before CodeBlock's unconditional finalizer since CodeBlock looks at the owner executable's installed CodeBlock in its finalizeUnconditionally.
     vm().forEachCodeBlockSpace(
         [&] (auto& space) {
             this->finalizeMarkedUnconditionalFinalizers<CodeBlock>(space.set);
         });
-    finalizeMarkedUnconditionalFinalizers<ExecutableToCodeBlockEdge>(vm().executableToCodeBlockEdgesWithFinalizers);
     finalizeMarkedUnconditionalFinalizers<StructureRareData>(vm().structureRareDataSpace);
     finalizeMarkedUnconditionalFinalizers<UnlinkedFunctionExecutable>(vm().unlinkedFunctionExecutableSpace.set);
     if (vm().m_weakSetSpace)
         finalizeMarkedUnconditionalFinalizers<JSWeakSet>(*vm().m_weakSetSpace);
     if (vm().m_weakMapSpace)
@@ -718,10 +711,11 @@
 {
 #if ENABLE(DFG_JIT)
     if (!VM::canUseJIT())
         return;
     m_vm.gatherScratchBufferRoots(roots);
+    m_vm.scanSideState(roots);
 #else
     UNUSED_PARAM(roots);
 #endif
 }
 
@@ -1047,17 +1041,17 @@
 }
 
 void Heap::sweepSynchronously()
 {
     MonotonicTime before { };
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         dataLog("Full sweep: ", capacity() / 1024, "kb ");
         before = MonotonicTime::now();
     }
-    m_objectSpace.sweep();
+    m_objectSpace.sweepBlocks();
     m_objectSpace.shrink();
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         MonotonicTime after = MonotonicTime::now();
         dataLog("=> ", capacity() / 1024, "kb, ", (after - before).milliseconds(), "ms");
     }
 }
 
@@ -1093,15 +1087,13 @@
         if (UNLIKELY(Options::useImmortalObjects()))
             sweeper().stopSweeping();
 
         bool alreadySweptInCollectSync = shouldSweepSynchronously();
         if (!alreadySweptInCollectSync) {
-            if (Options::logGC())
-                dataLog("[GC<", RawPointer(this), ">: ");
+            dataLogIf(Options::logGC(), "[GC<", RawPointer(this), ">: ");
             sweepSynchronously();
-            if (Options::logGC())
-                dataLog("]\n");
+            dataLogIf(Options::logGC(), "]\n");
         }
         m_objectSpace.assertNoUnswept();
 
         sweepAllLogicallyEmptyWeakBlocks();
         return;
@@ -1261,17 +1253,19 @@
         LockHolder locker(*m_threadLock);
         RELEASE_ASSERT(!m_requests.isEmpty());
         m_currentRequest = m_requests.first();
     }
 
-    if (Options::logGC())
-        dataLog("[GC<", RawPointer(this), ">: START ", gcConductorShortName(conn), " ", capacity() / 1024, "kb ");
+    dataLogIf(Options::logGC(), "[GC<", RawPointer(this), ">: START ", gcConductorShortName(conn), " ", capacity() / 1024, "kb ");
 
     m_beforeGC = MonotonicTime::now();
 
+    if (!Options::seedOfVMRandomForFuzzer())
+        vm().random().setSeed(cryptographicallyRandomNumber());
+
     if (m_collectionScope) {
-        dataLog("Collection scope already set during GC: ", *m_collectionScope, "\n");
+        dataLogLn("Collection scope already set during GC: ", *m_collectionScope);
         RELEASE_ASSERT_NOT_REACHED();
     }
 
     willStartCollection();
 
@@ -1328,11 +1322,11 @@
     SlotVisitor& slotVisitor = *m_collectorSlotVisitor;
 
     m_constraintSet->didStartMarking();
 
     m_scheduler->beginCollection();
-    if (Options::logGC())
+    if (UNLIKELY(Options::logGC()))
         m_scheduler->log();
 
     // After this, we will almost certainly fall through all of the "slotVisitor.isEmpty()"
     // checks because bootstrap would have put things into the visitor. So, we should fall
     // through to draining.
@@ -1356,11 +1350,11 @@
 {
     RELEASE_ASSERT(conn == GCConductor::Collector || m_currentThreadState);
 
     SlotVisitor& slotVisitor = *m_collectorSlotVisitor;
 
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         HashMap<const char*, size_t> visitMap;
         forEachSlotVisitor(
             [&] (SlotVisitor& slotVisitor) {
                 visitMap.add(slotVisitor.codeName(), slotVisitor.bytesVisited() / 1024);
             });
@@ -1400,12 +1394,11 @@
         }
 
         m_scheduler->didExecuteConstraints();
     }
 
-    if (Options::logGC())
-        dataLog(slotVisitor.collectorMarkStack().size(), "+", m_mutatorMarkStack->size() + slotVisitor.mutatorMarkStack().size(), " ");
+    dataLogIf(Options::logGC(), slotVisitor.collectorMarkStack().size(), "+", m_mutatorMarkStack->size() + slotVisitor.mutatorMarkStack().size(), " ");
 
     {
         ParallelModeEnabler enabler(slotVisitor);
         slotVisitor.drainInParallel(m_scheduler->timeToResume());
     }
@@ -1427,11 +1420,11 @@
     if (!m_scheduler->shouldResume())
         return true;
 
     m_scheduler->willResume();
 
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         double thisPauseMS = (MonotonicTime::now() - m_stopTime).milliseconds();
         dataLog("p=", thisPauseMS, "ms (max ", maxPauseMS(thisPauseMS), ")...]\n");
     }
 
     // Forgive the mutator for its past failures to keep up.
@@ -1471,16 +1464,15 @@
     return false;
 }
 
 NEVER_INLINE bool Heap::runReloopPhase(GCConductor conn)
 {
-    if (Options::logGC())
-        dataLog("[GC<", RawPointer(this), ">: ", gcConductorShortName(conn), " ");
+    dataLogIf(Options::logGC(), "[GC<", RawPointer(this), ">: ", gcConductorShortName(conn), " ");
 
     m_scheduler->didStop();
 
-    if (Options::logGC())
+    if (UNLIKELY(Options::logGC()))
         m_scheduler->log();
 
     return changePhase(conn, CollectorPhase::Fixpoint);
 }
 
@@ -1509,15 +1501,17 @@
     }
 
     if (vm().typeProfiler())
         vm().typeProfiler()->invalidateTypeSetCache(vm());
 
+    m_structureIDTable.flushOldTables();
+
     reapWeakHandles();
     pruneStaleEntriesFromWeakGCMaps();
     sweepArrayBuffers();
     snapshotUnswept();
-    finalizeUnconditionalFinalizers();
+    finalizeUnconditionalFinalizers(); // We rely on these unconditional finalizers running before clearCurrentlyExecuting since CodeBlock's finalizer relies on querying currently executing.
     removeDeadCompilerWorklistEntries();
     notifyIncrementalSweeper();
 
     m_codeBlocks->iterateCurrentlyExecuting(
         [&] (CodeBlock* codeBlock) {
@@ -1541,11 +1535,11 @@
     if (false) {
         dataLog("Heap state after GC:\n");
         m_objectSpace.dumpBits();
     }
 
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         double thisPauseMS = (m_afterGC - m_stopTime).milliseconds();
         dataLog("p=", thisPauseMS, "ms (max ", maxPauseMS(thisPauseMS), "), cycle ", (m_afterGC - m_beforeGC).milliseconds(), "ms END]\n");
     }
 
     {
@@ -1554,12 +1548,11 @@
         m_lastServedTicket++;
         clearMutatorWaiting();
     }
     ParkingLot::unparkAll(&m_worldState);
 
-    if (false)
-        dataLog("GC END!\n");
+    dataLogLnIf(Options::logGC(), "GC END!");
 
     setNeedFinalize();
 
     m_lastGCStartTime = m_currentGCStartTime;
     m_lastGCEndTime = MonotonicTime::now();
@@ -1656,11 +1649,10 @@
     UNUSED_PARAM(conn);
 
     if (auto* shadowChicken = vm().shadowChicken())
         shadowChicken->update(vm(), vm().topCallFrame);
 
-    m_structureIDTable.flushOldTables();
     m_objectSpace.stopAllocating();
 
     m_stopTime = MonotonicTime::now();
 }
 
@@ -2083,11 +2075,11 @@
 }
 
 void Heap::finalize()
 {
     MonotonicTime before;
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         before = MonotonicTime::now();
         dataLog("[GC<", RawPointer(this), ">: finalize ");
     }
 
     {
@@ -2106,11 +2098,11 @@
         callback.run(vm());
 
     if (shouldSweepSynchronously())
         sweepSynchronously();
 
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         MonotonicTime after = MonotonicTime::now();
         dataLog((after - before).milliseconds(), "ms]\n");
     }
 }
 
@@ -2147,12 +2139,16 @@
         });
 }
 
 void Heap::sweepInFinalize()
 {
-    m_objectSpace.sweepLargeAllocations();
-    vm().eagerlySweptDestructibleObjectSpace.sweep();
+    m_objectSpace.sweepPreciseAllocations();
+#if ENABLE(WEBASSEMBLY)
+    // We hold onto a lot of memory, so it makes a lot of sense to be swept eagerly.
+    if (vm().m_webAssemblyMemorySpace)
+        vm().m_webAssemblyMemorySpace->sweep();
+#endif
 }
 
 void Heap::suspendCompilerThreads()
 {
 #if ENABLE(DFG_JIT)
@@ -2166,26 +2162,19 @@
 #endif
 }
 
 void Heap::willStartCollection()
 {
-    if (Options::logGC())
-        dataLog("=> ");
+    dataLogIf(Options::logGC(), "=> ");
 
     if (shouldDoFullCollection()) {
         m_collectionScope = CollectionScope::Full;
         m_shouldDoFullCollection = false;
-        if (Options::logGC())
-            dataLog("FullCollection, ");
-        if (false)
-            dataLog("Full collection!\n");
+        dataLogIf(Options::logGC(), "FullCollection, ");
     } else {
         m_collectionScope = CollectionScope::Eden;
-        if (Options::logGC())
-            dataLog("EdenCollection, ");
-        if (false)
-            dataLog("Eden collection!\n");
+        dataLogIf(Options::logGC(), "EdenCollection, ");
     }
     if (m_collectionScope && m_collectionScope.value() == CollectionScope::Full) {
         m_sizeBeforeLastFullCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
         m_extraMemorySize = 0;
         m_deprecatedExtraMemorySize = 0;
@@ -2252,11 +2241,11 @@
     m_sweeper->startSweeping(*this);
 }
 
 void Heap::updateAllocationLimits()
 {
-    static const bool verbose = false;
+    static constexpr bool verbose = false;
 
     if (verbose) {
         dataLog("\n");
         dataLog("bytesAllocatedThisCycle = ", m_bytesAllocatedThisCycle, "\n");
     }
@@ -2278,11 +2267,11 @@
         dataLog("totalBytesVisited = ", m_totalBytesVisited, ", currentHeapSize = ", currentHeapSize, "\n");
 
     // It's up to the user to ensure that extraMemorySize() ends up corresponding to allocation-time
     // extra memory reporting.
     currentHeapSize += extraMemorySize();
-    if (!ASSERT_DISABLED) {
+    if (ASSERT_ENABLED) {
         Checked<size_t, RecordOverflow> checkedCurrentHeapSize = m_totalBytesVisited;
         checkedCurrentHeapSize += extraMemorySize();
         ASSERT(!checkedCurrentHeapSize.hasOverflowed() && checkedCurrentHeapSize.unsafeGet() == currentHeapSize);
     }
 
@@ -2330,22 +2319,21 @@
             ASSERT(currentHeapSize >= m_sizeAfterLastFullCollect);
             m_fullActivityCallback->didAllocate(*this, currentHeapSize - m_sizeAfterLastFullCollect);
         }
     }
 
-#if PLATFORM(IOS_FAMILY)
+#if USE(BMALLOC_MEMORY_FOOTPRINT_API)
     // Get critical memory threshold for next cycle.
     overCriticalMemoryThreshold(MemoryThresholdCallType::Direct);
 #endif
 
     m_sizeAfterLastCollect = currentHeapSize;
     if (verbose)
         dataLog("sizeAfterLastCollect = ", m_sizeAfterLastCollect, "\n");
     m_bytesAllocatedThisCycle = 0;
 
-    if (Options::logGC())
-        dataLog("=> ", currentHeapSize / 1024, "kb, ");
+    dataLogIf(Options::logGC(), "=> ", currentHeapSize / 1024, "kb, ");
 }
 
 void Heap::didFinishCollection()
 {
     m_afterGC = MonotonicTime::now();
@@ -2425,19 +2413,33 @@
         return false;
 
     return true;
 }
 
-void Heap::addFinalizer(JSCell* cell, Finalizer finalizer)
+void Heap::addFinalizer(JSCell* cell, CFinalizer finalizer)
 {
-    WeakSet::allocate(cell, &m_finalizerOwner, reinterpret_cast<void*>(finalizer)); // Balanced by FinalizerOwner::finalize().
+    WeakSet::allocate(cell, &m_cFinalizerOwner, bitwise_cast<void*>(finalizer)); // Balanced by CFinalizerOwner::finalize().
+}
+
+void Heap::addFinalizer(JSCell* cell, LambdaFinalizer function)
+{
+    WeakSet::allocate(cell, &m_lambdaFinalizerOwner, function.leakImpl()); // Balanced by LambdaFinalizerOwner::finalize().
+}
+
+void Heap::CFinalizerOwner::finalize(Handle<Unknown> handle, void* context)
+{
+    HandleSlot slot = handle.slot();
+    CFinalizer finalizer = bitwise_cast<CFinalizer>(context);
+    finalizer(slot->asCell());
+    WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
 }
 
-void Heap::FinalizerOwner::finalize(Handle<Unknown> handle, void* context)
+void Heap::LambdaFinalizerOwner::finalize(Handle<Unknown> handle, void* context)
 {
+    LambdaFinalizer::Impl* impl = bitwise_cast<LambdaFinalizer::Impl*>(context);
+    LambdaFinalizer finalizer(impl);
     HandleSlot slot = handle.slot();
-    Finalizer finalizer = reinterpret_cast<Finalizer>(context);
     finalizer(slot->asCell());
     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
 }
 
 void Heap::collectNowFullIfNotDoneRecently(Synchronousness synchronousness)
@@ -2627,11 +2629,11 @@
         if (m_bytesAllocatedThisCycle <= Options::gcMaxHeapSize())
             return;
     } else {
         size_t bytesAllowedThisCycle = m_maxEdenSize;
 
-#if PLATFORM(IOS_FAMILY)
+#if USE(BMALLOC_MEMORY_FOOTPRINT_API)
         if (overCriticalMemoryThreshold())
             bytesAllowedThisCycle = std::min(m_maxEdenSizeWhenCritical, bytesAllowedThisCycle);
 #endif
 
         if (m_bytesAllocatedThisCycle <= bytesAllowedThisCycle)
@@ -2768,12 +2770,12 @@
         [this] (SlotVisitor& slotVisitor) {
             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
 
 #if ENABLE(SAMPLING_PROFILER)
             if (SamplingProfiler* samplingProfiler = m_vm.samplingProfiler()) {
-                LockHolder locker(samplingProfiler->getLock());
-                samplingProfiler->processUnverifiedStackTraces();
+                auto locker = holdLock(samplingProfiler->getLock());
+                samplingProfiler->processUnverifiedStackTraces(locker);
                 samplingProfiler->visit(slotVisitor);
                 if (Options::logGC() == GCLogging::Verbose)
                     dataLog("Sampling Profiler data:\n", slotVisitor);
             }
 #endif // ENABLE(SAMPLING_PROFILER)
@@ -2866,11 +2868,11 @@
 }
 
 void Heap::notifyIsSafeToCollect()
 {
     MonotonicTime before;
-    if (Options::logGC()) {
+    if (UNLIKELY(Options::logGC())) {
         before = MonotonicTime::now();
         dataLog("[GC<", RawPointer(this), ">: starting ");
     }
 
     addCoreConstraints();
@@ -2906,12 +2908,11 @@
                     }
                 }
             });
     }
 
-    if (Options::logGC())
-        dataLog((MonotonicTime::now() - before).milliseconds(), "ms]\n");
+    dataLogIf(Options::logGC(), (MonotonicTime::now() - before).milliseconds(), "ms]\n");
 }
 
 void Heap::preventCollection()
 {
     if (!m_isSafeToCollect)
