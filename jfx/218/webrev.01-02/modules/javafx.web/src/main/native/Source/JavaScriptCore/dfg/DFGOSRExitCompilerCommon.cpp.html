<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExitCompilerCommon.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2013-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGOSRExitCompilerCommon.h&quot;
 28 
 29 #if ENABLE(DFG_JIT)
 30 
 31 #include &quot;Bytecodes.h&quot;
 32 #include &quot;CheckpointOSRExitSideState.h&quot;
 33 #include &quot;DFGJITCode.h&quot;
 34 #include &quot;DFGOperations.h&quot;
 35 #include &quot;JIT.h&quot;
 36 #include &quot;JSCJSValueInlines.h&quot;
 37 #include &quot;JSCInlines.h&quot;
 38 #include &quot;LLIntData.h&quot;
 39 #include &quot;LLIntThunks.h&quot;
 40 #include &quot;ProbeContext.h&quot;
 41 #include &quot;StructureStubInfo.h&quot;
 42 
 43 namespace JSC { namespace DFG {
 44 
 45 void handleExitCounts(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
 46 {
 47     if (!exitKindMayJettison(exit.m_kind)) {
 48         // FIXME: We may want to notice that we&#39;re frequently exiting
 49         // at an op_catch that we didn&#39;t compile an entrypoint for, and
 50         // then trigger a reoptimization of this CodeBlock:
 51         // https://bugs.webkit.org/show_bug.cgi?id=175842
 52         return;
 53     }
 54 
 55     jit.add32(AssemblyHelpers::TrustedImm32(1), AssemblyHelpers::AbsoluteAddress(&amp;exit.m_count));
 56 
 57     jit.move(AssemblyHelpers::TrustedImmPtr(jit.codeBlock()), GPRInfo::regT3);
 58 
 59     AssemblyHelpers::Jump tooFewFails;
 60 
 61     jit.load32(AssemblyHelpers::Address(GPRInfo::regT3, CodeBlock::offsetOfOSRExitCounter()), GPRInfo::regT2);
 62     jit.add32(AssemblyHelpers::TrustedImm32(1), GPRInfo::regT2);
 63     jit.store32(GPRInfo::regT2, AssemblyHelpers::Address(GPRInfo::regT3, CodeBlock::offsetOfOSRExitCounter()));
 64 
 65     jit.move(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), GPRInfo::regT0);
 66     AssemblyHelpers::Jump reoptimizeNow = jit.branch32(
 67         AssemblyHelpers::GreaterThanOrEqual,
 68         AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecuteCounter()),
 69         AssemblyHelpers::TrustedImm32(0));
 70 
 71     // We want to figure out if there&#39;s a possibility that we&#39;re in a loop. For the outermost
 72     // code block in the inline stack, we handle this appropriately by having the loop OSR trigger
 73     // check the exit count of the replacement of the CodeBlock from which we are OSRing. The
 74     // problem is the inlined functions, which might also have loops, but whose baseline versions
 75     // don&#39;t know where to look for the exit count. Figure out if those loops are severe enough
 76     // that we had tried to OSR enter. If so, then we should use the loop reoptimization trigger.
 77     // Otherwise, we should use the normal reoptimization trigger.
 78 
 79     AssemblyHelpers::JumpList loopThreshold;
 80 
 81     for (InlineCallFrame* inlineCallFrame = exit.m_codeOrigin.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;directCaller.inlineCallFrame()) {
 82         loopThreshold.append(
 83             jit.branchTest8(
 84                 AssemblyHelpers::NonZero,
 85                 AssemblyHelpers::AbsoluteAddress(
 86                     inlineCallFrame-&gt;baselineCodeBlock-&gt;ownerExecutable()-&gt;addressOfDidTryToEnterInLoop())));
 87     }
 88 
 89     jit.move(
 90         AssemblyHelpers::TrustedImm32(jit.codeBlock()-&gt;exitCountThresholdForReoptimization()),
 91         GPRInfo::regT1);
 92 
 93     if (!loopThreshold.empty()) {
 94         AssemblyHelpers::Jump done = jit.jump();
 95 
 96         loopThreshold.link(&amp;jit);
 97         jit.move(
 98             AssemblyHelpers::TrustedImm32(
 99                 jit.codeBlock()-&gt;exitCountThresholdForReoptimizationFromLoop()),
100             GPRInfo::regT1);
101 
102         done.link(&amp;jit);
103     }
104 
105     tooFewFails = jit.branch32(AssemblyHelpers::BelowOrEqual, GPRInfo::regT2, GPRInfo::regT1);
106 
107     reoptimizeNow.link(&amp;jit);
108 
109     jit.setupArguments&lt;decltype(operationTriggerReoptimizationNow)&gt;(GPRInfo::regT0, GPRInfo::regT3, AssemblyHelpers::TrustedImmPtr(&amp;exit));
110     jit.prepareCallOperation(vm);
111     jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationTriggerReoptimizationNow)), GPRInfo::nonArgGPR0);
112     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
113     AssemblyHelpers::Jump doneAdjusting = jit.jump();
114 
115     tooFewFails.link(&amp;jit);
116 
117     // Adjust the execution counter such that the target is to only optimize after a while.
118     int32_t activeThreshold =
119         jit.baselineCodeBlock()-&gt;adjustedCounterValue(
120             Options::thresholdForOptimizeAfterLongWarmUp());
121     int32_t targetValue = applyMemoryUsageHeuristicsAndConvertToInt(
122         activeThreshold, jit.baselineCodeBlock());
123     int32_t clippedValue;
124     switch (jit.codeBlock()-&gt;jitType()) {
125     case JITType::DFGJIT:
126         clippedValue = BaselineExecutionCounter::clippedThreshold(jit.codeBlock()-&gt;globalObject(), targetValue);
127         break;
128     case JITType::FTLJIT:
129         clippedValue = UpperTierExecutionCounter::clippedThreshold(jit.codeBlock()-&gt;globalObject(), targetValue);
130         break;
131     default:
132         RELEASE_ASSERT_NOT_REACHED();
133 #if COMPILER_QUIRK(CONSIDERS_UNREACHABLE_CODE)
134         clippedValue = 0; // Make some compilers, and mhahnenberg, happy.
135 #endif
136         break;
137     }
138     jit.store32(AssemblyHelpers::TrustedImm32(-clippedValue), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecuteCounter()));
139     jit.store32(AssemblyHelpers::TrustedImm32(activeThreshold), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionActiveThreshold()));
140     jit.store32(AssemblyHelpers::TrustedImm32(formattedTotalExecutionCount(clippedValue)), AssemblyHelpers::Address(GPRInfo::regT0, CodeBlock::offsetOfJITExecutionTotalCount()));
141 
142     doneAdjusting.link(&amp;jit);
143 }
144 
145 MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; callerReturnPC(CodeBlock* baselineCodeBlockForCaller, BytecodeIndex callBytecodeIndex, InlineCallFrame::Kind trueCallerCallKind, bool&amp; callerIsLLInt)
146 {
147     callerIsLLInt = Options::forceOSRExitToLLInt() || baselineCodeBlockForCaller-&gt;jitType() == JITType::InterpreterThunk;
148 
149     if (callBytecodeIndex.checkpoint()) {
150         if (!callerIsLLInt)
151             baselineCodeBlockForCaller-&gt;m_hasLinkedOSRExit = true;
152         return LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_from_inlined_call_trampoline);
153     }
154 
155     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jumpTarget;
156 
157     if (callerIsLLInt) {
158         const Instruction&amp; callInstruction = *baselineCodeBlockForCaller-&gt;instructions().at(callBytecodeIndex).ptr();
159 #define LLINT_RETURN_LOCATION(name) (callInstruction.isWide16() ? LLInt::getWide16CodePtr&lt;JSEntryPtrTag&gt;(name##_return_location) : (callInstruction.isWide32() ? LLInt::getWide32CodePtr&lt;JSEntryPtrTag&gt;(name##_return_location) : LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(name##_return_location)))
160 
161         switch (trueCallerCallKind) {
162         case InlineCallFrame::Call:
163             jumpTarget = LLINT_RETURN_LOCATION(op_call);
164             break;
165         case InlineCallFrame::Construct:
166             jumpTarget = LLINT_RETURN_LOCATION(op_construct);
167             break;
168         case InlineCallFrame::CallVarargs:
169             jumpTarget = LLINT_RETURN_LOCATION(op_call_varargs_slow);
170             break;
171         case InlineCallFrame::ConstructVarargs:
172             jumpTarget = LLINT_RETURN_LOCATION(op_construct_varargs_slow);
173             break;
174         case InlineCallFrame::GetterCall: {
175             if (callInstruction.opcodeID() == op_get_by_id)
176                 jumpTarget = LLINT_RETURN_LOCATION(op_get_by_id);
177             else if (callInstruction.opcodeID() == op_get_by_val)
178                 jumpTarget = LLINT_RETURN_LOCATION(op_get_by_val);
179             else
180                 RELEASE_ASSERT_NOT_REACHED();
181             break;
182         }
183         case InlineCallFrame::SetterCall: {
184             if (callInstruction.opcodeID() == op_put_by_id)
185                 jumpTarget = LLINT_RETURN_LOCATION(op_put_by_id);
186             else if (callInstruction.opcodeID() == op_put_by_val)
187                 jumpTarget = LLINT_RETURN_LOCATION(op_put_by_val);
188             else
189                 RELEASE_ASSERT_NOT_REACHED();
190             break;
191         }
192         default:
193             RELEASE_ASSERT_NOT_REACHED();
194         }
195 
196 #undef LLINT_RETURN_LOCATION
197 
198     } else {
199         baselineCodeBlockForCaller-&gt;m_hasLinkedOSRExit = true;
200 
201         switch (trueCallerCallKind) {
202         case InlineCallFrame::Call:
203         case InlineCallFrame::Construct:
204         case InlineCallFrame::CallVarargs:
205         case InlineCallFrame::ConstructVarargs: {
206             CallLinkInfo* callLinkInfo =
207                 baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);
208             RELEASE_ASSERT(callLinkInfo);
209 
210             jumpTarget = callLinkInfo-&gt;callReturnLocation().retagged&lt;JSEntryPtrTag&gt;();
211             break;
212         }
213 
214         case InlineCallFrame::GetterCall:
215         case InlineCallFrame::SetterCall: {
216             StructureStubInfo* stubInfo =
217                 baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));
218             RELEASE_ASSERT(stubInfo);
219 
220             jumpTarget = stubInfo-&gt;doneLocation.retagged&lt;JSEntryPtrTag&gt;();
221             break;
222         }
223 
224         default:
225             RELEASE_ASSERT_NOT_REACHED();
226         }
227     }
228 
229     return jumpTarget;
230 }
231 
232 CCallHelpers::Address calleeSaveSlot(InlineCallFrame* inlineCallFrame, CodeBlock* baselineCodeBlock, GPRReg calleeSave)
233 {
234     const RegisterAtOffsetList* calleeSaves = baselineCodeBlock-&gt;calleeSaveRegisters();
235     for (unsigned i = 0; i &lt; calleeSaves-&gt;size(); i++) {
236         RegisterAtOffset entry = calleeSaves-&gt;at(i);
237         if (entry.reg() != calleeSave)
238             continue;
239         return CCallHelpers::Address(CCallHelpers::framePointerRegister, static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset).offsetInBytes() + entry.offset());
240     }
241 
242     RELEASE_ASSERT_NOT_REACHED();
243     return CCallHelpers::Address(CCallHelpers::framePointerRegister);
244 }
245 
246 void reifyInlinedCallFrames(CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
247 {
248     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
249     // in presence of inlined tail calls.
250     // https://bugs.webkit.org/show_bug.cgi?id=147511
251     ASSERT(JITCode::isBaselineCode(jit.baselineCodeBlock()-&gt;jitType()));
252     jit.storePtr(AssemblyHelpers::TrustedImmPtr(jit.baselineCodeBlock()), AssemblyHelpers::addressFor(CallFrameSlot::codeBlock));
253 
254     const CodeOrigin* codeOrigin;
255     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {
256         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();
257         CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(*codeOrigin);
258         InlineCallFrame::Kind trueCallerCallKind;
259         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
260         GPRReg callerFrameGPR = GPRInfo::callFrameRegister;
261 
262         bool callerIsLLInt = false;
263 
264         if (!trueCaller) {
265             ASSERT(inlineCallFrame-&gt;isTail());
266             jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
267 #if CPU(ARM64E)
268             jit.addPtr(AssemblyHelpers::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, GPRInfo::regT2);
269             jit.untagPtr(GPRInfo::regT2, GPRInfo::regT3);
270             jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
271             jit.tagPtr(GPRInfo::regT2, GPRInfo::regT3);
272 #endif
273             jit.storePtr(GPRInfo::regT3, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
274             jit.loadPtr(AssemblyHelpers::Address(GPRInfo::callFrameRegister, CallFrame::callerFrameOffset()), GPRInfo::regT3);
275             callerFrameGPR = GPRInfo::regT3;
276         } else {
277             CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);
278             auto callBytecodeIndex = trueCaller-&gt;bytecodeIndex();
279             MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jumpTarget = callerReturnPC(baselineCodeBlockForCaller, callBytecodeIndex, trueCallerCallKind, callerIsLLInt);
280 
281             if (trueCaller-&gt;inlineCallFrame()) {
282                 jit.addPtr(
283                     AssemblyHelpers::TrustedImm32(trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue)),
284                     GPRInfo::callFrameRegister,
285                     GPRInfo::regT3);
286                 callerFrameGPR = GPRInfo::regT3;
287             }
288 
289 #if CPU(ARM64E)
290             jit.addPtr(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;returnPCOffset() + sizeof(void*)), GPRInfo::callFrameRegister, GPRInfo::regT2);
291             jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget.untaggedExecutableAddress()), GPRInfo::nonArgGPR0);
292             jit.tagPtr(GPRInfo::regT2, GPRInfo::nonArgGPR0);
293             jit.storePtr(GPRInfo::nonArgGPR0, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
294 #else
295             jit.storePtr(AssemblyHelpers::TrustedImmPtr(jumpTarget.untaggedExecutableAddress()), AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;returnPCOffset()));
296 #endif
297         }
298 
299         jit.storePtr(AssemblyHelpers::TrustedImmPtr(baselineCodeBlock), AssemblyHelpers::addressFor((VirtualRegister)(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock)));
300 
301         // Restore the inline call frame&#39;s callee save registers.
302         // If this inlined frame is a tail call that will return back to the original caller, we need to
303         // copy the prior contents of the tag registers already saved for the outer frame to this frame.
304         jit.emitSaveOrCopyCalleeSavesFor(
305             baselineCodeBlock,
306             static_cast&lt;VirtualRegister&gt;(inlineCallFrame-&gt;stackOffset),
307             trueCaller ? AssemblyHelpers::UseExistingTagRegisterContents : AssemblyHelpers::CopyBaselineCalleeSavedRegistersFromBaseFrame,
308             GPRInfo::regT2);
309 
310         if (callerIsLLInt) {
311             CodeBlock* baselineCodeBlockForCaller = jit.baselineCodeBlockFor(*trueCaller);
312             jit.storePtr(CCallHelpers::TrustedImmPtr(baselineCodeBlockForCaller-&gt;metadataTable()), calleeSaveSlot(inlineCallFrame, baselineCodeBlock, LLInt::Registers::metadataTableGPR));
313             jit.storePtr(CCallHelpers::TrustedImmPtr(baselineCodeBlockForCaller-&gt;instructionsRawPointer()), calleeSaveSlot(inlineCallFrame, baselineCodeBlock, LLInt::Registers::pbGPR));
314         }
315 
316         if (!inlineCallFrame-&gt;isVarargs())
317             jit.store32(AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis), AssemblyHelpers::payloadFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis)));
318         jit.storePtr(callerFrameGPR, AssemblyHelpers::addressForByteOffset(inlineCallFrame-&gt;callerFrameOffset()));
319         uint32_t locationBits = CallSiteIndex(baselineCodeBlock-&gt;bytecodeIndexForExit(codeOrigin-&gt;bytecodeIndex())).bits();
320         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCountIncludingThis)));
321 #if USE(JSVALUE64)
322         if (!inlineCallFrame-&gt;isClosureCall)
323             jit.store64(AssemblyHelpers::TrustedImm64(JSValue::encode(JSValue(inlineCallFrame-&gt;calleeConstant()))), AssemblyHelpers::addressFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));
324 #else // USE(JSVALUE64) // so this is the 32-bit part
325         jit.store32(AssemblyHelpers::TrustedImm32(JSValue::CellTag), AssemblyHelpers::tagFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));
326         if (!inlineCallFrame-&gt;isClosureCall)
327             jit.storePtr(AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeConstant()), AssemblyHelpers::payloadFor(VirtualRegister(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee)));
328 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
329     }
330 
331     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
332     if (codeOrigin) {
333         uint32_t locationBits = CallSiteIndex(BytecodeIndex(codeOrigin-&gt;bytecodeIndex().offset())).bits();
334         jit.store32(AssemblyHelpers::TrustedImm32(locationBits), AssemblyHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));
335     }
336 }
337 
338 static void osrWriteBarrier(VM&amp; vm, CCallHelpers&amp; jit, GPRReg owner, GPRReg scratch)
339 {
340     AssemblyHelpers::Jump ownerIsRememberedOrInEden = jit.barrierBranchWithoutFence(owner);
341 
342     jit.setupArguments&lt;decltype(operationOSRWriteBarrier)&gt;(&amp;vm, owner);
343     jit.prepareCallOperation(vm);
344     jit.move(MacroAssembler::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationOSRWriteBarrier)), scratch);
345     jit.call(scratch, OperationPtrTag);
346 
347     ownerIsRememberedOrInEden.link(&amp;jit);
348 }
349 
350 void adjustAndJumpToTarget(VM&amp; vm, CCallHelpers&amp; jit, const OSRExitBase&amp; exit)
351 {
352     jit.memoryFence();
353 
354     jit.move(
355         AssemblyHelpers::TrustedImmPtr(
356             jit.codeBlock()-&gt;baselineAlternative()), GPRInfo::argumentGPR1);
357     osrWriteBarrier(vm, jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);
358 
359     // We barrier all inlined frames -- and not just the current inline stack --
360     // because we don&#39;t know which inlined function owns the value profile that
361     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
362     // a and b are inlined, we might exit inside b due to a bad value loaded
363     // from a.
364     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns
365     // the value profile.
366     InlineCallFrameSet* inlineCallFrames = jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();
367     if (inlineCallFrames) {
368         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames) {
369             jit.move(
370                 AssemblyHelpers::TrustedImmPtr(
371                     inlineCallFrame-&gt;baselineCodeBlock.get()), GPRInfo::argumentGPR1);
372             osrWriteBarrier(vm, jit, GPRInfo::argumentGPR1, GPRInfo::nonArgGPR0);
373         }
374     }
375 
376     auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();
377     if (exitInlineCallFrame)
378         jit.addPtr(AssemblyHelpers::TrustedImm32(exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue)), GPRInfo::callFrameRegister);
379 
380     CodeBlock* codeBlockForExit = jit.baselineCodeBlockFor(exit.m_codeOrigin);
381     ASSERT(codeBlockForExit == codeBlockForExit-&gt;baselineVersion());
382     ASSERT(JITCode::isBaselineCode(codeBlockForExit-&gt;jitType()));
383 
384     void* jumpTarget;
385     bool exitToLLInt = Options::forceOSRExitToLLInt() || codeBlockForExit-&gt;jitType() == JITType::InterpreterThunk;
386     if (exitToLLInt) {
387         auto bytecodeIndex = exit.m_codeOrigin.bytecodeIndex();
388         const Instruction&amp; currentInstruction = *codeBlockForExit-&gt;instructions().at(bytecodeIndex).ptr();
389         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; destination;
390         if (bytecodeIndex.checkpoint())
391             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_trampoline);
392         else
393             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(currentInstruction);
394 
395         if (exit.isExceptionHandler()) {
396             jit.move(CCallHelpers::TrustedImmPtr(&amp;currentInstruction), GPRInfo::regT2);
397             jit.storePtr(GPRInfo::regT2, &amp;vm.targetInterpreterPCForThrow);
398         }
399 
400         jit.move(CCallHelpers::TrustedImmPtr(codeBlockForExit-&gt;metadataTable()), LLInt::Registers::metadataTableGPR);
401         jit.move(CCallHelpers::TrustedImmPtr(codeBlockForExit-&gt;instructionsRawPointer()), LLInt::Registers::pbGPR);
402         jit.move(CCallHelpers::TrustedImm32(bytecodeIndex.offset()), LLInt::Registers::pcGPR);
403         jumpTarget = destination.retagged&lt;OSRExitPtrTag&gt;().executableAddress();
404     } else {
405         codeBlockForExit-&gt;m_hasLinkedOSRExit = true;
406 
407         BytecodeIndex exitIndex = exit.m_codeOrigin.bytecodeIndex();
408         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; destination;
409         if (exitIndex.checkpoint())
410             destination = LLInt::getCodePtr&lt;JSEntryPtrTag&gt;(checkpoint_osr_exit_trampoline);
411         else {
412             ASSERT(codeBlockForExit-&gt;bytecodeIndexForExit(exitIndex) == exitIndex);
413             destination = codeBlockForExit-&gt;jitCodeMap().find(exitIndex);
414         }
415 
416         ASSERT(destination);
417 
418         jumpTarget = destination.retagged&lt;OSRExitPtrTag&gt;().executableAddress();
419     }
420 
421     jit.addPtr(AssemblyHelpers::TrustedImm32(JIT::stackPointerOffsetFor(codeBlockForExit) * sizeof(Register)), GPRInfo::callFrameRegister, AssemblyHelpers::stackPointerRegister);
422     if (exit.isExceptionHandler()) {
423         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
424         jit.storePtr(GPRInfo::callFrameRegister, vm.addressOfCallFrameForCatch());
425     }
426 
427     jit.move(AssemblyHelpers::TrustedImmPtr(jumpTarget), GPRInfo::regT2);
428     jit.farJump(GPRInfo::regT2, OSRExitPtrTag);
429 }
430 
431 } } // namespace JSC::DFG
432 
433 #endif // ENABLE(DFG_JIT)
434 
    </pre>
  </body>
</html>