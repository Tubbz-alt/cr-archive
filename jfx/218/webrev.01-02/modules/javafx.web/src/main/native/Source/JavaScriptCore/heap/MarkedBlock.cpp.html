<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
 14  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 15  * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
 17  * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 23  * THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;MarkedBlock.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;BlockDirectoryInlines.h&quot;
 31 #include &quot;FreeListInlines.h&quot;
 32 #include &quot;JSCast.h&quot;
 33 #include &quot;JSDestructibleObject.h&quot;
 34 #include &quot;JSCInlines.h&quot;
 35 #include &quot;MarkedBlockInlines.h&quot;
 36 #include &quot;SuperSampler.h&quot;
 37 #include &quot;SweepingScope.h&quot;
 38 #include &lt;wtf/CommaPrinter.h&gt;
 39 
 40 namespace JSC {
 41 namespace MarkedBlockInternal {
 42 static constexpr bool verbose = false;
 43 }
 44 
 45 static constexpr bool computeBalance = false;
 46 static size_t balance;
 47 
 48 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MarkedBlock);
 49 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MarkedBlockHandle);
 50 
 51 MarkedBlock::Handle* MarkedBlock::tryCreate(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator)
 52 {
 53     if (computeBalance) {
 54         balance++;
 55         if (!(balance % 10))
 56             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 57     }
 58     void* blockSpace = alignedMemoryAllocator-&gt;tryAllocateAlignedMemory(blockSize, blockSize);
 59     if (!blockSpace)
 60         return nullptr;
 61     if (scribbleFreeCells())
 62         scribble(blockSpace, blockSize);
 63     return new Handle(heap, alignedMemoryAllocator, blockSpace);
 64 }
 65 
 66 MarkedBlock::Handle::Handle(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator, void* blockSpace)
 67     : m_alignedMemoryAllocator(alignedMemoryAllocator)
 68     , m_weakSet(heap.vm())
 69 {
 70     m_block = new (NotNull, blockSpace) MarkedBlock(heap.vm(), *this);
 71 
 72     heap.didAllocateBlock(blockSize);
 73 }
 74 
 75 MarkedBlock::Handle::~Handle()
 76 {
 77     Heap&amp; heap = *this-&gt;heap();
 78     if (computeBalance) {
 79         balance--;
 80         if (!(balance % 10))
 81             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 82     }
 83     removeFromDirectory();
 84     m_block-&gt;~MarkedBlock();
 85     m_alignedMemoryAllocator-&gt;freeAlignedMemory(m_block);
 86     heap.didFreeBlock(blockSize);
 87 }
 88 
 89 MarkedBlock::MarkedBlock(VM&amp; vm, Handle&amp; handle)
 90 {
 91     new (&amp;footer()) Footer(vm, handle);
 92     if (MarkedBlockInternal::verbose)
 93         dataLog(RawPointer(this), &quot;: Allocated.\n&quot;);
 94 }
 95 
 96 MarkedBlock::~MarkedBlock()
 97 {
 98     footer().~Footer();
 99 }
100 
101 MarkedBlock::Footer::Footer(VM&amp; vm, Handle&amp; handle)
102     : m_handle(handle)
103     , m_vm(&amp;vm)
104     , m_markingVersion(MarkedSpace::nullVersion)
105     , m_newlyAllocatedVersion(MarkedSpace::nullVersion)
106 {
107 }
108 
109 MarkedBlock::Footer::~Footer()
110 {
111 }
112 
113 void MarkedBlock::Handle::unsweepWithNoNewlyAllocated()
114 {
115     RELEASE_ASSERT(m_isFreeListed);
116     m_isFreeListed = false;
117 }
118 
119 void MarkedBlock::Handle::stopAllocating(const FreeList&amp; freeList)
120 {
121     auto locker = holdLock(blockFooter().m_lock);
122 
123     if (MarkedBlockInternal::verbose)
124         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::stopAllocating!\n&quot;);
125     ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
126 
127     if (!isFreeListed()) {
128         if (MarkedBlockInternal::verbose)
129             dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
130         // This means that we either didn&#39;t use this block at all for allocation since last GC,
131         // or someone had already done stopAllocating() before.
132         ASSERT(freeList.allocationWillFail());
133         return;
134     }
135 
136     if (MarkedBlockInternal::verbose)
137         dataLog(&quot;Free list: &quot;, freeList, &quot;\n&quot;);
138 
139     // Roll back to a coherent state for Heap introspection. Cells newly
140     // allocated from our free list are not currently marked, so we need another
141     // way to tell what&#39;s live vs dead.
142 
143     blockFooter().m_newlyAllocated.clearAll();
144     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
145 
146     forEachCell(
147         [&amp;] (size_t, HeapCell* cell, HeapCell::Kind) -&gt; IterationStatus {
148             block().setNewlyAllocated(cell);
149             return IterationStatus::Continue;
150         });
151 
152     freeList.forEach(
153         [&amp;] (HeapCell* cell) {
154             if (MarkedBlockInternal::verbose)
155                 dataLog(&quot;Free cell: &quot;, RawPointer(cell), &quot;\n&quot;);
156             if (m_attributes.destruction == NeedsDestruction)
157                 cell-&gt;zap(HeapCell::StopAllocating);
158             block().clearNewlyAllocated(cell);
159         });
160 
161     m_isFreeListed = false;
162 }
163 
164 void MarkedBlock::Handle::lastChanceToFinalize()
165 {
166     directory()-&gt;setIsAllocated(NoLockingNecessary, this, false);
167     directory()-&gt;setIsDestructible(NoLockingNecessary, this, true);
168     blockFooter().m_marks.clearAll();
169     block().clearHasAnyMarked();
170     blockFooter().m_markingVersion = heap()-&gt;objectSpace().markingVersion();
171     m_weakSet.lastChanceToFinalize();
172     blockFooter().m_newlyAllocated.clearAll();
173     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
174     sweep(nullptr);
175 }
176 
177 void MarkedBlock::Handle::resumeAllocating(FreeList&amp; freeList)
178 {
179     {
180         auto locker = holdLock(blockFooter().m_lock);
181 
182         if (MarkedBlockInternal::verbose)
183             dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::resumeAllocating!\n&quot;);
184         ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
185         ASSERT(!isFreeListed());
186 
187         if (!block().hasAnyNewlyAllocated()) {
188             if (MarkedBlockInternal::verbose)
189                 dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
190             // This means we had already exhausted the block when we stopped allocation.
191             freeList.clear();
192             return;
193         }
194     }
195 
196     // Re-create our free list from before stopping allocation. Note that this may return an empty
197     // freelist, in which case the block will still be Marked!
198     sweep(&amp;freeList);
199 }
200 
201 void MarkedBlock::aboutToMarkSlow(HeapVersion markingVersion)
202 {
203     ASSERT(vm().heap.objectSpace().isMarking());
204     auto locker = holdLock(footer().m_lock);
205 
206     if (!areMarksStale(markingVersion))
207         return;
208 
209     BlockDirectory* directory = handle().directory();
210 
211     if (handle().directory()-&gt;isAllocated(holdLock(directory-&gt;bitvectorLock()), &amp;handle())
212         || !marksConveyLivenessDuringMarking(markingVersion)) {
213         if (MarkedBlockInternal::verbose)
214             dataLog(RawPointer(this), &quot;: Clearing marks without doing anything else.\n&quot;);
215         // We already know that the block is full and is already recognized as such, or that the
216         // block did not survive the previous GC. So, we can clear mark bits the old fashioned
217         // way. Note that it&#39;s possible for such a block to have newlyAllocated with an up-to-
218         // date version! If it does, then we want to leave the newlyAllocated alone, since that
219         // means that we had allocated in this previously empty block but did not fill it up, so
220         // we created a newlyAllocated.
221         footer().m_marks.clearAll();
222     } else {
223         if (MarkedBlockInternal::verbose)
224             dataLog(RawPointer(this), &quot;: Doing things.\n&quot;);
225         HeapVersion newlyAllocatedVersion = space()-&gt;newlyAllocatedVersion();
226         if (footer().m_newlyAllocatedVersion == newlyAllocatedVersion) {
227             // When do we get here? The block could not have been filled up. The newlyAllocated bits would
228             // have had to be created since the end of the last collection. The only things that create
229             // them are aboutToMarkSlow, lastChanceToFinalize, and stopAllocating. If it had been
230             // aboutToMarkSlow, then we shouldn&#39;t be here since the marks wouldn&#39;t be stale anymore. It
231             // cannot be lastChanceToFinalize. So it must be stopAllocating. That means that we just
232             // computed the newlyAllocated bits just before the start of an increment. When we are in that
233             // mode, it seems as if newlyAllocated should subsume marks.
234             ASSERT(footer().m_newlyAllocated.subsumes(footer().m_marks));
235             footer().m_marks.clearAll();
236         } else {
237             footer().m_newlyAllocated.setAndClear(footer().m_marks);
238             footer().m_newlyAllocatedVersion = newlyAllocatedVersion;
239         }
240     }
241     clearHasAnyMarked();
242     WTF::storeStoreFence();
243     footer().m_markingVersion = markingVersion;
244 
245     // This means we&#39;re the first ones to mark any object in this block.
246     directory-&gt;setIsMarkingNotEmpty(holdLock(directory-&gt;bitvectorLock()), &amp;handle(), true);
247 }
248 
249 void MarkedBlock::resetAllocated()
250 {
251     footer().m_newlyAllocated.clearAll();
252     footer().m_newlyAllocatedVersion = MarkedSpace::nullVersion;
253 }
254 
255 void MarkedBlock::resetMarks()
256 {
257     // We want aboutToMarkSlow() to see what the mark bits were after the last collection. It uses
258     // the version number to distinguish between the marks having already been stale before
259     // beginMarking(), or just stale now that beginMarking() bumped the version. If we have a version
260     // wraparound, then we will call this method before resetting the version to null. When the
261     // version is null, aboutToMarkSlow() will assume that the marks were not stale as of before
262     // beginMarking(). Hence the need to whip the marks into shape.
263     if (areMarksStale())
264         footer().m_marks.clearAll();
265     footer().m_markingVersion = MarkedSpace::nullVersion;
266 }
267 
268 #if ASSERT_ENABLED
269 void MarkedBlock::assertMarksNotStale()
270 {
271     ASSERT(footer().m_markingVersion == vm().heap.objectSpace().markingVersion());
272 }
273 #endif // ASSERT_ENABLED
274 
275 bool MarkedBlock::areMarksStale()
276 {
277     return areMarksStale(vm().heap.objectSpace().markingVersion());
278 }
279 
280 bool MarkedBlock::Handle::areMarksStale()
281 {
282     return m_block-&gt;areMarksStale();
283 }
284 
285 bool MarkedBlock::isMarked(const void* p)
286 {
287     return isMarked(vm().heap.objectSpace().markingVersion(), p);
288 }
289 
290 void MarkedBlock::Handle::didConsumeFreeList()
291 {
292     auto locker = holdLock(blockFooter().m_lock);
293     if (MarkedBlockInternal::verbose)
294         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::didConsumeFreeList!\n&quot;);
295     ASSERT(isFreeListed());
296     m_isFreeListed = false;
297     directory()-&gt;setIsAllocated(NoLockingNecessary, this, true);
298 }
299 
300 size_t MarkedBlock::markCount()
301 {
302     return areMarksStale() ? 0 : footer().m_marks.count();
303 }
304 
305 void MarkedBlock::clearHasAnyMarked()
306 {
307     footer().m_biasedMarkCount = footer().m_markCountBias;
308 }
309 
310 void MarkedBlock::noteMarkedSlow()
311 {
312     BlockDirectory* directory = handle().directory();
313     directory-&gt;setIsMarkingRetired(holdLock(directory-&gt;bitvectorLock()), &amp;handle(), true);
314 }
315 
316 void MarkedBlock::Handle::removeFromDirectory()
317 {
318     if (!m_directory)
319         return;
320 
321     m_directory-&gt;removeBlock(this);
322 }
323 
324 void MarkedBlock::Handle::didAddToDirectory(BlockDirectory* directory, unsigned index)
325 {
326     ASSERT(m_index == std::numeric_limits&lt;unsigned&gt;::max());
327     ASSERT(!m_directory);
328 
329     RELEASE_ASSERT(directory-&gt;subspace()-&gt;alignedMemoryAllocator() == m_alignedMemoryAllocator);
330 
331     m_index = index;
332     m_directory = directory;
333     blockFooter().m_subspace = directory-&gt;subspace();
334 
335     size_t cellSize = directory-&gt;cellSize();
336     m_atomsPerCell = (cellSize + atomSize - 1) / atomSize;
337     m_endAtom = endAtom - m_atomsPerCell + 1;
338 
339     m_attributes = directory-&gt;attributes();
340 
341     if (!isJSCellKind(m_attributes.cellKind))
342         RELEASE_ASSERT(m_attributes.destruction == DoesNotNeedDestruction);
343 
344     double markCountBias = -(Options::minMarkedBlockUtilization() * cellsPerBlock());
345 
346     // The mark count bias should be comfortably within this range.
347     RELEASE_ASSERT(markCountBias &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int16_t&gt;::min()));
348     RELEASE_ASSERT(markCountBias &lt; 0);
349 
350     // This means we haven&#39;t marked anything yet.
351     blockFooter().m_biasedMarkCount = blockFooter().m_markCountBias = static_cast&lt;int16_t&gt;(markCountBias);
352 }
353 
354 void MarkedBlock::Handle::didRemoveFromDirectory()
355 {
356     ASSERT(m_index != std::numeric_limits&lt;unsigned&gt;::max());
357     ASSERT(m_directory);
358 
359     m_index = std::numeric_limits&lt;unsigned&gt;::max();
360     m_directory = nullptr;
361     blockFooter().m_subspace = nullptr;
362 }
363 
364 #if ASSERT_ENABLED
365 void MarkedBlock::assertValidCell(VM&amp; vm, HeapCell* cell) const
366 {
367     RELEASE_ASSERT(&amp;vm == &amp;this-&gt;vm());
368     RELEASE_ASSERT(const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle().cellAlign(cell) == cell);
369 }
370 #endif // ASSERT_ENABLED
371 
372 void MarkedBlock::Handle::dumpState(PrintStream&amp; out)
373 {
374     CommaPrinter comma;
375     directory()-&gt;forEachBitVectorWithName(
376         holdLock(directory()-&gt;bitvectorLock()),
377         [&amp;](auto vectorRef, const char* name) {
378             out.print(comma, name, &quot;:&quot;, vectorRef[index()] ? &quot;YES&quot; : &quot;no&quot;);
379         });
380 }
381 
382 Subspace* MarkedBlock::Handle::subspace() const
383 {
384     return directory()-&gt;subspace();
385 }
386 
387 void MarkedBlock::Handle::sweep(FreeList* freeList)
388 {
389     SweepingScope sweepingScope(*heap());
390 
391     SweepMode sweepMode = freeList ? SweepToFreeList : SweepOnly;
392 
393     m_directory-&gt;setIsUnswept(NoLockingNecessary, this, false);
394 
395     m_weakSet.sweep();
396 
397     bool needsDestruction = m_attributes.destruction == NeedsDestruction
398         &amp;&amp; m_directory-&gt;isDestructible(NoLockingNecessary, this);
399 
400     if (sweepMode == SweepOnly &amp;&amp; !needsDestruction)
401         return;
402 
403     if (m_isFreeListed) {
404         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;sweep: block is free-listed.\n&quot;);
405         RELEASE_ASSERT_NOT_REACHED();
406     }
407 
408     if (isAllocated()) {
409         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;sweep: block is allocated.\n&quot;);
410         RELEASE_ASSERT_NOT_REACHED();
411     }
412 
413     if (space()-&gt;isMarking())
414         blockFooter().m_lock.lock();
415 
416     subspace()-&gt;didBeginSweepingToFreeList(this);
417 
418     if (needsDestruction) {
419         subspace()-&gt;finishSweep(*this, freeList);
420         return;
421     }
422 
423     // Handle the no-destructor specializations here, since we have the most of those. This
424     // ensures that they don&#39;t get re-specialized for every destructor space.
425 
426     EmptyMode emptyMode = this-&gt;emptyMode();
427     ScribbleMode scribbleMode = this-&gt;scribbleMode();
428     NewlyAllocatedMode newlyAllocatedMode = this-&gt;newlyAllocatedMode();
429     MarksMode marksMode = this-&gt;marksMode();
430 
431     auto trySpecialized = [&amp;] () -&gt; bool {
432         if (sweepMode != SweepToFreeList)
433             return false;
434         if (scribbleMode != DontScribble)
435             return false;
436         if (newlyAllocatedMode != DoesNotHaveNewlyAllocated)
437             return false;
438 
439         switch (emptyMode) {
440         case IsEmpty:
441             switch (marksMode) {
442             case MarksNotStale:
443                 specializedSweep&lt;true, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale, [] (VM&amp;, JSCell*) { });
444                 return true;
445             case MarksStale:
446                 specializedSweep&lt;true, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale, [] (VM&amp;, JSCell*) { });
447                 return true;
448             }
449             break;
450         case NotEmpty:
451             switch (marksMode) {
452             case MarksNotStale:
453                 specializedSweep&lt;true, NotEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale, [] (VM&amp;, JSCell*) { });
454                 return true;
455             case MarksStale:
456                 specializedSweep&lt;true, NotEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale, [] (VM&amp;, JSCell*) { });
457                 return true;
458             }
459             break;
460         }
461 
462         return false;
463     };
464 
465     if (trySpecialized())
466         return;
467 
468     // The template arguments don&#39;t matter because the first one is false.
469     specializedSweep&lt;false, IsEmpty, SweepOnly, BlockHasNoDestructors, DontScribble, HasNewlyAllocated, MarksStale&gt;(freeList, emptyMode, sweepMode, BlockHasNoDestructors, scribbleMode, newlyAllocatedMode, marksMode, [] (VM&amp;, JSCell*) { });
470 }
471 
472 bool MarkedBlock::Handle::isFreeListedCell(const void* target) const
473 {
474     ASSERT(isFreeListed());
475     return m_directory-&gt;isFreeListedCell(target);
476 }
477 
478 } // namespace JSC
479 
480 namespace WTF {
481 
482 void printInternal(PrintStream&amp; out, JSC::MarkedBlock::Handle::SweepMode mode)
483 {
484     switch (mode) {
485     case JSC::MarkedBlock::Handle::SweepToFreeList:
486         out.print(&quot;SweepToFreeList&quot;);
487         return;
488     case JSC::MarkedBlock::Handle::SweepOnly:
489         out.print(&quot;SweepOnly&quot;);
490         return;
491     }
492     RELEASE_ASSERT_NOT_REACHED();
493 }
494 
495 } // namespace WTF
496 
    </pre>
  </body>
</html>