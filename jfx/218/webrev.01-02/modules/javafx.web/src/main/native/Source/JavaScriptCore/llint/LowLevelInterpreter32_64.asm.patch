diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm
@@ -21,50 +21,35 @@
 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 # THE POSSIBILITY OF SUCH DAMAGE.
 
 
 # Utilities
-macro nextInstruction()
-    loadb [PC], t0
-    leap _g_opcodeMap, t1
-    jmp [t1, t0, 4], BytecodePtrTag
-end
-
-macro nextInstructionWide16()
-    loadh 1[PC], t0
-    leap _g_opcodeMapWide16, t1
-    jmp [t1, t0, 4], BytecodePtrTag
-end
-
-macro nextInstructionWide32()
-    loadi 1[PC], t0
-    leap _g_opcodeMapWide32, t1
-    jmp [t1, t0, 4], BytecodePtrTag
-end
+# FIXME:  Merge "getOperand" macros on 32 and 64 bits LLInt
+# https://bugs.webkit.org/show_bug.cgi?id=206342
 
 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
-    loadb constexpr %opcodeStruct%_%fieldName%_index[PC], dst
+    loadb constexpr %opcodeStruct%_%fieldName%_index + OpcodeIDNarrowSize[PB, PC, 1], dst
 end
 
 macro getOperandNarrow(opcodeStruct, fieldName, dst)
-    loadbsi constexpr %opcodeStruct%_%fieldName%_index[PC], dst
+    loadbsi constexpr %opcodeStruct%_%fieldName%_index + OpcodeIDNarrowSize[PB, PC, 1], dst
 end
 
 macro getuOperandWide16(opcodeStruct, fieldName, dst)
-    loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PC], dst
+    loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + OpcodeIDWide16Size[PB, PC, 1], dst
 end
 
 macro getOperandWide16(opcodeStruct, fieldName, dst)
-    loadhsi constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PC], dst
+    loadhsi constexpr %opcodeStruct%_%fieldName%_index * 2 + OpcodeIDWide16Size[PB, PC, 1], dst
 end
 
 macro getuOperandWide32(opcodeStruct, fieldName, dst)
-    loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PC], dst
+    loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + OpcodeIDWide32Size[PB, PC, 1], dst
 end
 
 macro getOperandWide32(opcodeStruct, fieldName, dst)
-    loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PC], dst
+    loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + OpcodeIDWide32Size[PB, PC, 1], dst
 end
 
 macro makeReturn(get, dispatch, fn)
     fn(macro(tag, payload)
         move tag, t5
@@ -89,12 +74,15 @@
         dispatch()
     end)
 end
 
 
+# After calling, calling bytecode is claiming input registers are not used.
 macro dispatchAfterCall(size, opcodeStruct, dispatch)
-    loadi ArgumentCount + TagOffset[cfr], PC
+    loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
+    loadp CodeBlock[cfr], PB
+    loadp CodeBlock::m_instructionsRawPointer[PB], PB
     get(size, opcodeStruct, m_dst, t3)
     storei r1, TagOffset[cfr, t3, 8]
     storei r0, PayloadOffset[cfr, t3, 8]
     metadata(size, opcodeStruct, t2, t3)
     valueProfile(opcodeStruct, t2, r1, r0)
@@ -140,15 +128,25 @@
     else
         error
     end
 end
 
+macro prepareStateForCCall()
+    addp PB, PC
+end
+
+macro restoreStateAfterCCall()
+    move r0, PC
+    subp PB, PC
+end
+
 macro callSlowPath(slowPath)
+    prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(slowPath)
-    move r0, PC
+    restoreStateAfterCCall()
 end
 
 macro doVMEntry(makeCall)
     functionPrologue()
     pushCalleeSaves()
@@ -260,11 +258,11 @@
 
 .copyArgsDone:
     storep sp, VM::topCallFrame[vm]
     storep cfr, VM::topEntryFrame[vm]
 
-    makeCall(entry, t3, t4)
+    makeCall(entry, protoCallFrame, t3, t4)
 
     if ARMv7
         vmEntryRecord(cfr, t3)
         move t3, sp
     else
@@ -317,48 +315,60 @@
     popCalleeSaves()
     functionEpilogue()
     ret
 end
 
-macro makeJavaScriptCall(entry, temp, unused)
+# a0, a2, t3, t4
+macro makeJavaScriptCall(entry, protoCallFrame, temp1, temp2)
     addp CallerFrameAndPCSize, sp
-    checkStackPointerAlignment(temp, 0xbad0dc02)
+    checkStackPointerAlignment(temp1, 0xbad0dc02)
     if C_LOOP or C_LOOP_WIN
         cloopCallJSFunction entry
     else
         call entry
     end
-    checkStackPointerAlignment(temp, 0xbad0dc03)
+    checkStackPointerAlignment(temp1, 0xbad0dc03)
     subp CallerFrameAndPCSize, sp
 end
 
-macro makeHostFunctionCall(entry, temp1, temp2)
+# a0, a2, t3, t4
+macro makeHostFunctionCall(entry, protoCallFrame, temp1, temp2)
     move entry, temp1
     storep cfr, [sp]
     if C_LOOP or C_LOOP_WIN
-        move sp, a0
+        loadp ProtoCallFrame::globalObject[protoCallFrame], a0
+        move sp, a1
         storep lr, PtrSize[sp]
         cloopCallNative temp1
     elsif X86 or X86_WIN
-        # Put callee frame pointer on stack as arg0, also put it in ecx for "fastcall" targets
+        # Put callee frame pointer on stack as arg1, also put it in ecx for "fastcall" targets
         move 0, temp2
         move temp2, 4[sp] # put 0 in ReturnPC
-        move sp, a0 # a0 is ecx
-        push temp2 # Push dummy arg1
+        move sp, a1 # a1 is edx
+        loadp ProtoCallFrame::globalObject[protoCallFrame], a0
+        push a1
         push a0
         call temp1
         addp 8, sp
+    elsif MIPS
+        move sp, a1
+        # We need to allocate stack space for 16 bytes (8-byte aligned)
+        # for 4 arguments, since callee can use this space.
+        subp 16, sp
+        loadp ProtoCallFrame::globalObject[protoCallFrame], a0
+        call temp1
+        addp 16, sp
     else
-        move sp, a0
+        loadp ProtoCallFrame::globalObject[protoCallFrame], a0
+        move sp, a1
         call temp1
     end
 end
 
 op(handleUncaughtException, macro()
     loadp Callee + PayloadOffset[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    convertCalleeToVM(t3)
     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
     storep 0, VM::callFrameForCatch[t3]
 
     loadp VM::topEntryFrame[t3], cfr
     if ARMv7
@@ -394,86 +404,99 @@
 # Debugging operation if you'd like to print an operand in the instruction stream. fromWhere
 # should be an immediate integer - any integer you like; use it to identify the place you're
 # debugging from. operand should likewise be an immediate, and should identify the operand
 # in the instruction stream you'd like to print out.
 macro traceOperand(fromWhere, operand)
+    prepareStateForCCall()
     move fromWhere, a2
     move operand, a3
     move cfr, a0
     move PC, a1
     cCall4(_llint_trace_operand)
-    move r0, PC
+    restoreStateAfterCCall()
     move r1, cfr
 end
 
 # Debugging operation if you'd like to print the value of an operand in the instruction
 # stream. Same as traceOperand(), but assumes that the operand is a register, and prints its
 # value.
 macro traceValue(fromWhere, operand)
+    prepareStateForCCall()
     move fromWhere, a2
     move operand, a3
     move cfr, a0
     move PC, a1
     cCall4(_llint_trace_value)
-    move r0, PC
+    restoreStateAfterCCall()
     move r1, cfr
 end
 
 # Call a slowPath for call opcodes.
 macro callCallSlowPath(slowPath, action)
-    storep PC, ArgumentCount + TagOffset[cfr]
+    storep PC, ArgumentCountIncludingThis + TagOffset[cfr]
+    prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(slowPath)
     action(r0, r1)
 end
 
 macro callTrapHandler(throwHandler)
-    storei PC, ArgumentCount + TagOffset[cfr]
+    storei PC, ArgumentCountIncludingThis + TagOffset[cfr]
+    prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(_llint_slow_path_handle_traps)
     btpnz r0, throwHandler
-    loadi ArgumentCount + TagOffset[cfr], PC
+    loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
 end
 
 macro checkSwitchToJITForLoop()
     checkSwitchToJIT(
         1,
         macro ()
-            storei PC, ArgumentCount + TagOffset[cfr]
+            storei PC, ArgumentCountIncludingThis + TagOffset[cfr]
+            prepareStateForCCall()
             move cfr, a0
             move PC, a1
             cCall2(_llint_loop_osr)
             btpz r0, .recover
             move r1, sp
             jmp r0
         .recover:
-            loadi ArgumentCount + TagOffset[cfr], PC
+            loadi ArgumentCountIncludingThis + TagOffset[cfr], PC
         end)
 end
 
 macro loadVariable(get, fieldName, indexReg, tagReg, payloadReg)
     get(fieldName, indexReg)
     loadi TagOffset[cfr, indexReg, 8], tagReg
     loadi PayloadOffset[cfr, indexReg, 8], payloadReg
 end
 
+# Index, tag, and payload must be different registers. Index is not
+# changed.
+macro loadConstant(size, index, tag, payload)
+    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
+        loadp CodeBlock[cfr], payload
+        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[payload], payload
+        subp FirstConstantRegisterIndex, index
+        loadp TagOffset[payload, index, 8], tag
+        loadp PayloadOffset[payload, index, 8], payload
+    end)
+end
+
 # Index, tag, and payload must be different registers. Index is not
 # changed.
 macro loadConstantOrVariable(size, index, tag, payload)
     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
         bigteq index, FirstConstantRegisterIndex, .constant
         loadi TagOffset[cfr, index, 8], tag
         loadi PayloadOffset[cfr, index, 8], payload
         jmp .done
     .constant:
-        loadp CodeBlock[cfr], payload
-        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[payload], payload
-        subp FirstConstantRegisterIndex, index
-        loadp TagOffset[payload, index, 8], tag
-        loadp PayloadOffset[payload, index, 8], payload
+        loadConstant(size, index, tag, payload)
     .done:
     end)
 end
 
 macro loadConstantOrVariableTag(size, index, tag)
@@ -548,18 +571,18 @@
 
 macro writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)
     skipIfIsRememberedOrInEden(
         cell,
         macro()
-            push cfr, PC
+            push PB, PC
             # We make two extra slots because cCall2 will poke.
             subp 8, sp
             move cell, a1 # cell can be a0
             move cfr, a0
             cCall2Void(_llint_write_barrier_slow)
             addp 8, sp
-            pop PC, cfr
+            pop PC, PB
             reloadAfterSlowPath()
         end)
 end
 
 macro writeBarrierOnOperand(size, get, cellFieldName)
@@ -614,12 +637,13 @@
 
 # Entrypoints into the interpreter
 
 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
 macro functionArityCheck(doneLabel, slowPath)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     biaeq t0, CodeBlock::m_numParameters[t1], doneLabel
+    prepareStateForCCall()
     move cfr, a0
     move PC, a1
     cCall2(slowPath)   # This slowPath has a simple protocol: t0 = 0 => no error, t0 != 0 => error
     btiz r0, .noError
 
@@ -632,11 +656,11 @@
     jmp _llint_throw_from_slow_path_trampoline
 
 .noError:
     move r1, t1 # r1 contains slotsToAdd.
     btiz t1, .continue
-    loadi PayloadOffset + ArgumentCount[cfr], t2
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t2
     addi CallFrameHeaderSlots, t2
 
     // Check if there are some unaligned slots we can use
     move t1, t3
     andi StackAlignmentSlots - 1, t3
@@ -680,58 +704,42 @@
     baddinz 1, t2, .fillLoop
 
 .continue:
     # Reload CodeBlock and PC, since the slow_path clobbered it.
     loadp CodeBlock[cfr], t1
-    loadp CodeBlock::m_instructionsRawPointer[t1], PC
+    loadp CodeBlock::m_instructionsRawPointer[t1], PB
+    move 0, PC
     jmp doneLabel
 end
 
-macro branchIfException(label)
-    loadp Callee + PayloadOffset[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
-    btpz VM::m_exception[t3], .noException
-    jmp label
-.noException:
-end
-
-
 # Instruction implementations
 
 _llint_op_enter:
     traceExecution()
     checkStackPointerAlignment(t2, 0xdead00e1)
-    loadp CodeBlock[cfr], t1                // t1<CodeBlock> = cfr.CodeBlock
-    loadi CodeBlock::m_numVars[t1], t2      // t2<size_t> = t1<CodeBlock>.m_numVars
+    loadp CodeBlock[cfr], t2                // t2<CodeBlock> = cfr.CodeBlock
+    loadi CodeBlock::m_numVars[t2], t2      // t2<size_t> = t2<CodeBlock>.m_numVars
     subi CalleeSaveSpaceAsVirtualRegisters, t2
     move cfr, t3
     subp CalleeSaveSpaceAsVirtualRegisters * SlotSize, t3
     btiz t2, .opEnterDone
     move UndefinedTag, t0
+    move 0, t1
     negi t2
 .opEnterLoop:
     storei t0, TagOffset[t3, t2, 8]
-    storei 0, PayloadOffset[t3, t2, 8]
+    storei t1, PayloadOffset[t3, t2, 8]
     addi 1, t2
     btinz t2, .opEnterLoop
 .opEnterDone:
-    writeBarrierOnCellWithReload(t1, macro ()
-        loadp CodeBlock[cfr], t1 # Reload CodeBlock
-    end)
-    # Checking traps.
-    loadp CodeBlock::m_vm[t1], t1
-    btpnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps
-.afterHandlingTraps:
+    callSlowPath(_slow_path_enter)
     dispatchOp(narrow, op_enter)
-.handleTraps:
-    callTrapHandler(_llint_throw_from_slow_path_trampoline)
-    jmp .afterHandlingTraps
+
 
 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
     get(m_index, t2)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     bilteq t0, t2, .opGetArgumentOutOfBounds
     loadi ThisArgumentOffset + TagOffset[cfr, t2, 8], t0
     loadi ThisArgumentOffset + PayloadOffset[cfr, t2, 8], t3
     return (t0, t3)
 
@@ -739,11 +747,11 @@
     return (UndefinedTag, 0)
 end)
 
 
 llintOpWithReturn(op_argument_count, OpArgumentCount, macro (size, get, dispatch, return)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     subi 1, t0
     return(Int32Tag, t0)
 end)
 
 
@@ -944,17 +952,24 @@
 
 strictEqualityJumpOp(jnstricteq, OpJnstricteq,
     macro (left, right, target) bineq left, right, target end)
 
 
-macro preOp(opcodeName, opcodeStruct, operation)
-    llintOp(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch)
+macro preOp(opcodeName, opcodeStruct, integerOperation)
+    llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
+        macro updateArithProfile(type)
+            orh type, %opcodeStruct%::Metadata::m_arithProfile + UnaryArithProfile::m_bits[t1]
+        end
+
+        metadata(t1, t2)
         get(m_srcDst, t0)
         bineq TagOffset[cfr, t0, 8], Int32Tag, .slow
-        loadi PayloadOffset[cfr, t0, 8], t1
-        operation(t1, .slow)
-        storei t1, PayloadOffset[cfr, t0, 8]
+        loadi PayloadOffset[cfr, t0, 8], t2
+        # Metadata in t1, srcDst in t2
+        integerOperation(t2, .slow)
+        storei t2, PayloadOffset[cfr, t0, 8]
+        updateArithProfile(ArithProfileInt)
         dispatch()
 
     .slow:
         callSlowPath(_slow_path_%opcodeName%)
         dispatch()
@@ -973,10 +988,23 @@
 .opToNumberSlow:
     callSlowPath(_slow_path_to_number)
     dispatch()
 end)
 
+llintOpWithProfile(op_to_numeric, OpToNumeric, macro (size, get, dispatch, return)
+    get(m_operand, t0)
+    loadConstantOrVariable(size, t0, t2, t3)
+    bieq t2, Int32Tag, .opToNumericIsInt
+    biaeq t2, LowestTag, .opToNumericSlow
+.opToNumericIsInt:
+    return(t2, t3)
+
+.opToNumericSlow:
+    callSlowPath(_slow_path_to_numeric)
+    dispatch()
+end)
+
 
 llintOpWithReturn(op_to_string, OpToString, macro (size, get, dispatch, return)
     get(m_operand, t0)
     loadConstantOrVariable(size, t0, t2, t3)
     bineq t2, CellTag, .opToStringSlow
@@ -1003,26 +1031,26 @@
 end)
 
 
 llintOpWithMetadata(op_negate, OpNegate, macro (size, get, dispatch, metadata, return)
 
-    macro arithProfile(type)
-        ori type, OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t5]
+    macro updateArithProfile(type)
+        orh type, OpNegate::Metadata::m_arithProfile + UnaryArithProfile::m_bits[t5]
     end
 
     metadata(t5, t0)
     get(m_operand, t0)
     loadConstantOrVariable(size, t0, t1, t2)
     bineq t1, Int32Tag, .opNegateSrcNotInt
     btiz t2, 0x7fffffff, .opNegateSlow
     negi t2
-    arithProfile(ArithProfileInt)
+    updateArithProfile(ArithProfileInt)
     return (Int32Tag, t2)
 .opNegateSrcNotInt:
     bia t1, LowestTag, .opNegateSlow
     xori 0x80000000, t1
-    arithProfile(ArithProfileNumber)
+    updateArithProfile(ArithProfileNumber)
     return(t1, t2)
 
 .opNegateSlow:
     callSlowPath(_slow_path_negate)
     dispatch()
@@ -1030,11 +1058,11 @@
 
 
 macro binaryOpCustomStore(opcodeName, opcodeStruct, integerOperationAndStore, doubleOperation)
     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
         macro arithProfile(type)
-            ori type, %opcodeStruct%::Metadata::m_arithProfile + ArithProfile::m_bits[t5]
+            orh type, %opcodeStruct%::Metadata::m_arithProfile + BinaryArithProfile::m_bits[t5]
         end
 
         metadata(t5, t2)
         get(m_rhs, t2)
         get(m_lhs, t0)
@@ -1051,11 +1079,11 @@
         # First operand is definitely not an int, the second operand could be anything.
         bia t2, LowestTag, .slow
         bib t3, LowestTag, .op1NotIntOp2Double
         bineq t3, Int32Tag, .slow
         arithProfile(ArithProfileNumberInt)
-        ci2d t1, ft1
+        ci2ds t1, ft1
         jmp .op1NotIntReady
     .op1NotIntOp2Double:
         fii2d t1, t3, ft1
         arithProfile(ArithProfileNumberNumber)
     .op1NotIntReady:
@@ -1068,11 +1096,11 @@
     .op2NotInt:
         # First operand is definitely an int, the second operand is definitely not.
         get(m_dst, t2)
         bia t3, LowestTag, .slow
         arithProfile(ArithProfileIntNumber)
-        ci2d t0, ft0
+        ci2ds t0, ft0
         fii2d t1, t3, ft1
         doubleOperation(ft1, ft0)
         stored ft0, [cfr, t2, 8]
         dispatch()
 
@@ -1117,12 +1145,12 @@
     macro (left, right) subd left, right end)
 
 
 binaryOpCustomStore(div, OpDiv,
     macro (int32Tag, left, right, slow, index)
-        ci2d left, ft0
-        ci2d right, ft1
+        ci2ds left, ft0
+        ci2ds right, ft1
         divd ft0, ft1
         bcd2i ft1, right, .notInt
         storei int32Tag, TagOffset[cfr, index, 8]
         storei right, PayloadOffset[cfr, index, 8]
         jmp .done
@@ -1403,10 +1431,17 @@
     return(t0, t1)
 
 .opGetByIdSlow:
     callSlowPath(_llint_slow_path_get_by_id)
     dispatch()
+
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_get_by_id, size)
+    metadata(t2, t3)
+    valueProfile(OpGetById, t2, r1, r0)
+    return(r1, r0)
+
 end)
 
 
 llintOpWithMetadata(op_put_by_id, OpPutById, macro (size, get, dispatch, metadata, return)
     writeBarrierOnOperands(size, get, m_base, m_value)
@@ -1465,10 +1500,15 @@
     dispatch()
 
 .opPutByIdSlow:
     callSlowPath(_llint_slow_path_put_by_id)
     dispatch()
+
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_put_by_id, size)
+    dispatch()
+
 end)
 
 
 llintOpWithMetadata(op_get_by_val, OpGetByVal, macro (size, get, dispatch, metadata, return)
     metadata(t5, t2)
@@ -1516,14 +1556,21 @@
     dispatch()
 
 .opGetByValSlow:
     callSlowPath(_llint_slow_path_get_by_val)
     dispatch()
+
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_get_by_val, size)
+    metadata(t2, t3)
+    valueProfile(OpGetByVal, t2, r1, r0)
+    return(r1, r0)
+
 end)
 
 
-macro putByValOp(opcodeName, opcodeStruct)
+macro putByValOp(opcodeName, opcodeStruct, osrExitPoint)
     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
         macro contiguousPutByVal(storeCallback)
             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], .outOfBounds
         .storeResult:
             get(m_value, t2)
@@ -1563,11 +1610,11 @@
             macro (operand, scratch, base, index)
                 const tag = scratch
                 const payload = operand
                 loadConstantOrVariable2Reg(size, operand, tag, payload)
                 bineq tag, Int32Tag, .notInt
-                ci2d payload, ft0
+                ci2ds payload, ft0
                 jmp .ready
             .notInt:
                 fii2d payload, tag, ft0
                 bdnequn ft0, ft0, .opPutByValSlow
             .ready:
@@ -1607,17 +1654,24 @@
     .opPutByValOutOfBounds:
         storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_outOfBounds[t5]
     .opPutByValSlow:
         callSlowPath(_llint_slow_path_%opcodeName%)
         dispatch()
+
+    .osrExitPoint:
+        osrExitPoint(size, dispatch)
     end)
 end
 
 
-putByValOp(put_by_val, OpPutByVal)
+putByValOp(put_by_val, OpPutByVal, macro (size, dispatch)
+.osrReturnPoint:
+    getterSetterOSRExitReturnPoint(op_put_by_val, size)
+    dispatch()
+end)
 
-putByValOp(put_by_val_direct, OpPutByValDirect)
+putByValOp(put_by_val_direct, OpPutByValDirect, macro (a, b) end)
 
 
 macro llintJumpTrueOrFalseOp(opcodeName, opcodeStruct, conditionOp)
     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
         get(m_condition, t1)
@@ -1695,21 +1749,19 @@
 undefinedOrNullJumpOp(jnundefined_or_null, OpJnundefinedOrNull,
     macro (value, target) bineq value, NullTag, target end)
 
 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
     get(m_value, t0)
-    getu(size, OpJneqPtr, m_specialPointer, t1)
-    loadp CodeBlock[cfr], t2
-    loadp CodeBlock::m_globalObject[t2], t2
+    get(m_specialPointer, t1)
+    loadConstant(size, t1, t3, t2)
     bineq TagOffset[cfr, t0, 8], CellTag, .opJneqPtrBranch
-    loadp JSGlobalObject::m_specialPointers[t2, t1, 4], t1
-    bpeq PayloadOffset[cfr, t0, 8], t1, .opJneqPtrFallThrough
+    bpeq PayloadOffset[cfr, t0, 8], t2, .opJneqPtrFallThrough
 .opJneqPtrBranch:
     metadata(t5, t2)
     storeb 1, OpJneqPtr::Metadata::m_hasJumped[t5]
     get(m_targetLabel, t0)
-    jumpImpl(t0)
+    jumpImpl(dispatchIndirect, t0)
 .opJneqPtrFallThrough:
     dispatch()
 end)
 
 
@@ -1753,21 +1805,21 @@
 
     .op1NotInt:
         bia t0, LowestTag, .slow
         bib t2, LowestTag, .op1NotIntOp2Double
         bineq t2, Int32Tag, .slow
-        ci2d t3, ft1
+        ci2ds t3, ft1
         jmp .op1NotIntReady
     .op1NotIntOp2Double:
         fii2d t3, t2, ft1
     .op1NotIntReady:
         fii2d t1, t0, ft0
         doubleCompare(ft0, ft1, .jumpTarget)
         dispatch()
 
     .op2NotInt:
-        ci2d t1, ft0
+        ci2ds t1, ft0
         bia t2, LowestTag, .slow
         fii2d t3, t2, ft1
         doubleCompare(ft0, ft1, .jumpTarget)
         dispatch()
 
@@ -1875,19 +1927,19 @@
         lshifti 3, t3
         negi t3
         addp cfr, t3  # t3 contains the new value of cfr
         storei t2, Callee + PayloadOffset[t3]
         getu(size, opcodeStruct, m_argc, t2)
-        storei PC, ArgumentCount + TagOffset[cfr]
-        storei t2, ArgumentCount + PayloadOffset[t3]
+        storei PC, ArgumentCountIncludingThis + TagOffset[cfr]
+        storei t2, ArgumentCountIncludingThis + PayloadOffset[t3]
         storei CellTag, Callee + TagOffset[t3]
         move t3, sp
         prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
-        callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
+        callTargetFunction(opcodeName, size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
 
     .opCallSlow:
-        slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
+        slowPathForCall(opcodeName, size, opcodeStruct, dispatch, slowPath, prepareCall)
     end)
 end
 
 llintOp(op_ret, OpRet, macro (size, get, dispatch)
     checkSwitchToJITForEpilogue()
@@ -1909,38 +1961,54 @@
     callSlowPath(_slow_path_to_primitive)
     dispatch()
 end)
 
 
+llintOpWithReturn(op_to_property_key, OpToPropertyKey, macro (size, get, dispatch, return)
+    get(m_src, t2)
+    loadConstantOrVariable(size, t2, t1, t0)
+    bineq t1, CellTag, .opToPropertyKeySlow
+    bbeq JSCell::m_type[t0], SymbolType, .done
+    bbneq JSCell::m_type[t0], StringType, .opToPropertyKeySlow
+
+.done:
+    return(t1, t0)
+
+.opToPropertyKeySlow:
+    callSlowPath(_slow_path_to_property_key)
+    dispatch()
+end)
+
+
 commonOp(llint_op_catch, macro() end, macro (size)
     # This is where we end up from the JIT's throw trampoline (because the
     # machine code return address will be set to _llint_op_catch), and from
     # the interpreter's throw trampoline (see _llint_throw_trampoline).
     # The throwing code must have known that we were throwing to the interpreter,
     # and have set VM::targetInterpreterPCForThrow.
     loadp Callee + PayloadOffset[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    convertCalleeToVM(t3)
     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
     loadp VM::callFrameForCatch[t3], cfr
     storep 0, VM::callFrameForCatch[t3]
     restoreStackPointerAfterCall()
 
     # restore metadataTable since we don't restore callee saves for CLoop during unwinding
     loadp CodeBlock[cfr], t1
     loadp CodeBlock::m_metadata[t1], metadataTable
+    loadp CodeBlock::m_instructionsRawPointer[t1], PB
 
-    loadi VM::targetInterpreterPCForThrow[t3], PC
+    loadp VM::targetInterpreterPCForThrow[t3], PC
+    subp PB, PC
 
     callSlowPath(_llint_slow_path_check_if_exception_is_uncatchable_and_notify_profiler)
     bpeq r1, 0, .isCatchableException
     jmp _llint_throw_from_slow_path_trampoline
 
 .isCatchableException:
-    loadp Callee + PayloadOffset[cfr], t3
-    andp MarkedBlockMask, t3
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
+    loadp CodeBlock[cfr], t3
+    loadp CodeBlock::m_vm[t3], t3
 
     loadp VM::m_exception[t3], t0
     storep 0, VM::m_exception[t3]
     get(size, OpCatch, m_exception, t2)
     storei t0, PayloadOffset[cfr, t2, 8]
@@ -1969,22 +2037,20 @@
 end)
 
 
 op(llint_throw_from_slow_path_trampoline, macro()
     loadp Callee[cfr], t1
-    andp MarkedBlockMask, t1
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
+    convertCalleeToVM(t1)
     copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(t1, t2)
 
     callSlowPath(_llint_slow_path_handle_exception)
 
     # When throwing from the interpreter (i.e. throwing from LLIntSlowPaths), so
     # the throw target is not necessarily interpreted code, we come to here.
     # This essentially emulates the JIT's throwing protocol.
     loadp Callee[cfr], t1
-    andp MarkedBlockMask, t1
-    loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
+    convertCalleeToVM(t1)
     jmp VM::targetMachinePCForThrow[t1]
 end)
 
 
 op(llint_throw_during_call_trampoline, macro()
@@ -1992,128 +2058,106 @@
     jmp _llint_throw_from_slow_path_trampoline
 end)
 
 
 macro nativeCallTrampoline(executableOffsetToFunction)
-
     functionPrologue()
     storep 0, CodeBlock[cfr]
-    loadi Callee + PayloadOffset[cfr], t1
-    // Callee is still in t1 for code below
+
     if X86 or X86_WIN
         subp 8, sp # align stack pointer
-        andp MarkedBlockMask, t1
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t3
-        storep cfr, VM::topCallFrame[t3]
-        move cfr, a0  # a0 = ecx
-        storep a0, [sp]
-        loadi Callee + PayloadOffset[cfr], t1
-        loadp JSFunction::m_executable[t1], t1
-        checkStackPointerAlignment(t3, 0xdead0001)
-        call executableOffsetToFunction[t1]
-        loadp Callee + PayloadOffset[cfr], t3
-        andp MarkedBlockMask, t3
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
-        addp 8, sp
-    elsif ARMv7 or C_LOOP or C_LOOP_WIN or MIPS
-        if MIPS
+        storep cfr, [sp]
+    elsif MIPS
         # calling convention says to save stack space for 4 first registers in
         # all cases. To match our 16-byte alignment, that means we need to
         # take 24 bytes
-            subp 24, sp
-        else
-            subp 8, sp # align stack pointer
-        end
-        # t1 already contains the Callee.
-        andp MarkedBlockMask, t1
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
-        storep cfr, VM::topCallFrame[t1]
-        move cfr, a0
-        loadi Callee + PayloadOffset[cfr], t1
-        loadp JSFunction::m_executable[t1], t1
-        checkStackPointerAlignment(t3, 0xdead0001)
-        if C_LOOP or C_LOOP_WIN
-            cloopCallNative executableOffsetToFunction[t1]
-        else
-            call executableOffsetToFunction[t1]
-        end
-        loadp Callee + PayloadOffset[cfr], t3
-        andp MarkedBlockMask, t3
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
-        if MIPS
-            addp 24, sp
-        else
-            addp 8, sp
-        end
+        subp 24, sp
     else
-        error
+        subp 8, sp # align stack pointer
     end
-    
+
+    loadp Callee + PayloadOffset[cfr], a0
+    loadp JSFunction::m_executableOrRareData[a0], a2
+    btpz a2, (constexpr JSFunction::rareDataTag), .isExecutable
+    loadp (FunctionRareData::m_executable - (constexpr JSFunction::rareDataTag))[a2], a2
+.isExecutable:
+    loadp JSFunction::m_scope[a0], a0
+    loadp JSGlobalObject::m_vm[a0], a1
+    storep cfr, VM::topCallFrame[a1]
+    move cfr, a1
+
+    checkStackPointerAlignment(t3, 0xdead0001)
+    if C_LOOP or C_LOOP_WIN
+        cloopCallNative executableOffsetToFunction[a2]
+    else
+        call executableOffsetToFunction[a2]
+    end
+
+    loadp Callee + PayloadOffset[cfr], t3
+    loadp JSFunction::m_scope[t3], t3
+    loadp JSGlobalObject::m_vm[t3], t3
+
+    if MIPS
+        addp 24, sp
+    else
+        addp 8, sp
+    end
+
     btpnz VM::m_exception[t3], .handleException
 
     functionEpilogue()
     ret
 
 .handleException:
-if X86 or X86_WIN
-    subp 8, sp # align stack pointer
-end
+    if X86 or X86_WIN
+        subp 8, sp # align stack pointer
+    end
     storep cfr, VM::topCallFrame[t3]
     jmp _llint_throw_from_slow_path_trampoline
 end
 
 
 macro internalFunctionCallTrampoline(offsetOfFunction)
     functionPrologue()
     storep 0, CodeBlock[cfr]
-    loadi Callee + PayloadOffset[cfr], t1
+
     // Callee is still in t1 for code below
     if X86 or X86_WIN
         subp 8, sp # align stack pointer
-        andp MarkedBlockMask, t1
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t3
-        storep cfr, VM::topCallFrame[t3]
-        move cfr, a0  # a0 = ecx
-        storep a0, [sp]
-        loadi Callee + PayloadOffset[cfr], t1
-        checkStackPointerAlignment(t3, 0xdead0001)
-        call offsetOfFunction[t1]
-        loadp Callee + PayloadOffset[cfr], t3
-        andp MarkedBlockMask, t3
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
-        addp 8, sp
-    elsif ARMv7 or C_LOOP or C_LOOP_WIN or MIPS
+        storep cfr, [sp]
+    else
         subp 8, sp # align stack pointer
-        # t1 already contains the Callee.
-        andp MarkedBlockMask, t1
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
-        storep cfr, VM::topCallFrame[t1]
-        move cfr, a0
-        loadi Callee + PayloadOffset[cfr], t1
-        checkStackPointerAlignment(t3, 0xdead0001)
-        if C_LOOP or C_LOOP_WIN
-            cloopCallNative offsetOfFunction[t1]
-        else
-            call offsetOfFunction[t1]
-        end
-        loadp Callee + PayloadOffset[cfr], t3
-        andp MarkedBlockMask, t3
-        loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
-        addp 8, sp
+    end
+
+    loadp Callee + PayloadOffset[cfr], a2
+    loadp InternalFunction::m_globalObject[a2], a0
+    loadp JSGlobalObject::m_vm[a0], a1
+    storep cfr, VM::topCallFrame[a1]
+    move cfr, a1
+
+    checkStackPointerAlignment(t3, 0xdead0001)
+    if C_LOOP or C_LOOP_WIN
+        cloopCallNative offsetOfFunction[a2]
     else
-        error
+        call offsetOfFunction[a2]
     end
 
+    loadp Callee + PayloadOffset[cfr], t3
+    loadp InternalFunction::m_globalObject[t3], t3
+    loadp JSGlobalObject::m_vm[t3], t3
+
+    addp 8, sp
+
     btpnz VM::m_exception[t3], .handleException
 
     functionEpilogue()
     ret
 
 .handleException:
-if X86 or X86_WIN
-    subp 8, sp # align stack pointer
-end
+    if X86 or X86_WIN
+        subp 8, sp # align stack pointer
+    end
     storep cfr, VM::topCallFrame[t3]
     jmp _llint_throw_from_slow_path_trampoline
 end
 
 
@@ -2513,11 +2557,11 @@
     dispatch()
 end)
 
 
 llintOpWithReturn(op_get_rest_length, OpGetRestLength, macro (size, get, dispatch, return)
-    loadi PayloadOffset + ArgumentCount[cfr], t0
+    loadi PayloadOffset + ArgumentCountIncludingThis[cfr], t0
     subi 1, t0
     getu(size, OpGetRestLength, m_numParametersToSkip, t1)
     bilteq t0, t1, .storeZero
     subi t1, t0
     jmp .finish
@@ -2526,10 +2570,32 @@
 .finish:
     return(Int32Tag, t0)
 end)
 
 
+llintOpWithProfile(op_get_internal_field, OpGetInternalField, macro (size, get, dispatch, return)
+    get(m_base, t0)
+    loadi PayloadOffset[cfr, t0, 8], t0
+    getu(size, OpGetInternalField, m_index, t1)
+    loadi JSInternalFieldObjectImpl_internalFields + TagOffset[t0, t1, SlotSize], t2
+    loadi JSInternalFieldObjectImpl_internalFields + PayloadOffset[t0, t1, SlotSize], t3
+    return(t2, t3)
+end)
+
+llintOp(op_put_internal_field, OpPutInternalField, macro (size, get, dispatch)
+    get(m_base, t0)
+    loadi PayloadOffset[cfr, t0, 8], t0
+    get(m_value, t1)
+    loadConstantOrVariable(size, t1, t2, t3)
+    getu(size, OpPutInternalField, m_index, t1)
+    storei t2, JSInternalFieldObjectImpl_internalFields + TagOffset[t0, t1, SlotSize]
+    storei t3, JSInternalFieldObjectImpl_internalFields + PayloadOffset[t0, t1, SlotSize]
+    writeBarrierOnOperand(size, get, m_base)
+    dispatch()
+end)
+
+
 llintOp(op_log_shadow_chicken_prologue, OpLogShadowChickenPrologue, macro (size, get, dispatch)
     acquireShadowChickenPacket(.opLogShadowChickenPrologueSlow)
     storep cfr, ShadowChicken::Packet::frame[t0]
     loadp CallerFrame[cfr], t1
     storep t1, ShadowChicken::Packet::callerFrame[t0]
