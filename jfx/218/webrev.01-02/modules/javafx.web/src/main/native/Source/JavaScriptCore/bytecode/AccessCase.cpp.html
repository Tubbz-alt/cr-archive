<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/AccessCase.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2017-2020 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;AccessCase.h&quot;
  28 
  29 #if ENABLE(JIT)
  30 
  31 #include &quot;CCallHelpers.h&quot;
  32 #include &quot;CacheableIdentifierInlines.h&quot;
  33 #include &quot;CallLinkInfo.h&quot;
  34 #include &quot;DOMJITGetterSetter.h&quot;
  35 #include &quot;DirectArguments.h&quot;
  36 #include &quot;GetterSetter.h&quot;
  37 #include &quot;GetterSetterAccessCase.h&quot;
  38 #include &quot;InstanceOfAccessCase.h&quot;
  39 #include &quot;IntrinsicGetterAccessCase.h&quot;
  40 #include &quot;JSCInlines.h&quot;
  41 #include &quot;JSModuleEnvironment.h&quot;
  42 #include &quot;JSModuleNamespaceObject.h&quot;
  43 #include &quot;LinkBuffer.h&quot;
  44 #include &quot;ModuleNamespaceAccessCase.h&quot;
  45 #include &quot;PolymorphicAccess.h&quot;
  46 #include &quot;ScopedArguments.h&quot;
  47 #include &quot;ScratchRegisterAllocator.h&quot;
  48 #include &quot;StructureStubInfo.h&quot;
  49 #include &quot;SuperSampler.h&quot;
  50 #include &quot;ThunkGenerators.h&quot;
  51 
  52 namespace JSC {
  53 
  54 namespace AccessCaseInternal {
  55 static constexpr bool verbose = false;
  56 }
  57 
  58 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(AccessCase);
  59 
  60 AccessCase::AccessCase(VM&amp; vm, JSCell* owner, AccessType type, CacheableIdentifier identifier, PropertyOffset offset, Structure* structure, const ObjectPropertyConditionSet&amp; conditionSet, std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain)
  61     : m_type(type)
  62     , m_offset(offset)
  63     , m_polyProtoAccessChain(WTFMove(prototypeAccessChain))
  64     , m_identifier(identifier)
  65 {
  66     m_structure.setMayBeNull(vm, owner, structure);
  67     m_conditionSet = conditionSet;
  68     RELEASE_ASSERT(m_conditionSet.isValid());
  69 }
  70 
  71 std::unique_ptr&lt;AccessCase&gt; AccessCase::create(VM&amp; vm, JSCell* owner, AccessType type, CacheableIdentifier identifier, PropertyOffset offset, Structure* structure, const ObjectPropertyConditionSet&amp; conditionSet, std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain)
  72 {
  73     switch (type) {
  74     case InHit:
  75     case InMiss:
  76         break;
  77     case ArrayLength:
  78     case StringLength:
  79     case DirectArgumentsLength:
  80     case ScopedArgumentsLength:
  81     case ModuleNamespaceLoad:
  82     case Replace:
  83     case InstanceOfGeneric:
  84     case IndexedInt32Load:
  85     case IndexedDoubleLoad:
  86     case IndexedContiguousLoad:
  87     case IndexedArrayStorageLoad:
  88     case IndexedScopedArgumentsLoad:
  89     case IndexedDirectArgumentsLoad:
  90     case IndexedTypedArrayInt8Load:
  91     case IndexedTypedArrayUint8Load:
  92     case IndexedTypedArrayUint8ClampedLoad:
  93     case IndexedTypedArrayInt16Load:
  94     case IndexedTypedArrayUint16Load:
  95     case IndexedTypedArrayInt32Load:
  96     case IndexedTypedArrayUint32Load:
  97     case IndexedTypedArrayFloat32Load:
  98     case IndexedTypedArrayFloat64Load:
  99     case IndexedStringLoad:
 100         RELEASE_ASSERT(!prototypeAccessChain);
 101         break;
 102     default:
 103         RELEASE_ASSERT_NOT_REACHED();
 104     };
 105 
 106     return std::unique_ptr&lt;AccessCase&gt;(new AccessCase(vm, owner, type, identifier, offset, structure, conditionSet, WTFMove(prototypeAccessChain)));
 107 }
 108 
 109 std::unique_ptr&lt;AccessCase&gt; AccessCase::create(
 110     VM&amp; vm, JSCell* owner, CacheableIdentifier identifier, PropertyOffset offset, Structure* oldStructure, Structure* newStructure,
 111     const ObjectPropertyConditionSet&amp; conditionSet, std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain)
 112 {
 113     RELEASE_ASSERT(oldStructure == newStructure-&gt;previousID());
 114 
 115     // Skip optimizing the case where we need a realloc, if we don&#39;t have
 116     // enough registers to make it happen.
 117     if (GPRInfo::numberOfRegisters &lt; 6
 118         &amp;&amp; oldStructure-&gt;outOfLineCapacity() != newStructure-&gt;outOfLineCapacity()
 119         &amp;&amp; oldStructure-&gt;outOfLineCapacity()) {
 120         return nullptr;
 121     }
 122 
 123     return std::unique_ptr&lt;AccessCase&gt;(new AccessCase(vm, owner, Transition, identifier, offset, newStructure, conditionSet, WTFMove(prototypeAccessChain)));
 124 }
 125 
 126 AccessCase::~AccessCase()
 127 {
 128 }
 129 
 130 std::unique_ptr&lt;AccessCase&gt; AccessCase::fromStructureStubInfo(
 131     VM&amp; vm, JSCell* owner, CacheableIdentifier identifier, StructureStubInfo&amp; stubInfo)
 132 {
 133     switch (stubInfo.cacheType()) {
 134     case CacheType::GetByIdSelf:
 135         RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
 136         return ProxyableAccessCase::create(vm, owner, Load, identifier, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
 137 
 138     case CacheType::PutByIdReplace:
 139         RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
 140         ASSERT(!identifier.isCell());
 141         return AccessCase::create(vm, owner, Replace, identifier, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
 142 
 143     case CacheType::InByIdSelf:
 144         RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
 145         ASSERT(!identifier.isCell());
 146         return AccessCase::create(vm, owner, InHit, identifier, stubInfo.u.byIdSelf.offset, stubInfo.u.byIdSelf.baseObjectStructure.get());
 147 
 148     case CacheType::ArrayLength:
 149         RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
 150         ASSERT(!identifier.isCell());
 151         return AccessCase::create(vm, owner, AccessCase::ArrayLength, identifier);
 152 
 153     case CacheType::StringLength:
 154         RELEASE_ASSERT(stubInfo.hasConstantIdentifier);
 155         ASSERT(!identifier.isCell());
 156         return AccessCase::create(vm, owner, AccessCase::StringLength, identifier);
 157 
 158     default:
 159         return nullptr;
 160     }
 161 }
 162 
 163 bool AccessCase::hasAlternateBase() const
 164 {
 165     return !conditionSet().isEmpty();
 166 }
 167 
 168 JSObject* AccessCase::alternateBase() const
 169 {
 170     return conditionSet().slotBaseCondition().object();
 171 }
 172 
 173 std::unique_ptr&lt;AccessCase&gt; AccessCase::clone() const
 174 {
 175     std::unique_ptr&lt;AccessCase&gt; result(new AccessCase(*this));
 176     result-&gt;resetState();
 177     return result;
 178 }
 179 
 180 Vector&lt;WatchpointSet*, 2&gt; AccessCase::commit(VM&amp; vm)
 181 {
 182     // It&#39;s fine to commit something that is already committed. That arises when we switch to using
 183     // newly allocated watchpoints. When it happens, it&#39;s not efficient - but we think that&#39;s OK
 184     // because most AccessCases have no extra watchpoints anyway.
 185     RELEASE_ASSERT(m_state == Primordial || m_state == Committed);
 186 
 187     Vector&lt;WatchpointSet*, 2&gt; result;
 188     Structure* structure = this-&gt;structure();
 189 
 190     if (m_identifier) {
 191         if ((structure &amp;&amp; structure-&gt;needImpurePropertyWatchpoint())
 192             || m_conditionSet.needImpurePropertyWatchpoint()
 193             || (m_polyProtoAccessChain &amp;&amp; m_polyProtoAccessChain-&gt;needImpurePropertyWatchpoint(vm)))
 194             result.append(vm.ensureWatchpointSetForImpureProperty(m_identifier.uid()));
 195     }
 196 
 197     if (additionalSet())
 198         result.append(additionalSet());
 199 
 200     if (structure
 201         &amp;&amp; structure-&gt;hasRareData()
 202         &amp;&amp; structure-&gt;rareData()-&gt;hasSharedPolyProtoWatchpoint()
 203         &amp;&amp; structure-&gt;rareData()-&gt;sharedPolyProtoWatchpoint()-&gt;isStillValid()) {
 204         WatchpointSet* set = structure-&gt;rareData()-&gt;sharedPolyProtoWatchpoint()-&gt;inflate();
 205         result.append(set);
 206     }
 207 
 208     m_state = Committed;
 209 
 210     return result;
 211 }
 212 
 213 bool AccessCase::guardedByStructureCheck(const StructureStubInfo&amp; stubInfo) const
 214 {
 215     if (!stubInfo.hasConstantIdentifier)
 216         return false;
 217     return guardedByStructureCheckSkippingConstantIdentifierCheck();
 218 }
 219 
 220 bool AccessCase::guardedByStructureCheckSkippingConstantIdentifierCheck() const
 221 {
 222     if (viaProxy())
 223         return false;
 224 
 225     if (m_polyProtoAccessChain)
 226         return false;
 227 
 228     switch (m_type) {
 229     case ArrayLength:
 230     case StringLength:
 231     case DirectArgumentsLength:
 232     case ScopedArgumentsLength:
 233     case ModuleNamespaceLoad:
 234     case InstanceOfHit:
 235     case InstanceOfMiss:
 236     case InstanceOfGeneric:
 237     case IndexedInt32Load:
 238     case IndexedDoubleLoad:
 239     case IndexedContiguousLoad:
 240     case IndexedArrayStorageLoad:
 241     case IndexedScopedArgumentsLoad:
 242     case IndexedDirectArgumentsLoad:
 243     case IndexedTypedArrayInt8Load:
 244     case IndexedTypedArrayUint8Load:
 245     case IndexedTypedArrayUint8ClampedLoad:
 246     case IndexedTypedArrayInt16Load:
 247     case IndexedTypedArrayUint16Load:
 248     case IndexedTypedArrayInt32Load:
 249     case IndexedTypedArrayUint32Load:
 250     case IndexedTypedArrayFloat32Load:
 251     case IndexedTypedArrayFloat64Load:
 252     case IndexedStringLoad:
 253         return false;
 254     default:
 255         return true;
 256     }
 257 }
 258 
 259 bool AccessCase::requiresIdentifierNameMatch() const
 260 {
 261     switch (m_type) {
 262     case Load:
 263     // We don&#39;t currently have a by_val for these puts, but we do care about the identifier.
 264     case Transition:
 265     case Replace:
 266     case Miss:
 267     case GetGetter:
 268     case Getter:
 269     case Setter:
 270     case CustomValueGetter:
 271     case CustomAccessorGetter:
 272     case CustomValueSetter:
 273     case CustomAccessorSetter:
 274     case IntrinsicGetter:
 275     case InHit:
 276     case InMiss:
 277     case ArrayLength:
 278     case StringLength:
 279     case DirectArgumentsLength:
 280     case ScopedArgumentsLength:
 281     case ModuleNamespaceLoad:
 282         return true;
 283     case InstanceOfHit:
 284     case InstanceOfMiss:
 285     case InstanceOfGeneric:
 286     case IndexedInt32Load:
 287     case IndexedDoubleLoad:
 288     case IndexedContiguousLoad:
 289     case IndexedArrayStorageLoad:
 290     case IndexedScopedArgumentsLoad:
 291     case IndexedDirectArgumentsLoad:
 292     case IndexedTypedArrayInt8Load:
 293     case IndexedTypedArrayUint8Load:
 294     case IndexedTypedArrayUint8ClampedLoad:
 295     case IndexedTypedArrayInt16Load:
 296     case IndexedTypedArrayUint16Load:
 297     case IndexedTypedArrayInt32Load:
 298     case IndexedTypedArrayUint32Load:
 299     case IndexedTypedArrayFloat32Load:
 300     case IndexedTypedArrayFloat64Load:
 301     case IndexedStringLoad:
 302         return false;
 303     }
 304     RELEASE_ASSERT_NOT_REACHED();
 305 }
 306 
 307 bool AccessCase::requiresInt32PropertyCheck() const
 308 {
 309     switch (m_type) {
 310     case Load:
 311     case Transition:
 312     case Replace:
 313     case Miss:
 314     case GetGetter:
 315     case Getter:
 316     case Setter:
 317     case CustomValueGetter:
 318     case CustomAccessorGetter:
 319     case CustomValueSetter:
 320     case CustomAccessorSetter:
 321     case IntrinsicGetter:
 322     case InHit:
 323     case InMiss:
 324     case ArrayLength:
 325     case StringLength:
 326     case DirectArgumentsLength:
 327     case ScopedArgumentsLength:
 328     case ModuleNamespaceLoad:
 329     case InstanceOfHit:
 330     case InstanceOfMiss:
 331     case InstanceOfGeneric:
 332         return false;
 333     case IndexedInt32Load:
 334     case IndexedDoubleLoad:
 335     case IndexedContiguousLoad:
 336     case IndexedArrayStorageLoad:
 337     case IndexedScopedArgumentsLoad:
 338     case IndexedDirectArgumentsLoad:
 339     case IndexedTypedArrayInt8Load:
 340     case IndexedTypedArrayUint8Load:
 341     case IndexedTypedArrayUint8ClampedLoad:
 342     case IndexedTypedArrayInt16Load:
 343     case IndexedTypedArrayUint16Load:
 344     case IndexedTypedArrayInt32Load:
 345     case IndexedTypedArrayUint32Load:
 346     case IndexedTypedArrayFloat32Load:
 347     case IndexedTypedArrayFloat64Load:
 348     case IndexedStringLoad:
 349         return true;
 350     }
 351     RELEASE_ASSERT_NOT_REACHED();
 352 }
 353 
 354 bool AccessCase::needsScratchFPR() const
 355 {
 356     switch (m_type) {
 357     case Load:
 358     case Transition:
 359     case Replace:
 360     case Miss:
 361     case GetGetter:
 362     case Getter:
 363     case Setter:
 364     case CustomValueGetter:
 365     case CustomAccessorGetter:
 366     case CustomValueSetter:
 367     case CustomAccessorSetter:
 368     case IntrinsicGetter:
 369     case InHit:
 370     case InMiss:
 371     case ArrayLength:
 372     case StringLength:
 373     case DirectArgumentsLength:
 374     case ScopedArgumentsLength:
 375     case ModuleNamespaceLoad:
 376     case InstanceOfHit:
 377     case InstanceOfMiss:
 378     case InstanceOfGeneric:
 379     case IndexedInt32Load:
 380     case IndexedContiguousLoad:
 381     case IndexedArrayStorageLoad:
 382     case IndexedScopedArgumentsLoad:
 383     case IndexedDirectArgumentsLoad:
 384     case IndexedTypedArrayInt8Load:
 385     case IndexedTypedArrayUint8Load:
 386     case IndexedTypedArrayUint8ClampedLoad:
 387     case IndexedTypedArrayInt16Load:
 388     case IndexedTypedArrayUint16Load:
 389     case IndexedTypedArrayInt32Load:
 390     case IndexedStringLoad:
 391         return false;
 392     case IndexedDoubleLoad:
 393     case IndexedTypedArrayFloat32Load:
 394     case IndexedTypedArrayFloat64Load:
 395     case IndexedTypedArrayUint32Load:
 396         return true;
 397     }
 398     RELEASE_ASSERT_NOT_REACHED();
 399 }
 400 
 401 template&lt;typename Functor&gt;
 402 void AccessCase::forEachDependentCell(VM&amp; vm, const Functor&amp; functor) const
 403 {
 404     m_conditionSet.forEachDependentCell(functor);
 405     if (m_structure)
 406         functor(m_structure.get());
 407     if (m_polyProtoAccessChain) {
 408         for (StructureID structureID : m_polyProtoAccessChain-&gt;chain())
 409             functor(vm.getStructure(structureID));
 410     }
 411 
 412     switch (type()) {
 413     case Getter:
 414     case Setter: {
 415         auto&amp; accessor = this-&gt;as&lt;GetterSetterAccessCase&gt;();
 416         if (accessor.callLinkInfo())
 417             accessor.callLinkInfo()-&gt;forEachDependentCell(functor);
 418         break;
 419     }
 420     case CustomValueGetter:
 421     case CustomValueSetter: {
 422         auto&amp; accessor = this-&gt;as&lt;GetterSetterAccessCase&gt;();
 423         if (accessor.customSlotBase())
 424             functor(accessor.customSlotBase());
 425         break;
 426     }
 427     case IntrinsicGetter: {
 428         auto&amp; intrinsic = this-&gt;as&lt;IntrinsicGetterAccessCase&gt;();
 429         if (intrinsic.intrinsicFunction())
 430             functor(intrinsic.intrinsicFunction());
 431         break;
 432     }
 433     case ModuleNamespaceLoad: {
 434         auto&amp; accessCase = this-&gt;as&lt;ModuleNamespaceAccessCase&gt;();
 435         if (accessCase.moduleNamespaceObject())
 436             functor(accessCase.moduleNamespaceObject());
 437         if (accessCase.moduleEnvironment())
 438             functor(accessCase.moduleEnvironment());
 439         break;
 440     }
 441     case InstanceOfHit:
 442     case InstanceOfMiss:
 443         if (as&lt;InstanceOfAccessCase&gt;().prototype())
 444             functor(as&lt;InstanceOfAccessCase&gt;().prototype());
 445         break;
 446     case CustomAccessorGetter:
 447     case CustomAccessorSetter:
 448     case Load:
 449     case Transition:
 450     case Replace:
 451     case Miss:
 452     case GetGetter:
 453     case InHit:
 454     case InMiss:
 455     case ArrayLength:
 456     case StringLength:
 457     case DirectArgumentsLength:
 458     case ScopedArgumentsLength:
 459     case InstanceOfGeneric:
 460     case IndexedInt32Load:
 461     case IndexedDoubleLoad:
 462     case IndexedContiguousLoad:
 463     case IndexedArrayStorageLoad:
 464     case IndexedScopedArgumentsLoad:
 465     case IndexedDirectArgumentsLoad:
 466     case IndexedTypedArrayInt8Load:
 467     case IndexedTypedArrayUint8Load:
 468     case IndexedTypedArrayUint8ClampedLoad:
 469     case IndexedTypedArrayInt16Load:
 470     case IndexedTypedArrayUint16Load:
 471     case IndexedTypedArrayInt32Load:
 472     case IndexedTypedArrayUint32Load:
 473     case IndexedTypedArrayFloat32Load:
 474     case IndexedTypedArrayFloat64Load:
 475     case IndexedStringLoad:
 476         break;
 477     }
 478 }
 479 
 480 bool AccessCase::doesCalls(VM&amp; vm, Vector&lt;JSCell*&gt;* cellsToMarkIfDoesCalls) const
 481 {
 482     bool doesCalls = false;
 483     switch (type()) {
 484     case Transition:
 485         doesCalls = newStructure()-&gt;outOfLineCapacity() != structure()-&gt;outOfLineCapacity() &amp;&amp; structure()-&gt;couldHaveIndexingHeader();
 486         break;
 487     case Getter:
 488     case Setter:
 489     case CustomValueGetter:
 490     case CustomAccessorGetter:
 491     case CustomValueSetter:
 492     case CustomAccessorSetter:
 493         doesCalls = true;
 494         break;
 495     case Load:
 496     case Replace:
 497     case Miss:
 498     case GetGetter:
 499     case IntrinsicGetter:
 500     case InHit:
 501     case InMiss:
 502     case ArrayLength:
 503     case StringLength:
 504     case DirectArgumentsLength:
 505     case ScopedArgumentsLength:
 506     case ModuleNamespaceLoad:
 507     case InstanceOfHit:
 508     case InstanceOfMiss:
 509     case InstanceOfGeneric:
 510     case IndexedInt32Load:
 511     case IndexedDoubleLoad:
 512     case IndexedContiguousLoad:
 513     case IndexedArrayStorageLoad:
 514     case IndexedScopedArgumentsLoad:
 515     case IndexedDirectArgumentsLoad:
 516     case IndexedTypedArrayInt8Load:
 517     case IndexedTypedArrayUint8Load:
 518     case IndexedTypedArrayUint8ClampedLoad:
 519     case IndexedTypedArrayInt16Load:
 520     case IndexedTypedArrayUint16Load:
 521     case IndexedTypedArrayInt32Load:
 522     case IndexedTypedArrayUint32Load:
 523     case IndexedTypedArrayFloat32Load:
 524     case IndexedTypedArrayFloat64Load:
 525     case IndexedStringLoad:
 526         doesCalls = false;
 527         break;
 528     }
 529 
 530     if (doesCalls &amp;&amp; cellsToMarkIfDoesCalls) {
 531         forEachDependentCell(vm, [&amp;](JSCell* cell) {
 532             cellsToMarkIfDoesCalls-&gt;append(cell);
 533         });
 534     }
 535     return doesCalls;
 536 }
 537 
 538 bool AccessCase::couldStillSucceed() const
 539 {
 540     for (const ObjectPropertyCondition&amp; condition : m_conditionSet) {
 541         if (condition.condition().kind() == PropertyCondition::Equivalence) {
 542             if (!condition.isWatchableAssumingImpurePropertyWatchpoint(PropertyCondition::WatchabilityEffort::EnsureWatchability))
 543                 return false;
 544         } else {
 545             if (!condition.structureEnsuresValidityAssumingImpurePropertyWatchpoint())
 546                 return false;
 547         }
 548     }
 549     return true;
 550 }
 551 
 552 bool AccessCase::canReplace(const AccessCase&amp; other) const
 553 {
 554     // This puts in a good effort to try to figure out if &#39;other&#39; is made superfluous by &#39;*this&#39;.
 555     // It&#39;s fine for this to return false if it&#39;s in doubt.
 556     //
 557     // Note that if A-&gt;guardedByStructureCheck() &amp;&amp; B-&gt;guardedByStructureCheck() then
 558     // A-&gt;canReplace(B) == B-&gt;canReplace(A).
 559 
 560     if (m_identifier != other.m_identifier)
 561         return false;
 562 
 563     switch (type()) {
 564     case IndexedInt32Load:
 565     case IndexedDoubleLoad:
 566     case IndexedContiguousLoad:
 567     case IndexedArrayStorageLoad:
 568     case ArrayLength:
 569     case StringLength:
 570     case DirectArgumentsLength:
 571     case ScopedArgumentsLength:
 572     case IndexedScopedArgumentsLoad:
 573     case IndexedDirectArgumentsLoad:
 574     case IndexedTypedArrayInt8Load:
 575     case IndexedTypedArrayUint8Load:
 576     case IndexedTypedArrayUint8ClampedLoad:
 577     case IndexedTypedArrayInt16Load:
 578     case IndexedTypedArrayUint16Load:
 579     case IndexedTypedArrayInt32Load:
 580     case IndexedTypedArrayUint32Load:
 581     case IndexedTypedArrayFloat32Load:
 582     case IndexedTypedArrayFloat64Load:
 583     case IndexedStringLoad:
 584         return other.type() == type();
 585 
 586     case ModuleNamespaceLoad: {
 587         if (other.type() != type())
 588             return false;
 589         auto&amp; thisCase = this-&gt;as&lt;ModuleNamespaceAccessCase&gt;();
 590         auto&amp; otherCase = this-&gt;as&lt;ModuleNamespaceAccessCase&gt;();
 591         return thisCase.moduleNamespaceObject() == otherCase.moduleNamespaceObject();
 592     }
 593 
 594     case InstanceOfHit:
 595     case InstanceOfMiss: {
 596         if (other.type() != type())
 597             return false;
 598 
 599         if (this-&gt;as&lt;InstanceOfAccessCase&gt;().prototype() != other.as&lt;InstanceOfAccessCase&gt;().prototype())
 600             return false;
 601 
 602         return structure() == other.structure();
 603     }
 604 
 605     case InstanceOfGeneric:
 606         switch (other.type()) {
 607         case InstanceOfGeneric:
 608         case InstanceOfHit:
 609         case InstanceOfMiss:
 610             return true;
 611         default:
 612             return false;
 613         }
 614 
 615     case Load:
 616     case Transition:
 617     case Replace:
 618     case Miss:
 619     case GetGetter:
 620     case Getter:
 621     case Setter:
 622     case CustomValueGetter:
 623     case CustomAccessorGetter:
 624     case CustomValueSetter:
 625     case CustomAccessorSetter:
 626     case IntrinsicGetter:
 627     case InHit:
 628     case InMiss:
 629         if (other.type() != type())
 630             return false;
 631 
 632         if (m_polyProtoAccessChain) {
 633             if (!other.m_polyProtoAccessChain)
 634                 return false;
 635             // This is the only check we need since PolyProtoAccessChain contains the base structure.
 636             // If we ever change it to contain only the prototype chain, we&#39;ll also need to change
 637             // this to check the base structure.
 638             return structure() == other.structure()
 639                 &amp;&amp; *m_polyProtoAccessChain == *other.m_polyProtoAccessChain;
 640         }
 641 
 642         if (!guardedByStructureCheckSkippingConstantIdentifierCheck() || !other.guardedByStructureCheckSkippingConstantIdentifierCheck())
 643             return false;
 644 
 645         return structure() == other.structure();
 646     }
 647     RELEASE_ASSERT_NOT_REACHED();
 648 }
 649 
 650 void AccessCase::dump(PrintStream&amp; out) const
 651 {
 652     out.print(&quot;\n&quot;, m_type, &quot;:(&quot;);
 653 
 654     CommaPrinter comma;
 655 
 656     out.print(comma, m_state);
 657 
 658     out.print(comma, &quot;ident = &#39;&quot;, m_identifier, &quot;&#39;&quot;);
 659     if (isValidOffset(m_offset))
 660         out.print(comma, &quot;offset = &quot;, m_offset);
 661 
 662     if (m_polyProtoAccessChain) {
 663         out.print(comma, &quot;prototype access chain = &quot;);
 664         m_polyProtoAccessChain-&gt;dump(structure(), out);
 665     } else {
 666         if (m_type == Transition)
 667             out.print(comma, &quot;structure = &quot;, pointerDump(structure()), &quot; -&gt; &quot;, pointerDump(newStructure()));
 668         else if (m_structure)
 669             out.print(comma, &quot;structure = &quot;, pointerDump(m_structure.get()));
 670     }
 671 
 672     if (!m_conditionSet.isEmpty())
 673         out.print(comma, &quot;conditions = &quot;, m_conditionSet);
 674 
 675     dumpImpl(out, comma);
 676     out.print(&quot;)&quot;);
 677 }
 678 
 679 bool AccessCase::visitWeak(VM&amp; vm) const
 680 {
 681     if (isAccessor()) {
 682         auto&amp; accessor = this-&gt;as&lt;GetterSetterAccessCase&gt;();
 683         if (accessor.callLinkInfo())
 684             accessor.callLinkInfo()-&gt;visitWeak(vm);
 685     }
 686 
 687     bool isValid = true;
 688     forEachDependentCell(vm, [&amp;](JSCell* cell) {
 689         isValid &amp;= vm.heap.isMarked(cell);
 690     });
 691     return isValid;
 692 }
 693 
 694 bool AccessCase::propagateTransitions(SlotVisitor&amp; visitor) const
 695 {
 696     bool result = true;
 697 
 698     if (m_structure)
 699         result &amp;= m_structure-&gt;markIfCheap(visitor);
 700 
 701     if (m_polyProtoAccessChain) {
 702         for (StructureID structureID : m_polyProtoAccessChain-&gt;chain())
 703             result &amp;= visitor.vm().getStructure(structureID)-&gt;markIfCheap(visitor);
 704     }
 705 
 706     switch (m_type) {
 707     case Transition:
 708         if (visitor.vm().heap.isMarked(m_structure-&gt;previousID()))
 709             visitor.appendUnbarriered(m_structure.get());
 710         else
 711             result = false;
 712         break;
 713     default:
 714         break;
 715     }
 716 
 717     return result;
 718 }
 719 
 720 void AccessCase::visitAggregate(SlotVisitor&amp; visitor) const
 721 {
 722     m_identifier.visitAggregate(visitor);
 723 }
 724 
 725 void AccessCase::generateWithGuard(
 726     AccessGenerationState&amp; state, CCallHelpers::JumpList&amp; fallThrough)
 727 {
 728     SuperSamplerScope superSamplerScope(false);
 729 
 730     checkConsistency(*state.stubInfo);
 731 
 732     RELEASE_ASSERT(m_state == Committed);
 733     m_state = Generated;
 734 
 735     CCallHelpers&amp; jit = *state.jit;
 736     StructureStubInfo&amp; stubInfo = *state.stubInfo;
 737     VM&amp; vm = state.m_vm;
 738     JSValueRegs valueRegs = state.valueRegs;
 739     GPRReg baseGPR = state.baseGPR;
 740     GPRReg scratchGPR = state.scratchGPR;
 741 
 742     if (requiresIdentifierNameMatch() &amp;&amp; !stubInfo.hasConstantIdentifier) {
 743         RELEASE_ASSERT(m_identifier);
 744         GPRReg propertyGPR = state.u.propertyGPR;
 745         // non-rope string check done inside polymorphic access.
 746 
 747         if (uid()-&gt;isSymbol())
 748             jit.loadPtr(MacroAssembler::Address(propertyGPR, Symbol::offsetOfSymbolImpl()), scratchGPR);
 749         else
 750             jit.loadPtr(MacroAssembler::Address(propertyGPR, JSString::offsetOfValue()), scratchGPR);
 751         fallThrough.append(jit.branchPtr(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImmPtr(uid())));
 752     }
 753 
 754     auto emitDefaultGuard = [&amp;] () {
 755         if (m_polyProtoAccessChain) {
 756             GPRReg baseForAccessGPR = state.scratchGPR;
 757             jit.move(state.baseGPR, baseForAccessGPR);
 758             m_polyProtoAccessChain-&gt;forEach(vm, structure(), [&amp;] (Structure* structure, bool atEnd) {
 759                 fallThrough.append(
 760                     jit.branchStructure(
 761                         CCallHelpers::NotEqual,
 762                         CCallHelpers::Address(baseForAccessGPR, JSCell::structureIDOffset()),
 763                         structure));
 764                 if (atEnd) {
 765                     if ((m_type == Miss || m_type == InMiss || m_type == Transition) &amp;&amp; structure-&gt;hasPolyProto()) {
 766                         // For a Miss/InMiss/Transition, we must ensure we&#39;re at the end when the last item is poly proto.
 767                         // Transitions must do this because they need to verify there isn&#39;t a setter in the chain.
 768                         // Miss/InMiss need to do this to ensure there isn&#39;t a new item at the end of the chain that
 769                         // has the property.
 770 #if USE(JSVALUE64)
 771                         jit.load64(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset)), baseForAccessGPR);
 772                         fallThrough.append(jit.branch64(CCallHelpers::NotEqual, baseForAccessGPR, CCallHelpers::TrustedImm64(JSValue::ValueNull)));
 773 #else
 774                         jit.load32(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset) + PayloadOffset), baseForAccessGPR);
 775                         fallThrough.append(jit.branchTestPtr(CCallHelpers::NonZero, baseForAccessGPR));
 776 #endif
 777                     }
 778                 } else {
 779                     if (structure-&gt;hasMonoProto()) {
 780                         JSValue prototype = structure-&gt;prototypeForLookup(state.m_globalObject);
 781                         RELEASE_ASSERT(prototype.isObject());
 782                         jit.move(CCallHelpers::TrustedImmPtr(asObject(prototype)), baseForAccessGPR);
 783                     } else {
 784                         RELEASE_ASSERT(structure-&gt;isObject()); // Primitives must have a stored prototype. We use prototypeForLookup for them.
 785 #if USE(JSVALUE64)
 786                         jit.load64(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset)), baseForAccessGPR);
 787                         fallThrough.append(jit.branch64(CCallHelpers::Equal, baseForAccessGPR, CCallHelpers::TrustedImm64(JSValue::ValueNull)));
 788 #else
 789                         jit.load32(MacroAssembler::Address(baseForAccessGPR, offsetRelativeToBase(knownPolyProtoOffset) + PayloadOffset), baseForAccessGPR);
 790                         fallThrough.append(jit.branchTestPtr(CCallHelpers::Zero, baseForAccessGPR));
 791 #endif
 792                     }
 793                 }
 794             });
 795             return;
 796         }
 797 
 798         if (viaProxy()) {
 799             fallThrough.append(
 800                 jit.branchIfNotType(baseGPR, PureForwardingProxyType));
 801 
 802             jit.loadPtr(CCallHelpers::Address(baseGPR, JSProxy::targetOffset()), scratchGPR);
 803 
 804             fallThrough.append(
 805                 jit.branchStructure(
 806                     CCallHelpers::NotEqual,
 807                     CCallHelpers::Address(scratchGPR, JSCell::structureIDOffset()),
 808                     structure()));
 809             return;
 810         }
 811 
 812         fallThrough.append(
 813             jit.branchStructure(
 814                 CCallHelpers::NotEqual,
 815                 CCallHelpers::Address(baseGPR, JSCell::structureIDOffset()),
 816                 structure()));
 817     };
 818 
 819     switch (m_type) {
 820     case ArrayLength: {
 821         ASSERT(!viaProxy());
 822         jit.load8(CCallHelpers::Address(baseGPR, JSCell::indexingTypeAndMiscOffset()), scratchGPR);
 823         fallThrough.append(
 824             jit.branchTest32(
 825                 CCallHelpers::Zero, scratchGPR, CCallHelpers::TrustedImm32(IsArray)));
 826         fallThrough.append(
 827             jit.branchTest32(
 828                 CCallHelpers::Zero, scratchGPR, CCallHelpers::TrustedImm32(IndexingShapeMask)));
 829         break;
 830     }
 831 
 832     case StringLength: {
 833         ASSERT(!viaProxy());
 834         fallThrough.append(
 835             jit.branchIfNotString(baseGPR));
 836         break;
 837     }
 838 
 839     case DirectArgumentsLength: {
 840         ASSERT(!viaProxy());
 841         fallThrough.append(
 842             jit.branchIfNotType(baseGPR, DirectArgumentsType));
 843 
 844         fallThrough.append(
 845             jit.branchTestPtr(
 846                 CCallHelpers::NonZero,
 847                 CCallHelpers::Address(baseGPR, DirectArguments::offsetOfMappedArguments())));
 848         jit.load32(
 849             CCallHelpers::Address(baseGPR, DirectArguments::offsetOfLength()),
 850             valueRegs.payloadGPR());
 851         jit.boxInt32(valueRegs.payloadGPR(), valueRegs);
 852         state.succeed();
 853         return;
 854     }
 855 
 856     case ScopedArgumentsLength: {
 857         ASSERT(!viaProxy());
 858         fallThrough.append(
 859             jit.branchIfNotType(baseGPR, ScopedArgumentsType));
 860 
 861         fallThrough.append(
 862             jit.branchTest8(
 863                 CCallHelpers::NonZero,
 864                 CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfOverrodeThings())));
 865         jit.load32(
 866             CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfTotalLength()),
 867             valueRegs.payloadGPR());
 868         jit.boxInt32(valueRegs.payloadGPR(), valueRegs);
 869         state.succeed();
 870         return;
 871     }
 872 
 873     case ModuleNamespaceLoad: {
 874         this-&gt;as&lt;ModuleNamespaceAccessCase&gt;().emit(state, fallThrough);
 875         return;
 876     }
 877 
 878     case IndexedScopedArgumentsLoad: {
 879         // This code is written such that the result could alias with the base or the property.
 880         GPRReg propertyGPR = state.u.propertyGPR;
 881 
 882         jit.load8(CCallHelpers::Address(baseGPR, JSCell::typeInfoTypeOffset()), scratchGPR);
 883         fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(ScopedArgumentsType)));
 884 
 885         ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
 886         allocator.lock(stubInfo.baseRegs());
 887         allocator.lock(valueRegs);
 888         allocator.lock(stubInfo.propertyRegs());
 889         allocator.lock(scratchGPR);
 890 
 891         GPRReg scratch2GPR = allocator.allocateScratchGPR();
 892         GPRReg scratch3GPR = allocator.allocateScratchGPR();
 893 
 894         ScratchRegisterAllocator::PreservedState preservedState = allocator.preserveReusedRegistersByPushing(
 895             jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
 896 
 897         CCallHelpers::JumpList failAndIgnore;
 898 
 899         failAndIgnore.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfTotalLength())));
 900 
 901         jit.loadPtr(CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfTable()), scratchGPR);
 902         jit.load32(CCallHelpers::Address(scratchGPR, ScopedArgumentsTable::offsetOfLength()), scratch2GPR);
 903         auto overflowCase = jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratch2GPR);
 904 
 905         jit.loadPtr(CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfScope()), scratch2GPR);
 906         jit.loadPtr(CCallHelpers::Address(scratchGPR, ScopedArgumentsTable::offsetOfArguments()), scratchGPR);
 907         jit.zeroExtend32ToPtr(propertyGPR, scratch3GPR);
 908         jit.load32(CCallHelpers::BaseIndex(scratchGPR, scratch3GPR, CCallHelpers::TimesFour), scratchGPR);
 909         failAndIgnore.append(jit.branch32(CCallHelpers::Equal, scratchGPR, CCallHelpers::TrustedImm32(ScopeOffset::invalidOffset)));
 910         jit.loadValue(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesEight, JSLexicalEnvironment::offsetOfVariables()), valueRegs);
 911         auto done = jit.jump();
 912 
 913         overflowCase.link(&amp;jit);
 914         jit.sub32(propertyGPR, scratch2GPR);
 915         jit.neg32(scratch2GPR);
 916         jit.loadPtr(CCallHelpers::Address(baseGPR, ScopedArguments::offsetOfStorage()), scratch3GPR);
 917 #if USE(JSVALUE64)
 918         jit.loadValue(CCallHelpers::BaseIndex(scratch3GPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratchGPR));
 919         failAndIgnore.append(jit.branchIfEmpty(scratchGPR));
 920         jit.move(scratchGPR, valueRegs.payloadGPR());
 921 #else
 922         jit.loadValue(CCallHelpers::BaseIndex(scratch3GPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratch2GPR, scratchGPR));
 923         failAndIgnore.append(jit.branchIfEmpty(scratch2GPR));
 924         jit.move(scratchGPR, valueRegs.payloadGPR());
 925         jit.move(scratch2GPR, valueRegs.tagGPR());
 926 #endif
 927 
 928         done.link(&amp;jit);
 929 
 930         allocator.restoreReusedRegistersByPopping(jit, preservedState);
 931         state.succeed();
 932 
 933         if (allocator.didReuseRegisters()) {
 934             failAndIgnore.link(&amp;jit);
 935             allocator.restoreReusedRegistersByPopping(jit, preservedState);
 936             state.failAndIgnore.append(jit.jump());
 937         } else
 938             state.failAndIgnore.append(failAndIgnore);
 939 
 940         return;
 941     }
 942 
 943     case IndexedDirectArgumentsLoad: {
 944         // This code is written such that the result could alias with the base or the property.
 945         GPRReg propertyGPR = state.u.propertyGPR;
 946         jit.load8(CCallHelpers::Address(baseGPR, JSCell::typeInfoTypeOffset()), scratchGPR);
 947         fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(DirectArgumentsType)));
 948 
 949         jit.load32(CCallHelpers::Address(baseGPR, DirectArguments::offsetOfLength()), scratchGPR);
 950         state.failAndRepatch.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratchGPR));
 951         state.failAndRepatch.append(jit.branchTestPtr(CCallHelpers::NonZero, CCallHelpers::Address(baseGPR, DirectArguments::offsetOfMappedArguments())));
 952         jit.zeroExtend32ToPtr(propertyGPR, scratchGPR);
 953         jit.loadValue(CCallHelpers::BaseIndex(baseGPR, scratchGPR, CCallHelpers::TimesEight, DirectArguments::storageOffset()), valueRegs);
 954         state.succeed();
 955         return;
 956     }
 957 
 958     case IndexedTypedArrayInt8Load:
 959     case IndexedTypedArrayUint8Load:
 960     case IndexedTypedArrayUint8ClampedLoad:
 961     case IndexedTypedArrayInt16Load:
 962     case IndexedTypedArrayUint16Load:
 963     case IndexedTypedArrayInt32Load:
 964     case IndexedTypedArrayUint32Load:
 965     case IndexedTypedArrayFloat32Load:
 966     case IndexedTypedArrayFloat64Load: {
 967         // This code is written such that the result could alias with the base or the property.
 968 
 969         TypedArrayType type = toTypedArrayType(m_type);
 970 
 971         GPRReg propertyGPR = state.u.propertyGPR;
 972 
 973 
 974         jit.load8(CCallHelpers::Address(baseGPR, JSCell::typeInfoTypeOffset()), scratchGPR);
 975         fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(typeForTypedArrayType(type))));
 976 
 977         jit.load32(CCallHelpers::Address(baseGPR, JSArrayBufferView::offsetOfLength()), scratchGPR);
 978         state.failAndRepatch.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratchGPR));
 979 
 980         ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
 981         allocator.lock(stubInfo.baseRegs());
 982         allocator.lock(valueRegs);
 983         allocator.lock(stubInfo.propertyRegs());
 984         allocator.lock(scratchGPR);
 985         GPRReg scratch2GPR = allocator.allocateScratchGPR();
 986 
 987         ScratchRegisterAllocator::PreservedState preservedState = allocator.preserveReusedRegistersByPushing(
 988             jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
 989 
 990         jit.loadPtr(CCallHelpers::Address(baseGPR, JSArrayBufferView::offsetOfVector()), scratch2GPR);
 991         jit.cageConditionally(Gigacage::Primitive, scratch2GPR, scratchGPR, scratchGPR);
 992 
 993         jit.signExtend32ToPtr(propertyGPR, scratchGPR);
 994         if (isInt(type)) {
 995             switch (elementSize(type)) {
 996             case 1:
 997                 if (JSC::isSigned(type))
 998                     jit.load8SignedExtendTo32(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesOne), valueRegs.payloadGPR());
 999                 else
1000                     jit.load8(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesOne), valueRegs.payloadGPR());
1001                 break;
1002             case 2:
1003                 if (JSC::isSigned(type))
1004                     jit.load16SignedExtendTo32(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesTwo), valueRegs.payloadGPR());
1005                 else
1006                     jit.load16(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesTwo), valueRegs.payloadGPR());
1007                 break;
1008             case 4:
1009                 jit.load32(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesFour), valueRegs.payloadGPR());
1010                 break;
1011             default:
1012                 CRASH();
1013             }
1014 
1015             CCallHelpers::Jump done;
1016             if (type == TypeUint32) {
1017                 RELEASE_ASSERT(state.scratchFPR != InvalidFPRReg);
1018                 auto canBeInt = jit.branch32(CCallHelpers::GreaterThanOrEqual, valueRegs.payloadGPR(), CCallHelpers::TrustedImm32(0));
1019 
1020                 jit.convertInt32ToDouble(valueRegs.payloadGPR(), state.scratchFPR);
1021                 jit.addDouble(CCallHelpers::AbsoluteAddress(&amp;CCallHelpers::twoToThe32), state.scratchFPR);
1022                 jit.boxDouble(state.scratchFPR, valueRegs);
1023                 done = jit.jump();
1024                 canBeInt.link(&amp;jit);
1025             }
1026 
1027             jit.boxInt32(valueRegs.payloadGPR(), valueRegs);
1028             if (done.isSet())
1029                 done.link(&amp;jit);
1030         } else {
1031             ASSERT(isFloat(type));
1032             RELEASE_ASSERT(state.scratchFPR != InvalidFPRReg);
1033             switch (elementSize(type)) {
1034             case 4:
1035                 jit.loadFloat(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesFour), state.scratchFPR);
1036                 jit.convertFloatToDouble(state.scratchFPR, state.scratchFPR);
1037                 break;
1038             case 8: {
1039                 jit.loadDouble(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesEight), state.scratchFPR);
1040                 break;
1041             }
1042             default:
1043                 CRASH();
1044             }
1045 
1046             jit.purifyNaN(state.scratchFPR);
1047             jit.boxDouble(state.scratchFPR, valueRegs);
1048         }
1049 
1050         allocator.restoreReusedRegistersByPopping(jit, preservedState);
1051         state.succeed();
1052 
1053         return;
1054     }
1055 
1056     case IndexedStringLoad: {
1057         // This code is written such that the result could alias with the base or the property.
1058         GPRReg propertyGPR = state.u.propertyGPR;
1059 
1060         fallThrough.append(jit.branchIfNotString(baseGPR));
1061 
1062         ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
1063         allocator.lock(stubInfo.baseRegs());
1064         allocator.lock(valueRegs);
1065         allocator.lock(stubInfo.propertyRegs());
1066         allocator.lock(scratchGPR);
1067         GPRReg scratch2GPR = allocator.allocateScratchGPR();
1068 
1069         CCallHelpers::JumpList failAndIgnore;
1070 
1071         ScratchRegisterAllocator::PreservedState preservedState = allocator.preserveReusedRegistersByPushing(
1072             jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
1073 
1074         jit.loadPtr(CCallHelpers::Address(baseGPR, JSString::offsetOfValue()), scratch2GPR);
1075         failAndIgnore.append(jit.branchIfRopeStringImpl(scratch2GPR));
1076         jit.load32(CCallHelpers::Address(scratch2GPR, StringImpl::lengthMemoryOffset()), scratchGPR);
1077 
1078         failAndIgnore.append(jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, scratchGPR));
1079 
1080         jit.load32(CCallHelpers::Address(scratch2GPR, StringImpl::flagsOffset()), scratchGPR);
1081         jit.loadPtr(CCallHelpers::Address(scratch2GPR, StringImpl::dataOffset()), scratch2GPR);
1082         auto is16Bit = jit.branchTest32(CCallHelpers::Zero, scratchGPR, CCallHelpers::TrustedImm32(StringImpl::flagIs8Bit()));
1083         jit.zeroExtend32ToPtr(propertyGPR, scratchGPR);
1084         jit.load8(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesOne, 0), scratch2GPR);
1085         auto is8BitLoadDone = jit.jump();
1086         is16Bit.link(&amp;jit);
1087         jit.zeroExtend32ToPtr(propertyGPR, scratchGPR);
1088         jit.load16(CCallHelpers::BaseIndex(scratch2GPR, scratchGPR, CCallHelpers::TimesTwo, 0), scratch2GPR);
1089         is8BitLoadDone.link(&amp;jit);
1090 
1091         failAndIgnore.append(jit.branch32(CCallHelpers::Above, scratch2GPR, CCallHelpers::TrustedImm32(maxSingleCharacterString)));
1092         jit.move(CCallHelpers::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), scratchGPR);
1093         jit.loadPtr(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::ScalePtr, 0), valueRegs.payloadGPR());
1094         jit.boxCell(valueRegs.payloadGPR(), valueRegs);
1095         allocator.restoreReusedRegistersByPopping(jit, preservedState);
1096         state.succeed();
1097 
1098         if (allocator.didReuseRegisters()) {
1099             failAndIgnore.link(&amp;jit);
1100             allocator.restoreReusedRegistersByPopping(jit, preservedState);
1101             state.failAndIgnore.append(jit.jump());
1102         } else
1103             state.failAndIgnore.append(failAndIgnore);
1104 
1105         return;
1106     }
1107 
1108     case IndexedInt32Load:
1109     case IndexedDoubleLoad:
1110     case IndexedContiguousLoad:
1111     case IndexedArrayStorageLoad: {
1112         // This code is written such that the result could alias with the base or the property.
1113         GPRReg propertyGPR = state.u.propertyGPR;
1114 
1115         // int32 check done in polymorphic access.
1116         jit.load8(CCallHelpers::Address(baseGPR, JSCell::indexingTypeAndMiscOffset()), scratchGPR);
1117         jit.and32(CCallHelpers::TrustedImm32(IndexingShapeMask), scratchGPR);
1118 
1119         CCallHelpers::Jump isOutOfBounds;
1120         CCallHelpers::Jump isEmpty;
1121 
1122         ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
1123         allocator.lock(stubInfo.baseRegs());
1124         allocator.lock(valueRegs);
1125         allocator.lock(stubInfo.propertyRegs());
1126         allocator.lock(scratchGPR);
1127         GPRReg scratch2GPR = allocator.allocateScratchGPR();
1128 #if USE(JSVALUE32_64)
1129         GPRReg scratch3GPR = allocator.allocateScratchGPR();
1130 #endif
1131         ScratchRegisterAllocator::PreservedState preservedState;
1132 
1133         CCallHelpers::JumpList failAndIgnore;
1134         auto preserveReusedRegisters = [&amp;] {
1135             preservedState = allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
1136         };
1137 
1138         if (m_type == IndexedArrayStorageLoad) {
1139             jit.add32(CCallHelpers::TrustedImm32(-ArrayStorageShape), scratchGPR, scratchGPR);
1140             fallThrough.append(jit.branch32(CCallHelpers::Above, scratchGPR, CCallHelpers::TrustedImm32(SlowPutArrayStorageShape - ArrayStorageShape)));
1141 
1142             preserveReusedRegisters();
1143 
1144             jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
1145             isOutOfBounds = jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, CCallHelpers::Address(scratchGPR, ArrayStorage::vectorLengthOffset()));
1146 
1147             jit.zeroExtend32ToPtr(propertyGPR, scratch2GPR);
1148 #if USE(JSVALUE64)
1149             jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight, ArrayStorage::vectorOffset()), JSValueRegs(scratchGPR));
1150             isEmpty = jit.branchIfEmpty(scratchGPR);
1151             jit.move(scratchGPR, valueRegs.payloadGPR());
1152 #else
1153             jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight, ArrayStorage::vectorOffset()), JSValueRegs(scratch3GPR, scratchGPR));
1154             isEmpty = jit.branchIfEmpty(scratch3GPR);
1155             jit.move(scratchGPR, valueRegs.payloadGPR());
1156             jit.move(scratch3GPR, valueRegs.tagGPR());
1157 #endif
1158         } else {
1159             IndexingType expectedShape;
1160             switch (m_type) {
1161             case IndexedInt32Load:
1162                 expectedShape = Int32Shape;
1163                 break;
1164             case IndexedDoubleLoad:
1165                 expectedShape = DoubleShape;
1166                 break;
1167             case IndexedContiguousLoad:
1168                 expectedShape = ContiguousShape;
1169                 break;
1170             default:
1171                 RELEASE_ASSERT_NOT_REACHED();
1172                 break;
1173             }
1174 
1175             fallThrough.append(jit.branch32(CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(expectedShape)));
1176 
1177             preserveReusedRegisters();
1178 
1179             jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
1180             isOutOfBounds = jit.branch32(CCallHelpers::AboveOrEqual, propertyGPR, CCallHelpers::Address(scratchGPR, Butterfly::offsetOfPublicLength()));
1181             jit.zeroExtend32ToPtr(propertyGPR, scratch2GPR);
1182             if (m_type == IndexedDoubleLoad) {
1183                 RELEASE_ASSERT(state.scratchFPR != InvalidFPRReg);
1184                 jit.loadDouble(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight), state.scratchFPR);
1185                 isEmpty = jit.branchIfNaN(state.scratchFPR);
1186                 jit.boxDouble(state.scratchFPR, valueRegs);
1187             } else {
1188 #if USE(JSVALUE64)
1189                 jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratchGPR));
1190                 isEmpty = jit.branchIfEmpty(scratchGPR);
1191                 jit.move(scratchGPR, valueRegs.payloadGPR());
1192 #else
1193                 jit.loadValue(CCallHelpers::BaseIndex(scratchGPR, scratch2GPR, CCallHelpers::TimesEight), JSValueRegs(scratch3GPR, scratchGPR));
1194                 isEmpty = jit.branchIfEmpty(scratch3GPR);
1195                 jit.move(scratchGPR, valueRegs.payloadGPR());
1196                 jit.move(scratch3GPR, valueRegs.tagGPR());
1197 #endif
1198             }
1199         }
1200 
1201         allocator.restoreReusedRegistersByPopping(jit, preservedState);
1202         state.succeed();
1203 
1204         if (allocator.didReuseRegisters()) {
1205             isOutOfBounds.link(&amp;jit);
1206             isEmpty.link(&amp;jit);
1207             allocator.restoreReusedRegistersByPopping(jit, preservedState);
1208             state.failAndIgnore.append(jit.jump());
1209         } else {
1210             state.failAndIgnore.append(isOutOfBounds);
1211             state.failAndIgnore.append(isEmpty);
1212         }
1213 
1214         return;
1215     }
1216 
1217     case InstanceOfHit:
1218     case InstanceOfMiss:
1219         emitDefaultGuard();
1220 
1221         fallThrough.append(
1222             jit.branchPtr(
1223                 CCallHelpers::NotEqual, state.u.prototypeGPR,
1224                 CCallHelpers::TrustedImmPtr(as&lt;InstanceOfAccessCase&gt;().prototype())));
1225         break;
1226 
1227     case InstanceOfGeneric: {
1228         GPRReg prototypeGPR = state.u.prototypeGPR;
1229         // Legend: value = `base instanceof prototypeGPR`.
1230 
1231         GPRReg valueGPR = valueRegs.payloadGPR();
1232 
1233         ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
1234         allocator.lock(stubInfo.baseRegs());
1235         allocator.lock(valueRegs);
1236         allocator.lock(stubInfo.propertyRegs());
1237         allocator.lock(scratchGPR);
1238 
1239         GPRReg scratch2GPR = allocator.allocateScratchGPR();
1240 
1241         if (!state.stubInfo-&gt;prototypeIsKnownObject)
1242             state.failAndIgnore.append(jit.branchIfNotObject(prototypeGPR));
1243 
1244         ScratchRegisterAllocator::PreservedState preservedState =
1245             allocator.preserveReusedRegistersByPushing(
1246                 jit,
1247                 ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
1248         CCallHelpers::Jump failAndIgnore;
1249 
1250         jit.move(baseGPR, valueGPR);
1251 
1252         CCallHelpers::Label loop(&amp;jit);
1253         failAndIgnore = jit.branchIfType(valueGPR, ProxyObjectType);
1254 
1255         jit.emitLoadStructure(vm, valueGPR, scratch2GPR, scratchGPR);
1256 #if USE(JSVALUE64)
1257         jit.load64(CCallHelpers::Address(scratch2GPR, Structure::prototypeOffset()), scratch2GPR);
1258         CCallHelpers::Jump hasMonoProto = jit.branchTest64(CCallHelpers::NonZero, scratch2GPR);
1259         jit.load64(
1260             CCallHelpers::Address(valueGPR, offsetRelativeToBase(knownPolyProtoOffset)),
1261             scratch2GPR);
1262         hasMonoProto.link(&amp;jit);
1263 #else
1264         jit.load32(
1265             CCallHelpers::Address(scratch2GPR, Structure::prototypeOffset() + TagOffset),
1266             scratchGPR);
1267         jit.load32(
1268             CCallHelpers::Address(scratch2GPR, Structure::prototypeOffset() + PayloadOffset),
1269             scratch2GPR);
1270         CCallHelpers::Jump hasMonoProto = jit.branch32(
1271             CCallHelpers::NotEqual, scratchGPR, CCallHelpers::TrustedImm32(JSValue::EmptyValueTag));
1272         jit.load32(
1273             CCallHelpers::Address(
1274                 valueGPR, offsetRelativeToBase(knownPolyProtoOffset) + PayloadOffset),
1275             scratch2GPR);
1276         hasMonoProto.link(&amp;jit);
1277 #endif
1278         jit.move(scratch2GPR, valueGPR);
1279 
1280         CCallHelpers::Jump isInstance = jit.branchPtr(CCallHelpers::Equal, valueGPR, prototypeGPR);
1281 
1282 #if USE(JSVALUE64)
1283         jit.branchIfCell(JSValueRegs(valueGPR)).linkTo(loop, &amp;jit);
1284 #else
1285         jit.branchTestPtr(CCallHelpers::NonZero, valueGPR).linkTo(loop, &amp;jit);
1286 #endif
1287 
1288         jit.boxBooleanPayload(false, valueGPR);
1289         allocator.restoreReusedRegistersByPopping(jit, preservedState);
1290         state.succeed();
1291 
1292         isInstance.link(&amp;jit);
1293         jit.boxBooleanPayload(true, valueGPR);
1294         allocator.restoreReusedRegistersByPopping(jit, preservedState);
1295         state.succeed();
1296 
1297         if (allocator.didReuseRegisters()) {
1298             failAndIgnore.link(&amp;jit);
1299             allocator.restoreReusedRegistersByPopping(jit, preservedState);
1300             state.failAndIgnore.append(jit.jump());
1301         } else
1302             state.failAndIgnore.append(failAndIgnore);
1303         return;
1304     }
1305 
1306     default:
1307         emitDefaultGuard();
1308         break;
1309     }
1310 
1311     generateImpl(state);
1312 }
1313 
1314 void AccessCase::generate(AccessGenerationState&amp; state)
1315 {
1316     RELEASE_ASSERT(m_state == Committed);
1317     RELEASE_ASSERT(state.stubInfo-&gt;hasConstantIdentifier);
1318     m_state = Generated;
1319 
1320     checkConsistency(*state.stubInfo);
1321 
1322     generateImpl(state);
1323 }
1324 
1325 void AccessCase::generateImpl(AccessGenerationState&amp; state)
1326 {
1327     SuperSamplerScope superSamplerScope(false);
1328     if (AccessCaseInternal::verbose)
1329         dataLog(&quot;\n\nGenerating code for: &quot;, *this, &quot;\n&quot;);
1330 
1331     ASSERT(m_state == Generated); // We rely on the callers setting this for us.
1332 
1333     CCallHelpers&amp; jit = *state.jit;
1334     VM&amp; vm = state.m_vm;
1335     CodeBlock* codeBlock = jit.codeBlock();
1336     StructureStubInfo&amp; stubInfo = *state.stubInfo;
1337     JSValueRegs valueRegs = state.valueRegs;
1338     GPRReg baseGPR = state.baseGPR;
1339     GPRReg thisGPR = stubInfo.thisValueIsInThisGPR() ? state.u.thisGPR : baseGPR;
1340     GPRReg scratchGPR = state.scratchGPR;
1341 
1342     for (const ObjectPropertyCondition&amp; condition : m_conditionSet) {
1343         RELEASE_ASSERT(!m_polyProtoAccessChain);
1344 
1345         if (condition.isWatchableAssumingImpurePropertyWatchpoint(PropertyCondition::WatchabilityEffort::EnsureWatchability)) {
1346             state.installWatchpoint(condition);
1347             continue;
1348         }
1349 
1350         // For now, we only allow equivalence when it&#39;s watchable.
1351         RELEASE_ASSERT(condition.condition().kind() != PropertyCondition::Equivalence);
1352 
1353         if (!condition.structureEnsuresValidityAssumingImpurePropertyWatchpoint()) {
1354             // The reason why this cannot happen is that we require that PolymorphicAccess calls
1355             // AccessCase::generate() only after it has verified that
1356             // AccessCase::couldStillSucceed() returned true.
1357 
1358             dataLog(&quot;This condition is no longer met: &quot;, condition, &quot;\n&quot;);
1359             RELEASE_ASSERT_NOT_REACHED();
1360         }
1361 
1362         // We will emit code that has a weak reference that isn&#39;t otherwise listed anywhere.
1363         Structure* structure = condition.object()-&gt;structure(vm);
1364         state.weakReferences.append(WriteBarrier&lt;JSCell&gt;(vm, codeBlock, structure));
1365 
1366         jit.move(CCallHelpers::TrustedImmPtr(condition.object()), scratchGPR);
1367         state.failAndRepatch.append(
1368             jit.branchStructure(
1369                 CCallHelpers::NotEqual,
1370                 CCallHelpers::Address(scratchGPR, JSCell::structureIDOffset()),
1371                 structure));
1372     }
1373 
1374     switch (m_type) {
1375     case InHit:
1376     case InMiss:
1377         jit.boxBoolean(m_type == InHit, valueRegs);
1378         state.succeed();
1379         return;
1380 
1381     case Miss:
1382         jit.moveTrustedValue(jsUndefined(), valueRegs);
1383         state.succeed();
1384         return;
1385 
1386     case InstanceOfHit:
1387     case InstanceOfMiss:
1388         jit.boxBooleanPayload(m_type == InstanceOfHit, valueRegs.payloadGPR());
1389         state.succeed();
1390         return;
1391 
1392     case Load:
1393     case GetGetter:
1394     case Getter:
1395     case Setter:
1396     case CustomValueGetter:
1397     case CustomAccessorGetter:
1398     case CustomValueSetter:
1399     case CustomAccessorSetter: {
1400         GPRReg valueRegsPayloadGPR = valueRegs.payloadGPR();
1401 
1402         if (isValidOffset(m_offset)) {
1403             Structure* currStructure;
1404             if (!hasAlternateBase())
1405                 currStructure = structure();
1406             else
1407                 currStructure = alternateBase()-&gt;structure(vm);
1408             currStructure-&gt;startWatchingPropertyForReplacements(vm, offset());
1409         }
1410 
1411         GPRReg baseForGetGPR;
1412         if (viaProxy()) {
1413             ASSERT(m_type != CustomValueSetter || m_type != CustomAccessorSetter); // Because setters need to not trash valueRegsPayloadGPR.
1414             if (m_type == Getter || m_type == Setter)
1415                 baseForGetGPR = scratchGPR;
1416             else
1417                 baseForGetGPR = valueRegsPayloadGPR;
1418 
1419             ASSERT((m_type != Getter &amp;&amp; m_type != Setter) || baseForGetGPR != baseGPR);
1420             ASSERT(m_type != Setter || baseForGetGPR != valueRegsPayloadGPR);
1421 
1422             jit.loadPtr(
1423                 CCallHelpers::Address(baseGPR, JSProxy::targetOffset()),
1424                 baseForGetGPR);
1425         } else
1426             baseForGetGPR = baseGPR;
1427 
1428         GPRReg baseForAccessGPR;
1429         if (m_polyProtoAccessChain) {
1430             // This isn&#39;t pretty, but we know we got here via generateWithGuard,
1431             // and it left the baseForAccess inside scratchGPR. We could re-derive the base,
1432             // but it&#39;d require emitting the same code to load the base twice.
1433             baseForAccessGPR = scratchGPR;
1434         } else {
1435             if (hasAlternateBase()) {
1436                 jit.move(
1437                     CCallHelpers::TrustedImmPtr(alternateBase()), scratchGPR);
1438                 baseForAccessGPR = scratchGPR;
1439             } else
1440                 baseForAccessGPR = baseForGetGPR;
1441         }
1442 
1443         GPRReg loadedValueGPR = InvalidGPRReg;
1444         if (m_type != CustomValueGetter &amp;&amp; m_type != CustomAccessorGetter &amp;&amp; m_type != CustomValueSetter &amp;&amp; m_type != CustomAccessorSetter) {
1445             if (m_type == Load || m_type == GetGetter)
1446                 loadedValueGPR = valueRegsPayloadGPR;
1447             else
1448                 loadedValueGPR = scratchGPR;
1449 
1450             ASSERT((m_type != Getter &amp;&amp; m_type != Setter) || loadedValueGPR != baseGPR);
1451             ASSERT(m_type != Setter || loadedValueGPR != valueRegsPayloadGPR);
1452 
1453             GPRReg storageGPR;
1454             if (isInlineOffset(m_offset))
1455                 storageGPR = baseForAccessGPR;
1456             else {
1457                 jit.loadPtr(
1458                     CCallHelpers::Address(baseForAccessGPR, JSObject::butterflyOffset()),
1459                     loadedValueGPR);
1460                 storageGPR = loadedValueGPR;
1461             }
1462 
1463 #if USE(JSVALUE64)
1464             jit.load64(
1465                 CCallHelpers::Address(storageGPR, offsetRelativeToBase(m_offset)), loadedValueGPR);
1466 #else
1467             if (m_type == Load || m_type == GetGetter) {
1468                 jit.load32(
1469                     CCallHelpers::Address(storageGPR, offsetRelativeToBase(m_offset) + TagOffset),
1470                     valueRegs.tagGPR());
1471             }
1472             jit.load32(
1473                 CCallHelpers::Address(storageGPR, offsetRelativeToBase(m_offset) + PayloadOffset),
1474                 loadedValueGPR);
1475 #endif
1476         }
1477 
1478         if (m_type == Load || m_type == GetGetter) {
1479             state.succeed();
1480             return;
1481         }
1482 
1483         if (m_type == CustomAccessorGetter &amp;&amp; this-&gt;as&lt;GetterSetterAccessCase&gt;().domAttribute()) {
1484             auto&amp; access = this-&gt;as&lt;GetterSetterAccessCase&gt;();
1485             // We do not need to emit CheckDOM operation since structure check ensures
1486             // that the structure of the given base value is structure()! So all we should
1487             // do is performing the CheckDOM thingy in IC compiling time here.
1488             if (!structure()-&gt;classInfo()-&gt;isSubClassOf(access.domAttribute()-&gt;classInfo)) {
1489                 state.failAndIgnore.append(jit.jump());
1490                 return;
1491             }
1492 
1493             if (Options::useDOMJIT() &amp;&amp; access.domAttribute()-&gt;domJIT) {
1494                 access.emitDOMJITGetter(state, access.domAttribute()-&gt;domJIT, baseForGetGPR);
1495                 return;
1496             }
1497         }
1498 
1499         // Stuff for custom getters/setters.
1500         CCallHelpers::Call operationCall;
1501 
1502         // Stuff for JS getters/setters.
1503         CCallHelpers::DataLabelPtr addressOfLinkFunctionCheck;
1504         CCallHelpers::Call fastPathCall;
1505         CCallHelpers::Call slowPathCall;
1506 
1507         // This also does the necessary calculations of whether or not we&#39;re an
1508         // exception handling call site.
1509         AccessGenerationState::SpillState spillState = state.preserveLiveRegistersToStackForCall();
1510 
1511         auto restoreLiveRegistersFromStackForCall = [&amp;](AccessGenerationState::SpillState&amp; spillState, bool callHasReturnValue) {
1512             RegisterSet dontRestore;
1513             if (callHasReturnValue) {
1514                 // This is the result value. We don&#39;t want to overwrite the result with what we stored to the stack.
1515                 // We sometimes have to store it to the stack just in case we throw an exception and need the original value.
1516                 dontRestore.set(valueRegs);
1517             }
1518             state.restoreLiveRegistersFromStackForCall(spillState, dontRestore);
1519         };
1520 
1521         jit.store32(
1522             CCallHelpers::TrustedImm32(state.callSiteIndexForExceptionHandlingOrOriginal().bits()),
1523             CCallHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));
1524 
1525         if (m_type == Getter || m_type == Setter) {
1526             auto&amp; access = this-&gt;as&lt;GetterSetterAccessCase&gt;();
1527             ASSERT(baseGPR != loadedValueGPR);
1528             ASSERT(m_type != Setter || valueRegsPayloadGPR != loadedValueGPR);
1529 
1530             // Create a JS call using a JS call inline cache. Assume that:
1531             //
1532             // - SP is aligned and represents the extent of the calling compiler&#39;s stack usage.
1533             //
1534             // - FP is set correctly (i.e. it points to the caller&#39;s call frame header).
1535             //
1536             // - SP - FP is an aligned difference.
1537             //
1538             // - Any byte between FP (exclusive) and SP (inclusive) could be live in the calling
1539             //   code.
1540             //
1541             // Therefore, we temporarily grow the stack for the purpose of the call and then
1542             // shrink it after.
1543 
1544             state.setSpillStateForJSGetterSetter(spillState);
1545 
1546             RELEASE_ASSERT(!access.callLinkInfo());
1547             CallLinkInfo* callLinkInfo = state.m_callLinkInfos.add();
1548             access.m_callLinkInfo = callLinkInfo;
1549 
1550             // FIXME: If we generated a polymorphic call stub that jumped back to the getter
1551             // stub, which then jumped back to the main code, then we&#39;d have a reachability
1552             // situation that the GC doesn&#39;t know about. The GC would ensure that the polymorphic
1553             // call stub stayed alive, and it would ensure that the main code stayed alive, but
1554             // it wouldn&#39;t know that the getter stub was alive. Ideally JIT stub routines would
1555             // be GC objects, and then we&#39;d be able to say that the polymorphic call stub has a
1556             // reference to the getter stub.
1557             // https://bugs.webkit.org/show_bug.cgi?id=148914
1558             callLinkInfo-&gt;disallowStubs();
1559 
1560             callLinkInfo-&gt;setUpCall(CallLinkInfo::Call, stubInfo.codeOrigin, loadedValueGPR);
1561 
1562             CCallHelpers::JumpList done;
1563 
1564             // There is a &quot;this&quot; argument.
1565             unsigned numberOfParameters = 1;
1566             // ... and a value argument if we&#39;re calling a setter.
1567             if (m_type == Setter)
1568                 numberOfParameters++;
1569 
1570             // Get the accessor; if there ain&#39;t one then the result is jsUndefined().
1571             if (m_type == Setter) {
1572                 jit.loadPtr(
1573                     CCallHelpers::Address(loadedValueGPR, GetterSetter::offsetOfSetter()),
1574                     loadedValueGPR);
1575             } else {
1576                 jit.loadPtr(
1577                     CCallHelpers::Address(loadedValueGPR, GetterSetter::offsetOfGetter()),
1578                     loadedValueGPR);
1579             }
1580 
1581             CCallHelpers::Jump returnUndefined = jit.branchTestPtr(
1582                 CCallHelpers::Zero, loadedValueGPR);
1583 
1584             unsigned numberOfRegsForCall = CallFrame::headerSizeInRegisters + roundArgumentCountToAlignFrame(numberOfParameters);
1585             ASSERT(!(numberOfRegsForCall % stackAlignmentRegisters()));
1586             unsigned numberOfBytesForCall = numberOfRegsForCall * sizeof(Register) - sizeof(CallerFrameAndPC);
1587 
1588             unsigned alignedNumberOfBytesForCall =
1589             WTF::roundUpToMultipleOf(stackAlignmentBytes(), numberOfBytesForCall);
1590 
1591             jit.subPtr(
1592                 CCallHelpers::TrustedImm32(alignedNumberOfBytesForCall),
1593                 CCallHelpers::stackPointerRegister);
1594 
1595             CCallHelpers::Address calleeFrame = CCallHelpers::Address(
1596                 CCallHelpers::stackPointerRegister,
1597                 -static_cast&lt;ptrdiff_t&gt;(sizeof(CallerFrameAndPC)));
1598 
1599             jit.store32(
1600                 CCallHelpers::TrustedImm32(numberOfParameters),
1601                 calleeFrame.withOffset(CallFrameSlot::argumentCountIncludingThis * sizeof(Register) + PayloadOffset));
1602 
1603             jit.storeCell(
1604                 loadedValueGPR, calleeFrame.withOffset(CallFrameSlot::callee * sizeof(Register)));
1605 
1606             jit.storeCell(
1607                 thisGPR,
1608                 calleeFrame.withOffset(virtualRegisterForArgumentIncludingThis(0).offset() * sizeof(Register)));
1609 
1610             if (m_type == Setter) {
1611                 jit.storeValue(
1612                     valueRegs,
1613                     calleeFrame.withOffset(
1614                         virtualRegisterForArgumentIncludingThis(1).offset() * sizeof(Register)));
1615             }
1616 
1617             CCallHelpers::Jump slowCase = jit.branchPtrWithPatch(
1618                 CCallHelpers::NotEqual, loadedValueGPR, addressOfLinkFunctionCheck,
1619                 CCallHelpers::TrustedImmPtr(nullptr));
1620 
1621             fastPathCall = jit.nearCall();
1622             if (m_type == Getter)
1623                 jit.setupResults(valueRegs);
1624             done.append(jit.jump());
1625 
1626             // FIXME: Revisit JSGlobalObject.
1627             // https://bugs.webkit.org/show_bug.cgi?id=203204
1628             slowCase.link(&amp;jit);
1629             jit.move(loadedValueGPR, GPRInfo::regT0);
1630 #if USE(JSVALUE32_64)
1631             // We *always* know that the getter/setter, if non-null, is a cell.
1632             jit.move(CCallHelpers::TrustedImm32(JSValue::CellTag), GPRInfo::regT1);
1633 #endif
1634             jit.move(CCallHelpers::TrustedImmPtr(access.callLinkInfo()), GPRInfo::regT2);
1635             jit.move(CCallHelpers::TrustedImmPtr(state.m_globalObject), GPRInfo::regT3);
1636             slowPathCall = jit.nearCall();
1637             if (m_type == Getter)
1638                 jit.setupResults(valueRegs);
1639             done.append(jit.jump());
1640 
1641             returnUndefined.link(&amp;jit);
1642             if (m_type == Getter)
1643                 jit.moveTrustedValue(jsUndefined(), valueRegs);
1644 
1645             done.link(&amp;jit);
1646 
1647             jit.addPtr(CCallHelpers::TrustedImm32((codeBlock-&gt;stackPointerOffset() * sizeof(Register)) - state.preservedReusedRegisterState.numberOfBytesPreserved - spillState.numberOfStackBytesUsedForRegisterPreservation),
1648                 GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
1649             bool callHasReturnValue = isGetter();
1650             restoreLiveRegistersFromStackForCall(spillState, callHasReturnValue);
1651 
1652             jit.addLinkTask([=, &amp;vm] (LinkBuffer&amp; linkBuffer) {
1653                 this-&gt;as&lt;GetterSetterAccessCase&gt;().callLinkInfo()-&gt;setCallLocations(
1654                     CodeLocationLabel&lt;JSInternalPtrTag&gt;(linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(slowPathCall)),
1655                     CodeLocationLabel&lt;JSInternalPtrTag&gt;(linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(addressOfLinkFunctionCheck)),
1656                     linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(fastPathCall));
1657 
1658                 linkBuffer.link(
1659                     slowPathCall,
1660                     CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.getCTIStub(linkCallThunkGenerator).code()));
1661             });
1662         } else {
1663             ASSERT(m_type == CustomValueGetter || m_type == CustomAccessorGetter || m_type == CustomValueSetter || m_type == CustomAccessorSetter);
1664 
1665             // Need to make room for the C call so any of our stack spillage isn&#39;t overwritten. It&#39;s
1666             // hard to track if someone did spillage or not, so we just assume that we always need
1667             // to make some space here.
1668             jit.makeSpaceOnStackForCCall();
1669 
1670             // Check if it is a super access
1671             GPRReg baseForCustomGetGPR = baseGPR != thisGPR ? thisGPR : baseForGetGPR;
1672 
1673             // getter: EncodedJSValue (*GetValueFunc)(JSGlobalObject*, EncodedJSValue thisValue, PropertyName);
1674             // setter: void (*PutValueFunc)(JSGlobalObject*, EncodedJSValue thisObject, EncodedJSValue value);
1675             // Custom values are passed the slotBase (the property holder), custom accessors are passed the thisVaule (reciever).
1676             // FIXME: Remove this differences in custom values and custom accessors.
1677             // https://bugs.webkit.org/show_bug.cgi?id=158014
1678             GPRReg baseForCustom = m_type == CustomValueGetter || m_type == CustomValueSetter ? baseForAccessGPR : baseForCustomGetGPR;
1679             // FIXME: Revisit JSGlobalObject.
1680             // https://bugs.webkit.org/show_bug.cgi?id=203204
1681             if (m_type == CustomValueGetter || m_type == CustomAccessorGetter) {
1682                 RELEASE_ASSERT(m_identifier);
1683                 jit.setupArguments&lt;PropertySlot::GetValueFunc&gt;(
1684                     CCallHelpers::TrustedImmPtr(codeBlock-&gt;globalObject()),
1685                     CCallHelpers::CellValue(baseForCustom),
1686                     CCallHelpers::TrustedImmPtr(uid()));
1687             } else {
1688                 jit.setupArguments&lt;PutPropertySlot::PutValueFunc&gt;(
1689                     CCallHelpers::TrustedImmPtr(codeBlock-&gt;globalObject()),
1690                     CCallHelpers::CellValue(baseForCustom),
1691                     valueRegs);
1692             }
1693             jit.storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);
1694 
1695             operationCall = jit.call(OperationPtrTag);
1696             jit.addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
1697                 linkBuffer.link(operationCall, this-&gt;as&lt;GetterSetterAccessCase&gt;().m_customAccessor);
1698             });
1699 
1700             if (m_type == CustomValueGetter || m_type == CustomAccessorGetter)
1701                 jit.setupResults(valueRegs);
1702             jit.reclaimSpaceOnStackForCCall();
1703 
1704             CCallHelpers::Jump noException =
1705             jit.emitExceptionCheck(vm, CCallHelpers::InvertedExceptionCheck);
1706 
1707             state.restoreLiveRegistersFromStackForCallWithThrownException(spillState);
1708             state.emitExplicitExceptionHandler();
1709 
1710             noException.link(&amp;jit);
1711             bool callHasReturnValue = isGetter();
1712             restoreLiveRegistersFromStackForCall(spillState, callHasReturnValue);
1713         }
1714         state.succeed();
1715         return;
1716     }
1717 
1718     case Replace: {
1719         if (isInlineOffset(m_offset)) {
1720             jit.storeValue(
1721                 valueRegs,
1722                 CCallHelpers::Address(
1723                     baseGPR,
1724                     JSObject::offsetOfInlineStorage() +
1725                     offsetInInlineStorage(m_offset) * sizeof(JSValue)));
1726         } else {
1727             jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
1728             jit.storeValue(
1729                 valueRegs,
1730                 CCallHelpers::Address(
1731                     scratchGPR, offsetInButterfly(m_offset) * sizeof(JSValue)));
1732         }
1733         state.succeed();
1734         return;
1735     }
1736 
1737     case Transition: {
1738         // AccessCase::transition() should have returned null if this wasn&#39;t true.
1739         RELEASE_ASSERT(GPRInfo::numberOfRegisters &gt;= 6 || !structure()-&gt;outOfLineCapacity() || structure()-&gt;outOfLineCapacity() == newStructure()-&gt;outOfLineCapacity());
1740 
1741         // NOTE: This logic is duplicated in AccessCase::doesCalls(). It&#39;s important that doesCalls() knows
1742         // exactly when this would make calls.
1743         bool allocating = newStructure()-&gt;outOfLineCapacity() != structure()-&gt;outOfLineCapacity();
1744         bool reallocating = allocating &amp;&amp; structure()-&gt;outOfLineCapacity();
1745         bool allocatingInline = allocating &amp;&amp; !structure()-&gt;couldHaveIndexingHeader();
1746 
1747         ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
1748         allocator.lock(stubInfo.baseRegs());
1749         allocator.lock(valueRegs);
1750         allocator.lock(scratchGPR);
1751 
1752         GPRReg scratchGPR2 = InvalidGPRReg;
1753         GPRReg scratchGPR3 = InvalidGPRReg;
1754         if (allocatingInline) {
1755             scratchGPR2 = allocator.allocateScratchGPR();
1756             scratchGPR3 = allocator.allocateScratchGPR();
1757         }
1758 
1759         ScratchRegisterAllocator::PreservedState preservedState =
1760             allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::SpaceForCCall);
1761 
1762         CCallHelpers::JumpList slowPath;
1763 
1764         ASSERT(structure()-&gt;transitionWatchpointSetHasBeenInvalidated());
1765 
1766         if (allocating) {
1767             size_t newSize = newStructure()-&gt;outOfLineCapacity() * sizeof(JSValue);
1768 
1769             if (allocatingInline) {
1770                 Allocator allocator = vm.jsValueGigacageAuxiliarySpace.allocatorFor(newSize, AllocatorForMode::AllocatorIfExists);
1771 
1772                 jit.emitAllocate(scratchGPR, JITAllocator::constant(allocator), scratchGPR2, scratchGPR3, slowPath);
1773                 jit.addPtr(CCallHelpers::TrustedImm32(newSize + sizeof(IndexingHeader)), scratchGPR);
1774 
1775                 size_t oldSize = structure()-&gt;outOfLineCapacity() * sizeof(JSValue);
1776                 ASSERT(newSize &gt; oldSize);
1777 
1778                 if (reallocating) {
1779                     // Handle the case where we are reallocating (i.e. the old structure/butterfly
1780                     // already had out-of-line property storage).
1781 
1782                     jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR3);
1783 
1784                     // We have scratchGPR = new storage, scratchGPR3 = old storage,
1785                     // scratchGPR2 = available
1786                     for (size_t offset = 0; offset &lt; oldSize; offset += sizeof(void*)) {
1787                         jit.loadPtr(
1788                             CCallHelpers::Address(
1789                                 scratchGPR3,
1790                                 -static_cast&lt;ptrdiff_t&gt;(
1791                                     offset + sizeof(JSValue) + sizeof(void*))),
1792                             scratchGPR2);
1793                         jit.storePtr(
1794                             scratchGPR2,
1795                             CCallHelpers::Address(
1796                                 scratchGPR,
1797                                 -static_cast&lt;ptrdiff_t&gt;(offset + sizeof(JSValue) + sizeof(void*))));
1798                     }
1799                 }
1800 
1801                 for (size_t offset = oldSize; offset &lt; newSize; offset += sizeof(void*))
1802                     jit.storePtr(CCallHelpers::TrustedImmPtr(nullptr), CCallHelpers::Address(scratchGPR, -static_cast&lt;ptrdiff_t&gt;(offset + sizeof(JSValue) + sizeof(void*))));
1803             } else {
1804                 // Handle the case where we are allocating out-of-line using an operation.
1805                 RegisterSet extraRegistersToPreserve;
1806                 extraRegistersToPreserve.set(baseGPR);
1807                 extraRegistersToPreserve.set(valueRegs);
1808                 AccessGenerationState::SpillState spillState = state.preserveLiveRegistersToStackForCall(extraRegistersToPreserve);
1809 
1810                 jit.store32(
1811                     CCallHelpers::TrustedImm32(
1812                         state.callSiteIndexForExceptionHandlingOrOriginal().bits()),
1813                     CCallHelpers::tagFor(CallFrameSlot::argumentCountIncludingThis));
1814 
1815                 jit.makeSpaceOnStackForCCall();
1816 
1817                 if (!reallocating) {
1818                     jit.setupArguments&lt;decltype(operationReallocateButterflyToHavePropertyStorageWithInitialCapacity)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm), baseGPR);
1819                     jit.prepareCallOperation(vm);
1820 
1821                     CCallHelpers::Call operationCall = jit.call(OperationPtrTag);
1822                     jit.addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
1823                         linkBuffer.link(
1824                             operationCall,
1825                             FunctionPtr&lt;OperationPtrTag&gt;(operationReallocateButterflyToHavePropertyStorageWithInitialCapacity));
1826                     });
1827                 } else {
1828                     // Handle the case where we are reallocating (i.e. the old structure/butterfly
1829                     // already had out-of-line property storage).
1830                     jit.setupArguments&lt;decltype(operationReallocateButterflyToGrowPropertyStorage)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm), baseGPR, CCallHelpers::TrustedImm32(newSize / sizeof(JSValue)));
1831                     jit.prepareCallOperation(vm);
1832 
1833                     CCallHelpers::Call operationCall = jit.call(OperationPtrTag);
1834                     jit.addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
1835                         linkBuffer.link(
1836                             operationCall,
1837                             FunctionPtr&lt;OperationPtrTag&gt;(operationReallocateButterflyToGrowPropertyStorage));
1838                     });
1839                 }
1840 
1841                 jit.reclaimSpaceOnStackForCCall();
1842                 jit.move(GPRInfo::returnValueGPR, scratchGPR);
1843 
1844                 CCallHelpers::Jump noException = jit.emitExceptionCheck(vm, CCallHelpers::InvertedExceptionCheck);
1845 
1846                 state.restoreLiveRegistersFromStackForCallWithThrownException(spillState);
1847                 state.emitExplicitExceptionHandler();
1848 
1849                 noException.link(&amp;jit);
1850                 RegisterSet resultRegisterToExclude;
1851                 resultRegisterToExclude.set(scratchGPR);
1852                 state.restoreLiveRegistersFromStackForCall(spillState, resultRegisterToExclude);
1853             }
1854         }
1855 
1856         if (isInlineOffset(m_offset)) {
1857             jit.storeValue(
1858                 valueRegs,
1859                 CCallHelpers::Address(
1860                     baseGPR,
1861                     JSObject::offsetOfInlineStorage() +
1862                     offsetInInlineStorage(m_offset) * sizeof(JSValue)));
1863         } else {
1864             if (!allocating)
1865                 jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
1866             jit.storeValue(
1867                 valueRegs,
1868                 CCallHelpers::Address(scratchGPR, offsetInButterfly(m_offset) * sizeof(JSValue)));
1869         }
1870 
1871         if (allocatingInline) {
1872             // If we were to have any indexed properties, then we would need to update the indexing mask on the base object.
1873             RELEASE_ASSERT(!newStructure()-&gt;couldHaveIndexingHeader());
1874             // We set the new butterfly and the structure last. Doing it this way ensures that
1875             // whatever we had done up to this point is forgotten if we choose to branch to slow
1876             // path.
1877             jit.nukeStructureAndStoreButterfly(vm, scratchGPR, baseGPR);
1878         }
1879 
1880         uint32_t structureBits = bitwise_cast&lt;uint32_t&gt;(newStructure()-&gt;id());
1881         jit.store32(
1882             CCallHelpers::TrustedImm32(structureBits),
1883             CCallHelpers::Address(baseGPR, JSCell::structureIDOffset()));
1884 
1885         allocator.restoreReusedRegistersByPopping(jit, preservedState);
1886         state.succeed();
1887 
1888         // We will have a slow path if we were allocating without the help of an operation.
1889         if (allocatingInline) {
1890             if (allocator.didReuseRegisters()) {
1891                 slowPath.link(&amp;jit);
1892                 allocator.restoreReusedRegistersByPopping(jit, preservedState);
1893                 state.failAndIgnore.append(jit.jump());
1894             } else
1895                 state.failAndIgnore.append(slowPath);
1896         } else
1897             RELEASE_ASSERT(slowPath.empty());
1898         return;
1899     }
1900 
1901     case ArrayLength: {
1902         jit.loadPtr(CCallHelpers::Address(baseGPR, JSObject::butterflyOffset()), scratchGPR);
1903         jit.load32(CCallHelpers::Address(scratchGPR, ArrayStorage::lengthOffset()), scratchGPR);
1904         state.failAndIgnore.append(
1905             jit.branch32(CCallHelpers::LessThan, scratchGPR, CCallHelpers::TrustedImm32(0)));
1906         jit.boxInt32(scratchGPR, valueRegs);
1907         state.succeed();
1908         return;
1909     }
1910 
1911     case StringLength: {
1912         jit.loadPtr(CCallHelpers::Address(baseGPR, JSString::offsetOfValue()), scratchGPR);
1913         auto isRope = jit.branchIfRopeStringImpl(scratchGPR);
1914         jit.load32(CCallHelpers::Address(scratchGPR, StringImpl::lengthMemoryOffset()), valueRegs.payloadGPR());
1915         auto done = jit.jump();
1916 
1917         isRope.link(&amp;jit);
1918         jit.load32(CCallHelpers::Address(baseGPR, JSRopeString::offsetOfLength()), valueRegs.payloadGPR());
1919 
1920         done.link(&amp;jit);
1921         jit.boxInt32(valueRegs.payloadGPR(), valueRegs);
1922         state.succeed();
1923         return;
1924     }
1925 
1926     case IntrinsicGetter: {
1927         RELEASE_ASSERT(isValidOffset(offset()));
1928 
1929         // We need to ensure the getter value does not move from under us. Note that GetterSetters
1930         // are immutable so we just need to watch the property not any value inside it.
1931         Structure* currStructure;
1932         if (!hasAlternateBase())
1933             currStructure = structure();
1934         else
1935             currStructure = alternateBase()-&gt;structure(vm);
1936         currStructure-&gt;startWatchingPropertyForReplacements(vm, offset());
1937 
1938         this-&gt;as&lt;IntrinsicGetterAccessCase&gt;().emitIntrinsicGetter(state);
1939         return;
1940     }
1941 
1942     case DirectArgumentsLength:
1943     case ScopedArgumentsLength:
1944     case ModuleNamespaceLoad:
1945     case InstanceOfGeneric:
1946     case IndexedInt32Load:
1947     case IndexedDoubleLoad:
1948     case IndexedContiguousLoad:
1949     case IndexedArrayStorageLoad:
1950     case IndexedScopedArgumentsLoad:
1951     case IndexedDirectArgumentsLoad:
1952     case IndexedTypedArrayInt8Load:
1953     case IndexedTypedArrayUint8Load:
1954     case IndexedTypedArrayUint8ClampedLoad:
1955     case IndexedTypedArrayInt16Load:
1956     case IndexedTypedArrayUint16Load:
1957     case IndexedTypedArrayInt32Load:
1958     case IndexedTypedArrayUint32Load:
1959     case IndexedTypedArrayFloat32Load:
1960     case IndexedTypedArrayFloat64Load:
1961     case IndexedStringLoad:
1962         // These need to be handled by generateWithGuard(), since the guard is part of the
1963         // algorithm. We can be sure that nobody will call generate() directly for these since they
1964         // are not guarded by structure checks.
1965         RELEASE_ASSERT_NOT_REACHED();
1966     }
1967 
1968     RELEASE_ASSERT_NOT_REACHED();
1969 }
1970 
1971 TypedArrayType AccessCase::toTypedArrayType(AccessType accessType)
1972 {
1973     switch (accessType) {
1974     case IndexedTypedArrayInt8Load:
1975         return TypeInt8;
1976     case IndexedTypedArrayUint8Load:
1977         return TypeUint8;
1978     case IndexedTypedArrayUint8ClampedLoad:
1979         return TypeUint8Clamped;
1980     case IndexedTypedArrayInt16Load:
1981         return TypeInt16;
1982     case IndexedTypedArrayUint16Load:
1983         return TypeUint16;
1984     case IndexedTypedArrayInt32Load:
1985         return TypeInt32;
1986     case IndexedTypedArrayUint32Load:
1987         return TypeUint32;
1988     case IndexedTypedArrayFloat32Load:
1989         return TypeFloat32;
1990     case IndexedTypedArrayFloat64Load:
1991         return TypeFloat64;
1992     default:
1993         RELEASE_ASSERT_NOT_REACHED();
1994     }
1995 }
1996 
1997 #if ASSERT_ENABLED
1998 void AccessCase::checkConsistency(StructureStubInfo&amp; stubInfo)
1999 {
2000     RELEASE_ASSERT(!(requiresInt32PropertyCheck() &amp;&amp; requiresIdentifierNameMatch()));
2001 
2002     if (stubInfo.hasConstantIdentifier) {
2003         RELEASE_ASSERT(!requiresInt32PropertyCheck());
2004         RELEASE_ASSERT(requiresIdentifierNameMatch());
2005     }
2006 }
2007 #endif // ASSERT_ENABLED
2008 
2009 } // namespace JSC
2010 
2011 #endif
    </pre>
  </body>
</html>