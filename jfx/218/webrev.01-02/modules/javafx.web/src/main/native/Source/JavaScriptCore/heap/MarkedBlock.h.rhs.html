<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  *  Copyright (C) 1999-2000 Harri Porten (porten@kde.org)
  3  *  Copyright (C) 2001 Peter Kelly (pmk@post.com)
  4  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.
  5  *
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;CellAttributes.h&quot;
 25 #include &quot;DestructionMode.h&quot;
 26 #include &quot;HeapCell.h&quot;
 27 #include &quot;IterationStatus.h&quot;
 28 #include &quot;WeakSet.h&quot;
<a name="1" id="anc1"></a><span class="line-added"> 29 #include &lt;algorithm&gt;</span>
 30 #include &lt;wtf/Atomics.h&gt;
 31 #include &lt;wtf/Bitmap.h&gt;
<a name="2" id="anc2"></a>
 32 #include &lt;wtf/CountingLock.h&gt;
<a name="3" id="anc3"></a><span class="line-added"> 33 #include &lt;wtf/HashFunctions.h&gt;</span>
<span class="line-added"> 34 #include &lt;wtf/PageBlock.h&gt;</span>
 35 #include &lt;wtf/StdLibExtras.h&gt;
 36 
 37 namespace JSC {
 38 
 39 class AlignedMemoryAllocator;
 40 class FreeList;
 41 class Heap;
 42 class JSCell;
 43 class BlockDirectory;
 44 class MarkedSpace;
 45 class SlotVisitor;
 46 class Subspace;
 47 
 48 typedef uint32_t HeapVersion;
 49 
 50 // A marked block is a page-aligned container for heap-allocated objects.
 51 // Objects are allocated within cells of the marked block. For a given
 52 // marked block, all cells have the same size. Objects smaller than the
 53 // cell size may be allocated in the marked block, in which case the
 54 // allocation suffers from internal fragmentation: wasted space whose
 55 // size is equal to the difference between the cell size and the object
 56 // size.
<a name="4" id="anc4"></a><span class="line-modified"> 57 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MarkedBlock);</span>
<span class="line-added"> 58 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(MarkedBlockHandle);</span>
 59 class MarkedBlock {
 60     WTF_MAKE_NONCOPYABLE(MarkedBlock);
<a name="5" id="anc5"></a><span class="line-added"> 61     WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(MarkedBlock);</span>
 62     friend class LLIntOffsetsExtractor;
 63     friend struct VerifyMarked;
 64 
 65 public:
 66     class Footer;
 67     class Handle;
 68 private:
 69     friend class Footer;
 70     friend class Handle;
 71 public:
 72     static constexpr size_t atomSize = 16; // bytes
 73 
 74     // Block size must be at least as large as the system page size.
<a name="6" id="anc6"></a><span class="line-modified"> 75     static constexpr size_t blockSize = std::max(16 * KB, CeilingOnPageSize);</span>




 76 
 77     static constexpr size_t blockMask = ~(blockSize - 1); // blockSize must be a power of two.
 78 
 79     static constexpr size_t atomsPerBlock = blockSize / atomSize;
 80 
<a name="7" id="anc7"></a><span class="line-added"> 81     static constexpr size_t maxNumberOfLowerTierCells = 8;</span>
<span class="line-added"> 82     static_assert(maxNumberOfLowerTierCells &lt;= 256);</span>
<span class="line-added"> 83 </span>
 84     static_assert(!(MarkedBlock::atomSize &amp; (MarkedBlock::atomSize - 1)), &quot;MarkedBlock::atomSize must be a power of two.&quot;);
 85     static_assert(!(MarkedBlock::blockSize &amp; (MarkedBlock::blockSize - 1)), &quot;MarkedBlock::blockSize must be a power of two.&quot;);
 86 
 87     struct VoidFunctor {
 88         typedef void ReturnType;
 89         void returnValue() { }
 90     };
 91 
 92     class CountFunctor {
 93     public:
 94         typedef size_t ReturnType;
 95 
 96         CountFunctor() : m_count(0) { }
 97         void count(size_t count) const { m_count += count; }
 98         ReturnType returnValue() const { return m_count; }
 99 
100     private:
101         // FIXME: This is mutable because we&#39;re using a functor rather than C++ lambdas.
102         // https://bugs.webkit.org/show_bug.cgi?id=159644
103         mutable ReturnType m_count;
104     };
105 
106     class Handle {
107         WTF_MAKE_NONCOPYABLE(Handle);
<a name="8" id="anc8"></a><span class="line-modified">108         WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(MarkedBlockHandle);</span>
109         friend class LLIntOffsetsExtractor;
110         friend class MarkedBlock;
111         friend struct VerifyMarked;
112     public:
113 
114         ~Handle();
115 
116         MarkedBlock&amp; block();
117         MarkedBlock::Footer&amp; blockFooter();
118 
119         void* cellAlign(void*);
120 
121         bool isEmpty();
122 
123         void lastChanceToFinalize();
124 
125         BlockDirectory* directory() const;
126         Subspace* subspace() const;
127         AlignedMemoryAllocator* alignedMemoryAllocator() const;
128         Heap* heap() const;
129         inline MarkedSpace* space() const;
130         VM&amp; vm() const;
131         WeakSet&amp; weakSet();
132 
133         enum SweepMode { SweepOnly, SweepToFreeList };
134 
135         // Sweeping ensures that destructors get called and removes the block from the unswept
136         // set. Sweeping to free list also removes the block from the empty set, if it was in that
137         // set. Sweeping with SweepOnly may add this block to the empty set, if the block is found
138         // to be empty. The free-list being null implies SweepOnly.
139         //
140         // Note that you need to make sure that the empty bit reflects reality. If it&#39;s not set
141         // and the block is freshly created, then we&#39;ll make the mistake of running destructors in
142         // the block. If it&#39;s not set and the block has nothing marked, then we&#39;ll make the
143         // mistake of making a pop freelist rather than a bump freelist.
144         void sweep(FreeList*);
145 
146         // This is to be called by Subspace.
147         template&lt;typename DestroyFunc&gt;
148         void finishSweepKnowingHeapCellType(FreeList*, const DestroyFunc&amp;);
149 
150         void unsweepWithNoNewlyAllocated();
151 
152         void shrink();
153 
154         void visitWeakSet(SlotVisitor&amp;);
155         void reapWeakSet();
156 
157         // While allocating from a free list, MarkedBlock temporarily has bogus
158         // cell liveness data. To restore accurate cell liveness data, call one
159         // of these functions:
160         void didConsumeFreeList(); // Call this once you&#39;ve allocated all the items in the free list.
161         void stopAllocating(const FreeList&amp;);
162         void resumeAllocating(FreeList&amp;); // Call this if you canonicalized a block for some non-collection related purpose.
163 
164         size_t cellSize();
165         inline unsigned cellsPerBlock();
166 
167         const CellAttributes&amp; attributes() const;
168         DestructionMode destruction() const;
169         bool needsDestruction() const;
170         HeapCell::Kind cellKind() const;
171 
172         size_t markCount();
173         size_t size();
174 
175         bool isAllocated();
176 
177         bool isLive(HeapVersion markingVersion, HeapVersion newlyAllocatedVersion, bool isMarking, const HeapCell*);
178         inline bool isLiveCell(HeapVersion markingVersion, HeapVersion newlyAllocatedVersion, bool isMarking, const void*);
179 
180         bool isLive(const HeapCell*);
181         bool isLiveCell(const void*);
182 
183         bool isFreeListedCell(const void* target) const;
184 
185         template &lt;typename Functor&gt; IterationStatus forEachCell(const Functor&amp;);
186         template &lt;typename Functor&gt; inline IterationStatus forEachLiveCell(const Functor&amp;);
187         template &lt;typename Functor&gt; inline IterationStatus forEachDeadCell(const Functor&amp;);
188         template &lt;typename Functor&gt; inline IterationStatus forEachMarkedCell(const Functor&amp;);
189 
190         JS_EXPORT_PRIVATE bool areMarksStale();
191         bool areMarksStaleForSweep();
192 
193         void assertMarksNotStale();
194 
195         bool isFreeListed() const { return m_isFreeListed; }
196 
<a name="9" id="anc9"></a><span class="line-modified">197         unsigned index() const { return m_index; }</span>
198 
199         void removeFromDirectory();
200 
<a name="10" id="anc10"></a><span class="line-modified">201         void didAddToDirectory(BlockDirectory*, unsigned index);</span>
202         void didRemoveFromDirectory();
203 
204         void* start() const { return &amp;m_block-&gt;atoms()[0]; }
205         void* end() const { return &amp;m_block-&gt;atoms()[m_endAtom]; }
206         bool contains(void* p) const { return start() &lt;= p &amp;&amp; p &lt; end(); }
207 
208         void dumpState(PrintStream&amp;);
209 
210     private:
211         Handle(Heap&amp;, AlignedMemoryAllocator*, void*);
212 
213         enum SweepDestructionMode { BlockHasNoDestructors, BlockHasDestructors, BlockHasDestructorsAndCollectorIsRunning };
214         enum ScribbleMode { DontScribble, Scribble };
215         enum EmptyMode { IsEmpty, NotEmpty };
216         enum NewlyAllocatedMode { HasNewlyAllocated, DoesNotHaveNewlyAllocated };
217         enum MarksMode { MarksStale, MarksNotStale };
218 
219         SweepDestructionMode sweepDestructionMode();
220         EmptyMode emptyMode();
221         ScribbleMode scribbleMode();
222         NewlyAllocatedMode newlyAllocatedMode();
223         MarksMode marksMode();
224 
225         template&lt;bool, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, typename DestroyFunc&gt;
226         void specializedSweep(FreeList*, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, const DestroyFunc&amp;);
227 
228         void setIsFreeListed();
229 
<a name="11" id="anc11"></a><span class="line-modified">230         unsigned m_atomsPerCell { std::numeric_limits&lt;unsigned&gt;::max() };</span>
<span class="line-modified">231         unsigned m_endAtom { std::numeric_limits&lt;unsigned&gt;::max() }; // This is a fuzzy end. Always test for &lt; m_endAtom.</span>



232 
233         CellAttributes m_attributes;
234         bool m_isFreeListed { false };
<a name="12" id="anc12"></a><span class="line-added">235         unsigned m_index { std::numeric_limits&lt;unsigned&gt;::max() };</span>
236 
237         AlignedMemoryAllocator* m_alignedMemoryAllocator { nullptr };
238         BlockDirectory* m_directory { nullptr };
<a name="13" id="anc13"></a>
239         WeakSet m_weakSet;
240 
241         MarkedBlock* m_block { nullptr };
242     };
243 
244 private:
245     static constexpr size_t atomAlignmentMask = atomSize - 1;
246 
247     typedef char Atom[atomSize];
248 
249 public:
250     class Footer {
251     public:
252         Footer(VM&amp;, Handle&amp;);
253         ~Footer();
254 
255     private:
256         friend class LLIntOffsetsExtractor;
257         friend class MarkedBlock;
258 
259         Handle&amp; m_handle;
260         // m_vm must remain a pointer (instead of a reference) because JSCLLIntOffsetsExtractor
261         // will fail otherwise.
262         VM* m_vm;
263         Subspace* m_subspace;
264 
265         CountingLock m_lock;
266 
267         // The actual mark count can be computed by doing: m_biasedMarkCount - m_markCountBias. Note
268         // that this count is racy. It will accurately detect whether or not exactly zero things were
269         // marked, but if N things got marked, then this may report anything in the range [1, N] (or
270         // before unbiased, it would be [1 + m_markCountBias, N + m_markCountBias].)
271         int16_t m_biasedMarkCount;
272 
273         // We bias the mark count so that if m_biasedMarkCount &gt;= 0 then the block should be retired.
274         // We go to all this trouble to make marking a bit faster: this way, marking knows when to
275         // retire a block using a js/jns on m_biasedMarkCount.
276         //
277         // For example, if a block has room for 100 objects and retirement happens whenever 90% are
278         // live, then m_markCountBias will be -90. This way, when marking begins, this will cause us to
279         // set m_biasedMarkCount to -90 as well, since:
280         //
281         //     m_biasedMarkCount = actualMarkCount + m_markCountBias.
282         //
283         // Marking an object will increment m_biasedMarkCount. Once 90 objects get marked, we will have
284         // m_biasedMarkCount = 0, which will trigger retirement. In other words, we want to set
285         // m_markCountBias like so:
286         //
287         //     m_markCountBias = -(minMarkedBlockUtilization * cellsPerBlock)
288         //
289         // All of this also means that you can detect if any objects are marked by doing:
290         //
291         //     m_biasedMarkCount != m_markCountBias
292         int16_t m_markCountBias;
293 
294         HeapVersion m_markingVersion;
295         HeapVersion m_newlyAllocatedVersion;
296 
297         Bitmap&lt;atomsPerBlock&gt; m_marks;
298         Bitmap&lt;atomsPerBlock&gt; m_newlyAllocated;
299     };
300 
301 private:
302     Footer&amp; footer();
303     const Footer&amp; footer() const;
304 
305 public:
306     static constexpr size_t endAtom = (blockSize - sizeof(Footer)) / atomSize;
307     static constexpr size_t payloadSize = endAtom * atomSize;
308     static constexpr size_t footerSize = blockSize - payloadSize;
309 
310     static_assert(payloadSize == ((blockSize - sizeof(MarkedBlock::Footer)) &amp; ~(atomSize - 1)), &quot;Payload size computed the alternate way should give the same result&quot;);
<a name="14" id="anc14"></a>


311 
312     static MarkedBlock::Handle* tryCreate(Heap&amp;, AlignedMemoryAllocator*);
313 
314     Handle&amp; handle();
315     const Handle&amp; handle() const;
316 
317     VM&amp; vm() const;
318     inline Heap* heap() const;
319     inline MarkedSpace* space() const;
320 
321     static bool isAtomAligned(const void*);
322     static MarkedBlock* blockFor(const void*);
<a name="15" id="anc15"></a><span class="line-modified">323     unsigned atomNumber(const void*);</span>
<span class="line-added">324     size_t candidateAtomNumber(const void*);</span>
325 
326     size_t markCount();
327 
328     bool isMarked(const void*);
329     bool isMarked(HeapVersion markingVersion, const void*);
330     bool isMarked(const void*, Dependency);
331     bool testAndSetMarked(const void*, Dependency);
332 
333     bool isAtom(const void*);
334     void clearMarked(const void*);
335 
336     bool isNewlyAllocated(const void*);
337     void setNewlyAllocated(const void*);
338     void clearNewlyAllocated(const void*);
339     const Bitmap&lt;atomsPerBlock&gt;&amp; newlyAllocated() const;
340 
341     HeapVersion newlyAllocatedVersion() const { return footer().m_newlyAllocatedVersion; }
342 
343     inline bool isNewlyAllocatedStale() const;
344 
345     inline bool hasAnyNewlyAllocated();
346     void resetAllocated();
347 
348     size_t cellSize();
349     const CellAttributes&amp; attributes() const;
350 
351     bool hasAnyMarked() const;
352     void noteMarked();
<a name="16" id="anc16"></a><span class="line-modified">353 #if ASSERT_ENABLED</span>


354     void assertValidCell(VM&amp;, HeapCell*) const;
<a name="17" id="anc17"></a><span class="line-added">355 #else</span>
<span class="line-added">356     void assertValidCell(VM&amp;, HeapCell*) const { }</span>
357 #endif
358 
359     WeakSet&amp; weakSet();
360 
361     JS_EXPORT_PRIVATE bool areMarksStale();
362     bool areMarksStale(HeapVersion markingVersion);
363 
364     Dependency aboutToMark(HeapVersion markingVersion);
365 
<a name="18" id="anc18"></a><span class="line-modified">366 #if ASSERT_ENABLED</span>


367     JS_EXPORT_PRIVATE void assertMarksNotStale();
<a name="19" id="anc19"></a><span class="line-added">368 #else</span>
<span class="line-added">369     void assertMarksNotStale() { }</span>
370 #endif
371 
372     void resetMarks();
373 
374     bool isMarkedRaw(const void* p);
375     HeapVersion markingVersion() const { return footer().m_markingVersion; }
376 
377     const Bitmap&lt;atomsPerBlock&gt;&amp; marks() const;
378 
379     CountingLock&amp; lock() { return footer().m_lock; }
380 
381     Subspace* subspace() const { return footer().m_subspace; }
382 
383     void populatePage() const
384     {
385         *bitwise_cast&lt;volatile uint8_t*&gt;(&amp;footer());
386     }
387 
388     static constexpr size_t offsetOfFooter = endAtom * atomSize;
389 
390 private:
391     MarkedBlock(VM&amp;, Handle&amp;);
392     ~MarkedBlock();
393     Atom* atoms();
394 
395     JS_EXPORT_PRIVATE void aboutToMarkSlow(HeapVersion markingVersion);
396     void clearHasAnyMarked();
397 
398     void noteMarkedSlow();
399 
400     inline bool marksConveyLivenessDuringMarking(HeapVersion markingVersion);
401     inline bool marksConveyLivenessDuringMarking(HeapVersion myMarkingVersion, HeapVersion markingVersion);
402 };
403 
404 inline MarkedBlock::Footer&amp; MarkedBlock::footer()
405 {
406     return *bitwise_cast&lt;MarkedBlock::Footer*&gt;(atoms() + endAtom);
407 }
408 
409 inline const MarkedBlock::Footer&amp; MarkedBlock::footer() const
410 {
411     return const_cast&lt;MarkedBlock*&gt;(this)-&gt;footer();
412 }
413 
414 inline MarkedBlock::Handle&amp; MarkedBlock::handle()
415 {
416     return footer().m_handle;
417 }
418 
419 inline const MarkedBlock::Handle&amp; MarkedBlock::handle() const
420 {
421     return const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle();
422 }
423 
424 inline MarkedBlock&amp; MarkedBlock::Handle::block()
425 {
426     return *m_block;
427 }
428 
429 inline MarkedBlock::Footer&amp; MarkedBlock::Handle::blockFooter()
430 {
431     return block().footer();
432 }
433 
434 inline MarkedBlock::Atom* MarkedBlock::atoms()
435 {
436     return reinterpret_cast&lt;Atom*&gt;(this);
437 }
438 
439 inline bool MarkedBlock::isAtomAligned(const void* p)
440 {
441     return !(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; atomAlignmentMask);
442 }
443 
444 inline void* MarkedBlock::Handle::cellAlign(void* p)
445 {
446     uintptr_t base = reinterpret_cast&lt;uintptr_t&gt;(block().atoms());
447     uintptr_t bits = reinterpret_cast&lt;uintptr_t&gt;(p);
448     bits -= base;
449     bits -= bits % cellSize();
450     bits += base;
451     return reinterpret_cast&lt;void*&gt;(bits);
452 }
453 
454 inline MarkedBlock* MarkedBlock::blockFor(const void* p)
455 {
456     return reinterpret_cast&lt;MarkedBlock*&gt;(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; blockMask);
457 }
458 
459 inline BlockDirectory* MarkedBlock::Handle::directory() const
460 {
461     return m_directory;
462 }
463 
464 inline AlignedMemoryAllocator* MarkedBlock::Handle::alignedMemoryAllocator() const
465 {
466     return m_alignedMemoryAllocator;
467 }
468 
469 inline Heap* MarkedBlock::Handle::heap() const
470 {
471     return m_weakSet.heap();
472 }
473 
474 inline VM&amp; MarkedBlock::Handle::vm() const
475 {
476     return m_weakSet.vm();
477 }
478 
479 inline VM&amp; MarkedBlock::vm() const
480 {
481     return *footer().m_vm;
482 }
483 
484 inline WeakSet&amp; MarkedBlock::Handle::weakSet()
485 {
486     return m_weakSet;
487 }
488 
489 inline WeakSet&amp; MarkedBlock::weakSet()
490 {
491     return handle().weakSet();
492 }
493 
494 inline void MarkedBlock::Handle::shrink()
495 {
496     m_weakSet.shrink();
497 }
498 
499 inline void MarkedBlock::Handle::visitWeakSet(SlotVisitor&amp; visitor)
500 {
501     return m_weakSet.visit(visitor);
502 }
503 
504 inline void MarkedBlock::Handle::reapWeakSet()
505 {
506     m_weakSet.reap();
507 }
508 
509 inline size_t MarkedBlock::Handle::cellSize()
510 {
511     return m_atomsPerCell * atomSize;
512 }
513 
514 inline size_t MarkedBlock::cellSize()
515 {
516     return handle().cellSize();
517 }
518 
519 inline const CellAttributes&amp; MarkedBlock::Handle::attributes() const
520 {
521     return m_attributes;
522 }
523 
524 inline const CellAttributes&amp; MarkedBlock::attributes() const
525 {
526     return handle().attributes();
527 }
528 
529 inline bool MarkedBlock::Handle::needsDestruction() const
530 {
531     return m_attributes.destruction == NeedsDestruction;
532 }
533 
534 inline DestructionMode MarkedBlock::Handle::destruction() const
535 {
536     return m_attributes.destruction;
537 }
538 
539 inline HeapCell::Kind MarkedBlock::Handle::cellKind() const
540 {
541     return m_attributes.cellKind;
542 }
543 
544 inline size_t MarkedBlock::Handle::markCount()
545 {
546     return m_block-&gt;markCount();
547 }
548 
549 inline size_t MarkedBlock::Handle::size()
550 {
551     return markCount() * cellSize();
552 }
553 
<a name="20" id="anc20"></a><span class="line-modified">554 inline size_t MarkedBlock::candidateAtomNumber(const void* p)</span>
555 {
<a name="21" id="anc21"></a><span class="line-added">556     // This function must return size_t instead of unsigned since pointer |p| is not guaranteed that this is within MarkedBlock.</span>
<span class="line-added">557     // See MarkedBlock::isAtom which can accept out-of-bound pointers.</span>
558     return (reinterpret_cast&lt;uintptr_t&gt;(p) - reinterpret_cast&lt;uintptr_t&gt;(this)) / atomSize;
559 }
560 
<a name="22" id="anc22"></a><span class="line-added">561 inline unsigned MarkedBlock::atomNumber(const void* p)</span>
<span class="line-added">562 {</span>
<span class="line-added">563     size_t atomNumber = candidateAtomNumber(p);</span>
<span class="line-added">564     ASSERT(atomNumber &lt; handle().m_endAtom);</span>
<span class="line-added">565     return atomNumber;</span>
<span class="line-added">566 }</span>
<span class="line-added">567 </span>
568 inline bool MarkedBlock::areMarksStale(HeapVersion markingVersion)
569 {
570     return markingVersion != footer().m_markingVersion;
571 }
572 
573 inline Dependency MarkedBlock::aboutToMark(HeapVersion markingVersion)
574 {
575     HeapVersion version = footer().m_markingVersion;
576     if (UNLIKELY(version != markingVersion))
577         aboutToMarkSlow(markingVersion);
578     return Dependency::fence(version);
579 }
580 
581 inline void MarkedBlock::Handle::assertMarksNotStale()
582 {
583     block().assertMarksNotStale();
584 }
585 
586 inline bool MarkedBlock::isMarkedRaw(const void* p)
587 {
588     return footer().m_marks.get(atomNumber(p));
589 }
590 
591 inline bool MarkedBlock::isMarked(HeapVersion markingVersion, const void* p)
592 {
593     HeapVersion version = footer().m_markingVersion;
594     if (UNLIKELY(version != markingVersion))
595         return false;
596     return footer().m_marks.get(atomNumber(p), Dependency::fence(version));
597 }
598 
599 inline bool MarkedBlock::isMarked(const void* p, Dependency dependency)
600 {
601     assertMarksNotStale();
602     return footer().m_marks.get(atomNumber(p), dependency);
603 }
604 
605 inline bool MarkedBlock::testAndSetMarked(const void* p, Dependency dependency)
606 {
607     assertMarksNotStale();
608     return footer().m_marks.concurrentTestAndSet(atomNumber(p), dependency);
609 }
610 
611 inline const Bitmap&lt;MarkedBlock::atomsPerBlock&gt;&amp; MarkedBlock::marks() const
612 {
613     return footer().m_marks;
614 }
615 
616 inline bool MarkedBlock::isNewlyAllocated(const void* p)
617 {
618     return footer().m_newlyAllocated.get(atomNumber(p));
619 }
620 
621 inline void MarkedBlock::setNewlyAllocated(const void* p)
622 {
623     footer().m_newlyAllocated.set(atomNumber(p));
624 }
625 
626 inline void MarkedBlock::clearNewlyAllocated(const void* p)
627 {
628     footer().m_newlyAllocated.clear(atomNumber(p));
629 }
630 
631 inline const Bitmap&lt;MarkedBlock::atomsPerBlock&gt;&amp; MarkedBlock::newlyAllocated() const
632 {
633     return footer().m_newlyAllocated;
634 }
635 
636 inline bool MarkedBlock::isAtom(const void* p)
637 {
638     ASSERT(MarkedBlock::isAtomAligned(p));
<a name="23" id="anc23"></a><span class="line-modified">639     size_t atomNumber = candidateAtomNumber(p);</span>
640     if (atomNumber % handle().m_atomsPerCell) // Filters pointers into cell middles.
641         return false;
642     if (atomNumber &gt;= handle().m_endAtom) // Filters pointers into invalid cells out of the range.
643         return false;
644     return true;
645 }
646 
647 template &lt;typename Functor&gt;
648 inline IterationStatus MarkedBlock::Handle::forEachCell(const Functor&amp; functor)
649 {
650     HeapCell::Kind kind = m_attributes.cellKind;
651     for (size_t i = 0; i &lt; m_endAtom; i += m_atomsPerCell) {
652         HeapCell* cell = reinterpret_cast_ptr&lt;HeapCell*&gt;(&amp;m_block-&gt;atoms()[i]);
<a name="24" id="anc24"></a><span class="line-modified">653         if (functor(i, cell, kind) == IterationStatus::Done)</span>
654             return IterationStatus::Done;
655     }
656     return IterationStatus::Continue;
657 }
658 
659 inline bool MarkedBlock::hasAnyMarked() const
660 {
661     return footer().m_biasedMarkCount != footer().m_markCountBias;
662 }
663 
664 inline void MarkedBlock::noteMarked()
665 {
666     // This is racy by design. We don&#39;t want to pay the price of an atomic increment!
667     int16_t biasedMarkCount = footer().m_biasedMarkCount;
668     ++biasedMarkCount;
669     footer().m_biasedMarkCount = biasedMarkCount;
670     if (UNLIKELY(!biasedMarkCount))
671         noteMarkedSlow();
672 }
673 
674 } // namespace JSC
675 
676 namespace WTF {
677 
678 struct MarkedBlockHash : PtrHash&lt;JSC::MarkedBlock*&gt; {
679     static unsigned hash(JSC::MarkedBlock* const&amp; key)
680     {
681         // Aligned VM regions tend to be monotonically increasing integers,
682         // which is a great hash function, but we have to remove the low bits,
683         // since they&#39;re always zero, which is a terrible hash function!
684         return reinterpret_cast&lt;uintptr_t&gt;(key) / JSC::MarkedBlock::blockSize;
685     }
686 };
687 
688 template&lt;&gt; struct DefaultHash&lt;JSC::MarkedBlock*&gt; {
689     typedef MarkedBlockHash Hash;
690 };
691 
692 void printInternal(PrintStream&amp; out, JSC::MarkedBlock::Handle::SweepMode);
693 
694 } // namespace WTF
<a name="25" id="anc25"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="25" type="hidden" />
</body>
</html>