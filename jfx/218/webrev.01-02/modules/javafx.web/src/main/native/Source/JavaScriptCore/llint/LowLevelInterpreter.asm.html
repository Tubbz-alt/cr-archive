<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 # Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 # Crash course on the language that this is written in (which I just call
  25 # &quot;assembly&quot; even though it&#39;s more than that):
  26 #
  27 # - Mostly gas-style operand ordering. The last operand tends to be the
  28 #   destination. So &quot;a := b&quot; is written as &quot;mov b, a&quot;. But unlike gas,
  29 #   comparisons are in-order, so &quot;if (a &lt; b)&quot; is written as
  30 #   &quot;bilt a, b, ...&quot;.
  31 #
  32 # - &quot;b&quot; = byte, &quot;h&quot; = 16-bit word, &quot;i&quot; = 32-bit word, &quot;p&quot; = pointer.
  33 #   For 32-bit, &quot;i&quot; and &quot;p&quot; are interchangeable except when an op supports one
  34 #   but not the other.
  35 #
  36 # - In general, valid operands for macro invocations and instructions are
  37 #   registers (eg &quot;t0&quot;), addresses (eg &quot;4[t0]&quot;), base-index addresses
  38 #   (eg &quot;7[t0, t1, 2]&quot;), absolute addresses (eg &quot;0xa0000000[]&quot;), or labels
  39 #   (eg &quot;_foo&quot; or &quot;.foo&quot;). Macro invocations can also take anonymous
  40 #   macros as operands. Instructions cannot take anonymous macros.
  41 #
  42 # - Labels must have names that begin with either &quot;_&quot; or &quot;.&quot;.  A &quot;.&quot; label
  43 #   is local and gets renamed before code gen to minimize namespace
  44 #   pollution. A &quot;_&quot; label is an extern symbol (i.e. &quot;.globl&quot;). The &quot;_&quot;
  45 #   may or may not be removed during code gen depending on whether the asm
  46 #   conventions for C name mangling on the target platform mandate a &quot;_&quot;
  47 #   prefix.
  48 #
  49 # - A &quot;macro&quot; is a lambda expression, which may be either anonymous or
  50 #   named. But this has caveats. &quot;macro&quot; can take zero or more arguments,
  51 #   which may be macros or any valid operands, but it can only return
  52 #   code. But you can do Turing-complete things via continuation passing
  53 #   style: &quot;macro foo (a, b) b(a, a) end foo(foo, foo)&quot;. Actually, don&#39;t do
  54 #   that, since you&#39;ll just crash the assembler.
  55 #
  56 # - An &quot;if&quot; is a conditional on settings. Any identifier supplied in the
  57 #   predicate of an &quot;if&quot; is assumed to be a #define that is available
  58 #   during code gen. So you can&#39;t use &quot;if&quot; for computation in a macro, but
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #
  77 #  - t0, t1, t2, t3, t4, and optionally t5, t6, and t7 are temporary registers that can get trashed on
  78 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  79 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  80 #
  81 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  82 #  two distinct registers
  83 #
  84 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  85 #  thus are pairwise distinct registers. Be mindful that:
  86 #    + On X86, there are no argument registers. a0 and a1 are edx and
  87 #    ecx following the fastcall convention, but you should still use the stack
  88 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  89 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  90 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  91 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  92 #    this for you.
  93 #
  94 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  95 #  you should be mindful of that in functions that are called directly from C.
  96 #  If you need more registers, you should push and pop them like a good
  97 #  assembly citizen, because any other register will be callee-saved on X86.
  98 #
  99 # You can additionally assume:
 100 #
 101 #  - a3, t2, t3, t4 and t5 are never return registers; t0, t1, a0, a1 and a2
 102 #  can be return registers.
 103 #
 104 #  - t4 and t5 are never argument registers, t3 can only be a3, t1 can only be
 105 #  a1; but t0 and t2 can be either a0 or a2.
 106 #
 107 #  - There are callee-save registers named csr0, csr1, ... csrN.
 108 #  The last three csr registers are used used to store the PC base and
 109 #  two special tag values (on 64-bits only). Don&#39;t use them for anything else.
 110 #
 111 # Additional platform-specific details (you shouldn&#39;t rely on this remaining
 112 # true):
 113 #
 114 #  - For consistency with the baseline JIT, t0 is always r0 (and t1 is always
 115 #  r1 on 32 bits platforms). You should use the r version when you need return
 116 #  registers, and the t version otherwise: code using t0 (or t1) should still
 117 #  work if swapped with e.g. t3, while code using r0 (or r1) should not. There
 118 #  *may* be legacy code relying on this.
 119 #
 120 #  - On all platforms other than X86, t0 can only be a0 and t2 can only be a2.
 121 #
 122 #  - On all platforms other than X86 and X86_64, a2 is not a return register.
 123 #  a2 is r0 on X86 (because we have so few registers) and r1 on X86_64 (because
 124 #  the ABI enforces it).
 125 #
 126 # The following floating-point registers are available:
 127 #
 128 #  - ft0-ft5 are temporary floating-point registers that get trashed on calls,
 129 #  and are pairwise distinct.
 130 #
 131 #  - fa0 and fa1 are the platform&#39;s customary floating-point argument
 132 #  registers, and are both distinct. On 64-bits platforms, fa2 and fa3 are
 133 #  additional floating-point argument registers.
 134 #
 135 #  - fr is the platform&#39;s customary floating-point return register
 136 #
 137 # You can assume that ft1-ft5 or fa1-fa3 are never fr, and that ftX is never
 138 # faY if X != Y.
 139 
 140 # First come the common protocols that both interpreters use. Note that each
 141 # of these must have an ASSERT() in LLIntData.cpp
 142 
 143 # Work-around for the fact that the toolchain&#39;s awareness of armv7k / armv7s
 144 # results in a separate slab in the fat binary, yet the offlineasm doesn&#39;t know
 145 # to expect it.
 146 if ARMv7k
 147 end
 148 if ARMv7s
 149 end
 150 
 151 # These declarations must match interpreter/JSStack.h.
 152 
 153 const PtrSize = constexpr (sizeof(void*))
 154 const MachineRegisterSize = constexpr (sizeof(CPURegister))
 155 const SlotSize = constexpr (sizeof(Register))
 156 
 157 if JSVALUE64
 158     const CallFrameHeaderSlots = 5
 159 else
 160     const CallFrameHeaderSlots = 4
 161     const CallFrameAlignSlots = 1
 162 end
 163 
 164 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 165 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)
 166 const JSInternalFieldObjectImpl_internalFields = JSInternalFieldObjectImpl::m_internalFields
 167 
 168 const StackAlignment = constexpr (stackAlignmentBytes())
 169 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 170 const StackAlignmentMask = StackAlignment - 1
 171 
 172 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 173 
 174 const CallerFrame = 0
 175 const ReturnPC = CallerFrame + MachineRegisterSize
 176 const CodeBlock = ReturnPC + MachineRegisterSize
 177 const Callee = CodeBlock + SlotSize
 178 const ArgumentCountIncludingThis = Callee + SlotSize
 179 const ThisArgumentOffset = ArgumentCountIncludingThis + SlotSize
 180 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 181 const CallFrameHeaderSize = ThisArgumentOffset
 182 
 183 const MetadataOffsetTable16Offset = 0
 184 const MetadataOffsetTable32Offset = constexpr UnlinkedMetadataTable::s_offset16TableSize
 185 const NumberOfJSOpcodeIDs = constexpr numOpcodeIDs
 186 
 187 # Some value representation constants.
 188 if JSVALUE64
 189     const TagOther        = constexpr JSValue::OtherTag
 190     const TagBool         = constexpr JSValue::BoolTag
 191     const TagUndefined    = constexpr JSValue::UndefinedTag
 192     const ValueEmpty      = constexpr JSValue::ValueEmpty
 193     const ValueFalse      = constexpr JSValue::ValueFalse
 194     const ValueTrue       = constexpr JSValue::ValueTrue
 195     const ValueUndefined  = constexpr JSValue::ValueUndefined
 196     const ValueNull       = constexpr JSValue::ValueNull
 197     const TagNumber       = constexpr JSValue::NumberTag
 198     const NotCellMask     = constexpr JSValue::NotCellMask
 199 else
 200     const Int32Tag = constexpr JSValue::Int32Tag
 201     const BooleanTag = constexpr JSValue::BooleanTag
 202     const NullTag = constexpr JSValue::NullTag
 203     const UndefinedTag = constexpr JSValue::UndefinedTag
 204     const CellTag = constexpr JSValue::CellTag
 205     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 206     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 207     const LowestTag = constexpr JSValue::LowestTag
 208 end
 209 
 210 if JSVALUE64
 211     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 212     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 213 end
 214 
 215 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 216 
 217 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 218     const CalleeSaveSpaceAsVirtualRegisters = 4
 219 elsif C_LOOP or C_LOOP_WIN
 220     const CalleeSaveSpaceAsVirtualRegisters = 1
 221 elsif ARMv7
 222     const CalleeSaveSpaceAsVirtualRegisters = 1
 223 elsif MIPS
 224     const CalleeSaveSpaceAsVirtualRegisters = 1
 225 else
 226     const CalleeSaveSpaceAsVirtualRegisters = 0
 227 end
 228 
 229 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 230 
 231 
 232 # Watchpoint states
 233 const ClearWatchpoint = constexpr ClearWatchpoint
 234 const IsWatched = constexpr IsWatched
 235 const IsInvalidated = constexpr IsInvalidated
 236 
 237 # ShadowChicken data
 238 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 239 
 240 # UnaryArithProfile data
 241 const ArithProfileInt = constexpr (UnaryArithProfile::observedIntBits())
 242 const ArithProfileNumber = constexpr (UnaryArithProfile::observedNumberBits())
 243 
 244 # BinaryArithProfile data
 245 const ArithProfileIntInt = constexpr (BinaryArithProfile::observedIntIntBits())
 246 const ArithProfileNumberInt = constexpr (BinaryArithProfile::observedNumberIntBits())
 247 const ArithProfileIntNumber = constexpr (BinaryArithProfile::observedIntNumberBits())
 248 const ArithProfileNumberNumber = constexpr (BinaryArithProfile::observedNumberNumberBits())
 249 
 250 # Pointer Tags
 251 const BytecodePtrTag = constexpr BytecodePtrTag
 252 const JSEntryPtrTag = constexpr JSEntryPtrTag
 253 const ExceptionHandlerPtrTag = constexpr ExceptionHandlerPtrTag
 254 const NoPtrTag = constexpr NoPtrTag
 255 const SlowPathPtrTag = constexpr SlowPathPtrTag
 256 
 257 # Some register conventions.
 258 # - We use a pair of registers to represent the PC: one register for the
 259 #   base of the bytecodes, and one register for the index.
 260 # - The PC base (or PB for short) must be stored in a callee-save register.
 261 # - C calls are still given the Instruction* rather than the PC index.
 262 #   This requires an add before the call, and a sub after.
 263 if JSVALUE64
 264     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 265     if ARM64 or ARM64E
 266         const metadataTable = csr6
 267         const PB = csr7
 268         const numberTag = csr8
 269         const notCellMask = csr9
 270     elsif X86_64
 271         const metadataTable = csr1
 272         const PB = csr2
 273         const numberTag = csr3
 274         const notCellMask = csr4
 275     elsif X86_64_WIN
 276         const metadataTable = csr3
 277         const PB = csr4
 278         const numberTag = csr5
 279         const notCellMask = csr6
 280     elsif C_LOOP or C_LOOP_WIN
 281         const PB = csr0
 282         const numberTag = csr1
 283         const notCellMask = csr2
 284         const metadataTable = csr3
 285     end
 286 
 287 else
 288     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 289     if C_LOOP or C_LOOP_WIN
 290         const PB = csr0
 291         const metadataTable = csr3
 292     elsif ARMv7
 293         const metadataTable = csr0
 294         const PB = csr1
 295     elsif MIPS
 296         const metadataTable = csr0
 297         const PB = csr1
 298     else
 299         error
 300     end
 301 end
 302 
 303 if GIGACAGE_ENABLED
 304     const GigacagePrimitiveBasePtrOffset = constexpr Gigacage::offsetOfPrimitiveGigacageBasePtr
 305     const GigacageJSValueBasePtrOffset = constexpr Gigacage::offsetOfJSValueGigacageBasePtr
 306 end
 307 
 308 # Opcode offsets
 309 const OpcodeIDNarrowSize = 1 # OpcodeID
 310 const OpcodeIDWide16Size = 2 # Wide16 Prefix + OpcodeID
 311 const OpcodeIDWide32Size = 2 # Wide32 Prefix + OpcodeID
 312 
 313 
 314 macro nextInstruction()
 315     loadb [PB, PC, 1], t0
 316     leap _g_opcodeMap, t1
 317     jmp [t1, t0, PtrSize], BytecodePtrTag
 318 end
 319 
 320 macro nextInstructionWide16()
 321     loadb OpcodeIDNarrowSize[PB, PC, 1], t0
 322     leap _g_opcodeMapWide16, t1
 323     jmp [t1, t0, PtrSize], BytecodePtrTag
 324 end
 325 
 326 macro nextInstructionWide32()
 327     loadb OpcodeIDNarrowSize[PB, PC, 1], t0
 328     leap _g_opcodeMapWide32, t1
 329     jmp [t1, t0, PtrSize], BytecodePtrTag
 330 end
 331 
 332 macro dispatch(advanceReg)
 333     addp advanceReg, PC
 334     nextInstruction()
 335 end
 336 
 337 macro dispatchIndirect(offsetReg)
 338     dispatch(offsetReg)
 339 end
 340 
 341 macro genericDispatchOp(dispatch, size, opcodeName)
 342     macro dispatchNarrow()
 343         dispatch((constexpr %opcodeName%_length - 1) * 1 + OpcodeIDNarrowSize)
 344     end
 345 
 346     macro dispatchWide16()
 347         dispatch((constexpr %opcodeName%_length - 1) * 2 + OpcodeIDWide16Size)
 348     end
 349 
 350     macro dispatchWide32()
 351         dispatch((constexpr %opcodeName%_length - 1) * 4 + OpcodeIDWide32Size)
 352     end
 353 
 354     size(dispatchNarrow, dispatchWide16, dispatchWide32, macro (dispatch) dispatch() end)
 355 end
 356 
 357 macro dispatchOp(size, opcodeName)
 358     genericDispatchOp(dispatch, size, opcodeName)
 359 end
 360 
 361 
 362 macro getu(size, opcodeStruct, fieldName, dst)
 363     size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)
 364         getu(opcodeStruct, fieldName, dst)
 365     end)
 366 end
 367 
 368 macro get(size, opcodeStruct, fieldName, dst)
 369     size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)
 370         get(opcodeStruct, fieldName, dst)
 371     end)
 372 end
 373 
 374 macro narrow(narrowFn, wide16Fn, wide32Fn, k)
 375     k(narrowFn)
 376 end
 377 
 378 macro wide16(narrowFn, wide16Fn, wide32Fn, k)
 379     k(wide16Fn)
 380 end
 381 
 382 macro wide32(narrowFn, wide16Fn, wide32Fn, k)
 383     k(wide32Fn)
 384 end
 385 
 386 macro metadata(size, opcode, dst, scratch)
 387     loadh (constexpr %opcode%::opcodeID * 2 + MetadataOffsetTable16Offset)[metadataTable], dst # offset = metadataTable&lt;uint16_t*&gt;[opcodeID]
 388     btinz dst, .setUpOffset
 389     loadi (constexpr %opcode%::opcodeID * 4 + MetadataOffsetTable32Offset)[metadataTable], dst # offset = metadataTable&lt;uint32_t*&gt;[opcodeID]
 390 .setUpOffset:
 391     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 392     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 393     addi scratch, dst # offset += scratch
 394     addp metadataTable, dst # return &amp;metadataTable[offset]
 395 end
 396 
 397 macro jumpImpl(dispatchIndirect, targetOffsetReg)
 398     btiz targetOffsetReg, .outOfLineJumpTarget
 399     dispatchIndirect(targetOffsetReg)
 400 .outOfLineJumpTarget:
 401     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 402     nextInstruction()
 403 end
 404 
 405 macro commonOp(label, prologue, fn)
 406 _%label%:
 407     prologue()
 408     fn(narrow)
 409     if ASSERT_ENABLED
 410         break
 411         break
 412     end
 413 
 414 # FIXME: We cannot enable wide16 bytecode in Windows CLoop. With MSVC, as CLoop::execute gets larger code
 415 # size, CLoop::execute gets higher stack height requirement. This makes CLoop::execute takes 160KB stack
 416 # per call, causes stack overflow error easily. For now, we disable wide16 optimization for Windows CLoop.
 417 # https://bugs.webkit.org/show_bug.cgi?id=198283
 418 if not C_LOOP_WIN
 419 _%label%_wide16:
 420     prologue()
 421     fn(wide16)
 422     if ASSERT_ENABLED
 423         break
 424         break
 425     end
 426 end
 427 
 428 _%label%_wide32:
 429     prologue()
 430     fn(wide32)
 431     if ASSERT_ENABLED
 432         break
 433         break
 434     end
 435 end
 436 
 437 macro op(l, fn)
 438     commonOp(l, macro () end, macro (size)
 439         size(fn, macro() end, macro() end, macro(gen) gen() end)
 440     end)
 441 end
 442 
 443 macro llintOp(opcodeName, opcodeStruct, fn)
 444     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 445         macro getImpl(fieldName, dst)
 446             get(size, opcodeStruct, fieldName, dst)
 447         end
 448 
 449         macro dispatchImpl()
 450             dispatchOp(size, opcodeName)
 451         end
 452 
 453         fn(size, getImpl, dispatchImpl)
 454     end)
 455 end
 456 
 457 macro llintOpWithReturn(opcodeName, opcodeStruct, fn)
 458     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 459         makeReturn(get, dispatch, macro (return)
 460             fn(size, get, dispatch, return)
 461         end)
 462     end)
 463 end
 464 
 465 macro llintOpWithMetadata(opcodeName, opcodeStruct, fn)
 466     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
 467         macro meta(dst, scratch)
 468             metadata(size, opcodeStruct, dst, scratch)
 469         end
 470         fn(size, get, dispatch, meta, return)
 471     end)
 472 end
 473 
 474 macro llintOpWithJump(opcodeName, opcodeStruct, impl)
 475     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 476         macro jump(fieldName)
 477             get(fieldName, t0)
 478             jumpImpl(dispatchIndirect, t0)
 479         end
 480 
 481         impl(size, get, jump, dispatch)
 482     end)
 483 end
 484 
 485 macro llintOpWithProfile(opcodeName, opcodeStruct, fn)
 486     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 487         makeReturnProfiled(opcodeStruct, get, metadata, dispatch, macro (returnProfiled)
 488             fn(size, get, dispatch, returnProfiled)
 489         end)
 490     end)
 491 end
 492 
 493 
 494 if X86_64_WIN
 495     const extraTempReg = t0
 496 else
 497     const extraTempReg = t5
 498 end
 499 
 500 # Constants for reasoning about value representation.
 501 const TagOffset = constexpr TagOffset
 502 const PayloadOffset = constexpr PayloadOffset
 503 
 504 # Constant for reasoning about butterflies.
 505 const IsArray                  = constexpr IsArray
 506 const IndexingShapeMask        = constexpr IndexingShapeMask
 507 const NoIndexingShape          = constexpr NoIndexingShape
 508 const Int32Shape               = constexpr Int32Shape
 509 const DoubleShape              = constexpr DoubleShape
 510 const ContiguousShape          = constexpr ContiguousShape
 511 const ArrayStorageShape        = constexpr ArrayStorageShape
 512 const SlowPutArrayStorageShape = constexpr SlowPutArrayStorageShape
 513 const CopyOnWrite              = constexpr CopyOnWrite
 514 
 515 # Type constants.
 516 const StringType = constexpr StringType
 517 const SymbolType = constexpr SymbolType
 518 const ObjectType = constexpr ObjectType
 519 const FinalObjectType = constexpr FinalObjectType
 520 const JSFunctionType = constexpr JSFunctionType
 521 const ArrayType = constexpr ArrayType
 522 const DerivedArrayType = constexpr DerivedArrayType
 523 const ProxyObjectType = constexpr ProxyObjectType
 524 
 525 # The typed array types need to be numbered in a particular order because of the manually written
 526 # switch statement in get_by_val and put_by_val.
 527 const Int8ArrayType = constexpr Int8ArrayType
 528 const Uint8ArrayType = constexpr Uint8ArrayType
 529 const Uint8ClampedArrayType = constexpr Uint8ClampedArrayType
 530 const Int16ArrayType = constexpr Int16ArrayType
 531 const Uint16ArrayType = constexpr Uint16ArrayType
 532 const Int32ArrayType = constexpr Int32ArrayType
 533 const Uint32ArrayType = constexpr Uint32ArrayType
 534 const Float32ArrayType = constexpr Float32ArrayType
 535 const Float64ArrayType = constexpr Float64ArrayType
 536 
 537 const FirstTypedArrayType = constexpr FirstTypedArrayType
 538 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 539 
 540 # Type flags constants.
 541 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 542 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 543 
 544 # Bytecode operand constants.
 545 const FirstConstantRegisterIndexNarrow = constexpr FirstConstantRegisterIndex8
 546 const FirstConstantRegisterIndexWide16 = constexpr FirstConstantRegisterIndex16
 547 const FirstConstantRegisterIndexWide32 = constexpr FirstConstantRegisterIndex
 548 
 549 # Code type constants.
 550 const GlobalCode = constexpr GlobalCode
 551 const EvalCode = constexpr EvalCode
 552 const FunctionCode = constexpr FunctionCode
 553 const ModuleCode = constexpr ModuleCode
 554 
 555 # The interpreter steals the tag word of the argument count.
 556 const LLIntReturnPC = ArgumentCountIncludingThis + TagOffset
 557 
 558 # String flags.
 559 const isRopeInPointer = constexpr JSString::isRopeInPointer
 560 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 561 
 562 # Copied from PropertyOffset.h
 563 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 564 
 565 # ResolveType
 566 const GlobalProperty = constexpr GlobalProperty
 567 const GlobalVar = constexpr GlobalVar
 568 const GlobalLexicalVar = constexpr GlobalLexicalVar
 569 const ClosureVar = constexpr ClosureVar
 570 const LocalClosureVar = constexpr LocalClosureVar
 571 const ModuleVar = constexpr ModuleVar
 572 const GlobalPropertyWithVarInjectionChecks = constexpr GlobalPropertyWithVarInjectionChecks
 573 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 574 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 575 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 576 
 577 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 578 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 579 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 580 const NotInitialization = constexpr InitializationMode::NotInitialization
 581 
 582 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 583 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 584 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter
 585 const PreciseAllocationHeaderSize = constexpr (PreciseAllocation::headerSize())
 586 const PreciseAllocationVMOffset = (PreciseAllocation::m_weakSet + WeakSet::m_vm - PreciseAllocationHeaderSize)
 587 
 588 const BlackThreshold = constexpr blackThreshold
 589 
 590 const VectorBufferOffset = Vector::m_buffer
 591 const VectorSizeOffset = Vector::m_size
 592 
 593 # Some common utilities.
 594 macro crash()
 595     if C_LOOP or C_LOOP_WIN
 596         cloopCrash
 597     else
 598         call _llint_crash
 599     end
 600 end
 601 
 602 macro assert(assertion)
 603     if ASSERT_ENABLED
 604         assertion(.ok)
 605         crash()
 606     .ok:
 607     end
 608 end
 609 
 610 macro assert_with(assertion, crash)
 611     if ASSERT_ENABLED
 612         assertion(.ok)
 613         crash()
 614     .ok:
 615     end
 616 end
 617 
 618 # The probe macro can be used to insert some debugging code without perturbing scalar
 619 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 620 # C probe callback function should not trash floating point registers.
 621 #
 622 # The macro you pass to probe() can pass whatever registers you like to your probe
 623 # callback function. However, you need to be mindful of which of the registers are
 624 # also used as argument registers, and ensure that you don&#39;t trash the register value
 625 # before storing it in the probe callback argument register that you desire.
 626 #
 627 # Here&#39;s an example of how it&#39;s used:
 628 #
 629 #     probe(
 630 #         macro()
 631 #             move cfr, a0 # pass the CallFrame* as arg0.
 632 #             move t0, a1 # pass the value of register t0 as arg1.
 633 #             call _cProbeCallbackFunction # to do whatever you want.
 634 #         end
 635 #     )
 636 #
 637 if X86_64 or ARM64 or ARM64E or ARMv7
 638     macro probe(action)
 639         # save all the registers that the LLInt may use.
 640         if ARM64 or ARM64E or ARMv7
 641             push cfr, lr
 642         end
 643         push a0, a1
 644         push a2, a3
 645         push t0, t1
 646         push t2, t3
 647         push t4, t5
 648         if ARM64 or ARM64E
 649             push csr0, csr1
 650             push csr2, csr3
 651             push csr4, csr5
 652             push csr6, csr7
 653             push csr8, csr9
 654         elsif ARMv7
 655             push csr0, csr1
 656         end
 657 
 658         action()
 659 
 660         # restore all the registers we saved previously.
 661         if ARM64 or ARM64E
 662             pop csr9, csr8
 663             pop csr7, csr6
 664             pop csr5, csr4
 665             pop csr3, csr2
 666             pop csr1, csr0
 667         elsif ARMv7
 668             pop csr1, csr0
 669         end
 670         pop t5, t4
 671         pop t3, t2
 672         pop t1, t0
 673         pop a3, a2
 674         pop a1, a0
 675         if ARM64 or ARM64E or ARMv7
 676             pop lr, cfr
 677         end
 678     end
 679 else
 680     macro probe(action)
 681     end
 682 end
 683 
 684 macro checkStackPointerAlignment(tempReg, location)
 685     if ASSERT_ENABLED
 686         if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
 687             # ARM64 and ARM64E will check for us!
 688             # C_LOOP or C_LOOP_WIN does not need the alignment, and can use a little perf
 689             # improvement from avoiding useless work.
 690         else
 691             if ARMv7
 692                 # ARM can&#39;t do logical ops with the sp as a source
 693                 move sp, tempReg
 694                 andp StackAlignmentMask, tempReg
 695             else
 696                 andp sp, StackAlignmentMask, tempReg
 697             end
 698             btpz tempReg, .stackPointerOkay
 699             move location, tempReg
 700             break
 701         .stackPointerOkay:
 702         end
 703     end
 704 end
 705 
 706 if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 707     const CalleeSaveRegisterCount = 0
 708 elsif ARMv7
 709     const CalleeSaveRegisterCount = 7
 710 elsif MIPS
 711     const CalleeSaveRegisterCount = 3
 712 elsif X86 or X86_WIN
 713     const CalleeSaveRegisterCount = 3
 714 end
 715 
 716 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 717 
 718 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 719 # callee save registers rounded up to keep the stack aligned
 720 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 721 
 722 macro pushCalleeSaves()
 723     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 724     elsif ARMv7
 725         emit &quot;push {r4-r6, r8-r11}&quot;
 726     elsif MIPS
 727         emit &quot;addiu $sp, $sp, -12&quot;
 728         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
 729         emit &quot;sw $s1, 4($sp)&quot; # csr1/PB
 730         emit &quot;sw $s4, 8($sp)&quot;
 731         # save $gp to $s4 so that we can restore it after a function call
 732         emit &quot;move $s4, $gp&quot;
 733     elsif X86
 734         emit &quot;push %esi&quot;
 735         emit &quot;push %edi&quot;
 736         emit &quot;push %ebx&quot;
 737     elsif X86_WIN
 738         emit &quot;push esi&quot;
 739         emit &quot;push edi&quot;
 740         emit &quot;push ebx&quot;
 741     end
 742 end
 743 
 744 macro popCalleeSaves()
 745     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
 746     elsif ARMv7
 747         emit &quot;pop {r4-r6, r8-r11}&quot;
 748     elsif MIPS
 749         emit &quot;lw $s0, 0($sp)&quot;
 750         emit &quot;lw $s1, 4($sp)&quot;
 751         emit &quot;lw $s4, 8($sp)&quot;
 752         emit &quot;addiu $sp, $sp, 12&quot;
 753     elsif X86
 754         emit &quot;pop %ebx&quot;
 755         emit &quot;pop %edi&quot;
 756         emit &quot;pop %esi&quot;
 757     elsif X86_WIN
 758         emit &quot;pop ebx&quot;
 759         emit &quot;pop edi&quot;
 760         emit &quot;pop esi&quot;
 761     end
 762 end
 763 
 764 macro preserveCallerPCAndCFR()
 765     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 766         push lr
 767         push cfr
 768     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 769         push cfr
 770     elsif ARM64 or ARM64E
 771         push cfr, lr
 772     else
 773         error
 774     end
 775     move sp, cfr
 776 end
 777 
 778 macro restoreCallerPCAndCFR()
 779     move cfr, sp
 780     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 781         pop cfr
 782         pop lr
 783     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 784         pop cfr
 785     elsif ARM64 or ARM64E
 786         pop lr, cfr
 787     end
 788 end
 789 
 790 macro preserveCalleeSavesUsedByLLInt()
 791     subp CalleeSaveSpaceStackAligned, sp
 792     if C_LOOP or C_LOOP_WIN
 793         storep metadataTable, -PtrSize[cfr]
 794 
 795     # Next ARMv7 and MIPS differ in how we store metadataTable and PB,
 796     # because this codes needs to be in sync with how registers are
 797     # restored in Baseline JIT (specifically in emitRestoreCalleeSavesFor).
 798     # emitRestoreCalleeSavesFor restores registers in order instead of by name.
 799     # However, ARMv7 and MIPS differ in the order in which registers are assigned
 800     # to metadataTable and PB, therefore they can also not have the same saving
 801     # order.
 802     elsif ARMv7
 803         storep metadataTable, -4[cfr]
 804         storep PB, -8[cfr]
 805     elsif MIPS
 806         storep PB, -4[cfr]
 807         storep metadataTable, -8[cfr]
 808     elsif ARM64 or ARM64E
 809         emit &quot;stp x27, x28, [x29, #-16]&quot;
 810         emit &quot;stp x25, x26, [x29, #-32]&quot;
 811     elsif X86
 812     elsif X86_WIN
 813     elsif X86_64
 814         storep csr4, -8[cfr]
 815         storep csr3, -16[cfr]
 816         storep csr2, -24[cfr]
 817         storep csr1, -32[cfr]
 818     elsif X86_64_WIN
 819         storep csr6, -8[cfr]
 820         storep csr5, -16[cfr]
 821         storep csr4, -24[cfr]
 822         storep csr3, -32[cfr]
 823     end
 824 end
 825 
 826 macro restoreCalleeSavesUsedByLLInt()
 827     if C_LOOP or C_LOOP_WIN
 828         loadp -PtrSize[cfr], metadataTable
 829     # To understand why ARMv7 and MIPS differ in restore order,
 830     # see comment in preserveCalleeSavesUsedByLLInt
 831     elsif ARMv7
 832         loadp -4[cfr], metadataTable
 833         loadp -8[cfr], PB
 834     elsif MIPS
 835         loadp -4[cfr], PB
 836         loadp -8[cfr], metadataTable
 837     elsif ARM64 or ARM64E
 838         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 839         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 840     elsif X86
 841     elsif X86_WIN
 842     elsif X86_64
 843         loadp -32[cfr], csr1
 844         loadp -24[cfr], csr2
 845         loadp -16[cfr], csr3
 846         loadp -8[cfr], csr4
 847     elsif X86_64_WIN
 848         loadp -32[cfr], csr3
 849         loadp -24[cfr], csr4
 850         loadp -16[cfr], csr5
 851         loadp -8[cfr], csr6
 852     end
 853 end
 854 
 855 macro copyCalleeSavesToEntryFrameCalleeSavesBuffer(entryFrame)
 856     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 857         vmEntryRecord(entryFrame, entryFrame)
 858         leap VMEntryRecord::calleeSaveRegistersBuffer[entryFrame], entryFrame
 859         if ARM64 or ARM64E
 860             storeq csr0, [entryFrame]
 861             storeq csr1, 8[entryFrame]
 862             storeq csr2, 16[entryFrame]
 863             storeq csr3, 24[entryFrame]
 864             storeq csr4, 32[entryFrame]
 865             storeq csr5, 40[entryFrame]
 866             storeq csr6, 48[entryFrame]
 867             storeq csr7, 56[entryFrame]
 868             storeq csr8, 64[entryFrame]
 869             storeq csr9, 72[entryFrame]
 870             stored csfr0, 80[entryFrame]
 871             stored csfr1, 88[entryFrame]
 872             stored csfr2, 96[entryFrame]
 873             stored csfr3, 104[entryFrame]
 874             stored csfr4, 112[entryFrame]
 875             stored csfr5, 120[entryFrame]
 876             stored csfr6, 128[entryFrame]
 877             stored csfr7, 136[entryFrame]
 878         elsif X86_64
 879             storeq csr0, [entryFrame]
 880             storeq csr1, 8[entryFrame]
 881             storeq csr2, 16[entryFrame]
 882             storeq csr3, 24[entryFrame]
 883             storeq csr4, 32[entryFrame]
 884         elsif X86_64_WIN
 885             storeq csr0, [entryFrame]
 886             storeq csr1, 8[entryFrame]
 887             storeq csr2, 16[entryFrame]
 888             storeq csr3, 24[entryFrame]
 889             storeq csr4, 32[entryFrame]
 890             storeq csr5, 40[entryFrame]
 891             storeq csr6, 48[entryFrame]
 892         elsif ARMv7 or MIPS
 893             storep csr0, [entryFrame]
 894             storep csr1, 4[entryFrame]
 895         end
 896     end
 897 end
 898 
 899 macro copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(vm, temp)
 900     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 901         loadp VM::topEntryFrame[vm], temp
 902         copyCalleeSavesToEntryFrameCalleeSavesBuffer(temp)
 903     end
 904 end
 905 
 906 macro restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(vm, temp)
 907     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 908         loadp VM::topEntryFrame[vm], temp
 909         vmEntryRecord(temp, temp)
 910         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 911         if ARM64 or ARM64E
 912             loadq [temp], csr0
 913             loadq 8[temp], csr1
 914             loadq 16[temp], csr2
 915             loadq 24[temp], csr3
 916             loadq 32[temp], csr4
 917             loadq 40[temp], csr5
 918             loadq 48[temp], csr6
 919             loadq 56[temp], csr7
 920             loadq 64[temp], csr8
 921             loadq 72[temp], csr9
 922             loadd 80[temp], csfr0
 923             loadd 88[temp], csfr1
 924             loadd 96[temp], csfr2
 925             loadd 104[temp], csfr3
 926             loadd 112[temp], csfr4
 927             loadd 120[temp], csfr5
 928             loadd 128[temp], csfr6
 929             loadd 136[temp], csfr7
 930         elsif X86_64
 931             loadq [temp], csr0
 932             loadq 8[temp], csr1
 933             loadq 16[temp], csr2
 934             loadq 24[temp], csr3
 935             loadq 32[temp], csr4
 936         elsif X86_64_WIN
 937             loadq [temp], csr0
 938             loadq 8[temp], csr1
 939             loadq 16[temp], csr2
 940             loadq 24[temp], csr3
 941             loadq 32[temp], csr4
 942             loadq 40[temp], csr5
 943             loadq 48[temp], csr6
 944         elsif ARMv7 or MIPS
 945             loadp [temp], csr0
 946             loadp 4[temp], csr1
 947         end
 948     end
 949 end
 950 
 951 macro preserveReturnAddressAfterCall(destinationRegister)
 952     if C_LOOP or C_LOOP_WIN or ARMv7 or ARM64 or ARM64E or MIPS
 953         # In C_LOOP or C_LOOP_WIN case, we&#39;re only preserving the bytecode vPC.
 954         move lr, destinationRegister
 955     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 956         pop destinationRegister
 957     else
 958         error
 959     end
 960 end
 961 
 962 macro functionPrologue()
 963     tagReturnAddress sp
 964     if X86 or X86_WIN or X86_64 or X86_64_WIN
 965         push cfr
 966     elsif ARM64 or ARM64E
 967         push cfr, lr
 968     elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 969         push lr
 970         push cfr
 971     end
 972     move sp, cfr
 973 end
 974 
 975 macro functionEpilogue()
 976     if X86 or X86_WIN or X86_64 or X86_64_WIN
 977         pop cfr
 978     elsif ARM64 or ARM64E
 979         pop lr, cfr
 980     elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
 981         pop cfr
 982         pop lr
 983     end
 984 end
 985 
 986 macro vmEntryRecord(entryFramePointer, resultReg)
 987     subp entryFramePointer, VMEntryTotalFrameSize, resultReg
 988 end
 989 
 990 macro getFrameRegisterSizeForCodeBlock(codeBlock, size)
 991     loadi CodeBlock::m_numCalleeLocals[codeBlock], size
 992     lshiftp 3, size
 993     addp maxFrameExtentForSlowPathCall, size
 994 end
 995 
 996 macro restoreStackPointerAfterCall()
 997     loadp CodeBlock[cfr], t2
 998     getFrameRegisterSizeForCodeBlock(t2, t2)
 999     if ARMv7
1000         subp cfr, t2, t2
1001         move t2, sp
1002     else
1003         subp cfr, t2, sp
1004     end
1005 end
1006 
1007 macro traceExecution()
1008     if TRACING
1009         callSlowPath(_llint_trace)
1010     end
1011 end
1012 
1013 macro defineOSRExitReturnLabel(opcodeName, size)
1014     macro defineNarrow()
1015         if not C_LOOP_WIN
1016             _%opcodeName%_return_location:
1017         end
1018     end
1019 
1020     macro defineWide16()
1021         if not C_LOOP_WIN
1022             _%opcodeName%_return_location_wide16:
1023         end
1024     end
1025 
1026     macro defineWide32()
1027         if not C_LOOP_WIN
1028             _%opcodeName%_return_location_wide32:
1029         end
1030     end
1031 
1032     size(defineNarrow, defineWide16, defineWide32, macro (f) f() end)
1033 end
1034 
1035 macro callTargetFunction(opcodeName, size, opcodeStruct, dispatch, callee, callPtrTag)
1036     if C_LOOP or C_LOOP_WIN
1037         cloopCallJSFunction callee
1038     else
1039         call callee, callPtrTag
1040     end
1041 
1042     if ARMv7 or MIPS
1043         # It is required in ARMv7 and MIPs because global label definitions
1044         # for those architectures generates a set of instructions
1045         # that can clobber LLInt execution, resulting in unexpected
1046         # crashes.
1047         restoreStackPointerAfterCall()
1048         dispatchAfterCall(size, opcodeStruct, dispatch)
1049     end
1050     defineOSRExitReturnLabel(opcodeName, size)
1051     restoreStackPointerAfterCall()
1052     dispatchAfterCall(size, opcodeStruct, dispatch)
1053 end
1054 
1055 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
1056     addp CallerFrameAndPCSize, sp
1057 end
1058 
1059 # sp points to the new frame
1060 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
1061     restoreCalleeSavesUsedByLLInt()
1062 
1063     loadi PayloadOffset + ArgumentCountIncludingThis[cfr], temp2
1064     loadp CodeBlock[cfr], temp1
1065     loadi CodeBlock::m_numParameters[temp1], temp1
1066     bilteq temp1, temp2, .noArityFixup
1067     move temp1, temp2
1068 
1069 .noArityFixup:
1070     # We assume &lt; 2^28 arguments
1071     muli SlotSize, temp2
1072     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
1073     andi ~StackAlignmentMask, temp2
1074 
1075     move cfr, temp1
1076     addp temp2, temp1
1077 
1078     loadi PayloadOffset + ArgumentCountIncludingThis[sp], temp2
1079     # We assume &lt; 2^28 arguments
1080     muli SlotSize, temp2
1081     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
1082     andi ~StackAlignmentMask, temp2
1083 
1084     if ARMv7 or ARM64 or ARM64E or C_LOOP or C_LOOP_WIN or MIPS
1085         addp CallerFrameAndPCSize, sp
1086         subi CallerFrameAndPCSize, temp2
1087         loadp CallerFrameAndPC::returnPC[cfr], lr
1088     else
1089         addp PtrSize, sp
1090         subi PtrSize, temp2
1091         loadp PtrSize[cfr], temp3
1092         storep temp3, [sp]
1093     end
1094 
1095     if ARM64E
1096         addp 16, cfr, temp3
1097         untagReturnAddress temp3
1098     end
1099 
1100     subp temp2, temp1
1101     loadp [cfr], cfr
1102 
1103 .copyLoop:
1104     if ARM64 and not ADDRESS64
1105         subi MachineRegisterSize, temp2
1106         loadq [sp, temp2, 1], temp3
1107         storeq temp3, [temp1, temp2, 1]
1108         btinz temp2, .copyLoop
1109     else
1110         subi PtrSize, temp2
1111         loadp [sp, temp2, 1], temp3
1112         storep temp3, [temp1, temp2, 1]
1113         btinz temp2, .copyLoop
1114     end
1115 
1116     move temp1, sp
1117     jmp callee, callPtrTag
1118 end
1119 
1120 macro slowPathForCall(opcodeName, size, opcodeStruct, dispatch, slowPath, prepareCall)
1121     callCallSlowPath(
1122         slowPath,
1123         # Those are r0 and r1
1124         macro (callee, calleeFramePtr)
1125             btpz calleeFramePtr, .dontUpdateSP
1126             move calleeFramePtr, sp
1127             prepareCall(callee, t2, t3, t4, SlowPathPtrTag)
1128         .dontUpdateSP:
1129             callTargetFunction(%opcodeName%_slow, size, opcodeStruct, dispatch, callee, SlowPathPtrTag)
1130         end)
1131 end
1132 
1133 macro getterSetterOSRExitReturnPoint(opName, size)
1134     crash() # We don&#39;t reach this in straight line code. We only reach it via returning to the code below when reconstructing stack frames during OSR exit.
1135 
1136     defineOSRExitReturnLabel(opName, size)
1137 
1138     restoreStackPointerAfterCall()
1139     loadi LLIntReturnPC[cfr], PC
1140 end
1141 
1142 macro arrayProfile(offset, cellAndIndexingType, metadata, scratch)
1143     const cell = cellAndIndexingType
1144     const indexingType = cellAndIndexingType 
1145     loadi JSCell::m_structureID[cell], scratch
1146     storei scratch, offset + ArrayProfile::m_lastSeenStructureID[metadata]
1147     loadb JSCell::m_indexingTypeAndMisc[cell], indexingType
1148 end
1149 
1150 macro skipIfIsRememberedOrInEden(cell, slowPath)
1151     memfence
1152     bba JSCell::m_cellState[cell], BlackThreshold, .done
1153     slowPath()
1154 .done:
1155 end
1156 
1157 macro notifyWrite(set, slow)
1158     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1159 end
1160 
1161 macro checkSwitchToJIT(increment, action)
1162     loadp CodeBlock[cfr], t0
1163     baddis increment, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t0], .continue
1164     action()
1165     .continue:
1166 end
1167 
1168 macro checkSwitchToJITForEpilogue()
1169     checkSwitchToJIT(
1170         10,
1171         macro ()
1172             callSlowPath(_llint_replace)
1173         end)
1174 end
1175 
1176 macro assertNotConstant(size, index)
1177     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
1178         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1179     end)
1180 end
1181 
1182 macro functionForCallCodeBlockGetter(targetRegister)
1183     if JSVALUE64
1184         loadp Callee[cfr], targetRegister
1185     else
1186         loadp Callee + PayloadOffset[cfr], targetRegister
1187     end
1188     loadp JSFunction::m_executableOrRareData[targetRegister], targetRegister
1189     btpz targetRegister, (constexpr JSFunction::rareDataTag), .isExecutable
1190     loadp (FunctionRareData::m_executable - (constexpr JSFunction::rareDataTag))[targetRegister], targetRegister
1191 .isExecutable:
1192     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1193     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1194 end
1195 
1196 macro functionForConstructCodeBlockGetter(targetRegister)
1197     if JSVALUE64
1198         loadp Callee[cfr], targetRegister
1199     else
1200         loadp Callee + PayloadOffset[cfr], targetRegister
1201     end
1202     loadp JSFunction::m_executableOrRareData[targetRegister], targetRegister
1203     btpz targetRegister, (constexpr JSFunction::rareDataTag), .isExecutable
1204     loadp (FunctionRareData::m_executable - (constexpr JSFunction::rareDataTag))[targetRegister], targetRegister
1205 .isExecutable:
1206     loadp FunctionExecutable::m_codeBlockForConstruct[targetRegister], targetRegister
1207     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1208 end
1209 
1210 macro notFunctionCodeBlockGetter(targetRegister)
1211     loadp CodeBlock[cfr], targetRegister
1212 end
1213 
1214 macro functionCodeBlockSetter(sourceRegister)
1215     storep sourceRegister, CodeBlock[cfr]
1216 end
1217 
1218 macro notFunctionCodeBlockSetter(sourceRegister)
1219     # Nothing to do!
1220 end
1221 
1222 macro convertCalleeToVM(callee)
1223     btpnz callee, (constexpr PreciseAllocation::halfAlignment), .preciseAllocation
1224     andp MarkedBlockMask, callee
1225     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[callee], callee
1226     jmp .done
1227 .preciseAllocation:
1228     loadp PreciseAllocationVMOffset[callee], callee
1229 .done:
1230 end
1231 
1232 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1233 # in t1. May also trigger prologue entry OSR.
1234 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1235     # Set up the call frame and check if we should OSR.
1236     tagReturnAddress sp
1237     preserveCallerPCAndCFR()
1238 
1239     if TRACING
1240         subp maxFrameExtentForSlowPathCall, sp
1241         callSlowPath(traceSlowPath)
1242         addp maxFrameExtentForSlowPathCall, sp
1243     end
1244     codeBlockGetter(t1)
1245     codeBlockSetter(t1)
1246     if not (C_LOOP or C_LOOP_WIN)
1247         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1248         if JSVALUE64
1249             move cfr, a0
1250             move PC, a1
1251             cCall2(osrSlowPath)
1252         else
1253             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1254             # Temporarily align stack pointer for this call.
1255             subp 8, sp
1256             move cfr, a0
1257             move PC, a1
1258             cCall2(osrSlowPath)
1259             addp 8, sp
1260         end
1261         btpz r0, .recover
1262         move cfr, sp # restore the previous sp
1263         # pop the callerFrame since we will jump to a function that wants to save it
1264         if ARM64 or ARM64E
1265             pop lr, cfr
1266             untagReturnAddress sp
1267         elsif ARMv7 or MIPS
1268             pop cfr
1269             pop lr
1270         else
1271             pop cfr
1272         end
1273         jmp r0, JSEntryPtrTag
1274     .recover:
1275         notFunctionCodeBlockGetter(t1)
1276     .continue:
1277     end
1278 
1279     preserveCalleeSavesUsedByLLInt()
1280 
1281     # Set up the PC.
1282     loadp CodeBlock::m_instructionsRawPointer[t1], PB
1283     move 0, PC
1284 
1285     # Get new sp in t0 and check stack height.
1286     getFrameRegisterSizeForCodeBlock(t1, t0)
1287     subp cfr, t0, t0
1288     bpa t0, cfr, .needStackCheck
1289     loadp CodeBlock::m_vm[t1], t2
1290     if C_LOOP or C_LOOP_WIN
1291         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1292     else
1293         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1294     end
1295 
1296 .needStackCheck:
1297     # Stack height check failed - need to call a slow_path.
1298     # Set up temporary stack pointer for call including callee saves
1299     subp maxFrameExtentForSlowPathCall, sp
1300     callSlowPath(_llint_stack_check)
1301     bpeq r1, 0, .stackHeightOKGetCodeBlock
1302 
1303     # We&#39;re throwing before the frame is fully set up. This frame will be
1304     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1305     # start unwinding. We need to do this before we change the cfr.
1306     restoreCalleeSavesUsedByLLInt()
1307 
1308     move r1, cfr
1309     jmp _llint_throw_from_slow_path_trampoline
1310 
1311 .stackHeightOKGetCodeBlock:
1312     # Stack check slow path returned that the stack was ok.
1313     # Since they were clobbered, need to get CodeBlock and new sp
1314     notFunctionCodeBlockGetter(t1)
1315     getFrameRegisterSizeForCodeBlock(t1, t0)
1316     subp cfr, t0, t0
1317 
1318 .stackHeightOK:
1319     if X86_64 or ARM64
1320         # We need to start zeroing from sp as it has been adjusted after saving callee saves.
1321         move sp, t2
1322         move t0, sp
1323 .zeroStackLoop:
1324         bpeq sp, t2, .zeroStackDone
1325         subp PtrSize, t2
1326         storep 0, [t2]
1327         jmp .zeroStackLoop
1328 .zeroStackDone:
1329     else
1330         move t0, sp
1331     end
1332 
1333     loadp CodeBlock::m_metadata[t1], metadataTable
1334 
1335     if JSVALUE64
1336         move TagNumber, numberTag
1337         addq TagOther, numberTag, notCellMask
1338     end
1339 end
1340 
1341 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
1342 # Must call dispatch(0) after calling this.
1343 macro functionInitialization(profileArgSkip)
1344     # Profile the arguments. Unfortunately, we have no choice but to do this. This
1345     # code is pretty horrendous because of the difference in ordering between
1346     # arguments and value profiles, the desire to have a simple loop-down-to-zero
1347     # loop, and the desire to use only three registers so as to preserve the PC and
1348     # the code block. It is likely that this code should be rewritten in a more
1349     # optimal way for architectures that have more than five registers available
1350     # for arbitrary use in the interpreter.
1351     loadi CodeBlock::m_numParameters[t1], t0
1352     addp -profileArgSkip, t0 # Use addi because that&#39;s what has the peephole
1353     assert(macro (ok) bpgteq t0, 0, ok end)
1354     btpz t0, .argumentProfileDone
1355     loadp CodeBlock::m_argumentValueProfiles + RefCountedArray::m_data[t1], t3
1356     btpz t3, .argumentProfileDone # When we can&#39;t JIT, we don&#39;t allocate any argument value profiles.
1357     mulp sizeof ValueProfile, t0, t2 # Aaaaahhhh! Need strength reduction!
1358     lshiftp 3, t0 # offset of last JSValue arguments on the stack.
1359     addp t2, t3 # pointer to end of ValueProfile array in CodeBlock::m_argumentValueProfiles.
1360 .argumentProfileLoop:
1361     if JSVALUE64
1362         loadq ThisArgumentOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1363         subp sizeof ValueProfile, t3
1364         storeq t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets[t3]
1365     else
1366         loadi ThisArgumentOffset + TagOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1367         subp sizeof ValueProfile, t3
1368         storei t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets + TagOffset[t3]
1369         loadi ThisArgumentOffset + PayloadOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1370         storei t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets + PayloadOffset[t3]
1371     end
1372     baddpnz -8, t0, .argumentProfileLoop
1373 .argumentProfileDone:
1374 end
1375 
1376 macro doReturn()
1377     restoreCalleeSavesUsedByLLInt()
1378     restoreCallerPCAndCFR()
1379     ret
1380 end
1381 
1382 # This break instruction is needed so that the synthesized llintPCRangeStart label
1383 # doesn&#39;t point to the exact same location as vmEntryToJavaScript which comes after it.
1384 # Otherwise, libunwind will report vmEntryToJavaScript as llintPCRangeStart in
1385 # stack traces.
1386 
1387     break
1388 
1389 # stub to call into JavaScript or Native functions
1390 # EncodedJSValue vmEntryToJavaScript(void* code, VM* vm, ProtoCallFrame* protoFrame)
1391 # EncodedJSValue vmEntryToNativeFunction(void* code, VM* vm, ProtoCallFrame* protoFrame)
1392 
1393 if C_LOOP or C_LOOP_WIN
1394     _llint_vm_entry_to_javascript:
1395 else
1396     global _vmEntryToJavaScript
1397     _vmEntryToJavaScript:
1398 end
1399     doVMEntry(makeJavaScriptCall)
1400 
1401 
1402 if C_LOOP or C_LOOP_WIN
1403     _llint_vm_entry_to_native:
1404 else
1405     global _vmEntryToNative
1406     _vmEntryToNative:
1407 end
1408     doVMEntry(makeHostFunctionCall)
1409 
1410 
1411 if not (C_LOOP or C_LOOP_WIN)
1412     # void sanitizeStackForVMImpl(VM* vm)
1413     global _sanitizeStackForVMImpl
1414     _sanitizeStackForVMImpl:
1415         tagReturnAddress sp
1416         # We need three non-aliased caller-save registers. We are guaranteed
1417         # this for a0, a1 and a2 on all architectures.
1418         if X86 or X86_WIN
1419             loadp 4[sp], a0
1420         end
1421         const vmOrStartSP = a0
1422         const address = a1
1423         const zeroValue = a2
1424     
1425         loadp VM::m_lastStackTop[vmOrStartSP], address
1426         move sp, zeroValue
1427         storep zeroValue, VM::m_lastStackTop[vmOrStartSP]
1428         move sp, vmOrStartSP
1429 
1430         bpbeq sp, address, .zeroFillDone
1431         move address, sp
1432 
1433         move 0, zeroValue
1434     .zeroFillLoop:
1435         storep zeroValue, [address]
1436         addp PtrSize, address
1437         bpa vmOrStartSP, address, .zeroFillLoop
1438 
1439     .zeroFillDone:
1440         move vmOrStartSP, sp
1441         ret
1442     
1443     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1444     global _vmEntryRecord
1445     _vmEntryRecord:
1446         tagReturnAddress sp
1447         if X86 or X86_WIN
1448             loadp 4[sp], a0
1449         end
1450 
1451         vmEntryRecord(a0, r0)
1452         ret
1453 end
1454 
1455 if C_LOOP or C_LOOP_WIN
1456     # Dummy entry point the C Loop uses to initialize.
1457     _llint_entry:
1458         crash()
1459 else
1460     macro initPCRelative(kind, pcBase)
1461         if X86_64 or X86_64_WIN or X86 or X86_WIN
1462             call _%kind%_relativePCBase
1463         _%kind%_relativePCBase:
1464             pop pcBase
1465         elsif ARM64 or ARM64E
1466         elsif ARMv7
1467         _%kind%_relativePCBase:
1468             move pc, pcBase
1469             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1470         elsif MIPS
1471             la _%kind%_relativePCBase, pcBase
1472             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
1473         _%kind%_relativePCBase:
1474         end
1475     end
1476 
1477     # The PC base is in t3, as this is what _llint_entry leaves behind through
1478     # initPCRelative(t3)
1479     macro setEntryAddressCommon(kind, index, label, map)
1480         if X86_64
1481             leap (label - _%kind%_relativePCBase)[t3], t4
1482             move index, t5
1483             storep t4, [map, t5, 8]
1484         elsif X86_64_WIN
1485             leap (label - _%kind%_relativePCBase)[t3], t4
1486             move index, t0
1487             storep t4, [map, t0, 8]
1488         elsif X86 or X86_WIN
1489             leap (label - _%kind%_relativePCBase)[t3], t4
1490             move index, t5
1491             storep t4, [map, t5, 4]
1492         elsif ARM64 or ARM64E
1493             pcrtoaddr label, t3
1494             move index, t4
1495             storep t3, [map, t4, PtrSize]
1496         elsif ARMv7
1497             mvlbl (label - _%kind%_relativePCBase), t4
1498             addp t4, t3, t4
1499             move index, t5
1500             storep t4, [map, t5, 4]
1501         elsif MIPS
1502             la label, t4
1503             la _%kind%_relativePCBase, t3
1504             subp t3, t4
1505             addp t4, t3, t4
1506             move index, t5
1507             storep t4, [map, t5, 4]
1508         end
1509     end
1510 
1511 
1512 
1513     macro includeEntriesAtOffset(kind, fn)
1514         macro setEntryAddress(index, label)
1515             setEntryAddressCommon(kind, index, label, a0)
1516         end
1517 
1518         macro setEntryAddressWide16(index, label)
1519              setEntryAddressCommon(kind, index, label, a1)
1520         end
1521 
1522         macro setEntryAddressWide32(index, label)
1523              setEntryAddressCommon(kind, index, label, a2)
1524         end
1525 
1526         fn()
1527     end
1528 
1529 
1530 macro entry(kind, initialize)
1531     global _%kind%_entry
1532     _%kind%_entry:
1533         functionPrologue()
1534         pushCalleeSaves()
1535         if X86 or X86_WIN
1536             loadp 20[sp], a0
1537             loadp 24[sp], a1
1538             loadp 28[sp], a2
1539         end
1540 
1541         initPCRelative(kind, t3)
1542 
1543         # Include generated bytecode initialization file.
1544         includeEntriesAtOffset(kind, initialize)
1545         popCalleeSaves()
1546         functionEpilogue()
1547         ret
1548 end
1549 
1550 # Entry point for the llint to initialize.
1551 entry(llint, macro()
1552     include InitBytecodes
1553 end)
1554 
1555 end // not (C_LOOP or C_LOOP_WIN)
1556 
1557 _llint_op_wide16:
1558     nextInstructionWide16()
1559 
1560 _llint_op_wide32:
1561     nextInstructionWide32()
1562 
1563 macro noWide(label)
1564 _%label%_wide16:
1565     crash()
1566 
1567 _%label%_wide32:
1568     crash()
1569 end
1570 
1571 noWide(llint_op_wide16)
1572 noWide(llint_op_wide32)
1573 noWide(llint_op_enter)
1574 
1575 op(llint_program_prologue, macro ()
1576     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1577     dispatch(0)
1578 end)
1579 
1580 
1581 op(llint_module_program_prologue, macro ()
1582     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1583     dispatch(0)
1584 end)
1585 
1586 
1587 op(llint_eval_prologue, macro ()
1588     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1589     dispatch(0)
1590 end)
1591 
1592 
1593 op(llint_function_for_call_prologue, macro ()
1594     prologue(functionForCallCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_call, _llint_trace_prologue_function_for_call)
1595     functionInitialization(0)
1596     dispatch(0)
1597 end)
1598     
1599 
1600 op(llint_function_for_construct_prologue, macro ()
1601     prologue(functionForConstructCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_construct, _llint_trace_prologue_function_for_construct)
1602     functionInitialization(1)
1603     dispatch(0)
1604 end)
1605     
1606 
1607 op(llint_function_for_call_arity_check, macro ()
1608     prologue(functionForCallCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_call_arityCheck, _llint_trace_arityCheck_for_call)
1609     functionArityCheck(.functionForCallBegin, _slow_path_call_arityCheck)
1610 .functionForCallBegin:
1611     functionInitialization(0)
1612     dispatch(0)
1613 end)
1614 
1615 
1616 op(llint_function_for_construct_arity_check, macro ()
1617     prologue(functionForConstructCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_construct_arityCheck, _llint_trace_arityCheck_for_construct)
1618     functionArityCheck(.functionForConstructBegin, _slow_path_construct_arityCheck)
1619 .functionForConstructBegin:
1620     functionInitialization(1)
1621     dispatch(0)
1622 end)
1623 
1624 
1625 # Value-representation-specific code.
1626 if JSVALUE64
1627     include LowLevelInterpreter64
1628 else
1629     include LowLevelInterpreter32_64
1630 end
1631 
1632 
1633 # Value-representation-agnostic code.
1634 macro slowPathOp(opcodeName)
1635     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1636         callSlowPath(_slow_path_%opcodeName%)
1637         dispatch()
1638     end)
1639 end
1640 
1641 slowPathOp(create_cloned_arguments)
1642 slowPathOp(create_arguments_butterfly)
1643 slowPathOp(create_direct_arguments)
1644 slowPathOp(create_lexical_environment)
1645 slowPathOp(create_rest)
1646 slowPathOp(create_scoped_arguments)
1647 slowPathOp(create_this)
1648 slowPathOp(create_promise)
1649 slowPathOp(create_generator)
1650 slowPathOp(create_async_generator)
1651 slowPathOp(define_accessor_property)
1652 slowPathOp(define_data_property)
1653 slowPathOp(enumerator_generic_pname)
1654 slowPathOp(enumerator_structure_pname)
1655 slowPathOp(get_by_id_with_this)
1656 slowPathOp(get_by_val_with_this)
1657 slowPathOp(get_direct_pname)
1658 slowPathOp(get_enumerable_length)
1659 slowPathOp(get_property_enumerator)
1660 slowPathOp(greater)
1661 slowPathOp(greatereq)
1662 slowPathOp(has_generic_property)
1663 slowPathOp(has_indexed_property)
1664 slowPathOp(has_structure_property)
1665 slowPathOp(in_by_id)
1666 slowPathOp(in_by_val)
1667 slowPathOp(is_function)
1668 slowPathOp(is_object_or_null)
1669 slowPathOp(less)
1670 slowPathOp(lesseq)
1671 slowPathOp(mod)
1672 slowPathOp(new_array_buffer)
1673 slowPathOp(new_array_with_spread)
1674 slowPathOp(pow)
1675 slowPathOp(push_with_scope)
1676 slowPathOp(put_by_id_with_this)
1677 slowPathOp(put_by_val_with_this)
1678 slowPathOp(resolve_scope_for_hoisting_func_decl_in_eval)
1679 slowPathOp(spread)
1680 slowPathOp(strcat)
1681 slowPathOp(throw_static_error)
1682 slowPathOp(to_index_string)
1683 slowPathOp(typeof)
1684 slowPathOp(unreachable)
1685 slowPathOp(new_promise)
1686 slowPathOp(new_generator)
1687 
1688 macro llintSlowPathOp(opcodeName)
1689     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1690         callSlowPath(_llint_slow_path_%opcodeName%)
1691         dispatch()
1692     end)
1693 end
1694 
1695 llintSlowPathOp(del_by_id)
1696 llintSlowPathOp(del_by_val)
1697 llintSlowPathOp(instanceof)
1698 llintSlowPathOp(instanceof_custom)
1699 llintSlowPathOp(new_array)
1700 llintSlowPathOp(new_array_with_size)
1701 llintSlowPathOp(new_async_func)
1702 llintSlowPathOp(new_async_func_exp)
1703 llintSlowPathOp(new_async_generator_func)
1704 llintSlowPathOp(new_async_generator_func_exp)
1705 llintSlowPathOp(new_func)
1706 llintSlowPathOp(new_func_exp)
1707 llintSlowPathOp(new_generator_func)
1708 llintSlowPathOp(new_generator_func_exp)
1709 llintSlowPathOp(new_object)
1710 llintSlowPathOp(new_regexp)
1711 llintSlowPathOp(put_getter_by_id)
1712 llintSlowPathOp(put_getter_by_val)
1713 llintSlowPathOp(put_getter_setter_by_id)
1714 llintSlowPathOp(put_setter_by_id)
1715 llintSlowPathOp(put_setter_by_val)
1716 llintSlowPathOp(set_function_name)
1717 llintSlowPathOp(super_sampler_begin)
1718 llintSlowPathOp(super_sampler_end)
1719 llintSlowPathOp(throw)
1720 llintSlowPathOp(try_get_by_id)
1721 
1722 llintOp(op_switch_string, unused, macro (unused, unused, unused)
1723     callSlowPath(_llint_slow_path_switch_string)
1724     nextInstruction()
1725 end)
1726 
1727 
1728 equalityComparisonOp(eq, OpEq,
1729     macro (left, right, result) cieq left, right, result end)
1730 
1731 
1732 equalityComparisonOp(neq, OpNeq,
1733     macro (left, right, result) cineq left, right, result end)
1734 
1735 
1736 compareUnsignedOp(below, OpBelow,
1737         macro (left, right, result) cib left, right, result end)
1738 
1739 
1740 compareUnsignedOp(beloweq, OpBeloweq,
1741         macro (left, right, result) cibeq left, right, result end)
1742 
1743 
1744 llintOpWithJump(op_jmp, OpJmp, macro (size, get, jump, dispatch)
1745     jump(m_targetLabel)
1746 end)
1747 
1748 
1749 llintJumpTrueOrFalseOp(
1750     jtrue, OpJtrue,
1751     macro (value, target) btinz value, 1, target end)
1752 
1753 
1754 llintJumpTrueOrFalseOp(
1755     jfalse, OpJfalse,
1756     macro (value, target) btiz value, 1, target end)
1757 
1758 
1759 compareJumpOp(
1760     jless, OpJless,
1761     macro (left, right, target) bilt left, right, target end,
1762     macro (left, right, target) bdlt left, right, target end)
1763 
1764 
1765 compareJumpOp(
1766     jnless, OpJnless,
1767     macro (left, right, target) bigteq left, right, target end,
1768     macro (left, right, target) bdgtequn left, right, target end)
1769 
1770 
1771 compareJumpOp(
1772     jgreater, OpJgreater,
1773     macro (left, right, target) bigt left, right, target end,
1774     macro (left, right, target) bdgt left, right, target end)
1775 
1776 
1777 compareJumpOp(
1778     jngreater, OpJngreater,
1779     macro (left, right, target) bilteq left, right, target end,
1780     macro (left, right, target) bdltequn left, right, target end)
1781 
1782 
1783 compareJumpOp(
1784     jlesseq, OpJlesseq,
1785     macro (left, right, target) bilteq left, right, target end,
1786     macro (left, right, target) bdlteq left, right, target end)
1787 
1788 
1789 compareJumpOp(
1790     jnlesseq, OpJnlesseq,
1791     macro (left, right, target) bigt left, right, target end,
1792     macro (left, right, target) bdgtun left, right, target end)
1793 
1794 
1795 compareJumpOp(
1796     jgreatereq, OpJgreatereq,
1797     macro (left, right, target) bigteq left, right, target end,
1798     macro (left, right, target) bdgteq left, right, target end)
1799 
1800 
1801 compareJumpOp(
1802     jngreatereq, OpJngreatereq,
1803     macro (left, right, target) bilt left, right, target end,
1804     macro (left, right, target) bdltun left, right, target end)
1805 
1806 
1807 equalityJumpOp(
1808     jeq, OpJeq,
1809     macro (left, right, target) bieq left, right, target end)
1810 
1811 
1812 equalityJumpOp(
1813     jneq, OpJneq,
1814     macro (left, right, target) bineq left, right, target end)
1815 
1816 
1817 compareUnsignedJumpOp(
1818     jbelow, OpJbelow,
1819     macro (left, right, target) bib left, right, target end)
1820 
1821 
1822 compareUnsignedJumpOp(
1823     jbeloweq, OpJbeloweq,
1824     macro (left, right, target) bibeq left, right, target end)
1825 
1826 
1827 preOp(inc, OpInc,
1828     macro (value, slow) baddio 1, value, slow end)
1829 
1830 preOp(dec, OpDec,
1831     macro (value, slow) bsubio 1, value, slow end)
1832 
1833 
1834 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
1835     checkSwitchToJITForLoop()
1836     dispatch()
1837 end)
1838 
1839 
1840 llintOp(op_check_traps, OpCheckTraps, macro (unused, unused, dispatch)
1841     loadp CodeBlock[cfr], t1
1842     loadp CodeBlock::m_vm[t1], t1
1843     loadb VM::m_traps+VMTraps::m_needTrapHandling[t1], t0
1844     btpnz t0, .handleTraps
1845 .afterHandlingTraps:
1846     dispatch()
1847 .handleTraps:
1848     callTrapHandler(.throwHandler)
1849     jmp .afterHandlingTraps
1850 .throwHandler:
1851     jmp _llint_throw_from_slow_path_trampoline
1852 end)
1853 
1854 
1855 # Returns the packet pointer in t0.
1856 macro acquireShadowChickenPacket(slow)
1857     loadp CodeBlock[cfr], t1
1858     loadp CodeBlock::m_vm[t1], t1
1859     loadp VM::m_shadowChicken[t1], t2
1860     loadp ShadowChicken::m_logCursor[t2], t0
1861     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1862     addp sizeof ShadowChicken::Packet, t0, t1
1863     storep t1, ShadowChicken::m_logCursor[t2]
1864 end
1865 
1866 
1867 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1868     dispatch()
1869 end)
1870 
1871 
1872 # we can&#39;t use callOp because we can&#39;t pass `call` as the opcode name, since it&#39;s an instruction name
1873 commonCallOp(op_call, _llint_slow_path_call, OpCall, prepareForRegularCall, macro (getu, metadata)
1874     arrayProfileForCall(OpCall, getu)
1875 end)
1876 
1877 
1878 macro callOp(opcodeName, opcodeStruct, prepareCall, fn)
1879     commonCallOp(op_%opcodeName%, _llint_slow_path_%opcodeName%, opcodeStruct, prepareCall, fn)
1880 end
1881 
1882 
1883 callOp(tail_call, OpTailCall, prepareForTailCall, macro (getu, metadata)
1884     arrayProfileForCall(OpTailCall, getu)
1885     checkSwitchToJITForEpilogue()
1886     # reload metadata since checkSwitchToJITForEpilogue() might have trashed t5
1887     metadata(t5, t0)
1888 end)
1889 
1890 
1891 callOp(construct, OpConstruct, prepareForRegularCall, macro (getu, metadata) end)
1892 
1893 
1894 macro branchIfException(exceptionTarget)
1895     loadp CodeBlock[cfr], t3
1896     loadp CodeBlock::m_vm[t3], t3
1897     btpz VM::m_exception[t3], .noException
1898     jmp exceptionTarget
1899 .noException:
1900 end
1901 
1902 macro doCallVarargs(opcodeName, size, opcodeStruct, dispatch, frameSlowPath, slowPath, prepareCall)
1903     callSlowPath(frameSlowPath)
1904     branchIfException(_llint_throw_from_slow_path_trampoline)
1905     # calleeFrame in r1
1906     if JSVALUE64
1907         move r1, sp
1908     else
1909         # The calleeFrame is not stack aligned, move down by CallerFrameAndPCSize to align
1910         if ARMv7
1911             subp r1, CallerFrameAndPCSize, t2
1912             move t2, sp
1913         else
1914             subp r1, CallerFrameAndPCSize, sp
1915         end
1916     end
1917     slowPathForCall(opcodeName, size, opcodeStruct, dispatch, slowPath, prepareCall)
1918 end
1919 
1920 
1921 llintOp(op_call_varargs, OpCallVarargs, macro (size, get, dispatch)
1922     doCallVarargs(op_call_varargs, size, OpCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_call_varargs, prepareForRegularCall)
1923 end)
1924 
1925 llintOp(op_tail_call_varargs, OpTailCallVarargs, macro (size, get, dispatch)
1926     checkSwitchToJITForEpilogue()
1927     # We lie and perform the tail call instead of preparing it since we can&#39;t
1928     # prepare the frame for a call opcode
1929     doCallVarargs(op_tail_call_varargs, size, OpTailCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_tail_call_varargs, prepareForTailCall)
1930 end)
1931 
1932 
1933 llintOp(op_tail_call_forward_arguments, OpTailCallForwardArguments, macro (size, get, dispatch)
1934     checkSwitchToJITForEpilogue()
1935     # We lie and perform the tail call instead of preparing it since we can&#39;t
1936     # prepare the frame for a call opcode
1937     doCallVarargs(op_tail_call_forward_arguments, size, OpTailCallForwardArguments, dispatch, _llint_slow_path_size_frame_for_forward_arguments, _llint_slow_path_tail_call_forward_arguments, prepareForTailCall)
1938 end)
1939 
1940 
1941 llintOp(op_construct_varargs, OpConstructVarargs, macro (size, get, dispatch)
1942     doCallVarargs(op_construct_varargs, size, OpConstructVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_construct_varargs, prepareForRegularCall)
1943 end)
1944 
1945 
1946 # Eval is executed in one of two modes:
1947 #
1948 # 1) We find that we&#39;re really invoking eval() in which case the
1949 #    execution is perfomed entirely inside the slow_path, and it
1950 #    returns the PC of a function that just returns the return value
1951 #    that the eval returned.
1952 #
1953 # 2) We find that we&#39;re invoking something called eval() that is not
1954 #    the real eval. Then the slow_path returns the PC of the thing to
1955 #    call, and we call it.
1956 #
1957 # This allows us to handle two cases, which would require a total of
1958 # up to four pieces of state that cannot be easily packed into two
1959 # registers (C functions can return up to two registers, easily):
1960 #
1961 # - The call frame register. This may or may not have been modified
1962 #   by the slow_path, but the convention is that it returns it. It&#39;s not
1963 #   totally clear if that&#39;s necessary, since the cfr is callee save.
1964 #   But that&#39;s our style in this here interpreter so we stick with it.
1965 #
1966 # - A bit to say if the slow_path successfully executed the eval and has
1967 #   the return value, or did not execute the eval but has a PC for us
1968 #   to call.
1969 #
1970 # - Either:
1971 #   - The JS return value (two registers), or
1972 #
1973 #   - The PC to call.
1974 #
1975 # It turns out to be easier to just always have this return the cfr
1976 # and a PC to call, and that PC may be a dummy thunk that just
1977 # returns the JS value that the eval returned.
1978 
1979 _llint_op_call_eval:
1980     slowPathForCall(
1981         op_call_eval_narrow,
1982         narrow,
1983         OpCallEval,
1984         macro () dispatchOp(narrow, op_call_eval) end,
1985         _llint_slow_path_call_eval,
1986         prepareForRegularCall)
1987 
1988 _llint_op_call_eval_wide16:
1989     slowPathForCall(
1990         op_call_eval_wide16,
1991         wide16,
1992         OpCallEval,
1993         macro () dispatchOp(wide16, op_call_eval) end,
1994         _llint_slow_path_call_eval_wide16,
1995         prepareForRegularCall)
1996 
1997 _llint_op_call_eval_wide32:
1998     slowPathForCall(
1999         op_call_eval_wide32,
2000         wide32,
2001         OpCallEval,
2002         macro () dispatchOp(wide32, op_call_eval) end,
2003         _llint_slow_path_call_eval_wide32,
2004         prepareForRegularCall)
2005 
2006 
2007 commonOp(llint_generic_return_point, macro () end, macro (size)
2008     dispatchAfterCall(size, OpCallEval, macro ()
2009         dispatchOp(size, op_call_eval)
2010     end)
2011 end)
2012 
2013 
2014 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
2015     dispatch()
2016 end)
2017 
2018 
2019 llintOp(op_yield, OpYield, macro (unused, unused, unused)
2020     notSupported()
2021 end)
2022 
2023 
2024 llintOp(op_create_generator_frame_environment, OpYield, macro (unused, unused, unused)
2025     notSupported()
2026 end)
2027 
2028 
2029 llintOp(op_debug, OpDebug, macro (unused, unused, dispatch)
2030     loadp CodeBlock[cfr], t0
2031     loadi CodeBlock::m_debuggerRequests[t0], t0
2032     btiz t0, .opDebugDone
2033     callSlowPath(_llint_slow_path_debug)
2034 .opDebugDone:                    
2035     dispatch()
2036 end)
2037 
2038 
2039 op(llint_native_call_trampoline, macro ()
2040     nativeCallTrampoline(NativeExecutable::m_function)
2041 end)
2042 
2043 
2044 op(llint_native_construct_trampoline, macro ()
2045     nativeCallTrampoline(NativeExecutable::m_constructor)
2046 end)
2047 
2048 
2049 op(llint_internal_function_call_trampoline, macro ()
2050     internalFunctionCallTrampoline(InternalFunction::m_functionForCall)
2051 end)
2052 
2053 
2054 op(llint_internal_function_construct_trampoline, macro ()
2055     internalFunctionCallTrampoline(InternalFunction::m_functionForConstruct)
2056 end)
2057 
2058 
2059 op(checkpoint_osr_exit_from_inlined_call_trampoline, macro ()
2060     if (JSVALUE64 and not (C_LOOP or C_LOOP_WIN)) or ARMv7 or MIPS
2061         restoreStackPointerAfterCall()
2062 
2063         # Make sure we move r0 to a1 first since r0 might be the same as a0, for instance, on arm.
2064         if ARMv7 or MIPS
2065             # Given _slow_path_checkpoint_osr_exit_from_inlined_call has
2066             # parameters as CallFrame* and EncodedJSValue,
2067             # we need to store call result on a2, a3 and call frame on a0,
2068             # leaving a1 as dummy value.
2069             move r1, a3
2070             move r0, a2
2071             move cfr, a0
2072             # We don&#39;t call saveStateForCCall() because we are going to use the bytecodeIndex from our side state.
2073             cCall4(_slow_path_checkpoint_osr_exit_from_inlined_call)
2074         else
2075             move r0, a1
2076             move cfr, a0
2077             # We don&#39;t call saveStateForCCall() because we are going to use the bytecodeIndex from our side state.
2078             cCall2(_slow_path_checkpoint_osr_exit_from_inlined_call)
2079         end
2080 
2081         restoreStateAfterCCall()
2082         branchIfException(_llint_throw_from_slow_path_trampoline)
2083         jmp r1, JSEntryPtrTag
2084     else
2085         notSupported()
2086     end
2087 end)
2088 
2089 op(checkpoint_osr_exit_trampoline, macro ()
2090     # FIXME: We can probably dispatch to the checkpoint handler directly but this was easier
2091     # and probably doesn&#39;t matter for performance.
2092     if (JSVALUE64 and not (C_LOOP or C_LOOP_WIN)) or ARMv7 or MIPS
2093         restoreStackPointerAfterCall()
2094 
2095         move cfr, a0
2096         # We don&#39;t call saveStateForCCall() because we are going to use the bytecodeIndex from our side state.
2097         cCall2(_slow_path_checkpoint_osr_exit)
2098         restoreStateAfterCCall()
2099         branchIfException(_llint_throw_from_slow_path_trampoline)
2100         jmp r1, JSEntryPtrTag
2101     else
2102         notSupported()
2103     end
2104 end)
2105 
2106 # Lastly, make sure that we can link even though we don&#39;t support all opcodes.
2107 # These opcodes should never arise when using LLInt or either JIT. We assert
2108 # as much.
2109 
2110 macro notSupported()
2111     if ASSERT_ENABLED
2112         crash()
2113     else
2114         # We should use whatever the smallest possible instruction is, just to
2115         # ensure that there is a gap between instruction labels. If multiple
2116         # smallest instructions exist, we should pick the one that is most
2117         # likely result in execution being halted. Currently that is the break
2118         # instruction on all architectures we&#39;re interested in. (Break is int3
2119         # on Intel, which is 1 byte, and bkpt on ARMv7, which is 2 bytes.)
2120         break
2121     end
2122 end
2123 
2124 // FIXME: We should not need the X86_64_WIN condition here, since WEBASSEMBLY should already be false on Windows
2125 // https://bugs.webkit.org/show_bug.cgi?id=203716
2126 if WEBASSEMBLY and not X86_64_WIN
2127 
2128 entry(wasm, macro()
2129     include InitWasm
2130 end)
2131 
2132 macro wasmScope()
2133     # Wrap the script in a macro since it overwrites some of the LLInt macros,
2134     # but we don&#39;t want to interfere with the LLInt opcodes
2135     include WebAssembly
2136 end
2137 wasmScope()
2138 
2139 else
2140 
2141 # These need to be defined even when WebAssembly is disabled
2142 op(wasm_function_prologue, macro ()
2143     crash()
2144 end)
2145 
2146 op(wasm_function_prologue_no_tls, macro ()
2147     crash()
2148 end)
2149 
2150 end
    </pre>
  </body>
</html>