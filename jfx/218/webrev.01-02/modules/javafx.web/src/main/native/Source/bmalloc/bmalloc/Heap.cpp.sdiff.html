<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="Gigacage.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2014-2018 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;Heap.h&quot;
 27 
 28 #include &quot;AvailableMemory.h&quot;
 29 #include &quot;BulkDecommit.h&quot;
 30 #include &quot;BumpAllocator.h&quot;
 31 #include &quot;Chunk.h&quot;
 32 #include &quot;CryptoRandom.h&quot;

 33 #include &quot;Environment.h&quot;
 34 #include &quot;Gigacage.h&quot;
<span class="line-modified"> 35 #include &quot;DebugHeap.h&quot;</span>
 36 #include &quot;PerProcess.h&quot;
 37 #include &quot;Scavenger.h&quot;
 38 #include &quot;SmallLine.h&quot;
 39 #include &quot;SmallPage.h&quot;
<span class="line-removed"> 40 #include &quot;VMHeap.h&quot;</span>
 41 #include &quot;bmalloc.h&quot;
 42 #include &lt;thread&gt;
 43 #include &lt;vector&gt;
 44 




 45 namespace bmalloc {
 46 
<span class="line-modified"> 47 Heap::Heap(HeapKind kind, std::lock_guard&lt;Mutex&gt;&amp;)</span>
<span class="line-modified"> 48     : m_kind(kind)</span>
<span class="line-removed"> 49     , m_vmPageSizePhysical(vmPageSizePhysical())</span>
 50 {
<span class="line-removed"> 51     RELEASE_BASSERT(vmPageSizePhysical() &gt;= smallPageSize);</span>
<span class="line-removed"> 52     RELEASE_BASSERT(vmPageSize() &gt;= vmPageSizePhysical());</span>
<span class="line-removed"> 53 </span>
<span class="line-removed"> 54     initializeLineMetadata();</span>
<span class="line-removed"> 55     initializePageMetadata();</span>
<span class="line-removed"> 56 </span>
 57     BASSERT(!Environment::get()-&gt;isDebugHeapEnabled());
 58 
 59     Gigacage::ensureGigacage();
 60 #if GIGACAGE_ENABLED
 61     if (usingGigacage()) {
 62         RELEASE_BASSERT(gigacageBasePtr());
 63         uint64_t random[2];
 64         cryptoRandom(reinterpret_cast&lt;unsigned char*&gt;(random), sizeof(random));
 65         size_t size = roundDownToMultipleOf(vmPageSize(), gigacageSize() - (random[0] % Gigacage::maximumCageSizeReductionForSlide));
 66         ptrdiff_t offset = roundDownToMultipleOf(vmPageSize(), random[1] % (gigacageSize() - size));
 67         void* base = reinterpret_cast&lt;unsigned char*&gt;(gigacageBasePtr()) + offset;
 68         m_largeFree.add(LargeRange(base, size, 0, 0));
 69     }
 70 #endif
 71 
 72     m_scavenger = Scavenger::get();
 73 }
 74 
 75 bool Heap::usingGigacage()
 76 {
 77     return isGigacage(m_kind) &amp;&amp; gigacageBasePtr();
 78 }
 79 
 80 void* Heap::gigacageBasePtr()
 81 {
 82     return Gigacage::basePtr(gigacageKind(m_kind));
 83 }
 84 
 85 size_t Heap::gigacageSize()
 86 {
 87     return Gigacage::size(gigacageKind(m_kind));
 88 }
 89 
<span class="line-modified"> 90 void Heap::initializeLineMetadata()</span>
<span class="line-removed"> 91 {</span>
<span class="line-removed"> 92     size_t sizeClassCount = bmalloc::sizeClass(smallLineSize);</span>
<span class="line-removed"> 93     size_t smallLineCount = m_vmPageSizePhysical / smallLineSize;</span>
<span class="line-removed"> 94     m_smallLineMetadata.grow(sizeClassCount * smallLineCount);</span>
<span class="line-removed"> 95 </span>
<span class="line-removed"> 96     for (size_t sizeClass = 0; sizeClass &lt; sizeClassCount; ++sizeClass) {</span>
<span class="line-removed"> 97         size_t size = objectSize(sizeClass);</span>
<span class="line-removed"> 98         LineMetadata* pageMetadata = &amp;m_smallLineMetadata[sizeClass * smallLineCount];</span>
<span class="line-removed"> 99 </span>
<span class="line-removed">100         size_t object = 0;</span>
<span class="line-removed">101         size_t line = 0;</span>
<span class="line-removed">102         while (object &lt; m_vmPageSizePhysical) {</span>
<span class="line-removed">103             line = object / smallLineSize;</span>
<span class="line-removed">104             size_t leftover = object % smallLineSize;</span>
<span class="line-removed">105 </span>
<span class="line-removed">106             size_t objectCount;</span>
<span class="line-removed">107             size_t remainder;</span>
<span class="line-removed">108             divideRoundingUp(smallLineSize - leftover, size, objectCount, remainder);</span>
<span class="line-removed">109 </span>
<span class="line-removed">110             pageMetadata[line] = { static_cast&lt;unsigned char&gt;(leftover), static_cast&lt;unsigned char&gt;(objectCount) };</span>
<span class="line-removed">111 </span>
<span class="line-removed">112             object += objectCount * size;</span>
<span class="line-removed">113         }</span>
<span class="line-removed">114 </span>
<span class="line-removed">115         // Don&#39;t allow the last object in a page to escape the page.</span>
<span class="line-removed">116         if (object &gt; m_vmPageSizePhysical) {</span>
<span class="line-removed">117             BASSERT(pageMetadata[line].objectCount);</span>
<span class="line-removed">118             --pageMetadata[line].objectCount;</span>
<span class="line-removed">119         }</span>
<span class="line-removed">120     }</span>
<span class="line-removed">121 }</span>
<span class="line-removed">122 </span>
<span class="line-removed">123 void Heap::initializePageMetadata()</span>
<span class="line-removed">124 {</span>
<span class="line-removed">125     auto computePageSize = [&amp;](size_t sizeClass) {</span>
<span class="line-removed">126         size_t size = objectSize(sizeClass);</span>
<span class="line-removed">127         if (sizeClass &lt; bmalloc::sizeClass(smallLineSize))</span>
<span class="line-removed">128             return m_vmPageSizePhysical;</span>
<span class="line-removed">129 </span>
<span class="line-removed">130         for (size_t pageSize = m_vmPageSizePhysical;</span>
<span class="line-removed">131             pageSize &lt; pageSizeMax;</span>
<span class="line-removed">132             pageSize += m_vmPageSizePhysical) {</span>
<span class="line-removed">133             RELEASE_BASSERT(pageSize &lt;= chunkSize / 2);</span>
<span class="line-removed">134             size_t waste = pageSize % size;</span>
<span class="line-removed">135             if (waste &lt;= pageSize / pageSizeWasteFactor)</span>
<span class="line-removed">136                 return pageSize;</span>
<span class="line-removed">137         }</span>
<span class="line-removed">138 </span>
<span class="line-removed">139         return pageSizeMax;</span>
<span class="line-removed">140     };</span>
<span class="line-removed">141 </span>
<span class="line-removed">142     for (size_t i = 0; i &lt; sizeClassCount; ++i)</span>
<span class="line-removed">143         m_pageClasses[i] = (computePageSize(i) - 1) / smallPageSize;</span>
<span class="line-removed">144 }</span>
<span class="line-removed">145 </span>
<span class="line-removed">146 size_t Heap::freeableMemory(std::lock_guard&lt;Mutex&gt;&amp;)</span>
147 {
148     return m_freeableMemory;
149 }
150 
151 size_t Heap::footprint()
152 {
153     return m_footprint;
154 }
155 
<span class="line-modified">156 void Heap::markAllLargeAsEligibile(std::lock_guard&lt;Mutex&gt;&amp;)</span>
157 {
158     m_largeFree.markAllAsEligibile();
159     m_hasPendingDecommits = false;
160     m_condition.notify_all();
161 }
162 
<span class="line-modified">163 void Heap::decommitLargeRange(std::lock_guard&lt;Mutex&gt;&amp;, LargeRange&amp; range, BulkDecommit&amp; decommitter)</span>
164 {
165     m_footprint -= range.totalPhysicalSize();
166     m_freeableMemory -= range.totalPhysicalSize();
167     decommitter.addLazy(range.begin(), range.size());
168     m_hasPendingDecommits = true;
169     range.setStartPhysicalSize(0);
170     range.setTotalPhysicalSize(0);
171     BASSERT(range.isEligibile());
172     range.setEligible(false);
173 #if ENABLE_PHYSICAL_PAGE_MAP
174     m_physicalPageMap.decommit(range.begin(), range.size());
175 #endif
176 }
177 
<span class="line-modified">178 void Heap::scavenge(std::lock_guard&lt;Mutex&gt;&amp; lock, BulkDecommit&amp; decommitter, size_t&amp; deferredDecommits)</span>




179 {
180     for (auto&amp; list : m_freePages) {
181         for (auto* chunk : list) {
182             for (auto* page : chunk-&gt;freePages()) {
183                 if (!page-&gt;hasPhysicalPages())
184                     continue;

185                 if (page-&gt;usedSinceLastScavenge()) {
186                     page-&gt;clearUsedSinceLastScavenge();
187                     deferredDecommits++;
188                     continue;
189                 }

190 
191                 size_t pageSize = bmalloc::pageSize(&amp;list - &amp;m_freePages[0]);
192                 size_t decommitSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
193                 m_freeableMemory -= decommitSize;
194                 m_footprint -= decommitSize;
195                 decommitter.addEager(page-&gt;begin()-&gt;begin(), pageSize);
196                 page-&gt;setHasPhysicalPages(false);
197 #if ENABLE_PHYSICAL_PAGE_MAP
198                 m_physicalPageMap.decommit(page-&gt;begin()-&gt;begin(), pageSize);
199 #endif
200             }
201         }
202     }
203 
204     for (auto&amp; list : m_chunkCache) {
205         while (!list.isEmpty())
206             deallocateSmallChunk(list.pop(), &amp;list - &amp;m_chunkCache[0]);
207     }
208 
209     for (LargeRange&amp; range : m_largeFree) {



210         if (range.usedSinceLastScavenge()) {
211             range.clearUsedSinceLastScavenge();
212             deferredDecommits++;
213             continue;
214         }

215         decommitLargeRange(lock, range, decommitter);
216     }




217 }
218 
<span class="line-modified">219 void Heap::deallocateLineCache(std::unique_lock&lt;Mutex&gt;&amp;, LineCache&amp; lineCache)</span>














220 {
221     for (auto&amp; list : lineCache) {
222         while (!list.isEmpty()) {
223             size_t sizeClass = &amp;list - &amp;lineCache[0];
224             m_lineCache[sizeClass].push(list.popFront());
225         }
226     }
227 }
228 
<span class="line-modified">229 void Heap::allocateSmallChunk(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t pageClass)</span>
230 {
231     RELEASE_BASSERT(isActiveHeapKind(m_kind));
232 
233     size_t pageSize = bmalloc::pageSize(pageClass);
234 
<span class="line-modified">235     Chunk* chunk = [&amp;]() {</span>
236         if (!m_chunkCache[pageClass].isEmpty())
237             return m_chunkCache[pageClass].pop();
238 
<span class="line-modified">239         void* memory = allocateLarge(lock, chunkSize, chunkSize);</span>




240 
241         Chunk* chunk = new (memory) Chunk(pageSize);
242 
243         m_objectTypes.set(chunk, ObjectType::Small);
244 

245         forEachPage(chunk, pageSize, [&amp;](SmallPage* page) {
246             page-&gt;setHasPhysicalPages(true);

247             page-&gt;setUsedSinceLastScavenge();

248             page-&gt;setHasFreeLines(lock, true);
249             chunk-&gt;freePages().push(page);

250         });
251 
<span class="line-modified">252         m_freeableMemory += chunkSize;</span>







253 
254         m_scavenger-&gt;schedule(0);
255 
256         return chunk;
257     }();
258 
<span class="line-modified">259     m_freePages[pageClass].push(chunk);</span>

260 }
261 
262 void Heap::deallocateSmallChunk(Chunk* chunk, size_t pageClass)
263 {
264     m_objectTypes.set(chunk, ObjectType::Large);
265 
266     size_t size = m_largeAllocated.remove(chunk);
267     size_t totalPhysicalSize = size;
268 
269     size_t accountedInFreeable = 0;
270 
271     bool hasPhysicalPages = true;
272     forEachPage(chunk, pageSize(pageClass), [&amp;](SmallPage* page) {
273         size_t physicalSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize(pageClass));
274         if (!page-&gt;hasPhysicalPages()) {
275             totalPhysicalSize -= physicalSize;
276             hasPhysicalPages = false;
277         } else
278             accountedInFreeable += physicalSize;
279     });
280 
281     m_freeableMemory -= accountedInFreeable;
282     m_freeableMemory += totalPhysicalSize;
283 
284     size_t startPhysicalSize = hasPhysicalPages ? size : 0;
285     m_largeFree.add(LargeRange(chunk, size, startPhysicalSize, totalPhysicalSize));
286 }
287 
<span class="line-modified">288 SmallPage* Heap::allocateSmallPage(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass, LineCache&amp; lineCache)</span>
289 {
290     RELEASE_BASSERT(isActiveHeapKind(m_kind));
291 
292     if (!lineCache[sizeClass].isEmpty())
293         return lineCache[sizeClass].popFront();
294 
295     if (!m_lineCache[sizeClass].isEmpty())
296         return m_lineCache[sizeClass].popFront();
297 
298     m_scavenger-&gt;didStartGrowing();
299 
<span class="line-modified">300     SmallPage* page = [&amp;]() {</span>
<span class="line-modified">301         size_t pageClass = m_pageClasses[sizeClass];</span>
302 
303         if (m_freePages[pageClass].isEmpty())
<span class="line-modified">304             allocateSmallChunk(lock, pageClass);</span>


305 
306         Chunk* chunk = m_freePages[pageClass].tail();
307 
308         chunk-&gt;ref();
309 
310         SmallPage* page = chunk-&gt;freePages().pop();
311         if (chunk-&gt;freePages().isEmpty())
312             m_freePages[pageClass].remove(chunk);
313 
314         size_t pageSize = bmalloc::pageSize(pageClass);
315         size_t physicalSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
316         if (page-&gt;hasPhysicalPages())
317             m_freeableMemory -= physicalSize;
318         else {
319             m_scavenger-&gt;scheduleIfUnderMemoryPressure(pageSize);
320             m_footprint += physicalSize;
321             vmAllocatePhysicalPagesSloppy(page-&gt;begin()-&gt;begin(), pageSize);
322             page-&gt;setHasPhysicalPages(true);
323 #if ENABLE_PHYSICAL_PAGE_MAP
324             m_physicalPageMap.commit(page-&gt;begin()-&gt;begin(), pageSize);
325 #endif
326         }

327         page-&gt;setUsedSinceLastScavenge();

328 
329         return page;
330     }();




331 
332     page-&gt;setSizeClass(sizeClass);
333     return page;
334 }
335 
<span class="line-modified">336 void Heap::deallocateSmallLine(std::unique_lock&lt;Mutex&gt;&amp; lock, Object object, LineCache&amp; lineCache)</span>
337 {
338     BASSERT(!object.line()-&gt;refCount(lock));
339     SmallPage* page = object.page();
340     page-&gt;deref(lock);
341 
342     if (!page-&gt;hasFreeLines(lock)) {
343         page-&gt;setHasFreeLines(lock, true);
344         lineCache[page-&gt;sizeClass()].push(page);
345     }
346 
347     if (page-&gt;refCount(lock))
348         return;
349 
<span class="line-modified">350     size_t sizeClass = page-&gt;sizeClass();</span>
<span class="line-removed">351     size_t pageClass = m_pageClasses[sizeClass];</span>
352 
353     m_freeableMemory += physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize(pageClass));
354 
355     List&lt;SmallPage&gt;::remove(page); // &#39;page&#39; may be in any thread&#39;s line cache.
356 
357     Chunk* chunk = Chunk::get(page);
358     if (chunk-&gt;freePages().isEmpty())
359         m_freePages[pageClass].push(chunk);
360     chunk-&gt;freePages().push(page);
361 
362     chunk-&gt;deref();
363 
364     if (!chunk-&gt;refCount()) {
365         m_freePages[pageClass].remove(chunk);
366 
367         if (!m_chunkCache[pageClass].isEmpty())
368             deallocateSmallChunk(m_chunkCache[pageClass].pop(), pageClass);
369 
370         m_chunkCache[pageClass].push(chunk);
371     }
372 
373     m_scavenger-&gt;schedule(pageSize(pageClass));
374 }
375 
376 void Heap::allocateSmallBumpRangesByMetadata(
<span class="line-modified">377     std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass,</span>
378     BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">379     LineCache&amp; lineCache)</span>
380 {

381     RELEASE_BASSERT(isActiveHeapKind(m_kind));
382 
<span class="line-modified">383     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache);</span>




384     SmallLine* lines = page-&gt;begin();
385     BASSERT(page-&gt;hasFreeLines(lock));
<span class="line-removed">386     size_t smallLineCount = m_vmPageSizePhysical / smallLineSize;</span>
<span class="line-removed">387     LineMetadata* pageMetadata = &amp;m_smallLineMetadata[sizeClass * smallLineCount];</span>
388 
389     auto findSmallBumpRange = [&amp;](size_t&amp; lineNumber) {
<span class="line-modified">390         for ( ; lineNumber &lt; smallLineCount; ++lineNumber) {</span>
391             if (!lines[lineNumber].refCount(lock)) {
<span class="line-modified">392                 if (pageMetadata[lineNumber].objectCount)</span>
393                     return true;
394             }
395         }
396         return false;
397     };
398 
399     auto allocateSmallBumpRange = [&amp;](size_t&amp; lineNumber) -&gt; BumpRange {
<span class="line-modified">400         char* begin = lines[lineNumber].begin() + pageMetadata[lineNumber].startOffset;</span>
401         unsigned short objectCount = 0;
402 
<span class="line-modified">403         for ( ; lineNumber &lt; smallLineCount; ++lineNumber) {</span>
404             if (lines[lineNumber].refCount(lock))
405                 break;
406 
<span class="line-modified">407             if (!pageMetadata[lineNumber].objectCount)</span>

408                 continue;
409 
<span class="line-modified">410             objectCount += pageMetadata[lineNumber].objectCount;</span>
<span class="line-modified">411             lines[lineNumber].ref(lock, pageMetadata[lineNumber].objectCount);</span>
412             page-&gt;ref(lock);
413         }
414         return { begin, objectCount };
415     };
416 
417     size_t lineNumber = 0;
418     for (;;) {
419         if (!findSmallBumpRange(lineNumber)) {
420             page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">421             BASSERT(allocator.canAllocate());</span>
422             return;
423         }
424 
425         // In a fragmented page, some free ranges might not fit in the cache.
426         if (rangeCache.size() == rangeCache.capacity()) {
427             lineCache[sizeClass].push(page);
<span class="line-modified">428             BASSERT(allocator.canAllocate());</span>
429             return;
430         }
431 
432         BumpRange bumpRange = allocateSmallBumpRange(lineNumber);
433         if (allocator.canAllocate())
434             rangeCache.push(bumpRange);
435         else
436             allocator.refill(bumpRange);
437     }
438 }
439 
440 void Heap::allocateSmallBumpRangesByObject(
<span class="line-modified">441     std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass,</span>
442     BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">443     LineCache&amp; lineCache)</span>
444 {

445     RELEASE_BASSERT(isActiveHeapKind(m_kind));
446 
447     size_t size = allocator.size();
<span class="line-modified">448     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache);</span>




449     BASSERT(page-&gt;hasFreeLines(lock));
450 
451     auto findSmallBumpRange = [&amp;](Object&amp; it, Object&amp; end) {
452         for ( ; it + size &lt;= end; it = it + size) {
453             if (!it.line()-&gt;refCount(lock))
454                 return true;
455         }
456         return false;
457     };
458 
459     auto allocateSmallBumpRange = [&amp;](Object&amp; it, Object&amp; end) -&gt; BumpRange {
460         char* begin = it.address();
461         unsigned short objectCount = 0;
462         for ( ; it + size &lt;= end; it = it + size) {
463             if (it.line()-&gt;refCount(lock))
464                 break;
465 
466             ++objectCount;
467             it.line()-&gt;ref(lock);
468             it.page()-&gt;ref(lock);
469         }
470         return { begin, objectCount };
471     };
472 
473     Object it(page-&gt;begin()-&gt;begin());
<span class="line-modified">474     Object end(it + pageSize(m_pageClasses[sizeClass]));</span>
475     for (;;) {
476         if (!findSmallBumpRange(it, end)) {
477             page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">478             BASSERT(allocator.canAllocate());</span>
479             return;
480         }
481 
482         // In a fragmented page, some free ranges might not fit in the cache.
483         if (rangeCache.size() == rangeCache.capacity()) {
484             lineCache[sizeClass].push(page);
<span class="line-modified">485             BASSERT(allocator.canAllocate());</span>
486             return;
487         }
488 
489         BumpRange bumpRange = allocateSmallBumpRange(it, end);
490         if (allocator.canAllocate())
491             rangeCache.push(bumpRange);
492         else
493             allocator.refill(bumpRange);
494     }
495 }
496 
<span class="line-modified">497 LargeRange Heap::splitAndAllocate(std::unique_lock&lt;Mutex&gt;&amp;, LargeRange&amp; range, size_t alignment, size_t size)</span>
498 {
499     RELEASE_BASSERT(isActiveHeapKind(m_kind));
500 
501     LargeRange prev;
502     LargeRange next;
503 
504     size_t alignmentMask = alignment - 1;
505     if (test(range.begin(), alignmentMask)) {
506         size_t prefixSize = roundUpToMultipleOf(alignment, range.begin()) - range.begin();
507         std::pair&lt;LargeRange, LargeRange&gt; pair = range.split(prefixSize);
508         prev = pair.first;
509         range = pair.second;
510     }
511 
512     if (range.size() - size &gt; size / pageSizeWasteFactor) {
513         std::pair&lt;LargeRange, LargeRange&gt; pair = range.split(size);
514         range = pair.first;
515         next = pair.second;
516     }
517 
</pre>
<hr />
<pre>
525         m_physicalPageMap.commit(range.begin(), range.size());
526 #endif
527     }
528 
529     if (prev) {
530         m_freeableMemory += prev.totalPhysicalSize();
531         m_largeFree.add(prev);
532     }
533 
534     if (next) {
535         m_freeableMemory += next.totalPhysicalSize();
536         m_largeFree.add(next);
537     }
538 
539     m_objectTypes.set(Chunk::get(range.begin()), ObjectType::Large);
540 
541     m_largeAllocated.set(range.begin(), range.size());
542     return range;
543 }
544 
<span class="line-modified">545 void* Heap::tryAllocateLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t alignment, size_t size)</span>
546 {








547     RELEASE_BASSERT(isActiveHeapKind(m_kind));
548 
549     BASSERT(isPowerOfTwo(alignment));
550 
551     m_scavenger-&gt;didStartGrowing();
552 
553     size_t roundedSize = size ? roundUpToMultipleOf(largeAlignment, size) : largeAlignment;
<span class="line-modified">554     if (roundedSize &lt; size) // Check for overflow</span>
<span class="line-removed">555         return nullptr;</span>
556     size = roundedSize;
557 
558     size_t roundedAlignment = roundUpToMultipleOf&lt;largeAlignment&gt;(alignment);
<span class="line-modified">559     if (roundedAlignment &lt; alignment) // Check for overflow</span>
<span class="line-removed">560         return nullptr;</span>
561     alignment = roundedAlignment;
562 
563     LargeRange range = m_largeFree.remove(alignment, size);
564     if (!range) {
565         if (m_hasPendingDecommits) {
566             m_condition.wait(lock, [&amp;]() { return !m_hasPendingDecommits; });
567             // Now we&#39;re guaranteed we&#39;re looking at all available memory.
<span class="line-modified">568             return tryAllocateLarge(lock, alignment, size);</span>
569         }
570 
<span class="line-modified">571         if (usingGigacage())</span>
<span class="line-removed">572             return nullptr;</span>
573 
<span class="line-modified">574         range = VMHeap::get()-&gt;tryAllocateLargeChunk(alignment, size);</span>
<span class="line-modified">575         if (!range)</span>
<span class="line-removed">576             return nullptr;</span>
577 
578         m_largeFree.add(range);
579         range = m_largeFree.remove(alignment, size);
580     }
581 
582     m_freeableMemory -= range.totalPhysicalSize();
583 
584     void* result = splitAndAllocate(lock, range, alignment, size).begin();




585     return result;


586 }
587 
<span class="line-modified">588 void* Heap::allocateLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t alignment, size_t size)</span>
589 {
<span class="line-modified">590     void* result = tryAllocateLarge(lock, alignment, size);</span>
<span class="line-modified">591     RELEASE_BASSERT(result);</span>
<span class="line-modified">592     return result;</span>


















593 }
594 
<span class="line-modified">595 bool Heap::isLarge(std::unique_lock&lt;Mutex&gt;&amp;, void* object)</span>
596 {
597     return m_objectTypes.get(Object(object).chunk()) == ObjectType::Large;
598 }
599 
<span class="line-modified">600 size_t Heap::largeSize(std::unique_lock&lt;Mutex&gt;&amp;, void* object)</span>
601 {
602     return m_largeAllocated.get(object);
603 }
604 
<span class="line-modified">605 void Heap::shrinkLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, const Range&amp; object, size_t newSize)</span>
606 {
607     BASSERT(object.size() &gt; newSize);
608 
609     size_t size = m_largeAllocated.remove(object.begin());
610     LargeRange range = LargeRange(object, size, size);
611     splitAndAllocate(lock, range, alignment, newSize);
612 
613     m_scavenger-&gt;schedule(size);
614 }
615 
<span class="line-modified">616 void Heap::deallocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, void* object)</span>
617 {
618     size_t size = m_largeAllocated.remove(object);
619     m_largeFree.add(LargeRange(object, size, size, size));
620     m_freeableMemory += size;
621     m_scavenger-&gt;schedule(size);
622 }
623 
624 void Heap::externalCommit(void* ptr, size_t size)
625 {
<span class="line-modified">626     std::unique_lock&lt;Mutex&gt; lock(Heap::mutex());</span>
627     externalCommit(lock, ptr, size);
628 }
629 
<span class="line-modified">630 void Heap::externalCommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t size)</span>
631 {
632     BUNUSED_PARAM(ptr);
633 
634     m_footprint += size;
635 #if ENABLE_PHYSICAL_PAGE_MAP
636     m_physicalPageMap.commit(ptr, size);
637 #endif
638 }
639 
640 void Heap::externalDecommit(void* ptr, size_t size)
641 {
<span class="line-modified">642     std::unique_lock&lt;Mutex&gt; lock(Heap::mutex());</span>
643     externalDecommit(lock, ptr, size);
644 }
645 
<span class="line-modified">646 void Heap::externalDecommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t size)</span>
647 {
648     BUNUSED_PARAM(ptr);
649 
650     m_footprint -= size;
651 #if ENABLE_PHYSICAL_PAGE_MAP
652     m_physicalPageMap.decommit(ptr, size);
653 #endif
654 }
655 
656 } // namespace bmalloc
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2014-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;Heap.h&quot;
 27 
 28 #include &quot;AvailableMemory.h&quot;
 29 #include &quot;BulkDecommit.h&quot;
 30 #include &quot;BumpAllocator.h&quot;
 31 #include &quot;Chunk.h&quot;
 32 #include &quot;CryptoRandom.h&quot;
<span class="line-added"> 33 #include &quot;DebugHeap.h&quot;</span>
 34 #include &quot;Environment.h&quot;
 35 #include &quot;Gigacage.h&quot;
<span class="line-modified"> 36 #include &quot;HeapConstants.h&quot;</span>
 37 #include &quot;PerProcess.h&quot;
 38 #include &quot;Scavenger.h&quot;
 39 #include &quot;SmallLine.h&quot;
 40 #include &quot;SmallPage.h&quot;

 41 #include &quot;bmalloc.h&quot;
 42 #include &lt;thread&gt;
 43 #include &lt;vector&gt;
 44 
<span class="line-added"> 45 #if BOS(DARWIN)</span>
<span class="line-added"> 46 #include &quot;Zone.h&quot;</span>
<span class="line-added"> 47 #endif</span>
<span class="line-added"> 48 </span>
 49 namespace bmalloc {
 50 
<span class="line-modified"> 51 Heap::Heap(HeapKind kind, LockHolder&amp;)</span>
<span class="line-modified"> 52     : m_kind { kind }, m_constants { *HeapConstants::get() }</span>

 53 {






 54     BASSERT(!Environment::get()-&gt;isDebugHeapEnabled());
 55 
 56     Gigacage::ensureGigacage();
 57 #if GIGACAGE_ENABLED
 58     if (usingGigacage()) {
 59         RELEASE_BASSERT(gigacageBasePtr());
 60         uint64_t random[2];
 61         cryptoRandom(reinterpret_cast&lt;unsigned char*&gt;(random), sizeof(random));
 62         size_t size = roundDownToMultipleOf(vmPageSize(), gigacageSize() - (random[0] % Gigacage::maximumCageSizeReductionForSlide));
 63         ptrdiff_t offset = roundDownToMultipleOf(vmPageSize(), random[1] % (gigacageSize() - size));
 64         void* base = reinterpret_cast&lt;unsigned char*&gt;(gigacageBasePtr()) + offset;
 65         m_largeFree.add(LargeRange(base, size, 0, 0));
 66     }
 67 #endif
 68 
 69     m_scavenger = Scavenger::get();
 70 }
 71 
 72 bool Heap::usingGigacage()
 73 {
 74     return isGigacage(m_kind) &amp;&amp; gigacageBasePtr();
 75 }
 76 
 77 void* Heap::gigacageBasePtr()
 78 {
 79     return Gigacage::basePtr(gigacageKind(m_kind));
 80 }
 81 
 82 size_t Heap::gigacageSize()
 83 {
 84     return Gigacage::size(gigacageKind(m_kind));
 85 }
 86 
<span class="line-modified"> 87 size_t Heap::freeableMemory(const LockHolder&amp;)</span>
























































 88 {
 89     return m_freeableMemory;
 90 }
 91 
 92 size_t Heap::footprint()
 93 {
 94     return m_footprint;
 95 }
 96 
<span class="line-modified"> 97 void Heap::markAllLargeAsEligibile(const LockHolder&amp;)</span>
 98 {
 99     m_largeFree.markAllAsEligibile();
100     m_hasPendingDecommits = false;
101     m_condition.notify_all();
102 }
103 
<span class="line-modified">104 void Heap::decommitLargeRange(const LockHolder&amp;, LargeRange&amp; range, BulkDecommit&amp; decommitter)</span>
105 {
106     m_footprint -= range.totalPhysicalSize();
107     m_freeableMemory -= range.totalPhysicalSize();
108     decommitter.addLazy(range.begin(), range.size());
109     m_hasPendingDecommits = true;
110     range.setStartPhysicalSize(0);
111     range.setTotalPhysicalSize(0);
112     BASSERT(range.isEligibile());
113     range.setEligible(false);
114 #if ENABLE_PHYSICAL_PAGE_MAP
115     m_physicalPageMap.decommit(range.begin(), range.size());
116 #endif
117 }
118 
<span class="line-modified">119 #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">120 void Heap::scavenge(const LockHolder&amp; lock, BulkDecommit&amp; decommitter)</span>
<span class="line-added">121 #else</span>
<span class="line-added">122 void Heap::scavenge(const LockHolder&amp; lock, BulkDecommit&amp; decommitter, size_t&amp; deferredDecommits)</span>
<span class="line-added">123 #endif</span>
124 {
125     for (auto&amp; list : m_freePages) {
126         for (auto* chunk : list) {
127             for (auto* page : chunk-&gt;freePages()) {
128                 if (!page-&gt;hasPhysicalPages())
129                     continue;
<span class="line-added">130 #if !BUSE(PARTIAL_SCAVENGE)</span>
131                 if (page-&gt;usedSinceLastScavenge()) {
132                     page-&gt;clearUsedSinceLastScavenge();
133                     deferredDecommits++;
134                     continue;
135                 }
<span class="line-added">136 #endif</span>
137 
138                 size_t pageSize = bmalloc::pageSize(&amp;list - &amp;m_freePages[0]);
139                 size_t decommitSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
140                 m_freeableMemory -= decommitSize;
141                 m_footprint -= decommitSize;
142                 decommitter.addEager(page-&gt;begin()-&gt;begin(), pageSize);
143                 page-&gt;setHasPhysicalPages(false);
144 #if ENABLE_PHYSICAL_PAGE_MAP
145                 m_physicalPageMap.decommit(page-&gt;begin()-&gt;begin(), pageSize);
146 #endif
147             }
148         }
149     }
150 
151     for (auto&amp; list : m_chunkCache) {
152         while (!list.isEmpty())
153             deallocateSmallChunk(list.pop(), &amp;list - &amp;m_chunkCache[0]);
154     }
155 
156     for (LargeRange&amp; range : m_largeFree) {
<span class="line-added">157 #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">158         m_highWatermark = std::min(m_highWatermark, static_cast&lt;void*&gt;(range.begin()));</span>
<span class="line-added">159 #else</span>
160         if (range.usedSinceLastScavenge()) {
161             range.clearUsedSinceLastScavenge();
162             deferredDecommits++;
163             continue;
164         }
<span class="line-added">165 #endif</span>
166         decommitLargeRange(lock, range, decommitter);
167     }
<span class="line-added">168 </span>
<span class="line-added">169 #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">170     m_freeableMemory = 0;</span>
<span class="line-added">171 #endif</span>
172 }
173 
<span class="line-modified">174 #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">175 void Heap::scavengeToHighWatermark(const LockHolder&amp; lock, BulkDecommit&amp; decommitter)</span>
<span class="line-added">176 {</span>
<span class="line-added">177     void* newHighWaterMark = nullptr;</span>
<span class="line-added">178     for (LargeRange&amp; range : m_largeFree) {</span>
<span class="line-added">179         if (range.begin() &lt;= m_highWatermark)</span>
<span class="line-added">180             newHighWaterMark = std::min(newHighWaterMark, static_cast&lt;void*&gt;(range.begin()));</span>
<span class="line-added">181         else</span>
<span class="line-added">182             decommitLargeRange(lock, range, decommitter);</span>
<span class="line-added">183     }</span>
<span class="line-added">184     m_highWatermark = newHighWaterMark;</span>
<span class="line-added">185 }</span>
<span class="line-added">186 #endif</span>
<span class="line-added">187 </span>
<span class="line-added">188 void Heap::deallocateLineCache(UniqueLockHolder&amp;, LineCache&amp; lineCache)</span>
189 {
190     for (auto&amp; list : lineCache) {
191         while (!list.isEmpty()) {
192             size_t sizeClass = &amp;list - &amp;lineCache[0];
193             m_lineCache[sizeClass].push(list.popFront());
194         }
195     }
196 }
197 
<span class="line-modified">198 void Heap::allocateSmallChunk(UniqueLockHolder&amp; lock, size_t pageClass, FailureAction action)</span>
199 {
200     RELEASE_BASSERT(isActiveHeapKind(m_kind));
201 
202     size_t pageSize = bmalloc::pageSize(pageClass);
203 
<span class="line-modified">204     Chunk* chunk = [&amp;]() -&gt; Chunk* {</span>
205         if (!m_chunkCache[pageClass].isEmpty())
206             return m_chunkCache[pageClass].pop();
207 
<span class="line-modified">208         void* memory = allocateLarge(lock, chunkSize, chunkSize, action);</span>
<span class="line-added">209         if (!memory) {</span>
<span class="line-added">210             BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">211             return nullptr;</span>
<span class="line-added">212         }</span>
213 
214         Chunk* chunk = new (memory) Chunk(pageSize);
215 
216         m_objectTypes.set(chunk, ObjectType::Small);
217 
<span class="line-added">218         size_t accountedInFreeable = 0;</span>
219         forEachPage(chunk, pageSize, [&amp;](SmallPage* page) {
220             page-&gt;setHasPhysicalPages(true);
<span class="line-added">221 #if !BUSE(PARTIAL_SCAVENGE)</span>
222             page-&gt;setUsedSinceLastScavenge();
<span class="line-added">223 #endif</span>
224             page-&gt;setHasFreeLines(lock, true);
225             chunk-&gt;freePages().push(page);
<span class="line-added">226             accountedInFreeable += pageSize;</span>
227         });
228 
<span class="line-modified">229         m_freeableMemory += accountedInFreeable;</span>
<span class="line-added">230 </span>
<span class="line-added">231         auto metadataSize = Chunk::metadataSize(pageSize);</span>
<span class="line-added">232         vmDeallocatePhysicalPagesSloppy(chunk-&gt;address(sizeof(Chunk)), metadataSize - sizeof(Chunk));</span>
<span class="line-added">233 </span>
<span class="line-added">234         auto decommitSize = chunkSize - metadataSize - accountedInFreeable;</span>
<span class="line-added">235         if (decommitSize &gt; 0)</span>
<span class="line-added">236             vmDeallocatePhysicalPagesSloppy(chunk-&gt;address(chunkSize - decommitSize), decommitSize);</span>
237 
238         m_scavenger-&gt;schedule(0);
239 
240         return chunk;
241     }();
242 
<span class="line-modified">243     if (chunk)</span>
<span class="line-added">244         m_freePages[pageClass].push(chunk);</span>
245 }
246 
247 void Heap::deallocateSmallChunk(Chunk* chunk, size_t pageClass)
248 {
249     m_objectTypes.set(chunk, ObjectType::Large);
250 
251     size_t size = m_largeAllocated.remove(chunk);
252     size_t totalPhysicalSize = size;
253 
254     size_t accountedInFreeable = 0;
255 
256     bool hasPhysicalPages = true;
257     forEachPage(chunk, pageSize(pageClass), [&amp;](SmallPage* page) {
258         size_t physicalSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize(pageClass));
259         if (!page-&gt;hasPhysicalPages()) {
260             totalPhysicalSize -= physicalSize;
261             hasPhysicalPages = false;
262         } else
263             accountedInFreeable += physicalSize;
264     });
265 
266     m_freeableMemory -= accountedInFreeable;
267     m_freeableMemory += totalPhysicalSize;
268 
269     size_t startPhysicalSize = hasPhysicalPages ? size : 0;
270     m_largeFree.add(LargeRange(chunk, size, startPhysicalSize, totalPhysicalSize));
271 }
272 
<span class="line-modified">273 SmallPage* Heap::allocateSmallPage(UniqueLockHolder&amp; lock, size_t sizeClass, LineCache&amp; lineCache, FailureAction action)</span>
274 {
275     RELEASE_BASSERT(isActiveHeapKind(m_kind));
276 
277     if (!lineCache[sizeClass].isEmpty())
278         return lineCache[sizeClass].popFront();
279 
280     if (!m_lineCache[sizeClass].isEmpty())
281         return m_lineCache[sizeClass].popFront();
282 
283     m_scavenger-&gt;didStartGrowing();
284 
<span class="line-modified">285     SmallPage* page = [&amp;]() -&gt; SmallPage* {</span>
<span class="line-modified">286         size_t pageClass = m_constants.pageClass(sizeClass);</span>
287 
288         if (m_freePages[pageClass].isEmpty())
<span class="line-modified">289             allocateSmallChunk(lock, pageClass, action);</span>
<span class="line-added">290         if (action == FailureAction::ReturnNull &amp;&amp; m_freePages[pageClass].isEmpty())</span>
<span class="line-added">291             return nullptr;</span>
292 
293         Chunk* chunk = m_freePages[pageClass].tail();
294 
295         chunk-&gt;ref();
296 
297         SmallPage* page = chunk-&gt;freePages().pop();
298         if (chunk-&gt;freePages().isEmpty())
299             m_freePages[pageClass].remove(chunk);
300 
301         size_t pageSize = bmalloc::pageSize(pageClass);
302         size_t physicalSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
303         if (page-&gt;hasPhysicalPages())
304             m_freeableMemory -= physicalSize;
305         else {
306             m_scavenger-&gt;scheduleIfUnderMemoryPressure(pageSize);
307             m_footprint += physicalSize;
308             vmAllocatePhysicalPagesSloppy(page-&gt;begin()-&gt;begin(), pageSize);
309             page-&gt;setHasPhysicalPages(true);
310 #if ENABLE_PHYSICAL_PAGE_MAP
311             m_physicalPageMap.commit(page-&gt;begin()-&gt;begin(), pageSize);
312 #endif
313         }
<span class="line-added">314 #if !BUSE(PARTIAL_SCAVENGE)</span>
315         page-&gt;setUsedSinceLastScavenge();
<span class="line-added">316 #endif</span>
317 
318         return page;
319     }();
<span class="line-added">320     if (!page) {</span>
<span class="line-added">321         BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">322         return nullptr;</span>
<span class="line-added">323     }</span>
324 
325     page-&gt;setSizeClass(sizeClass);
326     return page;
327 }
328 
<span class="line-modified">329 void Heap::deallocateSmallLine(UniqueLockHolder&amp; lock, Object object, LineCache&amp; lineCache)</span>
330 {
331     BASSERT(!object.line()-&gt;refCount(lock));
332     SmallPage* page = object.page();
333     page-&gt;deref(lock);
334 
335     if (!page-&gt;hasFreeLines(lock)) {
336         page-&gt;setHasFreeLines(lock, true);
337         lineCache[page-&gt;sizeClass()].push(page);
338     }
339 
340     if (page-&gt;refCount(lock))
341         return;
342 
<span class="line-modified">343     size_t pageClass = m_constants.pageClass(page-&gt;sizeClass());</span>

344 
345     m_freeableMemory += physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize(pageClass));
346 
347     List&lt;SmallPage&gt;::remove(page); // &#39;page&#39; may be in any thread&#39;s line cache.
348 
349     Chunk* chunk = Chunk::get(page);
350     if (chunk-&gt;freePages().isEmpty())
351         m_freePages[pageClass].push(chunk);
352     chunk-&gt;freePages().push(page);
353 
354     chunk-&gt;deref();
355 
356     if (!chunk-&gt;refCount()) {
357         m_freePages[pageClass].remove(chunk);
358 
359         if (!m_chunkCache[pageClass].isEmpty())
360             deallocateSmallChunk(m_chunkCache[pageClass].pop(), pageClass);
361 
362         m_chunkCache[pageClass].push(chunk);
363     }
364 
365     m_scavenger-&gt;schedule(pageSize(pageClass));
366 }
367 
368 void Heap::allocateSmallBumpRangesByMetadata(
<span class="line-modified">369     UniqueLockHolder&amp; lock, size_t sizeClass,</span>
370     BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">371     LineCache&amp; lineCache, FailureAction action)</span>
372 {
<span class="line-added">373     BUNUSED(action);</span>
374     RELEASE_BASSERT(isActiveHeapKind(m_kind));
375 
<span class="line-modified">376     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache, action);</span>
<span class="line-added">377     if (!page) {</span>
<span class="line-added">378         BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">379         return;</span>
<span class="line-added">380     }</span>
381     SmallLine* lines = page-&gt;begin();
382     BASSERT(page-&gt;hasFreeLines(lock));


383 
384     auto findSmallBumpRange = [&amp;](size_t&amp; lineNumber) {
<span class="line-modified">385         for ( ; lineNumber &lt; m_constants.smallLineCount(); ++lineNumber) {</span>
386             if (!lines[lineNumber].refCount(lock)) {
<span class="line-modified">387                 if (m_constants.objectCount(sizeClass, lineNumber))</span>
388                     return true;
389             }
390         }
391         return false;
392     };
393 
394     auto allocateSmallBumpRange = [&amp;](size_t&amp; lineNumber) -&gt; BumpRange {
<span class="line-modified">395         char* begin = lines[lineNumber].begin() + m_constants.startOffset(sizeClass, lineNumber);</span>
396         unsigned short objectCount = 0;
397 
<span class="line-modified">398         for ( ; lineNumber &lt; m_constants.smallLineCount(); ++lineNumber) {</span>
399             if (lines[lineNumber].refCount(lock))
400                 break;
401 
<span class="line-modified">402             auto lineObjectCount = m_constants.objectCount(sizeClass, lineNumber);</span>
<span class="line-added">403             if (!lineObjectCount)</span>
404                 continue;
405 
<span class="line-modified">406             objectCount += lineObjectCount;</span>
<span class="line-modified">407             lines[lineNumber].ref(lock, lineObjectCount);</span>
408             page-&gt;ref(lock);
409         }
410         return { begin, objectCount };
411     };
412 
413     size_t lineNumber = 0;
414     for (;;) {
415         if (!findSmallBumpRange(lineNumber)) {
416             page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">417             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
418             return;
419         }
420 
421         // In a fragmented page, some free ranges might not fit in the cache.
422         if (rangeCache.size() == rangeCache.capacity()) {
423             lineCache[sizeClass].push(page);
<span class="line-modified">424             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
425             return;
426         }
427 
428         BumpRange bumpRange = allocateSmallBumpRange(lineNumber);
429         if (allocator.canAllocate())
430             rangeCache.push(bumpRange);
431         else
432             allocator.refill(bumpRange);
433     }
434 }
435 
436 void Heap::allocateSmallBumpRangesByObject(
<span class="line-modified">437     UniqueLockHolder&amp; lock, size_t sizeClass,</span>
438     BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
<span class="line-modified">439     LineCache&amp; lineCache, FailureAction action)</span>
440 {
<span class="line-added">441     BUNUSED(action);</span>
442     RELEASE_BASSERT(isActiveHeapKind(m_kind));
443 
444     size_t size = allocator.size();
<span class="line-modified">445     SmallPage* page = allocateSmallPage(lock, sizeClass, lineCache, action);</span>
<span class="line-added">446     if (!page) {</span>
<span class="line-added">447         BASSERT(action == FailureAction::ReturnNull);</span>
<span class="line-added">448         return;</span>
<span class="line-added">449     }</span>
450     BASSERT(page-&gt;hasFreeLines(lock));
451 
452     auto findSmallBumpRange = [&amp;](Object&amp; it, Object&amp; end) {
453         for ( ; it + size &lt;= end; it = it + size) {
454             if (!it.line()-&gt;refCount(lock))
455                 return true;
456         }
457         return false;
458     };
459 
460     auto allocateSmallBumpRange = [&amp;](Object&amp; it, Object&amp; end) -&gt; BumpRange {
461         char* begin = it.address();
462         unsigned short objectCount = 0;
463         for ( ; it + size &lt;= end; it = it + size) {
464             if (it.line()-&gt;refCount(lock))
465                 break;
466 
467             ++objectCount;
468             it.line()-&gt;ref(lock);
469             it.page()-&gt;ref(lock);
470         }
471         return { begin, objectCount };
472     };
473 
474     Object it(page-&gt;begin()-&gt;begin());
<span class="line-modified">475     Object end(it + pageSize(m_constants.pageClass(page-&gt;sizeClass())));</span>
476     for (;;) {
477         if (!findSmallBumpRange(it, end)) {
478             page-&gt;setHasFreeLines(lock, false);
<span class="line-modified">479             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
480             return;
481         }
482 
483         // In a fragmented page, some free ranges might not fit in the cache.
484         if (rangeCache.size() == rangeCache.capacity()) {
485             lineCache[sizeClass].push(page);
<span class="line-modified">486             BASSERT(action == FailureAction::ReturnNull || allocator.canAllocate());</span>
487             return;
488         }
489 
490         BumpRange bumpRange = allocateSmallBumpRange(it, end);
491         if (allocator.canAllocate())
492             rangeCache.push(bumpRange);
493         else
494             allocator.refill(bumpRange);
495     }
496 }
497 
<span class="line-modified">498 LargeRange Heap::splitAndAllocate(UniqueLockHolder&amp;, LargeRange&amp; range, size_t alignment, size_t size)</span>
499 {
500     RELEASE_BASSERT(isActiveHeapKind(m_kind));
501 
502     LargeRange prev;
503     LargeRange next;
504 
505     size_t alignmentMask = alignment - 1;
506     if (test(range.begin(), alignmentMask)) {
507         size_t prefixSize = roundUpToMultipleOf(alignment, range.begin()) - range.begin();
508         std::pair&lt;LargeRange, LargeRange&gt; pair = range.split(prefixSize);
509         prev = pair.first;
510         range = pair.second;
511     }
512 
513     if (range.size() - size &gt; size / pageSizeWasteFactor) {
514         std::pair&lt;LargeRange, LargeRange&gt; pair = range.split(size);
515         range = pair.first;
516         next = pair.second;
517     }
518 
</pre>
<hr />
<pre>
526         m_physicalPageMap.commit(range.begin(), range.size());
527 #endif
528     }
529 
530     if (prev) {
531         m_freeableMemory += prev.totalPhysicalSize();
532         m_largeFree.add(prev);
533     }
534 
535     if (next) {
536         m_freeableMemory += next.totalPhysicalSize();
537         m_largeFree.add(next);
538     }
539 
540     m_objectTypes.set(Chunk::get(range.begin()), ObjectType::Large);
541 
542     m_largeAllocated.set(range.begin(), range.size());
543     return range;
544 }
545 
<span class="line-modified">546 void* Heap::allocateLarge(UniqueLockHolder&amp; lock, size_t alignment, size_t size, FailureAction action)</span>
547 {
<span class="line-added">548 #define ASSERT_OR_RETURN_ON_FAILURE(cond) do { \</span>
<span class="line-added">549         if (action == FailureAction::Crash) \</span>
<span class="line-added">550             RELEASE_BASSERT(cond); \</span>
<span class="line-added">551         else if (!(cond)) \</span>
<span class="line-added">552             return nullptr; \</span>
<span class="line-added">553     } while (false)</span>
<span class="line-added">554 </span>
<span class="line-added">555 </span>
556     RELEASE_BASSERT(isActiveHeapKind(m_kind));
557 
558     BASSERT(isPowerOfTwo(alignment));
559 
560     m_scavenger-&gt;didStartGrowing();
561 
562     size_t roundedSize = size ? roundUpToMultipleOf(largeAlignment, size) : largeAlignment;
<span class="line-modified">563     ASSERT_OR_RETURN_ON_FAILURE(roundedSize &gt;= size); // Check for overflow</span>

564     size = roundedSize;
565 
566     size_t roundedAlignment = roundUpToMultipleOf&lt;largeAlignment&gt;(alignment);
<span class="line-modified">567     ASSERT_OR_RETURN_ON_FAILURE(roundedAlignment &gt;= alignment); // Check for overflow</span>

568     alignment = roundedAlignment;
569 
570     LargeRange range = m_largeFree.remove(alignment, size);
571     if (!range) {
572         if (m_hasPendingDecommits) {
573             m_condition.wait(lock, [&amp;]() { return !m_hasPendingDecommits; });
574             // Now we&#39;re guaranteed we&#39;re looking at all available memory.
<span class="line-modified">575             return allocateLarge(lock, alignment, size, action);</span>
576         }
577 
<span class="line-modified">578         ASSERT_OR_RETURN_ON_FAILURE(!usingGigacage());</span>

579 
<span class="line-modified">580         range = tryAllocateLargeChunk(alignment, size);</span>
<span class="line-modified">581         ASSERT_OR_RETURN_ON_FAILURE(range);</span>

582 
583         m_largeFree.add(range);
584         range = m_largeFree.remove(alignment, size);
585     }
586 
587     m_freeableMemory -= range.totalPhysicalSize();
588 
589     void* result = splitAndAllocate(lock, range, alignment, size).begin();
<span class="line-added">590 #if BUSE(PARTIAL_SCAVENGE)</span>
<span class="line-added">591     m_highWatermark = std::max(m_highWatermark, result);</span>
<span class="line-added">592 #endif</span>
<span class="line-added">593     ASSERT_OR_RETURN_ON_FAILURE(result);</span>
594     return result;
<span class="line-added">595 </span>
<span class="line-added">596 #undef ASSERT_OR_RETURN_ON_FAILURE</span>
597 }
598 
<span class="line-modified">599 LargeRange Heap::tryAllocateLargeChunk(size_t alignment, size_t size)</span>
600 {
<span class="line-modified">601     // We allocate VM in aligned multiples to increase the chances that</span>
<span class="line-modified">602     // the OS will provide contiguous ranges that we can merge.</span>
<span class="line-modified">603     size_t roundedAlignment = roundUpToMultipleOf&lt;chunkSize&gt;(alignment);</span>
<span class="line-added">604     if (roundedAlignment &lt; alignment) // Check for overflow</span>
<span class="line-added">605         return LargeRange();</span>
<span class="line-added">606     alignment = roundedAlignment;</span>
<span class="line-added">607 </span>
<span class="line-added">608     size_t roundedSize = roundUpToMultipleOf&lt;chunkSize&gt;(size);</span>
<span class="line-added">609     if (roundedSize &lt; size) // Check for overflow</span>
<span class="line-added">610         return LargeRange();</span>
<span class="line-added">611     size = roundedSize;</span>
<span class="line-added">612 </span>
<span class="line-added">613     void* memory = tryVMAllocate(alignment, size);</span>
<span class="line-added">614     if (!memory)</span>
<span class="line-added">615         return LargeRange();</span>
<span class="line-added">616 </span>
<span class="line-added">617 #if BOS(DARWIN)</span>
<span class="line-added">618     PerProcess&lt;Zone&gt;::get()-&gt;addRange(Range(memory, size));</span>
<span class="line-added">619 #endif</span>
<span class="line-added">620 </span>
<span class="line-added">621     return LargeRange(memory, size, 0, 0);</span>
622 }
623 
<span class="line-modified">624 bool Heap::isLarge(UniqueLockHolder&amp;, void* object)</span>
625 {
626     return m_objectTypes.get(Object(object).chunk()) == ObjectType::Large;
627 }
628 
<span class="line-modified">629 size_t Heap::largeSize(UniqueLockHolder&amp;, void* object)</span>
630 {
631     return m_largeAllocated.get(object);
632 }
633 
<span class="line-modified">634 void Heap::shrinkLarge(UniqueLockHolder&amp; lock, const Range&amp; object, size_t newSize)</span>
635 {
636     BASSERT(object.size() &gt; newSize);
637 
638     size_t size = m_largeAllocated.remove(object.begin());
639     LargeRange range = LargeRange(object, size, size);
640     splitAndAllocate(lock, range, alignment, newSize);
641 
642     m_scavenger-&gt;schedule(size);
643 }
644 
<span class="line-modified">645 void Heap::deallocateLarge(UniqueLockHolder&amp;, void* object)</span>
646 {
647     size_t size = m_largeAllocated.remove(object);
648     m_largeFree.add(LargeRange(object, size, size, size));
649     m_freeableMemory += size;
650     m_scavenger-&gt;schedule(size);
651 }
652 
653 void Heap::externalCommit(void* ptr, size_t size)
654 {
<span class="line-modified">655     UniqueLockHolder lock(Heap::mutex());</span>
656     externalCommit(lock, ptr, size);
657 }
658 
<span class="line-modified">659 void Heap::externalCommit(UniqueLockHolder&amp;, void* ptr, size_t size)</span>
660 {
661     BUNUSED_PARAM(ptr);
662 
663     m_footprint += size;
664 #if ENABLE_PHYSICAL_PAGE_MAP
665     m_physicalPageMap.commit(ptr, size);
666 #endif
667 }
668 
669 void Heap::externalDecommit(void* ptr, size_t size)
670 {
<span class="line-modified">671     UniqueLockHolder lock(Heap::mutex());</span>
672     externalDecommit(lock, ptr, size);
673 }
674 
<span class="line-modified">675 void Heap::externalDecommit(UniqueLockHolder&amp;, void* ptr, size_t size)</span>
676 {
677     BUNUSED_PARAM(ptr);
678 
679     m_footprint -= size;
680 #if ENABLE_PHYSICAL_PAGE_MAP
681     m_physicalPageMap.decommit(ptr, size);
682 #endif
683 }
684 
685 } // namespace bmalloc
</pre>
</td>
</tr>
</table>
<center><a href="Gigacage.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>