<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2020 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;Repatch.h&quot;
  28 
  29 #if ENABLE(JIT)
  30 
  31 #include &quot;BinarySwitch.h&quot;
  32 #include &quot;CCallHelpers.h&quot;
  33 #include &quot;CacheableIdentifierInlines.h&quot;
  34 #include &quot;CallFrameShuffler.h&quot;
  35 #include &quot;DFGOperations.h&quot;
  36 #include &quot;DFGSpeculativeJIT.h&quot;
  37 #include &quot;DOMJITGetterSetter.h&quot;
  38 #include &quot;DirectArguments.h&quot;
  39 #include &quot;ExecutableBaseInlines.h&quot;
  40 #include &quot;FTLThunks.h&quot;
  41 #include &quot;FullCodeOrigin.h&quot;
  42 #include &quot;FunctionCodeBlock.h&quot;
  43 #include &quot;GCAwareJITStubRoutine.h&quot;
  44 #include &quot;GetterSetter.h&quot;
  45 #include &quot;GetterSetterAccessCase.h&quot;
  46 #include &quot;ICStats.h&quot;
  47 #include &quot;InlineAccess.h&quot;
  48 #include &quot;InstanceOfAccessCase.h&quot;
  49 #include &quot;IntrinsicGetterAccessCase.h&quot;
  50 #include &quot;JIT.h&quot;
  51 #include &quot;JITInlines.h&quot;
  52 #include &quot;JSCInlines.h&quot;
  53 #include &quot;JSModuleNamespaceObject.h&quot;
  54 #include &quot;JSWebAssembly.h&quot;
  55 #include &quot;JSWebAssemblyModule.h&quot;
  56 #include &quot;LinkBuffer.h&quot;
  57 #include &quot;ModuleNamespaceAccessCase.h&quot;
  58 #include &quot;PolymorphicAccess.h&quot;
  59 #include &quot;ScopedArguments.h&quot;
  60 #include &quot;ScratchRegisterAllocator.h&quot;
  61 #include &quot;StackAlignment.h&quot;
  62 #include &quot;StructureRareDataInlines.h&quot;
  63 #include &quot;StructureStubClearingWatchpoint.h&quot;
  64 #include &quot;StructureStubInfo.h&quot;
  65 #include &quot;SuperSampler.h&quot;
  66 #include &quot;ThunkGenerators.h&quot;
  67 #include &quot;WebAssemblyFunction.h&quot;
  68 #include &lt;wtf/CommaPrinter.h&gt;
  69 #include &lt;wtf/ListDump.h&gt;
  70 #include &lt;wtf/StringPrintStream.h&gt;
  71 
  72 namespace JSC {
  73 
  74 static FunctionPtr&lt;CFunctionPtrTag&gt; readPutICCallTarget(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call)
  75 {
  76     FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  77 #if ENABLE(FTL_JIT)
  78     if (codeBlock-&gt;jitType() == JITType::FTLJIT) {
  79         MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt; thunk = MacroAssemblerCodePtr&lt;OperationPtrTag&gt;::createFromExecutableAddress(target.executableAddress()).retagged&lt;JITThunkPtrTag&gt;();
  80         return codeBlock-&gt;vm().ftlThunks-&gt;keyForSlowPathCallThunk(thunk).callTarget().retagged&lt;CFunctionPtrTag&gt;();
  81     }
  82 #else
  83     UNUSED_PARAM(codeBlock);
  84 #endif // ENABLE(FTL_JIT)
  85     return target.retagged&lt;CFunctionPtrTag&gt;();
  86 }
  87 
  88 void ftlThunkAwareRepatchCall(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  89 {
  90 #if ENABLE(FTL_JIT)
  91     if (codeBlock-&gt;jitType() == JITType::FTLJIT) {
  92         VM&amp; vm = codeBlock-&gt;vm();
  93         FTL::Thunks&amp; thunks = *vm.ftlThunks;
  94         FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  95         auto slowPathThunk = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;::createFromExecutableAddress(target.retaggedExecutableAddress&lt;JITThunkPtrTag&gt;());
  96         FTL::SlowPathCallKey key = thunks.keyForSlowPathCallThunk(slowPathThunk);
  97         key = key.withCallTarget(newCalleeFunction);
  98         MacroAssembler::repatchCall(call, FunctionPtr&lt;OperationPtrTag&gt;(thunks.getSlowPathCallThunk(vm, key).retaggedCode&lt;OperationPtrTag&gt;()));
  99         return;
 100     }
 101 #else // ENABLE(FTL_JIT)
 102     UNUSED_PARAM(codeBlock);
 103 #endif // ENABLE(FTL_JIT)
 104     MacroAssembler::repatchCall(call, newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
 105 }
 106 
 107 enum InlineCacheAction {
 108     GiveUpOnCache,
 109     RetryCacheLater,
 110     AttemptToCache
 111 };
 112 
 113 static InlineCacheAction actionForCell(VM&amp; vm, JSCell* cell)
 114 {
 115     Structure* structure = cell-&gt;structure(vm);
 116 
 117     TypeInfo typeInfo = structure-&gt;typeInfo();
 118     if (typeInfo.prohibitsPropertyCaching())
 119         return GiveUpOnCache;
 120 
 121     if (structure-&gt;isUncacheableDictionary()) {
 122         if (structure-&gt;hasBeenFlattenedBefore())
 123             return GiveUpOnCache;
 124         // Flattening could have changed the offset, so return early for another try.
 125         asObject(cell)-&gt;flattenDictionaryObject(vm);
 126         return RetryCacheLater;
 127     }
 128 
 129     if (!structure-&gt;propertyAccessesAreCacheable())
 130         return GiveUpOnCache;
 131 
 132     return AttemptToCache;
 133 }
 134 
 135 static bool forceICFailure(JSGlobalObject*)
 136 {
 137     return Options::forceICFailure();
 138 }
 139 
 140 ALWAYS_INLINE static void fireWatchpointsAndClearStubIfNeeded(VM&amp; vm, StructureStubInfo&amp; stubInfo, CodeBlock* codeBlock, AccessGenerationResult&amp; result)
 141 {
 142     if (result.shouldResetStubAndFireWatchpoints()) {
 143         result.fireWatchpoints(vm);
 144         stubInfo.reset(codeBlock);
 145     }
 146 }
 147 
 148 inline FunctionPtr&lt;CFunctionPtrTag&gt; appropriateOptimizingGetByFunction(GetByKind kind)
 149 {
 150     switch (kind) {
 151     case GetByKind::Normal:
 152         return operationGetByIdOptimize;
 153     case GetByKind::WithThis:
 154         return operationGetByIdWithThisOptimize;
 155     case GetByKind::Try:
 156         return operationTryGetByIdOptimize;
 157     case GetByKind::Direct:
 158         return operationGetByIdDirectOptimize;
 159     case GetByKind::NormalByVal:
 160         return operationGetByValOptimize;
 161     }
 162     RELEASE_ASSERT_NOT_REACHED();
 163 }
 164 
 165 inline FunctionPtr&lt;CFunctionPtrTag&gt; appropriateGetByFunction(GetByKind kind)
 166 {
 167     switch (kind) {
 168     case GetByKind::Normal:
 169         return operationGetById;
 170     case GetByKind::WithThis:
 171         return operationGetByIdWithThis;
 172     case GetByKind::Try:
 173         return operationTryGetById;
 174     case GetByKind::Direct:
 175         return operationGetByIdDirect;
 176     case GetByKind::NormalByVal:
 177         return operationGetByValGeneric;
 178     }
 179     RELEASE_ASSERT_NOT_REACHED();
 180 }
 181 
 182 static InlineCacheAction tryCacheGetBy(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, CacheableIdentifier propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByKind kind)
 183 {
 184     VM&amp; vm = globalObject-&gt;vm();
 185     AccessGenerationResult result;
 186 
 187     {
 188         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, globalObject-&gt;vm().heap);
 189 
 190         if (forceICFailure(globalObject))
 191             return GiveUpOnCache;
 192 
 193         // FIXME: Cache property access for immediates.
 194         if (!baseValue.isCell())
 195             return GiveUpOnCache;
 196         JSCell* baseCell = baseValue.asCell();
 197 
 198         std::unique_ptr&lt;AccessCase&gt; newCase;
 199 
 200         if (propertyName == vm.propertyNames-&gt;length) {
 201             if (isJSArray(baseCell)) {
 202                 if (stubInfo.cacheType() == CacheType::Unset
 203                     &amp;&amp; slot.slotBase() == baseCell
 204                     &amp;&amp; InlineAccess::isCacheableArrayLength(stubInfo, jsCast&lt;JSArray*&gt;(baseCell))) {
 205 
 206                     bool generatedCodeInline = InlineAccess::generateArrayLength(stubInfo, jsCast&lt;JSArray*&gt;(baseCell));
 207                     if (generatedCodeInline) {
 208                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
 209                         stubInfo.initArrayLength();
 210                         return RetryCacheLater;
 211                     }
 212                 }
 213 
 214                 newCase = AccessCase::create(vm, codeBlock, AccessCase::ArrayLength, propertyName);
 215             } else if (isJSString(baseCell)) {
 216                 if (stubInfo.cacheType() == CacheType::Unset &amp;&amp; InlineAccess::isCacheableStringLength(stubInfo)) {
 217                     bool generatedCodeInline = InlineAccess::generateStringLength(stubInfo);
 218                     if (generatedCodeInline) {
 219                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
 220                         stubInfo.initStringLength();
 221                         return RetryCacheLater;
 222                     }
 223                 }
 224 
 225                 newCase = AccessCase::create(vm, codeBlock, AccessCase::StringLength, propertyName);
 226             } else if (DirectArguments* arguments = jsDynamicCast&lt;DirectArguments*&gt;(vm, baseCell)) {
 227                 // If there were overrides, then we can handle this as a normal property load! Guarding
 228                 // this with such a check enables us to add an IC case for that load if needed.
 229                 if (!arguments-&gt;overrodeThings())
 230                     newCase = AccessCase::create(vm, codeBlock, AccessCase::DirectArgumentsLength, propertyName);
 231             } else if (ScopedArguments* arguments = jsDynamicCast&lt;ScopedArguments*&gt;(vm, baseCell)) {
 232                 // Ditto.
 233                 if (!arguments-&gt;overrodeThings())
 234                     newCase = AccessCase::create(vm, codeBlock, AccessCase::ScopedArgumentsLength, propertyName);
 235             }
 236         }
 237 
 238         if (!propertyName.isSymbol() &amp;&amp; baseCell-&gt;inherits&lt;JSModuleNamespaceObject&gt;(vm) &amp;&amp; !slot.isUnset()) {
 239             if (auto moduleNamespaceSlot = slot.moduleNamespaceSlot())
 240                 newCase = ModuleNamespaceAccessCase::create(vm, codeBlock, propertyName, jsCast&lt;JSModuleNamespaceObject*&gt;(baseCell), moduleNamespaceSlot-&gt;environment, ScopeOffset(moduleNamespaceSlot-&gt;scopeOffset));
 241         }
 242 
 243         if (!newCase) {
 244             if (!slot.isCacheable() &amp;&amp; !slot.isUnset())
 245                 return GiveUpOnCache;
 246 
 247             ObjectPropertyConditionSet conditionSet;
 248             Structure* structure = baseCell-&gt;structure(vm);
 249 
 250             bool loadTargetFromProxy = false;
 251             if (baseCell-&gt;type() == PureForwardingProxyType) {
 252                 baseValue = jsCast&lt;JSProxy*&gt;(baseCell)-&gt;target();
 253                 baseCell = baseValue.asCell();
 254                 structure = baseCell-&gt;structure(vm);
 255                 loadTargetFromProxy = true;
 256             }
 257 
 258             InlineCacheAction action = actionForCell(vm, baseCell);
 259             if (action != AttemptToCache)
 260                 return action;
 261 
 262             // Optimize self access.
 263             if (stubInfo.cacheType() == CacheType::Unset
 264                 &amp;&amp; slot.isCacheableValue()
 265                 &amp;&amp; slot.slotBase() == baseValue
 266                 &amp;&amp; !slot.watchpointSet()
 267                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()
 268                 &amp;&amp; !loadTargetFromProxy) {
 269 
 270                 bool generatedCodeInline = InlineAccess::generateSelfPropertyAccess(stubInfo, structure, slot.cachedOffset());
 271                 if (generatedCodeInline) {
 272                     LOG_IC((ICEvent::GetBySelfPatch, structure-&gt;classInfo(), Identifier::fromUid(vm, propertyName.uid()), slot.slotBase() == baseValue));
 273                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 274                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
 275                     stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset(), propertyName);
 276                     return RetryCacheLater;
 277                 }
 278             }
 279 
 280             std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 281 
 282             PropertyOffset offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 283 
 284             if (slot.isCustom() &amp;&amp; slot.slotBase() == baseValue) {
 285                 // To cache self customs, we must disallow dictionaries because we
 286                 // need to be informed if the custom goes away since we cache the
 287                 // constant function pointer.
 288 
 289                 if (!prepareChainForCaching(globalObject, slot.slotBase(), slot.slotBase()))
 290                     return GiveUpOnCache;
 291             }
 292 
 293             if (slot.isUnset() || slot.slotBase() != baseValue) {
 294                 if (structure-&gt;typeInfo().prohibitsPropertyCaching())
 295                     return GiveUpOnCache;
 296 
 297                 if (structure-&gt;isDictionary()) {
 298                     if (structure-&gt;hasBeenFlattenedBefore())
 299                         return GiveUpOnCache;
 300                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseCell));
 301                     return RetryCacheLater; // We may have changed property offsets.
 302                 }
 303 
 304                 if (slot.isUnset() &amp;&amp; structure-&gt;typeInfo().getOwnPropertySlotIsImpureForPropertyAbsence())
 305                     return GiveUpOnCache;
 306 
 307                 // If a kind is GetByKind::Direct, we do not need to investigate prototype chains further.
 308                 // Cacheability just depends on the head structure.
 309                 if (kind != GetByKind::Direct) {
 310                     auto cacheStatus = prepareChainForCaching(globalObject, baseCell, slot);
 311                     if (!cacheStatus)
 312                         return GiveUpOnCache;
 313 
 314                     if (cacheStatus-&gt;flattenedDictionary) {
 315                         // Property offsets may have changed due to flattening. We&#39;ll cache later.
 316                         return RetryCacheLater;
 317                     }
 318 
 319                     if (cacheStatus-&gt;usesPolyProto) {
 320                         prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, slot);
 321                         if (!prototypeAccessChain)
 322                             return GiveUpOnCache;
 323                         RELEASE_ASSERT(slot.isCacheableCustom() || prototypeAccessChain-&gt;slotBaseStructure(vm, structure)-&gt;get(vm, propertyName.uid()) == offset);
 324                     } else {
 325                         // We use ObjectPropertyConditionSet instead for faster accesses.
 326                         prototypeAccessChain = nullptr;
 327 
 328                         // FIXME: Maybe this `if` should be inside generateConditionsForPropertyBlah.
 329                         // https://bugs.webkit.org/show_bug.cgi?id=185215
 330                         if (slot.isUnset()) {
 331                             conditionSet = generateConditionsForPropertyMiss(
 332                                 vm, codeBlock, globalObject, structure, propertyName.uid());
 333                         } else if (!slot.isCacheableCustom()) {
 334                             conditionSet = generateConditionsForPrototypePropertyHit(
 335                                 vm, codeBlock, globalObject, structure, slot.slotBase(),
 336                                 propertyName.uid());
 337                             RELEASE_ASSERT(!conditionSet.isValid() || conditionSet.slotBaseCondition().offset() == offset);
 338                         } else {
 339                             conditionSet = generateConditionsForPrototypePropertyHitCustom(
 340                                 vm, codeBlock, globalObject, structure, slot.slotBase(),
 341                                 propertyName.uid(), slot.attributes());
 342                         }
 343 
 344                         if (!conditionSet.isValid())
 345                             return GiveUpOnCache;
 346                     }
 347                 }
 348             }
 349 
 350             JSFunction* getter = nullptr;
 351             if (slot.isCacheableGetter())
 352                 getter = jsDynamicCast&lt;JSFunction*&gt;(vm, slot.getterSetter()-&gt;getter());
 353 
 354             Optional&lt;DOMAttributeAnnotation&gt; domAttribute;
 355             if (slot.isCacheableCustom() &amp;&amp; slot.domAttribute())
 356                 domAttribute = slot.domAttribute();
 357 
 358             if (kind == GetByKind::Try) {
 359                 AccessCase::AccessType type;
 360                 if (slot.isCacheableValue())
 361                     type = AccessCase::Load;
 362                 else if (slot.isUnset())
 363                     type = AccessCase::Miss;
 364                 else if (slot.isCacheableGetter())
 365                     type = AccessCase::GetGetter;
 366                 else
 367                     RELEASE_ASSERT_NOT_REACHED();
 368 
 369                 newCase = ProxyableAccessCase::create(vm, codeBlock, type, propertyName, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
 370             } else if (!loadTargetFromProxy &amp;&amp; getter &amp;&amp; IntrinsicGetterAccessCase::canEmitIntrinsicGetter(getter, structure))
 371                 newCase = IntrinsicGetterAccessCase::create(vm, codeBlock, propertyName, slot.cachedOffset(), structure, conditionSet, getter, WTFMove(prototypeAccessChain));
 372             else {
 373                 if (slot.isCacheableValue() || slot.isUnset()) {
 374                     newCase = ProxyableAccessCase::create(vm, codeBlock, slot.isUnset() ? AccessCase::Miss : AccessCase::Load,
 375                         propertyName, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
 376                 } else {
 377                     AccessCase::AccessType type;
 378                     if (slot.isCacheableGetter())
 379                         type = AccessCase::Getter;
 380                     else if (slot.attributes() &amp; PropertyAttribute::CustomAccessor)
 381                         type = AccessCase::CustomAccessorGetter;
 382                     else
 383                         type = AccessCase::CustomValueGetter;
 384 
 385                     if (kind == GetByKind::WithThis &amp;&amp; type == AccessCase::CustomAccessorGetter &amp;&amp; domAttribute)
 386                         return GiveUpOnCache;
 387 
 388                     newCase = GetterSetterAccessCase::create(
 389                         vm, codeBlock, type, propertyName, offset, structure, conditionSet, loadTargetFromProxy,
 390                         slot.watchpointSet(), slot.isCacheableCustom() ? slot.customGetter() : nullptr,
 391                         slot.isCacheableCustom() &amp;&amp; slot.slotBase() != baseValue ? slot.slotBase() : nullptr,
 392                         domAttribute, WTFMove(prototypeAccessChain));
 393                 }
 394             }
 395         }
 396 
 397         LOG_IC((ICEvent::GetByAddAccessCase, baseValue.classInfoOrNull(vm), Identifier::fromUid(vm, propertyName.uid()), slot.slotBase() == baseValue));
 398 
 399         result = stubInfo.addAccessCase(locker, codeBlock, propertyName, WTFMove(newCase));
 400 
 401         if (result.generatedSomeCode()) {
 402             LOG_IC((ICEvent::GetByReplaceWithJump, baseValue.classInfoOrNull(vm), Identifier::fromUid(vm, propertyName.uid()), slot.slotBase() == baseValue));
 403 
 404             RELEASE_ASSERT(result.code());
 405             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 406         }
 407     }
 408 
 409     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 410 
 411     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 412 }
 413 
 414 void repatchGetBy(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, CacheableIdentifier propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByKind kind)
 415 {
 416     SuperSamplerScope superSamplerScope(false);
 417 
 418     if (tryCacheGetBy(globalObject, codeBlock, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache)
 419         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateGetByFunction(kind));
 420 }
 421 
 422 
 423 static InlineCacheAction tryCacheArrayGetByVal(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, JSValue index, StructureStubInfo&amp; stubInfo)
 424 {
 425     if (!baseValue.isCell())
 426         return GiveUpOnCache;
 427 
 428     if (!index.isInt32())
 429         return RetryCacheLater;
 430 
 431     VM&amp; vm = globalObject-&gt;vm();
 432     AccessGenerationResult result;
 433 
 434     {
 435         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, globalObject-&gt;vm().heap);
 436 
 437         JSCell* base = baseValue.asCell();
 438 
 439         AccessCase::AccessType accessType;
 440         if (base-&gt;type() == DirectArgumentsType)
 441             accessType = AccessCase::IndexedDirectArgumentsLoad;
 442         else if (base-&gt;type() == ScopedArgumentsType)
 443             accessType = AccessCase::IndexedScopedArgumentsLoad;
 444         else if (base-&gt;type() == StringType)
 445             accessType = AccessCase::IndexedStringLoad;
 446         else if (isTypedView(base-&gt;classInfo(vm)-&gt;typedArrayStorageType)) {
 447             switch (base-&gt;classInfo(vm)-&gt;typedArrayStorageType) {
 448             case TypeInt8:
 449                 accessType = AccessCase::IndexedTypedArrayInt8Load;
 450                 break;
 451             case TypeUint8:
 452                 accessType = AccessCase::IndexedTypedArrayUint8Load;
 453                 break;
 454             case TypeUint8Clamped:
 455                 accessType = AccessCase::IndexedTypedArrayUint8ClampedLoad;
 456                 break;
 457             case TypeInt16:
 458                 accessType = AccessCase::IndexedTypedArrayInt16Load;
 459                 break;
 460             case TypeUint16:
 461                 accessType = AccessCase::IndexedTypedArrayUint16Load;
 462                 break;
 463             case TypeInt32:
 464                 accessType = AccessCase::IndexedTypedArrayInt32Load;
 465                 break;
 466             case TypeUint32:
 467                 accessType = AccessCase::IndexedTypedArrayUint32Load;
 468                 break;
 469             case TypeFloat32:
 470                 accessType = AccessCase::IndexedTypedArrayFloat32Load;
 471                 break;
 472             case TypeFloat64:
 473                 accessType = AccessCase::IndexedTypedArrayFloat64Load;
 474                 break;
 475             default:
 476                 RELEASE_ASSERT_NOT_REACHED();
 477             }
 478         } else {
 479             IndexingType indexingShape = base-&gt;indexingType() &amp; IndexingShapeMask;
 480             switch (indexingShape) {
 481             case Int32Shape:
 482                 accessType = AccessCase::IndexedInt32Load;
 483                 break;
 484             case DoubleShape:
 485                 accessType = AccessCase::IndexedDoubleLoad;
 486                 break;
 487             case ContiguousShape:
 488                 accessType = AccessCase::IndexedContiguousLoad;
 489                 break;
 490             case ArrayStorageShape:
 491                 accessType = AccessCase::IndexedArrayStorageLoad;
 492                 break;
 493             default:
 494                 return GiveUpOnCache;
 495             }
 496         }
 497 
 498         result = stubInfo.addAccessCase(locker, codeBlock, nullptr, AccessCase::create(vm, codeBlock, accessType, nullptr));
 499 
 500         if (result.generatedSomeCode()) {
 501             LOG_IC((ICEvent::GetByReplaceWithJump, baseValue.classInfoOrNull(vm), Identifier()));
 502 
 503             RELEASE_ASSERT(result.code());
 504             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 505         }
 506     }
 507 
 508     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 509     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 510 }
 511 
 512 void repatchArrayGetByVal(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue base, JSValue index, StructureStubInfo&amp; stubInfo)
 513 {
 514     if (tryCacheArrayGetByVal(globalObject, codeBlock, base, index, stubInfo) == GiveUpOnCache)
 515         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationGetByValGeneric);
 516 }
 517 
 518 static V_JITOperation_GSsiJJI appropriateGenericPutByIdFunction(const PutPropertySlot &amp;slot, PutKind putKind)
 519 {
 520     if (slot.isStrictMode()) {
 521         if (putKind == Direct)
 522             return operationPutByIdDirectStrict;
 523         return operationPutByIdStrict;
 524     }
 525     if (putKind == Direct)
 526         return operationPutByIdDirectNonStrict;
 527     return operationPutByIdNonStrict;
 528 }
 529 
 530 static V_JITOperation_GSsiJJI appropriateOptimizingPutByIdFunction(const PutPropertySlot &amp;slot, PutKind putKind)
 531 {
 532     if (slot.isStrictMode()) {
 533         if (putKind == Direct)
 534             return operationPutByIdDirectStrictOptimize;
 535         return operationPutByIdStrictOptimize;
 536     }
 537     if (putKind == Direct)
 538         return operationPutByIdDirectNonStrictOptimize;
 539     return operationPutByIdNonStrictOptimize;
 540 }
 541 
 542 static InlineCacheAction tryCachePutByID(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, Structure* oldStructure, const Identifier&amp; ident, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 543 {
 544     VM&amp; vm = globalObject-&gt;vm();
 545     AccessGenerationResult result;
 546     {
 547         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, globalObject-&gt;vm().heap);
 548 
 549         if (forceICFailure(globalObject))
 550             return GiveUpOnCache;
 551 
 552         if (!baseValue.isCell())
 553             return GiveUpOnCache;
 554 
 555         if (!slot.isCacheablePut() &amp;&amp; !slot.isCacheableCustom() &amp;&amp; !slot.isCacheableSetter())
 556             return GiveUpOnCache;
 557 
 558         // FIXME: We should try to do something smarter here...
 559         if (isCopyOnWrite(oldStructure-&gt;indexingMode()))
 560             return GiveUpOnCache;
 561         // We can&#39;t end up storing to a CoW on the prototype since it shouldn&#39;t own properties.
 562         ASSERT(!isCopyOnWrite(slot.base()-&gt;indexingMode()));
 563 
 564         if (!oldStructure-&gt;propertyAccessesAreCacheable())
 565             return GiveUpOnCache;
 566 
 567         std::unique_ptr&lt;AccessCase&gt; newCase;
 568         JSCell* baseCell = baseValue.asCell();
 569 
 570         if (slot.base() == baseValue &amp;&amp; slot.isCacheablePut()) {
 571             if (slot.type() == PutPropertySlot::ExistingProperty) {
 572                 // This assert helps catch bugs if we accidentally forget to disable caching
 573                 // when we transition then store to an existing property. This is common among
 574                 // paths that reify lazy properties. If we reify a lazy property and forget
 575                 // to disable caching, we may come down this path. The Replace IC does not
 576                 // know how to model these types of structure transitions (or any structure
 577                 // transition for that matter).
 578                 RELEASE_ASSERT(baseValue.asCell()-&gt;structure(vm) == oldStructure);
 579 
 580                 oldStructure-&gt;didCachePropertyReplacement(vm, slot.cachedOffset());
 581 
 582                 if (stubInfo.cacheType() == CacheType::Unset
 583                     &amp;&amp; InlineAccess::canGenerateSelfPropertyReplace(stubInfo, slot.cachedOffset())
 584                     &amp;&amp; !oldStructure-&gt;needImpurePropertyWatchpoint()) {
 585 
 586                     bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, oldStructure, slot.cachedOffset());
 587                     if (generatedCodeInline) {
 588                         LOG_IC((ICEvent::PutByIdSelfPatch, oldStructure-&gt;classInfo(), ident, slot.base() == baseValue));
 589                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingPutByIdFunction(slot, putKind));
 590                         stubInfo.initPutByIdReplace(codeBlock, oldStructure, slot.cachedOffset());
 591                         return RetryCacheLater;
 592                     }
 593                 }
 594 
 595                 newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, ident, slot.cachedOffset(), oldStructure);
 596             } else {
 597                 ASSERT(slot.type() == PutPropertySlot::NewProperty);
 598 
 599                 if (!oldStructure-&gt;isObject())
 600                     return GiveUpOnCache;
 601 
 602                 // If the old structure is dictionary, it means that this is one-on-one between an object and a structure.
 603                 // If this is NewProperty operation, generating IC for this does not offer any benefit because this transition never happens again.
 604                 if (oldStructure-&gt;isDictionary())
 605                     return RetryCacheLater;
 606 
 607                 PropertyOffset offset;
 608                 Structure* newStructure = Structure::addPropertyTransitionToExistingStructureConcurrently(oldStructure, ident.impl(), static_cast&lt;unsigned&gt;(PropertyAttribute::None), offset);
 609                 if (!newStructure || !newStructure-&gt;propertyAccessesAreCacheable())
 610                     return GiveUpOnCache;
 611 
 612                 // If JSObject::put is overridden by UserObject, UserObject::put performs side-effect on JSObject::put, and it neglects to mark the PutPropertySlot as non-cachaeble,
 613                 // then arbitrary structure transitions can happen during the put operation, and this generates wrong transition information here as if oldStructure -&gt; newStructure.
 614                 // In reality, the transition is oldStructure -&gt; something unknown structures -&gt; baseValue&#39;s structure.
 615                 // To guard against the embedder&#39;s potentially incorrect UserObject::put implementation, we should check for this condition and if found, and give up on caching the put.
 616                 ASSERT(baseValue.asCell()-&gt;structure(vm) == newStructure);
 617                 if (baseValue.asCell()-&gt;structure(vm) != newStructure)
 618                     return GiveUpOnCache;
 619 
 620                 ASSERT(newStructure-&gt;previousID() == oldStructure);
 621                 ASSERT(!newStructure-&gt;isDictionary());
 622                 ASSERT(newStructure-&gt;isObject());
 623 
 624                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 625                 ObjectPropertyConditionSet conditionSet;
 626                 if (putKind == NotDirect) {
 627                     auto cacheStatus = prepareChainForCaching(globalObject, baseCell, nullptr);
 628                     if (!cacheStatus)
 629                         return GiveUpOnCache;
 630 
 631                     if (cacheStatus-&gt;usesPolyProto) {
 632                         prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, nullptr);
 633                         if (!prototypeAccessChain)
 634                             return GiveUpOnCache;
 635                     } else {
 636                         prototypeAccessChain = nullptr;
 637                         conditionSet = generateConditionsForPropertySetterMiss(
 638                             vm, codeBlock, globalObject, newStructure, ident.impl());
 639                         if (!conditionSet.isValid())
 640                             return GiveUpOnCache;
 641                     }
 642                 }
 643 
 644                 newCase = AccessCase::create(vm, codeBlock, ident, offset, oldStructure, newStructure, conditionSet, WTFMove(prototypeAccessChain));
 645             }
 646         } else if (slot.isCacheableCustom() || slot.isCacheableSetter()) {
 647             if (slot.isCacheableCustom()) {
 648                 ObjectPropertyConditionSet conditionSet;
 649                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 650 
 651                 // We need to do this even if we&#39;re a self custom, since we must disallow dictionaries
 652                 // because we need to be informed if the custom goes away since we cache the constant
 653                 // function pointer.
 654                 auto cacheStatus = prepareChainForCaching(globalObject, baseCell, slot.base());
 655                 if (!cacheStatus)
 656                     return GiveUpOnCache;
 657 
 658                 if (slot.base() != baseValue) {
 659                     if (cacheStatus-&gt;usesPolyProto) {
 660                         prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, slot.base());
 661                         if (!prototypeAccessChain)
 662                             return GiveUpOnCache;
 663                     } else {
 664                         prototypeAccessChain = nullptr;
 665                         conditionSet = generateConditionsForPrototypePropertyHitCustom(
 666                             vm, codeBlock, globalObject, oldStructure, slot.base(), ident.impl(), static_cast&lt;unsigned&gt;(PropertyAttribute::None));
 667                         if (!conditionSet.isValid())
 668                             return GiveUpOnCache;
 669                     }
 670                 }
 671 
 672                 newCase = GetterSetterAccessCase::create(
 673                     vm, codeBlock, slot.isCustomAccessor() ? AccessCase::CustomAccessorSetter : AccessCase::CustomValueSetter, oldStructure, ident,
 674                     invalidOffset, conditionSet, WTFMove(prototypeAccessChain), slot.customSetter(), slot.base() != baseValue ? slot.base() : nullptr);
 675             } else {
 676                 ObjectPropertyConditionSet conditionSet;
 677                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 678                 PropertyOffset offset = slot.cachedOffset();
 679 
 680                 if (slot.base() != baseValue) {
 681                     auto cacheStatus = prepareChainForCaching(globalObject, baseCell, slot.base());
 682                     if (!cacheStatus)
 683                         return GiveUpOnCache;
 684                     if (cacheStatus-&gt;flattenedDictionary)
 685                         return RetryCacheLater;
 686 
 687                     if (cacheStatus-&gt;usesPolyProto) {
 688                         prototypeAccessChain = PolyProtoAccessChain::create(globalObject, baseCell, slot.base());
 689                         if (!prototypeAccessChain)
 690                             return GiveUpOnCache;
 691                         offset = prototypeAccessChain-&gt;slotBaseStructure(vm, baseCell-&gt;structure(vm))-&gt;get(vm, ident.impl());
 692                     } else {
 693                         prototypeAccessChain = nullptr;
 694                         conditionSet = generateConditionsForPrototypePropertyHit(
 695                             vm, codeBlock, globalObject, oldStructure, slot.base(), ident.impl());
 696                         if (!conditionSet.isValid())
 697                             return GiveUpOnCache;
 698 
 699                         if (!(conditionSet.slotBaseCondition().attributes() &amp; PropertyAttribute::Accessor))
 700                             return GiveUpOnCache;
 701 
 702                         offset = conditionSet.slotBaseCondition().offset();
 703                     }
 704                 }
 705 
 706                 newCase = GetterSetterAccessCase::create(
 707                     vm, codeBlock, AccessCase::Setter, oldStructure, ident, offset, conditionSet, WTFMove(prototypeAccessChain));
 708             }
 709         }
 710 
 711         LOG_IC((ICEvent::PutByIdAddAccessCase, oldStructure-&gt;classInfo(), ident, slot.base() == baseValue));
 712 
 713         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 714 
 715         if (result.generatedSomeCode()) {
 716             LOG_IC((ICEvent::PutByIdReplaceWithJump, oldStructure-&gt;classInfo(), ident, slot.base() == baseValue));
 717 
 718             RELEASE_ASSERT(result.code());
 719 
 720             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 721         }
 722     }
 723 
 724     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 725 
 726     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 727 }
 728 
 729 void repatchPutByID(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue baseValue, Structure* oldStructure, const Identifier&amp; propertyName, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 730 {
 731     SuperSamplerScope superSamplerScope(false);
 732 
 733     if (tryCachePutByID(globalObject, codeBlock, baseValue, oldStructure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache)
 734         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateGenericPutByIdFunction(slot, putKind));
 735 }
 736 
 737 static InlineCacheAction tryCacheInByID(
 738     JSGlobalObject* globalObject, CodeBlock* codeBlock, JSObject* base, const Identifier&amp; ident,
 739     bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 740 {
 741     VM&amp; vm = globalObject-&gt;vm();
 742     AccessGenerationResult result;
 743 
 744     {
 745         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, vm.heap);
 746         if (forceICFailure(globalObject))
 747             return GiveUpOnCache;
 748 
 749         if (!base-&gt;structure(vm)-&gt;propertyAccessesAreCacheable() || (!wasFound &amp;&amp; !base-&gt;structure(vm)-&gt;propertyAccessesAreCacheableForAbsence()))
 750             return GiveUpOnCache;
 751 
 752         if (wasFound) {
 753             if (!slot.isCacheable())
 754                 return GiveUpOnCache;
 755         }
 756 
 757         Structure* structure = base-&gt;structure(vm);
 758 
 759         std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 760         ObjectPropertyConditionSet conditionSet;
 761         if (wasFound) {
 762             InlineCacheAction action = actionForCell(vm, base);
 763             if (action != AttemptToCache)
 764                 return action;
 765 
 766             // Optimize self access.
 767             if (stubInfo.cacheType() == CacheType::Unset
 768                 &amp;&amp; slot.isCacheableValue()
 769                 &amp;&amp; slot.slotBase() == base
 770                 &amp;&amp; !slot.watchpointSet()
 771                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 772                 bool generatedCodeInline = InlineAccess::generateSelfInAccess(stubInfo, structure);
 773                 if (generatedCodeInline) {
 774                     LOG_IC((ICEvent::InByIdSelfPatch, structure-&gt;classInfo(), ident, slot.slotBase() == base));
 775                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 776                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInByIdOptimize);
 777                     stubInfo.initInByIdSelf(codeBlock, structure, slot.cachedOffset());
 778                     return RetryCacheLater;
 779                 }
 780             }
 781 
 782             if (slot.slotBase() != base) {
 783                 auto cacheStatus = prepareChainForCaching(globalObject, base, slot);
 784                 if (!cacheStatus)
 785                     return GiveUpOnCache;
 786                 if (cacheStatus-&gt;flattenedDictionary)
 787                     return RetryCacheLater;
 788 
 789                 if (cacheStatus-&gt;usesPolyProto) {
 790                     prototypeAccessChain = PolyProtoAccessChain::create(globalObject, base, slot);
 791                     if (!prototypeAccessChain)
 792                         return GiveUpOnCache;
 793                     RELEASE_ASSERT(slot.isCacheableCustom() || prototypeAccessChain-&gt;slotBaseStructure(vm, structure)-&gt;get(vm, ident.impl()) == slot.cachedOffset());
 794                 } else {
 795                     prototypeAccessChain = nullptr;
 796                     conditionSet = generateConditionsForPrototypePropertyHit(
 797                         vm, codeBlock, globalObject, structure, slot.slotBase(), ident.impl());
 798                     if (!conditionSet.isValid())
 799                         return GiveUpOnCache;
 800                     RELEASE_ASSERT(slot.isCacheableCustom() || conditionSet.slotBaseCondition().offset() == slot.cachedOffset());
 801                 }
 802             }
 803         } else {
 804             auto cacheStatus = prepareChainForCaching(globalObject, base, nullptr);
 805             if (!cacheStatus)
 806                 return GiveUpOnCache;
 807 
 808             if (cacheStatus-&gt;usesPolyProto) {
 809                 prototypeAccessChain = PolyProtoAccessChain::create(globalObject, base, slot);
 810                 if (!prototypeAccessChain)
 811                     return GiveUpOnCache;
 812             } else {
 813                 prototypeAccessChain = nullptr;
 814                 conditionSet = generateConditionsForPropertyMiss(
 815                     vm, codeBlock, globalObject, structure, ident.impl());
 816                 if (!conditionSet.isValid())
 817                     return GiveUpOnCache;
 818             }
 819         }
 820 
 821         LOG_IC((ICEvent::InAddAccessCase, structure-&gt;classInfo(), ident, slot.slotBase() == base));
 822 
 823         std::unique_ptr&lt;AccessCase&gt; newCase = AccessCase::create(
 824             vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, ident, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
 825 
 826         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 827 
 828         if (result.generatedSomeCode()) {
 829             LOG_IC((ICEvent::InReplaceWithJump, structure-&gt;classInfo(), ident, slot.slotBase() == base));
 830 
 831             RELEASE_ASSERT(result.code());
 832             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 833         }
 834     }
 835 
 836     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 837 
 838     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 839 }
 840 
 841 void repatchInByID(JSGlobalObject* globalObject, CodeBlock* codeBlock, JSObject* baseObject, const Identifier&amp; propertyName, bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 842 {
 843     SuperSamplerScope superSamplerScope(false);
 844 
 845     if (tryCacheInByID(globalObject, codeBlock, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache)
 846         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInById);
 847 }
 848 
 849 static InlineCacheAction tryCacheInstanceOf(
 850     JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 851     bool wasFound)
 852 {
 853     VM&amp; vm = globalObject-&gt;vm();
 854     AccessGenerationResult result;
 855 
 856     RELEASE_ASSERT(valueValue.isCell()); // shouldConsiderCaching rejects non-cells.
 857 
 858     if (forceICFailure(globalObject))
 859         return GiveUpOnCache;
 860 
 861     {
 862         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, vm.heap);
 863 
 864         JSCell* value = valueValue.asCell();
 865         Structure* structure = value-&gt;structure(vm);
 866         std::unique_ptr&lt;AccessCase&gt; newCase;
 867         JSObject* prototype = jsDynamicCast&lt;JSObject*&gt;(vm, prototypeValue);
 868         if (prototype) {
 869             if (!jsDynamicCast&lt;JSObject*&gt;(vm, value)) {
 870                 newCase = InstanceOfAccessCase::create(
 871                     vm, codeBlock, AccessCase::InstanceOfMiss, structure, ObjectPropertyConditionSet(),
 872                     prototype);
 873             } else if (structure-&gt;prototypeQueriesAreCacheable()) {
 874                 // FIXME: Teach this to do poly proto.
 875                 // https://bugs.webkit.org/show_bug.cgi?id=185663
 876                 prepareChainForCaching(globalObject, value, wasFound ? prototype : nullptr);
 877                 ObjectPropertyConditionSet conditionSet = generateConditionsForInstanceOf(
 878                     vm, codeBlock, globalObject, structure, prototype, wasFound);
 879 
 880                 if (conditionSet.isValid()) {
 881                     newCase = InstanceOfAccessCase::create(
 882                         vm, codeBlock,
 883                         wasFound ? AccessCase::InstanceOfHit : AccessCase::InstanceOfMiss,
 884                         structure, conditionSet, prototype);
 885                 }
 886             }
 887         }
 888 
 889         if (!newCase)
 890             newCase = AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric, Identifier());
 891 
 892         LOG_IC((ICEvent::InstanceOfAddAccessCase, structure-&gt;classInfo(), Identifier()));
 893 
 894         result = stubInfo.addAccessCase(locker, codeBlock, nullptr, WTFMove(newCase));
 895 
 896         if (result.generatedSomeCode()) {
 897             LOG_IC((ICEvent::InstanceOfReplaceWithJump, structure-&gt;classInfo(), Identifier()));
 898 
 899             RELEASE_ASSERT(result.code());
 900 
 901             MacroAssembler::repatchJump(
 902                 stubInfo.patchableJump(),
 903                 CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 904         }
 905     }
 906 
 907     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 908 
 909     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 910 }
 911 
 912 void repatchInstanceOf(
 913     JSGlobalObject* globalObject, CodeBlock* codeBlock, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 914     bool wasFound)
 915 {
 916     SuperSamplerScope superSamplerScope(false);
 917     if (tryCacheInstanceOf(globalObject, codeBlock, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
 918         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInstanceOfGeneric);
 919 }
 920 
 921 static void linkSlowFor(VM&amp;, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)
 922 {
 923     MacroAssembler::repatchNearCall(callLinkInfo.callReturnLocation(), CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(codeRef.code()));
 924 }
 925 
 926 static void linkSlowFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo, ThunkGenerator generator)
 927 {
 928     linkSlowFor(vm, callLinkInfo, vm.getCTIStub(generator).retagged&lt;JITStubRoutinePtrTag&gt;());
 929 }
 930 
 931 static void linkSlowFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
 932 {
 933     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
 934     linkSlowFor(vm, callLinkInfo, virtualThunk);
 935     callLinkInfo.setSlowStub(GCAwareJITStubRoutine::create(virtualThunk, vm));
 936 }
 937 
 938 static JSCell* webAssemblyOwner(JSCell* callee)
 939 {
 940 #if ENABLE(WEBASSEMBLY)
 941     // Each WebAssembly.Instance shares the stubs from their WebAssembly.Module, which are therefore the appropriate owner.
 942     return jsCast&lt;JSWebAssemblyModule*&gt;(callee);
 943 #else
 944     UNUSED_PARAM(callee);
 945     RELEASE_ASSERT_NOT_REACHED();
 946     return nullptr;
 947 #endif // ENABLE(WEBASSEMBLY)
 948 }
 949 
 950 void linkFor(
 951     VM&amp; vm, CallFrame* callFrame, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 952     JSObject* callee, MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 953 {
 954     ASSERT(!callLinkInfo.stub());
 955 
 956     CallFrame* callerFrame = callFrame-&gt;callerFrame();
 957     // Our caller must have a cell for a callee. When calling
 958     // this from Wasm, we ensure the callee is a cell.
 959     ASSERT(callerFrame-&gt;callee().isCell());
 960 
 961     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 962 
 963     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 964     JSCell* owner = isWebAssemblyModule(callerFrame-&gt;callee().asCell()) ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 965     ASSERT(owner);
 966 
 967     ASSERT(!callLinkInfo.isLinked());
 968     callLinkInfo.setCallee(vm, owner, callee);
 969     MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), callee);
 970     callLinkInfo.setLastSeenCallee(vm, owner, callee);
 971     if (shouldDumpDisassemblyFor(callerCodeBlock))
 972         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 973 
 974     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 975 
 976     if (calleeCodeBlock)
 977         calleeCodeBlock-&gt;linkIncomingCall(callerFrame, &amp;callLinkInfo);
 978 
 979     if (callLinkInfo.specializationKind() == CodeForCall &amp;&amp; callLinkInfo.allowStubs()) {
 980         linkSlowFor(vm, callLinkInfo, linkPolymorphicCallThunkGenerator);
 981         return;
 982     }
 983 
 984     linkSlowFor(vm, callLinkInfo);
 985 }
 986 
 987 void linkDirectFor(
 988     CallFrame* callFrame, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 989     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 990 {
 991     ASSERT(!callLinkInfo.stub());
 992 
 993     CodeBlock* callerCodeBlock = callFrame-&gt;codeBlock();
 994 
 995     VM&amp; vm = callerCodeBlock-&gt;vm();
 996 
 997     ASSERT(!callLinkInfo.isLinked());
 998     callLinkInfo.setCodeBlock(vm, callerCodeBlock, jsCast&lt;FunctionCodeBlock*&gt;(calleeCodeBlock));
 999     if (shouldDumpDisassemblyFor(callerCodeBlock))
1000         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
1001 
1002     if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
1003         MacroAssembler::repatchJumpToNop(callLinkInfo.patchableJump());
1004     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
1005 
1006     if (calleeCodeBlock)
1007         calleeCodeBlock-&gt;linkIncomingCall(callFrame, &amp;callLinkInfo);
1008 }
1009 
1010 void linkSlowFor(CallFrame* callFrame, CallLinkInfo&amp; callLinkInfo)
1011 {
1012     CodeBlock* callerCodeBlock = callFrame-&gt;callerFrame()-&gt;codeBlock();
1013     VM&amp; vm = callerCodeBlock-&gt;vm();
1014 
1015     linkSlowFor(vm, callLinkInfo);
1016 }
1017 
1018 static void revertCall(VM&amp; vm, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)
1019 {
1020     if (callLinkInfo.isDirect()) {
1021         callLinkInfo.clearCodeBlock();
1022         if (!callLinkInfo.clearedByJettison()) {
1023             if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
1024                 MacroAssembler::repatchJump(callLinkInfo.patchableJump(), callLinkInfo.slowPathStart());
1025             else
1026                 MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), callLinkInfo.slowPathStart());
1027         }
1028     } else {
1029         if (!callLinkInfo.clearedByJettison()) {
1030             MacroAssembler::revertJumpReplacementToBranchPtrWithPatch(
1031                 MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
1032                 callLinkInfo.calleeGPR(), 0);
1033             linkSlowFor(vm, callLinkInfo, codeRef);
1034             MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), nullptr);
1035         }
1036         callLinkInfo.clearCallee();
1037     }
1038     callLinkInfo.clearSeen();
1039     callLinkInfo.clearStub();
1040     callLinkInfo.clearSlowStub();
1041     if (callLinkInfo.isOnList())
1042         callLinkInfo.remove();
1043 }
1044 
1045 void unlinkFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
1046 {
1047     dataLogLnIf(Options::dumpDisassembly(), &quot;Unlinking call at &quot;, callLinkInfo.hotPathOther());
1048 
1049     revertCall(vm, callLinkInfo, vm.getCTIStub(linkCallThunkGenerator).retagged&lt;JITStubRoutinePtrTag&gt;());
1050 }
1051 
1052 static void linkVirtualFor(VM&amp; vm, CallFrame* callFrame, CallLinkInfo&amp; callLinkInfo)
1053 {
1054     CallFrame* callerFrame = callFrame-&gt;callerFrame();
1055     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
1056 
1057     dataLogLnIf(shouldDumpDisassemblyFor(callerCodeBlock),
1058         &quot;Linking virtual call at &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()));
1059 
1060     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
1061     revertCall(vm, callLinkInfo, virtualThunk);
1062     callLinkInfo.setSlowStub(GCAwareJITStubRoutine::create(virtualThunk, vm));
1063     callLinkInfo.setClearedByVirtual();
1064 }
1065 
1066 namespace {
1067 struct CallToCodePtr {
1068     CCallHelpers::Call call;
1069     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1070 };
1071 } // annonymous namespace
1072 
1073 void linkPolymorphicCall(JSGlobalObject* globalObject, CallFrame* callFrame, CallLinkInfo&amp; callLinkInfo, CallVariant newVariant)
1074 {
1075     RELEASE_ASSERT(callLinkInfo.allowStubs());
1076 
1077     CallFrame* callerFrame = callFrame-&gt;callerFrame();
1078     VM&amp; vm = globalObject-&gt;vm();
1079 
1080     // During execution of linkPolymorphicCall, we strongly assume that we never do GC.
1081     // GC jettisons CodeBlocks, changes CallLinkInfo etc. and breaks assumption done before and after this call.
1082     DeferGCForAWhile deferGCForAWhile(vm.heap);
1083 
1084     if (!newVariant) {
1085         linkVirtualFor(vm, callFrame, callLinkInfo);
1086         return;
1087     }
1088 
1089     // Our caller must be have a cell for a callee. When calling
1090     // this from Wasm, we ensure the callee is a cell.
1091     ASSERT(callerFrame-&gt;callee().isCell());
1092 
1093     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
1094     bool isWebAssembly = isWebAssemblyModule(callerFrame-&gt;callee().asCell());
1095 
1096     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
1097     JSCell* owner = isWebAssembly ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
1098     ASSERT(owner);
1099 
1100     CallVariantList list;
1101     if (PolymorphicCallStubRoutine* stub = callLinkInfo.stub())
1102         list = stub-&gt;variants();
1103     else if (JSObject* oldCallee = callLinkInfo.callee())
1104         list = CallVariantList { CallVariant(oldCallee) };
1105 
1106     list = variantListWithVariant(list, newVariant);
1107 
1108     // If there are any closure calls then it makes sense to treat all of them as closure calls.
1109     // This makes switching on callee cheaper. It also produces profiling that&#39;s easier on the DFG;
1110     // the DFG doesn&#39;t really want to deal with a combination of closure and non-closure callees.
1111     bool isClosureCall = false;
1112     for (CallVariant variant : list)  {
1113         if (variant.isClosureCall()) {
1114             list = despecifiedVariantList(list);
1115             isClosureCall = true;
1116             break;
1117         }
1118     }
1119 
1120     if (isClosureCall)
1121         callLinkInfo.setHasSeenClosure();
1122 
1123     Vector&lt;PolymorphicCallCase&gt; callCases;
1124     Vector&lt;int64_t&gt; caseValues;
1125 
1126     // Figure out what our cases are.
1127     for (CallVariant variant : list) {
1128         CodeBlock* codeBlock = nullptr;
1129         if (variant.executable() &amp;&amp; !variant.executable()-&gt;isHostFunction()) {
1130             ExecutableBase* executable = variant.executable();
1131             codeBlock = jsCast&lt;FunctionExecutable*&gt;(executable)-&gt;codeBlockForCall();
1132             // If we cannot handle a callee, either because we don&#39;t have a CodeBlock or because arity mismatch,
1133             // assume that it&#39;s better for this whole thing to be a virtual call.
1134             if (!codeBlock || callFrame-&gt;argumentCountIncludingThis() &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()) || callLinkInfo.isVarargs()) {
1135                 linkVirtualFor(vm, callFrame, callLinkInfo);
1136                 return;
1137             }
1138         }
1139 
1140         int64_t newCaseValue = 0;
1141         if (isClosureCall) {
1142             newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.executable());
1143             // FIXME: We could add a fast path for InternalFunction with closure call.
1144             // https://bugs.webkit.org/show_bug.cgi?id=179311
1145             if (!newCaseValue)
1146                 continue;
1147         } else {
1148             if (auto* function = variant.function())
1149                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(function);
1150             else
1151                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.internalFunction());
1152         }
1153 
1154         if (ASSERT_ENABLED) {
1155             if (caseValues.contains(newCaseValue)) {
1156                 dataLog(&quot;ERROR: Attempt to add duplicate case value.\n&quot;);
1157                 dataLog(&quot;Existing case values: &quot;);
1158                 CommaPrinter comma;
1159                 for (auto&amp; value : caseValues)
1160                     dataLog(comma, value);
1161                 dataLog(&quot;\n&quot;);
1162                 dataLog(&quot;Attempting to add: &quot;, newCaseValue, &quot;\n&quot;);
1163                 dataLog(&quot;Variant list: &quot;, listDump(callCases), &quot;\n&quot;);
1164                 RELEASE_ASSERT_NOT_REACHED();
1165             }
1166         }
1167 
1168         callCases.append(PolymorphicCallCase(variant, codeBlock));
1169         caseValues.append(newCaseValue);
1170     }
1171     ASSERT(callCases.size() == caseValues.size());
1172 
1173     // If we are over the limit, just use a normal virtual call.
1174     unsigned maxPolymorphicCallVariantListSize;
1175     if (isWebAssembly)
1176         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForWebAssemblyToJS();
1177     else if (callerCodeBlock-&gt;jitType() == JITCode::topTierJIT())
1178         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForTopTier();
1179     else
1180         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSize();
1181 
1182     // We use list.size() instead of callCases.size() because we respect CallVariant size for now.
1183     if (list.size() &gt; maxPolymorphicCallVariantListSize) {
1184         linkVirtualFor(vm, callFrame, callLinkInfo);
1185         return;
1186     }
1187 
1188     Vector&lt;CallToCodePtr&gt; calls(callCases.size());
1189     UniqueArray&lt;uint32_t&gt; fastCounts;
1190 
1191     if (!isWebAssembly &amp;&amp; callerCodeBlock-&gt;jitType() != JITCode::topTierJIT()) {
1192         fastCounts = makeUniqueArray&lt;uint32_t&gt;(callCases.size());
1193         memset(fastCounts.get(), 0, callCases.size() * sizeof(uint32_t));
1194     }
1195 
1196     GPRReg calleeGPR = callLinkInfo.calleeGPR();
1197 
1198     CCallHelpers stubJit(callerCodeBlock);
1199 
1200     std::unique_ptr&lt;CallFrameShuffler&gt; frameShuffler;
1201     if (callLinkInfo.frameShuffleData()) {
1202         ASSERT(callLinkInfo.isTailCall());
1203         frameShuffler = makeUnique&lt;CallFrameShuffler&gt;(stubJit, *callLinkInfo.frameShuffleData());
1204 #if USE(JSVALUE32_64)
1205         // We would have already checked that the callee is a cell, and we can
1206         // use the additional register this buys us.
1207         frameShuffler-&gt;assumeCalleeIsCell();
1208 #endif
1209         frameShuffler-&gt;lockGPR(calleeGPR);
1210     }
1211 
1212     GPRReg comparisonValueGPR;
1213     if (isClosureCall) {
1214         if (frameShuffler)
1215             comparisonValueGPR = frameShuffler-&gt;acquireGPR();
1216         else
1217             comparisonValueGPR = AssemblyHelpers::selectScratchGPR(calleeGPR);
1218     } else
1219         comparisonValueGPR = calleeGPR;
1220 
1221     GPRReg fastCountsBaseGPR;
1222     if (frameShuffler)
1223         fastCountsBaseGPR = frameShuffler-&gt;acquireGPR();
1224     else {
1225         fastCountsBaseGPR =
1226             AssemblyHelpers::selectScratchGPR(calleeGPR, comparisonValueGPR, GPRInfo::regT3);
1227     }
1228     stubJit.move(CCallHelpers::TrustedImmPtr(fastCounts.get()), fastCountsBaseGPR);
1229 
1230     if (!frameShuffler &amp;&amp; callLinkInfo.isTailCall()) {
1231         // We strongly assume that calleeGPR is not a callee save register in the slow path.
1232         ASSERT(!callerCodeBlock-&gt;calleeSaveRegisters()-&gt;find(calleeGPR));
1233         stubJit.emitRestoreCalleeSaves();
1234     }
1235 
1236     CCallHelpers::JumpList slowPath;
1237     if (isClosureCall) {
1238         // Verify that we have a function and stash the executable in scratchGPR.
1239 #if USE(JSVALUE64)
1240         if (callLinkInfo.isTailCall())
1241             slowPath.append(stubJit.branchIfNotCell(calleeGPR, DoNotHaveTagRegisters));
1242         else
1243             slowPath.append(stubJit.branchIfNotCell(calleeGPR));
1244 #else
1245         // We would have already checked that the callee is a cell.
1246 #endif
1247         // FIXME: We could add a fast path for InternalFunction with closure call.
1248         slowPath.append(stubJit.branchIfNotFunction(calleeGPR));
1249 
1250         stubJit.loadPtr(CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutableOrRareData()), comparisonValueGPR);
1251         auto hasExecutable = stubJit.branchTestPtr(CCallHelpers::Zero, comparisonValueGPR, CCallHelpers::TrustedImm32(JSFunction::rareDataTag));
1252         stubJit.loadPtr(CCallHelpers::Address(comparisonValueGPR, FunctionRareData::offsetOfExecutable() - JSFunction::rareDataTag), comparisonValueGPR);
1253         hasExecutable.link(&amp;stubJit);
1254     }
1255 
1256     BinarySwitch binarySwitch(comparisonValueGPR, caseValues, BinarySwitch::IntPtr);
1257     CCallHelpers::JumpList done;
1258     while (binarySwitch.advance(stubJit)) {
1259         size_t caseIndex = binarySwitch.caseIndex();
1260 
1261         CallVariant variant = callCases[caseIndex].variant();
1262 
1263         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1264         if (variant.executable()) {
1265             ASSERT(variant.executable()-&gt;hasJITCodeForCall());
1266 
1267             codePtr = jsToWasmICCodePtr(vm, callLinkInfo.specializationKind(), variant.function());
1268             if (!codePtr)
1269                 codePtr = variant.executable()-&gt;generatedJITCodeForCall()-&gt;addressForCall(ArityCheckNotRequired);
1270         } else {
1271             ASSERT(variant.internalFunction());
1272             codePtr = vm.getCTIInternalFunctionTrampolineFor(CodeForCall);
1273         }
1274 
1275         if (fastCounts) {
1276             stubJit.add32(
1277                 CCallHelpers::TrustedImm32(1),
1278                 CCallHelpers::Address(fastCountsBaseGPR, caseIndex * sizeof(uint32_t)));
1279         }
1280         if (frameShuffler) {
1281             CallFrameShuffler(stubJit, frameShuffler-&gt;snapshot()).prepareForTailCall();
1282             calls[caseIndex].call = stubJit.nearTailCall();
1283         } else if (callLinkInfo.isTailCall()) {
1284             stubJit.prepareForTailCallSlow();
1285             calls[caseIndex].call = stubJit.nearTailCall();
1286         } else
1287             calls[caseIndex].call = stubJit.nearCall();
1288         calls[caseIndex].codePtr = codePtr;
1289         done.append(stubJit.jump());
1290     }
1291 
1292     slowPath.link(&amp;stubJit);
1293     binarySwitch.fallThrough().link(&amp;stubJit);
1294 
1295     if (frameShuffler) {
1296         frameShuffler-&gt;releaseGPR(calleeGPR);
1297         frameShuffler-&gt;releaseGPR(comparisonValueGPR);
1298         frameShuffler-&gt;releaseGPR(fastCountsBaseGPR);
1299 #if USE(JSVALUE32_64)
1300         frameShuffler-&gt;setCalleeJSValueRegs(JSValueRegs(GPRInfo::regT1, GPRInfo::regT0));
1301 #else
1302         frameShuffler-&gt;setCalleeJSValueRegs(JSValueRegs(GPRInfo::regT0));
1303 #endif
1304         frameShuffler-&gt;prepareForSlowPath();
1305     } else {
1306         stubJit.move(calleeGPR, GPRInfo::regT0);
1307 #if USE(JSVALUE32_64)
1308         stubJit.move(CCallHelpers::TrustedImm32(JSValue::CellTag), GPRInfo::regT1);
1309 #endif
1310     }
1311     stubJit.move(CCallHelpers::TrustedImmPtr(globalObject), GPRInfo::regT3);
1312     stubJit.move(CCallHelpers::TrustedImmPtr(&amp;callLinkInfo), GPRInfo::regT2);
1313     stubJit.move(CCallHelpers::TrustedImmPtr(callLinkInfo.callReturnLocation().untaggedExecutableAddress()), GPRInfo::regT4);
1314 
1315     stubJit.restoreReturnAddressBeforeReturn(GPRInfo::regT4);
1316     AssemblyHelpers::Jump slow = stubJit.jump();
1317 
1318     LinkBuffer patchBuffer(stubJit, owner, JITCompilationCanFail);
1319     if (patchBuffer.didFailToAllocate()) {
1320         linkVirtualFor(vm, callFrame, callLinkInfo);
1321         return;
1322     }
1323 
1324     RELEASE_ASSERT(callCases.size() == calls.size());
1325     for (CallToCodePtr callToCodePtr : calls) {
1326 #if CPU(ARM_THUMB2)
1327         // Tail call special-casing ensures proper linking on ARM Thumb2, where a tail call jumps to an address
1328         // with a non-decorated bottom bit but a normal call calls an address with a decorated bottom bit.
1329         bool isTailCall = callToCodePtr.call.isFlagSet(CCallHelpers::Call::Tail);
1330         void* target = isTailCall ? callToCodePtr.codePtr.dataLocation() : callToCodePtr.codePtr.executableAddress();
1331         patchBuffer.link(callToCodePtr.call, FunctionPtr&lt;JSEntryPtrTag&gt;(MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt;::createFromExecutableAddress(target)));
1332 #else
1333         patchBuffer.link(callToCodePtr.call, FunctionPtr&lt;JSEntryPtrTag&gt;(callToCodePtr.codePtr));
1334 #endif
1335     }
1336     if (isWebAssembly || JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType()))
1337         patchBuffer.link(done, callLinkInfo.callReturnLocation().labelAtOffset(0));
1338     else
1339         patchBuffer.link(done, callLinkInfo.hotPathOther().labelAtOffset(0));
1340     patchBuffer.link(slow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.getCTIStub(linkPolymorphicCallThunkGenerator).code()));
1341 
1342     auto stubRoutine = adoptRef(*new PolymorphicCallStubRoutine(
1343         FINALIZE_CODE_FOR(
1344             callerCodeBlock, patchBuffer, JITStubRoutinePtrTag,
1345             &quot;Polymorphic call stub for %s, return point %p, targets %s&quot;,
1346                 isWebAssembly ? &quot;WebAssembly&quot; : toCString(*callerCodeBlock).data(), callLinkInfo.callReturnLocation().labelAtOffset(0).executableAddress(),
1347                 toCString(listDump(callCases)).data()),
1348         vm, owner, callFrame-&gt;callerFrame(), callLinkInfo, callCases,
1349         WTFMove(fastCounts)));
1350 
1351     MacroAssembler::replaceWithJump(
1352         MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
1353         CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(stubRoutine-&gt;code().code()));
1354     // The original slow path is unreachable on 64-bits, but still
1355     // reachable on 32-bits since a non-cell callee will always
1356     // trigger the slow path
1357     linkSlowFor(vm, callLinkInfo);
1358 
1359     // If there had been a previous stub routine, that one will die as soon as the GC runs and sees
1360     // that it&#39;s no longer on stack.
1361     callLinkInfo.setStub(WTFMove(stubRoutine));
1362 
1363     // The call link info no longer has a call cache apart from the jump to the polymorphic call
1364     // stub.
1365     if (callLinkInfo.isOnList())
1366         callLinkInfo.remove();
1367 }
1368 
1369 void resetGetBy(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, GetByKind kind)
1370 {
1371     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, appropriateOptimizingGetByFunction(kind));
1372     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation);
1373 }
1374 
1375 void resetPutByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1376 {
1377     V_JITOperation_GSsiJJI unoptimizedFunction = reinterpret_cast&lt;V_JITOperation_GSsiJJI&gt;(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation).executableAddress());
1378     V_JITOperation_GSsiJJI optimizedFunction;
1379     if (unoptimizedFunction == operationPutByIdStrict || unoptimizedFunction == operationPutByIdStrictOptimize)
1380         optimizedFunction = operationPutByIdStrictOptimize;
1381     else if (unoptimizedFunction == operationPutByIdNonStrict || unoptimizedFunction == operationPutByIdNonStrictOptimize)
1382         optimizedFunction = operationPutByIdNonStrictOptimize;
1383     else if (unoptimizedFunction == operationPutByIdDirectStrict || unoptimizedFunction == operationPutByIdDirectStrictOptimize)
1384         optimizedFunction = operationPutByIdDirectStrictOptimize;
1385     else {
1386         ASSERT(unoptimizedFunction == operationPutByIdDirectNonStrict || unoptimizedFunction == operationPutByIdDirectNonStrictOptimize);
1387         optimizedFunction = operationPutByIdDirectNonStrictOptimize;
1388     }
1389 
1390     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, optimizedFunction);
1391     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation);
1392 }
1393 
1394 static void resetPatchableJump(StructureStubInfo&amp; stubInfo)
1395 {
1396     MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation);
1397 }
1398 
1399 void resetInByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1400 {
1401     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation, operationInByIdOptimize);
1402     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation);
1403 }
1404 
1405 void resetInstanceOf(StructureStubInfo&amp; stubInfo)
1406 {
1407     resetPatchableJump(stubInfo);
1408 }
1409 
1410 MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jsToWasmICCodePtr(VM&amp; vm, CodeSpecializationKind kind, JSObject* callee)
1411 {
1412 #if ENABLE(WEBASSEMBLY)
1413     if (!callee)
1414         return nullptr;
1415     if (kind != CodeForCall)
1416         return nullptr;
1417     if (auto* wasmFunction = jsDynamicCast&lt;WebAssemblyFunction*&gt;(vm, callee))
1418         return wasmFunction-&gt;jsCallEntrypoint();
1419 #else
1420     UNUSED_PARAM(vm);
1421     UNUSED_PARAM(kind);
1422     UNUSED_PARAM(callee);
1423 #endif
1424     return nullptr;
1425 }
1426 
1427 } // namespace JSC
1428 
1429 #endif
    </pre>
  </body>
</html>