<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2014-2020 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;PolymorphicAccess.h&quot;
 28 
 29 #if ENABLE(JIT)
 30 
 31 #include &quot;BinarySwitch.h&quot;
 32 #include &quot;CCallHelpers.h&quot;
 33 #include &quot;CacheableIdentifierInlines.h&quot;
 34 #include &quot;CodeBlock.h&quot;
 35 #include &quot;FullCodeOrigin.h&quot;
 36 #include &quot;Heap.h&quot;
 37 #include &quot;JITOperations.h&quot;
 38 #include &quot;JSCInlines.h&quot;
 39 #include &quot;LinkBuffer.h&quot;
 40 #include &quot;StructureStubClearingWatchpoint.h&quot;
 41 #include &quot;StructureStubInfo.h&quot;
 42 #include &quot;SuperSampler.h&quot;
 43 #include &lt;wtf/CommaPrinter.h&gt;
 44 #include &lt;wtf/ListDump.h&gt;
 45 
 46 namespace JSC {
 47 
 48 namespace PolymorphicAccessInternal {
 49 static constexpr bool verbose = false;
 50 }
 51 
 52 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(PolymorphicAccess);
 53 
 54 void AccessGenerationResult::dump(PrintStream&amp; out) const
 55 {
 56     out.print(m_kind);
 57     if (m_code)
 58         out.print(&quot;:&quot;, m_code);
 59 }
 60 
 61 void AccessGenerationState::installWatchpoint(const ObjectPropertyCondition&amp; condition)
 62 {
 63     WatchpointsOnStructureStubInfo::ensureReferenceAndInstallWatchpoint(
 64         watchpoints, jit-&gt;codeBlock(), stubInfo, condition);
 65 }
 66 
 67 void AccessGenerationState::restoreScratch()
 68 {
 69     allocator-&gt;restoreReusedRegistersByPopping(*jit, preservedReusedRegisterState);
 70 }
 71 
 72 void AccessGenerationState::succeed()
 73 {
 74     restoreScratch();
 75     success.append(jit-&gt;jump());
 76 }
 77 
 78 const RegisterSet&amp; AccessGenerationState::liveRegistersForCall()
 79 {
 80     if (!m_calculatedRegistersForCallAndExceptionHandling)
 81         calculateLiveRegistersForCallAndExceptionHandling();
 82     return m_liveRegistersForCall;
 83 }
 84 
 85 const RegisterSet&amp; AccessGenerationState::liveRegistersToPreserveAtExceptionHandlingCallSite()
 86 {
 87     if (!m_calculatedRegistersForCallAndExceptionHandling)
 88         calculateLiveRegistersForCallAndExceptionHandling();
 89     return m_liveRegistersToPreserveAtExceptionHandlingCallSite;
 90 }
 91 
 92 static RegisterSet calleeSaveRegisters()
 93 {
 94     RegisterSet result = RegisterSet::registersToNotSaveForJSCall();
 95     result.filter(RegisterSet::registersToNotSaveForCCall());
 96     return result;
 97 }
 98 
 99 const RegisterSet&amp; AccessGenerationState::calculateLiveRegistersForCallAndExceptionHandling()
100 {
101     if (!m_calculatedRegistersForCallAndExceptionHandling) {
102         m_calculatedRegistersForCallAndExceptionHandling = true;
103 
104         m_liveRegistersToPreserveAtExceptionHandlingCallSite = jit-&gt;codeBlock()-&gt;jitCode()-&gt;liveRegistersToPreserveAtExceptionHandlingCallSite(jit-&gt;codeBlock(), stubInfo-&gt;callSiteIndex);
105         m_needsToRestoreRegistersIfException = m_liveRegistersToPreserveAtExceptionHandlingCallSite.numberOfSetRegisters() &gt; 0;
106         if (m_needsToRestoreRegistersIfException)
107             RELEASE_ASSERT(JITCode::isOptimizingJIT(jit-&gt;codeBlock()-&gt;jitType()));
108 
109         m_liveRegistersForCall = RegisterSet(m_liveRegistersToPreserveAtExceptionHandlingCallSite, allocator-&gt;usedRegisters());
110         m_liveRegistersForCall.exclude(calleeSaveRegisters());
111     }
112     return m_liveRegistersForCall;
113 }
114 
115 auto AccessGenerationState::preserveLiveRegistersToStackForCall(const RegisterSet&amp; extra) -&gt; SpillState
116 {
117     RegisterSet liveRegisters = liveRegistersForCall();
118     liveRegisters.merge(extra);
119 
120     unsigned extraStackPadding = 0;
121     unsigned numberOfStackBytesUsedForRegisterPreservation = ScratchRegisterAllocator::preserveRegistersToStackForCall(*jit, liveRegisters, extraStackPadding);
122     return SpillState {
123         WTFMove(liveRegisters),
124         numberOfStackBytesUsedForRegisterPreservation
125     };
126 }
127 
128 void AccessGenerationState::restoreLiveRegistersFromStackForCallWithThrownException(const SpillState&amp; spillState)
129 {
130     // Even if we&#39;re a getter, we don&#39;t want to ignore the result value like we normally do
131     // because the getter threw, and therefore, didn&#39;t return a value that means anything.
132     // Instead, we want to restore that register to what it was upon entering the getter
133     // inline cache. The subtlety here is if the base and the result are the same register,
134     // and the getter threw, we want OSR exit to see the original base value, not the result
135     // of the getter call.
136     RegisterSet dontRestore = spillState.spilledRegisters;
137     // As an optimization here, we only need to restore what is live for exception handling.
138     // We can construct the dontRestore set to accomplish this goal by having it contain only
139     // what is live for call but not live for exception handling. By ignoring things that are
140     // only live at the call but not the exception handler, we will only restore things live
141     // at the exception handler.
142     dontRestore.exclude(liveRegistersToPreserveAtExceptionHandlingCallSite());
143     restoreLiveRegistersFromStackForCall(spillState, dontRestore);
144 }
145 
146 void AccessGenerationState::restoreLiveRegistersFromStackForCall(const SpillState&amp; spillState, const RegisterSet&amp; dontRestore)
147 {
148     unsigned extraStackPadding = 0;
149     ScratchRegisterAllocator::restoreRegistersFromStackForCall(*jit, spillState.spilledRegisters, dontRestore, spillState.numberOfStackBytesUsedForRegisterPreservation, extraStackPadding);
150 }
151 
152 CallSiteIndex AccessGenerationState::callSiteIndexForExceptionHandlingOrOriginal()
153 {
154     if (!m_calculatedRegistersForCallAndExceptionHandling)
155         calculateLiveRegistersForCallAndExceptionHandling();
156 
157     if (!m_calculatedCallSiteIndex) {
158         m_calculatedCallSiteIndex = true;
159 
160         if (m_needsToRestoreRegistersIfException)
161             m_callSiteIndex = jit-&gt;codeBlock()-&gt;newExceptionHandlingCallSiteIndex(stubInfo-&gt;callSiteIndex);
162         else
163             m_callSiteIndex = originalCallSiteIndex();
164     }
165 
166     return m_callSiteIndex;
167 }
168 
169 DisposableCallSiteIndex AccessGenerationState::callSiteIndexForExceptionHandling()
170 {
171     RELEASE_ASSERT(m_calculatedRegistersForCallAndExceptionHandling);
172     RELEASE_ASSERT(m_needsToRestoreRegistersIfException);
173     RELEASE_ASSERT(m_calculatedCallSiteIndex);
174     return DisposableCallSiteIndex::fromCallSiteIndex(m_callSiteIndex);
175 }
176 
177 const HandlerInfo&amp; AccessGenerationState::originalExceptionHandler()
178 {
179     if (!m_calculatedRegistersForCallAndExceptionHandling)
180         calculateLiveRegistersForCallAndExceptionHandling();
181 
182     RELEASE_ASSERT(m_needsToRestoreRegistersIfException);
183     HandlerInfo* exceptionHandler = jit-&gt;codeBlock()-&gt;handlerForIndex(stubInfo-&gt;callSiteIndex.bits());
184     RELEASE_ASSERT(exceptionHandler);
185     return *exceptionHandler;
186 }
187 
188 CallSiteIndex AccessGenerationState::originalCallSiteIndex() const { return stubInfo-&gt;callSiteIndex; }
189 
190 void AccessGenerationState::emitExplicitExceptionHandler()
191 {
192     restoreScratch();
193     jit-&gt;pushToSave(GPRInfo::regT0);
194     jit-&gt;loadPtr(&amp;m_vm.topEntryFrame, GPRInfo::regT0);
195     jit-&gt;copyCalleeSavesToEntryFrameCalleeSavesBuffer(GPRInfo::regT0);
196     jit-&gt;popToRestore(GPRInfo::regT0);
197 
198     if (needsToRestoreRegistersIfException()) {
199         // To the JIT that produces the original exception handling
200         // call site, they will expect the OSR exit to be arrived
201         // at from genericUnwind. Therefore we must model what genericUnwind
202         // does here. I.e, set callFrameForCatch and copy callee saves.
203 
204         jit-&gt;storePtr(GPRInfo::callFrameRegister, m_vm.addressOfCallFrameForCatch());
205         CCallHelpers::Jump jumpToOSRExitExceptionHandler = jit-&gt;jump();
206 
207         // We don&#39;t need to insert a new exception handler in the table
208         // because we&#39;re doing a manual exception check here. i.e, we&#39;ll
209         // never arrive here from genericUnwind().
210         HandlerInfo originalHandler = originalExceptionHandler();
211         jit-&gt;addLinkTask(
212             [=] (LinkBuffer&amp; linkBuffer) {
213                 linkBuffer.link(jumpToOSRExitExceptionHandler, originalHandler.nativeCode);
214             });
215     } else {
216         jit-&gt;setupArguments&lt;decltype(operationLookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(&amp;m_vm));
217         jit-&gt;prepareCallOperation(m_vm);
218         CCallHelpers::Call lookupExceptionHandlerCall = jit-&gt;call(OperationPtrTag);
219         jit-&gt;addLinkTask(
220             [=] (LinkBuffer&amp; linkBuffer) {
221                 linkBuffer.link(lookupExceptionHandlerCall, FunctionPtr&lt;OperationPtrTag&gt;(operationLookupExceptionHandler));
222             });
223         jit-&gt;jumpToExceptionHandler(m_vm);
224     }
225 }
226 
227 PolymorphicAccess::PolymorphicAccess() { }
228 PolymorphicAccess::~PolymorphicAccess() { }
229 
230 AccessGenerationResult PolymorphicAccess::addCases(
231     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo,
232     Vector&lt;std::unique_ptr&lt;AccessCase&gt;, 2&gt; originalCasesToAdd)
233 {
234     SuperSamplerScope superSamplerScope(false);
235 
236     // This method will add the originalCasesToAdd to the list one at a time while preserving the
237     // invariants:
238     // - If a newly added case canReplace() any existing case, then the existing case is removed before
239     //   the new case is added. Removal doesn&#39;t change order of the list. Any number of existing cases
240     //   can be removed via the canReplace() rule.
241     // - Cases in the list always appear in ascending order of time of addition. Therefore, if you
242     //   cascade through the cases in reverse order, you will get the most recent cases first.
243     // - If this method fails (returns null, doesn&#39;t add the cases), then both the previous case list
244     //   and the previous stub are kept intact and the new cases are destroyed. It&#39;s OK to attempt to
245     //   add more things after failure.
246 
247     // First ensure that the originalCasesToAdd doesn&#39;t contain duplicates.
248     Vector&lt;std::unique_ptr&lt;AccessCase&gt;&gt; casesToAdd;
249     for (unsigned i = 0; i &lt; originalCasesToAdd.size(); ++i) {
250         std::unique_ptr&lt;AccessCase&gt; myCase = WTFMove(originalCasesToAdd[i]);
251 
252         // Add it only if it is not replaced by the subsequent cases in the list.
253         bool found = false;
254         for (unsigned j = i + 1; j &lt; originalCasesToAdd.size(); ++j) {
255             if (originalCasesToAdd[j]-&gt;canReplace(*myCase)) {
256                 found = true;
257                 break;
258             }
259         }
260 
261         if (found)
262             continue;
263 
264         casesToAdd.append(WTFMove(myCase));
265     }
266 
267     if (PolymorphicAccessInternal::verbose)
268         dataLog(&quot;casesToAdd: &quot;, listDump(casesToAdd), &quot;\n&quot;);
269 
270     // If there aren&#39;t any cases to add, then fail on the grounds that there&#39;s no point to generating a
271     // new stub that will be identical to the old one. Returning null should tell the caller to just
272     // keep doing what they were doing before.
273     if (casesToAdd.isEmpty())
274         return AccessGenerationResult::MadeNoChanges;
275 
276     if (stubInfo.accessType != AccessType::InstanceOf) {
277         bool shouldReset = false;
278         AccessGenerationResult resetResult(AccessGenerationResult::ResetStubAndFireWatchpoints);
279         auto considerPolyProtoReset = [&amp;] (Structure* a, Structure* b) {
280             if (Structure::shouldConvertToPolyProto(a, b)) {
281                 // For now, we only reset if this is our first time invalidating this watchpoint.
282                 // The reason we don&#39;t immediately fire this watchpoint is that we may be already
283                 // watching the poly proto watchpoint, which if fired, would destroy us. We let
284                 // the person handling the result to do a delayed fire.
285                 ASSERT(a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint().get() == b-&gt;rareData()-&gt;sharedPolyProtoWatchpoint().get());
286                 if (a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint()-&gt;isStillValid()) {
287                     shouldReset = true;
288                     resetResult.addWatchpointToFire(*a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint(), StringFireDetail(&quot;Detected poly proto optimization opportunity.&quot;));
289                 }
290             }
291         };
292 
293         for (auto&amp; caseToAdd : casesToAdd) {
294             for (auto&amp; existingCase : m_list) {
295                 Structure* a = caseToAdd-&gt;structure();
296                 Structure* b = existingCase-&gt;structure();
297                 considerPolyProtoReset(a, b);
298             }
299         }
300         for (unsigned i = 0; i &lt; casesToAdd.size(); ++i) {
301             for (unsigned j = i + 1; j &lt; casesToAdd.size(); ++j) {
302                 Structure* a = casesToAdd[i]-&gt;structure();
303                 Structure* b = casesToAdd[j]-&gt;structure();
304                 considerPolyProtoReset(a, b);
305             }
306         }
307 
308         if (shouldReset)
309             return resetResult;
310     }
311 
312     // Now add things to the new list. Note that at this point, we will still have old cases that
313     // may be replaced by the new ones. That&#39;s fine. We will sort that out when we regenerate.
314     for (auto&amp; caseToAdd : casesToAdd) {
315         commit(locker, vm, m_watchpoints, codeBlock, stubInfo, *caseToAdd);
316         m_list.append(WTFMove(caseToAdd));
317     }
318 
319     if (PolymorphicAccessInternal::verbose)
320         dataLog(&quot;After addCases: m_list: &quot;, listDump(m_list), &quot;\n&quot;);
321 
322     return AccessGenerationResult::Buffered;
323 }
324 
325 AccessGenerationResult PolymorphicAccess::addCase(
326     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, std::unique_ptr&lt;AccessCase&gt; newAccess)
327 {
328     Vector&lt;std::unique_ptr&lt;AccessCase&gt;, 2&gt; newAccesses;
329     newAccesses.append(WTFMove(newAccess));
330     return addCases(locker, vm, codeBlock, stubInfo, WTFMove(newAccesses));
331 }
332 
333 bool PolymorphicAccess::visitWeak(VM&amp; vm) const
334 {
335     for (unsigned i = 0; i &lt; size(); ++i) {
336         if (!at(i).visitWeak(vm))
337             return false;
338     }
339     if (Vector&lt;WriteBarrier&lt;JSCell&gt;&gt;* weakReferences = m_weakReferences.get()) {
340         for (WriteBarrier&lt;JSCell&gt;&amp; weakReference : *weakReferences) {
341             if (!vm.heap.isMarked(weakReference.get()))
342                 return false;
343         }
344     }
345     return true;
346 }
347 
348 bool PolymorphicAccess::propagateTransitions(SlotVisitor&amp; visitor) const
349 {
350     bool result = true;
351     for (unsigned i = 0; i &lt; size(); ++i)
352         result &amp;= at(i).propagateTransitions(visitor);
353     return result;
354 }
355 
356 void PolymorphicAccess::visitAggregate(SlotVisitor&amp; visitor)
357 {
358     for (unsigned i = 0; i &lt; size(); ++i)
359         at(i).visitAggregate(visitor);
360 }
361 
362 void PolymorphicAccess::dump(PrintStream&amp; out) const
363 {
364     out.print(RawPointer(this), &quot;:[&quot;);
365     CommaPrinter comma;
366     for (auto&amp; entry : m_list)
367         out.print(comma, *entry);
368     out.print(&quot;]&quot;);
369 }
370 
371 void PolymorphicAccess::commit(
372     const GCSafeConcurrentJSLocker&amp;, VM&amp; vm, std::unique_ptr&lt;WatchpointsOnStructureStubInfo&gt;&amp; watchpoints, CodeBlock* codeBlock,
373     StructureStubInfo&amp; stubInfo, AccessCase&amp; accessCase)
374 {
375     // NOTE: We currently assume that this is relatively rare. It mainly arises for accesses to
376     // properties on DOM nodes. For sure we cache many DOM node accesses, but even in
377     // Real Pages (TM), we appear to spend most of our time caching accesses to properties on
378     // vanilla objects or exotic objects from within JSC (like Arguments, those are super popular).
379     // Those common kinds of JSC object accesses don&#39;t hit this case.
380 
381     for (WatchpointSet* set : accessCase.commit(vm)) {
382         Watchpoint* watchpoint =
383             WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
384                 watchpoints, codeBlock, &amp;stubInfo);
385 
386         set-&gt;add(watchpoint);
387     }
388 }
389 
390 AccessGenerationResult PolymorphicAccess::regenerate(
391     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
392 {
393     SuperSamplerScope superSamplerScope(false);
394 
395     if (PolymorphicAccessInternal::verbose)
396         dataLog(&quot;Regenerate with m_list: &quot;, listDump(m_list), &quot;\n&quot;);
397 
398     AccessGenerationState state(vm, codeBlock-&gt;globalObject());
399 
400     state.access = this;
401     state.stubInfo = &amp;stubInfo;
402 
403     state.baseGPR = stubInfo.baseGPR;
404     state.u.thisGPR = stubInfo.regs.thisGPR;
405     state.valueRegs = stubInfo.valueRegs();
406 
407     // Regenerating is our opportunity to figure out what our list of cases should look like. We
408     // do this here. The newly produced &#39;cases&#39; list may be smaller than m_list. We don&#39;t edit
409     // m_list in-place because we may still fail, in which case we want the PolymorphicAccess object
410     // to be unmutated. For sure, we want it to hang onto any data structures that may be referenced
411     // from the code of the current stub (aka previous).
412     ListType cases;
413     unsigned srcIndex = 0;
414     unsigned dstIndex = 0;
415     while (srcIndex &lt; m_list.size()) {
416         std::unique_ptr&lt;AccessCase&gt; someCase = WTFMove(m_list[srcIndex++]);
417 
418         // If the case had been generated, then we have to keep the original in m_list in case we
419         // fail to regenerate. That case may have data structures that are used by the code that it
420         // had generated. If the case had not been generated, then we want to remove it from m_list.
421         bool isGenerated = someCase-&gt;state() == AccessCase::Generated;
422 
423         [&amp;] () {
424             if (!someCase-&gt;couldStillSucceed())
425                 return;
426 
427             // Figure out if this is replaced by any later case. Given two cases A and B where A
428             // comes first in the case list, we know that A would have triggered first if we had
429             // generated the cases in a cascade. That&#39;s why this loop asks B-&gt;canReplace(A) but not
430             // A-&gt;canReplace(B). If A-&gt;canReplace(B) was true then A would never have requested
431             // repatching in cases where Repatch.cpp would have then gone on to generate B. If that
432             // did happen by some fluke, then we&#39;d just miss the redundancy here, which wouldn&#39;t be
433             // incorrect - just slow. However, if A&#39;s checks failed and Repatch.cpp concluded that
434             // this new condition could be handled by B and B-&gt;canReplace(A), then this says that we
435             // don&#39;t need A anymore.
436             //
437             // If we can generate a binary switch, then A-&gt;canReplace(B) == B-&gt;canReplace(A). So,
438             // it doesn&#39;t matter that we only do the check in one direction.
439             for (unsigned j = srcIndex; j &lt; m_list.size(); ++j) {
440                 if (m_list[j]-&gt;canReplace(*someCase))
441                     return;
442             }
443 
444             if (isGenerated)
445                 cases.append(someCase-&gt;clone());
446             else
447                 cases.append(WTFMove(someCase));
448         }();
449 
450         if (isGenerated)
451             m_list[dstIndex++] = WTFMove(someCase);
452     }
453     m_list.resize(dstIndex);
454 
455     ScratchRegisterAllocator allocator(stubInfo.usedRegisters);
456     state.allocator = &amp;allocator;
457     allocator.lock(state.baseGPR);
458     if (state.u.thisGPR != InvalidGPRReg)
459         allocator.lock(state.u.thisGPR);
460     allocator.lock(state.valueRegs);
461 #if USE(JSVALUE32_64)
462     allocator.lock(stubInfo.baseTagGPR);
463     if (stubInfo.v.thisTagGPR != InvalidGPRReg)
464         allocator.lock(stubInfo.v.thisTagGPR);
465 #endif
466 
467     state.scratchGPR = allocator.allocateScratchGPR();
468 
469     for (auto&amp; accessCase : cases) {
470         if (accessCase-&gt;needsScratchFPR()) {
471             state.scratchFPR = allocator.allocateScratchFPR();
472             break;
473         }
474     }
475 
476     CCallHelpers jit(codeBlock);
477     state.jit = &amp;jit;
478 
479     state.preservedReusedRegisterState =
480         allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
481 
482     bool generatedFinalCode = false;
483 
484     // If the resulting set of cases is so big that we would stop caching and this is InstanceOf,
485     // then we want to generate the generic InstanceOf and then stop.
486     if (cases.size() &gt;= Options::maxAccessVariantListSize()
487         &amp;&amp; stubInfo.accessType == AccessType::InstanceOf) {
488         while (!cases.isEmpty())
489             m_list.append(cases.takeLast());
490         cases.append(AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric, nullptr));
491         generatedFinalCode = true;
492     }
493 
494     if (PolymorphicAccessInternal::verbose)
495         dataLog(&quot;Optimized cases: &quot;, listDump(cases), &quot;\n&quot;);
496 
497     // At this point we&#39;re convinced that &#39;cases&#39; contains the cases that we want to JIT now and we
498     // won&#39;t change that set anymore.
499 
500     bool allGuardedByStructureCheck = true;
501     bool hasJSGetterSetterCall = false;
502     bool needsInt32PropertyCheck = false;
503     bool needsStringPropertyCheck = false;
504     bool needsSymbolPropertyCheck = false;
505     for (auto&amp; newCase : cases) {
506         if (!stubInfo.hasConstantIdentifier) {
507             if (newCase-&gt;requiresIdentifierNameMatch()) {
508                 if (newCase-&gt;uid()-&gt;isSymbol())
509                     needsSymbolPropertyCheck = true;
510                 else
511                     needsStringPropertyCheck = true;
512             } else if (newCase-&gt;requiresInt32PropertyCheck())
513                 needsInt32PropertyCheck = true;
514         }
515         commit(locker, vm, state.watchpoints, codeBlock, stubInfo, *newCase);
516         allGuardedByStructureCheck &amp;= newCase-&gt;guardedByStructureCheck(stubInfo);
517         if (newCase-&gt;type() == AccessCase::Getter || newCase-&gt;type() == AccessCase::Setter)
518             hasJSGetterSetterCall = true;
519     }
520 
521     if (cases.isEmpty()) {
522         // This is super unlikely, but we make it legal anyway.
523         state.failAndRepatch.append(jit.jump());
524     } else if (!allGuardedByStructureCheck || cases.size() == 1) {
525         // If there are any proxies in the list, we cannot just use a binary switch over the structure.
526         // We need to resort to a cascade. A cascade also happens to be optimal if we only have just
527         // one case.
528         CCallHelpers::JumpList fallThrough;
529         if (needsInt32PropertyCheck || needsStringPropertyCheck || needsSymbolPropertyCheck) {
530             if (needsInt32PropertyCheck) {
531                 CCallHelpers::Jump notInt32;
532 
533                 if (!stubInfo.propertyIsInt32) {
534 #if USE(JSVALUE64)
535                     notInt32 = jit.branchIfNotInt32(state.u.propertyGPR);
536 #else
537                     notInt32 = jit.branchIfNotInt32(state.stubInfo-&gt;v.propertyTagGPR);
538 #endif
539                 }
540                 for (unsigned i = cases.size(); i--;) {
541                     fallThrough.link(&amp;jit);
542                     fallThrough.clear();
543                     if (cases[i]-&gt;requiresInt32PropertyCheck())
544                         cases[i]-&gt;generateWithGuard(state, fallThrough);
545                 }
546 
547                 if (needsStringPropertyCheck || needsSymbolPropertyCheck) {
548                     if (notInt32.isSet())
549                         notInt32.link(&amp;jit);
550                     fallThrough.link(&amp;jit);
551                     fallThrough.clear();
552                 } else {
553                     if (notInt32.isSet())
554                         state.failAndRepatch.append(notInt32);
555                 }
556             }
557 
558             if (needsStringPropertyCheck) {
559                 CCallHelpers::JumpList notString;
560                 GPRReg propertyGPR = state.u.propertyGPR;
561                 if (!stubInfo.propertyIsString) {
562 #if USE(JSVALUE32_64)
563                     GPRReg propertyTagGPR = state.stubInfo-&gt;v.propertyTagGPR;
564                     notString.append(jit.branchIfNotCell(propertyTagGPR));
565 #else
566                     notString.append(jit.branchIfNotCell(propertyGPR));
567 #endif
568                     notString.append(jit.branchIfNotString(propertyGPR));
569                 }
570 
571                 jit.loadPtr(MacroAssembler::Address(propertyGPR, JSString::offsetOfValue()), state.scratchGPR);
572 
573                 state.failAndRepatch.append(jit.branchIfRopeStringImpl(state.scratchGPR));
574 
575                 for (unsigned i = cases.size(); i--;) {
576                     fallThrough.link(&amp;jit);
577                     fallThrough.clear();
578                     if (cases[i]-&gt;requiresIdentifierNameMatch() &amp;&amp; !cases[i]-&gt;uid()-&gt;isSymbol())
579                         cases[i]-&gt;generateWithGuard(state, fallThrough);
580                 }
581 
582                 if (needsSymbolPropertyCheck) {
583                     notString.link(&amp;jit);
584                     fallThrough.link(&amp;jit);
585                     fallThrough.clear();
586                 } else
587                     state.failAndRepatch.append(notString);
588             }
589 
590             if (needsSymbolPropertyCheck) {
591                 CCallHelpers::JumpList notSymbol;
592                 if (!stubInfo.propertyIsSymbol) {
593                     GPRReg propertyGPR = state.u.propertyGPR;
594 #if USE(JSVALUE32_64)
595                     GPRReg propertyTagGPR = state.stubInfo-&gt;v.propertyTagGPR;
596                     notSymbol.append(jit.branchIfNotCell(propertyTagGPR));
597 #else
598                     notSymbol.append(jit.branchIfNotCell(propertyGPR));
599 #endif
600                     notSymbol.append(jit.branchIfNotSymbol(propertyGPR));
601                 }
602 
603                 for (unsigned i = cases.size(); i--;) {
604                     fallThrough.link(&amp;jit);
605                     fallThrough.clear();
606                     if (cases[i]-&gt;requiresIdentifierNameMatch() &amp;&amp; cases[i]-&gt;uid()-&gt;isSymbol())
607                         cases[i]-&gt;generateWithGuard(state, fallThrough);
608                 }
609 
610                 state.failAndRepatch.append(notSymbol);
611             }
612         } else {
613             // Cascade through the list, preferring newer entries.
614             for (unsigned i = cases.size(); i--;) {
615                 fallThrough.link(&amp;jit);
616                 fallThrough.clear();
617                 cases[i]-&gt;generateWithGuard(state, fallThrough);
618             }
619         }
620 
621         state.failAndRepatch.append(fallThrough);
622 
623     } else {
624         jit.load32(
625             CCallHelpers::Address(state.baseGPR, JSCell::structureIDOffset()),
626             state.scratchGPR);
627 
628         Vector&lt;int64_t&gt; caseValues(cases.size());
629         for (unsigned i = 0; i &lt; cases.size(); ++i)
630             caseValues[i] = bitwise_cast&lt;int32_t&gt;(cases[i]-&gt;structure()-&gt;id());
631 
632         BinarySwitch binarySwitch(state.scratchGPR, caseValues, BinarySwitch::Int32);
633         while (binarySwitch.advance(jit))
634             cases[binarySwitch.caseIndex()]-&gt;generate(state);
635         state.failAndRepatch.append(binarySwitch.fallThrough());
636     }
637 
638     if (!state.failAndIgnore.empty()) {
639         state.failAndIgnore.link(&amp;jit);
640 
641         // Make sure that the inline cache optimization code knows that we are taking slow path because
642         // of something that isn&#39;t patchable. The slow path will decrement &quot;countdown&quot; and will only
643         // patch things if the countdown reaches zero. We increment the slow path count here to ensure
644         // that the slow path does not try to patch.
645 #if CPU(X86) || CPU(X86_64)
646         jit.move(CCallHelpers::TrustedImmPtr(&amp;stubInfo.countdown), state.scratchGPR);
647         jit.add8(CCallHelpers::TrustedImm32(1), CCallHelpers::Address(state.scratchGPR));
648 #else
649         jit.load8(&amp;stubInfo.countdown, state.scratchGPR);
650         jit.add32(CCallHelpers::TrustedImm32(1), state.scratchGPR);
651         jit.store8(state.scratchGPR, &amp;stubInfo.countdown);
652 #endif
653     }
654 
655     CCallHelpers::JumpList failure;
656     if (allocator.didReuseRegisters()) {
657         state.failAndRepatch.link(&amp;jit);
658         state.restoreScratch();
659     } else
660         failure = state.failAndRepatch;
661     failure.append(jit.jump());
662 
663     CodeBlock* codeBlockThatOwnsExceptionHandlers = nullptr;
664     DisposableCallSiteIndex callSiteIndexForExceptionHandling;
665     if (state.needsToRestoreRegistersIfException() &amp;&amp; hasJSGetterSetterCall) {
666         // Emit the exception handler.
667         // Note that this code is only reachable when doing genericUnwind from a pure JS getter/setter .
668         // Note also that this is not reachable from custom getter/setter. Custom getter/setters will have
669         // their own exception handling logic that doesn&#39;t go through genericUnwind.
670         MacroAssembler::Label makeshiftCatchHandler = jit.label();
671 
672         int stackPointerOffset = codeBlock-&gt;stackPointerOffset() * sizeof(EncodedJSValue);
673         AccessGenerationState::SpillState spillStateForJSGetterSetter = state.spillStateForJSGetterSetter();
674         ASSERT(!spillStateForJSGetterSetter.isEmpty());
675         stackPointerOffset -= state.preservedReusedRegisterState.numberOfBytesPreserved;
676         stackPointerOffset -= spillStateForJSGetterSetter.numberOfStackBytesUsedForRegisterPreservation;
677 
678         jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);
679         jit.addPtr(CCallHelpers::TrustedImm32(stackPointerOffset), GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
680 
681         state.restoreLiveRegistersFromStackForCallWithThrownException(spillStateForJSGetterSetter);
682         state.restoreScratch();
683         CCallHelpers::Jump jumpToOSRExitExceptionHandler = jit.jump();
684 
685         HandlerInfo oldHandler = state.originalExceptionHandler();
686         DisposableCallSiteIndex newExceptionHandlingCallSite = state.callSiteIndexForExceptionHandling();
687         jit.addLinkTask(
688             [=] (LinkBuffer&amp; linkBuffer) {
689                 linkBuffer.link(jumpToOSRExitExceptionHandler, oldHandler.nativeCode);
690 
691                 HandlerInfo handlerToRegister = oldHandler;
692                 handlerToRegister.nativeCode = linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(makeshiftCatchHandler);
693                 handlerToRegister.start = newExceptionHandlingCallSite.bits();
694                 handlerToRegister.end = newExceptionHandlingCallSite.bits() + 1;
695                 codeBlock-&gt;appendExceptionHandler(handlerToRegister);
696             });
697 
698         // We set these to indicate to the stub to remove itself from the CodeBlock&#39;s
699         // exception handler table when it is deallocated.
700         codeBlockThatOwnsExceptionHandlers = codeBlock;
701         ASSERT(JITCode::isOptimizingJIT(codeBlockThatOwnsExceptionHandlers-&gt;jitType()));
702         callSiteIndexForExceptionHandling = state.callSiteIndexForExceptionHandling();
703     }
704 
705     LinkBuffer linkBuffer(jit, codeBlock, JITCompilationCanFail);
706     if (linkBuffer.didFailToAllocate()) {
707         if (PolymorphicAccessInternal::verbose)
708             dataLog(&quot;Did fail to allocate.\n&quot;);
709         return AccessGenerationResult::GaveUp;
710     }
711 
712     CodeLocationLabel&lt;JSInternalPtrTag&gt; successLabel = stubInfo.doneLocation;
713 
714     linkBuffer.link(state.success, successLabel);
715 
716     linkBuffer.link(failure, stubInfo.slowPathStartLocation);
717 
718     if (PolymorphicAccessInternal::verbose)
719         dataLog(FullCodeOrigin(codeBlock, stubInfo.codeOrigin), &quot;: Generating polymorphic access stub for &quot;, listDump(cases), &quot;\n&quot;);
720 
721     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; code = FINALIZE_CODE_FOR(
722         codeBlock, linkBuffer, JITStubRoutinePtrTag,
723         &quot;%s&quot;, toCString(&quot;Access stub for &quot;, *codeBlock, &quot; &quot;, stubInfo.codeOrigin, &quot; with return point &quot;, successLabel, &quot;: &quot;, listDump(cases)).data());
724 
725     bool doesCalls = false;
726     Vector&lt;JSCell*&gt; cellsToMark;
727     for (auto&amp; entry : cases)
728         doesCalls |= entry-&gt;doesCalls(vm, &amp;cellsToMark);
729 
730     m_stubRoutine = createJITStubRoutine(code, vm, codeBlock, doesCalls, cellsToMark, WTFMove(state.m_callLinkInfos), codeBlockThatOwnsExceptionHandlers, callSiteIndexForExceptionHandling);
731     m_watchpoints = WTFMove(state.watchpoints);
732     if (!state.weakReferences.isEmpty()) {
733         state.weakReferences.shrinkToFit();
734         m_weakReferences = makeUnique&lt;Vector&lt;WriteBarrier&lt;JSCell&gt;&gt;&gt;(WTFMove(state.weakReferences));
735     }
736     if (PolymorphicAccessInternal::verbose)
737         dataLog(&quot;Returning: &quot;, code.code(), &quot;\n&quot;);
738 
739     m_list = WTFMove(cases);
740     m_list.shrinkToFit();
741 
742     AccessGenerationResult::Kind resultKind;
743     if (m_list.size() &gt;= Options::maxAccessVariantListSize() || generatedFinalCode)
744         resultKind = AccessGenerationResult::GeneratedFinalCode;
745     else
746         resultKind = AccessGenerationResult::GeneratedNewCode;
747 
748     return AccessGenerationResult(resultKind, code.code());
749 }
750 
751 void PolymorphicAccess::aboutToDie()
752 {
753     if (m_stubRoutine)
754         m_stubRoutine-&gt;aboutToDie();
755 }
756 
757 } // namespace JSC
758 
759 namespace WTF {
760 
761 using namespace JSC;
762 
763 void printInternal(PrintStream&amp; out, AccessGenerationResult::Kind kind)
764 {
765     switch (kind) {
766     case AccessGenerationResult::MadeNoChanges:
767         out.print(&quot;MadeNoChanges&quot;);
768         return;
769     case AccessGenerationResult::GaveUp:
770         out.print(&quot;GaveUp&quot;);
771         return;
772     case AccessGenerationResult::Buffered:
773         out.print(&quot;Buffered&quot;);
774         return;
775     case AccessGenerationResult::GeneratedNewCode:
776         out.print(&quot;GeneratedNewCode&quot;);
777         return;
778     case AccessGenerationResult::GeneratedFinalCode:
779         out.print(&quot;GeneratedFinalCode&quot;);
780         return;
781     case AccessGenerationResult::ResetStubAndFireWatchpoints:
782         out.print(&quot;ResetStubAndFireWatchpoints&quot;);
783         return;
784     }
785 
786     RELEASE_ASSERT_NOT_REACHED();
787 }
788 
789 void printInternal(PrintStream&amp; out, AccessCase::AccessType type)
790 {
791     switch (type) {
792     case AccessCase::Load:
793         out.print(&quot;Load&quot;);
794         return;
795     case AccessCase::Transition:
796         out.print(&quot;Transition&quot;);
797         return;
798     case AccessCase::Replace:
799         out.print(&quot;Replace&quot;);
800         return;
801     case AccessCase::Miss:
802         out.print(&quot;Miss&quot;);
803         return;
804     case AccessCase::GetGetter:
805         out.print(&quot;GetGetter&quot;);
806         return;
807     case AccessCase::Getter:
808         out.print(&quot;Getter&quot;);
809         return;
810     case AccessCase::Setter:
811         out.print(&quot;Setter&quot;);
812         return;
813     case AccessCase::CustomValueGetter:
814         out.print(&quot;CustomValueGetter&quot;);
815         return;
816     case AccessCase::CustomAccessorGetter:
817         out.print(&quot;CustomAccessorGetter&quot;);
818         return;
819     case AccessCase::CustomValueSetter:
820         out.print(&quot;CustomValueSetter&quot;);
821         return;
822     case AccessCase::CustomAccessorSetter:
823         out.print(&quot;CustomAccessorSetter&quot;);
824         return;
825     case AccessCase::IntrinsicGetter:
826         out.print(&quot;IntrinsicGetter&quot;);
827         return;
828     case AccessCase::InHit:
829         out.print(&quot;InHit&quot;);
830         return;
831     case AccessCase::InMiss:
832         out.print(&quot;InMiss&quot;);
833         return;
834     case AccessCase::ArrayLength:
835         out.print(&quot;ArrayLength&quot;);
836         return;
837     case AccessCase::StringLength:
838         out.print(&quot;StringLength&quot;);
839         return;
840     case AccessCase::DirectArgumentsLength:
841         out.print(&quot;DirectArgumentsLength&quot;);
842         return;
843     case AccessCase::ScopedArgumentsLength:
844         out.print(&quot;ScopedArgumentsLength&quot;);
845         return;
846     case AccessCase::ModuleNamespaceLoad:
847         out.print(&quot;ModuleNamespaceLoad&quot;);
848         return;
849     case AccessCase::InstanceOfHit:
850         out.print(&quot;InstanceOfHit&quot;);
851         return;
852     case AccessCase::InstanceOfMiss:
853         out.print(&quot;InstanceOfMiss&quot;);
854         return;
855     case AccessCase::InstanceOfGeneric:
856         out.print(&quot;InstanceOfGeneric&quot;);
857         return;
858     case AccessCase::IndexedInt32Load:
859         out.print(&quot;IndexedInt32Load&quot;);
860         return;
861     case AccessCase::IndexedDoubleLoad:
862         out.print(&quot;IndexedDoubleLoad&quot;);
863         return;
864     case AccessCase::IndexedContiguousLoad:
865         out.print(&quot;IndexedContiguousLoad&quot;);
866         return;
867     case AccessCase::IndexedArrayStorageLoad:
868         out.print(&quot;IndexedArrayStorageLoad&quot;);
869         return;
870     case AccessCase::IndexedScopedArgumentsLoad:
871         out.print(&quot;IndexedScopedArgumentsLoad&quot;);
872         return;
873     case AccessCase::IndexedDirectArgumentsLoad:
874         out.print(&quot;IndexedDirectArgumentsLoad&quot;);
875         return;
876     case AccessCase::IndexedTypedArrayInt8Load:
877         out.print(&quot;IndexedTypedArrayInt8Load&quot;);
878         return;
879     case AccessCase::IndexedTypedArrayUint8Load:
880         out.print(&quot;IndexedTypedArrayUint8Load&quot;);
881         return;
882     case AccessCase::IndexedTypedArrayUint8ClampedLoad:
883         out.print(&quot;IndexedTypedArrayUint8ClampedLoad&quot;);
884         return;
885     case AccessCase::IndexedTypedArrayInt16Load:
886         out.print(&quot;IndexedTypedArrayInt16Load&quot;);
887         return;
888     case AccessCase::IndexedTypedArrayUint16Load:
889         out.print(&quot;IndexedTypedArrayUint16Load&quot;);
890         return;
891     case AccessCase::IndexedTypedArrayInt32Load:
892         out.print(&quot;IndexedTypedArrayInt32Load&quot;);
893         return;
894     case AccessCase::IndexedTypedArrayUint32Load:
895         out.print(&quot;IndexedTypedArrayUint32Load&quot;);
896         return;
897     case AccessCase::IndexedTypedArrayFloat32Load:
898         out.print(&quot;IndexedTypedArrayFloat32Load&quot;);
899         return;
900     case AccessCase::IndexedTypedArrayFloat64Load:
901         out.print(&quot;IndexedTypedArrayFloat64Load&quot;);
902         return;
903     case AccessCase::IndexedStringLoad:
904         out.print(&quot;IndexedStringLoad&quot;);
905         return;
906     }
907 
908     RELEASE_ASSERT_NOT_REACHED();
909 }
910 
911 void printInternal(PrintStream&amp; out, AccessCase::State state)
912 {
913     switch (state) {
914     case AccessCase::Primordial:
915         out.print(&quot;Primordial&quot;);
916         return;
917     case AccessCase::Committed:
918         out.print(&quot;Committed&quot;);
919         return;
920     case AccessCase::Generated:
921         out.print(&quot;Generated&quot;);
922         return;
923     }
924 
925     RELEASE_ASSERT_NOT_REACHED();
926 }
927 
928 } // namespace WTF
929 
930 #endif // ENABLE(JIT)
931 
932 
    </pre>
  </body>
</html>