<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (C) 2008-2020 Apple Inc. All rights reserved.</span>
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #include &quot;config.h&quot;
  31 #include &quot;CodeBlock.h&quot;
  32 
  33 #include &quot;ArithProfile.h&quot;
  34 #include &quot;BasicBlockLocation.h&quot;
  35 #include &quot;BytecodeDumper.h&quot;
  36 #include &quot;BytecodeGenerator.h&quot;
  37 #include &quot;BytecodeLivenessAnalysis.h&quot;
  38 #include &quot;BytecodeStructs.h&quot;
  39 #include &quot;BytecodeUseDef.h&quot;
  40 #include &quot;CallLinkStatus.h&quot;
<a name="2" id="anc2"></a><span class="line-added">  41 #include &quot;CheckpointOSRExitSideState.h&quot;</span>
  42 #include &quot;CodeBlockInlines.h&quot;
  43 #include &quot;CodeBlockSet.h&quot;
  44 #include &quot;DFGCapabilities.h&quot;
  45 #include &quot;DFGCommon.h&quot;
  46 #include &quot;DFGDriver.h&quot;
  47 #include &quot;DFGJITCode.h&quot;
  48 #include &quot;DFGWorklist.h&quot;
  49 #include &quot;Debugger.h&quot;
  50 #include &quot;EvalCodeBlock.h&quot;
  51 #include &quot;FullCodeOrigin.h&quot;
  52 #include &quot;FunctionCodeBlock.h&quot;
  53 #include &quot;FunctionExecutableDump.h&quot;
  54 #include &quot;GetPutInfo.h&quot;
  55 #include &quot;InlineCallFrame.h&quot;
  56 #include &quot;Instruction.h&quot;
  57 #include &quot;InstructionStream.h&quot;
  58 #include &quot;InterpreterInlines.h&quot;
  59 #include &quot;IsoCellSetInlines.h&quot;
  60 #include &quot;JIT.h&quot;
  61 #include &quot;JITMathIC.h&quot;
  62 #include &quot;JSBigInt.h&quot;
  63 #include &quot;JSCInlines.h&quot;
  64 #include &quot;JSCJSValue.h&quot;
  65 #include &quot;JSFunction.h&quot;
  66 #include &quot;JSLexicalEnvironment.h&quot;
  67 #include &quot;JSModuleEnvironment.h&quot;
  68 #include &quot;JSSet.h&quot;
  69 #include &quot;JSString.h&quot;
  70 #include &quot;JSTemplateObjectDescriptor.h&quot;
  71 #include &quot;LLIntData.h&quot;
  72 #include &quot;LLIntEntrypoint.h&quot;
  73 #include &quot;LLIntPrototypeLoadAdaptiveStructureWatchpoint.h&quot;
  74 #include &quot;LowLevelInterpreter.h&quot;
  75 #include &quot;MetadataTable.h&quot;
  76 #include &quot;ModuleProgramCodeBlock.h&quot;
  77 #include &quot;ObjectAllocationProfileInlines.h&quot;
  78 #include &quot;OpcodeInlines.h&quot;
  79 #include &quot;PCToCodeOriginMap.h&quot;
  80 #include &quot;PolymorphicAccess.h&quot;
  81 #include &quot;ProfilerDatabase.h&quot;
  82 #include &quot;ProgramCodeBlock.h&quot;
  83 #include &quot;ReduceWhitespace.h&quot;
  84 #include &quot;Repatch.h&quot;
  85 #include &quot;SlotVisitorInlines.h&quot;
  86 #include &quot;StackVisitor.h&quot;
  87 #include &quot;StructureStubInfo.h&quot;
  88 #include &quot;TypeLocationCache.h&quot;
  89 #include &quot;TypeProfiler.h&quot;
  90 #include &quot;VMInlines.h&quot;
  91 #include &lt;wtf/BagToHashMap.h&gt;
  92 #include &lt;wtf/CommaPrinter.h&gt;
  93 #include &lt;wtf/Forward.h&gt;
  94 #include &lt;wtf/SimpleStats.h&gt;
  95 #include &lt;wtf/StringPrintStream.h&gt;
  96 #include &lt;wtf/text/StringConcatenateNumbers.h&gt;
  97 #include &lt;wtf/text/UniquedStringImpl.h&gt;
  98 
  99 #if ENABLE(ASSEMBLER)
 100 #include &quot;RegisterAtOffsetList.h&quot;
 101 #endif
 102 
 103 #if ENABLE(DFG_JIT)
 104 #include &quot;DFGOperations.h&quot;
 105 #endif
 106 
 107 #if ENABLE(FTL_JIT)
 108 #include &quot;FTLJITCode.h&quot;
 109 #endif
 110 
 111 namespace JSC {
 112 
<a name="3" id="anc3"></a><span class="line-added"> 113 DEFINE_ALLOCATOR_WITH_HEAP_IDENTIFIER(CodeBlockRareData);</span>
<span class="line-added"> 114 </span>
 115 const ClassInfo CodeBlock::s_info = {
 116     &quot;CodeBlock&quot;, nullptr, nullptr, nullptr,
 117     CREATE_METHOD_TABLE(CodeBlock)
 118 };
 119 
 120 CString CodeBlock::inferredName() const
 121 {
 122     switch (codeType()) {
 123     case GlobalCode:
 124         return &quot;&lt;global&gt;&quot;;
 125     case EvalCode:
 126         return &quot;&lt;eval&gt;&quot;;
 127     case FunctionCode:
 128         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;ecmaName().utf8();
 129     case ModuleCode:
 130         return &quot;&lt;module&gt;&quot;;
 131     default:
 132         CRASH();
 133         return CString(&quot;&quot;, 0);
 134     }
 135 }
 136 
 137 bool CodeBlock::hasHash() const
 138 {
 139     return !!m_hash;
 140 }
 141 
 142 bool CodeBlock::isSafeToComputeHash() const
 143 {
 144     return !isCompilationThread();
 145 }
 146 
 147 CodeBlockHash CodeBlock::hash() const
 148 {
 149     if (!m_hash) {
 150         RELEASE_ASSERT(isSafeToComputeHash());
 151         m_hash = CodeBlockHash(ownerExecutable()-&gt;source(), specializationKind());
 152     }
 153     return m_hash;
 154 }
 155 
 156 CString CodeBlock::sourceCodeForTools() const
 157 {
 158     if (codeType() != FunctionCode)
 159         return ownerExecutable()-&gt;source().toUTF8();
 160 
 161     SourceProvider* provider = source().provider();
 162     FunctionExecutable* executable = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable());
 163     UnlinkedFunctionExecutable* unlinked = executable-&gt;unlinkedExecutable();
 164     unsigned unlinkedStartOffset = unlinked-&gt;startOffset();
 165     unsigned linkedStartOffset = executable-&gt;source().startOffset();
 166     int delta = linkedStartOffset - unlinkedStartOffset;
 167     unsigned rangeStart = delta + unlinked-&gt;unlinkedFunctionNameStart();
 168     unsigned rangeEnd = delta + unlinked-&gt;startOffset() + unlinked-&gt;sourceLength();
 169     return toCString(
 170         &quot;function &quot;,
 171         provider-&gt;source().substring(rangeStart, rangeEnd - rangeStart).utf8());
 172 }
 173 
 174 CString CodeBlock::sourceCodeOnOneLine() const
 175 {
 176     return reduceWhitespace(sourceCodeForTools());
 177 }
 178 
 179 CString CodeBlock::hashAsStringIfPossible() const
 180 {
 181     if (hasHash() || isSafeToComputeHash())
 182         return toCString(hash());
 183     return &quot;&lt;no-hash&gt;&quot;;
 184 }
 185 
 186 void CodeBlock::dumpAssumingJITType(PrintStream&amp; out, JITType jitType) const
 187 {
 188     out.print(inferredName(), &quot;#&quot;, hashAsStringIfPossible());
 189     out.print(&quot;:[&quot;, RawPointer(this), &quot;-&gt;&quot;);
 190     if (!!m_alternative)
 191         out.print(RawPointer(alternative()), &quot;-&gt;&quot;);
 192     out.print(RawPointer(ownerExecutable()), &quot;, &quot;, jitType, codeType());
 193 
 194     if (codeType() == FunctionCode)
 195         out.print(specializationKind());
 196     out.print(&quot;, &quot;, instructionsSize());
 197     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_shouldAlwaysBeInlined)
 198         out.print(&quot; (ShouldAlwaysBeInlined)&quot;);
 199     if (ownerExecutable()-&gt;neverInline())
 200         out.print(&quot; (NeverInline)&quot;);
 201     if (ownerExecutable()-&gt;neverOptimize())
 202         out.print(&quot; (NeverOptimize)&quot;);
 203     else if (ownerExecutable()-&gt;neverFTLOptimize())
 204         out.print(&quot; (NeverFTLOptimize)&quot;);
 205     if (ownerExecutable()-&gt;didTryToEnterInLoop())
 206         out.print(&quot; (DidTryToEnterInLoop)&quot;);
 207     if (ownerExecutable()-&gt;isStrictMode())
 208         out.print(&quot; (StrictMode)&quot;);
 209     if (m_didFailJITCompilation)
 210         out.print(&quot; (JITFail)&quot;);
 211     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_didFailFTLCompilation)
 212         out.print(&quot; (FTLFail)&quot;);
 213     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_hasBeenCompiledWithFTL)
 214         out.print(&quot; (HadFTLReplacement)&quot;);
 215     out.print(&quot;]&quot;);
 216 }
 217 
 218 void CodeBlock::dump(PrintStream&amp; out) const
 219 {
 220     dumpAssumingJITType(out, jitType());
 221 }
 222 
 223 void CodeBlock::dumpSource()
 224 {
 225     dumpSource(WTF::dataFile());
 226 }
 227 
 228 void CodeBlock::dumpSource(PrintStream&amp; out)
 229 {
 230     ScriptExecutable* executable = ownerExecutable();
 231     if (executable-&gt;isFunctionExecutable()) {
 232         FunctionExecutable* functionExecutable = reinterpret_cast&lt;FunctionExecutable*&gt;(executable);
 233         StringView source = functionExecutable-&gt;source().provider()-&gt;getRange(
 234             functionExecutable-&gt;parametersStartOffset(),
 235             functionExecutable-&gt;typeProfilingEndOffset(vm()) + 1); // Type profiling end offset is the character before the &#39;}&#39;.
 236 
 237         out.print(&quot;function &quot;, inferredName(), source);
 238         return;
 239     }
 240     out.print(executable-&gt;source().view());
 241 }
 242 
 243 void CodeBlock::dumpBytecode()
 244 {
 245     dumpBytecode(WTF::dataFile());
 246 }
 247 
 248 void CodeBlock::dumpBytecode(PrintStream&amp; out)
 249 {
 250     ICStatusMap statusMap;
 251     getICStatusMap(statusMap);
<a name="4" id="anc4"></a><span class="line-modified"> 252     CodeBlockBytecodeDumper&lt;CodeBlock&gt;::dumpBlock(this, instructions(), out, statusMap);</span>
 253 }
 254 
 255 void CodeBlock::dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; statusMap)
 256 {
 257     BytecodeDumper&lt;CodeBlock&gt;::dumpBytecode(this, out, it, statusMap);
 258 }
 259 
 260 void CodeBlock::dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; statusMap)
 261 {
 262     const auto it = instructions().at(bytecodeOffset);
 263     dumpBytecode(out, it, statusMap);
 264 }
 265 
 266 namespace {
 267 
 268 class PutToScopeFireDetail : public FireDetail {
 269 public:
 270     PutToScopeFireDetail(CodeBlock* codeBlock, const Identifier&amp; ident)
 271         : m_codeBlock(codeBlock)
 272         , m_ident(ident)
 273     {
 274     }
 275 
 276     void dump(PrintStream&amp; out) const override
 277     {
 278         out.print(&quot;Linking put_to_scope in &quot;, FunctionExecutableDump(jsCast&lt;FunctionExecutable*&gt;(m_codeBlock-&gt;ownerExecutable())), &quot; for &quot;, m_ident);
 279     }
 280 
 281 private:
 282     CodeBlock* m_codeBlock;
 283     const Identifier&amp; m_ident;
 284 };
 285 
 286 } // anonymous namespace
 287 
 288 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, CopyParsedBlockTag, CodeBlock&amp; other)
 289     : JSCell(vm, structure)
 290     , m_globalObject(other.m_globalObject)
 291     , m_shouldAlwaysBeInlined(true)
 292 #if ENABLE(JIT)
 293     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 294 #endif
 295     , m_didFailJITCompilation(false)
 296     , m_didFailFTLCompilation(false)
 297     , m_hasBeenCompiledWithFTL(false)
<a name="5" id="anc5"></a><span class="line-added"> 298     , m_hasLinkedOSRExit(false)</span>
<span class="line-added"> 299     , m_isEligibleForLLIntDowngrade(false)</span>
 300     , m_numCalleeLocals(other.m_numCalleeLocals)
 301     , m_numVars(other.m_numVars)
 302     , m_numberOfArgumentsToSkip(other.m_numberOfArgumentsToSkip)
 303     , m_hasDebuggerStatement(false)
 304     , m_steppingMode(SteppingModeDisabled)
 305     , m_numBreakpoints(0)
 306     , m_bytecodeCost(other.m_bytecodeCost)
 307     , m_scopeRegister(other.m_scopeRegister)
 308     , m_hash(other.m_hash)
 309     , m_unlinkedCode(other.vm(), this, other.m_unlinkedCode.get())
 310     , m_ownerExecutable(other.vm(), this, other.m_ownerExecutable.get())
 311     , m_vm(other.m_vm)
 312     , m_instructionsRawPointer(other.m_instructionsRawPointer)
 313     , m_constantRegisters(other.m_constantRegisters)
 314     , m_constantsSourceCodeRepresentation(other.m_constantsSourceCodeRepresentation)
 315     , m_functionDecls(other.m_functionDecls)
 316     , m_functionExprs(other.m_functionExprs)
 317     , m_osrExitCounter(0)
 318     , m_optimizationDelayCounter(0)
 319     , m_reoptimizationRetryCounter(0)
 320     , m_metadata(other.m_metadata)
 321     , m_creationTime(MonotonicTime::now())
 322 {
 323     ASSERT(heap()-&gt;isDeferred());
 324     ASSERT(m_scopeRegister.isLocal());
 325 
 326     ASSERT(source().provider());
 327     setNumParameters(other.numParameters());
 328 
 329     vm.heap.codeBlockSet().add(this);
 330 }
 331 
 332 void CodeBlock::finishCreation(VM&amp; vm, CopyParsedBlockTag, CodeBlock&amp; other)
 333 {
 334     Base::finishCreation(vm);
 335     finishCreationCommon(vm);
 336 
 337     optimizeAfterWarmUp();
 338     jitAfterWarmUp();
 339 
 340     if (other.m_rareData) {
 341         createRareDataIfNecessary();
 342 
 343         m_rareData-&gt;m_exceptionHandlers = other.m_rareData-&gt;m_exceptionHandlers;
 344         m_rareData-&gt;m_switchJumpTables = other.m_rareData-&gt;m_switchJumpTables;
 345         m_rareData-&gt;m_stringSwitchJumpTables = other.m_rareData-&gt;m_stringSwitchJumpTables;
 346     }
 347 }
 348 
 349 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock, JSScope* scope)
 350     : JSCell(vm, structure)
 351     , m_globalObject(vm, this, scope-&gt;globalObject(vm))
 352     , m_shouldAlwaysBeInlined(true)
 353 #if ENABLE(JIT)
 354     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 355 #endif
 356     , m_didFailJITCompilation(false)
 357     , m_didFailFTLCompilation(false)
 358     , m_hasBeenCompiledWithFTL(false)
<a name="6" id="anc6"></a><span class="line-added"> 359     , m_hasLinkedOSRExit(false)</span>
<span class="line-added"> 360     , m_isEligibleForLLIntDowngrade(false)</span>
 361     , m_numCalleeLocals(unlinkedCodeBlock-&gt;numCalleeLocals())
 362     , m_numVars(unlinkedCodeBlock-&gt;numVars())
 363     , m_hasDebuggerStatement(false)
 364     , m_steppingMode(SteppingModeDisabled)
 365     , m_numBreakpoints(0)
 366     , m_scopeRegister(unlinkedCodeBlock-&gt;scopeRegister())
 367     , m_unlinkedCode(vm, this, unlinkedCodeBlock)
 368     , m_ownerExecutable(vm, this, ownerExecutable)
 369     , m_vm(&amp;vm)
 370     , m_instructionsRawPointer(unlinkedCodeBlock-&gt;instructions().rawPointer())
 371     , m_osrExitCounter(0)
 372     , m_optimizationDelayCounter(0)
 373     , m_reoptimizationRetryCounter(0)
 374     , m_metadata(unlinkedCodeBlock-&gt;metadata().link())
 375     , m_creationTime(MonotonicTime::now())
 376 {
 377     ASSERT(heap()-&gt;isDeferred());
 378     ASSERT(m_scopeRegister.isLocal());
 379 
 380     ASSERT(source().provider());
 381     setNumParameters(unlinkedCodeBlock-&gt;numParameters());
 382 
 383     vm.heap.codeBlockSet().add(this);
 384 }
 385 
 386 // The main purpose of this function is to generate linked bytecode from unlinked bytecode. The process
 387 // of linking is taking an abstract representation of bytecode and tying it to a GlobalObject and scope
 388 // chain. For example, this process allows us to cache the depth of lexical environment reads that reach
 389 // outside of this CodeBlock&#39;s compilation unit. It also allows us to generate particular constants that
 390 // we can&#39;t generate during unlinked bytecode generation. This process is not allowed to generate control
 391 // flow or introduce new locals. The reason for this is we rely on liveness analysis to be the same for
 392 // all the CodeBlocks of an UnlinkedCodeBlock. We rely on this fact by caching the liveness analysis
 393 // inside UnlinkedCodeBlock.
 394 bool CodeBlock::finishCreation(VM&amp; vm, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock,
 395     JSScope* scope)
 396 {
 397     Base::finishCreation(vm);
 398     finishCreationCommon(vm);
 399 
 400     auto throwScope = DECLARE_THROW_SCOPE(vm);
 401 
 402     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())
 403         vm.functionHasExecutedCache()-&gt;removeUnexecutedRange(ownerExecutable-&gt;sourceID(), ownerExecutable-&gt;typeProfilingStartOffset(vm), ownerExecutable-&gt;typeProfilingEndOffset(vm));
 404 
 405     ScriptExecutable* topLevelExecutable = ownerExecutable-&gt;topLevelExecutable();
 406     setConstantRegisters(unlinkedCodeBlock-&gt;constantRegisters(), unlinkedCodeBlock-&gt;constantsSourceCodeRepresentation(), topLevelExecutable);
 407     RETURN_IF_EXCEPTION(throwScope, false);
 408 
<a name="7" id="anc7"></a>





 409     // We already have the cloned symbol table for the module environment since we need to instantiate
 410     // the module environments before linking the code block. We replace the stored symbol table with the already cloned one.
 411     if (UnlinkedModuleProgramCodeBlock* unlinkedModuleProgramCodeBlock = jsDynamicCast&lt;UnlinkedModuleProgramCodeBlock*&gt;(vm, unlinkedCodeBlock)) {
 412         SymbolTable* clonedSymbolTable = jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable)-&gt;moduleEnvironmentSymbolTable();
 413         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {
 414             ConcurrentJSLocker locker(clonedSymbolTable-&gt;m_lock);
 415             clonedSymbolTable-&gt;prepareForTypeProfiling(locker);
 416         }
<a name="8" id="anc8"></a><span class="line-modified"> 417         replaceConstant(VirtualRegister(unlinkedModuleProgramCodeBlock-&gt;moduleEnvironmentSymbolTableConstantRegisterOffset()), clonedSymbolTable);</span>
 418     }
 419 
 420     bool shouldUpdateFunctionHasExecutedCache = m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes();
 421     m_functionDecls = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionDecls());
 422     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionDecls(), i = 0; i &lt; count; ++i) {
 423         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionDecl(i);
 424         if (shouldUpdateFunctionHasExecutedCache)
 425             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
 426         m_functionDecls[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));
 427     }
 428 
 429     m_functionExprs = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionExprs());
 430     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionExprs(), i = 0; i &lt; count; ++i) {
 431         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionExpr(i);
 432         if (shouldUpdateFunctionHasExecutedCache)
 433             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
 434         m_functionExprs[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));
 435     }
 436 
 437     if (unlinkedCodeBlock-&gt;hasRareData()) {
 438         createRareDataIfNecessary();
 439 
 440         setConstantIdentifierSetRegisters(vm, unlinkedCodeBlock-&gt;constantIdentifierSets());
 441         RETURN_IF_EXCEPTION(throwScope, false);
 442 
 443         if (size_t count = unlinkedCodeBlock-&gt;numberOfExceptionHandlers()) {
 444             m_rareData-&gt;m_exceptionHandlers.resizeToFit(count);
 445             for (size_t i = 0; i &lt; count; i++) {
 446                 const UnlinkedHandlerInfo&amp; unlinkedHandler = unlinkedCodeBlock-&gt;exceptionHandler(i);
 447                 HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];
 448 #if ENABLE(JIT)
<a name="9" id="anc9"></a><span class="line-modified"> 449                 auto&amp; instruction = *instructions().at(unlinkedHandler.target).ptr();</span>
<span class="line-modified"> 450                 MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(instruction);</span>






 451                 handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));
 452 #else
 453                 handler.initialize(unlinkedHandler);
 454 #endif
 455             }
 456         }
 457 
 458         if (size_t count = unlinkedCodeBlock-&gt;numberOfStringSwitchJumpTables()) {
 459             m_rareData-&gt;m_stringSwitchJumpTables.grow(count);
 460             for (size_t i = 0; i &lt; count; i++) {
 461                 UnlinkedStringJumpTable::StringOffsetTable::iterator ptr = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.begin();
 462                 UnlinkedStringJumpTable::StringOffsetTable::iterator end = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.end();
 463                 for (; ptr != end; ++ptr) {
 464                     OffsetLocation offset;
 465                     offset.branchOffset = ptr-&gt;value.branchOffset;
 466                     m_rareData-&gt;m_stringSwitchJumpTables[i].offsetTable.add(ptr-&gt;key, offset);
 467                 }
 468             }
 469         }
 470 
 471         if (size_t count = unlinkedCodeBlock-&gt;numberOfSwitchJumpTables()) {
 472             m_rareData-&gt;m_switchJumpTables.grow(count);
 473             for (size_t i = 0; i &lt; count; i++) {
 474                 UnlinkedSimpleJumpTable&amp; sourceTable = unlinkedCodeBlock-&gt;switchJumpTable(i);
 475                 SimpleJumpTable&amp; destTable = m_rareData-&gt;m_switchJumpTables[i];
<a name="10" id="anc10"></a><span class="line-modified"> 476                 destTable.branchOffsets.resizeToFit(sourceTable.branchOffsets.size());</span>
<span class="line-added"> 477                 std::copy(sourceTable.branchOffsets.begin(), sourceTable.branchOffsets.end(), destTable.branchOffsets.begin());</span>
 478                 destTable.min = sourceTable.min;
 479             }
 480         }
 481     }
 482 
 483     // Bookkeep the strongly referenced module environments.
 484     HashSet&lt;JSModuleEnvironment*&gt; stronglyReferencedModuleEnvironments;
 485 
 486     auto link_profile = [&amp;](const auto&amp; /*instruction*/, auto /*bytecode*/, auto&amp; /*metadata*/) {
 487         m_numberOfNonArgumentValueProfiles++;
 488     };
 489 
 490     auto link_objectAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 491         metadata.m_objectAllocationProfile.initializeProfile(vm, m_globalObject.get(), this, m_globalObject-&gt;objectPrototype(), bytecode.m_inlineCapacity);
 492     };
 493 
 494     auto link_arrayAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 495         metadata.m_arrayAllocationProfile.initializeIndexingMode(bytecode.m_recommendedIndexingType);
 496     };
 497 
 498 #define LINK_FIELD(__field) \
 499     WTF_LAZY_JOIN(link_, __field)(instruction, bytecode, metadata);
 500 
 501 #define INITIALIZE_METADATA(__op) \
 502     auto bytecode = instruction-&gt;as&lt;__op&gt;(); \
 503     auto&amp; metadata = bytecode.metadata(this); \
 504     new (&amp;metadata) __op::Metadata { bytecode }; \
 505 
 506 #define CASE(__op) case __op::opcodeID
 507 
 508 #define LINK(...) \
 509     CASE(WTF_LAZY_FIRST(__VA_ARGS__)): { \
 510         INITIALIZE_METADATA(WTF_LAZY_FIRST(__VA_ARGS__)) \
 511         WTF_LAZY_HAS_REST(__VA_ARGS__)({ \
 512             WTF_LAZY_FOR_EACH_TERM(LINK_FIELD,  WTF_LAZY_REST_(__VA_ARGS__)) \
 513         }) \
 514         break; \
 515     }
 516 
 517     const InstructionStream&amp; instructionStream = instructions();
 518     for (const auto&amp; instruction : instructionStream) {
 519         OpcodeID opcodeID = instruction-&gt;opcodeID();
 520         m_bytecodeCost += opcodeLengths[opcodeID];
 521         switch (opcodeID) {
 522         LINK(OpHasIndexedProperty)
 523 
 524         LINK(OpCallVarargs, profile)
 525         LINK(OpTailCallVarargs, profile)
 526         LINK(OpTailCallForwardArguments, profile)
 527         LINK(OpConstructVarargs, profile)
 528         LINK(OpGetByVal, profile)
 529 
 530         LINK(OpGetDirectPname, profile)
 531         LINK(OpGetByIdWithThis, profile)
 532         LINK(OpTryGetById, profile)
 533         LINK(OpGetByIdDirect, profile)
 534         LINK(OpGetByValWithThis, profile)
 535         LINK(OpGetFromArguments, profile)
 536         LINK(OpToNumber, profile)
<a name="11" id="anc11"></a><span class="line-added"> 537         LINK(OpToNumeric, profile)</span>
 538         LINK(OpToObject, profile)
 539         LINK(OpGetArgument, profile)
<a name="12" id="anc12"></a><span class="line-added"> 540         LINK(OpGetInternalField, profile)</span>
 541         LINK(OpToThis, profile)
 542         LINK(OpBitand, profile)
 543         LINK(OpBitor, profile)
 544         LINK(OpBitnot, profile)
 545         LINK(OpBitxor, profile)
 546         LINK(OpLshift, profile)
<a name="13" id="anc13"></a><span class="line-added"> 547         LINK(OpRshift, profile)</span>
 548 
 549         LINK(OpGetById, profile)
 550 
 551         LINK(OpCall, profile)
 552         LINK(OpTailCall, profile)
 553         LINK(OpCallEval, profile)
 554         LINK(OpConstruct, profile)
 555 
 556         LINK(OpInByVal)
 557         LINK(OpPutByVal)
 558         LINK(OpPutByValDirect)
 559 
 560         LINK(OpNewArray)
 561         LINK(OpNewArrayWithSize)
 562         LINK(OpNewArrayBuffer, arrayAllocationProfile)
 563 
 564         LINK(OpNewObject, objectAllocationProfile)
 565 
 566         LINK(OpPutById)
 567         LINK(OpCreateThis)
<a name="14" id="anc14"></a><span class="line-added"> 568         LINK(OpCreatePromise)</span>
<span class="line-added"> 569         LINK(OpCreateGenerator)</span>
 570 
 571         LINK(OpAdd)
 572         LINK(OpMul)
 573         LINK(OpDiv)
 574         LINK(OpSub)
 575 
 576         LINK(OpNegate)
<a name="15" id="anc15"></a><span class="line-added"> 577         LINK(OpInc)</span>
<span class="line-added"> 578         LINK(OpDec)</span>
 579 
 580         LINK(OpJneqPtr)
 581 
 582         LINK(OpCatch)
 583         LINK(OpProfileControlFlow)
 584 
 585         case op_resolve_scope: {
 586             INITIALIZE_METADATA(OpResolveScope)
 587 
 588             const Identifier&amp; ident = identifier(bytecode.m_var);
 589             RELEASE_ASSERT(bytecode.m_resolveType != LocalClosureVar);
 590 
<a name="16" id="anc16"></a><span class="line-modified"> 591             ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);</span>
 592             RETURN_IF_EXCEPTION(throwScope, false);
 593 
 594             metadata.m_resolveType = op.type;
 595             metadata.m_localScopeDepth = op.depth;
 596             if (op.lexicalEnvironment) {
 597                 if (op.type == ModuleVar) {
 598                     // Keep the linked module environment strongly referenced.
 599                     if (stronglyReferencedModuleEnvironments.add(jsCast&lt;JSModuleEnvironment*&gt;(op.lexicalEnvironment)).isNewEntry)
<a name="17" id="anc17"></a><span class="line-modified"> 600                         addConstant(ConcurrentJSLocker(m_lock), op.lexicalEnvironment);</span>
 601                     metadata.m_lexicalEnvironment.set(vm, this, op.lexicalEnvironment);
 602                 } else
 603                     metadata.m_symbolTable.set(vm, this, op.lexicalEnvironment-&gt;symbolTable());
 604             } else if (JSScope* constantScope = JSScope::constantScopeForCodeBlock(op.type, this)) {
 605                 metadata.m_constantScope.set(vm, this, constantScope);
 606                 if (op.type == GlobalProperty || op.type == GlobalPropertyWithVarInjectionChecks)
 607                     metadata.m_globalLexicalBindingEpoch = m_globalObject-&gt;globalLexicalBindingEpoch();
 608             } else
 609                 metadata.m_globalObject.clear();
 610             break;
 611         }
 612 
 613         case op_get_from_scope: {
 614             INITIALIZE_METADATA(OpGetFromScope)
 615 
 616             link_profile(instruction, bytecode, metadata);
 617             metadata.m_watchpointSet = nullptr;
 618 
 619             ASSERT(!isInitialization(bytecode.m_getPutInfo.initializationMode()));
 620             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 621                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 622                 break;
 623             }
 624 
 625             const Identifier&amp; ident = identifier(bytecode.m_var);
<a name="18" id="anc18"></a><span class="line-modified"> 626             ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_getPutInfo.resolveType(), InitializationMode::NotInitialization);</span>
 627             RETURN_IF_EXCEPTION(throwScope, false);
 628 
 629             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 630             if (op.type == ModuleVar)
 631                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 632             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 633                 metadata.m_watchpointSet = op.watchpointSet;
 634             else if (op.structure)
 635                 metadata.m_structure.set(vm, this, op.structure);
 636             metadata.m_operand = op.operand;
 637             break;
 638         }
 639 
 640         case op_put_to_scope: {
 641             INITIALIZE_METADATA(OpPutToScope)
 642 
 643             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 644                 // Only do watching if the property we&#39;re putting to is not anonymous.
 645                 if (bytecode.m_var != UINT_MAX) {
<a name="19" id="anc19"></a><span class="line-modified"> 646                     SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable()));</span>
 647                     const Identifier&amp; ident = identifier(bytecode.m_var);
 648                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 649                     auto iter = symbolTable-&gt;find(locker, ident.impl());
 650                     ASSERT(iter != symbolTable-&gt;end(locker));
 651                     iter-&gt;value.prepareToWatch();
 652                     metadata.m_watchpointSet = iter-&gt;value.watchpointSet();
 653                 } else
 654                     metadata.m_watchpointSet = nullptr;
 655                 break;
 656             }
 657 
 658             const Identifier&amp; ident = identifier(bytecode.m_var);
 659             metadata.m_watchpointSet = nullptr;
<a name="20" id="anc20"></a><span class="line-modified"> 660             ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), bytecode.m_symbolTableOrScopeDepth.scopeDepth(), scope, ident, Put, bytecode.m_getPutInfo.resolveType(), bytecode.m_getPutInfo.initializationMode());</span>
 661             RETURN_IF_EXCEPTION(throwScope, false);
 662 
 663             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 664             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 665                 metadata.m_watchpointSet = op.watchpointSet;
 666             else if (op.type == ClosureVar || op.type == ClosureVarWithVarInjectionChecks) {
 667                 if (op.watchpointSet)
 668                     op.watchpointSet-&gt;invalidate(vm, PutToScopeFireDetail(this, ident));
 669             } else if (op.structure)
 670                 metadata.m_structure.set(vm, this, op.structure);
 671             metadata.m_operand = op.operand;
 672             break;
 673         }
 674 
 675         case op_profile_type: {
 676             RELEASE_ASSERT(m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes());
 677 
 678             INITIALIZE_METADATA(OpProfileType)
 679 
 680             size_t instructionOffset = instruction.offset() + instruction-&gt;size() - 1;
 681             unsigned divotStart, divotEnd;
 682             GlobalVariableID globalVariableID = 0;
 683             RefPtr&lt;TypeSet&gt; globalTypeSet;
 684             bool shouldAnalyze = m_unlinkedCode-&gt;typeProfilerExpressionInfoForBytecodeOffset(instructionOffset, divotStart, divotEnd);
 685             SymbolTable* symbolTable = nullptr;
 686 
 687             switch (bytecode.m_flag) {
 688             case ProfileTypeBytecodeClosureVar: {
 689                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 690                 unsigned localScopeDepth = bytecode.m_symbolTableOrScopeDepth.scopeDepth();
 691                 // Even though type profiling may be profiling either a Get or a Put, we can always claim a Get because
 692                 // we&#39;re abstractly &quot;read&quot;ing from a JSScope.
<a name="21" id="anc21"></a><span class="line-modified"> 693                 ResolveOp op = JSScope::abstractResolve(m_globalObject.get(), localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);</span>
 694                 RETURN_IF_EXCEPTION(throwScope, false);
 695 
 696                 if (op.type == ClosureVar || op.type == ModuleVar)
 697                     symbolTable = op.lexicalEnvironment-&gt;symbolTable();
 698                 else if (op.type == GlobalVar)
 699                     symbolTable = m_globalObject.get()-&gt;symbolTable();
 700 
 701                 UniquedStringImpl* impl = (op.type == ModuleVar) ? op.importedName.get() : ident.impl();
 702                 if (symbolTable) {
 703                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 704                     // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 705                     symbolTable-&gt;prepareForTypeProfiling(locker);
 706                     globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, impl, vm);
 707                     globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, impl, vm);
 708                 } else
 709                     globalVariableID = TypeProfilerNoGlobalIDExists;
 710 
 711                 break;
 712             }
 713             case ProfileTypeBytecodeLocallyResolved: {
<a name="22" id="anc22"></a><span class="line-modified"> 714                 SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable()));</span>

 715                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 716                 ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 717                 // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 718                 globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, ident.impl(), vm);
 719                 globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, ident.impl(), vm);
 720 
 721                 break;
 722             }
 723             case ProfileTypeBytecodeDoesNotHaveGlobalID:
 724             case ProfileTypeBytecodeFunctionArgument: {
 725                 globalVariableID = TypeProfilerNoGlobalIDExists;
 726                 break;
 727             }
 728             case ProfileTypeBytecodeFunctionReturnStatement: {
 729                 RELEASE_ASSERT(ownerExecutable-&gt;isFunctionExecutable());
 730                 globalTypeSet = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable)-&gt;returnStatementTypeSet();
 731                 globalVariableID = TypeProfilerReturnStatement;
 732                 if (!shouldAnalyze) {
 733                     // Because a return statement can be added implicitly to return undefined at the end of a function,
 734                     // and these nodes don&#39;t emit expression ranges because they aren&#39;t in the actual source text of
 735                     // the user&#39;s program, give the type profiler some range to identify these return statements.
 736                     // Currently, the text offset that is used as identification is &quot;f&quot; in the function keyword
 737                     // and is stored on TypeLocation&#39;s m_divotForFunctionOffsetIfReturnStatement member variable.
 738                     divotStart = divotEnd = ownerExecutable-&gt;typeProfilingStartOffset(vm);
 739                     shouldAnalyze = true;
 740                 }
 741                 break;
 742             }
 743             }
 744 
 745             std::pair&lt;TypeLocation*, bool&gt; locationPair = vm.typeProfiler()-&gt;typeLocationCache()-&gt;getTypeLocation(globalVariableID,
 746                 ownerExecutable-&gt;sourceID(), divotStart, divotEnd, WTFMove(globalTypeSet), &amp;vm);
 747             TypeLocation* location = locationPair.first;
 748             bool isNewLocation = locationPair.second;
 749 
 750             if (bytecode.m_flag == ProfileTypeBytecodeFunctionReturnStatement)
 751                 location-&gt;m_divotForFunctionOffsetIfReturnStatement = ownerExecutable-&gt;typeProfilingStartOffset(vm);
 752 
 753             if (shouldAnalyze &amp;&amp; isNewLocation)
 754                 vm.typeProfiler()-&gt;insertNewLocation(location);
 755 
 756             metadata.m_typeLocation = location;
 757             break;
 758         }
 759 
 760         case op_debug: {
<a name="23" id="anc23"></a><span class="line-modified"> 761             if (instruction-&gt;as&lt;OpDebug&gt;().m_debugHookType == DidReachDebuggerStatement)</span>
 762                 m_hasDebuggerStatement = true;
 763             break;
 764         }
 765 
 766         case op_create_rest: {
 767             int numberOfArgumentsToSkip = instruction-&gt;as&lt;OpCreateRest&gt;().m_numParametersToSkip;
 768             ASSERT_UNUSED(numberOfArgumentsToSkip, numberOfArgumentsToSkip &gt;= 0);
 769             // This is used when rematerializing the rest parameter during OSR exit in the FTL JIT.&quot;);
 770             m_numberOfArgumentsToSkip = numberOfArgumentsToSkip;
 771             break;
 772         }
 773 
 774         default:
 775             break;
 776         }
 777     }
 778 
 779 #undef CASE
 780 #undef INITIALIZE_METADATA
 781 #undef LINK_FIELD
 782 #undef LINK
 783 
 784     if (m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())
 785         insertBasicBlockBoundariesForControlFlowProfiler();
 786 
 787     // Set optimization thresholds only after instructions is initialized, since these
 788     // rely on the instruction count (and are in theory permitted to also inspect the
 789     // instruction stream to more accurate assess the cost of tier-up).
 790     optimizeAfterWarmUp();
 791     jitAfterWarmUp();
 792 
 793     // If the concurrent thread will want the code block&#39;s hash, then compute it here
 794     // synchronously.
 795     if (Options::alwaysComputeHash())
 796         hash();
 797 
 798     if (Options::dumpGeneratedBytecodes())
 799         dumpBytecode();
 800 
 801     if (m_metadata)
 802         vm.heap.reportExtraMemoryAllocated(m_metadata-&gt;sizeInBytes());
 803 
 804     return true;
 805 }
 806 
 807 void CodeBlock::finishCreationCommon(VM&amp; vm)
 808 {
 809     m_ownerEdge.set(vm, this, ExecutableToCodeBlockEdge::create(vm, this));
 810 }
 811 
 812 CodeBlock::~CodeBlock()
 813 {
 814     VM&amp; vm = *m_vm;
 815 
 816 #if ENABLE(DFG_JIT)
 817     // The JITCode (and its corresponding DFG::CommonData) may outlive the CodeBlock by
 818     // a short amount of time after the CodeBlock is destructed. For example, the
 819     // Interpreter::execute methods will ref JITCode before invoking it. This can
 820     // result in the JITCode having a non-zero refCount when its owner CodeBlock is
 821     // destructed.
 822     //
 823     // Hence, we cannot rely on DFG::CommonData destruction to clear these now invalid
 824     // watchpoints in a timely manner. We&#39;ll ensure they are cleared here eagerly.
 825     //
 826     // We only need to do this for a DFG/FTL CodeBlock because only these will have a
 827     // DFG:CommonData. Hence, the LLInt and Baseline will not have any of these watchpoints.
 828     //
 829     // Note also that the LLIntPrototypeLoadAdaptiveStructureWatchpoint is also related
 830     // to the CodeBlock. However, its lifecycle is tied directly to the CodeBlock, and
 831     // will be automatically cleared when the CodeBlock destructs.
 832 
 833     if (JITCode::isOptimizingJIT(jitType()))
 834         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();
 835 #endif
 836     vm.heap.codeBlockSet().remove(this);
 837 
 838     if (UNLIKELY(vm.m_perBytecodeProfiler))
 839         vm.m_perBytecodeProfiler-&gt;notifyDestruction(this);
 840 
 841     if (!vm.heap.isShuttingDown() &amp;&amp; unlinkedCodeBlock()-&gt;didOptimize() == MixedTriState)
 842         unlinkedCodeBlock()-&gt;setDidOptimize(FalseTriState);
 843 
 844 #if ENABLE(VERBOSE_VALUE_PROFILE)
 845     dumpValueProfiles();
 846 #endif
 847 
 848     // We may be destroyed before any CodeBlocks that refer to us are destroyed.
 849     // Consider that two CodeBlocks become unreachable at the same time. There
 850     // is no guarantee about the order in which the CodeBlocks are destroyed.
 851     // So, if we don&#39;t remove incoming calls, and get destroyed before the
 852     // CodeBlock(s) that have calls into us, then the CallLinkInfo vector&#39;s
 853     // destructor will try to remove nodes from our (no longer valid) linked list.
 854     unlinkIncomingCalls();
 855 
 856     // Note that our outgoing calls will be removed from other CodeBlocks&#39;
 857     // m_incomingCalls linked lists through the execution of the ~CallLinkInfo
 858     // destructors.
 859 
 860 #if ENABLE(JIT)
 861     if (auto* jitData = m_jitData.get()) {
 862         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
 863             stubInfo-&gt;aboutToDie();
 864             stubInfo-&gt;deref();
 865         }
 866     }
 867 #endif // ENABLE(JIT)
 868 }
 869 
<a name="24" id="anc24"></a><span class="line-modified"> 870 void CodeBlock::setConstantIdentifierSetRegisters(VM&amp; vm, const RefCountedArray&lt;ConstantIdentifierSetEntry&gt;&amp; constants)</span>
 871 {
 872     auto scope = DECLARE_THROW_SCOPE(vm);
 873     JSGlobalObject* globalObject = m_globalObject.get();
<a name="25" id="anc25"></a>
 874 
 875     for (const auto&amp; entry : constants) {
 876         const IdentifierSet&amp; set = entry.first;
 877 
 878         Structure* setStructure = globalObject-&gt;setStructure();
 879         RETURN_IF_EXCEPTION(scope, void());
<a name="26" id="anc26"></a><span class="line-modified"> 880         JSSet* jsSet = JSSet::create(globalObject, vm, setStructure, set.size());</span>
 881         RETURN_IF_EXCEPTION(scope, void());
 882 
<a name="27" id="anc27"></a><span class="line-modified"> 883         for (const auto&amp; setEntry : set) {</span>
 884             JSString* jsString = jsOwnedString(vm, setEntry.get());
<a name="28" id="anc28"></a><span class="line-modified"> 885             jsSet-&gt;add(globalObject, jsString);</span>
 886             RETURN_IF_EXCEPTION(scope, void());
 887         }
 888         m_constantRegisters[entry.second].set(vm, this, jsSet);
 889     }
 890 }
 891 
<a name="29" id="anc29"></a><span class="line-modified"> 892 void CodeBlock::setConstantRegisters(const RefCountedArray&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const RefCountedArray&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable)</span>
 893 {
 894     VM&amp; vm = *m_vm;
 895     auto scope = DECLARE_THROW_SCOPE(vm);
 896     JSGlobalObject* globalObject = m_globalObject.get();
<a name="30" id="anc30"></a>
 897 
 898     ASSERT(constants.size() == constantsSourceCodeRepresentation.size());
 899     size_t count = constants.size();
<a name="31" id="anc31"></a><span class="line-modified"> 900     {</span>
<span class="line-added"> 901         ConcurrentJSLocker locker(m_lock);</span>
<span class="line-added"> 902         m_constantRegisters.resizeToFit(count);</span>
<span class="line-added"> 903         m_constantsSourceCodeRepresentation.resizeToFit(count);</span>
<span class="line-added"> 904     }</span>
 905     for (size_t i = 0; i &lt; count; i++) {
 906         JSValue constant = constants[i].get();
<a name="32" id="anc32"></a><span class="line-modified"> 907         SourceCodeRepresentation representation = constantsSourceCodeRepresentation[i];</span>
<span class="line-modified"> 908         m_constantsSourceCodeRepresentation[i] = representation;</span>
<span class="line-modified"> 909         switch (representation) {</span>
<span class="line-modified"> 910         case SourceCodeRepresentation::LinkTimeConstant:</span>
<span class="line-modified"> 911             constant = globalObject-&gt;linkTimeConstant(static_cast&lt;LinkTimeConstant&gt;(constant.asInt32AsAnyInt()));</span>
<span class="line-modified"> 912             break;</span>
<span class="line-modified"> 913         case SourceCodeRepresentation::Other:</span>
<span class="line-modified"> 914         case SourceCodeRepresentation::Integer:</span>
<span class="line-added"> 915         case SourceCodeRepresentation::Double:</span>
<span class="line-added"> 916             if (!constant.isEmpty()) {</span>
<span class="line-added"> 917                 if (constant.isCell()) {</span>
<span class="line-added"> 918                     JSCell* cell = constant.asCell();</span>
<span class="line-added"> 919                     if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm, cell)) {</span>
<span class="line-added"> 920                         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
<span class="line-added"> 921                             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);</span>
<span class="line-added"> 922                             symbolTable-&gt;prepareForTypeProfiling(locker);</span>
<span class="line-added"> 923                         }</span>
<span class="line-added"> 924 </span>
<span class="line-added"> 925                         SymbolTable* clone = symbolTable-&gt;cloneScopePart(vm);</span>
<span class="line-added"> 926                         if (wasCompiledWithDebuggingOpcodes())</span>
<span class="line-added"> 927                             clone-&gt;setRareDataCodeBlock(this);</span>
<span class="line-added"> 928 </span>
<span class="line-added"> 929                         constant = clone;</span>
<span class="line-added"> 930                     } else if (auto* descriptor = jsDynamicCast&lt;JSTemplateObjectDescriptor*&gt;(vm, cell)) {</span>
<span class="line-added"> 931                         auto* templateObject = topLevelExecutable-&gt;createTemplateObject(globalObject, descriptor);</span>
<span class="line-added"> 932                         RETURN_IF_EXCEPTION(scope, void());</span>
<span class="line-added"> 933                         constant = templateObject;</span>
 934                     }
<a name="33" id="anc33"></a>









 935                 }
 936             }
<a name="34" id="anc34"></a><span class="line-added"> 937             break;</span>
 938         }
<a name="35" id="anc35"></a>
 939         m_constantRegisters[i].set(vm, this, constant);
 940     }
<a name="36" id="anc36"></a>

 941 }
 942 
 943 void CodeBlock::setAlternative(VM&amp; vm, CodeBlock* alternative)
 944 {
 945     RELEASE_ASSERT(alternative);
 946     RELEASE_ASSERT(alternative-&gt;jitCode());
 947     m_alternative.set(vm, this, alternative);
 948 }
 949 
 950 void CodeBlock::setNumParameters(int newValue)
 951 {
 952     m_numParameters = newValue;
 953 
 954     m_argumentValueProfiles = RefCountedArray&lt;ValueProfile&gt;(vm().canUseJIT() ? newValue : 0);
 955 }
 956 
 957 CodeBlock* CodeBlock::specialOSREntryBlockOrNull()
 958 {
 959 #if ENABLE(FTL_JIT)
 960     if (jitType() != JITType::DFGJIT)
 961         return 0;
 962     DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
 963     return jitCode-&gt;osrEntryBlock();
 964 #else // ENABLE(FTL_JIT)
 965     return 0;
 966 #endif // ENABLE(FTL_JIT)
 967 }
 968 
 969 size_t CodeBlock::estimatedSize(JSCell* cell, VM&amp; vm)
 970 {
 971     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 972     size_t extraMemoryAllocated = 0;
 973     if (thisObject-&gt;m_metadata)
 974         extraMemoryAllocated += thisObject-&gt;m_metadata-&gt;sizeInBytes();
 975     RefPtr&lt;JITCode&gt; jitCode = thisObject-&gt;m_jitCode;
 976     if (jitCode &amp;&amp; !jitCode-&gt;isShared())
 977         extraMemoryAllocated += jitCode-&gt;size();
 978     return Base::estimatedSize(cell, vm) + extraMemoryAllocated;
 979 }
 980 
 981 void CodeBlock::visitChildren(JSCell* cell, SlotVisitor&amp; visitor)
 982 {
 983     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 984     ASSERT_GC_OBJECT_INHERITS(thisObject, info());
 985     Base::visitChildren(cell, visitor);
 986     visitor.append(thisObject-&gt;m_ownerEdge);
 987     thisObject-&gt;visitChildren(visitor);
 988 }
 989 
 990 void CodeBlock::visitChildren(SlotVisitor&amp; visitor)
 991 {
 992     ConcurrentJSLocker locker(m_lock);
 993     if (CodeBlock* otherBlock = specialOSREntryBlockOrNull())
 994         visitor.appendUnbarriered(otherBlock);
 995 
 996     size_t extraMemory = 0;
 997     if (m_metadata)
 998         extraMemory += m_metadata-&gt;sizeInBytes();
 999     if (m_jitCode &amp;&amp; !m_jitCode-&gt;isShared())
1000         extraMemory += m_jitCode-&gt;size();
1001     visitor.reportExtraMemoryVisited(extraMemory);
1002 
1003     stronglyVisitStrongReferences(locker, visitor);
1004     stronglyVisitWeakReferences(locker, visitor);
1005 
1006     VM::SpaceAndSet::setFor(*subspace()).add(this);
1007 }
1008 
1009 bool CodeBlock::shouldVisitStrongly(const ConcurrentJSLocker&amp; locker)
1010 {
1011     if (Options::forceCodeBlockLiveness())
1012         return true;
1013 
1014     if (shouldJettisonDueToOldAge(locker))
1015         return false;
1016 
1017     // Interpreter and Baseline JIT CodeBlocks don&#39;t need to be jettisoned when
1018     // their weak references go stale. So if a basline JIT CodeBlock gets
1019     // scanned, we can assume that this means that it&#39;s live.
1020     if (!JITCode::isOptimizingJIT(jitType()))
1021         return true;
1022 
1023     return false;
1024 }
1025 
1026 bool CodeBlock::shouldJettisonDueToWeakReference(VM&amp; vm)
1027 {
1028     if (!JITCode::isOptimizingJIT(jitType()))
1029         return false;
1030     return !vm.heap.isMarked(this);
1031 }
1032 
1033 static Seconds timeToLive(JITType jitType)
1034 {
1035     if (UNLIKELY(Options::useEagerCodeBlockJettisonTiming())) {
1036         switch (jitType) {
1037         case JITType::InterpreterThunk:
1038             return 10_ms;
1039         case JITType::BaselineJIT:
1040             return 30_ms;
1041         case JITType::DFGJIT:
1042             return 40_ms;
1043         case JITType::FTLJIT:
1044             return 120_ms;
1045         default:
1046             return Seconds::infinity();
1047         }
1048     }
1049 
1050     switch (jitType) {
1051     case JITType::InterpreterThunk:
1052         return 5_s;
1053     case JITType::BaselineJIT:
1054         // Effectively 10 additional seconds, since BaselineJIT and
1055         // InterpreterThunk share a CodeBlock.
1056         return 15_s;
1057     case JITType::DFGJIT:
1058         return 20_s;
1059     case JITType::FTLJIT:
1060         return 60_s;
1061     default:
1062         return Seconds::infinity();
1063     }
1064 }
1065 
1066 bool CodeBlock::shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;)
1067 {
1068     if (m_vm-&gt;heap.isMarked(this))
1069         return false;
1070 
1071     if (UNLIKELY(Options::forceCodeBlockToJettisonDueToOldAge()))
1072         return true;
1073 
1074     if (timeSinceCreation() &lt; timeToLive(jitType()))
1075         return false;
1076 
1077     return true;
1078 }
1079 
1080 #if ENABLE(DFG_JIT)
1081 static bool shouldMarkTransition(VM&amp; vm, DFG::WeakReferenceTransition&amp; transition)
1082 {
1083     if (transition.m_codeOrigin &amp;&amp; !vm.heap.isMarked(transition.m_codeOrigin.get()))
1084         return false;
1085 
1086     if (!vm.heap.isMarked(transition.m_from.get()))
1087         return false;
1088 
1089     return true;
1090 }
<a name="37" id="anc37"></a><span class="line-added">1091 </span>
<span class="line-added">1092 BytecodeIndex CodeBlock::bytecodeIndexForExit(BytecodeIndex exitIndex) const</span>
<span class="line-added">1093 {</span>
<span class="line-added">1094     if (exitIndex.checkpoint()) {</span>
<span class="line-added">1095         const auto&amp; instruction = instructions().at(exitIndex);</span>
<span class="line-added">1096         exitIndex = instruction.next().index();</span>
<span class="line-added">1097     }</span>
<span class="line-added">1098     return exitIndex;</span>
<span class="line-added">1099 }</span>
1100 #endif // ENABLE(DFG_JIT)
1101 
1102 void CodeBlock::propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1103 {
<a name="38" id="anc38"></a>

1104     VM&amp; vm = *m_vm;
1105 
1106     if (jitType() == JITType::InterpreterThunk) {
<a name="39" id="anc39"></a><span class="line-modified">1107         if (m_metadata) {</span>
<span class="line-modified">1108             m_metadata-&gt;forEach&lt;OpPutById&gt;([&amp;] (auto&amp; metadata) {</span>




1109                 StructureID oldStructureID = metadata.m_oldStructureID;
1110                 StructureID newStructureID = metadata.m_newStructureID;
1111                 if (!oldStructureID || !newStructureID)
<a name="40" id="anc40"></a><span class="line-modified">1112                     return;</span>
1113                 Structure* oldStructure =
1114                     vm.heap.structureIDTable().get(oldStructureID);
1115                 Structure* newStructure =
1116                     vm.heap.structureIDTable().get(newStructureID);
1117                 if (vm.heap.isMarked(oldStructure))
1118                     visitor.appendUnbarriered(newStructure);
<a name="41" id="anc41"></a><span class="line-modified">1119             });</span>

1120         }
1121     }
1122 
1123 #if ENABLE(JIT)
1124     if (JITCode::isJIT(jitType())) {
1125         if (auto* jitData = m_jitData.get()) {
1126             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1127                 stubInfo-&gt;propagateTransitions(visitor);
1128         }
1129     }
1130 #endif // ENABLE(JIT)
1131 
1132 #if ENABLE(DFG_JIT)
1133     if (JITCode::isOptimizingJIT(jitType())) {
1134         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1135 
1136         dfgCommon-&gt;recordedStatuses.markIfCheap(visitor);
1137 
<a name="42" id="anc42"></a><span class="line-modified">1138         for (StructureID structureID : dfgCommon-&gt;weakStructureReferences)</span>
<span class="line-modified">1139             vm.getStructure(structureID)-&gt;markIfCheap(visitor);</span>
1140 
1141         for (auto&amp; transition : dfgCommon-&gt;transitions) {
1142             if (shouldMarkTransition(vm, transition)) {
1143                 // If the following three things are live, then the target of the
1144                 // transition is also live:
1145                 //
1146                 // - This code block. We know it&#39;s live already because otherwise
1147                 //   we wouldn&#39;t be scanning ourselves.
1148                 //
1149                 // - The code origin of the transition. Transitions may arise from
1150                 //   code that was inlined. They are not relevant if the user&#39;s
1151                 //   object that is required for the inlinee to run is no longer
1152                 //   live.
1153                 //
1154                 // - The source of the transition. The transition checks if some
1155                 //   heap location holds the source, and if so, stores the target.
1156                 //   Hence the source must be live for the transition to be live.
1157                 //
1158                 // We also short-circuit the liveness if the structure is harmless
1159                 // to mark (i.e. its global object and prototype are both already
1160                 // live).
1161 
1162                 visitor.append(transition.m_to);
1163             }
1164         }
1165     }
1166 #endif // ENABLE(DFG_JIT)
1167 }
1168 
1169 void CodeBlock::determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1170 {
1171     UNUSED_PARAM(visitor);
1172 
1173 #if ENABLE(DFG_JIT)
1174     VM&amp; vm = *m_vm;
1175     if (vm.heap.isMarked(this))
1176         return;
1177 
1178     // In rare and weird cases, this could be called on a baseline CodeBlock. One that I found was
1179     // that we might decide that the CodeBlock should be jettisoned due to old age, so the
1180     // isMarked check doesn&#39;t protect us.
1181     if (!JITCode::isOptimizingJIT(jitType()))
1182         return;
1183 
1184     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1185     // Now check all of our weak references. If all of them are live, then we
1186     // have proved liveness and so we scan our strong references. If at end of
1187     // GC we still have not proved liveness, then this code block is toast.
1188     bool allAreLiveSoFar = true;
1189     for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1190         JSCell* reference = dfgCommon-&gt;weakReferences[i].get();
1191         ASSERT(!jsDynamicCast&lt;CodeBlock*&gt;(vm, reference));
1192         if (!vm.heap.isMarked(reference)) {
1193             allAreLiveSoFar = false;
1194             break;
1195         }
1196     }
1197     if (allAreLiveSoFar) {
<a name="43" id="anc43"></a><span class="line-modified">1198         for (StructureID structureID : dfgCommon-&gt;weakStructureReferences) {</span>
<span class="line-modified">1199             Structure* structure = vm.getStructure(structureID);</span>
<span class="line-added">1200             if (!vm.heap.isMarked(structure)) {</span>
1201                 allAreLiveSoFar = false;
1202                 break;
1203             }
1204         }
1205     }
1206 
1207     // If some weak references are dead, then this fixpoint iteration was
1208     // unsuccessful.
1209     if (!allAreLiveSoFar)
1210         return;
1211 
1212     // All weak references are live. Record this information so we don&#39;t
1213     // come back here again, and scan the strong references.
1214     visitor.appendUnbarriered(this);
1215 #endif // ENABLE(DFG_JIT)
1216 }
1217 
1218 void CodeBlock::finalizeLLIntInlineCaches()
1219 {
1220     VM&amp; vm = *m_vm;
<a name="44" id="anc44"></a>
1221 
<a name="45" id="anc45"></a><span class="line-modified">1222     if (m_metadata) {</span>
<span class="line-modified">1223         // FIXME: https://bugs.webkit.org/show_bug.cgi?id=166418</span>
<span class="line-modified">1224         // We need to add optimizations for op_resolve_scope_for_hoisting_func_decl_in_eval to do link time scope resolution.</span>









1225 
<a name="46" id="anc46"></a><span class="line-modified">1226         m_metadata-&gt;forEach&lt;OpGetById&gt;([&amp;] (auto&amp; metadata) {</span>





1227             if (metadata.m_modeMetadata.mode != GetByIdMode::Default)
<a name="47" id="anc47"></a><span class="line-modified">1228                 return;</span>
1229             StructureID oldStructureID = metadata.m_modeMetadata.defaultMode.structureID;
1230             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
<a name="48" id="anc48"></a><span class="line-modified">1231                 return;</span>
<span class="line-modified">1232             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt property access.&quot;);</span>

1233             LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(metadata);
<a name="49" id="anc49"></a><span class="line-modified">1234         });</span>
<span class="line-modified">1235 </span>
<span class="line-modified">1236         m_metadata-&gt;forEach&lt;OpGetByIdDirect&gt;([&amp;] (auto&amp; metadata) {</span>

1237             StructureID oldStructureID = metadata.m_structureID;
1238             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
<a name="50" id="anc50"></a><span class="line-modified">1239                 return;</span>
<span class="line-modified">1240             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt property access.&quot;);</span>

1241             metadata.m_structureID = 0;
1242             metadata.m_offset = 0;
<a name="51" id="anc51"></a><span class="line-modified">1243         });</span>
<span class="line-modified">1244 </span>
<span class="line-modified">1245         m_metadata-&gt;forEach&lt;OpPutById&gt;([&amp;] (auto&amp; metadata) {</span>

1246             StructureID oldStructureID = metadata.m_oldStructureID;
1247             StructureID newStructureID = metadata.m_newStructureID;
1248             StructureChain* chain = metadata.m_structureChain.get();
1249             if ((!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))
1250                 &amp;&amp; (!newStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(newStructureID)))
1251                 &amp;&amp; (!chain || vm.heap.isMarked(chain)))
<a name="52" id="anc52"></a><span class="line-modified">1252                 return;</span>
<span class="line-modified">1253             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt put transition.&quot;);</span>

1254             metadata.m_oldStructureID = 0;
1255             metadata.m_offset = 0;
1256             metadata.m_newStructureID = 0;
1257             metadata.m_structureChain.clear();
<a name="53" id="anc53"></a><span class="line-modified">1258         });</span>
<span class="line-modified">1259 </span>
<span class="line-modified">1260         m_metadata-&gt;forEach&lt;OpToThis&gt;([&amp;] (auto&amp; metadata) {</span>





1261             if (!metadata.m_cachedStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(metadata.m_cachedStructureID)))
<a name="54" id="anc54"></a><span class="line-modified">1262                 return;</span>
1263             if (Options::verboseOSR()) {
1264                 Structure* structure = vm.heap.structureIDTable().get(metadata.m_cachedStructureID);
1265                 dataLogF(&quot;Clearing LLInt to_this with structure %p.\n&quot;, structure);
1266             }
1267             metadata.m_cachedStructureID = 0;
1268             metadata.m_toThisStatus = merge(metadata.m_toThisStatus, ToThisClearedByGC);
<a name="55" id="anc55"></a><span class="line-modified">1269         });</span>
<span class="line-modified">1270 </span>
<span class="line-modified">1271         auto handleCreateBytecode = [&amp;] (auto&amp; metadata, ASCIILiteral name) {</span>

1272             auto&amp; cacheWriteBarrier = metadata.m_cachedCallee;
1273             if (!cacheWriteBarrier || cacheWriteBarrier.unvalidatedGet() == JSCell::seenMultipleCalleeObjects())
<a name="56" id="anc56"></a><span class="line-modified">1274                 return;</span>
1275             JSCell* cachedFunction = cacheWriteBarrier.get();
1276             if (vm.heap.isMarked(cachedFunction))
<a name="57" id="anc57"></a><span class="line-modified">1277                 return;</span>
<span class="line-modified">1278             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt &quot;, name, &quot; with cached callee &quot;, RawPointer(cachedFunction), &quot;.&quot;);</span>

1279             cacheWriteBarrier.clear();
<a name="58" id="anc58"></a><span class="line-modified">1280         };</span>
<span class="line-modified">1281 </span>
<span class="line-modified">1282         m_metadata-&gt;forEach&lt;OpCreateThis&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1283             handleCreateBytecode(metadata, &quot;op_create_this&quot;_s);</span>
<span class="line-added">1284         });</span>
<span class="line-added">1285         m_metadata-&gt;forEach&lt;OpCreatePromise&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1286             handleCreateBytecode(metadata, &quot;op_create_promise&quot;_s);</span>
<span class="line-added">1287         });</span>
<span class="line-added">1288         m_metadata-&gt;forEach&lt;OpCreateGenerator&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1289             handleCreateBytecode(metadata, &quot;op_create_generator&quot;_s);</span>
<span class="line-added">1290         });</span>
<span class="line-added">1291         m_metadata-&gt;forEach&lt;OpCreateAsyncGenerator&gt;([&amp;] (auto&amp; metadata) {</span>
<span class="line-added">1292             handleCreateBytecode(metadata, &quot;op_create_async_generator&quot;_s);</span>
<span class="line-added">1293         });</span>
<span class="line-added">1294 </span>
<span class="line-added">1295         m_metadata-&gt;forEach&lt;OpResolveScope&gt;([&amp;] (auto&amp; metadata) {</span>
1296             // Right now this isn&#39;t strictly necessary. Any symbol tables that this will refer to
1297             // are for outer functions, and we refer to those functions strongly, and they refer
1298             // to the symbol table strongly. But it&#39;s nice to be on the safe side.
<a name="59" id="anc59"></a>
1299             WriteBarrierBase&lt;SymbolTable&gt;&amp; symbolTable = metadata.m_symbolTable;
1300             if (!symbolTable || vm.heap.isMarked(symbolTable.get()))
<a name="60" id="anc60"></a><span class="line-modified">1301                 return;</span>
<span class="line-modified">1302             dataLogLnIf(Options::verboseOSR(), &quot;Clearing dead symbolTable &quot;, RawPointer(symbolTable.get()));</span>

1303             symbolTable.clear();
<a name="61" id="anc61"></a><span class="line-modified">1304         });</span>
<span class="line-modified">1305 </span>
<span class="line-modified">1306         auto handleGetPutFromScope = [&amp;] (auto&amp; metadata) {</span>
<span class="line-modified">1307             GetPutInfo getPutInfo = metadata.m_getPutInfo;</span>
<span class="line-modified">1308             if (getPutInfo.resolveType() == GlobalVar || getPutInfo.resolveType() == GlobalVarWithVarInjectionChecks</span>
<span class="line-modified">1309                 || getPutInfo.resolveType() == LocalClosureVar || getPutInfo.resolveType() == GlobalLexicalVar || getPutInfo.resolveType() == GlobalLexicalVarWithVarInjectionChecks)</span>
<span class="line-modified">1310                 return;</span>
<span class="line-modified">1311             WriteBarrierBase&lt;Structure&gt;&amp; structure = metadata.m_structure;</span>
<span class="line-modified">1312             if (!structure || vm.heap.isMarked(structure.get()))</span>
<span class="line-modified">1313                 return;</span>
<span class="line-modified">1314             dataLogLnIf(Options::verboseOSR(), &quot;Clearing scope access with structure &quot;, RawPointer(structure.get()));</span>
<span class="line-modified">1315             structure.clear();</span>
<span class="line-added">1316         };</span>
<span class="line-added">1317 </span>
<span class="line-added">1318         m_metadata-&gt;forEach&lt;OpGetFromScope&gt;(handleGetPutFromScope);</span>
<span class="line-added">1319         m_metadata-&gt;forEach&lt;OpPutToScope&gt;(handleGetPutFromScope);</span>
1320     }
1321 
1322     // We can&#39;t just remove all the sets when we clear the caches since we might have created a watchpoint set
1323     // then cleared the cache without GCing in between.
1324     m_llintGetByIdWatchpointMap.removeIf([&amp;] (const StructureWatchpointMap::KeyValuePairType&amp; pair) -&gt; bool {
1325         auto clear = [&amp;] () {
1326             auto&amp; instruction = instructions().at(std::get&lt;1&gt;(pair.key));
1327             OpcodeID opcode = instruction-&gt;opcodeID();
1328             if (opcode == op_get_by_id) {
<a name="62" id="anc62"></a><span class="line-modified">1329                 dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt property access.&quot;);</span>

1330                 LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(instruction-&gt;as&lt;OpGetById&gt;().metadata(this));
1331             }
1332             return true;
1333         };
1334 
1335         if (!vm.heap.isMarked(vm.heap.structureIDTable().get(std::get&lt;0&gt;(pair.key))))
1336             return clear();
1337 
1338         for (const LLIntPrototypeLoadAdaptiveStructureWatchpoint&amp; watchpoint : pair.value) {
1339             if (!watchpoint.key().isStillLive(vm))
1340                 return clear();
1341         }
1342 
1343         return false;
1344     });
1345 
1346     forEachLLIntCallLinkInfo([&amp;](LLIntCallLinkInfo&amp; callLinkInfo) {
1347         if (callLinkInfo.isLinked() &amp;&amp; !vm.heap.isMarked(callLinkInfo.callee())) {
<a name="63" id="anc63"></a><span class="line-modified">1348             dataLogLnIf(Options::verboseOSR(), &quot;Clearing LLInt call from &quot;, *this);</span>

1349             callLinkInfo.unlink();
1350         }
1351         if (callLinkInfo.lastSeenCallee() &amp;&amp; !vm.heap.isMarked(callLinkInfo.lastSeenCallee()))
1352             callLinkInfo.clearLastSeenCallee();
1353     });
1354 }
1355 
1356 #if ENABLE(JIT)
1357 CodeBlock::JITData&amp; CodeBlock::ensureJITDataSlow(const ConcurrentJSLocker&amp;)
1358 {
1359     ASSERT(!m_jitData);
<a name="64" id="anc64"></a><span class="line-modified">1360     auto jitData = makeUnique&lt;JITData&gt;();</span>
<span class="line-added">1361     // calleeSaveRegisters() can access m_jitData without taking a lock from Baseline JIT. This is OK since JITData::m_calleeSaveRegisters is filled in DFG and FTL CodeBlocks.</span>
<span class="line-added">1362     // But we should not see garbage pointer in that case. We ensure JITData::m_calleeSaveRegisters is initialized as nullptr before exposing it to BaselineJIT by store-store-fence.</span>
<span class="line-added">1363     WTF::storeStoreFence();</span>
<span class="line-added">1364     m_jitData = WTFMove(jitData);</span>
1365     return *m_jitData;
1366 }
1367 
1368 void CodeBlock::finalizeBaselineJITInlineCaches()
1369 {
1370     if (auto* jitData = m_jitData.get()) {
1371         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1372             callLinkInfo-&gt;visitWeak(vm());
1373 
1374         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1375             stubInfo-&gt;visitWeakReferences(this);
1376     }
1377 }
1378 #endif
1379 
1380 void CodeBlock::finalizeUnconditionally(VM&amp; vm)
1381 {
1382     UNUSED_PARAM(vm);
1383 
1384     updateAllPredictions();
1385 
<a name="65" id="anc65"></a><span class="line-added">1386 #if ENABLE(JIT)</span>
<span class="line-added">1387     bool isEligibleForLLIntDowngrade = m_isEligibleForLLIntDowngrade;</span>
<span class="line-added">1388     m_isEligibleForLLIntDowngrade = false;</span>
<span class="line-added">1389     // If BaselineJIT code is not executing, and an optimized replacement exists, we attempt</span>
<span class="line-added">1390     // to discard baseline JIT code and reinstall LLInt code to save JIT memory.</span>
<span class="line-added">1391     if (Options::useLLInt() &amp;&amp; !m_hasLinkedOSRExit &amp;&amp; jitType() == JITType::BaselineJIT &amp;&amp; !m_vm-&gt;heap.codeBlockSet().isCurrentlyExecuting(this)) {</span>
<span class="line-added">1392         if (CodeBlock* optimizedCodeBlock = optimizedReplacement()) {</span>
<span class="line-added">1393             if (!optimizedCodeBlock-&gt;m_osrExitCounter) {</span>
<span class="line-added">1394                 if (isEligibleForLLIntDowngrade) {</span>
<span class="line-added">1395                     m_jitCode = nullptr;</span>
<span class="line-added">1396                     LLInt::setEntrypoint(this);</span>
<span class="line-added">1397                     RELEASE_ASSERT(jitType() == JITType::InterpreterThunk);</span>
<span class="line-added">1398 </span>
<span class="line-added">1399                     for (size_t i = 0; i &lt; m_unlinkedCode-&gt;numberOfExceptionHandlers(); i++) {</span>
<span class="line-added">1400                         const UnlinkedHandlerInfo&amp; unlinkedHandler = m_unlinkedCode-&gt;exceptionHandler(i);</span>
<span class="line-added">1401                         HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];</span>
<span class="line-added">1402                         auto&amp; instruction = *instructions().at(unlinkedHandler.target).ptr();</span>
<span class="line-added">1403                         MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(instruction);</span>
<span class="line-added">1404                         handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));</span>
<span class="line-added">1405                     }</span>
<span class="line-added">1406 </span>
<span class="line-added">1407                     unlinkIncomingCalls();</span>
<span class="line-added">1408 </span>
<span class="line-added">1409                     // It&#39;s safe to clear these out here because in finalizeUnconditionally all compiler threads</span>
<span class="line-added">1410                     // are safepointed, meaning they&#39;re running either before or after bytecode parser, and bytecode</span>
<span class="line-added">1411                     // parser is the only data structure pointing into the various *infos.</span>
<span class="line-added">1412                     resetJITData();</span>
<span class="line-added">1413                 } else</span>
<span class="line-added">1414                     m_isEligibleForLLIntDowngrade = true;</span>
<span class="line-added">1415             }</span>
<span class="line-added">1416         }</span>
<span class="line-added">1417     }</span>
<span class="line-added">1418 </span>
<span class="line-added">1419 #endif</span>
<span class="line-added">1420 </span>
1421     if (JITCode::couldBeInterpreted(jitType()))
1422         finalizeLLIntInlineCaches();
1423 
1424 #if ENABLE(JIT)
1425     if (!!jitCode())
1426         finalizeBaselineJITInlineCaches();
1427 #endif
1428 
1429 #if ENABLE(DFG_JIT)
1430     if (JITCode::isOptimizingJIT(jitType())) {
1431         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1432         dfgCommon-&gt;recordedStatuses.finalize(vm);
1433     }
1434 #endif // ENABLE(DFG_JIT)
1435 
1436     auto updateActivity = [&amp;] {
1437         if (!VM::useUnlinkedCodeBlockJettisoning())
1438             return;
1439         JITCode* jitCode = m_jitCode.get();
1440         double count = 0;
1441         bool alwaysActive = false;
1442         switch (JITCode::jitTypeFor(jitCode)) {
1443         case JITType::None:
1444         case JITType::HostCallThunk:
1445             return;
1446         case JITType::InterpreterThunk:
1447             count = m_llintExecuteCounter.count();
1448             break;
1449         case JITType::BaselineJIT:
1450             count = m_jitExecuteCounter.count();
1451             break;
1452         case JITType::DFGJIT:
1453 #if ENABLE(FTL_JIT)
1454             count = static_cast&lt;DFG::JITCode*&gt;(jitCode)-&gt;tierUpCounter.count();
1455 #else
1456             alwaysActive = true;
1457 #endif
1458             break;
1459         case JITType::FTLJIT:
1460             alwaysActive = true;
1461             break;
1462         }
1463         if (alwaysActive || m_previousCounter &lt; count) {
1464             // CodeBlock is active right now, so resetting UnlinkedCodeBlock&#39;s age.
1465             m_unlinkedCode-&gt;resetAge();
1466         }
1467         m_previousCounter = count;
1468     };
1469     updateActivity();
1470 
1471     VM::SpaceAndSet::setFor(*subspace()).remove(this);
1472 }
1473 
1474 void CodeBlock::destroy(JSCell* cell)
1475 {
1476     static_cast&lt;CodeBlock*&gt;(cell)-&gt;~CodeBlock();
1477 }
1478 
1479 void CodeBlock::getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result)
1480 {
1481 #if ENABLE(JIT)
1482     if (JITCode::isJIT(jitType())) {
1483         if (auto* jitData = m_jitData.get()) {
1484             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1485                 result.add(stubInfo-&gt;codeOrigin, ICStatus()).iterator-&gt;value.stubInfo = stubInfo;
1486             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1487                 result.add(callLinkInfo-&gt;codeOrigin(), ICStatus()).iterator-&gt;value.callLinkInfo = callLinkInfo;
1488             for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1489                 result.add(CodeOrigin(byValInfo-&gt;bytecodeIndex), ICStatus()).iterator-&gt;value.byValInfo = byValInfo;
1490         }
1491 #if ENABLE(DFG_JIT)
1492         if (JITCode::isOptimizingJIT(jitType())) {
1493             DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1494             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.calls)
1495                 result.add(pair.first, ICStatus()).iterator-&gt;value.callStatus = pair.second.get();
1496             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.gets)
1497                 result.add(pair.first, ICStatus()).iterator-&gt;value.getStatus = pair.second.get();
1498             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.puts)
1499                 result.add(pair.first, ICStatus()).iterator-&gt;value.putStatus = pair.second.get();
1500             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.ins)
1501                 result.add(pair.first, ICStatus()).iterator-&gt;value.inStatus = pair.second.get();
1502         }
1503 #endif
1504     }
1505 #else
1506     UNUSED_PARAM(result);
1507 #endif
1508 }
1509 
1510 void CodeBlock::getICStatusMap(ICStatusMap&amp; result)
1511 {
1512     ConcurrentJSLocker locker(m_lock);
1513     getICStatusMap(locker, result);
1514 }
1515 
1516 #if ENABLE(JIT)
1517 StructureStubInfo* CodeBlock::addStubInfo(AccessType accessType)
1518 {
1519     ConcurrentJSLocker locker(m_lock);
1520     return ensureJITData(locker).m_stubInfos.add(accessType);
1521 }
1522 
<a name="66" id="anc66"></a><span class="line-modified">1523 JITAddIC* CodeBlock::addJITAddIC(BinaryArithProfile* arithProfile)</span>
1524 {
1525     ConcurrentJSLocker locker(m_lock);
1526     return ensureJITData(locker).m_addICs.add(arithProfile);
1527 }
1528 
<a name="67" id="anc67"></a><span class="line-modified">1529 JITMulIC* CodeBlock::addJITMulIC(BinaryArithProfile* arithProfile)</span>
1530 {
1531     ConcurrentJSLocker locker(m_lock);
1532     return ensureJITData(locker).m_mulICs.add(arithProfile);
1533 }
1534 
<a name="68" id="anc68"></a><span class="line-modified">1535 JITSubIC* CodeBlock::addJITSubIC(BinaryArithProfile* arithProfile)</span>
1536 {
1537     ConcurrentJSLocker locker(m_lock);
1538     return ensureJITData(locker).m_subICs.add(arithProfile);
1539 }
1540 
<a name="69" id="anc69"></a><span class="line-modified">1541 JITNegIC* CodeBlock::addJITNegIC(UnaryArithProfile* arithProfile)</span>
1542 {
1543     ConcurrentJSLocker locker(m_lock);
1544     return ensureJITData(locker).m_negICs.add(arithProfile);
1545 }
1546 
1547 StructureStubInfo* CodeBlock::findStubInfo(CodeOrigin codeOrigin)
1548 {
1549     ConcurrentJSLocker locker(m_lock);
1550     if (auto* jitData = m_jitData.get()) {
1551         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
1552             if (stubInfo-&gt;codeOrigin == codeOrigin)
1553                 return stubInfo;
1554         }
1555     }
1556     return nullptr;
1557 }
1558 
1559 ByValInfo* CodeBlock::addByValInfo()
1560 {
1561     ConcurrentJSLocker locker(m_lock);
1562     return ensureJITData(locker).m_byValInfos.add();
1563 }
1564 
1565 CallLinkInfo* CodeBlock::addCallLinkInfo()
1566 {
1567     ConcurrentJSLocker locker(m_lock);
1568     return ensureJITData(locker).m_callLinkInfos.add();
1569 }
1570 
<a name="70" id="anc70"></a><span class="line-modified">1571 CallLinkInfo* CodeBlock::getCallLinkInfoForBytecodeIndex(BytecodeIndex index)</span>
1572 {
1573     ConcurrentJSLocker locker(m_lock);
1574     if (auto* jitData = m_jitData.get()) {
1575         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos) {
1576             if (callLinkInfo-&gt;codeOrigin() == CodeOrigin(index))
1577                 return callLinkInfo;
1578         }
1579     }
1580     return nullptr;
1581 }
1582 
<a name="71" id="anc71"></a><span class="line-modified">1583 void CodeBlock::setRareCaseProfiles(RefCountedArray&lt;RareCaseProfile&gt;&amp;&amp; rareCaseProfiles)</span>
1584 {
1585     ConcurrentJSLocker locker(m_lock);
<a name="72" id="anc72"></a><span class="line-modified">1586     ensureJITData(locker).m_rareCaseProfiles = WTFMove(rareCaseProfiles);</span>


1587 }
1588 
<a name="73" id="anc73"></a><span class="line-modified">1589 RareCaseProfile* CodeBlock::rareCaseProfileForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex bytecodeIndex)</span>
1590 {
1591     if (auto* jitData = m_jitData.get()) {
<a name="74" id="anc74"></a><span class="line-modified">1592         return tryBinarySearch&lt;RareCaseProfile, BytecodeIndex&gt;(</span>
<span class="line-modified">1593             jitData-&gt;m_rareCaseProfiles, jitData-&gt;m_rareCaseProfiles.size(), bytecodeIndex,</span>
<span class="line-modified">1594             getRareCaseProfileBytecodeIndex);</span>
1595     }
1596     return nullptr;
1597 }
1598 
<a name="75" id="anc75"></a><span class="line-modified">1599 unsigned CodeBlock::rareCaseProfileCountForBytecodeIndex(const ConcurrentJSLocker&amp; locker, BytecodeIndex bytecodeIndex)</span>
1600 {
<a name="76" id="anc76"></a><span class="line-modified">1601     RareCaseProfile* profile = rareCaseProfileForBytecodeIndex(locker, bytecodeIndex);</span>
1602     if (profile)
1603         return profile-&gt;m_counter;
1604     return 0;
1605 }
1606 
1607 void CodeBlock::setCalleeSaveRegisters(RegisterSet calleeSaveRegisters)
1608 {
1609     ConcurrentJSLocker locker(m_lock);
1610     ensureJITData(locker).m_calleeSaveRegisters = makeUnique&lt;RegisterAtOffsetList&gt;(calleeSaveRegisters);
1611 }
1612 
1613 void CodeBlock::setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt; registerAtOffsetList)
1614 {
1615     ConcurrentJSLocker locker(m_lock);
1616     ensureJITData(locker).m_calleeSaveRegisters = WTFMove(registerAtOffsetList);
1617 }
1618 
1619 void CodeBlock::resetJITData()
1620 {
1621     RELEASE_ASSERT(!JITCode::isJIT(jitType()));
1622     ConcurrentJSLocker locker(m_lock);
1623 
1624     if (auto* jitData = m_jitData.get()) {
1625         // We can clear these because no other thread will have references to any stub infos, call
1626         // link infos, or by val infos if we don&#39;t have JIT code. Attempts to query these data
1627         // structures using the concurrent API (getICStatusMap and friends) will return nothing if we
<a name="77" id="anc77"></a><span class="line-modified">1628         // don&#39;t have JIT code. So it&#39;s safe to call this if we fail a baseline JIT compile.</span>
<span class="line-modified">1629         //</span>
<span class="line-modified">1630         // We also call this from finalizeUnconditionally when we degrade from baseline JIT to LLInt</span>
<span class="line-modified">1631         // code. This is safe to do since all compiler threads are safepointed in finalizeUnconditionally,</span>
<span class="line-added">1632         // which means we&#39;ve made it past bytecode parsing. Only the bytecode parser will hold onto</span>
<span class="line-added">1633         // references to these various *infos via its use of ICStatusMap. Also, OSR exit might point to</span>
<span class="line-added">1634         // these *infos, but when we have an OSR exit linked to this CodeBlock, we won&#39;t downgrade</span>
<span class="line-added">1635         // to LLInt.</span>
<span class="line-added">1636 </span>
<span class="line-added">1637         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {</span>
<span class="line-added">1638             stubInfo-&gt;aboutToDie();</span>
<span class="line-added">1639             stubInfo-&gt;deref();</span>
<span class="line-added">1640         }</span>
<span class="line-added">1641 </span>
1642         // We can clear this because the DFG&#39;s queries to these data structures are guarded by whether
1643         // there is JIT code.
<a name="78" id="anc78"></a><span class="line-modified">1644 </span>
<span class="line-added">1645         m_jitData = nullptr;</span>
1646     }
1647 }
1648 #endif
1649 
1650 void CodeBlock::visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1651 {
1652     // We strongly visit OSR exits targets because we don&#39;t want to deal with
1653     // the complexity of generating an exit target CodeBlock on demand and
1654     // guaranteeing that it matches the details of the CodeBlock we compiled
1655     // the OSR exit against.
1656 
1657     visitor.append(m_alternative);
1658 
1659 #if ENABLE(DFG_JIT)
1660     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1661     if (dfgCommon-&gt;inlineCallFrames) {
1662         for (auto* inlineCallFrame : *dfgCommon-&gt;inlineCallFrames) {
1663             ASSERT(inlineCallFrame-&gt;baselineCodeBlock);
1664             visitor.append(inlineCallFrame-&gt;baselineCodeBlock);
1665         }
1666     }
1667 #endif
1668 }
1669 
1670 void CodeBlock::stronglyVisitStrongReferences(const ConcurrentJSLocker&amp; locker, SlotVisitor&amp; visitor)
1671 {
1672     UNUSED_PARAM(locker);
1673 
1674     visitor.append(m_globalObject);
1675     visitor.append(m_ownerExecutable); // This is extra important since it causes the ExecutableToCodeBlockEdge to be marked.
1676     visitor.append(m_unlinkedCode);
1677     if (m_rareData)
1678         m_rareData-&gt;m_directEvalCodeCache.visitAggregate(visitor);
1679     visitor.appendValues(m_constantRegisters.data(), m_constantRegisters.size());
1680     for (auto&amp; functionExpr : m_functionExprs)
1681         visitor.append(functionExpr);
1682     for (auto&amp; functionDecl : m_functionDecls)
1683         visitor.append(functionDecl);
1684     forEachObjectAllocationProfile([&amp;](ObjectAllocationProfile&amp; objectAllocationProfile) {
1685         objectAllocationProfile.visitAggregate(visitor);
1686     });
1687 
1688 #if ENABLE(JIT)
1689     if (auto* jitData = m_jitData.get()) {
1690         for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1691             visitor.append(byValInfo-&gt;cachedSymbol);
<a name="79" id="anc79"></a><span class="line-added">1692         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)</span>
<span class="line-added">1693             stubInfo-&gt;visitAggregate(visitor);</span>
1694     }
1695 #endif
1696 
1697 #if ENABLE(DFG_JIT)
<a name="80" id="anc80"></a><span class="line-modified">1698     if (JITCode::isOptimizingJIT(jitType())) {</span>
<span class="line-added">1699         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();</span>
<span class="line-added">1700         dfgCommon-&gt;recordedStatuses.visitAggregate(visitor);</span>
1701         visitOSRExitTargets(locker, visitor);
<a name="81" id="anc81"></a><span class="line-added">1702     }</span>
1703 #endif
1704 }
1705 
1706 void CodeBlock::stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1707 {
1708     UNUSED_PARAM(visitor);
1709 
1710 #if ENABLE(DFG_JIT)
1711     if (!JITCode::isOptimizingJIT(jitType()))
1712         return;
1713 
1714     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1715 
1716     for (auto&amp; transition : dfgCommon-&gt;transitions) {
1717         if (!!transition.m_codeOrigin)
1718             visitor.append(transition.m_codeOrigin); // Almost certainly not necessary, since the code origin should also be a weak reference. Better to be safe, though.
1719         visitor.append(transition.m_from);
1720         visitor.append(transition.m_to);
1721     }
1722 
1723     for (auto&amp; weakReference : dfgCommon-&gt;weakReferences)
1724         visitor.append(weakReference);
1725 
<a name="82" id="anc82"></a><span class="line-modified">1726     for (StructureID structureID : dfgCommon-&gt;weakStructureReferences)</span>
<span class="line-modified">1727         visitor.appendUnbarriered(visitor.vm().getStructure(structureID));</span>
1728 
1729     dfgCommon-&gt;livenessHasBeenProved = true;
1730 #endif
1731 }
1732 
1733 CodeBlock* CodeBlock::baselineAlternative()
1734 {
1735 #if ENABLE(JIT)
1736     CodeBlock* result = this;
1737     while (result-&gt;alternative())
1738         result = result-&gt;alternative();
1739     RELEASE_ASSERT(result);
1740     RELEASE_ASSERT(JITCode::isBaselineCode(result-&gt;jitType()) || result-&gt;jitType() == JITType::None);
1741     return result;
1742 #else
1743     return this;
1744 #endif
1745 }
1746 
1747 CodeBlock* CodeBlock::baselineVersion()
1748 {
1749 #if ENABLE(JIT)
1750     JITType selfJITType = jitType();
1751     if (JITCode::isBaselineCode(selfJITType))
1752         return this;
1753     CodeBlock* result = replacement();
1754     if (!result) {
1755         if (JITCode::isOptimizingJIT(selfJITType)) {
1756             // The replacement can be null if we&#39;ve had a memory clean up and the executable
1757             // has been purged of its codeBlocks (see ExecutableBase::clearCode()). Regardless,
1758             // the current codeBlock is still live on the stack, and as an optimizing JIT
1759             // codeBlock, it will keep its baselineAlternative() alive for us to fetch below.
1760             result = this;
1761         } else {
1762             // This can happen if we&#39;re creating the original CodeBlock for an executable.
1763             // Assume that we&#39;re the baseline CodeBlock.
1764             RELEASE_ASSERT(selfJITType == JITType::None);
1765             return this;
1766         }
1767     }
1768     result = result-&gt;baselineAlternative();
1769     ASSERT(result);
1770     return result;
1771 #else
1772     return this;
1773 #endif
1774 }
1775 
1776 #if ENABLE(JIT)
<a name="83" id="anc83"></a><span class="line-modified">1777 CodeBlock* CodeBlock::optimizedReplacement(JITType typeToReplace)</span>
1778 {
1779     CodeBlock* replacement = this-&gt;replacement();
<a name="84" id="anc84"></a><span class="line-modified">1780     if (!replacement)</span>
<span class="line-added">1781         return nullptr;</span>
<span class="line-added">1782     if (JITCode::isHigherTier(replacement-&gt;jitType(), typeToReplace))</span>
<span class="line-added">1783         return replacement;</span>
<span class="line-added">1784     return nullptr;</span>
<span class="line-added">1785 }</span>
<span class="line-added">1786 </span>
<span class="line-added">1787 CodeBlock* CodeBlock::optimizedReplacement()</span>
<span class="line-added">1788 {</span>
<span class="line-added">1789     return optimizedReplacement(jitType());</span>
<span class="line-added">1790 }</span>
<span class="line-added">1791 </span>
<span class="line-added">1792 bool CodeBlock::hasOptimizedReplacement(JITType typeToReplace)</span>
<span class="line-added">1793 {</span>
<span class="line-added">1794     return !!optimizedReplacement(typeToReplace);</span>
1795 }
1796 
1797 bool CodeBlock::hasOptimizedReplacement()
1798 {
1799     return hasOptimizedReplacement(jitType());
1800 }
1801 #endif
1802 
<a name="85" id="anc85"></a><span class="line-modified">1803 HandlerInfo* CodeBlock::handlerForBytecodeIndex(BytecodeIndex bytecodeIndex, RequiredHandler requiredHandler)</span>
1804 {
<a name="86" id="anc86"></a><span class="line-modified">1805     RELEASE_ASSERT(bytecodeIndex.offset() &lt; instructions().size());</span>
<span class="line-modified">1806     return handlerForIndex(bytecodeIndex.offset(), requiredHandler);</span>
1807 }
1808 
1809 HandlerInfo* CodeBlock::handlerForIndex(unsigned index, RequiredHandler requiredHandler)
1810 {
1811     if (!m_rareData)
1812         return 0;
<a name="87" id="anc87"></a><span class="line-modified">1813     return HandlerInfo::handlerForIndex&lt;HandlerInfo&gt;(m_rareData-&gt;m_exceptionHandlers, index, requiredHandler);</span>
1814 }
1815 
1816 DisposableCallSiteIndex CodeBlock::newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite)
1817 {
1818 #if ENABLE(DFG_JIT)
1819     RELEASE_ASSERT(JITCode::isOptimizingJIT(jitType()));
1820     RELEASE_ASSERT(canGetCodeOrigin(originalCallSite));
1821     ASSERT(!!handlerForIndex(originalCallSite.bits()));
1822     CodeOrigin originalOrigin = codeOrigin(originalCallSite);
1823     return m_jitCode-&gt;dfgCommon()-&gt;addDisposableCallSiteIndex(originalOrigin);
1824 #else
1825     // We never create new on-the-fly exception handling
1826     // call sites outside the DFG/FTL inline caches.
1827     UNUSED_PARAM(originalCallSite);
1828     RELEASE_ASSERT_NOT_REACHED();
1829     return DisposableCallSiteIndex(0u);
1830 #endif
1831 }
1832 
1833 
1834 
<a name="88" id="anc88"></a><span class="line-modified">1835 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
1836 {
<a name="89" id="anc89"></a><span class="line-modified">1837     auto&amp; instruction = instructions().at(bytecodeIndex);</span>
1838     OpCatch op = instruction-&gt;as&lt;OpCatch&gt;();
1839     auto&amp; metadata = op.metadata(this);
1840     if (!!metadata.m_buffer) {
<a name="90" id="anc90"></a><span class="line-modified">1841 #if ASSERT_ENABLED</span>
1842         ConcurrentJSLocker locker(m_lock);
1843         bool found = false;
1844         auto* rareData = m_rareData.get();
1845         ASSERT(rareData);
1846         for (auto&amp; profile : rareData-&gt;m_catchProfiles) {
1847             if (profile.get() == metadata.m_buffer) {
1848                 found = true;
1849                 break;
1850             }
1851         }
1852         ASSERT(found);
<a name="91" id="anc91"></a><span class="line-modified">1853 #endif // ASSERT_ENABLED</span>
1854         return;
1855     }
1856 
<a name="92" id="anc92"></a><span class="line-modified">1857     ensureCatchLivenessIsComputedForBytecodeIndexSlow(op, bytecodeIndex);</span>
1858 }
1859 
<a name="93" id="anc93"></a><span class="line-modified">1860 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeIndexSlow(const OpCatch&amp; op, BytecodeIndex bytecodeIndex)</span>
1861 {
1862     BytecodeLivenessAnalysis&amp; bytecodeLiveness = livenessAnalysis();
1863 
1864     // We get the live-out set of variables at op_catch, not the live-in. This
1865     // is because the variables that the op_catch defines might be dead, and
1866     // we can avoid profiling them and extracting them when doing OSR entry
1867     // into the DFG.
1868 
<a name="94" id="anc94"></a><span class="line-modified">1869     auto nextOffset = instructions().at(bytecodeIndex).next().offset();</span>
<span class="line-modified">1870     FastBitVector liveLocals = bytecodeLiveness.getLivenessInfoAtBytecodeIndex(this, BytecodeIndex(nextOffset));</span>
1871     Vector&lt;VirtualRegister&gt; liveOperands;
1872     liveOperands.reserveInitialCapacity(liveLocals.bitCount());
1873     liveLocals.forEachSetBit([&amp;] (unsigned liveLocal) {
1874         liveOperands.append(virtualRegisterForLocal(liveLocal));
1875     });
1876 
1877     for (int i = 0; i &lt; numParameters(); ++i)
<a name="95" id="anc95"></a><span class="line-modified">1878         liveOperands.append(virtualRegisterForArgumentIncludingThis(i));</span>
1879 
<a name="96" id="anc96"></a><span class="line-modified">1880     auto profiles = makeUnique&lt;ValueProfileAndVirtualRegisterBuffer&gt;(liveOperands.size());</span>
1881     RELEASE_ASSERT(profiles-&gt;m_size == liveOperands.size());
1882     for (unsigned i = 0; i &lt; profiles-&gt;m_size; ++i)
<a name="97" id="anc97"></a><span class="line-modified">1883         profiles-&gt;m_buffer.get()[i].m_operand = liveOperands[i];</span>
1884 
1885     createRareDataIfNecessary();
1886 
1887     // The compiler thread will read this pointer value and then proceed to dereference it
1888     // if it is not null. We need to make sure all above stores happen before this store so
1889     // the compiler thread reads fully initialized data.
1890     WTF::storeStoreFence();
1891 
1892     op.metadata(this).m_buffer = profiles.get();
1893     {
1894         ConcurrentJSLocker locker(m_lock);
1895         m_rareData-&gt;m_catchProfiles.append(WTFMove(profiles));
1896     }
1897 }
1898 
1899 void CodeBlock::removeExceptionHandlerForCallSite(DisposableCallSiteIndex callSiteIndex)
1900 {
1901     RELEASE_ASSERT(m_rareData);
1902     Vector&lt;HandlerInfo&gt;&amp; exceptionHandlers = m_rareData-&gt;m_exceptionHandlers;
1903     unsigned index = callSiteIndex.bits();
1904     for (size_t i = 0; i &lt; exceptionHandlers.size(); ++i) {
1905         HandlerInfo&amp; handler = exceptionHandlers[i];
1906         if (handler.start &lt;= index &amp;&amp; handler.end &gt; index) {
1907             exceptionHandlers.remove(i);
1908             return;
1909         }
1910     }
1911 
1912     RELEASE_ASSERT_NOT_REACHED();
1913 }
1914 
<a name="98" id="anc98"></a><span class="line-modified">1915 unsigned CodeBlock::lineNumberForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
1916 {
<a name="99" id="anc99"></a><span class="line-modified">1917     RELEASE_ASSERT(bytecodeIndex.offset() &lt; instructions().size());</span>
<span class="line-modified">1918     return ownerExecutable()-&gt;firstLine() + m_unlinkedCode-&gt;lineNumberForBytecodeIndex(bytecodeIndex);</span>
1919 }
1920 
<a name="100" id="anc100"></a><span class="line-modified">1921 unsigned CodeBlock::columnNumberForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
1922 {
1923     int divot;
1924     int startOffset;
1925     int endOffset;
1926     unsigned line;
1927     unsigned column;
<a name="101" id="anc101"></a><span class="line-modified">1928     expressionRangeForBytecodeIndex(bytecodeIndex, divot, startOffset, endOffset, line, column);</span>
1929     return column;
1930 }
1931 
<a name="102" id="anc102"></a><span class="line-modified">1932 void CodeBlock::expressionRangeForBytecodeIndex(BytecodeIndex bytecodeIndex, int&amp; divot, int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const</span>
1933 {
<a name="103" id="anc103"></a><span class="line-modified">1934     m_unlinkedCode-&gt;expressionRangeForBytecodeIndex(bytecodeIndex, divot, startOffset, endOffset, line, column);</span>
1935     divot += sourceOffset();
1936     column += line ? 1 : firstLineColumnOffset();
1937     line += ownerExecutable()-&gt;firstLine();
1938 }
1939 
<a name="104" id="anc104"></a><span class="line-modified">1940 bool CodeBlock::hasOpDebugForLineAndColumn(unsigned line, Optional&lt;unsigned&gt; column)</span>
1941 {
1942     const InstructionStream&amp; instructionStream = instructions();
1943     for (const auto&amp; it : instructionStream) {
1944         if (it-&gt;is&lt;OpDebug&gt;()) {
1945             int unused;
1946             unsigned opDebugLine;
1947             unsigned opDebugColumn;
<a name="105" id="anc105"></a><span class="line-modified">1948             expressionRangeForBytecodeIndex(it.index(), unused, unused, unused, opDebugLine, opDebugColumn);</span>
<span class="line-modified">1949             if (line == opDebugLine &amp;&amp; (!column || column == opDebugColumn))</span>
1950                 return true;
1951         }
1952     }
1953     return false;
1954 }
1955 
<a name="106" id="anc106"></a><span class="line-modified">1956 void CodeBlock::shrinkToFit(const ConcurrentJSLocker&amp;, ShrinkMode shrinkMode)</span>
1957 {
<a name="107" id="anc107"></a><span class="line-modified">1958 #if USE(JSVALUE32_64)</span>
<span class="line-modified">1959     // Only 32bit Baseline JIT is touching m_constantRegisters address directly.</span>
<span class="line-modified">1960     if (shrinkMode == ShrinkMode::EarlyShrink)</span>





1961         m_constantRegisters.shrinkToFit();
<a name="108" id="anc108"></a><span class="line-modified">1962 #else</span>
<span class="line-added">1963     m_constantRegisters.shrinkToFit();</span>
<span class="line-added">1964 #endif</span>
<span class="line-added">1965     m_constantsSourceCodeRepresentation.shrinkToFit();</span>
1966 
<a name="109" id="anc109"></a><span class="line-added">1967     if (shrinkMode == ShrinkMode::EarlyShrink) {</span>
1968         if (m_rareData) {
1969             m_rareData-&gt;m_switchJumpTables.shrinkToFit();
1970             m_rareData-&gt;m_stringSwitchJumpTables.shrinkToFit();
1971         }
1972     } // else don&#39;t shrink these, because we would have already pointed pointers into these tables.
1973 }
1974 
1975 #if ENABLE(JIT)
<a name="110" id="anc110"></a><span class="line-modified">1976 void CodeBlock::linkIncomingCall(CallFrame* callerFrame, CallLinkInfo* incoming)</span>
1977 {
1978     noticeIncomingCall(callerFrame);
1979     ConcurrentJSLocker locker(m_lock);
1980     ensureJITData(locker).m_incomingCalls.push(incoming);
1981 }
1982 
<a name="111" id="anc111"></a><span class="line-modified">1983 void CodeBlock::linkIncomingPolymorphicCall(CallFrame* callerFrame, PolymorphicCallNode* incoming)</span>
1984 {
1985     noticeIncomingCall(callerFrame);
1986     {
1987         ConcurrentJSLocker locker(m_lock);
1988         ensureJITData(locker).m_incomingPolymorphicCalls.push(incoming);
1989     }
1990 }
1991 #endif // ENABLE(JIT)
1992 
1993 void CodeBlock::unlinkIncomingCalls()
1994 {
1995     while (m_incomingLLIntCalls.begin() != m_incomingLLIntCalls.end())
1996         m_incomingLLIntCalls.begin()-&gt;unlink();
1997 #if ENABLE(JIT)
1998     JITData* jitData = nullptr;
1999     {
2000         ConcurrentJSLocker locker(m_lock);
2001         jitData = m_jitData.get();
2002     }
2003     if (jitData) {
2004         while (jitData-&gt;m_incomingCalls.begin() != jitData-&gt;m_incomingCalls.end())
2005             jitData-&gt;m_incomingCalls.begin()-&gt;unlink(vm());
2006         while (jitData-&gt;m_incomingPolymorphicCalls.begin() != jitData-&gt;m_incomingPolymorphicCalls.end())
2007             jitData-&gt;m_incomingPolymorphicCalls.begin()-&gt;unlink(vm());
2008     }
2009 #endif // ENABLE(JIT)
2010 }
2011 
<a name="112" id="anc112"></a><span class="line-modified">2012 void CodeBlock::linkIncomingCall(CallFrame* callerFrame, LLIntCallLinkInfo* incoming)</span>
2013 {
2014     noticeIncomingCall(callerFrame);
2015     m_incomingLLIntCalls.push(incoming);
2016 }
2017 
2018 CodeBlock* CodeBlock::newReplacement()
2019 {
2020     return ownerExecutable()-&gt;newReplacementCodeBlockFor(specializationKind());
2021 }
2022 
2023 #if ENABLE(JIT)
2024 CodeBlock* CodeBlock::replacement()
2025 {
2026     const ClassInfo* classInfo = this-&gt;classInfo(vm());
2027 
2028     if (classInfo == FunctionCodeBlock::info())
2029         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;codeBlockFor(isConstructor() ? CodeForConstruct : CodeForCall);
2030 
2031     if (classInfo == EvalCodeBlock::info())
2032         return jsCast&lt;EvalExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
2033 
2034     if (classInfo == ProgramCodeBlock::info())
2035         return jsCast&lt;ProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
2036 
2037     if (classInfo == ModuleProgramCodeBlock::info())
2038         return jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
2039 
2040     RELEASE_ASSERT_NOT_REACHED();
2041     return nullptr;
2042 }
2043 
2044 DFG::CapabilityLevel CodeBlock::computeCapabilityLevel()
2045 {
2046     const ClassInfo* classInfo = this-&gt;classInfo(vm());
2047 
2048     if (classInfo == FunctionCodeBlock::info()) {
2049         if (isConstructor())
2050             return DFG::functionForConstructCapabilityLevel(this);
2051         return DFG::functionForCallCapabilityLevel(this);
2052     }
2053 
2054     if (classInfo == EvalCodeBlock::info())
2055         return DFG::evalCapabilityLevel(this);
2056 
2057     if (classInfo == ProgramCodeBlock::info())
2058         return DFG::programCapabilityLevel(this);
2059 
2060     if (classInfo == ModuleProgramCodeBlock::info())
2061         return DFG::programCapabilityLevel(this);
2062 
2063     RELEASE_ASSERT_NOT_REACHED();
2064     return DFG::CannotCompile;
2065 }
2066 
2067 #endif // ENABLE(JIT)
2068 
2069 void CodeBlock::jettison(Profiler::JettisonReason reason, ReoptimizationMode mode, const FireDetail* detail)
2070 {
2071 #if !ENABLE(DFG_JIT)
2072     UNUSED_PARAM(mode);
2073     UNUSED_PARAM(detail);
2074 #endif
2075 
2076     VM&amp; vm = *m_vm;
2077 
2078     CodeBlock* codeBlock = this; // Placate GCC for use in CODEBLOCK_LOG_EVENT  (does not like this).
2079     CODEBLOCK_LOG_EVENT(codeBlock, &quot;jettison&quot;, (&quot;due to &quot;, reason, &quot;, counting = &quot;, mode == CountReoptimization, &quot;, detail = &quot;, pointerDump(detail)));
2080 
2081     RELEASE_ASSERT(reason != Profiler::NotJettisoned);
2082 
2083 #if ENABLE(DFG_JIT)
2084     if (DFG::shouldDumpDisassembly()) {
2085         dataLog(&quot;Jettisoning &quot;, *this);
2086         if (mode == CountReoptimization)
2087             dataLog(&quot; and counting reoptimization&quot;);
2088         dataLog(&quot; due to &quot;, reason);
2089         if (detail)
2090             dataLog(&quot;, &quot;, *detail);
2091         dataLog(&quot;.\n&quot;);
2092     }
2093 
2094     if (reason == Profiler::JettisonDueToWeakReference) {
2095         if (DFG::shouldDumpDisassembly()) {
2096             dataLog(*this, &quot; will be jettisoned because of the following dead references:\n&quot;);
2097             DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
2098             for (auto&amp; transition : dfgCommon-&gt;transitions) {
2099                 JSCell* origin = transition.m_codeOrigin.get();
2100                 JSCell* from = transition.m_from.get();
2101                 JSCell* to = transition.m_to.get();
2102                 if ((!origin || vm.heap.isMarked(origin)) &amp;&amp; vm.heap.isMarked(from))
2103                     continue;
2104                 dataLog(&quot;    Transition under &quot;, RawPointer(origin), &quot;, &quot;, RawPointer(from), &quot; -&gt; &quot;, RawPointer(to), &quot;.\n&quot;);
2105             }
2106             for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
2107                 JSCell* weak = dfgCommon-&gt;weakReferences[i].get();
2108                 if (vm.heap.isMarked(weak))
2109                     continue;
2110                 dataLog(&quot;    Weak reference &quot;, RawPointer(weak), &quot;.\n&quot;);
2111             }
2112         }
2113     }
2114 #endif // ENABLE(DFG_JIT)
2115 
2116     DeferGCForAWhile deferGC(*heap());
2117 
2118     // We want to accomplish two things here:
2119     // 1) Make sure that if this CodeBlock is on the stack right now, then if we return to it
2120     //    we should OSR exit at the top of the next bytecode instruction after the return.
2121     // 2) Make sure that if we call the owner executable, then we shouldn&#39;t call this CodeBlock.
2122 
2123 #if ENABLE(DFG_JIT)
2124     if (JITCode::isOptimizingJIT(jitType()))
2125         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();
2126 
2127     if (reason != Profiler::JettisonDueToOldAge) {
2128         Profiler::Compilation* compilation = jitCode()-&gt;dfgCommon()-&gt;compilation.get();
2129         if (UNLIKELY(compilation))
2130             compilation-&gt;setJettisonReason(reason, detail);
2131 
2132         // This accomplishes (1), and does its own book-keeping about whether it has already happened.
2133         if (!jitCode()-&gt;dfgCommon()-&gt;invalidate()) {
2134             // We&#39;ve already been invalidated.
2135             RELEASE_ASSERT(this != replacement() || (vm.heap.isCurrentThreadBusy() &amp;&amp; !vm.heap.isMarked(ownerExecutable())));
2136             return;
2137         }
2138     }
2139 
2140     if (DFG::shouldDumpDisassembly())
2141         dataLog(&quot;    Did invalidate &quot;, *this, &quot;\n&quot;);
2142 
2143     // Count the reoptimization if that&#39;s what the user wanted.
2144     if (mode == CountReoptimization) {
2145         // FIXME: Maybe this should call alternative().
2146         // https://bugs.webkit.org/show_bug.cgi?id=123677
2147         baselineAlternative()-&gt;countReoptimization();
2148         if (DFG::shouldDumpDisassembly())
2149             dataLog(&quot;    Did count reoptimization for &quot;, *this, &quot;\n&quot;);
2150     }
2151 
2152     if (this != replacement()) {
2153         // This means that we were never the entrypoint. This can happen for OSR entry code
2154         // blocks.
2155         return;
2156     }
2157 
2158     if (alternative())
2159         alternative()-&gt;optimizeAfterWarmUp();
2160 
2161     if (reason != Profiler::JettisonDueToOldAge &amp;&amp; reason != Profiler::JettisonDueToVMTraps)
2162         tallyFrequentExitSites();
2163 #endif // ENABLE(DFG_JIT)
2164 
2165     // Jettison can happen during GC. We don&#39;t want to install code to a dead executable
2166     // because that would add a dead object to the remembered set.
2167     if (vm.heap.isCurrentThreadBusy() &amp;&amp; !vm.heap.isMarked(ownerExecutable()))
2168         return;
2169 
2170 #if ENABLE(JIT)
2171     {
2172         ConcurrentJSLocker locker(m_lock);
2173         if (JITData* jitData = m_jitData.get()) {
2174             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
2175                 callLinkInfo-&gt;setClearedByJettison();
2176         }
2177     }
2178 #endif
2179 
2180     // This accomplishes (2).
2181     ownerExecutable()-&gt;installCode(vm, alternative(), codeType(), specializationKind());
2182 
2183 #if ENABLE(DFG_JIT)
2184     if (DFG::shouldDumpDisassembly())
2185         dataLog(&quot;    Did install baseline version of &quot;, *this, &quot;\n&quot;);
2186 #endif // ENABLE(DFG_JIT)
2187 }
2188 
2189 JSGlobalObject* CodeBlock::globalObjectFor(CodeOrigin codeOrigin)
2190 {
2191     auto* inlineCallFrame = codeOrigin.inlineCallFrame();
2192     if (!inlineCallFrame)
2193         return globalObject();
2194     return inlineCallFrame-&gt;baselineCodeBlock-&gt;globalObject();
2195 }
2196 
2197 class RecursionCheckFunctor {
2198 public:
2199     RecursionCheckFunctor(CallFrame* startCallFrame, CodeBlock* codeBlock, unsigned depthToCheck)
2200         : m_startCallFrame(startCallFrame)
2201         , m_codeBlock(codeBlock)
2202         , m_depthToCheck(depthToCheck)
2203         , m_foundStartCallFrame(false)
2204         , m_didRecurse(false)
2205     { }
2206 
2207     StackVisitor::Status operator()(StackVisitor&amp; visitor) const
2208     {
2209         CallFrame* currentCallFrame = visitor-&gt;callFrame();
2210 
2211         if (currentCallFrame == m_startCallFrame)
2212             m_foundStartCallFrame = true;
2213 
2214         if (m_foundStartCallFrame) {
2215             if (visitor-&gt;callFrame()-&gt;codeBlock() == m_codeBlock) {
2216                 m_didRecurse = true;
2217                 return StackVisitor::Done;
2218             }
2219 
2220             if (!m_depthToCheck--)
2221                 return StackVisitor::Done;
2222         }
2223 
2224         return StackVisitor::Continue;
2225     }
2226 
2227     bool didRecurse() const { return m_didRecurse; }
2228 
2229 private:
2230     CallFrame* m_startCallFrame;
2231     CodeBlock* m_codeBlock;
2232     mutable unsigned m_depthToCheck;
2233     mutable bool m_foundStartCallFrame;
2234     mutable bool m_didRecurse;
2235 };
2236 
<a name="113" id="anc113"></a><span class="line-modified">2237 void CodeBlock::noticeIncomingCall(CallFrame* callerFrame)</span>
2238 {
2239     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
2240 
<a name="114" id="anc114"></a><span class="line-modified">2241     dataLogLnIf(Options::verboseCallLink(), &quot;Noticing call link from &quot;, pointerDump(callerCodeBlock), &quot; to &quot;, *this);</span>

2242 
2243 #if ENABLE(DFG_JIT)
2244     if (!m_shouldAlwaysBeInlined)
2245         return;
2246 
2247     if (!callerCodeBlock) {
2248         m_shouldAlwaysBeInlined = false;
<a name="115" id="anc115"></a><span class="line-modified">2249         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is native.&quot;);</span>

2250         return;
2251     }
2252 
2253     if (!hasBaselineJITProfiling())
2254         return;
2255 
2256     if (!DFG::mightInlineFunction(this))
2257         return;
2258 
2259     if (!canInline(capabilityLevelState()))
2260         return;
2261 
2262     if (!DFG::isSmallEnoughToInlineCodeInto(callerCodeBlock)) {
2263         m_shouldAlwaysBeInlined = false;
<a name="116" id="anc116"></a><span class="line-modified">2264         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is too large.&quot;);</span>

2265         return;
2266     }
2267 
2268     if (callerCodeBlock-&gt;jitType() == JITType::InterpreterThunk) {
2269         // If the caller is still in the interpreter, then we can&#39;t expect inlining to
2270         // happen anytime soon. Assume it&#39;s profitable to optimize it separately. This
2271         // ensures that a function is SABI only if it is called no more frequently than
2272         // any of its callers.
2273         m_shouldAlwaysBeInlined = false;
<a name="117" id="anc117"></a><span class="line-modified">2274         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is in LLInt.&quot;);</span>

2275         return;
2276     }
2277 
2278     if (JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType())) {
2279         m_shouldAlwaysBeInlined = false;
<a name="118" id="anc118"></a><span class="line-modified">2280         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI bcause caller was already optimized.&quot;);</span>

2281         return;
2282     }
2283 
2284     if (callerCodeBlock-&gt;codeType() != FunctionCode) {
2285         // If the caller is either eval or global code, assume that that won&#39;t be
2286         // optimized anytime soon. For eval code this is particularly true since we
2287         // delay eval optimization by a *lot*.
2288         m_shouldAlwaysBeInlined = false;
<a name="119" id="anc119"></a><span class="line-modified">2289         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because caller is not a function.&quot;);</span>

2290         return;
2291     }
2292 
2293     // Recursive calls won&#39;t be inlined.
2294     RecursionCheckFunctor functor(callerFrame, this, Options::maximumInliningDepth());
<a name="120" id="anc120"></a><span class="line-modified">2295     vm().topCallFrame-&gt;iterate(vm(), functor);</span>
2296 
2297     if (functor.didRecurse()) {
<a name="121" id="anc121"></a><span class="line-modified">2298         dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because recursion was detected.&quot;);</span>

2299         m_shouldAlwaysBeInlined = false;
2300         return;
2301     }
2302 
2303     if (callerCodeBlock-&gt;capabilityLevelState() == DFG::CapabilityLevelNotSet) {
2304         dataLog(&quot;In call from &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot; to &quot;, *this, &quot;: caller&#39;s DFG capability level is not set.\n&quot;);
2305         CRASH();
2306     }
2307 
2308     if (canCompile(callerCodeBlock-&gt;capabilityLevelState()))
2309         return;
2310 
<a name="122" id="anc122"></a><span class="line-modified">2311     dataLogLnIf(Options::verboseCallLink(), &quot;    Clearing SABI because the caller is not a DFG candidate.&quot;);</span>

2312 
2313     m_shouldAlwaysBeInlined = false;
2314 #endif
2315 }
2316 
2317 unsigned CodeBlock::reoptimizationRetryCounter() const
2318 {
2319 #if ENABLE(JIT)
2320     ASSERT(m_reoptimizationRetryCounter &lt;= Options::reoptimizationRetryCounterMax());
2321     return m_reoptimizationRetryCounter;
2322 #else
2323     return 0;
2324 #endif // ENABLE(JIT)
2325 }
2326 
2327 #if !ENABLE(C_LOOP)
2328 const RegisterAtOffsetList* CodeBlock::calleeSaveRegisters() const
2329 {
2330 #if ENABLE(JIT)
2331     if (auto* jitData = m_jitData.get()) {
2332         if (const RegisterAtOffsetList* registers = jitData-&gt;m_calleeSaveRegisters.get())
2333             return registers;
2334     }
2335 #endif
2336     return &amp;RegisterAtOffsetList::llintBaselineCalleeSaveRegisters();
2337 }
2338 
2339 
2340 static size_t roundCalleeSaveSpaceAsVirtualRegisters(size_t calleeSaveRegisters)
2341 {
2342 
2343     return (WTF::roundUpToMultipleOf(sizeof(Register), calleeSaveRegisters * sizeof(CPURegister)) / sizeof(Register));
2344 
2345 }
2346 
2347 size_t CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters()
2348 {
2349     return roundCalleeSaveSpaceAsVirtualRegisters(numberOfLLIntBaselineCalleeSaveRegisters());
2350 }
2351 
2352 size_t CodeBlock::calleeSaveSpaceAsVirtualRegisters()
2353 {
2354     return roundCalleeSaveSpaceAsVirtualRegisters(calleeSaveRegisters()-&gt;size());
2355 }
2356 #endif
2357 
2358 #if ENABLE(JIT)
2359 
2360 void CodeBlock::countReoptimization()
2361 {
2362     m_reoptimizationRetryCounter++;
2363     if (m_reoptimizationRetryCounter &gt; Options::reoptimizationRetryCounterMax())
2364         m_reoptimizationRetryCounter = Options::reoptimizationRetryCounterMax();
2365 }
2366 
2367 unsigned CodeBlock::numberOfDFGCompiles()
2368 {
2369     ASSERT(JITCode::isBaselineCode(jitType()));
2370     if (Options::testTheFTL()) {
2371         if (m_didFailFTLCompilation)
2372             return 1000000;
2373         return (m_hasBeenCompiledWithFTL ? 1 : 0) + m_reoptimizationRetryCounter;
2374     }
2375     CodeBlock* replacement = this-&gt;replacement();
2376     return ((replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType())) ? 1 : 0) + m_reoptimizationRetryCounter;
2377 }
2378 
2379 int32_t CodeBlock::codeTypeThresholdMultiplier() const
2380 {
2381     if (codeType() == EvalCode)
2382         return Options::evalThresholdMultiplier();
2383 
2384     return 1;
2385 }
2386 
2387 double CodeBlock::optimizationThresholdScalingFactor()
2388 {
2389     // This expression arises from doing a least-squares fit of
2390     //
2391     // F[x_] =: a * Sqrt[x + b] + Abs[c * x] + d
2392     //
2393     // against the data points:
2394     //
2395     //    x       F[x_]
2396     //    10       0.9          (smallest reasonable code block)
2397     //   200       1.0          (typical small-ish code block)
2398     //   320       1.2          (something I saw in 3d-cube that I wanted to optimize)
2399     //  1268       5.0          (something I saw in 3d-cube that I didn&#39;t want to optimize)
2400     //  4000       5.5          (random large size, used to cause the function to converge to a shallow curve of some sort)
2401     // 10000       6.0          (similar to above)
2402     //
2403     // I achieve the minimization using the following Mathematica code:
2404     //
2405     // MyFunctionTemplate[x_, a_, b_, c_, d_] := a*Sqrt[x + b] + Abs[c*x] + d
2406     //
2407     // samples = {{10, 0.9}, {200, 1}, {320, 1.2}, {1268, 5}, {4000, 5.5}, {10000, 6}}
2408     //
2409     // solution =
2410     //     Minimize[Plus @@ ((MyFunctionTemplate[#[[1]], a, b, c, d] - #[[2]])^2 &amp; /@ samples),
2411     //         {a, b, c, d}][[2]]
2412     //
2413     // And the code below (to initialize a, b, c, d) is generated by:
2414     //
2415     // Print[&quot;const double &quot; &lt;&gt; ToString[#[[1]]] &lt;&gt; &quot; = &quot; &lt;&gt;
2416     //     If[#[[2]] &lt; 0.00001, &quot;0.0&quot;, ToString[#[[2]]]] &lt;&gt; &quot;;&quot;] &amp; /@ solution
2417     //
2418     // We&#39;ve long known the following to be true:
2419     // - Small code blocks are cheap to optimize and so we should do it sooner rather
2420     //   than later.
2421     // - Large code blocks are expensive to optimize and so we should postpone doing so,
2422     //   and sometimes have a large enough threshold that we never optimize them.
2423     // - The difference in cost is not totally linear because (a) just invoking the
2424     //   DFG incurs some base cost and (b) for large code blocks there is enough slop
2425     //   in the correlation between instruction count and the actual compilation cost
2426     //   that for those large blocks, the instruction count should not have a strong
2427     //   influence on our threshold.
2428     //
2429     // I knew the goals but I didn&#39;t know how to achieve them; so I picked an interesting
2430     // example where the heuristics were right (code block in 3d-cube with instruction
2431     // count 320, which got compiled early as it should have been) and one where they were
2432     // totally wrong (code block in 3d-cube with instruction count 1268, which was expensive
2433     // to compile and didn&#39;t run often enough to warrant compilation in my opinion), and
2434     // then threw in additional data points that represented my own guess of what our
2435     // heuristics should do for some round-numbered examples.
2436     //
2437     // The expression to which I decided to fit the data arose because I started with an
2438     // affine function, and then did two things: put the linear part in an Abs to ensure
2439     // that the fit didn&#39;t end up choosing a negative value of c (which would result in
2440     // the function turning over and going negative for large x) and I threw in a Sqrt
2441     // term because Sqrt represents my intution that the function should be more sensitive
2442     // to small changes in small values of x, but less sensitive when x gets large.
2443 
2444     // Note that the current fit essentially eliminates the linear portion of the
2445     // expression (c == 0.0).
2446     const double a = 0.061504;
2447     const double b = 1.02406;
2448     const double c = 0.0;
2449     const double d = 0.825914;
2450 
2451     double bytecodeCost = this-&gt;bytecodeCost();
2452 
2453     ASSERT(bytecodeCost); // Make sure this is called only after we have an instruction stream; otherwise it&#39;ll just return the value of d, which makes no sense.
2454 
2455     double result = d + a * sqrt(bytecodeCost + b) + c * bytecodeCost;
2456 
2457     result *= codeTypeThresholdMultiplier();
2458 
<a name="123" id="anc123"></a><span class="line-modified">2459     dataLogLnIf(Options::verboseOSR(),</span>
<span class="line-modified">2460         *this, &quot;: bytecode cost is &quot;, bytecodeCost,</span>
<span class="line-modified">2461         &quot;, scaling execution counter by &quot;, result, &quot; * &quot;, codeTypeThresholdMultiplier());</span>



2462     return result;
2463 }
2464 
2465 static int32_t clipThreshold(double threshold)
2466 {
2467     if (threshold &lt; 1.0)
2468         return 1;
2469 
2470     if (threshold &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::max()))
2471         return std::numeric_limits&lt;int32_t&gt;::max();
2472 
2473     return static_cast&lt;int32_t&gt;(threshold);
2474 }
2475 
2476 int32_t CodeBlock::adjustedCounterValue(int32_t desiredThreshold)
2477 {
2478     return clipThreshold(
2479         static_cast&lt;double&gt;(desiredThreshold) *
2480         optimizationThresholdScalingFactor() *
2481         (1 &lt;&lt; reoptimizationRetryCounter()));
2482 }
2483 
2484 bool CodeBlock::checkIfOptimizationThresholdReached()
2485 {
2486 #if ENABLE(DFG_JIT)
2487     if (DFG::Worklist* worklist = DFG::existingGlobalDFGWorklistOrNull()) {
2488         if (worklist-&gt;compilationState(DFG::CompilationKey(this, DFG::DFGMode))
2489             == DFG::Worklist::Compiled) {
2490             optimizeNextInvocation();
2491             return true;
2492         }
2493     }
2494 #endif
2495 
2496     return m_jitExecuteCounter.checkIfThresholdCrossedAndSet(this);
2497 }
2498 
2499 #if ENABLE(DFG_JIT)
2500 auto CodeBlock::updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp; exitState) -&gt; OptimizeAction
2501 {
2502     DFG::OSRExitBase&amp; exit = exitState.exit;
2503     if (!exitKindMayJettison(exit.m_kind)) {
2504         // FIXME: We may want to notice that we&#39;re frequently exiting
2505         // at an op_catch that we didn&#39;t compile an entrypoint for, and
2506         // then trigger a reoptimization of this CodeBlock:
2507         // https://bugs.webkit.org/show_bug.cgi?id=175842
2508         return OptimizeAction::None;
2509     }
2510 
2511     exit.m_count++;
2512     m_osrExitCounter++;
2513 
2514     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
2515     ASSERT(baselineCodeBlock == baselineAlternative());
2516     if (UNLIKELY(baselineCodeBlock-&gt;jitExecuteCounter().hasCrossedThreshold()))
2517         return OptimizeAction::ReoptimizeNow;
2518 
2519     // We want to figure out if there&#39;s a possibility that we&#39;re in a loop. For the outermost
2520     // code block in the inline stack, we handle this appropriately by having the loop OSR trigger
2521     // check the exit count of the replacement of the CodeBlock from which we are OSRing. The
2522     // problem is the inlined functions, which might also have loops, but whose baseline versions
2523     // don&#39;t know where to look for the exit count. Figure out if those loops are severe enough
2524     // that we had tried to OSR enter. If so, then we should use the loop reoptimization trigger.
2525     // Otherwise, we should use the normal reoptimization trigger.
2526 
2527     bool didTryToEnterInLoop = false;
2528     for (InlineCallFrame* inlineCallFrame = exit.m_codeOrigin.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;directCaller.inlineCallFrame()) {
2529         if (inlineCallFrame-&gt;baselineCodeBlock-&gt;ownerExecutable()-&gt;didTryToEnterInLoop()) {
2530             didTryToEnterInLoop = true;
2531             break;
2532         }
2533     }
2534 
2535     uint32_t exitCountThreshold = didTryToEnterInLoop
2536         ? exitCountThresholdForReoptimizationFromLoop()
2537         : exitCountThresholdForReoptimization();
2538 
2539     if (m_osrExitCounter &gt; exitCountThreshold)
2540         return OptimizeAction::ReoptimizeNow;
2541 
2542     // Too few fails. Adjust the execution counter such that the target is to only optimize after a while.
2543     baselineCodeBlock-&gt;m_jitExecuteCounter.setNewThresholdForOSRExit(exitState.activeThreshold, exitState.memoryUsageAdjustedThreshold);
2544     return OptimizeAction::None;
2545 }
2546 #endif
2547 
2548 void CodeBlock::optimizeNextInvocation()
2549 {
<a name="124" id="anc124"></a><span class="line-modified">2550     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing next invocation.&quot;);</span>

2551     m_jitExecuteCounter.setNewThreshold(0, this);
2552 }
2553 
2554 void CodeBlock::dontOptimizeAnytimeSoon()
2555 {
<a name="125" id="anc125"></a><span class="line-modified">2556     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Not optimizing anytime soon.&quot;);</span>

2557     m_jitExecuteCounter.deferIndefinitely();
2558 }
2559 
2560 void CodeBlock::optimizeAfterWarmUp()
2561 {
<a name="126" id="anc126"></a><span class="line-modified">2562     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing after warm-up.&quot;);</span>

2563 #if ENABLE(DFG_JIT)
2564     m_jitExecuteCounter.setNewThreshold(
2565         adjustedCounterValue(Options::thresholdForOptimizeAfterWarmUp()), this);
2566 #endif
2567 }
2568 
2569 void CodeBlock::optimizeAfterLongWarmUp()
2570 {
<a name="127" id="anc127"></a><span class="line-modified">2571     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing after long warm-up.&quot;);</span>

2572 #if ENABLE(DFG_JIT)
2573     m_jitExecuteCounter.setNewThreshold(
2574         adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp()), this);
2575 #endif
2576 }
2577 
2578 void CodeBlock::optimizeSoon()
2579 {
<a name="128" id="anc128"></a><span class="line-modified">2580     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Optimizing soon.&quot;);</span>

2581 #if ENABLE(DFG_JIT)
2582     m_jitExecuteCounter.setNewThreshold(
2583         adjustedCounterValue(Options::thresholdForOptimizeSoon()), this);
2584 #endif
2585 }
2586 
2587 void CodeBlock::forceOptimizationSlowPathConcurrently()
2588 {
<a name="129" id="anc129"></a><span class="line-modified">2589     dataLogLnIf(Options::verboseOSR(), *this, &quot;: Forcing slow path concurrently.&quot;);</span>

2590     m_jitExecuteCounter.forceSlowPathConcurrently();
2591 }
2592 
2593 #if ENABLE(DFG_JIT)
2594 void CodeBlock::setOptimizationThresholdBasedOnCompilationResult(CompilationResult result)
2595 {
2596     JITType type = jitType();
2597     if (type != JITType::BaselineJIT) {
<a name="130" id="anc130"></a><span class="line-modified">2598         dataLogLn(*this, &quot;: expected to have baseline code but have &quot;, type);</span>
2599         CRASH_WITH_INFO(bitwise_cast&lt;uintptr_t&gt;(jitCode().get()), static_cast&lt;uint8_t&gt;(type));
2600     }
2601 
2602     CodeBlock* replacement = this-&gt;replacement();
2603     bool hasReplacement = (replacement &amp;&amp; replacement != this);
2604     if ((result == CompilationSuccessful) != hasReplacement) {
2605         dataLog(*this, &quot;: we have result = &quot;, result, &quot; but &quot;);
2606         if (replacement == this)
2607             dataLog(&quot;we are our own replacement.\n&quot;);
2608         else
2609             dataLog(&quot;our replacement is &quot;, pointerDump(replacement), &quot;\n&quot;);
2610         RELEASE_ASSERT_NOT_REACHED();
2611     }
2612 
2613     switch (result) {
2614     case CompilationSuccessful:
2615         RELEASE_ASSERT(replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType()));
2616         optimizeNextInvocation();
2617         return;
2618     case CompilationFailed:
2619         dontOptimizeAnytimeSoon();
2620         return;
2621     case CompilationDeferred:
2622         // We&#39;d like to do dontOptimizeAnytimeSoon() but we cannot because
2623         // forceOptimizationSlowPathConcurrently() is inherently racy. It won&#39;t
2624         // necessarily guarantee anything. So, we make sure that even if that
2625         // function ends up being a no-op, we still eventually retry and realize
2626         // that we have optimized code ready.
2627         optimizeAfterWarmUp();
2628         return;
2629     case CompilationInvalidated:
2630         // Retry with exponential backoff.
2631         countReoptimization();
2632         optimizeAfterWarmUp();
2633         return;
2634     }
2635 
2636     dataLog(&quot;Unrecognized result: &quot;, static_cast&lt;int&gt;(result), &quot;\n&quot;);
2637     RELEASE_ASSERT_NOT_REACHED();
2638 }
2639 
2640 #endif
2641 
2642 uint32_t CodeBlock::adjustedExitCountThreshold(uint32_t desiredThreshold)
2643 {
2644     ASSERT(JITCode::isOptimizingJIT(jitType()));
2645     // Compute this the lame way so we don&#39;t saturate. This is called infrequently
2646     // enough that this loop won&#39;t hurt us.
2647     unsigned result = desiredThreshold;
2648     for (unsigned n = baselineVersion()-&gt;reoptimizationRetryCounter(); n--;) {
2649         unsigned newResult = result &lt;&lt; 1;
2650         if (newResult &lt; result)
2651             return std::numeric_limits&lt;uint32_t&gt;::max();
2652         result = newResult;
2653     }
2654     return result;
2655 }
2656 
2657 uint32_t CodeBlock::exitCountThresholdForReoptimization()
2658 {
2659     return adjustedExitCountThreshold(Options::osrExitCountForReoptimization() * codeTypeThresholdMultiplier());
2660 }
2661 
2662 uint32_t CodeBlock::exitCountThresholdForReoptimizationFromLoop()
2663 {
2664     return adjustedExitCountThreshold(Options::osrExitCountForReoptimizationFromLoop() * codeTypeThresholdMultiplier());
2665 }
2666 
2667 bool CodeBlock::shouldReoptimizeNow()
2668 {
2669     return osrExitCounter() &gt;= exitCountThresholdForReoptimization();
2670 }
2671 
2672 bool CodeBlock::shouldReoptimizeFromLoopNow()
2673 {
2674     return osrExitCounter() &gt;= exitCountThresholdForReoptimizationFromLoop();
2675 }
2676 #endif
2677 
<a name="131" id="anc131"></a><span class="line-modified">2678 ArrayProfile* CodeBlock::getArrayProfile(const ConcurrentJSLocker&amp;, BytecodeIndex bytecodeIndex)</span>
2679 {
<a name="132" id="anc132"></a><span class="line-modified">2680     auto instruction = instructions().at(bytecodeIndex);</span>
2681     switch (instruction-&gt;opcodeID()) {
2682 #define CASE1(Op) \
2683     case Op::opcodeID: \
2684         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_arrayProfile;
2685 
2686 #define CASE2(Op) \
2687     case Op::opcodeID: \
2688         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_callLinkInfo.m_arrayProfile;
2689 
2690     FOR_EACH_OPCODE_WITH_ARRAY_PROFILE(CASE1)
2691     FOR_EACH_OPCODE_WITH_LLINT_CALL_LINK_INFO(CASE2)
2692 
2693 #undef CASE1
2694 #undef CASE2
2695 
2696     case OpGetById::opcodeID: {
2697         auto bytecode = instruction-&gt;as&lt;OpGetById&gt;();
2698         auto&amp; metadata = bytecode.metadata(this);
2699         if (metadata.m_modeMetadata.mode == GetByIdMode::ArrayLength)
2700             return &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile;
2701         break;
2702     }
2703     default:
2704         break;
2705     }
2706 
2707     return nullptr;
2708 }
2709 
<a name="133" id="anc133"></a><span class="line-modified">2710 ArrayProfile* CodeBlock::getArrayProfile(BytecodeIndex bytecodeIndex)</span>
2711 {
2712     ConcurrentJSLocker locker(m_lock);
<a name="134" id="anc134"></a><span class="line-modified">2713     return getArrayProfile(locker, bytecodeIndex);</span>
2714 }
2715 
2716 #if ENABLE(DFG_JIT)
2717 Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; CodeBlock::codeOrigins()
2718 {
2719     return m_jitCode-&gt;dfgCommon()-&gt;codeOrigins;
2720 }
2721 
2722 size_t CodeBlock::numberOfDFGIdentifiers() const
2723 {
2724     if (!JITCode::isOptimizingJIT(jitType()))
2725         return 0;
2726 
2727     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers.size();
2728 }
2729 
2730 const Identifier&amp; CodeBlock::identifier(int index) const
2731 {
2732     size_t unlinkedIdentifiers = m_unlinkedCode-&gt;numberOfIdentifiers();
2733     if (static_cast&lt;unsigned&gt;(index) &lt; unlinkedIdentifiers)
2734         return m_unlinkedCode-&gt;identifier(index);
2735     ASSERT(JITCode::isOptimizingJIT(jitType()));
2736     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers[index - unlinkedIdentifiers];
2737 }
2738 #endif // ENABLE(DFG_JIT)
2739 
2740 void CodeBlock::updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles)
2741 {
2742     ConcurrentJSLocker locker(m_lock);
2743 
2744     numberOfLiveNonArgumentValueProfiles = 0;
2745     numberOfSamplesInProfiles = 0; // If this divided by ValueProfile::numberOfBuckets equals numberOfValueProfiles() then value profiles are full.
2746 
2747     forEachValueProfile([&amp;](ValueProfile&amp; profile, bool isArgument) {
2748         unsigned numSamples = profile.totalNumberOfSamples();
2749         static_assert(ValueProfile::numberOfBuckets == 1);
2750         if (numSamples &gt; ValueProfile::numberOfBuckets)
2751             numSamples = ValueProfile::numberOfBuckets; // We don&#39;t want profiles that are extremely hot to be given more weight.
2752         numberOfSamplesInProfiles += numSamples;
2753         if (isArgument) {
2754             profile.computeUpdatedPrediction(locker);
2755             return;
2756         }
2757         if (profile.numberOfSamples() || profile.isSampledBefore())
2758             numberOfLiveNonArgumentValueProfiles++;
2759         profile.computeUpdatedPrediction(locker);
2760     });
2761 
2762     if (auto* rareData = m_rareData.get()) {
2763         for (auto&amp; profileBucket : rareData-&gt;m_catchProfiles) {
<a name="135" id="anc135"></a><span class="line-modified">2764             profileBucket-&gt;forEach([&amp;] (ValueProfileAndVirtualRegister&amp; profile) {</span>
2765                 profile.computeUpdatedPrediction(locker);
2766             });
2767         }
2768     }
2769 
2770 #if ENABLE(DFG_JIT)
2771     lazyOperandValueProfiles(locker).computeUpdatedPredictions(locker);
2772 #endif
2773 }
2774 
2775 void CodeBlock::updateAllValueProfilePredictions()
2776 {
2777     unsigned ignoredValue1, ignoredValue2;
2778     updateAllValueProfilePredictionsAndCountLiveness(ignoredValue1, ignoredValue2);
2779 }
2780 
2781 void CodeBlock::updateAllArrayPredictions()
2782 {
2783     ConcurrentJSLocker locker(m_lock);
2784 
2785     forEachArrayProfile([&amp;](ArrayProfile&amp; profile) {
2786         profile.computeUpdatedPrediction(locker, this);
2787     });
2788 
2789     forEachArrayAllocationProfile([&amp;](ArrayAllocationProfile&amp; profile) {
2790         profile.updateProfile();
2791     });
2792 }
2793 
2794 void CodeBlock::updateAllPredictions()
2795 {
2796     updateAllValueProfilePredictions();
2797     updateAllArrayPredictions();
2798 }
2799 
2800 bool CodeBlock::shouldOptimizeNow()
2801 {
<a name="136" id="anc136"></a><span class="line-modified">2802     dataLogLnIf(Options::verboseOSR(), &quot;Considering optimizing &quot;, *this, &quot;...&quot;);</span>

2803 
2804     if (m_optimizationDelayCounter &gt;= Options::maximumOptimizationDelay())
2805         return true;
2806 
2807     updateAllArrayPredictions();
2808 
2809     unsigned numberOfLiveNonArgumentValueProfiles;
2810     unsigned numberOfSamplesInProfiles;
2811     updateAllValueProfilePredictionsAndCountLiveness(numberOfLiveNonArgumentValueProfiles, numberOfSamplesInProfiles);
2812 
2813     if (Options::verboseOSR()) {
2814         dataLogF(
2815             &quot;Profile hotness: %lf (%u / %u), %lf (%u / %u)\n&quot;,
2816             (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles(),
2817             numberOfLiveNonArgumentValueProfiles, numberOfNonArgumentValueProfiles(),
2818             (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / numberOfNonArgumentValueProfiles(),
2819             numberOfSamplesInProfiles, ValueProfile::numberOfBuckets * numberOfNonArgumentValueProfiles());
2820     }
2821 
2822     if ((!numberOfNonArgumentValueProfiles() || (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles() &gt;= Options::desiredProfileLivenessRate())
2823         &amp;&amp; (!totalNumberOfValueProfiles() || (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / totalNumberOfValueProfiles() &gt;= Options::desiredProfileFullnessRate())
2824         &amp;&amp; static_cast&lt;unsigned&gt;(m_optimizationDelayCounter) + 1 &gt;= Options::minimumOptimizationDelay())
2825         return true;
2826 
2827     ASSERT(m_optimizationDelayCounter &lt; std::numeric_limits&lt;uint8_t&gt;::max());
2828     m_optimizationDelayCounter++;
2829     optimizeAfterWarmUp();
2830     return false;
2831 }
2832 
2833 #if ENABLE(DFG_JIT)
2834 void CodeBlock::tallyFrequentExitSites()
2835 {
2836     ASSERT(JITCode::isOptimizingJIT(jitType()));
<a name="137" id="anc137"></a><span class="line-modified">2837     ASSERT(JITCode::isBaselineCode(alternative()-&gt;jitType()));</span>
2838 
2839     CodeBlock* profiledBlock = alternative();
2840 
2841     switch (jitType()) {
2842     case JITType::DFGJIT: {
2843         DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
2844         for (auto&amp; exit : jitCode-&gt;osrExit)
2845             exit.considerAddingAsFrequentExitSite(profiledBlock);
2846         break;
2847     }
2848 
2849 #if ENABLE(FTL_JIT)
2850     case JITType::FTLJIT: {
2851         // There is no easy way to avoid duplicating this code since the FTL::JITCode::osrExit
2852         // vector contains a totally different type, that just so happens to behave like
2853         // DFG::JITCode::osrExit.
2854         FTL::JITCode* jitCode = m_jitCode-&gt;ftl();
2855         for (unsigned i = 0; i &lt; jitCode-&gt;osrExit.size(); ++i) {
2856             FTL::OSRExit&amp; exit = jitCode-&gt;osrExit[i];
2857             exit.considerAddingAsFrequentExitSite(profiledBlock);
2858         }
2859         break;
2860     }
2861 #endif
2862 
2863     default:
2864         RELEASE_ASSERT_NOT_REACHED();
2865         break;
2866     }
2867 }
2868 #endif // ENABLE(DFG_JIT)
2869 
2870 void CodeBlock::notifyLexicalBindingUpdate()
2871 {
2872     // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
2873     // https://bugs.webkit.org/show_bug.cgi?id=193347
2874     if (scriptMode() == JSParserScriptMode::Module)
2875         return;
2876     JSGlobalObject* globalObject = m_globalObject.get();
2877     JSGlobalLexicalEnvironment* globalLexicalEnvironment = jsCast&lt;JSGlobalLexicalEnvironment*&gt;(globalObject-&gt;globalScope());
2878     SymbolTable* symbolTable = globalLexicalEnvironment-&gt;symbolTable();
2879 
2880     ConcurrentJSLocker locker(m_lock);
2881 
2882     auto isShadowed = [&amp;] (UniquedStringImpl* uid) {
2883         ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
2884         return symbolTable-&gt;contains(locker, uid);
2885     };
2886 
2887     const InstructionStream&amp; instructionStream = instructions();
2888     for (const auto&amp; instruction : instructionStream) {
2889         OpcodeID opcodeID = instruction-&gt;opcodeID();
2890         switch (opcodeID) {
2891         case op_resolve_scope: {
2892             auto bytecode = instruction-&gt;as&lt;OpResolveScope&gt;();
2893             auto&amp; metadata = bytecode.metadata(this);
2894             ResolveType originalResolveType = metadata.m_resolveType;
2895             if (originalResolveType == GlobalProperty || originalResolveType == GlobalPropertyWithVarInjectionChecks) {
2896                 const Identifier&amp; ident = identifier(bytecode.m_var);
2897                 if (isShadowed(ident.impl()))
2898                     metadata.m_globalLexicalBindingEpoch = 0;
2899                 else
2900                     metadata.m_globalLexicalBindingEpoch = globalObject-&gt;globalLexicalBindingEpoch();
2901             }
2902             break;
2903         }
2904         default:
2905             break;
2906         }
2907     }
2908 }
2909 
2910 #if ENABLE(VERBOSE_VALUE_PROFILE)
2911 void CodeBlock::dumpValueProfiles()
2912 {
2913     dataLog(&quot;ValueProfile for &quot;, *this, &quot;:\n&quot;);
2914     forEachValueProfile([](ValueProfile&amp; profile, bool isArgument) {
2915         if (isArgument)
2916             dataLogF(&quot;   arg: &quot;);
2917         else
2918             dataLogF(&quot;   bc: &quot;);
2919         if (!profile.numberOfSamples() &amp;&amp; profile.m_prediction == SpecNone) {
2920             dataLogF(&quot;&lt;empty&gt;\n&quot;);
2921             continue;
2922         }
2923         profile.dump(WTF::dataFile());
2924         dataLogF(&quot;\n&quot;);
2925     });
2926     dataLog(&quot;RareCaseProfile for &quot;, *this, &quot;:\n&quot;);
2927     if (auto* jitData = m_jitData.get()) {
2928         for (RareCaseProfile* profile : jitData-&gt;m_rareCaseProfiles)
2929             dataLogF(&quot;   bc = %d: %u\n&quot;, profile-&gt;m_bytecodeOffset, profile-&gt;m_counter);
2930     }
2931 }
2932 #endif // ENABLE(VERBOSE_VALUE_PROFILE)
2933 
2934 unsigned CodeBlock::frameRegisterCount()
2935 {
2936     switch (jitType()) {
2937     case JITType::InterpreterThunk:
2938         return LLInt::frameRegisterCountFor(this);
2939 
2940 #if ENABLE(JIT)
2941     case JITType::BaselineJIT:
2942         return JIT::frameRegisterCountFor(this);
2943 #endif // ENABLE(JIT)
2944 
2945 #if ENABLE(DFG_JIT)
2946     case JITType::DFGJIT:
2947     case JITType::FTLJIT:
2948         return jitCode()-&gt;dfgCommon()-&gt;frameRegisterCount;
2949 #endif // ENABLE(DFG_JIT)
2950 
2951     default:
2952         RELEASE_ASSERT_NOT_REACHED();
2953         return 0;
2954     }
2955 }
2956 
2957 int CodeBlock::stackPointerOffset()
2958 {
2959     return virtualRegisterForLocal(frameRegisterCount() - 1).offset();
2960 }
2961 
2962 size_t CodeBlock::predictedMachineCodeSize()
2963 {
2964     VM* vm = m_vm;
2965     // This will be called from CodeBlock::CodeBlock before either m_vm or the
2966     // instructions have been initialized. It&#39;s OK to return 0 because what will really
2967     // matter is the recomputation of this value when the slow path is triggered.
2968     if (!vm)
2969         return 0;
2970 
2971     if (!*vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT)
2972         return 0; // It&#39;s as good of a prediction as we&#39;ll get.
2973 
2974     // Be conservative: return a size that will be an overestimation 84% of the time.
2975     double multiplier = vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;mean() +
2976         vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;standardDeviation();
2977 
2978     // Be paranoid: silently reject bogus multipiers. Silently doing the &quot;wrong&quot; thing
2979     // here is OK, since this whole method is just a heuristic.
2980     if (multiplier &lt; 0 || multiplier &gt; 1000)
2981         return 0;
2982 
2983     double doubleResult = multiplier * bytecodeCost();
2984 
2985     // Be even more paranoid: silently reject values that won&#39;t fit into a size_t. If
2986     // the function is so huge that we can&#39;t even fit it into virtual memory then we
2987     // should probably have some other guards in place to prevent us from even getting
2988     // to this point.
2989     if (doubleResult &gt; std::numeric_limits&lt;size_t&gt;::max())
2990         return 0;
2991 
2992     return static_cast&lt;size_t&gt;(doubleResult);
2993 }
2994 
2995 String CodeBlock::nameForRegister(VirtualRegister virtualRegister)
2996 {
2997     for (auto&amp; constantRegister : m_constantRegisters) {
2998         if (constantRegister.get().isEmpty())
2999             continue;
3000         if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm(), constantRegister.get())) {
3001             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
3002             auto end = symbolTable-&gt;end(locker);
3003             for (auto ptr = symbolTable-&gt;begin(locker); ptr != end; ++ptr) {
3004                 if (ptr-&gt;value.varOffset() == VarOffset(virtualRegister)) {
3005                     // FIXME: This won&#39;t work from the compilation thread.
3006                     // https://bugs.webkit.org/show_bug.cgi?id=115300
3007                     return ptr-&gt;key.get();
3008                 }
3009             }
3010         }
3011     }
3012     if (virtualRegister == thisRegister())
3013         return &quot;this&quot;_s;
3014     if (virtualRegister.isArgument())
3015         return makeString(&quot;arguments[&quot;, pad(&#39; &#39;, 3, virtualRegister.toArgument()), &#39;]&#39;);
3016 
3017     return emptyString();
3018 }
3019 
<a name="138" id="anc138"></a><span class="line-modified">3020 ValueProfile* CodeBlock::tryGetValueProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
3021 {
<a name="139" id="anc139"></a><span class="line-modified">3022     auto instruction = instructions().at(bytecodeIndex);</span>
3023     switch (instruction-&gt;opcodeID()) {
3024 
3025 #define CASE(Op) \
3026     case Op::opcodeID: \
3027         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_profile;
3028 
3029         FOR_EACH_OPCODE_WITH_VALUE_PROFILE(CASE)
3030 
3031 #undef CASE
3032 
3033     default:
3034         return nullptr;
3035 
3036     }
3037 }
3038 
<a name="140" id="anc140"></a><span class="line-modified">3039 SpeculatedType CodeBlock::valueProfilePredictionForBytecodeIndex(const ConcurrentJSLocker&amp; locker, BytecodeIndex bytecodeIndex)</span>
3040 {
<a name="141" id="anc141"></a><span class="line-modified">3041     if (ValueProfile* valueProfile = tryGetValueProfileForBytecodeIndex(bytecodeIndex))</span>
3042         return valueProfile-&gt;computeUpdatedPrediction(locker);
3043     return SpecNone;
3044 }
3045 
<a name="142" id="anc142"></a><span class="line-modified">3046 ValueProfile&amp; CodeBlock::valueProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
3047 {
<a name="143" id="anc143"></a><span class="line-modified">3048     return *tryGetValueProfileForBytecodeIndex(bytecodeIndex);</span>
3049 }
3050 
3051 void CodeBlock::validate()
3052 {
3053     BytecodeLivenessAnalysis liveness(this); // Compute directly from scratch so it doesn&#39;t effect CodeBlock footprint.
3054 
<a name="144" id="anc144"></a><span class="line-modified">3055     FastBitVector liveAtHead = liveness.getLivenessInfoAtBytecodeIndex(this, BytecodeIndex(0));</span>
3056 
3057     if (liveAtHead.numBits() != static_cast&lt;size_t&gt;(m_numCalleeLocals)) {
3058         beginValidationDidFail();
3059         dataLog(&quot;    Wrong number of bits in result!\n&quot;);
3060         dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3061         dataLog(&quot;    Bit count: &quot;, liveAtHead.numBits(), &quot;\n&quot;);
3062         endValidationDidFail();
3063     }
3064 
3065     for (unsigned i = m_numCalleeLocals; i--;) {
3066         VirtualRegister reg = virtualRegisterForLocal(i);
3067 
3068         if (liveAtHead[i]) {
3069             beginValidationDidFail();
3070             dataLog(&quot;    Variable &quot;, reg, &quot; is expected to be dead.\n&quot;);
3071             dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3072             endValidationDidFail();
3073         }
3074     }
3075 
3076     const InstructionStream&amp; instructionStream = instructions();
3077     for (const auto&amp; instruction : instructionStream) {
3078         OpcodeID opcode = instruction-&gt;opcodeID();
<a name="145" id="anc145"></a><span class="line-modified">3079         if (!!baselineAlternative()-&gt;handlerForBytecodeIndex(BytecodeIndex(instruction.offset()))) {</span>
3080             if (opcode == op_catch || opcode == op_enter) {
3081                 // op_catch/op_enter logically represent an entrypoint. Entrypoints are not allowed to be
3082                 // inside of a try block because they are responsible for bootstrapping state. And they
3083                 // are never allowed throw an exception because of this. We rely on this when compiling
3084                 // in the DFG. Because an entrypoint never throws, the bytecode generator will never
3085                 // allow once inside a try block.
3086                 beginValidationDidFail();
3087                 dataLog(&quot;    entrypoint not allowed inside a try block.&quot;);
3088                 endValidationDidFail();
3089             }
3090         }
3091     }
3092 }
3093 
3094 void CodeBlock::beginValidationDidFail()
3095 {
3096     dataLog(&quot;Validation failure in &quot;, *this, &quot;:\n&quot;);
3097     dataLog(&quot;\n&quot;);
3098 }
3099 
3100 void CodeBlock::endValidationDidFail()
3101 {
3102     dataLog(&quot;\n&quot;);
3103     dumpBytecode();
3104     dataLog(&quot;\n&quot;);
3105     dataLog(&quot;Validation failure.\n&quot;);
3106     RELEASE_ASSERT_NOT_REACHED();
3107 }
3108 
3109 void CodeBlock::addBreakpoint(unsigned numBreakpoints)
3110 {
3111     m_numBreakpoints += numBreakpoints;
3112     ASSERT(m_numBreakpoints);
3113     if (JITCode::isOptimizingJIT(jitType()))
3114         jettison(Profiler::JettisonDueToDebuggerBreakpoint);
3115 }
3116 
3117 void CodeBlock::setSteppingMode(CodeBlock::SteppingMode mode)
3118 {
3119     m_steppingMode = mode;
3120     if (mode == SteppingModeEnabled &amp;&amp; JITCode::isOptimizingJIT(jitType()))
3121         jettison(Profiler::JettisonDueToDebuggerStepping);
3122 }
3123 
3124 int CodeBlock::outOfLineJumpOffset(const Instruction* pc)
3125 {
3126     int offset = bytecodeOffset(pc);
3127     return m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3128 }
3129 
3130 const Instruction* CodeBlock::outOfLineJumpTarget(const Instruction* pc)
3131 {
3132     int offset = bytecodeOffset(pc);
3133     int target = m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3134     return instructions().at(offset + target).ptr();
3135 }
3136 
<a name="146" id="anc146"></a><span class="line-modified">3137 BinaryArithProfile* CodeBlock::binaryArithProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
<span class="line-added">3138 {</span>
<span class="line-added">3139     return binaryArithProfileForPC(instructions().at(bytecodeIndex.offset()).ptr());</span>
<span class="line-added">3140 }</span>
<span class="line-added">3141 </span>
<span class="line-added">3142 UnaryArithProfile* CodeBlock::unaryArithProfileForBytecodeIndex(BytecodeIndex bytecodeIndex)</span>
3143 {
<a name="147" id="anc147"></a><span class="line-modified">3144     return unaryArithProfileForPC(instructions().at(bytecodeIndex.offset()).ptr());</span>
3145 }
3146 
<a name="148" id="anc148"></a><span class="line-modified">3147 BinaryArithProfile* CodeBlock::binaryArithProfileForPC(const Instruction* pc)</span>
3148 {
3149     switch (pc-&gt;opcodeID()) {
<a name="149" id="anc149"></a>

3150     case op_add:
3151         return &amp;pc-&gt;as&lt;OpAdd&gt;().metadata(this).m_arithProfile;
3152     case op_mul:
3153         return &amp;pc-&gt;as&lt;OpMul&gt;().metadata(this).m_arithProfile;
3154     case op_sub:
3155         return &amp;pc-&gt;as&lt;OpSub&gt;().metadata(this).m_arithProfile;
3156     case op_div:
3157         return &amp;pc-&gt;as&lt;OpDiv&gt;().metadata(this).m_arithProfile;
3158     default:
3159         break;
3160     }
3161 
3162     return nullptr;
3163 }
3164 
<a name="150" id="anc150"></a><span class="line-modified">3165 UnaryArithProfile* CodeBlock::unaryArithProfileForPC(const Instruction* pc)</span>
<span class="line-added">3166 {</span>
<span class="line-added">3167     switch (pc-&gt;opcodeID()) {</span>
<span class="line-added">3168     case op_negate:</span>
<span class="line-added">3169         return &amp;pc-&gt;as&lt;OpNegate&gt;().metadata(this).m_arithProfile;</span>
<span class="line-added">3170     case op_inc:</span>
<span class="line-added">3171         return &amp;pc-&gt;as&lt;OpInc&gt;().metadata(this).m_arithProfile;</span>
<span class="line-added">3172     case op_dec:</span>
<span class="line-added">3173         return &amp;pc-&gt;as&lt;OpDec&gt;().metadata(this).m_arithProfile;</span>
<span class="line-added">3174     default:</span>
<span class="line-added">3175         break;</span>
<span class="line-added">3176     }</span>
<span class="line-added">3177 </span>
<span class="line-added">3178     return nullptr;</span>
<span class="line-added">3179 }</span>
<span class="line-added">3180 </span>
<span class="line-added">3181 bool CodeBlock::couldTakeSpecialArithFastCase(BytecodeIndex bytecodeIndex)</span>
3182 {
3183     if (!hasBaselineJITProfiling())
3184         return false;
<a name="151" id="anc151"></a><span class="line-modified">3185     BinaryArithProfile* profile = binaryArithProfileForBytecodeIndex(bytecodeIndex);</span>
3186     if (!profile)
3187         return false;
3188     return profile-&gt;tookSpecialFastPath();
3189 }
3190 
3191 #if ENABLE(JIT)
3192 DFG::CapabilityLevel CodeBlock::capabilityLevel()
3193 {
3194     DFG::CapabilityLevel result = computeCapabilityLevel();
3195     m_capabilityLevelState = result;
3196     return result;
3197 }
3198 #endif
3199 
3200 void CodeBlock::insertBasicBlockBoundariesForControlFlowProfiler()
3201 {
3202     if (!unlinkedCodeBlock()-&gt;hasOpProfileControlFlowBytecodeOffsets())
3203         return;
<a name="152" id="anc152"></a><span class="line-modified">3204     const RefCountedArray&lt;InstructionStream::Offset&gt;&amp; bytecodeOffsets = unlinkedCodeBlock()-&gt;opProfileControlFlowBytecodeOffsets();</span>
3205     for (size_t i = 0, offsetsLength = bytecodeOffsets.size(); i &lt; offsetsLength; i++) {
3206         // Because op_profile_control_flow is emitted at the beginning of every basic block, finding
3207         // the next op_profile_control_flow will give us the text range of a single basic block.
3208         size_t startIdx = bytecodeOffsets[i];
3209         auto instruction = instructions().at(startIdx);
3210         RELEASE_ASSERT(instruction-&gt;opcodeID() == op_profile_control_flow);
3211         auto bytecode = instruction-&gt;as&lt;OpProfileControlFlow&gt;();
3212         auto&amp; metadata = bytecode.metadata(this);
3213         int basicBlockStartOffset = bytecode.m_textOffset;
3214         int basicBlockEndOffset;
3215         if (i + 1 &lt; offsetsLength) {
3216             size_t endIdx = bytecodeOffsets[i + 1];
3217             auto endInstruction = instructions().at(endIdx);
3218             RELEASE_ASSERT(endInstruction-&gt;opcodeID() == op_profile_control_flow);
3219             basicBlockEndOffset = endInstruction-&gt;as&lt;OpProfileControlFlow&gt;().m_textOffset - 1;
3220         } else {
3221             basicBlockEndOffset = sourceOffset() + ownerExecutable()-&gt;source().length() - 1; // Offset before the closing brace.
3222             basicBlockStartOffset = std::min(basicBlockStartOffset, basicBlockEndOffset); // Some start offsets may be at the closing brace, ensure it is the offset before.
3223         }
3224 
3225         // The following check allows for the same textual JavaScript basic block to have its bytecode emitted more
3226         // than once and still play nice with the control flow profiler. When basicBlockStartOffset is larger than
3227         // basicBlockEndOffset, it indicates that the bytecode generator has emitted code for the same AST node
3228         // more than once (for example: ForInNode, Finally blocks in TryNode, etc). Though these are different
3229         // basic blocks at the bytecode level, they are generated from the same textual basic block in the JavaScript
3230         // program. The condition:
3231         // (basicBlockEndOffset &lt; basicBlockStartOffset)
3232         // is encountered when op_profile_control_flow lies across the boundary of these duplicated bytecode basic
3233         // blocks and the textual offset goes from the end of the duplicated block back to the beginning. These
3234         // ranges are dummy ranges and are ignored. The duplicated bytecode basic blocks point to the same
3235         // internal data structure, so if any of them execute, it will record the same textual basic block in the
3236         // JavaScript program as executing.
3237         // At the bytecode level, this situation looks like:
3238         // j: op_profile_control_flow (from j-&gt;k, we have basicBlockEndOffset &lt; basicBlockStartOffset)
3239         // ...
3240         // k: op_profile_control_flow (we want to skip over the j-&gt;k block and start fresh at offset k as the start of a new basic block k-&gt;m).
3241         // ...
3242         // m: op_profile_control_flow
3243         if (basicBlockEndOffset &lt; basicBlockStartOffset) {
3244             RELEASE_ASSERT(i + 1 &lt; offsetsLength); // We should never encounter dummy blocks at the end of a CodeBlock.
3245             metadata.m_basicBlockLocation = vm().controlFlowProfiler()-&gt;dummyBasicBlock();
3246             continue;
3247         }
3248 
3249         BasicBlockLocation* basicBlockLocation = vm().controlFlowProfiler()-&gt;getBasicBlockLocation(ownerExecutable()-&gt;sourceID(), basicBlockStartOffset, basicBlockEndOffset);
3250 
3251         // Find all functions that are enclosed within the range: [basicBlockStartOffset, basicBlockEndOffset]
3252         // and insert these functions&#39; start/end offsets as gaps in the current BasicBlockLocation.
3253         // This is necessary because in the original source text of a JavaScript program,
3254         // function literals form new basic blocks boundaries, but they aren&#39;t represented
3255         // inside the CodeBlock&#39;s instruction stream.
3256         auto insertFunctionGaps = [basicBlockLocation, basicBlockStartOffset, basicBlockEndOffset] (const WriteBarrier&lt;FunctionExecutable&gt;&amp; functionExecutable) {
3257             const UnlinkedFunctionExecutable* executable = functionExecutable-&gt;unlinkedExecutable();
3258             int functionStart = executable-&gt;typeProfilingStartOffset();
3259             int functionEnd = executable-&gt;typeProfilingEndOffset();
3260             if (functionStart &gt;= basicBlockStartOffset &amp;&amp; functionEnd &lt;= basicBlockEndOffset)
3261                 basicBlockLocation-&gt;insertGap(functionStart, functionEnd);
3262         };
3263 
3264         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionDecls)
3265             insertFunctionGaps(executable);
3266         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionExprs)
3267             insertFunctionGaps(executable);
3268 
3269         metadata.m_basicBlockLocation = basicBlockLocation;
3270     }
3271 }
3272 
3273 #if ENABLE(JIT)
3274 void CodeBlock::setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp; map)
3275 {
3276     ConcurrentJSLocker locker(m_lock);
3277     ensureJITData(locker).m_pcToCodeOriginMap = WTFMove(map);
3278 }
3279 
3280 Optional&lt;CodeOrigin&gt; CodeBlock::findPC(void* pc)
3281 {
3282     {
3283         ConcurrentJSLocker locker(m_lock);
3284         if (auto* jitData = m_jitData.get()) {
3285             if (jitData-&gt;m_pcToCodeOriginMap) {
3286                 if (Optional&lt;CodeOrigin&gt; codeOrigin = jitData-&gt;m_pcToCodeOriginMap-&gt;findPC(pc))
3287                     return codeOrigin;
3288             }
3289 
3290             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
3291                 if (stubInfo-&gt;containsPC(pc))
3292                     return Optional&lt;CodeOrigin&gt;(stubInfo-&gt;codeOrigin);
3293             }
3294         }
3295     }
3296 
3297     if (Optional&lt;CodeOrigin&gt; codeOrigin = m_jitCode-&gt;findPC(this, pc))
3298         return codeOrigin;
3299 
3300     return WTF::nullopt;
3301 }
3302 #endif // ENABLE(JIT)
3303 
<a name="153" id="anc153"></a><span class="line-modified">3304 Optional&lt;BytecodeIndex&gt; CodeBlock::bytecodeIndexFromCallSiteIndex(CallSiteIndex callSiteIndex)</span>
3305 {
<a name="154" id="anc154"></a><span class="line-modified">3306     Optional&lt;BytecodeIndex&gt; bytecodeIndex;</span>
3307     JITType jitType = this-&gt;jitType();
<a name="155" id="anc155"></a><span class="line-modified">3308     if (jitType == JITType::InterpreterThunk || jitType == JITType::BaselineJIT)</span>
<span class="line-modified">3309         bytecodeIndex = callSiteIndex.bytecodeIndex();</span>
<span class="line-modified">3310     else if (jitType == JITType::DFGJIT || jitType == JITType::FTLJIT) {</span>





3311 #if ENABLE(DFG_JIT)
3312         RELEASE_ASSERT(canGetCodeOrigin(callSiteIndex));
3313         CodeOrigin origin = codeOrigin(callSiteIndex);
<a name="156" id="anc156"></a><span class="line-modified">3314         bytecodeIndex = origin.bytecodeIndex();</span>
3315 #else
3316         RELEASE_ASSERT_NOT_REACHED();
3317 #endif
3318     }
3319 
<a name="157" id="anc157"></a><span class="line-modified">3320     return bytecodeIndex;</span>
3321 }
3322 
3323 int32_t CodeBlock::thresholdForJIT(int32_t threshold)
3324 {
3325     switch (unlinkedCodeBlock()-&gt;didOptimize()) {
3326     case MixedTriState:
3327         return threshold;
3328     case FalseTriState:
3329         return threshold * 4;
3330     case TrueTriState:
3331         return threshold / 2;
3332     }
3333     ASSERT_NOT_REACHED();
3334     return threshold;
3335 }
3336 
3337 void CodeBlock::jitAfterWarmUp()
3338 {
3339     m_llintExecuteCounter.setNewThreshold(thresholdForJIT(Options::thresholdForJITAfterWarmUp()), this);
3340 }
3341 
3342 void CodeBlock::jitSoon()
3343 {
3344     m_llintExecuteCounter.setNewThreshold(thresholdForJIT(Options::thresholdForJITSoon()), this);
3345 }
3346 
3347 bool CodeBlock::hasInstalledVMTrapBreakpoints() const
3348 {
3349 #if ENABLE(SIGNAL_BASED_VM_TRAPS)
3350     // This function may be called from a signal handler. We need to be
3351     // careful to not call anything that is not signal handler safe, e.g.
3352     // we should not perturb the refCount of m_jitCode.
3353     if (!JITCode::isOptimizingJIT(jitType()))
3354         return false;
3355     return m_jitCode-&gt;dfgCommon()-&gt;hasInstalledVMTrapsBreakpoints();
3356 #else
3357     return false;
3358 #endif
3359 }
3360 
3361 bool CodeBlock::installVMTrapBreakpoints()
3362 {
3363 #if ENABLE(SIGNAL_BASED_VM_TRAPS)
3364     // This function may be called from a signal handler. We need to be
3365     // careful to not call anything that is not signal handler safe, e.g.
3366     // we should not perturb the refCount of m_jitCode.
3367     if (!JITCode::isOptimizingJIT(jitType()))
3368         return false;
3369     auto&amp; commonData = *m_jitCode-&gt;dfgCommon();
3370     commonData.installVMTrapBreakpoints(this);
3371     return true;
3372 #else
3373     UNREACHABLE_FOR_PLATFORM();
3374     return false;
3375 #endif
3376 }
3377 
3378 void CodeBlock::dumpMathICStats()
3379 {
3380 #if ENABLE(MATH_IC_STATS)
3381     double numAdds = 0.0;
3382     double totalAddSize = 0.0;
3383     double numMuls = 0.0;
3384     double totalMulSize = 0.0;
3385     double numNegs = 0.0;
3386     double totalNegSize = 0.0;
3387     double numSubs = 0.0;
3388     double totalSubSize = 0.0;
3389 
3390     auto countICs = [&amp;] (CodeBlock* codeBlock) {
3391         if (auto* jitData = codeBlock-&gt;m_jitData.get()) {
3392             for (JITAddIC* addIC : jitData-&gt;m_addICs) {
3393                 numAdds++;
3394                 totalAddSize += addIC-&gt;codeSize();
3395             }
3396 
3397             for (JITMulIC* mulIC : jitData-&gt;m_mulICs) {
3398                 numMuls++;
3399                 totalMulSize += mulIC-&gt;codeSize();
3400             }
3401 
3402             for (JITNegIC* negIC : jitData-&gt;m_negICs) {
3403                 numNegs++;
3404                 totalNegSize += negIC-&gt;codeSize();
3405             }
3406 
3407             for (JITSubIC* subIC : jitData-&gt;m_subICs) {
3408                 numSubs++;
3409                 totalSubSize += subIC-&gt;codeSize();
3410             }
3411         }
3412     };
3413     heap()-&gt;forEachCodeBlock(countICs);
3414 
3415     dataLog(&quot;Num Adds: &quot;, numAdds, &quot;\n&quot;);
3416     dataLog(&quot;Total Add size in bytes: &quot;, totalAddSize, &quot;\n&quot;);
3417     dataLog(&quot;Average Add size: &quot;, totalAddSize / numAdds, &quot;\n&quot;);
3418     dataLog(&quot;\n&quot;);
3419     dataLog(&quot;Num Muls: &quot;, numMuls, &quot;\n&quot;);
3420     dataLog(&quot;Total Mul size in bytes: &quot;, totalMulSize, &quot;\n&quot;);
3421     dataLog(&quot;Average Mul size: &quot;, totalMulSize / numMuls, &quot;\n&quot;);
3422     dataLog(&quot;\n&quot;);
3423     dataLog(&quot;Num Negs: &quot;, numNegs, &quot;\n&quot;);
3424     dataLog(&quot;Total Neg size in bytes: &quot;, totalNegSize, &quot;\n&quot;);
3425     dataLog(&quot;Average Neg size: &quot;, totalNegSize / numNegs, &quot;\n&quot;);
3426     dataLog(&quot;\n&quot;);
3427     dataLog(&quot;Num Subs: &quot;, numSubs, &quot;\n&quot;);
3428     dataLog(&quot;Total Sub size in bytes: &quot;, totalSubSize, &quot;\n&quot;);
3429     dataLog(&quot;Average Sub size: &quot;, totalSubSize / numSubs, &quot;\n&quot;);
3430 
3431     dataLog(&quot;-----------------------\n&quot;);
3432 #endif
3433 }
3434 
3435 void setPrinter(Printer::PrintRecord&amp; record, CodeBlock* codeBlock)
3436 {
3437     Printer::setPrinter(record, toCString(codeBlock));
3438 }
3439 
3440 } // namespace JSC
3441 
3442 namespace WTF {
3443 
3444 void printInternal(PrintStream&amp; out, JSC::CodeBlock* codeBlock)
3445 {
3446     if (UNLIKELY(!codeBlock)) {
3447         out.print(&quot;&lt;null codeBlock&gt;&quot;);
3448         return;
3449     }
3450     out.print(*codeBlock);
3451 }
3452 
3453 } // namespace WTF
<a name="158" id="anc158"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="158" type="hidden" />
</body>
</html>