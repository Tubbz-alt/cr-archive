<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGCPSRethreadingPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGCallArrayAllocatorSlowPathGenerator.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
 31 #include &quot;DFGAbstractHeap.h&quot;
 32 #include &quot;DFGBlockMapInlines.h&quot;
 33 #include &quot;DFGClobberSet.h&quot;
 34 #include &quot;DFGClobberize.h&quot;
 35 #include &quot;DFGDominators.h&quot;
 36 #include &quot;DFGGraph.h&quot;
 37 #include &quot;DFGPhase.h&quot;
 38 #include &quot;JSCInlines.h&quot;
 39 #include &lt;array&gt;
 40 
 41 namespace JSC { namespace DFG {
 42 
 43 // This file contains two CSE implementations: local and global. LocalCSE typically runs when we&#39;re
 44 // in DFG mode, i.e. we want to compile quickly. LocalCSE contains a lot of optimizations for
 45 // compile time. GlobalCSE, on the other hand, is fairly straight-forward. It will find more
 46 // optimization opportunities by virtue of being global.
 47 
 48 namespace {
 49 
 50 namespace DFGCSEPhaseInternal {
<span class="line-modified"> 51 static const bool verbose = false;</span>
 52 }
 53 
 54 class ImpureDataSlot {
 55     WTF_MAKE_NONCOPYABLE(ImpureDataSlot);
 56     WTF_MAKE_FAST_ALLOCATED;
 57 public:
 58     ImpureDataSlot(HeapLocation key, LazyNode value, unsigned hash)
 59         : key(key), value(value), hash(hash)
 60     { }
 61 
 62     HeapLocation key;
 63     LazyNode value;
 64     unsigned hash;
 65 };
 66 
 67 struct ImpureDataSlotHash : public DefaultHash&lt;std::unique_ptr&lt;ImpureDataSlot&gt;&gt;::Hash {
 68     static unsigned hash(const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; key)
 69     {
 70         return key-&gt;hash;
 71     }
</pre>
<hr />
<pre>
130     LazyNode get(const HeapLocation&amp; location) const
131     {
132         LazyNode result = getImpl(location);
133 #if !defined(NDEBUG)
134         ASSERT(result == m_debugImpureData.get(location));
135 #endif
136         return result;
137     }
138 
139     void clobber(AbstractHeap heap, bool clobberConservatively)
140     {
141         switch (heap.kind()) {
142         case World: {
143             clear();
144             break;
145         }
146         case SideState:
147             break;
148         case Stack: {
149             ASSERT(!heap.payload().isTop());
<span class="line-modified">150             ASSERT(heap.payload().value() == heap.payload().value32());</span>
<span class="line-removed">151             m_abstractHeapStackMap.remove(heap.payload().value32());</span>
152             if (clobberConservatively)
153                 m_fallbackStackMap.clear();
154             else
155                 clobber(m_fallbackStackMap, heap);
156             break;
157         }
158         default:
159             if (clobberConservatively)
160                 m_heapMap.clear();
161             else
162                 clobber(m_heapMap, heap);
163             break;
164         }
165 #if !defined(NDEBUG)
166         m_debugImpureData.removeIf([heap, clobberConservatively, this](const HashMap&lt;HeapLocation, LazyNode&gt;::KeyValuePairType&amp; pair) -&gt; bool {
167             switch (heap.kind()) {
168             case World:
169             case SideState:
170                 break;
171             case Stack: {
172                 if (!clobberConservatively)
173                     break;
174                 if (pair.key.heap().kind() == Stack) {
<span class="line-modified">175                     auto iterator = m_abstractHeapStackMap.find(pair.key.heap().payload().value32());</span>
176                     if (iterator != m_abstractHeapStackMap.end() &amp;&amp; iterator-&gt;value-&gt;key == pair.key)
177                         return false;
178                     return true;
179                 }
180                 break;
181             }
182             default: {
183                 if (!clobberConservatively)
184                     break;
185                 AbstractHeapKind kind = pair.key.heap().kind();
186                 if (kind != World &amp;&amp; kind != SideState &amp;&amp; kind != Stack)
187                     return true;
188                 break;
189             }
190             }
191             return heap.overlaps(pair.key.heap());
192         });
193         ASSERT(m_debugImpureData.size()
194             == (m_heapMap.size()
195                 + m_abstractHeapStackMap.size()
</pre>
<hr />
<pre>
209         m_fallbackStackMap.clear();
210         m_heapMap.clear();
211 #if !defined(NDEBUG)
212         m_debugImpureData.clear();
213 #endif
214     }
215 
216 private:
217     typedef HashSet&lt;std::unique_ptr&lt;ImpureDataSlot&gt;, ImpureDataSlotHash&gt; Map;
218 
219     const ImpureDataSlot* addImpl(const HeapLocation&amp; location, const LazyNode&amp; node)
220     {
221         switch (location.heap().kind()) {
222         case World:
223         case SideState:
224             RELEASE_ASSERT_NOT_REACHED();
225         case Stack: {
226             AbstractHeap abstractHeap = location.heap();
227             if (abstractHeap.payload().isTop())
228                 return add(m_fallbackStackMap, location, node);
<span class="line-modified">229             ASSERT(abstractHeap.payload().value() == abstractHeap.payload().value32());</span>
<span class="line-removed">230             auto addResult = m_abstractHeapStackMap.add(abstractHeap.payload().value32(), nullptr);</span>
231             if (addResult.isNewEntry) {
232                 addResult.iterator-&gt;value.reset(new ImpureDataSlot {location, node, 0});
233                 return nullptr;
234             }
235             if (addResult.iterator-&gt;value-&gt;key == location)
236                 return addResult.iterator-&gt;value.get();
237             return add(m_fallbackStackMap, location, node);
238         }
239         default:
240             return add(m_heapMap, location, node);
241         }
242         return nullptr;
243     }
244 
245     LazyNode getImpl(const HeapLocation&amp; location) const
246     {
247         switch (location.heap().kind()) {
248         case World:
249         case SideState:
250             RELEASE_ASSERT_NOT_REACHED();
251         case Stack: {
<span class="line-modified">252             ASSERT(location.heap().payload().value() == location.heap().payload().value32());</span>
<span class="line-removed">253             auto iterator = m_abstractHeapStackMap.find(location.heap().payload().value32());</span>
254             if (iterator != m_abstractHeapStackMap.end()
255                 &amp;&amp; iterator-&gt;value-&gt;key == location)
256                 return iterator-&gt;value-&gt;value;
257             return get(m_fallbackStackMap, location);
258         }
259         default:
260             return get(m_heapMap, location);
261         }
262         return LazyNode();
263     }
264 
265     static const ImpureDataSlot* add(Map&amp; map, const HeapLocation&amp; location, const LazyNode&amp; node)
266     {
267         auto result = map.add&lt;ImpureDataTranslator&gt;(location);
268         if (result.isNewEntry) {
269             (*result.iterator)-&gt;value = node;
270             return nullptr;
271         }
272         return result.iterator-&gt;get();
273     }
</pre>
<hr />
<pre>
281     }
282 
283     static void clobber(Map&amp; map, AbstractHeap heap)
284     {
285         map.removeIf([heap](const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot) -&gt; bool {
286             return heap.overlaps(slot-&gt;key.heap());
287         });
288     }
289 
290     // The majority of Impure Stack Slots are unique per value.
291     // This is very useful for fast clobber(), we can just remove the slot addressed by AbstractHeap
292     // in O(1).
293     //
294     // When there are conflict, any additional HeapLocation is added in the fallback map.
295     // This works well because fallbackStackMap remains tiny.
296     //
297     // One cannot assume a unique ImpureData is in m_abstractHeapStackMap. It may have been
298     // a duplicate in the past and now only live in m_fallbackStackMap.
299     //
300     // Obviously, TOP always goes into m_fallbackStackMap since it does not have a unique value.
<span class="line-modified">301     HashMap&lt;int32_t, std::unique_ptr&lt;ImpureDataSlot&gt;, DefaultHash&lt;int32_t&gt;::Hash, WTF::SignedWithZeroKeyHashTraits&lt;int32_t&gt;&gt; m_abstractHeapStackMap;</span>
302     Map m_fallbackStackMap;
303 
304     Map m_heapMap;
305 
306 #if !defined(NDEBUG)
307     HashMap&lt;HeapLocation, LazyNode&gt; m_debugImpureData;
308 #endif
309 };
310 
311 class LocalCSEPhase : public Phase {
312 public:
313     LocalCSEPhase(Graph&amp; graph)
314         : Phase(graph, &quot;local common subexpression elimination&quot;)
315         , m_smallBlock(graph)
316         , m_largeBlock(graph)
317         , m_hugeBlock(graph)
318     {
319     }
320 
321     bool run()
</pre>
<hr />
<pre>
336                 changed |= m_smallBlock.run(block);
337             else if (block-&gt;size() &lt;= Options::maxDFGNodesInBasicBlockForPreciseAnalysis())
338                 changed |= m_largeBlock.run(block);
339             else
340                 changed |= m_hugeBlock.run(block);
341         }
342 
343         return changed;
344     }
345 
346 private:
347     class SmallMaps {
348     public:
349         // This permits SmallMaps to be used for blocks that have up to 100 nodes. In practice,
350         // fewer than half of the nodes in a block have pure defs, and even fewer have impure defs.
351         // Thus, a capacity limit of 100 probably means that somewhere around ~40 things may end up
352         // in one of these &quot;small&quot; list-based maps. That number still seems largeish, except that
353         // the overhead of HashMaps can be quite high currently: clearing them, or even removing
354         // enough things from them, deletes (or resizes) their backing store eagerly. Hence
355         // HashMaps induce a lot of malloc traffic.
<span class="line-modified">356         static const unsigned capacity = 100;</span>
357 
358         SmallMaps()
359             : m_pureLength(0)
360             , m_impureLength(0)
361         {
362         }
363 
364         void clear()
365         {
366             m_pureLength = 0;
367             m_impureLength = 0;
368         }
369 
370         void write(AbstractHeap heap)
371         {
372             if (heap.kind() == SideState)
373                 return;
374 
375             for (unsigned i = 0; i &lt; m_impureLength; ++i) {
376                 if (heap.overlaps(m_impureMap[i].key.heap()))
377                     m_impureMap[i--] = m_impureMap[--m_impureLength];
378             }
379         }
380 
381         Node* addPure(PureValue value, Node* node)
382         {
383             for (unsigned i = m_pureLength; i--;) {
384                 if (m_pureMap[i].key == value)
385                     return m_pureMap[i].value;
386             }
387 
<span class="line-modified">388             ASSERT(m_pureLength &lt; capacity);</span>
389             m_pureMap[m_pureLength++] = WTF::KeyValuePair&lt;PureValue, Node*&gt;(value, node);
390             return nullptr;
391         }
392 
393         LazyNode findReplacement(HeapLocation location)
394         {
395             for (unsigned i = m_impureLength; i--;) {
396                 if (m_impureMap[i].key == location)
397                     return m_impureMap[i].value;
398             }
399             return nullptr;
400         }
401 
402         LazyNode addImpure(HeapLocation location, LazyNode node)
403         {
404             // FIXME: If we are using small maps, we must not def() derived values.
405             // For now the only derived values we def() are constant-based.
406             if (location.index() &amp;&amp; !location.index().isNode())
407                 return nullptr;
408             if (LazyNode result = findReplacement(location))
409                 return result;
<span class="line-modified">410             ASSERT(m_impureLength &lt; capacity);</span>
411             m_impureMap[m_impureLength++] = WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt;(location, node);
412             return nullptr;
413         }
414 
415     private:
416         WTF::KeyValuePair&lt;PureValue, Node*&gt; m_pureMap[capacity];
417         WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt; m_impureMap[capacity];
418         unsigned m_pureLength;
419         unsigned m_impureLength;
420     };
421 
422     class LargeMaps {
423     public:
424         LargeMaps()
425         {
426         }
427 
428         void clear()
429         {
430             m_pureMap.clear();
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
 31 #include &quot;DFGAbstractHeap.h&quot;
 32 #include &quot;DFGBlockMapInlines.h&quot;
 33 #include &quot;DFGClobberSet.h&quot;
 34 #include &quot;DFGClobberize.h&quot;
 35 #include &quot;DFGDominators.h&quot;
 36 #include &quot;DFGGraph.h&quot;
 37 #include &quot;DFGPhase.h&quot;
 38 #include &quot;JSCInlines.h&quot;
 39 #include &lt;array&gt;
 40 
 41 namespace JSC { namespace DFG {
 42 
 43 // This file contains two CSE implementations: local and global. LocalCSE typically runs when we&#39;re
 44 // in DFG mode, i.e. we want to compile quickly. LocalCSE contains a lot of optimizations for
 45 // compile time. GlobalCSE, on the other hand, is fairly straight-forward. It will find more
 46 // optimization opportunities by virtue of being global.
 47 
 48 namespace {
 49 
 50 namespace DFGCSEPhaseInternal {
<span class="line-modified"> 51 static constexpr bool verbose = false;</span>
 52 }
 53 
 54 class ImpureDataSlot {
 55     WTF_MAKE_NONCOPYABLE(ImpureDataSlot);
 56     WTF_MAKE_FAST_ALLOCATED;
 57 public:
 58     ImpureDataSlot(HeapLocation key, LazyNode value, unsigned hash)
 59         : key(key), value(value), hash(hash)
 60     { }
 61 
 62     HeapLocation key;
 63     LazyNode value;
 64     unsigned hash;
 65 };
 66 
 67 struct ImpureDataSlotHash : public DefaultHash&lt;std::unique_ptr&lt;ImpureDataSlot&gt;&gt;::Hash {
 68     static unsigned hash(const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; key)
 69     {
 70         return key-&gt;hash;
 71     }
</pre>
<hr />
<pre>
130     LazyNode get(const HeapLocation&amp; location) const
131     {
132         LazyNode result = getImpl(location);
133 #if !defined(NDEBUG)
134         ASSERT(result == m_debugImpureData.get(location));
135 #endif
136         return result;
137     }
138 
139     void clobber(AbstractHeap heap, bool clobberConservatively)
140     {
141         switch (heap.kind()) {
142         case World: {
143             clear();
144             break;
145         }
146         case SideState:
147             break;
148         case Stack: {
149             ASSERT(!heap.payload().isTop());
<span class="line-modified">150             m_abstractHeapStackMap.remove(heap.payload().value());</span>

151             if (clobberConservatively)
152                 m_fallbackStackMap.clear();
153             else
154                 clobber(m_fallbackStackMap, heap);
155             break;
156         }
157         default:
158             if (clobberConservatively)
159                 m_heapMap.clear();
160             else
161                 clobber(m_heapMap, heap);
162             break;
163         }
164 #if !defined(NDEBUG)
165         m_debugImpureData.removeIf([heap, clobberConservatively, this](const HashMap&lt;HeapLocation, LazyNode&gt;::KeyValuePairType&amp; pair) -&gt; bool {
166             switch (heap.kind()) {
167             case World:
168             case SideState:
169                 break;
170             case Stack: {
171                 if (!clobberConservatively)
172                     break;
173                 if (pair.key.heap().kind() == Stack) {
<span class="line-modified">174                     auto iterator = m_abstractHeapStackMap.find(pair.key.heap().payload().value());</span>
175                     if (iterator != m_abstractHeapStackMap.end() &amp;&amp; iterator-&gt;value-&gt;key == pair.key)
176                         return false;
177                     return true;
178                 }
179                 break;
180             }
181             default: {
182                 if (!clobberConservatively)
183                     break;
184                 AbstractHeapKind kind = pair.key.heap().kind();
185                 if (kind != World &amp;&amp; kind != SideState &amp;&amp; kind != Stack)
186                     return true;
187                 break;
188             }
189             }
190             return heap.overlaps(pair.key.heap());
191         });
192         ASSERT(m_debugImpureData.size()
193             == (m_heapMap.size()
194                 + m_abstractHeapStackMap.size()
</pre>
<hr />
<pre>
208         m_fallbackStackMap.clear();
209         m_heapMap.clear();
210 #if !defined(NDEBUG)
211         m_debugImpureData.clear();
212 #endif
213     }
214 
215 private:
216     typedef HashSet&lt;std::unique_ptr&lt;ImpureDataSlot&gt;, ImpureDataSlotHash&gt; Map;
217 
218     const ImpureDataSlot* addImpl(const HeapLocation&amp; location, const LazyNode&amp; node)
219     {
220         switch (location.heap().kind()) {
221         case World:
222         case SideState:
223             RELEASE_ASSERT_NOT_REACHED();
224         case Stack: {
225             AbstractHeap abstractHeap = location.heap();
226             if (abstractHeap.payload().isTop())
227                 return add(m_fallbackStackMap, location, node);
<span class="line-modified">228             auto addResult = m_abstractHeapStackMap.add(abstractHeap.payload().value(), nullptr);</span>

229             if (addResult.isNewEntry) {
230                 addResult.iterator-&gt;value.reset(new ImpureDataSlot {location, node, 0});
231                 return nullptr;
232             }
233             if (addResult.iterator-&gt;value-&gt;key == location)
234                 return addResult.iterator-&gt;value.get();
235             return add(m_fallbackStackMap, location, node);
236         }
237         default:
238             return add(m_heapMap, location, node);
239         }
240         return nullptr;
241     }
242 
243     LazyNode getImpl(const HeapLocation&amp; location) const
244     {
245         switch (location.heap().kind()) {
246         case World:
247         case SideState:
248             RELEASE_ASSERT_NOT_REACHED();
249         case Stack: {
<span class="line-modified">250             auto iterator = m_abstractHeapStackMap.find(location.heap().payload().value());</span>

251             if (iterator != m_abstractHeapStackMap.end()
252                 &amp;&amp; iterator-&gt;value-&gt;key == location)
253                 return iterator-&gt;value-&gt;value;
254             return get(m_fallbackStackMap, location);
255         }
256         default:
257             return get(m_heapMap, location);
258         }
259         return LazyNode();
260     }
261 
262     static const ImpureDataSlot* add(Map&amp; map, const HeapLocation&amp; location, const LazyNode&amp; node)
263     {
264         auto result = map.add&lt;ImpureDataTranslator&gt;(location);
265         if (result.isNewEntry) {
266             (*result.iterator)-&gt;value = node;
267             return nullptr;
268         }
269         return result.iterator-&gt;get();
270     }
</pre>
<hr />
<pre>
278     }
279 
280     static void clobber(Map&amp; map, AbstractHeap heap)
281     {
282         map.removeIf([heap](const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot) -&gt; bool {
283             return heap.overlaps(slot-&gt;key.heap());
284         });
285     }
286 
287     // The majority of Impure Stack Slots are unique per value.
288     // This is very useful for fast clobber(), we can just remove the slot addressed by AbstractHeap
289     // in O(1).
290     //
291     // When there are conflict, any additional HeapLocation is added in the fallback map.
292     // This works well because fallbackStackMap remains tiny.
293     //
294     // One cannot assume a unique ImpureData is in m_abstractHeapStackMap. It may have been
295     // a duplicate in the past and now only live in m_fallbackStackMap.
296     //
297     // Obviously, TOP always goes into m_fallbackStackMap since it does not have a unique value.
<span class="line-modified">298     HashMap&lt;int64_t, std::unique_ptr&lt;ImpureDataSlot&gt;, DefaultHash&lt;int64_t&gt;::Hash, WTF::SignedWithZeroKeyHashTraits&lt;int64_t&gt;&gt; m_abstractHeapStackMap;</span>
299     Map m_fallbackStackMap;
300 
301     Map m_heapMap;
302 
303 #if !defined(NDEBUG)
304     HashMap&lt;HeapLocation, LazyNode&gt; m_debugImpureData;
305 #endif
306 };
307 
308 class LocalCSEPhase : public Phase {
309 public:
310     LocalCSEPhase(Graph&amp; graph)
311         : Phase(graph, &quot;local common subexpression elimination&quot;)
312         , m_smallBlock(graph)
313         , m_largeBlock(graph)
314         , m_hugeBlock(graph)
315     {
316     }
317 
318     bool run()
</pre>
<hr />
<pre>
333                 changed |= m_smallBlock.run(block);
334             else if (block-&gt;size() &lt;= Options::maxDFGNodesInBasicBlockForPreciseAnalysis())
335                 changed |= m_largeBlock.run(block);
336             else
337                 changed |= m_hugeBlock.run(block);
338         }
339 
340         return changed;
341     }
342 
343 private:
344     class SmallMaps {
345     public:
346         // This permits SmallMaps to be used for blocks that have up to 100 nodes. In practice,
347         // fewer than half of the nodes in a block have pure defs, and even fewer have impure defs.
348         // Thus, a capacity limit of 100 probably means that somewhere around ~40 things may end up
349         // in one of these &quot;small&quot; list-based maps. That number still seems largeish, except that
350         // the overhead of HashMaps can be quite high currently: clearing them, or even removing
351         // enough things from them, deletes (or resizes) their backing store eagerly. Hence
352         // HashMaps induce a lot of malloc traffic.
<span class="line-modified">353         static constexpr unsigned capacity = 100;</span>
354 
355         SmallMaps()
356             : m_pureLength(0)
357             , m_impureLength(0)
358         {
359         }
360 
361         void clear()
362         {
363             m_pureLength = 0;
364             m_impureLength = 0;
365         }
366 
367         void write(AbstractHeap heap)
368         {
369             if (heap.kind() == SideState)
370                 return;
371 
372             for (unsigned i = 0; i &lt; m_impureLength; ++i) {
373                 if (heap.overlaps(m_impureMap[i].key.heap()))
374                     m_impureMap[i--] = m_impureMap[--m_impureLength];
375             }
376         }
377 
378         Node* addPure(PureValue value, Node* node)
379         {
380             for (unsigned i = m_pureLength; i--;) {
381                 if (m_pureMap[i].key == value)
382                     return m_pureMap[i].value;
383             }
384 
<span class="line-modified">385             RELEASE_ASSERT(m_pureLength &lt; capacity);</span>
386             m_pureMap[m_pureLength++] = WTF::KeyValuePair&lt;PureValue, Node*&gt;(value, node);
387             return nullptr;
388         }
389 
390         LazyNode findReplacement(HeapLocation location)
391         {
392             for (unsigned i = m_impureLength; i--;) {
393                 if (m_impureMap[i].key == location)
394                     return m_impureMap[i].value;
395             }
396             return nullptr;
397         }
398 
399         LazyNode addImpure(HeapLocation location, LazyNode node)
400         {
401             // FIXME: If we are using small maps, we must not def() derived values.
402             // For now the only derived values we def() are constant-based.
403             if (location.index() &amp;&amp; !location.index().isNode())
404                 return nullptr;
405             if (LazyNode result = findReplacement(location))
406                 return result;
<span class="line-modified">407             RELEASE_ASSERT(m_impureLength &lt; capacity);</span>
408             m_impureMap[m_impureLength++] = WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt;(location, node);
409             return nullptr;
410         }
411 
412     private:
413         WTF::KeyValuePair&lt;PureValue, Node*&gt; m_pureMap[capacity];
414         WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt; m_impureMap[capacity];
415         unsigned m_pureLength;
416         unsigned m_impureLength;
417     };
418 
419     class LargeMaps {
420     public:
421         LargeMaps()
422         {
423         }
424 
425         void clear()
426         {
427             m_pureMap.clear();
</pre>
</td>
</tr>
</table>
<center><a href="DFGCPSRethreadingPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGCallArrayAllocatorSlowPathGenerator.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>