diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp
@@ -43,20 +43,50 @@
 GenerateAndAllocateRegisters::GenerateAndAllocateRegisters(Code& code)
     : m_code(code)
     , m_map(code)
 { }
 
+ALWAYS_INLINE void GenerateAndAllocateRegisters::checkConsistency()
+{
+    // This isn't exactly the right option for this but adding a new one for just this seems silly.
+    if (Options::validateGraph() || Options::validateGraphAtEachPhase()) {
+        m_code.forEachTmp([&] (Tmp tmp) {
+            Reg reg = m_map[tmp].reg;
+            if (!reg)
+                return;
+
+            ASSERT(!m_availableRegs[tmp.bank()].contains(reg));
+            ASSERT(m_currentAllocation->at(reg) == tmp);
+        });
+
+        for (Reg reg : RegisterSet::allRegisters()) {
+            if (isDisallowedRegister(reg))
+                continue;
+
+            Tmp tmp = m_currentAllocation->at(reg);
+            if (!tmp) {
+                ASSERT(m_availableRegs[bankForReg(reg)].contains(reg));
+                continue;
+            }
+
+            ASSERT(!m_availableRegs[tmp.bank()].contains(reg));
+            ASSERT(m_map[tmp].reg == reg);
+        }
+    }
+}
+
 void GenerateAndAllocateRegisters::buildLiveRanges(UnifiedTmpLiveness& liveness)
 {
     m_liveRangeEnd = TmpMap<size_t>(m_code, 0);
 
     m_globalInstIndex = 0;
     for (BasicBlock* block : m_code) {
         for (Tmp tmp : liveness.liveAtHead(block)) {
             if (!tmp.isReg())
                 m_liveRangeEnd[tmp] = m_globalInstIndex;
         }
+        ++m_globalInstIndex;
         for (Inst& inst : *block) {
             inst.forEachTmpFast([&] (Tmp tmp) {
                 if (!tmp.isReg())
                     m_liveRangeEnd[tmp] = m_globalInstIndex;
             });
@@ -64,10 +94,11 @@
         }
         for (Tmp tmp : liveness.liveAtTail(block)) {
             if (!tmp.isReg())
                 m_liveRangeEnd[tmp] = m_globalInstIndex;
         }
+        ++m_globalInstIndex;
     }
 }
 
 void GenerateAndAllocateRegisters::insertBlocksForFlushAfterTerminalPatchpoints()
 {
@@ -121,10 +152,22 @@
     jit.move(CCallHelpers::TrustedImmPtr(offsetFromFP), reg);
     jit.add64(GPRInfo::callFrameRegister, reg);
     return CCallHelpers::Address(reg);
 }
 
+ALWAYS_INLINE void GenerateAndAllocateRegisters::release(Tmp tmp, Reg reg)
+{
+    ASSERT(reg);
+    ASSERT(m_currentAllocation->at(reg) == tmp);
+    m_currentAllocation->at(reg) = Tmp();
+    ASSERT(!m_availableRegs[tmp.bank()].contains(reg));
+    m_availableRegs[tmp.bank()].set(reg);
+    ASSERT(m_map[tmp].reg == reg);
+    m_map[tmp].reg = Reg();
+}
+
+
 ALWAYS_INLINE void GenerateAndAllocateRegisters::flush(Tmp tmp, Reg reg)
 {
     ASSERT(tmp);
     intptr_t offset = m_map[tmp].spillSlot->offsetFromFP();
     if (tmp.isGP())
@@ -135,14 +178,12 @@
 
 ALWAYS_INLINE void GenerateAndAllocateRegisters::spill(Tmp tmp, Reg reg)
 {
     ASSERT(reg);
     ASSERT(m_map[tmp].reg == reg);
-    m_availableRegs[tmp.bank()].set(reg);
-    m_currentAllocation->at(reg) = Tmp();
     flush(tmp, reg);
-    m_map[tmp].reg = Reg();
+    release(tmp, reg);
 }
 
 ALWAYS_INLINE void GenerateAndAllocateRegisters::alloc(Tmp tmp, Reg reg, bool isDef)
 {
     if (Tmp occupyingTmp = m_currentAllocation->at(reg))
@@ -178,14 +219,11 @@
         if (tmp.isReg())
             continue;
         if (m_liveRangeEnd[tmp] >= m_globalInstIndex)
             continue;
 
-        Reg reg = Reg::fromIndex(i);
-        m_map[tmp].reg = Reg();
-        m_availableRegs[tmp.bank()].set(reg);
-        m_currentAllocation->at(i) = Tmp();
+        release(tmp, Reg::fromIndex(i));
     }
 }
 
 ALWAYS_INLINE bool GenerateAndAllocateRegisters::assignTmp(Tmp& tmp, Bank bank, bool isDef)
 {
@@ -242,34 +280,89 @@
 {
     // We pessimistically assume we use all callee saves.
     handleCalleeSaves(m_code, RegisterSet::calleeSaveRegisters());
     allocateEscapedStackSlots(m_code);
 
-    // Each Tmp gets its own stack slot.
-    auto createStackSlot = [&] (const Tmp& tmp) {
-        TmpData data;
-        data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);
-        data.reg = Reg();
-        m_map[tmp] = data;
-#if !ASSERT_DISABLED
-        m_allTmps[tmp.bank()].append(tmp);
-#endif
-    };
+    insertBlocksForFlushAfterTerminalPatchpoints();
 
+#if ASSERT_ENABLED
     m_code.forEachTmp([&] (Tmp tmp) {
         ASSERT(!tmp.isReg());
-        createStackSlot(tmp);
+        m_allTmps[tmp.bank()].append(tmp);
     });
+#endif
+
+    m_liveness = makeUnique<UnifiedTmpLiveness>(m_code);
+
+    {
+        buildLiveRanges(*m_liveness);
+
+        Vector<StackSlot*, 16> freeSlots;
+        Vector<StackSlot*, 4> toFree;
+        m_globalInstIndex = 0;
+        for (BasicBlock* block : m_code) {
+            auto assignStackSlotToTmp = [&] (Tmp tmp) {
+                if (tmp.isReg())
+                    return;
+
+                TmpData& data = m_map[tmp];
+                if (data.spillSlot) {
+                    if (m_liveRangeEnd[tmp] == m_globalInstIndex)
+                        toFree.append(data.spillSlot);
+                    return;
+                }
+
+                if (freeSlots.size())
+                    data.spillSlot = freeSlots.takeLast();
+                else
+                    data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);
+                data.reg = Reg();
+            };
+
+            auto flushToFreeList = [&] {
+                for (auto* stackSlot : toFree)
+                    freeSlots.append(stackSlot);
+                toFree.clear();
+            };
+
+            for (Tmp tmp : m_liveness->liveAtHead(block))
+                assignStackSlotToTmp(tmp);
+            flushToFreeList();
+
+            ++m_globalInstIndex;
+
+            for (Inst& inst : *block) {
+                Vector<Tmp, 4> seenTmps;
+                inst.forEachTmpFast([&] (Tmp tmp) {
+                    if (seenTmps.contains(tmp))
+                        return;
+                    seenTmps.append(tmp);
+                    assignStackSlotToTmp(tmp);
+                });
+
+                flushToFreeList();
+                ++m_globalInstIndex;
+            }
+
+            for (Tmp tmp : m_liveness->liveAtTail(block))
+                assignStackSlotToTmp(tmp);
+            flushToFreeList();
+
+            ++m_globalInstIndex;
+        }
+    }
 
     m_allowedRegisters = RegisterSet();
 
     forEachBank([&] (Bank bank) {
         m_registers[bank] = m_code.regsInPriorityOrder(bank);
 
         for (Reg reg : m_registers[bank]) {
             m_allowedRegisters.set(reg);
-            createStackSlot(Tmp(reg));
+            TmpData& data = m_map[Tmp(reg)];
+            data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);
+            data.reg = Reg();
         }
     });
 
     {
         unsigned nextIndex = 0;
@@ -285,30 +378,53 @@
     updateFrameSizeBasedOnStackSlots(m_code);
     m_code.setStackIsAllocated(true);
 
     lowerStackArgs(m_code);
 
+#if ASSERT_ENABLED
     // Verify none of these passes add any tmps.
-#if !ASSERT_DISABLED
     forEachBank([&] (Bank bank) {
-        ASSERT(m_allTmps[bank].size() - m_registers[bank].size() == m_code.numTmps(bank));
+        ASSERT(m_allTmps[bank].size() == m_code.numTmps(bank));
     });
+
+    {
+        // Verify that lowerStackArgs didn't change Tmp liveness at the boundaries for the Tmps and Registers we model.
+        UnifiedTmpLiveness liveness(m_code);
+        for (BasicBlock* block : m_code) {
+            auto assertLivenessAreEqual = [&] (auto a, auto b) {
+                HashSet<Tmp> livenessA;
+                HashSet<Tmp> livenessB;
+                for (Tmp tmp : a) {
+                    if (tmp.isReg() && isDisallowedRegister(tmp.reg()))
+                        continue;
+                    livenessA.add(tmp);
+                }
+                for (Tmp tmp : b) {
+                    if (tmp.isReg() && isDisallowedRegister(tmp.reg()))
+                        continue;
+                    livenessB.add(tmp);
+                }
+
+                ASSERT(livenessA == livenessB);
+            };
+
+            assertLivenessAreEqual(m_liveness->liveAtHead(block), liveness.liveAtHead(block));
+            assertLivenessAreEqual(m_liveness->liveAtTail(block), liveness.liveAtTail(block));
+        }
+    }
 #endif
 }
 
 void GenerateAndAllocateRegisters::generate(CCallHelpers& jit)
 {
     m_jit = &jit;
 
     TimingScope timingScope("Air::generateAndAllocateRegisters");
 
-    insertBlocksForFlushAfterTerminalPatchpoints();
-
     DisallowMacroScratchRegisterUsage disallowScratch(*m_jit);
 
-    UnifiedTmpLiveness liveness(m_code);
-    buildLiveRanges(liveness);
+    buildLiveRanges(*m_liveness);
 
     IndexMap<BasicBlock*, IndexMap<Reg, Tmp>> currentAllocationMap(m_code.size());
     {
         IndexMap<Reg, Tmp> defaultCurrentAllocation(Reg::maxIndex() + 1);
         for (BasicBlock* block : m_code)
@@ -317,11 +433,11 @@
         // The only things live that are in registers at the root blocks are
         // the explicitly named registers that are live.
 
         for (unsigned i = m_code.numEntrypoints(); i--;) {
             BasicBlock* entrypoint = m_code.entrypoint(i).block();
-            for (Tmp tmp : liveness.liveAtHead(entrypoint)) {
+            for (Tmp tmp : m_liveness->liveAtHead(entrypoint)) {
                 if (tmp.isReg())
                     currentAllocationMap[entrypoint][tmp.reg()] = tmp;
             }
         }
     }
@@ -379,11 +495,11 @@
                 data.continueLabel = m_jit->label();
             }
         }
 
         forEachBank([&] (Bank bank) {
-#if !ASSERT_DISABLED
+#if ASSERT_ENABLED
             // By default, everything is spilled at block boundaries. We do this after we process each block
             // so we don't have to walk all Tmps, since #Tmps >> #Available regs. Instead, we walk the register file at
             // each block boundary and clear entries in this map.
             for (Tmp tmp : m_allTmps[bank])
                 ASSERT(m_map[tmp].reg == Reg());
@@ -405,12 +521,16 @@
             Reg reg = Reg::fromIndex(i);
             m_map[tmp].reg = reg;
             m_availableRegs[tmp.bank()].clear(reg);
         }
 
+        ++m_globalInstIndex;
+
         bool isReplayingSameInst = false;
         for (size_t instIndex = 0; instIndex < block->size(); ++instIndex) {
+            checkConsistency();
+
             if (instIndex && !isReplayingSameInst)
                 startLabel = m_jit->labelIgnoringWatchpoints();
 
             context.indexInBlock = instIndex;
 
@@ -419,10 +539,50 @@
             m_didAlreadyFreeDeadSlots = false;
 
             m_namedUsedRegs = RegisterSet();
             m_namedDefdRegs = RegisterSet();
 
+            bool needsToGenerate = ([&] () -> bool {
+                // FIXME: We should consider trying to figure out if we can also elide Mov32s
+                if (!(inst.kind.opcode == Move || inst.kind.opcode == MoveDouble))
+                    return true;
+
+                ASSERT(inst.args.size() >= 2);
+                Arg source = inst.args[0];
+                Arg dest = inst.args[1];
+                if (!source.isTmp() || !dest.isTmp())
+                    return true;
+
+                // FIXME: We don't track where the last use of a reg is globally so we don't know where we can elide them.
+                ASSERT(source.isReg() || m_liveRangeEnd[source.tmp()] >= m_globalInstIndex);
+                if (source.isReg() || m_liveRangeEnd[source.tmp()] != m_globalInstIndex)
+                    return true;
+
+                // If we are doing a self move at the end of the temps liveness we can trivially elide the move.
+                if (source == dest)
+                    return false;
+
+                Reg sourceReg = m_map[source.tmp()].reg;
+                // If the value is not already materialized into a register we may still move it into one so let the normal generation code run.
+                if (!sourceReg)
+                    return true;
+
+                ASSERT(m_currentAllocation->at(sourceReg) == source.tmp());
+
+                if (dest.isReg() && dest.reg() != sourceReg)
+                    return true;
+
+                if (Reg oldReg = m_map[dest.tmp()].reg)
+                    release(dest.tmp(), oldReg);
+
+                m_map[dest.tmp()].reg = sourceReg;
+                m_currentAllocation->at(sourceReg) = dest.tmp();
+                m_map[source.tmp()].reg = Reg();
+                return false;
+            })();
+            checkConsistency();
+
             inst.forEachArg([&] (Arg& arg, Arg::Role role, Bank, Width) {
                 if (!arg.isTmp())
                     return;
 
                 Tmp tmp = arg.tmp();
@@ -480,11 +640,11 @@
             };
 
             allocNamed(m_namedUsedRegs, false); // Must come before the defd registers since we may use and def the same register.
             allocNamed(m_namedDefdRegs, true);
 
-            {
+            if (needsToGenerate) {
                 auto tryAllocate = [&] {
                     Vector<Tmp*, 8> usesToAlloc;
                     Vector<Tmp*, 8> defsToAlloc;
 
                     inst.forEachTmp([&] (Tmp& tmp, Arg::Role role, Bank, Width) {
@@ -593,32 +753,34 @@
                         currentAllocationMap[successor] = currentAllocation;
                     else
                         everySuccessorGetsOurRegisterState = false;
                 }
                 if (!everySuccessorGetsOurRegisterState) {
-                    for (Tmp tmp : liveness.liveAtTail(block)) {
+                    for (Tmp tmp : m_liveness->liveAtTail(block)) {
                         if (tmp.isReg() && isDisallowedRegister(tmp.reg()))
                             continue;
                         if (Reg reg = m_map[tmp].reg)
                             flush(tmp, reg);
                     }
                 }
             }
 
             if (!inst.isTerminal()) {
-                CCallHelpers::Jump jump = inst.generate(*m_jit, context);
+                CCallHelpers::Jump jump;
+                if (needsToGenerate)
+                    jump = inst.generate(*m_jit, context);
                 ASSERT_UNUSED(jump, !jump.isSet());
 
                 for (Reg reg : clobberedRegisters) {
                     Tmp tmp(reg);
                     ASSERT(currentAllocation[reg] == tmp);
                     m_availableRegs[tmp.bank()].set(reg);
                     m_currentAllocation->at(reg) = Tmp();
                     m_map[tmp].reg = Reg();
                 }
             } else {
-                bool needsToGenerate = true;
+                ASSERT(needsToGenerate);
                 if (inst.kind.opcode == Jump && block->successorBlock(0) == m_code.findNextBlock(block))
                     needsToGenerate = false;
 
                 if (isReturn(inst.kind.opcode)) {
                     needsToGenerate = false;
@@ -670,10 +832,12 @@
         // null entries.
         for (size_t i = 0; i < currentAllocation.size(); ++i) {
             if (Tmp tmp = currentAllocation[i])
                 m_map[tmp].reg = Reg();
         }
+
+        ++m_globalInstIndex;
     }
 
     for (auto& entry : m_blocksAfterTerminalPatchForSpilling) {
         entry.value.jump.linkTo(m_jit->label(), m_jit);
         const HashMap<Tmp, Arg*>& spills = entry.value.defdTmps;
