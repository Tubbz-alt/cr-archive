<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/PreciseAllocation.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2016-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;PreciseAllocation.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;Heap.h&quot;
 31 #include &quot;IsoCellSetInlines.h&quot;
 32 #include &quot;JSCInlines.h&quot;
 33 #include &quot;Operations.h&quot;
 34 #include &quot;SubspaceInlines.h&quot;
 35 
 36 namespace JSC {
 37 
 38 static inline bool isAlignedForPreciseAllocation(void* memory)
 39 {
 40     uintptr_t allocatedPointer = bitwise_cast&lt;uintptr_t&gt;(memory);
 41     return !(allocatedPointer &amp; (PreciseAllocation::alignment - 1));
 42 }
 43 
 44 PreciseAllocation* PreciseAllocation::tryCreate(Heap&amp; heap, size_t size, Subspace* subspace, unsigned indexInSpace)
 45 {
 46     if (validateDFGDoesGC)
 47         RELEASE_ASSERT(heap.expectDoesGC());
 48 
 49     size_t adjustedAlignmentAllocationSize = headerSize() + size + halfAlignment;
 50     static_assert(halfAlignment == 8, &quot;We assume that memory returned by malloc has alignment &gt;= 8.&quot;);
 51 
 52     // We must use tryAllocateMemory instead of tryAllocateAlignedMemory since we want to use &quot;realloc&quot; feature.
 53     void* space = subspace-&gt;alignedMemoryAllocator()-&gt;tryAllocateMemory(adjustedAlignmentAllocationSize);
 54     if (!space)
 55         return nullptr;
 56 
 57     bool adjustedAlignment = false;
 58     if (!isAlignedForPreciseAllocation(space)) {
 59         space = bitwise_cast&lt;void*&gt;(bitwise_cast&lt;uintptr_t&gt;(space) + halfAlignment);
 60         adjustedAlignment = true;
 61         ASSERT(isAlignedForPreciseAllocation(space));
 62     }
 63 
 64     if (scribbleFreeCells())
 65         scribble(space, size);
 66     return new (NotNull, space) PreciseAllocation(heap, size, subspace, indexInSpace, adjustedAlignment);
 67 }
 68 
 69 PreciseAllocation* PreciseAllocation::tryReallocate(size_t size, Subspace* subspace)
 70 {
 71     ASSERT(!isLowerTier());
 72     size_t adjustedAlignmentAllocationSize = headerSize() + size + halfAlignment;
 73     static_assert(halfAlignment == 8, &quot;We assume that memory returned by malloc has alignment &gt;= 8.&quot;);
 74 
 75     ASSERT(subspace == m_subspace);
 76 
 77     unsigned oldCellSize = m_cellSize;
 78     bool oldAdjustedAlignment = m_adjustedAlignment;
 79     void* oldBasePointer = basePointer();
 80 
 81     void* newBasePointer = subspace-&gt;alignedMemoryAllocator()-&gt;tryReallocateMemory(oldBasePointer, adjustedAlignmentAllocationSize);
 82     if (!newBasePointer)
 83         return nullptr;
 84 
 85     PreciseAllocation* newAllocation = bitwise_cast&lt;PreciseAllocation*&gt;(newBasePointer);
 86     bool newAdjustedAlignment = false;
 87     if (!isAlignedForPreciseAllocation(newBasePointer)) {
 88         newAdjustedAlignment = true;
 89         newAllocation = bitwise_cast&lt;PreciseAllocation*&gt;(bitwise_cast&lt;uintptr_t&gt;(newBasePointer) + halfAlignment);
 90         ASSERT(isAlignedForPreciseAllocation(static_cast&lt;void*&gt;(newAllocation)));
 91     }
 92 
 93     // We have 4 patterns.
 94     // oldAdjustedAlignment = true  newAdjustedAlignment = true  =&gt; Do nothing.
 95     // oldAdjustedAlignment = true  newAdjustedAlignment = false =&gt; Shift forward by halfAlignment
 96     // oldAdjustedAlignment = false newAdjustedAlignment = true  =&gt; Shift backward by halfAlignment
 97     // oldAdjustedAlignment = false newAdjustedAlignment = false =&gt; Do nothing.
 98 
 99     if (oldAdjustedAlignment != newAdjustedAlignment) {
100         if (oldAdjustedAlignment) {
101             ASSERT(!newAdjustedAlignment);
102             ASSERT(newAllocation == newBasePointer);
103             // Old   [ 8 ][  content  ]
104             // Now   [   ][  content  ]
105             // New   [  content  ]...
106             memmove(newBasePointer, bitwise_cast&lt;char*&gt;(newBasePointer) + halfAlignment, oldCellSize + PreciseAllocation::headerSize());
107         } else {
108             ASSERT(newAdjustedAlignment);
109             ASSERT(newAllocation != newBasePointer);
110             ASSERT(newAllocation == bitwise_cast&lt;void*&gt;(bitwise_cast&lt;char*&gt;(newBasePointer) + halfAlignment));
111             // Old   [  content  ]
112             // Now   [  content  ][   ]
113             // New   [ 8 ][  content  ]
114             memmove(bitwise_cast&lt;char*&gt;(newBasePointer) + halfAlignment, newBasePointer, oldCellSize + PreciseAllocation::headerSize());
115         }
116     }
117 
118     newAllocation-&gt;m_cellSize = size;
119     newAllocation-&gt;m_adjustedAlignment = newAdjustedAlignment;
120     return newAllocation;
121 }
122 
123 
124 PreciseAllocation* PreciseAllocation::createForLowerTier(Heap&amp; heap, size_t size, Subspace* subspace, uint8_t lowerTierIndex)
125 {
126     if (validateDFGDoesGC)
127         RELEASE_ASSERT(heap.expectDoesGC());
128 
129     size_t adjustedAlignmentAllocationSize = headerSize() + size + halfAlignment;
130     static_assert(halfAlignment == 8, &quot;We assume that memory returned by malloc has alignment &gt;= 8.&quot;);
131 
132     void* space = subspace-&gt;alignedMemoryAllocator()-&gt;tryAllocateMemory(adjustedAlignmentAllocationSize);
133     RELEASE_ASSERT(space);
134 
135     bool adjustedAlignment = false;
136     if (!isAlignedForPreciseAllocation(space)) {
137         space = bitwise_cast&lt;void*&gt;(bitwise_cast&lt;uintptr_t&gt;(space) + halfAlignment);
138         adjustedAlignment = true;
139         ASSERT(isAlignedForPreciseAllocation(space));
140     }
141 
142     if (scribbleFreeCells())
143         scribble(space, size);
144     PreciseAllocation* preciseAllocation = new (NotNull, space) PreciseAllocation(heap, size, subspace, 0, adjustedAlignment);
145     preciseAllocation-&gt;m_lowerTierIndex = lowerTierIndex;
146     return preciseAllocation;
147 }
148 
149 PreciseAllocation* PreciseAllocation::reuseForLowerTier()
150 {
151     Heap&amp; heap = *this-&gt;heap();
152     size_t size = m_cellSize;
153     Subspace* subspace = m_subspace;
154     bool adjustedAlignment = m_adjustedAlignment;
155     uint8_t lowerTierIndex = m_lowerTierIndex;
156     void* basePointer = this-&gt;basePointer();
157 
158     this-&gt;~PreciseAllocation();
159 
160     void* space = basePointer;
161     ASSERT((!isAlignedForPreciseAllocation(basePointer)) == adjustedAlignment);
162     if (adjustedAlignment)
163         space = bitwise_cast&lt;void*&gt;(bitwise_cast&lt;uintptr_t&gt;(basePointer) + halfAlignment);
164 
165     PreciseAllocation* preciseAllocation = new (NotNull, space) PreciseAllocation(heap, size, subspace, 0, adjustedAlignment);
166     preciseAllocation-&gt;m_lowerTierIndex = lowerTierIndex;
167     preciseAllocation-&gt;m_hasValidCell = false;
168     return preciseAllocation;
169 }
170 
171 PreciseAllocation::PreciseAllocation(Heap&amp; heap, size_t size, Subspace* subspace, unsigned indexInSpace, bool adjustedAlignment)
172     : m_indexInSpace(indexInSpace)
173     , m_cellSize(size)
174     , m_isNewlyAllocated(true)
175     , m_hasValidCell(true)
176     , m_adjustedAlignment(adjustedAlignment)
177     , m_attributes(subspace-&gt;attributes())
178     , m_subspace(subspace)
179     , m_weakSet(heap.vm())
180 {
181     m_isMarked.store(0);
182     ASSERT(cell()-&gt;isPreciseAllocation());
183 }
184 
185 PreciseAllocation::~PreciseAllocation()
186 {
187     if (isOnList())
188         remove();
189 }
190 
191 void PreciseAllocation::lastChanceToFinalize()
192 {
193     m_weakSet.lastChanceToFinalize();
194     clearMarked();
195     clearNewlyAllocated();
196     sweep();
197 }
198 
199 void PreciseAllocation::shrink()
200 {
201     m_weakSet.shrink();
202 }
203 
204 void PreciseAllocation::visitWeakSet(SlotVisitor&amp; visitor)
205 {
206     m_weakSet.visit(visitor);
207 }
208 
209 void PreciseAllocation::reapWeakSet()
210 {
211     return m_weakSet.reap();
212 }
213 
214 void PreciseAllocation::flip()
215 {
216     ASSERT(heap()-&gt;collectionScope() == CollectionScope::Full);
217     clearMarked();
218 }
219 
220 bool PreciseAllocation::isEmpty()
221 {
222     return !isMarked() &amp;&amp; m_weakSet.isEmpty() &amp;&amp; !isNewlyAllocated();
223 }
224 
225 void PreciseAllocation::sweep()
226 {
227     m_weakSet.sweep();
228 
229     if (m_hasValidCell &amp;&amp; !isLive()) {
230         if (m_attributes.destruction == NeedsDestruction)
231             m_subspace-&gt;destroy(vm(), static_cast&lt;JSCell*&gt;(cell()));
232         // We should clear IsoCellSet&#39;s bit before actually destroying PreciseAllocation
233         // since PreciseAllocation&#39;s destruction can be delayed until its WeakSet is cleared.
234         if (isLowerTier())
235             static_cast&lt;IsoSubspace*&gt;(m_subspace)-&gt;clearIsoCellSetBit(this);
236         m_hasValidCell = false;
237     }
238 }
239 
240 void PreciseAllocation::destroy()
241 {
242     AlignedMemoryAllocator* allocator = m_subspace-&gt;alignedMemoryAllocator();
243     void* basePointer = this-&gt;basePointer();
244     this-&gt;~PreciseAllocation();
245     allocator-&gt;freeMemory(basePointer);
246 }
247 
248 void PreciseAllocation::dump(PrintStream&amp; out) const
249 {
250     out.print(RawPointer(this), &quot;:(cell at &quot;, RawPointer(cell()), &quot; with size &quot;, m_cellSize, &quot; and attributes &quot;, m_attributes, &quot;)&quot;);
251 }
252 
253 #if ASSERT_ENABLED
254 void PreciseAllocation::assertValidCell(VM&amp; vm, HeapCell* cell) const
255 {
256     ASSERT(&amp;vm == &amp;this-&gt;vm());
257     ASSERT(cell == this-&gt;cell());
258     ASSERT(m_hasValidCell);
259 }
260 #endif
261 
262 } // namespace JSC
263 
    </pre>
  </body>
</html>