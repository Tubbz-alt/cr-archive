<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #pragma once
  31 
  32 #include &quot;ArrayProfile.h&quot;
  33 #include &quot;ByValInfo.h&quot;
  34 #include &quot;BytecodeConventions.h&quot;
  35 #include &quot;CallLinkInfo.h&quot;
  36 #include &quot;CodeBlockHash.h&quot;
  37 #include &quot;CodeOrigin.h&quot;
  38 #include &quot;CodeType.h&quot;
  39 #include &quot;CompilationResult.h&quot;
  40 #include &quot;ConcurrentJSLock.h&quot;
  41 #include &quot;DFGCommon.h&quot;
  42 #include &quot;DirectEvalCodeCache.h&quot;
  43 #include &quot;EvalExecutable.h&quot;
  44 #include &quot;ExecutionCounter.h&quot;
  45 #include &quot;ExpressionRangeInfo.h&quot;
  46 #include &quot;FunctionExecutable.h&quot;
  47 #include &quot;HandlerInfo.h&quot;
  48 #include &quot;ICStatusMap.h&quot;
  49 #include &quot;Instruction.h&quot;
  50 #include &quot;InstructionStream.h&quot;
  51 #include &quot;JITCode.h&quot;
  52 #include &quot;JITCodeMap.h&quot;
  53 #include &quot;JITMathICForwards.h&quot;
  54 #include &quot;JSCast.h&quot;
  55 #include &quot;JSGlobalObject.h&quot;
  56 #include &quot;JumpTable.h&quot;
  57 #include &quot;LLIntCallLinkInfo.h&quot;
  58 #include &quot;LazyOperandValueProfile.h&quot;
  59 #include &quot;MetadataTable.h&quot;
  60 #include &quot;ModuleProgramExecutable.h&quot;
  61 #include &quot;ObjectAllocationProfile.h&quot;
  62 #include &quot;Options.h&quot;
  63 #include &quot;Printer.h&quot;
  64 #include &quot;ProfilerJettisonReason.h&quot;
  65 #include &quot;ProgramExecutable.h&quot;
  66 #include &quot;PutPropertySlot.h&quot;
  67 #include &quot;ValueProfile.h&quot;
  68 #include &quot;VirtualRegister.h&quot;
  69 #include &quot;Watchpoint.h&quot;
  70 #include &lt;wtf/Bag.h&gt;
  71 #include &lt;wtf/FastMalloc.h&gt;
  72 #include &lt;wtf/RefCountedArray.h&gt;
  73 #include &lt;wtf/RefPtr.h&gt;
  74 #include &lt;wtf/SegmentedVector.h&gt;
  75 #include &lt;wtf/Vector.h&gt;
  76 #include &lt;wtf/text/WTFString.h&gt;
  77 
  78 namespace JSC {
  79 
  80 #if ENABLE(DFG_JIT)
  81 namespace DFG {
  82 struct OSRExitState;
  83 } // namespace DFG
  84 #endif
  85 
  86 class UnaryArithProfile;
  87 class BinaryArithProfile;
  88 class BytecodeLivenessAnalysis;
  89 class CodeBlockSet;
  90 class ExecutableToCodeBlockEdge;
  91 class JSModuleEnvironment;
  92 class LLIntOffsetsExtractor;
  93 class LLIntPrototypeLoadAdaptiveStructureWatchpoint;
  94 class MetadataTable;
  95 class PCToCodeOriginMap;
  96 class RegisterAtOffsetList;
  97 class StructureStubInfo;
  98 
  99 DECLARE_ALLOCATOR_WITH_HEAP_IDENTIFIER(CodeBlockRareData);
 100 
 101 enum class AccessType : int8_t;
 102 
 103 struct OpCatch;
 104 
 105 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 106 
 107 class CodeBlock : public JSCell {
 108     typedef JSCell Base;
 109     friend class BytecodeLivenessAnalysis;
 110     friend class JIT;
 111     friend class LLIntOffsetsExtractor;
 112 
 113 public:
 114 
 115     enum CopyParsedBlockTag { CopyParsedBlock };
 116 
 117     static constexpr unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;
 118     static constexpr bool needsDestruction = true;
 119 
 120     template&lt;typename, SubspaceAccess&gt;
 121     static void subspaceFor(VM&amp;)
 122     {
 123         RELEASE_ASSERT_NOT_REACHED();
 124     }
 125     // GC strongly assumes CodeBlock is not a PreciseAllocation for now.
 126     static constexpr uint8_t numberOfLowerTierCells = 0;
 127 
 128     DECLARE_INFO;
 129 
 130 protected:
 131     CodeBlock(VM&amp;, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);
 132     CodeBlock(VM&amp;, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 133 
 134     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 135     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 136 
 137     void finishCreationCommon(VM&amp;);
 138 
 139     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 140 
 141 public:
 142     JS_EXPORT_PRIVATE ~CodeBlock();
 143 
 144     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 145 
 146     CString inferredName() const;
 147     CodeBlockHash hash() const;
 148     bool hasHash() const;
 149     bool isSafeToComputeHash() const;
 150     CString hashAsStringIfPossible() const;
 151     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 152     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
 153     void dumpAssumingJITType(PrintStream&amp;, JITType) const;
 154     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 155 
 156     MetadataTable* metadataTable() const { return m_metadata.get(); }
 157 
 158     int numParameters() const { return m_numParameters; }
 159     void setNumParameters(int newValue);
 160 
 161     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 162 
 163     int numCalleeLocals() const { return m_numCalleeLocals; }
 164 
 165     int numVars() const { return m_numVars; }
 166     int numTmps() const { return m_unlinkedCode-&gt;hasCheckpoints() * maxNumCheckpointTmps; }
 167 
 168     int* addressOfNumParameters() { return &amp;m_numParameters; }
 169     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 170 
 171     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 172     void setAlternative(VM&amp;, CodeBlock*);
 173 
 174     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 175     {
 176         Functor f(std::forward&lt;Functor&gt;(functor));
 177         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 178         codeBlocks.append(this);
 179 
 180         while (!codeBlocks.isEmpty()) {
 181             CodeBlock* currentCodeBlock = codeBlocks.takeLast();
 182             f(currentCodeBlock);
 183 
 184             if (CodeBlock* alternative = currentCodeBlock-&gt;alternative())
 185                 codeBlocks.append(alternative);
 186             if (CodeBlock* osrEntryBlock = currentCodeBlock-&gt;specialOSREntryBlockOrNull())
 187                 codeBlocks.append(osrEntryBlock);
 188         }
 189     }
 190 
 191     CodeSpecializationKind specializationKind() const
 192     {
 193         return specializationFromIsConstruct(isConstructor());
 194     }
 195 
 196     CodeBlock* alternativeForJettison();
 197     JS_EXPORT_PRIVATE CodeBlock* baselineAlternative();
 198 
 199     // FIXME: Get rid of this.
 200     // https://bugs.webkit.org/show_bug.cgi?id=123677
 201     CodeBlock* baselineVersion();
 202 
 203     static size_t estimatedSize(JSCell*, VM&amp;);
 204     static void visitChildren(JSCell*, SlotVisitor&amp;);
 205     static void destroy(JSCell*);
 206     void visitChildren(SlotVisitor&amp;);
 207     void finalizeUnconditionally(VM&amp;);
 208 
 209     void notifyLexicalBindingUpdate();
 210 
 211     void dumpSource();
 212     void dumpSource(PrintStream&amp;);
 213 
 214     void dumpBytecode();
 215     void dumpBytecode(PrintStream&amp;);
 216     void dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; = ICStatusMap());
 217     void dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; = ICStatusMap());
 218 
 219     void dumpExceptionHandlers(PrintStream&amp;);
 220     void printStructures(PrintStream&amp;, const Instruction*);
 221     void printStructure(PrintStream&amp;, const char* name, const Instruction*, int operand);
 222 
 223     void dumpMathICStats();
 224 
 225     bool isStrictMode() const { return m_unlinkedCode-&gt;isStrictMode(); }
 226     bool isConstructor() const { return m_unlinkedCode-&gt;isConstructor(); }
 227     ECMAMode ecmaMode() const { return isStrictMode() ? StrictMode : NotStrictMode; }
 228     CodeType codeType() const { return m_unlinkedCode-&gt;codeType(); }
 229 
 230     JSParserScriptMode scriptMode() const { return m_unlinkedCode-&gt;scriptMode(); }
 231 
 232     bool hasInstalledVMTrapBreakpoints() const;
 233     bool installVMTrapBreakpoints();
 234 
 235     inline bool isKnownNotImmediate(VirtualRegister reg)
 236     {
 237         if (reg == thisRegister() &amp;&amp; !isStrictMode())
 238             return true;
 239 
 240         if (reg.isConstant())
 241             return getConstant(reg).isCell();
 242 
 243         return false;
 244     }
 245 
 246     ALWAYS_INLINE bool isTemporaryRegister(VirtualRegister reg)
 247     {
 248         return reg.offset() &gt;= m_numVars;
 249     }
 250 
 251     HandlerInfo* handlerForBytecodeIndex(BytecodeIndex, RequiredHandler = RequiredHandler::AnyHandler);
 252     HandlerInfo* handlerForIndex(unsigned, RequiredHandler = RequiredHandler::AnyHandler);
 253     void removeExceptionHandlerForCallSite(DisposableCallSiteIndex);
 254     unsigned lineNumberForBytecodeIndex(BytecodeIndex);
 255     unsigned columnNumberForBytecodeIndex(BytecodeIndex);
 256     void expressionRangeForBytecodeIndex(BytecodeIndex, int&amp; divot,
 257         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 258 
 259     Optional&lt;BytecodeIndex&gt; bytecodeIndexFromCallSiteIndex(CallSiteIndex);
 260 
 261     // Because we might throw out baseline JIT code and all its baseline JIT data (m_jitData),
 262     // you need to be careful about the lifetime of when you use the return value of this function.
 263     // The return value may have raw pointers into this data structure that gets thrown away.
 264     // Specifically, you need to ensure that no GC can be finalized (typically that means no
 265     // allocations) between calling this and the last use of it.
 266     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 267     void getICStatusMap(ICStatusMap&amp; result);
 268 
 269 #if ENABLE(JIT)
 270     struct JITData {
 271         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 272 
 273         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 274         Bag&lt;JITAddIC&gt; m_addICs;
 275         Bag&lt;JITMulIC&gt; m_mulICs;
 276         Bag&lt;JITNegIC&gt; m_negICs;
 277         Bag&lt;JITSubIC&gt; m_subICs;
 278         Bag&lt;ByValInfo&gt; m_byValInfos;
 279         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
 280         SentinelLinkedList&lt;CallLinkInfo, PackedRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;
 281         SentinelLinkedList&lt;PolymorphicCallNode, PackedRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;
 282         RefCountedArray&lt;RareCaseProfile&gt; m_rareCaseProfiles;
 283         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 284         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 285         JITCodeMap m_jitCodeMap;
 286     };
 287 
 288     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 289     {
 290         if (LIKELY(m_jitData))
 291             return *m_jitData;
 292         return ensureJITDataSlow(locker);
 293     }
 294     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 295 
 296     JITAddIC* addJITAddIC(BinaryArithProfile*);
 297     JITMulIC* addJITMulIC(BinaryArithProfile*);
 298     JITNegIC* addJITNegIC(UnaryArithProfile*);
 299     JITSubIC* addJITSubIC(BinaryArithProfile*);
 300 
 301     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
 302     JITAddIC* addMathIC(BinaryArithProfile* profile) { return addJITAddIC(profile); }
 303 
 304     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
 305     JITMulIC* addMathIC(BinaryArithProfile* profile) { return addJITMulIC(profile); }
 306 
 307     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
 308     JITNegIC* addMathIC(UnaryArithProfile* profile) { return addJITNegIC(profile); }
 309 
 310     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
 311     JITSubIC* addMathIC(BinaryArithProfile* profile) { return addJITSubIC(profile); }
 312 
 313     StructureStubInfo* addStubInfo(AccessType);
 314 
 315     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 316     // stub info.
 317     StructureStubInfo* findStubInfo(CodeOrigin);
 318 
 319     ByValInfo* addByValInfo();
 320 
 321     CallLinkInfo* addCallLinkInfo();
 322 
 323     // This is a slow function call used primarily for compiling OSR exits in the case
 324     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 325     // looking for a CallLinkInfoMap to amortize the cost of calling this.
 326     CallLinkInfo* getCallLinkInfoForBytecodeIndex(BytecodeIndex);
 327 
 328     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 329     {
 330         ConcurrentJSLocker locker(m_lock);
 331         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
 332     }
 333     const JITCodeMap&amp; jitCodeMap()
 334     {
 335         ConcurrentJSLocker locker(m_lock);
 336         return ensureJITData(locker).m_jitCodeMap;
 337     }
 338 
 339     void setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp;);
 340     Optional&lt;CodeOrigin&gt; findPC(void* pc);
 341 
 342     void setCalleeSaveRegisters(RegisterSet);
 343     void setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt;);
 344 
 345     void setRareCaseProfiles(RefCountedArray&lt;RareCaseProfile&gt;&amp;&amp;);
 346     RareCaseProfile* rareCaseProfileForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex);
 347     unsigned rareCaseProfileCountForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex);
 348 
 349     bool likelyToTakeSlowCase(BytecodeIndex bytecodeIndex)
 350     {
 351         if (!hasBaselineJITProfiling())
 352             return false;
 353         ConcurrentJSLocker locker(m_lock);
 354         unsigned value = rareCaseProfileCountForBytecodeIndex(locker, bytecodeIndex);
 355         return value &gt;= Options::likelyToTakeSlowCaseMinimumCount();
 356     }
 357 
 358     bool couldTakeSlowCase(BytecodeIndex bytecodeIndex)
 359     {
 360         if (!hasBaselineJITProfiling())
 361             return false;
 362         ConcurrentJSLocker locker(m_lock);
 363         unsigned value = rareCaseProfileCountForBytecodeIndex(locker, bytecodeIndex);
 364         return value &gt;= Options::couldTakeSlowCaseMinimumCount();
 365     }
 366 
 367     // We call this when we want to reattempt compiling something with the baseline JIT. Ideally
 368     // the baseline JIT would not add data to CodeBlock, but instead it would put its data into
 369     // a newly created JITCode, which could be thrown away if we bail on JIT compilation. Then we
 370     // would be able to get rid of this silly function.
 371     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=159061
 372     void resetJITData();
 373 #endif // ENABLE(JIT)
 374 
 375     void unlinkIncomingCalls();
 376 
 377 #if ENABLE(JIT)
 378     void linkIncomingCall(CallFrame* callerFrame, CallLinkInfo*);
 379     void linkIncomingPolymorphicCall(CallFrame* callerFrame, PolymorphicCallNode*);
 380 #endif // ENABLE(JIT)
 381 
 382     void linkIncomingCall(CallFrame* callerFrame, LLIntCallLinkInfo*);
 383 
 384     const Instruction* outOfLineJumpTarget(const Instruction* pc);
 385     int outOfLineJumpOffset(InstructionStream::Offset offset)
 386     {
 387         return m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
 388     }
 389     int outOfLineJumpOffset(const Instruction* pc);
 390     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 391     {
 392         return outOfLineJumpOffset(instruction.ptr());
 393     }
 394 
 395     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 396     {
 397         const auto* instructionsBegin = instructions().at(0).ptr();
 398         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 399         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 400         return returnAddress - instructionsBegin;
 401     }
 402 
 403     inline BytecodeIndex bytecodeIndex(const Instruction* returnAddress)
 404     {
 405         return BytecodeIndex(bytecodeOffset(returnAddress));
 406     }
 407 
 408     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 409 
 410     size_t predictedMachineCodeSize();
 411 
 412     unsigned instructionsSize() const { return instructions().size(); }
 413     unsigned bytecodeCost() const { return m_bytecodeCost; }
 414 
 415     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 416     CodeBlock* newReplacement();
 417 
 418     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 419     {
 420         if (!code-&gt;isShared())
 421             heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());
 422 
 423         ConcurrentJSLocker locker(m_lock);
 424         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 425         m_jitCode = WTFMove(code);
 426     }
 427 
 428     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 429     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
 430     JITType jitType() const
 431     {
 432         JITCode* jitCode = m_jitCode.get();
 433         WTF::loadLoadFence();
 434         JITType result = JITCode::jitTypeFor(jitCode);
 435         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 436         return result;
 437     }
 438 
 439     bool hasBaselineJITProfiling() const
 440     {
 441         return jitType() == JITType::BaselineJIT;
 442     }
 443 
 444 #if ENABLE(JIT)
 445     CodeBlock* replacement();
 446 
 447     DFG::CapabilityLevel computeCapabilityLevel();
 448     DFG::CapabilityLevel capabilityLevel();
 449     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 450 
 451     CodeBlock* optimizedReplacement(JITType typeToReplace);
 452     CodeBlock* optimizedReplacement(); // the typeToReplace is my JITType
 453     bool hasOptimizedReplacement(JITType typeToReplace);
 454     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 455 #endif
 456 
 457     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 458 
 459     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 460 
 461     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 462 
 463     VM&amp; vm() const { return *m_vm; }
 464 
 465     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 466 
 467     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 468 
 469     void setScopeRegister(VirtualRegister scopeRegister)
 470     {
 471         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 472         m_scopeRegister = scopeRegister;
 473     }
 474 
 475     VirtualRegister scopeRegister() const
 476     {
 477         return m_scopeRegister;
 478     }
 479 
 480     PutPropertySlot::Context putByIdContext() const
 481     {
 482         if (codeType() == EvalCode)
 483             return PutPropertySlot::PutByIdEval;
 484         return PutPropertySlot::PutById;
 485     }
 486 
 487     const SourceCode&amp; source() const { return m_ownerExecutable-&gt;source(); }
 488     unsigned sourceOffset() const { return m_ownerExecutable-&gt;source().startOffset(); }
 489     unsigned firstLineColumnOffset() const { return m_ownerExecutable-&gt;startColumn(); }
 490 
 491     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 492     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 493 
 494     String nameForRegister(VirtualRegister);
 495 
 496     unsigned numberOfArgumentValueProfiles()
 497     {
 498         ASSERT(m_numParameters &gt;= 0);
 499         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm().canUseJIT());
 500         return m_argumentValueProfiles.size();
 501     }
 502 
 503     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 504     {
 505         ASSERT(vm().canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.
 506         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];
 507         return result;
 508     }
 509 
 510     ValueProfile&amp; valueProfileForBytecodeIndex(BytecodeIndex);
 511     SpeculatedType valueProfilePredictionForBytecodeIndex(const ConcurrentJSLocker&amp;, BytecodeIndex);
 512 
 513     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 514     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 515     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 516     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 517     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 518 
 519     BinaryArithProfile* binaryArithProfileForBytecodeIndex(BytecodeIndex);
 520     UnaryArithProfile* unaryArithProfileForBytecodeIndex(BytecodeIndex);
 521     BinaryArithProfile* binaryArithProfileForPC(const Instruction*);
 522     UnaryArithProfile* unaryArithProfileForPC(const Instruction*);
 523 
 524     bool couldTakeSpecialArithFastCase(BytecodeIndex bytecodeOffset);
 525 
 526     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, BytecodeIndex);
 527     ArrayProfile* getArrayProfile(BytecodeIndex);
 528 
 529     // Exception handling support
 530 
 531     size_t numberOfExceptionHandlers() const { return m_rareData ? m_rareData-&gt;m_exceptionHandlers.size() : 0; }
 532     HandlerInfo&amp; exceptionHandler(int index) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_exceptionHandlers[index]; }
 533 
 534     bool hasExpressionInfo() { return m_unlinkedCode-&gt;hasExpressionInfo(); }
 535 
 536 #if ENABLE(DFG_JIT)
 537     Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; codeOrigins();
 538 
 539     // Having code origins implies that there has been some inlining.
 540     bool hasCodeOrigins()
 541     {
 542         return JITCode::isOptimizingJIT(jitType());
 543     }
 544 
 545     bool canGetCodeOrigin(CallSiteIndex index)
 546     {
 547         if (!hasCodeOrigins())
 548             return false;
 549         return index.bits() &lt; codeOrigins().size();
 550     }
 551 
 552     CodeOrigin codeOrigin(CallSiteIndex index)
 553     {
 554         return codeOrigins()[index.bits()];
 555     }
 556 
 557     CompressedLazyOperandValueProfileHolder&amp; lazyOperandValueProfiles(const ConcurrentJSLocker&amp;)
 558     {
 559         return m_lazyOperandValueProfiles;
 560     }
 561 #endif // ENABLE(DFG_JIT)
 562 
 563     // Constant Pool
 564 #if ENABLE(DFG_JIT)
 565     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers() + numberOfDFGIdentifiers(); }
 566     size_t numberOfDFGIdentifiers() const;
 567     const Identifier&amp; identifier(int index) const;
 568 #else
 569     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers(); }
 570     const Identifier&amp; identifier(int index) const { return m_unlinkedCode-&gt;identifier(index); }
 571 #endif
 572 
 573     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants() { return m_constantRegisters; }
 574     Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation() { return m_constantsSourceCodeRepresentation; }
 575     unsigned addConstant(const ConcurrentJSLocker&amp;, JSValue v)
 576     {
 577         unsigned result = m_constantRegisters.size();
 578         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 579         m_constantRegisters.last().set(*m_vm, this, v);
 580         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 581         return result;
 582     }
 583 
 584     unsigned addConstantLazily(const ConcurrentJSLocker&amp;)
 585     {
 586         unsigned result = m_constantRegisters.size();
 587         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 588         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 589         return result;
 590     }
 591 
 592     const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constantRegisters() { return m_constantRegisters; }
 593     WriteBarrier&lt;Unknown&gt;&amp; constantRegister(VirtualRegister reg) { return m_constantRegisters[reg.toConstantIndex()]; }
 594     ALWAYS_INLINE JSValue getConstant(VirtualRegister reg) const { return m_constantRegisters[reg.toConstantIndex()].get(); }
 595     ALWAYS_INLINE SourceCodeRepresentation constantSourceCodeRepresentation(VirtualRegister reg) const { return m_constantsSourceCodeRepresentation[reg.toConstantIndex()]; }
 596 
 597     FunctionExecutable* functionDecl(int index) { return m_functionDecls[index].get(); }
 598     int numberOfFunctionDecls() { return m_functionDecls.size(); }
 599     FunctionExecutable* functionExpr(int index) { return m_functionExprs[index].get(); }
 600 
 601     const BitVector&amp; bitVector(size_t i) { return m_unlinkedCode-&gt;bitVector(i); }
 602 
 603     Heap* heap() const { return &amp;m_vm-&gt;heap; }
 604     JSGlobalObject* globalObject() { return m_globalObject.get(); }
 605 
 606     JSGlobalObject* globalObjectFor(CodeOrigin);
 607 
 608     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 609     {
 610         return m_unlinkedCode-&gt;livenessAnalysis(this);
 611     }
 612 
 613     void validate();
 614 
 615     // Jump Tables
 616 
 617     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }
 618     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 619     void clearSwitchJumpTables()
 620     {
 621         if (!m_rareData)
 622             return;
 623         m_rareData-&gt;m_switchJumpTables.clear();
 624     }
 625 #if ENABLE(DFG_JIT)
 626     void addSwitchJumpTableFromProfiledCodeBlock(SimpleJumpTable&amp; profiled)
 627     {
 628         createRareDataIfNecessary();
 629         m_rareData-&gt;m_switchJumpTables.append(profiled.cloneNonJITPart());
 630     }
 631 #endif
 632 
 633     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }
 634     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 635 
 636     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 637 
 638     enum class ShrinkMode {
 639         // Shrink prior to generating machine code that may point directly into vectors.
 640         EarlyShrink,
 641 
 642         // Shrink after generating machine code, and after possibly creating new vectors
 643         // and appending to others. At this time it is not safe to shrink certain vectors
 644         // because we would have generated machine code that references them directly.
 645         LateShrink,
 646     };
 647     void shrinkToFit(const ConcurrentJSLocker&amp;, ShrinkMode);
 648 
 649     // Functions for controlling when JITting kicks in, in a mixed mode
 650     // execution world.
 651 
 652     bool checkIfJITThresholdReached()
 653     {
 654         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 655     }
 656 
 657     void dontJITAnytimeSoon()
 658     {
 659         m_llintExecuteCounter.deferIndefinitely();
 660     }
 661 
 662     int32_t thresholdForJIT(int32_t threshold);
 663     void jitAfterWarmUp();
 664     void jitSoon();
 665 
 666     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 667     {
 668         return m_llintExecuteCounter;
 669     }
 670 
 671     typedef HashMap&lt;std::tuple&lt;StructureID, unsigned&gt;, Vector&lt;LLIntPrototypeLoadAdaptiveStructureWatchpoint&gt;&gt; StructureWatchpointMap;
 672     StructureWatchpointMap&amp; llintGetByIdWatchpointMap() { return m_llintGetByIdWatchpointMap; }
 673 
 674     // Functions for controlling when tiered compilation kicks in. This
 675     // controls both when the optimizing compiler is invoked and when OSR
 676     // entry happens. Two triggers exist: the loop trigger and the return
 677     // trigger. In either case, when an addition to m_jitExecuteCounter
 678     // causes it to become non-negative, the optimizing compiler is
 679     // invoked. This includes a fast check to see if this CodeBlock has
 680     // already been optimized (i.e. replacement() returns a CodeBlock
 681     // that was optimized with a higher tier JIT than this one). In the
 682     // case of the loop trigger, if the optimized compilation succeeds
 683     // (or has already succeeded in the past) then OSR is attempted to
 684     // redirect program flow into the optimized code.
 685 
 686     // These functions are called from within the optimization triggers,
 687     // and are used as a single point at which we define the heuristics
 688     // for how much warm-up is mandated before the next optimization
 689     // trigger files. All CodeBlocks start out with optimizeAfterWarmUp(),
 690     // as this is called from the CodeBlock constructor.
 691 
 692     // When we observe a lot of speculation failures, we trigger a
 693     // reoptimization. But each time, we increase the optimization trigger
 694     // to avoid thrashing.
 695     JS_EXPORT_PRIVATE unsigned reoptimizationRetryCounter() const;
 696     void countReoptimization();
 697 
 698 #if !ENABLE(C_LOOP)
 699     const RegisterAtOffsetList* calleeSaveRegisters() const;
 700 
 701     static unsigned numberOfLLIntBaselineCalleeSaveRegisters() { return RegisterSet::llintBaselineCalleeSaveRegisters().numberOfSetRegisters(); }
 702     static size_t llintBaselineCalleeSaveSpaceAsVirtualRegisters();
 703     size_t calleeSaveSpaceAsVirtualRegisters();
 704 #else
 705     static unsigned numberOfLLIntBaselineCalleeSaveRegisters() { return 0; }
 706     static size_t llintBaselineCalleeSaveSpaceAsVirtualRegisters() { return 1; };
 707     size_t calleeSaveSpaceAsVirtualRegisters() { return 0; }
 708 #endif
 709 
 710 #if ENABLE(JIT)
 711     unsigned numberOfDFGCompiles();
 712 
 713     int32_t codeTypeThresholdMultiplier() const;
 714 
 715     int32_t adjustedCounterValue(int32_t desiredThreshold);
 716 
 717     int32_t* addressOfJITExecuteCounter()
 718     {
 719         return &amp;m_jitExecuteCounter.m_counter;
 720     }
 721 
 722     static ptrdiff_t offsetOfJITExecuteCounter() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_counter); }
 723     static ptrdiff_t offsetOfJITExecutionActiveThreshold() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_activeThreshold); }
 724     static ptrdiff_t offsetOfJITExecutionTotalCount() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_totalCount); }
 725 
 726     const BaselineExecutionCounter&amp; jitExecuteCounter() const { return m_jitExecuteCounter; }
 727 
 728     unsigned optimizationDelayCounter() const { return m_optimizationDelayCounter; }
 729 
 730     // Check if the optimization threshold has been reached, and if not,
 731     // adjust the heuristics accordingly. Returns true if the threshold has
 732     // been reached.
 733     bool checkIfOptimizationThresholdReached();
 734 
 735     // Call this to force the next optimization trigger to fire. This is
 736     // rarely wise, since optimization triggers are typically more
 737     // expensive than executing baseline code.
 738     void optimizeNextInvocation();
 739 
 740     // Call this to prevent optimization from happening again. Note that
 741     // optimization will still happen after roughly 2^29 invocations,
 742     // so this is really meant to delay that as much as possible. This
 743     // is called if optimization failed, and we expect it to fail in
 744     // the future as well.
 745     void dontOptimizeAnytimeSoon();
 746 
 747     // Call this to reinitialize the counter to its starting state,
 748     // forcing a warm-up to happen before the next optimization trigger
 749     // fires. This is called in the CodeBlock constructor. It also
 750     // makes sense to call this if an OSR exit occurred. Note that
 751     // OSR exit code is code generated, so the value of the execute
 752     // counter that this corresponds to is also available directly.
 753     void optimizeAfterWarmUp();
 754 
 755     // Call this to force an optimization trigger to fire only after
 756     // a lot of warm-up.
 757     void optimizeAfterLongWarmUp();
 758 
 759     // Call this to cause an optimization trigger to fire soon, but
 760     // not necessarily the next one. This makes sense if optimization
 761     // succeeds. Successful optimization means that all calls are
 762     // relinked to the optimized code, so this only affects call
 763     // frames that are still executing this CodeBlock. The value here
 764     // is tuned to strike a balance between the cost of OSR entry
 765     // (which is too high to warrant making every loop back edge to
 766     // trigger OSR immediately) and the cost of executing baseline
 767     // code (which is high enough that we don&#39;t necessarily want to
 768     // have a full warm-up). The intuition for calling this instead of
 769     // optimizeNextInvocation() is for the case of recursive functions
 770     // with loops. Consider that there may be N call frames of some
 771     // recursive function, for a reasonably large value of N. The top
 772     // one triggers optimization, and then returns, and then all of
 773     // the others return. We don&#39;t want optimization to be triggered on
 774     // each return, as that would be superfluous. It only makes sense
 775     // to trigger optimization if one of those functions becomes hot
 776     // in the baseline code.
 777     void optimizeSoon();
 778 
 779     void forceOptimizationSlowPathConcurrently();
 780 
 781     void setOptimizationThresholdBasedOnCompilationResult(CompilationResult);
 782 
 783     BytecodeIndex bytecodeIndexForExit(BytecodeIndex) const;
 784     uint32_t osrExitCounter() const { return m_osrExitCounter; }
 785 
 786     void countOSRExit() { m_osrExitCounter++; }
 787 
 788     enum class OptimizeAction { None, ReoptimizeNow };
 789 #if ENABLE(DFG_JIT)
 790     OptimizeAction updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp;);
 791 #endif
 792 
 793     static ptrdiff_t offsetOfOSRExitCounter() { return OBJECT_OFFSETOF(CodeBlock, m_osrExitCounter); }
 794 
 795     uint32_t adjustedExitCountThreshold(uint32_t desiredThreshold);
 796     uint32_t exitCountThresholdForReoptimization();
 797     uint32_t exitCountThresholdForReoptimizationFromLoop();
 798     bool shouldReoptimizeNow();
 799     bool shouldReoptimizeFromLoopNow();
 800 
 801 #else // No JIT
 802     void optimizeAfterWarmUp() { }
 803     unsigned numberOfDFGCompiles() { return 0; }
 804 #endif
 805 
 806     bool shouldOptimizeNow();
 807     void updateAllValueProfilePredictions();
 808     void updateAllArrayPredictions();
 809     void updateAllPredictions();
 810 
 811     unsigned frameRegisterCount();
 812     int stackPointerOffset();
 813 
 814     bool hasOpDebugForLineAndColumn(unsigned line, Optional&lt;unsigned&gt; column);
 815 
 816     bool hasDebuggerRequests() const { return m_debuggerRequests; }
 817     void* debuggerRequestsAddress() { return &amp;m_debuggerRequests; }
 818 
 819     void addBreakpoint(unsigned numBreakpoints);
 820     void removeBreakpoint(unsigned numBreakpoints)
 821     {
 822         ASSERT(m_numBreakpoints &gt;= numBreakpoints);
 823         m_numBreakpoints -= numBreakpoints;
 824     }
 825 
 826     enum SteppingMode {
 827         SteppingModeDisabled,
 828         SteppingModeEnabled
 829     };
 830     void setSteppingMode(SteppingMode);
 831 
 832     void clearDebuggerRequests()
 833     {
 834         m_steppingMode = SteppingModeDisabled;
 835         m_numBreakpoints = 0;
 836     }
 837 
 838     bool wasCompiledWithDebuggingOpcodes() const { return m_unlinkedCode-&gt;wasCompiledWithDebuggingOpcodes(); }
 839 
 840     // This is intentionally public; it&#39;s the responsibility of anyone doing any
 841     // of the following to hold the lock:
 842     //
 843     // - Modifying any inline cache in this code block.
 844     //
 845     // - Quering any inline cache in this code block, from a thread other than
 846     //   the main thread.
 847     //
 848     // Additionally, it&#39;s only legal to modify the inline cache on the main
 849     // thread. This means that the main thread can query the inline cache without
 850     // locking. This is crucial since executing the inline cache is effectively
 851     // &quot;querying&quot; it.
 852     //
 853     // Another exception to the rules is that the GC can do whatever it wants
 854     // without holding any locks, because the GC is guaranteed to wait until any
 855     // concurrent compilation threads finish what they&#39;re doing.
 856     mutable ConcurrentJSLock m_lock;
 857 
 858     bool m_shouldAlwaysBeInlined; // Not a bitfield because the JIT wants to store to it.
 859 
 860 #if ENABLE(JIT)
 861     unsigned m_capabilityLevelState : 2; // DFG::CapabilityLevel
 862 #endif
 863 
 864     bool m_allTransitionsHaveBeenMarked : 1; // Initialized and used on every GC.
 865 
 866     bool m_didFailJITCompilation : 1;
 867     bool m_didFailFTLCompilation : 1;
 868     bool m_hasBeenCompiledWithFTL : 1;
 869 
 870     bool m_hasLinkedOSRExit : 1;
 871     bool m_isEligibleForLLIntDowngrade : 1;
 872 
 873     // Internal methods for use by validation code. It would be private if it wasn&#39;t
 874     // for the fact that we use it from anonymous namespaces.
 875     void beginValidationDidFail();
 876     NO_RETURN_DUE_TO_CRASH void endValidationDidFail();
 877 
 878     struct RareData {
 879         WTF_MAKE_STRUCT_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(CodeBlockRareData);
 880     public:
 881         Vector&lt;HandlerInfo&gt; m_exceptionHandlers;
 882 
 883         // Jump Tables
 884         Vector&lt;SimpleJumpTable&gt; m_switchJumpTables;
 885         Vector&lt;StringJumpTable&gt; m_stringSwitchJumpTables;
 886 
 887         Vector&lt;std::unique_ptr&lt;ValueProfileAndVirtualRegisterBuffer&gt;&gt; m_catchProfiles;
 888 
 889         DirectEvalCodeCache m_directEvalCodeCache;
 890     };
 891 
 892     void clearExceptionHandlers()
 893     {
 894         if (m_rareData)
 895             m_rareData-&gt;m_exceptionHandlers.clear();
 896     }
 897 
 898     void appendExceptionHandler(const HandlerInfo&amp; handler)
 899     {
 900         createRareDataIfNecessary(); // We may be handling the exception of an inlined call frame.
 901         m_rareData-&gt;m_exceptionHandlers.append(handler);
 902     }
 903 
 904     DisposableCallSiteIndex newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite);
 905 
 906     void ensureCatchLivenessIsComputedForBytecodeIndex(BytecodeIndex);
 907 
 908     bool hasTailCalls() const { return m_unlinkedCode-&gt;hasTailCalls(); }
 909 
 910     template&lt;typename Metadata&gt;
 911     Metadata&amp; metadata(OpcodeID opcodeID, unsigned metadataID)
 912     {
 913         ASSERT(m_metadata);
 914         return bitwise_cast&lt;Metadata*&gt;(m_metadata-&gt;get(opcodeID))[metadataID];
 915     }
 916 
 917     size_t metadataSizeInBytes()
 918     {
 919         return m_unlinkedCode-&gt;metadataSizeInBytes();
 920     }
 921 
 922     MetadataTable* metadataTable() { return m_metadata.get(); }
 923     const void* instructionsRawPointer() { return m_instructionsRawPointer; }
 924 
 925 protected:
 926     void finalizeLLIntInlineCaches();
 927 #if ENABLE(JIT)
 928     void finalizeBaselineJITInlineCaches();
 929 #endif
 930 #if ENABLE(DFG_JIT)
 931     void tallyFrequentExitSites();
 932 #else
 933     void tallyFrequentExitSites() { }
 934 #endif
 935 
 936 private:
 937     friend class CodeBlockSet;
 938     friend class ExecutableToCodeBlockEdge;
 939 
 940     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 941 
 942     CodeBlock* specialOSREntryBlockOrNull();
 943 
 944     void noticeIncomingCall(CallFrame* callerFrame);
 945 
 946     double optimizationThresholdScalingFactor();
 947 
 948     void updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);
 949 
 950     void setConstantIdentifierSetRegisters(VM&amp;, const RefCountedArray&lt;ConstantIdentifierSetEntry&gt;&amp; constants);
 951 
 952     void setConstantRegisters(const RefCountedArray&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const RefCountedArray&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable);
 953 
 954     void replaceConstant(VirtualRegister reg, JSValue value)
 955     {
 956         ASSERT(reg.isConstant() &amp;&amp; static_cast&lt;size_t&gt;(reg.toConstantIndex()) &lt; m_constantRegisters.size());
 957         m_constantRegisters[reg.toConstantIndex()].set(*m_vm, this, value);
 958     }
 959 
 960     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
 961     bool shouldJettisonDueToWeakReference(VM&amp;);
 962     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 963 
 964     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 965     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 966 
 967     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 968     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 969     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 970 
 971     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 972     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
 973     ValueProfile* tryGetValueProfileForBytecodeIndex(BytecodeIndex);
 974 
 975     Seconds timeSinceCreation()
 976     {
 977         return MonotonicTime::now() - m_creationTime;
 978     }
 979 
 980     void createRareDataIfNecessary()
 981     {
 982         if (!m_rareData) {
 983             auto rareData = makeUnique&lt;RareData&gt;();
 984             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 985             m_rareData = WTFMove(rareData);
 986         }
 987     }
 988 
 989     void insertBasicBlockBoundariesForControlFlowProfiler();
 990     void ensureCatchLivenessIsComputedForBytecodeIndexSlow(const OpCatch&amp;, BytecodeIndex);
 991 
 992     int m_numCalleeLocals;
 993     int m_numVars;
 994     int m_numParameters;
 995     int m_numberOfArgumentsToSkip { 0 };
 996     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 997     union {
 998         unsigned m_debuggerRequests;
 999         struct {
1000             unsigned m_hasDebuggerStatement : 1;
1001             unsigned m_steppingMode : 1;
1002             unsigned m_numBreakpoints : 30;
1003         };
1004     };
1005     unsigned m_bytecodeCost { 0 };
1006     VirtualRegister m_scopeRegister;
1007     mutable CodeBlockHash m_hash;
1008 
1009     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
1010     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
1011     WriteBarrier&lt;ExecutableToCodeBlockEdge&gt; m_ownerEdge;
1012     // m_vm must be a pointer (instead of a reference) because the JSCLLIntOffsetsExtractor
1013     // cannot handle it being a reference.
1014     VM* m_vm;
1015 
1016     const void* m_instructionsRawPointer { nullptr };
1017     SentinelLinkedList&lt;LLIntCallLinkInfo, PackedRawSentinelNode&lt;LLIntCallLinkInfo&gt;&gt; m_incomingLLIntCalls;
1018     StructureWatchpointMap m_llintGetByIdWatchpointMap;
1019     RefPtr&lt;JITCode&gt; m_jitCode;
1020 #if ENABLE(JIT)
1021     std::unique_ptr&lt;JITData&gt; m_jitData;
1022 #endif
1023 #if ENABLE(DFG_JIT)
1024     // This is relevant to non-DFG code blocks that serve as the profiled code block
1025     // for DFG code blocks.
1026     CompressedLazyOperandValueProfileHolder m_lazyOperandValueProfiles;
1027 #endif
1028     RefCountedArray&lt;ValueProfile&gt; m_argumentValueProfiles;
1029 
1030     // Constant Pool
1031     COMPILE_ASSERT(sizeof(Register) == sizeof(WriteBarrier&lt;Unknown&gt;), Register_must_be_same_size_as_WriteBarrier_Unknown);
1032     // TODO: This could just be a pointer to m_unlinkedCodeBlock&#39;s data, but the DFG mutates
1033     // it, so we&#39;re stuck with it for now.
1034     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt; m_constantRegisters;
1035     Vector&lt;SourceCodeRepresentation&gt; m_constantsSourceCodeRepresentation;
1036     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionDecls;
1037     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
1038 
1039     WriteBarrier&lt;CodeBlock&gt; m_alternative;
1040 
1041     BaselineExecutionCounter m_llintExecuteCounter;
1042 
1043     BaselineExecutionCounter m_jitExecuteCounter;
1044     uint32_t m_osrExitCounter;
1045 
1046     uint16_t m_optimizationDelayCounter;
1047     uint16_t m_reoptimizationRetryCounter;
1048 
1049     RefPtr&lt;MetadataTable&gt; m_metadata;
1050 
1051     MonotonicTime m_creationTime;
1052     double m_previousCounter { 0 };
1053 
1054     std::unique_ptr&lt;RareData&gt; m_rareData;
1055 };
1056 
1057 template &lt;typename ExecutableType&gt;
1058 Exception* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)
1059 {
1060     if (hasJITCodeFor(kind)) {
1061         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1062             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1063         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1064             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1065         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1066             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1067         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1068             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1069         else
1070             RELEASE_ASSERT_NOT_REACHED();
1071         return nullptr;
1072     }
1073     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1074 }
1075 
1076 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
1077     do { \
1078         if (codeBlock) \
1079             (codeBlock-&gt;vm().logEvent(codeBlock, summary, [&amp;] () { return toCString details; })); \
1080     } while (0)
1081 
1082 
1083 void setPrinter(Printer::PrintRecord&amp;, CodeBlock*);
1084 
1085 } // namespace JSC
1086 
1087 namespace WTF {
1088 
1089 JS_EXPORT_PRIVATE void printInternal(PrintStream&amp;, JSC::CodeBlock*);
1090 
1091 } // namespace WTF
    </pre>
  </body>
</html>