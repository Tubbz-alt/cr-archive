<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/gc/shenandoah/shenandoahCollectionSet.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2016, 2019, Red Hat, Inc. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahCollectionSet.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
 31 #include &quot;runtime/atomic.hpp&quot;
 32 #include &quot;services/memTracker.hpp&quot;
 33 #include &quot;utilities/copy.hpp&quot;
 34 
 35 ShenandoahCollectionSet::ShenandoahCollectionSet(ShenandoahHeap* heap, char* heap_base, size_t size) :
 36   _map_size(heap-&gt;num_regions()),
 37   _region_size_bytes_shift(ShenandoahHeapRegion::region_size_bytes_shift()),
 38   _map_space(align_up(((uintx)heap_base + size) &gt;&gt; _region_size_bytes_shift, os::vm_allocation_granularity())),
 39   _cset_map(_map_space.base() + ((uintx)heap_base &gt;&gt; _region_size_bytes_shift)),
 40   _biased_cset_map(_map_space.base()),
 41   _heap(heap),
 42   _garbage(0),
 43   _live_data(0),
 44   _used(0),
 45   _region_count(0),
 46   _current_index(0) {
 47 
 48   // The collection set map is reserved to cover the entire heap *and* zero addresses.
 49   // This is needed to accept in-cset checks for both heap oops and NULLs, freeing
 50   // high-performance code from checking for NULL first.
 51   //
 52   // Since heap_base can be far away, committing the entire map would waste memory.
 53   // Therefore, we only commit the parts that are needed to operate: the heap view,
 54   // and the zero page.
 55   //
 56   // Note: we could instead commit the entire map, and piggyback on OS virtual memory
 57   // subsystem for mapping not-yet-written-to pages to a single physical backing page,
 58   // but this is not guaranteed, and would confuse NMT and other memory accounting tools.
 59 
 60   MemTracker::record_virtual_memory_type(_map_space.base(), mtGC);
 61 
 62   size_t page_size = (size_t)os::vm_page_size();
 63 
 64   if (!_map_space.special()) {
 65     // Commit entire pages that cover the heap cset map.
 66     char* bot_addr = align_down(_cset_map, page_size);
 67     char* top_addr = align_up(_cset_map + _map_size, page_size);
 68     os::commit_memory_or_exit(bot_addr, pointer_delta(top_addr, bot_addr, 1), false,
 69                               &quot;Unable to commit collection set bitmap: heap&quot;);
 70 
 71     // Commit the zero page, if not yet covered by heap cset map.
 72     if (bot_addr != _biased_cset_map) {
 73       os::commit_memory_or_exit(_biased_cset_map, page_size, false,
 74                                 &quot;Unable to commit collection set bitmap: zero page&quot;);
 75     }
 76   }
 77 
 78   Copy::zero_to_bytes(_cset_map, _map_size);
 79   Copy::zero_to_bytes(_biased_cset_map, page_size);
 80 }
 81 
 82 void ShenandoahCollectionSet::add_region(ShenandoahHeapRegion* r) {
 83   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Must be at a safepoint&quot;);
 84   assert(Thread::current()-&gt;is_VM_thread(), &quot;Must be VMThread&quot;);
 85   assert(!is_in(r), &quot;Already in collection set&quot;);
 86   _cset_map[r-&gt;region_number()] = 1;
 87   _region_count ++;
 88   _garbage += r-&gt;garbage();
 89   _live_data += r-&gt;get_live_data_bytes();
 90   _used += r-&gt;used();
 91 }
 92 
 93 bool ShenandoahCollectionSet::add_region_check_for_duplicates(ShenandoahHeapRegion* r) {
 94   if (!is_in(r)) {
 95     add_region(r);
 96     return true;
 97   } else {
 98     return false;
 99   }
100 }
101 
102 void ShenandoahCollectionSet::remove_region(ShenandoahHeapRegion* r) {
103   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Must be at a safepoint&quot;);
104   assert(Thread::current()-&gt;is_VM_thread(), &quot;Must be VMThread&quot;);
105   assert(is_in(r), &quot;Not in collection set&quot;);
106   _cset_map[r-&gt;region_number()] = 0;
107   _region_count --;
108 }
109 
110 void ShenandoahCollectionSet::update_region_status() {
111   for (size_t index = 0; index &lt; _heap-&gt;num_regions(); index ++) {
112     ShenandoahHeapRegion* r = _heap-&gt;get_region(index);
113     if (is_in(r)) {
114       r-&gt;make_cset();
115     } else {
116       assert (!r-&gt;is_cset(), &quot;should not be cset&quot;);
117     }
118   }
119 }
120 
121 void ShenandoahCollectionSet::clear() {
122   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Must be at a safepoint&quot;);
123   Copy::zero_to_bytes(_cset_map, _map_size);
124 
125 #ifdef ASSERT
126   for (size_t index = 0; index &lt; _heap-&gt;num_regions(); index ++) {
127     assert (!_heap-&gt;get_region(index)-&gt;is_cset(), &quot;should have been cleared before&quot;);
128   }
129 #endif
130 
131   _garbage = 0;
132   _live_data = 0;
133   _used = 0;
134 
135   _region_count = 0;
136   _current_index = 0;
137 }
138 
139 ShenandoahHeapRegion* ShenandoahCollectionSet::claim_next() {
140   size_t num_regions = _heap-&gt;num_regions();
141   if (_current_index &gt;= (jint)num_regions) {
142     return NULL;
143   }
144 
145   jint saved_current = _current_index;
146   size_t index = (size_t)saved_current;
147 
148   while(index &lt; num_regions) {
149     if (is_in(index)) {
150       jint cur = Atomic::cmpxchg(&amp;_current_index, saved_current, (jint)(index + 1));
151       assert(cur &gt;= (jint)saved_current, &quot;Must move forward&quot;);
152       if (cur == saved_current) {
153         assert(is_in(index), &quot;Invariant&quot;);
154         return _heap-&gt;get_region(index);
155       } else {
156         index = (size_t)cur;
157         saved_current = cur;
158       }
159     } else {
160       index ++;
161     }
162   }
163   return NULL;
164 }
165 
166 ShenandoahHeapRegion* ShenandoahCollectionSet::next() {
167   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Must be at a safepoint&quot;);
168   assert(Thread::current()-&gt;is_VM_thread(), &quot;Must be VMThread&quot;);
169   size_t num_regions = _heap-&gt;num_regions();
170   for (size_t index = (size_t)_current_index; index &lt; num_regions; index ++) {
171     if (is_in(index)) {
172       _current_index = (jint)(index + 1);
173       return _heap-&gt;get_region(index);
174     }
175   }
176 
177   return NULL;
178 }
179 
180 void ShenandoahCollectionSet::print_on(outputStream* out) const {
181   out-&gt;print_cr(&quot;Collection Set : &quot; SIZE_FORMAT &quot;&quot;, count());
182 
183   debug_only(size_t regions = 0;)
184   for (size_t index = 0; index &lt; _heap-&gt;num_regions(); index ++) {
185     if (is_in(index)) {
186       _heap-&gt;get_region(index)-&gt;print_on(out);
187       debug_only(regions ++;)
188     }
189   }
190   assert(regions == count(), &quot;Must match&quot;);
191 }
    </pre>
  </body>
</html>