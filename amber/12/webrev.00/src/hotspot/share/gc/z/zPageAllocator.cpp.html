<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/gc/z/zPageAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2015, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
 26 #include &quot;gc/z/zAddress.inline.hpp&quot;
 27 #include &quot;gc/z/zCollectedHeap.hpp&quot;
 28 #include &quot;gc/z/zFuture.inline.hpp&quot;
 29 #include &quot;gc/z/zGlobals.hpp&quot;
 30 #include &quot;gc/z/zLock.inline.hpp&quot;
 31 #include &quot;gc/z/zPage.inline.hpp&quot;
 32 #include &quot;gc/z/zPageAllocator.hpp&quot;
 33 #include &quot;gc/z/zPageCache.inline.hpp&quot;
 34 #include &quot;gc/z/zSafeDelete.inline.hpp&quot;
 35 #include &quot;gc/z/zStat.hpp&quot;
 36 #include &quot;gc/z/zTask.hpp&quot;
 37 #include &quot;gc/z/zTracer.inline.hpp&quot;
 38 #include &quot;gc/z/zWorkers.hpp&quot;
 39 #include &quot;jfr/jfrEvents.hpp&quot;
 40 #include &quot;runtime/globals.hpp&quot;
 41 #include &quot;runtime/init.hpp&quot;
 42 #include &quot;runtime/java.hpp&quot;
 43 #include &quot;utilities/debug.hpp&quot;
 44 
 45 static const ZStatCounter       ZCounterAllocationRate(&quot;Memory&quot;, &quot;Allocation Rate&quot;, ZStatUnitBytesPerSecond);
 46 static const ZStatCounter       ZCounterPageCacheFlush(&quot;Memory&quot;, &quot;Page Cache Flush&quot;, ZStatUnitBytesPerSecond);
 47 static const ZStatCounter       ZCounterUncommit(&quot;Memory&quot;, &quot;Uncommit&quot;, ZStatUnitBytesPerSecond);
 48 static const ZStatCriticalPhase ZCriticalPhaseAllocationStall(&quot;Allocation Stall&quot;);
 49 
 50 class ZPageAllocRequest : public StackObj {
 51   friend class ZList&lt;ZPageAllocRequest&gt;;
 52 
 53 private:
 54   const uint8_t                _type;
 55   const size_t                 _size;
 56   const ZAllocationFlags       _flags;
 57   const unsigned int           _total_collections;
 58   ZListNode&lt;ZPageAllocRequest&gt; _node;
 59   ZFuture&lt;ZPage*&gt;              _result;
 60 
 61 public:
 62   ZPageAllocRequest(uint8_t type, size_t size, ZAllocationFlags flags, unsigned int total_collections) :
 63       _type(type),
 64       _size(size),
 65       _flags(flags),
 66       _total_collections(total_collections),
 67       _node(),
 68       _result() {}
 69 
 70   uint8_t type() const {
 71     return _type;
 72   }
 73 
 74   size_t size() const {
 75     return _size;
 76   }
 77 
 78   ZAllocationFlags flags() const {
 79     return _flags;
 80   }
 81 
 82   unsigned int total_collections() const {
 83     return _total_collections;
 84   }
 85 
 86   ZPage* peek() {
 87     return _result.peek();
 88   }
 89 
 90   ZPage* wait() {
 91     return _result.get();
 92   }
 93 
 94   void satisfy(ZPage* page) {
 95     _result.set(page);
 96   }
 97 };
 98 
 99 ZPage* const ZPageAllocator::gc_marker = (ZPage*)-1;
100 
101 ZPageAllocator::ZPageAllocator(ZWorkers* workers,
102                                size_t min_capacity,
103                                size_t initial_capacity,
104                                size_t max_capacity,
105                                size_t max_reserve) :
106     _lock(),
107     _virtual(max_capacity),
108     _physical(),
109     _cache(),
110     _min_capacity(min_capacity),
111     _max_capacity(max_capacity),
112     _max_reserve(max_reserve),
113     _current_max_capacity(max_capacity),
114     _capacity(0),
115     _used_high(0),
116     _used_low(0),
117     _used(0),
118     _allocated(0),
119     _reclaimed(0),
120     _queue(),
121     _satisfied(),
122     _safe_delete(),
123     _uncommit(false),
124     _initialized(false) {
125 
126   if (!_virtual.is_initialized() || !_physical.is_initialized()) {
127     return;
128   }
129 
130   log_info(gc, init)(&quot;Min Capacity: &quot; SIZE_FORMAT &quot;M&quot;, min_capacity / M);
131   log_info(gc, init)(&quot;Initial Capacity: &quot; SIZE_FORMAT &quot;M&quot;, initial_capacity / M);
132   log_info(gc, init)(&quot;Max Capacity: &quot; SIZE_FORMAT &quot;M&quot;, max_capacity / M);
133   log_info(gc, init)(&quot;Max Reserve: &quot; SIZE_FORMAT &quot;M&quot;, max_reserve / M);
134   log_info(gc, init)(&quot;Pre-touch: %s&quot;, AlwaysPreTouch ? &quot;Enabled&quot; : &quot;Disabled&quot;);
135 
136   // Warn if system limits could stop us from reaching max capacity
137   _physical.warn_commit_limits(max_capacity);
138 
139   // Commit initial capacity
140   _capacity = _physical.commit(initial_capacity);
141   if (_capacity != initial_capacity) {
142     log_error(gc)(&quot;Failed to allocate initial Java heap (&quot; SIZE_FORMAT &quot;M)&quot;, initial_capacity / M);
143     return;
144   }
145 
146   // If uncommit is not explicitly disabled, max capacity is greater than
147   // min capacity, and uncommit is supported by the platform, then we will
148   // try to uncommit unused memory.
149   _uncommit = ZUncommit &amp;&amp; (max_capacity &gt; min_capacity) &amp;&amp; _physical.supports_uncommit();
150   if (_uncommit) {
151     log_info(gc, init)(&quot;Uncommit: Enabled, Delay: &quot; UINTX_FORMAT &quot;s&quot;, ZUncommitDelay);
152   } else {
153     log_info(gc, init)(&quot;Uncommit: Disabled&quot;);
154   }
155 
156   // Pre-map initial capacity
157   prime_cache(workers, initial_capacity);
158 
159   // Successfully initialized
160   _initialized = true;
161 }
162 
163 class ZPreTouchTask : public ZTask {
164 private:
165   const ZPhysicalMemoryManager* const _physical;
166   volatile uintptr_t                  _start;
167   const uintptr_t                     _end;
168 
169 public:
170   ZPreTouchTask(const ZPhysicalMemoryManager* physical, uintptr_t start, uintptr_t end) :
171       ZTask(&quot;ZPreTouchTask&quot;),
172       _physical(physical),
173       _start(start),
174       _end(end) {}
175 
176   virtual void work() {
177     for (;;) {
178       // Get granule offset
179       const size_t size = ZGranuleSize;
180       const uintptr_t offset = Atomic::fetch_and_add(&amp;_start, size);
181       if (offset &gt;= _end) {
182         // Done
183         break;
184       }
185 
186       // Pre-touch granule
187       _physical-&gt;pretouch(offset, size);
188     }
189   }
190 };
191 
192 void ZPageAllocator::prime_cache(ZWorkers* workers, size_t size) {
193   // Allocate physical memory
194   const ZPhysicalMemory pmem = _physical.alloc(size);
195   guarantee(!pmem.is_null(), &quot;Invalid size&quot;);
196 
197   // Allocate virtual memory
198   const ZVirtualMemory vmem = _virtual.alloc(size, true /* alloc_from_front */);
199   guarantee(!vmem.is_null(), &quot;Invalid size&quot;);
200 
201   // Allocate page
202   ZPage* const page = new ZPage(vmem, pmem);
203 
204   // Map page
205   map_page(page);
206   page-&gt;set_pre_mapped();
207 
208   if (AlwaysPreTouch) {
209     // Pre-touch page
210     ZPreTouchTask task(&amp;_physical, page-&gt;start(), page-&gt;end());
211     workers-&gt;run_parallel(&amp;task);
212   }
213 
214   // Add page to cache
215   page-&gt;set_last_used();
216   _cache.free_page(page);
217 }
218 
219 bool ZPageAllocator::is_initialized() const {
220   return _initialized;
221 }
222 
223 size_t ZPageAllocator::min_capacity() const {
224   return _min_capacity;
225 }
226 
227 size_t ZPageAllocator::max_capacity() const {
228   return _max_capacity;
229 }
230 
231 size_t ZPageAllocator::soft_max_capacity() const {
232   // Note that SoftMaxHeapSize is a manageable flag
233   return MIN2(SoftMaxHeapSize, _current_max_capacity);
234 }
235 
236 size_t ZPageAllocator::capacity() const {
237   return _capacity;
238 }
239 
240 size_t ZPageAllocator::max_reserve() const {
241   return _max_reserve;
242 }
243 
244 size_t ZPageAllocator::used_high() const {
245   return _used_high;
246 }
247 
248 size_t ZPageAllocator::used_low() const {
249   return _used_low;
250 }
251 
252 size_t ZPageAllocator::used() const {
253   return _used;
254 }
255 
256 size_t ZPageAllocator::unused() const {
257   const ssize_t unused = (ssize_t)_capacity - (ssize_t)_used - (ssize_t)_max_reserve;
258   return unused &gt; 0 ? (size_t)unused : 0;
259 }
260 
261 size_t ZPageAllocator::allocated() const {
262   return _allocated;
263 }
264 
265 size_t ZPageAllocator::reclaimed() const {
266   return _reclaimed &gt; 0 ? (size_t)_reclaimed : 0;
267 }
268 
269 void ZPageAllocator::reset_statistics() {
270   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
271   _allocated = 0;
272   _reclaimed = 0;
273   _used_high = _used_low = _used;
274 }
275 
276 void ZPageAllocator::increase_used(size_t size, bool relocation) {
277   if (relocation) {
278     // Allocating a page for the purpose of relocation has a
279     // negative contribution to the number of reclaimed bytes.
280     _reclaimed -= size;
281   }
282   _allocated += size;
283   _used += size;
284   if (_used &gt; _used_high) {
285     _used_high = _used;
286   }
287 }
288 
289 void ZPageAllocator::decrease_used(size_t size, bool reclaimed) {
290   if (reclaimed) {
291     // Only pages explicitly released with the reclaimed flag set
292     // counts as reclaimed bytes. This flag is typically true when
293     // a worker releases a page after relocation, and is typically
294     // false when we release a page to undo an allocation.
295     _reclaimed += size;
296   }
297   _used -= size;
298   if (_used &lt; _used_low) {
299     _used_low = _used;
300   }
301 }
302 
303 ZPage* ZPageAllocator::create_page(uint8_t type, size_t size) {
304   // Allocate virtual memory
305   const ZVirtualMemory vmem = _virtual.alloc(size);
306   if (vmem.is_null()) {
307     // Out of address space
308     return NULL;
309   }
310 
311   // Allocate physical memory
312   const ZPhysicalMemory pmem = _physical.alloc(size);
313   assert(!pmem.is_null(), &quot;Invalid size&quot;);
314 
315   // Allocate page
316   return new ZPage(type, vmem, pmem);
317 }
318 
319 void ZPageAllocator::destroy_page(ZPage* page) {
320   const ZVirtualMemory&amp; vmem = page-&gt;virtual_memory();
321   const ZPhysicalMemory&amp; pmem = page-&gt;physical_memory();
322 
323   // Unmap memory
324   _physical.unmap(pmem, vmem.start());
325 
326   // Free physical memory
327   _physical.free(pmem);
328 
329   // Free virtual memory
330   _virtual.free(vmem);
331 
332   // Delete page safely
333   _safe_delete(page);
334 }
335 
336 void ZPageAllocator::map_page(const ZPage* page) const {
337   // Map physical memory
338   _physical.map(page-&gt;physical_memory(), page-&gt;start());
339 }
340 
341 size_t ZPageAllocator::max_available(bool no_reserve) const {
342   size_t available = _current_max_capacity - _used;
343 
344   if (no_reserve) {
345     // The reserve should not be considered available
346     available -= MIN2(available, _max_reserve);
347   }
348 
349   return available;
350 }
351 
352 bool ZPageAllocator::ensure_available(size_t size, bool no_reserve) {
353   if (max_available(no_reserve) &lt; size) {
354     // Not enough free memory
355     return false;
356   }
357 
358   // We add the max_reserve to the requested size to avoid losing
359   // the reserve because of failure to increase capacity before
360   // reaching max capacity.
361   size += _max_reserve;
362 
363   // Don&#39;t try to increase capacity if enough unused capacity
364   // is available or if current max capacity has been reached.
365   const size_t available = _capacity - _used;
366   if (available &lt; size &amp;&amp; _capacity &lt; _current_max_capacity) {
367     // Try to increase capacity
368     const size_t commit = MIN2(size - available, _current_max_capacity - _capacity);
369     const size_t committed = _physical.commit(commit);
370     _capacity += committed;
371 
372     log_trace(gc, heap)(&quot;Make Available: Size: &quot; SIZE_FORMAT &quot;M, NoReserve: %s, &quot;
373                         &quot;Available: &quot; SIZE_FORMAT &quot;M, Commit: &quot; SIZE_FORMAT &quot;M, &quot;
374                         &quot;Committed: &quot; SIZE_FORMAT &quot;M, Capacity: &quot; SIZE_FORMAT &quot;M&quot;,
375                         size / M, no_reserve ? &quot;True&quot; : &quot;False&quot;, available / M,
376                         commit / M, committed / M, _capacity / M);
377 
378     if (committed != commit) {
379       // Failed, or partly failed, to increase capacity. Adjust current
380       // max capacity to avoid further attempts to increase capacity.
381       log_error(gc)(&quot;Forced to lower max Java heap size from &quot;
382                     SIZE_FORMAT &quot;M(%.0f%%) to &quot; SIZE_FORMAT &quot;M(%.0f%%)&quot;,
383                     _current_max_capacity / M, percent_of(_current_max_capacity, _max_capacity),
384                     _capacity / M, percent_of(_capacity, _max_capacity));
385 
386       _current_max_capacity = _capacity;
387     }
388   }
389 
390   if (!no_reserve) {
391     size -= _max_reserve;
392   }
393 
394   const size_t new_available = _capacity - _used;
395   return new_available &gt;= size;
396 }
397 
398 void ZPageAllocator::ensure_uncached_available(size_t size) {
399   assert(_capacity - _used &gt;= size, &quot;Invalid size&quot;);
400   const size_t uncached_available = _capacity - _used - _cache.available();
401   if (size &gt; uncached_available) {
402     flush_cache_for_allocation(size - uncached_available);
403   }
404 }
405 
406 ZPage* ZPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, bool no_reserve) {
407   if (!ensure_available(size, no_reserve)) {
408     // Not enough free memory
409     return NULL;
410   }
411 
412   // Try allocate page from the cache
413   ZPage* const page = _cache.alloc_page(type, size);
414   if (page != NULL) {
415     return page;
416   }
417 
418   // Try flush pages from the cache
419   ensure_uncached_available(size);
420 
421   // Create new page
422   return create_page(type, size);
423 }
424 
425 ZPage* ZPageAllocator::alloc_page_common(uint8_t type, size_t size, ZAllocationFlags flags) {
426   EventZPageAllocation event;
427 
428   ZPage* const page = alloc_page_common_inner(type, size, flags.no_reserve());
429   if (page == NULL) {
430     // Out of memory
431     return NULL;
432   }
433 
434   // Update used statistics
435   increase_used(size, flags.relocation());
436 
437   // Send trace event
438   event.commit(type, size, _used, max_available(flags.no_reserve()),
439                _cache.available(), flags.non_blocking(), flags.no_reserve());
440 
441   return page;
442 }
443 
444 void ZPageAllocator::check_out_of_memory_during_initialization() {
445   if (!is_init_completed()) {
446     vm_exit_during_initialization(&quot;java.lang.OutOfMemoryError&quot;, &quot;Java heap too small&quot;);
447   }
448 }
449 
450 ZPage* ZPageAllocator::alloc_page_blocking(uint8_t type, size_t size, ZAllocationFlags flags) {
451   // Prepare to block
452   ZPageAllocRequest request(type, size, flags, ZCollectedHeap::heap()-&gt;total_collections());
453 
454   _lock.lock();
455 
456   // Try non-blocking allocation
457   ZPage* page = alloc_page_common(type, size, flags);
458   if (page == NULL) {
459     // Allocation failed, enqueue request
460     _queue.insert_last(&amp;request);
461   }
462 
463   _lock.unlock();
464 
465   if (page == NULL) {
466     // Allocation failed
467     ZStatTimer timer(ZCriticalPhaseAllocationStall);
468     EventZAllocationStall event;
469 
470     // We can only block if VM is fully initialized
471     check_out_of_memory_during_initialization();
472 
473     do {
474       // Start asynchronous GC
475       ZCollectedHeap::heap()-&gt;collect(GCCause::_z_allocation_stall);
476 
477       // Wait for allocation to complete or fail
478       page = request.wait();
479     } while (page == gc_marker);
480 
481     {
482       //
483       // We grab the lock here for two different reasons:
484       //
485       // 1) Guard deletion of underlying semaphore. This is a workaround for
486       // a bug in sem_post() in glibc &lt; 2.21, where it&#39;s not safe to destroy
487       // the semaphore immediately after returning from sem_wait(). The
488       // reason is that sem_post() can touch the semaphore after a waiting
489       // thread have returned from sem_wait(). To avoid this race we are
490       // forcing the waiting thread to acquire/release the lock held by the
491       // posting thread. https://sourceware.org/bugzilla/show_bug.cgi?id=12674
492       //
493       // 2) Guard the list of satisfied pages.
494       //
495       ZLocker&lt;ZLock&gt; locker(&amp;_lock);
496       _satisfied.remove(&amp;request);
497     }
498 
499     event.commit(type, size);
500   }
501 
502   return page;
503 }
504 
505 ZPage* ZPageAllocator::alloc_page_nonblocking(uint8_t type, size_t size, ZAllocationFlags flags) {
506   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
507   return alloc_page_common(type, size, flags);
508 }
509 
510 ZPage* ZPageAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {
511   ZPage* const page = flags.non_blocking()
512                       ? alloc_page_nonblocking(type, size, flags)
513                       : alloc_page_blocking(type, size, flags);
514   if (page == NULL) {
515     // Out of memory
516     return NULL;
517   }
518 
519   // Map page if needed
520   if (!page-&gt;is_mapped()) {
521     map_page(page);
522   }
523 
524   // Reset page. This updates the page&#39;s sequence number and must
525   // be done after page allocation, which potentially blocked in
526   // a safepoint where the global sequence number was updated.
527   page-&gt;reset();
528 
529   // Update allocation statistics. Exclude worker threads to avoid
530   // artificial inflation of the allocation rate due to relocation.
531   if (!flags.worker_thread()) {
532     // Note that there are two allocation rate counters, which have
533     // different purposes and are sampled at different frequencies.
534     const size_t bytes = page-&gt;size();
535     ZStatInc(ZCounterAllocationRate, bytes);
536     ZStatInc(ZStatAllocRate::counter(), bytes);
537   }
538 
539   return page;
540 }
541 
542 void ZPageAllocator::satisfy_alloc_queue() {
543   for (;;) {
544     ZPageAllocRequest* const request = _queue.first();
545     if (request == NULL) {
546       // Allocation queue is empty
547       return;
548     }
549 
550     ZPage* const page = alloc_page_common(request-&gt;type(), request-&gt;size(), request-&gt;flags());
551     if (page == NULL) {
552       // Allocation could not be satisfied, give up
553       return;
554     }
555 
556     // Allocation succeeded, dequeue and satisfy request. Note that
557     // the dequeue operation must happen first, since the request
558     // will immediately be deallocated once it has been satisfied.
559     _queue.remove(request);
560     _satisfied.insert_first(request);
561     request-&gt;satisfy(page);
562   }
563 }
564 
565 void ZPageAllocator::free_page(ZPage* page, bool reclaimed) {
566   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
567 
568   // Update used statistics
569   decrease_used(page-&gt;size(), reclaimed);
570 
571   // Set time when last used
572   page-&gt;set_last_used();
573 
574   // Cache page
575   _cache.free_page(page);
576 
577   // Try satisfy blocked allocations
578   satisfy_alloc_queue();
579 }
580 
581 size_t ZPageAllocator::flush_cache(ZPageCacheFlushClosure* cl, bool for_allocation) {
582   EventZPageCacheFlush event;
583 
584   ZList&lt;ZPage&gt; list;
585 
586   // Flush pages
587   _cache.flush(cl, &amp;list);
588 
589   const size_t overflushed = cl-&gt;overflushed();
590   if (overflushed &gt; 0) {
591     // Overflushed, keep part of last page
592     ZPage* const page = list.last()-&gt;split(overflushed);
593     _cache.free_page(page);
594   }
595 
596   // Destroy pages
597   size_t flushed = 0;
598   for (ZPage* page = list.remove_first(); page != NULL; page = list.remove_first()) {
599     flushed += page-&gt;size();
600     destroy_page(page);
601   }
602 
603   // Send event
604   event.commit(flushed, for_allocation);
605 
606   return flushed;
607 }
608 
609 class ZPageCacheFlushForAllocationClosure : public ZPageCacheFlushClosure {
610 public:
611   ZPageCacheFlushForAllocationClosure(size_t requested) :
612       ZPageCacheFlushClosure(requested) {}
613 
614   virtual bool do_page(const ZPage* page) {
615     if (_flushed &lt; _requested) {
616       // Flush page
617       _flushed += page-&gt;size();
618       return true;
619     }
620 
621     // Don&#39;t flush page
622     return false;
623   }
624 };
625 
626 void ZPageAllocator::flush_cache_for_allocation(size_t requested) {
627   assert(requested &lt;= _cache.available(), &quot;Invalid request&quot;);
628 
629   // Flush pages
630   ZPageCacheFlushForAllocationClosure cl(requested);
631   const size_t flushed = flush_cache(&amp;cl, true /* for_allocation */);
632 
633   assert(requested == flushed, &quot;Failed to flush&quot;);
634 
635   const size_t cached_after = _cache.available();
636   const size_t cached_before = cached_after + flushed;
637 
638   log_info(gc, heap)(&quot;Page Cache: &quot; SIZE_FORMAT &quot;M(%.0f%%)-&gt;&quot; SIZE_FORMAT &quot;M(%.0f%%), &quot;
639                      &quot;Flushed: &quot; SIZE_FORMAT &quot;M&quot;,
640                      cached_before / M, percent_of(cached_before, max_capacity()),
641                      cached_after / M, percent_of(cached_after, max_capacity()),
642                      flushed / M);
643 
644   // Update statistics
645   ZStatInc(ZCounterPageCacheFlush, flushed);
646 }
647 
648 class ZPageCacheFlushForUncommitClosure : public ZPageCacheFlushClosure {
649 private:
650   const uint64_t _now;
651   const uint64_t _delay;
652   uint64_t       _timeout;
653 
654 public:
655   ZPageCacheFlushForUncommitClosure(size_t requested, uint64_t delay) :
656       ZPageCacheFlushClosure(requested),
657       _now(os::elapsedTime()),
658       _delay(delay),
659       _timeout(_delay) {}
660 
661   virtual bool do_page(const ZPage* page) {
662     const uint64_t expires = page-&gt;last_used() + _delay;
663     const uint64_t timeout = expires - MIN2(expires, _now);
664 
665     if (_flushed &lt; _requested &amp;&amp; timeout == 0) {
666       // Flush page
667       _flushed += page-&gt;size();
668       return true;
669     }
670 
671     // Record shortest non-expired timeout
672     _timeout = MIN2(_timeout, timeout);
673 
674     // Don&#39;t flush page
675     return false;
676   }
677 
678   uint64_t timeout() const {
679     return _timeout;
680   }
681 };
682 
683 uint64_t ZPageAllocator::uncommit(uint64_t delay) {
684   // Set the default timeout, when no pages are found in the
685   // cache or when uncommit is disabled, equal to the delay.
686   uint64_t timeout = delay;
687 
688   if (!_uncommit) {
689     // Disabled
690     return timeout;
691   }
692 
693   EventZUncommit event;
694   size_t capacity_before;
695   size_t capacity_after;
696   size_t uncommitted;
697 
698   {
699     SuspendibleThreadSetJoiner joiner;
700     ZLocker&lt;ZLock&gt; locker(&amp;_lock);
701 
702     // Don&#39;t flush more than we will uncommit. Never uncommit
703     // the reserve, and never uncommit below min capacity.
704     const size_t needed = MIN2(_used + _max_reserve, _current_max_capacity);
705     const size_t guarded = MAX2(needed, _min_capacity);
706     const size_t uncommittable = _capacity - guarded;
707     const size_t uncached_available = _capacity - _used - _cache.available();
708     size_t uncommit = MIN2(uncommittable, uncached_available);
709     const size_t flush = uncommittable - uncommit;
710 
711     if (flush &gt; 0) {
712       // Flush pages to uncommit
713       ZPageCacheFlushForUncommitClosure cl(flush, delay);
714       uncommit += flush_cache(&amp;cl, false /* for_allocation */);
715       timeout = cl.timeout();
716     }
717 
718     // Uncommit
719     uncommitted = _physical.uncommit(uncommit);
720     _capacity -= uncommitted;
721 
722     capacity_after = _capacity;
723     capacity_before = capacity_after + uncommitted;
724   }
725 
726   if (uncommitted &gt; 0) {
727     log_info(gc, heap)(&quot;Capacity: &quot; SIZE_FORMAT &quot;M(%.0f%%)-&gt;&quot; SIZE_FORMAT &quot;M(%.0f%%), &quot;
728                        &quot;Uncommitted: &quot; SIZE_FORMAT &quot;M&quot;,
729                        capacity_before / M, percent_of(capacity_before, max_capacity()),
730                        capacity_after / M, percent_of(capacity_after, max_capacity()),
731                        uncommitted / M);
732 
733     // Send event
734     event.commit(capacity_before, capacity_after, uncommitted);
735 
736     // Update statistics
737     ZStatInc(ZCounterUncommit, uncommitted);
738   }
739 
740   return timeout;
741 }
742 
743 void ZPageAllocator::enable_deferred_delete() const {
744   _safe_delete.enable_deferred_delete();
745 }
746 
747 void ZPageAllocator::disable_deferred_delete() const {
748   _safe_delete.disable_deferred_delete();
749 }
750 
751 void ZPageAllocator::debug_map_page(const ZPage* page) const {
752   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
753   _physical.debug_map(page-&gt;physical_memory(), page-&gt;start());
754 }
755 
756 void ZPageAllocator::debug_unmap_page(const ZPage* page) const {
757   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
758   _physical.debug_unmap(page-&gt;physical_memory(), page-&gt;start());
759 }
760 
761 void ZPageAllocator::pages_do(ZPageClosure* cl) const {
762   ZListIterator&lt;ZPageAllocRequest&gt; iter(&amp;_satisfied);
763   for (ZPageAllocRequest* request; iter.next(&amp;request);) {
764     const ZPage* const page = request-&gt;peek();
765     if (page != NULL) {
766       cl-&gt;do_page(page);
767     }
768   }
769 
770   _cache.pages_do(cl);
771 }
772 
773 bool ZPageAllocator::is_alloc_stalled() const {
774   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
775   return !_queue.is_empty();
776 }
777 
778 void ZPageAllocator::check_out_of_memory() {
779   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
780 
781   // Fail allocation requests that were enqueued before the
782   // last GC cycle started, otherwise start a new GC cycle.
783   for (ZPageAllocRequest* request = _queue.first(); request != NULL; request = _queue.first()) {
784     if (request-&gt;total_collections() == ZCollectedHeap::heap()-&gt;total_collections()) {
785       // Start a new GC cycle, keep allocation requests enqueued
786       request-&gt;satisfy(gc_marker);
787       return;
788     }
789 
790     // Out of memory, fail allocation request
791     _queue.remove(request);
792     _satisfied.insert_first(request);
793     request-&gt;satisfy(NULL);
794   }
795 }
    </pre>
  </body>
</html>