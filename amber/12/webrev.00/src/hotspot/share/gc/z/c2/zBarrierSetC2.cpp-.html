<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/gc/z/c2/zBarrierSetC2.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2015, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;classfile/javaClasses.hpp&quot;
 26 #include &quot;gc/z/c2/zBarrierSetC2.hpp&quot;
 27 #include &quot;gc/z/zBarrierSet.hpp&quot;
 28 #include &quot;gc/z/zBarrierSetAssembler.hpp&quot;
 29 #include &quot;gc/z/zBarrierSetRuntime.hpp&quot;
 30 #include &quot;opto/arraycopynode.hpp&quot;
 31 #include &quot;opto/addnode.hpp&quot;
 32 #include &quot;opto/block.hpp&quot;
 33 #include &quot;opto/compile.hpp&quot;
 34 #include &quot;opto/graphKit.hpp&quot;
 35 #include &quot;opto/machnode.hpp&quot;
 36 #include &quot;opto/macro.hpp&quot;
 37 #include &quot;opto/memnode.hpp&quot;
 38 #include &quot;opto/node.hpp&quot;
 39 #include &quot;opto/regalloc.hpp&quot;
 40 #include &quot;opto/rootnode.hpp&quot;
 41 #include &quot;opto/type.hpp&quot;
 42 #include &quot;utilities/growableArray.hpp&quot;
 43 #include &quot;utilities/macros.hpp&quot;
 44 
 45 class ZBarrierSetC2State : public ResourceObj {
 46 private:
 47   GrowableArray&lt;ZLoadBarrierStubC2*&gt;* _stubs;
 48   Node_Array                          _live;
 49 
 50 public:
 51   ZBarrierSetC2State(Arena* arena) :
 52     _stubs(new (arena) GrowableArray&lt;ZLoadBarrierStubC2*&gt;(arena, 8,  0, NULL)),
 53     _live(arena) {}
 54 
 55   GrowableArray&lt;ZLoadBarrierStubC2*&gt;* stubs() {
 56     return _stubs;
 57   }
 58 
 59   RegMask* live(const Node* node) {
 60     if (!node-&gt;is_Mach()) {
 61       // Don&#39;t need liveness for non-MachNodes
 62       return NULL;
 63     }
 64 
 65     const MachNode* const mach = node-&gt;as_Mach();
 66     if (mach-&gt;barrier_data() != ZLoadBarrierStrong &amp;&amp;
 67         mach-&gt;barrier_data() != ZLoadBarrierWeak) {
 68       // Don&#39;t need liveness data for nodes without barriers
 69       return NULL;
 70     }
 71 
 72     RegMask* live = (RegMask*)_live[node-&gt;_idx];
 73     if (live == NULL) {
 74       live = new (Compile::current()-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask))) RegMask();
 75       _live.map(node-&gt;_idx, (Node*)live);
 76     }
 77 
 78     return live;
 79   }
 80 };
 81 
 82 static ZBarrierSetC2State* barrier_set_state() {
 83   return reinterpret_cast&lt;ZBarrierSetC2State*&gt;(Compile::current()-&gt;barrier_set_state());
 84 }
 85 
 86 ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, bool weak) {
 87   ZLoadBarrierStubC2* const stub = new (Compile::current()-&gt;comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref, tmp, weak);
 88   if (!Compile::current()-&gt;in_scratch_emit_size()) {
 89     barrier_set_state()-&gt;stubs()-&gt;append(stub);
 90   }
 91 
 92   return stub;
 93 }
 94 
 95 ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, bool weak) :
 96     _node(node),
 97     _ref_addr(ref_addr),
 98     _ref(ref),
 99     _tmp(tmp),
100     _weak(weak),
101     _entry(),
102     _continuation() {
103   assert_different_registers(ref, ref_addr.base());
104   assert_different_registers(ref, ref_addr.index());
105 }
106 
107 Address ZLoadBarrierStubC2::ref_addr() const {
108   return _ref_addr;
109 }
110 
111 Register ZLoadBarrierStubC2::ref() const {
112   return _ref;
113 }
114 
115 Register ZLoadBarrierStubC2::tmp() const {
116   return _tmp;
117 }
118 
119 address ZLoadBarrierStubC2::slow_path() const {
120   const DecoratorSet decorators = _weak ? ON_WEAK_OOP_REF : ON_STRONG_OOP_REF;
121   return ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators);
122 }
123 
124 RegMask&amp; ZLoadBarrierStubC2::live() const {
125   return *barrier_set_state()-&gt;live(_node);
126 }
127 
128 Label* ZLoadBarrierStubC2::entry() {
129   // The _entry will never be bound when in_scratch_emit_size() is true.
130   // However, we still need to return a label that is not bound now, but
131   // will eventually be bound. Any lable will do, as it will only act as
132   // a placeholder, so we return the _continuation label.
133   return Compile::current()-&gt;in_scratch_emit_size() ? &amp;_continuation : &amp;_entry;
134 }
135 
136 Label* ZLoadBarrierStubC2::continuation() {
137   return &amp;_continuation;
138 }
139 
140 void* ZBarrierSetC2::create_barrier_state(Arena* comp_arena) const {
141   return new (comp_arena) ZBarrierSetC2State(comp_arena);
142 }
143 
144 void ZBarrierSetC2::late_barrier_analysis() const {
145   analyze_dominating_barriers();
146   compute_liveness_at_stubs();
147 }
148 
149 void ZBarrierSetC2::emit_stubs(CodeBuffer&amp; cb) const {
150   MacroAssembler masm(&amp;cb);
151   GrowableArray&lt;ZLoadBarrierStubC2*&gt;* const stubs = barrier_set_state()-&gt;stubs();
152 
153   for (int i = 0; i &lt; stubs-&gt;length(); i++) {
154     // Make sure there is enough space in the code buffer
155     if (cb.insts()-&gt;maybe_expand_to_ensure_remaining(Compile::MAX_inst_size) &amp;&amp; cb.blob() == NULL) {
156       ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
157       return;
158     }
159 
160     ZBarrierSet::assembler()-&gt;generate_c2_load_barrier_stub(&amp;masm, stubs-&gt;at(i));
161   }
162 
163   masm.flush();
164 }
165 
166 int ZBarrierSetC2::estimate_stub_size() const {
167   Compile* const C = Compile::current();
168   BufferBlob* const blob = C-&gt;scratch_buffer_blob();
169   GrowableArray&lt;ZLoadBarrierStubC2*&gt;* const stubs = barrier_set_state()-&gt;stubs();
170   int size = 0;
171 
172   for (int i = 0; i &lt; stubs-&gt;length(); i++) {
173     CodeBuffer cb(blob-&gt;content_begin(), (address)C-&gt;scratch_locs_memory() - blob-&gt;content_begin());
174     MacroAssembler masm(&amp;cb);
175     ZBarrierSet::assembler()-&gt;generate_c2_load_barrier_stub(&amp;masm, stubs-&gt;at(i));
176     size += cb.insts_size();
177   }
178 
179   return size;
180 }
181 
182 static void set_barrier_data(C2Access&amp; access) {
183   if (ZBarrierSet::barrier_needed(access.decorators(), access.type())) {
184     if (access.decorators() &amp; ON_WEAK_OOP_REF) {
185       access.set_barrier_data(ZLoadBarrierWeak);
186     } else {
187       access.set_barrier_data(ZLoadBarrierStrong);
188     }
189   }
190 }
191 
192 Node* ZBarrierSetC2::load_at_resolved(C2Access&amp; access, const Type* val_type) const {
193   set_barrier_data(access);
194   return BarrierSetC2::load_at_resolved(access, val_type);
195 }
196 
197 Node* ZBarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
198                                                     Node* new_val, const Type* val_type) const {
199   set_barrier_data(access);
200   return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, val_type);
201 }
202 
203 Node* ZBarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
204                                                      Node* new_val, const Type* value_type) const {
205   set_barrier_data(access);
206   return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);
207 }
208 
209 Node* ZBarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* val_type) const {
210   set_barrier_data(access);
211   return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, val_type);
212 }
213 
214 bool ZBarrierSetC2::array_copy_requires_gc_barriers(bool tightly_coupled_alloc, BasicType type,
215                                                     bool is_clone, ArrayCopyPhase phase) const {
216   return type == T_OBJECT || type == T_ARRAY;
217 }
218 
219 // This TypeFunc assumes a 64bit system
220 static const TypeFunc* clone_type() {
221   // Create input type (domain)
222   const Type** domain_fields = TypeTuple::fields(4);
223   domain_fields[TypeFunc::Parms + 0] = TypeInstPtr::NOTNULL;  // src
224   domain_fields[TypeFunc::Parms + 1] = TypeInstPtr::NOTNULL;  // dst
225   domain_fields[TypeFunc::Parms + 2] = TypeLong::LONG;        // size lower
226   domain_fields[TypeFunc::Parms + 3] = Type::HALF;            // size upper
227   const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);
228 
229   // Create result type (range)
230   const Type** range_fields = TypeTuple::fields(0);
231   const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);
232 
233   return TypeFunc::make(domain, range);
234 }
235 
236 void ZBarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {
237   Node* const src = ac-&gt;in(ArrayCopyNode::Src);
238   if (ac-&gt;is_clone_array()) {
239     // Clone primitive array
240     BarrierSetC2::clone_at_expansion(phase, ac);
241     return;
242   }
243 
244   // Clone instance
245   Node* const ctrl       = ac-&gt;in(TypeFunc::Control);
246   Node* const mem        = ac-&gt;in(TypeFunc::Memory);
247   Node* const dst        = ac-&gt;in(ArrayCopyNode::Dest);
248   Node* const size       = ac-&gt;in(ArrayCopyNode::Length);
249 
250   assert(ac-&gt;is_clone_inst(), &quot;Sanity check&quot;);
251   assert(size-&gt;bottom_type()-&gt;is_long(), &quot;Should be long&quot;);
252 
253   // The native clone we are calling here expects the instance size in words
254   // Add header/offset size to payload size to get instance size.
255   Node* const base_offset = phase-&gt;longcon(arraycopy_payload_base_offset(false) &gt;&gt; LogBytesPerLong);
256   Node* const full_size = phase-&gt;transform_later(new AddLNode(size, base_offset));
257 
258   Node* const call = phase-&gt;make_leaf_call(ctrl,
259                                            mem,
260                                            clone_type(),
261                                            ZBarrierSetRuntime::clone_addr(),
262                                            &quot;ZBarrierSetRuntime::clone&quot;,
263                                            TypeRawPtr::BOTTOM,
264                                            src,
265                                            dst,
266                                            full_size,
267                                            phase-&gt;top());
268   phase-&gt;transform_later(call);
269   phase-&gt;igvn().replace_node(ac, call);
270 }
271 
272 // == Dominating barrier elision ==
273 
274 static bool block_has_safepoint(const Block* block, uint from, uint to) {
275   for (uint i = from; i &lt; to; i++) {
276     if (block-&gt;get_node(i)-&gt;is_MachSafePoint()) {
277       // Safepoint found
278       return true;
279     }
280   }
281 
282   // Safepoint not found
283   return false;
284 }
285 
286 static bool block_has_safepoint(const Block* block) {
287   return block_has_safepoint(block, 0, block-&gt;number_of_nodes());
288 }
289 
290 static uint block_index(const Block* block, const Node* node) {
291   for (uint j = 0; j &lt; block-&gt;number_of_nodes(); ++j) {
292     if (block-&gt;get_node(j) == node) {
293       return j;
294     }
295   }
296   ShouldNotReachHere();
297   return 0;
298 }
299 
300 void ZBarrierSetC2::analyze_dominating_barriers() const {
301   ResourceMark rm;
302   Compile* const C = Compile::current();
303   PhaseCFG* const cfg = C-&gt;cfg();
304   Block_List worklist;
305   Node_List mem_ops;
306   Node_List barrier_loads;
307 
308   // Step 1 - Find accesses, and track them in lists
309   for (uint i = 0; i &lt; cfg-&gt;number_of_blocks(); ++i) {
310     const Block* const block = cfg-&gt;get_block(i);
311     for (uint j = 0; j &lt; block-&gt;number_of_nodes(); ++j) {
312       const Node* const node = block-&gt;get_node(j);
313       if (!node-&gt;is_Mach()) {
314         continue;
315       }
316 
317       MachNode* const mach = node-&gt;as_Mach();
318       switch (mach-&gt;ideal_Opcode()) {
319       case Op_LoadP:
320       case Op_CompareAndExchangeP:
321       case Op_CompareAndSwapP:
322       case Op_GetAndSetP:
323         if (mach-&gt;barrier_data() == ZLoadBarrierStrong) {
324           barrier_loads.push(mach);
325         }
326       case Op_StoreP:
327         mem_ops.push(mach);
328         break;
329 
330       default:
331         break;
332       }
333     }
334   }
335 
336   // Step 2 - Find dominating accesses for each load
337   for (uint i = 0; i &lt; barrier_loads.size(); i++) {
338     MachNode* const load = barrier_loads.at(i)-&gt;as_Mach();
339     const TypePtr* load_adr_type = NULL;
340     intptr_t load_offset = 0;
341     const Node* const load_obj = load-&gt;get_base_and_disp(load_offset, load_adr_type);
342     Block* const load_block = cfg-&gt;get_block_for_node(load);
343     const uint load_index = block_index(load_block, load);
344 
345     for (uint j = 0; j &lt; mem_ops.size(); j++) {
346       MachNode* mem = mem_ops.at(j)-&gt;as_Mach();
347       const TypePtr* mem_adr_type = NULL;
348       intptr_t mem_offset = 0;
349       const Node* mem_obj = mem-&gt;get_base_and_disp(mem_offset, mem_adr_type);
350       Block* mem_block = cfg-&gt;get_block_for_node(mem);
351       uint mem_index = block_index(mem_block, mem);
352 
353       if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||
354           load_obj == NULL || mem_obj == NULL ||
355           load_offset &lt; 0 || mem_offset &lt; 0) {
356         continue;
357       }
358 
359       if (mem_obj != load_obj || mem_offset != load_offset) {
360         // Not the same addresses, not a candidate
361         continue;
362       }
363 
364       if (load_block == mem_block) {
365         // Earlier accesses in the same block
366         if (mem_index &lt; load_index &amp;&amp; !block_has_safepoint(mem_block, mem_index + 1, load_index)) {
367           load-&gt;set_barrier_data(ZLoadBarrierElided);
368         }
369       } else if (mem_block-&gt;dominates(load_block)) {
370         // Dominating block? Look around for safepoints
371         ResourceMark rm;
372         Block_List stack;
373         VectorSet visited(Thread::current()-&gt;resource_area());
374         stack.push(load_block);
375         bool safepoint_found = block_has_safepoint(load_block);
376         while (!safepoint_found &amp;&amp; stack.size() &gt; 0) {
377           Block* block = stack.pop();
378           if (visited.test_set(block-&gt;_pre_order)) {
379             continue;
380           }
381           if (block_has_safepoint(block)) {
382             safepoint_found = true;
383             break;
384           }
385           if (block == mem_block) {
386             continue;
387           }
388 
389           // Push predecessor blocks
390           for (uint p = 1; p &lt; block-&gt;num_preds(); ++p) {
391             Block* pred = cfg-&gt;get_block_for_node(block-&gt;pred(p));
392             stack.push(pred);
393           }
394         }
395 
396         if (!safepoint_found) {
397           load-&gt;set_barrier_data(ZLoadBarrierElided);
398         }
399       }
400     }
401   }
402 }
403 
404 // == Reduced spilling optimization ==
405 
406 void ZBarrierSetC2::compute_liveness_at_stubs() const {
407   ResourceMark rm;
408   Compile* const C = Compile::current();
409   Arena* const A = Thread::current()-&gt;resource_area();
410   PhaseCFG* const cfg = C-&gt;cfg();
411   PhaseRegAlloc* const regalloc = C-&gt;regalloc();
412   RegMask* const live = NEW_ARENA_ARRAY(A, RegMask, cfg-&gt;number_of_blocks() * sizeof(RegMask));
413   ZBarrierSetAssembler* const bs = ZBarrierSet::assembler();
414   Block_List worklist;
415 
416   for (uint i = 0; i &lt; cfg-&gt;number_of_blocks(); ++i) {
417     new ((void*)(live + i)) RegMask();
418     worklist.push(cfg-&gt;get_block(i));
419   }
420 
421   while (worklist.size() &gt; 0) {
422     const Block* const block = worklist.pop();
423     RegMask&amp; old_live = live[block-&gt;_pre_order];
424     RegMask new_live;
425 
426     // Initialize to union of successors
427     for (uint i = 0; i &lt; block-&gt;_num_succs; i++) {
428       const uint succ_id = block-&gt;_succs[i]-&gt;_pre_order;
429       new_live.OR(live[succ_id]);
430     }
431 
432     // Walk block backwards, computing liveness
433     for (int i = block-&gt;number_of_nodes() - 1; i &gt;= 0; --i) {
434       const Node* const node = block-&gt;get_node(i);
435 
436       // Remove def bits
437       const OptoReg::Name first = bs-&gt;refine_register(node, regalloc-&gt;get_reg_first(node));
438       const OptoReg::Name second = bs-&gt;refine_register(node, regalloc-&gt;get_reg_second(node));
439       if (first != OptoReg::Bad) {
440         new_live.Remove(first);
441       }
442       if (second != OptoReg::Bad) {
443         new_live.Remove(second);
444       }
445 
446       // Add use bits
447       for (uint j = 1; j &lt; node-&gt;req(); ++j) {
448         const Node* const use = node-&gt;in(j);
449         const OptoReg::Name first = bs-&gt;refine_register(use, regalloc-&gt;get_reg_first(use));
450         const OptoReg::Name second = bs-&gt;refine_register(use, regalloc-&gt;get_reg_second(use));
451         if (first != OptoReg::Bad) {
452           new_live.Insert(first);
453         }
454         if (second != OptoReg::Bad) {
455           new_live.Insert(second);
456         }
457       }
458 
459       // If this node tracks liveness, update it
460       RegMask* const regs = barrier_set_state()-&gt;live(node);
461       if (regs != NULL) {
462         regs-&gt;OR(new_live);
463       }
464     }
465 
466     // Now at block top, see if we have any changes
467     new_live.SUBTRACT(old_live);
468     if (new_live.is_NotEmpty()) {
469       // Liveness has refined, update and propagate to prior blocks
470       old_live.OR(new_live);
471       for (uint i = 1; i &lt; block-&gt;num_preds(); ++i) {
472         Block* const pred = cfg-&gt;get_block_for_node(block-&gt;pred(i));
473         worklist.push(pred);
474       }
475     }
476   }
477 }
    </pre>
  </body>
</html>