<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1Arguments.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 64,10 ***</span>
<span class="line-new-header">--- 64,11 ---</span>
  #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  #include &quot;gc/g1/g1VMOperations.hpp&quot;
  #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
<span class="line-added">+ #include &quot;gc/shared/concurrentGCBreakpoints.hpp&quot;</span>
  #include &quot;gc/shared/gcBehaviours.hpp&quot;
  #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  #include &quot;gc/shared/gcId.hpp&quot;
  #include &quot;gc/shared/gcLocker.hpp&quot;
  #include &quot;gc/shared/gcTimer.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 206,18 ***</span>
    }
    return res;
  }
  
  HeapWord*
<span class="line-modified">! G1CollectedHeap::humongous_obj_allocate_initialize_regions(uint first,</span>
                                                             uint num_regions,
                                                             size_t word_size) {
<span class="line-modified">!   assert(first != G1_NO_HRM_INDEX, &quot;pre-condition&quot;);</span>
    assert(is_humongous(word_size), &quot;word_size should be humongous&quot;);
    assert(num_regions * HeapRegion::GrainWords &gt;= word_size, &quot;pre-condition&quot;);
  
    // Index of last region in the series.
    uint last = first + num_regions - 1;
  
    // We need to initialize the region(s) we just discovered. This is
    // a bit tricky given that it can happen concurrently with
    // refinement threads refining cards on these regions and
<span class="line-new-header">--- 207,19 ---</span>
    }
    return res;
  }
  
  HeapWord*
<span class="line-modified">! G1CollectedHeap::humongous_obj_allocate_initialize_regions(HeapRegion* first_hr,</span>
                                                             uint num_regions,
                                                             size_t word_size) {
<span class="line-modified">!   assert(first_hr != NULL, &quot;pre-condition&quot;);</span>
    assert(is_humongous(word_size), &quot;word_size should be humongous&quot;);
    assert(num_regions * HeapRegion::GrainWords &gt;= word_size, &quot;pre-condition&quot;);
  
    // Index of last region in the series.
<span class="line-added">+   uint first = first_hr-&gt;hrm_index();</span>
    uint last = first + num_regions - 1;
  
    // We need to initialize the region(s) we just discovered. This is
    // a bit tricky given that it can happen concurrently with
    // refinement threads refining cards on these regions and
</pre>
<hr />
<pre>
<span class="line-old-header">*** 228,14 ***</span>
  
    // The word size sum of all the regions we will allocate.
    size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;
    assert(word_size &lt;= word_size_sum, &quot;sanity&quot;);
  
<span class="line-modified">!   // This will be the &quot;starts humongous&quot; region.</span>
<span class="line-modified">!   HeapRegion* first_hr = region_at(first);</span>
<span class="line-removed">-   // The header of the new object will be placed at the bottom of</span>
<span class="line-removed">-   // the first region.</span>
    HeapWord* new_obj = first_hr-&gt;bottom();
    // This will be the new top of the new object.
    HeapWord* obj_top = new_obj + word_size;
  
    // First, we need to zero the header of the space that we will be
<span class="line-new-header">--- 230,12 ---</span>
  
    // The word size sum of all the regions we will allocate.
    size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;
    assert(word_size &lt;= word_size_sum, &quot;sanity&quot;);
  
<span class="line-modified">!   // The passed in hr will be the &quot;starts humongous&quot; region. The header</span>
<span class="line-modified">!   // of the new object will be placed at the bottom of this region.</span>
    HeapWord* new_obj = first_hr-&gt;bottom();
    // This will be the new top of the new object.
    HeapWord* obj_top = new_obj + word_size;
  
    // First, we need to zero the header of the space that we will be
</pre>
<hr />
<pre>
<span class="line-old-header">*** 337,61 ***</span>
  HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {
    assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
  
    _verifier-&gt;verify_region_sets_optional();
  
<span class="line-removed">-   uint first = G1_NO_HRM_INDEX;</span>
    uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);
  
<span class="line-modified">!   if (obj_regions == 1) {</span>
<span class="line-modified">!     // Only one region to allocate, try to use a fast path by directly allocating</span>
<span class="line-modified">!     // from the free lists. Do not try to expand here, we will potentially do that</span>
<span class="line-removed">-     // later.</span>
<span class="line-removed">-     HeapRegion* hr = new_region(word_size, HeapRegionType::Humongous, false /* do_expand */);</span>
<span class="line-removed">-     if (hr != NULL) {</span>
<span class="line-removed">-       first = hr-&gt;hrm_index();</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-   } else {</span>
<span class="line-removed">-     // Policy: Try only empty regions (i.e. already committed first). Maybe we</span>
<span class="line-removed">-     // are lucky enough to find some.</span>
<span class="line-removed">-     first = _hrm-&gt;find_contiguous_only_empty(obj_regions);</span>
<span class="line-removed">-     if (first != G1_NO_HRM_INDEX) {</span>
<span class="line-removed">-       _hrm-&gt;allocate_free_regions_starting_at(first, obj_regions);</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   if (first == G1_NO_HRM_INDEX) {</span>
      // Policy: We could not find enough regions for the humongous object in the
      // free list. Look through the heap to find a mix of free and uncommitted regions.
<span class="line-modified">!     // If so, try expansion.</span>
<span class="line-modified">!     first = _hrm-&gt;find_contiguous_empty_or_unavailable(obj_regions);</span>
<span class="line-modified">!     if (first != G1_NO_HRM_INDEX) {</span>
<span class="line-modified">!       // We found something. Make sure these regions are committed, i.e. expand</span>
<span class="line-modified">!       // the heap. Alternatively we could do a defragmentation GC.</span>
<span class="line-modified">!       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (humongous allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,</span>
<span class="line-removed">-                                     word_size * HeapWordSize);</span>
<span class="line-removed">- </span>
<span class="line-removed">-       _hrm-&gt;expand_at(first, obj_regions, workers());</span>
        policy()-&gt;record_new_heap_size(num_regions());
<span class="line-removed">- </span>
<span class="line-removed">- #ifdef ASSERT</span>
<span class="line-removed">-       for (uint i = first; i &lt; first + obj_regions; ++i) {</span>
<span class="line-removed">-         HeapRegion* hr = region_at(i);</span>
<span class="line-removed">-         assert(hr-&gt;is_free(), &quot;sanity&quot;);</span>
<span class="line-removed">-         assert(hr-&gt;is_empty(), &quot;sanity&quot;);</span>
<span class="line-removed">-         assert(is_on_master_free_list(hr), &quot;sanity&quot;);</span>
<span class="line-removed">-       }</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">-       _hrm-&gt;allocate_free_regions_starting_at(first, obj_regions);</span>
      } else {
        // Policy: Potentially trigger a defragmentation GC.
      }
    }
  
    HeapWord* result = NULL;
<span class="line-modified">!   if (first != G1_NO_HRM_INDEX) {</span>
<span class="line-modified">!     result = humongous_obj_allocate_initialize_regions(first, obj_regions, word_size);</span>
      assert(result != NULL, &quot;it should always return a valid result&quot;);
  
      // A successful humongous object allocation changes the used space
      // information of the old generation so we need to recalculate the
      // sizes and update the jstat counters here.
<span class="line-new-header">--- 337,32 ---</span>
  HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {
    assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
  
    _verifier-&gt;verify_region_sets_optional();
  
    uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);
  
<span class="line-modified">!   // Policy: First try to allocate a humongous object in the free list.</span>
<span class="line-modified">!   HeapRegion* humongous_start = _hrm-&gt;allocate_humongous(obj_regions);</span>
<span class="line-modified">!   if (humongous_start == NULL) {</span>
      // Policy: We could not find enough regions for the humongous object in the
      // free list. Look through the heap to find a mix of free and uncommitted regions.
<span class="line-modified">!     // If so, expand the heap and allocate the humongous object.</span>
<span class="line-modified">!     humongous_start = _hrm-&gt;expand_and_allocate_humongous(obj_regions);</span>
<span class="line-modified">!     if (humongous_start != NULL) {</span>
<span class="line-modified">!       // We managed to find a region by expanding the heap.</span>
<span class="line-modified">!       log_debug(gc, ergo, heap)(&quot;Heap expansion (humongous allocation request). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,</span>
<span class="line-modified">!                                 word_size * HeapWordSize);</span>
        policy()-&gt;record_new_heap_size(num_regions());
      } else {
        // Policy: Potentially trigger a defragmentation GC.
      }
    }
  
    HeapWord* result = NULL;
<span class="line-modified">!   if (humongous_start != NULL) {</span>
<span class="line-modified">!     result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);</span>
      assert(result != NULL, &quot;it should always return a valid result&quot;);
  
      // A successful humongous object allocation changes the used space
      // information of the old generation so we need to recalculate the
      // sizes and update the jstat counters here.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1797,14 ***</span>
    _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
  
    // Create the G1ConcurrentMark data structure and thread.
    // (Must do this late, so that &quot;max_regions&quot; is defined.)
    _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-removed">-   if (!_cm-&gt;completed_initialization()) {</span>
<span class="line-removed">-     vm_shutdown_during_initialization(&quot;Could not initialize G1ConcurrentMark&quot;);</span>
<span class="line-removed">-     return JNI_ENOMEM;</span>
<span class="line-removed">-   }</span>
    _cm_thread = _cm-&gt;cm_thread();
  
    // Now expand into the initial heap size.
    if (!expand(init_byte_size, _workers)) {
      vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
<span class="line-new-header">--- 1768,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2001,10 ***</span>
<span class="line-new-header">--- 1968,11 ---</span>
  
  bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
    switch (cause) {
      case GCCause::_g1_humongous_allocation: return true;
      case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
<span class="line-added">+     case GCCause::_wb_breakpoint:           return true;</span>
      default:                                return is_user_requested_concurrent_full_gc(cause);
    }
  }
  
  bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2171,28 ***</span>
        gc_counter = total_collections();
        old_marking_started_after = _old_marking_cycles_started;
        old_marking_completed_after = _old_marking_cycles_completed;
      }
  
<span class="line-modified">!     if (!GCCause::is_user_requested_gc(cause)) {</span>
        // For an &quot;automatic&quot; (not user-requested) collection, we just need to
        // ensure that progress is made.
        //
        // Request is finished if any of
        // (1) the VMOp successfully performed a GC,
        // (2) a concurrent cycle was already in progress,
<span class="line-modified">!       // (3) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">!       // (4) a Full GC was performed.</span>
<span class="line-modified">!       // Cases (3) and (4) are detected together by a change to</span>
        // _old_marking_cycles_started.
        //
<span class="line-modified">!       // Note that (1) does not imply (3).  If we&#39;re still in the mixed</span>
        // phase of an earlier concurrent collection, the request to make the
        // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
        // both conditions we&#39;ll spin doing back-to-back collections.
        if (op.gc_succeeded() ||
            op.cycle_already_in_progress() ||
            (old_marking_started_before != old_marking_started_after)) {
          LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
          return true;
        }
      } else {                    // User-requested GC.
<span class="line-new-header">--- 2139,46 ---</span>
        gc_counter = total_collections();
        old_marking_started_after = _old_marking_cycles_started;
        old_marking_completed_after = _old_marking_cycles_completed;
      }
  
<span class="line-modified">!     if (cause == GCCause::_wb_breakpoint) {</span>
<span class="line-added">+       if (op.gc_succeeded()) {</span>
<span class="line-added">+         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="line-added">+         return true;</span>
<span class="line-added">+       }</span>
<span class="line-added">+       // When _wb_breakpoint there can&#39;t be another cycle or deferred.</span>
<span class="line-added">+       assert(!op.cycle_already_in_progress(), &quot;invariant&quot;);</span>
<span class="line-added">+       assert(!op.whitebox_attached(), &quot;invariant&quot;);</span>
<span class="line-added">+       // Concurrent cycle attempt might have been cancelled by some other</span>
<span class="line-added">+       // collection, so retry.  Unlike other cases below, we want to retry</span>
<span class="line-added">+       // even if cancelled by a STW full collection, because we really want</span>
<span class="line-added">+       // to start a concurrent cycle.</span>
<span class="line-added">+       if (old_marking_started_before != old_marking_started_after) {</span>
<span class="line-added">+         LOG_COLLECT_CONCURRENTLY(cause, &quot;ignoring STW full GC&quot;);</span>
<span class="line-added">+         old_marking_started_before = old_marking_started_after;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     } else if (!GCCause::is_user_requested_gc(cause)) {</span>
        // For an &quot;automatic&quot; (not user-requested) collection, we just need to
        // ensure that progress is made.
        //
        // Request is finished if any of
        // (1) the VMOp successfully performed a GC,
        // (2) a concurrent cycle was already in progress,
<span class="line-modified">!       // (3) whitebox is controlling concurrent cycles,</span>
<span class="line-modified">!       // (4) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">!       // (5) a Full GC was performed.</span>
<span class="line-added">+       // Cases (4) and (5) are detected together by a change to</span>
        // _old_marking_cycles_started.
        //
<span class="line-modified">!       // Note that (1) does not imply (4).  If we&#39;re still in the mixed</span>
        // phase of an earlier concurrent collection, the request to make the
        // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
        // both conditions we&#39;ll spin doing back-to-back collections.
        if (op.gc_succeeded() ||
            op.cycle_already_in_progress() ||
<span class="line-added">+           op.whitebox_attached() ||</span>
            (old_marking_started_before != old_marking_started_after)) {
          LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
          return true;
        }
      } else {                    // User-requested GC.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2242,24 ***</span>
        // wait &amp;etc should have recognized as finishing this request.  This
        // differs from a non-user-request, where gc_succeeded does not imply
        // a new cycle was started.
        assert(!op.gc_succeeded(), &quot;invariant&quot;);
  
<span class="line-removed">-       // If VMOp failed because a cycle was already in progress, it is now</span>
<span class="line-removed">-       // complete.  But it didn&#39;t finish this user-requested GC, so try</span>
<span class="line-removed">-       // again.</span>
        if (op.cycle_already_in_progress()) {
          LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
          continue;
        }
      }
  
      // Collection failed and should be retried.
      assert(op.transient_failure(), &quot;invariant&quot;);
  
<span class="line-removed">-     // If GCLocker is active, wait until clear before retrying.</span>
      if (GCLocker::is_active_and_needs_gc()) {
        LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
        GCLocker::stall_until_clear();
      }
  
      LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
<span class="line-new-header">--- 2228,36 ---</span>
        // wait &amp;etc should have recognized as finishing this request.  This
        // differs from a non-user-request, where gc_succeeded does not imply
        // a new cycle was started.
        assert(!op.gc_succeeded(), &quot;invariant&quot;);
  
        if (op.cycle_already_in_progress()) {
<span class="line-added">+         // If VMOp failed because a cycle was already in progress, it</span>
<span class="line-added">+         // is now complete.  But it didn&#39;t finish this user-requested</span>
<span class="line-added">+         // GC, so try again.</span>
          LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
          continue;
<span class="line-added">+       } else if (op.whitebox_attached()) {</span>
<span class="line-added">+         // If WhiteBox wants control, wait for notification of a state</span>
<span class="line-added">+         // change in the controller, then try again.  Don&#39;t wait for</span>
<span class="line-added">+         // release of control, since collections may complete while in</span>
<span class="line-added">+         // control.  Note: This won&#39;t recognize a STW full collection</span>
<span class="line-added">+         // while waiting; we can&#39;t wait on multiple monitors.</span>
<span class="line-added">+         LOG_COLLECT_CONCURRENTLY(cause, &quot;whitebox control stall&quot;);</span>
<span class="line-added">+         MonitorLocker ml(ConcurrentGCBreakpoints::monitor());</span>
<span class="line-added">+         if (ConcurrentGCBreakpoints::is_controlled()) {</span>
<span class="line-added">+           ml.wait();</span>
<span class="line-added">+         }</span>
<span class="line-added">+         continue;</span>
        }
      }
  
      // Collection failed and should be retried.
      assert(op.transient_failure(), &quot;invariant&quot;);
  
      if (GCLocker::is_active_and_needs_gc()) {
<span class="line-added">+       // If GCLocker is active, wait until clear before retrying.</span>
        LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
        GCLocker::stall_until_clear();
      }
  
      LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2451,18 ***</span>
  
  void G1CollectedHeap::verify(VerifyOption vo) {
    _verifier-&gt;verify(vo);
  }
  
<span class="line-modified">! bool G1CollectedHeap::supports_concurrent_phase_control() const {</span>
    return true;
  }
  
<span class="line-removed">- bool G1CollectedHeap::request_concurrent_phase(const char* phase) {</span>
<span class="line-removed">-   return _cm_thread-&gt;request_concurrent_phase(phase);</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
  bool G1CollectedHeap::is_heterogeneous_heap() const {
    return G1Arguments::is_heterogeneous_heap();
  }
  
  class PrintRegionClosure: public HeapRegionClosure {
<span class="line-new-header">--- 2449,14 ---</span>
  
  void G1CollectedHeap::verify(VerifyOption vo) {
    _verifier-&gt;verify(vo);
  }
  
<span class="line-modified">! bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {</span>
    return true;
  }
  
  bool G1CollectedHeap::is_heterogeneous_heap() const {
    return G1Arguments::is_heterogeneous_heap();
  }
  
  class PrintRegionClosure: public HeapRegionClosure {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3176,10 ***</span>
<span class="line-new-header">--- 3170,11 ---</span>
      // thread(s) could be running concurrently with us. Make sure that anything
      // after this point does not assume that we are the only GC thread running.
      // Note: of course, the actual marking work will not start until the safepoint
      // itself is released in SuspendibleThreadSet::desynchronize().
      do_concurrent_mark();
<span class="line-added">+     ConcurrentGCBreakpoints::notify_idle_to_active();</span>
    }
  }
  
  void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
    G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 4865,12 ***</span>
    if (index != G1_NO_HRM_INDEX) {
      if (expanded) {
        log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
                                  HeapRegion::GrainWords * HeapWordSize);
      }
<span class="line-modified">!     _hrm-&gt;allocate_free_regions_starting_at(index, 1);</span>
<span class="line-removed">-     return region_at(index);</span>
    }
    return NULL;
  }
  
  // Optimized nmethod scanning
<span class="line-new-header">--- 4860,11 ---</span>
    if (index != G1_NO_HRM_INDEX) {
      if (expanded) {
        log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
                                  HeapRegion::GrainWords * HeapWordSize);
      }
<span class="line-modified">!     return _hrm-&gt;allocate_free_regions_starting_at(index, 1);</span>
    }
    return NULL;
  }
  
  // Optimized nmethod scanning
</pre>
<center><a href="g1Arguments.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>