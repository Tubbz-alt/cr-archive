<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1Arguments.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;

  69 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  70 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  71 #include &quot;gc/shared/gcId.hpp&quot;
  72 #include &quot;gc/shared/gcLocker.hpp&quot;
  73 #include &quot;gc/shared/gcTimer.hpp&quot;
  74 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  75 #include &quot;gc/shared/generationSpec.hpp&quot;
  76 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  77 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  78 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  79 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  80 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  81 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
  82 #include &quot;gc/shared/taskTerminator.hpp&quot;
  83 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  84 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  85 #include &quot;gc/shared/workerPolicy.hpp&quot;
  86 #include &quot;logging/log.hpp&quot;
  87 #include &quot;memory/allocation.hpp&quot;
  88 #include &quot;memory/iterator.hpp&quot;
</pre>
<hr />
<pre>
 191     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (region allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,
 192                               word_size * HeapWordSize);
 193 
 194     assert(word_size * HeapWordSize &lt; HeapRegion::GrainBytes,
 195            &quot;This kind of expansion should never be more than one region. Size: &quot; SIZE_FORMAT,
 196            word_size * HeapWordSize);
 197     if (expand_single_region(node_index)) {
 198       // Given that expand_single_region() succeeded in expanding the heap, and we
 199       // always expand the heap by an amount aligned to the heap
 200       // region size, the free list should in theory not be empty.
 201       // In either case allocate_free_region() will check for NULL.
 202       res = _hrm-&gt;allocate_free_region(type, node_index);
 203     } else {
 204       _expand_heap_after_alloc_failure = false;
 205     }
 206   }
 207   return res;
 208 }
 209 
 210 HeapWord*
<span class="line-modified"> 211 G1CollectedHeap::humongous_obj_allocate_initialize_regions(uint first,</span>
 212                                                            uint num_regions,
 213                                                            size_t word_size) {
<span class="line-modified"> 214   assert(first != G1_NO_HRM_INDEX, &quot;pre-condition&quot;);</span>
 215   assert(is_humongous(word_size), &quot;word_size should be humongous&quot;);
 216   assert(num_regions * HeapRegion::GrainWords &gt;= word_size, &quot;pre-condition&quot;);
 217 
 218   // Index of last region in the series.

 219   uint last = first + num_regions - 1;
 220 
 221   // We need to initialize the region(s) we just discovered. This is
 222   // a bit tricky given that it can happen concurrently with
 223   // refinement threads refining cards on these regions and
 224   // potentially wanting to refine the BOT as they are scanning
 225   // those cards (this can happen shortly after a cleanup; see CR
 226   // 6991377). So we have to set up the region(s) carefully and in
 227   // a specific order.
 228 
 229   // The word size sum of all the regions we will allocate.
 230   size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;
 231   assert(word_size &lt;= word_size_sum, &quot;sanity&quot;);
 232 
<span class="line-modified"> 233   // This will be the &quot;starts humongous&quot; region.</span>
<span class="line-modified"> 234   HeapRegion* first_hr = region_at(first);</span>
<span class="line-removed"> 235   // The header of the new object will be placed at the bottom of</span>
<span class="line-removed"> 236   // the first region.</span>
 237   HeapWord* new_obj = first_hr-&gt;bottom();
 238   // This will be the new top of the new object.
 239   HeapWord* obj_top = new_obj + word_size;
 240 
 241   // First, we need to zero the header of the space that we will be
 242   // allocating. When we update top further down, some refinement
 243   // threads might try to scan the region. By zeroing the header we
 244   // ensure that any thread that will try to scan the region will
 245   // come across the zero klass word and bail out.
 246   //
 247   // NOTE: It would not have been correct to have used
 248   // CollectedHeap::fill_with_object() and make the space look like
 249   // an int array. The thread that is doing the allocation will
 250   // later update the object header to a potentially different array
 251   // type and, for a very short period of time, the klass and length
 252   // fields will be inconsistent. This could cause a refinement
 253   // thread to calculate the object size incorrectly.
 254   Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);
 255 
 256   // Next, pad out the unused tail of the last region with filler
</pre>
<hr />
<pre>
 322     _humongous_set.add(hr);
 323     _hr_printer.alloc(hr);
 324   }
 325 
 326   return new_obj;
 327 }
 328 
 329 size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {
 330   assert(is_humongous(word_size), &quot;Object of size &quot; SIZE_FORMAT &quot; must be humongous here&quot;, word_size);
 331   return align_up(word_size, HeapRegion::GrainWords) / HeapRegion::GrainWords;
 332 }
 333 
 334 // If could fit into free regions w/o expansion, try.
 335 // Otherwise, if can expand, do so.
 336 // Otherwise, if using ex regions might help, try with ex given back.
 337 HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {
 338   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
 339 
 340   _verifier-&gt;verify_region_sets_optional();
 341 
<span class="line-removed"> 342   uint first = G1_NO_HRM_INDEX;</span>
 343   uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);
 344 
<span class="line-modified"> 345   if (obj_regions == 1) {</span>
<span class="line-modified"> 346     // Only one region to allocate, try to use a fast path by directly allocating</span>
<span class="line-modified"> 347     // from the free lists. Do not try to expand here, we will potentially do that</span>
<span class="line-removed"> 348     // later.</span>
<span class="line-removed"> 349     HeapRegion* hr = new_region(word_size, HeapRegionType::Humongous, false /* do_expand */);</span>
<span class="line-removed"> 350     if (hr != NULL) {</span>
<span class="line-removed"> 351       first = hr-&gt;hrm_index();</span>
<span class="line-removed"> 352     }</span>
<span class="line-removed"> 353   } else {</span>
<span class="line-removed"> 354     // Policy: Try only empty regions (i.e. already committed first). Maybe we</span>
<span class="line-removed"> 355     // are lucky enough to find some.</span>
<span class="line-removed"> 356     first = _hrm-&gt;find_contiguous_only_empty(obj_regions);</span>
<span class="line-removed"> 357     if (first != G1_NO_HRM_INDEX) {</span>
<span class="line-removed"> 358       _hrm-&gt;allocate_free_regions_starting_at(first, obj_regions);</span>
<span class="line-removed"> 359     }</span>
<span class="line-removed"> 360   }</span>
<span class="line-removed"> 361 </span>
<span class="line-removed"> 362   if (first == G1_NO_HRM_INDEX) {</span>
 363     // Policy: We could not find enough regions for the humongous object in the
 364     // free list. Look through the heap to find a mix of free and uncommitted regions.
<span class="line-modified"> 365     // If so, try expansion.</span>
<span class="line-modified"> 366     first = _hrm-&gt;find_contiguous_empty_or_unavailable(obj_regions);</span>
<span class="line-modified"> 367     if (first != G1_NO_HRM_INDEX) {</span>
<span class="line-modified"> 368       // We found something. Make sure these regions are committed, i.e. expand</span>
<span class="line-modified"> 369       // the heap. Alternatively we could do a defragmentation GC.</span>
<span class="line-modified"> 370       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (humongous allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,</span>
<span class="line-removed"> 371                                     word_size * HeapWordSize);</span>
<span class="line-removed"> 372 </span>
<span class="line-removed"> 373       _hrm-&gt;expand_at(first, obj_regions, workers());</span>
 374       policy()-&gt;record_new_heap_size(num_regions());
<span class="line-removed"> 375 </span>
<span class="line-removed"> 376 #ifdef ASSERT</span>
<span class="line-removed"> 377       for (uint i = first; i &lt; first + obj_regions; ++i) {</span>
<span class="line-removed"> 378         HeapRegion* hr = region_at(i);</span>
<span class="line-removed"> 379         assert(hr-&gt;is_free(), &quot;sanity&quot;);</span>
<span class="line-removed"> 380         assert(hr-&gt;is_empty(), &quot;sanity&quot;);</span>
<span class="line-removed"> 381         assert(is_on_master_free_list(hr), &quot;sanity&quot;);</span>
<span class="line-removed"> 382       }</span>
<span class="line-removed"> 383 #endif</span>
<span class="line-removed"> 384       _hrm-&gt;allocate_free_regions_starting_at(first, obj_regions);</span>
 385     } else {
 386       // Policy: Potentially trigger a defragmentation GC.
 387     }
 388   }
 389 
 390   HeapWord* result = NULL;
<span class="line-modified"> 391   if (first != G1_NO_HRM_INDEX) {</span>
<span class="line-modified"> 392     result = humongous_obj_allocate_initialize_regions(first, obj_regions, word_size);</span>
 393     assert(result != NULL, &quot;it should always return a valid result&quot;);
 394 
 395     // A successful humongous object allocation changes the used space
 396     // information of the old generation so we need to recalculate the
 397     // sizes and update the jstat counters here.
 398     g1mm()-&gt;update_sizes();
 399   }
 400 
 401   _verifier-&gt;verify_region_sets_optional();
 402 
 403   return result;
 404 }
 405 
 406 HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,
 407                                              size_t requested_size,
 408                                              size_t* actual_size) {
 409   assert_heap_not_locked_and_not_at_safepoint();
 410   assert(!is_humongous(requested_size), &quot;we do not allow humongous TLABs&quot;);
 411 
 412   return attempt_allocation(min_size, requested_size, actual_size);
</pre>
<hr />
<pre>
1782     HeapWord* end = _hrm-&gt;reserved().end();
1783     size_t granularity = HeapRegion::GrainBytes;
1784 
1785     _region_attr.initialize(start, end, granularity);
1786     _humongous_reclaim_candidates.initialize(start, end, granularity);
1787   }
1788 
1789   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1790                           true /* are_GC_task_threads */,
1791                           false /* are_ConcurrentGC_threads */);
1792   if (_workers == NULL) {
1793     return JNI_ENOMEM;
1794   }
1795   _workers-&gt;initialize_workers();
1796 
1797   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1798 
1799   // Create the G1ConcurrentMark data structure and thread.
1800   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1801   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-removed">1802   if (!_cm-&gt;completed_initialization()) {</span>
<span class="line-removed">1803     vm_shutdown_during_initialization(&quot;Could not initialize G1ConcurrentMark&quot;);</span>
<span class="line-removed">1804     return JNI_ENOMEM;</span>
<span class="line-removed">1805   }</span>
1806   _cm_thread = _cm-&gt;cm_thread();
1807 
1808   // Now expand into the initial heap size.
1809   if (!expand(init_byte_size, _workers)) {
1810     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1811     return JNI_ENOMEM;
1812   }
1813 
1814   // Perform any initialization actions delegated to the policy.
1815   policy()-&gt;init(this, &amp;_collection_set);
1816 
1817   jint ecode = initialize_concurrent_refinement();
1818   if (ecode != JNI_OK) {
1819     return ecode;
1820   }
1821 
1822   ecode = initialize_young_gen_sampling_thread();
1823   if (ecode != JNI_OK) {
1824     return ecode;
1825   }
</pre>
<hr />
<pre>
1986 
1987 size_t G1CollectedHeap::recalculate_used() const {
1988   SumUsedClosure blk;
1989   heap_region_iterate(&amp;blk);
1990   return blk.result();
1991 }
1992 
1993 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1994   switch (cause) {
1995     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1996     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1997     case GCCause::_wb_conc_mark:                        return true;
1998     default :                                           return false;
1999   }
2000 }
2001 
2002 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2003   switch (cause) {
2004     case GCCause::_g1_humongous_allocation: return true;
2005     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;

2006     default:                                return is_user_requested_concurrent_full_gc(cause);
2007   }
2008 }
2009 
2010 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
2011   if (policy()-&gt;force_upgrade_to_full()) {
2012     return true;
2013   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2014     return false;
2015   } else if (has_regions_left_for_allocation()) {
2016     return false;
2017   } else {
2018     return true;
2019   }
2020 }
2021 
2022 #ifndef PRODUCT
2023 void G1CollectedHeap::allocate_dummy_regions() {
2024   // Let&#39;s fill up most of the region
2025   size_t word_size = HeapRegion::GrainWords - 1024;
</pre>
<hr />
<pre>
2156       return false;
2157     }
2158 
2159     // Lock to get consistent set of values.
2160     uint old_marking_started_after;
2161     uint old_marking_completed_after;
2162     {
2163       MutexLocker ml(Heap_lock);
2164       // Update gc_counter for retrying VMOp if needed. Captured here to be
2165       // consistent with the values we use below for termination tests.  If
2166       // a retry is needed after a possible wait, and another collection
2167       // occurs in the meantime, it will cause our retry to be skipped and
2168       // we&#39;ll recheck for termination with updated conditions from that
2169       // more recent collection.  That&#39;s what we want, rather than having
2170       // our retry possibly perform an unnecessary collection.
2171       gc_counter = total_collections();
2172       old_marking_started_after = _old_marking_cycles_started;
2173       old_marking_completed_after = _old_marking_cycles_completed;
2174     }
2175 
<span class="line-modified">2176     if (!GCCause::is_user_requested_gc(cause)) {</span>
















2177       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2178       // ensure that progress is made.
2179       //
2180       // Request is finished if any of
2181       // (1) the VMOp successfully performed a GC,
2182       // (2) a concurrent cycle was already in progress,
<span class="line-modified">2183       // (3) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2184       // (4) a Full GC was performed.</span>
<span class="line-modified">2185       // Cases (3) and (4) are detected together by a change to</span>

2186       // _old_marking_cycles_started.
2187       //
<span class="line-modified">2188       // Note that (1) does not imply (3).  If we&#39;re still in the mixed</span>
2189       // phase of an earlier concurrent collection, the request to make the
2190       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
2191       // both conditions we&#39;ll spin doing back-to-back collections.
2192       if (op.gc_succeeded() ||
2193           op.cycle_already_in_progress() ||

2194           (old_marking_started_before != old_marking_started_after)) {
2195         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2196         return true;
2197       }
2198     } else {                    // User-requested GC.
2199       // For a user-requested collection, we want to ensure that a complete
2200       // full collection has been performed before returning, but without
2201       // waiting for more than needed.
2202 
2203       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2204       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2205       // should do otherwise.  Trying again just does back to back GCs.
2206       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2207       // to meet the goal of ensuring a full collection was performed.
2208       assert(!op.gc_succeeded() ||
2209              (old_marking_started_before != old_marking_started_after),
2210              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,
2211              BOOL_TO_STR(op.gc_succeeded()),
2212              old_marking_started_before, old_marking_started_after);
2213 
</pre>
<hr />
<pre>
2227         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);
2228         MonitorLocker ml(G1OldGCCount_lock);
2229         while (gc_counter_less_than(_old_marking_cycles_completed,
2230                                     old_marking_started_after)) {
2231           ml.wait();
2232         }
2233         // Request is finished if the collection we just waited for was
2234         // started after this request.
2235         if (old_marking_started_before != old_marking_started_after) {
2236           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);
2237           return true;
2238         }
2239       }
2240 
2241       // If VMOp was successful then it started a new cycle that the above
2242       // wait &amp;etc should have recognized as finishing this request.  This
2243       // differs from a non-user-request, where gc_succeeded does not imply
2244       // a new cycle was started.
2245       assert(!op.gc_succeeded(), &quot;invariant&quot;);
2246 
<span class="line-removed">2247       // If VMOp failed because a cycle was already in progress, it is now</span>
<span class="line-removed">2248       // complete.  But it didn&#39;t finish this user-requested GC, so try</span>
<span class="line-removed">2249       // again.</span>
2250       if (op.cycle_already_in_progress()) {



2251         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
2252         continue;












2253       }
2254     }
2255 
2256     // Collection failed and should be retried.
2257     assert(op.transient_failure(), &quot;invariant&quot;);
2258 
<span class="line-removed">2259     // If GCLocker is active, wait until clear before retrying.</span>
2260     if (GCLocker::is_active_and_needs_gc()) {

2261       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
2262       GCLocker::stall_until_clear();
2263     }
2264 
2265     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
2266   }
2267 }
2268 
2269 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2270   assert_heap_not_locked();
2271 
2272   // Lock to get consistent set of values.
2273   uint gc_count_before;
2274   uint full_gc_count_before;
2275   uint old_marking_started_before;
2276   {
2277     MutexLocker ml(Heap_lock);
2278     gc_count_before = total_collections();
2279     full_gc_count_before = total_full_collections();
2280     old_marking_started_before = _old_marking_cycles_started;
</pre>
<hr />
<pre>
2436   }
2437   return ret_val;
2438 }
2439 
2440 void G1CollectedHeap::deduplicate_string(oop str) {
2441   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2442 
2443   if (G1StringDedup::is_enabled()) {
2444     G1StringDedup::deduplicate(str);
2445   }
2446 }
2447 
2448 void G1CollectedHeap::prepare_for_verify() {
2449   _verifier-&gt;prepare_for_verify();
2450 }
2451 
2452 void G1CollectedHeap::verify(VerifyOption vo) {
2453   _verifier-&gt;verify(vo);
2454 }
2455 
<span class="line-modified">2456 bool G1CollectedHeap::supports_concurrent_phase_control() const {</span>
2457   return true;
2458 }
2459 
<span class="line-removed">2460 bool G1CollectedHeap::request_concurrent_phase(const char* phase) {</span>
<span class="line-removed">2461   return _cm_thread-&gt;request_concurrent_phase(phase);</span>
<span class="line-removed">2462 }</span>
<span class="line-removed">2463 </span>
2464 bool G1CollectedHeap::is_heterogeneous_heap() const {
2465   return G1Arguments::is_heterogeneous_heap();
2466 }
2467 
2468 class PrintRegionClosure: public HeapRegionClosure {
2469   outputStream* _st;
2470 public:
2471   PrintRegionClosure(outputStream* st) : _st(st) {}
2472   bool do_heap_region(HeapRegion* r) {
2473     r-&gt;print_on(_st);
2474     return false;
2475   }
2476 };
2477 
2478 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2479                                        const HeapRegion* hr,
2480                                        const VerifyOption vo) const {
2481   switch (vo) {
2482   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2483   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
</pre>
<hr />
<pre>
3161     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3162     // before any GC notifications are raised.
3163     g1mm()-&gt;update_sizes();
3164 
3165     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3166     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3167     _gc_timer_stw-&gt;register_gc_end();
3168     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3169   }
3170   // It should now be safe to tell the concurrent mark thread to start
3171   // without its logging output interfering with the logging output
3172   // that came from the pause.
3173 
3174   if (should_start_conc_mark) {
3175     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3176     // thread(s) could be running concurrently with us. Make sure that anything
3177     // after this point does not assume that we are the only GC thread running.
3178     // Note: of course, the actual marking work will not start until the safepoint
3179     // itself is released in SuspendibleThreadSet::desynchronize().
3180     do_concurrent_mark();

3181   }
3182 }
3183 
3184 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3185   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3186   workers()-&gt;run_task(&amp;rsfp_task);
3187 }
3188 
3189 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3190   double remove_self_forwards_start = os::elapsedTime();
3191 
3192   remove_self_forwarding_pointers(rdcqs);
3193   _preserved_marks_set.restore(workers());
3194 
3195   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3196 }
3197 
3198 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3199   if (!_evacuation_failed) {
3200     _evacuation_failed = true;
</pre>
<hr />
<pre>
4850     assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());
4851     _survivor.add_used_bytes(allocated_bytes);
4852   }
4853 
4854   bool const during_im = collector_state()-&gt;in_initial_mark_gc();
4855   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
4856     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
4857   }
4858   _hr_printer.retire(alloc_region);
4859 }
4860 
4861 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4862   bool expanded = false;
4863   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4864 
4865   if (index != G1_NO_HRM_INDEX) {
4866     if (expanded) {
4867       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
4868                                 HeapRegion::GrainWords * HeapWordSize);
4869     }
<span class="line-modified">4870     _hrm-&gt;allocate_free_regions_starting_at(index, 1);</span>
<span class="line-removed">4871     return region_at(index);</span>
4872   }
4873   return NULL;
4874 }
4875 
4876 // Optimized nmethod scanning
4877 
4878 class RegisterNMethodOopClosure: public OopClosure {
4879   G1CollectedHeap* _g1h;
4880   nmethod* _nm;
4881 
4882   template &lt;class T&gt; void do_oop_work(T* p) {
4883     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
4884     if (!CompressedOops::is_null(heap_oop)) {
4885       oop obj = CompressedOops::decode_not_null(heap_oop);
4886       HeapRegion* hr = _g1h-&gt;heap_region_containing(obj);
4887       assert(!hr-&gt;is_continues_humongous(),
4888              &quot;trying to add code root &quot; PTR_FORMAT &quot; in continuation of humongous region &quot; HR_FORMAT
4889              &quot; starting at &quot; HR_FORMAT,
4890              p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr-&gt;humongous_start_region()));
4891 
</pre>
</td>
<td>
<hr />
<pre>
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
<span class="line-added">  69 #include &quot;gc/shared/concurrentGCBreakpoints.hpp&quot;</span>
  70 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  71 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  72 #include &quot;gc/shared/gcId.hpp&quot;
  73 #include &quot;gc/shared/gcLocker.hpp&quot;
  74 #include &quot;gc/shared/gcTimer.hpp&quot;
  75 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  76 #include &quot;gc/shared/generationSpec.hpp&quot;
  77 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  78 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  79 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  80 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  81 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  82 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
  83 #include &quot;gc/shared/taskTerminator.hpp&quot;
  84 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  85 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  86 #include &quot;gc/shared/workerPolicy.hpp&quot;
  87 #include &quot;logging/log.hpp&quot;
  88 #include &quot;memory/allocation.hpp&quot;
  89 #include &quot;memory/iterator.hpp&quot;
</pre>
<hr />
<pre>
 192     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (region allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,
 193                               word_size * HeapWordSize);
 194 
 195     assert(word_size * HeapWordSize &lt; HeapRegion::GrainBytes,
 196            &quot;This kind of expansion should never be more than one region. Size: &quot; SIZE_FORMAT,
 197            word_size * HeapWordSize);
 198     if (expand_single_region(node_index)) {
 199       // Given that expand_single_region() succeeded in expanding the heap, and we
 200       // always expand the heap by an amount aligned to the heap
 201       // region size, the free list should in theory not be empty.
 202       // In either case allocate_free_region() will check for NULL.
 203       res = _hrm-&gt;allocate_free_region(type, node_index);
 204     } else {
 205       _expand_heap_after_alloc_failure = false;
 206     }
 207   }
 208   return res;
 209 }
 210 
 211 HeapWord*
<span class="line-modified"> 212 G1CollectedHeap::humongous_obj_allocate_initialize_regions(HeapRegion* first_hr,</span>
 213                                                            uint num_regions,
 214                                                            size_t word_size) {
<span class="line-modified"> 215   assert(first_hr != NULL, &quot;pre-condition&quot;);</span>
 216   assert(is_humongous(word_size), &quot;word_size should be humongous&quot;);
 217   assert(num_regions * HeapRegion::GrainWords &gt;= word_size, &quot;pre-condition&quot;);
 218 
 219   // Index of last region in the series.
<span class="line-added"> 220   uint first = first_hr-&gt;hrm_index();</span>
 221   uint last = first + num_regions - 1;
 222 
 223   // We need to initialize the region(s) we just discovered. This is
 224   // a bit tricky given that it can happen concurrently with
 225   // refinement threads refining cards on these regions and
 226   // potentially wanting to refine the BOT as they are scanning
 227   // those cards (this can happen shortly after a cleanup; see CR
 228   // 6991377). So we have to set up the region(s) carefully and in
 229   // a specific order.
 230 
 231   // The word size sum of all the regions we will allocate.
 232   size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;
 233   assert(word_size &lt;= word_size_sum, &quot;sanity&quot;);
 234 
<span class="line-modified"> 235   // The passed in hr will be the &quot;starts humongous&quot; region. The header</span>
<span class="line-modified"> 236   // of the new object will be placed at the bottom of this region.</span>


 237   HeapWord* new_obj = first_hr-&gt;bottom();
 238   // This will be the new top of the new object.
 239   HeapWord* obj_top = new_obj + word_size;
 240 
 241   // First, we need to zero the header of the space that we will be
 242   // allocating. When we update top further down, some refinement
 243   // threads might try to scan the region. By zeroing the header we
 244   // ensure that any thread that will try to scan the region will
 245   // come across the zero klass word and bail out.
 246   //
 247   // NOTE: It would not have been correct to have used
 248   // CollectedHeap::fill_with_object() and make the space look like
 249   // an int array. The thread that is doing the allocation will
 250   // later update the object header to a potentially different array
 251   // type and, for a very short period of time, the klass and length
 252   // fields will be inconsistent. This could cause a refinement
 253   // thread to calculate the object size incorrectly.
 254   Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);
 255 
 256   // Next, pad out the unused tail of the last region with filler
</pre>
<hr />
<pre>
 322     _humongous_set.add(hr);
 323     _hr_printer.alloc(hr);
 324   }
 325 
 326   return new_obj;
 327 }
 328 
 329 size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {
 330   assert(is_humongous(word_size), &quot;Object of size &quot; SIZE_FORMAT &quot; must be humongous here&quot;, word_size);
 331   return align_up(word_size, HeapRegion::GrainWords) / HeapRegion::GrainWords;
 332 }
 333 
 334 // If could fit into free regions w/o expansion, try.
 335 // Otherwise, if can expand, do so.
 336 // Otherwise, if using ex regions might help, try with ex given back.
 337 HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {
 338   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
 339 
 340   _verifier-&gt;verify_region_sets_optional();
 341 

 342   uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);
 343 
<span class="line-modified"> 344   // Policy: First try to allocate a humongous object in the free list.</span>
<span class="line-modified"> 345   HeapRegion* humongous_start = _hrm-&gt;allocate_humongous(obj_regions);</span>
<span class="line-modified"> 346   if (humongous_start == NULL) {</span>















 347     // Policy: We could not find enough regions for the humongous object in the
 348     // free list. Look through the heap to find a mix of free and uncommitted regions.
<span class="line-modified"> 349     // If so, expand the heap and allocate the humongous object.</span>
<span class="line-modified"> 350     humongous_start = _hrm-&gt;expand_and_allocate_humongous(obj_regions);</span>
<span class="line-modified"> 351     if (humongous_start != NULL) {</span>
<span class="line-modified"> 352       // We managed to find a region by expanding the heap.</span>
<span class="line-modified"> 353       log_debug(gc, ergo, heap)(&quot;Heap expansion (humongous allocation request). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,</span>
<span class="line-modified"> 354                                 word_size * HeapWordSize);</span>



 355       policy()-&gt;record_new_heap_size(num_regions());










 356     } else {
 357       // Policy: Potentially trigger a defragmentation GC.
 358     }
 359   }
 360 
 361   HeapWord* result = NULL;
<span class="line-modified"> 362   if (humongous_start != NULL) {</span>
<span class="line-modified"> 363     result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);</span>
 364     assert(result != NULL, &quot;it should always return a valid result&quot;);
 365 
 366     // A successful humongous object allocation changes the used space
 367     // information of the old generation so we need to recalculate the
 368     // sizes and update the jstat counters here.
 369     g1mm()-&gt;update_sizes();
 370   }
 371 
 372   _verifier-&gt;verify_region_sets_optional();
 373 
 374   return result;
 375 }
 376 
 377 HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,
 378                                              size_t requested_size,
 379                                              size_t* actual_size) {
 380   assert_heap_not_locked_and_not_at_safepoint();
 381   assert(!is_humongous(requested_size), &quot;we do not allow humongous TLABs&quot;);
 382 
 383   return attempt_allocation(min_size, requested_size, actual_size);
</pre>
<hr />
<pre>
1753     HeapWord* end = _hrm-&gt;reserved().end();
1754     size_t granularity = HeapRegion::GrainBytes;
1755 
1756     _region_attr.initialize(start, end, granularity);
1757     _humongous_reclaim_candidates.initialize(start, end, granularity);
1758   }
1759 
1760   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1761                           true /* are_GC_task_threads */,
1762                           false /* are_ConcurrentGC_threads */);
1763   if (_workers == NULL) {
1764     return JNI_ENOMEM;
1765   }
1766   _workers-&gt;initialize_workers();
1767 
1768   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1769 
1770   // Create the G1ConcurrentMark data structure and thread.
1771   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1772   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);




1773   _cm_thread = _cm-&gt;cm_thread();
1774 
1775   // Now expand into the initial heap size.
1776   if (!expand(init_byte_size, _workers)) {
1777     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1778     return JNI_ENOMEM;
1779   }
1780 
1781   // Perform any initialization actions delegated to the policy.
1782   policy()-&gt;init(this, &amp;_collection_set);
1783 
1784   jint ecode = initialize_concurrent_refinement();
1785   if (ecode != JNI_OK) {
1786     return ecode;
1787   }
1788 
1789   ecode = initialize_young_gen_sampling_thread();
1790   if (ecode != JNI_OK) {
1791     return ecode;
1792   }
</pre>
<hr />
<pre>
1953 
1954 size_t G1CollectedHeap::recalculate_used() const {
1955   SumUsedClosure blk;
1956   heap_region_iterate(&amp;blk);
1957   return blk.result();
1958 }
1959 
1960 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1961   switch (cause) {
1962     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1963     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1964     case GCCause::_wb_conc_mark:                        return true;
1965     default :                                           return false;
1966   }
1967 }
1968 
1969 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
1970   switch (cause) {
1971     case GCCause::_g1_humongous_allocation: return true;
1972     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
<span class="line-added">1973     case GCCause::_wb_breakpoint:           return true;</span>
1974     default:                                return is_user_requested_concurrent_full_gc(cause);
1975   }
1976 }
1977 
1978 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
1979   if (policy()-&gt;force_upgrade_to_full()) {
1980     return true;
1981   } else if (should_do_concurrent_full_gc(_gc_cause)) {
1982     return false;
1983   } else if (has_regions_left_for_allocation()) {
1984     return false;
1985   } else {
1986     return true;
1987   }
1988 }
1989 
1990 #ifndef PRODUCT
1991 void G1CollectedHeap::allocate_dummy_regions() {
1992   // Let&#39;s fill up most of the region
1993   size_t word_size = HeapRegion::GrainWords - 1024;
</pre>
<hr />
<pre>
2124       return false;
2125     }
2126 
2127     // Lock to get consistent set of values.
2128     uint old_marking_started_after;
2129     uint old_marking_completed_after;
2130     {
2131       MutexLocker ml(Heap_lock);
2132       // Update gc_counter for retrying VMOp if needed. Captured here to be
2133       // consistent with the values we use below for termination tests.  If
2134       // a retry is needed after a possible wait, and another collection
2135       // occurs in the meantime, it will cause our retry to be skipped and
2136       // we&#39;ll recheck for termination with updated conditions from that
2137       // more recent collection.  That&#39;s what we want, rather than having
2138       // our retry possibly perform an unnecessary collection.
2139       gc_counter = total_collections();
2140       old_marking_started_after = _old_marking_cycles_started;
2141       old_marking_completed_after = _old_marking_cycles_completed;
2142     }
2143 
<span class="line-modified">2144     if (cause == GCCause::_wb_breakpoint) {</span>
<span class="line-added">2145       if (op.gc_succeeded()) {</span>
<span class="line-added">2146         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="line-added">2147         return true;</span>
<span class="line-added">2148       }</span>
<span class="line-added">2149       // When _wb_breakpoint there can&#39;t be another cycle or deferred.</span>
<span class="line-added">2150       assert(!op.cycle_already_in_progress(), &quot;invariant&quot;);</span>
<span class="line-added">2151       assert(!op.whitebox_attached(), &quot;invariant&quot;);</span>
<span class="line-added">2152       // Concurrent cycle attempt might have been cancelled by some other</span>
<span class="line-added">2153       // collection, so retry.  Unlike other cases below, we want to retry</span>
<span class="line-added">2154       // even if cancelled by a STW full collection, because we really want</span>
<span class="line-added">2155       // to start a concurrent cycle.</span>
<span class="line-added">2156       if (old_marking_started_before != old_marking_started_after) {</span>
<span class="line-added">2157         LOG_COLLECT_CONCURRENTLY(cause, &quot;ignoring STW full GC&quot;);</span>
<span class="line-added">2158         old_marking_started_before = old_marking_started_after;</span>
<span class="line-added">2159       }</span>
<span class="line-added">2160     } else if (!GCCause::is_user_requested_gc(cause)) {</span>
2161       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2162       // ensure that progress is made.
2163       //
2164       // Request is finished if any of
2165       // (1) the VMOp successfully performed a GC,
2166       // (2) a concurrent cycle was already in progress,
<span class="line-modified">2167       // (3) whitebox is controlling concurrent cycles,</span>
<span class="line-modified">2168       // (4) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2169       // (5) a Full GC was performed.</span>
<span class="line-added">2170       // Cases (4) and (5) are detected together by a change to</span>
2171       // _old_marking_cycles_started.
2172       //
<span class="line-modified">2173       // Note that (1) does not imply (4).  If we&#39;re still in the mixed</span>
2174       // phase of an earlier concurrent collection, the request to make the
2175       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
2176       // both conditions we&#39;ll spin doing back-to-back collections.
2177       if (op.gc_succeeded() ||
2178           op.cycle_already_in_progress() ||
<span class="line-added">2179           op.whitebox_attached() ||</span>
2180           (old_marking_started_before != old_marking_started_after)) {
2181         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2182         return true;
2183       }
2184     } else {                    // User-requested GC.
2185       // For a user-requested collection, we want to ensure that a complete
2186       // full collection has been performed before returning, but without
2187       // waiting for more than needed.
2188 
2189       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2190       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2191       // should do otherwise.  Trying again just does back to back GCs.
2192       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2193       // to meet the goal of ensuring a full collection was performed.
2194       assert(!op.gc_succeeded() ||
2195              (old_marking_started_before != old_marking_started_after),
2196              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,
2197              BOOL_TO_STR(op.gc_succeeded()),
2198              old_marking_started_before, old_marking_started_after);
2199 
</pre>
<hr />
<pre>
2213         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);
2214         MonitorLocker ml(G1OldGCCount_lock);
2215         while (gc_counter_less_than(_old_marking_cycles_completed,
2216                                     old_marking_started_after)) {
2217           ml.wait();
2218         }
2219         // Request is finished if the collection we just waited for was
2220         // started after this request.
2221         if (old_marking_started_before != old_marking_started_after) {
2222           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);
2223           return true;
2224         }
2225       }
2226 
2227       // If VMOp was successful then it started a new cycle that the above
2228       // wait &amp;etc should have recognized as finishing this request.  This
2229       // differs from a non-user-request, where gc_succeeded does not imply
2230       // a new cycle was started.
2231       assert(!op.gc_succeeded(), &quot;invariant&quot;);
2232 



2233       if (op.cycle_already_in_progress()) {
<span class="line-added">2234         // If VMOp failed because a cycle was already in progress, it</span>
<span class="line-added">2235         // is now complete.  But it didn&#39;t finish this user-requested</span>
<span class="line-added">2236         // GC, so try again.</span>
2237         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
2238         continue;
<span class="line-added">2239       } else if (op.whitebox_attached()) {</span>
<span class="line-added">2240         // If WhiteBox wants control, wait for notification of a state</span>
<span class="line-added">2241         // change in the controller, then try again.  Don&#39;t wait for</span>
<span class="line-added">2242         // release of control, since collections may complete while in</span>
<span class="line-added">2243         // control.  Note: This won&#39;t recognize a STW full collection</span>
<span class="line-added">2244         // while waiting; we can&#39;t wait on multiple monitors.</span>
<span class="line-added">2245         LOG_COLLECT_CONCURRENTLY(cause, &quot;whitebox control stall&quot;);</span>
<span class="line-added">2246         MonitorLocker ml(ConcurrentGCBreakpoints::monitor());</span>
<span class="line-added">2247         if (ConcurrentGCBreakpoints::is_controlled()) {</span>
<span class="line-added">2248           ml.wait();</span>
<span class="line-added">2249         }</span>
<span class="line-added">2250         continue;</span>
2251       }
2252     }
2253 
2254     // Collection failed and should be retried.
2255     assert(op.transient_failure(), &quot;invariant&quot;);
2256 

2257     if (GCLocker::is_active_and_needs_gc()) {
<span class="line-added">2258       // If GCLocker is active, wait until clear before retrying.</span>
2259       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
2260       GCLocker::stall_until_clear();
2261     }
2262 
2263     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
2264   }
2265 }
2266 
2267 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2268   assert_heap_not_locked();
2269 
2270   // Lock to get consistent set of values.
2271   uint gc_count_before;
2272   uint full_gc_count_before;
2273   uint old_marking_started_before;
2274   {
2275     MutexLocker ml(Heap_lock);
2276     gc_count_before = total_collections();
2277     full_gc_count_before = total_full_collections();
2278     old_marking_started_before = _old_marking_cycles_started;
</pre>
<hr />
<pre>
2434   }
2435   return ret_val;
2436 }
2437 
2438 void G1CollectedHeap::deduplicate_string(oop str) {
2439   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2440 
2441   if (G1StringDedup::is_enabled()) {
2442     G1StringDedup::deduplicate(str);
2443   }
2444 }
2445 
2446 void G1CollectedHeap::prepare_for_verify() {
2447   _verifier-&gt;prepare_for_verify();
2448 }
2449 
2450 void G1CollectedHeap::verify(VerifyOption vo) {
2451   _verifier-&gt;verify(vo);
2452 }
2453 
<span class="line-modified">2454 bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {</span>
2455   return true;
2456 }
2457 




2458 bool G1CollectedHeap::is_heterogeneous_heap() const {
2459   return G1Arguments::is_heterogeneous_heap();
2460 }
2461 
2462 class PrintRegionClosure: public HeapRegionClosure {
2463   outputStream* _st;
2464 public:
2465   PrintRegionClosure(outputStream* st) : _st(st) {}
2466   bool do_heap_region(HeapRegion* r) {
2467     r-&gt;print_on(_st);
2468     return false;
2469   }
2470 };
2471 
2472 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2473                                        const HeapRegion* hr,
2474                                        const VerifyOption vo) const {
2475   switch (vo) {
2476   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2477   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
</pre>
<hr />
<pre>
3155     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3156     // before any GC notifications are raised.
3157     g1mm()-&gt;update_sizes();
3158 
3159     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3160     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3161     _gc_timer_stw-&gt;register_gc_end();
3162     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3163   }
3164   // It should now be safe to tell the concurrent mark thread to start
3165   // without its logging output interfering with the logging output
3166   // that came from the pause.
3167 
3168   if (should_start_conc_mark) {
3169     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3170     // thread(s) could be running concurrently with us. Make sure that anything
3171     // after this point does not assume that we are the only GC thread running.
3172     // Note: of course, the actual marking work will not start until the safepoint
3173     // itself is released in SuspendibleThreadSet::desynchronize().
3174     do_concurrent_mark();
<span class="line-added">3175     ConcurrentGCBreakpoints::notify_idle_to_active();</span>
3176   }
3177 }
3178 
3179 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3180   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3181   workers()-&gt;run_task(&amp;rsfp_task);
3182 }
3183 
3184 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3185   double remove_self_forwards_start = os::elapsedTime();
3186 
3187   remove_self_forwarding_pointers(rdcqs);
3188   _preserved_marks_set.restore(workers());
3189 
3190   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3191 }
3192 
3193 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3194   if (!_evacuation_failed) {
3195     _evacuation_failed = true;
</pre>
<hr />
<pre>
4845     assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());
4846     _survivor.add_used_bytes(allocated_bytes);
4847   }
4848 
4849   bool const during_im = collector_state()-&gt;in_initial_mark_gc();
4850   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
4851     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
4852   }
4853   _hr_printer.retire(alloc_region);
4854 }
4855 
4856 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4857   bool expanded = false;
4858   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4859 
4860   if (index != G1_NO_HRM_INDEX) {
4861     if (expanded) {
4862       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
4863                                 HeapRegion::GrainWords * HeapWordSize);
4864     }
<span class="line-modified">4865     return _hrm-&gt;allocate_free_regions_starting_at(index, 1);</span>

4866   }
4867   return NULL;
4868 }
4869 
4870 // Optimized nmethod scanning
4871 
4872 class RegisterNMethodOopClosure: public OopClosure {
4873   G1CollectedHeap* _g1h;
4874   nmethod* _nm;
4875 
4876   template &lt;class T&gt; void do_oop_work(T* p) {
4877     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
4878     if (!CompressedOops::is_null(heap_oop)) {
4879       oop obj = CompressedOops::decode_not_null(heap_oop);
4880       HeapRegion* hr = _g1h-&gt;heap_region_containing(obj);
4881       assert(!hr-&gt;is_continues_humongous(),
4882              &quot;trying to add code root &quot; PTR_FORMAT &quot; in continuation of humongous region &quot; HR_FORMAT
4883              &quot; starting at &quot; HR_FORMAT,
4884              p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr-&gt;humongous_start_region()));
4885 
</pre>
</td>
</tr>
</table>
<center><a href="g1Arguments.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>