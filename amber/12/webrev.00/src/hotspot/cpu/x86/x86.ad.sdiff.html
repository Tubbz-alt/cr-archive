<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="vm_version_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_32.ad.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/x86.ad</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 








1168 %} // end source_hpp
1169 
1170 source %{
1171 
1172 #include &quot;opto/addnode.hpp&quot;
































1173 
1174 // Emit exception handler code.
1175 // Stuff framesize into a register and call a VM stub routine.
1176 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1177 
1178   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1179   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1180   MacroAssembler _masm(&amp;cbuf);</span>
1181   address base = __ start_a_stub(size_exception_handler());
1182   if (base == NULL) {
1183     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1184     return 0;  // CodeBuffer::expand failed
1185   }
1186   int offset = __ offset();
1187   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1188   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1189   __ end_a_stub();
1190   return offset;
1191 }
1192 
1193 // Emit deopt handler code.
1194 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1195 
1196   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1197   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1198   MacroAssembler _masm(&amp;cbuf);</span>
1199   address base = __ start_a_stub(size_deopt_handler());
1200   if (base == NULL) {
1201     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1202     return 0;  // CodeBuffer::expand failed
1203   }
1204   int offset = __ offset();
1205 
1206 #ifdef _LP64
1207   address the_pc = (address) __ pc();
1208   Label next;
1209   // push a &quot;the_pc&quot; on the stack without destroying any registers
1210   // as they all may be live.
1211 
1212   // push address of &quot;next&quot;
1213   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1214   __ bind(next);
1215   // adjust it so it matches &quot;the_pc&quot;
1216   __ subptr(Address(rsp, 0), __ offset() - offset);
1217 #else
1218   InternalAddress here(__ pc());
</pre>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
<span class="line-removed">1280     case Op_AddReductionVL:</span>
<span class="line-removed">1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here</span>
<span class="line-removed">1282         return false;</span>
<span class="line-removed">1283       }</span>
<span class="line-removed">1284       break;</span>
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
<span class="line-modified">1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3</span>



1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
<span class="line-removed">1298     case Op_AddReductionVF:</span>
<span class="line-removed">1299     case Op_AddReductionVD:</span>
<span class="line-removed">1300     case Op_MulReductionVF:</span>
<span class="line-removed">1301     case Op_MulReductionVD:</span>
<span class="line-removed">1302       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-removed">1303         return false;</span>
<span class="line-removed">1304       }</span>
<span class="line-removed">1305       break;</span>
1306     case Op_SqrtVD:
1307     case Op_SqrtVF:
1308       if (UseAVX &lt; 1) { // enabled for AVX only
1309         return false;
1310       }
1311       break;
1312     case Op_CompareAndSwapL:
1313 #ifdef _LP64
1314     case Op_CompareAndSwapP:
1315 #endif
1316       if (!VM_Version::supports_cx8()) {
1317         return false;
1318       }
1319       break;
1320     case Op_CMoveVF:
1321     case Op_CMoveVD:
1322       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1323         return false;
1324       }
1325       break;
1326     case Op_StrIndexOf:
1327       if (!UseSSE42Intrinsics) {
1328         return false;
1329       }
1330       break;
1331     case Op_StrIndexOfChar:
1332       if (!UseSSE42Intrinsics) {
1333         return false;
1334       }
1335       break;
1336     case Op_OnSpinWait:
1337       if (VM_Version::supports_on_spin_wait() == false) {
1338         return false;
1339       }
1340       break;
<span class="line-removed">1341     case Op_MulAddVS2VI:</span>
<span class="line-removed">1342     case Op_RShiftVL:</span>
<span class="line-removed">1343     case Op_AbsVD:</span>
<span class="line-removed">1344     case Op_NegVD:</span>
<span class="line-removed">1345       if (UseSSE &lt; 2) {</span>
<span class="line-removed">1346         return false;</span>
<span class="line-removed">1347       }</span>
<span class="line-removed">1348       break;</span>
1349     case Op_MulVB:
1350     case Op_LShiftVB:
1351     case Op_RShiftVB:
1352     case Op_URShiftVB:
1353       if (UseSSE &lt; 4) {
1354         return false;
1355       }
1356       break;
1357 #ifdef _LP64
1358     case Op_MaxD:
1359     case Op_MaxF:
1360     case Op_MinD:
1361     case Op_MinF:
1362       if (UseAVX &lt; 1) { // enabled for AVX only
1363         return false;
1364       }
1365       break;
1366 #endif
1367     case Op_CacheWB:
1368     case Op_CacheWBPreSync:
1369     case Op_CacheWBPostSync:
1370       if (!VM_Version::supports_data_cache_line_flush()) {
1371         return false;
1372       }
1373       break;
1374     case Op_RoundDoubleMode:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379     case Op_RoundDoubleModeV:
1380       if (VM_Version::supports_avx() == false) {
1381         return false; // 128bit vroundpd is not available
1382       }
1383       break;























1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
1404   if (!vector_size_supported(bt, vlen)) {
1405     return false;
1406   }
1407   // Special cases which require vector length follow:
1408   //   * implementation limitations
1409   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1410   //   * 128bit vroundpd instruction is present only in AVX1

1411   switch (opcode) {
1412     case Op_AbsVF:
1413     case Op_NegVF:
1414       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1415         return false; // 512bit vandps and vxorps are not available
1416       }
1417       break;
1418     case Op_AbsVD:
1419     case Op_NegVD:
1420       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1421         return false; // 512bit vandpd and vxorpd are not available
1422       }
1423       break;
1424     case Op_CMoveVF:
1425       if (vlen != 8) {
1426         return false; // implementation limitation (only vcmov8F_reg is present)
1427       }
1428       break;






1429     case Op_CMoveVD:
1430       if (vlen != 4) {
1431         return false; // implementation limitation (only vcmov4D_reg is present)
1432       }
1433       break;
1434   }
1435   return true;  // Per default match rules are supported.
1436 }
1437 
1438 // x86 supports generic vector operands: vec and legVec.
1439 const bool Matcher::supports_generic_vector_operands = true;
1440 
1441 MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1442   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1443   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1444   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1445       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1446     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1447     return new legVecZOper();
1448   }
</pre>
<hr />
<pre>
1607     mstack.push(shift-&gt;in(2), Matcher::Visit);
1608     Node *conv = shift-&gt;in(1);
1609 #ifdef _LP64
1610     // Allow Matcher to match the rule which bypass
1611     // ConvI2L operation for an array index on LP64
1612     // if the index value is positive.
1613     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1614         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1615         // Are there other uses besides address expressions?
1616         !matcher-&gt;is_visited(conv)) {
1617       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1618       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1619     } else
1620 #endif
1621       mstack.push(conv, Matcher::Pre_Visit);
1622     return true;
1623   }
1624   return false;
1625 }
1626 













































































































1627 // Should the Matcher clone shifts on addressing modes, expecting them
1628 // to be subsumed into complex addressing expressions or compute them
1629 // into registers?
<span class="line-modified">1630 bool Matcher::clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {</span>
1631   Node *off = m-&gt;in(AddPNode::Offset);
1632   if (off-&gt;is_Con()) {
1633     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1634     Node *adr = m-&gt;in(AddPNode::Address);
1635 
1636     // Intel can handle 2 adds in addressing mode
1637     // AtomicAdd is not an addressing expression.
1638     // Cheap to find it by looking for screwy base.
1639     if (adr-&gt;is_AddP() &amp;&amp;
1640         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1641         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1642         // Are there other uses besides address expressions?
1643         !is_visited(adr)) {
1644       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1645       Node *shift = adr-&gt;in(AddPNode::Offset);
1646       if (!clone_shift(shift, this, mstack, address_visited)) {
1647         mstack.push(shift, Pre_Visit);
1648       }
1649       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1650       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
</pre>
<hr />
<pre>
1656     mstack.push(off, Visit);
1657     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1658     return true;
1659   } else if (clone_shift(off, this, mstack, address_visited)) {
1660     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1661     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1662     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1663     return true;
1664   }
1665   return false;
1666 }
1667 
1668 void Compile::reshape_address(AddPNode* addp) {
1669 }
1670 
1671 static inline uint vector_length(const MachNode* n) {
1672   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1673   return vt-&gt;length();
1674 }
1675 






1676 static inline uint vector_length_in_bytes(const MachNode* n) {
1677   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1678   return vt-&gt;length_in_bytes();
1679 }
1680 
1681 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1682   uint def_idx = use-&gt;operand_index(opnd);
1683   Node* def = use-&gt;in(def_idx);
1684   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1685 }
1686 
1687 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1688   switch(vector_length_in_bytes(n)) {
1689     case  4: // fall-through
1690     case  8: // fall-through
1691     case 16: return Assembler::AVX_128bit;
1692     case 32: return Assembler::AVX_256bit;
1693     case 64: return Assembler::AVX_512bit;
1694 
1695     default: {
1696       ShouldNotReachHere();
1697       return Assembler::AVX_NoVec;
1698     }
1699   }
1700 }
1701 
1702 // Helper methods for MachSpillCopyNode::implementation().
1703 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1704                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1705   // In 64-bit VM size calculation is very complex. Emitting instructions
1706   // into scratch buffer is used to get size in 64-bit VM.
1707   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1708   assert(ireg == Op_VecS || // 32bit vector
1709          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1710          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1711          &quot;no non-adjacent vector moves&quot; );
1712   if (cbuf) {
<span class="line-modified">1713     MacroAssembler _masm(cbuf);</span>
1714     int offset = __ offset();
1715     switch (ireg) {
1716     case Op_VecS: // copy whole register
1717     case Op_VecD:
1718     case Op_VecX:
1719 #ifndef _LP64
1720       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1721 #else
1722       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1723         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1724       } else {
1725         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1726      }
1727 #endif
1728       break;
1729     case Op_VecY:
1730 #ifndef _LP64
1731       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1732 #else
1733       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
</pre>
<hr />
<pre>
1759       break;
1760     case Op_VecY:
1761     case Op_VecZ:
1762       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1763       break;
1764     default:
1765       ShouldNotReachHere();
1766     }
1767 #endif
1768   }
1769   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1770   return (UseAVX &gt; 2) ? 6 : 4;
1771 }
1772 
1773 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1774                      int stack_offset, int reg, uint ireg, outputStream* st) {
1775   // In 64-bit VM size calculation is very complex. Emitting instructions
1776   // into scratch buffer is used to get size in 64-bit VM.
1777   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1778   if (cbuf) {
<span class="line-modified">1779     MacroAssembler _masm(cbuf);</span>
1780     int offset = __ offset();
1781     if (is_load) {
1782       switch (ireg) {
1783       case Op_VecS:
1784         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1785         break;
1786       case Op_VecD:
1787         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1788         break;
1789       case Op_VecX:
1790 #ifndef _LP64
1791         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1792 #else
1793         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1794           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1795         } else {
1796           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1797           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1798         }
1799 #endif
</pre>
<hr />
<pre>
1962 static inline jlong replicate8_imm(int con, int width) {
1963   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
1964   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
1965   int bit_width = width * 8;
1966   jlong val = con;
1967   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
1968   while(bit_width &lt; 64) {
1969     val |= (val &lt;&lt; bit_width);
1970     bit_width &lt;&lt;= 1;
1971   }
1972   return val;
1973 }
1974 
1975 #ifndef PRODUCT
1976   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
1977     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
1978   }
1979 #endif
1980 
1981   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
<span class="line-modified">1982     MacroAssembler _masm(&amp;cbuf);</span>
1983     __ nop(_count);
1984   }
1985 
1986   uint MachNopNode::size(PhaseRegAlloc*) const {
1987     return _count;
1988   }
1989 
1990 #ifndef PRODUCT
1991   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
1992     st-&gt;print(&quot;# breakpoint&quot;);
1993   }
1994 #endif
1995 
1996   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
<span class="line-modified">1997     MacroAssembler _masm(&amp;cbuf);</span>
1998     __ int3();
1999   }
2000 
2001   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2002     return MachNode::size(ra_);
2003   }
2004 
2005 %}
2006 
2007 encode %{
2008 
2009   enc_class call_epilog %{
2010     if (VerifyStackAtCalls) {
2011       // Check that stack depth is unchanged: find majik cookie on stack
2012       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
<span class="line-modified">2013       MacroAssembler _masm(&amp;cbuf);</span>
2014       Label L;
2015       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2016       __ jccb(Assembler::equal, L);
2017       // Die if stack mismatch
2018       __ int3();
2019       __ bind(L);
2020     }
2021   %}
2022 
2023 %}
2024 
2025 
2026 //----------OPERANDS-----------------------------------------------------------
2027 // Operand definitions must precede instruction definitions for correct parsing
2028 // in the ADLC because operands constitute user defined types which are used in
2029 // instruction definitions.
2030 
2031 // Vectors
2032 
2033 // Dummy generic vector class. Should be used for all vector operands.
</pre>
<hr />
<pre>
3105   match(Set mem (StoreVector mem src));
3106   ins_cost(145);
3107   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3108   ins_encode %{
3109     switch (vector_length_in_bytes(this, $src)) {
3110       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3111       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3112       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3113       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3114       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3115       default: ShouldNotReachHere();
3116     }
3117   %}
3118   ins_pipe( pipe_slow );
3119 %}
3120 
3121 // ====================REPLICATE=======================================
3122 
3123 // Replicate byte scalar to be vector
3124 instruct ReplB_reg(vec dst, rRegI src) %{
<span class="line-removed">3125   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3126             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;); // vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_reg_leg</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
<span class="line-removed">3151 instruct ReplB_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3152   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions</span>
<span class="line-removed">3153   match(Set dst (ReplicateB src));</span>
<span class="line-removed">3154   format %{ &quot;replicateB $dst,$src&quot; %}</span>
<span class="line-removed">3155   ins_encode %{</span>
<span class="line-removed">3156     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3157     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3158     __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3159     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3160     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3161     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3162     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3163   %}</span>
<span class="line-removed">3164   ins_pipe( pipe_slow );</span>
<span class="line-removed">3165 %}</span>
<span class="line-removed">3166 </span>
3167 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3168   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32 &amp;&amp; VM_Version::supports_avx512vlbw()) || // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3169             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions</span>
3170   match(Set dst (ReplicateB (LoadB mem)));
3171   format %{ &quot;replicateB $dst,$mem&quot; %}
3172   ins_encode %{
<span class="line-removed">3173     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
3174     int vector_len = vector_length_encoding(this);
3175     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3176   %}
3177   ins_pipe( pipe_slow );
3178 %}
3179 
3180 instruct ReplB_imm(vec dst, immI con) %{
<span class="line-removed">3181   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3182             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3183   match(Set dst (ReplicateB con));
3184   format %{ &quot;replicateB $dst,$con&quot; %}
3185   ins_encode %{
3186     uint vlen = vector_length(this);
3187     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3188     if (vlen == 4) {
3189       __ movdl($dst$$XMMRegister, const_addr);
3190     } else {
3191       __ movq($dst$$XMMRegister, const_addr);
3192       if (vlen &gt;= 16) {
<span class="line-modified">3193         if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
3194           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3195           __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3196         } else {

3197           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-removed">3198           if (vlen &gt;= 32) {</span>
<span class="line-removed">3199              assert(vlen == 32, &quot;sanity&quot;);// vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_imm_leg</span>
<span class="line-removed">3200             __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3201           }</span>
3202         }
3203       }
3204     }
3205   %}
3206   ins_pipe( pipe_slow );
3207 %}
3208 
<span class="line-removed">3209 instruct ReplB_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3210   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3211   match(Set dst (ReplicateB con));</span>
<span class="line-removed">3212   format %{ &quot;replicateB $dst,$con&quot; %}</span>
<span class="line-removed">3213   ins_encode %{</span>
<span class="line-removed">3214     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));</span>
<span class="line-removed">3215     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3216     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3217     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3218   %}</span>
<span class="line-removed">3219   ins_pipe( pipe_slow );</span>
<span class="line-removed">3220 %}</span>
<span class="line-removed">3221 </span>
3222 // Replicate byte scalar zero to be vector
3223 instruct ReplB_zero(vec dst, immI0 zero) %{
3224   match(Set dst (ReplicateB zero));
3225   format %{ &quot;replicateB $dst,$zero&quot; %}
3226   ins_encode %{
3227     uint vlen = vector_length(this);
3228     if (vlen &lt;= 16) {
3229       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3230     } else {
3231       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3232       int vlen_enc = vector_length_encoding(this);
3233       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3234     }
3235   %}
3236   ins_pipe( fpu_reg_reg );
3237 %}
3238 
3239 // ====================ReplicateS=======================================
3240 
3241 instruct ReplS_reg(vec dst, rRegI src) %{
<span class="line-removed">3242   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3243             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3244   match(Set dst (ReplicateS src));
3245   format %{ &quot;replicateS $dst,$src&quot; %}
3246   ins_encode %{
3247     uint vlen = vector_length(this);
3248     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3249       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3250       int vlen_enc = vector_length_encoding(this);
3251       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3252     } else {
3253       __ movdl($dst$$XMMRegister, $src$$Register);
3254       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3255       if (vlen &gt;= 8) {
3256         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         if (vlen &gt;= 16) {
<span class="line-modified">3258           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_reg_leg</span>
3259           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3260         }
3261       }
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
<span class="line-removed">3267 instruct ReplS_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3268   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3269   match(Set dst (ReplicateS src));</span>
<span class="line-removed">3270   format %{ &quot;replicateS $dst,$src&quot; %}</span>
<span class="line-removed">3271   ins_encode %{</span>
<span class="line-removed">3272     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3273     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3274     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3275     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3276     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3277   %}</span>
<span class="line-removed">3278   ins_pipe( pipe_slow );</span>
<span class="line-removed">3279 %}</span>
<span class="line-removed">3280 </span>
3281 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3282   predicate((n-&gt;as_Vector()-&gt;length() &gt;= 4  &amp;&amp;</span>
<span class="line-removed">3283              n-&gt;as_Vector()-&gt;length() &lt;= 16 &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3284             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
<span class="line-removed">3285   match(Set dst (ReplicateS (LoadS mem)));</span>
<span class="line-removed">3286   format %{ &quot;replicateS $dst,$mem&quot; %}</span>
<span class="line-removed">3287   ins_encode %{</span>
<span class="line-removed">3288     uint vlen = vector_length(this);</span>
<span class="line-removed">3289     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3290       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-removed">3291       int vlen_enc = vector_length_encoding(this);</span>
<span class="line-removed">3292       __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>
<span class="line-removed">3293     } else {</span>
<span class="line-removed">3294       __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3295       if (vlen &gt;= 8) {</span>
<span class="line-removed">3296         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3297         if (vlen &gt;= 16) {</span>
<span class="line-removed">3298           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_mem_leg</span>
<span class="line-removed">3299           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3300         }</span>
<span class="line-removed">3301       }</span>
<span class="line-removed">3302     }</span>
<span class="line-removed">3303   %}</span>
<span class="line-removed">3304   ins_pipe( pipe_slow );</span>
<span class="line-removed">3305 %}</span>
<span class="line-removed">3306 </span>
<span class="line-removed">3307 instruct ReplS_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3308   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
3309   match(Set dst (ReplicateS (LoadS mem)));
3310   format %{ &quot;replicateS $dst,$mem&quot; %}
3311   ins_encode %{
<span class="line-modified">3312     __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3313     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3314     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3315     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
3316   %}
3317   ins_pipe( pipe_slow );
3318 %}
3319 
3320 instruct ReplS_imm(vec dst, immI con) %{
<span class="line-removed">3321   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3322             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3323   match(Set dst (ReplicateS con));
3324   format %{ &quot;replicateS $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
<span class="line-modified">3327     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3328     if (vlen == 2) {
<span class="line-modified">3329       __ movdl($dst$$XMMRegister, constaddr);</span>
3330     } else {
<span class="line-modified">3331       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3332       if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for &lt;512bit operands</span>
<span class="line-modified">3333         assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-modified">3334         int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3335         __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3336       } else {</span>
<span class="line-modified">3337         __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3338         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3339         if (vlen &gt;= 16) {</span>
<span class="line-removed">3340           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_imm_leg</span>
<span class="line-removed">3341           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3342         }
3343       }
3344     }
3345   %}
3346   ins_pipe( fpu_reg_reg );
3347 %}
3348 
<span class="line-removed">3349 instruct ReplS_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3350   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3351   match(Set dst (ReplicateS con));</span>
<span class="line-removed">3352   format %{ &quot;replicateS $dst,$con&quot; %}</span>
<span class="line-removed">3353   ins_encode %{</span>
<span class="line-removed">3354     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));</span>
<span class="line-removed">3355     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3356     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3357     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3358   %}</span>
<span class="line-removed">3359   ins_pipe( pipe_slow );</span>
<span class="line-removed">3360 %}</span>
<span class="line-removed">3361 </span>
3362 instruct ReplS_zero(vec dst, immI0 zero) %{
3363   match(Set dst (ReplicateS zero));
3364   format %{ &quot;replicateS $dst,$zero&quot; %}
3365   ins_encode %{
3366     uint vlen = vector_length(this);
3367     if (vlen &lt;= 8) {
3368       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3369     } else {
3370       int vlen_enc = vector_length_encoding(this);
3371       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3372     }
3373   %}
3374   ins_pipe( fpu_reg_reg );
3375 %}
3376 
3377 // ====================ReplicateI=======================================
3378 
3379 instruct ReplI_reg(vec dst, rRegI src) %{
3380   match(Set dst (ReplicateI src));
3381   format %{ &quot;replicateI $dst,$src&quot; %}
3382   ins_encode %{
3383     uint vlen = vector_length(this);
3384     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3385       int vlen_enc = vector_length_encoding(this);
3386       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3387     } else {
3388       __ movdl($dst$$XMMRegister, $src$$Register);
3389       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3390       if (vlen &gt;= 8) {
3391         assert(vlen == 8, &quot;sanity&quot;);
3392         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3393       }
3394     }
3395   %}
3396   ins_pipe( pipe_slow );
3397 %}
3398 
3399 instruct ReplI_mem(vec dst, memory mem) %{
<span class="line-removed">3400   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3401   match(Set dst (ReplicateI (LoadI mem)));
3402   format %{ &quot;replicateI $dst,$mem&quot; %}
3403   ins_encode %{
3404     uint vlen = vector_length(this);
3405     if (vlen &lt;= 4) {
<span class="line-modified">3406       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3407     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3408       int vector_len = vector_length_encoding(this);
3409       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3410     } else {</span>
<span class="line-removed">3411       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3412       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3413       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3414     }
3415   %}
3416   ins_pipe( pipe_slow );
3417 %}
3418 
3419 instruct ReplI_imm(vec dst, immI con) %{
3420   match(Set dst (ReplicateI con));
3421   format %{ &quot;replicateI $dst,$con&quot; %}
3422   ins_encode %{
3423     uint vlen = vector_length(this);
<span class="line-modified">3424     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3425     if (vlen == 2) {</span>
<span class="line-modified">3426       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3427     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>




3428       int vector_len = vector_length_encoding(this);
<span class="line-modified">3429       __ movq($dst$$XMMRegister, constaddr);</span>
3430       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
<span class="line-removed">3431     } else {</span>
<span class="line-removed">3432       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-removed">3433       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3434       if (vlen &gt;= 8) {</span>
<span class="line-removed">3435         assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3436         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3437       }</span>
3438     }
3439   %}
3440   ins_pipe( pipe_slow );
3441 %}
3442 
3443 // Replicate integer (4 byte) scalar zero to be vector
3444 instruct ReplI_zero(vec dst, immI0 zero) %{
3445   match(Set dst (ReplicateI zero));
3446   format %{ &quot;replicateI $dst,$zero&quot; %}
3447   ins_encode %{
3448     uint vlen = vector_length(this);
3449     if (vlen &lt;= 4) {
3450       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3451     } else {
3452       int vlen_enc = vector_length_encoding(this);
3453       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3454     }
3455   %}
3456   ins_pipe( fpu_reg_reg );
3457 %}
3458 














3459 // ====================ReplicateL=======================================
3460 
3461 #ifdef _LP64
3462 // Replicate long (8 byte) scalar to be vector
3463 instruct ReplL_reg(vec dst, rRegL src) %{
3464   match(Set dst (ReplicateL src));
3465   format %{ &quot;replicateL $dst,$src&quot; %}
3466   ins_encode %{
3467     uint vlen = vector_length(this);
3468     if (vlen == 2) {
3469       __ movdq($dst$$XMMRegister, $src$$Register);
3470       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3471     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3472       int vlen_enc = vector_length_encoding(this);
3473       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3474     } else {
3475       assert(vlen == 4, &quot;sanity&quot;);
3476       __ movdq($dst$$XMMRegister, $src$$Register);
3477       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3478       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
</pre>
<hr />
<pre>
3527     } else {
3528       int vector_len = Assembler::AVX_512bit;
3529       __ movdl($dst$$XMMRegister, $src$$Register);
3530       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3531       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3532       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3533     }
3534   %}
3535   ins_pipe( pipe_slow );
3536 %}
3537 #endif // _LP64
3538 
3539 instruct ReplL_mem(vec dst, memory mem) %{
3540   match(Set dst (ReplicateL (LoadL mem)));
3541   format %{ &quot;replicateL $dst,$mem&quot; %}
3542   ins_encode %{
3543     uint vlen = vector_length(this);
3544     if (vlen == 2) {
3545       __ movq($dst$$XMMRegister, $mem$$Address);
3546       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3547     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3548       int vlen_enc = vector_length_encoding(this);
3549       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
<span class="line-removed">3550     } else {</span>
<span class="line-removed">3551       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3552       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-removed">3553       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3554       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3555     }
3556   %}
3557   ins_pipe( pipe_slow );
3558 %}
3559 
3560 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3561 instruct ReplL_imm(vec dst, immL con) %{
3562   match(Set dst (ReplicateL con));
3563   format %{ &quot;replicateL $dst,$con&quot; %}
3564   ins_encode %{
3565     uint vlen = vector_length(this);
3566     InternalAddress const_addr = $constantaddress($con);
3567     if (vlen == 2) {
3568       __ movq($dst$$XMMRegister, const_addr);
3569       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3570     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3571       int vlen_enc = vector_length_encoding(this);
3572       __ movq($dst$$XMMRegister, const_addr);
3573       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
<span class="line-removed">3574     } else {</span>
<span class="line-removed">3575       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3576       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-removed">3577       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3578       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3579     }
3580   %}
3581   ins_pipe( pipe_slow );
3582 %}
3583 
3584 instruct ReplL_zero(vec dst, immL0 zero) %{
3585   match(Set dst (ReplicateL zero));
3586   format %{ &quot;replicateL $dst,$zero&quot; %}
3587   ins_encode %{
3588     int vlen = vector_length(this);
3589     if (vlen == 2) {
3590       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3591     } else {
3592       int vlen_enc = vector_length_encoding(this);
3593       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3594     }
3595   %}
3596   ins_pipe( fpu_reg_reg );
3597 %}
3598 












3599 // ====================ReplicateF=======================================
3600 
3601 instruct ReplF_reg(vec dst, vlRegF src) %{
3602   match(Set dst (ReplicateF src));
3603   format %{ &quot;replicateF $dst,$src&quot; %}
3604   ins_encode %{
3605     uint vlen = vector_length(this);
3606     if (vlen &lt;= 4) {
3607       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3608     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3609       int vector_len = vector_length_encoding(this);
<span class="line-modified">3610       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3611     } else {
3612       assert(vlen == 8, &quot;sanity&quot;);
3613       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3614       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3615     }
3616   %}
3617   ins_pipe( pipe_slow );
3618 %}
3619 
3620 instruct ReplF_mem(vec dst, memory mem) %{
<span class="line-removed">3621   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3622   match(Set dst (ReplicateF (LoadF mem)));
3623   format %{ &quot;replicateF $dst,$mem&quot; %}
3624   ins_encode %{
3625     uint vlen = vector_length(this);
3626     if (vlen &lt;= 4) {
<span class="line-modified">3627       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3628     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3629       int vector_len = vector_length_encoding(this);
3630       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3631     } else {</span>
<span class="line-removed">3632       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3633       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3634       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3635     }
3636   %}
3637   ins_pipe( pipe_slow );
3638 %}
3639 
3640 instruct ReplF_zero(vec dst, immF0 zero) %{
3641   match(Set dst (ReplicateF zero));
3642   format %{ &quot;replicateF $dst,$zero&quot; %}
3643   ins_encode %{
3644     uint vlen = vector_length(this);
3645     if (vlen &lt;= 4) {
3646       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3647     } else {
3648       int vlen_enc = vector_length_encoding(this);
3649       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3650     }
3651   %}
3652   ins_pipe( fpu_reg_reg );
3653 %}
3654 
3655 // ====================ReplicateD=======================================
3656 
3657 // Replicate double (8 bytes) scalar to be vector
3658 instruct ReplD_reg(vec dst, vlRegD src) %{
3659   match(Set dst (ReplicateD src));
3660   format %{ &quot;replicateD $dst,$src&quot; %}
3661   ins_encode %{
3662     uint vlen = vector_length(this);
3663     if (vlen == 2) {
3664       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3665     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3666       int vector_len = vector_length_encoding(this);
<span class="line-modified">3667       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3668     } else {
3669       assert(vlen == 4, &quot;sanity&quot;);
3670       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3671       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3672     }
3673   %}
3674   ins_pipe( pipe_slow );
3675 %}
3676 
3677 instruct ReplD_mem(vec dst, memory mem) %{
<span class="line-removed">3678   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3679   match(Set dst (ReplicateD (LoadD mem)));
3680   format %{ &quot;replicateD $dst,$mem&quot; %}
3681   ins_encode %{
3682     uint vlen = vector_length(this);
3683     if (vlen == 2) {
<span class="line-modified">3684       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-modified">3685     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3686       int vector_len = vector_length_encoding(this);
3687       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3688     } else {</span>
<span class="line-removed">3689       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3690       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-removed">3691       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3692     }
3693   %}
3694   ins_pipe( pipe_slow );
3695 %}
3696 
3697 instruct ReplD_zero(vec dst, immD0 zero) %{
3698   match(Set dst (ReplicateD zero));
3699   format %{ &quot;replicateD $dst,$zero&quot; %}
3700   ins_encode %{
3701     uint vlen = vector_length(this);
3702     if (vlen == 2) {
3703       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3704     } else {
3705       int vlen_enc = vector_length_encoding(this);
3706       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3707     }
3708   %}
3709   ins_pipe( fpu_reg_reg );
3710 %}
3711 
3712 // ====================REDUCTION ARITHMETIC=======================================

3713 
<span class="line-modified">3714 // =======================AddReductionVI==========================================</span>
<span class="line-modified">3715 </span>
<span class="line-modified">3716 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3717   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">3718   match(Set dst (AddReductionVI src1 src2));</span>
<span class="line-removed">3719   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3720   format %{ &quot;vector_add2I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3721   ins_encode %{</span>
<span class="line-removed">3722     if (UseAVX &gt; 2) {</span>
<span class="line-removed">3723       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3724       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">3725       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3726       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3727       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3728       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3729     } else if (VM_Version::supports_avxonly()) {</span>
<span class="line-removed">3730       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3731       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3732       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3733       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3734       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3735     } else {</span>
<span class="line-removed">3736       assert(UseSSE &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3737       __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3738       __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3739       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3740       __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3741       __ movdl($dst$$Register, $tmp$$XMMRegister);</span>
<span class="line-removed">3742     }</span>
<span class="line-removed">3743   %}</span>
<span class="line-removed">3744   ins_pipe( pipe_slow );</span>
<span class="line-removed">3745 %}</span>
<span class="line-removed">3746 </span>
<span class="line-removed">3747 instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3748   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
3749   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3750   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3751   format %{ &quot;vector_add4I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-modified">3752   ins_encode %{</span>
<span class="line-modified">3753     if (UseAVX &gt; 2) {</span>
<span class="line-modified">3754       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-modified">3755       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-modified">3756       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-modified">3757       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-modified">3758       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-modified">3759       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3760       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3761       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3762     } else if (VM_Version::supports_avxonly()) {</span>
<span class="line-removed">3763       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3764       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3765       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3766       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3767       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3768       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3769     } else {</span>
<span class="line-removed">3770       assert(UseSSE &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3771       __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3772       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3773       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3774       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3775       __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3776       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3777     }</span>
<span class="line-removed">3778   %}</span>
<span class="line-removed">3779   ins_pipe( pipe_slow );</span>
<span class="line-removed">3780 %}</span>
<span class="line-removed">3781 </span>
<span class="line-removed">3782 instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3783   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">3784   match(Set dst (AddReductionVI src1 src2));</span>
<span class="line-removed">3785   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3786   format %{ &quot;vector_add8I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3787   ins_encode %{</span>
<span class="line-removed">3788     if (UseAVX &gt; 2) {</span>
<span class="line-removed">3789       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3790       __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3791       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3792       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">3793       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3794       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">3795       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3796       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3797       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3798       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3799     } else {</span>
<span class="line-removed">3800       assert(UseAVX &gt; 0, &quot;&quot;);</span>
<span class="line-removed">3801       int vector_len = Assembler::AVX_256bit;</span>
<span class="line-removed">3802       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3803       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3804       __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3805       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3806       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3807       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3808       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3809     }</span>
3810   %}
3811   ins_pipe( pipe_slow );
3812 %}
3813 
<span class="line-modified">3814 instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{</span>
<span class="line-modified">3815   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>

3816   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3817   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);</span>
<span class="line-modified">3818   format %{ &quot;vector_add16I_reduction $dst,$src1,$src2&quot; %}</span>




3819   ins_encode %{
<span class="line-modified">3820     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3821     __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-modified">3822     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);</span>
<span class="line-removed">3823     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);</span>
<span class="line-removed">3824     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">3825     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3826     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">3827     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3828     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3829     __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3830     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
3831   %}
3832   ins_pipe( pipe_slow );
3833 %}
3834 
<span class="line-modified">3835 // =======================AddReductionVL==========================================</span>
3836 
3837 #ifdef _LP64
<span class="line-modified">3838 instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">3839   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-modified">3840   match(Set dst (AddReductionVL src1 src2));</span>
<span class="line-removed">3841   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3842   format %{ &quot;vector_add2L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3843   ins_encode %{</span>
<span class="line-removed">3844     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3845     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">3846     __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3847     __ movdq($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3848     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3849     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3850   %}</span>
<span class="line-removed">3851   ins_pipe( pipe_slow );</span>
<span class="line-removed">3852 %}</span>
<span class="line-removed">3853 </span>
<span class="line-removed">3854 instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3855   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
3856   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3857   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3858   format %{ &quot;vector_add4L_reduction $dst,$src1,$src2&quot; %}</span>




3859   ins_encode %{
<span class="line-modified">3860     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3861     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3862     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);</span>
<span class="line-removed">3863     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">3864     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3865     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3866     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3867     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
3868   %}
3869   ins_pipe( pipe_slow );
3870 %}
3871 
<span class="line-modified">3872 instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">3873   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>

3874   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3875   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3876   format %{ &quot;vector_addL_reduction $dst,$src1,$src2&quot; %}</span>




3877   ins_encode %{
<span class="line-modified">3878     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3879     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3880     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">3881     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3882     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3883     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">3884     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3885     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3886     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3887     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
3888   %}
3889   ins_pipe( pipe_slow );
3890 %}
3891 #endif // _LP64
3892 
<span class="line-modified">3893 // =======================AddReductionVF==========================================</span>
<span class="line-removed">3894 </span>
<span class="line-removed">3895 instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">3896   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">3897   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3898   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">3899   format %{ &quot;vector_add2F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3900   ins_encode %{</span>
<span class="line-removed">3901     if (UseAVX &gt; 0) {</span>
<span class="line-removed">3902       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3903       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3904       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3905     } else {</span>
<span class="line-removed">3906       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3907       __ addss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3908       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3909       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3910     }</span>
<span class="line-removed">3911   %}</span>
<span class="line-removed">3912   ins_pipe( pipe_slow );</span>
<span class="line-removed">3913 %}</span>
<span class="line-removed">3914 </span>
<span class="line-removed">3915 instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">3916   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">3917   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3918   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">3919   format %{ &quot;vector_add4F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3920   ins_encode %{</span>
<span class="line-removed">3921     if (UseAVX &gt; 0) {</span>
<span class="line-removed">3922       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3923       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3924       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3925       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3926       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3927       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3928       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3929     } else {</span>
<span class="line-removed">3930       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3931       __ addss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3932       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3933       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3934       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3935       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3936       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3937       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3938     }</span>
<span class="line-removed">3939   %}</span>
<span class="line-removed">3940   ins_pipe( pipe_slow );</span>
<span class="line-removed">3941 %}</span>
<span class="line-removed">3942 </span>
<span class="line-removed">3943 </span>
<span class="line-removed">3944 instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3945   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">3946   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3947   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">3948   format %{ &quot;vector_add8F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3949   ins_encode %{</span>
<span class="line-removed">3950     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3951     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3952     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3953     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3954     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3955     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3956     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3957     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3958     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3959     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3960     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3961     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3962     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3963     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3964     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3965     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3966   %}</span>
<span class="line-removed">3967   ins_pipe( pipe_slow );</span>
<span class="line-removed">3968 %}</span>
<span class="line-removed">3969 </span>
<span class="line-removed">3970 instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">3971   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">3972   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3973   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">3974   format %{ &quot;vector_add16F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3975   ins_encode %{</span>
<span class="line-removed">3976     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3977     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3978     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3979     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3980     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3981     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3982     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3983     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3984     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">3985     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3986     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3987     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3988     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3989     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3990     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3991     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3992     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">3993     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3994     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3995     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3996     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3997     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3998     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3999     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4000     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4001     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4002     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4003     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4004     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4005     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4006     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4007     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4008   %}</span>
<span class="line-removed">4009   ins_pipe( pipe_slow );</span>
<span class="line-removed">4010 %}</span>
<span class="line-removed">4011 </span>
<span class="line-removed">4012 // =======================AddReductionVD==========================================</span>
<span class="line-removed">4013 </span>
<span class="line-removed">4014 instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4015   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4016   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4017   effect(TEMP tmp, TEMP dst);</span>
<span class="line-removed">4018   format %{ &quot;vector_add2D_reduction  $dst,$src2&quot; %}</span>
<span class="line-removed">4019   ins_encode %{</span>
<span class="line-removed">4020     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4021       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4022       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4023       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4024     } else {</span>
<span class="line-removed">4025       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4026       __ addsd($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4027       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4028       __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4029     }</span>
<span class="line-removed">4030   %}</span>
<span class="line-removed">4031   ins_pipe( pipe_slow );</span>
<span class="line-removed">4032 %}</span>
4033 
<span class="line-modified">4034 instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4035   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-modified">4036   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-modified">4037   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4038   format %{ &quot;vector_add4D_reduction $dst,$dst,$src2&quot; %}</span>

4039   ins_encode %{
<span class="line-modified">4040     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4041     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4042     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4043     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4044     __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4045     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4046     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4047     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4048   %}</span>
<span class="line-removed">4049   ins_pipe( pipe_slow );</span>
<span class="line-removed">4050 %}</span>
<span class="line-removed">4051 </span>
<span class="line-removed">4052 instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">4053   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">4054   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4055   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4056   format %{ &quot;vector_add8D_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4057   ins_encode %{</span>
<span class="line-removed">4058     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4059     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4060     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4061     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4062     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4063     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4064     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4065     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4066     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4067     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4068     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4069     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4070     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4071     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4072     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4073     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4074   %}</span>
<span class="line-removed">4075   ins_pipe( pipe_slow );</span>
<span class="line-removed">4076 %}</span>
<span class="line-removed">4077 </span>
<span class="line-removed">4078 // =======================MulReductionVI==========================================</span>
<span class="line-removed">4079 </span>
<span class="line-removed">4080 instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4081   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4082   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4083   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4084   format %{ &quot;vector_mul2I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4085   ins_encode %{</span>
<span class="line-removed">4086     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4087       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4088       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4089       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4090       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4091       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4092       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4093     } else {</span>
<span class="line-removed">4094       assert(UseSSE &gt; 3, &quot;required&quot;);</span>
<span class="line-removed">4095       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4096       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4097       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4098       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4099       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4100     }</span>
<span class="line-removed">4101   %}</span>
<span class="line-removed">4102   ins_pipe( pipe_slow );</span>
<span class="line-removed">4103 %}</span>
<span class="line-removed">4104 </span>
<span class="line-removed">4105 instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4106   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4107   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4108   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4109   format %{ &quot;vector_mul4I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4110   ins_encode %{</span>
<span class="line-removed">4111     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4112       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4113       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4114       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4115       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4116       __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4117       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4118       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4119       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4120     } else {</span>
<span class="line-removed">4121       assert(UseSSE &gt; 3, &quot;required&quot;);</span>
<span class="line-removed">4122       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4123       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4124       __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);</span>
<span class="line-removed">4125       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4126       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4127       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4128       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4129     }</span>
<span class="line-removed">4130   %}</span>
<span class="line-removed">4131   ins_pipe( pipe_slow );</span>
<span class="line-removed">4132 %}</span>
<span class="line-removed">4133 </span>
<span class="line-removed">4134 instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4135   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">4136   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4137   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4138   format %{ &quot;vector_mul8I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4139   ins_encode %{</span>
<span class="line-removed">4140     assert(UseAVX &gt; 1, &quot;required&quot;);</span>
<span class="line-removed">4141     int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4142     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4143     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">4144     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">4145     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4146     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4147     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4148     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4149     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4150     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4151   %}</span>
<span class="line-removed">4152   ins_pipe( pipe_slow );</span>
<span class="line-removed">4153 %}</span>
<span class="line-removed">4154 </span>
<span class="line-removed">4155 instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{</span>
<span class="line-removed">4156   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">4157   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4158   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);</span>
<span class="line-removed">4159   format %{ &quot;vector_mul16I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4160   ins_encode %{</span>
<span class="line-removed">4161     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4162     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4163     __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">4164     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);</span>
<span class="line-removed">4165     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);</span>
<span class="line-removed">4166     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">4167     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4168     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4169     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4170     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4171     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4172     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4173   %}</span>
<span class="line-removed">4174   ins_pipe( pipe_slow );</span>
<span class="line-removed">4175 %}</span>
<span class="line-removed">4176 </span>
<span class="line-removed">4177 // =======================MulReductionVL==========================================</span>
<span class="line-removed">4178 </span>
<span class="line-removed">4179 #ifdef _LP64</span>
<span class="line-removed">4180 instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4181   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4182   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-removed">4183   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4184   format %{ &quot;vector_mul2L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4185   ins_encode %{</span>
<span class="line-removed">4186     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-removed">4187     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4188     __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4189     __ movdq($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4190     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4191     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4192   %}</span>
<span class="line-removed">4193   ins_pipe( pipe_slow );</span>
<span class="line-removed">4194 %}</span>
<span class="line-removed">4195 </span>
<span class="line-removed">4196 instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4197   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4198   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-removed">4199   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4200   format %{ &quot;vector_mul4L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4201   ins_encode %{</span>
<span class="line-removed">4202     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-removed">4203     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4204     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);</span>
<span class="line-removed">4205     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4206     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4207     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4208     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4209     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
4210   %}
4211   ins_pipe( pipe_slow );
4212 %}
4213 
<span class="line-modified">4214 instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">4215   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-modified">4216   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">4217   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">4218   format %{ &quot;vector_mul8L_reduction $dst,$src1,$src2&quot; %}</span>

4219   ins_encode %{
<span class="line-modified">4220     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-modified">4221     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4222     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">4223     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4224     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4225     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4226     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4227     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4228     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4229     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
4230   %}
4231   ins_pipe( pipe_slow );
4232 %}
<span class="line-removed">4233 #endif</span>
<span class="line-removed">4234 </span>
<span class="line-removed">4235 // =======================MulReductionVF==========================================</span>
4236 
<span class="line-modified">4237 instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-modified">4238   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-modified">4239   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-modified">4240   effect(TEMP dst, TEMP tmp);</span>
<span class="line-modified">4241   format %{ &quot;vector_mul2F_reduction $dst,$dst,$src2&quot; %}</span>

4242   ins_encode %{
<span class="line-modified">4243     if (UseAVX &gt; 0) {</span>
<span class="line-modified">4244       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4245       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4246       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4247     } else {</span>
<span class="line-removed">4248       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4249       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4250       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4251       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4252     }</span>
4253   %}
4254   ins_pipe( pipe_slow );
4255 %}
4256 
<span class="line-modified">4257 instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4258   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4259   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-removed">4260   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">4261   format %{ &quot;vector_mul4F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4262   ins_encode %{</span>
<span class="line-removed">4263     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4264       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4265       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4266       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4267       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4268       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4269       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4270       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4271     } else {</span>
<span class="line-removed">4272       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4273       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4274       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4275       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4276       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4277       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4278       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4279       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4280     }</span>
<span class="line-removed">4281   %}</span>
<span class="line-removed">4282   ins_pipe( pipe_slow );</span>
<span class="line-removed">4283 %}</span>
4284 
<span class="line-modified">4285 instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4286   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-modified">4287   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-modified">4288   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4289   format %{ &quot;vector_mul8F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-modified">4290   ins_encode %{</span>
<span class="line-removed">4291     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4292     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4293     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4294     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4295     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4296     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4297     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4298     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4299     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4300     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4301     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4302     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4303     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4304     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4305     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4306     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4307   %}</span>
<span class="line-removed">4308   ins_pipe( pipe_slow );</span>
<span class="line-removed">4309 %}</span>
<span class="line-removed">4310 </span>
<span class="line-removed">4311 instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">4312   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">4313   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-removed">4314   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4315   format %{ &quot;vector_mul16F_reduction $dst,$dst,$src2&quot; %}</span>
4316   ins_encode %{
<span class="line-modified">4317     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">4318     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4319     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4320     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4321     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4322     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4323     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4324     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4325     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4326     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4327     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4328     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4329     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4330     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4331     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4332     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4333     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4334     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4335     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4336     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4337     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4338     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4339     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4340     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4341     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4342     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4343     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4344     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4345     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4346     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4347     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4348     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4349   %}</span>
<span class="line-removed">4350   ins_pipe( pipe_slow );</span>
<span class="line-removed">4351 %}</span>
<span class="line-removed">4352 </span>
<span class="line-removed">4353 // =======================MulReductionVD==========================================</span>
<span class="line-removed">4354 </span>
<span class="line-removed">4355 instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4356   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4357   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-removed">4358   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">4359   format %{ &quot;vector_mul2D_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4360   ins_encode %{</span>
<span class="line-removed">4361     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4362       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4363       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4364       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4365     } else {</span>
<span class="line-removed">4366       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4367       __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4368       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4369       __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4370     }</span>
4371   %}
4372   ins_pipe( pipe_slow );
4373 %}
4374 
<span class="line-modified">4375 </span>
<span class="line-modified">4376 instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4377   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 2</span>
<span class="line-modified">4378   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-modified">4379   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4380   format %{ &quot;vector_mul4D_reduction  $dst,$dst,$src2&quot; %}</span>
4381   ins_encode %{
<span class="line-modified">4382     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4383     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4384     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4385     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4386     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4387     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4388     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4389     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
4390   %}
4391   ins_pipe( pipe_slow );
4392 %}
4393 
<span class="line-modified">4394 instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">4395   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 2</span>
<span class="line-modified">4396   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-modified">4397   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4398   format %{ &quot;vector_mul8D_reduction $dst,$dst,$src2&quot; %}</span>

4399   ins_encode %{
<span class="line-modified">4400     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4401     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4402     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4403     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4404     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4405     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4406     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4407     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4408     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4409     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4410     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4411     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4412     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4413     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4414     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4415     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
4416   %}
4417   ins_pipe( pipe_slow );
4418 %}
4419 
4420 // ====================VECTOR ARITHMETIC=======================================
4421 
4422 // --------------------------------- ADD --------------------------------------
4423 
4424 // Bytes vector add
4425 instruct vaddB(vec dst, vec src) %{
4426   predicate(UseAVX == 0);
4427   match(Set dst (AddVB dst src));
4428   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
4429   ins_encode %{
4430     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
4431   %}
4432   ins_pipe( pipe_slow );
4433 %}
4434 
4435 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
</pre>
<hr />
<pre>
5767   ins_encode %{
5768     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5769   %}
5770   ins_pipe( pipe_slow );
5771 %}
5772 
5773 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5774   predicate(UseAVX &gt; 0);
5775   match(Set dst (MulAddVS2VI src1 src2));
5776   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5777   ins_encode %{
5778     int vector_len = vector_length_encoding(this);
5779     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5780   %}
5781   ins_pipe( pipe_slow );
5782 %}
5783 
5784 // --------------------------------- Vector Multiply Add Add ----------------------------------
5785 
5786 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5787   predicate(VM_Version::supports_vnni());</span>
5788   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5789   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5790   ins_encode %{
5791     assert(UseAVX &gt; 2, &quot;required&quot;);
5792     int vector_len = vector_length_encoding(this);
5793     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5794   %}
5795   ins_pipe( pipe_slow );
5796   ins_cost(10);
5797 %}
5798 
5799 // --------------------------------- PopCount --------------------------------------
5800 
5801 instruct vpopcountI(vec dst, vec src) %{
5802   match(Set dst (PopCountVI src));
5803   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5804   ins_encode %{
5805     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5806 
5807     int vector_len = vector_length_encoding(this);
5808     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5809   %}
5810   ins_pipe( pipe_slow );
5811 %}
























</pre>
</td>
<td>
<hr />
<pre>
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
<span class="line-added">1168 class Node::PD {</span>
<span class="line-added">1169 public:</span>
<span class="line-added">1170   enum NodeFlags {</span>
<span class="line-added">1171     Flag_intel_jcc_erratum = Node::_last_flag &lt;&lt; 1,</span>
<span class="line-added">1172     _last_flag             = Flag_intel_jcc_erratum</span>
<span class="line-added">1173   };</span>
<span class="line-added">1174 };</span>
<span class="line-added">1175 </span>
1176 %} // end source_hpp
1177 
1178 source %{
1179 
1180 #include &quot;opto/addnode.hpp&quot;
<span class="line-added">1181 #include &quot;c2_intelJccErratum_x86.hpp&quot;</span>
<span class="line-added">1182 </span>
<span class="line-added">1183 void PhaseOutput::pd_perform_mach_node_analysis() {</span>
<span class="line-added">1184   if (VM_Version::has_intel_jcc_erratum()) {</span>
<span class="line-added">1185     int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C-&gt;cfg(), C-&gt;regalloc());</span>
<span class="line-added">1186     _buf_sizes._code += extra_padding;</span>
<span class="line-added">1187   }</span>
<span class="line-added">1188 }</span>
<span class="line-added">1189 </span>
<span class="line-added">1190 int MachNode::pd_alignment_required() const {</span>
<span class="line-added">1191   PhaseOutput* output = Compile::current()-&gt;output();</span>
<span class="line-added">1192   Block* block = output-&gt;block();</span>
<span class="line-added">1193   int index = output-&gt;index();</span>
<span class="line-added">1194   if (VM_Version::has_intel_jcc_erratum() &amp;&amp; IntelJccErratum::is_jcc_erratum_branch(block, this, index)) {</span>
<span class="line-added">1195     // Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.</span>
<span class="line-added">1196     return IntelJccErratum::largest_jcc_size() + 1;</span>
<span class="line-added">1197   } else {</span>
<span class="line-added">1198     return 1;</span>
<span class="line-added">1199   }</span>
<span class="line-added">1200 }</span>
<span class="line-added">1201 </span>
<span class="line-added">1202 int MachNode::compute_padding(int current_offset) const {</span>
<span class="line-added">1203   if (flags() &amp; Node::PD::Flag_intel_jcc_erratum) {</span>
<span class="line-added">1204     Compile* C = Compile::current();</span>
<span class="line-added">1205     PhaseOutput* output = C-&gt;output();</span>
<span class="line-added">1206     Block* block = output-&gt;block();</span>
<span class="line-added">1207     int index = output-&gt;index();</span>
<span class="line-added">1208     return IntelJccErratum::compute_padding(current_offset, this, block, index, C-&gt;regalloc());</span>
<span class="line-added">1209   } else {</span>
<span class="line-added">1210     return 0;</span>
<span class="line-added">1211   }</span>
<span class="line-added">1212 }</span>
1213 
1214 // Emit exception handler code.
1215 // Stuff framesize into a register and call a VM stub routine.
1216 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1217 
1218   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1219   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1220   C2_MacroAssembler _masm(&amp;cbuf);</span>
1221   address base = __ start_a_stub(size_exception_handler());
1222   if (base == NULL) {
1223     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1224     return 0;  // CodeBuffer::expand failed
1225   }
1226   int offset = __ offset();
1227   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1228   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1229   __ end_a_stub();
1230   return offset;
1231 }
1232 
1233 // Emit deopt handler code.
1234 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1235 
1236   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1237   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1238   C2_MacroAssembler _masm(&amp;cbuf);</span>
1239   address base = __ start_a_stub(size_deopt_handler());
1240   if (base == NULL) {
1241     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1242     return 0;  // CodeBuffer::expand failed
1243   }
1244   int offset = __ offset();
1245 
1246 #ifdef _LP64
1247   address the_pc = (address) __ pc();
1248   Label next;
1249   // push a &quot;the_pc&quot; on the stack without destroying any registers
1250   // as they all may be live.
1251 
1252   // push address of &quot;next&quot;
1253   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1254   __ bind(next);
1255   // adjust it so it matches &quot;the_pc&quot;
1256   __ subptr(Address(rsp, 0), __ offset() - offset);
1257 #else
1258   InternalAddress here(__ pc());
</pre>
<hr />
<pre>
1285   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1286 
1287 //=============================================================================
1288 const bool Matcher::match_rule_supported(int opcode) {
1289   if (!has_match_rule(opcode)) {
1290     return false; // no match rule present
1291   }
1292   switch (opcode) {
1293     case Op_AbsVL:
1294       if (UseAVX &lt; 3) {
1295         return false;
1296       }
1297       break;
1298     case Op_PopCountI:
1299     case Op_PopCountL:
1300       if (!UsePopCountInstruction) {
1301         return false;
1302       }
1303       break;
1304     case Op_PopCountVI:
<span class="line-modified">1305       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {</span>
1306         return false;
1307       }
1308       break;
1309     case Op_MulVI:
1310       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1311         return false;
1312       }
1313       break;
1314     case Op_MulVL:
1315     case Op_MulReductionVL:
1316       if (VM_Version::supports_avx512dq() == false) {
1317         return false;
1318       }
1319       break;





1320     case Op_AbsVB:
1321     case Op_AbsVS:
1322     case Op_AbsVI:
1323     case Op_AddReductionVI:
<span class="line-modified">1324     case Op_AndReductionV:</span>
<span class="line-added">1325     case Op_OrReductionV:</span>
<span class="line-added">1326     case Op_XorReductionV:</span>
<span class="line-added">1327       if (UseSSE &lt; 3) { // requires at least SSSE3</span>
1328         return false;
1329       }
1330       break;
1331     case Op_MulReductionVI:
1332       if (UseSSE &lt; 4) { // requires at least SSE4
1333         return false;
1334       }
1335       break;








1336     case Op_SqrtVD:
1337     case Op_SqrtVF:
1338       if (UseAVX &lt; 1) { // enabled for AVX only
1339         return false;
1340       }
1341       break;
1342     case Op_CompareAndSwapL:
1343 #ifdef _LP64
1344     case Op_CompareAndSwapP:
1345 #endif
1346       if (!VM_Version::supports_cx8()) {
1347         return false;
1348       }
1349       break;
1350     case Op_CMoveVF:
1351     case Op_CMoveVD:
1352       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1353         return false;
1354       }
1355       break;
1356     case Op_StrIndexOf:
1357       if (!UseSSE42Intrinsics) {
1358         return false;
1359       }
1360       break;
1361     case Op_StrIndexOfChar:
1362       if (!UseSSE42Intrinsics) {
1363         return false;
1364       }
1365       break;
1366     case Op_OnSpinWait:
1367       if (VM_Version::supports_on_spin_wait() == false) {
1368         return false;
1369       }
1370       break;








1371     case Op_MulVB:
1372     case Op_LShiftVB:
1373     case Op_RShiftVB:
1374     case Op_URShiftVB:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379 #ifdef _LP64
1380     case Op_MaxD:
1381     case Op_MaxF:
1382     case Op_MinD:
1383     case Op_MinF:
1384       if (UseAVX &lt; 1) { // enabled for AVX only
1385         return false;
1386       }
1387       break;
1388 #endif
1389     case Op_CacheWB:
1390     case Op_CacheWBPreSync:
1391     case Op_CacheWBPostSync:
1392       if (!VM_Version::supports_data_cache_line_flush()) {
1393         return false;
1394       }
1395       break;
1396     case Op_RoundDoubleMode:
1397       if (UseSSE &lt; 4) {
1398         return false;
1399       }
1400       break;
1401     case Op_RoundDoubleModeV:
1402       if (VM_Version::supports_avx() == false) {
1403         return false; // 128bit vroundpd is not available
1404       }
1405       break;
<span class="line-added">1406     case Op_MacroLogicV:</span>
<span class="line-added">1407       if (UseAVX &lt; 3 || !UseVectorMacroLogic) {</span>
<span class="line-added">1408         return false;</span>
<span class="line-added">1409       }</span>
<span class="line-added">1410       break;</span>
<span class="line-added">1411 #ifndef _LP64</span>
<span class="line-added">1412     case Op_AddReductionVF:</span>
<span class="line-added">1413     case Op_AddReductionVD:</span>
<span class="line-added">1414     case Op_MulReductionVF:</span>
<span class="line-added">1415     case Op_MulReductionVD:</span>
<span class="line-added">1416       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-added">1417         return false;</span>
<span class="line-added">1418       }</span>
<span class="line-added">1419       break;</span>
<span class="line-added">1420     case Op_MulAddVS2VI:</span>
<span class="line-added">1421     case Op_RShiftVL:</span>
<span class="line-added">1422     case Op_AbsVD:</span>
<span class="line-added">1423     case Op_NegVD:</span>
<span class="line-added">1424       if (UseSSE &lt; 2) {</span>
<span class="line-added">1425         return false;</span>
<span class="line-added">1426       }</span>
<span class="line-added">1427       break;</span>
<span class="line-added">1428 #endif // !LP64</span>
1429   }
1430   return true;  // Match rules are supported by default.
1431 }
1432 
1433 //------------------------------------------------------------------------
1434 
1435 // Identify extra cases that we might want to provide match rules for vector nodes and
1436 // other intrinsics guarded with vector length (vlen) and element type (bt).
1437 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1438   if (!match_rule_supported(opcode)) {
1439     return false;
1440   }
1441   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1442   //   * SSE2 supports 128bit vectors for all types;
1443   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1444   //   * AVX2 supports 256bit vectors for all types;
1445   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1446   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1447   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1448   // And MaxVectorSize is taken into account as well.
1449   if (!vector_size_supported(bt, vlen)) {
1450     return false;
1451   }
1452   // Special cases which require vector length follow:
1453   //   * implementation limitations
1454   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1455   //   * 128bit vroundpd instruction is present only in AVX1
<span class="line-added">1456   int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;</span>
1457   switch (opcode) {
1458     case Op_AbsVF:
1459     case Op_NegVF:
1460       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1461         return false; // 512bit vandps and vxorps are not available
1462       }
1463       break;
1464     case Op_AbsVD:
1465     case Op_NegVD:
1466       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1467         return false; // 512bit vandpd and vxorpd are not available
1468       }
1469       break;
1470     case Op_CMoveVF:
1471       if (vlen != 8) {
1472         return false; // implementation limitation (only vcmov8F_reg is present)
1473       }
1474       break;
<span class="line-added">1475     case Op_MacroLogicV:</span>
<span class="line-added">1476       if (!VM_Version::supports_evex() ||</span>
<span class="line-added">1477           ((size_in_bits != 512) &amp;&amp; !VM_Version::supports_avx512vl())) {</span>
<span class="line-added">1478         return false;</span>
<span class="line-added">1479       }</span>
<span class="line-added">1480       break;</span>
1481     case Op_CMoveVD:
1482       if (vlen != 4) {
1483         return false; // implementation limitation (only vcmov4D_reg is present)
1484       }
1485       break;
1486   }
1487   return true;  // Per default match rules are supported.
1488 }
1489 
1490 // x86 supports generic vector operands: vec and legVec.
1491 const bool Matcher::supports_generic_vector_operands = true;
1492 
1493 MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1494   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1495   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1496   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1497       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1498     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1499     return new legVecZOper();
1500   }
</pre>
<hr />
<pre>
1659     mstack.push(shift-&gt;in(2), Matcher::Visit);
1660     Node *conv = shift-&gt;in(1);
1661 #ifdef _LP64
1662     // Allow Matcher to match the rule which bypass
1663     // ConvI2L operation for an array index on LP64
1664     // if the index value is positive.
1665     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1666         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1667         // Are there other uses besides address expressions?
1668         !matcher-&gt;is_visited(conv)) {
1669       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1670       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1671     } else
1672 #endif
1673       mstack.push(conv, Matcher::Pre_Visit);
1674     return true;
1675   }
1676   return false;
1677 }
1678 
<span class="line-added">1679 // This function identifies sub-graphs in which a &#39;load&#39; node is</span>
<span class="line-added">1680 // input to two different nodes, and such that it can be matched</span>
<span class="line-added">1681 // with BMI instructions like blsi, blsr, etc.</span>
<span class="line-added">1682 // Example : for b = -a[i] &amp; a[i] can be matched to blsi r32, m32.</span>
<span class="line-added">1683 // The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*</span>
<span class="line-added">1684 // refers to the same node.</span>
<span class="line-added">1685 //</span>
<span class="line-added">1686 // Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)</span>
<span class="line-added">1687 // This is a temporary solution until we make DAGs expressible in ADL.</span>
<span class="line-added">1688 template&lt;typename ConType&gt;</span>
<span class="line-added">1689 class FusedPatternMatcher {</span>
<span class="line-added">1690   Node* _op1_node;</span>
<span class="line-added">1691   Node* _mop_node;</span>
<span class="line-added">1692   int _con_op;</span>
<span class="line-added">1693 </span>
<span class="line-added">1694   static int match_next(Node* n, int next_op, int next_op_idx) {</span>
<span class="line-added">1695     if (n-&gt;in(1) == NULL || n-&gt;in(2) == NULL) {</span>
<span class="line-added">1696       return -1;</span>
<span class="line-added">1697     }</span>
<span class="line-added">1698 </span>
<span class="line-added">1699     if (next_op_idx == -1) { // n is commutative, try rotations</span>
<span class="line-added">1700       if (n-&gt;in(1)-&gt;Opcode() == next_op) {</span>
<span class="line-added">1701         return 1;</span>
<span class="line-added">1702       } else if (n-&gt;in(2)-&gt;Opcode() == next_op) {</span>
<span class="line-added">1703         return 2;</span>
<span class="line-added">1704       }</span>
<span class="line-added">1705     } else {</span>
<span class="line-added">1706       assert(next_op_idx &gt; 0 &amp;&amp; next_op_idx &lt;= 2, &quot;Bad argument index&quot;);</span>
<span class="line-added">1707       if (n-&gt;in(next_op_idx)-&gt;Opcode() == next_op) {</span>
<span class="line-added">1708         return next_op_idx;</span>
<span class="line-added">1709       }</span>
<span class="line-added">1710     }</span>
<span class="line-added">1711     return -1;</span>
<span class="line-added">1712   }</span>
<span class="line-added">1713 </span>
<span class="line-added">1714  public:</span>
<span class="line-added">1715   FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :</span>
<span class="line-added">1716     _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }</span>
<span class="line-added">1717 </span>
<span class="line-added">1718   bool match(int op1, int op1_op2_idx,  // op1 and the index of the op1-&gt;op2 edge, -1 if op1 is commutative</span>
<span class="line-added">1719              int op2, int op2_con_idx,  // op2 and the index of the op2-&gt;con edge, -1 if op2 is commutative</span>
<span class="line-added">1720              typename ConType::NativeType con_value) {</span>
<span class="line-added">1721     if (_op1_node-&gt;Opcode() != op1) {</span>
<span class="line-added">1722       return false;</span>
<span class="line-added">1723     }</span>
<span class="line-added">1724     if (_mop_node-&gt;outcnt() &gt; 2) {</span>
<span class="line-added">1725       return false;</span>
<span class="line-added">1726     }</span>
<span class="line-added">1727     op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);</span>
<span class="line-added">1728     if (op1_op2_idx == -1) {</span>
<span class="line-added">1729       return false;</span>
<span class="line-added">1730     }</span>
<span class="line-added">1731     // Memory operation must be the other edge</span>
<span class="line-added">1732     int op1_mop_idx = (op1_op2_idx &amp; 1) + 1;</span>
<span class="line-added">1733 </span>
<span class="line-added">1734     // Check that the mop node is really what we want</span>
<span class="line-added">1735     if (_op1_node-&gt;in(op1_mop_idx) == _mop_node) {</span>
<span class="line-added">1736       Node* op2_node = _op1_node-&gt;in(op1_op2_idx);</span>
<span class="line-added">1737       if (op2_node-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">1738         return false;</span>
<span class="line-added">1739       }</span>
<span class="line-added">1740       assert(op2_node-&gt;Opcode() == op2, &quot;Should be&quot;);</span>
<span class="line-added">1741       op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);</span>
<span class="line-added">1742       if (op2_con_idx == -1) {</span>
<span class="line-added">1743         return false;</span>
<span class="line-added">1744       }</span>
<span class="line-added">1745       // Memory operation must be the other edge</span>
<span class="line-added">1746       int op2_mop_idx = (op2_con_idx &amp; 1) + 1;</span>
<span class="line-added">1747       // Check that the memory operation is the same node</span>
<span class="line-added">1748       if (op2_node-&gt;in(op2_mop_idx) == _mop_node) {</span>
<span class="line-added">1749         // Now check the constant</span>
<span class="line-added">1750         const Type* con_type = op2_node-&gt;in(op2_con_idx)-&gt;bottom_type();</span>
<span class="line-added">1751         if (con_type != Type::TOP &amp;&amp; ConType::as_self(con_type)-&gt;get_con() == con_value) {</span>
<span class="line-added">1752           return true;</span>
<span class="line-added">1753         }</span>
<span class="line-added">1754       }</span>
<span class="line-added">1755     }</span>
<span class="line-added">1756     return false;</span>
<span class="line-added">1757   }</span>
<span class="line-added">1758 };</span>
<span class="line-added">1759 </span>
<span class="line-added">1760 static bool is_bmi_pattern(Node* n, Node* m) {</span>
<span class="line-added">1761   assert(UseBMI1Instructions, &quot;sanity&quot;);</span>
<span class="line-added">1762   if (n != NULL &amp;&amp; m != NULL) {</span>
<span class="line-added">1763     if (m-&gt;Opcode() == Op_LoadI) {</span>
<span class="line-added">1764       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);</span>
<span class="line-added">1765       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||</span>
<span class="line-added">1766              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||</span>
<span class="line-added">1767              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);</span>
<span class="line-added">1768     } else if (m-&gt;Opcode() == Op_LoadL) {</span>
<span class="line-added">1769       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);</span>
<span class="line-added">1770       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||</span>
<span class="line-added">1771              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||</span>
<span class="line-added">1772              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);</span>
<span class="line-added">1773     }</span>
<span class="line-added">1774   }</span>
<span class="line-added">1775   return false;</span>
<span class="line-added">1776 }</span>
<span class="line-added">1777 </span>
<span class="line-added">1778 // Should the matcher clone input &#39;m&#39; of node &#39;n&#39;?</span>
<span class="line-added">1779 bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack&amp; mstack) {</span>
<span class="line-added">1780   // If &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone the input &#39;m&#39;.</span>
<span class="line-added">1781   if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {</span>
<span class="line-added">1782     mstack.push(m, Visit);</span>
<span class="line-added">1783     return true;</span>
<span class="line-added">1784   }</span>
<span class="line-added">1785   return false;</span>
<span class="line-added">1786 }</span>
<span class="line-added">1787 </span>
1788 // Should the Matcher clone shifts on addressing modes, expecting them
1789 // to be subsumed into complex addressing expressions or compute them
1790 // into registers?
<span class="line-modified">1791 bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {</span>
1792   Node *off = m-&gt;in(AddPNode::Offset);
1793   if (off-&gt;is_Con()) {
1794     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1795     Node *adr = m-&gt;in(AddPNode::Address);
1796 
1797     // Intel can handle 2 adds in addressing mode
1798     // AtomicAdd is not an addressing expression.
1799     // Cheap to find it by looking for screwy base.
1800     if (adr-&gt;is_AddP() &amp;&amp;
1801         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1802         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1803         // Are there other uses besides address expressions?
1804         !is_visited(adr)) {
1805       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1806       Node *shift = adr-&gt;in(AddPNode::Offset);
1807       if (!clone_shift(shift, this, mstack, address_visited)) {
1808         mstack.push(shift, Pre_Visit);
1809       }
1810       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1811       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
</pre>
<hr />
<pre>
1817     mstack.push(off, Visit);
1818     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1819     return true;
1820   } else if (clone_shift(off, this, mstack, address_visited)) {
1821     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1822     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1823     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1824     return true;
1825   }
1826   return false;
1827 }
1828 
1829 void Compile::reshape_address(AddPNode* addp) {
1830 }
1831 
1832 static inline uint vector_length(const MachNode* n) {
1833   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1834   return vt-&gt;length();
1835 }
1836 
<span class="line-added">1837 static inline uint vector_length(const MachNode* use, MachOper* opnd) {</span>
<span class="line-added">1838   uint def_idx = use-&gt;operand_index(opnd);</span>
<span class="line-added">1839   Node* def = use-&gt;in(def_idx);</span>
<span class="line-added">1840   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length();</span>
<span class="line-added">1841 }</span>
<span class="line-added">1842 </span>
1843 static inline uint vector_length_in_bytes(const MachNode* n) {
1844   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1845   return vt-&gt;length_in_bytes();
1846 }
1847 
1848 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1849   uint def_idx = use-&gt;operand_index(opnd);
1850   Node* def = use-&gt;in(def_idx);
1851   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1852 }
1853 
1854 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1855   switch(vector_length_in_bytes(n)) {
1856     case  4: // fall-through
1857     case  8: // fall-through
1858     case 16: return Assembler::AVX_128bit;
1859     case 32: return Assembler::AVX_256bit;
1860     case 64: return Assembler::AVX_512bit;
1861 
1862     default: {
1863       ShouldNotReachHere();
1864       return Assembler::AVX_NoVec;
1865     }
1866   }
1867 }
1868 
1869 // Helper methods for MachSpillCopyNode::implementation().
1870 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1871                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1872   // In 64-bit VM size calculation is very complex. Emitting instructions
1873   // into scratch buffer is used to get size in 64-bit VM.
1874   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1875   assert(ireg == Op_VecS || // 32bit vector
1876          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1877          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1878          &quot;no non-adjacent vector moves&quot; );
1879   if (cbuf) {
<span class="line-modified">1880     C2_MacroAssembler _masm(cbuf);</span>
1881     int offset = __ offset();
1882     switch (ireg) {
1883     case Op_VecS: // copy whole register
1884     case Op_VecD:
1885     case Op_VecX:
1886 #ifndef _LP64
1887       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1888 #else
1889       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1890         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1891       } else {
1892         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1893      }
1894 #endif
1895       break;
1896     case Op_VecY:
1897 #ifndef _LP64
1898       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1899 #else
1900       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
</pre>
<hr />
<pre>
1926       break;
1927     case Op_VecY:
1928     case Op_VecZ:
1929       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1930       break;
1931     default:
1932       ShouldNotReachHere();
1933     }
1934 #endif
1935   }
1936   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1937   return (UseAVX &gt; 2) ? 6 : 4;
1938 }
1939 
1940 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1941                      int stack_offset, int reg, uint ireg, outputStream* st) {
1942   // In 64-bit VM size calculation is very complex. Emitting instructions
1943   // into scratch buffer is used to get size in 64-bit VM.
1944   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1945   if (cbuf) {
<span class="line-modified">1946     C2_MacroAssembler _masm(cbuf);</span>
1947     int offset = __ offset();
1948     if (is_load) {
1949       switch (ireg) {
1950       case Op_VecS:
1951         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1952         break;
1953       case Op_VecD:
1954         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1955         break;
1956       case Op_VecX:
1957 #ifndef _LP64
1958         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1959 #else
1960         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1961           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1962         } else {
1963           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1964           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1965         }
1966 #endif
</pre>
<hr />
<pre>
2129 static inline jlong replicate8_imm(int con, int width) {
2130   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
2131   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
2132   int bit_width = width * 8;
2133   jlong val = con;
2134   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
2135   while(bit_width &lt; 64) {
2136     val |= (val &lt;&lt; bit_width);
2137     bit_width &lt;&lt;= 1;
2138   }
2139   return val;
2140 }
2141 
2142 #ifndef PRODUCT
2143   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
2144     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
2145   }
2146 #endif
2147 
2148   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
<span class="line-modified">2149     C2_MacroAssembler _masm(&amp;cbuf);</span>
2150     __ nop(_count);
2151   }
2152 
2153   uint MachNopNode::size(PhaseRegAlloc*) const {
2154     return _count;
2155   }
2156 
2157 #ifndef PRODUCT
2158   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
2159     st-&gt;print(&quot;# breakpoint&quot;);
2160   }
2161 #endif
2162 
2163   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
<span class="line-modified">2164     C2_MacroAssembler _masm(&amp;cbuf);</span>
2165     __ int3();
2166   }
2167 
2168   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2169     return MachNode::size(ra_);
2170   }
2171 
2172 %}
2173 
2174 encode %{
2175 
2176   enc_class call_epilog %{
2177     if (VerifyStackAtCalls) {
2178       // Check that stack depth is unchanged: find majik cookie on stack
2179       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
<span class="line-modified">2180       C2_MacroAssembler _masm(&amp;cbuf);</span>
2181       Label L;
2182       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2183       __ jccb(Assembler::equal, L);
2184       // Die if stack mismatch
2185       __ int3();
2186       __ bind(L);
2187     }
2188   %}
2189 
2190 %}
2191 
2192 
2193 //----------OPERANDS-----------------------------------------------------------
2194 // Operand definitions must precede instruction definitions for correct parsing
2195 // in the ADLC because operands constitute user defined types which are used in
2196 // instruction definitions.
2197 
2198 // Vectors
2199 
2200 // Dummy generic vector class. Should be used for all vector operands.
</pre>
<hr />
<pre>
3272   match(Set mem (StoreVector mem src));
3273   ins_cost(145);
3274   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3275   ins_encode %{
3276     switch (vector_length_in_bytes(this, $src)) {
3277       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3278       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3279       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3280       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3281       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3282       default: ShouldNotReachHere();
3283     }
3284   %}
3285   ins_pipe( pipe_slow );
3286 %}
3287 
3288 // ====================REPLICATE=======================================
3289 
3290 // Replicate byte scalar to be vector
3291 instruct ReplB_reg(vec dst, rRegI src) %{


3292   match(Set dst (ReplicateB src));
3293   format %{ &quot;replicateB $dst,$src&quot; %}
3294   ins_encode %{
3295     uint vlen = vector_length(this);
3296     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3297       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW</span>
3298       int vlen_enc = vector_length_encoding(this);
3299       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3300     } else {
3301       __ movdl($dst$$XMMRegister, $src$$Register);
3302       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3303       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3304       if (vlen &gt;= 16) {
3305         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3306         if (vlen &gt;= 32) {
<span class="line-modified">3307           assert(vlen == 32, &quot;sanity&quot;);</span>
3308           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3309         }
3310       }
3311     }
3312   %}
3313   ins_pipe( pipe_slow );
3314 %}
3315 
















3316 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3317   predicate(VM_Version::supports_avx2());</span>

3318   match(Set dst (ReplicateB (LoadB mem)));
3319   format %{ &quot;replicateB $dst,$mem&quot; %}
3320   ins_encode %{

3321     int vector_len = vector_length_encoding(this);
3322     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3323   %}
3324   ins_pipe( pipe_slow );
3325 %}
3326 
3327 instruct ReplB_imm(vec dst, immI con) %{


3328   match(Set dst (ReplicateB con));
3329   format %{ &quot;replicateB $dst,$con&quot; %}
3330   ins_encode %{
3331     uint vlen = vector_length(this);
3332     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3333     if (vlen == 4) {
3334       __ movdl($dst$$XMMRegister, const_addr);
3335     } else {
3336       __ movq($dst$$XMMRegister, const_addr);
3337       if (vlen &gt;= 16) {
<span class="line-modified">3338         if (VM_Version::supports_avx2()) {</span>
3339           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3340           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3341         } else {
<span class="line-added">3342           assert(vlen == 16, &quot;sanity&quot;);</span>
3343           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);




3344         }
3345       }
3346     }
3347   %}
3348   ins_pipe( pipe_slow );
3349 %}
3350 













3351 // Replicate byte scalar zero to be vector
3352 instruct ReplB_zero(vec dst, immI0 zero) %{
3353   match(Set dst (ReplicateB zero));
3354   format %{ &quot;replicateB $dst,$zero&quot; %}
3355   ins_encode %{
3356     uint vlen = vector_length(this);
3357     if (vlen &lt;= 16) {
3358       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3359     } else {
3360       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3361       int vlen_enc = vector_length_encoding(this);
3362       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3363     }
3364   %}
3365   ins_pipe( fpu_reg_reg );
3366 %}
3367 
3368 // ====================ReplicateS=======================================
3369 
3370 instruct ReplS_reg(vec dst, rRegI src) %{


3371   match(Set dst (ReplicateS src));
3372   format %{ &quot;replicateS $dst,$src&quot; %}
3373   ins_encode %{
3374     uint vlen = vector_length(this);
3375     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3376       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW</span>
3377       int vlen_enc = vector_length_encoding(this);
3378       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3379     } else {
3380       __ movdl($dst$$XMMRegister, $src$$Register);
3381       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3382       if (vlen &gt;= 8) {
3383         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3384         if (vlen &gt;= 16) {
<span class="line-modified">3385           assert(vlen == 16, &quot;sanity&quot;);</span>
3386           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3387         }
3388       }
3389     }
3390   %}
3391   ins_pipe( pipe_slow );
3392 %}
3393 














3394 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3395   predicate(VM_Version::supports_avx2());</span>


























3396   match(Set dst (ReplicateS (LoadS mem)));
3397   format %{ &quot;replicateS $dst,$mem&quot; %}
3398   ins_encode %{
<span class="line-modified">3399     int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3400     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>


3401   %}
3402   ins_pipe( pipe_slow );
3403 %}
3404 
3405 instruct ReplS_imm(vec dst, immI con) %{


3406   match(Set dst (ReplicateS con));
3407   format %{ &quot;replicateS $dst,$con&quot; %}
3408   ins_encode %{
3409     uint vlen = vector_length(this);
<span class="line-modified">3410     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3411     if (vlen == 2) {
<span class="line-modified">3412       __ movdl($dst$$XMMRegister, const_addr);</span>
3413     } else {
<span class="line-modified">3414       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3415       if (vlen &gt;= 8) {</span>
<span class="line-modified">3416         if (VM_Version::supports_avx2()) {</span>
<span class="line-modified">3417           int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3418           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3419         } else {</span>
<span class="line-modified">3420           assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-modified">3421           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>



3422         }
3423       }
3424     }
3425   %}
3426   ins_pipe( fpu_reg_reg );
3427 %}
3428 













3429 instruct ReplS_zero(vec dst, immI0 zero) %{
3430   match(Set dst (ReplicateS zero));
3431   format %{ &quot;replicateS $dst,$zero&quot; %}
3432   ins_encode %{
3433     uint vlen = vector_length(this);
3434     if (vlen &lt;= 8) {
3435       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3436     } else {
3437       int vlen_enc = vector_length_encoding(this);
3438       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3439     }
3440   %}
3441   ins_pipe( fpu_reg_reg );
3442 %}
3443 
3444 // ====================ReplicateI=======================================
3445 
3446 instruct ReplI_reg(vec dst, rRegI src) %{
3447   match(Set dst (ReplicateI src));
3448   format %{ &quot;replicateI $dst,$src&quot; %}
3449   ins_encode %{
3450     uint vlen = vector_length(this);
3451     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3452       int vlen_enc = vector_length_encoding(this);
3453       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3454     } else {
3455       __ movdl($dst$$XMMRegister, $src$$Register);
3456       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3457       if (vlen &gt;= 8) {
3458         assert(vlen == 8, &quot;sanity&quot;);
3459         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3460       }
3461     }
3462   %}
3463   ins_pipe( pipe_slow );
3464 %}
3465 
3466 instruct ReplI_mem(vec dst, memory mem) %{

3467   match(Set dst (ReplicateI (LoadI mem)));
3468   format %{ &quot;replicateI $dst,$mem&quot; %}
3469   ins_encode %{
3470     uint vlen = vector_length(this);
3471     if (vlen &lt;= 4) {
<span class="line-modified">3472       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3473       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3474     } else {</span>
<span class="line-added">3475       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3476       int vector_len = vector_length_encoding(this);
3477       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);




3478     }
3479   %}
3480   ins_pipe( pipe_slow );
3481 %}
3482 
3483 instruct ReplI_imm(vec dst, immI con) %{
3484   match(Set dst (ReplicateI con));
3485   format %{ &quot;replicateI $dst,$con&quot; %}
3486   ins_encode %{
3487     uint vlen = vector_length(this);
<span class="line-modified">3488     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3489     if (vlen &lt;= 4) {</span>
<span class="line-modified">3490       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3491       if (vlen == 4) {</span>
<span class="line-added">3492         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-added">3493       }</span>
<span class="line-added">3494     } else {</span>
<span class="line-added">3495       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3496       int vector_len = vector_length_encoding(this);
<span class="line-modified">3497       __ movq($dst$$XMMRegister, const_addr);</span>
3498       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);







3499     }
3500   %}
3501   ins_pipe( pipe_slow );
3502 %}
3503 
3504 // Replicate integer (4 byte) scalar zero to be vector
3505 instruct ReplI_zero(vec dst, immI0 zero) %{
3506   match(Set dst (ReplicateI zero));
3507   format %{ &quot;replicateI $dst,$zero&quot; %}
3508   ins_encode %{
3509     uint vlen = vector_length(this);
3510     if (vlen &lt;= 4) {
3511       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3512     } else {
3513       int vlen_enc = vector_length_encoding(this);
3514       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3515     }
3516   %}
3517   ins_pipe( fpu_reg_reg );
3518 %}
3519 
<span class="line-added">3520 instruct ReplI_M1(vec dst, immI_M1 con) %{</span>
<span class="line-added">3521   predicate(UseAVX &gt; 0);</span>
<span class="line-added">3522   match(Set dst (ReplicateB con));</span>
<span class="line-added">3523   match(Set dst (ReplicateS con));</span>
<span class="line-added">3524   match(Set dst (ReplicateI con));</span>
<span class="line-added">3525   effect(TEMP dst);</span>
<span class="line-added">3526   format %{ &quot;vallones $dst&quot; %}</span>
<span class="line-added">3527   ins_encode %{</span>
<span class="line-added">3528     int vector_len = vector_length_encoding(this);</span>
<span class="line-added">3529     __ vallones($dst$$XMMRegister, vector_len);</span>
<span class="line-added">3530   %}</span>
<span class="line-added">3531   ins_pipe( pipe_slow );</span>
<span class="line-added">3532 %}</span>
<span class="line-added">3533 </span>
3534 // ====================ReplicateL=======================================
3535 
3536 #ifdef _LP64
3537 // Replicate long (8 byte) scalar to be vector
3538 instruct ReplL_reg(vec dst, rRegL src) %{
3539   match(Set dst (ReplicateL src));
3540   format %{ &quot;replicateL $dst,$src&quot; %}
3541   ins_encode %{
3542     uint vlen = vector_length(this);
3543     if (vlen == 2) {
3544       __ movdq($dst$$XMMRegister, $src$$Register);
3545       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3546     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3547       int vlen_enc = vector_length_encoding(this);
3548       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3549     } else {
3550       assert(vlen == 4, &quot;sanity&quot;);
3551       __ movdq($dst$$XMMRegister, $src$$Register);
3552       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3553       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
</pre>
<hr />
<pre>
3602     } else {
3603       int vector_len = Assembler::AVX_512bit;
3604       __ movdl($dst$$XMMRegister, $src$$Register);
3605       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3606       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3607       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3608     }
3609   %}
3610   ins_pipe( pipe_slow );
3611 %}
3612 #endif // _LP64
3613 
3614 instruct ReplL_mem(vec dst, memory mem) %{
3615   match(Set dst (ReplicateL (LoadL mem)));
3616   format %{ &quot;replicateL $dst,$mem&quot; %}
3617   ins_encode %{
3618     uint vlen = vector_length(this);
3619     if (vlen == 2) {
3620       __ movq($dst$$XMMRegister, $mem$$Address);
3621       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3622     } else {</span>
<span class="line-added">3623       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3624       int vlen_enc = vector_length_encoding(this);
3625       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);





3626     }
3627   %}
3628   ins_pipe( pipe_slow );
3629 %}
3630 
3631 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3632 instruct ReplL_imm(vec dst, immL con) %{
3633   match(Set dst (ReplicateL con));
3634   format %{ &quot;replicateL $dst,$con&quot; %}
3635   ins_encode %{
3636     uint vlen = vector_length(this);
3637     InternalAddress const_addr = $constantaddress($con);
3638     if (vlen == 2) {
3639       __ movq($dst$$XMMRegister, const_addr);
3640       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3641     } else {</span>
<span class="line-added">3642       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3643       int vlen_enc = vector_length_encoding(this);
3644       __ movq($dst$$XMMRegister, const_addr);
3645       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);





3646     }
3647   %}
3648   ins_pipe( pipe_slow );
3649 %}
3650 
3651 instruct ReplL_zero(vec dst, immL0 zero) %{
3652   match(Set dst (ReplicateL zero));
3653   format %{ &quot;replicateL $dst,$zero&quot; %}
3654   ins_encode %{
3655     int vlen = vector_length(this);
3656     if (vlen == 2) {
3657       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3658     } else {
3659       int vlen_enc = vector_length_encoding(this);
3660       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3661     }
3662   %}
3663   ins_pipe( fpu_reg_reg );
3664 %}
3665 
<span class="line-added">3666 instruct ReplL_M1(vec dst, immL_M1 con) %{</span>
<span class="line-added">3667   predicate(UseAVX &gt; 0);</span>
<span class="line-added">3668   match(Set dst (ReplicateL con));</span>
<span class="line-added">3669   effect(TEMP dst);</span>
<span class="line-added">3670   format %{ &quot;vallones $dst&quot; %}</span>
<span class="line-added">3671   ins_encode %{</span>
<span class="line-added">3672     int vector_len = vector_length_encoding(this);</span>
<span class="line-added">3673     __ vallones($dst$$XMMRegister, vector_len);</span>
<span class="line-added">3674   %}</span>
<span class="line-added">3675   ins_pipe( pipe_slow );</span>
<span class="line-added">3676 %}</span>
<span class="line-added">3677 </span>
3678 // ====================ReplicateF=======================================
3679 
3680 instruct ReplF_reg(vec dst, vlRegF src) %{
3681   match(Set dst (ReplicateF src));
3682   format %{ &quot;replicateF $dst,$src&quot; %}
3683   ins_encode %{
3684     uint vlen = vector_length(this);
3685     if (vlen &lt;= 4) {
3686       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3687    } else if (VM_Version::supports_avx2()) {</span>
3688       int vector_len = vector_length_encoding(this);
<span class="line-modified">3689       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3690     } else {
3691       assert(vlen == 8, &quot;sanity&quot;);
3692       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3693       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3694     }
3695   %}
3696   ins_pipe( pipe_slow );
3697 %}
3698 
3699 instruct ReplF_mem(vec dst, memory mem) %{

3700   match(Set dst (ReplicateF (LoadF mem)));
3701   format %{ &quot;replicateF $dst,$mem&quot; %}
3702   ins_encode %{
3703     uint vlen = vector_length(this);
3704     if (vlen &lt;= 4) {
<span class="line-modified">3705       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3706       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3707     } else {</span>
<span class="line-added">3708       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
3709       int vector_len = vector_length_encoding(this);
3710       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);




3711     }
3712   %}
3713   ins_pipe( pipe_slow );
3714 %}
3715 
3716 instruct ReplF_zero(vec dst, immF0 zero) %{
3717   match(Set dst (ReplicateF zero));
3718   format %{ &quot;replicateF $dst,$zero&quot; %}
3719   ins_encode %{
3720     uint vlen = vector_length(this);
3721     if (vlen &lt;= 4) {
3722       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3723     } else {
3724       int vlen_enc = vector_length_encoding(this);
3725       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3726     }
3727   %}
3728   ins_pipe( fpu_reg_reg );
3729 %}
3730 
3731 // ====================ReplicateD=======================================
3732 
3733 // Replicate double (8 bytes) scalar to be vector
3734 instruct ReplD_reg(vec dst, vlRegD src) %{
3735   match(Set dst (ReplicateD src));
3736   format %{ &quot;replicateD $dst,$src&quot; %}
3737   ins_encode %{
3738     uint vlen = vector_length(this);
3739     if (vlen == 2) {
3740       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3741     } else if (VM_Version::supports_avx2()) {</span>
3742       int vector_len = vector_length_encoding(this);
<span class="line-modified">3743       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3744     } else {
3745       assert(vlen == 4, &quot;sanity&quot;);
3746       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3747       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3748     }
3749   %}
3750   ins_pipe( pipe_slow );
3751 %}
3752 
3753 instruct ReplD_mem(vec dst, memory mem) %{

3754   match(Set dst (ReplicateD (LoadD mem)));
3755   format %{ &quot;replicateD $dst,$mem&quot; %}
3756   ins_encode %{
3757     uint vlen = vector_length(this);
3758     if (vlen == 2) {
<span class="line-modified">3759       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3760       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);</span>
<span class="line-added">3761     } else {</span>
<span class="line-added">3762       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
3763       int vector_len = vector_length_encoding(this);
3764       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);




3765     }
3766   %}
3767   ins_pipe( pipe_slow );
3768 %}
3769 
3770 instruct ReplD_zero(vec dst, immD0 zero) %{
3771   match(Set dst (ReplicateD zero));
3772   format %{ &quot;replicateD $dst,$zero&quot; %}
3773   ins_encode %{
3774     uint vlen = vector_length(this);
3775     if (vlen == 2) {
3776       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3777     } else {
3778       int vlen_enc = vector_length_encoding(this);
3779       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3780     }
3781   %}
3782   ins_pipe( fpu_reg_reg );
3783 %}
3784 
3785 // ====================REDUCTION ARITHMETIC=======================================
<span class="line-added">3786 // =======================Int Reduction==========================================</span>
3787 
<span class="line-modified">3788 instruct reductionI(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3789   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;</span>
<span class="line-modified">3790             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 16);</span>
































3791   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3792   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-modified">3793   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-modified">3794   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-modified">3795   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-modified">3796   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-modified">3797   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
<span class="line-modified">3798   ins_encode %{</span>
<span class="line-modified">3799     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3800     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3801     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>


















































3802   %}
3803   ins_pipe( pipe_slow );
3804 %}
3805 
<span class="line-modified">3806 instruct reduction16I(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3807   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;</span>
<span class="line-added">3808             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);</span>
3809   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3810   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-modified">3811   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-added">3812   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-added">3813   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-added">3814   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3815   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3816   ins_encode %{
<span class="line-modified">3817     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3818     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3819     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>








3820   %}
3821   ins_pipe( pipe_slow );
3822 %}
3823 
<span class="line-modified">3824 // =======================Long Reduction==========================================</span>
3825 
3826 #ifdef _LP64
<span class="line-modified">3827 instruct reductionL(rRegL dst, rRegL src1, vec src2, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3828   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;</span>
<span class="line-modified">3829             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 8);</span>















3830   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3831   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">3832   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-added">3833   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-added">3834   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-added">3835   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3836   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3837   ins_encode %{
<span class="line-modified">3838     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3839     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3840     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>





3841   %}
3842   ins_pipe( pipe_slow );
3843 %}
3844 
<span class="line-modified">3845 instruct reduction8L(rRegL dst, rRegL src1, legVec src2, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3846   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;</span>
<span class="line-added">3847             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);</span>
3848   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3849   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">3850   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-added">3851   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-added">3852   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-added">3853   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3854   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3855   ins_encode %{
<span class="line-modified">3856     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3857     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3858     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>







3859   %}
3860   ins_pipe( pipe_slow );
3861 %}
3862 #endif // _LP64
3863 
<span class="line-modified">3864 // =======================Float Reduction==========================================</span>











































































































































3865 
<span class="line-modified">3866 instruct reductionF128(regF dst, vec src, vec vtmp) %{</span>
<span class="line-modified">3867   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt;= 4);</span>
<span class="line-modified">3868   match(Set dst (AddReductionVF dst src));</span>
<span class="line-modified">3869   match(Set dst (MulReductionVF dst src));</span>
<span class="line-modified">3870   effect(TEMP dst, TEMP vtmp);</span>
<span class="line-added">3871   format %{ &quot;vector_reduction_fp  $dst,$src ; using $vtmp as TEMP&quot; %}</span>
3872   ins_encode %{
<span class="line-modified">3873     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3874     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3875     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);</span>







































































































































































3876   %}
3877   ins_pipe( pipe_slow );
3878 %}
3879 
<span class="line-modified">3880 instruct reduction8F(regF dst, vec src, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3881   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);</span>
<span class="line-modified">3882   match(Set dst (AddReductionVF dst src));</span>
<span class="line-modified">3883   match(Set dst (MulReductionVF dst src));</span>
<span class="line-modified">3884   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3885   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3886   ins_encode %{
<span class="line-modified">3887     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3888     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3889     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>







3890   %}
3891   ins_pipe( pipe_slow );
3892 %}



3893 
<span class="line-modified">3894 instruct reduction16F(regF dst, legVec src, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3895   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);</span>
<span class="line-modified">3896   match(Set dst (AddReductionVF dst src));</span>
<span class="line-modified">3897   match(Set dst (MulReductionVF dst src));</span>
<span class="line-modified">3898   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3899   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3900   ins_encode %{
<span class="line-modified">3901     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3902     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3903     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>







3904   %}
3905   ins_pipe( pipe_slow );
3906 %}
3907 
<span class="line-modified">3908 // =======================Double Reduction==========================================</span>


























3909 
<span class="line-modified">3910 instruct reduction2D(regD dst, vec src, vec vtmp) %{</span>
<span class="line-modified">3911   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2);</span>
<span class="line-modified">3912   match(Set dst (AddReductionVD dst src));</span>
<span class="line-modified">3913   match(Set dst (MulReductionVD dst src));</span>
<span class="line-modified">3914   effect(TEMP dst, TEMP vtmp);</span>
<span class="line-modified">3915   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp as TEMP&quot; %}</span>

























3916   ins_encode %{
<span class="line-modified">3917     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3918     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3919     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);</span>



















































3920   %}
3921   ins_pipe( pipe_slow );
3922 %}
3923 
<span class="line-modified">3924 instruct reduction4D(regD dst, vec src, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3925   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4);</span>
<span class="line-modified">3926   match(Set dst (AddReductionVD dst src));</span>
<span class="line-modified">3927   match(Set dst (MulReductionVD dst src));</span>
<span class="line-modified">3928   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-modified">3929   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3930   ins_encode %{
<span class="line-modified">3931     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3932     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3933     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>





3934   %}
3935   ins_pipe( pipe_slow );
3936 %}
3937 
<span class="line-modified">3938 instruct reduction8D(regD dst, legVec src, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3939   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);</span>
<span class="line-modified">3940   match(Set dst (AddReductionVD dst src));</span>
<span class="line-modified">3941   match(Set dst (MulReductionVD dst src));</span>
<span class="line-modified">3942   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3943   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3944   ins_encode %{
<span class="line-modified">3945     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3946     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3947     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>













3948   %}
3949   ins_pipe( pipe_slow );
3950 %}
3951 
3952 // ====================VECTOR ARITHMETIC=======================================
3953 
3954 // --------------------------------- ADD --------------------------------------
3955 
3956 // Bytes vector add
3957 instruct vaddB(vec dst, vec src) %{
3958   predicate(UseAVX == 0);
3959   match(Set dst (AddVB dst src));
3960   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
3961   ins_encode %{
3962     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
3963   %}
3964   ins_pipe( pipe_slow );
3965 %}
3966 
3967 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
</pre>
<hr />
<pre>
5299   ins_encode %{
5300     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5301   %}
5302   ins_pipe( pipe_slow );
5303 %}
5304 
5305 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5306   predicate(UseAVX &gt; 0);
5307   match(Set dst (MulAddVS2VI src1 src2));
5308   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5309   ins_encode %{
5310     int vector_len = vector_length_encoding(this);
5311     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5312   %}
5313   ins_pipe( pipe_slow );
5314 %}
5315 
5316 // --------------------------------- Vector Multiply Add Add ----------------------------------
5317 
5318 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5319   predicate(VM_Version::supports_avx512_vnni());</span>
5320   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5321   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5322   ins_encode %{
5323     assert(UseAVX &gt; 2, &quot;required&quot;);
5324     int vector_len = vector_length_encoding(this);
5325     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5326   %}
5327   ins_pipe( pipe_slow );
5328   ins_cost(10);
5329 %}
5330 
5331 // --------------------------------- PopCount --------------------------------------
5332 
5333 instruct vpopcountI(vec dst, vec src) %{
5334   match(Set dst (PopCountVI src));
5335   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5336   ins_encode %{
5337     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5338 
5339     int vector_len = vector_length_encoding(this);
5340     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5341   %}
5342   ins_pipe( pipe_slow );
5343 %}
<span class="line-added">5344 </span>
<span class="line-added">5345 // --------------------------------- Bitwise Ternary Logic ----------------------------------</span>
<span class="line-added">5346 </span>
<span class="line-added">5347 instruct vpternlogdB(vec dst, vec src2, vec src3, immU8 func) %{</span>
<span class="line-added">5348   match(Set dst (MacroLogicV (Binary dst src2) (Binary src3 func)));</span>
<span class="line-added">5349   effect(TEMP dst);</span>
<span class="line-added">5350   format %{ &quot;vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic&quot; %}</span>
<span class="line-added">5351   ins_encode %{</span>
<span class="line-added">5352     int vector_len = vector_length_encoding(this);</span>
<span class="line-added">5353     __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$XMMRegister, vector_len);</span>
<span class="line-added">5354   %}</span>
<span class="line-added">5355   ins_pipe( pipe_slow );</span>
<span class="line-added">5356 %}</span>
<span class="line-added">5357 </span>
<span class="line-added">5358 instruct vpternlogdB_mem(vec dst, vec src2, memory src3, immU8 func) %{</span>
<span class="line-added">5359   match(Set dst (MacroLogicV (Binary dst src2) (Binary (LoadVector src3) func)));</span>
<span class="line-added">5360   effect(TEMP dst);</span>
<span class="line-added">5361   format %{ &quot;vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic&quot; %}</span>
<span class="line-added">5362   ins_encode %{</span>
<span class="line-added">5363     int vector_len = vector_length_encoding(this);</span>
<span class="line-added">5364     __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$Address, vector_len);</span>
<span class="line-added">5365   %}</span>
<span class="line-added">5366   ins_pipe( pipe_slow );</span>
<span class="line-added">5367 %}</span>
</pre>
</td>
</tr>
</table>
<center><a href="vm_version_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_32.ad.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>