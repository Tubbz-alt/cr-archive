<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/z/zPageAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="zMark.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="zPageAllocator.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/z/zPageAllocator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2015, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
 26 #include &quot;gc/z/zAddress.inline.hpp&quot;
 27 #include &quot;gc/z/zCollectedHeap.hpp&quot;
 28 #include &quot;gc/z/zFuture.inline.hpp&quot;
 29 #include &quot;gc/z/zGlobals.hpp&quot;
 30 #include &quot;gc/z/zLock.inline.hpp&quot;
 31 #include &quot;gc/z/zPage.inline.hpp&quot;
 32 #include &quot;gc/z/zPageAllocator.hpp&quot;
 33 #include &quot;gc/z/zPageCache.inline.hpp&quot;
 34 #include &quot;gc/z/zSafeDelete.inline.hpp&quot;
 35 #include &quot;gc/z/zStat.hpp&quot;
 36 #include &quot;gc/z/zTask.hpp&quot;
 37 #include &quot;gc/z/zTracer.inline.hpp&quot;
 38 #include &quot;gc/z/zWorkers.hpp&quot;

 39 #include &quot;runtime/globals.hpp&quot;
 40 #include &quot;runtime/init.hpp&quot;
 41 #include &quot;runtime/java.hpp&quot;
 42 #include &quot;utilities/debug.hpp&quot;
 43 
 44 static const ZStatCounter       ZCounterAllocationRate(&quot;Memory&quot;, &quot;Allocation Rate&quot;, ZStatUnitBytesPerSecond);
 45 static const ZStatCounter       ZCounterPageCacheFlush(&quot;Memory&quot;, &quot;Page Cache Flush&quot;, ZStatUnitBytesPerSecond);
 46 static const ZStatCounter       ZCounterUncommit(&quot;Memory&quot;, &quot;Uncommit&quot;, ZStatUnitBytesPerSecond);
 47 static const ZStatCriticalPhase ZCriticalPhaseAllocationStall(&quot;Allocation Stall&quot;);
 48 
 49 class ZPageAllocRequest : public StackObj {
 50   friend class ZList&lt;ZPageAllocRequest&gt;;
 51 
 52 private:
 53   const uint8_t                _type;
 54   const size_t                 _size;
 55   const ZAllocationFlags       _flags;
 56   const unsigned int           _total_collections;
 57   ZListNode&lt;ZPageAllocRequest&gt; _node;
 58   ZFuture&lt;ZPage*&gt;              _result;
</pre>
<hr />
<pre>
405 ZPage* ZPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, bool no_reserve) {
406   if (!ensure_available(size, no_reserve)) {
407     // Not enough free memory
408     return NULL;
409   }
410 
411   // Try allocate page from the cache
412   ZPage* const page = _cache.alloc_page(type, size);
413   if (page != NULL) {
414     return page;
415   }
416 
417   // Try flush pages from the cache
418   ensure_uncached_available(size);
419 
420   // Create new page
421   return create_page(type, size);
422 }
423 
424 ZPage* ZPageAllocator::alloc_page_common(uint8_t type, size_t size, ZAllocationFlags flags) {


425   ZPage* const page = alloc_page_common_inner(type, size, flags.no_reserve());
426   if (page == NULL) {
427     // Out of memory
428     return NULL;
429   }
430 
431   // Update used statistics
432   increase_used(size, flags.relocation());
433 
434   // Send trace event
<span class="line-modified">435   ZTracer::tracer()-&gt;report_page_alloc(size, _used, max_available(flags.no_reserve()), _cache.available(), flags);</span>

436 
437   return page;
438 }
439 
440 void ZPageAllocator::check_out_of_memory_during_initialization() {
441   if (!is_init_completed()) {
442     vm_exit_during_initialization(&quot;java.lang.OutOfMemoryError&quot;, &quot;Java heap too small&quot;);
443   }
444 }
445 
446 ZPage* ZPageAllocator::alloc_page_blocking(uint8_t type, size_t size, ZAllocationFlags flags) {
447   // Prepare to block
448   ZPageAllocRequest request(type, size, flags, ZCollectedHeap::heap()-&gt;total_collections());
449 
450   _lock.lock();
451 
452   // Try non-blocking allocation
453   ZPage* page = alloc_page_common(type, size, flags);
454   if (page == NULL) {
455     // Allocation failed, enqueue request
456     _queue.insert_last(&amp;request);
457   }
458 
459   _lock.unlock();
460 
461   if (page == NULL) {
462     // Allocation failed
463     ZStatTimer timer(ZCriticalPhaseAllocationStall);

464 
465     // We can only block if VM is fully initialized
466     check_out_of_memory_during_initialization();
467 
468     do {
469       // Start asynchronous GC
470       ZCollectedHeap::heap()-&gt;collect(GCCause::_z_allocation_stall);
471 
472       // Wait for allocation to complete or fail
473       page = request.wait();
474     } while (page == gc_marker);
475 
476     {
477       //
478       // We grab the lock here for two different reasons:
479       //
480       // 1) Guard deletion of underlying semaphore. This is a workaround for
481       // a bug in sem_post() in glibc &lt; 2.21, where it&#39;s not safe to destroy
482       // the semaphore immediately after returning from sem_wait(). The
483       // reason is that sem_post() can touch the semaphore after a waiting
484       // thread have returned from sem_wait(). To avoid this race we are
485       // forcing the waiting thread to acquire/release the lock held by the
486       // posting thread. https://sourceware.org/bugzilla/show_bug.cgi?id=12674
487       //
488       // 2) Guard the list of satisfied pages.
489       //
490       ZLocker&lt;ZLock&gt; locker(&amp;_lock);
491       _satisfied.remove(&amp;request);
492     }


493   }
494 
495   return page;
496 }
497 
498 ZPage* ZPageAllocator::alloc_page_nonblocking(uint8_t type, size_t size, ZAllocationFlags flags) {
499   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
500   return alloc_page_common(type, size, flags);
501 }
502 
503 ZPage* ZPageAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {
504   ZPage* const page = flags.non_blocking()
505                       ? alloc_page_nonblocking(type, size, flags)
506                       : alloc_page_blocking(type, size, flags);
507   if (page == NULL) {
508     // Out of memory
509     return NULL;
510   }
511 
512   // Map page if needed
</pre>
<hr />
<pre>
554     request-&gt;satisfy(page);
555   }
556 }
557 
558 void ZPageAllocator::free_page(ZPage* page, bool reclaimed) {
559   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
560 
561   // Update used statistics
562   decrease_used(page-&gt;size(), reclaimed);
563 
564   // Set time when last used
565   page-&gt;set_last_used();
566 
567   // Cache page
568   _cache.free_page(page);
569 
570   // Try satisfy blocked allocations
571   satisfy_alloc_queue();
572 }
573 
<span class="line-modified">574 size_t ZPageAllocator::flush_cache(ZPageCacheFlushClosure* cl) {</span>


575   ZList&lt;ZPage&gt; list;
576 
577   // Flush pages
578   _cache.flush(cl, &amp;list);
579 
580   const size_t overflushed = cl-&gt;overflushed();
581   if (overflushed &gt; 0) {
582     // Overflushed, keep part of last page
583     ZPage* const page = list.last()-&gt;split(overflushed);
584     _cache.free_page(page);
585   }
586 
587   // Destroy pages
588   size_t flushed = 0;
589   for (ZPage* page = list.remove_first(); page != NULL; page = list.remove_first()) {
590     flushed += page-&gt;size();
591     destroy_page(page);
592   }
593 



594   return flushed;
595 }
596 
597 class ZPageCacheFlushForAllocationClosure : public ZPageCacheFlushClosure {
598 public:
599   ZPageCacheFlushForAllocationClosure(size_t requested) :
600       ZPageCacheFlushClosure(requested) {}
601 
602   virtual bool do_page(const ZPage* page) {
603     if (_flushed &lt; _requested) {
604       // Flush page
605       _flushed += page-&gt;size();
606       return true;
607     }
608 
609     // Don&#39;t flush page
610     return false;
611   }
612 };
613 
614 void ZPageAllocator::flush_cache_for_allocation(size_t requested) {
615   assert(requested &lt;= _cache.available(), &quot;Invalid request&quot;);
616 
617   // Flush pages
618   ZPageCacheFlushForAllocationClosure cl(requested);
<span class="line-modified">619   const size_t flushed = flush_cache(&amp;cl);</span>
620 
621   assert(requested == flushed, &quot;Failed to flush&quot;);
622 
623   const size_t cached_after = _cache.available();
624   const size_t cached_before = cached_after + flushed;
625 
626   log_info(gc, heap)(&quot;Page Cache: &quot; SIZE_FORMAT &quot;M(%.0f%%)-&gt;&quot; SIZE_FORMAT &quot;M(%.0f%%), &quot;
627                      &quot;Flushed: &quot; SIZE_FORMAT &quot;M&quot;,
628                      cached_before / M, percent_of(cached_before, max_capacity()),
629                      cached_after / M, percent_of(cached_after, max_capacity()),
630                      flushed / M);
631 
632   // Update statistics
633   ZStatInc(ZCounterPageCacheFlush, flushed);
634 }
635 
636 class ZPageCacheFlushForUncommitClosure : public ZPageCacheFlushClosure {
637 private:
638   const uint64_t _now;
639   const uint64_t _delay;
</pre>
<hr />
<pre>
661 
662     // Don&#39;t flush page
663     return false;
664   }
665 
666   uint64_t timeout() const {
667     return _timeout;
668   }
669 };
670 
671 uint64_t ZPageAllocator::uncommit(uint64_t delay) {
672   // Set the default timeout, when no pages are found in the
673   // cache or when uncommit is disabled, equal to the delay.
674   uint64_t timeout = delay;
675 
676   if (!_uncommit) {
677     // Disabled
678     return timeout;
679   }
680 

681   size_t capacity_before;
682   size_t capacity_after;
683   size_t uncommitted;
684 
685   {
686     SuspendibleThreadSetJoiner joiner;
687     ZLocker&lt;ZLock&gt; locker(&amp;_lock);
688 
689     // Don&#39;t flush more than we will uncommit. Never uncommit
690     // the reserve, and never uncommit below min capacity.
691     const size_t needed = MIN2(_used + _max_reserve, _current_max_capacity);
692     const size_t guarded = MAX2(needed, _min_capacity);
693     const size_t uncommittable = _capacity - guarded;
694     const size_t uncached_available = _capacity - _used - _cache.available();
695     size_t uncommit = MIN2(uncommittable, uncached_available);
696     const size_t flush = uncommittable - uncommit;
697 
698     if (flush &gt; 0) {
699       // Flush pages to uncommit
700       ZPageCacheFlushForUncommitClosure cl(flush, delay);
<span class="line-modified">701       uncommit += flush_cache(&amp;cl);</span>
702       timeout = cl.timeout();
703     }
704 
705     // Uncommit
706     uncommitted = _physical.uncommit(uncommit);
707     _capacity -= uncommitted;
708 
709     capacity_after = _capacity;
710     capacity_before = capacity_after + uncommitted;
711   }
712 
713   if (uncommitted &gt; 0) {
714     log_info(gc, heap)(&quot;Capacity: &quot; SIZE_FORMAT &quot;M(%.0f%%)-&gt;&quot; SIZE_FORMAT &quot;M(%.0f%%), &quot;
715                        &quot;Uncommitted: &quot; SIZE_FORMAT &quot;M&quot;,
716                        capacity_before / M, percent_of(capacity_before, max_capacity()),
717                        capacity_after / M, percent_of(capacity_after, max_capacity()),
718                        uncommitted / M);
719 



720     // Update statistics
721     ZStatInc(ZCounterUncommit, uncommitted);
722   }
723 
724   return timeout;
725 }
726 
727 void ZPageAllocator::enable_deferred_delete() const {
728   _safe_delete.enable_deferred_delete();
729 }
730 
731 void ZPageAllocator::disable_deferred_delete() const {
732   _safe_delete.disable_deferred_delete();
733 }
734 
735 void ZPageAllocator::debug_map_page(const ZPage* page) const {
736   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
737   _physical.debug_map(page-&gt;physical_memory(), page-&gt;start());
738 }
739 
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2015, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
 26 #include &quot;gc/z/zAddress.inline.hpp&quot;
 27 #include &quot;gc/z/zCollectedHeap.hpp&quot;
 28 #include &quot;gc/z/zFuture.inline.hpp&quot;
 29 #include &quot;gc/z/zGlobals.hpp&quot;
 30 #include &quot;gc/z/zLock.inline.hpp&quot;
 31 #include &quot;gc/z/zPage.inline.hpp&quot;
 32 #include &quot;gc/z/zPageAllocator.hpp&quot;
 33 #include &quot;gc/z/zPageCache.inline.hpp&quot;
 34 #include &quot;gc/z/zSafeDelete.inline.hpp&quot;
 35 #include &quot;gc/z/zStat.hpp&quot;
 36 #include &quot;gc/z/zTask.hpp&quot;
 37 #include &quot;gc/z/zTracer.inline.hpp&quot;
 38 #include &quot;gc/z/zWorkers.hpp&quot;
<span class="line-added"> 39 #include &quot;jfr/jfrEvents.hpp&quot;</span>
 40 #include &quot;runtime/globals.hpp&quot;
 41 #include &quot;runtime/init.hpp&quot;
 42 #include &quot;runtime/java.hpp&quot;
 43 #include &quot;utilities/debug.hpp&quot;
 44 
 45 static const ZStatCounter       ZCounterAllocationRate(&quot;Memory&quot;, &quot;Allocation Rate&quot;, ZStatUnitBytesPerSecond);
 46 static const ZStatCounter       ZCounterPageCacheFlush(&quot;Memory&quot;, &quot;Page Cache Flush&quot;, ZStatUnitBytesPerSecond);
 47 static const ZStatCounter       ZCounterUncommit(&quot;Memory&quot;, &quot;Uncommit&quot;, ZStatUnitBytesPerSecond);
 48 static const ZStatCriticalPhase ZCriticalPhaseAllocationStall(&quot;Allocation Stall&quot;);
 49 
 50 class ZPageAllocRequest : public StackObj {
 51   friend class ZList&lt;ZPageAllocRequest&gt;;
 52 
 53 private:
 54   const uint8_t                _type;
 55   const size_t                 _size;
 56   const ZAllocationFlags       _flags;
 57   const unsigned int           _total_collections;
 58   ZListNode&lt;ZPageAllocRequest&gt; _node;
 59   ZFuture&lt;ZPage*&gt;              _result;
</pre>
<hr />
<pre>
406 ZPage* ZPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, bool no_reserve) {
407   if (!ensure_available(size, no_reserve)) {
408     // Not enough free memory
409     return NULL;
410   }
411 
412   // Try allocate page from the cache
413   ZPage* const page = _cache.alloc_page(type, size);
414   if (page != NULL) {
415     return page;
416   }
417 
418   // Try flush pages from the cache
419   ensure_uncached_available(size);
420 
421   // Create new page
422   return create_page(type, size);
423 }
424 
425 ZPage* ZPageAllocator::alloc_page_common(uint8_t type, size_t size, ZAllocationFlags flags) {
<span class="line-added">426   EventZPageAllocation event;</span>
<span class="line-added">427 </span>
428   ZPage* const page = alloc_page_common_inner(type, size, flags.no_reserve());
429   if (page == NULL) {
430     // Out of memory
431     return NULL;
432   }
433 
434   // Update used statistics
435   increase_used(size, flags.relocation());
436 
437   // Send trace event
<span class="line-modified">438   event.commit(type, size, _used, max_available(flags.no_reserve()),</span>
<span class="line-added">439                _cache.available(), flags.non_blocking(), flags.no_reserve());</span>
440 
441   return page;
442 }
443 
444 void ZPageAllocator::check_out_of_memory_during_initialization() {
445   if (!is_init_completed()) {
446     vm_exit_during_initialization(&quot;java.lang.OutOfMemoryError&quot;, &quot;Java heap too small&quot;);
447   }
448 }
449 
450 ZPage* ZPageAllocator::alloc_page_blocking(uint8_t type, size_t size, ZAllocationFlags flags) {
451   // Prepare to block
452   ZPageAllocRequest request(type, size, flags, ZCollectedHeap::heap()-&gt;total_collections());
453 
454   _lock.lock();
455 
456   // Try non-blocking allocation
457   ZPage* page = alloc_page_common(type, size, flags);
458   if (page == NULL) {
459     // Allocation failed, enqueue request
460     _queue.insert_last(&amp;request);
461   }
462 
463   _lock.unlock();
464 
465   if (page == NULL) {
466     // Allocation failed
467     ZStatTimer timer(ZCriticalPhaseAllocationStall);
<span class="line-added">468     EventZAllocationStall event;</span>
469 
470     // We can only block if VM is fully initialized
471     check_out_of_memory_during_initialization();
472 
473     do {
474       // Start asynchronous GC
475       ZCollectedHeap::heap()-&gt;collect(GCCause::_z_allocation_stall);
476 
477       // Wait for allocation to complete or fail
478       page = request.wait();
479     } while (page == gc_marker);
480 
481     {
482       //
483       // We grab the lock here for two different reasons:
484       //
485       // 1) Guard deletion of underlying semaphore. This is a workaround for
486       // a bug in sem_post() in glibc &lt; 2.21, where it&#39;s not safe to destroy
487       // the semaphore immediately after returning from sem_wait(). The
488       // reason is that sem_post() can touch the semaphore after a waiting
489       // thread have returned from sem_wait(). To avoid this race we are
490       // forcing the waiting thread to acquire/release the lock held by the
491       // posting thread. https://sourceware.org/bugzilla/show_bug.cgi?id=12674
492       //
493       // 2) Guard the list of satisfied pages.
494       //
495       ZLocker&lt;ZLock&gt; locker(&amp;_lock);
496       _satisfied.remove(&amp;request);
497     }
<span class="line-added">498 </span>
<span class="line-added">499     event.commit(type, size);</span>
500   }
501 
502   return page;
503 }
504 
505 ZPage* ZPageAllocator::alloc_page_nonblocking(uint8_t type, size_t size, ZAllocationFlags flags) {
506   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
507   return alloc_page_common(type, size, flags);
508 }
509 
510 ZPage* ZPageAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {
511   ZPage* const page = flags.non_blocking()
512                       ? alloc_page_nonblocking(type, size, flags)
513                       : alloc_page_blocking(type, size, flags);
514   if (page == NULL) {
515     // Out of memory
516     return NULL;
517   }
518 
519   // Map page if needed
</pre>
<hr />
<pre>
561     request-&gt;satisfy(page);
562   }
563 }
564 
565 void ZPageAllocator::free_page(ZPage* page, bool reclaimed) {
566   ZLocker&lt;ZLock&gt; locker(&amp;_lock);
567 
568   // Update used statistics
569   decrease_used(page-&gt;size(), reclaimed);
570 
571   // Set time when last used
572   page-&gt;set_last_used();
573 
574   // Cache page
575   _cache.free_page(page);
576 
577   // Try satisfy blocked allocations
578   satisfy_alloc_queue();
579 }
580 
<span class="line-modified">581 size_t ZPageAllocator::flush_cache(ZPageCacheFlushClosure* cl, bool for_allocation) {</span>
<span class="line-added">582   EventZPageCacheFlush event;</span>
<span class="line-added">583 </span>
584   ZList&lt;ZPage&gt; list;
585 
586   // Flush pages
587   _cache.flush(cl, &amp;list);
588 
589   const size_t overflushed = cl-&gt;overflushed();
590   if (overflushed &gt; 0) {
591     // Overflushed, keep part of last page
592     ZPage* const page = list.last()-&gt;split(overflushed);
593     _cache.free_page(page);
594   }
595 
596   // Destroy pages
597   size_t flushed = 0;
598   for (ZPage* page = list.remove_first(); page != NULL; page = list.remove_first()) {
599     flushed += page-&gt;size();
600     destroy_page(page);
601   }
602 
<span class="line-added">603   // Send event</span>
<span class="line-added">604   event.commit(flushed, for_allocation);</span>
<span class="line-added">605 </span>
606   return flushed;
607 }
608 
609 class ZPageCacheFlushForAllocationClosure : public ZPageCacheFlushClosure {
610 public:
611   ZPageCacheFlushForAllocationClosure(size_t requested) :
612       ZPageCacheFlushClosure(requested) {}
613 
614   virtual bool do_page(const ZPage* page) {
615     if (_flushed &lt; _requested) {
616       // Flush page
617       _flushed += page-&gt;size();
618       return true;
619     }
620 
621     // Don&#39;t flush page
622     return false;
623   }
624 };
625 
626 void ZPageAllocator::flush_cache_for_allocation(size_t requested) {
627   assert(requested &lt;= _cache.available(), &quot;Invalid request&quot;);
628 
629   // Flush pages
630   ZPageCacheFlushForAllocationClosure cl(requested);
<span class="line-modified">631   const size_t flushed = flush_cache(&amp;cl, true /* for_allocation */);</span>
632 
633   assert(requested == flushed, &quot;Failed to flush&quot;);
634 
635   const size_t cached_after = _cache.available();
636   const size_t cached_before = cached_after + flushed;
637 
638   log_info(gc, heap)(&quot;Page Cache: &quot; SIZE_FORMAT &quot;M(%.0f%%)-&gt;&quot; SIZE_FORMAT &quot;M(%.0f%%), &quot;
639                      &quot;Flushed: &quot; SIZE_FORMAT &quot;M&quot;,
640                      cached_before / M, percent_of(cached_before, max_capacity()),
641                      cached_after / M, percent_of(cached_after, max_capacity()),
642                      flushed / M);
643 
644   // Update statistics
645   ZStatInc(ZCounterPageCacheFlush, flushed);
646 }
647 
648 class ZPageCacheFlushForUncommitClosure : public ZPageCacheFlushClosure {
649 private:
650   const uint64_t _now;
651   const uint64_t _delay;
</pre>
<hr />
<pre>
673 
674     // Don&#39;t flush page
675     return false;
676   }
677 
678   uint64_t timeout() const {
679     return _timeout;
680   }
681 };
682 
683 uint64_t ZPageAllocator::uncommit(uint64_t delay) {
684   // Set the default timeout, when no pages are found in the
685   // cache or when uncommit is disabled, equal to the delay.
686   uint64_t timeout = delay;
687 
688   if (!_uncommit) {
689     // Disabled
690     return timeout;
691   }
692 
<span class="line-added">693   EventZUncommit event;</span>
694   size_t capacity_before;
695   size_t capacity_after;
696   size_t uncommitted;
697 
698   {
699     SuspendibleThreadSetJoiner joiner;
700     ZLocker&lt;ZLock&gt; locker(&amp;_lock);
701 
702     // Don&#39;t flush more than we will uncommit. Never uncommit
703     // the reserve, and never uncommit below min capacity.
704     const size_t needed = MIN2(_used + _max_reserve, _current_max_capacity);
705     const size_t guarded = MAX2(needed, _min_capacity);
706     const size_t uncommittable = _capacity - guarded;
707     const size_t uncached_available = _capacity - _used - _cache.available();
708     size_t uncommit = MIN2(uncommittable, uncached_available);
709     const size_t flush = uncommittable - uncommit;
710 
711     if (flush &gt; 0) {
712       // Flush pages to uncommit
713       ZPageCacheFlushForUncommitClosure cl(flush, delay);
<span class="line-modified">714       uncommit += flush_cache(&amp;cl, false /* for_allocation */);</span>
715       timeout = cl.timeout();
716     }
717 
718     // Uncommit
719     uncommitted = _physical.uncommit(uncommit);
720     _capacity -= uncommitted;
721 
722     capacity_after = _capacity;
723     capacity_before = capacity_after + uncommitted;
724   }
725 
726   if (uncommitted &gt; 0) {
727     log_info(gc, heap)(&quot;Capacity: &quot; SIZE_FORMAT &quot;M(%.0f%%)-&gt;&quot; SIZE_FORMAT &quot;M(%.0f%%), &quot;
728                        &quot;Uncommitted: &quot; SIZE_FORMAT &quot;M&quot;,
729                        capacity_before / M, percent_of(capacity_before, max_capacity()),
730                        capacity_after / M, percent_of(capacity_after, max_capacity()),
731                        uncommitted / M);
732 
<span class="line-added">733     // Send event</span>
<span class="line-added">734     event.commit(capacity_before, capacity_after, uncommitted);</span>
<span class="line-added">735 </span>
736     // Update statistics
737     ZStatInc(ZCounterUncommit, uncommitted);
738   }
739 
740   return timeout;
741 }
742 
743 void ZPageAllocator::enable_deferred_delete() const {
744   _safe_delete.enable_deferred_delete();
745 }
746 
747 void ZPageAllocator::disable_deferred_delete() const {
748   _safe_delete.disable_deferred_delete();
749 }
750 
751 void ZPageAllocator::debug_map_page(const ZPage* page) const {
752   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
753   _physical.debug_map(page-&gt;physical_memory(), page-&gt;start());
754 }
755 
</pre>
</td>
</tr>
</table>
<center><a href="zMark.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="zPageAllocator.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>