<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1ConcurrentRefine.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1DirtyCardQueue.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 38 #include &quot;runtime/atomic.hpp&quot;
 39 #include &quot;runtime/os.hpp&quot;
 40 #include &quot;runtime/safepoint.hpp&quot;
 41 #include &quot;runtime/thread.inline.hpp&quot;
 42 #include &quot;runtime/threadSMR.hpp&quot;
 43 #include &quot;utilities/globalCounter.inline.hpp&quot;
 44 #include &quot;utilities/macros.hpp&quot;
 45 #include &quot;utilities/quickSort.hpp&quot;
 46 
 47 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 48   // Dirty card queues are always active, so we create them with their
 49   // active field set to true.
 50   PtrQueue(qset, true /* active */)
 51 { }
 52 
 53 G1DirtyCardQueue::~G1DirtyCardQueue() {
 54   flush();
 55 }
 56 
 57 void G1DirtyCardQueue::handle_completed_buffer() {
<span class="line-modified"> 58   assert(_buf != NULL, &quot;precondition&quot;);</span>
 59   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
<span class="line-modified"> 60   G1DirtyCardQueueSet* dcqs = dirty_card_qset();</span>
<span class="line-modified"> 61   if (dcqs-&gt;process_or_enqueue_completed_buffer(node)) {</span>
<span class="line-removed"> 62     reset();                    // Buffer fully processed, reset index.</span>
<span class="line-removed"> 63   } else {</span>
<span class="line-removed"> 64     allocate_buffer();          // Buffer enqueued, get a new one.</span>
<span class="line-removed"> 65   }</span>
 66 }
 67 
 68 // Assumed to be zero by concurrent threads.
 69 static uint par_ids_start() { return 0; }
 70 
 71 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 72   PtrQueueSet(allocator),
 73   _primary_refinement_thread(NULL),
 74   _num_cards(0),
 75   _completed(),
 76   _paused(),
 77   _free_ids(par_ids_start(), num_par_ids()),
 78   _process_cards_threshold(ProcessCardsThresholdNever),
 79   _max_cards(MaxCardsUnlimited),
<span class="line-modified"> 80   _max_cards_padding(0),</span>
 81   _mutator_refined_cards_counters(NEW_C_HEAP_ARRAY(size_t, num_par_ids(), mtGC))
 82 {
 83   ::memset(_mutator_refined_cards_counters, 0, num_par_ids() * sizeof(size_t));
 84   _all_active = true;
 85 }
 86 
 87 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
 88   abandon_completed_buffers();
 89   FREE_C_HEAP_ARRAY(size_t, _mutator_refined_cards_counters);
 90 }
 91 
 92 // Determines how many mutator threads can process the buffers in parallel.
 93 uint G1DirtyCardQueueSet::num_par_ids() {
 94   return (uint)os::initial_active_processor_count();
 95 }
 96 
 97 size_t G1DirtyCardQueueSet::total_mutator_refined_cards() const {
 98   size_t sum = 0;
 99   for (uint i = 0; i &lt; num_par_ids(); ++i) {
100     sum += _mutator_refined_cards_counters[i];
</pre>
<hr />
<pre>
210 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {
211   assert_at_safepoint();
212   HeadTail result(Atomic::load(&amp;_head), Atomic::load(&amp;_tail));
213   Atomic::store(&amp;_head, (BufferNode*)NULL);
214   Atomic::store(&amp;_tail, (BufferNode*)NULL);
215   return result;
216 }
217 
218 void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {
219   assert(cbn != NULL, &quot;precondition&quot;);
220   // Increment _num_cards before adding to queue, so queue removal doesn&#39;t
221   // need to deal with _num_cards possibly going negative.
222   size_t new_num_cards = Atomic::add(&amp;_num_cards, buffer_size() - cbn-&gt;index());
223   _completed.push(*cbn);
224   if ((new_num_cards &gt; process_cards_threshold()) &amp;&amp;
225       (_primary_refinement_thread != NULL)) {
226     _primary_refinement_thread-&gt;activate();
227   }
228 }
229 
<span class="line-modified">230 BufferNode* G1DirtyCardQueueSet::get_completed_buffer(size_t stop_at) {</span>
<span class="line-removed">231   if (Atomic::load_acquire(&amp;_num_cards) &lt; stop_at) {</span>
<span class="line-removed">232     return NULL;</span>
<span class="line-removed">233   }</span>
<span class="line-removed">234 </span>
235   BufferNode* result = _completed.pop();
236   if (result == NULL) {         // Unlikely if no paused buffers.
237     enqueue_previous_paused_buffers();
238     result = _completed.pop();
239     if (result == NULL) return NULL;
240   }
241   Atomic::sub(&amp;_num_cards, buffer_size() - result-&gt;index());
242   return result;
243 }
244 
245 #ifdef ASSERT
246 void G1DirtyCardQueueSet::verify_num_cards() const {
247   size_t actual = 0;
248   BufferNode* cur = _completed.top();
249   for ( ; cur != NULL; cur = cur-&gt;next()) {
250     actual += buffer_size() - cur-&gt;index();
251   }
252   assert(actual == Atomic::load(&amp;_num_cards),
253          &quot;Num entries in completed buffers should be &quot; SIZE_FORMAT &quot; but are &quot; SIZE_FORMAT,
254          Atomic::load(&amp;_num_cards), actual);
</pre>
<hr />
<pre>
531     // humongous object allocation (see comment at the StoreStore fence before
532     // setting the regions&#39; tops in humongous allocation path).
533     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
534     // wrto each other. We need both set, in any order, to proceed.
535     OrderAccess::fence();
536     sort_cards(first_clean_index);
537     return refine_cleaned_cards(first_clean_index);
538   }
539 };
540 
541 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
542                                         uint worker_id,
543                                         size_t* total_refined_cards) {
544   G1RefineBufferedCards buffered_cards(node,
545                                        buffer_size(),
546                                        worker_id,
547                                        total_refined_cards);
548   return buffered_cards.refine();
549 }
550 
<span class="line-modified">551 #ifndef ASSERT</span>
<span class="line-modified">552 #define assert_fully_consumed(node, buffer_size)</span>
<span class="line-modified">553 #else</span>
<span class="line-modified">554 #define assert_fully_consumed(node, buffer_size)                \</span>
<span class="line-modified">555   do {                                                          \</span>
<span class="line-modified">556     size_t _afc_index = (node)-&gt;index();                        \</span>
<span class="line-modified">557     size_t _afc_size = (buffer_size);                           \</span>
<span class="line-modified">558     assert(_afc_index == _afc_size,                             \</span>
<span class="line-modified">559            &quot;Buffer was not fully consumed as claimed: index: &quot;  \</span>
<span class="line-modified">560            SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,                  \</span>
<span class="line-modified">561             _afc_index, _afc_size);                             \</span>
<span class="line-modified">562   } while (0)</span>
<span class="line-removed">563 #endif // ASSERT</span>
<span class="line-removed">564 </span>
<span class="line-removed">565 bool G1DirtyCardQueueSet::process_or_enqueue_completed_buffer(BufferNode* node) {</span>
<span class="line-removed">566   if (Thread::current()-&gt;is_Java_thread()) {</span>
<span class="line-removed">567     // If the number of buffers exceeds the limit, make this Java</span>
<span class="line-removed">568     // thread do the processing itself.  Calculation is racy but we</span>
<span class="line-removed">569     // don&#39;t need precision here.  The add of padding could overflow,</span>
<span class="line-removed">570     // which is treated as unlimited.</span>
<span class="line-removed">571     size_t limit = max_cards() + max_cards_padding();</span>
<span class="line-removed">572     if ((num_cards() &gt; limit) &amp;&amp; (limit &gt;= max_cards())) {</span>
<span class="line-removed">573       if (mut_process_buffer(node)) {</span>
<span class="line-removed">574         return true;</span>
<span class="line-removed">575       }</span>
<span class="line-removed">576       // Buffer was incompletely processed because of a pending safepoint</span>
<span class="line-removed">577       // request.  Unlike with refinement thread processing, for mutator</span>
<span class="line-removed">578       // processing the buffer did not come from the completed buffer queue,</span>
<span class="line-removed">579       // so it is okay to add it to the queue rather than to the paused set.</span>
<span class="line-removed">580       // Indeed, it can&#39;t be added to the paused set because we didn&#39;t pass</span>
<span class="line-removed">581       // through enqueue_previous_paused_buffers.</span>
<span class="line-removed">582     }</span>
583   }
<span class="line-removed">584   enqueue_completed_buffer(node);</span>
<span class="line-removed">585   return false;</span>
586 }
587 
<span class="line-modified">588 bool G1DirtyCardQueueSet::mut_process_buffer(BufferNode* node) {</span>

















589   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
590   uint counter_index = worker_id - par_ids_start();
591   size_t* counter = &amp;_mutator_refined_cards_counters[counter_index];
<span class="line-modified">592   bool result = refine_buffer(node, worker_id, counter);</span>
593   _free_ids.release_par_id(worker_id); // release the id
594 
<span class="line-modified">595   if (result) {</span>
<span class="line-modified">596     assert_fully_consumed(node, buffer_size());</span>
<span class="line-removed">597   }</span>
<span class="line-removed">598   return result;</span>
599 }
600 
601 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
602                                                                size_t stop_at,
603                                                                size_t* total_refined_cards) {
<span class="line-modified">604   BufferNode* node = get_completed_buffer(stop_at);</span>
<span class="line-modified">605   if (node == NULL) {</span>
<span class="line-modified">606     return false;</span>
<span class="line-modified">607   } else if (refine_buffer(node, worker_id, total_refined_cards)) {</span>
<span class="line-modified">608     assert_fully_consumed(node, buffer_size());</span>
<span class="line-modified">609     // Done with fully processed buffer.</span>
<span class="line-modified">610     deallocate_buffer(node);</span>
<span class="line-modified">611     return true;</span>
<span class="line-modified">612   } else {</span>
<span class="line-removed">613     // Buffer incompletely processed because there is a pending safepoint.</span>
<span class="line-removed">614     // Record partially processed buffer, to be finished later.</span>
<span class="line-removed">615     record_paused_buffer(node);</span>
<span class="line-removed">616     return true;</span>
<span class="line-removed">617   }</span>
618 }
619 
620 void G1DirtyCardQueueSet::abandon_logs() {
621   assert_at_safepoint();
622   abandon_completed_buffers();
623 
624   // Since abandon is done only at safepoints, we can safely manipulate
625   // these queues.
626   struct AbandonThreadLogClosure : public ThreadClosure {
627     virtual void do_thread(Thread* t) {
628       G1ThreadLocalData::dirty_card_queue(t).reset();
629     }
630   } closure;
631   Threads::threads_do(&amp;closure);
632 
633   G1BarrierSet::shared_dirty_card_queue().reset();
634 }
635 
636 void G1DirtyCardQueueSet::concatenate_logs() {
637   // Iterate over all the threads, if we find a partial log add it to
</pre>
<hr />
<pre>
639   // of outstanding buffers.
640   assert_at_safepoint();
641   size_t old_limit = max_cards();
642   set_max_cards(MaxCardsUnlimited);
643 
644   struct ConcatenateThreadLogClosure : public ThreadClosure {
645     virtual void do_thread(Thread* t) {
646       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
647       if (!dcq.is_empty()) {
648         dcq.flush();
649       }
650     }
651   } closure;
652   Threads::threads_do(&amp;closure);
653 
654   G1BarrierSet::shared_dirty_card_queue().flush();
655   enqueue_all_paused_buffers();
656   verify_num_cards();
657   set_max_cards(old_limit);
658 }

























</pre>
</td>
<td>
<hr />
<pre>
 38 #include &quot;runtime/atomic.hpp&quot;
 39 #include &quot;runtime/os.hpp&quot;
 40 #include &quot;runtime/safepoint.hpp&quot;
 41 #include &quot;runtime/thread.inline.hpp&quot;
 42 #include &quot;runtime/threadSMR.hpp&quot;
 43 #include &quot;utilities/globalCounter.inline.hpp&quot;
 44 #include &quot;utilities/macros.hpp&quot;
 45 #include &quot;utilities/quickSort.hpp&quot;
 46 
 47 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 48   // Dirty card queues are always active, so we create them with their
 49   // active field set to true.
 50   PtrQueue(qset, true /* active */)
 51 { }
 52 
 53 G1DirtyCardQueue::~G1DirtyCardQueue() {
 54   flush();
 55 }
 56 
 57 void G1DirtyCardQueue::handle_completed_buffer() {
<span class="line-modified"> 58   assert(!is_empty(), &quot;precondition&quot;);</span>
 59   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
<span class="line-modified"> 60   allocate_buffer();</span>
<span class="line-modified"> 61   dirty_card_qset()-&gt;handle_completed_buffer(node);</span>




 62 }
 63 
 64 // Assumed to be zero by concurrent threads.
 65 static uint par_ids_start() { return 0; }
 66 
 67 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 68   PtrQueueSet(allocator),
 69   _primary_refinement_thread(NULL),
 70   _num_cards(0),
 71   _completed(),
 72   _paused(),
 73   _free_ids(par_ids_start(), num_par_ids()),
 74   _process_cards_threshold(ProcessCardsThresholdNever),
 75   _max_cards(MaxCardsUnlimited),
<span class="line-modified"> 76   _padded_max_cards(MaxCardsUnlimited),</span>
 77   _mutator_refined_cards_counters(NEW_C_HEAP_ARRAY(size_t, num_par_ids(), mtGC))
 78 {
 79   ::memset(_mutator_refined_cards_counters, 0, num_par_ids() * sizeof(size_t));
 80   _all_active = true;
 81 }
 82 
 83 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
 84   abandon_completed_buffers();
 85   FREE_C_HEAP_ARRAY(size_t, _mutator_refined_cards_counters);
 86 }
 87 
 88 // Determines how many mutator threads can process the buffers in parallel.
 89 uint G1DirtyCardQueueSet::num_par_ids() {
 90   return (uint)os::initial_active_processor_count();
 91 }
 92 
 93 size_t G1DirtyCardQueueSet::total_mutator_refined_cards() const {
 94   size_t sum = 0;
 95   for (uint i = 0; i &lt; num_par_ids(); ++i) {
 96     sum += _mutator_refined_cards_counters[i];
</pre>
<hr />
<pre>
206 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {
207   assert_at_safepoint();
208   HeadTail result(Atomic::load(&amp;_head), Atomic::load(&amp;_tail));
209   Atomic::store(&amp;_head, (BufferNode*)NULL);
210   Atomic::store(&amp;_tail, (BufferNode*)NULL);
211   return result;
212 }
213 
214 void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {
215   assert(cbn != NULL, &quot;precondition&quot;);
216   // Increment _num_cards before adding to queue, so queue removal doesn&#39;t
217   // need to deal with _num_cards possibly going negative.
218   size_t new_num_cards = Atomic::add(&amp;_num_cards, buffer_size() - cbn-&gt;index());
219   _completed.push(*cbn);
220   if ((new_num_cards &gt; process_cards_threshold()) &amp;&amp;
221       (_primary_refinement_thread != NULL)) {
222     _primary_refinement_thread-&gt;activate();
223   }
224 }
225 
<span class="line-modified">226 BufferNode* G1DirtyCardQueueSet::get_completed_buffer() {</span>




227   BufferNode* result = _completed.pop();
228   if (result == NULL) {         // Unlikely if no paused buffers.
229     enqueue_previous_paused_buffers();
230     result = _completed.pop();
231     if (result == NULL) return NULL;
232   }
233   Atomic::sub(&amp;_num_cards, buffer_size() - result-&gt;index());
234   return result;
235 }
236 
237 #ifdef ASSERT
238 void G1DirtyCardQueueSet::verify_num_cards() const {
239   size_t actual = 0;
240   BufferNode* cur = _completed.top();
241   for ( ; cur != NULL; cur = cur-&gt;next()) {
242     actual += buffer_size() - cur-&gt;index();
243   }
244   assert(actual == Atomic::load(&amp;_num_cards),
245          &quot;Num entries in completed buffers should be &quot; SIZE_FORMAT &quot; but are &quot; SIZE_FORMAT,
246          Atomic::load(&amp;_num_cards), actual);
</pre>
<hr />
<pre>
523     // humongous object allocation (see comment at the StoreStore fence before
524     // setting the regions&#39; tops in humongous allocation path).
525     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
526     // wrto each other. We need both set, in any order, to proceed.
527     OrderAccess::fence();
528     sort_cards(first_clean_index);
529     return refine_cleaned_cards(first_clean_index);
530   }
531 };
532 
533 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
534                                         uint worker_id,
535                                         size_t* total_refined_cards) {
536   G1RefineBufferedCards buffered_cards(node,
537                                        buffer_size(),
538                                        worker_id,
539                                        total_refined_cards);
540   return buffered_cards.refine();
541 }
542 
<span class="line-modified">543 void G1DirtyCardQueueSet::handle_refined_buffer(BufferNode* node,</span>
<span class="line-modified">544                                                 bool fully_processed) {</span>
<span class="line-modified">545   if (fully_processed) {</span>
<span class="line-modified">546     assert(node-&gt;index() == buffer_size(),</span>
<span class="line-modified">547            &quot;Buffer not fully consumed: index: &quot; SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,</span>
<span class="line-modified">548            node-&gt;index(), buffer_size());</span>
<span class="line-modified">549     deallocate_buffer(node);</span>
<span class="line-modified">550   } else {</span>
<span class="line-modified">551     assert(node-&gt;index() &lt; buffer_size(), &quot;Buffer fully consumed.&quot;);</span>
<span class="line-modified">552     // Buffer incompletely processed because there is a pending safepoint.</span>
<span class="line-modified">553     // Record partially processed buffer, to be finished later.</span>
<span class="line-modified">554     record_paused_buffer(node);</span>




















555   }


556 }
557 
<span class="line-modified">558 void G1DirtyCardQueueSet::handle_completed_buffer(BufferNode* new_node) {</span>
<span class="line-added">559   enqueue_completed_buffer(new_node);</span>
<span class="line-added">560 </span>
<span class="line-added">561   // No need for mutator refinement if number of cards is below limit.</span>
<span class="line-added">562   if (Atomic::load(&amp;_num_cards) &lt;= Atomic::load(&amp;_padded_max_cards)) {</span>
<span class="line-added">563     return;</span>
<span class="line-added">564   }</span>
<span class="line-added">565 </span>
<span class="line-added">566   // Only Java threads perform mutator refinement.</span>
<span class="line-added">567   if (!Thread::current()-&gt;is_Java_thread()) {</span>
<span class="line-added">568     return;</span>
<span class="line-added">569   }</span>
<span class="line-added">570 </span>
<span class="line-added">571   BufferNode* node = get_completed_buffer();</span>
<span class="line-added">572   if (node == NULL) return;     // Didn&#39;t get a buffer to process.</span>
<span class="line-added">573 </span>
<span class="line-added">574   // Refine cards in buffer.</span>
<span class="line-added">575 </span>
576   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
577   uint counter_index = worker_id - par_ids_start();
578   size_t* counter = &amp;_mutator_refined_cards_counters[counter_index];
<span class="line-modified">579   bool fully_processed = refine_buffer(node, worker_id, counter);</span>
580   _free_ids.release_par_id(worker_id); // release the id
581 
<span class="line-modified">582   // Deal with buffer after releasing id, to let another thread use id.</span>
<span class="line-modified">583   handle_refined_buffer(node, fully_processed);</span>


584 }
585 
586 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
587                                                                size_t stop_at,
588                                                                size_t* total_refined_cards) {
<span class="line-modified">589   // Not enough cards to trigger processing.</span>
<span class="line-modified">590   if (Atomic::load(&amp;_num_cards) &lt;= stop_at) return false;</span>
<span class="line-modified">591 </span>
<span class="line-modified">592   BufferNode* node = get_completed_buffer();</span>
<span class="line-modified">593   if (node == NULL) return false; // Didn&#39;t get a buffer to process.</span>
<span class="line-modified">594 </span>
<span class="line-modified">595   bool fully_processed = refine_buffer(node, worker_id, total_refined_cards);</span>
<span class="line-modified">596   handle_refined_buffer(node, fully_processed);</span>
<span class="line-modified">597   return true;</span>





598 }
599 
600 void G1DirtyCardQueueSet::abandon_logs() {
601   assert_at_safepoint();
602   abandon_completed_buffers();
603 
604   // Since abandon is done only at safepoints, we can safely manipulate
605   // these queues.
606   struct AbandonThreadLogClosure : public ThreadClosure {
607     virtual void do_thread(Thread* t) {
608       G1ThreadLocalData::dirty_card_queue(t).reset();
609     }
610   } closure;
611   Threads::threads_do(&amp;closure);
612 
613   G1BarrierSet::shared_dirty_card_queue().reset();
614 }
615 
616 void G1DirtyCardQueueSet::concatenate_logs() {
617   // Iterate over all the threads, if we find a partial log add it to
</pre>
<hr />
<pre>
619   // of outstanding buffers.
620   assert_at_safepoint();
621   size_t old_limit = max_cards();
622   set_max_cards(MaxCardsUnlimited);
623 
624   struct ConcatenateThreadLogClosure : public ThreadClosure {
625     virtual void do_thread(Thread* t) {
626       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
627       if (!dcq.is_empty()) {
628         dcq.flush();
629       }
630     }
631   } closure;
632   Threads::threads_do(&amp;closure);
633 
634   G1BarrierSet::shared_dirty_card_queue().flush();
635   enqueue_all_paused_buffers();
636   verify_num_cards();
637   set_max_cards(old_limit);
638 }
<span class="line-added">639 </span>
<span class="line-added">640 size_t G1DirtyCardQueueSet::max_cards() const {</span>
<span class="line-added">641   return _max_cards;</span>
<span class="line-added">642 }</span>
<span class="line-added">643 </span>
<span class="line-added">644 void G1DirtyCardQueueSet::set_max_cards(size_t value) {</span>
<span class="line-added">645   _max_cards = value;</span>
<span class="line-added">646   Atomic::store(&amp;_padded_max_cards, value);</span>
<span class="line-added">647 }</span>
<span class="line-added">648 </span>
<span class="line-added">649 void G1DirtyCardQueueSet::set_max_cards_padding(size_t padding) {</span>
<span class="line-added">650   // Compute sum, clipping to max.</span>
<span class="line-added">651   size_t limit = _max_cards + padding;</span>
<span class="line-added">652   if (limit &lt; padding) {        // Check for overflow.</span>
<span class="line-added">653     limit = MaxCardsUnlimited;</span>
<span class="line-added">654   }</span>
<span class="line-added">655   Atomic::store(&amp;_padded_max_cards, limit);</span>
<span class="line-added">656 }</span>
<span class="line-added">657 </span>
<span class="line-added">658 void G1DirtyCardQueueSet::discard_max_cards_padding() {</span>
<span class="line-added">659   // Being racy here is okay, since all threads store the same value.</span>
<span class="line-added">660   if (_max_cards != Atomic::load(&amp;_padded_max_cards)) {</span>
<span class="line-added">661     Atomic::store(&amp;_padded_max_cards, _max_cards);</span>
<span class="line-added">662   }</span>
<span class="line-added">663 }</span>
</pre>
</td>
</tr>
</table>
<center><a href="g1ConcurrentRefine.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1DirtyCardQueue.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>