diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -74,10 +74,11 @@
 #include "runtime/stubRoutines.hpp"
 #include "runtime/timer.hpp"
 #include "utilities/align.hpp"
 #include "utilities/copy.hpp"
 #include "utilities/macros.hpp"
+#include "utilities/resourceHash.hpp"
 
 
 // -------------------- Compile::mach_constant_base_node -----------------------
 // Constant table base node singleton.
 MachConstantBaseNode* Compile::mach_constant_base_node() {
@@ -248,11 +249,11 @@
   { ttyLocker ttyl;
     if (xtty != NULL)  xtty->head("statistics type='opto'");
     Parse::print_statistics();
     PhaseCCP::print_statistics();
     PhaseRegAlloc::print_statistics();
-    Scheduling::print_statistics();
+    PhaseOutput::print_statistics();
     PhasePeephole::print_statistics();
     PhaseIdealLoop::print_statistics();
     if (xtty != NULL)  xtty->tail("statistics");
   }
   if (_intrinsic_hist_flags[vmIntrinsics::_none] != 0) {
@@ -260,21 +261,10 @@
     print_intrinsic_statistics();
   }
 }
 #endif //PRODUCT
 
-// Support for bundling info
-Bundle* Compile::node_bundling(const Node *n) {
-  assert(valid_bundle_info(n), "oob");
-  return &_node_bundling_base[n->_idx];
-}
-
-bool Compile::valid_bundle_info(const Node *n) {
-  return (_node_bundling_limit > n->_idx);
-}
-
-
 void Compile::gvn_replace_by(Node* n, Node* nn) {
   for (DUIterator_Last imin, i = n->last_outs(imin); i >= imin; ) {
     Node* use = n->last_out(i);
     bool is_in_table = initial_gvn()->hash_delete(use);
     uint uses_found = 0;
@@ -421,28 +411,10 @@
   remove_useless_late_inlines(&_boxing_late_inlines, useful);
   remove_useless_late_inlines(&_late_inlines, useful);
   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 }
 
-//------------------------------frame_size_in_words-----------------------------
-// frame_slots in units of words
-int Compile::frame_size_in_words() const {
-  // shift is 0 in LP32 and 1 in LP64
-  const int shift = (LogBytesPerWord - LogBytesPerInt);
-  int words = _frame_slots >> shift;
-  assert( words << shift == _frame_slots, "frame size must be properly aligned in LP64" );
-  return words;
-}
-
-// To bang the stack of this compiled method we use the stack size
-// that the interpreter would need in case of a deoptimization. This
-// removes the need to bang the stack in the deoptimization blob which
-// in turn simplifies stack overflow handling.
-int Compile::bang_size_in_bytes() const {
-  return MAX2(frame_size_in_bytes() + os::extra_bang_size_in_bytes(), _interpreter_frame_size);
-}
-
 // ============================================================================
 //------------------------------CompileWrapper---------------------------------
 class CompileWrapper : public StackObj {
   Compile *const _compile;
  public:
@@ -466,18 +438,15 @@
   compile->set_last_tf(NULL, NULL);
   compile->set_indexSet_arena(NULL);
   compile->set_indexSet_free_block_list(NULL);
   compile->init_type_arena();
   Type::Initialize(compile);
-  _compile->set_scratch_buffer_blob(NULL);
   _compile->begin_method();
   _compile->clone_map().set_debug(_compile->has_method() && _compile->directive()->CloneMapDebugOption);
 }
 CompileWrapper::~CompileWrapper() {
   _compile->end_method();
-  if (_compile->scratch_buffer_blob() != NULL)
-    BufferBlob::free(_compile->scratch_buffer_blob());
   _compile->env()->set_compiler_data(NULL);
 }
 
 
 //----------------------------print_compile_messages---------------------------
@@ -518,117 +487,19 @@
     }
   }
 #endif
 }
 
-
-//-----------------------init_scratch_buffer_blob------------------------------
-// Construct a temporary BufferBlob and cache it for this compile.
-void Compile::init_scratch_buffer_blob(int const_size) {
-  // If there is already a scratch buffer blob allocated and the
-  // constant section is big enough, use it.  Otherwise free the
-  // current and allocate a new one.
-  BufferBlob* blob = scratch_buffer_blob();
-  if ((blob != NULL) && (const_size <= _scratch_const_size)) {
-    // Use the current blob.
-  } else {
-    if (blob != NULL) {
-      BufferBlob::free(blob);
-    }
-
-    ResourceMark rm;
-    _scratch_const_size = const_size;
-    int size = C2Compiler::initial_code_buffer_size(const_size);
-    blob = BufferBlob::create("Compile::scratch_buffer", size);
-    // Record the buffer blob for next time.
-    set_scratch_buffer_blob(blob);
-    // Have we run out of code space?
-    if (scratch_buffer_blob() == NULL) {
-      // Let CompilerBroker disable further compilations.
-      record_failure("Not enough space for scratch buffer in CodeCache");
-      return;
-    }
-  }
-
-  // Initialize the relocation buffers
-  relocInfo* locs_buf = (relocInfo*) blob->content_end() - MAX_locs_size;
-  set_scratch_locs_memory(locs_buf);
-}
-
-
-//-----------------------scratch_emit_size-------------------------------------
-// Helper function that computes size by emitting code
-uint Compile::scratch_emit_size(const Node* n) {
-  // Start scratch_emit_size section.
-  set_in_scratch_emit_size(true);
-
-  // Emit into a trash buffer and count bytes emitted.
-  // This is a pretty expensive way to compute a size,
-  // but it works well enough if seldom used.
-  // All common fixed-size instructions are given a size
-  // method by the AD file.
-  // Note that the scratch buffer blob and locs memory are
-  // allocated at the beginning of the compile task, and
-  // may be shared by several calls to scratch_emit_size.
-  // The allocation of the scratch buffer blob is particularly
-  // expensive, since it has to grab the code cache lock.
-  BufferBlob* blob = this->scratch_buffer_blob();
-  assert(blob != NULL, "Initialize BufferBlob at start");
-  assert(blob->size() > MAX_inst_size, "sanity");
-  relocInfo* locs_buf = scratch_locs_memory();
-  address blob_begin = blob->content_begin();
-  address blob_end   = (address)locs_buf;
-  assert(blob->contains(blob_end), "sanity");
-  CodeBuffer buf(blob_begin, blob_end - blob_begin);
-  buf.initialize_consts_size(_scratch_const_size);
-  buf.initialize_stubs_size(MAX_stubs_size);
-  assert(locs_buf != NULL, "sanity");
-  int lsize = MAX_locs_size / 3;
-  buf.consts()->initialize_shared_locs(&locs_buf[lsize * 0], lsize);
-  buf.insts()->initialize_shared_locs( &locs_buf[lsize * 1], lsize);
-  buf.stubs()->initialize_shared_locs( &locs_buf[lsize * 2], lsize);
-  // Mark as scratch buffer.
-  buf.consts()->set_scratch_emit();
-  buf.insts()->set_scratch_emit();
-  buf.stubs()->set_scratch_emit();
-
-  // Do the emission.
-
-  Label fakeL; // Fake label for branch instructions.
-  Label*   saveL = NULL;
-  uint save_bnum = 0;
-  bool is_branch = n->is_MachBranch();
-  if (is_branch) {
-    MacroAssembler masm(&buf);
-    masm.bind(fakeL);
-    n->as_MachBranch()->save_label(&saveL, &save_bnum);
-    n->as_MachBranch()->label_set(&fakeL, 0);
-  }
-  n->emit(buf, this->regalloc());
-
-  // Emitting into the scratch buffer should not fail
-  assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
-
-  if (is_branch) // Restore label.
-    n->as_MachBranch()->label_set(saveL, save_bnum);
-
-  // End scratch_emit_size section.
-  set_in_scratch_emit_size(false);
-
-  return buf.insts_size();
-}
-
-
 // ============================================================================
 //------------------------------Compile standard-------------------------------
 debug_only( int Compile::_debug_idx = 100000; )
 
 // Compile a method.  entry_bci is -1 for normal compilations and indicates
 // the continuation bci for on stack replacement.
 
 
-Compile::Compile( ciEnv* ci_env, C2Compiler* compiler, ciMethod* target, int osr_bci,
+Compile::Compile( ciEnv* ci_env, ciMethod* target, int osr_bci,
                   bool subsume_loads, bool do_escape_analysis, bool eliminate_boxing, DirectiveSet* directive)
                 : Phase(Compiler),
                   _compile_id(ci_env->compile_id()),
                   _save_argument_registers(false),
                   _subsume_loads(subsume_loads),
@@ -638,12 +509,10 @@
                   _entry_bci(osr_bci),
                   _stub_function(NULL),
                   _stub_name(NULL),
                   _stub_entry_point(NULL),
                   _max_node_limit(MaxNodeLimit),
-                  _orig_pc_slot(0),
-                  _orig_pc_slot_offset_in_bytes(0),
                   _inlining_progress(false),
                   _inlining_incrementally(false),
                   _do_cleanup(false),
                   _has_reserved_stack_access(target->has_reserved_stack_access()),
 #ifndef PRODUCT
@@ -681,16 +550,11 @@
                   _print_inlining_idx(0),
                   _print_inlining_output(NULL),
                   _replay_inline_data(NULL),
                   _java_calls(0),
                   _inner_loops(0),
-                  _interpreter_frame_size(0),
-                  _node_bundling_limit(0),
-                  _node_bundling_base(NULL),
-                  _code_buffer("Compile::Fill_buffer"),
-                  _scratch_const_size(-1),
-                  _in_scratch_emit_size(false)
+                  _interpreter_frame_size(0)
 #ifndef PRODUCT
                   , _in_dump_cnt(0)
 #endif
 {
   C = this;
@@ -904,56 +768,19 @@
     env()->dump_inline_data(_compile_id);
   }
 
   // Now that we know the size of all the monitors we can add a fixed slot
   // for the original deopt pc.
-
-  _orig_pc_slot =  fixed_slots();
-  int next_slot = _orig_pc_slot + (sizeof(address) / VMRegImpl::stack_slot_size);
+  int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
   set_fixed_slots(next_slot);
 
   // Compute when to use implicit null checks. Used by matching trap based
   // nodes and NullCheck optimization.
   set_allowed_deopt_reasons();
 
   // Now generate code
   Code_Gen();
-  if (failing())  return;
-
-  // Check if we want to skip execution of all compiled code.
-  {
-#ifndef PRODUCT
-    if (OptoNoExecute) {
-      record_method_not_compilable("+OptoNoExecute");  // Flag as failed
-      return;
-    }
-#endif
-    TracePhase tp("install_code", &timers[_t_registerMethod]);
-
-    if (is_osr_compilation()) {
-      _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
-      _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
-    } else {
-      _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
-      _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
-    }
-
-    env()->register_method(_method, _entry_bci,
-                           &_code_offsets,
-                           _orig_pc_slot_offset_in_bytes,
-                           code_buffer(),
-                           frame_size_in_words(), _oop_map_set,
-                           &_handler_table, &_inc_table,
-                           compiler,
-                           has_unsafe_access(),
-                           SharedRuntime::is_wide_vector(max_vector_size()),
-                           rtm_state()
-                           );
-
-    if (log() != NULL) // Print code cache state into compiler log
-      log()->code_cache_state();
-  }
 }
 
 //------------------------------Compile----------------------------------------
 // Compile a runtime stub
 Compile::Compile( ciEnv* ci_env,
@@ -975,12 +802,10 @@
     _entry_bci(InvocationEntryBci),
     _stub_function(stub_function),
     _stub_name(stub_name),
     _stub_entry_point(NULL),
     _max_node_limit(MaxNodeLimit),
-    _orig_pc_slot(0),
-    _orig_pc_slot_offset_in_bytes(0),
     _inlining_progress(false),
     _inlining_incrementally(false),
     _has_reserved_stack_access(false),
 #ifndef PRODUCT
     _trace_opto_output(directive->TraceOptoOutputOption),
@@ -1014,13 +839,10 @@
     _print_inlining_output(NULL),
     _replay_inline_data(NULL),
     _java_calls(0),
     _inner_loops(0),
     _interpreter_frame_size(0),
-    _node_bundling_limit(0),
-    _node_bundling_base(NULL),
-    _code_buffer("Compile::Fill_buffer"),
 #ifndef PRODUCT
     _in_dump_cnt(0),
 #endif
     _allowed_reasons(0) {
   C = this;
@@ -1051,38 +873,12 @@
     GraphKit kit;
     kit.gen_stub(stub_function, stub_name, is_fancy_jump, pass_tls, return_pc);
   }
 
   NOT_PRODUCT( verify_graph_edges(); )
-  Code_Gen();
-  if (failing())  return;
-
-
-  // Entry point will be accessed using compile->stub_entry_point();
-  if (code_buffer() == NULL) {
-    Matcher::soft_match_failure();
-  } else {
-    if (PrintAssembly && (WizardMode || Verbose))
-      tty->print_cr("### Stub::%s", stub_name);
 
-    if (!failing()) {
-      assert(_fixed_slots == 0, "no fixed slots used for runtime stubs");
-
-      // Make the NMethod
-      // For now we mark the frame as never safe for profile stackwalking
-      RuntimeStub *rs = RuntimeStub::new_runtime_stub(stub_name,
-                                                      code_buffer(),
-                                                      CodeOffsets::frame_never_safe,
-                                                      // _code_offsets.value(CodeOffsets::Frame_Complete),
-                                                      frame_size_in_words(),
-                                                      _oop_map_set,
-                                                      save_arg_registers);
-      assert(rs != NULL && rs->is_runtime_stub(), "sanity check");
-
-      _stub_entry_point = rs->entry_point();
-    }
-  }
+  Code_Gen();
 }
 
 //------------------------------Init-------------------------------------------
 // Prepare for a single compilation
 void Compile::Init(int aliaslevel) {
@@ -1212,10 +1008,13 @@
   _predicate_opaqs = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _expensive_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _range_check_casts = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   register_library_intrinsics();
+#ifdef ASSERT
+  _type_verify_symmetry = true;
+#endif
 }
 
 //---------------------------init_start----------------------------------------
 // Install the StartNode on this compile object.
 void Compile::init_start(StartNode* s) {
@@ -2428,10 +2227,15 @@
   if (opaque4_count() > 0) {
     C->remove_opaque4_nodes(igvn);
     igvn.optimize();
   }
 
+  if (C->max_vector_size() > 0) {
+    C->optimize_logic_cones(igvn);
+    igvn.optimize();
+  }
+
   DEBUG_ONLY( _modified_nodes = NULL; )
  } // (End scope of igvn; run destructor if necessary for asserts.)
 
  process_print_inlining();
  // A method with only infinite loops has no edges entering loops from root
@@ -2444,10 +2248,321 @@
  }
 
  print_method(PHASE_OPTIMIZE_FINISHED, 2);
 }
 
+//---------------------------- Bitwise operation packing optimization ---------------------------
+
+static bool is_vector_unary_bitwise_op(Node* n) {
+  return n->Opcode() == Op_XorV &&
+         VectorNode::is_vector_bitwise_not_pattern(n);
+}
+
+static bool is_vector_binary_bitwise_op(Node* n) {
+  switch (n->Opcode()) {
+    case Op_AndV:
+    case Op_OrV:
+      return true;
+
+    case Op_XorV:
+      return !is_vector_unary_bitwise_op(n);
+
+    default:
+      return false;
+  }
+}
+
+static bool is_vector_ternary_bitwise_op(Node* n) {
+  return n->Opcode() == Op_MacroLogicV;
+}
+
+static bool is_vector_bitwise_op(Node* n) {
+  return is_vector_unary_bitwise_op(n)  ||
+         is_vector_binary_bitwise_op(n) ||
+         is_vector_ternary_bitwise_op(n);
+}
+
+static bool is_vector_bitwise_cone_root(Node* n) {
+  if (!is_vector_bitwise_op(n)) {
+    return false;
+  }
+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {
+    if (is_vector_bitwise_op(n->fast_out(i))) {
+      return false;
+    }
+  }
+  return true;
+}
+
+static uint collect_unique_inputs(Node* n, Unique_Node_List& partition, Unique_Node_List& inputs) {
+  uint cnt = 0;
+  if (is_vector_bitwise_op(n)) {
+    if (VectorNode::is_vector_bitwise_not_pattern(n)) {
+      for (uint i = 1; i < n->req(); i++) {
+        Node* in = n->in(i);
+        bool skip = VectorNode::is_all_ones_vector(in);
+        if (!skip && !inputs.member(in)) {
+          inputs.push(in);
+          cnt++;
+        }
+      }
+      assert(cnt <= 1, "not unary");
+    } else {
+      uint last_req = n->req();
+      if (is_vector_ternary_bitwise_op(n)) {
+        last_req = n->req() - 1; // skip last input
+      }
+      for (uint i = 1; i < last_req; i++) {
+        Node* def = n->in(i);
+        if (!inputs.member(def)) {
+          inputs.push(def);
+          cnt++;
+        }
+      }
+    }
+    partition.push(n);
+  } else { // not a bitwise operations
+    if (!inputs.member(n)) {
+      inputs.push(n);
+      cnt++;
+    }
+  }
+  return cnt;
+}
+
+void Compile::collect_logic_cone_roots(Unique_Node_List& list) {
+  Unique_Node_List useful_nodes;
+  C->identify_useful_nodes(useful_nodes);
+
+  for (uint i = 0; i < useful_nodes.size(); i++) {
+    Node* n = useful_nodes.at(i);
+    if (is_vector_bitwise_cone_root(n)) {
+      list.push(n);
+    }
+  }
+}
+
+Node* Compile::xform_to_MacroLogicV(PhaseIterGVN& igvn,
+                                    const TypeVect* vt,
+                                    Unique_Node_List& partition,
+                                    Unique_Node_List& inputs) {
+  assert(partition.size() == 2 || partition.size() == 3, "not supported");
+  assert(inputs.size()    == 2 || inputs.size()    == 3, "not supported");
+  assert(Matcher::match_rule_supported_vector(Op_MacroLogicV, vt->length(), vt->element_basic_type()), "not supported");
+
+  Node* in1 = inputs.at(0);
+  Node* in2 = inputs.at(1);
+  Node* in3 = (inputs.size() == 3 ? inputs.at(2) : in2);
+
+  uint func = compute_truth_table(partition, inputs);
+  return igvn.transform(MacroLogicVNode::make(igvn, in3, in2, in1, func, vt));
+}
+
+static uint extract_bit(uint func, uint pos) {
+  return (func & (1 << pos)) >> pos;
+}
+
+//
+//  A macro logic node represents a truth table. It has 4 inputs,
+//  First three inputs corresponds to 3 columns of a truth table
+//  and fourth input captures the logic function.
+//
+//  eg.  fn = (in1 AND in2) OR in3;
+//
+//      MacroNode(in1,in2,in3,fn)
+//
+//  -----------------
+//  in1 in2 in3  fn
+//  -----------------
+//  0    0   0    0
+//  0    0   1    1
+//  0    1   0    0
+//  0    1   1    1
+//  1    0   0    0
+//  1    0   1    1
+//  1    1   0    1
+//  1    1   1    1
+//
+
+uint Compile::eval_macro_logic_op(uint func, uint in1 , uint in2, uint in3) {
+  int res = 0;
+  for (int i = 0; i < 8; i++) {
+    int bit1 = extract_bit(in1, i);
+    int bit2 = extract_bit(in2, i);
+    int bit3 = extract_bit(in3, i);
+
+    int func_bit_pos = (bit1 << 2 | bit2 << 1 | bit3);
+    int func_bit = extract_bit(func, func_bit_pos);
+
+    res |= func_bit << i;
+  }
+  return res;
+}
+
+static uint eval_operand(Node* n, ResourceHashtable<Node*,uint>& eval_map) {
+  assert(n != NULL, "");
+  assert(eval_map.contains(n), "absent");
+  return *(eval_map.get(n));
+}
+
+static void eval_operands(Node* n,
+                          uint& func1, uint& func2, uint& func3,
+                          ResourceHashtable<Node*,uint>& eval_map) {
+  assert(is_vector_bitwise_op(n), "");
+  func1 = eval_operand(n->in(1), eval_map);
+
+  if (is_vector_binary_bitwise_op(n)) {
+    func2 = eval_operand(n->in(2), eval_map);
+  } else if (is_vector_ternary_bitwise_op(n)) {
+    func2 = eval_operand(n->in(2), eval_map);
+    func3 = eval_operand(n->in(3), eval_map);
+  } else {
+    assert(is_vector_unary_bitwise_op(n), "not unary");
+  }
+}
+
+uint Compile::compute_truth_table(Unique_Node_List& partition, Unique_Node_List& inputs) {
+  assert(inputs.size() <= 3, "sanity");
+  ResourceMark rm;
+  uint res = 0;
+  ResourceHashtable<Node*,uint> eval_map;
+
+  // Populate precomputed functions for inputs.
+  // Each input corresponds to one column of 3 input truth-table.
+  uint input_funcs[] = { 0xAA,   // (_, _, a) -> a
+                         0xCC,   // (_, b, _) -> b
+                         0xF0 }; // (c, _, _) -> c
+  for (uint i = 0; i < inputs.size(); i++) {
+    eval_map.put(inputs.at(i), input_funcs[i]);
+  }
+
+  for (uint i = 0; i < partition.size(); i++) {
+    Node* n = partition.at(i);
+
+    uint func1 = 0, func2 = 0, func3 = 0;
+    eval_operands(n, func1, func2, func3, eval_map);
+
+    switch (n->Opcode()) {
+      case Op_OrV:
+        assert(func3 == 0, "not binary");
+        res = func1 | func2;
+        break;
+      case Op_AndV:
+        assert(func3 == 0, "not binary");
+        res = func1 & func2;
+        break;
+      case Op_XorV:
+        if (VectorNode::is_vector_bitwise_not_pattern(n)) {
+          assert(func2 == 0 && func3 == 0, "not unary");
+          res = (~func1) & 0xFF;
+        } else {
+          assert(func3 == 0, "not binary");
+          res = func1 ^ func2;
+        }
+        break;
+      case Op_MacroLogicV:
+        // Ordering of inputs may change during evaluation of sub-tree
+        // containing MacroLogic node as a child node, thus a re-evaluation
+        // makes sure that function is evaluated in context of current
+        // inputs.
+        res = eval_macro_logic_op(n->in(4)->get_int(), func1, func2, func3);
+        break;
+
+      default: assert(false, "not supported: %s", n->Name());
+    }
+    assert(res <= 0xFF, "invalid");
+    eval_map.put(n, res);
+  }
+  return res;
+}
+
+bool Compile::compute_logic_cone(Node* n, Unique_Node_List& partition, Unique_Node_List& inputs) {
+  assert(partition.size() == 0, "not empty");
+  assert(inputs.size() == 0, "not empty");
+  assert(!is_vector_ternary_bitwise_op(n), "not supported");
+
+  bool is_unary_op = is_vector_unary_bitwise_op(n);
+  if (is_unary_op) {
+    assert(collect_unique_inputs(n, partition, inputs) == 1, "not unary");
+    return false; // too few inputs
+  }
+
+  assert(is_vector_binary_bitwise_op(n), "not binary");
+  Node* in1 = n->in(1);
+  Node* in2 = n->in(2);
+
+  int in1_unique_inputs_cnt = collect_unique_inputs(in1, partition, inputs);
+  int in2_unique_inputs_cnt = collect_unique_inputs(in2, partition, inputs);
+  partition.push(n);
+
+  // Too many inputs?
+  if (inputs.size() > 3) {
+    partition.clear();
+    inputs.clear();
+    { // Recompute in2 inputs
+      Unique_Node_List not_used;
+      in2_unique_inputs_cnt = collect_unique_inputs(in2, not_used, not_used);
+    }
+    // Pick the node with minimum number of inputs.
+    if (in1_unique_inputs_cnt >= 3 && in2_unique_inputs_cnt >= 3) {
+      return false; // still too many inputs
+    }
+    // Recompute partition & inputs.
+    Node* child       = (in1_unique_inputs_cnt < in2_unique_inputs_cnt ? in1 : in2);
+    collect_unique_inputs(child, partition, inputs);
+
+    Node* other_input = (in1_unique_inputs_cnt < in2_unique_inputs_cnt ? in2 : in1);
+    inputs.push(other_input);
+
+    partition.push(n);
+  }
+
+  return (partition.size() == 2 || partition.size() == 3) &&
+         (inputs.size()    == 2 || inputs.size()    == 3);
+}
+
+void Compile::process_logic_cone_root(PhaseIterGVN &igvn, Node *n, VectorSet &visited) {
+  assert(is_vector_bitwise_op(n), "not a root");
+
+  visited.set(n->_idx);
+
+  // 1) Do a DFS walk over the logic cone.
+  for (uint i = 1; i < n->req(); i++) {
+    Node* in = n->in(i);
+    if (!visited.test(in->_idx) && is_vector_bitwise_op(in)) {
+      process_logic_cone_root(igvn, in, visited);
+    }
+  }
+
+  // 2) Bottom up traversal: Merge node[s] with
+  // the parent to form macro logic node.
+  Unique_Node_List partition;
+  Unique_Node_List inputs;
+  if (compute_logic_cone(n, partition, inputs)) {
+    const TypeVect* vt = n->bottom_type()->is_vect();
+    Node* macro_logic = xform_to_MacroLogicV(igvn, vt, partition, inputs);
+    igvn.replace_node(n, macro_logic);
+  }
+}
+
+void Compile::optimize_logic_cones(PhaseIterGVN &igvn) {
+  ResourceMark rm;
+  if (Matcher::match_rule_supported(Op_MacroLogicV)) {
+    Unique_Node_List list;
+    collect_logic_cone_roots(list);
+
+    while (list.size() > 0) {
+      Node* n = list.pop();
+      const TypeVect* vt = n->bottom_type()->is_vect();
+      bool supported = Matcher::match_rule_supported_vector(Op_MacroLogicV, vt->length(), vt->element_basic_type());
+      if (supported) {
+        VectorSet visited(comp_arena());
+        process_logic_cone_root(igvn, n, visited);
+      }
+    }
+  }
+}
 
 //------------------------------Code_Gen---------------------------------------
 // Given a graph, generate code for it
 void Compile::Code_Gen() {
   if (failing()) {
@@ -2544,158 +2659,24 @@
     cfg.postalloc_expand(_regalloc);
   }
 
   // Convert Nodes to instruction bits in a buffer
   {
-    TraceTime tp("output", &timers[_t_output], CITime);
-    Output();
+    TracePhase tp("output", &timers[_t_output]);
+    PhaseOutput output;
+    output.Output();
+    if (failing())  return;
+    output.install();
   }
 
   print_method(PHASE_FINAL_CODE);
 
   // He's dead, Jim.
   _cfg     = (PhaseCFG*)((intptr_t)0xdeadbeef);
   _regalloc = (PhaseChaitin*)((intptr_t)0xdeadbeef);
 }
 
-
-//------------------------------dump_asm---------------------------------------
-// Dump formatted assembly
-#if defined(SUPPORT_OPTO_ASSEMBLY)
-void Compile::dump_asm_on(outputStream* st, int* pcs, uint pc_limit) {
-
-  int pc_digits = 3; // #chars required for pc
-  int sb_chars  = 3; // #chars for "start bundle" indicator
-  int tab_size  = 8;
-  if (pcs != NULL) {
-    int max_pc = 0;
-    for (uint i = 0; i < pc_limit; i++) {
-      max_pc = (max_pc < pcs[i]) ? pcs[i] : max_pc;
-    }
-    pc_digits  = ((max_pc < 4096) ? 3 : ((max_pc < 65536) ? 4 : ((max_pc < 65536*256) ? 6 : 8))); // #chars required for pc
-  }
-  int prefix_len = ((pc_digits + sb_chars + tab_size - 1)/tab_size)*tab_size;
-
-  bool cut_short = false;
-  st->print_cr("#");
-  st->print("#  ");  _tf->dump_on(st);  st->cr();
-  st->print_cr("#");
-
-  // For all blocks
-  int pc = 0x0;                 // Program counter
-  char starts_bundle = ' ';
-  _regalloc->dump_frame();
-
-  Node *n = NULL;
-  for (uint i = 0; i < _cfg->number_of_blocks(); i++) {
-    if (VMThread::should_terminate()) {
-      cut_short = true;
-      break;
-    }
-    Block* block = _cfg->get_block(i);
-    if (block->is_connector() && !Verbose) {
-      continue;
-    }
-    n = block->head();
-    if ((pcs != NULL) && (n->_idx < pc_limit)) {
-      pc = pcs[n->_idx];
-      st->print("%*.*x", pc_digits, pc_digits, pc);
-    }
-    st->fill_to(prefix_len);
-    block->dump_head(_cfg, st);
-    if (block->is_connector()) {
-      st->fill_to(prefix_len);
-      st->print_cr("# Empty connector block");
-    } else if (block->num_preds() == 2 && block->pred(1)->is_CatchProj() && block->pred(1)->as_CatchProj()->_con == CatchProjNode::fall_through_index) {
-      st->fill_to(prefix_len);
-      st->print_cr("# Block is sole successor of call");
-    }
-
-    // For all instructions
-    Node *delay = NULL;
-    for (uint j = 0; j < block->number_of_nodes(); j++) {
-      if (VMThread::should_terminate()) {
-        cut_short = true;
-        break;
-      }
-      n = block->get_node(j);
-      if (valid_bundle_info(n)) {
-        Bundle* bundle = node_bundling(n);
-        if (bundle->used_in_unconditional_delay()) {
-          delay = n;
-          continue;
-        }
-        if (bundle->starts_bundle()) {
-          starts_bundle = '+';
-        }
-      }
-
-      if (WizardMode) {
-        n->dump();
-      }
-
-      if( !n->is_Region() &&    // Dont print in the Assembly
-          !n->is_Phi() &&       // a few noisely useless nodes
-          !n->is_Proj() &&
-          !n->is_MachTemp() &&
-          !n->is_SafePointScalarObject() &&
-          !n->is_Catch() &&     // Would be nice to print exception table targets
-          !n->is_MergeMem() &&  // Not very interesting
-          !n->is_top() &&       // Debug info table constants
-          !(n->is_Con() && !n->is_Mach())// Debug info table constants
-          ) {
-        if ((pcs != NULL) && (n->_idx < pc_limit)) {
-          pc = pcs[n->_idx];
-          st->print("%*.*x", pc_digits, pc_digits, pc);
-        } else {
-          st->fill_to(pc_digits);
-        }
-        st->print(" %c ", starts_bundle);
-        starts_bundle = ' ';
-        st->fill_to(prefix_len);
-        n->format(_regalloc, st);
-        st->cr();
-      }
-
-      // If we have an instruction with a delay slot, and have seen a delay,
-      // then back up and print it
-      if (valid_bundle_info(n) && node_bundling(n)->use_unconditional_delay()) {
-        // Coverity finding - Explicit null dereferenced.
-        guarantee(delay != NULL, "no unconditional delay instruction");
-        if (WizardMode) delay->dump();
-
-        if (node_bundling(delay)->starts_bundle())
-          starts_bundle = '+';
-        if ((pcs != NULL) && (n->_idx < pc_limit)) {
-          pc = pcs[n->_idx];
-          st->print("%*.*x", pc_digits, pc_digits, pc);
-        } else {
-          st->fill_to(pc_digits);
-        }
-        st->print(" %c ", starts_bundle);
-        starts_bundle = ' ';
-        st->fill_to(prefix_len);
-        delay->format(_regalloc, st);
-        st->cr();
-        delay = NULL;
-      }
-
-      // Dump the exception table as well
-      if( n->is_Catch() && (Verbose || WizardMode) ) {
-        // Print the exception table for this offset
-        _handler_table.print_subtable_for(pc);
-      }
-      st->bol(); // Make sure we start on a new line
-    }
-    st->cr(); // one empty line between blocks
-    assert(cut_short || delay == NULL, "no unconditional delay branch");
-  } // End of per-block dump
-
-  if (cut_short)  st->print_cr("*** disassembly is cut short ***");
-}
-#endif
-
 //------------------------------Final_Reshape_Counts---------------------------
 // This class defines counters to help identify when a method
 // may/must be executed using hardware with only 24-bit precision.
 struct Final_Reshape_Counts : public StackObj {
   int  _call_count;             // count non-inlined 'common' calls
@@ -3371,10 +3352,13 @@
   case Op_MulReductionVL:
   case Op_MulReductionVF:
   case Op_MulReductionVD:
   case Op_MinReductionV:
   case Op_MaxReductionV:
+  case Op_AndReductionV:
+  case Op_OrReductionV:
+  case Op_XorReductionV:
     break;
 
   case Op_PackB:
   case Op_PackS:
   case Op_PackI:
@@ -3984,226 +3968,10 @@
   if (_log != NULL) {
     _log->done("phase name='%s' nodes='%d' live='%d'", _phase_name, C->unique(), C->live_nodes());
   }
 }
 
-//=============================================================================
-// Two Constant's are equal when the type and the value are equal.
-bool Compile::Constant::operator==(const Constant& other) {
-  if (type()          != other.type()         )  return false;
-  if (can_be_reused() != other.can_be_reused())  return false;
-  // For floating point values we compare the bit pattern.
-  switch (type()) {
-  case T_INT:
-  case T_FLOAT:   return (_v._value.i == other._v._value.i);
-  case T_LONG:
-  case T_DOUBLE:  return (_v._value.j == other._v._value.j);
-  case T_OBJECT:
-  case T_ADDRESS: return (_v._value.l == other._v._value.l);
-  case T_VOID:    return (_v._value.l == other._v._value.l);  // jump-table entries
-  case T_METADATA: return (_v._metadata == other._v._metadata);
-  default: ShouldNotReachHere(); return false;
-  }
-}
-
-static int type_to_size_in_bytes(BasicType t) {
-  switch (t) {
-  case T_INT:     return sizeof(jint   );
-  case T_LONG:    return sizeof(jlong  );
-  case T_FLOAT:   return sizeof(jfloat );
-  case T_DOUBLE:  return sizeof(jdouble);
-  case T_METADATA: return sizeof(Metadata*);
-    // We use T_VOID as marker for jump-table entries (labels) which
-    // need an internal word relocation.
-  case T_VOID:
-  case T_ADDRESS:
-  case T_OBJECT:  return sizeof(jobject);
-  default:
-    ShouldNotReachHere();
-    return -1;
-  }
-}
-
-int Compile::ConstantTable::qsort_comparator(Constant* a, Constant* b) {
-  // sort descending
-  if (a->freq() > b->freq())  return -1;
-  if (a->freq() < b->freq())  return  1;
-  return 0;
-}
-
-void Compile::ConstantTable::calculate_offsets_and_size() {
-  // First, sort the array by frequencies.
-  _constants.sort(qsort_comparator);
-
-#ifdef ASSERT
-  // Make sure all jump-table entries were sorted to the end of the
-  // array (they have a negative frequency).
-  bool found_void = false;
-  for (int i = 0; i < _constants.length(); i++) {
-    Constant con = _constants.at(i);
-    if (con.type() == T_VOID)
-      found_void = true;  // jump-tables
-    else
-      assert(!found_void, "wrong sorting");
-  }
-#endif
-
-  int offset = 0;
-  for (int i = 0; i < _constants.length(); i++) {
-    Constant* con = _constants.adr_at(i);
-
-    // Align offset for type.
-    int typesize = type_to_size_in_bytes(con->type());
-    offset = align_up(offset, typesize);
-    con->set_offset(offset);   // set constant's offset
-
-    if (con->type() == T_VOID) {
-      MachConstantNode* n = (MachConstantNode*) con->get_jobject();
-      offset = offset + typesize * n->outcnt();  // expand jump-table
-    } else {
-      offset = offset + typesize;
-    }
-  }
-
-  // Align size up to the next section start (which is insts; see
-  // CodeBuffer::align_at_start).
-  assert(_size == -1, "already set?");
-  _size = align_up(offset, (int)CodeEntryAlignment);
-}
-
-void Compile::ConstantTable::emit(CodeBuffer& cb) {
-  MacroAssembler _masm(&cb);
-  for (int i = 0; i < _constants.length(); i++) {
-    Constant con = _constants.at(i);
-    address constant_addr = NULL;
-    switch (con.type()) {
-    case T_INT:    constant_addr = _masm.int_constant(   con.get_jint()   ); break;
-    case T_LONG:   constant_addr = _masm.long_constant(  con.get_jlong()  ); break;
-    case T_FLOAT:  constant_addr = _masm.float_constant( con.get_jfloat() ); break;
-    case T_DOUBLE: constant_addr = _masm.double_constant(con.get_jdouble()); break;
-    case T_OBJECT: {
-      jobject obj = con.get_jobject();
-      int oop_index = _masm.oop_recorder()->find_index(obj);
-      constant_addr = _masm.address_constant((address) obj, oop_Relocation::spec(oop_index));
-      break;
-    }
-    case T_ADDRESS: {
-      address addr = (address) con.get_jobject();
-      constant_addr = _masm.address_constant(addr);
-      break;
-    }
-    // We use T_VOID as marker for jump-table entries (labels) which
-    // need an internal word relocation.
-    case T_VOID: {
-      MachConstantNode* n = (MachConstantNode*) con.get_jobject();
-      // Fill the jump-table with a dummy word.  The real value is
-      // filled in later in fill_jump_table.
-      address dummy = (address) n;
-      constant_addr = _masm.address_constant(dummy);
-      // Expand jump-table
-      for (uint i = 1; i < n->outcnt(); i++) {
-        address temp_addr = _masm.address_constant(dummy + i);
-        assert(temp_addr, "consts section too small");
-      }
-      break;
-    }
-    case T_METADATA: {
-      Metadata* obj = con.get_metadata();
-      int metadata_index = _masm.oop_recorder()->find_index(obj);
-      constant_addr = _masm.address_constant((address) obj, metadata_Relocation::spec(metadata_index));
-      break;
-    }
-    default: ShouldNotReachHere();
-    }
-    assert(constant_addr, "consts section too small");
-    assert((constant_addr - _masm.code()->consts()->start()) == con.offset(),
-            "must be: %d == %d", (int) (constant_addr - _masm.code()->consts()->start()), (int)(con.offset()));
-  }
-}
-
-int Compile::ConstantTable::find_offset(Constant& con) const {
-  int idx = _constants.find(con);
-  guarantee(idx != -1, "constant must be in constant table");
-  int offset = _constants.at(idx).offset();
-  guarantee(offset != -1, "constant table not emitted yet?");
-  return offset;
-}
-
-void Compile::ConstantTable::add(Constant& con) {
-  if (con.can_be_reused()) {
-    int idx = _constants.find(con);
-    if (idx != -1 && _constants.at(idx).can_be_reused()) {
-      _constants.adr_at(idx)->inc_freq(con.freq());  // increase the frequency by the current value
-      return;
-    }
-  }
-  (void) _constants.append(con);
-}
-
-Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, BasicType type, jvalue value) {
-  Block* b = Compile::current()->cfg()->get_block_for_node(n);
-  Constant con(type, value, b->_freq);
-  add(con);
-  return con;
-}
-
-Compile::Constant Compile::ConstantTable::add(Metadata* metadata) {
-  Constant con(metadata);
-  add(con);
-  return con;
-}
-
-Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, MachOper* oper) {
-  jvalue value;
-  BasicType type = oper->type()->basic_type();
-  switch (type) {
-  case T_LONG:    value.j = oper->constantL(); break;
-  case T_FLOAT:   value.f = oper->constantF(); break;
-  case T_DOUBLE:  value.d = oper->constantD(); break;
-  case T_OBJECT:
-  case T_ADDRESS: value.l = (jobject) oper->constant(); break;
-  case T_METADATA: return add((Metadata*)oper->constant()); break;
-  default: guarantee(false, "unhandled type: %s", type2name(type));
-  }
-  return add(n, type, value);
-}
-
-Compile::Constant Compile::ConstantTable::add_jump_table(MachConstantNode* n) {
-  jvalue value;
-  // We can use the node pointer here to identify the right jump-table
-  // as this method is called from Compile::Fill_buffer right before
-  // the MachNodes are emitted and the jump-table is filled (means the
-  // MachNode pointers do not change anymore).
-  value.l = (jobject) n;
-  Constant con(T_VOID, value, next_jump_table_freq(), false);  // Labels of a jump-table cannot be reused.
-  add(con);
-  return con;
-}
-
-void Compile::ConstantTable::fill_jump_table(CodeBuffer& cb, MachConstantNode* n, GrowableArray<Label*> labels) const {
-  // If called from Compile::scratch_emit_size do nothing.
-  if (Compile::current()->in_scratch_emit_size())  return;
-
-  assert(labels.is_nonempty(), "must be");
-  assert((uint) labels.length() == n->outcnt(), "must be equal: %d == %d", labels.length(), n->outcnt());
-
-  // Since MachConstantNode::constant_offset() also contains
-  // table_base_offset() we need to subtract the table_base_offset()
-  // to get the plain offset into the constant table.
-  int offset = n->constant_offset() - table_base_offset();
-
-  MacroAssembler _masm(&cb);
-  address* jump_table_base = (address*) (_masm.code()->consts()->start() + offset);
-
-  for (uint i = 0; i < n->outcnt(); i++) {
-    address* constant_addr = &jump_table_base[i];
-    assert(*constant_addr == (((address) n) + i), "all jump-table entries must contain adjusted node pointer: " INTPTR_FORMAT " == " INTPTR_FORMAT, p2i(*constant_addr), p2i(((address) n) + i));
-    *constant_addr = cb.consts()->target(*labels.at(i), (address) constant_addr);
-    cb.consts()->relocate((address) constant_addr, relocInfo::internal_word_type);
-  }
-}
-
 //----------------------------static_subtype_check-----------------------------
 // Shortcut important common cases when superklass is exact:
 // (0) superklass is java.lang.Object (can occur in reflective code)
 // (1) subklass is already limited to a subtype of superklass => always ok
 // (2) subklass does not overlap with superklass => always fail
@@ -4240,13 +4008,10 @@
     if (!ik->has_subklass() && !ik->is_interface()) {
       if (!ik->is_final()) {
         // Add a dependency if there is a chance of a later subclass.
         dependencies()->assert_leaf_type(ik);
       }
-      if (ik->is_abstract()) {
-        return SSC_always_false;
-      }
       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
     }
   } else {
     // A primitive array type has no subtypes.
     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
@@ -4774,5 +4539,23 @@
   if (val != 0) {
     NodeCloneInfo ni(val);
     ni.dump();
   }
 }
+
+
+// Move Allocate nodes to the start of the list
+void Compile::sort_macro_nodes() {
+  int count = macro_count();
+  int allocates = 0;
+  for (int i = 0; i < count; i++) {
+    Node* n = macro_node(i);
+    if (n->is_Allocate()) {
+      if (i != allocates) {
+        Node* tmp = macro_node(allocates);
+        _macro_nodes->at_put(allocates, n);
+        _macro_nodes->at_put(i, tmp);
+      }
+      allocates++;
+    }
+  }
+}
