<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/synchronizer.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sweeper.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="thread.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/synchronizer.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1264   // Induce STW safepoint to trim monitors
1265   // Ultimately, this results in a call to deflate_idle_monitors() in the near future.
1266   // More precisely, trigger a cleanup safepoint as the number
1267   // of active monitors passes the specified threshold.
1268   // TODO: assert thread state is reasonable
1269 
1270   if (Atomic::xchg(&amp;_forceMonitorScavenge, 1) == 0) {
1271     VMThread::check_for_forced_cleanup();
1272   }
1273 }
1274 
1275 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1276   // A large MAXPRIVATE value reduces both list lock contention
1277   // and list coherency traffic, but also tends to increase the
1278   // number of ObjectMonitors in circulation as well as the STW
1279   // scavenge costs.  As usual, we lean toward time in space-time
1280   // tradeoffs.
1281   const int MAXPRIVATE = 1024;
1282   NoSafepointVerifier nsv;
1283 
<span class="line-removed">1284   stringStream ss;</span>
1285   for (;;) {
1286     ObjectMonitor* m;
1287 
1288     // 1: try to allocate from the thread&#39;s local om_free_list.
1289     // Threads will attempt to allocate first from their local list, then
1290     // from the global list, and only after those attempts fail will the
1291     // thread attempt to instantiate new monitors. Thread-local free lists
1292     // improve allocation latency, as well as reducing coherency traffic
1293     // on the shared global list.
1294     m = take_from_start_of_om_free_list(self);
1295     if (m != NULL) {
1296       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1297       prepend_to_om_in_use_list(self, m);
1298       return m;
1299     }
1300 
1301     // 2: try to allocate from the global om_list_globals._free_list
1302     // If we&#39;re using thread-local free lists then try
1303     // to reprovision the caller&#39;s free list.
1304     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
</pre>
<hr />
<pre>
1371 }
1372 
1373 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1374 // In practice there&#39;s no need to clamp or limit the number of
1375 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1376 // we&#39;ll call om_release() is to return a monitor to the free list after
1377 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1378 // accumulate on a thread&#39;s free list.
1379 //
1380 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1381 // free list must have their object field set to null. This prevents the
1382 // scavenger -- deflate_monitor_list() -- from reclaiming them while we
1383 // are trying to release them.
1384 
1385 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1386                                     bool from_per_thread_alloc) {
1387   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1388   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1389   NoSafepointVerifier nsv;
1390 
<span class="line-modified">1391   stringStream ss;</span>
<span class="line-modified">1392   guarantee((m-&gt;is_busy() | m-&gt;_recursions) == 0, &quot;freeing in-use monitor: &quot;</span>
<span class="line-modified">1393             &quot;%s, recursions=&quot; INTX_FORMAT, m-&gt;is_busy_to_string(&amp;ss),</span>
<span class="line-modified">1394             m-&gt;_recursions);</span>

1395   // _next_om is used for both per-thread in-use and free lists so
1396   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1397   if (from_per_thread_alloc) {
1398     // Need to remove &#39;m&#39; from om_in_use_list.
1399     ObjectMonitor* mid = NULL;
1400     ObjectMonitor* next = NULL;
1401 
1402     // This list walk can only race with another list walker since
1403     // deflation can only happen at a safepoint so we don&#39;t have to
1404     // worry about an ObjectMonitor being removed from this list
1405     // while we are walking it.
1406 
1407     // Lock the list head to avoid racing with another list walker.
1408     if ((mid = get_list_head_locked(&amp;self-&gt;om_in_use_list)) == NULL) {
1409       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; in-use list must not be empty.&quot;, p2i(self));
1410     }
1411     next = unmarked_next(mid);
1412     if (m == mid) {
1413       // First special case:
1414       // &#39;m&#39; matches mid, is the list head and is locked. Switch the list
</pre>
<hr />
<pre>
1537     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1538     //
1539     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1540     // monitor which will be linked to om_list_globals._free_list below.
1541     //
1542     // Account for the free list head before the loop since it is
1543     // already locked (by this thread):
1544     free_tail = free_list;
1545     free_count++;
1546     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1547       if (is_locked(s)) {
1548         // s is locked so there must be a racing walker thread ahead
1549         // of us so we&#39;ll give it a chance to finish.
1550         while (is_locked(s)) {
1551           os::naked_short_sleep(1);
1552         }
1553       }
1554       free_tail = s;
1555       free_count++;
1556       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-modified">1557       stringStream ss;</span>
<span class="line-modified">1558       guarantee(!s-&gt;is_busy(), &quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));</span>


1559     }
1560     guarantee(free_tail != NULL, &quot;invariant&quot;);
1561     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
1562     assert(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;
1563            &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);
1564     Atomic::store(&amp;self-&gt;om_free_count, 0);
1565     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1566     om_unlock(free_list);
1567   }
1568 
1569   if (free_tail != NULL) {
1570     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1571   }
1572 
1573   if (in_use_tail != NULL) {
1574     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1575   }
1576 
1577   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1578   LogStreamHandle(Info, monitorinflation) lsh_info;
</pre>
</td>
<td>
<hr />
<pre>
1264   // Induce STW safepoint to trim monitors
1265   // Ultimately, this results in a call to deflate_idle_monitors() in the near future.
1266   // More precisely, trigger a cleanup safepoint as the number
1267   // of active monitors passes the specified threshold.
1268   // TODO: assert thread state is reasonable
1269 
1270   if (Atomic::xchg(&amp;_forceMonitorScavenge, 1) == 0) {
1271     VMThread::check_for_forced_cleanup();
1272   }
1273 }
1274 
1275 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1276   // A large MAXPRIVATE value reduces both list lock contention
1277   // and list coherency traffic, but also tends to increase the
1278   // number of ObjectMonitors in circulation as well as the STW
1279   // scavenge costs.  As usual, we lean toward time in space-time
1280   // tradeoffs.
1281   const int MAXPRIVATE = 1024;
1282   NoSafepointVerifier nsv;
1283 

1284   for (;;) {
1285     ObjectMonitor* m;
1286 
1287     // 1: try to allocate from the thread&#39;s local om_free_list.
1288     // Threads will attempt to allocate first from their local list, then
1289     // from the global list, and only after those attempts fail will the
1290     // thread attempt to instantiate new monitors. Thread-local free lists
1291     // improve allocation latency, as well as reducing coherency traffic
1292     // on the shared global list.
1293     m = take_from_start_of_om_free_list(self);
1294     if (m != NULL) {
1295       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1296       prepend_to_om_in_use_list(self, m);
1297       return m;
1298     }
1299 
1300     // 2: try to allocate from the global om_list_globals._free_list
1301     // If we&#39;re using thread-local free lists then try
1302     // to reprovision the caller&#39;s free list.
1303     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
</pre>
<hr />
<pre>
1370 }
1371 
1372 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1373 // In practice there&#39;s no need to clamp or limit the number of
1374 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1375 // we&#39;ll call om_release() is to return a monitor to the free list after
1376 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1377 // accumulate on a thread&#39;s free list.
1378 //
1379 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1380 // free list must have their object field set to null. This prevents the
1381 // scavenger -- deflate_monitor_list() -- from reclaiming them while we
1382 // are trying to release them.
1383 
1384 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1385                                     bool from_per_thread_alloc) {
1386   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1387   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1388   NoSafepointVerifier nsv;
1389 
<span class="line-modified">1390   if ((m-&gt;is_busy() | m-&gt;_recursions) != 0) {</span>
<span class="line-modified">1391     stringStream ss;</span>
<span class="line-modified">1392     fatal(&quot;freeing in-use monitor: %s, recursions=&quot; INTX_FORMAT,</span>
<span class="line-modified">1393           m-&gt;is_busy_to_string(&amp;ss), m-&gt;_recursions);</span>
<span class="line-added">1394   }</span>
1395   // _next_om is used for both per-thread in-use and free lists so
1396   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1397   if (from_per_thread_alloc) {
1398     // Need to remove &#39;m&#39; from om_in_use_list.
1399     ObjectMonitor* mid = NULL;
1400     ObjectMonitor* next = NULL;
1401 
1402     // This list walk can only race with another list walker since
1403     // deflation can only happen at a safepoint so we don&#39;t have to
1404     // worry about an ObjectMonitor being removed from this list
1405     // while we are walking it.
1406 
1407     // Lock the list head to avoid racing with another list walker.
1408     if ((mid = get_list_head_locked(&amp;self-&gt;om_in_use_list)) == NULL) {
1409       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; in-use list must not be empty.&quot;, p2i(self));
1410     }
1411     next = unmarked_next(mid);
1412     if (m == mid) {
1413       // First special case:
1414       // &#39;m&#39; matches mid, is the list head and is locked. Switch the list
</pre>
<hr />
<pre>
1537     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1538     //
1539     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1540     // monitor which will be linked to om_list_globals._free_list below.
1541     //
1542     // Account for the free list head before the loop since it is
1543     // already locked (by this thread):
1544     free_tail = free_list;
1545     free_count++;
1546     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1547       if (is_locked(s)) {
1548         // s is locked so there must be a racing walker thread ahead
1549         // of us so we&#39;ll give it a chance to finish.
1550         while (is_locked(s)) {
1551           os::naked_short_sleep(1);
1552         }
1553       }
1554       free_tail = s;
1555       free_count++;
1556       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-modified">1557       if (s-&gt;is_busy()) {</span>
<span class="line-modified">1558         stringStream ss;</span>
<span class="line-added">1559         fatal(&quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));</span>
<span class="line-added">1560       }</span>
1561     }
1562     guarantee(free_tail != NULL, &quot;invariant&quot;);
1563     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
1564     assert(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;
1565            &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);
1566     Atomic::store(&amp;self-&gt;om_free_count, 0);
1567     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1568     om_unlock(free_list);
1569   }
1570 
1571   if (free_tail != NULL) {
1572     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1573   }
1574 
1575   if (in_use_tail != NULL) {
1576     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1577   }
1578 
1579   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1580   LogStreamHandle(Info, monitorinflation) lsh_info;
</pre>
</td>
</tr>
</table>
<center><a href="sweeper.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="thread.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>