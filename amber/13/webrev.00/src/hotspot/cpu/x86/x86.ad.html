<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 //
   2 // Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
   3 // DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4 //
   5 // This code is free software; you can redistribute it and/or modify it
   6 // under the terms of the GNU General Public License version 2 only, as
   7 // published by the Free Software Foundation.
   8 //
   9 // This code is distributed in the hope that it will be useful, but WITHOUT
  10 // ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11 // FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12 // version 2 for more details (a copy is included in the LICENSE file that
  13 // accompanied this code).
  14 //
  15 // You should have received a copy of the GNU General Public License version
  16 // 2 along with this work; if not, write to the Free Software Foundation,
  17 // Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18 //
  19 // Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20 // or visit www.oracle.com if you need additional information or have any
  21 // questions.
  22 //
  23 //
  24 
  25 // X86 Common Architecture Description File
  26 
  27 //----------REGISTER DEFINITION BLOCK------------------------------------------
  28 // This information is used by the matcher and the register allocator to
  29 // describe individual registers and classes of registers within the target
  30 // archtecture.
  31 
  32 register %{
  33 //----------Architecture Description Register Definitions----------------------
  34 // General Registers
  35 // &quot;reg_def&quot;  name ( register save type, C convention save type,
  36 //                   ideal register type, encoding );
  37 // Register Save Types:
  38 //
  39 // NS  = No-Save:       The register allocator assumes that these registers
  40 //                      can be used without saving upon entry to the method, &amp;
  41 //                      that they do not need to be saved at call sites.
  42 //
  43 // SOC = Save-On-Call:  The register allocator assumes that these registers
  44 //                      can be used without saving upon entry to the method,
  45 //                      but that they must be saved at call sites.
  46 //
  47 // SOE = Save-On-Entry: The register allocator assumes that these registers
  48 //                      must be saved before using them upon entry to the
  49 //                      method, but they do not need to be saved at call
  50 //                      sites.
  51 //
  52 // AS  = Always-Save:   The register allocator assumes that these registers
  53 //                      must be saved before using them upon entry to the
  54 //                      method, &amp; that they must be saved at call sites.
  55 //
  56 // Ideal Register Type is used to determine how to save &amp; restore a
  57 // register.  Op_RegI will get spilled with LoadI/StoreI, Op_RegP will get
  58 // spilled with LoadP/StoreP.  If the register supports both, use Op_RegI.
  59 //
  60 // The encoding number is the actual bit-pattern placed into the opcodes.
  61 
  62 // XMM registers.  512-bit registers or 8 words each, labeled (a)-p.
  63 // Word a in each register holds a Float, words ab hold a Double.
  64 // The whole registers are used in SSE4.2 version intrinsics,
  65 // array copy stubs and superword operations (see UseSSE42Intrinsics,
  66 // UseXMMForArrayCopy and UseSuperword flags).
  67 // For pre EVEX enabled architectures:
  68 //      XMM8-XMM15 must be encoded with REX (VEX for UseAVX)
  69 // For EVEX enabled architectures:
  70 //      XMM8-XMM31 must be encoded with REX (EVEX for UseAVX).
  71 //
  72 // Linux ABI:   No register preserved across function calls
  73 //              XMM0-XMM7 might hold parameters
  74 // Windows ABI: XMM6-XMM31 preserved across function calls
  75 //              XMM0-XMM3 might hold parameters
  76 
  77 reg_def XMM0 ( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg());
  78 reg_def XMM0b( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(1));
  79 reg_def XMM0c( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(2));
  80 reg_def XMM0d( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(3));
  81 reg_def XMM0e( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(4));
  82 reg_def XMM0f( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(5));
  83 reg_def XMM0g( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(6));
  84 reg_def XMM0h( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(7));
  85 reg_def XMM0i( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(8));
  86 reg_def XMM0j( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(9));
  87 reg_def XMM0k( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(10));
  88 reg_def XMM0l( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(11));
  89 reg_def XMM0m( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(12));
  90 reg_def XMM0n( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(13));
  91 reg_def XMM0o( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(14));
  92 reg_def XMM0p( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(15));
  93 
  94 reg_def XMM1 ( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg());
  95 reg_def XMM1b( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(1));
  96 reg_def XMM1c( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(2));
  97 reg_def XMM1d( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(3));
  98 reg_def XMM1e( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(4));
  99 reg_def XMM1f( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(5));
 100 reg_def XMM1g( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(6));
 101 reg_def XMM1h( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(7));
 102 reg_def XMM1i( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(8));
 103 reg_def XMM1j( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(9));
 104 reg_def XMM1k( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(10));
 105 reg_def XMM1l( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(11));
 106 reg_def XMM1m( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(12));
 107 reg_def XMM1n( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(13));
 108 reg_def XMM1o( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(14));
 109 reg_def XMM1p( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(15));
 110 
 111 reg_def XMM2 ( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg());
 112 reg_def XMM2b( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(1));
 113 reg_def XMM2c( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(2));
 114 reg_def XMM2d( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(3));
 115 reg_def XMM2e( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(4));
 116 reg_def XMM2f( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(5));
 117 reg_def XMM2g( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(6));
 118 reg_def XMM2h( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(7));
 119 reg_def XMM2i( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(8));
 120 reg_def XMM2j( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(9));
 121 reg_def XMM2k( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(10));
 122 reg_def XMM2l( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(11));
 123 reg_def XMM2m( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(12));
 124 reg_def XMM2n( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(13));
 125 reg_def XMM2o( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(14));
 126 reg_def XMM2p( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(15));
 127 
 128 reg_def XMM3 ( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg());
 129 reg_def XMM3b( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(1));
 130 reg_def XMM3c( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(2));
 131 reg_def XMM3d( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(3));
 132 reg_def XMM3e( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(4));
 133 reg_def XMM3f( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(5));
 134 reg_def XMM3g( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(6));
 135 reg_def XMM3h( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(7));
 136 reg_def XMM3i( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(8));
 137 reg_def XMM3j( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(9));
 138 reg_def XMM3k( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(10));
 139 reg_def XMM3l( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(11));
 140 reg_def XMM3m( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(12));
 141 reg_def XMM3n( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(13));
 142 reg_def XMM3o( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(14));
 143 reg_def XMM3p( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(15));
 144 
 145 reg_def XMM4 ( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg());
 146 reg_def XMM4b( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(1));
 147 reg_def XMM4c( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(2));
 148 reg_def XMM4d( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(3));
 149 reg_def XMM4e( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(4));
 150 reg_def XMM4f( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(5));
 151 reg_def XMM4g( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(6));
 152 reg_def XMM4h( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(7));
 153 reg_def XMM4i( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(8));
 154 reg_def XMM4j( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(9));
 155 reg_def XMM4k( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(10));
 156 reg_def XMM4l( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(11));
 157 reg_def XMM4m( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(12));
 158 reg_def XMM4n( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(13));
 159 reg_def XMM4o( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(14));
 160 reg_def XMM4p( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(15));
 161 
 162 reg_def XMM5 ( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg());
 163 reg_def XMM5b( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(1));
 164 reg_def XMM5c( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(2));
 165 reg_def XMM5d( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(3));
 166 reg_def XMM5e( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(4));
 167 reg_def XMM5f( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(5));
 168 reg_def XMM5g( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(6));
 169 reg_def XMM5h( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(7));
 170 reg_def XMM5i( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(8));
 171 reg_def XMM5j( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(9));
 172 reg_def XMM5k( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(10));
 173 reg_def XMM5l( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(11));
 174 reg_def XMM5m( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(12));
 175 reg_def XMM5n( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(13));
 176 reg_def XMM5o( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(14));
 177 reg_def XMM5p( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(15));
 178 
 179 reg_def XMM6 ( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg());
 180 reg_def XMM6b( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(1));
 181 reg_def XMM6c( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(2));
 182 reg_def XMM6d( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(3));
 183 reg_def XMM6e( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(4));
 184 reg_def XMM6f( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(5));
 185 reg_def XMM6g( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(6));
 186 reg_def XMM6h( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(7));
 187 reg_def XMM6i( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(8));
 188 reg_def XMM6j( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(9));
 189 reg_def XMM6k( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(10));
 190 reg_def XMM6l( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(11));
 191 reg_def XMM6m( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(12));
 192 reg_def XMM6n( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(13));
 193 reg_def XMM6o( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(14));
 194 reg_def XMM6p( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(15));
 195 
 196 reg_def XMM7 ( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg());
 197 reg_def XMM7b( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(1));
 198 reg_def XMM7c( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(2));
 199 reg_def XMM7d( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(3));
 200 reg_def XMM7e( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(4));
 201 reg_def XMM7f( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(5));
 202 reg_def XMM7g( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(6));
 203 reg_def XMM7h( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(7));
 204 reg_def XMM7i( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(8));
 205 reg_def XMM7j( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(9));
 206 reg_def XMM7k( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(10));
 207 reg_def XMM7l( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(11));
 208 reg_def XMM7m( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(12));
 209 reg_def XMM7n( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(13));
 210 reg_def XMM7o( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(14));
 211 reg_def XMM7p( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(15));
 212 
 213 #ifdef _LP64
 214 
 215 reg_def XMM8 ( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg());
 216 reg_def XMM8b( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(1));
 217 reg_def XMM8c( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(2));
 218 reg_def XMM8d( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(3));
 219 reg_def XMM8e( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(4));
 220 reg_def XMM8f( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(5));
 221 reg_def XMM8g( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(6));
 222 reg_def XMM8h( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(7));
 223 reg_def XMM8i( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(8));
 224 reg_def XMM8j( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(9));
 225 reg_def XMM8k( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(10));
 226 reg_def XMM8l( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(11));
 227 reg_def XMM8m( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(12));
 228 reg_def XMM8n( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(13));
 229 reg_def XMM8o( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(14));
 230 reg_def XMM8p( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(15));
 231 
 232 reg_def XMM9 ( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg());
 233 reg_def XMM9b( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(1));
 234 reg_def XMM9c( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(2));
 235 reg_def XMM9d( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(3));
 236 reg_def XMM9e( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(4));
 237 reg_def XMM9f( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(5));
 238 reg_def XMM9g( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(6));
 239 reg_def XMM9h( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(7));
 240 reg_def XMM9i( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(8));
 241 reg_def XMM9j( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(9));
 242 reg_def XMM9k( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(10));
 243 reg_def XMM9l( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(11));
 244 reg_def XMM9m( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(12));
 245 reg_def XMM9n( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(13));
 246 reg_def XMM9o( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(14));
 247 reg_def XMM9p( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(15));
 248 
 249 reg_def XMM10 ( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg());
 250 reg_def XMM10b( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(1));
 251 reg_def XMM10c( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(2));
 252 reg_def XMM10d( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(3));
 253 reg_def XMM10e( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(4));
 254 reg_def XMM10f( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(5));
 255 reg_def XMM10g( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(6));
 256 reg_def XMM10h( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(7));
 257 reg_def XMM10i( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(8));
 258 reg_def XMM10j( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(9));
 259 reg_def XMM10k( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(10));
 260 reg_def XMM10l( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(11));
 261 reg_def XMM10m( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(12));
 262 reg_def XMM10n( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(13));
 263 reg_def XMM10o( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(14));
 264 reg_def XMM10p( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(15));
 265 
 266 reg_def XMM11 ( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg());
 267 reg_def XMM11b( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(1));
 268 reg_def XMM11c( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(2));
 269 reg_def XMM11d( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(3));
 270 reg_def XMM11e( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(4));
 271 reg_def XMM11f( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(5));
 272 reg_def XMM11g( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(6));
 273 reg_def XMM11h( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(7));
 274 reg_def XMM11i( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(8));
 275 reg_def XMM11j( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(9));
 276 reg_def XMM11k( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(10));
 277 reg_def XMM11l( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(11));
 278 reg_def XMM11m( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(12));
 279 reg_def XMM11n( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(13));
 280 reg_def XMM11o( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(14));
 281 reg_def XMM11p( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(15));
 282 
 283 reg_def XMM12 ( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg());
 284 reg_def XMM12b( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(1));
 285 reg_def XMM12c( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(2));
 286 reg_def XMM12d( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(3));
 287 reg_def XMM12e( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(4));
 288 reg_def XMM12f( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(5));
 289 reg_def XMM12g( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(6));
 290 reg_def XMM12h( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(7));
 291 reg_def XMM12i( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(8));
 292 reg_def XMM12j( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(9));
 293 reg_def XMM12k( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(10));
 294 reg_def XMM12l( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(11));
 295 reg_def XMM12m( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(12));
 296 reg_def XMM12n( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(13));
 297 reg_def XMM12o( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(14));
 298 reg_def XMM12p( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(15));
 299 
 300 reg_def XMM13 ( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg());
 301 reg_def XMM13b( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(1));
 302 reg_def XMM13c( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(2));
 303 reg_def XMM13d( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(3));
 304 reg_def XMM13e( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(4));
 305 reg_def XMM13f( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(5));
 306 reg_def XMM13g( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(6));
 307 reg_def XMM13h( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(7));
 308 reg_def XMM13i( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(8));
 309 reg_def XMM13j( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(9));
 310 reg_def XMM13k( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(10));
 311 reg_def XMM13l( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(11));
 312 reg_def XMM13m( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(12));
 313 reg_def XMM13n( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(13));
 314 reg_def XMM13o( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(14));
 315 reg_def XMM13p( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(15));
 316 
 317 reg_def XMM14 ( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg());
 318 reg_def XMM14b( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(1));
 319 reg_def XMM14c( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(2));
 320 reg_def XMM14d( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(3));
 321 reg_def XMM14e( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(4));
 322 reg_def XMM14f( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(5));
 323 reg_def XMM14g( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(6));
 324 reg_def XMM14h( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(7));
 325 reg_def XMM14i( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(8));
 326 reg_def XMM14j( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(9));
 327 reg_def XMM14k( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(10));
 328 reg_def XMM14l( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(11));
 329 reg_def XMM14m( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(12));
 330 reg_def XMM14n( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(13));
 331 reg_def XMM14o( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(14));
 332 reg_def XMM14p( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(15));
 333 
 334 reg_def XMM15 ( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg());
 335 reg_def XMM15b( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(1));
 336 reg_def XMM15c( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(2));
 337 reg_def XMM15d( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(3));
 338 reg_def XMM15e( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(4));
 339 reg_def XMM15f( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(5));
 340 reg_def XMM15g( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(6));
 341 reg_def XMM15h( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(7));
 342 reg_def XMM15i( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(8));
 343 reg_def XMM15j( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(9));
 344 reg_def XMM15k( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(10));
 345 reg_def XMM15l( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(11));
 346 reg_def XMM15m( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(12));
 347 reg_def XMM15n( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(13));
 348 reg_def XMM15o( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(14));
 349 reg_def XMM15p( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(15));
 350 
 351 reg_def XMM16 ( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg());
 352 reg_def XMM16b( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(1));
 353 reg_def XMM16c( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(2));
 354 reg_def XMM16d( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(3));
 355 reg_def XMM16e( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(4));
 356 reg_def XMM16f( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(5));
 357 reg_def XMM16g( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(6));
 358 reg_def XMM16h( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(7));
 359 reg_def XMM16i( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(8));
 360 reg_def XMM16j( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(9));
 361 reg_def XMM16k( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(10));
 362 reg_def XMM16l( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(11));
 363 reg_def XMM16m( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(12));
 364 reg_def XMM16n( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(13));
 365 reg_def XMM16o( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(14));
 366 reg_def XMM16p( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(15));
 367 
 368 reg_def XMM17 ( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg());
 369 reg_def XMM17b( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(1));
 370 reg_def XMM17c( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(2));
 371 reg_def XMM17d( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(3));
 372 reg_def XMM17e( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(4));
 373 reg_def XMM17f( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(5));
 374 reg_def XMM17g( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(6));
 375 reg_def XMM17h( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(7));
 376 reg_def XMM17i( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(8));
 377 reg_def XMM17j( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(9));
 378 reg_def XMM17k( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(10));
 379 reg_def XMM17l( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(11));
 380 reg_def XMM17m( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(12));
 381 reg_def XMM17n( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(13));
 382 reg_def XMM17o( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(14));
 383 reg_def XMM17p( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(15));
 384 
 385 reg_def XMM18 ( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg());
 386 reg_def XMM18b( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(1));
 387 reg_def XMM18c( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(2));
 388 reg_def XMM18d( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(3));
 389 reg_def XMM18e( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(4));
 390 reg_def XMM18f( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(5));
 391 reg_def XMM18g( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(6));
 392 reg_def XMM18h( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(7));
 393 reg_def XMM18i( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(8));
 394 reg_def XMM18j( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(9));
 395 reg_def XMM18k( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(10));
 396 reg_def XMM18l( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(11));
 397 reg_def XMM18m( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(12));
 398 reg_def XMM18n( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(13));
 399 reg_def XMM18o( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(14));
 400 reg_def XMM18p( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(15));
 401 
 402 reg_def XMM19 ( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg());
 403 reg_def XMM19b( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(1));
 404 reg_def XMM19c( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(2));
 405 reg_def XMM19d( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(3));
 406 reg_def XMM19e( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(4));
 407 reg_def XMM19f( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(5));
 408 reg_def XMM19g( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(6));
 409 reg_def XMM19h( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(7));
 410 reg_def XMM19i( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(8));
 411 reg_def XMM19j( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(9));
 412 reg_def XMM19k( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(10));
 413 reg_def XMM19l( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(11));
 414 reg_def XMM19m( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(12));
 415 reg_def XMM19n( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(13));
 416 reg_def XMM19o( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(14));
 417 reg_def XMM19p( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(15));
 418 
 419 reg_def XMM20 ( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg());
 420 reg_def XMM20b( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(1));
 421 reg_def XMM20c( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(2));
 422 reg_def XMM20d( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(3));
 423 reg_def XMM20e( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(4));
 424 reg_def XMM20f( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(5));
 425 reg_def XMM20g( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(6));
 426 reg_def XMM20h( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(7));
 427 reg_def XMM20i( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(8));
 428 reg_def XMM20j( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(9));
 429 reg_def XMM20k( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(10));
 430 reg_def XMM20l( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(11));
 431 reg_def XMM20m( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(12));
 432 reg_def XMM20n( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(13));
 433 reg_def XMM20o( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(14));
 434 reg_def XMM20p( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(15));
 435 
 436 reg_def XMM21 ( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg());
 437 reg_def XMM21b( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(1));
 438 reg_def XMM21c( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(2));
 439 reg_def XMM21d( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(3));
 440 reg_def XMM21e( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(4));
 441 reg_def XMM21f( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(5));
 442 reg_def XMM21g( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(6));
 443 reg_def XMM21h( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(7));
 444 reg_def XMM21i( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(8));
 445 reg_def XMM21j( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(9));
 446 reg_def XMM21k( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(10));
 447 reg_def XMM21l( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(11));
 448 reg_def XMM21m( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(12));
 449 reg_def XMM21n( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(13));
 450 reg_def XMM21o( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(14));
 451 reg_def XMM21p( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(15));
 452 
 453 reg_def XMM22 ( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg());
 454 reg_def XMM22b( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(1));
 455 reg_def XMM22c( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(2));
 456 reg_def XMM22d( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(3));
 457 reg_def XMM22e( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(4));
 458 reg_def XMM22f( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(5));
 459 reg_def XMM22g( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(6));
 460 reg_def XMM22h( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(7));
 461 reg_def XMM22i( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(8));
 462 reg_def XMM22j( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(9));
 463 reg_def XMM22k( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(10));
 464 reg_def XMM22l( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(11));
 465 reg_def XMM22m( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(12));
 466 reg_def XMM22n( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(13));
 467 reg_def XMM22o( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(14));
 468 reg_def XMM22p( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(15));
 469 
 470 reg_def XMM23 ( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg());
 471 reg_def XMM23b( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(1));
 472 reg_def XMM23c( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(2));
 473 reg_def XMM23d( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(3));
 474 reg_def XMM23e( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(4));
 475 reg_def XMM23f( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(5));
 476 reg_def XMM23g( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(6));
 477 reg_def XMM23h( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(7));
 478 reg_def XMM23i( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(8));
 479 reg_def XMM23j( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(9));
 480 reg_def XMM23k( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(10));
 481 reg_def XMM23l( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(11));
 482 reg_def XMM23m( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(12));
 483 reg_def XMM23n( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(13));
 484 reg_def XMM23o( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(14));
 485 reg_def XMM23p( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(15));
 486 
 487 reg_def XMM24 ( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg());
 488 reg_def XMM24b( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(1));
 489 reg_def XMM24c( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(2));
 490 reg_def XMM24d( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(3));
 491 reg_def XMM24e( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(4));
 492 reg_def XMM24f( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(5));
 493 reg_def XMM24g( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(6));
 494 reg_def XMM24h( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(7));
 495 reg_def XMM24i( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(8));
 496 reg_def XMM24j( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(9));
 497 reg_def XMM24k( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(10));
 498 reg_def XMM24l( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(11));
 499 reg_def XMM24m( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(12));
 500 reg_def XMM24n( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(13));
 501 reg_def XMM24o( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(14));
 502 reg_def XMM24p( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(15));
 503 
 504 reg_def XMM25 ( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg());
 505 reg_def XMM25b( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(1));
 506 reg_def XMM25c( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(2));
 507 reg_def XMM25d( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(3));
 508 reg_def XMM25e( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(4));
 509 reg_def XMM25f( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(5));
 510 reg_def XMM25g( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(6));
 511 reg_def XMM25h( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(7));
 512 reg_def XMM25i( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(8));
 513 reg_def XMM25j( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(9));
 514 reg_def XMM25k( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(10));
 515 reg_def XMM25l( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(11));
 516 reg_def XMM25m( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(12));
 517 reg_def XMM25n( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(13));
 518 reg_def XMM25o( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(14));
 519 reg_def XMM25p( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(15));
 520 
 521 reg_def XMM26 ( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg());
 522 reg_def XMM26b( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(1));
 523 reg_def XMM26c( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(2));
 524 reg_def XMM26d( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(3));
 525 reg_def XMM26e( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(4));
 526 reg_def XMM26f( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(5));
 527 reg_def XMM26g( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(6));
 528 reg_def XMM26h( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(7));
 529 reg_def XMM26i( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(8));
 530 reg_def XMM26j( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(9));
 531 reg_def XMM26k( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(10));
 532 reg_def XMM26l( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(11));
 533 reg_def XMM26m( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(12));
 534 reg_def XMM26n( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(13));
 535 reg_def XMM26o( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(14));
 536 reg_def XMM26p( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(15));
 537 
 538 reg_def XMM27 ( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg());
 539 reg_def XMM27b( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(1));
 540 reg_def XMM27c( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(2));
 541 reg_def XMM27d( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(3));
 542 reg_def XMM27e( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(4));
 543 reg_def XMM27f( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(5));
 544 reg_def XMM27g( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(6));
 545 reg_def XMM27h( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(7));
 546 reg_def XMM27i( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(8));
 547 reg_def XMM27j( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(9));
 548 reg_def XMM27k( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(10));
 549 reg_def XMM27l( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(11));
 550 reg_def XMM27m( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(12));
 551 reg_def XMM27n( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(13));
 552 reg_def XMM27o( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(14));
 553 reg_def XMM27p( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(15));
 554 
 555 reg_def XMM28 ( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg());
 556 reg_def XMM28b( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(1));
 557 reg_def XMM28c( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(2));
 558 reg_def XMM28d( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(3));
 559 reg_def XMM28e( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(4));
 560 reg_def XMM28f( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(5));
 561 reg_def XMM28g( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(6));
 562 reg_def XMM28h( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(7));
 563 reg_def XMM28i( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(8));
 564 reg_def XMM28j( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(9));
 565 reg_def XMM28k( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(10));
 566 reg_def XMM28l( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(11));
 567 reg_def XMM28m( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(12));
 568 reg_def XMM28n( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(13));
 569 reg_def XMM28o( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(14));
 570 reg_def XMM28p( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(15));
 571 
 572 reg_def XMM29 ( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg());
 573 reg_def XMM29b( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(1));
 574 reg_def XMM29c( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(2));
 575 reg_def XMM29d( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(3));
 576 reg_def XMM29e( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(4));
 577 reg_def XMM29f( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(5));
 578 reg_def XMM29g( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(6));
 579 reg_def XMM29h( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(7));
 580 reg_def XMM29i( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(8));
 581 reg_def XMM29j( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(9));
 582 reg_def XMM29k( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(10));
 583 reg_def XMM29l( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(11));
 584 reg_def XMM29m( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(12));
 585 reg_def XMM29n( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(13));
 586 reg_def XMM29o( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(14));
 587 reg_def XMM29p( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(15));
 588 
 589 reg_def XMM30 ( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg());
 590 reg_def XMM30b( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(1));
 591 reg_def XMM30c( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(2));
 592 reg_def XMM30d( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(3));
 593 reg_def XMM30e( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(4));
 594 reg_def XMM30f( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(5));
 595 reg_def XMM30g( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(6));
 596 reg_def XMM30h( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(7));
 597 reg_def XMM30i( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(8));
 598 reg_def XMM30j( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(9));
 599 reg_def XMM30k( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(10));
 600 reg_def XMM30l( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(11));
 601 reg_def XMM30m( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(12));
 602 reg_def XMM30n( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(13));
 603 reg_def XMM30o( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(14));
 604 reg_def XMM30p( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(15));
 605 
 606 reg_def XMM31 ( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg());
 607 reg_def XMM31b( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(1));
 608 reg_def XMM31c( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(2));
 609 reg_def XMM31d( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(3));
 610 reg_def XMM31e( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(4));
 611 reg_def XMM31f( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(5));
 612 reg_def XMM31g( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(6));
 613 reg_def XMM31h( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(7));
 614 reg_def XMM31i( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(8));
 615 reg_def XMM31j( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(9));
 616 reg_def XMM31k( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(10));
 617 reg_def XMM31l( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(11));
 618 reg_def XMM31m( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(12));
 619 reg_def XMM31n( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(13));
 620 reg_def XMM31o( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(14));
 621 reg_def XMM31p( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(15));
 622 
 623 #endif // _LP64
 624 
 625 #ifdef _LP64
 626 reg_def RFLAGS(SOC, SOC, 0, 16, VMRegImpl::Bad());
 627 #else
 628 reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());
 629 #endif // _LP64
 630 
 631 alloc_class chunk1(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
 632                    XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
 633                    XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
 634                    XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
 635                    XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
 636                    XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
 637                    XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
 638                    XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
 639 #ifdef _LP64
 640                   ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
 641                    XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
 642                    XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
 643                    XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
 644                    XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
 645                    XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
 646                    XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
 647                    XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
 648                   ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
 649                    XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
 650                    XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
 651                    XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
 652                    XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
 653                    XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
 654                    XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
 655                    XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
 656                    XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
 657                    XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
 658                    XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
 659                    XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
 660                    XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
 661                    XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
 662                    XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
 663                    XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
 664 #endif
 665                       );
 666 
 667 // flags allocation class should be last.
 668 alloc_class chunk2(RFLAGS);
 669 
 670 // Singleton class for condition codes
 671 reg_class int_flags(RFLAGS);
 672 
 673 // Class for pre evex float registers
 674 reg_class float_reg_legacy(XMM0,
 675                     XMM1,
 676                     XMM2,
 677                     XMM3,
 678                     XMM4,
 679                     XMM5,
 680                     XMM6,
 681                     XMM7
 682 #ifdef _LP64
 683                    ,XMM8,
 684                     XMM9,
 685                     XMM10,
 686                     XMM11,
 687                     XMM12,
 688                     XMM13,
 689                     XMM14,
 690                     XMM15
 691 #endif
 692                     );
 693 
 694 // Class for evex float registers
 695 reg_class float_reg_evex(XMM0,
 696                     XMM1,
 697                     XMM2,
 698                     XMM3,
 699                     XMM4,
 700                     XMM5,
 701                     XMM6,
 702                     XMM7
 703 #ifdef _LP64
 704                    ,XMM8,
 705                     XMM9,
 706                     XMM10,
 707                     XMM11,
 708                     XMM12,
 709                     XMM13,
 710                     XMM14,
 711                     XMM15,
 712                     XMM16,
 713                     XMM17,
 714                     XMM18,
 715                     XMM19,
 716                     XMM20,
 717                     XMM21,
 718                     XMM22,
 719                     XMM23,
 720                     XMM24,
 721                     XMM25,
 722                     XMM26,
 723                     XMM27,
 724                     XMM28,
 725                     XMM29,
 726                     XMM30,
 727                     XMM31
 728 #endif
 729                     );
 730 
 731 reg_class_dynamic float_reg(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() %} );
 732 reg_class_dynamic float_reg_vl(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 733 
 734 // Class for pre evex double registers
 735 reg_class double_reg_legacy(XMM0,  XMM0b,
 736                      XMM1,  XMM1b,
 737                      XMM2,  XMM2b,
 738                      XMM3,  XMM3b,
 739                      XMM4,  XMM4b,
 740                      XMM5,  XMM5b,
 741                      XMM6,  XMM6b,
 742                      XMM7,  XMM7b
 743 #ifdef _LP64
 744                     ,XMM8,  XMM8b,
 745                      XMM9,  XMM9b,
 746                      XMM10, XMM10b,
 747                      XMM11, XMM11b,
 748                      XMM12, XMM12b,
 749                      XMM13, XMM13b,
 750                      XMM14, XMM14b,
 751                      XMM15, XMM15b
 752 #endif
 753                      );
 754 
 755 // Class for evex double registers
 756 reg_class double_reg_evex(XMM0,  XMM0b,
 757                      XMM1,  XMM1b,
 758                      XMM2,  XMM2b,
 759                      XMM3,  XMM3b,
 760                      XMM4,  XMM4b,
 761                      XMM5,  XMM5b,
 762                      XMM6,  XMM6b,
 763                      XMM7,  XMM7b
 764 #ifdef _LP64
 765                     ,XMM8,  XMM8b,
 766                      XMM9,  XMM9b,
 767                      XMM10, XMM10b,
 768                      XMM11, XMM11b,
 769                      XMM12, XMM12b,
 770                      XMM13, XMM13b,
 771                      XMM14, XMM14b,
 772                      XMM15, XMM15b,
 773                      XMM16, XMM16b,
 774                      XMM17, XMM17b,
 775                      XMM18, XMM18b,
 776                      XMM19, XMM19b,
 777                      XMM20, XMM20b,
 778                      XMM21, XMM21b,
 779                      XMM22, XMM22b,
 780                      XMM23, XMM23b,
 781                      XMM24, XMM24b,
 782                      XMM25, XMM25b,
 783                      XMM26, XMM26b,
 784                      XMM27, XMM27b,
 785                      XMM28, XMM28b,
 786                      XMM29, XMM29b,
 787                      XMM30, XMM30b,
 788                      XMM31, XMM31b
 789 #endif
 790                      );
 791 
 792 reg_class_dynamic double_reg(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() %} );
 793 reg_class_dynamic double_reg_vl(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 794 
 795 // Class for pre evex 32bit vector registers
 796 reg_class vectors_reg_legacy(XMM0,
 797                       XMM1,
 798                       XMM2,
 799                       XMM3,
 800                       XMM4,
 801                       XMM5,
 802                       XMM6,
 803                       XMM7
 804 #ifdef _LP64
 805                      ,XMM8,
 806                       XMM9,
 807                       XMM10,
 808                       XMM11,
 809                       XMM12,
 810                       XMM13,
 811                       XMM14,
 812                       XMM15
 813 #endif
 814                       );
 815 
 816 // Class for evex 32bit vector registers
 817 reg_class vectors_reg_evex(XMM0,
 818                       XMM1,
 819                       XMM2,
 820                       XMM3,
 821                       XMM4,
 822                       XMM5,
 823                       XMM6,
 824                       XMM7
 825 #ifdef _LP64
 826                      ,XMM8,
 827                       XMM9,
 828                       XMM10,
 829                       XMM11,
 830                       XMM12,
 831                       XMM13,
 832                       XMM14,
 833                       XMM15,
 834                       XMM16,
 835                       XMM17,
 836                       XMM18,
 837                       XMM19,
 838                       XMM20,
 839                       XMM21,
 840                       XMM22,
 841                       XMM23,
 842                       XMM24,
 843                       XMM25,
 844                       XMM26,
 845                       XMM27,
 846                       XMM28,
 847                       XMM29,
 848                       XMM30,
 849                       XMM31
 850 #endif
 851                       );
 852 
 853 reg_class_dynamic vectors_reg(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_evex() %} );
 854 reg_class_dynamic vectors_reg_vlbwdq(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 855 
 856 // Class for all 64bit vector registers
 857 reg_class vectord_reg_legacy(XMM0,  XMM0b,
 858                       XMM1,  XMM1b,
 859                       XMM2,  XMM2b,
 860                       XMM3,  XMM3b,
 861                       XMM4,  XMM4b,
 862                       XMM5,  XMM5b,
 863                       XMM6,  XMM6b,
 864                       XMM7,  XMM7b
 865 #ifdef _LP64
 866                      ,XMM8,  XMM8b,
 867                       XMM9,  XMM9b,
 868                       XMM10, XMM10b,
 869                       XMM11, XMM11b,
 870                       XMM12, XMM12b,
 871                       XMM13, XMM13b,
 872                       XMM14, XMM14b,
 873                       XMM15, XMM15b
 874 #endif
 875                       );
 876 
 877 // Class for all 64bit vector registers
 878 reg_class vectord_reg_evex(XMM0,  XMM0b,
 879                       XMM1,  XMM1b,
 880                       XMM2,  XMM2b,
 881                       XMM3,  XMM3b,
 882                       XMM4,  XMM4b,
 883                       XMM5,  XMM5b,
 884                       XMM6,  XMM6b,
 885                       XMM7,  XMM7b
 886 #ifdef _LP64
 887                      ,XMM8,  XMM8b,
 888                       XMM9,  XMM9b,
 889                       XMM10, XMM10b,
 890                       XMM11, XMM11b,
 891                       XMM12, XMM12b,
 892                       XMM13, XMM13b,
 893                       XMM14, XMM14b,
 894                       XMM15, XMM15b,
 895                       XMM16, XMM16b,
 896                       XMM17, XMM17b,
 897                       XMM18, XMM18b,
 898                       XMM19, XMM19b,
 899                       XMM20, XMM20b,
 900                       XMM21, XMM21b,
 901                       XMM22, XMM22b,
 902                       XMM23, XMM23b,
 903                       XMM24, XMM24b,
 904                       XMM25, XMM25b,
 905                       XMM26, XMM26b,
 906                       XMM27, XMM27b,
 907                       XMM28, XMM28b,
 908                       XMM29, XMM29b,
 909                       XMM30, XMM30b,
 910                       XMM31, XMM31b
 911 #endif
 912                       );
 913 
 914 reg_class_dynamic vectord_reg(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_evex() %} );
 915 reg_class_dynamic vectord_reg_vlbwdq(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 916 
 917 // Class for all 128bit vector registers
 918 reg_class vectorx_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,
 919                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 920                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 921                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 922                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 923                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 924                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 925                       XMM7,  XMM7b,  XMM7c,  XMM7d
 926 #ifdef _LP64
 927                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 928                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 929                       XMM10, XMM10b, XMM10c, XMM10d,
 930                       XMM11, XMM11b, XMM11c, XMM11d,
 931                       XMM12, XMM12b, XMM12c, XMM12d,
 932                       XMM13, XMM13b, XMM13c, XMM13d,
 933                       XMM14, XMM14b, XMM14c, XMM14d,
 934                       XMM15, XMM15b, XMM15c, XMM15d
 935 #endif
 936                       );
 937 
 938 // Class for all 128bit vector registers
 939 reg_class vectorx_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,
 940                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 941                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 942                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 943                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 944                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 945                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 946                       XMM7,  XMM7b,  XMM7c,  XMM7d
 947 #ifdef _LP64
 948                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 949                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 950                       XMM10, XMM10b, XMM10c, XMM10d,
 951                       XMM11, XMM11b, XMM11c, XMM11d,
 952                       XMM12, XMM12b, XMM12c, XMM12d,
 953                       XMM13, XMM13b, XMM13c, XMM13d,
 954                       XMM14, XMM14b, XMM14c, XMM14d,
 955                       XMM15, XMM15b, XMM15c, XMM15d,
 956                       XMM16, XMM16b, XMM16c, XMM16d,
 957                       XMM17, XMM17b, XMM17c, XMM17d,
 958                       XMM18, XMM18b, XMM18c, XMM18d,
 959                       XMM19, XMM19b, XMM19c, XMM19d,
 960                       XMM20, XMM20b, XMM20c, XMM20d,
 961                       XMM21, XMM21b, XMM21c, XMM21d,
 962                       XMM22, XMM22b, XMM22c, XMM22d,
 963                       XMM23, XMM23b, XMM23c, XMM23d,
 964                       XMM24, XMM24b, XMM24c, XMM24d,
 965                       XMM25, XMM25b, XMM25c, XMM25d,
 966                       XMM26, XMM26b, XMM26c, XMM26d,
 967                       XMM27, XMM27b, XMM27c, XMM27d,
 968                       XMM28, XMM28b, XMM28c, XMM28d,
 969                       XMM29, XMM29b, XMM29c, XMM29d,
 970                       XMM30, XMM30b, XMM30c, XMM30d,
 971                       XMM31, XMM31b, XMM31c, XMM31d
 972 #endif
 973                       );
 974 
 975 reg_class_dynamic vectorx_reg(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_evex() %} );
 976 reg_class_dynamic vectorx_reg_vlbwdq(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 977 
 978 // Class for all 256bit vector registers
 979 reg_class vectory_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
 980                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
 981                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
 982                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
 983                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
 984                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
 985                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
 986                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
 987 #ifdef _LP64
 988                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
 989                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
 990                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
 991                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
 992                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
 993                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
 994                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
 995                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h
 996 #endif
 997                       );
 998 
 999 // Class for all 256bit vector registers
1000 reg_class vectory_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
1001                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
1002                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
1003                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
1004                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
1005                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
1006                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
1007                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
1008 #ifdef _LP64
1009                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
1010                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
1011                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
1012                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
1013                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
1014                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
1015                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
1016                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h,
1017                       XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h,
1018                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h,
1019                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h,
1020                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h,
1021                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h,
1022                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h,
1023                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h,
1024                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h,
1025                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h,
1026                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h,
1027                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h,
1028                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h,
1029                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h,
1030                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h,
1031                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h,
1032                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h
1033 #endif
1034                       );
1035 
1036 reg_class_dynamic vectory_reg(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_evex() %} );
1037 reg_class_dynamic vectory_reg_vlbwdq(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
1038 
1039 // Class for all 512bit vector registers
1040 reg_class vectorz_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1041                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1042                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1043                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1044                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1045                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1046                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1047                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1048 #ifdef _LP64
1049                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1050                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1051                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1052                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1053                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1054                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1055                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1056                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1057                      ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
1058                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
1059                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
1060                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
1061                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
1062                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
1063                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
1064                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
1065                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
1066                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
1067                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
1068                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
1069                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
1070                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
1071                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
1072                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
1073 #endif
1074                       );
1075 
1076 // Class for restricted 512bit vector registers
1077 reg_class vectorz_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1078                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1079                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1080                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1081                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1082                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1083                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1084                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1085 #ifdef _LP64
1086                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1087                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1088                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1089                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1090                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1091                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1092                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1093                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1094 #endif
1095                       );
1096 
1097 reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
1098 reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
1099 
1100 %}
1101 
1102 
1103 //----------SOURCE BLOCK-------------------------------------------------------
1104 // This is a block of C++ code which provides values, functions, and
1105 // definitions necessary in the rest of the architecture description
1106 
1107 source_hpp %{
1108 // Header information of the source block.
1109 // Method declarations/definitions which are used outside
1110 // the ad-scope can conveniently be defined here.
1111 //
1112 // To keep related declarations/definitions/uses close together,
1113 // we switch between source %{ }% and source_hpp %{ }% freely as needed.
1114 
1115 class NativeJump;
1116 
1117 class CallStubImpl {
1118 
1119   //--------------------------------------------------------------
1120   //---&lt;  Used for optimization in Compile::shorten_branches  &gt;---
1121   //--------------------------------------------------------------
1122 
1123  public:
1124   // Size of call trampoline stub.
1125   static uint size_call_trampoline() {
1126     return 0; // no call trampolines on this platform
1127   }
1128 
1129   // number of relocations needed by a call trampoline stub
1130   static uint reloc_call_trampoline() {
1131     return 0; // no call trampolines on this platform
1132   }
1133 };
1134 
1135 class HandlerImpl {
1136 
1137  public:
1138 
1139   static int emit_exception_handler(CodeBuffer &amp;cbuf);
1140   static int emit_deopt_handler(CodeBuffer&amp; cbuf);
1141 
1142   static uint size_exception_handler() {
1143     // NativeCall instruction size is the same as NativeJump.
1144     // exception handler starts out as jump and can be patched to
1145     // a call be deoptimization.  (4932387)
1146     // Note that this value is also credited (in output.cpp) to
1147     // the size of the code section.
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
1168 class Node::PD {
1169 public:
1170   enum NodeFlags {
1171     Flag_intel_jcc_erratum = Node::_last_flag &lt;&lt; 1,
1172     _last_flag             = Flag_intel_jcc_erratum
1173   };
1174 };
1175 
1176 %} // end source_hpp
1177 
1178 source %{
1179 
1180 #include &quot;opto/addnode.hpp&quot;
1181 #include &quot;c2_intelJccErratum_x86.hpp&quot;
1182 
1183 void PhaseOutput::pd_perform_mach_node_analysis() {
1184   if (VM_Version::has_intel_jcc_erratum()) {
1185     int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C-&gt;cfg(), C-&gt;regalloc());
1186     _buf_sizes._code += extra_padding;
1187   }
1188 }
1189 
1190 int MachNode::pd_alignment_required() const {
1191   PhaseOutput* output = Compile::current()-&gt;output();
1192   Block* block = output-&gt;block();
1193   int index = output-&gt;index();
1194   if (VM_Version::has_intel_jcc_erratum() &amp;&amp; IntelJccErratum::is_jcc_erratum_branch(block, this, index)) {
1195     // Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.
1196     return IntelJccErratum::largest_jcc_size() + 1;
1197   } else {
1198     return 1;
1199   }
1200 }
1201 
1202 int MachNode::compute_padding(int current_offset) const {
1203   if (flags() &amp; Node::PD::Flag_intel_jcc_erratum) {
1204     Compile* C = Compile::current();
1205     PhaseOutput* output = C-&gt;output();
1206     Block* block = output-&gt;block();
1207     int index = output-&gt;index();
1208     return IntelJccErratum::compute_padding(current_offset, this, block, index, C-&gt;regalloc());
1209   } else {
1210     return 0;
1211   }
1212 }
1213 
1214 // Emit exception handler code.
1215 // Stuff framesize into a register and call a VM stub routine.
1216 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1217 
1218   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1219   // That&#39;s why we must use the macroassembler to generate a handler.
1220   C2_MacroAssembler _masm(&amp;cbuf);
1221   address base = __ start_a_stub(size_exception_handler());
1222   if (base == NULL) {
1223     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1224     return 0;  // CodeBuffer::expand failed
1225   }
1226   int offset = __ offset();
1227   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1228   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1229   __ end_a_stub();
1230   return offset;
1231 }
1232 
1233 // Emit deopt handler code.
1234 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1235 
1236   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1237   // That&#39;s why we must use the macroassembler to generate a handler.
1238   C2_MacroAssembler _masm(&amp;cbuf);
1239   address base = __ start_a_stub(size_deopt_handler());
1240   if (base == NULL) {
1241     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1242     return 0;  // CodeBuffer::expand failed
1243   }
1244   int offset = __ offset();
1245 
1246 #ifdef _LP64
1247   address the_pc = (address) __ pc();
1248   Label next;
1249   // push a &quot;the_pc&quot; on the stack without destroying any registers
1250   // as they all may be live.
1251 
1252   // push address of &quot;next&quot;
1253   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1254   __ bind(next);
1255   // adjust it so it matches &quot;the_pc&quot;
1256   __ subptr(Address(rsp, 0), __ offset() - offset);
1257 #else
1258   InternalAddress here(__ pc());
1259   __ pushptr(here.addr());
1260 #endif
1261 
1262   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
1263   assert(__ offset() - offset &lt;= (int) size_deopt_handler(), &quot;overflow %d&quot;, (__ offset() - offset));
1264   __ end_a_stub();
1265   return offset;
1266 }
1267 
1268 
1269 //=============================================================================
1270 
1271   // Float masks come from different places depending on platform.
1272 #ifdef _LP64
1273   static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }
1274   static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }
1275   static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }
1276   static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }
1277 #else
1278   static address float_signmask()  { return (address)float_signmask_pool; }
1279   static address float_signflip()  { return (address)float_signflip_pool; }
1280   static address double_signmask() { return (address)double_signmask_pool; }
1281   static address double_signflip() { return (address)double_signflip_pool; }
1282 #endif
1283   static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
1284   static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
1285   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1286 
1287 //=============================================================================
1288 const bool Matcher::match_rule_supported(int opcode) {
1289   if (!has_match_rule(opcode)) {
1290     return false; // no match rule present
1291   }
1292   switch (opcode) {
1293     case Op_AbsVL:
1294       if (UseAVX &lt; 3) {
1295         return false;
1296       }
1297       break;
1298     case Op_PopCountI:
1299     case Op_PopCountL:
1300       if (!UsePopCountInstruction) {
1301         return false;
1302       }
1303       break;
1304     case Op_PopCountVI:
1305       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {
1306         return false;
1307       }
1308       break;
1309     case Op_MulVI:
1310       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1311         return false;
1312       }
1313       break;
1314     case Op_MulVL:
1315     case Op_MulReductionVL:
1316       if (VM_Version::supports_avx512dq() == false) {
1317         return false;
1318       }
1319       break;
1320     case Op_AbsVB:
1321     case Op_AbsVS:
1322     case Op_AbsVI:
1323     case Op_AddReductionVI:
1324     case Op_AndReductionV:
1325     case Op_OrReductionV:
1326     case Op_XorReductionV:
1327       if (UseSSE &lt; 3) { // requires at least SSSE3
1328         return false;
1329       }
1330       break;
1331     case Op_MulReductionVI:
1332       if (UseSSE &lt; 4) { // requires at least SSE4
1333         return false;
1334       }
1335       break;
1336     case Op_SqrtVD:
1337     case Op_SqrtVF:
1338       if (UseAVX &lt; 1) { // enabled for AVX only
1339         return false;
1340       }
1341       break;
1342     case Op_CompareAndSwapL:
1343 #ifdef _LP64
1344     case Op_CompareAndSwapP:
1345 #endif
1346       if (!VM_Version::supports_cx8()) {
1347         return false;
1348       }
1349       break;
1350     case Op_CMoveVF:
1351     case Op_CMoveVD:
1352       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1353         return false;
1354       }
1355       break;
1356     case Op_StrIndexOf:
1357       if (!UseSSE42Intrinsics) {
1358         return false;
1359       }
1360       break;
1361     case Op_StrIndexOfChar:
1362       if (!UseSSE42Intrinsics) {
1363         return false;
1364       }
1365       break;
1366     case Op_OnSpinWait:
1367       if (VM_Version::supports_on_spin_wait() == false) {
1368         return false;
1369       }
1370       break;
1371     case Op_MulVB:
1372     case Op_LShiftVB:
1373     case Op_RShiftVB:
1374     case Op_URShiftVB:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379 #ifdef _LP64
1380     case Op_MaxD:
1381     case Op_MaxF:
1382     case Op_MinD:
1383     case Op_MinF:
1384       if (UseAVX &lt; 1) { // enabled for AVX only
1385         return false;
1386       }
1387       break;
1388 #endif
1389     case Op_CacheWB:
1390     case Op_CacheWBPreSync:
1391     case Op_CacheWBPostSync:
1392       if (!VM_Version::supports_data_cache_line_flush()) {
1393         return false;
1394       }
1395       break;
1396     case Op_RoundDoubleMode:
1397       if (UseSSE &lt; 4) {
1398         return false;
1399       }
1400       break;
1401     case Op_RoundDoubleModeV:
1402       if (VM_Version::supports_avx() == false) {
1403         return false; // 128bit vroundpd is not available
1404       }
1405       break;
1406     case Op_MacroLogicV:
1407       if (UseAVX &lt; 3 || !UseVectorMacroLogic) {
1408         return false;
1409       }
1410       break;
1411 #ifndef _LP64
1412     case Op_AddReductionVF:
1413     case Op_AddReductionVD:
1414     case Op_MulReductionVF:
1415     case Op_MulReductionVD:
1416       if (UseSSE &lt; 1) { // requires at least SSE
1417         return false;
1418       }
1419       break;
1420     case Op_MulAddVS2VI:
1421     case Op_RShiftVL:
1422     case Op_AbsVD:
1423     case Op_NegVD:
1424       if (UseSSE &lt; 2) {
1425         return false;
1426       }
1427       break;
1428 #endif // !LP64
1429   }
1430   return true;  // Match rules are supported by default.
1431 }
1432 
1433 //------------------------------------------------------------------------
1434 
1435 // Identify extra cases that we might want to provide match rules for vector nodes and
1436 // other intrinsics guarded with vector length (vlen) and element type (bt).
1437 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1438   if (!match_rule_supported(opcode)) {
1439     return false;
1440   }
1441   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1442   //   * SSE2 supports 128bit vectors for all types;
1443   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1444   //   * AVX2 supports 256bit vectors for all types;
1445   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1446   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1447   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1448   // And MaxVectorSize is taken into account as well.
1449   if (!vector_size_supported(bt, vlen)) {
1450     return false;
1451   }
1452   // Special cases which require vector length follow:
1453   //   * implementation limitations
1454   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1455   //   * 128bit vroundpd instruction is present only in AVX1
1456   int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;
1457   switch (opcode) {
1458     case Op_AbsVF:
1459     case Op_NegVF:
1460       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1461         return false; // 512bit vandps and vxorps are not available
1462       }
1463       break;
1464     case Op_AbsVD:
1465     case Op_NegVD:
1466       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1467         return false; // 512bit vandpd and vxorpd are not available
1468       }
1469       break;
1470     case Op_CMoveVF:
1471       if (vlen != 8) {
1472         return false; // implementation limitation (only vcmov8F_reg is present)
1473       }
1474       break;
1475     case Op_MacroLogicV:
1476       if (!VM_Version::supports_evex() ||
1477           ((size_in_bits != 512) &amp;&amp; !VM_Version::supports_avx512vl())) {
1478         return false;
1479       }
1480       break;
1481     case Op_CMoveVD:
1482       if (vlen != 4) {
1483         return false; // implementation limitation (only vcmov4D_reg is present)
1484       }
1485       break;
1486   }
1487   return true;  // Per default match rules are supported.
1488 }
1489 
1490 // x86 supports generic vector operands: vec and legVec.
1491 const bool Matcher::supports_generic_vector_operands = true;
1492 
1493 MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1494   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1495   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1496   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1497       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1498     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1499     return new legVecZOper();
1500   }
1501   if (legacy) {
1502     switch (ideal_reg) {
1503       case Op_VecS: return new legVecSOper();
1504       case Op_VecD: return new legVecDOper();
1505       case Op_VecX: return new legVecXOper();
1506       case Op_VecY: return new legVecYOper();
1507       case Op_VecZ: return new legVecZOper();
1508     }
1509   } else {
1510     switch (ideal_reg) {
1511       case Op_VecS: return new vecSOper();
1512       case Op_VecD: return new vecDOper();
1513       case Op_VecX: return new vecXOper();
1514       case Op_VecY: return new vecYOper();
1515       case Op_VecZ: return new vecZOper();
1516     }
1517   }
1518   ShouldNotReachHere();
1519   return NULL;
1520 }
1521 
1522 bool Matcher::is_generic_reg2reg_move(MachNode* m) {
1523   switch (m-&gt;rule()) {
1524     case MoveVec2Leg_rule:
1525     case MoveLeg2Vec_rule:
1526       return true;
1527     default:
1528       return false;
1529   }
1530 }
1531 
1532 bool Matcher::is_generic_vector(MachOper* opnd) {
1533   switch (opnd-&gt;opcode()) {
1534     case VEC:
1535     case LEGVEC:
1536       return true;
1537     default:
1538       return false;
1539   }
1540 }
1541 
1542 //------------------------------------------------------------------------
1543 
1544 const bool Matcher::has_predicated_vectors(void) {
1545   bool ret_value = false;
1546   if (UseAVX &gt; 2) {
1547     ret_value = VM_Version::supports_avx512vl();
1548   }
1549 
1550   return ret_value;
1551 }
1552 
1553 const int Matcher::float_pressure(int default_pressure_threshold) {
1554   int float_pressure_threshold = default_pressure_threshold;
1555 #ifdef _LP64
1556   if (UseAVX &gt; 2) {
1557     // Increase pressure threshold on machines with AVX3 which have
1558     // 2x more XMM registers.
1559     float_pressure_threshold = default_pressure_threshold * 2;
1560   }
1561 #endif
1562   return float_pressure_threshold;
1563 }
1564 
1565 // Max vector size in bytes. 0 if not supported.
1566 const int Matcher::vector_width_in_bytes(BasicType bt) {
1567   assert(is_java_primitive(bt), &quot;only primitive type vectors&quot;);
1568   if (UseSSE &lt; 2) return 0;
1569   // SSE2 supports 128bit vectors for all types.
1570   // AVX2 supports 256bit vectors for all types.
1571   // AVX2/EVEX supports 512bit vectors for all types.
1572   int size = (UseAVX &gt; 1) ? (1 &lt;&lt; UseAVX) * 8 : 16;
1573   // AVX1 supports 256bit vectors only for FLOAT and DOUBLE.
1574   if (UseAVX &gt; 0 &amp;&amp; (bt == T_FLOAT || bt == T_DOUBLE))
1575     size = (UseAVX &gt; 2) ? 64 : 32;
1576   if (UseAVX &gt; 2 &amp;&amp; (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))
1577     size = (VM_Version::supports_avx512bw()) ? 64 : 32;
1578   // Use flag to limit vector size.
1579   size = MIN2(size,(int)MaxVectorSize);
1580   // Minimum 2 values in vector (or 4 for bytes).
1581   switch (bt) {
1582   case T_DOUBLE:
1583   case T_LONG:
1584     if (size &lt; 16) return 0;
1585     break;
1586   case T_FLOAT:
1587   case T_INT:
1588     if (size &lt; 8) return 0;
1589     break;
1590   case T_BOOLEAN:
1591     if (size &lt; 4) return 0;
1592     break;
1593   case T_CHAR:
1594     if (size &lt; 4) return 0;
1595     break;
1596   case T_BYTE:
1597     if (size &lt; 4) return 0;
1598     break;
1599   case T_SHORT:
1600     if (size &lt; 4) return 0;
1601     break;
1602   default:
1603     ShouldNotReachHere();
1604   }
1605   return size;
1606 }
1607 
1608 // Limits on vector size (number of elements) loaded into vector.
1609 const int Matcher::max_vector_size(const BasicType bt) {
1610   return vector_width_in_bytes(bt)/type2aelembytes(bt);
1611 }
1612 const int Matcher::min_vector_size(const BasicType bt) {
1613   int max_size = max_vector_size(bt);
1614   // Min size which can be loaded into vector is 4 bytes.
1615   int size = (type2aelembytes(bt) == 1) ? 4 : 2;
1616   return MIN2(size,max_size);
1617 }
1618 
1619 // Vector ideal reg corresponding to specified size in bytes
1620 const uint Matcher::vector_ideal_reg(int size) {
1621   assert(MaxVectorSize &gt;= size, &quot;&quot;);
1622   switch(size) {
1623     case  4: return Op_VecS;
1624     case  8: return Op_VecD;
1625     case 16: return Op_VecX;
1626     case 32: return Op_VecY;
1627     case 64: return Op_VecZ;
1628   }
1629   ShouldNotReachHere();
1630   return 0;
1631 }
1632 
1633 // Only lowest bits of xmm reg are used for vector shift count.
1634 const uint Matcher::vector_shift_count_ideal_reg(int size) {
1635   return Op_VecS;
1636 }
1637 
1638 // x86 supports misaligned vectors store/load.
1639 const bool Matcher::misaligned_vectors_ok() {
1640   return true;
1641 }
1642 
1643 // x86 AES instructions are compatible with SunJCE expanded
1644 // keys, hence we do not need to pass the original key to stubs
1645 const bool Matcher::pass_original_key_for_aes() {
1646   return false;
1647 }
1648 
1649 
1650 const bool Matcher::convi2l_type_required = true;
1651 
1652 // Check for shift by small constant as well
1653 static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1654   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
1655       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
1656       // Are there other uses besides address expressions?
1657       !matcher-&gt;is_visited(shift)) {
1658     address_visited.set(shift-&gt;_idx); // Flag as address_visited
1659     mstack.push(shift-&gt;in(2), Matcher::Visit);
1660     Node *conv = shift-&gt;in(1);
1661 #ifdef _LP64
1662     // Allow Matcher to match the rule which bypass
1663     // ConvI2L operation for an array index on LP64
1664     // if the index value is positive.
1665     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1666         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1667         // Are there other uses besides address expressions?
1668         !matcher-&gt;is_visited(conv)) {
1669       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1670       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1671     } else
1672 #endif
1673       mstack.push(conv, Matcher::Pre_Visit);
1674     return true;
1675   }
1676   return false;
1677 }
1678 
1679 // This function identifies sub-graphs in which a &#39;load&#39; node is
1680 // input to two different nodes, and such that it can be matched
1681 // with BMI instructions like blsi, blsr, etc.
1682 // Example : for b = -a[i] &amp; a[i] can be matched to blsi r32, m32.
1683 // The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*
1684 // refers to the same node.
1685 //
1686 // Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)
1687 // This is a temporary solution until we make DAGs expressible in ADL.
1688 template&lt;typename ConType&gt;
1689 class FusedPatternMatcher {
1690   Node* _op1_node;
1691   Node* _mop_node;
1692   int _con_op;
1693 
1694   static int match_next(Node* n, int next_op, int next_op_idx) {
1695     if (n-&gt;in(1) == NULL || n-&gt;in(2) == NULL) {
1696       return -1;
1697     }
1698 
1699     if (next_op_idx == -1) { // n is commutative, try rotations
1700       if (n-&gt;in(1)-&gt;Opcode() == next_op) {
1701         return 1;
1702       } else if (n-&gt;in(2)-&gt;Opcode() == next_op) {
1703         return 2;
1704       }
1705     } else {
1706       assert(next_op_idx &gt; 0 &amp;&amp; next_op_idx &lt;= 2, &quot;Bad argument index&quot;);
1707       if (n-&gt;in(next_op_idx)-&gt;Opcode() == next_op) {
1708         return next_op_idx;
1709       }
1710     }
1711     return -1;
1712   }
1713 
1714  public:
1715   FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :
1716     _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }
1717 
1718   bool match(int op1, int op1_op2_idx,  // op1 and the index of the op1-&gt;op2 edge, -1 if op1 is commutative
1719              int op2, int op2_con_idx,  // op2 and the index of the op2-&gt;con edge, -1 if op2 is commutative
1720              typename ConType::NativeType con_value) {
1721     if (_op1_node-&gt;Opcode() != op1) {
1722       return false;
1723     }
1724     if (_mop_node-&gt;outcnt() &gt; 2) {
1725       return false;
1726     }
1727     op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);
1728     if (op1_op2_idx == -1) {
1729       return false;
1730     }
1731     // Memory operation must be the other edge
1732     int op1_mop_idx = (op1_op2_idx &amp; 1) + 1;
1733 
1734     // Check that the mop node is really what we want
1735     if (_op1_node-&gt;in(op1_mop_idx) == _mop_node) {
1736       Node* op2_node = _op1_node-&gt;in(op1_op2_idx);
1737       if (op2_node-&gt;outcnt() &gt; 1) {
1738         return false;
1739       }
1740       assert(op2_node-&gt;Opcode() == op2, &quot;Should be&quot;);
1741       op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);
1742       if (op2_con_idx == -1) {
1743         return false;
1744       }
1745       // Memory operation must be the other edge
1746       int op2_mop_idx = (op2_con_idx &amp; 1) + 1;
1747       // Check that the memory operation is the same node
1748       if (op2_node-&gt;in(op2_mop_idx) == _mop_node) {
1749         // Now check the constant
1750         const Type* con_type = op2_node-&gt;in(op2_con_idx)-&gt;bottom_type();
1751         if (con_type != Type::TOP &amp;&amp; ConType::as_self(con_type)-&gt;get_con() == con_value) {
1752           return true;
1753         }
1754       }
1755     }
1756     return false;
1757   }
1758 };
1759 
1760 static bool is_bmi_pattern(Node* n, Node* m) {
1761   assert(UseBMI1Instructions, &quot;sanity&quot;);
1762   if (n != NULL &amp;&amp; m != NULL) {
1763     if (m-&gt;Opcode() == Op_LoadI) {
1764       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
1765       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
1766              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
1767              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
1768     } else if (m-&gt;Opcode() == Op_LoadL) {
1769       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
1770       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
1771              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
1772              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
1773     }
1774   }
1775   return false;
1776 }
1777 
1778 // Should the matcher clone input &#39;m&#39; of node &#39;n&#39;?
1779 bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack&amp; mstack) {
1780   // If &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone the input &#39;m&#39;.
1781   if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
1782     mstack.push(m, Visit);
1783     return true;
1784   }
1785   return false;
1786 }
1787 
1788 // Should the Matcher clone shifts on addressing modes, expecting them
1789 // to be subsumed into complex addressing expressions or compute them
1790 // into registers?
1791 bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1792   Node *off = m-&gt;in(AddPNode::Offset);
1793   if (off-&gt;is_Con()) {
1794     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1795     Node *adr = m-&gt;in(AddPNode::Address);
1796 
1797     // Intel can handle 2 adds in addressing mode
1798     // AtomicAdd is not an addressing expression.
1799     // Cheap to find it by looking for screwy base.
1800     if (adr-&gt;is_AddP() &amp;&amp;
1801         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1802         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1803         // Are there other uses besides address expressions?
1804         !is_visited(adr)) {
1805       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1806       Node *shift = adr-&gt;in(AddPNode::Offset);
1807       if (!clone_shift(shift, this, mstack, address_visited)) {
1808         mstack.push(shift, Pre_Visit);
1809       }
1810       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1811       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
1812     } else {
1813       mstack.push(adr, Pre_Visit);
1814     }
1815 
1816     // Clone X+offset as it also folds into most addressing expressions
1817     mstack.push(off, Visit);
1818     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1819     return true;
1820   } else if (clone_shift(off, this, mstack, address_visited)) {
1821     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1822     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1823     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1824     return true;
1825   }
1826   return false;
1827 }
1828 
1829 void Compile::reshape_address(AddPNode* addp) {
1830 }
1831 
1832 static inline uint vector_length(const MachNode* n) {
1833   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1834   return vt-&gt;length();
1835 }
1836 
1837 static inline uint vector_length(const MachNode* use, MachOper* opnd) {
1838   uint def_idx = use-&gt;operand_index(opnd);
1839   Node* def = use-&gt;in(def_idx);
1840   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length();
1841 }
1842 
1843 static inline uint vector_length_in_bytes(const MachNode* n) {
1844   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1845   return vt-&gt;length_in_bytes();
1846 }
1847 
1848 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1849   uint def_idx = use-&gt;operand_index(opnd);
1850   Node* def = use-&gt;in(def_idx);
1851   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1852 }
1853 
1854 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1855   switch(vector_length_in_bytes(n)) {
1856     case  4: // fall-through
1857     case  8: // fall-through
1858     case 16: return Assembler::AVX_128bit;
1859     case 32: return Assembler::AVX_256bit;
1860     case 64: return Assembler::AVX_512bit;
1861 
1862     default: {
1863       ShouldNotReachHere();
1864       return Assembler::AVX_NoVec;
1865     }
1866   }
1867 }
1868 
1869 // Helper methods for MachSpillCopyNode::implementation().
1870 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1871                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1872   // In 64-bit VM size calculation is very complex. Emitting instructions
1873   // into scratch buffer is used to get size in 64-bit VM.
1874   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1875   assert(ireg == Op_VecS || // 32bit vector
1876          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1877          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1878          &quot;no non-adjacent vector moves&quot; );
1879   if (cbuf) {
1880     C2_MacroAssembler _masm(cbuf);
1881     int offset = __ offset();
1882     switch (ireg) {
1883     case Op_VecS: // copy whole register
1884     case Op_VecD:
1885     case Op_VecX:
1886 #ifndef _LP64
1887       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1888 #else
1889       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1890         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1891       } else {
1892         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1893      }
1894 #endif
1895       break;
1896     case Op_VecY:
1897 #ifndef _LP64
1898       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1899 #else
1900       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1901         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1902       } else {
1903         __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1904      }
1905 #endif
1906       break;
1907     case Op_VecZ:
1908       __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);
1909       break;
1910     default:
1911       ShouldNotReachHere();
1912     }
1913     int size = __ offset() - offset;
1914 #ifdef ASSERT
1915     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1916     assert(!do_size || size == 4, &quot;incorrect size calculattion&quot;);
1917 #endif
1918     return size;
1919 #ifndef PRODUCT
1920   } else if (!do_size) {
1921     switch (ireg) {
1922     case Op_VecS:
1923     case Op_VecD:
1924     case Op_VecX:
1925       st-&gt;print(&quot;movdqu  %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1926       break;
1927     case Op_VecY:
1928     case Op_VecZ:
1929       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1930       break;
1931     default:
1932       ShouldNotReachHere();
1933     }
1934 #endif
1935   }
1936   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1937   return (UseAVX &gt; 2) ? 6 : 4;
1938 }
1939 
1940 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1941                      int stack_offset, int reg, uint ireg, outputStream* st) {
1942   // In 64-bit VM size calculation is very complex. Emitting instructions
1943   // into scratch buffer is used to get size in 64-bit VM.
1944   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1945   if (cbuf) {
1946     C2_MacroAssembler _masm(cbuf);
1947     int offset = __ offset();
1948     if (is_load) {
1949       switch (ireg) {
1950       case Op_VecS:
1951         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1952         break;
1953       case Op_VecD:
1954         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1955         break;
1956       case Op_VecX:
1957 #ifndef _LP64
1958         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1959 #else
1960         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1961           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1962         } else {
1963           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1964           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1965         }
1966 #endif
1967         break;
1968       case Op_VecY:
1969 #ifndef _LP64
1970         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1971 #else
1972         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1973           __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1974         } else {
1975           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1976           __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1977         }
1978 #endif
1979         break;
1980       case Op_VecZ:
1981         __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);
1982         break;
1983       default:
1984         ShouldNotReachHere();
1985       }
1986     } else { // store
1987       switch (ireg) {
1988       case Op_VecS:
1989         __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1990         break;
1991       case Op_VecD:
1992         __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1993         break;
1994       case Op_VecX:
1995 #ifndef _LP64
1996         __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1997 #else
1998         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1999           __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
2000         }
2001         else {
2002           __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
2003         }
2004 #endif
2005         break;
2006       case Op_VecY:
2007 #ifndef _LP64
2008         __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
2009 #else
2010         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
2011           __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
2012         }
2013         else {
2014           __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
2015         }
2016 #endif
2017         break;
2018       case Op_VecZ:
2019         __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);
2020         break;
2021       default:
2022         ShouldNotReachHere();
2023       }
2024     }
2025     int size = __ offset() - offset;
2026 #ifdef ASSERT
2027     int offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt; 0x80) ? 1 : (UseAVX &gt; 2) ? 6 : 4);
2028     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
2029     assert(!do_size || size == (5+offset_size), &quot;incorrect size calculattion&quot;);
2030 #endif
2031     return size;
2032 #ifndef PRODUCT
2033   } else if (!do_size) {
2034     if (is_load) {
2035       switch (ireg) {
2036       case Op_VecS:
2037         st-&gt;print(&quot;movd    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2038         break;
2039       case Op_VecD:
2040         st-&gt;print(&quot;movq    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2041         break;
2042        case Op_VecX:
2043         st-&gt;print(&quot;movdqu  %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2044         break;
2045       case Op_VecY:
2046       case Op_VecZ:
2047         st-&gt;print(&quot;vmovdqu %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2048         break;
2049       default:
2050         ShouldNotReachHere();
2051       }
2052     } else { // store
2053       switch (ireg) {
2054       case Op_VecS:
2055         st-&gt;print(&quot;movd    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2056         break;
2057       case Op_VecD:
2058         st-&gt;print(&quot;movq    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2059         break;
2060        case Op_VecX:
2061         st-&gt;print(&quot;movdqu  [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2062         break;
2063       case Op_VecY:
2064       case Op_VecZ:
2065         st-&gt;print(&quot;vmovdqu [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2066         break;
2067       default:
2068         ShouldNotReachHere();
2069       }
2070     }
2071 #endif
2072   }
2073   bool is_single_byte = false;
2074   int vec_len = 0;
2075   if ((UseAVX &gt; 2) &amp;&amp; (stack_offset != 0)) {
2076     int tuple_type = Assembler::EVEX_FVM;
2077     int input_size = Assembler::EVEX_32bit;
2078     switch (ireg) {
2079     case Op_VecS:
2080       tuple_type = Assembler::EVEX_T1S;
2081       break;
2082     case Op_VecD:
2083       tuple_type = Assembler::EVEX_T1S;
2084       input_size = Assembler::EVEX_64bit;
2085       break;
2086     case Op_VecX:
2087       break;
2088     case Op_VecY:
2089       vec_len = 1;
2090       break;
2091     case Op_VecZ:
2092       vec_len = 2;
2093       break;
2094     }
2095     is_single_byte = Assembler::query_compressed_disp_byte(stack_offset, true, vec_len, tuple_type, input_size, 0);
2096   }
2097   int offset_size = 0;
2098   int size = 5;
2099   if (UseAVX &gt; 2 ) {
2100     if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len == 2)) {
2101       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
2102       size += 2; // Need an additional two bytes for EVEX encoding
2103     } else if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len &lt; 2)) {
2104       offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
2105     } else {
2106       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
2107       size += 2; // Need an additional two bytes for EVEX encodding
2108     }
2109   } else {
2110     offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
2111   }
2112   // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
2113   return size+offset_size;
2114 }
2115 
2116 static inline jint replicate4_imm(int con, int width) {
2117   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 32bit.
2118   assert(width == 1 || width == 2, &quot;only byte or short types here&quot;);
2119   int bit_width = width * 8;
2120   jint val = con;
2121   val &amp;= (1 &lt;&lt; bit_width) - 1;  // mask off sign bits
2122   while(bit_width &lt; 32) {
2123     val |= (val &lt;&lt; bit_width);
2124     bit_width &lt;&lt;= 1;
2125   }
2126   return val;
2127 }
2128 
2129 static inline jlong replicate8_imm(int con, int width) {
2130   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
2131   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
2132   int bit_width = width * 8;
2133   jlong val = con;
2134   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
2135   while(bit_width &lt; 64) {
2136     val |= (val &lt;&lt; bit_width);
2137     bit_width &lt;&lt;= 1;
2138   }
2139   return val;
2140 }
2141 
2142 #ifndef PRODUCT
2143   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
2144     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
2145   }
2146 #endif
2147 
2148   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
2149     C2_MacroAssembler _masm(&amp;cbuf);
2150     __ nop(_count);
2151   }
2152 
2153   uint MachNopNode::size(PhaseRegAlloc*) const {
2154     return _count;
2155   }
2156 
2157 #ifndef PRODUCT
2158   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
2159     st-&gt;print(&quot;# breakpoint&quot;);
2160   }
2161 #endif
2162 
2163   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
2164     C2_MacroAssembler _masm(&amp;cbuf);
2165     __ int3();
2166   }
2167 
2168   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2169     return MachNode::size(ra_);
2170   }
2171 
2172 %}
2173 
2174 encode %{
2175 
2176   enc_class call_epilog %{
2177     if (VerifyStackAtCalls) {
2178       // Check that stack depth is unchanged: find majik cookie on stack
2179       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
2180       C2_MacroAssembler _masm(&amp;cbuf);
2181       Label L;
2182       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2183       __ jccb(Assembler::equal, L);
2184       // Die if stack mismatch
2185       __ int3();
2186       __ bind(L);
2187     }
2188   %}
2189 
2190 %}
2191 
2192 
2193 //----------OPERANDS-----------------------------------------------------------
2194 // Operand definitions must precede instruction definitions for correct parsing
2195 // in the ADLC because operands constitute user defined types which are used in
2196 // instruction definitions.
2197 
2198 // Vectors
2199 
2200 // Dummy generic vector class. Should be used for all vector operands.
2201 // Replaced with vec[SDXYZ] during post-selection pass.
2202 operand vec() %{
2203   constraint(ALLOC_IN_RC(dynamic));
2204   match(VecX);
2205   match(VecY);
2206   match(VecZ);
2207   match(VecS);
2208   match(VecD);
2209 
2210   format %{ %}
2211   interface(REG_INTER);
2212 %}
2213 
2214 // Dummy generic legacy vector class. Should be used for all legacy vector operands.
2215 // Replaced with legVec[SDXYZ] during post-selection cleanup.
2216 // Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
2217 // runtime code generation via reg_class_dynamic.
2218 operand legVec() %{
2219   constraint(ALLOC_IN_RC(dynamic));
2220   match(VecX);
2221   match(VecY);
2222   match(VecZ);
2223   match(VecS);
2224   match(VecD);
2225 
2226   format %{ %}
2227   interface(REG_INTER);
2228 %}
2229 
2230 // Replaces vec during post-selection cleanup. See above.
2231 operand vecS() %{
2232   constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
2233   match(VecS);
2234 
2235   format %{ %}
2236   interface(REG_INTER);
2237 %}
2238 
2239 // Replaces legVec during post-selection cleanup. See above.
2240 operand legVecS() %{
2241   constraint(ALLOC_IN_RC(vectors_reg_legacy));
2242   match(VecS);
2243 
2244   format %{ %}
2245   interface(REG_INTER);
2246 %}
2247 
2248 // Replaces vec during post-selection cleanup. See above.
2249 operand vecD() %{
2250   constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
2251   match(VecD);
2252 
2253   format %{ %}
2254   interface(REG_INTER);
2255 %}
2256 
2257 // Replaces legVec during post-selection cleanup. See above.
2258 operand legVecD() %{
2259   constraint(ALLOC_IN_RC(vectord_reg_legacy));
2260   match(VecD);
2261 
2262   format %{ %}
2263   interface(REG_INTER);
2264 %}
2265 
2266 // Replaces vec during post-selection cleanup. See above.
2267 operand vecX() %{
2268   constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
2269   match(VecX);
2270 
2271   format %{ %}
2272   interface(REG_INTER);
2273 %}
2274 
2275 // Replaces legVec during post-selection cleanup. See above.
2276 operand legVecX() %{
2277   constraint(ALLOC_IN_RC(vectorx_reg_legacy));
2278   match(VecX);
2279 
2280   format %{ %}
2281   interface(REG_INTER);
2282 %}
2283 
2284 // Replaces vec during post-selection cleanup. See above.
2285 operand vecY() %{
2286   constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
2287   match(VecY);
2288 
2289   format %{ %}
2290   interface(REG_INTER);
2291 %}
2292 
2293 // Replaces legVec during post-selection cleanup. See above.
2294 operand legVecY() %{
2295   constraint(ALLOC_IN_RC(vectory_reg_legacy));
2296   match(VecY);
2297 
2298   format %{ %}
2299   interface(REG_INTER);
2300 %}
2301 
2302 // Replaces vec during post-selection cleanup. See above.
2303 operand vecZ() %{
2304   constraint(ALLOC_IN_RC(vectorz_reg));
2305   match(VecZ);
2306 
2307   format %{ %}
2308   interface(REG_INTER);
2309 %}
2310 
2311 // Replaces legVec during post-selection cleanup. See above.
2312 operand legVecZ() %{
2313   constraint(ALLOC_IN_RC(vectorz_reg_legacy));
2314   match(VecZ);
2315 
2316   format %{ %}
2317   interface(REG_INTER);
2318 %}
2319 
2320 // Comparison Code for FP conditional move
2321 operand cmpOp_vcmppd() %{
2322   match(Bool);
2323 
2324   predicate(n-&gt;as_Bool()-&gt;_test._test != BoolTest::overflow &amp;&amp;
2325             n-&gt;as_Bool()-&gt;_test._test != BoolTest::no_overflow);
2326   format %{ &quot;&quot; %}
2327   interface(COND_INTER) %{
2328     equal        (0x0, &quot;eq&quot;);
2329     less         (0x1, &quot;lt&quot;);
2330     less_equal   (0x2, &quot;le&quot;);
2331     not_equal    (0xC, &quot;ne&quot;);
2332     greater_equal(0xD, &quot;ge&quot;);
2333     greater      (0xE, &quot;gt&quot;);
2334     //TODO cannot compile (adlc breaks) without two next lines with error:
2335     // x86_64.ad(13987) Syntax Error: :In operand cmpOp_vcmppd: Do not support this encode constant: &#39; %{
2336     // equal&#39; for overflow.
2337     overflow     (0x20, &quot;o&quot;);  // not really supported by the instruction
2338     no_overflow  (0x21, &quot;no&quot;); // not really supported by the instruction
2339   %}
2340 %}
2341 
2342 
2343 // INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)
2344 
2345 // ============================================================================
2346 
2347 instruct ShouldNotReachHere() %{
2348   match(Halt);
2349   format %{ &quot;ud2\t# ShouldNotReachHere&quot; %}
2350   ins_encode %{
2351     __ stop(_halt_reason);
2352   %}
2353   ins_pipe(pipe_slow);
2354 %}
2355 
2356 // =================================EVEX special===============================
2357 
2358 instruct setMask(rRegI dst, rRegI src) %{
2359   predicate(Matcher::has_predicated_vectors());
2360   match(Set dst (SetVectMaskI  src));
2361   effect(TEMP dst);
2362   format %{ &quot;setvectmask   $dst, $src&quot; %}
2363   ins_encode %{
2364     __ setvectmask($dst$$Register, $src$$Register);
2365   %}
2366   ins_pipe(pipe_slow);
2367 %}
2368 
2369 // ============================================================================
2370 
2371 instruct addF_reg(regF dst, regF src) %{
2372   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2373   match(Set dst (AddF dst src));
2374 
2375   format %{ &quot;addss   $dst, $src&quot; %}
2376   ins_cost(150);
2377   ins_encode %{
2378     __ addss($dst$$XMMRegister, $src$$XMMRegister);
2379   %}
2380   ins_pipe(pipe_slow);
2381 %}
2382 
2383 instruct addF_mem(regF dst, memory src) %{
2384   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2385   match(Set dst (AddF dst (LoadF src)));
2386 
2387   format %{ &quot;addss   $dst, $src&quot; %}
2388   ins_cost(150);
2389   ins_encode %{
2390     __ addss($dst$$XMMRegister, $src$$Address);
2391   %}
2392   ins_pipe(pipe_slow);
2393 %}
2394 
2395 instruct addF_imm(regF dst, immF con) %{
2396   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2397   match(Set dst (AddF dst con));
2398   format %{ &quot;addss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2399   ins_cost(150);
2400   ins_encode %{
2401     __ addss($dst$$XMMRegister, $constantaddress($con));
2402   %}
2403   ins_pipe(pipe_slow);
2404 %}
2405 
2406 instruct addF_reg_reg(regF dst, regF src1, regF src2) %{
2407   predicate(UseAVX &gt; 0);
2408   match(Set dst (AddF src1 src2));
2409 
2410   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2411   ins_cost(150);
2412   ins_encode %{
2413     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2414   %}
2415   ins_pipe(pipe_slow);
2416 %}
2417 
2418 instruct addF_reg_mem(regF dst, regF src1, memory src2) %{
2419   predicate(UseAVX &gt; 0);
2420   match(Set dst (AddF src1 (LoadF src2)));
2421 
2422   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2423   ins_cost(150);
2424   ins_encode %{
2425     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2426   %}
2427   ins_pipe(pipe_slow);
2428 %}
2429 
2430 instruct addF_reg_imm(regF dst, regF src, immF con) %{
2431   predicate(UseAVX &gt; 0);
2432   match(Set dst (AddF src con));
2433 
2434   format %{ &quot;vaddss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2435   ins_cost(150);
2436   ins_encode %{
2437     __ vaddss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2438   %}
2439   ins_pipe(pipe_slow);
2440 %}
2441 
2442 instruct addD_reg(regD dst, regD src) %{
2443   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2444   match(Set dst (AddD dst src));
2445 
2446   format %{ &quot;addsd   $dst, $src&quot; %}
2447   ins_cost(150);
2448   ins_encode %{
2449     __ addsd($dst$$XMMRegister, $src$$XMMRegister);
2450   %}
2451   ins_pipe(pipe_slow);
2452 %}
2453 
2454 instruct addD_mem(regD dst, memory src) %{
2455   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2456   match(Set dst (AddD dst (LoadD src)));
2457 
2458   format %{ &quot;addsd   $dst, $src&quot; %}
2459   ins_cost(150);
2460   ins_encode %{
2461     __ addsd($dst$$XMMRegister, $src$$Address);
2462   %}
2463   ins_pipe(pipe_slow);
2464 %}
2465 
2466 instruct addD_imm(regD dst, immD con) %{
2467   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2468   match(Set dst (AddD dst con));
2469   format %{ &quot;addsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2470   ins_cost(150);
2471   ins_encode %{
2472     __ addsd($dst$$XMMRegister, $constantaddress($con));
2473   %}
2474   ins_pipe(pipe_slow);
2475 %}
2476 
2477 instruct addD_reg_reg(regD dst, regD src1, regD src2) %{
2478   predicate(UseAVX &gt; 0);
2479   match(Set dst (AddD src1 src2));
2480 
2481   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2482   ins_cost(150);
2483   ins_encode %{
2484     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2485   %}
2486   ins_pipe(pipe_slow);
2487 %}
2488 
2489 instruct addD_reg_mem(regD dst, regD src1, memory src2) %{
2490   predicate(UseAVX &gt; 0);
2491   match(Set dst (AddD src1 (LoadD src2)));
2492 
2493   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2494   ins_cost(150);
2495   ins_encode %{
2496     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2497   %}
2498   ins_pipe(pipe_slow);
2499 %}
2500 
2501 instruct addD_reg_imm(regD dst, regD src, immD con) %{
2502   predicate(UseAVX &gt; 0);
2503   match(Set dst (AddD src con));
2504 
2505   format %{ &quot;vaddsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2506   ins_cost(150);
2507   ins_encode %{
2508     __ vaddsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2509   %}
2510   ins_pipe(pipe_slow);
2511 %}
2512 
2513 instruct subF_reg(regF dst, regF src) %{
2514   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2515   match(Set dst (SubF dst src));
2516 
2517   format %{ &quot;subss   $dst, $src&quot; %}
2518   ins_cost(150);
2519   ins_encode %{
2520     __ subss($dst$$XMMRegister, $src$$XMMRegister);
2521   %}
2522   ins_pipe(pipe_slow);
2523 %}
2524 
2525 instruct subF_mem(regF dst, memory src) %{
2526   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2527   match(Set dst (SubF dst (LoadF src)));
2528 
2529   format %{ &quot;subss   $dst, $src&quot; %}
2530   ins_cost(150);
2531   ins_encode %{
2532     __ subss($dst$$XMMRegister, $src$$Address);
2533   %}
2534   ins_pipe(pipe_slow);
2535 %}
2536 
2537 instruct subF_imm(regF dst, immF con) %{
2538   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2539   match(Set dst (SubF dst con));
2540   format %{ &quot;subss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2541   ins_cost(150);
2542   ins_encode %{
2543     __ subss($dst$$XMMRegister, $constantaddress($con));
2544   %}
2545   ins_pipe(pipe_slow);
2546 %}
2547 
2548 instruct subF_reg_reg(regF dst, regF src1, regF src2) %{
2549   predicate(UseAVX &gt; 0);
2550   match(Set dst (SubF src1 src2));
2551 
2552   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2553   ins_cost(150);
2554   ins_encode %{
2555     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2556   %}
2557   ins_pipe(pipe_slow);
2558 %}
2559 
2560 instruct subF_reg_mem(regF dst, regF src1, memory src2) %{
2561   predicate(UseAVX &gt; 0);
2562   match(Set dst (SubF src1 (LoadF src2)));
2563 
2564   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2565   ins_cost(150);
2566   ins_encode %{
2567     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2568   %}
2569   ins_pipe(pipe_slow);
2570 %}
2571 
2572 instruct subF_reg_imm(regF dst, regF src, immF con) %{
2573   predicate(UseAVX &gt; 0);
2574   match(Set dst (SubF src con));
2575 
2576   format %{ &quot;vsubss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2577   ins_cost(150);
2578   ins_encode %{
2579     __ vsubss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2580   %}
2581   ins_pipe(pipe_slow);
2582 %}
2583 
2584 instruct subD_reg(regD dst, regD src) %{
2585   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2586   match(Set dst (SubD dst src));
2587 
2588   format %{ &quot;subsd   $dst, $src&quot; %}
2589   ins_cost(150);
2590   ins_encode %{
2591     __ subsd($dst$$XMMRegister, $src$$XMMRegister);
2592   %}
2593   ins_pipe(pipe_slow);
2594 %}
2595 
2596 instruct subD_mem(regD dst, memory src) %{
2597   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2598   match(Set dst (SubD dst (LoadD src)));
2599 
2600   format %{ &quot;subsd   $dst, $src&quot; %}
2601   ins_cost(150);
2602   ins_encode %{
2603     __ subsd($dst$$XMMRegister, $src$$Address);
2604   %}
2605   ins_pipe(pipe_slow);
2606 %}
2607 
2608 instruct subD_imm(regD dst, immD con) %{
2609   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2610   match(Set dst (SubD dst con));
2611   format %{ &quot;subsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2612   ins_cost(150);
2613   ins_encode %{
2614     __ subsd($dst$$XMMRegister, $constantaddress($con));
2615   %}
2616   ins_pipe(pipe_slow);
2617 %}
2618 
2619 instruct subD_reg_reg(regD dst, regD src1, regD src2) %{
2620   predicate(UseAVX &gt; 0);
2621   match(Set dst (SubD src1 src2));
2622 
2623   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2624   ins_cost(150);
2625   ins_encode %{
2626     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2627   %}
2628   ins_pipe(pipe_slow);
2629 %}
2630 
2631 instruct subD_reg_mem(regD dst, regD src1, memory src2) %{
2632   predicate(UseAVX &gt; 0);
2633   match(Set dst (SubD src1 (LoadD src2)));
2634 
2635   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2636   ins_cost(150);
2637   ins_encode %{
2638     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2639   %}
2640   ins_pipe(pipe_slow);
2641 %}
2642 
2643 instruct subD_reg_imm(regD dst, regD src, immD con) %{
2644   predicate(UseAVX &gt; 0);
2645   match(Set dst (SubD src con));
2646 
2647   format %{ &quot;vsubsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2648   ins_cost(150);
2649   ins_encode %{
2650     __ vsubsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2651   %}
2652   ins_pipe(pipe_slow);
2653 %}
2654 
2655 instruct mulF_reg(regF dst, regF src) %{
2656   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2657   match(Set dst (MulF dst src));
2658 
2659   format %{ &quot;mulss   $dst, $src&quot; %}
2660   ins_cost(150);
2661   ins_encode %{
2662     __ mulss($dst$$XMMRegister, $src$$XMMRegister);
2663   %}
2664   ins_pipe(pipe_slow);
2665 %}
2666 
2667 instruct mulF_mem(regF dst, memory src) %{
2668   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2669   match(Set dst (MulF dst (LoadF src)));
2670 
2671   format %{ &quot;mulss   $dst, $src&quot; %}
2672   ins_cost(150);
2673   ins_encode %{
2674     __ mulss($dst$$XMMRegister, $src$$Address);
2675   %}
2676   ins_pipe(pipe_slow);
2677 %}
2678 
2679 instruct mulF_imm(regF dst, immF con) %{
2680   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2681   match(Set dst (MulF dst con));
2682   format %{ &quot;mulss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2683   ins_cost(150);
2684   ins_encode %{
2685     __ mulss($dst$$XMMRegister, $constantaddress($con));
2686   %}
2687   ins_pipe(pipe_slow);
2688 %}
2689 
2690 instruct mulF_reg_reg(regF dst, regF src1, regF src2) %{
2691   predicate(UseAVX &gt; 0);
2692   match(Set dst (MulF src1 src2));
2693 
2694   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2695   ins_cost(150);
2696   ins_encode %{
2697     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2698   %}
2699   ins_pipe(pipe_slow);
2700 %}
2701 
2702 instruct mulF_reg_mem(regF dst, regF src1, memory src2) %{
2703   predicate(UseAVX &gt; 0);
2704   match(Set dst (MulF src1 (LoadF src2)));
2705 
2706   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2707   ins_cost(150);
2708   ins_encode %{
2709     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2710   %}
2711   ins_pipe(pipe_slow);
2712 %}
2713 
2714 instruct mulF_reg_imm(regF dst, regF src, immF con) %{
2715   predicate(UseAVX &gt; 0);
2716   match(Set dst (MulF src con));
2717 
2718   format %{ &quot;vmulss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2719   ins_cost(150);
2720   ins_encode %{
2721     __ vmulss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2722   %}
2723   ins_pipe(pipe_slow);
2724 %}
2725 
2726 instruct mulD_reg(regD dst, regD src) %{
2727   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2728   match(Set dst (MulD dst src));
2729 
2730   format %{ &quot;mulsd   $dst, $src&quot; %}
2731   ins_cost(150);
2732   ins_encode %{
2733     __ mulsd($dst$$XMMRegister, $src$$XMMRegister);
2734   %}
2735   ins_pipe(pipe_slow);
2736 %}
2737 
2738 instruct mulD_mem(regD dst, memory src) %{
2739   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2740   match(Set dst (MulD dst (LoadD src)));
2741 
2742   format %{ &quot;mulsd   $dst, $src&quot; %}
2743   ins_cost(150);
2744   ins_encode %{
2745     __ mulsd($dst$$XMMRegister, $src$$Address);
2746   %}
2747   ins_pipe(pipe_slow);
2748 %}
2749 
2750 instruct mulD_imm(regD dst, immD con) %{
2751   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2752   match(Set dst (MulD dst con));
2753   format %{ &quot;mulsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2754   ins_cost(150);
2755   ins_encode %{
2756     __ mulsd($dst$$XMMRegister, $constantaddress($con));
2757   %}
2758   ins_pipe(pipe_slow);
2759 %}
2760 
2761 instruct mulD_reg_reg(regD dst, regD src1, regD src2) %{
2762   predicate(UseAVX &gt; 0);
2763   match(Set dst (MulD src1 src2));
2764 
2765   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2766   ins_cost(150);
2767   ins_encode %{
2768     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2769   %}
2770   ins_pipe(pipe_slow);
2771 %}
2772 
2773 instruct mulD_reg_mem(regD dst, regD src1, memory src2) %{
2774   predicate(UseAVX &gt; 0);
2775   match(Set dst (MulD src1 (LoadD src2)));
2776 
2777   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2778   ins_cost(150);
2779   ins_encode %{
2780     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2781   %}
2782   ins_pipe(pipe_slow);
2783 %}
2784 
2785 instruct mulD_reg_imm(regD dst, regD src, immD con) %{
2786   predicate(UseAVX &gt; 0);
2787   match(Set dst (MulD src con));
2788 
2789   format %{ &quot;vmulsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2790   ins_cost(150);
2791   ins_encode %{
2792     __ vmulsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2793   %}
2794   ins_pipe(pipe_slow);
2795 %}
2796 
2797 instruct divF_reg(regF dst, regF src) %{
2798   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2799   match(Set dst (DivF dst src));
2800 
2801   format %{ &quot;divss   $dst, $src&quot; %}
2802   ins_cost(150);
2803   ins_encode %{
2804     __ divss($dst$$XMMRegister, $src$$XMMRegister);
2805   %}
2806   ins_pipe(pipe_slow);
2807 %}
2808 
2809 instruct divF_mem(regF dst, memory src) %{
2810   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2811   match(Set dst (DivF dst (LoadF src)));
2812 
2813   format %{ &quot;divss   $dst, $src&quot; %}
2814   ins_cost(150);
2815   ins_encode %{
2816     __ divss($dst$$XMMRegister, $src$$Address);
2817   %}
2818   ins_pipe(pipe_slow);
2819 %}
2820 
2821 instruct divF_imm(regF dst, immF con) %{
2822   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2823   match(Set dst (DivF dst con));
2824   format %{ &quot;divss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2825   ins_cost(150);
2826   ins_encode %{
2827     __ divss($dst$$XMMRegister, $constantaddress($con));
2828   %}
2829   ins_pipe(pipe_slow);
2830 %}
2831 
2832 instruct divF_reg_reg(regF dst, regF src1, regF src2) %{
2833   predicate(UseAVX &gt; 0);
2834   match(Set dst (DivF src1 src2));
2835 
2836   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2837   ins_cost(150);
2838   ins_encode %{
2839     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2840   %}
2841   ins_pipe(pipe_slow);
2842 %}
2843 
2844 instruct divF_reg_mem(regF dst, regF src1, memory src2) %{
2845   predicate(UseAVX &gt; 0);
2846   match(Set dst (DivF src1 (LoadF src2)));
2847 
2848   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2849   ins_cost(150);
2850   ins_encode %{
2851     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2852   %}
2853   ins_pipe(pipe_slow);
2854 %}
2855 
2856 instruct divF_reg_imm(regF dst, regF src, immF con) %{
2857   predicate(UseAVX &gt; 0);
2858   match(Set dst (DivF src con));
2859 
2860   format %{ &quot;vdivss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2861   ins_cost(150);
2862   ins_encode %{
2863     __ vdivss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2864   %}
2865   ins_pipe(pipe_slow);
2866 %}
2867 
2868 instruct divD_reg(regD dst, regD src) %{
2869   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2870   match(Set dst (DivD dst src));
2871 
2872   format %{ &quot;divsd   $dst, $src&quot; %}
2873   ins_cost(150);
2874   ins_encode %{
2875     __ divsd($dst$$XMMRegister, $src$$XMMRegister);
2876   %}
2877   ins_pipe(pipe_slow);
2878 %}
2879 
2880 instruct divD_mem(regD dst, memory src) %{
2881   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2882   match(Set dst (DivD dst (LoadD src)));
2883 
2884   format %{ &quot;divsd   $dst, $src&quot; %}
2885   ins_cost(150);
2886   ins_encode %{
2887     __ divsd($dst$$XMMRegister, $src$$Address);
2888   %}
2889   ins_pipe(pipe_slow);
2890 %}
2891 
2892 instruct divD_imm(regD dst, immD con) %{
2893   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2894   match(Set dst (DivD dst con));
2895   format %{ &quot;divsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2896   ins_cost(150);
2897   ins_encode %{
2898     __ divsd($dst$$XMMRegister, $constantaddress($con));
2899   %}
2900   ins_pipe(pipe_slow);
2901 %}
2902 
2903 instruct divD_reg_reg(regD dst, regD src1, regD src2) %{
2904   predicate(UseAVX &gt; 0);
2905   match(Set dst (DivD src1 src2));
2906 
2907   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2908   ins_cost(150);
2909   ins_encode %{
2910     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2911   %}
2912   ins_pipe(pipe_slow);
2913 %}
2914 
2915 instruct divD_reg_mem(regD dst, regD src1, memory src2) %{
2916   predicate(UseAVX &gt; 0);
2917   match(Set dst (DivD src1 (LoadD src2)));
2918 
2919   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2920   ins_cost(150);
2921   ins_encode %{
2922     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2923   %}
2924   ins_pipe(pipe_slow);
2925 %}
2926 
2927 instruct divD_reg_imm(regD dst, regD src, immD con) %{
2928   predicate(UseAVX &gt; 0);
2929   match(Set dst (DivD src con));
2930 
2931   format %{ &quot;vdivsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2932   ins_cost(150);
2933   ins_encode %{
2934     __ vdivsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2935   %}
2936   ins_pipe(pipe_slow);
2937 %}
2938 
2939 instruct absF_reg(regF dst) %{
2940   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2941   match(Set dst (AbsF dst));
2942   ins_cost(150);
2943   format %{ &quot;andps   $dst, [0x7fffffff]\t# abs float by sign masking&quot; %}
2944   ins_encode %{
2945     __ andps($dst$$XMMRegister, ExternalAddress(float_signmask()));
2946   %}
2947   ins_pipe(pipe_slow);
2948 %}
2949 
2950 instruct absF_reg_reg(vlRegF dst, vlRegF src) %{
2951   predicate(UseAVX &gt; 0);
2952   match(Set dst (AbsF src));
2953   ins_cost(150);
2954   format %{ &quot;vandps  $dst, $src, [0x7fffffff]\t# abs float by sign masking&quot; %}
2955   ins_encode %{
2956     int vector_len = 0;
2957     __ vandps($dst$$XMMRegister, $src$$XMMRegister,
2958               ExternalAddress(float_signmask()), vector_len);
2959   %}
2960   ins_pipe(pipe_slow);
2961 %}
2962 
2963 instruct absD_reg(regD dst) %{
2964   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2965   match(Set dst (AbsD dst));
2966   ins_cost(150);
2967   format %{ &quot;andpd   $dst, [0x7fffffffffffffff]\t&quot;
2968             &quot;# abs double by sign masking&quot; %}
2969   ins_encode %{
2970     __ andpd($dst$$XMMRegister, ExternalAddress(double_signmask()));
2971   %}
2972   ins_pipe(pipe_slow);
2973 %}
2974 
2975 instruct absD_reg_reg(vlRegD dst, vlRegD src) %{
2976   predicate(UseAVX &gt; 0);
2977   match(Set dst (AbsD src));
2978   ins_cost(150);
2979   format %{ &quot;vandpd  $dst, $src, [0x7fffffffffffffff]\t&quot;
2980             &quot;# abs double by sign masking&quot; %}
2981   ins_encode %{
2982     int vector_len = 0;
2983     __ vandpd($dst$$XMMRegister, $src$$XMMRegister,
2984               ExternalAddress(double_signmask()), vector_len);
2985   %}
2986   ins_pipe(pipe_slow);
2987 %}
2988 
2989 instruct negF_reg(regF dst) %{
2990   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2991   match(Set dst (NegF dst));
2992   ins_cost(150);
2993   format %{ &quot;xorps   $dst, [0x80000000]\t# neg float by sign flipping&quot; %}
2994   ins_encode %{
2995     __ xorps($dst$$XMMRegister, ExternalAddress(float_signflip()));
2996   %}
2997   ins_pipe(pipe_slow);
2998 %}
2999 
3000 instruct negF_reg_reg(vlRegF dst, vlRegF src) %{
3001   predicate(UseAVX &gt; 0);
3002   match(Set dst (NegF src));
3003   ins_cost(150);
3004   format %{ &quot;vnegatess  $dst, $src, [0x80000000]\t# neg float by sign flipping&quot; %}
3005   ins_encode %{
3006     __ vnegatess($dst$$XMMRegister, $src$$XMMRegister,
3007                  ExternalAddress(float_signflip()));
3008   %}
3009   ins_pipe(pipe_slow);
3010 %}
3011 
3012 instruct negD_reg(regD dst) %{
3013   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
3014   match(Set dst (NegD dst));
3015   ins_cost(150);
3016   format %{ &quot;xorpd   $dst, [0x8000000000000000]\t&quot;
3017             &quot;# neg double by sign flipping&quot; %}
3018   ins_encode %{
3019     __ xorpd($dst$$XMMRegister, ExternalAddress(double_signflip()));
3020   %}
3021   ins_pipe(pipe_slow);
3022 %}
3023 
3024 instruct negD_reg_reg(vlRegD dst, vlRegD src) %{
3025   predicate(UseAVX &gt; 0);
3026   match(Set dst (NegD src));
3027   ins_cost(150);
3028   format %{ &quot;vnegatesd  $dst, $src, [0x8000000000000000]\t&quot;
3029             &quot;# neg double by sign flipping&quot; %}
3030   ins_encode %{
3031     __ vnegatesd($dst$$XMMRegister, $src$$XMMRegister,
3032                  ExternalAddress(double_signflip()));
3033   %}
3034   ins_pipe(pipe_slow);
3035 %}
3036 
3037 instruct sqrtF_reg(regF dst, regF src) %{
3038   predicate(UseSSE&gt;=1);
3039   match(Set dst (SqrtF src));
3040 
3041   format %{ &quot;sqrtss  $dst, $src&quot; %}
3042   ins_cost(150);
3043   ins_encode %{
3044     __ sqrtss($dst$$XMMRegister, $src$$XMMRegister);
3045   %}
3046   ins_pipe(pipe_slow);
3047 %}
3048 
3049 instruct sqrtF_mem(regF dst, memory src) %{
3050   predicate(UseSSE&gt;=1);
3051   match(Set dst (SqrtF (LoadF src)));
3052 
3053   format %{ &quot;sqrtss  $dst, $src&quot; %}
3054   ins_cost(150);
3055   ins_encode %{
3056     __ sqrtss($dst$$XMMRegister, $src$$Address);
3057   %}
3058   ins_pipe(pipe_slow);
3059 %}
3060 
3061 instruct sqrtF_imm(regF dst, immF con) %{
3062   predicate(UseSSE&gt;=1);
3063   match(Set dst (SqrtF con));
3064 
3065   format %{ &quot;sqrtss  $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
3066   ins_cost(150);
3067   ins_encode %{
3068     __ sqrtss($dst$$XMMRegister, $constantaddress($con));
3069   %}
3070   ins_pipe(pipe_slow);
3071 %}
3072 
3073 instruct sqrtD_reg(regD dst, regD src) %{
3074   predicate(UseSSE&gt;=2);
3075   match(Set dst (SqrtD src));
3076 
3077   format %{ &quot;sqrtsd  $dst, $src&quot; %}
3078   ins_cost(150);
3079   ins_encode %{
3080     __ sqrtsd($dst$$XMMRegister, $src$$XMMRegister);
3081   %}
3082   ins_pipe(pipe_slow);
3083 %}
3084 
3085 instruct sqrtD_mem(regD dst, memory src) %{
3086   predicate(UseSSE&gt;=2);
3087   match(Set dst (SqrtD (LoadD src)));
3088 
3089   format %{ &quot;sqrtsd  $dst, $src&quot; %}
3090   ins_cost(150);
3091   ins_encode %{
3092     __ sqrtsd($dst$$XMMRegister, $src$$Address);
3093   %}
3094   ins_pipe(pipe_slow);
3095 %}
3096 
3097 instruct sqrtD_imm(regD dst, immD con) %{
3098   predicate(UseSSE&gt;=2);
3099   match(Set dst (SqrtD con));
3100   format %{ &quot;sqrtsd  $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
3101   ins_cost(150);
3102   ins_encode %{
3103     __ sqrtsd($dst$$XMMRegister, $constantaddress($con));
3104   %}
3105   ins_pipe(pipe_slow);
3106 %}
3107 
3108 
3109 #ifdef _LP64
3110 instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
3111   match(Set dst (RoundDoubleMode src rmode));
3112   format %{ &quot;roundsd $dst,$src&quot; %}
3113   ins_cost(150);
3114   ins_encode %{
3115     assert(UseSSE &gt;= 4, &quot;required&quot;);
3116     __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
3117   %}
3118   ins_pipe(pipe_slow);
3119 %}
3120 
3121 instruct roundD_mem(legRegD dst, memory src, immU8 rmode) %{
3122   match(Set dst (RoundDoubleMode (LoadD src) rmode));
3123   format %{ &quot;roundsd $dst,$src&quot; %}
3124   ins_cost(150);
3125   ins_encode %{
3126     assert(UseSSE &gt;= 4, &quot;required&quot;);
3127     __ roundsd($dst$$XMMRegister, $src$$Address, $rmode$$constant);
3128   %}
3129   ins_pipe(pipe_slow);
3130 %}
3131 
3132 instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{
3133   match(Set dst (RoundDoubleMode con rmode));
3134   effect(TEMP scratch_reg);
3135   format %{ &quot;roundsd $dst,[$constantaddress]\t# load from constant table: double=$con&quot; %}
3136   ins_cost(150);
3137   ins_encode %{
3138     assert(UseSSE &gt;= 4, &quot;required&quot;);
3139     __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);
3140   %}
3141   ins_pipe(pipe_slow);
3142 %}
3143 
3144 instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
3145   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3146   match(Set dst (RoundDoubleModeV src rmode));
3147   format %{ &quot;vroundpd $dst,$src,$rmode\t! round packedD&quot; %}
3148   ins_encode %{
3149     assert(UseAVX &gt; 0, &quot;required&quot;);
3150     int vector_len = vector_length_encoding(this);
3151     __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vector_len);
3152   %}
3153   ins_pipe( pipe_slow );
3154 %}
3155 
3156 instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
3157   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3158   match(Set dst (RoundDoubleModeV src rmode));
3159   format %{ &quot;vrndscalepd $dst,$src,$rmode\t! round packed8D&quot; %}
3160   ins_encode %{
3161     assert(UseAVX &gt; 2, &quot;required&quot;);
3162     __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
3163   %}
3164   ins_pipe( pipe_slow );
3165 %}
3166 
3167 instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
3168   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3169   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3170   format %{ &quot;vroundpd $dst, $mem, $rmode\t! round packedD&quot; %}
3171   ins_encode %{
3172     assert(UseAVX &gt; 0, &quot;required&quot;);
3173     int vector_len = vector_length_encoding(this);
3174     __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vector_len);
3175   %}
3176   ins_pipe( pipe_slow );
3177 %}
3178 
3179 instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
3180   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3181   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3182   format %{ &quot;vrndscalepd $dst,$mem,$rmode\t! round packed8D&quot; %}
3183   ins_encode %{
3184     assert(UseAVX &gt; 2, &quot;required&quot;);
3185     __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
3186   %}
3187   ins_pipe( pipe_slow );
3188 %}
3189 #endif // _LP64
3190 
3191 instruct onspinwait() %{
3192   match(OnSpinWait);
3193   ins_cost(200);
3194 
3195   format %{
3196     $$template
3197     $$emit$$&quot;pause\t! membar_onspinwait&quot;
3198   %}
3199   ins_encode %{
3200     __ pause();
3201   %}
3202   ins_pipe(pipe_slow);
3203 %}
3204 
3205 // a * b + c
3206 instruct fmaD_reg(regD a, regD b, regD c) %{
3207   predicate(UseFMA);
3208   match(Set c (FmaD  c (Binary a b)));
3209   format %{ &quot;fmasd $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3210   ins_cost(150);
3211   ins_encode %{
3212     __ fmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3213   %}
3214   ins_pipe( pipe_slow );
3215 %}
3216 
3217 // a * b + c
3218 instruct fmaF_reg(regF a, regF b, regF c) %{
3219   predicate(UseFMA);
3220   match(Set c (FmaF  c (Binary a b)));
3221   format %{ &quot;fmass $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3222   ins_cost(150);
3223   ins_encode %{
3224     __ fmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3225   %}
3226   ins_pipe( pipe_slow );
3227 %}
3228 
3229 // ====================VECTOR INSTRUCTIONS=====================================
3230 
3231 // Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
3232 instruct MoveVec2Leg(legVec dst, vec src) %{
3233   match(Set dst src);
3234   format %{ &quot;&quot; %}
3235   ins_encode %{
3236     ShouldNotReachHere();
3237   %}
3238   ins_pipe( fpu_reg_reg );
3239 %}
3240 
3241 instruct MoveLeg2Vec(vec dst, legVec src) %{
3242   match(Set dst src);
3243   format %{ &quot;&quot; %}
3244   ins_encode %{
3245     ShouldNotReachHere();
3246   %}
3247   ins_pipe( fpu_reg_reg );
3248 %}
3249 
3250 // ============================================================================
3251 
3252 // Load vectors
3253 instruct loadV(vec dst, memory mem) %{
3254   match(Set dst (LoadVector mem));
3255   ins_cost(125);
3256   format %{ &quot;load_vector $dst,$mem&quot; %}
3257   ins_encode %{
3258     switch (vector_length_in_bytes(this)) {
3259       case  4: __ movdl    ($dst$$XMMRegister, $mem$$Address); break;
3260       case  8: __ movq     ($dst$$XMMRegister, $mem$$Address); break;
3261       case 16: __ movdqu   ($dst$$XMMRegister, $mem$$Address); break;
3262       case 32: __ vmovdqu  ($dst$$XMMRegister, $mem$$Address); break;
3263       case 64: __ evmovdqul($dst$$XMMRegister, $mem$$Address, Assembler::AVX_512bit); break;
3264       default: ShouldNotReachHere();
3265     }
3266   %}
3267   ins_pipe( pipe_slow );
3268 %}
3269 
3270 // Store vectors generic operand pattern.
3271 instruct storeV(memory mem, vec src) %{
3272   match(Set mem (StoreVector mem src));
3273   ins_cost(145);
3274   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3275   ins_encode %{
3276     switch (vector_length_in_bytes(this, $src)) {
3277       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3278       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3279       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3280       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3281       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3282       default: ShouldNotReachHere();
3283     }
3284   %}
3285   ins_pipe( pipe_slow );
3286 %}
3287 
3288 // ====================REPLICATE=======================================
3289 
3290 // Replicate byte scalar to be vector
3291 instruct ReplB_reg(vec dst, rRegI src) %{
3292   match(Set dst (ReplicateB src));
3293   format %{ &quot;replicateB $dst,$src&quot; %}
3294   ins_encode %{
3295     uint vlen = vector_length(this);
3296     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3297       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW
3298       int vlen_enc = vector_length_encoding(this);
3299       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3300     } else {
3301       __ movdl($dst$$XMMRegister, $src$$Register);
3302       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3303       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3304       if (vlen &gt;= 16) {
3305         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3306         if (vlen &gt;= 32) {
3307           assert(vlen == 32, &quot;sanity&quot;);
3308           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3309         }
3310       }
3311     }
3312   %}
3313   ins_pipe( pipe_slow );
3314 %}
3315 
3316 instruct ReplB_mem(vec dst, memory mem) %{
3317   predicate(VM_Version::supports_avx2());
3318   match(Set dst (ReplicateB (LoadB mem)));
3319   format %{ &quot;replicateB $dst,$mem&quot; %}
3320   ins_encode %{
3321     int vector_len = vector_length_encoding(this);
3322     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3323   %}
3324   ins_pipe( pipe_slow );
3325 %}
3326 
3327 instruct ReplB_imm(vec dst, immI con) %{
3328   match(Set dst (ReplicateB con));
3329   format %{ &quot;replicateB $dst,$con&quot; %}
3330   ins_encode %{
3331     uint vlen = vector_length(this);
3332     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3333     if (vlen == 4) {
3334       __ movdl($dst$$XMMRegister, const_addr);
3335     } else {
3336       __ movq($dst$$XMMRegister, const_addr);
3337       if (vlen &gt;= 16) {
3338         if (VM_Version::supports_avx2()) {
3339           int vlen_enc = vector_length_encoding(this);
3340           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3341         } else {
3342           assert(vlen == 16, &quot;sanity&quot;);
3343           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3344         }
3345       }
3346     }
3347   %}
3348   ins_pipe( pipe_slow );
3349 %}
3350 
3351 // Replicate byte scalar zero to be vector
3352 instruct ReplB_zero(vec dst, immI0 zero) %{
3353   match(Set dst (ReplicateB zero));
3354   format %{ &quot;replicateB $dst,$zero&quot; %}
3355   ins_encode %{
3356     uint vlen = vector_length(this);
3357     if (vlen &lt;= 16) {
3358       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3359     } else {
3360       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3361       int vlen_enc = vector_length_encoding(this);
3362       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3363     }
3364   %}
3365   ins_pipe( fpu_reg_reg );
3366 %}
3367 
3368 // ====================ReplicateS=======================================
3369 
3370 instruct ReplS_reg(vec dst, rRegI src) %{
3371   match(Set dst (ReplicateS src));
3372   format %{ &quot;replicateS $dst,$src&quot; %}
3373   ins_encode %{
3374     uint vlen = vector_length(this);
3375     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3376       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW
3377       int vlen_enc = vector_length_encoding(this);
3378       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3379     } else {
3380       __ movdl($dst$$XMMRegister, $src$$Register);
3381       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3382       if (vlen &gt;= 8) {
3383         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3384         if (vlen &gt;= 16) {
3385           assert(vlen == 16, &quot;sanity&quot;);
3386           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3387         }
3388       }
3389     }
3390   %}
3391   ins_pipe( pipe_slow );
3392 %}
3393 
3394 instruct ReplS_mem(vec dst, memory mem) %{
3395   predicate(VM_Version::supports_avx2());
3396   match(Set dst (ReplicateS (LoadS mem)));
3397   format %{ &quot;replicateS $dst,$mem&quot; %}
3398   ins_encode %{
3399     int vlen_enc = vector_length_encoding(this);
3400     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
3401   %}
3402   ins_pipe( pipe_slow );
3403 %}
3404 
3405 instruct ReplS_imm(vec dst, immI con) %{
3406   match(Set dst (ReplicateS con));
3407   format %{ &quot;replicateS $dst,$con&quot; %}
3408   ins_encode %{
3409     uint vlen = vector_length(this);
3410     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));
3411     if (vlen == 2) {
3412       __ movdl($dst$$XMMRegister, const_addr);
3413     } else {
3414       __ movq($dst$$XMMRegister, const_addr);
3415       if (vlen &gt;= 8) {
3416         if (VM_Version::supports_avx2()) {
3417           int vlen_enc = vector_length_encoding(this);
3418           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3419         } else {
3420           assert(vlen == 8, &quot;sanity&quot;);
3421           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3422         }
3423       }
3424     }
3425   %}
3426   ins_pipe( fpu_reg_reg );
3427 %}
3428 
3429 instruct ReplS_zero(vec dst, immI0 zero) %{
3430   match(Set dst (ReplicateS zero));
3431   format %{ &quot;replicateS $dst,$zero&quot; %}
3432   ins_encode %{
3433     uint vlen = vector_length(this);
3434     if (vlen &lt;= 8) {
3435       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3436     } else {
3437       int vlen_enc = vector_length_encoding(this);
3438       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3439     }
3440   %}
3441   ins_pipe( fpu_reg_reg );
3442 %}
3443 
3444 // ====================ReplicateI=======================================
3445 
3446 instruct ReplI_reg(vec dst, rRegI src) %{
3447   match(Set dst (ReplicateI src));
3448   format %{ &quot;replicateI $dst,$src&quot; %}
3449   ins_encode %{
3450     uint vlen = vector_length(this);
3451     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3452       int vlen_enc = vector_length_encoding(this);
3453       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3454     } else {
3455       __ movdl($dst$$XMMRegister, $src$$Register);
3456       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3457       if (vlen &gt;= 8) {
3458         assert(vlen == 8, &quot;sanity&quot;);
3459         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3460       }
3461     }
3462   %}
3463   ins_pipe( pipe_slow );
3464 %}
3465 
3466 instruct ReplI_mem(vec dst, memory mem) %{
3467   match(Set dst (ReplicateI (LoadI mem)));
3468   format %{ &quot;replicateI $dst,$mem&quot; %}
3469   ins_encode %{
3470     uint vlen = vector_length(this);
3471     if (vlen &lt;= 4) {
3472       __ movdl($dst$$XMMRegister, $mem$$Address);
3473       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3474     } else {
3475       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3476       int vector_len = vector_length_encoding(this);
3477       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
3478     }
3479   %}
3480   ins_pipe( pipe_slow );
3481 %}
3482 
3483 instruct ReplI_imm(vec dst, immI con) %{
3484   match(Set dst (ReplicateI con));
3485   format %{ &quot;replicateI $dst,$con&quot; %}
3486   ins_encode %{
3487     uint vlen = vector_length(this);
3488     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));
3489     if (vlen &lt;= 4) {
3490       __ movq($dst$$XMMRegister, const_addr);
3491       if (vlen == 4) {
3492         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3493       }
3494     } else {
3495       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3496       int vector_len = vector_length_encoding(this);
3497       __ movq($dst$$XMMRegister, const_addr);
3498       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3499     }
3500   %}
3501   ins_pipe( pipe_slow );
3502 %}
3503 
3504 // Replicate integer (4 byte) scalar zero to be vector
3505 instruct ReplI_zero(vec dst, immI0 zero) %{
3506   match(Set dst (ReplicateI zero));
3507   format %{ &quot;replicateI $dst,$zero&quot; %}
3508   ins_encode %{
3509     uint vlen = vector_length(this);
3510     if (vlen &lt;= 4) {
3511       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3512     } else {
3513       int vlen_enc = vector_length_encoding(this);
3514       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3515     }
3516   %}
3517   ins_pipe( fpu_reg_reg );
3518 %}
3519 
3520 instruct ReplI_M1(vec dst, immI_M1 con) %{
3521   predicate(UseAVX &gt; 0);
3522   match(Set dst (ReplicateB con));
3523   match(Set dst (ReplicateS con));
3524   match(Set dst (ReplicateI con));
3525   effect(TEMP dst);
3526   format %{ &quot;vallones $dst&quot; %}
3527   ins_encode %{
3528     int vector_len = vector_length_encoding(this);
3529     __ vallones($dst$$XMMRegister, vector_len);
3530   %}
3531   ins_pipe( pipe_slow );
3532 %}
3533 
3534 // ====================ReplicateL=======================================
3535 
3536 #ifdef _LP64
3537 // Replicate long (8 byte) scalar to be vector
3538 instruct ReplL_reg(vec dst, rRegL src) %{
3539   match(Set dst (ReplicateL src));
3540   format %{ &quot;replicateL $dst,$src&quot; %}
3541   ins_encode %{
3542     uint vlen = vector_length(this);
3543     if (vlen == 2) {
3544       __ movdq($dst$$XMMRegister, $src$$Register);
3545       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3546     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3547       int vlen_enc = vector_length_encoding(this);
3548       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3549     } else {
3550       assert(vlen == 4, &quot;sanity&quot;);
3551       __ movdq($dst$$XMMRegister, $src$$Register);
3552       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3553       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3554     }
3555   %}
3556   ins_pipe( pipe_slow );
3557 %}
3558 #else // _LP64
3559 // Replicate long (8 byte) scalar to be vector
3560 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3561   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3562   match(Set dst (ReplicateL src));
3563   effect(TEMP dst, USE src, TEMP tmp);
3564   format %{ &quot;replicateL $dst,$src&quot; %}
3565   ins_encode %{
3566     uint vlen = vector_length(this);
3567     if (vlen == 2) {
3568       __ movdl($dst$$XMMRegister, $src$$Register);
3569       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3570       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3571       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3572     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3573       int vector_len = Assembler::AVX_256bit;
3574       __ movdl($dst$$XMMRegister, $src$$Register);
3575       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3576       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3577       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3578     } else {
3579       __ movdl($dst$$XMMRegister, $src$$Register);
3580       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3581       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3582       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3583       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3584     }
3585   %}
3586   ins_pipe( pipe_slow );
3587 %}
3588 
3589 instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{
3590   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3591   match(Set dst (ReplicateL src));
3592   effect(TEMP dst, USE src, TEMP tmp);
3593   format %{ &quot;replicateL $dst,$src&quot; %}
3594   ins_encode %{
3595     if (VM_Version::supports_avx512vl()) {
3596       __ movdl($dst$$XMMRegister, $src$$Register);
3597       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3598       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3599       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3600       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3601       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3602     } else {
3603       int vector_len = Assembler::AVX_512bit;
3604       __ movdl($dst$$XMMRegister, $src$$Register);
3605       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3606       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3607       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3608     }
3609   %}
3610   ins_pipe( pipe_slow );
3611 %}
3612 #endif // _LP64
3613 
3614 instruct ReplL_mem(vec dst, memory mem) %{
3615   match(Set dst (ReplicateL (LoadL mem)));
3616   format %{ &quot;replicateL $dst,$mem&quot; %}
3617   ins_encode %{
3618     uint vlen = vector_length(this);
3619     if (vlen == 2) {
3620       __ movq($dst$$XMMRegister, $mem$$Address);
3621       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3622     } else {
3623       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3624       int vlen_enc = vector_length_encoding(this);
3625       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
3626     }
3627   %}
3628   ins_pipe( pipe_slow );
3629 %}
3630 
3631 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3632 instruct ReplL_imm(vec dst, immL con) %{
3633   match(Set dst (ReplicateL con));
3634   format %{ &quot;replicateL $dst,$con&quot; %}
3635   ins_encode %{
3636     uint vlen = vector_length(this);
3637     InternalAddress const_addr = $constantaddress($con);
3638     if (vlen == 2) {
3639       __ movq($dst$$XMMRegister, const_addr);
3640       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3641     } else {
3642       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3643       int vlen_enc = vector_length_encoding(this);
3644       __ movq($dst$$XMMRegister, const_addr);
3645       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3646     }
3647   %}
3648   ins_pipe( pipe_slow );
3649 %}
3650 
3651 instruct ReplL_zero(vec dst, immL0 zero) %{
3652   match(Set dst (ReplicateL zero));
3653   format %{ &quot;replicateL $dst,$zero&quot; %}
3654   ins_encode %{
3655     int vlen = vector_length(this);
3656     if (vlen == 2) {
3657       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3658     } else {
3659       int vlen_enc = vector_length_encoding(this);
3660       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3661     }
3662   %}
3663   ins_pipe( fpu_reg_reg );
3664 %}
3665 
3666 instruct ReplL_M1(vec dst, immL_M1 con) %{
3667   predicate(UseAVX &gt; 0);
3668   match(Set dst (ReplicateL con));
3669   effect(TEMP dst);
3670   format %{ &quot;vallones $dst&quot; %}
3671   ins_encode %{
3672     int vector_len = vector_length_encoding(this);
3673     __ vallones($dst$$XMMRegister, vector_len);
3674   %}
3675   ins_pipe( pipe_slow );
3676 %}
3677 
3678 // ====================ReplicateF=======================================
3679 
3680 instruct ReplF_reg(vec dst, vlRegF src) %{
3681   match(Set dst (ReplicateF src));
3682   format %{ &quot;replicateF $dst,$src&quot; %}
3683   ins_encode %{
3684     uint vlen = vector_length(this);
3685     if (vlen &lt;= 4) {
3686       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3687    } else if (VM_Version::supports_avx2()) {
3688       int vector_len = vector_length_encoding(this);
3689       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
3690     } else {
3691       assert(vlen == 8, &quot;sanity&quot;);
3692       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3693       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3694     }
3695   %}
3696   ins_pipe( pipe_slow );
3697 %}
3698 
3699 instruct ReplF_mem(vec dst, memory mem) %{
3700   match(Set dst (ReplicateF (LoadF mem)));
3701   format %{ &quot;replicateF $dst,$mem&quot; %}
3702   ins_encode %{
3703     uint vlen = vector_length(this);
3704     if (vlen &lt;= 4) {
3705       __ movdl($dst$$XMMRegister, $mem$$Address);
3706       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3707     } else {
3708       assert(VM_Version::supports_avx(), &quot;sanity&quot;);
3709       int vector_len = vector_length_encoding(this);
3710       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
3711     }
3712   %}
3713   ins_pipe( pipe_slow );
3714 %}
3715 
3716 instruct ReplF_zero(vec dst, immF0 zero) %{
3717   match(Set dst (ReplicateF zero));
3718   format %{ &quot;replicateF $dst,$zero&quot; %}
3719   ins_encode %{
3720     uint vlen = vector_length(this);
3721     if (vlen &lt;= 4) {
3722       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3723     } else {
3724       int vlen_enc = vector_length_encoding(this);
3725       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3726     }
3727   %}
3728   ins_pipe( fpu_reg_reg );
3729 %}
3730 
3731 // ====================ReplicateD=======================================
3732 
3733 // Replicate double (8 bytes) scalar to be vector
3734 instruct ReplD_reg(vec dst, vlRegD src) %{
3735   match(Set dst (ReplicateD src));
3736   format %{ &quot;replicateD $dst,$src&quot; %}
3737   ins_encode %{
3738     uint vlen = vector_length(this);
3739     if (vlen == 2) {
3740       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3741     } else if (VM_Version::supports_avx2()) {
3742       int vector_len = vector_length_encoding(this);
3743       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
3744     } else {
3745       assert(vlen == 4, &quot;sanity&quot;);
3746       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3747       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3748     }
3749   %}
3750   ins_pipe( pipe_slow );
3751 %}
3752 
3753 instruct ReplD_mem(vec dst, memory mem) %{
3754   match(Set dst (ReplicateD (LoadD mem)));
3755   format %{ &quot;replicateD $dst,$mem&quot; %}
3756   ins_encode %{
3757     uint vlen = vector_length(this);
3758     if (vlen == 2) {
3759       __ movq($dst$$XMMRegister, $mem$$Address);
3760       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);
3761     } else {
3762       assert(VM_Version::supports_avx(), &quot;sanity&quot;);
3763       int vector_len = vector_length_encoding(this);
3764       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
3765     }
3766   %}
3767   ins_pipe( pipe_slow );
3768 %}
3769 
3770 instruct ReplD_zero(vec dst, immD0 zero) %{
3771   match(Set dst (ReplicateD zero));
3772   format %{ &quot;replicateD $dst,$zero&quot; %}
3773   ins_encode %{
3774     uint vlen = vector_length(this);
3775     if (vlen == 2) {
3776       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3777     } else {
3778       int vlen_enc = vector_length_encoding(this);
3779       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3780     }
3781   %}
3782   ins_pipe( fpu_reg_reg );
3783 %}
3784 
3785 // ====================REDUCTION ARITHMETIC=======================================
3786 // =======================Int Reduction==========================================
3787 
3788 instruct reductionI(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{
3789   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;
3790             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 16);
3791   match(Set dst (AddReductionVI src1 src2));
3792   match(Set dst (MulReductionVI src1 src2));
3793   match(Set dst (AndReductionV  src1 src2));
3794   match(Set dst ( OrReductionV  src1 src2));
3795   match(Set dst (XorReductionV  src1 src2));
3796   effect(TEMP vtmp1, TEMP vtmp2);
3797   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3798   ins_encode %{
3799     int opcode = this-&gt;ideal_Opcode();
3800     int vlen = vector_length(this, $src2);
3801     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3802   %}
3803   ins_pipe( pipe_slow );
3804 %}
3805 
3806 instruct reduction16I(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
3807   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;
3808             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);
3809   match(Set dst (AddReductionVI src1 src2));
3810   match(Set dst (MulReductionVI src1 src2));
3811   match(Set dst (AndReductionV  src1 src2));
3812   match(Set dst ( OrReductionV  src1 src2));
3813   match(Set dst (XorReductionV  src1 src2));
3814   effect(TEMP vtmp1, TEMP vtmp2);
3815   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3816   ins_encode %{
3817     int opcode = this-&gt;ideal_Opcode();
3818     int vlen = vector_length(this, $src2);
3819     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3820   %}
3821   ins_pipe( pipe_slow );
3822 %}
3823 
3824 // =======================Long Reduction==========================================
3825 
3826 #ifdef _LP64
3827 instruct reductionL(rRegL dst, rRegL src1, vec src2, vec vtmp1, vec vtmp2) %{
3828   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;
3829             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 8);
3830   match(Set dst (AddReductionVL src1 src2));
3831   match(Set dst (MulReductionVL src1 src2));
3832   match(Set dst (AndReductionV  src1 src2));
3833   match(Set dst ( OrReductionV  src1 src2));
3834   match(Set dst (XorReductionV  src1 src2));
3835   effect(TEMP vtmp1, TEMP vtmp2);
3836   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3837   ins_encode %{
3838     int opcode = this-&gt;ideal_Opcode();
3839     int vlen = vector_length(this, $src2);
3840     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3841   %}
3842   ins_pipe( pipe_slow );
3843 %}
3844 
3845 instruct reduction8L(rRegL dst, rRegL src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
3846   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;
3847             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);
3848   match(Set dst (AddReductionVL src1 src2));
3849   match(Set dst (MulReductionVL src1 src2));
3850   match(Set dst (AndReductionV  src1 src2));
3851   match(Set dst ( OrReductionV  src1 src2));
3852   match(Set dst (XorReductionV  src1 src2));
3853   effect(TEMP vtmp1, TEMP vtmp2);
3854   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3855   ins_encode %{
3856     int opcode = this-&gt;ideal_Opcode();
3857     int vlen = vector_length(this, $src2);
3858     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3859   %}
3860   ins_pipe( pipe_slow );
3861 %}
3862 #endif // _LP64
3863 
3864 // =======================Float Reduction==========================================
3865 
3866 instruct reductionF128(regF dst, vec src, vec vtmp) %{
3867   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt;= 4);
3868   match(Set dst (AddReductionVF dst src));
3869   match(Set dst (MulReductionVF dst src));
3870   effect(TEMP dst, TEMP vtmp);
3871   format %{ &quot;vector_reduction_fp  $dst,$src ; using $vtmp as TEMP&quot; %}
3872   ins_encode %{
3873     int opcode = this-&gt;ideal_Opcode();
3874     int vlen = vector_length(this, $src);
3875     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
3876   %}
3877   ins_pipe( pipe_slow );
3878 %}
3879 
3880 instruct reduction8F(regF dst, vec src, vec vtmp1, vec vtmp2) %{
3881   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);
3882   match(Set dst (AddReductionVF dst src));
3883   match(Set dst (MulReductionVF dst src));
3884   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3885   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3886   ins_encode %{
3887     int opcode = this-&gt;ideal_Opcode();
3888     int vlen = vector_length(this, $src);
3889     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3890   %}
3891   ins_pipe( pipe_slow );
3892 %}
3893 
3894 instruct reduction16F(regF dst, legVec src, legVec vtmp1, legVec vtmp2) %{
3895   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);
3896   match(Set dst (AddReductionVF dst src));
3897   match(Set dst (MulReductionVF dst src));
3898   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3899   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3900   ins_encode %{
3901     int opcode = this-&gt;ideal_Opcode();
3902     int vlen = vector_length(this, $src);
3903     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3904   %}
3905   ins_pipe( pipe_slow );
3906 %}
3907 
3908 // =======================Double Reduction==========================================
3909 
3910 instruct reduction2D(regD dst, vec src, vec vtmp) %{
3911   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2);
3912   match(Set dst (AddReductionVD dst src));
3913   match(Set dst (MulReductionVD dst src));
3914   effect(TEMP dst, TEMP vtmp);
3915   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp as TEMP&quot; %}
3916   ins_encode %{
3917     int opcode = this-&gt;ideal_Opcode();
3918     int vlen = vector_length(this, $src);
3919     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
3920   %}
3921   ins_pipe( pipe_slow );
3922 %}
3923 
3924 instruct reduction4D(regD dst, vec src, vec vtmp1, vec vtmp2) %{
3925   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4);
3926   match(Set dst (AddReductionVD dst src));
3927   match(Set dst (MulReductionVD dst src));
3928   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3929   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3930   ins_encode %{
3931     int opcode = this-&gt;ideal_Opcode();
3932     int vlen = vector_length(this, $src);
3933     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3934   %}
3935   ins_pipe( pipe_slow );
3936 %}
3937 
3938 instruct reduction8D(regD dst, legVec src, legVec vtmp1, legVec vtmp2) %{
3939   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);
3940   match(Set dst (AddReductionVD dst src));
3941   match(Set dst (MulReductionVD dst src));
3942   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3943   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3944   ins_encode %{
3945     int opcode = this-&gt;ideal_Opcode();
3946     int vlen = vector_length(this, $src);
3947     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3948   %}
3949   ins_pipe( pipe_slow );
3950 %}
3951 
3952 // ====================VECTOR ARITHMETIC=======================================
3953 
3954 // --------------------------------- ADD --------------------------------------
3955 
3956 // Bytes vector add
3957 instruct vaddB(vec dst, vec src) %{
3958   predicate(UseAVX == 0);
3959   match(Set dst (AddVB dst src));
3960   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
3961   ins_encode %{
3962     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
3963   %}
3964   ins_pipe( pipe_slow );
3965 %}
3966 
3967 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
3968   predicate(UseAVX &gt; 0);
3969   match(Set dst (AddVB src1 src2));
3970   format %{ &quot;vpaddb  $dst,$src1,$src2\t! add packedB&quot; %}
3971   ins_encode %{
3972     int vector_len = vector_length_encoding(this);
3973     __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
3974   %}
3975   ins_pipe( pipe_slow );
3976 %}
3977 
3978 instruct vaddB_mem(vec dst, vec src, memory mem) %{
3979   predicate(UseAVX &gt; 0);
3980   match(Set dst (AddVB src (LoadVector mem)));
3981   format %{ &quot;vpaddb  $dst,$src,$mem\t! add packedB&quot; %}
3982   ins_encode %{
3983     int vector_len = vector_length_encoding(this);
3984     __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
3985   %}
3986   ins_pipe( pipe_slow );
3987 %}
3988 
3989 // Shorts/Chars vector add
3990 instruct vaddS(vec dst, vec src) %{
3991   predicate(UseAVX == 0);
3992   match(Set dst (AddVS dst src));
3993   format %{ &quot;paddw   $dst,$src\t! add packedS&quot; %}
3994   ins_encode %{
3995     __ paddw($dst$$XMMRegister, $src$$XMMRegister);
3996   %}
3997   ins_pipe( pipe_slow );
3998 %}
3999 
4000 instruct vaddS_reg(vec dst, vec src1, vec src2) %{
4001   predicate(UseAVX &gt; 0);
4002   match(Set dst (AddVS src1 src2));
4003   format %{ &quot;vpaddw  $dst,$src1,$src2\t! add packedS&quot; %}
4004   ins_encode %{
4005     int vector_len = vector_length_encoding(this);
4006     __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4007   %}
4008   ins_pipe( pipe_slow );
4009 %}
4010 
4011 instruct vaddS_mem(vec dst, vec src, memory mem) %{
4012   predicate(UseAVX &gt; 0);
4013   match(Set dst (AddVS src (LoadVector mem)));
4014   format %{ &quot;vpaddw  $dst,$src,$mem\t! add packedS&quot; %}
4015   ins_encode %{
4016     int vector_len = vector_length_encoding(this);
4017     __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4018   %}
4019   ins_pipe( pipe_slow );
4020 %}
4021 
4022 // Integers vector add
4023 instruct vaddI(vec dst, vec src) %{
4024   predicate(UseAVX == 0);
4025   match(Set dst (AddVI dst src));
4026   format %{ &quot;paddd   $dst,$src\t! add packedI&quot; %}
4027   ins_encode %{
4028     __ paddd($dst$$XMMRegister, $src$$XMMRegister);
4029   %}
4030   ins_pipe( pipe_slow );
4031 %}
4032 
4033 instruct vaddI_reg(vec dst, vec src1, vec src2) %{
4034   predicate(UseAVX &gt; 0);
4035   match(Set dst (AddVI src1 src2));
4036   format %{ &quot;vpaddd  $dst,$src1,$src2\t! add packedI&quot; %}
4037   ins_encode %{
4038     int vector_len = vector_length_encoding(this);
4039     __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4040   %}
4041   ins_pipe( pipe_slow );
4042 %}
4043 
4044 
4045 instruct vaddI_mem(vec dst, vec src, memory mem) %{
4046   predicate(UseAVX &gt; 0);
4047   match(Set dst (AddVI src (LoadVector mem)));
4048   format %{ &quot;vpaddd  $dst,$src,$mem\t! add packedI&quot; %}
4049   ins_encode %{
4050     int vector_len = vector_length_encoding(this);
4051     __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4052   %}
4053   ins_pipe( pipe_slow );
4054 %}
4055 
4056 // Longs vector add
4057 instruct vaddL(vec dst, vec src) %{
4058   predicate(UseAVX == 0);
4059   match(Set dst (AddVL dst src));
4060   format %{ &quot;paddq   $dst,$src\t! add packedL&quot; %}
4061   ins_encode %{
4062     __ paddq($dst$$XMMRegister, $src$$XMMRegister);
4063   %}
4064   ins_pipe( pipe_slow );
4065 %}
4066 
4067 instruct vaddL_reg(vec dst, vec src1, vec src2) %{
4068   predicate(UseAVX &gt; 0);
4069   match(Set dst (AddVL src1 src2));
4070   format %{ &quot;vpaddq  $dst,$src1,$src2\t! add packedL&quot; %}
4071   ins_encode %{
4072     int vector_len = vector_length_encoding(this);
4073     __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4074   %}
4075   ins_pipe( pipe_slow );
4076 %}
4077 
4078 instruct vaddL_mem(vec dst, vec src, memory mem) %{
4079   predicate(UseAVX &gt; 0);
4080   match(Set dst (AddVL src (LoadVector mem)));
4081   format %{ &quot;vpaddq  $dst,$src,$mem\t! add packedL&quot; %}
4082   ins_encode %{
4083     int vector_len = vector_length_encoding(this);
4084     __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4085   %}
4086   ins_pipe( pipe_slow );
4087 %}
4088 
4089 // Floats vector add
4090 instruct vaddF(vec dst, vec src) %{
4091   predicate(UseAVX == 0);
4092   match(Set dst (AddVF dst src));
4093   format %{ &quot;addps   $dst,$src\t! add packedF&quot; %}
4094   ins_encode %{
4095     __ addps($dst$$XMMRegister, $src$$XMMRegister);
4096   %}
4097   ins_pipe( pipe_slow );
4098 %}
4099 
4100 instruct vaddF_reg(vec dst, vec src1, vec src2) %{
4101   predicate(UseAVX &gt; 0);
4102   match(Set dst (AddVF src1 src2));
4103   format %{ &quot;vaddps  $dst,$src1,$src2\t! add packedF&quot; %}
4104   ins_encode %{
4105     int vector_len = vector_length_encoding(this);
4106     __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4107   %}
4108   ins_pipe( pipe_slow );
4109 %}
4110 
4111 instruct vaddF_mem(vec dst, vec src, memory mem) %{
4112   predicate(UseAVX &gt; 0);
4113   match(Set dst (AddVF src (LoadVector mem)));
4114   format %{ &quot;vaddps  $dst,$src,$mem\t! add packedF&quot; %}
4115   ins_encode %{
4116     int vector_len = vector_length_encoding(this);
4117     __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4118   %}
4119   ins_pipe( pipe_slow );
4120 %}
4121 
4122 // Doubles vector add
4123 instruct vaddD(vec dst, vec src) %{
4124   predicate(UseAVX == 0);
4125   match(Set dst (AddVD dst src));
4126   format %{ &quot;addpd   $dst,$src\t! add packedD&quot; %}
4127   ins_encode %{
4128     __ addpd($dst$$XMMRegister, $src$$XMMRegister);
4129   %}
4130   ins_pipe( pipe_slow );
4131 %}
4132 
4133 instruct vaddD_reg(vec dst, vec src1, vec src2) %{
4134   predicate(UseAVX &gt; 0);
4135   match(Set dst (AddVD src1 src2));
4136   format %{ &quot;vaddpd  $dst,$src1,$src2\t! add packedD&quot; %}
4137   ins_encode %{
4138     int vector_len = vector_length_encoding(this);
4139     __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4140   %}
4141   ins_pipe( pipe_slow );
4142 %}
4143 
4144 instruct vaddD_mem(vec dst, vec src, memory mem) %{
4145   predicate(UseAVX &gt; 0);
4146   match(Set dst (AddVD src (LoadVector mem)));
4147   format %{ &quot;vaddpd  $dst,$src,$mem\t! add packedD&quot; %}
4148   ins_encode %{
4149     int vector_len = vector_length_encoding(this);
4150     __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4151   %}
4152   ins_pipe( pipe_slow );
4153 %}
4154 
4155 // --------------------------------- SUB --------------------------------------
4156 
4157 // Bytes vector sub
4158 instruct vsubB(vec dst, vec src) %{
4159   predicate(UseAVX == 0);
4160   match(Set dst (SubVB dst src));
4161   format %{ &quot;psubb   $dst,$src\t! sub packedB&quot; %}
4162   ins_encode %{
4163     __ psubb($dst$$XMMRegister, $src$$XMMRegister);
4164   %}
4165   ins_pipe( pipe_slow );
4166 %}
4167 
4168 instruct vsubB_reg(vec dst, vec src1, vec src2) %{
4169   predicate(UseAVX &gt; 0);
4170   match(Set dst (SubVB src1 src2));
4171   format %{ &quot;vpsubb  $dst,$src1,$src2\t! sub packedB&quot; %}
4172   ins_encode %{
4173     int vector_len = vector_length_encoding(this);
4174     __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4175   %}
4176   ins_pipe( pipe_slow );
4177 %}
4178 
4179 instruct vsubB_mem(vec dst, vec src, memory mem) %{
4180   predicate(UseAVX &gt; 0);
4181   match(Set dst (SubVB src (LoadVector mem)));
4182   format %{ &quot;vpsubb  $dst,$src,$mem\t! sub packedB&quot; %}
4183   ins_encode %{
4184     int vector_len = vector_length_encoding(this);
4185     __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4186   %}
4187   ins_pipe( pipe_slow );
4188 %}
4189 
4190 // Shorts/Chars vector sub
4191 instruct vsubS(vec dst, vec src) %{
4192   predicate(UseAVX == 0);
4193   match(Set dst (SubVS dst src));
4194   format %{ &quot;psubw   $dst,$src\t! sub packedS&quot; %}
4195   ins_encode %{
4196     __ psubw($dst$$XMMRegister, $src$$XMMRegister);
4197   %}
4198   ins_pipe( pipe_slow );
4199 %}
4200 
4201 
4202 instruct vsubS_reg(vec dst, vec src1, vec src2) %{
4203   predicate(UseAVX &gt; 0);
4204   match(Set dst (SubVS src1 src2));
4205   format %{ &quot;vpsubw  $dst,$src1,$src2\t! sub packedS&quot; %}
4206   ins_encode %{
4207     int vector_len = vector_length_encoding(this);
4208     __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4209   %}
4210   ins_pipe( pipe_slow );
4211 %}
4212 
4213 instruct vsubS_mem(vec dst, vec src, memory mem) %{
4214   predicate(UseAVX &gt; 0);
4215   match(Set dst (SubVS src (LoadVector mem)));
4216   format %{ &quot;vpsubw  $dst,$src,$mem\t! sub packedS&quot; %}
4217   ins_encode %{
4218     int vector_len = vector_length_encoding(this);
4219     __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4220   %}
4221   ins_pipe( pipe_slow );
4222 %}
4223 
4224 // Integers vector sub
4225 instruct vsubI(vec dst, vec src) %{
4226   predicate(UseAVX == 0);
4227   match(Set dst (SubVI dst src));
4228   format %{ &quot;psubd   $dst,$src\t! sub packedI&quot; %}
4229   ins_encode %{
4230     __ psubd($dst$$XMMRegister, $src$$XMMRegister);
4231   %}
4232   ins_pipe( pipe_slow );
4233 %}
4234 
4235 instruct vsubI_reg(vec dst, vec src1, vec src2) %{
4236   predicate(UseAVX &gt; 0);
4237   match(Set dst (SubVI src1 src2));
4238   format %{ &quot;vpsubd  $dst,$src1,$src2\t! sub packedI&quot; %}
4239   ins_encode %{
4240     int vector_len = vector_length_encoding(this);
4241     __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4242   %}
4243   ins_pipe( pipe_slow );
4244 %}
4245 
4246 instruct vsubI_mem(vec dst, vec src, memory mem) %{
4247   predicate(UseAVX &gt; 0);
4248   match(Set dst (SubVI src (LoadVector mem)));
4249   format %{ &quot;vpsubd  $dst,$src,$mem\t! sub packedI&quot; %}
4250   ins_encode %{
4251     int vector_len = vector_length_encoding(this);
4252     __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4253   %}
4254   ins_pipe( pipe_slow );
4255 %}
4256 
4257 // Longs vector sub
4258 instruct vsubL(vec dst, vec src) %{
4259   predicate(UseAVX == 0);
4260   match(Set dst (SubVL dst src));
4261   format %{ &quot;psubq   $dst,$src\t! sub packedL&quot; %}
4262   ins_encode %{
4263     __ psubq($dst$$XMMRegister, $src$$XMMRegister);
4264   %}
4265   ins_pipe( pipe_slow );
4266 %}
4267 
4268 instruct vsubL_reg(vec dst, vec src1, vec src2) %{
4269   predicate(UseAVX &gt; 0);
4270   match(Set dst (SubVL src1 src2));
4271   format %{ &quot;vpsubq  $dst,$src1,$src2\t! sub packedL&quot; %}
4272   ins_encode %{
4273     int vector_len = vector_length_encoding(this);
4274     __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4275   %}
4276   ins_pipe( pipe_slow );
4277 %}
4278 
4279 
4280 instruct vsubL_mem(vec dst, vec src, memory mem) %{
4281   predicate(UseAVX &gt; 0);
4282   match(Set dst (SubVL src (LoadVector mem)));
4283   format %{ &quot;vpsubq  $dst,$src,$mem\t! sub packedL&quot; %}
4284   ins_encode %{
4285     int vector_len = vector_length_encoding(this);
4286     __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4287   %}
4288   ins_pipe( pipe_slow );
4289 %}
4290 
4291 // Floats vector sub
4292 instruct vsubF(vec dst, vec src) %{
4293   predicate(UseAVX == 0);
4294   match(Set dst (SubVF dst src));
4295   format %{ &quot;subps   $dst,$src\t! sub packedF&quot; %}
4296   ins_encode %{
4297     __ subps($dst$$XMMRegister, $src$$XMMRegister);
4298   %}
4299   ins_pipe( pipe_slow );
4300 %}
4301 
4302 instruct vsubF_reg(vec dst, vec src1, vec src2) %{
4303   predicate(UseAVX &gt; 0);
4304   match(Set dst (SubVF src1 src2));
4305   format %{ &quot;vsubps  $dst,$src1,$src2\t! sub packedF&quot; %}
4306   ins_encode %{
4307     int vector_len = vector_length_encoding(this);
4308     __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4309   %}
4310   ins_pipe( pipe_slow );
4311 %}
4312 
4313 instruct vsubF_mem(vec dst, vec src, memory mem) %{
4314   predicate(UseAVX &gt; 0);
4315   match(Set dst (SubVF src (LoadVector mem)));
4316   format %{ &quot;vsubps  $dst,$src,$mem\t! sub packedF&quot; %}
4317   ins_encode %{
4318     int vector_len = vector_length_encoding(this);
4319     __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4320   %}
4321   ins_pipe( pipe_slow );
4322 %}
4323 
4324 // Doubles vector sub
4325 instruct vsubD(vec dst, vec src) %{
4326   predicate(UseAVX == 0);
4327   match(Set dst (SubVD dst src));
4328   format %{ &quot;subpd   $dst,$src\t! sub packedD&quot; %}
4329   ins_encode %{
4330     __ subpd($dst$$XMMRegister, $src$$XMMRegister);
4331   %}
4332   ins_pipe( pipe_slow );
4333 %}
4334 
4335 instruct vsubD_reg(vec dst, vec src1, vec src2) %{
4336   predicate(UseAVX &gt; 0);
4337   match(Set dst (SubVD src1 src2));
4338   format %{ &quot;vsubpd  $dst,$src1,$src2\t! sub packedD&quot; %}
4339   ins_encode %{
4340     int vector_len = vector_length_encoding(this);
4341     __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4342   %}
4343   ins_pipe( pipe_slow );
4344 %}
4345 
4346 instruct vsubD_mem(vec dst, vec src, memory mem) %{
4347   predicate(UseAVX &gt; 0);
4348   match(Set dst (SubVD src (LoadVector mem)));
4349   format %{ &quot;vsubpd  $dst,$src,$mem\t! sub packedD&quot; %}
4350   ins_encode %{
4351     int vector_len = vector_length_encoding(this);
4352     __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4353   %}
4354   ins_pipe( pipe_slow );
4355 %}
4356 
4357 // --------------------------------- MUL --------------------------------------
4358 
4359 // Byte vector mul
4360 instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4361   predicate(n-&gt;as_Vector()-&gt;length() == 4 ||
4362             n-&gt;as_Vector()-&gt;length() == 8);
4363   match(Set dst (MulVB src1 src2));
4364   effect(TEMP dst, TEMP tmp, TEMP scratch);
4365   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4366   ins_encode %{
4367     assert(UseSSE &gt; 3, &quot;required&quot;);
4368     __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);
4369     __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);
4370     __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);
4371     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4372     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4373     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4374   %}
4375   ins_pipe( pipe_slow );
4376 %}
4377 
4378 instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4379   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4380   match(Set dst (MulVB src1 src2));
4381   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4382   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4383   ins_encode %{
4384     assert(UseSSE &gt; 3, &quot;required&quot;);
4385     __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);
4386     __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);
4387     __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);
4388     __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);
4389     __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);
4390     __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);
4391     __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);
4392     __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);
4393     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4394     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
4395     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
4396     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
4397   %}
4398   ins_pipe( pipe_slow );
4399 %}
4400 
4401 instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4402   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
4403   match(Set dst (MulVB src1 src2));
4404   effect(TEMP dst, TEMP tmp, TEMP scratch);
4405   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4406   ins_encode %{
4407   int vector_len = Assembler::AVX_256bit;
4408     __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vector_len);
4409     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4410     __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vector_len);
4411     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4412     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
4413     __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);
4414     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);
4415   %}
4416   ins_pipe( pipe_slow );
4417 %}
4418 
4419 instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4420   predicate(n-&gt;as_Vector()-&gt;length() == 32);
4421   match(Set dst (MulVB src1 src2));
4422   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4423   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4424   ins_encode %{
4425     assert(UseAVX &gt; 1, &quot;required&quot;);
4426     int vector_len = Assembler::AVX_256bit;
4427     __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4428     __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);
4429     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4430     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4431     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4432     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4433     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4434     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4435     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4436     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4437     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4438     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4439     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4440     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
4441   %}
4442   ins_pipe( pipe_slow );
4443 %}
4444 
4445 instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4446   predicate(n-&gt;as_Vector()-&gt;length() == 64);
4447   match(Set dst (MulVB src1 src2));
4448   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4449   format %{&quot;vector_mulB $dst,$src1,$src2\n\t&quot; %}
4450   ins_encode %{
4451     assert(UseAVX &gt; 2, &quot;required&quot;);
4452     int vector_len = Assembler::AVX_512bit;
4453     __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4454     __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);
4455     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4456     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4457     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4458     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4459     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4460     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4461     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4462     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4463     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4464     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4465     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4466     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
4467     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4468   %}
4469   ins_pipe( pipe_slow );
4470 %}
4471 
4472 // Shorts/Chars vector mul
4473 instruct vmulS(vec dst, vec src) %{
4474   predicate(UseAVX == 0);
4475   match(Set dst (MulVS dst src));
4476   format %{ &quot;pmullw $dst,$src\t! mul packedS&quot; %}
4477   ins_encode %{
4478     __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
4479   %}
4480   ins_pipe( pipe_slow );
4481 %}
4482 
4483 instruct vmulS_reg(vec dst, vec src1, vec src2) %{
4484   predicate(UseAVX &gt; 0);
4485   match(Set dst (MulVS src1 src2));
4486   format %{ &quot;vpmullw $dst,$src1,$src2\t! mul packedS&quot; %}
4487   ins_encode %{
4488     int vector_len = vector_length_encoding(this);
4489     __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4490   %}
4491   ins_pipe( pipe_slow );
4492 %}
4493 
4494 instruct vmulS_mem(vec dst, vec src, memory mem) %{
4495   predicate(UseAVX &gt; 0);
4496   match(Set dst (MulVS src (LoadVector mem)));
4497   format %{ &quot;vpmullw $dst,$src,$mem\t! mul packedS&quot; %}
4498   ins_encode %{
4499     int vector_len = vector_length_encoding(this);
4500     __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4501   %}
4502   ins_pipe( pipe_slow );
4503 %}
4504 
4505 // Integers vector mul
4506 instruct vmulI(vec dst, vec src) %{
4507   predicate(UseAVX == 0);
4508   match(Set dst (MulVI dst src));
4509   format %{ &quot;pmulld  $dst,$src\t! mul packedI&quot; %}
4510   ins_encode %{
4511     assert(UseSSE &gt; 3, &quot;required&quot;);
4512     __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
4513   %}
4514   ins_pipe( pipe_slow );
4515 %}
4516 
4517 instruct vmulI_reg(vec dst, vec src1, vec src2) %{
4518   predicate(UseAVX &gt; 0);
4519   match(Set dst (MulVI src1 src2));
4520   format %{ &quot;vpmulld $dst,$src1,$src2\t! mul packedI&quot; %}
4521   ins_encode %{
4522     int vector_len = vector_length_encoding(this);
4523     __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4524   %}
4525   ins_pipe( pipe_slow );
4526 %}
4527 
4528 instruct vmulI_mem(vec dst, vec src, memory mem) %{
4529   predicate(UseAVX &gt; 0);
4530   match(Set dst (MulVI src (LoadVector mem)));
4531   format %{ &quot;vpmulld $dst,$src,$mem\t! mul packedI&quot; %}
4532   ins_encode %{
4533     int vector_len = vector_length_encoding(this);
4534     __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4535   %}
4536   ins_pipe( pipe_slow );
4537 %}
4538 
4539 // Longs vector mul
4540 instruct vmulL_reg(vec dst, vec src1, vec src2) %{
4541   match(Set dst (MulVL src1 src2));
4542   format %{ &quot;vpmullq $dst,$src1,$src2\t! mul packedL&quot; %}
4543   ins_encode %{
4544     assert(UseAVX &gt; 2, &quot;required&quot;);
4545     int vector_len = vector_length_encoding(this);
4546     __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4547   %}
4548   ins_pipe( pipe_slow );
4549 %}
4550 
4551 instruct vmulL_mem(vec dst, vec src, memory mem) %{
4552   match(Set dst (MulVL src (LoadVector mem)));
4553   format %{ &quot;vpmullq $dst,$src,$mem\t! mul packedL&quot; %}
4554   ins_encode %{
4555     assert(UseAVX &gt; 2, &quot;required&quot;);
4556     int vector_len = vector_length_encoding(this);
4557     __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4558   %}
4559   ins_pipe( pipe_slow );
4560 %}
4561 
4562 // Floats vector mul
4563 instruct vmulF(vec dst, vec src) %{
4564   predicate(UseAVX == 0);
4565   match(Set dst (MulVF dst src));
4566   format %{ &quot;mulps   $dst,$src\t! mul packedF&quot; %}
4567   ins_encode %{
4568     __ mulps($dst$$XMMRegister, $src$$XMMRegister);
4569   %}
4570   ins_pipe( pipe_slow );
4571 %}
4572 
4573 instruct vmulF_reg(vec dst, vec src1, vec src2) %{
4574   predicate(UseAVX &gt; 0);
4575   match(Set dst (MulVF src1 src2));
4576   format %{ &quot;vmulps  $dst,$src1,$src2\t! mul packedF&quot; %}
4577   ins_encode %{
4578     int vector_len = vector_length_encoding(this);
4579     __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4580   %}
4581   ins_pipe( pipe_slow );
4582 %}
4583 
4584 instruct vmulF_mem(vec dst, vec src, memory mem) %{
4585   predicate(UseAVX &gt; 0);
4586   match(Set dst (MulVF src (LoadVector mem)));
4587   format %{ &quot;vmulps  $dst,$src,$mem\t! mul packedF&quot; %}
4588   ins_encode %{
4589     int vector_len = vector_length_encoding(this);
4590     __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4591   %}
4592   ins_pipe( pipe_slow );
4593 %}
4594 
4595 // Doubles vector mul
4596 instruct vmulD(vec dst, vec src) %{
4597   predicate(UseAVX == 0);
4598   match(Set dst (MulVD dst src));
4599   format %{ &quot;mulpd   $dst,$src\t! mul packedD&quot; %}
4600   ins_encode %{
4601     __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
4602   %}
4603   ins_pipe( pipe_slow );
4604 %}
4605 
4606 instruct vmulD_reg(vec dst, vec src1, vec src2) %{
4607   predicate(UseAVX &gt; 0);
4608   match(Set dst (MulVD src1 src2));
4609   format %{ &quot;vmulpd  $dst,$src1,$src2\t! mul packedD&quot; %}
4610   ins_encode %{
4611     int vector_len = vector_length_encoding(this);
4612     __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4613   %}
4614   ins_pipe( pipe_slow );
4615 %}
4616 
4617 instruct vmulD_mem(vec dst, vec src, memory mem) %{
4618   predicate(UseAVX &gt; 0);
4619   match(Set dst (MulVD src (LoadVector mem)));
4620   format %{ &quot;vmulpd  $dst,$src,$mem\t! mul packedD&quot; %}
4621   ins_encode %{
4622     int vector_len = vector_length_encoding(this);
4623     __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4624   %}
4625   ins_pipe( pipe_slow );
4626 %}
4627 
4628 instruct vcmov8F_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
4629   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 8);
4630   match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
4631   effect(TEMP dst, USE src1, USE src2);
4632   format %{ &quot;cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t&quot;
4633             &quot;blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t&quot;
4634          %}
4635   ins_encode %{
4636     int vector_len = 1;
4637     int cond = (Assembler::Condition)($copnd$$cmpcode);
4638     __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
4639     __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
4640   %}
4641   ins_pipe( pipe_slow );
4642 %}
4643 
4644 instruct vcmov4D_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
4645   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 4);
4646   match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
4647   effect(TEMP dst, USE src1, USE src2);
4648   format %{ &quot;cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t&quot;
4649             &quot;blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t&quot;
4650          %}
4651   ins_encode %{
4652     int vector_len = 1;
4653     int cond = (Assembler::Condition)($copnd$$cmpcode);
4654     __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
4655     __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
4656   %}
4657   ins_pipe( pipe_slow );
4658 %}
4659 
4660 // --------------------------------- DIV --------------------------------------
4661 
4662 // Floats vector div
4663 instruct vdivF(vec dst, vec src) %{
4664   predicate(UseAVX == 0);
4665   match(Set dst (DivVF dst src));
4666   format %{ &quot;divps   $dst,$src\t! div packedF&quot; %}
4667   ins_encode %{
4668     __ divps($dst$$XMMRegister, $src$$XMMRegister);
4669   %}
4670   ins_pipe( pipe_slow );
4671 %}
4672 
4673 instruct vdivF_reg(vec dst, vec src1, vec src2) %{
4674   predicate(UseAVX &gt; 0);
4675   match(Set dst (DivVF src1 src2));
4676   format %{ &quot;vdivps  $dst,$src1,$src2\t! div packedF&quot; %}
4677   ins_encode %{
4678     int vector_len = vector_length_encoding(this);
4679     __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4680   %}
4681   ins_pipe( pipe_slow );
4682 %}
4683 
4684 instruct vdivF_mem(vec dst, vec src, memory mem) %{
4685   predicate(UseAVX &gt; 0);
4686   match(Set dst (DivVF src (LoadVector mem)));
4687   format %{ &quot;vdivps  $dst,$src,$mem\t! div packedF&quot; %}
4688   ins_encode %{
4689     int vector_len = vector_length_encoding(this);
4690     __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4691   %}
4692   ins_pipe( pipe_slow );
4693 %}
4694 
4695 // Doubles vector div
4696 instruct vdivD(vec dst, vec src) %{
4697   predicate(UseAVX == 0);
4698   match(Set dst (DivVD dst src));
4699   format %{ &quot;divpd   $dst,$src\t! div packedD&quot; %}
4700   ins_encode %{
4701     __ divpd($dst$$XMMRegister, $src$$XMMRegister);
4702   %}
4703   ins_pipe( pipe_slow );
4704 %}
4705 
4706 instruct vdivD_reg(vec dst, vec src1, vec src2) %{
4707   predicate(UseAVX &gt; 0);
4708   match(Set dst (DivVD src1 src2));
4709   format %{ &quot;vdivpd  $dst,$src1,$src2\t! div packedD&quot; %}
4710   ins_encode %{
4711     int vector_len = vector_length_encoding(this);
4712     __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4713   %}
4714   ins_pipe( pipe_slow );
4715 %}
4716 
4717 instruct vdivD_mem(vec dst, vec src, memory mem) %{
4718   predicate(UseAVX &gt; 0);
4719   match(Set dst (DivVD src (LoadVector mem)));
4720   format %{ &quot;vdivpd  $dst,$src,$mem\t! div packedD&quot; %}
4721   ins_encode %{
4722     int vector_len = vector_length_encoding(this);
4723     __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4724   %}
4725   ins_pipe( pipe_slow );
4726 %}
4727 
4728 // --------------------------------- Sqrt --------------------------------------
4729 
4730 instruct vsqrtF_reg(vec dst, vec src) %{
4731   match(Set dst (SqrtVF src));
4732   format %{ &quot;vsqrtps  $dst,$src\t! sqrt packedF&quot; %}
4733   ins_encode %{
4734     assert(UseAVX &gt; 0, &quot;required&quot;);
4735     int vector_len = vector_length_encoding(this);
4736     __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
4737   %}
4738   ins_pipe( pipe_slow );
4739 %}
4740 
4741 instruct vsqrtF_mem(vec dst, memory mem) %{
4742   match(Set dst (SqrtVF (LoadVector mem)));
4743   format %{ &quot;vsqrtps  $dst,$mem\t! sqrt packedF&quot; %}
4744   ins_encode %{
4745     assert(UseAVX &gt; 0, &quot;required&quot;);
4746     int vector_len = vector_length_encoding(this);
4747     __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
4748   %}
4749   ins_pipe( pipe_slow );
4750 %}
4751 
4752 // Floating point vector sqrt
4753 instruct vsqrtD_reg(vec dst, vec src) %{
4754   match(Set dst (SqrtVD src));
4755   format %{ &quot;vsqrtpd  $dst,$src\t! sqrt packedD&quot; %}
4756   ins_encode %{
4757     assert(UseAVX &gt; 0, &quot;required&quot;);
4758     int vector_len = vector_length_encoding(this);
4759     __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
4760   %}
4761   ins_pipe( pipe_slow );
4762 %}
4763 
4764 instruct vsqrtD_mem(vec dst, memory mem) %{
4765   match(Set dst (SqrtVD (LoadVector mem)));
4766   format %{ &quot;vsqrtpd  $dst,$mem\t! sqrt packedD&quot; %}
4767   ins_encode %{
4768     assert(UseAVX &gt; 0, &quot;required&quot;);
4769     int vector_len = vector_length_encoding(this);
4770     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
4771   %}
4772   ins_pipe( pipe_slow );
4773 %}
4774 
4775 // ------------------------------ Shift ---------------------------------------
4776 
4777 // Left and right shift count vectors are the same on x86
4778 // (only lowest bits of xmm reg are used for count).
4779 instruct vshiftcnt(vec dst, rRegI cnt) %{
4780   match(Set dst (LShiftCntV cnt));
4781   match(Set dst (RShiftCntV cnt));
4782   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
4783   ins_encode %{
4784     __ movdl($dst$$XMMRegister, $cnt$$Register);
4785   %}
4786   ins_pipe( pipe_slow );
4787 %}
4788 
4789 // Byte vector shift
4790 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4791   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
4792   match(Set dst (LShiftVB src shift));
4793   match(Set dst (RShiftVB src shift));
4794   match(Set dst (URShiftVB src shift));
4795   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
4796   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4797   ins_encode %{
4798     assert(UseSSE &gt; 3, &quot;required&quot;);
4799     int opcode = this-&gt;ideal_Opcode();
4800     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
4801     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
4802     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4803     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4804     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4805   %}
4806   ins_pipe( pipe_slow );
4807 %}
4808 
4809 instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
4810   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4811   match(Set dst (LShiftVB src shift));
4812   match(Set dst (RShiftVB src shift));
4813   match(Set dst (URShiftVB src shift));
4814   effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);
4815   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4816   ins_encode %{
4817     assert(UseSSE &gt; 3, &quot;required&quot;);
4818     int opcode = this-&gt;ideal_Opcode();
4819 
4820     __ vextendbw(opcode, $tmp1$$XMMRegister, $src$$XMMRegister);
4821     __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
4822     __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
4823     __ vextendbw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
4824     __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
4825     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4826     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
4827     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
4828     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
4829   %}
4830   ins_pipe( pipe_slow );
4831 %}
4832 
4833 instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4834   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
4835   match(Set dst (LShiftVB src shift));
4836   match(Set dst (RShiftVB src shift));
4837   match(Set dst (URShiftVB src shift));
4838   effect(TEMP dst, TEMP tmp, TEMP scratch);
4839   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4840   ins_encode %{
4841     int opcode = this-&gt;ideal_Opcode();
4842     int vector_len = Assembler::AVX_256bit;
4843     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister, vector_len);
4844     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
4845     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
4846     __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
4847     __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
4848   %}
4849   ins_pipe( pipe_slow );
4850 %}
4851 
4852 instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4853   predicate(n-&gt;as_Vector()-&gt;length() == 32);
4854   match(Set dst (LShiftVB src shift));
4855   match(Set dst (RShiftVB src shift));
4856   match(Set dst (URShiftVB src shift));
4857   effect(TEMP dst, TEMP tmp, TEMP scratch);
4858   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4859   ins_encode %{
4860     assert(UseAVX &gt; 1, &quot;required&quot;);
4861     int opcode = this-&gt;ideal_Opcode();
4862     int vector_len = Assembler::AVX_256bit;
4863     __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
4864     __ vextendbw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
4865     __ vextendbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, vector_len);
4866     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
4867     __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vector_len);
4868     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
4869     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
4870     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
4871     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
4872   %}
4873   ins_pipe( pipe_slow );
4874 %}
4875 
4876 instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
4877   predicate(n-&gt;as_Vector()-&gt;length() == 64);
4878   match(Set dst (LShiftVB src shift));
4879   match(Set dst (RShiftVB src shift));
4880   match(Set dst (URShiftVB src shift));
4881   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4882   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4883   ins_encode %{
4884     assert(UseAVX &gt; 2, &quot;required&quot;);
4885     int opcode = this-&gt;ideal_Opcode();
4886     int vector_len = Assembler::AVX_512bit;
4887     __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
4888     __ vextendbw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4889     __ vextendbw(opcode, $tmp2$$XMMRegister, $src$$XMMRegister, vector_len);
4890     __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vector_len);
4891     __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vector_len);
4892     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4893     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4894     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4895     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4896     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4897     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
4898     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4899   %}
4900   ins_pipe( pipe_slow );
4901 %}
4902 
4903 // Shorts vector logical right shift produces incorrect Java result
4904 // for negative data because java code convert short value into int with
4905 // sign extension before a shift. But char vectors are fine since chars are
4906 // unsigned values.
4907 // Shorts/Chars vector left shift
4908 instruct vshiftS(vec dst, vec src, vec shift) %{
4909   match(Set dst (LShiftVS src shift));
4910   match(Set dst (RShiftVS src shift));
4911   match(Set dst (URShiftVS src shift));
4912   effect(TEMP dst, USE src, USE shift);
4913   format %{ &quot;vshiftw  $dst,$src,$shift\t! shift packedS&quot; %}
4914   ins_encode %{
4915     int opcode = this-&gt;ideal_Opcode();
4916     if (UseAVX &gt; 0) {
4917       int vlen_enc = vector_length_encoding(this);
4918       __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
4919     } else {
4920       int vlen = vector_length(this);
4921       if (vlen == 2) {
4922         __ movflt($dst$$XMMRegister, $src$$XMMRegister);
4923         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4924       } else if (vlen == 4) {
4925         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
4926         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4927       } else {
4928         assert (vlen == 8, &quot;sanity&quot;);
4929         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4930         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4931       }
4932     }
4933   %}
4934   ins_pipe( pipe_slow );
4935 %}
4936 
4937 // Integers vector left shift
4938 instruct vshiftI(vec dst, vec src, vec shift) %{
4939   match(Set dst (LShiftVI src shift));
4940   match(Set dst (RShiftVI src shift));
4941   match(Set dst (URShiftVI src shift));
4942   effect(TEMP dst, USE src, USE shift);
4943   format %{ &quot;vshiftd  $dst,$src,$shift\t! shift packedI&quot; %}
4944   ins_encode %{
4945     int opcode = this-&gt;ideal_Opcode();
4946     if (UseAVX &gt; 0) {
4947       int vector_len = vector_length_encoding(this);
4948       __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
4949     } else {
4950       int vlen = vector_length(this);
4951       if (vlen == 2) {
4952         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
4953         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4954       } else {
4955         assert(vlen == 4, &quot;sanity&quot;);
4956         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4957         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4958       }
4959     }
4960   %}
4961   ins_pipe( pipe_slow );
4962 %}
4963 
4964 // Longs vector shift
4965 instruct vshiftL(vec dst, vec src, vec shift) %{
4966   match(Set dst (LShiftVL src shift));
4967   match(Set dst (URShiftVL src shift));
4968   effect(TEMP dst, USE src, USE shift);
4969   format %{ &quot;vshiftq  $dst,$src,$shift\t! shift packedL&quot; %}
4970   ins_encode %{
4971     int opcode = this-&gt;ideal_Opcode();
4972     if (UseAVX &gt; 0) {
4973       int vector_len = vector_length_encoding(this);
4974       __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
4975     } else {
4976       assert(vector_length(this) == 2, &quot;&quot;);
4977       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4978       __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4979     }
4980   %}
4981   ins_pipe( pipe_slow );
4982 %}
4983 
4984 // -------------------ArithmeticRightShift -----------------------------------
4985 // Long vector arithmetic right shift
4986 instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4987   predicate(UseAVX &lt;= 2);
4988   match(Set dst (RShiftVL src shift));
4989   effect(TEMP dst, TEMP tmp, TEMP scratch);
4990   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
4991   ins_encode %{
4992     uint vlen = vector_length(this);
4993     if (vlen == 2) {
4994       assert(UseSSE &gt;= 2, &quot;required&quot;);
4995       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4996       __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
4997       __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
4998       __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
4999       __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
5000       __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
5001     } else {
5002       assert(vlen == 4, &quot;sanity&quot;);
5003       assert(UseAVX &gt; 1, &quot;required&quot;);
5004       int vector_len = Assembler::AVX_256bit;
5005       __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5006       __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5007       __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5008       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5009       __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5010     }
5011   %}
5012   ins_pipe( pipe_slow );
5013 %}
5014 
5015 instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
5016   predicate(UseAVX &gt; 2);
5017   match(Set dst (RShiftVL src shift));
5018   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5019   ins_encode %{
5020     int vector_len = vector_length_encoding(this);
5021     __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5022   %}
5023   ins_pipe( pipe_slow );
5024 %}
5025 
5026 // --------------------------------- AND --------------------------------------
5027 
5028 instruct vand(vec dst, vec src) %{
5029   predicate(UseAVX == 0);
5030   match(Set dst (AndV dst src));
5031   format %{ &quot;pand    $dst,$src\t! and vectors&quot; %}
5032   ins_encode %{
5033     __ pand($dst$$XMMRegister, $src$$XMMRegister);
5034   %}
5035   ins_pipe( pipe_slow );
5036 %}
5037 
5038 instruct vand_reg(vec dst, vec src1, vec src2) %{
5039   predicate(UseAVX &gt; 0);
5040   match(Set dst (AndV src1 src2));
5041   format %{ &quot;vpand   $dst,$src1,$src2\t! and vectors&quot; %}
5042   ins_encode %{
5043     int vector_len = vector_length_encoding(this);
5044     __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5045   %}
5046   ins_pipe( pipe_slow );
5047 %}
5048 
5049 instruct vand_mem(vec dst, vec src, memory mem) %{
5050   predicate(UseAVX &gt; 0);
5051   match(Set dst (AndV src (LoadVector mem)));
5052   format %{ &quot;vpand   $dst,$src,$mem\t! and vectors&quot; %}
5053   ins_encode %{
5054     int vector_len = vector_length_encoding(this);
5055     __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5056   %}
5057   ins_pipe( pipe_slow );
5058 %}
5059 
5060 // --------------------------------- OR ---------------------------------------
5061 
5062 instruct vor(vec dst, vec src) %{
5063   predicate(UseAVX == 0);
5064   match(Set dst (OrV dst src));
5065   format %{ &quot;por     $dst,$src\t! or vectors&quot; %}
5066   ins_encode %{
5067     __ por($dst$$XMMRegister, $src$$XMMRegister);
5068   %}
5069   ins_pipe( pipe_slow );
5070 %}
5071 
5072 instruct vor_reg(vec dst, vec src1, vec src2) %{
5073   predicate(UseAVX &gt; 0);
5074   match(Set dst (OrV src1 src2));
5075   format %{ &quot;vpor    $dst,$src1,$src2\t! or vectors&quot; %}
5076   ins_encode %{
5077     int vector_len = vector_length_encoding(this);
5078     __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5079   %}
5080   ins_pipe( pipe_slow );
5081 %}
5082 
5083 instruct vor_mem(vec dst, vec src, memory mem) %{
5084   predicate(UseAVX &gt; 0);
5085   match(Set dst (OrV src (LoadVector mem)));
5086   format %{ &quot;vpor    $dst,$src,$mem\t! or vectors&quot; %}
5087   ins_encode %{
5088     int vector_len = vector_length_encoding(this);
5089     __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5090   %}
5091   ins_pipe( pipe_slow );
5092 %}
5093 
5094 // --------------------------------- XOR --------------------------------------
5095 
5096 instruct vxor(vec dst, vec src) %{
5097   predicate(UseAVX == 0);
5098   match(Set dst (XorV dst src));
5099   format %{ &quot;pxor    $dst,$src\t! xor vectors&quot; %}
5100   ins_encode %{
5101     __ pxor($dst$$XMMRegister, $src$$XMMRegister);
5102   %}
5103   ins_pipe( pipe_slow );
5104 %}
5105 
5106 instruct vxor_reg(vec dst, vec src1, vec src2) %{
5107   predicate(UseAVX &gt; 0);
5108   match(Set dst (XorV src1 src2));
5109   format %{ &quot;vpxor   $dst,$src1,$src2\t! xor vectors&quot; %}
5110   ins_encode %{
5111     int vector_len = vector_length_encoding(this);
5112     __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5113   %}
5114   ins_pipe( pipe_slow );
5115 %}
5116 
5117 instruct vxor_mem(vec dst, vec src, memory mem) %{
5118   predicate(UseAVX &gt; 0);
5119   match(Set dst (XorV src (LoadVector mem)));
5120   format %{ &quot;vpxor   $dst,$src,$mem\t! xor vectors&quot; %}
5121   ins_encode %{
5122     int vector_len = vector_length_encoding(this);
5123     __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5124   %}
5125   ins_pipe( pipe_slow );
5126 %}
5127 
5128 // --------------------------------- ABS --------------------------------------
5129 // a = |a|
5130 instruct vabsB_reg(vec dst, vec src) %{
5131   match(Set dst (AbsVB  src));
5132   format %{ &quot;vabsb $dst,$src\t# $dst = |$src| abs packedB&quot; %}
5133   ins_encode %{
5134     uint vlen = vector_length(this);
5135     if (vlen &lt;= 16) {
5136       __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
5137     } else {
5138       int vlen_enc = vector_length_encoding(this);
5139       __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5140     }
5141   %}
5142   ins_pipe( pipe_slow );
5143 %}
5144 
5145 instruct vabsS_reg(vec dst, vec src) %{
5146   match(Set dst (AbsVS  src));
5147   format %{ &quot;vabsw $dst,$src\t# $dst = |$src| abs packedS&quot; %}
5148   ins_encode %{
5149     uint vlen = vector_length(this);
5150     if (vlen &lt;= 8) {
5151       __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
5152     } else {
5153       int vlen_enc = vector_length_encoding(this);
5154       __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5155     }
5156   %}
5157   ins_pipe( pipe_slow );
5158 %}
5159 
5160 instruct vabsI_reg(vec dst, vec src) %{
5161   match(Set dst (AbsVI  src));
5162   format %{ &quot;pabsd $dst,$src\t# $dst = |$src| abs packedI&quot; %}
5163   ins_encode %{
5164     uint vlen = vector_length(this);
5165     if (vlen &lt;= 4) {
5166       __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
5167     } else {
5168       int vlen_enc = vector_length_encoding(this);
5169       __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5170     }
5171   %}
5172   ins_pipe( pipe_slow );
5173 %}
5174 
5175 instruct vabsL_reg(vec dst, vec src) %{
5176   match(Set dst (AbsVL  src));
5177   format %{ &quot;evpabsq $dst,$src\t# $dst = |$src| abs packedL&quot; %}
5178   ins_encode %{
5179     assert(UseAVX &gt; 2, &quot;required&quot;);
5180     int vector_len = vector_length_encoding(this);
5181     __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5182   %}
5183   ins_pipe( pipe_slow );
5184 %}
5185 
5186 // --------------------------------- ABSNEG --------------------------------------
5187 
5188 instruct vabsnegF(vec dst, vec src, rRegI scratch) %{
5189   predicate(n-&gt;as_Vector()-&gt;length() != 4); // handled by 1-operand instruction vabsneg4F
5190   match(Set dst (AbsVF src));
5191   match(Set dst (NegVF src));
5192   effect(TEMP scratch);
5193   format %{ &quot;vabsnegf $dst,$src,[mask]\t# absneg packedF&quot; %}
5194   ins_cost(150);
5195   ins_encode %{
5196     int opcode = this-&gt;ideal_Opcode();
5197     int vlen = vector_length(this);
5198     if (vlen == 2) {
5199       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5200     } else {
5201       assert(vlen == 8 || vlen == 16, &quot;required&quot;);
5202       int vlen_enc = vector_length_encoding(this);
5203       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5204     }
5205   %}
5206   ins_pipe( pipe_slow );
5207 %}
5208 
5209 instruct vabsneg4F(vec dst, rRegI scratch) %{
5210   predicate(n-&gt;as_Vector()-&gt;length() == 4);
5211   match(Set dst (AbsVF dst));
5212   match(Set dst (NegVF dst));
5213   effect(TEMP scratch);
5214   format %{ &quot;vabsnegf $dst,[mask]\t# absneg packed4F&quot; %}
5215   ins_cost(150);
5216   ins_encode %{
5217     int opcode = this-&gt;ideal_Opcode();
5218     __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);
5219   %}
5220   ins_pipe( pipe_slow );
5221 %}
5222 
5223 instruct vabsnegD(vec dst, vec src, rRegI scratch) %{
5224   match(Set dst (AbsVD  src));
5225   match(Set dst (NegVD  src));
5226   effect(TEMP scratch);
5227   format %{ &quot;vabsnegd $dst,$src,[mask]\t# absneg packedD&quot; %}
5228   ins_encode %{
5229     int opcode = this-&gt;ideal_Opcode();
5230     uint vlen = vector_length(this);
5231     if (vlen == 2) {
5232       assert(UseSSE &gt;= 2, &quot;required&quot;);
5233       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5234     } else {
5235       int vlen_enc = vector_length_encoding(this);
5236       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5237     }
5238   %}
5239   ins_pipe( pipe_slow );
5240 %}
5241 
5242 // --------------------------------- FMA --------------------------------------
5243 // a * b + c
5244 
5245 instruct vfmaF_reg(vec a, vec b, vec c) %{
5246   match(Set c (FmaVF  c (Binary a b)));
5247   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5248   ins_cost(150);
5249   ins_encode %{
5250     assert(UseFMA, &quot;not enabled&quot;);
5251     int vector_len = vector_length_encoding(this);
5252     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5253   %}
5254   ins_pipe( pipe_slow );
5255 %}
5256 
5257 instruct vfmaF_mem(vec a, memory b, vec c) %{
5258   match(Set c (FmaVF  c (Binary a (LoadVector b))));
5259   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5260   ins_cost(150);
5261   ins_encode %{
5262     assert(UseFMA, &quot;not enabled&quot;);
5263     int vector_len = vector_length_encoding(this);
5264     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5265   %}
5266   ins_pipe( pipe_slow );
5267 %}
5268 
5269 instruct vfmaD_reg(vec a, vec b, vec c) %{
5270   match(Set c (FmaVD  c (Binary a b)));
5271   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5272   ins_cost(150);
5273   ins_encode %{
5274     assert(UseFMA, &quot;not enabled&quot;);
5275     int vector_len = vector_length_encoding(this);
5276     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5277   %}
5278   ins_pipe( pipe_slow );
5279 %}
5280 
5281 instruct vfmaD_mem(vec a, memory b, vec c) %{
5282   match(Set c (FmaVD  c (Binary a (LoadVector b))));
5283   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5284   ins_cost(150);
5285   ins_encode %{
5286     assert(UseFMA, &quot;not enabled&quot;);
5287     int vector_len = vector_length_encoding(this);
5288     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5289   %}
5290   ins_pipe( pipe_slow );
5291 %}
5292 
5293 // --------------------------------- Vector Multiply Add --------------------------------------
5294 
5295 instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
5296   predicate(UseAVX == 0);
5297   match(Set dst (MulAddVS2VI dst src1));
5298   format %{ &quot;pmaddwd $dst,$dst,$src1\t! muladd packedStoI&quot; %}
5299   ins_encode %{
5300     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5301   %}
5302   ins_pipe( pipe_slow );
5303 %}
5304 
5305 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5306   predicate(UseAVX &gt; 0);
5307   match(Set dst (MulAddVS2VI src1 src2));
5308   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5309   ins_encode %{
5310     int vector_len = vector_length_encoding(this);
5311     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5312   %}
5313   ins_pipe( pipe_slow );
5314 %}
5315 
5316 // --------------------------------- Vector Multiply Add Add ----------------------------------
5317 
5318 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
5319   predicate(VM_Version::supports_avx512_vnni());
5320   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5321   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5322   ins_encode %{
5323     assert(UseAVX &gt; 2, &quot;required&quot;);
5324     int vector_len = vector_length_encoding(this);
5325     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5326   %}
5327   ins_pipe( pipe_slow );
5328   ins_cost(10);
5329 %}
5330 
5331 // --------------------------------- PopCount --------------------------------------
5332 
5333 instruct vpopcountI(vec dst, vec src) %{
5334   match(Set dst (PopCountVI src));
5335   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5336   ins_encode %{
5337     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5338 
5339     int vector_len = vector_length_encoding(this);
5340     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5341   %}
5342   ins_pipe( pipe_slow );
5343 %}
5344 
5345 // --------------------------------- Bitwise Ternary Logic ----------------------------------
5346 
5347 instruct vpternlogdB(vec dst, vec src2, vec src3, immU8 func) %{
5348   match(Set dst (MacroLogicV (Binary dst src2) (Binary src3 func)));
5349   effect(TEMP dst);
5350   format %{ &quot;vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic&quot; %}
5351   ins_encode %{
5352     int vector_len = vector_length_encoding(this);
5353     __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$XMMRegister, vector_len);
5354   %}
5355   ins_pipe( pipe_slow );
5356 %}
5357 
5358 instruct vpternlogdB_mem(vec dst, vec src2, memory src3, immU8 func) %{
5359   match(Set dst (MacroLogicV (Binary dst src2) (Binary (LoadVector src3) func)));
5360   effect(TEMP dst);
5361   format %{ &quot;vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic&quot; %}
5362   ins_encode %{
5363     int vector_len = vector_length_encoding(this);
5364     __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$Address, vector_len);
5365   %}
5366   ins_pipe( pipe_slow );
5367 %}
    </pre>
  </body>
</html>