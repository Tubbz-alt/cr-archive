<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 //
   2 // Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
   3 // DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4 //
   5 // This code is free software; you can redistribute it and/or modify it
   6 // under the terms of the GNU General Public License version 2 only, as
   7 // published by the Free Software Foundation.
   8 //
   9 // This code is distributed in the hope that it will be useful, but WITHOUT
  10 // ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11 // FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12 // version 2 for more details (a copy is included in the LICENSE file that
  13 // accompanied this code).
  14 //
  15 // You should have received a copy of the GNU General Public License version
  16 // 2 along with this work; if not, write to the Free Software Foundation,
  17 // Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18 //
  19 // Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20 // or visit www.oracle.com if you need additional information or have any
  21 // questions.
  22 //
  23 //
  24 
  25 // X86 Common Architecture Description File
  26 
  27 //----------REGISTER DEFINITION BLOCK------------------------------------------
  28 // This information is used by the matcher and the register allocator to
  29 // describe individual registers and classes of registers within the target
  30 // archtecture.
  31 
  32 register %{
  33 //----------Architecture Description Register Definitions----------------------
  34 // General Registers
  35 // &quot;reg_def&quot;  name ( register save type, C convention save type,
  36 //                   ideal register type, encoding );
  37 // Register Save Types:
  38 //
  39 // NS  = No-Save:       The register allocator assumes that these registers
  40 //                      can be used without saving upon entry to the method, &amp;
  41 //                      that they do not need to be saved at call sites.
  42 //
  43 // SOC = Save-On-Call:  The register allocator assumes that these registers
  44 //                      can be used without saving upon entry to the method,
  45 //                      but that they must be saved at call sites.
  46 //
  47 // SOE = Save-On-Entry: The register allocator assumes that these registers
  48 //                      must be saved before using them upon entry to the
  49 //                      method, but they do not need to be saved at call
  50 //                      sites.
  51 //
  52 // AS  = Always-Save:   The register allocator assumes that these registers
  53 //                      must be saved before using them upon entry to the
  54 //                      method, &amp; that they must be saved at call sites.
  55 //
  56 // Ideal Register Type is used to determine how to save &amp; restore a
  57 // register.  Op_RegI will get spilled with LoadI/StoreI, Op_RegP will get
  58 // spilled with LoadP/StoreP.  If the register supports both, use Op_RegI.
  59 //
  60 // The encoding number is the actual bit-pattern placed into the opcodes.
  61 
  62 // XMM registers.  512-bit registers or 8 words each, labeled (a)-p.
  63 // Word a in each register holds a Float, words ab hold a Double.
  64 // The whole registers are used in SSE4.2 version intrinsics,
  65 // array copy stubs and superword operations (see UseSSE42Intrinsics,
  66 // UseXMMForArrayCopy and UseSuperword flags).
  67 // For pre EVEX enabled architectures:
  68 //      XMM8-XMM15 must be encoded with REX (VEX for UseAVX)
  69 // For EVEX enabled architectures:
  70 //      XMM8-XMM31 must be encoded with REX (EVEX for UseAVX).
  71 //
  72 // Linux ABI:   No register preserved across function calls
  73 //              XMM0-XMM7 might hold parameters
  74 // Windows ABI: XMM6-XMM31 preserved across function calls
  75 //              XMM0-XMM3 might hold parameters
  76 
  77 reg_def XMM0 ( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg());
  78 reg_def XMM0b( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(1));
  79 reg_def XMM0c( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(2));
  80 reg_def XMM0d( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(3));
  81 reg_def XMM0e( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(4));
  82 reg_def XMM0f( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(5));
  83 reg_def XMM0g( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(6));
  84 reg_def XMM0h( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(7));
  85 reg_def XMM0i( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(8));
  86 reg_def XMM0j( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(9));
  87 reg_def XMM0k( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(10));
  88 reg_def XMM0l( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(11));
  89 reg_def XMM0m( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(12));
  90 reg_def XMM0n( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(13));
  91 reg_def XMM0o( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(14));
  92 reg_def XMM0p( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(15));
  93 
  94 reg_def XMM1 ( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg());
  95 reg_def XMM1b( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(1));
  96 reg_def XMM1c( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(2));
  97 reg_def XMM1d( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(3));
  98 reg_def XMM1e( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(4));
  99 reg_def XMM1f( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(5));
 100 reg_def XMM1g( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(6));
 101 reg_def XMM1h( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(7));
 102 reg_def XMM1i( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(8));
 103 reg_def XMM1j( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(9));
 104 reg_def XMM1k( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(10));
 105 reg_def XMM1l( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(11));
 106 reg_def XMM1m( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(12));
 107 reg_def XMM1n( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(13));
 108 reg_def XMM1o( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(14));
 109 reg_def XMM1p( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(15));
 110 
 111 reg_def XMM2 ( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg());
 112 reg_def XMM2b( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(1));
 113 reg_def XMM2c( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(2));
 114 reg_def XMM2d( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(3));
 115 reg_def XMM2e( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(4));
 116 reg_def XMM2f( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(5));
 117 reg_def XMM2g( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(6));
 118 reg_def XMM2h( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(7));
 119 reg_def XMM2i( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(8));
 120 reg_def XMM2j( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(9));
 121 reg_def XMM2k( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(10));
 122 reg_def XMM2l( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(11));
 123 reg_def XMM2m( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(12));
 124 reg_def XMM2n( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(13));
 125 reg_def XMM2o( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(14));
 126 reg_def XMM2p( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(15));
 127 
 128 reg_def XMM3 ( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg());
 129 reg_def XMM3b( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(1));
 130 reg_def XMM3c( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(2));
 131 reg_def XMM3d( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(3));
 132 reg_def XMM3e( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(4));
 133 reg_def XMM3f( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(5));
 134 reg_def XMM3g( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(6));
 135 reg_def XMM3h( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(7));
 136 reg_def XMM3i( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(8));
 137 reg_def XMM3j( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(9));
 138 reg_def XMM3k( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(10));
 139 reg_def XMM3l( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(11));
 140 reg_def XMM3m( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(12));
 141 reg_def XMM3n( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(13));
 142 reg_def XMM3o( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(14));
 143 reg_def XMM3p( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(15));
 144 
 145 reg_def XMM4 ( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg());
 146 reg_def XMM4b( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(1));
 147 reg_def XMM4c( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(2));
 148 reg_def XMM4d( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(3));
 149 reg_def XMM4e( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(4));
 150 reg_def XMM4f( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(5));
 151 reg_def XMM4g( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(6));
 152 reg_def XMM4h( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(7));
 153 reg_def XMM4i( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(8));
 154 reg_def XMM4j( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(9));
 155 reg_def XMM4k( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(10));
 156 reg_def XMM4l( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(11));
 157 reg_def XMM4m( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(12));
 158 reg_def XMM4n( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(13));
 159 reg_def XMM4o( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(14));
 160 reg_def XMM4p( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(15));
 161 
 162 reg_def XMM5 ( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg());
 163 reg_def XMM5b( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(1));
 164 reg_def XMM5c( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(2));
 165 reg_def XMM5d( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(3));
 166 reg_def XMM5e( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(4));
 167 reg_def XMM5f( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(5));
 168 reg_def XMM5g( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(6));
 169 reg_def XMM5h( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(7));
 170 reg_def XMM5i( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(8));
 171 reg_def XMM5j( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(9));
 172 reg_def XMM5k( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(10));
 173 reg_def XMM5l( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(11));
 174 reg_def XMM5m( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(12));
 175 reg_def XMM5n( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(13));
 176 reg_def XMM5o( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(14));
 177 reg_def XMM5p( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(15));
 178 
 179 reg_def XMM6 ( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg());
 180 reg_def XMM6b( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(1));
 181 reg_def XMM6c( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(2));
 182 reg_def XMM6d( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(3));
 183 reg_def XMM6e( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(4));
 184 reg_def XMM6f( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(5));
 185 reg_def XMM6g( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(6));
 186 reg_def XMM6h( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(7));
 187 reg_def XMM6i( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(8));
 188 reg_def XMM6j( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(9));
 189 reg_def XMM6k( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(10));
 190 reg_def XMM6l( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(11));
 191 reg_def XMM6m( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(12));
 192 reg_def XMM6n( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(13));
 193 reg_def XMM6o( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(14));
 194 reg_def XMM6p( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(15));
 195 
 196 reg_def XMM7 ( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg());
 197 reg_def XMM7b( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(1));
 198 reg_def XMM7c( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(2));
 199 reg_def XMM7d( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(3));
 200 reg_def XMM7e( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(4));
 201 reg_def XMM7f( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(5));
 202 reg_def XMM7g( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(6));
 203 reg_def XMM7h( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(7));
 204 reg_def XMM7i( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(8));
 205 reg_def XMM7j( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(9));
 206 reg_def XMM7k( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(10));
 207 reg_def XMM7l( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(11));
 208 reg_def XMM7m( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(12));
 209 reg_def XMM7n( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(13));
 210 reg_def XMM7o( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(14));
 211 reg_def XMM7p( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(15));
 212 
 213 #ifdef _LP64
 214 
 215 reg_def XMM8 ( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg());
 216 reg_def XMM8b( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(1));
 217 reg_def XMM8c( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(2));
 218 reg_def XMM8d( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(3));
 219 reg_def XMM8e( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(4));
 220 reg_def XMM8f( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(5));
 221 reg_def XMM8g( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(6));
 222 reg_def XMM8h( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(7));
 223 reg_def XMM8i( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(8));
 224 reg_def XMM8j( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(9));
 225 reg_def XMM8k( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(10));
 226 reg_def XMM8l( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(11));
 227 reg_def XMM8m( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(12));
 228 reg_def XMM8n( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(13));
 229 reg_def XMM8o( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(14));
 230 reg_def XMM8p( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(15));
 231 
 232 reg_def XMM9 ( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg());
 233 reg_def XMM9b( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(1));
 234 reg_def XMM9c( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(2));
 235 reg_def XMM9d( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(3));
 236 reg_def XMM9e( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(4));
 237 reg_def XMM9f( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(5));
 238 reg_def XMM9g( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(6));
 239 reg_def XMM9h( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(7));
 240 reg_def XMM9i( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(8));
 241 reg_def XMM9j( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(9));
 242 reg_def XMM9k( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(10));
 243 reg_def XMM9l( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(11));
 244 reg_def XMM9m( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(12));
 245 reg_def XMM9n( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(13));
 246 reg_def XMM9o( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(14));
 247 reg_def XMM9p( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(15));
 248 
 249 reg_def XMM10 ( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg());
 250 reg_def XMM10b( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(1));
 251 reg_def XMM10c( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(2));
 252 reg_def XMM10d( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(3));
 253 reg_def XMM10e( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(4));
 254 reg_def XMM10f( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(5));
 255 reg_def XMM10g( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(6));
 256 reg_def XMM10h( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(7));
 257 reg_def XMM10i( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(8));
 258 reg_def XMM10j( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(9));
 259 reg_def XMM10k( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(10));
 260 reg_def XMM10l( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(11));
 261 reg_def XMM10m( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(12));
 262 reg_def XMM10n( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(13));
 263 reg_def XMM10o( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(14));
 264 reg_def XMM10p( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(15));
 265 
 266 reg_def XMM11 ( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg());
 267 reg_def XMM11b( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(1));
 268 reg_def XMM11c( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(2));
 269 reg_def XMM11d( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(3));
 270 reg_def XMM11e( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(4));
 271 reg_def XMM11f( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(5));
 272 reg_def XMM11g( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(6));
 273 reg_def XMM11h( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(7));
 274 reg_def XMM11i( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(8));
 275 reg_def XMM11j( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(9));
 276 reg_def XMM11k( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(10));
 277 reg_def XMM11l( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(11));
 278 reg_def XMM11m( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(12));
 279 reg_def XMM11n( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(13));
 280 reg_def XMM11o( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(14));
 281 reg_def XMM11p( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(15));
 282 
 283 reg_def XMM12 ( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg());
 284 reg_def XMM12b( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(1));
 285 reg_def XMM12c( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(2));
 286 reg_def XMM12d( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(3));
 287 reg_def XMM12e( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(4));
 288 reg_def XMM12f( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(5));
 289 reg_def XMM12g( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(6));
 290 reg_def XMM12h( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(7));
 291 reg_def XMM12i( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(8));
 292 reg_def XMM12j( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(9));
 293 reg_def XMM12k( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(10));
 294 reg_def XMM12l( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(11));
 295 reg_def XMM12m( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(12));
 296 reg_def XMM12n( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(13));
 297 reg_def XMM12o( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(14));
 298 reg_def XMM12p( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(15));
 299 
 300 reg_def XMM13 ( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg());
 301 reg_def XMM13b( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(1));
 302 reg_def XMM13c( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(2));
 303 reg_def XMM13d( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(3));
 304 reg_def XMM13e( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(4));
 305 reg_def XMM13f( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(5));
 306 reg_def XMM13g( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(6));
 307 reg_def XMM13h( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(7));
 308 reg_def XMM13i( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(8));
 309 reg_def XMM13j( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(9));
 310 reg_def XMM13k( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(10));
 311 reg_def XMM13l( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(11));
 312 reg_def XMM13m( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(12));
 313 reg_def XMM13n( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(13));
 314 reg_def XMM13o( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(14));
 315 reg_def XMM13p( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(15));
 316 
 317 reg_def XMM14 ( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg());
 318 reg_def XMM14b( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(1));
 319 reg_def XMM14c( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(2));
 320 reg_def XMM14d( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(3));
 321 reg_def XMM14e( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(4));
 322 reg_def XMM14f( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(5));
 323 reg_def XMM14g( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(6));
 324 reg_def XMM14h( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(7));
 325 reg_def XMM14i( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(8));
 326 reg_def XMM14j( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(9));
 327 reg_def XMM14k( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(10));
 328 reg_def XMM14l( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(11));
 329 reg_def XMM14m( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(12));
 330 reg_def XMM14n( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(13));
 331 reg_def XMM14o( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(14));
 332 reg_def XMM14p( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(15));
 333 
 334 reg_def XMM15 ( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg());
 335 reg_def XMM15b( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(1));
 336 reg_def XMM15c( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(2));
 337 reg_def XMM15d( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(3));
 338 reg_def XMM15e( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(4));
 339 reg_def XMM15f( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(5));
 340 reg_def XMM15g( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(6));
 341 reg_def XMM15h( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(7));
 342 reg_def XMM15i( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(8));
 343 reg_def XMM15j( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(9));
 344 reg_def XMM15k( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(10));
 345 reg_def XMM15l( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(11));
 346 reg_def XMM15m( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(12));
 347 reg_def XMM15n( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(13));
 348 reg_def XMM15o( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(14));
 349 reg_def XMM15p( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(15));
 350 
 351 reg_def XMM16 ( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg());
 352 reg_def XMM16b( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(1));
 353 reg_def XMM16c( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(2));
 354 reg_def XMM16d( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(3));
 355 reg_def XMM16e( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(4));
 356 reg_def XMM16f( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(5));
 357 reg_def XMM16g( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(6));
 358 reg_def XMM16h( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(7));
 359 reg_def XMM16i( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(8));
 360 reg_def XMM16j( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(9));
 361 reg_def XMM16k( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(10));
 362 reg_def XMM16l( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(11));
 363 reg_def XMM16m( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(12));
 364 reg_def XMM16n( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(13));
 365 reg_def XMM16o( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(14));
 366 reg_def XMM16p( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(15));
 367 
 368 reg_def XMM17 ( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg());
 369 reg_def XMM17b( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(1));
 370 reg_def XMM17c( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(2));
 371 reg_def XMM17d( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(3));
 372 reg_def XMM17e( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(4));
 373 reg_def XMM17f( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(5));
 374 reg_def XMM17g( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(6));
 375 reg_def XMM17h( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(7));
 376 reg_def XMM17i( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(8));
 377 reg_def XMM17j( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(9));
 378 reg_def XMM17k( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(10));
 379 reg_def XMM17l( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(11));
 380 reg_def XMM17m( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(12));
 381 reg_def XMM17n( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(13));
 382 reg_def XMM17o( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(14));
 383 reg_def XMM17p( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(15));
 384 
 385 reg_def XMM18 ( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg());
 386 reg_def XMM18b( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(1));
 387 reg_def XMM18c( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(2));
 388 reg_def XMM18d( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(3));
 389 reg_def XMM18e( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(4));
 390 reg_def XMM18f( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(5));
 391 reg_def XMM18g( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(6));
 392 reg_def XMM18h( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(7));
 393 reg_def XMM18i( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(8));
 394 reg_def XMM18j( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(9));
 395 reg_def XMM18k( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(10));
 396 reg_def XMM18l( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(11));
 397 reg_def XMM18m( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(12));
 398 reg_def XMM18n( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(13));
 399 reg_def XMM18o( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(14));
 400 reg_def XMM18p( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(15));
 401 
 402 reg_def XMM19 ( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg());
 403 reg_def XMM19b( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(1));
 404 reg_def XMM19c( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(2));
 405 reg_def XMM19d( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(3));
 406 reg_def XMM19e( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(4));
 407 reg_def XMM19f( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(5));
 408 reg_def XMM19g( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(6));
 409 reg_def XMM19h( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(7));
 410 reg_def XMM19i( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(8));
 411 reg_def XMM19j( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(9));
 412 reg_def XMM19k( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(10));
 413 reg_def XMM19l( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(11));
 414 reg_def XMM19m( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(12));
 415 reg_def XMM19n( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(13));
 416 reg_def XMM19o( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(14));
 417 reg_def XMM19p( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(15));
 418 
 419 reg_def XMM20 ( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg());
 420 reg_def XMM20b( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(1));
 421 reg_def XMM20c( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(2));
 422 reg_def XMM20d( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(3));
 423 reg_def XMM20e( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(4));
 424 reg_def XMM20f( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(5));
 425 reg_def XMM20g( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(6));
 426 reg_def XMM20h( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(7));
 427 reg_def XMM20i( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(8));
 428 reg_def XMM20j( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(9));
 429 reg_def XMM20k( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(10));
 430 reg_def XMM20l( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(11));
 431 reg_def XMM20m( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(12));
 432 reg_def XMM20n( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(13));
 433 reg_def XMM20o( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(14));
 434 reg_def XMM20p( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(15));
 435 
 436 reg_def XMM21 ( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg());
 437 reg_def XMM21b( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(1));
 438 reg_def XMM21c( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(2));
 439 reg_def XMM21d( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(3));
 440 reg_def XMM21e( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(4));
 441 reg_def XMM21f( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(5));
 442 reg_def XMM21g( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(6));
 443 reg_def XMM21h( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(7));
 444 reg_def XMM21i( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(8));
 445 reg_def XMM21j( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(9));
 446 reg_def XMM21k( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(10));
 447 reg_def XMM21l( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(11));
 448 reg_def XMM21m( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(12));
 449 reg_def XMM21n( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(13));
 450 reg_def XMM21o( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(14));
 451 reg_def XMM21p( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(15));
 452 
 453 reg_def XMM22 ( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg());
 454 reg_def XMM22b( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(1));
 455 reg_def XMM22c( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(2));
 456 reg_def XMM22d( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(3));
 457 reg_def XMM22e( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(4));
 458 reg_def XMM22f( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(5));
 459 reg_def XMM22g( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(6));
 460 reg_def XMM22h( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(7));
 461 reg_def XMM22i( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(8));
 462 reg_def XMM22j( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(9));
 463 reg_def XMM22k( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(10));
 464 reg_def XMM22l( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(11));
 465 reg_def XMM22m( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(12));
 466 reg_def XMM22n( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(13));
 467 reg_def XMM22o( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(14));
 468 reg_def XMM22p( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(15));
 469 
 470 reg_def XMM23 ( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg());
 471 reg_def XMM23b( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(1));
 472 reg_def XMM23c( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(2));
 473 reg_def XMM23d( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(3));
 474 reg_def XMM23e( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(4));
 475 reg_def XMM23f( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(5));
 476 reg_def XMM23g( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(6));
 477 reg_def XMM23h( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(7));
 478 reg_def XMM23i( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(8));
 479 reg_def XMM23j( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(9));
 480 reg_def XMM23k( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(10));
 481 reg_def XMM23l( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(11));
 482 reg_def XMM23m( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(12));
 483 reg_def XMM23n( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(13));
 484 reg_def XMM23o( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(14));
 485 reg_def XMM23p( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(15));
 486 
 487 reg_def XMM24 ( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg());
 488 reg_def XMM24b( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(1));
 489 reg_def XMM24c( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(2));
 490 reg_def XMM24d( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(3));
 491 reg_def XMM24e( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(4));
 492 reg_def XMM24f( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(5));
 493 reg_def XMM24g( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(6));
 494 reg_def XMM24h( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(7));
 495 reg_def XMM24i( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(8));
 496 reg_def XMM24j( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(9));
 497 reg_def XMM24k( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(10));
 498 reg_def XMM24l( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(11));
 499 reg_def XMM24m( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(12));
 500 reg_def XMM24n( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(13));
 501 reg_def XMM24o( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(14));
 502 reg_def XMM24p( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(15));
 503 
 504 reg_def XMM25 ( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg());
 505 reg_def XMM25b( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(1));
 506 reg_def XMM25c( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(2));
 507 reg_def XMM25d( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(3));
 508 reg_def XMM25e( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(4));
 509 reg_def XMM25f( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(5));
 510 reg_def XMM25g( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(6));
 511 reg_def XMM25h( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(7));
 512 reg_def XMM25i( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(8));
 513 reg_def XMM25j( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(9));
 514 reg_def XMM25k( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(10));
 515 reg_def XMM25l( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(11));
 516 reg_def XMM25m( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(12));
 517 reg_def XMM25n( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(13));
 518 reg_def XMM25o( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(14));
 519 reg_def XMM25p( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(15));
 520 
 521 reg_def XMM26 ( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg());
 522 reg_def XMM26b( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(1));
 523 reg_def XMM26c( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(2));
 524 reg_def XMM26d( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(3));
 525 reg_def XMM26e( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(4));
 526 reg_def XMM26f( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(5));
 527 reg_def XMM26g( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(6));
 528 reg_def XMM26h( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(7));
 529 reg_def XMM26i( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(8));
 530 reg_def XMM26j( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(9));
 531 reg_def XMM26k( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(10));
 532 reg_def XMM26l( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(11));
 533 reg_def XMM26m( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(12));
 534 reg_def XMM26n( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(13));
 535 reg_def XMM26o( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(14));
 536 reg_def XMM26p( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(15));
 537 
 538 reg_def XMM27 ( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg());
 539 reg_def XMM27b( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(1));
 540 reg_def XMM27c( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(2));
 541 reg_def XMM27d( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(3));
 542 reg_def XMM27e( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(4));
 543 reg_def XMM27f( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(5));
 544 reg_def XMM27g( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(6));
 545 reg_def XMM27h( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(7));
 546 reg_def XMM27i( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(8));
 547 reg_def XMM27j( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(9));
 548 reg_def XMM27k( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(10));
 549 reg_def XMM27l( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(11));
 550 reg_def XMM27m( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(12));
 551 reg_def XMM27n( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(13));
 552 reg_def XMM27o( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(14));
 553 reg_def XMM27p( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(15));
 554 
 555 reg_def XMM28 ( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg());
 556 reg_def XMM28b( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(1));
 557 reg_def XMM28c( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(2));
 558 reg_def XMM28d( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(3));
 559 reg_def XMM28e( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(4));
 560 reg_def XMM28f( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(5));
 561 reg_def XMM28g( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(6));
 562 reg_def XMM28h( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(7));
 563 reg_def XMM28i( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(8));
 564 reg_def XMM28j( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(9));
 565 reg_def XMM28k( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(10));
 566 reg_def XMM28l( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(11));
 567 reg_def XMM28m( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(12));
 568 reg_def XMM28n( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(13));
 569 reg_def XMM28o( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(14));
 570 reg_def XMM28p( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(15));
 571 
 572 reg_def XMM29 ( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg());
 573 reg_def XMM29b( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(1));
 574 reg_def XMM29c( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(2));
 575 reg_def XMM29d( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(3));
 576 reg_def XMM29e( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(4));
 577 reg_def XMM29f( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(5));
 578 reg_def XMM29g( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(6));
 579 reg_def XMM29h( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(7));
 580 reg_def XMM29i( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(8));
 581 reg_def XMM29j( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(9));
 582 reg_def XMM29k( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(10));
 583 reg_def XMM29l( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(11));
 584 reg_def XMM29m( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(12));
 585 reg_def XMM29n( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(13));
 586 reg_def XMM29o( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(14));
 587 reg_def XMM29p( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(15));
 588 
 589 reg_def XMM30 ( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg());
 590 reg_def XMM30b( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(1));
 591 reg_def XMM30c( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(2));
 592 reg_def XMM30d( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(3));
 593 reg_def XMM30e( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(4));
 594 reg_def XMM30f( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(5));
 595 reg_def XMM30g( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(6));
 596 reg_def XMM30h( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(7));
 597 reg_def XMM30i( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(8));
 598 reg_def XMM30j( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(9));
 599 reg_def XMM30k( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(10));
 600 reg_def XMM30l( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(11));
 601 reg_def XMM30m( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(12));
 602 reg_def XMM30n( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(13));
 603 reg_def XMM30o( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(14));
 604 reg_def XMM30p( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(15));
 605 
 606 reg_def XMM31 ( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg());
 607 reg_def XMM31b( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(1));
 608 reg_def XMM31c( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(2));
 609 reg_def XMM31d( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(3));
 610 reg_def XMM31e( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(4));
 611 reg_def XMM31f( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(5));
 612 reg_def XMM31g( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(6));
 613 reg_def XMM31h( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(7));
 614 reg_def XMM31i( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(8));
 615 reg_def XMM31j( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(9));
 616 reg_def XMM31k( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(10));
 617 reg_def XMM31l( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(11));
 618 reg_def XMM31m( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(12));
 619 reg_def XMM31n( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(13));
 620 reg_def XMM31o( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(14));
 621 reg_def XMM31p( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(15));
 622 
 623 #endif // _LP64
 624 
 625 #ifdef _LP64
 626 reg_def RFLAGS(SOC, SOC, 0, 16, VMRegImpl::Bad());
 627 #else
 628 reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());
 629 #endif // _LP64
 630 
 631 alloc_class chunk1(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
 632                    XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
 633                    XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
 634                    XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
 635                    XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
 636                    XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
 637                    XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
 638                    XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
 639 #ifdef _LP64
 640                   ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
 641                    XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
 642                    XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
 643                    XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
 644                    XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
 645                    XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
 646                    XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
 647                    XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
 648                   ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
 649                    XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
 650                    XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
 651                    XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
 652                    XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
 653                    XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
 654                    XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
 655                    XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
 656                    XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
 657                    XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
 658                    XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
 659                    XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
 660                    XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
 661                    XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
 662                    XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
 663                    XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
 664 #endif
 665                       );
 666 
 667 // flags allocation class should be last.
 668 alloc_class chunk2(RFLAGS);
 669 
 670 // Singleton class for condition codes
 671 reg_class int_flags(RFLAGS);
 672 
 673 // Class for pre evex float registers
 674 reg_class float_reg_legacy(XMM0,
 675                     XMM1,
 676                     XMM2,
 677                     XMM3,
 678                     XMM4,
 679                     XMM5,
 680                     XMM6,
 681                     XMM7
 682 #ifdef _LP64
 683                    ,XMM8,
 684                     XMM9,
 685                     XMM10,
 686                     XMM11,
 687                     XMM12,
 688                     XMM13,
 689                     XMM14,
 690                     XMM15
 691 #endif
 692                     );
 693 
 694 // Class for evex float registers
 695 reg_class float_reg_evex(XMM0,
 696                     XMM1,
 697                     XMM2,
 698                     XMM3,
 699                     XMM4,
 700                     XMM5,
 701                     XMM6,
 702                     XMM7
 703 #ifdef _LP64
 704                    ,XMM8,
 705                     XMM9,
 706                     XMM10,
 707                     XMM11,
 708                     XMM12,
 709                     XMM13,
 710                     XMM14,
 711                     XMM15,
 712                     XMM16,
 713                     XMM17,
 714                     XMM18,
 715                     XMM19,
 716                     XMM20,
 717                     XMM21,
 718                     XMM22,
 719                     XMM23,
 720                     XMM24,
 721                     XMM25,
 722                     XMM26,
 723                     XMM27,
 724                     XMM28,
 725                     XMM29,
 726                     XMM30,
 727                     XMM31
 728 #endif
 729                     );
 730 
 731 reg_class_dynamic float_reg(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() %} );
 732 reg_class_dynamic float_reg_vl(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 733 
 734 // Class for pre evex double registers
 735 reg_class double_reg_legacy(XMM0,  XMM0b,
 736                      XMM1,  XMM1b,
 737                      XMM2,  XMM2b,
 738                      XMM3,  XMM3b,
 739                      XMM4,  XMM4b,
 740                      XMM5,  XMM5b,
 741                      XMM6,  XMM6b,
 742                      XMM7,  XMM7b
 743 #ifdef _LP64
 744                     ,XMM8,  XMM8b,
 745                      XMM9,  XMM9b,
 746                      XMM10, XMM10b,
 747                      XMM11, XMM11b,
 748                      XMM12, XMM12b,
 749                      XMM13, XMM13b,
 750                      XMM14, XMM14b,
 751                      XMM15, XMM15b
 752 #endif
 753                      );
 754 
 755 // Class for evex double registers
 756 reg_class double_reg_evex(XMM0,  XMM0b,
 757                      XMM1,  XMM1b,
 758                      XMM2,  XMM2b,
 759                      XMM3,  XMM3b,
 760                      XMM4,  XMM4b,
 761                      XMM5,  XMM5b,
 762                      XMM6,  XMM6b,
 763                      XMM7,  XMM7b
 764 #ifdef _LP64
 765                     ,XMM8,  XMM8b,
 766                      XMM9,  XMM9b,
 767                      XMM10, XMM10b,
 768                      XMM11, XMM11b,
 769                      XMM12, XMM12b,
 770                      XMM13, XMM13b,
 771                      XMM14, XMM14b,
 772                      XMM15, XMM15b,
 773                      XMM16, XMM16b,
 774                      XMM17, XMM17b,
 775                      XMM18, XMM18b,
 776                      XMM19, XMM19b,
 777                      XMM20, XMM20b,
 778                      XMM21, XMM21b,
 779                      XMM22, XMM22b,
 780                      XMM23, XMM23b,
 781                      XMM24, XMM24b,
 782                      XMM25, XMM25b,
 783                      XMM26, XMM26b,
 784                      XMM27, XMM27b,
 785                      XMM28, XMM28b,
 786                      XMM29, XMM29b,
 787                      XMM30, XMM30b,
 788                      XMM31, XMM31b
 789 #endif
 790                      );
 791 
 792 reg_class_dynamic double_reg(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() %} );
 793 reg_class_dynamic double_reg_vl(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 794 
 795 // Class for pre evex 32bit vector registers
 796 reg_class vectors_reg_legacy(XMM0,
 797                       XMM1,
 798                       XMM2,
 799                       XMM3,
 800                       XMM4,
 801                       XMM5,
 802                       XMM6,
 803                       XMM7
 804 #ifdef _LP64
 805                      ,XMM8,
 806                       XMM9,
 807                       XMM10,
 808                       XMM11,
 809                       XMM12,
 810                       XMM13,
 811                       XMM14,
 812                       XMM15
 813 #endif
 814                       );
 815 
 816 // Class for evex 32bit vector registers
 817 reg_class vectors_reg_evex(XMM0,
 818                       XMM1,
 819                       XMM2,
 820                       XMM3,
 821                       XMM4,
 822                       XMM5,
 823                       XMM6,
 824                       XMM7
 825 #ifdef _LP64
 826                      ,XMM8,
 827                       XMM9,
 828                       XMM10,
 829                       XMM11,
 830                       XMM12,
 831                       XMM13,
 832                       XMM14,
 833                       XMM15,
 834                       XMM16,
 835                       XMM17,
 836                       XMM18,
 837                       XMM19,
 838                       XMM20,
 839                       XMM21,
 840                       XMM22,
 841                       XMM23,
 842                       XMM24,
 843                       XMM25,
 844                       XMM26,
 845                       XMM27,
 846                       XMM28,
 847                       XMM29,
 848                       XMM30,
 849                       XMM31
 850 #endif
 851                       );
 852 
 853 reg_class_dynamic vectors_reg(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_evex() %} );
 854 reg_class_dynamic vectors_reg_vlbwdq(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 855 
 856 // Class for all 64bit vector registers
 857 reg_class vectord_reg_legacy(XMM0,  XMM0b,
 858                       XMM1,  XMM1b,
 859                       XMM2,  XMM2b,
 860                       XMM3,  XMM3b,
 861                       XMM4,  XMM4b,
 862                       XMM5,  XMM5b,
 863                       XMM6,  XMM6b,
 864                       XMM7,  XMM7b
 865 #ifdef _LP64
 866                      ,XMM8,  XMM8b,
 867                       XMM9,  XMM9b,
 868                       XMM10, XMM10b,
 869                       XMM11, XMM11b,
 870                       XMM12, XMM12b,
 871                       XMM13, XMM13b,
 872                       XMM14, XMM14b,
 873                       XMM15, XMM15b
 874 #endif
 875                       );
 876 
 877 // Class for all 64bit vector registers
 878 reg_class vectord_reg_evex(XMM0,  XMM0b,
 879                       XMM1,  XMM1b,
 880                       XMM2,  XMM2b,
 881                       XMM3,  XMM3b,
 882                       XMM4,  XMM4b,
 883                       XMM5,  XMM5b,
 884                       XMM6,  XMM6b,
 885                       XMM7,  XMM7b
 886 #ifdef _LP64
 887                      ,XMM8,  XMM8b,
 888                       XMM9,  XMM9b,
 889                       XMM10, XMM10b,
 890                       XMM11, XMM11b,
 891                       XMM12, XMM12b,
 892                       XMM13, XMM13b,
 893                       XMM14, XMM14b,
 894                       XMM15, XMM15b,
 895                       XMM16, XMM16b,
 896                       XMM17, XMM17b,
 897                       XMM18, XMM18b,
 898                       XMM19, XMM19b,
 899                       XMM20, XMM20b,
 900                       XMM21, XMM21b,
 901                       XMM22, XMM22b,
 902                       XMM23, XMM23b,
 903                       XMM24, XMM24b,
 904                       XMM25, XMM25b,
 905                       XMM26, XMM26b,
 906                       XMM27, XMM27b,
 907                       XMM28, XMM28b,
 908                       XMM29, XMM29b,
 909                       XMM30, XMM30b,
 910                       XMM31, XMM31b
 911 #endif
 912                       );
 913 
 914 reg_class_dynamic vectord_reg(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_evex() %} );
 915 reg_class_dynamic vectord_reg_vlbwdq(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 916 
 917 // Class for all 128bit vector registers
 918 reg_class vectorx_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,
 919                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 920                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 921                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 922                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 923                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 924                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 925                       XMM7,  XMM7b,  XMM7c,  XMM7d
 926 #ifdef _LP64
 927                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 928                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 929                       XMM10, XMM10b, XMM10c, XMM10d,
 930                       XMM11, XMM11b, XMM11c, XMM11d,
 931                       XMM12, XMM12b, XMM12c, XMM12d,
 932                       XMM13, XMM13b, XMM13c, XMM13d,
 933                       XMM14, XMM14b, XMM14c, XMM14d,
 934                       XMM15, XMM15b, XMM15c, XMM15d
 935 #endif
 936                       );
 937 
 938 // Class for all 128bit vector registers
 939 reg_class vectorx_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,
 940                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 941                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 942                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 943                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 944                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 945                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 946                       XMM7,  XMM7b,  XMM7c,  XMM7d
 947 #ifdef _LP64
 948                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 949                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 950                       XMM10, XMM10b, XMM10c, XMM10d,
 951                       XMM11, XMM11b, XMM11c, XMM11d,
 952                       XMM12, XMM12b, XMM12c, XMM12d,
 953                       XMM13, XMM13b, XMM13c, XMM13d,
 954                       XMM14, XMM14b, XMM14c, XMM14d,
 955                       XMM15, XMM15b, XMM15c, XMM15d,
 956                       XMM16, XMM16b, XMM16c, XMM16d,
 957                       XMM17, XMM17b, XMM17c, XMM17d,
 958                       XMM18, XMM18b, XMM18c, XMM18d,
 959                       XMM19, XMM19b, XMM19c, XMM19d,
 960                       XMM20, XMM20b, XMM20c, XMM20d,
 961                       XMM21, XMM21b, XMM21c, XMM21d,
 962                       XMM22, XMM22b, XMM22c, XMM22d,
 963                       XMM23, XMM23b, XMM23c, XMM23d,
 964                       XMM24, XMM24b, XMM24c, XMM24d,
 965                       XMM25, XMM25b, XMM25c, XMM25d,
 966                       XMM26, XMM26b, XMM26c, XMM26d,
 967                       XMM27, XMM27b, XMM27c, XMM27d,
 968                       XMM28, XMM28b, XMM28c, XMM28d,
 969                       XMM29, XMM29b, XMM29c, XMM29d,
 970                       XMM30, XMM30b, XMM30c, XMM30d,
 971                       XMM31, XMM31b, XMM31c, XMM31d
 972 #endif
 973                       );
 974 
 975 reg_class_dynamic vectorx_reg(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_evex() %} );
 976 reg_class_dynamic vectorx_reg_vlbwdq(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 977 
 978 // Class for all 256bit vector registers
 979 reg_class vectory_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
 980                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
 981                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
 982                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
 983                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
 984                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
 985                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
 986                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
 987 #ifdef _LP64
 988                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
 989                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
 990                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
 991                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
 992                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
 993                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
 994                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
 995                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h
 996 #endif
 997                       );
 998 
 999 // Class for all 256bit vector registers
1000 reg_class vectory_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
1001                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
1002                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
1003                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
1004                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
1005                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
1006                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
1007                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
1008 #ifdef _LP64
1009                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
1010                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
1011                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
1012                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
1013                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
1014                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
1015                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
1016                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h,
1017                       XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h,
1018                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h,
1019                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h,
1020                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h,
1021                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h,
1022                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h,
1023                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h,
1024                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h,
1025                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h,
1026                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h,
1027                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h,
1028                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h,
1029                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h,
1030                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h,
1031                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h,
1032                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h
1033 #endif
1034                       );
1035 
1036 reg_class_dynamic vectory_reg(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_evex() %} );
1037 reg_class_dynamic vectory_reg_vlbwdq(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
1038 
1039 // Class for all 512bit vector registers
1040 reg_class vectorz_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1041                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1042                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1043                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1044                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1045                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1046                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1047                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1048 #ifdef _LP64
1049                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1050                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1051                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1052                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1053                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1054                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1055                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1056                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1057                      ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
1058                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
1059                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
1060                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
1061                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
1062                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
1063                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
1064                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
1065                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
1066                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
1067                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
1068                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
1069                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
1070                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
1071                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
1072                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
1073 #endif
1074                       );
1075 
1076 // Class for restricted 512bit vector registers
1077 reg_class vectorz_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1078                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1079                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1080                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1081                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1082                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1083                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1084                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1085 #ifdef _LP64
1086                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1087                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1088                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1089                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1090                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1091                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1092                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1093                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1094 #endif
1095                       );
1096 
1097 reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
1098 reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
1099 
1100 %}
1101 
1102 
1103 //----------SOURCE BLOCK-------------------------------------------------------
1104 // This is a block of C++ code which provides values, functions, and
1105 // definitions necessary in the rest of the architecture description
1106 
1107 source_hpp %{
1108 // Header information of the source block.
1109 // Method declarations/definitions which are used outside
1110 // the ad-scope can conveniently be defined here.
1111 //
1112 // To keep related declarations/definitions/uses close together,
1113 // we switch between source %{ }% and source_hpp %{ }% freely as needed.
1114 
1115 class NativeJump;
1116 
1117 class CallStubImpl {
1118 
1119   //--------------------------------------------------------------
1120   //---&lt;  Used for optimization in Compile::shorten_branches  &gt;---
1121   //--------------------------------------------------------------
1122 
1123  public:
1124   // Size of call trampoline stub.
1125   static uint size_call_trampoline() {
1126     return 0; // no call trampolines on this platform
1127   }
1128 
1129   // number of relocations needed by a call trampoline stub
1130   static uint reloc_call_trampoline() {
1131     return 0; // no call trampolines on this platform
1132   }
1133 };
1134 
1135 class HandlerImpl {
1136 
1137  public:
1138 
1139   static int emit_exception_handler(CodeBuffer &amp;cbuf);
1140   static int emit_deopt_handler(CodeBuffer&amp; cbuf);
1141 
1142   static uint size_exception_handler() {
1143     // NativeCall instruction size is the same as NativeJump.
1144     // exception handler starts out as jump and can be patched to
1145     // a call be deoptimization.  (4932387)
1146     // Note that this value is also credited (in output.cpp) to
1147     // the size of the code section.
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
<a name="1" id="anc1"></a>







1168 %} // end source_hpp
1169 
1170 source %{
1171 
1172 #include &quot;opto/addnode.hpp&quot;
<a name="2" id="anc2"></a>































1173 
1174 // Emit exception handler code.
1175 // Stuff framesize into a register and call a VM stub routine.
1176 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1177 
1178   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1179   // That&#39;s why we must use the macroassembler to generate a handler.
<a name="3" id="anc3"></a><span class="line-modified">1180   MacroAssembler _masm(&amp;cbuf);</span>
1181   address base = __ start_a_stub(size_exception_handler());
1182   if (base == NULL) {
1183     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1184     return 0;  // CodeBuffer::expand failed
1185   }
1186   int offset = __ offset();
1187   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1188   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1189   __ end_a_stub();
1190   return offset;
1191 }
1192 
1193 // Emit deopt handler code.
1194 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1195 
1196   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1197   // That&#39;s why we must use the macroassembler to generate a handler.
<a name="4" id="anc4"></a><span class="line-modified">1198   MacroAssembler _masm(&amp;cbuf);</span>
1199   address base = __ start_a_stub(size_deopt_handler());
1200   if (base == NULL) {
1201     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1202     return 0;  // CodeBuffer::expand failed
1203   }
1204   int offset = __ offset();
1205 
1206 #ifdef _LP64
1207   address the_pc = (address) __ pc();
1208   Label next;
1209   // push a &quot;the_pc&quot; on the stack without destroying any registers
1210   // as they all may be live.
1211 
1212   // push address of &quot;next&quot;
1213   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1214   __ bind(next);
1215   // adjust it so it matches &quot;the_pc&quot;
1216   __ subptr(Address(rsp, 0), __ offset() - offset);
1217 #else
1218   InternalAddress here(__ pc());
1219   __ pushptr(here.addr());
1220 #endif
1221 
1222   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
1223   assert(__ offset() - offset &lt;= (int) size_deopt_handler(), &quot;overflow %d&quot;, (__ offset() - offset));
1224   __ end_a_stub();
1225   return offset;
1226 }
1227 
1228 
1229 //=============================================================================
1230 
1231   // Float masks come from different places depending on platform.
1232 #ifdef _LP64
1233   static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }
1234   static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }
1235   static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }
1236   static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }
1237 #else
1238   static address float_signmask()  { return (address)float_signmask_pool; }
1239   static address float_signflip()  { return (address)float_signflip_pool; }
1240   static address double_signmask() { return (address)double_signmask_pool; }
1241   static address double_signflip() { return (address)double_signflip_pool; }
1242 #endif
1243   static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
1244   static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<a name="5" id="anc5"></a><span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
<a name="6" id="anc6"></a><span class="line-removed">1280     case Op_AddReductionVL:</span>
<span class="line-removed">1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here</span>
<span class="line-removed">1282         return false;</span>
<span class="line-removed">1283       }</span>
<span class="line-removed">1284       break;</span>
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
<a name="7" id="anc7"></a><span class="line-modified">1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3</span>



1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
<a name="8" id="anc8"></a><span class="line-removed">1298     case Op_AddReductionVF:</span>
<span class="line-removed">1299     case Op_AddReductionVD:</span>
<span class="line-removed">1300     case Op_MulReductionVF:</span>
<span class="line-removed">1301     case Op_MulReductionVD:</span>
<span class="line-removed">1302       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-removed">1303         return false;</span>
<span class="line-removed">1304       }</span>
<span class="line-removed">1305       break;</span>
1306     case Op_SqrtVD:
1307     case Op_SqrtVF:
1308       if (UseAVX &lt; 1) { // enabled for AVX only
1309         return false;
1310       }
1311       break;
1312     case Op_CompareAndSwapL:
1313 #ifdef _LP64
1314     case Op_CompareAndSwapP:
1315 #endif
1316       if (!VM_Version::supports_cx8()) {
1317         return false;
1318       }
1319       break;
1320     case Op_CMoveVF:
1321     case Op_CMoveVD:
1322       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1323         return false;
1324       }
1325       break;
1326     case Op_StrIndexOf:
1327       if (!UseSSE42Intrinsics) {
1328         return false;
1329       }
1330       break;
1331     case Op_StrIndexOfChar:
1332       if (!UseSSE42Intrinsics) {
1333         return false;
1334       }
1335       break;
1336     case Op_OnSpinWait:
1337       if (VM_Version::supports_on_spin_wait() == false) {
1338         return false;
1339       }
1340       break;
<a name="9" id="anc9"></a><span class="line-removed">1341     case Op_MulAddVS2VI:</span>
<span class="line-removed">1342     case Op_RShiftVL:</span>
<span class="line-removed">1343     case Op_AbsVD:</span>
<span class="line-removed">1344     case Op_NegVD:</span>
<span class="line-removed">1345       if (UseSSE &lt; 2) {</span>
<span class="line-removed">1346         return false;</span>
<span class="line-removed">1347       }</span>
<span class="line-removed">1348       break;</span>
1349     case Op_MulVB:
1350     case Op_LShiftVB:
1351     case Op_RShiftVB:
1352     case Op_URShiftVB:
1353       if (UseSSE &lt; 4) {
1354         return false;
1355       }
1356       break;
1357 #ifdef _LP64
1358     case Op_MaxD:
1359     case Op_MaxF:
1360     case Op_MinD:
1361     case Op_MinF:
1362       if (UseAVX &lt; 1) { // enabled for AVX only
1363         return false;
1364       }
1365       break;
1366 #endif
1367     case Op_CacheWB:
1368     case Op_CacheWBPreSync:
1369     case Op_CacheWBPostSync:
1370       if (!VM_Version::supports_data_cache_line_flush()) {
1371         return false;
1372       }
1373       break;
1374     case Op_RoundDoubleMode:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379     case Op_RoundDoubleModeV:
1380       if (VM_Version::supports_avx() == false) {
1381         return false; // 128bit vroundpd is not available
1382       }
1383       break;
<a name="10" id="anc10"></a>






















1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
1404   if (!vector_size_supported(bt, vlen)) {
1405     return false;
1406   }
1407   // Special cases which require vector length follow:
1408   //   * implementation limitations
1409   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1410   //   * 128bit vroundpd instruction is present only in AVX1
<a name="11" id="anc11"></a>
1411   switch (opcode) {
1412     case Op_AbsVF:
1413     case Op_NegVF:
1414       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1415         return false; // 512bit vandps and vxorps are not available
1416       }
1417       break;
1418     case Op_AbsVD:
1419     case Op_NegVD:
1420       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1421         return false; // 512bit vandpd and vxorpd are not available
1422       }
1423       break;
1424     case Op_CMoveVF:
1425       if (vlen != 8) {
1426         return false; // implementation limitation (only vcmov8F_reg is present)
1427       }
1428       break;
<a name="12" id="anc12"></a>





1429     case Op_CMoveVD:
1430       if (vlen != 4) {
1431         return false; // implementation limitation (only vcmov4D_reg is present)
1432       }
1433       break;
1434   }
1435   return true;  // Per default match rules are supported.
1436 }
1437 
1438 // x86 supports generic vector operands: vec and legVec.
1439 const bool Matcher::supports_generic_vector_operands = true;
1440 
1441 MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1442   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1443   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1444   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1445       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1446     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1447     return new legVecZOper();
1448   }
1449   if (legacy) {
1450     switch (ideal_reg) {
1451       case Op_VecS: return new legVecSOper();
1452       case Op_VecD: return new legVecDOper();
1453       case Op_VecX: return new legVecXOper();
1454       case Op_VecY: return new legVecYOper();
1455       case Op_VecZ: return new legVecZOper();
1456     }
1457   } else {
1458     switch (ideal_reg) {
1459       case Op_VecS: return new vecSOper();
1460       case Op_VecD: return new vecDOper();
1461       case Op_VecX: return new vecXOper();
1462       case Op_VecY: return new vecYOper();
1463       case Op_VecZ: return new vecZOper();
1464     }
1465   }
1466   ShouldNotReachHere();
1467   return NULL;
1468 }
1469 
1470 bool Matcher::is_generic_reg2reg_move(MachNode* m) {
1471   switch (m-&gt;rule()) {
1472     case MoveVec2Leg_rule:
1473     case MoveLeg2Vec_rule:
1474       return true;
1475     default:
1476       return false;
1477   }
1478 }
1479 
1480 bool Matcher::is_generic_vector(MachOper* opnd) {
1481   switch (opnd-&gt;opcode()) {
1482     case VEC:
1483     case LEGVEC:
1484       return true;
1485     default:
1486       return false;
1487   }
1488 }
1489 
1490 //------------------------------------------------------------------------
1491 
1492 const bool Matcher::has_predicated_vectors(void) {
1493   bool ret_value = false;
1494   if (UseAVX &gt; 2) {
1495     ret_value = VM_Version::supports_avx512vl();
1496   }
1497 
1498   return ret_value;
1499 }
1500 
1501 const int Matcher::float_pressure(int default_pressure_threshold) {
1502   int float_pressure_threshold = default_pressure_threshold;
1503 #ifdef _LP64
1504   if (UseAVX &gt; 2) {
1505     // Increase pressure threshold on machines with AVX3 which have
1506     // 2x more XMM registers.
1507     float_pressure_threshold = default_pressure_threshold * 2;
1508   }
1509 #endif
1510   return float_pressure_threshold;
1511 }
1512 
1513 // Max vector size in bytes. 0 if not supported.
1514 const int Matcher::vector_width_in_bytes(BasicType bt) {
1515   assert(is_java_primitive(bt), &quot;only primitive type vectors&quot;);
1516   if (UseSSE &lt; 2) return 0;
1517   // SSE2 supports 128bit vectors for all types.
1518   // AVX2 supports 256bit vectors for all types.
1519   // AVX2/EVEX supports 512bit vectors for all types.
1520   int size = (UseAVX &gt; 1) ? (1 &lt;&lt; UseAVX) * 8 : 16;
1521   // AVX1 supports 256bit vectors only for FLOAT and DOUBLE.
1522   if (UseAVX &gt; 0 &amp;&amp; (bt == T_FLOAT || bt == T_DOUBLE))
1523     size = (UseAVX &gt; 2) ? 64 : 32;
1524   if (UseAVX &gt; 2 &amp;&amp; (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))
1525     size = (VM_Version::supports_avx512bw()) ? 64 : 32;
1526   // Use flag to limit vector size.
1527   size = MIN2(size,(int)MaxVectorSize);
1528   // Minimum 2 values in vector (or 4 for bytes).
1529   switch (bt) {
1530   case T_DOUBLE:
1531   case T_LONG:
1532     if (size &lt; 16) return 0;
1533     break;
1534   case T_FLOAT:
1535   case T_INT:
1536     if (size &lt; 8) return 0;
1537     break;
1538   case T_BOOLEAN:
1539     if (size &lt; 4) return 0;
1540     break;
1541   case T_CHAR:
1542     if (size &lt; 4) return 0;
1543     break;
1544   case T_BYTE:
1545     if (size &lt; 4) return 0;
1546     break;
1547   case T_SHORT:
1548     if (size &lt; 4) return 0;
1549     break;
1550   default:
1551     ShouldNotReachHere();
1552   }
1553   return size;
1554 }
1555 
1556 // Limits on vector size (number of elements) loaded into vector.
1557 const int Matcher::max_vector_size(const BasicType bt) {
1558   return vector_width_in_bytes(bt)/type2aelembytes(bt);
1559 }
1560 const int Matcher::min_vector_size(const BasicType bt) {
1561   int max_size = max_vector_size(bt);
1562   // Min size which can be loaded into vector is 4 bytes.
1563   int size = (type2aelembytes(bt) == 1) ? 4 : 2;
1564   return MIN2(size,max_size);
1565 }
1566 
1567 // Vector ideal reg corresponding to specified size in bytes
1568 const uint Matcher::vector_ideal_reg(int size) {
1569   assert(MaxVectorSize &gt;= size, &quot;&quot;);
1570   switch(size) {
1571     case  4: return Op_VecS;
1572     case  8: return Op_VecD;
1573     case 16: return Op_VecX;
1574     case 32: return Op_VecY;
1575     case 64: return Op_VecZ;
1576   }
1577   ShouldNotReachHere();
1578   return 0;
1579 }
1580 
1581 // Only lowest bits of xmm reg are used for vector shift count.
1582 const uint Matcher::vector_shift_count_ideal_reg(int size) {
1583   return Op_VecS;
1584 }
1585 
1586 // x86 supports misaligned vectors store/load.
1587 const bool Matcher::misaligned_vectors_ok() {
1588   return true;
1589 }
1590 
1591 // x86 AES instructions are compatible with SunJCE expanded
1592 // keys, hence we do not need to pass the original key to stubs
1593 const bool Matcher::pass_original_key_for_aes() {
1594   return false;
1595 }
1596 
1597 
1598 const bool Matcher::convi2l_type_required = true;
1599 
1600 // Check for shift by small constant as well
1601 static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1602   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
1603       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
1604       // Are there other uses besides address expressions?
1605       !matcher-&gt;is_visited(shift)) {
1606     address_visited.set(shift-&gt;_idx); // Flag as address_visited
1607     mstack.push(shift-&gt;in(2), Matcher::Visit);
1608     Node *conv = shift-&gt;in(1);
1609 #ifdef _LP64
1610     // Allow Matcher to match the rule which bypass
1611     // ConvI2L operation for an array index on LP64
1612     // if the index value is positive.
1613     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1614         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1615         // Are there other uses besides address expressions?
1616         !matcher-&gt;is_visited(conv)) {
1617       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1618       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1619     } else
1620 #endif
1621       mstack.push(conv, Matcher::Pre_Visit);
1622     return true;
1623   }
1624   return false;
1625 }
1626 
<a name="13" id="anc13"></a>












































































































1627 // Should the Matcher clone shifts on addressing modes, expecting them
1628 // to be subsumed into complex addressing expressions or compute them
1629 // into registers?
<a name="14" id="anc14"></a><span class="line-modified">1630 bool Matcher::clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {</span>
1631   Node *off = m-&gt;in(AddPNode::Offset);
1632   if (off-&gt;is_Con()) {
1633     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1634     Node *adr = m-&gt;in(AddPNode::Address);
1635 
1636     // Intel can handle 2 adds in addressing mode
1637     // AtomicAdd is not an addressing expression.
1638     // Cheap to find it by looking for screwy base.
1639     if (adr-&gt;is_AddP() &amp;&amp;
1640         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1641         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1642         // Are there other uses besides address expressions?
1643         !is_visited(adr)) {
1644       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1645       Node *shift = adr-&gt;in(AddPNode::Offset);
1646       if (!clone_shift(shift, this, mstack, address_visited)) {
1647         mstack.push(shift, Pre_Visit);
1648       }
1649       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1650       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
1651     } else {
1652       mstack.push(adr, Pre_Visit);
1653     }
1654 
1655     // Clone X+offset as it also folds into most addressing expressions
1656     mstack.push(off, Visit);
1657     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1658     return true;
1659   } else if (clone_shift(off, this, mstack, address_visited)) {
1660     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1661     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1662     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1663     return true;
1664   }
1665   return false;
1666 }
1667 
1668 void Compile::reshape_address(AddPNode* addp) {
1669 }
1670 
1671 static inline uint vector_length(const MachNode* n) {
1672   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1673   return vt-&gt;length();
1674 }
1675 
<a name="15" id="anc15"></a>





1676 static inline uint vector_length_in_bytes(const MachNode* n) {
1677   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1678   return vt-&gt;length_in_bytes();
1679 }
1680 
1681 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1682   uint def_idx = use-&gt;operand_index(opnd);
1683   Node* def = use-&gt;in(def_idx);
1684   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1685 }
1686 
1687 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1688   switch(vector_length_in_bytes(n)) {
1689     case  4: // fall-through
1690     case  8: // fall-through
1691     case 16: return Assembler::AVX_128bit;
1692     case 32: return Assembler::AVX_256bit;
1693     case 64: return Assembler::AVX_512bit;
1694 
1695     default: {
1696       ShouldNotReachHere();
1697       return Assembler::AVX_NoVec;
1698     }
1699   }
1700 }
1701 
1702 // Helper methods for MachSpillCopyNode::implementation().
1703 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1704                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1705   // In 64-bit VM size calculation is very complex. Emitting instructions
1706   // into scratch buffer is used to get size in 64-bit VM.
1707   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1708   assert(ireg == Op_VecS || // 32bit vector
1709          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1710          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1711          &quot;no non-adjacent vector moves&quot; );
1712   if (cbuf) {
<a name="16" id="anc16"></a><span class="line-modified">1713     MacroAssembler _masm(cbuf);</span>
1714     int offset = __ offset();
1715     switch (ireg) {
1716     case Op_VecS: // copy whole register
1717     case Op_VecD:
1718     case Op_VecX:
1719 #ifndef _LP64
1720       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1721 #else
1722       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1723         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1724       } else {
1725         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1726      }
1727 #endif
1728       break;
1729     case Op_VecY:
1730 #ifndef _LP64
1731       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1732 #else
1733       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1734         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1735       } else {
1736         __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1737      }
1738 #endif
1739       break;
1740     case Op_VecZ:
1741       __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);
1742       break;
1743     default:
1744       ShouldNotReachHere();
1745     }
1746     int size = __ offset() - offset;
1747 #ifdef ASSERT
1748     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1749     assert(!do_size || size == 4, &quot;incorrect size calculattion&quot;);
1750 #endif
1751     return size;
1752 #ifndef PRODUCT
1753   } else if (!do_size) {
1754     switch (ireg) {
1755     case Op_VecS:
1756     case Op_VecD:
1757     case Op_VecX:
1758       st-&gt;print(&quot;movdqu  %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1759       break;
1760     case Op_VecY:
1761     case Op_VecZ:
1762       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1763       break;
1764     default:
1765       ShouldNotReachHere();
1766     }
1767 #endif
1768   }
1769   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1770   return (UseAVX &gt; 2) ? 6 : 4;
1771 }
1772 
1773 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1774                      int stack_offset, int reg, uint ireg, outputStream* st) {
1775   // In 64-bit VM size calculation is very complex. Emitting instructions
1776   // into scratch buffer is used to get size in 64-bit VM.
1777   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1778   if (cbuf) {
<a name="17" id="anc17"></a><span class="line-modified">1779     MacroAssembler _masm(cbuf);</span>
1780     int offset = __ offset();
1781     if (is_load) {
1782       switch (ireg) {
1783       case Op_VecS:
1784         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1785         break;
1786       case Op_VecD:
1787         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1788         break;
1789       case Op_VecX:
1790 #ifndef _LP64
1791         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1792 #else
1793         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1794           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1795         } else {
1796           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1797           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1798         }
1799 #endif
1800         break;
1801       case Op_VecY:
1802 #ifndef _LP64
1803         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1804 #else
1805         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1806           __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1807         } else {
1808           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1809           __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1810         }
1811 #endif
1812         break;
1813       case Op_VecZ:
1814         __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);
1815         break;
1816       default:
1817         ShouldNotReachHere();
1818       }
1819     } else { // store
1820       switch (ireg) {
1821       case Op_VecS:
1822         __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1823         break;
1824       case Op_VecD:
1825         __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1826         break;
1827       case Op_VecX:
1828 #ifndef _LP64
1829         __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1830 #else
1831         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1832           __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1833         }
1834         else {
1835           __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1836         }
1837 #endif
1838         break;
1839       case Op_VecY:
1840 #ifndef _LP64
1841         __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1842 #else
1843         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1844           __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1845         }
1846         else {
1847           __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1848         }
1849 #endif
1850         break;
1851       case Op_VecZ:
1852         __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1853         break;
1854       default:
1855         ShouldNotReachHere();
1856       }
1857     }
1858     int size = __ offset() - offset;
1859 #ifdef ASSERT
1860     int offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt; 0x80) ? 1 : (UseAVX &gt; 2) ? 6 : 4);
1861     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1862     assert(!do_size || size == (5+offset_size), &quot;incorrect size calculattion&quot;);
1863 #endif
1864     return size;
1865 #ifndef PRODUCT
1866   } else if (!do_size) {
1867     if (is_load) {
1868       switch (ireg) {
1869       case Op_VecS:
1870         st-&gt;print(&quot;movd    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1871         break;
1872       case Op_VecD:
1873         st-&gt;print(&quot;movq    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1874         break;
1875        case Op_VecX:
1876         st-&gt;print(&quot;movdqu  %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1877         break;
1878       case Op_VecY:
1879       case Op_VecZ:
1880         st-&gt;print(&quot;vmovdqu %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1881         break;
1882       default:
1883         ShouldNotReachHere();
1884       }
1885     } else { // store
1886       switch (ireg) {
1887       case Op_VecS:
1888         st-&gt;print(&quot;movd    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1889         break;
1890       case Op_VecD:
1891         st-&gt;print(&quot;movq    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1892         break;
1893        case Op_VecX:
1894         st-&gt;print(&quot;movdqu  [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1895         break;
1896       case Op_VecY:
1897       case Op_VecZ:
1898         st-&gt;print(&quot;vmovdqu [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1899         break;
1900       default:
1901         ShouldNotReachHere();
1902       }
1903     }
1904 #endif
1905   }
1906   bool is_single_byte = false;
1907   int vec_len = 0;
1908   if ((UseAVX &gt; 2) &amp;&amp; (stack_offset != 0)) {
1909     int tuple_type = Assembler::EVEX_FVM;
1910     int input_size = Assembler::EVEX_32bit;
1911     switch (ireg) {
1912     case Op_VecS:
1913       tuple_type = Assembler::EVEX_T1S;
1914       break;
1915     case Op_VecD:
1916       tuple_type = Assembler::EVEX_T1S;
1917       input_size = Assembler::EVEX_64bit;
1918       break;
1919     case Op_VecX:
1920       break;
1921     case Op_VecY:
1922       vec_len = 1;
1923       break;
1924     case Op_VecZ:
1925       vec_len = 2;
1926       break;
1927     }
1928     is_single_byte = Assembler::query_compressed_disp_byte(stack_offset, true, vec_len, tuple_type, input_size, 0);
1929   }
1930   int offset_size = 0;
1931   int size = 5;
1932   if (UseAVX &gt; 2 ) {
1933     if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len == 2)) {
1934       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
1935       size += 2; // Need an additional two bytes for EVEX encoding
1936     } else if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len &lt; 2)) {
1937       offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
1938     } else {
1939       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
1940       size += 2; // Need an additional two bytes for EVEX encodding
1941     }
1942   } else {
1943     offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
1944   }
1945   // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1946   return size+offset_size;
1947 }
1948 
1949 static inline jint replicate4_imm(int con, int width) {
1950   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 32bit.
1951   assert(width == 1 || width == 2, &quot;only byte or short types here&quot;);
1952   int bit_width = width * 8;
1953   jint val = con;
1954   val &amp;= (1 &lt;&lt; bit_width) - 1;  // mask off sign bits
1955   while(bit_width &lt; 32) {
1956     val |= (val &lt;&lt; bit_width);
1957     bit_width &lt;&lt;= 1;
1958   }
1959   return val;
1960 }
1961 
1962 static inline jlong replicate8_imm(int con, int width) {
1963   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
1964   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
1965   int bit_width = width * 8;
1966   jlong val = con;
1967   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
1968   while(bit_width &lt; 64) {
1969     val |= (val &lt;&lt; bit_width);
1970     bit_width &lt;&lt;= 1;
1971   }
1972   return val;
1973 }
1974 
1975 #ifndef PRODUCT
1976   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
1977     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
1978   }
1979 #endif
1980 
1981   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
<a name="18" id="anc18"></a><span class="line-modified">1982     MacroAssembler _masm(&amp;cbuf);</span>
1983     __ nop(_count);
1984   }
1985 
1986   uint MachNopNode::size(PhaseRegAlloc*) const {
1987     return _count;
1988   }
1989 
1990 #ifndef PRODUCT
1991   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
1992     st-&gt;print(&quot;# breakpoint&quot;);
1993   }
1994 #endif
1995 
1996   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
<a name="19" id="anc19"></a><span class="line-modified">1997     MacroAssembler _masm(&amp;cbuf);</span>
1998     __ int3();
1999   }
2000 
2001   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2002     return MachNode::size(ra_);
2003   }
2004 
2005 %}
2006 
2007 encode %{
2008 
2009   enc_class call_epilog %{
2010     if (VerifyStackAtCalls) {
2011       // Check that stack depth is unchanged: find majik cookie on stack
2012       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
<a name="20" id="anc20"></a><span class="line-modified">2013       MacroAssembler _masm(&amp;cbuf);</span>
2014       Label L;
2015       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2016       __ jccb(Assembler::equal, L);
2017       // Die if stack mismatch
2018       __ int3();
2019       __ bind(L);
2020     }
2021   %}
2022 
2023 %}
2024 
2025 
2026 //----------OPERANDS-----------------------------------------------------------
2027 // Operand definitions must precede instruction definitions for correct parsing
2028 // in the ADLC because operands constitute user defined types which are used in
2029 // instruction definitions.
2030 
2031 // Vectors
2032 
2033 // Dummy generic vector class. Should be used for all vector operands.
2034 // Replaced with vec[SDXYZ] during post-selection pass.
2035 operand vec() %{
2036   constraint(ALLOC_IN_RC(dynamic));
2037   match(VecX);
2038   match(VecY);
2039   match(VecZ);
2040   match(VecS);
2041   match(VecD);
2042 
2043   format %{ %}
2044   interface(REG_INTER);
2045 %}
2046 
2047 // Dummy generic legacy vector class. Should be used for all legacy vector operands.
2048 // Replaced with legVec[SDXYZ] during post-selection cleanup.
2049 // Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
2050 // runtime code generation via reg_class_dynamic.
2051 operand legVec() %{
2052   constraint(ALLOC_IN_RC(dynamic));
2053   match(VecX);
2054   match(VecY);
2055   match(VecZ);
2056   match(VecS);
2057   match(VecD);
2058 
2059   format %{ %}
2060   interface(REG_INTER);
2061 %}
2062 
2063 // Replaces vec during post-selection cleanup. See above.
2064 operand vecS() %{
2065   constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
2066   match(VecS);
2067 
2068   format %{ %}
2069   interface(REG_INTER);
2070 %}
2071 
2072 // Replaces legVec during post-selection cleanup. See above.
2073 operand legVecS() %{
2074   constraint(ALLOC_IN_RC(vectors_reg_legacy));
2075   match(VecS);
2076 
2077   format %{ %}
2078   interface(REG_INTER);
2079 %}
2080 
2081 // Replaces vec during post-selection cleanup. See above.
2082 operand vecD() %{
2083   constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
2084   match(VecD);
2085 
2086   format %{ %}
2087   interface(REG_INTER);
2088 %}
2089 
2090 // Replaces legVec during post-selection cleanup. See above.
2091 operand legVecD() %{
2092   constraint(ALLOC_IN_RC(vectord_reg_legacy));
2093   match(VecD);
2094 
2095   format %{ %}
2096   interface(REG_INTER);
2097 %}
2098 
2099 // Replaces vec during post-selection cleanup. See above.
2100 operand vecX() %{
2101   constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
2102   match(VecX);
2103 
2104   format %{ %}
2105   interface(REG_INTER);
2106 %}
2107 
2108 // Replaces legVec during post-selection cleanup. See above.
2109 operand legVecX() %{
2110   constraint(ALLOC_IN_RC(vectorx_reg_legacy));
2111   match(VecX);
2112 
2113   format %{ %}
2114   interface(REG_INTER);
2115 %}
2116 
2117 // Replaces vec during post-selection cleanup. See above.
2118 operand vecY() %{
2119   constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
2120   match(VecY);
2121 
2122   format %{ %}
2123   interface(REG_INTER);
2124 %}
2125 
2126 // Replaces legVec during post-selection cleanup. See above.
2127 operand legVecY() %{
2128   constraint(ALLOC_IN_RC(vectory_reg_legacy));
2129   match(VecY);
2130 
2131   format %{ %}
2132   interface(REG_INTER);
2133 %}
2134 
2135 // Replaces vec during post-selection cleanup. See above.
2136 operand vecZ() %{
2137   constraint(ALLOC_IN_RC(vectorz_reg));
2138   match(VecZ);
2139 
2140   format %{ %}
2141   interface(REG_INTER);
2142 %}
2143 
2144 // Replaces legVec during post-selection cleanup. See above.
2145 operand legVecZ() %{
2146   constraint(ALLOC_IN_RC(vectorz_reg_legacy));
2147   match(VecZ);
2148 
2149   format %{ %}
2150   interface(REG_INTER);
2151 %}
2152 
2153 // Comparison Code for FP conditional move
2154 operand cmpOp_vcmppd() %{
2155   match(Bool);
2156 
2157   predicate(n-&gt;as_Bool()-&gt;_test._test != BoolTest::overflow &amp;&amp;
2158             n-&gt;as_Bool()-&gt;_test._test != BoolTest::no_overflow);
2159   format %{ &quot;&quot; %}
2160   interface(COND_INTER) %{
2161     equal        (0x0, &quot;eq&quot;);
2162     less         (0x1, &quot;lt&quot;);
2163     less_equal   (0x2, &quot;le&quot;);
2164     not_equal    (0xC, &quot;ne&quot;);
2165     greater_equal(0xD, &quot;ge&quot;);
2166     greater      (0xE, &quot;gt&quot;);
2167     //TODO cannot compile (adlc breaks) without two next lines with error:
2168     // x86_64.ad(13987) Syntax Error: :In operand cmpOp_vcmppd: Do not support this encode constant: &#39; %{
2169     // equal&#39; for overflow.
2170     overflow     (0x20, &quot;o&quot;);  // not really supported by the instruction
2171     no_overflow  (0x21, &quot;no&quot;); // not really supported by the instruction
2172   %}
2173 %}
2174 
2175 
2176 // INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)
2177 
2178 // ============================================================================
2179 
2180 instruct ShouldNotReachHere() %{
2181   match(Halt);
2182   format %{ &quot;ud2\t# ShouldNotReachHere&quot; %}
2183   ins_encode %{
2184     __ stop(_halt_reason);
2185   %}
2186   ins_pipe(pipe_slow);
2187 %}
2188 
2189 // =================================EVEX special===============================
2190 
2191 instruct setMask(rRegI dst, rRegI src) %{
2192   predicate(Matcher::has_predicated_vectors());
2193   match(Set dst (SetVectMaskI  src));
2194   effect(TEMP dst);
2195   format %{ &quot;setvectmask   $dst, $src&quot; %}
2196   ins_encode %{
2197     __ setvectmask($dst$$Register, $src$$Register);
2198   %}
2199   ins_pipe(pipe_slow);
2200 %}
2201 
2202 // ============================================================================
2203 
2204 instruct addF_reg(regF dst, regF src) %{
2205   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2206   match(Set dst (AddF dst src));
2207 
2208   format %{ &quot;addss   $dst, $src&quot; %}
2209   ins_cost(150);
2210   ins_encode %{
2211     __ addss($dst$$XMMRegister, $src$$XMMRegister);
2212   %}
2213   ins_pipe(pipe_slow);
2214 %}
2215 
2216 instruct addF_mem(regF dst, memory src) %{
2217   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2218   match(Set dst (AddF dst (LoadF src)));
2219 
2220   format %{ &quot;addss   $dst, $src&quot; %}
2221   ins_cost(150);
2222   ins_encode %{
2223     __ addss($dst$$XMMRegister, $src$$Address);
2224   %}
2225   ins_pipe(pipe_slow);
2226 %}
2227 
2228 instruct addF_imm(regF dst, immF con) %{
2229   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2230   match(Set dst (AddF dst con));
2231   format %{ &quot;addss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2232   ins_cost(150);
2233   ins_encode %{
2234     __ addss($dst$$XMMRegister, $constantaddress($con));
2235   %}
2236   ins_pipe(pipe_slow);
2237 %}
2238 
2239 instruct addF_reg_reg(regF dst, regF src1, regF src2) %{
2240   predicate(UseAVX &gt; 0);
2241   match(Set dst (AddF src1 src2));
2242 
2243   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2244   ins_cost(150);
2245   ins_encode %{
2246     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2247   %}
2248   ins_pipe(pipe_slow);
2249 %}
2250 
2251 instruct addF_reg_mem(regF dst, regF src1, memory src2) %{
2252   predicate(UseAVX &gt; 0);
2253   match(Set dst (AddF src1 (LoadF src2)));
2254 
2255   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2256   ins_cost(150);
2257   ins_encode %{
2258     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2259   %}
2260   ins_pipe(pipe_slow);
2261 %}
2262 
2263 instruct addF_reg_imm(regF dst, regF src, immF con) %{
2264   predicate(UseAVX &gt; 0);
2265   match(Set dst (AddF src con));
2266 
2267   format %{ &quot;vaddss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2268   ins_cost(150);
2269   ins_encode %{
2270     __ vaddss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2271   %}
2272   ins_pipe(pipe_slow);
2273 %}
2274 
2275 instruct addD_reg(regD dst, regD src) %{
2276   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2277   match(Set dst (AddD dst src));
2278 
2279   format %{ &quot;addsd   $dst, $src&quot; %}
2280   ins_cost(150);
2281   ins_encode %{
2282     __ addsd($dst$$XMMRegister, $src$$XMMRegister);
2283   %}
2284   ins_pipe(pipe_slow);
2285 %}
2286 
2287 instruct addD_mem(regD dst, memory src) %{
2288   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2289   match(Set dst (AddD dst (LoadD src)));
2290 
2291   format %{ &quot;addsd   $dst, $src&quot; %}
2292   ins_cost(150);
2293   ins_encode %{
2294     __ addsd($dst$$XMMRegister, $src$$Address);
2295   %}
2296   ins_pipe(pipe_slow);
2297 %}
2298 
2299 instruct addD_imm(regD dst, immD con) %{
2300   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2301   match(Set dst (AddD dst con));
2302   format %{ &quot;addsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2303   ins_cost(150);
2304   ins_encode %{
2305     __ addsd($dst$$XMMRegister, $constantaddress($con));
2306   %}
2307   ins_pipe(pipe_slow);
2308 %}
2309 
2310 instruct addD_reg_reg(regD dst, regD src1, regD src2) %{
2311   predicate(UseAVX &gt; 0);
2312   match(Set dst (AddD src1 src2));
2313 
2314   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2315   ins_cost(150);
2316   ins_encode %{
2317     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2318   %}
2319   ins_pipe(pipe_slow);
2320 %}
2321 
2322 instruct addD_reg_mem(regD dst, regD src1, memory src2) %{
2323   predicate(UseAVX &gt; 0);
2324   match(Set dst (AddD src1 (LoadD src2)));
2325 
2326   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2327   ins_cost(150);
2328   ins_encode %{
2329     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2330   %}
2331   ins_pipe(pipe_slow);
2332 %}
2333 
2334 instruct addD_reg_imm(regD dst, regD src, immD con) %{
2335   predicate(UseAVX &gt; 0);
2336   match(Set dst (AddD src con));
2337 
2338   format %{ &quot;vaddsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2339   ins_cost(150);
2340   ins_encode %{
2341     __ vaddsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2342   %}
2343   ins_pipe(pipe_slow);
2344 %}
2345 
2346 instruct subF_reg(regF dst, regF src) %{
2347   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2348   match(Set dst (SubF dst src));
2349 
2350   format %{ &quot;subss   $dst, $src&quot; %}
2351   ins_cost(150);
2352   ins_encode %{
2353     __ subss($dst$$XMMRegister, $src$$XMMRegister);
2354   %}
2355   ins_pipe(pipe_slow);
2356 %}
2357 
2358 instruct subF_mem(regF dst, memory src) %{
2359   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2360   match(Set dst (SubF dst (LoadF src)));
2361 
2362   format %{ &quot;subss   $dst, $src&quot; %}
2363   ins_cost(150);
2364   ins_encode %{
2365     __ subss($dst$$XMMRegister, $src$$Address);
2366   %}
2367   ins_pipe(pipe_slow);
2368 %}
2369 
2370 instruct subF_imm(regF dst, immF con) %{
2371   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2372   match(Set dst (SubF dst con));
2373   format %{ &quot;subss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2374   ins_cost(150);
2375   ins_encode %{
2376     __ subss($dst$$XMMRegister, $constantaddress($con));
2377   %}
2378   ins_pipe(pipe_slow);
2379 %}
2380 
2381 instruct subF_reg_reg(regF dst, regF src1, regF src2) %{
2382   predicate(UseAVX &gt; 0);
2383   match(Set dst (SubF src1 src2));
2384 
2385   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2386   ins_cost(150);
2387   ins_encode %{
2388     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2389   %}
2390   ins_pipe(pipe_slow);
2391 %}
2392 
2393 instruct subF_reg_mem(regF dst, regF src1, memory src2) %{
2394   predicate(UseAVX &gt; 0);
2395   match(Set dst (SubF src1 (LoadF src2)));
2396 
2397   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2398   ins_cost(150);
2399   ins_encode %{
2400     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2401   %}
2402   ins_pipe(pipe_slow);
2403 %}
2404 
2405 instruct subF_reg_imm(regF dst, regF src, immF con) %{
2406   predicate(UseAVX &gt; 0);
2407   match(Set dst (SubF src con));
2408 
2409   format %{ &quot;vsubss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2410   ins_cost(150);
2411   ins_encode %{
2412     __ vsubss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2413   %}
2414   ins_pipe(pipe_slow);
2415 %}
2416 
2417 instruct subD_reg(regD dst, regD src) %{
2418   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2419   match(Set dst (SubD dst src));
2420 
2421   format %{ &quot;subsd   $dst, $src&quot; %}
2422   ins_cost(150);
2423   ins_encode %{
2424     __ subsd($dst$$XMMRegister, $src$$XMMRegister);
2425   %}
2426   ins_pipe(pipe_slow);
2427 %}
2428 
2429 instruct subD_mem(regD dst, memory src) %{
2430   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2431   match(Set dst (SubD dst (LoadD src)));
2432 
2433   format %{ &quot;subsd   $dst, $src&quot; %}
2434   ins_cost(150);
2435   ins_encode %{
2436     __ subsd($dst$$XMMRegister, $src$$Address);
2437   %}
2438   ins_pipe(pipe_slow);
2439 %}
2440 
2441 instruct subD_imm(regD dst, immD con) %{
2442   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2443   match(Set dst (SubD dst con));
2444   format %{ &quot;subsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2445   ins_cost(150);
2446   ins_encode %{
2447     __ subsd($dst$$XMMRegister, $constantaddress($con));
2448   %}
2449   ins_pipe(pipe_slow);
2450 %}
2451 
2452 instruct subD_reg_reg(regD dst, regD src1, regD src2) %{
2453   predicate(UseAVX &gt; 0);
2454   match(Set dst (SubD src1 src2));
2455 
2456   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2457   ins_cost(150);
2458   ins_encode %{
2459     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2460   %}
2461   ins_pipe(pipe_slow);
2462 %}
2463 
2464 instruct subD_reg_mem(regD dst, regD src1, memory src2) %{
2465   predicate(UseAVX &gt; 0);
2466   match(Set dst (SubD src1 (LoadD src2)));
2467 
2468   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2469   ins_cost(150);
2470   ins_encode %{
2471     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2472   %}
2473   ins_pipe(pipe_slow);
2474 %}
2475 
2476 instruct subD_reg_imm(regD dst, regD src, immD con) %{
2477   predicate(UseAVX &gt; 0);
2478   match(Set dst (SubD src con));
2479 
2480   format %{ &quot;vsubsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2481   ins_cost(150);
2482   ins_encode %{
2483     __ vsubsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2484   %}
2485   ins_pipe(pipe_slow);
2486 %}
2487 
2488 instruct mulF_reg(regF dst, regF src) %{
2489   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2490   match(Set dst (MulF dst src));
2491 
2492   format %{ &quot;mulss   $dst, $src&quot; %}
2493   ins_cost(150);
2494   ins_encode %{
2495     __ mulss($dst$$XMMRegister, $src$$XMMRegister);
2496   %}
2497   ins_pipe(pipe_slow);
2498 %}
2499 
2500 instruct mulF_mem(regF dst, memory src) %{
2501   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2502   match(Set dst (MulF dst (LoadF src)));
2503 
2504   format %{ &quot;mulss   $dst, $src&quot; %}
2505   ins_cost(150);
2506   ins_encode %{
2507     __ mulss($dst$$XMMRegister, $src$$Address);
2508   %}
2509   ins_pipe(pipe_slow);
2510 %}
2511 
2512 instruct mulF_imm(regF dst, immF con) %{
2513   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2514   match(Set dst (MulF dst con));
2515   format %{ &quot;mulss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2516   ins_cost(150);
2517   ins_encode %{
2518     __ mulss($dst$$XMMRegister, $constantaddress($con));
2519   %}
2520   ins_pipe(pipe_slow);
2521 %}
2522 
2523 instruct mulF_reg_reg(regF dst, regF src1, regF src2) %{
2524   predicate(UseAVX &gt; 0);
2525   match(Set dst (MulF src1 src2));
2526 
2527   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2528   ins_cost(150);
2529   ins_encode %{
2530     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2531   %}
2532   ins_pipe(pipe_slow);
2533 %}
2534 
2535 instruct mulF_reg_mem(regF dst, regF src1, memory src2) %{
2536   predicate(UseAVX &gt; 0);
2537   match(Set dst (MulF src1 (LoadF src2)));
2538 
2539   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2540   ins_cost(150);
2541   ins_encode %{
2542     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2543   %}
2544   ins_pipe(pipe_slow);
2545 %}
2546 
2547 instruct mulF_reg_imm(regF dst, regF src, immF con) %{
2548   predicate(UseAVX &gt; 0);
2549   match(Set dst (MulF src con));
2550 
2551   format %{ &quot;vmulss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2552   ins_cost(150);
2553   ins_encode %{
2554     __ vmulss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2555   %}
2556   ins_pipe(pipe_slow);
2557 %}
2558 
2559 instruct mulD_reg(regD dst, regD src) %{
2560   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2561   match(Set dst (MulD dst src));
2562 
2563   format %{ &quot;mulsd   $dst, $src&quot; %}
2564   ins_cost(150);
2565   ins_encode %{
2566     __ mulsd($dst$$XMMRegister, $src$$XMMRegister);
2567   %}
2568   ins_pipe(pipe_slow);
2569 %}
2570 
2571 instruct mulD_mem(regD dst, memory src) %{
2572   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2573   match(Set dst (MulD dst (LoadD src)));
2574 
2575   format %{ &quot;mulsd   $dst, $src&quot; %}
2576   ins_cost(150);
2577   ins_encode %{
2578     __ mulsd($dst$$XMMRegister, $src$$Address);
2579   %}
2580   ins_pipe(pipe_slow);
2581 %}
2582 
2583 instruct mulD_imm(regD dst, immD con) %{
2584   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2585   match(Set dst (MulD dst con));
2586   format %{ &quot;mulsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2587   ins_cost(150);
2588   ins_encode %{
2589     __ mulsd($dst$$XMMRegister, $constantaddress($con));
2590   %}
2591   ins_pipe(pipe_slow);
2592 %}
2593 
2594 instruct mulD_reg_reg(regD dst, regD src1, regD src2) %{
2595   predicate(UseAVX &gt; 0);
2596   match(Set dst (MulD src1 src2));
2597 
2598   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2599   ins_cost(150);
2600   ins_encode %{
2601     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2602   %}
2603   ins_pipe(pipe_slow);
2604 %}
2605 
2606 instruct mulD_reg_mem(regD dst, regD src1, memory src2) %{
2607   predicate(UseAVX &gt; 0);
2608   match(Set dst (MulD src1 (LoadD src2)));
2609 
2610   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2611   ins_cost(150);
2612   ins_encode %{
2613     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2614   %}
2615   ins_pipe(pipe_slow);
2616 %}
2617 
2618 instruct mulD_reg_imm(regD dst, regD src, immD con) %{
2619   predicate(UseAVX &gt; 0);
2620   match(Set dst (MulD src con));
2621 
2622   format %{ &quot;vmulsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2623   ins_cost(150);
2624   ins_encode %{
2625     __ vmulsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2626   %}
2627   ins_pipe(pipe_slow);
2628 %}
2629 
2630 instruct divF_reg(regF dst, regF src) %{
2631   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2632   match(Set dst (DivF dst src));
2633 
2634   format %{ &quot;divss   $dst, $src&quot; %}
2635   ins_cost(150);
2636   ins_encode %{
2637     __ divss($dst$$XMMRegister, $src$$XMMRegister);
2638   %}
2639   ins_pipe(pipe_slow);
2640 %}
2641 
2642 instruct divF_mem(regF dst, memory src) %{
2643   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2644   match(Set dst (DivF dst (LoadF src)));
2645 
2646   format %{ &quot;divss   $dst, $src&quot; %}
2647   ins_cost(150);
2648   ins_encode %{
2649     __ divss($dst$$XMMRegister, $src$$Address);
2650   %}
2651   ins_pipe(pipe_slow);
2652 %}
2653 
2654 instruct divF_imm(regF dst, immF con) %{
2655   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2656   match(Set dst (DivF dst con));
2657   format %{ &quot;divss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2658   ins_cost(150);
2659   ins_encode %{
2660     __ divss($dst$$XMMRegister, $constantaddress($con));
2661   %}
2662   ins_pipe(pipe_slow);
2663 %}
2664 
2665 instruct divF_reg_reg(regF dst, regF src1, regF src2) %{
2666   predicate(UseAVX &gt; 0);
2667   match(Set dst (DivF src1 src2));
2668 
2669   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2670   ins_cost(150);
2671   ins_encode %{
2672     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2673   %}
2674   ins_pipe(pipe_slow);
2675 %}
2676 
2677 instruct divF_reg_mem(regF dst, regF src1, memory src2) %{
2678   predicate(UseAVX &gt; 0);
2679   match(Set dst (DivF src1 (LoadF src2)));
2680 
2681   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2682   ins_cost(150);
2683   ins_encode %{
2684     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2685   %}
2686   ins_pipe(pipe_slow);
2687 %}
2688 
2689 instruct divF_reg_imm(regF dst, regF src, immF con) %{
2690   predicate(UseAVX &gt; 0);
2691   match(Set dst (DivF src con));
2692 
2693   format %{ &quot;vdivss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2694   ins_cost(150);
2695   ins_encode %{
2696     __ vdivss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2697   %}
2698   ins_pipe(pipe_slow);
2699 %}
2700 
2701 instruct divD_reg(regD dst, regD src) %{
2702   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2703   match(Set dst (DivD dst src));
2704 
2705   format %{ &quot;divsd   $dst, $src&quot; %}
2706   ins_cost(150);
2707   ins_encode %{
2708     __ divsd($dst$$XMMRegister, $src$$XMMRegister);
2709   %}
2710   ins_pipe(pipe_slow);
2711 %}
2712 
2713 instruct divD_mem(regD dst, memory src) %{
2714   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2715   match(Set dst (DivD dst (LoadD src)));
2716 
2717   format %{ &quot;divsd   $dst, $src&quot; %}
2718   ins_cost(150);
2719   ins_encode %{
2720     __ divsd($dst$$XMMRegister, $src$$Address);
2721   %}
2722   ins_pipe(pipe_slow);
2723 %}
2724 
2725 instruct divD_imm(regD dst, immD con) %{
2726   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2727   match(Set dst (DivD dst con));
2728   format %{ &quot;divsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2729   ins_cost(150);
2730   ins_encode %{
2731     __ divsd($dst$$XMMRegister, $constantaddress($con));
2732   %}
2733   ins_pipe(pipe_slow);
2734 %}
2735 
2736 instruct divD_reg_reg(regD dst, regD src1, regD src2) %{
2737   predicate(UseAVX &gt; 0);
2738   match(Set dst (DivD src1 src2));
2739 
2740   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2741   ins_cost(150);
2742   ins_encode %{
2743     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2744   %}
2745   ins_pipe(pipe_slow);
2746 %}
2747 
2748 instruct divD_reg_mem(regD dst, regD src1, memory src2) %{
2749   predicate(UseAVX &gt; 0);
2750   match(Set dst (DivD src1 (LoadD src2)));
2751 
2752   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2753   ins_cost(150);
2754   ins_encode %{
2755     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2756   %}
2757   ins_pipe(pipe_slow);
2758 %}
2759 
2760 instruct divD_reg_imm(regD dst, regD src, immD con) %{
2761   predicate(UseAVX &gt; 0);
2762   match(Set dst (DivD src con));
2763 
2764   format %{ &quot;vdivsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2765   ins_cost(150);
2766   ins_encode %{
2767     __ vdivsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2768   %}
2769   ins_pipe(pipe_slow);
2770 %}
2771 
2772 instruct absF_reg(regF dst) %{
2773   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2774   match(Set dst (AbsF dst));
2775   ins_cost(150);
2776   format %{ &quot;andps   $dst, [0x7fffffff]\t# abs float by sign masking&quot; %}
2777   ins_encode %{
2778     __ andps($dst$$XMMRegister, ExternalAddress(float_signmask()));
2779   %}
2780   ins_pipe(pipe_slow);
2781 %}
2782 
2783 instruct absF_reg_reg(vlRegF dst, vlRegF src) %{
2784   predicate(UseAVX &gt; 0);
2785   match(Set dst (AbsF src));
2786   ins_cost(150);
2787   format %{ &quot;vandps  $dst, $src, [0x7fffffff]\t# abs float by sign masking&quot; %}
2788   ins_encode %{
2789     int vector_len = 0;
2790     __ vandps($dst$$XMMRegister, $src$$XMMRegister,
2791               ExternalAddress(float_signmask()), vector_len);
2792   %}
2793   ins_pipe(pipe_slow);
2794 %}
2795 
2796 instruct absD_reg(regD dst) %{
2797   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2798   match(Set dst (AbsD dst));
2799   ins_cost(150);
2800   format %{ &quot;andpd   $dst, [0x7fffffffffffffff]\t&quot;
2801             &quot;# abs double by sign masking&quot; %}
2802   ins_encode %{
2803     __ andpd($dst$$XMMRegister, ExternalAddress(double_signmask()));
2804   %}
2805   ins_pipe(pipe_slow);
2806 %}
2807 
2808 instruct absD_reg_reg(vlRegD dst, vlRegD src) %{
2809   predicate(UseAVX &gt; 0);
2810   match(Set dst (AbsD src));
2811   ins_cost(150);
2812   format %{ &quot;vandpd  $dst, $src, [0x7fffffffffffffff]\t&quot;
2813             &quot;# abs double by sign masking&quot; %}
2814   ins_encode %{
2815     int vector_len = 0;
2816     __ vandpd($dst$$XMMRegister, $src$$XMMRegister,
2817               ExternalAddress(double_signmask()), vector_len);
2818   %}
2819   ins_pipe(pipe_slow);
2820 %}
2821 
2822 instruct negF_reg(regF dst) %{
2823   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2824   match(Set dst (NegF dst));
2825   ins_cost(150);
2826   format %{ &quot;xorps   $dst, [0x80000000]\t# neg float by sign flipping&quot; %}
2827   ins_encode %{
2828     __ xorps($dst$$XMMRegister, ExternalAddress(float_signflip()));
2829   %}
2830   ins_pipe(pipe_slow);
2831 %}
2832 
2833 instruct negF_reg_reg(vlRegF dst, vlRegF src) %{
2834   predicate(UseAVX &gt; 0);
2835   match(Set dst (NegF src));
2836   ins_cost(150);
2837   format %{ &quot;vnegatess  $dst, $src, [0x80000000]\t# neg float by sign flipping&quot; %}
2838   ins_encode %{
2839     __ vnegatess($dst$$XMMRegister, $src$$XMMRegister,
2840                  ExternalAddress(float_signflip()));
2841   %}
2842   ins_pipe(pipe_slow);
2843 %}
2844 
2845 instruct negD_reg(regD dst) %{
2846   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2847   match(Set dst (NegD dst));
2848   ins_cost(150);
2849   format %{ &quot;xorpd   $dst, [0x8000000000000000]\t&quot;
2850             &quot;# neg double by sign flipping&quot; %}
2851   ins_encode %{
2852     __ xorpd($dst$$XMMRegister, ExternalAddress(double_signflip()));
2853   %}
2854   ins_pipe(pipe_slow);
2855 %}
2856 
2857 instruct negD_reg_reg(vlRegD dst, vlRegD src) %{
2858   predicate(UseAVX &gt; 0);
2859   match(Set dst (NegD src));
2860   ins_cost(150);
2861   format %{ &quot;vnegatesd  $dst, $src, [0x8000000000000000]\t&quot;
2862             &quot;# neg double by sign flipping&quot; %}
2863   ins_encode %{
2864     __ vnegatesd($dst$$XMMRegister, $src$$XMMRegister,
2865                  ExternalAddress(double_signflip()));
2866   %}
2867   ins_pipe(pipe_slow);
2868 %}
2869 
2870 instruct sqrtF_reg(regF dst, regF src) %{
2871   predicate(UseSSE&gt;=1);
2872   match(Set dst (SqrtF src));
2873 
2874   format %{ &quot;sqrtss  $dst, $src&quot; %}
2875   ins_cost(150);
2876   ins_encode %{
2877     __ sqrtss($dst$$XMMRegister, $src$$XMMRegister);
2878   %}
2879   ins_pipe(pipe_slow);
2880 %}
2881 
2882 instruct sqrtF_mem(regF dst, memory src) %{
2883   predicate(UseSSE&gt;=1);
2884   match(Set dst (SqrtF (LoadF src)));
2885 
2886   format %{ &quot;sqrtss  $dst, $src&quot; %}
2887   ins_cost(150);
2888   ins_encode %{
2889     __ sqrtss($dst$$XMMRegister, $src$$Address);
2890   %}
2891   ins_pipe(pipe_slow);
2892 %}
2893 
2894 instruct sqrtF_imm(regF dst, immF con) %{
2895   predicate(UseSSE&gt;=1);
2896   match(Set dst (SqrtF con));
2897 
2898   format %{ &quot;sqrtss  $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2899   ins_cost(150);
2900   ins_encode %{
2901     __ sqrtss($dst$$XMMRegister, $constantaddress($con));
2902   %}
2903   ins_pipe(pipe_slow);
2904 %}
2905 
2906 instruct sqrtD_reg(regD dst, regD src) %{
2907   predicate(UseSSE&gt;=2);
2908   match(Set dst (SqrtD src));
2909 
2910   format %{ &quot;sqrtsd  $dst, $src&quot; %}
2911   ins_cost(150);
2912   ins_encode %{
2913     __ sqrtsd($dst$$XMMRegister, $src$$XMMRegister);
2914   %}
2915   ins_pipe(pipe_slow);
2916 %}
2917 
2918 instruct sqrtD_mem(regD dst, memory src) %{
2919   predicate(UseSSE&gt;=2);
2920   match(Set dst (SqrtD (LoadD src)));
2921 
2922   format %{ &quot;sqrtsd  $dst, $src&quot; %}
2923   ins_cost(150);
2924   ins_encode %{
2925     __ sqrtsd($dst$$XMMRegister, $src$$Address);
2926   %}
2927   ins_pipe(pipe_slow);
2928 %}
2929 
2930 instruct sqrtD_imm(regD dst, immD con) %{
2931   predicate(UseSSE&gt;=2);
2932   match(Set dst (SqrtD con));
2933   format %{ &quot;sqrtsd  $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2934   ins_cost(150);
2935   ins_encode %{
2936     __ sqrtsd($dst$$XMMRegister, $constantaddress($con));
2937   %}
2938   ins_pipe(pipe_slow);
2939 %}
2940 
2941 
2942 #ifdef _LP64
2943 instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
2944   match(Set dst (RoundDoubleMode src rmode));
2945   format %{ &quot;roundsd $dst,$src&quot; %}
2946   ins_cost(150);
2947   ins_encode %{
2948     assert(UseSSE &gt;= 4, &quot;required&quot;);
2949     __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
2950   %}
2951   ins_pipe(pipe_slow);
2952 %}
2953 
2954 instruct roundD_mem(legRegD dst, memory src, immU8 rmode) %{
2955   match(Set dst (RoundDoubleMode (LoadD src) rmode));
2956   format %{ &quot;roundsd $dst,$src&quot; %}
2957   ins_cost(150);
2958   ins_encode %{
2959     assert(UseSSE &gt;= 4, &quot;required&quot;);
2960     __ roundsd($dst$$XMMRegister, $src$$Address, $rmode$$constant);
2961   %}
2962   ins_pipe(pipe_slow);
2963 %}
2964 
2965 instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{
2966   match(Set dst (RoundDoubleMode con rmode));
2967   effect(TEMP scratch_reg);
2968   format %{ &quot;roundsd $dst,[$constantaddress]\t# load from constant table: double=$con&quot; %}
2969   ins_cost(150);
2970   ins_encode %{
2971     assert(UseSSE &gt;= 4, &quot;required&quot;);
2972     __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);
2973   %}
2974   ins_pipe(pipe_slow);
2975 %}
2976 
2977 instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
2978   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
2979   match(Set dst (RoundDoubleModeV src rmode));
2980   format %{ &quot;vroundpd $dst,$src,$rmode\t! round packedD&quot; %}
2981   ins_encode %{
2982     assert(UseAVX &gt; 0, &quot;required&quot;);
2983     int vector_len = vector_length_encoding(this);
2984     __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vector_len);
2985   %}
2986   ins_pipe( pipe_slow );
2987 %}
2988 
2989 instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
2990   predicate(n-&gt;as_Vector()-&gt;length() == 8);
2991   match(Set dst (RoundDoubleModeV src rmode));
2992   format %{ &quot;vrndscalepd $dst,$src,$rmode\t! round packed8D&quot; %}
2993   ins_encode %{
2994     assert(UseAVX &gt; 2, &quot;required&quot;);
2995     __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
2996   %}
2997   ins_pipe( pipe_slow );
2998 %}
2999 
3000 instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
3001   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3002   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3003   format %{ &quot;vroundpd $dst, $mem, $rmode\t! round packedD&quot; %}
3004   ins_encode %{
3005     assert(UseAVX &gt; 0, &quot;required&quot;);
3006     int vector_len = vector_length_encoding(this);
3007     __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vector_len);
3008   %}
3009   ins_pipe( pipe_slow );
3010 %}
3011 
3012 instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
3013   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3014   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3015   format %{ &quot;vrndscalepd $dst,$mem,$rmode\t! round packed8D&quot; %}
3016   ins_encode %{
3017     assert(UseAVX &gt; 2, &quot;required&quot;);
3018     __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
3019   %}
3020   ins_pipe( pipe_slow );
3021 %}
3022 #endif // _LP64
3023 
3024 instruct onspinwait() %{
3025   match(OnSpinWait);
3026   ins_cost(200);
3027 
3028   format %{
3029     $$template
3030     $$emit$$&quot;pause\t! membar_onspinwait&quot;
3031   %}
3032   ins_encode %{
3033     __ pause();
3034   %}
3035   ins_pipe(pipe_slow);
3036 %}
3037 
3038 // a * b + c
3039 instruct fmaD_reg(regD a, regD b, regD c) %{
3040   predicate(UseFMA);
3041   match(Set c (FmaD  c (Binary a b)));
3042   format %{ &quot;fmasd $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3043   ins_cost(150);
3044   ins_encode %{
3045     __ fmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3046   %}
3047   ins_pipe( pipe_slow );
3048 %}
3049 
3050 // a * b + c
3051 instruct fmaF_reg(regF a, regF b, regF c) %{
3052   predicate(UseFMA);
3053   match(Set c (FmaF  c (Binary a b)));
3054   format %{ &quot;fmass $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3055   ins_cost(150);
3056   ins_encode %{
3057     __ fmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3058   %}
3059   ins_pipe( pipe_slow );
3060 %}
3061 
3062 // ====================VECTOR INSTRUCTIONS=====================================
3063 
3064 // Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
3065 instruct MoveVec2Leg(legVec dst, vec src) %{
3066   match(Set dst src);
3067   format %{ &quot;&quot; %}
3068   ins_encode %{
3069     ShouldNotReachHere();
3070   %}
3071   ins_pipe( fpu_reg_reg );
3072 %}
3073 
3074 instruct MoveLeg2Vec(vec dst, legVec src) %{
3075   match(Set dst src);
3076   format %{ &quot;&quot; %}
3077   ins_encode %{
3078     ShouldNotReachHere();
3079   %}
3080   ins_pipe( fpu_reg_reg );
3081 %}
3082 
3083 // ============================================================================
3084 
3085 // Load vectors
3086 instruct loadV(vec dst, memory mem) %{
3087   match(Set dst (LoadVector mem));
3088   ins_cost(125);
3089   format %{ &quot;load_vector $dst,$mem&quot; %}
3090   ins_encode %{
3091     switch (vector_length_in_bytes(this)) {
3092       case  4: __ movdl    ($dst$$XMMRegister, $mem$$Address); break;
3093       case  8: __ movq     ($dst$$XMMRegister, $mem$$Address); break;
3094       case 16: __ movdqu   ($dst$$XMMRegister, $mem$$Address); break;
3095       case 32: __ vmovdqu  ($dst$$XMMRegister, $mem$$Address); break;
3096       case 64: __ evmovdqul($dst$$XMMRegister, $mem$$Address, Assembler::AVX_512bit); break;
3097       default: ShouldNotReachHere();
3098     }
3099   %}
3100   ins_pipe( pipe_slow );
3101 %}
3102 
3103 // Store vectors generic operand pattern.
3104 instruct storeV(memory mem, vec src) %{
3105   match(Set mem (StoreVector mem src));
3106   ins_cost(145);
3107   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3108   ins_encode %{
3109     switch (vector_length_in_bytes(this, $src)) {
3110       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3111       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3112       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3113       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3114       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3115       default: ShouldNotReachHere();
3116     }
3117   %}
3118   ins_pipe( pipe_slow );
3119 %}
3120 
3121 // ====================REPLICATE=======================================
3122 
3123 // Replicate byte scalar to be vector
3124 instruct ReplB_reg(vec dst, rRegI src) %{
<a name="21" id="anc21"></a><span class="line-removed">3125   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3126             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<a name="22" id="anc22"></a><span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<a name="23" id="anc23"></a><span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;); // vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_reg_leg</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
<a name="24" id="anc24"></a><span class="line-removed">3151 instruct ReplB_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3152   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions</span>
<span class="line-removed">3153   match(Set dst (ReplicateB src));</span>
<span class="line-removed">3154   format %{ &quot;replicateB $dst,$src&quot; %}</span>
<span class="line-removed">3155   ins_encode %{</span>
<span class="line-removed">3156     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3157     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3158     __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3159     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3160     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3161     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3162     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3163   %}</span>
<span class="line-removed">3164   ins_pipe( pipe_slow );</span>
<span class="line-removed">3165 %}</span>
<span class="line-removed">3166 </span>
3167 instruct ReplB_mem(vec dst, memory mem) %{
<a name="25" id="anc25"></a><span class="line-modified">3168   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32 &amp;&amp; VM_Version::supports_avx512vlbw()) || // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3169             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions</span>
3170   match(Set dst (ReplicateB (LoadB mem)));
3171   format %{ &quot;replicateB $dst,$mem&quot; %}
3172   ins_encode %{
<a name="26" id="anc26"></a><span class="line-removed">3173     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
3174     int vector_len = vector_length_encoding(this);
3175     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3176   %}
3177   ins_pipe( pipe_slow );
3178 %}
3179 
3180 instruct ReplB_imm(vec dst, immI con) %{
<a name="27" id="anc27"></a><span class="line-removed">3181   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3182             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3183   match(Set dst (ReplicateB con));
3184   format %{ &quot;replicateB $dst,$con&quot; %}
3185   ins_encode %{
3186     uint vlen = vector_length(this);
3187     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3188     if (vlen == 4) {
3189       __ movdl($dst$$XMMRegister, const_addr);
3190     } else {
3191       __ movq($dst$$XMMRegister, const_addr);
3192       if (vlen &gt;= 16) {
<a name="28" id="anc28"></a><span class="line-modified">3193         if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
3194           int vlen_enc = vector_length_encoding(this);
<a name="29" id="anc29"></a><span class="line-modified">3195           __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3196         } else {
<a name="30" id="anc30"></a>
3197           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<a name="31" id="anc31"></a><span class="line-removed">3198           if (vlen &gt;= 32) {</span>
<span class="line-removed">3199              assert(vlen == 32, &quot;sanity&quot;);// vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_imm_leg</span>
<span class="line-removed">3200             __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3201           }</span>
3202         }
3203       }
3204     }
3205   %}
3206   ins_pipe( pipe_slow );
3207 %}
3208 
<a name="32" id="anc32"></a><span class="line-removed">3209 instruct ReplB_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3210   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3211   match(Set dst (ReplicateB con));</span>
<span class="line-removed">3212   format %{ &quot;replicateB $dst,$con&quot; %}</span>
<span class="line-removed">3213   ins_encode %{</span>
<span class="line-removed">3214     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));</span>
<span class="line-removed">3215     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3216     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3217     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3218   %}</span>
<span class="line-removed">3219   ins_pipe( pipe_slow );</span>
<span class="line-removed">3220 %}</span>
<span class="line-removed">3221 </span>
3222 // Replicate byte scalar zero to be vector
3223 instruct ReplB_zero(vec dst, immI0 zero) %{
3224   match(Set dst (ReplicateB zero));
3225   format %{ &quot;replicateB $dst,$zero&quot; %}
3226   ins_encode %{
3227     uint vlen = vector_length(this);
3228     if (vlen &lt;= 16) {
3229       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3230     } else {
3231       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3232       int vlen_enc = vector_length_encoding(this);
3233       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3234     }
3235   %}
3236   ins_pipe( fpu_reg_reg );
3237 %}
3238 
3239 // ====================ReplicateS=======================================
3240 
3241 instruct ReplS_reg(vec dst, rRegI src) %{
<a name="33" id="anc33"></a><span class="line-removed">3242   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3243             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3244   match(Set dst (ReplicateS src));
3245   format %{ &quot;replicateS $dst,$src&quot; %}
3246   ins_encode %{
3247     uint vlen = vector_length(this);
3248     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<a name="34" id="anc34"></a><span class="line-modified">3249       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3250       int vlen_enc = vector_length_encoding(this);
3251       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3252     } else {
3253       __ movdl($dst$$XMMRegister, $src$$Register);
3254       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3255       if (vlen &gt;= 8) {
3256         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         if (vlen &gt;= 16) {
<a name="35" id="anc35"></a><span class="line-modified">3258           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_reg_leg</span>
3259           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3260         }
3261       }
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
<a name="36" id="anc36"></a><span class="line-removed">3267 instruct ReplS_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3268   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3269   match(Set dst (ReplicateS src));</span>
<span class="line-removed">3270   format %{ &quot;replicateS $dst,$src&quot; %}</span>
<span class="line-removed">3271   ins_encode %{</span>
<span class="line-removed">3272     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3273     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3274     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3275     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3276     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3277   %}</span>
<span class="line-removed">3278   ins_pipe( pipe_slow );</span>
<span class="line-removed">3279 %}</span>
<span class="line-removed">3280 </span>
3281 instruct ReplS_mem(vec dst, memory mem) %{
<a name="37" id="anc37"></a><span class="line-modified">3282   predicate((n-&gt;as_Vector()-&gt;length() &gt;= 4  &amp;&amp;</span>
<span class="line-removed">3283              n-&gt;as_Vector()-&gt;length() &lt;= 16 &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3284             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
<span class="line-removed">3285   match(Set dst (ReplicateS (LoadS mem)));</span>
<span class="line-removed">3286   format %{ &quot;replicateS $dst,$mem&quot; %}</span>
<span class="line-removed">3287   ins_encode %{</span>
<span class="line-removed">3288     uint vlen = vector_length(this);</span>
<span class="line-removed">3289     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3290       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-removed">3291       int vlen_enc = vector_length_encoding(this);</span>
<span class="line-removed">3292       __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>
<span class="line-removed">3293     } else {</span>
<span class="line-removed">3294       __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3295       if (vlen &gt;= 8) {</span>
<span class="line-removed">3296         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3297         if (vlen &gt;= 16) {</span>
<span class="line-removed">3298           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_mem_leg</span>
<span class="line-removed">3299           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3300         }</span>
<span class="line-removed">3301       }</span>
<span class="line-removed">3302     }</span>
<span class="line-removed">3303   %}</span>
<span class="line-removed">3304   ins_pipe( pipe_slow );</span>
<span class="line-removed">3305 %}</span>
<span class="line-removed">3306 </span>
<span class="line-removed">3307 instruct ReplS_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3308   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
3309   match(Set dst (ReplicateS (LoadS mem)));
3310   format %{ &quot;replicateS $dst,$mem&quot; %}
3311   ins_encode %{
<a name="38" id="anc38"></a><span class="line-modified">3312     __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3313     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3314     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3315     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
3316   %}
3317   ins_pipe( pipe_slow );
3318 %}
3319 
3320 instruct ReplS_imm(vec dst, immI con) %{
<a name="39" id="anc39"></a><span class="line-removed">3321   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3322             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3323   match(Set dst (ReplicateS con));
3324   format %{ &quot;replicateS $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
<a name="40" id="anc40"></a><span class="line-modified">3327     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3328     if (vlen == 2) {
<a name="41" id="anc41"></a><span class="line-modified">3329       __ movdl($dst$$XMMRegister, constaddr);</span>
3330     } else {
<a name="42" id="anc42"></a><span class="line-modified">3331       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3332       if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for &lt;512bit operands</span>
<span class="line-modified">3333         assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-modified">3334         int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3335         __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3336       } else {</span>
<span class="line-modified">3337         __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3338         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3339         if (vlen &gt;= 16) {</span>
<span class="line-removed">3340           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_imm_leg</span>
<span class="line-removed">3341           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3342         }
3343       }
3344     }
3345   %}
3346   ins_pipe( fpu_reg_reg );
3347 %}
3348 
<a name="43" id="anc43"></a><span class="line-removed">3349 instruct ReplS_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3350   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3351   match(Set dst (ReplicateS con));</span>
<span class="line-removed">3352   format %{ &quot;replicateS $dst,$con&quot; %}</span>
<span class="line-removed">3353   ins_encode %{</span>
<span class="line-removed">3354     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));</span>
<span class="line-removed">3355     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3356     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3357     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3358   %}</span>
<span class="line-removed">3359   ins_pipe( pipe_slow );</span>
<span class="line-removed">3360 %}</span>
<span class="line-removed">3361 </span>
3362 instruct ReplS_zero(vec dst, immI0 zero) %{
3363   match(Set dst (ReplicateS zero));
3364   format %{ &quot;replicateS $dst,$zero&quot; %}
3365   ins_encode %{
3366     uint vlen = vector_length(this);
3367     if (vlen &lt;= 8) {
3368       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3369     } else {
3370       int vlen_enc = vector_length_encoding(this);
3371       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3372     }
3373   %}
3374   ins_pipe( fpu_reg_reg );
3375 %}
3376 
3377 // ====================ReplicateI=======================================
3378 
3379 instruct ReplI_reg(vec dst, rRegI src) %{
3380   match(Set dst (ReplicateI src));
3381   format %{ &quot;replicateI $dst,$src&quot; %}
3382   ins_encode %{
3383     uint vlen = vector_length(this);
3384     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3385       int vlen_enc = vector_length_encoding(this);
3386       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3387     } else {
3388       __ movdl($dst$$XMMRegister, $src$$Register);
3389       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3390       if (vlen &gt;= 8) {
3391         assert(vlen == 8, &quot;sanity&quot;);
3392         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3393       }
3394     }
3395   %}
3396   ins_pipe( pipe_slow );
3397 %}
3398 
3399 instruct ReplI_mem(vec dst, memory mem) %{
<a name="44" id="anc44"></a><span class="line-removed">3400   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3401   match(Set dst (ReplicateI (LoadI mem)));
3402   format %{ &quot;replicateI $dst,$mem&quot; %}
3403   ins_encode %{
3404     uint vlen = vector_length(this);
3405     if (vlen &lt;= 4) {
<a name="45" id="anc45"></a><span class="line-modified">3406       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3407     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3408       int vector_len = vector_length_encoding(this);
3409       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
<a name="46" id="anc46"></a><span class="line-removed">3410     } else {</span>
<span class="line-removed">3411       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3412       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3413       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3414     }
3415   %}
3416   ins_pipe( pipe_slow );
3417 %}
3418 
3419 instruct ReplI_imm(vec dst, immI con) %{
3420   match(Set dst (ReplicateI con));
3421   format %{ &quot;replicateI $dst,$con&quot; %}
3422   ins_encode %{
3423     uint vlen = vector_length(this);
<a name="47" id="anc47"></a><span class="line-modified">3424     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3425     if (vlen == 2) {</span>
<span class="line-modified">3426       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3427     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>




3428       int vector_len = vector_length_encoding(this);
<a name="48" id="anc48"></a><span class="line-modified">3429       __ movq($dst$$XMMRegister, constaddr);</span>
3430       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
<a name="49" id="anc49"></a><span class="line-removed">3431     } else {</span>
<span class="line-removed">3432       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-removed">3433       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3434       if (vlen &gt;= 8) {</span>
<span class="line-removed">3435         assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3436         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3437       }</span>
3438     }
3439   %}
3440   ins_pipe( pipe_slow );
3441 %}
3442 
3443 // Replicate integer (4 byte) scalar zero to be vector
3444 instruct ReplI_zero(vec dst, immI0 zero) %{
3445   match(Set dst (ReplicateI zero));
3446   format %{ &quot;replicateI $dst,$zero&quot; %}
3447   ins_encode %{
3448     uint vlen = vector_length(this);
3449     if (vlen &lt;= 4) {
3450       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3451     } else {
3452       int vlen_enc = vector_length_encoding(this);
3453       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3454     }
3455   %}
3456   ins_pipe( fpu_reg_reg );
3457 %}
3458 
<a name="50" id="anc50"></a>













3459 // ====================ReplicateL=======================================
3460 
3461 #ifdef _LP64
3462 // Replicate long (8 byte) scalar to be vector
3463 instruct ReplL_reg(vec dst, rRegL src) %{
3464   match(Set dst (ReplicateL src));
3465   format %{ &quot;replicateL $dst,$src&quot; %}
3466   ins_encode %{
3467     uint vlen = vector_length(this);
3468     if (vlen == 2) {
3469       __ movdq($dst$$XMMRegister, $src$$Register);
3470       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3471     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3472       int vlen_enc = vector_length_encoding(this);
3473       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3474     } else {
3475       assert(vlen == 4, &quot;sanity&quot;);
3476       __ movdq($dst$$XMMRegister, $src$$Register);
3477       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3478       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3479     }
3480   %}
3481   ins_pipe( pipe_slow );
3482 %}
3483 #else // _LP64
3484 // Replicate long (8 byte) scalar to be vector
3485 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3486   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3487   match(Set dst (ReplicateL src));
3488   effect(TEMP dst, USE src, TEMP tmp);
3489   format %{ &quot;replicateL $dst,$src&quot; %}
3490   ins_encode %{
3491     uint vlen = vector_length(this);
3492     if (vlen == 2) {
3493       __ movdl($dst$$XMMRegister, $src$$Register);
3494       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3495       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3496       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3497     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3498       int vector_len = Assembler::AVX_256bit;
3499       __ movdl($dst$$XMMRegister, $src$$Register);
3500       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3501       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3502       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3503     } else {
3504       __ movdl($dst$$XMMRegister, $src$$Register);
3505       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3506       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3507       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3508       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3509     }
3510   %}
3511   ins_pipe( pipe_slow );
3512 %}
3513 
3514 instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{
3515   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3516   match(Set dst (ReplicateL src));
3517   effect(TEMP dst, USE src, TEMP tmp);
3518   format %{ &quot;replicateL $dst,$src&quot; %}
3519   ins_encode %{
3520     if (VM_Version::supports_avx512vl()) {
3521       __ movdl($dst$$XMMRegister, $src$$Register);
3522       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3523       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3524       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3525       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3526       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3527     } else {
3528       int vector_len = Assembler::AVX_512bit;
3529       __ movdl($dst$$XMMRegister, $src$$Register);
3530       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3531       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3532       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3533     }
3534   %}
3535   ins_pipe( pipe_slow );
3536 %}
3537 #endif // _LP64
3538 
3539 instruct ReplL_mem(vec dst, memory mem) %{
3540   match(Set dst (ReplicateL (LoadL mem)));
3541   format %{ &quot;replicateL $dst,$mem&quot; %}
3542   ins_encode %{
3543     uint vlen = vector_length(this);
3544     if (vlen == 2) {
3545       __ movq($dst$$XMMRegister, $mem$$Address);
3546       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<a name="51" id="anc51"></a><span class="line-modified">3547     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3548       int vlen_enc = vector_length_encoding(this);
3549       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
<a name="52" id="anc52"></a><span class="line-removed">3550     } else {</span>
<span class="line-removed">3551       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3552       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-removed">3553       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3554       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3555     }
3556   %}
3557   ins_pipe( pipe_slow );
3558 %}
3559 
3560 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3561 instruct ReplL_imm(vec dst, immL con) %{
3562   match(Set dst (ReplicateL con));
3563   format %{ &quot;replicateL $dst,$con&quot; %}
3564   ins_encode %{
3565     uint vlen = vector_length(this);
3566     InternalAddress const_addr = $constantaddress($con);
3567     if (vlen == 2) {
3568       __ movq($dst$$XMMRegister, const_addr);
3569       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<a name="53" id="anc53"></a><span class="line-modified">3570     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3571       int vlen_enc = vector_length_encoding(this);
3572       __ movq($dst$$XMMRegister, const_addr);
3573       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
<a name="54" id="anc54"></a><span class="line-removed">3574     } else {</span>
<span class="line-removed">3575       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3576       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-removed">3577       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3578       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3579     }
3580   %}
3581   ins_pipe( pipe_slow );
3582 %}
3583 
3584 instruct ReplL_zero(vec dst, immL0 zero) %{
3585   match(Set dst (ReplicateL zero));
3586   format %{ &quot;replicateL $dst,$zero&quot; %}
3587   ins_encode %{
3588     int vlen = vector_length(this);
3589     if (vlen == 2) {
3590       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3591     } else {
3592       int vlen_enc = vector_length_encoding(this);
3593       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3594     }
3595   %}
3596   ins_pipe( fpu_reg_reg );
3597 %}
3598 
<a name="55" id="anc55"></a>











3599 // ====================ReplicateF=======================================
3600 
3601 instruct ReplF_reg(vec dst, vlRegF src) %{
3602   match(Set dst (ReplicateF src));
3603   format %{ &quot;replicateF $dst,$src&quot; %}
3604   ins_encode %{
3605     uint vlen = vector_length(this);
3606     if (vlen &lt;= 4) {
3607       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<a name="56" id="anc56"></a><span class="line-modified">3608     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3609       int vector_len = vector_length_encoding(this);
<a name="57" id="anc57"></a><span class="line-modified">3610       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3611     } else {
3612       assert(vlen == 8, &quot;sanity&quot;);
3613       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3614       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3615     }
3616   %}
3617   ins_pipe( pipe_slow );
3618 %}
3619 
3620 instruct ReplF_mem(vec dst, memory mem) %{
<a name="58" id="anc58"></a><span class="line-removed">3621   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3622   match(Set dst (ReplicateF (LoadF mem)));
3623   format %{ &quot;replicateF $dst,$mem&quot; %}
3624   ins_encode %{
3625     uint vlen = vector_length(this);
3626     if (vlen &lt;= 4) {
<a name="59" id="anc59"></a><span class="line-modified">3627       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3628     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3629       int vector_len = vector_length_encoding(this);
3630       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
<a name="60" id="anc60"></a><span class="line-removed">3631     } else {</span>
<span class="line-removed">3632       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3633       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3634       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3635     }
3636   %}
3637   ins_pipe( pipe_slow );
3638 %}
3639 
3640 instruct ReplF_zero(vec dst, immF0 zero) %{
3641   match(Set dst (ReplicateF zero));
3642   format %{ &quot;replicateF $dst,$zero&quot; %}
3643   ins_encode %{
3644     uint vlen = vector_length(this);
3645     if (vlen &lt;= 4) {
3646       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3647     } else {
3648       int vlen_enc = vector_length_encoding(this);
3649       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3650     }
3651   %}
3652   ins_pipe( fpu_reg_reg );
3653 %}
3654 
3655 // ====================ReplicateD=======================================
3656 
3657 // Replicate double (8 bytes) scalar to be vector
3658 instruct ReplD_reg(vec dst, vlRegD src) %{
3659   match(Set dst (ReplicateD src));
3660   format %{ &quot;replicateD $dst,$src&quot; %}
3661   ins_encode %{
3662     uint vlen = vector_length(this);
3663     if (vlen == 2) {
3664       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<a name="61" id="anc61"></a><span class="line-modified">3665     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3666       int vector_len = vector_length_encoding(this);
<a name="62" id="anc62"></a><span class="line-modified">3667       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3668     } else {
3669       assert(vlen == 4, &quot;sanity&quot;);
3670       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3671       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3672     }
3673   %}
3674   ins_pipe( pipe_slow );
3675 %}
3676 
3677 instruct ReplD_mem(vec dst, memory mem) %{
<a name="63" id="anc63"></a><span class="line-removed">3678   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3679   match(Set dst (ReplicateD (LoadD mem)));
3680   format %{ &quot;replicateD $dst,$mem&quot; %}
3681   ins_encode %{
3682     uint vlen = vector_length(this);
3683     if (vlen == 2) {
<a name="64" id="anc64"></a><span class="line-modified">3684       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-modified">3685     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3686       int vector_len = vector_length_encoding(this);
3687       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
<a name="65" id="anc65"></a><span class="line-removed">3688     } else {</span>
<span class="line-removed">3689       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3690       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-removed">3691       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3692     }
3693   %}
3694   ins_pipe( pipe_slow );
3695 %}
3696 
3697 instruct ReplD_zero(vec dst, immD0 zero) %{
3698   match(Set dst (ReplicateD zero));
3699   format %{ &quot;replicateD $dst,$zero&quot; %}
3700   ins_encode %{
3701     uint vlen = vector_length(this);
3702     if (vlen == 2) {
3703       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3704     } else {
3705       int vlen_enc = vector_length_encoding(this);
3706       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3707     }
3708   %}
3709   ins_pipe( fpu_reg_reg );
3710 %}
3711 
3712 // ====================REDUCTION ARITHMETIC=======================================
<a name="66" id="anc66"></a>
3713 
<a name="67" id="anc67"></a><span class="line-modified">3714 // =======================AddReductionVI==========================================</span>
<span class="line-modified">3715 </span>
<span class="line-modified">3716 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3717   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">3718   match(Set dst (AddReductionVI src1 src2));</span>
<span class="line-removed">3719   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3720   format %{ &quot;vector_add2I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3721   ins_encode %{</span>
<span class="line-removed">3722     if (UseAVX &gt; 2) {</span>
<span class="line-removed">3723       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3724       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">3725       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3726       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3727       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3728       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3729     } else if (VM_Version::supports_avxonly()) {</span>
<span class="line-removed">3730       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3731       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3732       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3733       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3734       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3735     } else {</span>
<span class="line-removed">3736       assert(UseSSE &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3737       __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3738       __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3739       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3740       __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3741       __ movdl($dst$$Register, $tmp$$XMMRegister);</span>
<span class="line-removed">3742     }</span>
<span class="line-removed">3743   %}</span>
<span class="line-removed">3744   ins_pipe( pipe_slow );</span>
<span class="line-removed">3745 %}</span>
<span class="line-removed">3746 </span>
<span class="line-removed">3747 instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3748   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
3749   match(Set dst (AddReductionVI src1 src2));
<a name="68" id="anc68"></a><span class="line-modified">3750   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3751   format %{ &quot;vector_add4I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-modified">3752   ins_encode %{</span>
<span class="line-modified">3753     if (UseAVX &gt; 2) {</span>
<span class="line-modified">3754       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-modified">3755       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-modified">3756       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-modified">3757       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-modified">3758       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-modified">3759       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3760       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3761       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3762     } else if (VM_Version::supports_avxonly()) {</span>
<span class="line-removed">3763       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3764       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3765       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3766       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3767       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3768       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3769     } else {</span>
<span class="line-removed">3770       assert(UseSSE &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3771       __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3772       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3773       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3774       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3775       __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3776       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3777     }</span>
<span class="line-removed">3778   %}</span>
<span class="line-removed">3779   ins_pipe( pipe_slow );</span>
<span class="line-removed">3780 %}</span>
<span class="line-removed">3781 </span>
<span class="line-removed">3782 instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3783   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">3784   match(Set dst (AddReductionVI src1 src2));</span>
<span class="line-removed">3785   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3786   format %{ &quot;vector_add8I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3787   ins_encode %{</span>
<span class="line-removed">3788     if (UseAVX &gt; 2) {</span>
<span class="line-removed">3789       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3790       __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3791       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3792       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">3793       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3794       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">3795       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3796       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3797       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3798       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3799     } else {</span>
<span class="line-removed">3800       assert(UseAVX &gt; 0, &quot;&quot;);</span>
<span class="line-removed">3801       int vector_len = Assembler::AVX_256bit;</span>
<span class="line-removed">3802       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3803       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3804       __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3805       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3806       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3807       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3808       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3809     }</span>
3810   %}
3811   ins_pipe( pipe_slow );
3812 %}
3813 
<a name="69" id="anc69"></a><span class="line-modified">3814 instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{</span>
<span class="line-modified">3815   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>

3816   match(Set dst (AddReductionVI src1 src2));
<a name="70" id="anc70"></a><span class="line-modified">3817   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);</span>
<span class="line-modified">3818   format %{ &quot;vector_add16I_reduction $dst,$src1,$src2&quot; %}</span>




3819   ins_encode %{
<a name="71" id="anc71"></a><span class="line-modified">3820     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3821     __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-modified">3822     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);</span>
<span class="line-removed">3823     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);</span>
<span class="line-removed">3824     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">3825     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3826     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">3827     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3828     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3829     __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3830     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
3831   %}
3832   ins_pipe( pipe_slow );
3833 %}
3834 
<a name="72" id="anc72"></a><span class="line-modified">3835 // =======================AddReductionVL==========================================</span>
3836 
3837 #ifdef _LP64
<a name="73" id="anc73"></a><span class="line-modified">3838 instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">3839   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-modified">3840   match(Set dst (AddReductionVL src1 src2));</span>
<span class="line-removed">3841   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3842   format %{ &quot;vector_add2L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3843   ins_encode %{</span>
<span class="line-removed">3844     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3845     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">3846     __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3847     __ movdq($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3848     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3849     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3850   %}</span>
<span class="line-removed">3851   ins_pipe( pipe_slow );</span>
<span class="line-removed">3852 %}</span>
<span class="line-removed">3853 </span>
<span class="line-removed">3854 instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3855   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
3856   match(Set dst (AddReductionVL src1 src2));
<a name="74" id="anc74"></a><span class="line-modified">3857   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3858   format %{ &quot;vector_add4L_reduction $dst,$src1,$src2&quot; %}</span>




3859   ins_encode %{
<a name="75" id="anc75"></a><span class="line-modified">3860     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3861     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3862     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);</span>
<span class="line-removed">3863     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">3864     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3865     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3866     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3867     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
3868   %}
3869   ins_pipe( pipe_slow );
3870 %}
3871 
<a name="76" id="anc76"></a><span class="line-modified">3872 instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">3873   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>

3874   match(Set dst (AddReductionVL src1 src2));
<a name="77" id="anc77"></a><span class="line-modified">3875   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3876   format %{ &quot;vector_addL_reduction $dst,$src1,$src2&quot; %}</span>




3877   ins_encode %{
<a name="78" id="anc78"></a><span class="line-modified">3878     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3879     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3880     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">3881     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3882     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3883     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">3884     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3885     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3886     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3887     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
3888   %}
3889   ins_pipe( pipe_slow );
3890 %}
3891 #endif // _LP64
3892 
<a name="79" id="anc79"></a><span class="line-modified">3893 // =======================AddReductionVF==========================================</span>
<span class="line-removed">3894 </span>
<span class="line-removed">3895 instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">3896   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">3897   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3898   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">3899   format %{ &quot;vector_add2F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3900   ins_encode %{</span>
<span class="line-removed">3901     if (UseAVX &gt; 0) {</span>
<span class="line-removed">3902       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3903       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3904       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3905     } else {</span>
<span class="line-removed">3906       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3907       __ addss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3908       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3909       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3910     }</span>
<span class="line-removed">3911   %}</span>
<span class="line-removed">3912   ins_pipe( pipe_slow );</span>
<span class="line-removed">3913 %}</span>
<span class="line-removed">3914 </span>
<span class="line-removed">3915 instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">3916   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">3917   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3918   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">3919   format %{ &quot;vector_add4F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3920   ins_encode %{</span>
<span class="line-removed">3921     if (UseAVX &gt; 0) {</span>
<span class="line-removed">3922       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3923       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3924       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3925       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3926       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3927       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3928       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3929     } else {</span>
<span class="line-removed">3930       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3931       __ addss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3932       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3933       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3934       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3935       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3936       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3937       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3938     }</span>
<span class="line-removed">3939   %}</span>
<span class="line-removed">3940   ins_pipe( pipe_slow );</span>
<span class="line-removed">3941 %}</span>
<span class="line-removed">3942 </span>
<span class="line-removed">3943 </span>
<span class="line-removed">3944 instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3945   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">3946   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3947   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">3948   format %{ &quot;vector_add8F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3949   ins_encode %{</span>
<span class="line-removed">3950     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3951     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3952     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3953     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3954     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3955     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3956     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3957     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3958     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3959     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3960     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3961     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3962     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3963     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3964     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3965     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3966   %}</span>
<span class="line-removed">3967   ins_pipe( pipe_slow );</span>
<span class="line-removed">3968 %}</span>
<span class="line-removed">3969 </span>
<span class="line-removed">3970 instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">3971   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">3972   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3973   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">3974   format %{ &quot;vector_add16F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3975   ins_encode %{</span>
<span class="line-removed">3976     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3977     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3978     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3979     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3980     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3981     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3982     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3983     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3984     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">3985     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3986     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3987     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3988     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3989     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3990     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3991     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3992     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">3993     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3994     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3995     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3996     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3997     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3998     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3999     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4000     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4001     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4002     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4003     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4004     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4005     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4006     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4007     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4008   %}</span>
<span class="line-removed">4009   ins_pipe( pipe_slow );</span>
<span class="line-removed">4010 %}</span>
<span class="line-removed">4011 </span>
<span class="line-removed">4012 // =======================AddReductionVD==========================================</span>
<span class="line-removed">4013 </span>
<span class="line-removed">4014 instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4015   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4016   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4017   effect(TEMP tmp, TEMP dst);</span>
<span class="line-removed">4018   format %{ &quot;vector_add2D_reduction  $dst,$src2&quot; %}</span>
<span class="line-removed">4019   ins_encode %{</span>
<span class="line-removed">4020     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4021       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4022       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4023       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4024     } else {</span>
<span class="line-removed">4025       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4026       __ addsd($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4027       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4028       __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4029     }</span>
<span class="line-removed">4030   %}</span>
<span class="line-removed">4031   ins_pipe( pipe_slow );</span>
<span class="line-removed">4032 %}</span>
4033 
<a name="80" id="anc80"></a><span class="line-modified">4034 instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4035   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-modified">4036   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-modified">4037   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4038   format %{ &quot;vector_add4D_reduction $dst,$dst,$src2&quot; %}</span>

4039   ins_encode %{
<a name="81" id="anc81"></a><span class="line-modified">4040     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4041     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4042     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4043     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4044     __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4045     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4046     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4047     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4048   %}</span>
<span class="line-removed">4049   ins_pipe( pipe_slow );</span>
<span class="line-removed">4050 %}</span>
<span class="line-removed">4051 </span>
<span class="line-removed">4052 instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">4053   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">4054   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4055   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4056   format %{ &quot;vector_add8D_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4057   ins_encode %{</span>
<span class="line-removed">4058     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4059     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4060     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4061     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4062     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4063     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4064     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4065     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4066     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4067     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4068     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4069     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4070     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4071     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4072     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4073     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4074   %}</span>
<span class="line-removed">4075   ins_pipe( pipe_slow );</span>
<span class="line-removed">4076 %}</span>
<span class="line-removed">4077 </span>
<span class="line-removed">4078 // =======================MulReductionVI==========================================</span>
<span class="line-removed">4079 </span>
<span class="line-removed">4080 instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4081   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4082   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4083   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4084   format %{ &quot;vector_mul2I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4085   ins_encode %{</span>
<span class="line-removed">4086     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4087       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4088       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4089       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4090       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4091       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4092       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4093     } else {</span>
<span class="line-removed">4094       assert(UseSSE &gt; 3, &quot;required&quot;);</span>
<span class="line-removed">4095       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4096       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4097       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4098       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4099       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4100     }</span>
<span class="line-removed">4101   %}</span>
<span class="line-removed">4102   ins_pipe( pipe_slow );</span>
<span class="line-removed">4103 %}</span>
<span class="line-removed">4104 </span>
<span class="line-removed">4105 instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4106   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4107   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4108   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4109   format %{ &quot;vector_mul4I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4110   ins_encode %{</span>
<span class="line-removed">4111     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4112       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4113       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4114       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4115       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4116       __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4117       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4118       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4119       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4120     } else {</span>
<span class="line-removed">4121       assert(UseSSE &gt; 3, &quot;required&quot;);</span>
<span class="line-removed">4122       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4123       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4124       __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);</span>
<span class="line-removed">4125       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4126       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4127       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4128       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4129     }</span>
<span class="line-removed">4130   %}</span>
<span class="line-removed">4131   ins_pipe( pipe_slow );</span>
<span class="line-removed">4132 %}</span>
<span class="line-removed">4133 </span>
<span class="line-removed">4134 instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4135   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">4136   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4137   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4138   format %{ &quot;vector_mul8I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4139   ins_encode %{</span>
<span class="line-removed">4140     assert(UseAVX &gt; 1, &quot;required&quot;);</span>
<span class="line-removed">4141     int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4142     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4143     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">4144     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">4145     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4146     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4147     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4148     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4149     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4150     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4151   %}</span>
<span class="line-removed">4152   ins_pipe( pipe_slow );</span>
<span class="line-removed">4153 %}</span>
<span class="line-removed">4154 </span>
<span class="line-removed">4155 instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{</span>
<span class="line-removed">4156   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">4157   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4158   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);</span>
<span class="line-removed">4159   format %{ &quot;vector_mul16I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4160   ins_encode %{</span>
<span class="line-removed">4161     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4162     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4163     __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">4164     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);</span>
<span class="line-removed">4165     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);</span>
<span class="line-removed">4166     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">4167     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4168     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4169     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4170     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4171     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4172     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4173   %}</span>
<span class="line-removed">4174   ins_pipe( pipe_slow );</span>
<span class="line-removed">4175 %}</span>
<span class="line-removed">4176 </span>
<span class="line-removed">4177 // =======================MulReductionVL==========================================</span>
<span class="line-removed">4178 </span>
<span class="line-removed">4179 #ifdef _LP64</span>
<span class="line-removed">4180 instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4181   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4182   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-removed">4183   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4184   format %{ &quot;vector_mul2L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4185   ins_encode %{</span>
<span class="line-removed">4186     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-removed">4187     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4188     __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4189     __ movdq($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4190     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4191     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4192   %}</span>
<span class="line-removed">4193   ins_pipe( pipe_slow );</span>
<span class="line-removed">4194 %}</span>
<span class="line-removed">4195 </span>
<span class="line-removed">4196 instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4197   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4198   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-removed">4199   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4200   format %{ &quot;vector_mul4L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4201   ins_encode %{</span>
<span class="line-removed">4202     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-removed">4203     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4204     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);</span>
<span class="line-removed">4205     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4206     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4207     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4208     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4209     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
4210   %}
4211   ins_pipe( pipe_slow );
4212 %}
4213 
<a name="82" id="anc82"></a><span class="line-modified">4214 instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">4215   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-modified">4216   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">4217   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">4218   format %{ &quot;vector_mul8L_reduction $dst,$src1,$src2&quot; %}</span>

4219   ins_encode %{
<a name="83" id="anc83"></a><span class="line-modified">4220     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-modified">4221     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4222     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">4223     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4224     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4225     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4226     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4227     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4228     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4229     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
4230   %}
4231   ins_pipe( pipe_slow );
4232 %}
<a name="84" id="anc84"></a><span class="line-removed">4233 #endif</span>
<span class="line-removed">4234 </span>
<span class="line-removed">4235 // =======================MulReductionVF==========================================</span>
4236 
<a name="85" id="anc85"></a><span class="line-modified">4237 instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-modified">4238   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-modified">4239   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-modified">4240   effect(TEMP dst, TEMP tmp);</span>
<span class="line-modified">4241   format %{ &quot;vector_mul2F_reduction $dst,$dst,$src2&quot; %}</span>

4242   ins_encode %{
<a name="86" id="anc86"></a><span class="line-modified">4243     if (UseAVX &gt; 0) {</span>
<span class="line-modified">4244       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4245       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4246       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4247     } else {</span>
<span class="line-removed">4248       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4249       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4250       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4251       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4252     }</span>
4253   %}
4254   ins_pipe( pipe_slow );
4255 %}
4256 
<a name="87" id="anc87"></a><span class="line-modified">4257 instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4258   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4259   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-removed">4260   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">4261   format %{ &quot;vector_mul4F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4262   ins_encode %{</span>
<span class="line-removed">4263     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4264       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4265       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4266       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4267       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4268       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4269       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4270       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4271     } else {</span>
<span class="line-removed">4272       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4273       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4274       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4275       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4276       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4277       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4278       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4279       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4280     }</span>
<span class="line-removed">4281   %}</span>
<span class="line-removed">4282   ins_pipe( pipe_slow );</span>
<span class="line-removed">4283 %}</span>
4284 
<a name="88" id="anc88"></a><span class="line-modified">4285 instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4286   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-modified">4287   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-modified">4288   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4289   format %{ &quot;vector_mul8F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-modified">4290   ins_encode %{</span>
<span class="line-removed">4291     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4292     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4293     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4294     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4295     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4296     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4297     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4298     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4299     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4300     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4301     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4302     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4303     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4304     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4305     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4306     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4307   %}</span>
<span class="line-removed">4308   ins_pipe( pipe_slow );</span>
<span class="line-removed">4309 %}</span>
<span class="line-removed">4310 </span>
<span class="line-removed">4311 instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">4312   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">4313   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-removed">4314   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4315   format %{ &quot;vector_mul16F_reduction $dst,$dst,$src2&quot; %}</span>
4316   ins_encode %{
<a name="89" id="anc89"></a><span class="line-modified">4317     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">4318     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4319     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4320     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4321     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4322     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4323     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4324     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4325     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4326     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4327     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4328     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4329     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4330     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4331     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4332     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4333     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4334     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4335     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4336     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4337     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4338     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4339     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4340     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4341     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4342     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4343     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4344     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4345     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4346     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4347     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4348     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4349   %}</span>
<span class="line-removed">4350   ins_pipe( pipe_slow );</span>
<span class="line-removed">4351 %}</span>
<span class="line-removed">4352 </span>
<span class="line-removed">4353 // =======================MulReductionVD==========================================</span>
<span class="line-removed">4354 </span>
<span class="line-removed">4355 instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4356   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4357   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-removed">4358   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">4359   format %{ &quot;vector_mul2D_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4360   ins_encode %{</span>
<span class="line-removed">4361     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4362       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4363       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4364       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4365     } else {</span>
<span class="line-removed">4366       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4367       __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4368       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4369       __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4370     }</span>
4371   %}
4372   ins_pipe( pipe_slow );
4373 %}
4374 
<a name="90" id="anc90"></a><span class="line-modified">4375 </span>
<span class="line-modified">4376 instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4377   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 2</span>
<span class="line-modified">4378   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-modified">4379   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4380   format %{ &quot;vector_mul4D_reduction  $dst,$dst,$src2&quot; %}</span>
4381   ins_encode %{
<a name="91" id="anc91"></a><span class="line-modified">4382     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4383     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4384     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4385     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4386     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4387     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4388     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4389     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
4390   %}
4391   ins_pipe( pipe_slow );
4392 %}
4393 
<a name="92" id="anc92"></a><span class="line-modified">4394 instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">4395   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 2</span>
<span class="line-modified">4396   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-modified">4397   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4398   format %{ &quot;vector_mul8D_reduction $dst,$dst,$src2&quot; %}</span>

4399   ins_encode %{
<a name="93" id="anc93"></a><span class="line-modified">4400     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4401     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4402     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4403     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4404     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4405     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4406     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4407     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4408     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4409     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4410     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4411     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4412     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4413     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4414     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4415     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
4416   %}
4417   ins_pipe( pipe_slow );
4418 %}
4419 
4420 // ====================VECTOR ARITHMETIC=======================================
4421 
4422 // --------------------------------- ADD --------------------------------------
4423 
4424 // Bytes vector add
4425 instruct vaddB(vec dst, vec src) %{
4426   predicate(UseAVX == 0);
4427   match(Set dst (AddVB dst src));
4428   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
4429   ins_encode %{
4430     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
4431   %}
4432   ins_pipe( pipe_slow );
4433 %}
4434 
4435 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
4436   predicate(UseAVX &gt; 0);
4437   match(Set dst (AddVB src1 src2));
4438   format %{ &quot;vpaddb  $dst,$src1,$src2\t! add packedB&quot; %}
4439   ins_encode %{
4440     int vector_len = vector_length_encoding(this);
4441     __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4442   %}
4443   ins_pipe( pipe_slow );
4444 %}
4445 
4446 instruct vaddB_mem(vec dst, vec src, memory mem) %{
4447   predicate(UseAVX &gt; 0);
4448   match(Set dst (AddVB src (LoadVector mem)));
4449   format %{ &quot;vpaddb  $dst,$src,$mem\t! add packedB&quot; %}
4450   ins_encode %{
4451     int vector_len = vector_length_encoding(this);
4452     __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4453   %}
4454   ins_pipe( pipe_slow );
4455 %}
4456 
4457 // Shorts/Chars vector add
4458 instruct vaddS(vec dst, vec src) %{
4459   predicate(UseAVX == 0);
4460   match(Set dst (AddVS dst src));
4461   format %{ &quot;paddw   $dst,$src\t! add packedS&quot; %}
4462   ins_encode %{
4463     __ paddw($dst$$XMMRegister, $src$$XMMRegister);
4464   %}
4465   ins_pipe( pipe_slow );
4466 %}
4467 
4468 instruct vaddS_reg(vec dst, vec src1, vec src2) %{
4469   predicate(UseAVX &gt; 0);
4470   match(Set dst (AddVS src1 src2));
4471   format %{ &quot;vpaddw  $dst,$src1,$src2\t! add packedS&quot; %}
4472   ins_encode %{
4473     int vector_len = vector_length_encoding(this);
4474     __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4475   %}
4476   ins_pipe( pipe_slow );
4477 %}
4478 
4479 instruct vaddS_mem(vec dst, vec src, memory mem) %{
4480   predicate(UseAVX &gt; 0);
4481   match(Set dst (AddVS src (LoadVector mem)));
4482   format %{ &quot;vpaddw  $dst,$src,$mem\t! add packedS&quot; %}
4483   ins_encode %{
4484     int vector_len = vector_length_encoding(this);
4485     __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4486   %}
4487   ins_pipe( pipe_slow );
4488 %}
4489 
4490 // Integers vector add
4491 instruct vaddI(vec dst, vec src) %{
4492   predicate(UseAVX == 0);
4493   match(Set dst (AddVI dst src));
4494   format %{ &quot;paddd   $dst,$src\t! add packedI&quot; %}
4495   ins_encode %{
4496     __ paddd($dst$$XMMRegister, $src$$XMMRegister);
4497   %}
4498   ins_pipe( pipe_slow );
4499 %}
4500 
4501 instruct vaddI_reg(vec dst, vec src1, vec src2) %{
4502   predicate(UseAVX &gt; 0);
4503   match(Set dst (AddVI src1 src2));
4504   format %{ &quot;vpaddd  $dst,$src1,$src2\t! add packedI&quot; %}
4505   ins_encode %{
4506     int vector_len = vector_length_encoding(this);
4507     __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4508   %}
4509   ins_pipe( pipe_slow );
4510 %}
4511 
4512 
4513 instruct vaddI_mem(vec dst, vec src, memory mem) %{
4514   predicate(UseAVX &gt; 0);
4515   match(Set dst (AddVI src (LoadVector mem)));
4516   format %{ &quot;vpaddd  $dst,$src,$mem\t! add packedI&quot; %}
4517   ins_encode %{
4518     int vector_len = vector_length_encoding(this);
4519     __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4520   %}
4521   ins_pipe( pipe_slow );
4522 %}
4523 
4524 // Longs vector add
4525 instruct vaddL(vec dst, vec src) %{
4526   predicate(UseAVX == 0);
4527   match(Set dst (AddVL dst src));
4528   format %{ &quot;paddq   $dst,$src\t! add packedL&quot; %}
4529   ins_encode %{
4530     __ paddq($dst$$XMMRegister, $src$$XMMRegister);
4531   %}
4532   ins_pipe( pipe_slow );
4533 %}
4534 
4535 instruct vaddL_reg(vec dst, vec src1, vec src2) %{
4536   predicate(UseAVX &gt; 0);
4537   match(Set dst (AddVL src1 src2));
4538   format %{ &quot;vpaddq  $dst,$src1,$src2\t! add packedL&quot; %}
4539   ins_encode %{
4540     int vector_len = vector_length_encoding(this);
4541     __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4542   %}
4543   ins_pipe( pipe_slow );
4544 %}
4545 
4546 instruct vaddL_mem(vec dst, vec src, memory mem) %{
4547   predicate(UseAVX &gt; 0);
4548   match(Set dst (AddVL src (LoadVector mem)));
4549   format %{ &quot;vpaddq  $dst,$src,$mem\t! add packedL&quot; %}
4550   ins_encode %{
4551     int vector_len = vector_length_encoding(this);
4552     __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4553   %}
4554   ins_pipe( pipe_slow );
4555 %}
4556 
4557 // Floats vector add
4558 instruct vaddF(vec dst, vec src) %{
4559   predicate(UseAVX == 0);
4560   match(Set dst (AddVF dst src));
4561   format %{ &quot;addps   $dst,$src\t! add packedF&quot; %}
4562   ins_encode %{
4563     __ addps($dst$$XMMRegister, $src$$XMMRegister);
4564   %}
4565   ins_pipe( pipe_slow );
4566 %}
4567 
4568 instruct vaddF_reg(vec dst, vec src1, vec src2) %{
4569   predicate(UseAVX &gt; 0);
4570   match(Set dst (AddVF src1 src2));
4571   format %{ &quot;vaddps  $dst,$src1,$src2\t! add packedF&quot; %}
4572   ins_encode %{
4573     int vector_len = vector_length_encoding(this);
4574     __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4575   %}
4576   ins_pipe( pipe_slow );
4577 %}
4578 
4579 instruct vaddF_mem(vec dst, vec src, memory mem) %{
4580   predicate(UseAVX &gt; 0);
4581   match(Set dst (AddVF src (LoadVector mem)));
4582   format %{ &quot;vaddps  $dst,$src,$mem\t! add packedF&quot; %}
4583   ins_encode %{
4584     int vector_len = vector_length_encoding(this);
4585     __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4586   %}
4587   ins_pipe( pipe_slow );
4588 %}
4589 
4590 // Doubles vector add
4591 instruct vaddD(vec dst, vec src) %{
4592   predicate(UseAVX == 0);
4593   match(Set dst (AddVD dst src));
4594   format %{ &quot;addpd   $dst,$src\t! add packedD&quot; %}
4595   ins_encode %{
4596     __ addpd($dst$$XMMRegister, $src$$XMMRegister);
4597   %}
4598   ins_pipe( pipe_slow );
4599 %}
4600 
4601 instruct vaddD_reg(vec dst, vec src1, vec src2) %{
4602   predicate(UseAVX &gt; 0);
4603   match(Set dst (AddVD src1 src2));
4604   format %{ &quot;vaddpd  $dst,$src1,$src2\t! add packedD&quot; %}
4605   ins_encode %{
4606     int vector_len = vector_length_encoding(this);
4607     __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4608   %}
4609   ins_pipe( pipe_slow );
4610 %}
4611 
4612 instruct vaddD_mem(vec dst, vec src, memory mem) %{
4613   predicate(UseAVX &gt; 0);
4614   match(Set dst (AddVD src (LoadVector mem)));
4615   format %{ &quot;vaddpd  $dst,$src,$mem\t! add packedD&quot; %}
4616   ins_encode %{
4617     int vector_len = vector_length_encoding(this);
4618     __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4619   %}
4620   ins_pipe( pipe_slow );
4621 %}
4622 
4623 // --------------------------------- SUB --------------------------------------
4624 
4625 // Bytes vector sub
4626 instruct vsubB(vec dst, vec src) %{
4627   predicate(UseAVX == 0);
4628   match(Set dst (SubVB dst src));
4629   format %{ &quot;psubb   $dst,$src\t! sub packedB&quot; %}
4630   ins_encode %{
4631     __ psubb($dst$$XMMRegister, $src$$XMMRegister);
4632   %}
4633   ins_pipe( pipe_slow );
4634 %}
4635 
4636 instruct vsubB_reg(vec dst, vec src1, vec src2) %{
4637   predicate(UseAVX &gt; 0);
4638   match(Set dst (SubVB src1 src2));
4639   format %{ &quot;vpsubb  $dst,$src1,$src2\t! sub packedB&quot; %}
4640   ins_encode %{
4641     int vector_len = vector_length_encoding(this);
4642     __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4643   %}
4644   ins_pipe( pipe_slow );
4645 %}
4646 
4647 instruct vsubB_mem(vec dst, vec src, memory mem) %{
4648   predicate(UseAVX &gt; 0);
4649   match(Set dst (SubVB src (LoadVector mem)));
4650   format %{ &quot;vpsubb  $dst,$src,$mem\t! sub packedB&quot; %}
4651   ins_encode %{
4652     int vector_len = vector_length_encoding(this);
4653     __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4654   %}
4655   ins_pipe( pipe_slow );
4656 %}
4657 
4658 // Shorts/Chars vector sub
4659 instruct vsubS(vec dst, vec src) %{
4660   predicate(UseAVX == 0);
4661   match(Set dst (SubVS dst src));
4662   format %{ &quot;psubw   $dst,$src\t! sub packedS&quot; %}
4663   ins_encode %{
4664     __ psubw($dst$$XMMRegister, $src$$XMMRegister);
4665   %}
4666   ins_pipe( pipe_slow );
4667 %}
4668 
4669 
4670 instruct vsubS_reg(vec dst, vec src1, vec src2) %{
4671   predicate(UseAVX &gt; 0);
4672   match(Set dst (SubVS src1 src2));
4673   format %{ &quot;vpsubw  $dst,$src1,$src2\t! sub packedS&quot; %}
4674   ins_encode %{
4675     int vector_len = vector_length_encoding(this);
4676     __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4677   %}
4678   ins_pipe( pipe_slow );
4679 %}
4680 
4681 instruct vsubS_mem(vec dst, vec src, memory mem) %{
4682   predicate(UseAVX &gt; 0);
4683   match(Set dst (SubVS src (LoadVector mem)));
4684   format %{ &quot;vpsubw  $dst,$src,$mem\t! sub packedS&quot; %}
4685   ins_encode %{
4686     int vector_len = vector_length_encoding(this);
4687     __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4688   %}
4689   ins_pipe( pipe_slow );
4690 %}
4691 
4692 // Integers vector sub
4693 instruct vsubI(vec dst, vec src) %{
4694   predicate(UseAVX == 0);
4695   match(Set dst (SubVI dst src));
4696   format %{ &quot;psubd   $dst,$src\t! sub packedI&quot; %}
4697   ins_encode %{
4698     __ psubd($dst$$XMMRegister, $src$$XMMRegister);
4699   %}
4700   ins_pipe( pipe_slow );
4701 %}
4702 
4703 instruct vsubI_reg(vec dst, vec src1, vec src2) %{
4704   predicate(UseAVX &gt; 0);
4705   match(Set dst (SubVI src1 src2));
4706   format %{ &quot;vpsubd  $dst,$src1,$src2\t! sub packedI&quot; %}
4707   ins_encode %{
4708     int vector_len = vector_length_encoding(this);
4709     __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4710   %}
4711   ins_pipe( pipe_slow );
4712 %}
4713 
4714 instruct vsubI_mem(vec dst, vec src, memory mem) %{
4715   predicate(UseAVX &gt; 0);
4716   match(Set dst (SubVI src (LoadVector mem)));
4717   format %{ &quot;vpsubd  $dst,$src,$mem\t! sub packedI&quot; %}
4718   ins_encode %{
4719     int vector_len = vector_length_encoding(this);
4720     __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4721   %}
4722   ins_pipe( pipe_slow );
4723 %}
4724 
4725 // Longs vector sub
4726 instruct vsubL(vec dst, vec src) %{
4727   predicate(UseAVX == 0);
4728   match(Set dst (SubVL dst src));
4729   format %{ &quot;psubq   $dst,$src\t! sub packedL&quot; %}
4730   ins_encode %{
4731     __ psubq($dst$$XMMRegister, $src$$XMMRegister);
4732   %}
4733   ins_pipe( pipe_slow );
4734 %}
4735 
4736 instruct vsubL_reg(vec dst, vec src1, vec src2) %{
4737   predicate(UseAVX &gt; 0);
4738   match(Set dst (SubVL src1 src2));
4739   format %{ &quot;vpsubq  $dst,$src1,$src2\t! sub packedL&quot; %}
4740   ins_encode %{
4741     int vector_len = vector_length_encoding(this);
4742     __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4743   %}
4744   ins_pipe( pipe_slow );
4745 %}
4746 
4747 
4748 instruct vsubL_mem(vec dst, vec src, memory mem) %{
4749   predicate(UseAVX &gt; 0);
4750   match(Set dst (SubVL src (LoadVector mem)));
4751   format %{ &quot;vpsubq  $dst,$src,$mem\t! sub packedL&quot; %}
4752   ins_encode %{
4753     int vector_len = vector_length_encoding(this);
4754     __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4755   %}
4756   ins_pipe( pipe_slow );
4757 %}
4758 
4759 // Floats vector sub
4760 instruct vsubF(vec dst, vec src) %{
4761   predicate(UseAVX == 0);
4762   match(Set dst (SubVF dst src));
4763   format %{ &quot;subps   $dst,$src\t! sub packedF&quot; %}
4764   ins_encode %{
4765     __ subps($dst$$XMMRegister, $src$$XMMRegister);
4766   %}
4767   ins_pipe( pipe_slow );
4768 %}
4769 
4770 instruct vsubF_reg(vec dst, vec src1, vec src2) %{
4771   predicate(UseAVX &gt; 0);
4772   match(Set dst (SubVF src1 src2));
4773   format %{ &quot;vsubps  $dst,$src1,$src2\t! sub packedF&quot; %}
4774   ins_encode %{
4775     int vector_len = vector_length_encoding(this);
4776     __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4777   %}
4778   ins_pipe( pipe_slow );
4779 %}
4780 
4781 instruct vsubF_mem(vec dst, vec src, memory mem) %{
4782   predicate(UseAVX &gt; 0);
4783   match(Set dst (SubVF src (LoadVector mem)));
4784   format %{ &quot;vsubps  $dst,$src,$mem\t! sub packedF&quot; %}
4785   ins_encode %{
4786     int vector_len = vector_length_encoding(this);
4787     __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4788   %}
4789   ins_pipe( pipe_slow );
4790 %}
4791 
4792 // Doubles vector sub
4793 instruct vsubD(vec dst, vec src) %{
4794   predicate(UseAVX == 0);
4795   match(Set dst (SubVD dst src));
4796   format %{ &quot;subpd   $dst,$src\t! sub packedD&quot; %}
4797   ins_encode %{
4798     __ subpd($dst$$XMMRegister, $src$$XMMRegister);
4799   %}
4800   ins_pipe( pipe_slow );
4801 %}
4802 
4803 instruct vsubD_reg(vec dst, vec src1, vec src2) %{
4804   predicate(UseAVX &gt; 0);
4805   match(Set dst (SubVD src1 src2));
4806   format %{ &quot;vsubpd  $dst,$src1,$src2\t! sub packedD&quot; %}
4807   ins_encode %{
4808     int vector_len = vector_length_encoding(this);
4809     __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4810   %}
4811   ins_pipe( pipe_slow );
4812 %}
4813 
4814 instruct vsubD_mem(vec dst, vec src, memory mem) %{
4815   predicate(UseAVX &gt; 0);
4816   match(Set dst (SubVD src (LoadVector mem)));
4817   format %{ &quot;vsubpd  $dst,$src,$mem\t! sub packedD&quot; %}
4818   ins_encode %{
4819     int vector_len = vector_length_encoding(this);
4820     __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4821   %}
4822   ins_pipe( pipe_slow );
4823 %}
4824 
4825 // --------------------------------- MUL --------------------------------------
4826 
4827 // Byte vector mul
4828 instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4829   predicate(n-&gt;as_Vector()-&gt;length() == 4 ||
4830             n-&gt;as_Vector()-&gt;length() == 8);
4831   match(Set dst (MulVB src1 src2));
4832   effect(TEMP dst, TEMP tmp, TEMP scratch);
4833   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4834   ins_encode %{
4835     assert(UseSSE &gt; 3, &quot;required&quot;);
4836     __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);
4837     __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);
4838     __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);
4839     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4840     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4841     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4842   %}
4843   ins_pipe( pipe_slow );
4844 %}
4845 
4846 instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4847   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4848   match(Set dst (MulVB src1 src2));
4849   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4850   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4851   ins_encode %{
4852     assert(UseSSE &gt; 3, &quot;required&quot;);
4853     __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);
4854     __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);
4855     __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);
4856     __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);
4857     __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);
4858     __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);
4859     __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);
4860     __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);
4861     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4862     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
4863     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
4864     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
4865   %}
4866   ins_pipe( pipe_slow );
4867 %}
4868 
4869 instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4870   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
4871   match(Set dst (MulVB src1 src2));
4872   effect(TEMP dst, TEMP tmp, TEMP scratch);
4873   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4874   ins_encode %{
4875   int vector_len = Assembler::AVX_256bit;
4876     __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vector_len);
4877     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4878     __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vector_len);
4879     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4880     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
4881     __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);
4882     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);
4883   %}
4884   ins_pipe( pipe_slow );
4885 %}
4886 
4887 instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4888   predicate(n-&gt;as_Vector()-&gt;length() == 32);
4889   match(Set dst (MulVB src1 src2));
4890   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4891   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4892   ins_encode %{
4893     assert(UseAVX &gt; 1, &quot;required&quot;);
4894     int vector_len = Assembler::AVX_256bit;
4895     __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4896     __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);
4897     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4898     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4899     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4900     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4901     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4902     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4903     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4904     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4905     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4906     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4907     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4908     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
4909   %}
4910   ins_pipe( pipe_slow );
4911 %}
4912 
4913 instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4914   predicate(n-&gt;as_Vector()-&gt;length() == 64);
4915   match(Set dst (MulVB src1 src2));
4916   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4917   format %{&quot;vector_mulB $dst,$src1,$src2\n\t&quot; %}
4918   ins_encode %{
4919     assert(UseAVX &gt; 2, &quot;required&quot;);
4920     int vector_len = Assembler::AVX_512bit;
4921     __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4922     __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);
4923     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4924     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4925     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4926     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4927     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4928     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4929     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4930     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4931     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4932     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4933     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4934     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
4935     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4936   %}
4937   ins_pipe( pipe_slow );
4938 %}
4939 
4940 // Shorts/Chars vector mul
4941 instruct vmulS(vec dst, vec src) %{
4942   predicate(UseAVX == 0);
4943   match(Set dst (MulVS dst src));
4944   format %{ &quot;pmullw $dst,$src\t! mul packedS&quot; %}
4945   ins_encode %{
4946     __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
4947   %}
4948   ins_pipe( pipe_slow );
4949 %}
4950 
4951 instruct vmulS_reg(vec dst, vec src1, vec src2) %{
4952   predicate(UseAVX &gt; 0);
4953   match(Set dst (MulVS src1 src2));
4954   format %{ &quot;vpmullw $dst,$src1,$src2\t! mul packedS&quot; %}
4955   ins_encode %{
4956     int vector_len = vector_length_encoding(this);
4957     __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4958   %}
4959   ins_pipe( pipe_slow );
4960 %}
4961 
4962 instruct vmulS_mem(vec dst, vec src, memory mem) %{
4963   predicate(UseAVX &gt; 0);
4964   match(Set dst (MulVS src (LoadVector mem)));
4965   format %{ &quot;vpmullw $dst,$src,$mem\t! mul packedS&quot; %}
4966   ins_encode %{
4967     int vector_len = vector_length_encoding(this);
4968     __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4969   %}
4970   ins_pipe( pipe_slow );
4971 %}
4972 
4973 // Integers vector mul
4974 instruct vmulI(vec dst, vec src) %{
4975   predicate(UseAVX == 0);
4976   match(Set dst (MulVI dst src));
4977   format %{ &quot;pmulld  $dst,$src\t! mul packedI&quot; %}
4978   ins_encode %{
4979     assert(UseSSE &gt; 3, &quot;required&quot;);
4980     __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
4981   %}
4982   ins_pipe( pipe_slow );
4983 %}
4984 
4985 instruct vmulI_reg(vec dst, vec src1, vec src2) %{
4986   predicate(UseAVX &gt; 0);
4987   match(Set dst (MulVI src1 src2));
4988   format %{ &quot;vpmulld $dst,$src1,$src2\t! mul packedI&quot; %}
4989   ins_encode %{
4990     int vector_len = vector_length_encoding(this);
4991     __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4992   %}
4993   ins_pipe( pipe_slow );
4994 %}
4995 
4996 instruct vmulI_mem(vec dst, vec src, memory mem) %{
4997   predicate(UseAVX &gt; 0);
4998   match(Set dst (MulVI src (LoadVector mem)));
4999   format %{ &quot;vpmulld $dst,$src,$mem\t! mul packedI&quot; %}
5000   ins_encode %{
5001     int vector_len = vector_length_encoding(this);
5002     __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5003   %}
5004   ins_pipe( pipe_slow );
5005 %}
5006 
5007 // Longs vector mul
5008 instruct vmulL_reg(vec dst, vec src1, vec src2) %{
5009   match(Set dst (MulVL src1 src2));
5010   format %{ &quot;vpmullq $dst,$src1,$src2\t! mul packedL&quot; %}
5011   ins_encode %{
5012     assert(UseAVX &gt; 2, &quot;required&quot;);
5013     int vector_len = vector_length_encoding(this);
5014     __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5015   %}
5016   ins_pipe( pipe_slow );
5017 %}
5018 
5019 instruct vmulL_mem(vec dst, vec src, memory mem) %{
5020   match(Set dst (MulVL src (LoadVector mem)));
5021   format %{ &quot;vpmullq $dst,$src,$mem\t! mul packedL&quot; %}
5022   ins_encode %{
5023     assert(UseAVX &gt; 2, &quot;required&quot;);
5024     int vector_len = vector_length_encoding(this);
5025     __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5026   %}
5027   ins_pipe( pipe_slow );
5028 %}
5029 
5030 // Floats vector mul
5031 instruct vmulF(vec dst, vec src) %{
5032   predicate(UseAVX == 0);
5033   match(Set dst (MulVF dst src));
5034   format %{ &quot;mulps   $dst,$src\t! mul packedF&quot; %}
5035   ins_encode %{
5036     __ mulps($dst$$XMMRegister, $src$$XMMRegister);
5037   %}
5038   ins_pipe( pipe_slow );
5039 %}
5040 
5041 instruct vmulF_reg(vec dst, vec src1, vec src2) %{
5042   predicate(UseAVX &gt; 0);
5043   match(Set dst (MulVF src1 src2));
5044   format %{ &quot;vmulps  $dst,$src1,$src2\t! mul packedF&quot; %}
5045   ins_encode %{
5046     int vector_len = vector_length_encoding(this);
5047     __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5048   %}
5049   ins_pipe( pipe_slow );
5050 %}
5051 
5052 instruct vmulF_mem(vec dst, vec src, memory mem) %{
5053   predicate(UseAVX &gt; 0);
5054   match(Set dst (MulVF src (LoadVector mem)));
5055   format %{ &quot;vmulps  $dst,$src,$mem\t! mul packedF&quot; %}
5056   ins_encode %{
5057     int vector_len = vector_length_encoding(this);
5058     __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5059   %}
5060   ins_pipe( pipe_slow );
5061 %}
5062 
5063 // Doubles vector mul
5064 instruct vmulD(vec dst, vec src) %{
5065   predicate(UseAVX == 0);
5066   match(Set dst (MulVD dst src));
5067   format %{ &quot;mulpd   $dst,$src\t! mul packedD&quot; %}
5068   ins_encode %{
5069     __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
5070   %}
5071   ins_pipe( pipe_slow );
5072 %}
5073 
5074 instruct vmulD_reg(vec dst, vec src1, vec src2) %{
5075   predicate(UseAVX &gt; 0);
5076   match(Set dst (MulVD src1 src2));
5077   format %{ &quot;vmulpd  $dst,$src1,$src2\t! mul packedD&quot; %}
5078   ins_encode %{
5079     int vector_len = vector_length_encoding(this);
5080     __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5081   %}
5082   ins_pipe( pipe_slow );
5083 %}
5084 
5085 instruct vmulD_mem(vec dst, vec src, memory mem) %{
5086   predicate(UseAVX &gt; 0);
5087   match(Set dst (MulVD src (LoadVector mem)));
5088   format %{ &quot;vmulpd  $dst,$src,$mem\t! mul packedD&quot; %}
5089   ins_encode %{
5090     int vector_len = vector_length_encoding(this);
5091     __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5092   %}
5093   ins_pipe( pipe_slow );
5094 %}
5095 
5096 instruct vcmov8F_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
5097   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 8);
5098   match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
5099   effect(TEMP dst, USE src1, USE src2);
5100   format %{ &quot;cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t&quot;
5101             &quot;blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t&quot;
5102          %}
5103   ins_encode %{
5104     int vector_len = 1;
5105     int cond = (Assembler::Condition)($copnd$$cmpcode);
5106     __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
5107     __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
5108   %}
5109   ins_pipe( pipe_slow );
5110 %}
5111 
5112 instruct vcmov4D_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
5113   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 4);
5114   match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
5115   effect(TEMP dst, USE src1, USE src2);
5116   format %{ &quot;cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t&quot;
5117             &quot;blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t&quot;
5118          %}
5119   ins_encode %{
5120     int vector_len = 1;
5121     int cond = (Assembler::Condition)($copnd$$cmpcode);
5122     __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
5123     __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
5124   %}
5125   ins_pipe( pipe_slow );
5126 %}
5127 
5128 // --------------------------------- DIV --------------------------------------
5129 
5130 // Floats vector div
5131 instruct vdivF(vec dst, vec src) %{
5132   predicate(UseAVX == 0);
5133   match(Set dst (DivVF dst src));
5134   format %{ &quot;divps   $dst,$src\t! div packedF&quot; %}
5135   ins_encode %{
5136     __ divps($dst$$XMMRegister, $src$$XMMRegister);
5137   %}
5138   ins_pipe( pipe_slow );
5139 %}
5140 
5141 instruct vdivF_reg(vec dst, vec src1, vec src2) %{
5142   predicate(UseAVX &gt; 0);
5143   match(Set dst (DivVF src1 src2));
5144   format %{ &quot;vdivps  $dst,$src1,$src2\t! div packedF&quot; %}
5145   ins_encode %{
5146     int vector_len = vector_length_encoding(this);
5147     __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5148   %}
5149   ins_pipe( pipe_slow );
5150 %}
5151 
5152 instruct vdivF_mem(vec dst, vec src, memory mem) %{
5153   predicate(UseAVX &gt; 0);
5154   match(Set dst (DivVF src (LoadVector mem)));
5155   format %{ &quot;vdivps  $dst,$src,$mem\t! div packedF&quot; %}
5156   ins_encode %{
5157     int vector_len = vector_length_encoding(this);
5158     __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5159   %}
5160   ins_pipe( pipe_slow );
5161 %}
5162 
5163 // Doubles vector div
5164 instruct vdivD(vec dst, vec src) %{
5165   predicate(UseAVX == 0);
5166   match(Set dst (DivVD dst src));
5167   format %{ &quot;divpd   $dst,$src\t! div packedD&quot; %}
5168   ins_encode %{
5169     __ divpd($dst$$XMMRegister, $src$$XMMRegister);
5170   %}
5171   ins_pipe( pipe_slow );
5172 %}
5173 
5174 instruct vdivD_reg(vec dst, vec src1, vec src2) %{
5175   predicate(UseAVX &gt; 0);
5176   match(Set dst (DivVD src1 src2));
5177   format %{ &quot;vdivpd  $dst,$src1,$src2\t! div packedD&quot; %}
5178   ins_encode %{
5179     int vector_len = vector_length_encoding(this);
5180     __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5181   %}
5182   ins_pipe( pipe_slow );
5183 %}
5184 
5185 instruct vdivD_mem(vec dst, vec src, memory mem) %{
5186   predicate(UseAVX &gt; 0);
5187   match(Set dst (DivVD src (LoadVector mem)));
5188   format %{ &quot;vdivpd  $dst,$src,$mem\t! div packedD&quot; %}
5189   ins_encode %{
5190     int vector_len = vector_length_encoding(this);
5191     __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5192   %}
5193   ins_pipe( pipe_slow );
5194 %}
5195 
5196 // --------------------------------- Sqrt --------------------------------------
5197 
5198 instruct vsqrtF_reg(vec dst, vec src) %{
5199   match(Set dst (SqrtVF src));
5200   format %{ &quot;vsqrtps  $dst,$src\t! sqrt packedF&quot; %}
5201   ins_encode %{
5202     assert(UseAVX &gt; 0, &quot;required&quot;);
5203     int vector_len = vector_length_encoding(this);
5204     __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5205   %}
5206   ins_pipe( pipe_slow );
5207 %}
5208 
5209 instruct vsqrtF_mem(vec dst, memory mem) %{
5210   match(Set dst (SqrtVF (LoadVector mem)));
5211   format %{ &quot;vsqrtps  $dst,$mem\t! sqrt packedF&quot; %}
5212   ins_encode %{
5213     assert(UseAVX &gt; 0, &quot;required&quot;);
5214     int vector_len = vector_length_encoding(this);
5215     __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
5216   %}
5217   ins_pipe( pipe_slow );
5218 %}
5219 
5220 // Floating point vector sqrt
5221 instruct vsqrtD_reg(vec dst, vec src) %{
5222   match(Set dst (SqrtVD src));
5223   format %{ &quot;vsqrtpd  $dst,$src\t! sqrt packedD&quot; %}
5224   ins_encode %{
5225     assert(UseAVX &gt; 0, &quot;required&quot;);
5226     int vector_len = vector_length_encoding(this);
5227     __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5228   %}
5229   ins_pipe( pipe_slow );
5230 %}
5231 
5232 instruct vsqrtD_mem(vec dst, memory mem) %{
5233   match(Set dst (SqrtVD (LoadVector mem)));
5234   format %{ &quot;vsqrtpd  $dst,$mem\t! sqrt packedD&quot; %}
5235   ins_encode %{
5236     assert(UseAVX &gt; 0, &quot;required&quot;);
5237     int vector_len = vector_length_encoding(this);
5238     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
5239   %}
5240   ins_pipe( pipe_slow );
5241 %}
5242 
5243 // ------------------------------ Shift ---------------------------------------
5244 
5245 // Left and right shift count vectors are the same on x86
5246 // (only lowest bits of xmm reg are used for count).
5247 instruct vshiftcnt(vec dst, rRegI cnt) %{
5248   match(Set dst (LShiftCntV cnt));
5249   match(Set dst (RShiftCntV cnt));
5250   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
5251   ins_encode %{
5252     __ movdl($dst$$XMMRegister, $cnt$$Register);
5253   %}
5254   ins_pipe( pipe_slow );
5255 %}
5256 
5257 // Byte vector shift
5258 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5259   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
5260   match(Set dst (LShiftVB src shift));
5261   match(Set dst (RShiftVB src shift));
5262   match(Set dst (URShiftVB src shift));
5263   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
5264   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5265   ins_encode %{
5266     assert(UseSSE &gt; 3, &quot;required&quot;);
5267     int opcode = this-&gt;ideal_Opcode();
5268     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
5269     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
5270     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5271     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
5272     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
5273   %}
5274   ins_pipe( pipe_slow );
5275 %}
5276 
5277 instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
5278   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
5279   match(Set dst (LShiftVB src shift));
5280   match(Set dst (RShiftVB src shift));
5281   match(Set dst (URShiftVB src shift));
5282   effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);
5283   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5284   ins_encode %{
5285     assert(UseSSE &gt; 3, &quot;required&quot;);
5286     int opcode = this-&gt;ideal_Opcode();
5287 
5288     __ vextendbw(opcode, $tmp1$$XMMRegister, $src$$XMMRegister);
5289     __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
5290     __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
5291     __ vextendbw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
5292     __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
5293     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5294     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
5295     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
5296     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
5297   %}
5298   ins_pipe( pipe_slow );
5299 %}
5300 
5301 instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5302   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
5303   match(Set dst (LShiftVB src shift));
5304   match(Set dst (RShiftVB src shift));
5305   match(Set dst (URShiftVB src shift));
5306   effect(TEMP dst, TEMP tmp, TEMP scratch);
5307   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5308   ins_encode %{
5309     int opcode = this-&gt;ideal_Opcode();
5310     int vector_len = Assembler::AVX_256bit;
5311     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister, vector_len);
5312     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5313     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5314     __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
5315     __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
5316   %}
5317   ins_pipe( pipe_slow );
5318 %}
5319 
5320 instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5321   predicate(n-&gt;as_Vector()-&gt;length() == 32);
5322   match(Set dst (LShiftVB src shift));
5323   match(Set dst (RShiftVB src shift));
5324   match(Set dst (URShiftVB src shift));
5325   effect(TEMP dst, TEMP tmp, TEMP scratch);
5326   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5327   ins_encode %{
5328     assert(UseAVX &gt; 1, &quot;required&quot;);
5329     int opcode = this-&gt;ideal_Opcode();
5330     int vector_len = Assembler::AVX_256bit;
5331     __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
5332     __ vextendbw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
5333     __ vextendbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, vector_len);
5334     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5335     __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vector_len);
5336     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5337     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5338     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5339     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
5340   %}
5341   ins_pipe( pipe_slow );
5342 %}
5343 
5344 instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
5345   predicate(n-&gt;as_Vector()-&gt;length() == 64);
5346   match(Set dst (LShiftVB src shift));
5347   match(Set dst (RShiftVB src shift));
5348   match(Set dst (URShiftVB src shift));
5349   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
5350   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5351   ins_encode %{
5352     assert(UseAVX &gt; 2, &quot;required&quot;);
5353     int opcode = this-&gt;ideal_Opcode();
5354     int vector_len = Assembler::AVX_512bit;
5355     __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
5356     __ vextendbw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
5357     __ vextendbw(opcode, $tmp2$$XMMRegister, $src$$XMMRegister, vector_len);
5358     __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vector_len);
5359     __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vector_len);
5360     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5361     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5362     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5363     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5364     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
5365     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
5366     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5367   %}
5368   ins_pipe( pipe_slow );
5369 %}
5370 
5371 // Shorts vector logical right shift produces incorrect Java result
5372 // for negative data because java code convert short value into int with
5373 // sign extension before a shift. But char vectors are fine since chars are
5374 // unsigned values.
5375 // Shorts/Chars vector left shift
5376 instruct vshiftS(vec dst, vec src, vec shift) %{
5377   match(Set dst (LShiftVS src shift));
5378   match(Set dst (RShiftVS src shift));
5379   match(Set dst (URShiftVS src shift));
5380   effect(TEMP dst, USE src, USE shift);
5381   format %{ &quot;vshiftw  $dst,$src,$shift\t! shift packedS&quot; %}
5382   ins_encode %{
5383     int opcode = this-&gt;ideal_Opcode();
5384     if (UseAVX &gt; 0) {
5385       int vlen_enc = vector_length_encoding(this);
5386       __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
5387     } else {
5388       int vlen = vector_length(this);
5389       if (vlen == 2) {
5390         __ movflt($dst$$XMMRegister, $src$$XMMRegister);
5391         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5392       } else if (vlen == 4) {
5393         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
5394         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5395       } else {
5396         assert (vlen == 8, &quot;sanity&quot;);
5397         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5398         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5399       }
5400     }
5401   %}
5402   ins_pipe( pipe_slow );
5403 %}
5404 
5405 // Integers vector left shift
5406 instruct vshiftI(vec dst, vec src, vec shift) %{
5407   match(Set dst (LShiftVI src shift));
5408   match(Set dst (RShiftVI src shift));
5409   match(Set dst (URShiftVI src shift));
5410   effect(TEMP dst, USE src, USE shift);
5411   format %{ &quot;vshiftd  $dst,$src,$shift\t! shift packedI&quot; %}
5412   ins_encode %{
5413     int opcode = this-&gt;ideal_Opcode();
5414     if (UseAVX &gt; 0) {
5415       int vector_len = vector_length_encoding(this);
5416       __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5417     } else {
5418       int vlen = vector_length(this);
5419       if (vlen == 2) {
5420         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
5421         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5422       } else {
5423         assert(vlen == 4, &quot;sanity&quot;);
5424         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5425         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5426       }
5427     }
5428   %}
5429   ins_pipe( pipe_slow );
5430 %}
5431 
5432 // Longs vector shift
5433 instruct vshiftL(vec dst, vec src, vec shift) %{
5434   match(Set dst (LShiftVL src shift));
5435   match(Set dst (URShiftVL src shift));
5436   effect(TEMP dst, USE src, USE shift);
5437   format %{ &quot;vshiftq  $dst,$src,$shift\t! shift packedL&quot; %}
5438   ins_encode %{
5439     int opcode = this-&gt;ideal_Opcode();
5440     if (UseAVX &gt; 0) {
5441       int vector_len = vector_length_encoding(this);
5442       __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5443     } else {
5444       assert(vector_length(this) == 2, &quot;&quot;);
5445       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5446       __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5447     }
5448   %}
5449   ins_pipe( pipe_slow );
5450 %}
5451 
5452 // -------------------ArithmeticRightShift -----------------------------------
5453 // Long vector arithmetic right shift
5454 instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5455   predicate(UseAVX &lt;= 2);
5456   match(Set dst (RShiftVL src shift));
5457   effect(TEMP dst, TEMP tmp, TEMP scratch);
5458   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5459   ins_encode %{
5460     uint vlen = vector_length(this);
5461     if (vlen == 2) {
5462       assert(UseSSE &gt;= 2, &quot;required&quot;);
5463       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5464       __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
5465       __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5466       __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
5467       __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
5468       __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
5469     } else {
5470       assert(vlen == 4, &quot;sanity&quot;);
5471       assert(UseAVX &gt; 1, &quot;required&quot;);
5472       int vector_len = Assembler::AVX_256bit;
5473       __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5474       __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5475       __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5476       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5477       __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5478     }
5479   %}
5480   ins_pipe( pipe_slow );
5481 %}
5482 
5483 instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
5484   predicate(UseAVX &gt; 2);
5485   match(Set dst (RShiftVL src shift));
5486   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5487   ins_encode %{
5488     int vector_len = vector_length_encoding(this);
5489     __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5490   %}
5491   ins_pipe( pipe_slow );
5492 %}
5493 
5494 // --------------------------------- AND --------------------------------------
5495 
5496 instruct vand(vec dst, vec src) %{
5497   predicate(UseAVX == 0);
5498   match(Set dst (AndV dst src));
5499   format %{ &quot;pand    $dst,$src\t! and vectors&quot; %}
5500   ins_encode %{
5501     __ pand($dst$$XMMRegister, $src$$XMMRegister);
5502   %}
5503   ins_pipe( pipe_slow );
5504 %}
5505 
5506 instruct vand_reg(vec dst, vec src1, vec src2) %{
5507   predicate(UseAVX &gt; 0);
5508   match(Set dst (AndV src1 src2));
5509   format %{ &quot;vpand   $dst,$src1,$src2\t! and vectors&quot; %}
5510   ins_encode %{
5511     int vector_len = vector_length_encoding(this);
5512     __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5513   %}
5514   ins_pipe( pipe_slow );
5515 %}
5516 
5517 instruct vand_mem(vec dst, vec src, memory mem) %{
5518   predicate(UseAVX &gt; 0);
5519   match(Set dst (AndV src (LoadVector mem)));
5520   format %{ &quot;vpand   $dst,$src,$mem\t! and vectors&quot; %}
5521   ins_encode %{
5522     int vector_len = vector_length_encoding(this);
5523     __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5524   %}
5525   ins_pipe( pipe_slow );
5526 %}
5527 
5528 // --------------------------------- OR ---------------------------------------
5529 
5530 instruct vor(vec dst, vec src) %{
5531   predicate(UseAVX == 0);
5532   match(Set dst (OrV dst src));
5533   format %{ &quot;por     $dst,$src\t! or vectors&quot; %}
5534   ins_encode %{
5535     __ por($dst$$XMMRegister, $src$$XMMRegister);
5536   %}
5537   ins_pipe( pipe_slow );
5538 %}
5539 
5540 instruct vor_reg(vec dst, vec src1, vec src2) %{
5541   predicate(UseAVX &gt; 0);
5542   match(Set dst (OrV src1 src2));
5543   format %{ &quot;vpor    $dst,$src1,$src2\t! or vectors&quot; %}
5544   ins_encode %{
5545     int vector_len = vector_length_encoding(this);
5546     __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5547   %}
5548   ins_pipe( pipe_slow );
5549 %}
5550 
5551 instruct vor_mem(vec dst, vec src, memory mem) %{
5552   predicate(UseAVX &gt; 0);
5553   match(Set dst (OrV src (LoadVector mem)));
5554   format %{ &quot;vpor    $dst,$src,$mem\t! or vectors&quot; %}
5555   ins_encode %{
5556     int vector_len = vector_length_encoding(this);
5557     __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5558   %}
5559   ins_pipe( pipe_slow );
5560 %}
5561 
5562 // --------------------------------- XOR --------------------------------------
5563 
5564 instruct vxor(vec dst, vec src) %{
5565   predicate(UseAVX == 0);
5566   match(Set dst (XorV dst src));
5567   format %{ &quot;pxor    $dst,$src\t! xor vectors&quot; %}
5568   ins_encode %{
5569     __ pxor($dst$$XMMRegister, $src$$XMMRegister);
5570   %}
5571   ins_pipe( pipe_slow );
5572 %}
5573 
5574 instruct vxor_reg(vec dst, vec src1, vec src2) %{
5575   predicate(UseAVX &gt; 0);
5576   match(Set dst (XorV src1 src2));
5577   format %{ &quot;vpxor   $dst,$src1,$src2\t! xor vectors&quot; %}
5578   ins_encode %{
5579     int vector_len = vector_length_encoding(this);
5580     __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5581   %}
5582   ins_pipe( pipe_slow );
5583 %}
5584 
5585 instruct vxor_mem(vec dst, vec src, memory mem) %{
5586   predicate(UseAVX &gt; 0);
5587   match(Set dst (XorV src (LoadVector mem)));
5588   format %{ &quot;vpxor   $dst,$src,$mem\t! xor vectors&quot; %}
5589   ins_encode %{
5590     int vector_len = vector_length_encoding(this);
5591     __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5592   %}
5593   ins_pipe( pipe_slow );
5594 %}
5595 
5596 // --------------------------------- ABS --------------------------------------
5597 // a = |a|
5598 instruct vabsB_reg(vec dst, vec src) %{
5599   match(Set dst (AbsVB  src));
5600   format %{ &quot;vabsb $dst,$src\t# $dst = |$src| abs packedB&quot; %}
5601   ins_encode %{
5602     uint vlen = vector_length(this);
5603     if (vlen &lt;= 16) {
5604       __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
5605     } else {
5606       int vlen_enc = vector_length_encoding(this);
5607       __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5608     }
5609   %}
5610   ins_pipe( pipe_slow );
5611 %}
5612 
5613 instruct vabsS_reg(vec dst, vec src) %{
5614   match(Set dst (AbsVS  src));
5615   format %{ &quot;vabsw $dst,$src\t# $dst = |$src| abs packedS&quot; %}
5616   ins_encode %{
5617     uint vlen = vector_length(this);
5618     if (vlen &lt;= 8) {
5619       __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
5620     } else {
5621       int vlen_enc = vector_length_encoding(this);
5622       __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5623     }
5624   %}
5625   ins_pipe( pipe_slow );
5626 %}
5627 
5628 instruct vabsI_reg(vec dst, vec src) %{
5629   match(Set dst (AbsVI  src));
5630   format %{ &quot;pabsd $dst,$src\t# $dst = |$src| abs packedI&quot; %}
5631   ins_encode %{
5632     uint vlen = vector_length(this);
5633     if (vlen &lt;= 4) {
5634       __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
5635     } else {
5636       int vlen_enc = vector_length_encoding(this);
5637       __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5638     }
5639   %}
5640   ins_pipe( pipe_slow );
5641 %}
5642 
5643 instruct vabsL_reg(vec dst, vec src) %{
5644   match(Set dst (AbsVL  src));
5645   format %{ &quot;evpabsq $dst,$src\t# $dst = |$src| abs packedL&quot; %}
5646   ins_encode %{
5647     assert(UseAVX &gt; 2, &quot;required&quot;);
5648     int vector_len = vector_length_encoding(this);
5649     __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5650   %}
5651   ins_pipe( pipe_slow );
5652 %}
5653 
5654 // --------------------------------- ABSNEG --------------------------------------
5655 
5656 instruct vabsnegF(vec dst, vec src, rRegI scratch) %{
5657   predicate(n-&gt;as_Vector()-&gt;length() != 4); // handled by 1-operand instruction vabsneg4F
5658   match(Set dst (AbsVF src));
5659   match(Set dst (NegVF src));
5660   effect(TEMP scratch);
5661   format %{ &quot;vabsnegf $dst,$src,[mask]\t# absneg packedF&quot; %}
5662   ins_cost(150);
5663   ins_encode %{
5664     int opcode = this-&gt;ideal_Opcode();
5665     int vlen = vector_length(this);
5666     if (vlen == 2) {
5667       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5668     } else {
5669       assert(vlen == 8 || vlen == 16, &quot;required&quot;);
5670       int vlen_enc = vector_length_encoding(this);
5671       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5672     }
5673   %}
5674   ins_pipe( pipe_slow );
5675 %}
5676 
5677 instruct vabsneg4F(vec dst, rRegI scratch) %{
5678   predicate(n-&gt;as_Vector()-&gt;length() == 4);
5679   match(Set dst (AbsVF dst));
5680   match(Set dst (NegVF dst));
5681   effect(TEMP scratch);
5682   format %{ &quot;vabsnegf $dst,[mask]\t# absneg packed4F&quot; %}
5683   ins_cost(150);
5684   ins_encode %{
5685     int opcode = this-&gt;ideal_Opcode();
5686     __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);
5687   %}
5688   ins_pipe( pipe_slow );
5689 %}
5690 
5691 instruct vabsnegD(vec dst, vec src, rRegI scratch) %{
5692   match(Set dst (AbsVD  src));
5693   match(Set dst (NegVD  src));
5694   effect(TEMP scratch);
5695   format %{ &quot;vabsnegd $dst,$src,[mask]\t# absneg packedD&quot; %}
5696   ins_encode %{
5697     int opcode = this-&gt;ideal_Opcode();
5698     uint vlen = vector_length(this);
5699     if (vlen == 2) {
5700       assert(UseSSE &gt;= 2, &quot;required&quot;);
5701       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5702     } else {
5703       int vlen_enc = vector_length_encoding(this);
5704       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5705     }
5706   %}
5707   ins_pipe( pipe_slow );
5708 %}
5709 
5710 // --------------------------------- FMA --------------------------------------
5711 // a * b + c
5712 
5713 instruct vfmaF_reg(vec a, vec b, vec c) %{
5714   match(Set c (FmaVF  c (Binary a b)));
5715   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5716   ins_cost(150);
5717   ins_encode %{
5718     assert(UseFMA, &quot;not enabled&quot;);
5719     int vector_len = vector_length_encoding(this);
5720     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5721   %}
5722   ins_pipe( pipe_slow );
5723 %}
5724 
5725 instruct vfmaF_mem(vec a, memory b, vec c) %{
5726   match(Set c (FmaVF  c (Binary a (LoadVector b))));
5727   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5728   ins_cost(150);
5729   ins_encode %{
5730     assert(UseFMA, &quot;not enabled&quot;);
5731     int vector_len = vector_length_encoding(this);
5732     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5733   %}
5734   ins_pipe( pipe_slow );
5735 %}
5736 
5737 instruct vfmaD_reg(vec a, vec b, vec c) %{
5738   match(Set c (FmaVD  c (Binary a b)));
5739   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5740   ins_cost(150);
5741   ins_encode %{
5742     assert(UseFMA, &quot;not enabled&quot;);
5743     int vector_len = vector_length_encoding(this);
5744     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5745   %}
5746   ins_pipe( pipe_slow );
5747 %}
5748 
5749 instruct vfmaD_mem(vec a, memory b, vec c) %{
5750   match(Set c (FmaVD  c (Binary a (LoadVector b))));
5751   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5752   ins_cost(150);
5753   ins_encode %{
5754     assert(UseFMA, &quot;not enabled&quot;);
5755     int vector_len = vector_length_encoding(this);
5756     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5757   %}
5758   ins_pipe( pipe_slow );
5759 %}
5760 
5761 // --------------------------------- Vector Multiply Add --------------------------------------
5762 
5763 instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
5764   predicate(UseAVX == 0);
5765   match(Set dst (MulAddVS2VI dst src1));
5766   format %{ &quot;pmaddwd $dst,$dst,$src1\t! muladd packedStoI&quot; %}
5767   ins_encode %{
5768     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5769   %}
5770   ins_pipe( pipe_slow );
5771 %}
5772 
5773 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5774   predicate(UseAVX &gt; 0);
5775   match(Set dst (MulAddVS2VI src1 src2));
5776   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5777   ins_encode %{
5778     int vector_len = vector_length_encoding(this);
5779     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5780   %}
5781   ins_pipe( pipe_slow );
5782 %}
5783 
5784 // --------------------------------- Vector Multiply Add Add ----------------------------------
5785 
5786 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<a name="94" id="anc94"></a><span class="line-modified">5787   predicate(VM_Version::supports_vnni());</span>
5788   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5789   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5790   ins_encode %{
5791     assert(UseAVX &gt; 2, &quot;required&quot;);
5792     int vector_len = vector_length_encoding(this);
5793     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5794   %}
5795   ins_pipe( pipe_slow );
5796   ins_cost(10);
5797 %}
5798 
5799 // --------------------------------- PopCount --------------------------------------
5800 
5801 instruct vpopcountI(vec dst, vec src) %{
5802   match(Set dst (PopCountVI src));
5803   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5804   ins_encode %{
5805     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5806 
5807     int vector_len = vector_length_encoding(this);
5808     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5809   %}
5810   ins_pipe( pipe_slow );
5811 %}
<a name="95" id="anc95"></a>























<a name="96" id="anc96"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="96" type="hidden" />
</body>
</html>