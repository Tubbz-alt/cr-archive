<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/macroAssembler_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroAssembler_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="nativeInst_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/macroAssembler_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 141   void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2 = noreg);
 142   void store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2 = noreg);
 143 
 144   // Support for inc/dec with optimal instruction selection depending on value
 145 
 146   void increment(Register reg, int value = 1) { LP64_ONLY(incrementq(reg, value)) NOT_LP64(incrementl(reg, value)) ; }
 147   void decrement(Register reg, int value = 1) { LP64_ONLY(decrementq(reg, value)) NOT_LP64(decrementl(reg, value)) ; }
 148 
 149   void decrementl(Address dst, int value = 1);
 150   void decrementl(Register reg, int value = 1);
 151 
 152   void decrementq(Register reg, int value = 1);
 153   void decrementq(Address dst, int value = 1);
 154 
 155   void incrementl(Address dst, int value = 1);
 156   void incrementl(Register reg, int value = 1);
 157 
 158   void incrementq(Register reg, int value = 1);
 159   void incrementq(Address dst, int value = 1);
 160 
<span class="line-removed"> 161 #ifdef COMPILER2</span>
<span class="line-removed"> 162   // special instructions for EVEX</span>
<span class="line-removed"> 163   void setvectmask(Register dst, Register src);</span>
<span class="line-removed"> 164   void restorevectmask();</span>
<span class="line-removed"> 165 #endif</span>
<span class="line-removed"> 166 </span>
 167   // Support optimal SSE move instructions.
 168   void movflt(XMMRegister dst, XMMRegister src) {
 169     if (dst-&gt; encoding() == src-&gt;encoding()) return;
 170     if (UseXmmRegToRegMoveAll) { movaps(dst, src); return; }
 171     else                       { movss (dst, src); return; }
 172   }
 173   void movflt(XMMRegister dst, Address src) { movss(dst, src); }
 174   void movflt(XMMRegister dst, AddressLiteral src);
 175   void movflt(Address dst, XMMRegister src) { movss(dst, src); }
 176 
 177   void movdbl(XMMRegister dst, XMMRegister src) {
 178     if (dst-&gt; encoding() == src-&gt;encoding()) return;
 179     if (UseXmmRegToRegMoveAll) { movapd(dst, src); return; }
 180     else                       { movsd (dst, src); return; }
 181   }
 182 
 183   void movdbl(XMMRegister dst, AddressLiteral src);
 184 
 185   void movdbl(XMMRegister dst, Address src) {
 186     if (UseXmmLoadAndClearUpper) { movsd (dst, src); return; }
</pre>
<hr />
<pre>
 664   void verify_tlab();
 665 
 666   // Biased locking support
 667   // lock_reg and obj_reg must be loaded up with the appropriate values.
 668   // swap_reg must be rax, and is killed.
 669   // tmp_reg is optional. If it is supplied (i.e., != noreg) it will
 670   // be killed; if not supplied, push/pop will be used internally to
 671   // allocate a temporary (inefficient, avoid if possible).
 672   // Optional slow case is for implementations (interpreter and C1) which branch to
 673   // slow case directly. Leaves condition codes set for C2&#39;s Fast_Lock node.
 674   // Returns offset of first potentially-faulting instruction for null
 675   // check info (currently consumed only by C1). If
 676   // swap_reg_contains_mark is true then returns -1 as it is assumed
 677   // the calling code has already passed any potential faults.
 678   int biased_locking_enter(Register lock_reg, Register obj_reg,
 679                            Register swap_reg, Register tmp_reg,
 680                            bool swap_reg_contains_mark,
 681                            Label&amp; done, Label* slow_case = NULL,
 682                            BiasedLockingCounters* counters = NULL);
 683   void biased_locking_exit (Register obj_reg, Register temp_reg, Label&amp; done);
<span class="line-removed"> 684 #ifdef COMPILER2</span>
<span class="line-removed"> 685   // Code used by cmpFastLock and cmpFastUnlock mach instructions in .ad file.</span>
<span class="line-removed"> 686   // See full desription in macroAssembler_x86.cpp.</span>
<span class="line-removed"> 687   void fast_lock(Register obj, Register box, Register tmp,</span>
<span class="line-removed"> 688                  Register scr, Register cx1, Register cx2,</span>
<span class="line-removed"> 689                  BiasedLockingCounters* counters,</span>
<span class="line-removed"> 690                  RTMLockingCounters* rtm_counters,</span>
<span class="line-removed"> 691                  RTMLockingCounters* stack_rtm_counters,</span>
<span class="line-removed"> 692                  Metadata* method_data,</span>
<span class="line-removed"> 693                  bool use_rtm, bool profile_rtm);</span>
<span class="line-removed"> 694   void fast_unlock(Register obj, Register box, Register tmp, bool use_rtm);</span>
<span class="line-removed"> 695 #if INCLUDE_RTM_OPT</span>
<span class="line-removed"> 696   void rtm_counters_update(Register abort_status, Register rtm_counters);</span>
<span class="line-removed"> 697   void branch_on_random_using_rdtsc(Register tmp, Register scr, int count, Label&amp; brLabel);</span>
<span class="line-removed"> 698   void rtm_abort_ratio_calculation(Register tmp, Register rtm_counters_reg,</span>
<span class="line-removed"> 699                                    RTMLockingCounters* rtm_counters,</span>
<span class="line-removed"> 700                                    Metadata* method_data);</span>
<span class="line-removed"> 701   void rtm_profiling(Register abort_status_Reg, Register rtm_counters_Reg,</span>
<span class="line-removed"> 702                      RTMLockingCounters* rtm_counters, Metadata* method_data, bool profile_rtm);</span>
<span class="line-removed"> 703   void rtm_retry_lock_on_abort(Register retry_count, Register abort_status, Label&amp; retryLabel);</span>
<span class="line-removed"> 704   void rtm_retry_lock_on_busy(Register retry_count, Register box, Register tmp, Register scr, Label&amp; retryLabel);</span>
<span class="line-removed"> 705   void rtm_stack_locking(Register obj, Register tmp, Register scr,</span>
<span class="line-removed"> 706                          Register retry_on_abort_count,</span>
<span class="line-removed"> 707                          RTMLockingCounters* stack_rtm_counters,</span>
<span class="line-removed"> 708                          Metadata* method_data, bool profile_rtm,</span>
<span class="line-removed"> 709                          Label&amp; DONE_LABEL, Label&amp; IsInflated);</span>
<span class="line-removed"> 710   void rtm_inflated_locking(Register obj, Register box, Register tmp,</span>
<span class="line-removed"> 711                             Register scr, Register retry_on_busy_count,</span>
<span class="line-removed"> 712                             Register retry_on_abort_count,</span>
<span class="line-removed"> 713                             RTMLockingCounters* rtm_counters,</span>
<span class="line-removed"> 714                             Metadata* method_data, bool profile_rtm,</span>
<span class="line-removed"> 715                             Label&amp; DONE_LABEL);</span>
<span class="line-removed"> 716 #endif</span>
<span class="line-removed"> 717 #endif</span>
 718 
 719   Condition negate_condition(Condition cond);
 720 
 721   // Instructions that use AddressLiteral operands. These instruction can handle 32bit/64bit
 722   // operands. In general the names are modified to avoid hiding the instruction in Assembler
 723   // so that we don&#39;t need to implement all the varieties in the Assembler with trivial wrappers
 724   // here in MacroAssembler. The major exception to this rule is call
 725 
 726   // Arithmetics
 727 
 728 
 729   void addptr(Address dst, int32_t src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)) ; }
 730   void addptr(Address dst, Register src);
 731 
 732   void addptr(Register dst, Address src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)); }
 733   void addptr(Register dst, int32_t src);
 734   void addptr(Register dst, Register src);
 735   void addptr(Register dst, RegisterOrConstant src) {
 736     if (src.is_constant()) addptr(dst, (int) src.as_constant());
 737     else                   addptr(dst,       src.as_register());
</pre>
<hr />
<pre>
1618   // Import other mov() methods from the parent class or else
1619   // they will be hidden by the following overriding declaration.
1620   using Assembler::movdl;
1621   using Assembler::movq;
1622   void movdl(XMMRegister dst, AddressLiteral src);
1623   void movq(XMMRegister dst, AddressLiteral src);
1624 
1625   // Can push value or effective address
1626   void pushptr(AddressLiteral src);
1627 
1628   void pushptr(Address src) { LP64_ONLY(pushq(src)) NOT_LP64(pushl(src)); }
1629   void popptr(Address src) { LP64_ONLY(popq(src)) NOT_LP64(popl(src)); }
1630 
1631   void pushoop(jobject obj);
1632   void pushklass(Metadata* obj);
1633 
1634   // sign extend as need a l to ptr sized element
1635   void movl2ptr(Register dst, Address src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src)); }
1636   void movl2ptr(Register dst, Register src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(if (dst != src) movl(dst, src)); }
1637 
<span class="line-removed">1638 #ifdef COMPILER2</span>
<span class="line-removed">1639   // Generic instructions support for use in .ad files C2 code generation</span>
<span class="line-removed">1640   void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr);</span>
<span class="line-removed">1641   void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);</span>
<span class="line-removed">1642   void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr);</span>
<span class="line-removed">1643   void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);</span>
<span class="line-removed">1644   void vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len);</span>
<span class="line-removed">1645   void vextendbw(bool sign, XMMRegister dst, XMMRegister src);</span>
<span class="line-removed">1646   void vshiftd(int opcode, XMMRegister dst, XMMRegister src);</span>
<span class="line-removed">1647   void vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);</span>
<span class="line-removed">1648   void vshiftw(int opcode, XMMRegister dst, XMMRegister src);</span>
<span class="line-removed">1649   void vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);</span>
<span class="line-removed">1650   void vshiftq(int opcode, XMMRegister dst, XMMRegister src);</span>
<span class="line-removed">1651   void vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);</span>
<span class="line-removed">1652 #endif</span>
1653 

1654   // C2 compiled method&#39;s prolog code.
1655   void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);
1656 
1657   // clear memory of size &#39;cnt&#39; qwords, starting at &#39;base&#39;;
1658   // if &#39;is_large&#39; is set, do not try to produce short loop
1659   void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);
1660 
1661   // clear memory of size &#39;cnt&#39; qwords, starting at &#39;base&#39; using XMM/YMM registers
1662   void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);
1663 
<span class="line-removed">1664 #ifdef COMPILER2</span>
<span class="line-removed">1665   void string_indexof_char(Register str1, Register cnt1, Register ch, Register result,</span>
<span class="line-removed">1666                            XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp);</span>
<span class="line-removed">1667 </span>
<span class="line-removed">1668   // IndexOf strings.</span>
<span class="line-removed">1669   // Small strings are loaded through stack if they cross page boundary.</span>
<span class="line-removed">1670   void string_indexof(Register str1, Register str2,</span>
<span class="line-removed">1671                       Register cnt1, Register cnt2,</span>
<span class="line-removed">1672                       int int_cnt2,  Register result,</span>
<span class="line-removed">1673                       XMMRegister vec, Register tmp,</span>
<span class="line-removed">1674                       int ae);</span>
<span class="line-removed">1675 </span>
<span class="line-removed">1676   // IndexOf for constant substrings with size &gt;= 8 elements</span>
<span class="line-removed">1677   // which don&#39;t need to be loaded through stack.</span>
<span class="line-removed">1678   void string_indexofC8(Register str1, Register str2,</span>
<span class="line-removed">1679                       Register cnt1, Register cnt2,</span>
<span class="line-removed">1680                       int int_cnt2,  Register result,</span>
<span class="line-removed">1681                       XMMRegister vec, Register tmp,</span>
<span class="line-removed">1682                       int ae);</span>
<span class="line-removed">1683 </span>
<span class="line-removed">1684     // Smallest code: we don&#39;t need to load through stack,</span>
<span class="line-removed">1685     // check string tail.</span>
<span class="line-removed">1686 </span>
<span class="line-removed">1687   // helper function for string_compare</span>
<span class="line-removed">1688   void load_next_elements(Register elem1, Register elem2, Register str1, Register str2,</span>
<span class="line-removed">1689                           Address::ScaleFactor scale, Address::ScaleFactor scale1,</span>
<span class="line-removed">1690                           Address::ScaleFactor scale2, Register index, int ae);</span>
<span class="line-removed">1691   // Compare strings.</span>
<span class="line-removed">1692   void string_compare(Register str1, Register str2,</span>
<span class="line-removed">1693                       Register cnt1, Register cnt2, Register result,</span>
<span class="line-removed">1694                       XMMRegister vec1, int ae);</span>
<span class="line-removed">1695 </span>
<span class="line-removed">1696   // Search for Non-ASCII character (Negative byte value) in a byte array,</span>
<span class="line-removed">1697   // return true if it has any and false otherwise.</span>
<span class="line-removed">1698   void has_negatives(Register ary1, Register len,</span>
<span class="line-removed">1699                      Register result, Register tmp1,</span>
<span class="line-removed">1700                      XMMRegister vec1, XMMRegister vec2);</span>
<span class="line-removed">1701 </span>
<span class="line-removed">1702   // Compare char[] or byte[] arrays.</span>
<span class="line-removed">1703   void arrays_equals(bool is_array_equ, Register ary1, Register ary2,</span>
<span class="line-removed">1704                      Register limit, Register result, Register chr,</span>
<span class="line-removed">1705                      XMMRegister vec1, XMMRegister vec2, bool is_char);</span>
<span class="line-removed">1706 </span>
<span class="line-removed">1707 #endif</span>
<span class="line-removed">1708 </span>
1709   // Fill primitive arrays
1710   void generate_fill(BasicType t, bool aligned,
1711                      Register to, Register value, Register count,
1712                      Register rtmp, XMMRegister xtmp);
1713 
1714   void encode_iso_array(Register src, Register dst, Register len,
1715                         XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3,
1716                         XMMRegister tmp4, Register tmp5, Register result);
1717 
1718 #ifdef _LP64
1719   void add2_with_carry(Register dest_hi, Register dest_lo, Register src1, Register src2);
1720   void multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
1721                              Register y, Register y_idx, Register z,
1722                              Register carry, Register product,
1723                              Register idx, Register kdx);
1724   void multiply_add_128_x_128(Register x_xstart, Register y, Register z,
1725                               Register yz_idx, Register idx,
1726                               Register carry, Register product, int offset);
1727   void multiply_128_x_128_bmi2_loop(Register y, Register z,
1728                                     Register carry, Register carry2,
</pre>
<hr />
<pre>
1803   void fold_128bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset);
1804 
1805   // Compress char[] array to byte[].
1806   void char_array_compress(Register src, Register dst, Register len,
1807                            XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3,
1808                            XMMRegister tmp4, Register tmp5, Register result);
1809 
1810   // Inflate byte[] array to char[].
1811   void byte_array_inflate(Register src, Register dst, Register len,
1812                           XMMRegister tmp1, Register tmp2);
1813 
1814 #ifdef _LP64
1815   void convert_f2i(Register dst, XMMRegister src);
1816   void convert_d2i(Register dst, XMMRegister src);
1817   void convert_f2l(Register dst, XMMRegister src);
1818   void convert_d2l(Register dst, XMMRegister src);
1819 
1820   void cache_wb(Address line);
1821   void cache_wbsync(bool is_pre);
1822 #endif // _LP64


1823 };
1824 
1825 /**
1826  * class SkipIfEqual:
1827  *
1828  * Instantiating this class will result in assembly code being output that will
1829  * jump around any code emitted between the creation of the instance and it&#39;s
1830  * automatic destruction at the end of a scope block, depending on the value of
1831  * the flag passed to the constructor, which will be checked at run-time.
1832  */
1833 class SkipIfEqual {
1834  private:
1835   MacroAssembler* _masm;
1836   Label _label;
1837 
1838  public:
1839    SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value);
1840    ~SkipIfEqual();
1841 };
1842 
</pre>
</td>
<td>
<hr />
<pre>
 141   void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2 = noreg);
 142   void store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2 = noreg);
 143 
 144   // Support for inc/dec with optimal instruction selection depending on value
 145 
 146   void increment(Register reg, int value = 1) { LP64_ONLY(incrementq(reg, value)) NOT_LP64(incrementl(reg, value)) ; }
 147   void decrement(Register reg, int value = 1) { LP64_ONLY(decrementq(reg, value)) NOT_LP64(decrementl(reg, value)) ; }
 148 
 149   void decrementl(Address dst, int value = 1);
 150   void decrementl(Register reg, int value = 1);
 151 
 152   void decrementq(Register reg, int value = 1);
 153   void decrementq(Address dst, int value = 1);
 154 
 155   void incrementl(Address dst, int value = 1);
 156   void incrementl(Register reg, int value = 1);
 157 
 158   void incrementq(Register reg, int value = 1);
 159   void incrementq(Address dst, int value = 1);
 160 






 161   // Support optimal SSE move instructions.
 162   void movflt(XMMRegister dst, XMMRegister src) {
 163     if (dst-&gt; encoding() == src-&gt;encoding()) return;
 164     if (UseXmmRegToRegMoveAll) { movaps(dst, src); return; }
 165     else                       { movss (dst, src); return; }
 166   }
 167   void movflt(XMMRegister dst, Address src) { movss(dst, src); }
 168   void movflt(XMMRegister dst, AddressLiteral src);
 169   void movflt(Address dst, XMMRegister src) { movss(dst, src); }
 170 
 171   void movdbl(XMMRegister dst, XMMRegister src) {
 172     if (dst-&gt; encoding() == src-&gt;encoding()) return;
 173     if (UseXmmRegToRegMoveAll) { movapd(dst, src); return; }
 174     else                       { movsd (dst, src); return; }
 175   }
 176 
 177   void movdbl(XMMRegister dst, AddressLiteral src);
 178 
 179   void movdbl(XMMRegister dst, Address src) {
 180     if (UseXmmLoadAndClearUpper) { movsd (dst, src); return; }
</pre>
<hr />
<pre>
 658   void verify_tlab();
 659 
 660   // Biased locking support
 661   // lock_reg and obj_reg must be loaded up with the appropriate values.
 662   // swap_reg must be rax, and is killed.
 663   // tmp_reg is optional. If it is supplied (i.e., != noreg) it will
 664   // be killed; if not supplied, push/pop will be used internally to
 665   // allocate a temporary (inefficient, avoid if possible).
 666   // Optional slow case is for implementations (interpreter and C1) which branch to
 667   // slow case directly. Leaves condition codes set for C2&#39;s Fast_Lock node.
 668   // Returns offset of first potentially-faulting instruction for null
 669   // check info (currently consumed only by C1). If
 670   // swap_reg_contains_mark is true then returns -1 as it is assumed
 671   // the calling code has already passed any potential faults.
 672   int biased_locking_enter(Register lock_reg, Register obj_reg,
 673                            Register swap_reg, Register tmp_reg,
 674                            bool swap_reg_contains_mark,
 675                            Label&amp; done, Label* slow_case = NULL,
 676                            BiasedLockingCounters* counters = NULL);
 677   void biased_locking_exit (Register obj_reg, Register temp_reg, Label&amp; done);


































 678 
 679   Condition negate_condition(Condition cond);
 680 
 681   // Instructions that use AddressLiteral operands. These instruction can handle 32bit/64bit
 682   // operands. In general the names are modified to avoid hiding the instruction in Assembler
 683   // so that we don&#39;t need to implement all the varieties in the Assembler with trivial wrappers
 684   // here in MacroAssembler. The major exception to this rule is call
 685 
 686   // Arithmetics
 687 
 688 
 689   void addptr(Address dst, int32_t src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)) ; }
 690   void addptr(Address dst, Register src);
 691 
 692   void addptr(Register dst, Address src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)); }
 693   void addptr(Register dst, int32_t src);
 694   void addptr(Register dst, Register src);
 695   void addptr(Register dst, RegisterOrConstant src) {
 696     if (src.is_constant()) addptr(dst, (int) src.as_constant());
 697     else                   addptr(dst,       src.as_register());
</pre>
<hr />
<pre>
1578   // Import other mov() methods from the parent class or else
1579   // they will be hidden by the following overriding declaration.
1580   using Assembler::movdl;
1581   using Assembler::movq;
1582   void movdl(XMMRegister dst, AddressLiteral src);
1583   void movq(XMMRegister dst, AddressLiteral src);
1584 
1585   // Can push value or effective address
1586   void pushptr(AddressLiteral src);
1587 
1588   void pushptr(Address src) { LP64_ONLY(pushq(src)) NOT_LP64(pushl(src)); }
1589   void popptr(Address src) { LP64_ONLY(popq(src)) NOT_LP64(popl(src)); }
1590 
1591   void pushoop(jobject obj);
1592   void pushklass(Metadata* obj);
1593 
1594   // sign extend as need a l to ptr sized element
1595   void movl2ptr(Register dst, Address src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src)); }
1596   void movl2ptr(Register dst, Register src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(if (dst != src) movl(dst, src)); }
1597 















1598 
<span class="line-added">1599  public:</span>
1600   // C2 compiled method&#39;s prolog code.
1601   void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);
1602 
1603   // clear memory of size &#39;cnt&#39; qwords, starting at &#39;base&#39;;
1604   // if &#39;is_large&#39; is set, do not try to produce short loop
1605   void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);
1606 
1607   // clear memory of size &#39;cnt&#39; qwords, starting at &#39;base&#39; using XMM/YMM registers
1608   void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);
1609 













































1610   // Fill primitive arrays
1611   void generate_fill(BasicType t, bool aligned,
1612                      Register to, Register value, Register count,
1613                      Register rtmp, XMMRegister xtmp);
1614 
1615   void encode_iso_array(Register src, Register dst, Register len,
1616                         XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3,
1617                         XMMRegister tmp4, Register tmp5, Register result);
1618 
1619 #ifdef _LP64
1620   void add2_with_carry(Register dest_hi, Register dest_lo, Register src1, Register src2);
1621   void multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
1622                              Register y, Register y_idx, Register z,
1623                              Register carry, Register product,
1624                              Register idx, Register kdx);
1625   void multiply_add_128_x_128(Register x_xstart, Register y, Register z,
1626                               Register yz_idx, Register idx,
1627                               Register carry, Register product, int offset);
1628   void multiply_128_x_128_bmi2_loop(Register y, Register z,
1629                                     Register carry, Register carry2,
</pre>
<hr />
<pre>
1704   void fold_128bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset);
1705 
1706   // Compress char[] array to byte[].
1707   void char_array_compress(Register src, Register dst, Register len,
1708                            XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3,
1709                            XMMRegister tmp4, Register tmp5, Register result);
1710 
1711   // Inflate byte[] array to char[].
1712   void byte_array_inflate(Register src, Register dst, Register len,
1713                           XMMRegister tmp1, Register tmp2);
1714 
1715 #ifdef _LP64
1716   void convert_f2i(Register dst, XMMRegister src);
1717   void convert_d2i(Register dst, XMMRegister src);
1718   void convert_f2l(Register dst, XMMRegister src);
1719   void convert_d2l(Register dst, XMMRegister src);
1720 
1721   void cache_wb(Address line);
1722   void cache_wbsync(bool is_pre);
1723 #endif // _LP64
<span class="line-added">1724 </span>
<span class="line-added">1725   void vallones(XMMRegister dst, int vector_len);</span>
1726 };
1727 
1728 /**
1729  * class SkipIfEqual:
1730  *
1731  * Instantiating this class will result in assembly code being output that will
1732  * jump around any code emitted between the creation of the instance and it&#39;s
1733  * automatic destruction at the end of a scope block, depending on the value of
1734  * the flag passed to the constructor, which will be checked at run-time.
1735  */
1736 class SkipIfEqual {
1737  private:
1738   MacroAssembler* _masm;
1739   Label _label;
1740 
1741  public:
1742    SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value);
1743    ~SkipIfEqual();
1744 };
1745 
</pre>
</td>
</tr>
</table>
<center><a href="macroAssembler_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="nativeInst_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>