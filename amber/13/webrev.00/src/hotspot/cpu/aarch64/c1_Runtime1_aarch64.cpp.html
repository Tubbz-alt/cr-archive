<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/aarch64/c1_Runtime1_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/assembler.hpp&quot;
  28 #include &quot;c1/c1_CodeStubs.hpp&quot;
  29 #include &quot;c1/c1_Defs.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;compiler/disassembler.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  35 #include &quot;interpreter/interpreter.hpp&quot;
  36 #include &quot;memory/universe.hpp&quot;
  37 #include &quot;nativeInst_aarch64.hpp&quot;
  38 #include &quot;oops/compiledICHolder.hpp&quot;
  39 #include &quot;oops/oop.inline.hpp&quot;
  40 #include &quot;prims/jvmtiExport.hpp&quot;
  41 #include &quot;register_aarch64.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;runtime/signature.hpp&quot;
  44 #include &quot;runtime/vframe.hpp&quot;
  45 #include &quot;runtime/vframeArray.hpp&quot;
  46 #include &quot;utilities/powerOfTwo.hpp&quot;
  47 #include &quot;vmreg_aarch64.inline.hpp&quot;
  48 
  49 
  50 // Implementation of StubAssembler
  51 
  52 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, int args_size) {
  53   // setup registers
  54   assert(!(oop_result1-&gt;is_valid() || metadata_result-&gt;is_valid()) || oop_result1 != metadata_result, &quot;registers must be different&quot;);
  55   assert(oop_result1 != rthread &amp;&amp; metadata_result != rthread, &quot;registers must be different&quot;);
  56   assert(args_size &gt;= 0, &quot;illegal args_size&quot;);
  57   bool align_stack = false;
  58 
  59   mov(c_rarg0, rthread);
  60   set_num_rt_args(0); // Nothing on stack
  61 
  62   Label retaddr;
  63   set_last_Java_frame(sp, rfp, retaddr, rscratch1);
  64 
  65   // do the call
  66   lea(rscratch1, RuntimeAddress(entry));
  67   blr(rscratch1);
  68   bind(retaddr);
  69   int call_offset = offset();
  70   // verify callee-saved register
  71 #ifdef ASSERT
  72   push(r0, sp);
  73   { Label L;
  74     get_thread(r0);
  75     cmp(rthread, r0);
  76     br(Assembler::EQ, L);
  77     stop(&quot;StubAssembler::call_RT: rthread not callee saved?&quot;);
  78     bind(L);
  79   }
  80   pop(r0, sp);
  81 #endif
  82   reset_last_Java_frame(true);
  83   maybe_isb();
  84 
  85   // check for pending exceptions
  86   { Label L;
  87     // check for pending exceptions (java_thread is set upon return)
  88     ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
  89     cbz(rscratch1, L);
  90     // exception pending =&gt; remove activation and forward to exception handler
  91     // make sure that the vm_results are cleared
  92     if (oop_result1-&gt;is_valid()) {
  93       str(zr, Address(rthread, JavaThread::vm_result_offset()));
  94     }
  95     if (metadata_result-&gt;is_valid()) {
  96       str(zr, Address(rthread, JavaThread::vm_result_2_offset()));
  97     }
  98     if (frame_size() == no_frame_size) {
  99       leave();
 100       far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
 101     } else if (_stub_id == Runtime1::forward_exception_id) {
 102       should_not_reach_here();
 103     } else {
 104       far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::forward_exception_id)));
 105     }
 106     bind(L);
 107   }
 108   // get oop results if there are any and reset the values in the thread
 109   if (oop_result1-&gt;is_valid()) {
 110     get_vm_result(oop_result1, rthread);
 111   }
 112   if (metadata_result-&gt;is_valid()) {
 113     get_vm_result_2(metadata_result, rthread);
 114   }
 115   return call_offset;
 116 }
 117 
 118 
 119 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1) {
 120   mov(c_rarg1, arg1);
 121   return call_RT(oop_result1, metadata_result, entry, 1);
 122 }
 123 
 124 
 125 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2) {
 126   if (c_rarg1 == arg2) {
 127     if (c_rarg2 == arg1) {
 128       mov(rscratch1, arg1);
 129       mov(arg1, arg2);
 130       mov(arg2, rscratch1);
 131     } else {
 132       mov(c_rarg2, arg2);
 133       mov(c_rarg1, arg1);
 134     }
 135   } else {
 136     mov(c_rarg1, arg1);
 137     mov(c_rarg2, arg2);
 138   }
 139   return call_RT(oop_result1, metadata_result, entry, 2);
 140 }
 141 
 142 
 143 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {
 144   // if there is any conflict use the stack
 145   if (arg1 == c_rarg2 || arg1 == c_rarg3 ||
 146       arg2 == c_rarg1 || arg1 == c_rarg3 ||
 147       arg3 == c_rarg1 || arg1 == c_rarg2) {
 148     stp(arg3, arg2, Address(pre(sp, 2 * wordSize)));
 149     stp(arg1, zr, Address(pre(sp, -2 * wordSize)));
 150     ldp(c_rarg1, zr, Address(post(sp, 2 * wordSize)));
 151     ldp(c_rarg3, c_rarg2, Address(post(sp, 2 * wordSize)));
 152   } else {
 153     mov(c_rarg1, arg1);
 154     mov(c_rarg2, arg2);
 155     mov(c_rarg3, arg3);
 156   }
 157   return call_RT(oop_result1, metadata_result, entry, 3);
 158 }
 159 
 160 // Implementation of StubFrame
 161 
 162 class StubFrame: public StackObj {
 163  private:
 164   StubAssembler* _sasm;
 165 
 166  public:
 167   StubFrame(StubAssembler* sasm, const char* name, bool must_gc_arguments);
 168   void load_argument(int offset_in_words, Register reg);
 169 
 170   ~StubFrame();
 171 };;
 172 
 173 void StubAssembler::prologue(const char* name, bool must_gc_arguments) {
 174   set_info(name, must_gc_arguments);
 175   enter();
 176 }
 177 
 178 void StubAssembler::epilogue() {
 179   leave();
 180   ret(lr);
 181 }
 182 
 183 #define __ _sasm-&gt;
 184 
 185 StubFrame::StubFrame(StubAssembler* sasm, const char* name, bool must_gc_arguments) {
 186   _sasm = sasm;
 187   __ prologue(name, must_gc_arguments);
 188 }
 189 
 190 // load parameters that were stored with LIR_Assembler::store_parameter
 191 // Note: offsets for store_parameter and load_argument must match
 192 void StubFrame::load_argument(int offset_in_words, Register reg) {
 193   __ load_parameter(offset_in_words, reg);
 194 }
 195 
 196 
 197 StubFrame::~StubFrame() {
 198   __ epilogue();
 199 }
 200 
 201 #undef __
 202 
 203 
 204 // Implementation of Runtime1
 205 
 206 #define __ sasm-&gt;
 207 
 208 const int float_regs_as_doubles_size_in_slots = pd_nof_fpu_regs_frame_map * 2;
 209 
 210 // Stack layout for saving/restoring  all the registers needed during a runtime
 211 // call (this includes deoptimization)
 212 // Note: note that users of this frame may well have arguments to some runtime
 213 // while these values are on the stack. These positions neglect those arguments
 214 // but the code in save_live_registers will take the argument count into
 215 // account.
 216 //
 217 
 218 enum reg_save_layout {
 219   reg_save_frame_size = 32 /* float */ + 32 /* integer */
 220 };
 221 
 222 // Save off registers which might be killed by calls into the runtime.
 223 // Tries to smart of about FP registers.  In particular we separate
 224 // saving and describing the FPU registers for deoptimization since we
 225 // have to save the FPU registers twice if we describe them.  The
 226 // deopt blob is the only thing which needs to describe FPU registers.
 227 // In all other cases it should be sufficient to simply save their
 228 // current value.
 229 
 230 static int cpu_reg_save_offsets[FrameMap::nof_cpu_regs];
 231 static int fpu_reg_save_offsets[FrameMap::nof_fpu_regs];
 232 static int reg_save_size_in_words;
 233 static int frame_size_in_bytes = -1;
 234 
 235 static OopMap* generate_oop_map(StubAssembler* sasm, bool save_fpu_registers) {
 236   int frame_size_in_bytes = reg_save_frame_size * BytesPerWord;
 237   sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
 238   int frame_size_in_slots = frame_size_in_bytes / sizeof(jint);
 239   OopMap* oop_map = new OopMap(frame_size_in_slots, 0);
 240 
 241   for (int i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 242     Register r = as_Register(i);
 243     if (i &lt;= 18 &amp;&amp; i != rscratch1-&gt;encoding() &amp;&amp; i != rscratch2-&gt;encoding()) {
 244       int sp_offset = cpu_reg_save_offsets[i];
 245       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset),
 246                                 r-&gt;as_VMReg());
 247     }
 248   }
 249 
 250   if (save_fpu_registers) {
 251     for (int i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 252       FloatRegister r = as_FloatRegister(i);
 253       {
 254         int sp_offset = fpu_reg_save_offsets[i];
 255         oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset),
 256                                   r-&gt;as_VMReg());
 257       }
 258     }
 259   }
 260   return oop_map;
 261 }
 262 
 263 static OopMap* save_live_registers(StubAssembler* sasm,
 264                                    bool save_fpu_registers = true) {
 265   __ block_comment(&quot;save_live_registers&quot;);
 266 
 267   __ push(RegSet::range(r0, r29), sp);         // integer registers except lr &amp; sp
 268 
 269   if (save_fpu_registers) {
 270     for (int i = 31; i&gt;= 0; i -= 4) {
 271       __ sub(sp, sp, 4 * wordSize); // no pre-increment for st1. Emulate it without modifying other registers
 272       __ st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
 273           as_FloatRegister(i), __ T1D, Address(sp));
 274     }
 275   } else {
 276     __ add(sp, sp, -32 * wordSize);
 277   }
 278 
 279   return generate_oop_map(sasm, save_fpu_registers);
 280 }
 281 
 282 static void restore_live_registers(StubAssembler* sasm, bool restore_fpu_registers = true) {
 283   if (restore_fpu_registers) {
 284     for (int i = 0; i &lt; 32; i += 4)
 285       __ ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
 286           as_FloatRegister(i+3), __ T1D, Address(__ post(sp, 4 * wordSize)));
 287   } else {
 288     __ add(sp, sp, 32 * wordSize);
 289   }
 290 
 291   __ pop(RegSet::range(r0, r29), sp);
 292 }
 293 
 294 static void restore_live_registers_except_r0(StubAssembler* sasm, bool restore_fpu_registers = true)  {
 295 
 296   if (restore_fpu_registers) {
 297     for (int i = 0; i &lt; 32; i += 4)
 298       __ ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
 299           as_FloatRegister(i+3), __ T1D, Address(__ post(sp, 4 * wordSize)));
 300   } else {
 301     __ add(sp, sp, 32 * wordSize);
 302   }
 303 
 304   __ ldp(zr, r1, Address(__ post(sp, 16)));
 305   __ pop(RegSet::range(r2, r29), sp);
 306 }
 307 
 308 
 309 
 310 void Runtime1::initialize_pd() {
 311   int i;
 312   int sp_offset = 0;
 313 
 314   // all float registers are saved explicitly
 315   assert(FrameMap::nof_fpu_regs == 32, &quot;double registers not handled here&quot;);
 316   for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 317     fpu_reg_save_offsets[i] = sp_offset;
 318     sp_offset += 2;   // SP offsets are in halfwords
 319   }
 320 
 321   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 322     Register r = as_Register(i);
 323     cpu_reg_save_offsets[i] = sp_offset;
 324     sp_offset += 2;   // SP offsets are in halfwords
 325   }
 326 }
 327 
 328 
 329 // target: the entry point of the method that creates and posts the exception oop
 330 // has_argument: true if the exception needs arguments (passed in rscratch1 and rscratch2)
 331 
 332 OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {
 333   // make a frame and preserve the caller&#39;s caller-save registers
 334   OopMap* oop_map = save_live_registers(sasm);
 335   int call_offset;
 336   if (!has_argument) {
 337     call_offset = __ call_RT(noreg, noreg, target);
 338   } else {
 339     __ mov(c_rarg1, rscratch1);
 340     __ mov(c_rarg2, rscratch2);
 341     call_offset = __ call_RT(noreg, noreg, target);
 342   }
 343   OopMapSet* oop_maps = new OopMapSet();
 344   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 345 
 346   __ should_not_reach_here();
 347   return oop_maps;
 348 }
 349 
 350 
 351 OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler *sasm) {
 352   __ block_comment(&quot;generate_handle_exception&quot;);
 353 
 354   // incoming parameters
 355   const Register exception_oop = r0;
 356   const Register exception_pc  = r3;
 357   // other registers used in this stub
 358 
 359   // Save registers, if required.
 360   OopMapSet* oop_maps = new OopMapSet();
 361   OopMap* oop_map = NULL;
 362   switch (id) {
 363   case forward_exception_id:
 364     // We&#39;re handling an exception in the context of a compiled frame.
 365     // The registers have been saved in the standard places.  Perform
 366     // an exception lookup in the caller and dispatch to the handler
 367     // if found.  Otherwise unwind and dispatch to the callers
 368     // exception handler.
 369     oop_map = generate_oop_map(sasm, 1 /*thread*/);
 370 
 371     // load and clear pending exception oop into r0
 372     __ ldr(exception_oop, Address(rthread, Thread::pending_exception_offset()));
 373     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 374 
 375     // load issuing PC (the return address for this stub) into r3
 376     __ ldr(exception_pc, Address(rfp, 1*BytesPerWord));
 377 
 378     // make sure that the vm_results are cleared (may be unnecessary)
 379     __ str(zr, Address(rthread, JavaThread::vm_result_offset()));
 380     __ str(zr, Address(rthread, JavaThread::vm_result_2_offset()));
 381     break;
 382   case handle_exception_nofpu_id:
 383   case handle_exception_id:
 384     // At this point all registers MAY be live.
 385     oop_map = save_live_registers(sasm, id != handle_exception_nofpu_id);
 386     break;
 387   case handle_exception_from_callee_id: {
 388     // At this point all registers except exception oop (r0) and
 389     // exception pc (lr) are dead.
 390     const int frame_size = 2 /*fp, return address*/;
 391     oop_map = new OopMap(frame_size * VMRegImpl::slots_per_word, 0);
 392     sasm-&gt;set_frame_size(frame_size);
 393     break;
 394   }
 395   default:
 396     __ should_not_reach_here();
 397     break;
 398   }
 399 
 400   // verify that only r0 and r3 are valid at this time
 401   __ invalidate_registers(false, true, true, false, true, true);
 402   // verify that r0 contains a valid exception
 403   __ verify_not_null_oop(exception_oop);
 404 
 405 #ifdef ASSERT
 406   // check that fields in JavaThread for exception oop and issuing pc are
 407   // empty before writing to them
 408   Label oop_empty;
 409   __ ldr(rscratch1, Address(rthread, JavaThread::exception_oop_offset()));
 410   __ cbz(rscratch1, oop_empty);
 411   __ stop(&quot;exception oop already set&quot;);
 412   __ bind(oop_empty);
 413 
 414   Label pc_empty;
 415   __ ldr(rscratch1, Address(rthread, JavaThread::exception_pc_offset()));
 416   __ cbz(rscratch1, pc_empty);
 417   __ stop(&quot;exception pc already set&quot;);
 418   __ bind(pc_empty);
 419 #endif
 420 
 421   // save exception oop and issuing pc into JavaThread
 422   // (exception handler will load it from here)
 423   __ str(exception_oop, Address(rthread, JavaThread::exception_oop_offset()));
 424   __ str(exception_pc, Address(rthread, JavaThread::exception_pc_offset()));
 425 
 426   // patch throwing pc into return address (has bci &amp; oop map)
 427   __ str(exception_pc, Address(rfp, 1*BytesPerWord));
 428 
 429   // compute the exception handler.
 430   // the exception oop and the throwing pc are read from the fields in JavaThread
 431   int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));
 432   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 433 
 434   // r0: handler address
 435   //      will be the deopt blob if nmethod was deoptimized while we looked up
 436   //      handler regardless of whether handler existed in the nmethod.
 437 
 438   // only r0 is valid at this time, all other registers have been destroyed by the runtime call
 439   __ invalidate_registers(false, true, true, true, true, true);
 440 
 441   // patch the return address, this stub will directly return to the exception handler
 442   __ str(r0, Address(rfp, 1*BytesPerWord));
 443 
 444   switch (id) {
 445   case forward_exception_id:
 446   case handle_exception_nofpu_id:
 447   case handle_exception_id:
 448     // Restore the registers that were saved at the beginning.
 449     restore_live_registers(sasm, id != handle_exception_nofpu_id);
 450     break;
 451   case handle_exception_from_callee_id:
 452     // Pop the return address.
 453     __ leave();
 454     __ ret(lr);  // jump to exception handler
 455     break;
 456   default:  ShouldNotReachHere();
 457   }
 458 
 459   return oop_maps;
 460 }
 461 
 462 
 463 void Runtime1::generate_unwind_exception(StubAssembler *sasm) {
 464   // incoming parameters
 465   const Register exception_oop = r0;
 466   // callee-saved copy of exception_oop during runtime call
 467   const Register exception_oop_callee_saved = r19;
 468   // other registers used in this stub
 469   const Register exception_pc = r3;
 470   const Register handler_addr = r1;
 471 
 472   // verify that only r0, is valid at this time
 473   __ invalidate_registers(false, true, true, true, true, true);
 474 
 475 #ifdef ASSERT
 476   // check that fields in JavaThread for exception oop and issuing pc are empty
 477   Label oop_empty;
 478   __ ldr(rscratch1, Address(rthread, JavaThread::exception_oop_offset()));
 479   __ cbz(rscratch1, oop_empty);
 480   __ stop(&quot;exception oop must be empty&quot;);
 481   __ bind(oop_empty);
 482 
 483   Label pc_empty;
 484   __ ldr(rscratch1, Address(rthread, JavaThread::exception_pc_offset()));
 485   __ cbz(rscratch1, pc_empty);
 486   __ stop(&quot;exception pc must be empty&quot;);
 487   __ bind(pc_empty);
 488 #endif
 489 
 490   // Save our return address because
 491   // exception_handler_for_return_address will destroy it.  We also
 492   // save exception_oop
 493   __ stp(lr, exception_oop, Address(__ pre(sp, -2 * wordSize)));
 494 
 495   // search the exception handler address of the caller (using the return address)
 496   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), rthread, lr);
 497   // r0: exception handler address of the caller
 498 
 499   // Only R0 is valid at this time; all other registers have been
 500   // destroyed by the call.
 501   __ invalidate_registers(false, true, true, true, false, true);
 502 
 503   // move result of call into correct register
 504   __ mov(handler_addr, r0);
 505 
 506   // get throwing pc (= return address).
 507   // lr has been destroyed by the call
 508   __ ldp(lr, exception_oop, Address(__ post(sp, 2 * wordSize)));
 509   __ mov(r3, lr);
 510 
 511   __ verify_not_null_oop(exception_oop);
 512 
 513   // continue at exception handler (return address removed)
 514   // note: do *not* remove arguments when unwinding the
 515   //       activation since the caller assumes having
 516   //       all arguments on the stack when entering the
 517   //       runtime to determine the exception handler
 518   //       (GC happens at call site with arguments!)
 519   // r0: exception oop
 520   // r3: throwing pc
 521   // r1: exception handler
 522   __ br(handler_addr);
 523 }
 524 
 525 
 526 
 527 OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {
 528   // use the maximum number of runtime-arguments here because it is difficult to
 529   // distinguish each RT-Call.
 530   // Note: This number affects also the RT-Call in generate_handle_exception because
 531   //       the oop-map is shared for all calls.
 532   DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
 533   assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
 534 
 535   OopMap* oop_map = save_live_registers(sasm);
 536 
 537   __ mov(c_rarg0, rthread);
 538   Label retaddr;
 539   __ set_last_Java_frame(sp, rfp, retaddr, rscratch1);
 540   // do the call
 541   __ lea(rscratch1, RuntimeAddress(target));
 542   __ blr(rscratch1);
 543   __ bind(retaddr);
 544   OopMapSet* oop_maps = new OopMapSet();
 545   oop_maps-&gt;add_gc_map(__ offset(), oop_map);
 546   // verify callee-saved register
 547 #ifdef ASSERT
 548   { Label L;
 549     __ get_thread(rscratch1);
 550     __ cmp(rthread, rscratch1);
 551     __ br(Assembler::EQ, L);
 552     __ stop(&quot;StubAssembler::call_RT: rthread not callee saved?&quot;);
 553     __ bind(L);
 554   }
 555 #endif
 556   __ reset_last_Java_frame(true);
 557   __ maybe_isb();
 558 
 559   // check for pending exceptions
 560   { Label L;
 561     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 562     __ cbz(rscratch1, L);
 563     // exception pending =&gt; remove activation and forward to exception handler
 564 
 565     { Label L1;
 566       __ cbnz(r0, L1);                                  // have we deoptimized?
 567       __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::forward_exception_id)));
 568       __ bind(L1);
 569     }
 570 
 571     // the deopt blob expects exceptions in the special fields of
 572     // JavaThread, so copy and clear pending exception.
 573 
 574     // load and clear pending exception
 575     __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
 576     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 577 
 578     // check that there is really a valid exception
 579     __ verify_not_null_oop(r0);
 580 
 581     // load throwing pc: this is the return address of the stub
 582     __ mov(r3, lr);
 583 
 584 #ifdef ASSERT
 585     // check that fields in JavaThread for exception oop and issuing pc are empty
 586     Label oop_empty;
 587     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 588     __ cbz(rscratch1, oop_empty);
 589     __ stop(&quot;exception oop must be empty&quot;);
 590     __ bind(oop_empty);
 591 
 592     Label pc_empty;
 593     __ ldr(rscratch1, Address(rthread, JavaThread::exception_pc_offset()));
 594     __ cbz(rscratch1, pc_empty);
 595     __ stop(&quot;exception pc must be empty&quot;);
 596     __ bind(pc_empty);
 597 #endif
 598 
 599     // store exception oop and throwing pc to JavaThread
 600     __ str(r0, Address(rthread, JavaThread::exception_oop_offset()));
 601     __ str(r3, Address(rthread, JavaThread::exception_pc_offset()));
 602 
 603     restore_live_registers(sasm);
 604 
 605     __ leave();
 606 
 607     // Forward the exception directly to deopt blob. We can blow no
 608     // registers and must leave throwing pc on the stack.  A patch may
 609     // have values live in registers so the entry point with the
 610     // exception in tls.
 611     __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_exception_in_tls()));
 612 
 613     __ bind(L);
 614   }
 615 
 616 
 617   // Runtime will return true if the nmethod has been deoptimized during
 618   // the patching process. In that case we must do a deopt reexecute instead.
 619 
 620   Label cont;
 621 
 622   __ cbz(r0, cont);                                 // have we deoptimized?
 623 
 624   // Will reexecute. Proper return address is already on the stack we just restore
 625   // registers, pop all of our frame but the return address and jump to the deopt blob
 626   restore_live_registers(sasm);
 627   __ leave();
 628   __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_reexecution()));
 629 
 630   __ bind(cont);
 631   restore_live_registers(sasm);
 632   __ leave();
 633   __ ret(lr);
 634 
 635   return oop_maps;
 636 }
 637 
 638 
 639 OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {
 640 
 641   const Register exception_oop = r0;
 642   const Register exception_pc  = r3;
 643 
 644   // for better readability
 645   const bool must_gc_arguments = true;
 646   const bool dont_gc_arguments = false;
 647 
 648   // default value; overwritten for some optimized stubs that are called from methods that do not use the fpu
 649   bool save_fpu_registers = true;
 650 
 651   // stub code &amp; info for the different stubs
 652   OopMapSet* oop_maps = NULL;
 653   OopMap* oop_map = NULL;
 654   switch (id) {
 655     {
 656     case forward_exception_id:
 657       {
 658         oop_maps = generate_handle_exception(id, sasm);
 659         __ leave();
 660         __ ret(lr);
 661       }
 662       break;
 663 
 664     case throw_div0_exception_id:
 665       { StubFrame f(sasm, &quot;throw_div0_exception&quot;, dont_gc_arguments);
 666         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);
 667       }
 668       break;
 669 
 670     case throw_null_pointer_exception_id:
 671       { StubFrame f(sasm, &quot;throw_null_pointer_exception&quot;, dont_gc_arguments);
 672         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);
 673       }
 674       break;
 675 
 676     case new_instance_id:
 677     case fast_new_instance_id:
 678     case fast_new_instance_init_check_id:
 679       {
 680         Register klass = r3; // Incoming
 681         Register obj   = r0; // Result
 682 
 683         if (id == new_instance_id) {
 684           __ set_info(&quot;new_instance&quot;, dont_gc_arguments);
 685         } else if (id == fast_new_instance_id) {
 686           __ set_info(&quot;fast new_instance&quot;, dont_gc_arguments);
 687         } else {
 688           assert(id == fast_new_instance_init_check_id, &quot;bad StubID&quot;);
 689           __ set_info(&quot;fast new_instance init check&quot;, dont_gc_arguments);
 690         }
 691 
 692         // If TLAB is disabled, see if there is support for inlining contiguous
 693         // allocations.
 694         // Otherwise, just go to the slow path.
 695         if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &amp;&amp;
 696             !UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc()) {
 697           Label slow_path;
 698           Register obj_size = r2;
 699           Register t1       = r19;
 700           Register t2       = r4;
 701           assert_different_registers(klass, obj, obj_size, t1, t2);
 702 
 703           __ stp(r19, zr, Address(__ pre(sp, -2 * wordSize)));
 704 
 705           if (id == fast_new_instance_init_check_id) {
 706             // make sure the klass is initialized
 707             __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));
 708             __ cmpw(rscratch1, InstanceKlass::fully_initialized);
 709             __ br(Assembler::NE, slow_path);
 710           }
 711 
 712 #ifdef ASSERT
 713           // assert object can be fast path allocated
 714           {
 715             Label ok, not_ok;
 716             __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));
 717             __ cmp(obj_size, (u1)0);
 718             __ br(Assembler::LE, not_ok);  // make sure it&#39;s an instance (LH &gt; 0)
 719             __ tstw(obj_size, Klass::_lh_instance_slow_path_bit);
 720             __ br(Assembler::EQ, ok);
 721             __ bind(not_ok);
 722             __ stop(&quot;assert(can be fast path allocated)&quot;);
 723             __ should_not_reach_here();
 724             __ bind(ok);
 725           }
 726 #endif // ASSERT
 727 
 728           // get the instance size (size is postive so movl is fine for 64bit)
 729           __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));
 730 
 731           __ eden_allocate(obj, obj_size, 0, t1, slow_path);
 732 
 733           __ initialize_object(obj, klass, obj_size, 0, t1, t2, /* is_tlab_allocated */ false);
 734           __ verify_oop(obj);
 735           __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));
 736           __ ret(lr);
 737 
 738           __ bind(slow_path);
 739           __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));
 740         }
 741 
 742         __ enter();
 743         OopMap* map = save_live_registers(sasm);
 744         int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);
 745         oop_maps = new OopMapSet();
 746         oop_maps-&gt;add_gc_map(call_offset, map);
 747         restore_live_registers_except_r0(sasm);
 748         __ verify_oop(obj);
 749         __ leave();
 750         __ ret(lr);
 751 
 752         // r0,: new instance
 753       }
 754 
 755       break;
 756 
 757     case counter_overflow_id:
 758       {
 759         Register bci = r0, method = r1;
 760         __ enter();
 761         OopMap* map = save_live_registers(sasm);
 762         // Retrieve bci
 763         __ ldrw(bci, Address(rfp, 2*BytesPerWord));
 764         // And a pointer to the Method*
 765         __ ldr(method, Address(rfp, 3*BytesPerWord));
 766         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, counter_overflow), bci, method);
 767         oop_maps = new OopMapSet();
 768         oop_maps-&gt;add_gc_map(call_offset, map);
 769         restore_live_registers(sasm);
 770         __ leave();
 771         __ ret(lr);
 772       }
 773       break;
 774 
 775     case new_type_array_id:
 776     case new_object_array_id:
 777       {
 778         Register length   = r19; // Incoming
 779         Register klass    = r3; // Incoming
 780         Register obj      = r0; // Result
 781 
 782         if (id == new_type_array_id) {
 783           __ set_info(&quot;new_type_array&quot;, dont_gc_arguments);
 784         } else {
 785           __ set_info(&quot;new_object_array&quot;, dont_gc_arguments);
 786         }
 787 
 788 #ifdef ASSERT
 789         // assert object type is really an array of the proper kind
 790         {
 791           Label ok;
 792           Register t0 = obj;
 793           __ ldrw(t0, Address(klass, Klass::layout_helper_offset()));
 794           __ asrw(t0, t0, Klass::_lh_array_tag_shift);
 795           int tag = ((id == new_type_array_id)
 796                      ? Klass::_lh_array_tag_type_value
 797                      : Klass::_lh_array_tag_obj_value);
 798           __ mov(rscratch1, tag);
 799           __ cmpw(t0, rscratch1);
 800           __ br(Assembler::EQ, ok);
 801           __ stop(&quot;assert(is an array klass)&quot;);
 802           __ should_not_reach_here();
 803           __ bind(ok);
 804         }
 805 #endif // ASSERT
 806 
 807         // If TLAB is disabled, see if there is support for inlining contiguous
 808         // allocations.
 809         // Otherwise, just go to the slow path.
 810         if (!UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc()) {
 811           Register arr_size = r4;
 812           Register t1       = r2;
 813           Register t2       = r5;
 814           Label slow_path;
 815           assert_different_registers(length, klass, obj, arr_size, t1, t2);
 816 
 817           // check that array length is small enough for fast path.
 818           __ mov(rscratch1, C1_MacroAssembler::max_array_allocation_length);
 819           __ cmpw(length, rscratch1);
 820           __ br(Assembler::HI, slow_path);
 821 
 822           // get the allocation size: round_up(hdr + length &lt;&lt; (layout_helper &amp; 0x1F))
 823           // since size is positive ldrw does right thing on 64bit
 824           __ ldrw(t1, Address(klass, Klass::layout_helper_offset()));
 825           // since size is positive movw does right thing on 64bit
 826           __ movw(arr_size, length);
 827           __ lslvw(arr_size, length, t1);
 828           __ ubfx(t1, t1, Klass::_lh_header_size_shift,
 829                   exact_log2(Klass::_lh_header_size_mask + 1));
 830           __ add(arr_size, arr_size, t1);
 831           __ add(arr_size, arr_size, MinObjAlignmentInBytesMask); // align up
 832           __ andr(arr_size, arr_size, ~MinObjAlignmentInBytesMask);
 833 
 834           __ eden_allocate(obj, arr_size, 0, t1, slow_path);  // preserves arr_size
 835 
 836           __ initialize_header(obj, klass, length, t1, t2);
 837           __ ldrb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift / BitsPerByte)));
 838           assert(Klass::_lh_header_size_shift % BitsPerByte == 0, &quot;bytewise&quot;);
 839           assert(Klass::_lh_header_size_mask &lt;= 0xFF, &quot;bytewise&quot;);
 840           __ andr(t1, t1, Klass::_lh_header_size_mask);
 841           __ sub(arr_size, arr_size, t1);  // body length
 842           __ add(t1, t1, obj);       // body start
 843           __ initialize_body(t1, arr_size, 0, t2);
 844           __ membar(Assembler::StoreStore);
 845           __ verify_oop(obj);
 846 
 847           __ ret(lr);
 848 
 849           __ bind(slow_path);
 850         }
 851 
 852         __ enter();
 853         OopMap* map = save_live_registers(sasm);
 854         int call_offset;
 855         if (id == new_type_array_id) {
 856           call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_type_array), klass, length);
 857         } else {
 858           call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_object_array), klass, length);
 859         }
 860 
 861         oop_maps = new OopMapSet();
 862         oop_maps-&gt;add_gc_map(call_offset, map);
 863         restore_live_registers_except_r0(sasm);
 864 
 865         __ verify_oop(obj);
 866         __ leave();
 867         __ ret(lr);
 868 
 869         // r0: new array
 870       }
 871       break;
 872 
 873     case new_multi_array_id:
 874       { StubFrame f(sasm, &quot;new_multi_array&quot;, dont_gc_arguments);
 875         // r0,: klass
 876         // r19,: rank
 877         // r2: address of 1st dimension
 878         OopMap* map = save_live_registers(sasm);
 879         __ mov(c_rarg1, r0);
 880         __ mov(c_rarg3, r2);
 881         __ mov(c_rarg2, r19);
 882         int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, new_multi_array), r1, r2, r3);
 883 
 884         oop_maps = new OopMapSet();
 885         oop_maps-&gt;add_gc_map(call_offset, map);
 886         restore_live_registers_except_r0(sasm);
 887 
 888         // r0,: new multi array
 889         __ verify_oop(r0);
 890       }
 891       break;
 892 
 893     case register_finalizer_id:
 894       {
 895         __ set_info(&quot;register_finalizer&quot;, dont_gc_arguments);
 896 
 897         // This is called via call_runtime so the arguments
 898         // will be place in C abi locations
 899 
 900         __ verify_oop(c_rarg0);
 901 
 902         // load the klass and check the has finalizer flag
 903         Label register_finalizer;
 904         Register t = r5;
 905         __ load_klass(t, r0);
 906         __ ldrw(t, Address(t, Klass::access_flags_offset()));
 907         __ tbnz(t, exact_log2(JVM_ACC_HAS_FINALIZER), register_finalizer);
 908         __ ret(lr);
 909 
 910         __ bind(register_finalizer);
 911         __ enter();
 912         OopMap* oop_map = save_live_registers(sasm);
 913         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), r0);
 914         oop_maps = new OopMapSet();
 915         oop_maps-&gt;add_gc_map(call_offset, oop_map);
 916 
 917         // Now restore all the live registers
 918         restore_live_registers(sasm);
 919 
 920         __ leave();
 921         __ ret(lr);
 922       }
 923       break;
 924 
 925     case throw_class_cast_exception_id:
 926       { StubFrame f(sasm, &quot;throw_class_cast_exception&quot;, dont_gc_arguments);
 927         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);
 928       }
 929       break;
 930 
 931     case throw_incompatible_class_change_error_id:
 932       { StubFrame f(sasm, &quot;throw_incompatible_class_cast_exception&quot;, dont_gc_arguments);
 933         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);
 934       }
 935       break;
 936 
 937     case slow_subtype_check_id:
 938       {
 939         // Typical calling sequence:
 940         // __ push(klass_RInfo);  // object klass or other subclass
 941         // __ push(sup_k_RInfo);  // array element klass or other superclass
 942         // __ bl(slow_subtype_check);
 943         // Note that the subclass is pushed first, and is therefore deepest.
 944         enum layout {
 945           r0_off, r0_off_hi,
 946           r2_off, r2_off_hi,
 947           r4_off, r4_off_hi,
 948           r5_off, r5_off_hi,
 949           sup_k_off, sup_k_off_hi,
 950           klass_off, klass_off_hi,
 951           framesize,
 952           result_off = sup_k_off
 953         };
 954 
 955         __ set_info(&quot;slow_subtype_check&quot;, dont_gc_arguments);
 956         __ push(RegSet::of(r0, r2, r4, r5), sp);
 957 
 958         // This is called by pushing args and not with C abi
 959         // __ ldr(r4, Address(sp, (klass_off) * VMRegImpl::stack_slot_size)); // subclass
 960         // __ ldr(r0, Address(sp, (sup_k_off) * VMRegImpl::stack_slot_size)); // superclass
 961 
 962         __ ldp(r4, r0, Address(sp, (sup_k_off) * VMRegImpl::stack_slot_size));
 963 
 964         Label miss;
 965         __ check_klass_subtype_slow_path(r4, r0, r2, r5, NULL, &amp;miss);
 966 
 967         // fallthrough on success:
 968         __ mov(rscratch1, 1);
 969         __ str(rscratch1, Address(sp, (result_off) * VMRegImpl::stack_slot_size)); // result
 970         __ pop(RegSet::of(r0, r2, r4, r5), sp);
 971         __ ret(lr);
 972 
 973         __ bind(miss);
 974         __ str(zr, Address(sp, (result_off) * VMRegImpl::stack_slot_size)); // result
 975         __ pop(RegSet::of(r0, r2, r4, r5), sp);
 976         __ ret(lr);
 977       }
 978       break;
 979 
 980     case monitorenter_nofpu_id:
 981       save_fpu_registers = false;
 982       // fall through
 983     case monitorenter_id:
 984       {
 985         StubFrame f(sasm, &quot;monitorenter&quot;, dont_gc_arguments);
 986         OopMap* map = save_live_registers(sasm, save_fpu_registers);
 987 
 988         // Called with store_parameter and not C abi
 989 
 990         f.load_argument(1, r0); // r0,: object
 991         f.load_argument(0, r1); // r1,: lock address
 992 
 993         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), r0, r1);
 994 
 995         oop_maps = new OopMapSet();
 996         oop_maps-&gt;add_gc_map(call_offset, map);
 997         restore_live_registers(sasm, save_fpu_registers);
 998       }
 999       break;
1000 
1001     case monitorexit_nofpu_id:
1002       save_fpu_registers = false;
1003       // fall through
1004     case monitorexit_id:
1005       {
1006         StubFrame f(sasm, &quot;monitorexit&quot;, dont_gc_arguments);
1007         OopMap* map = save_live_registers(sasm, save_fpu_registers);
1008 
1009         // Called with store_parameter and not C abi
1010 
1011         f.load_argument(0, r0); // r0,: lock address
1012 
1013         // note: really a leaf routine but must setup last java sp
1014         //       =&gt; use call_RT for now (speed can be improved by
1015         //       doing last java sp setup manually)
1016         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), r0);
1017 
1018         oop_maps = new OopMapSet();
1019         oop_maps-&gt;add_gc_map(call_offset, map);
1020         restore_live_registers(sasm, save_fpu_registers);
1021       }
1022       break;
1023 
1024     case deoptimize_id:
1025       {
1026         StubFrame f(sasm, &quot;deoptimize&quot;, dont_gc_arguments);
1027         OopMap* oop_map = save_live_registers(sasm);
1028         f.load_argument(0, c_rarg1);
1029         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, deoptimize), c_rarg1);
1030 
1031         oop_maps = new OopMapSet();
1032         oop_maps-&gt;add_gc_map(call_offset, oop_map);
1033         restore_live_registers(sasm);
1034         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
1035         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
1036         __ leave();
1037         __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_reexecution()));
1038       }
1039       break;
1040 
1041     case throw_range_check_failed_id:
1042       { StubFrame f(sasm, &quot;range_check_failed&quot;, dont_gc_arguments);
1043         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), true);
1044       }
1045       break;
1046 
1047     case unwind_exception_id:
1048       { __ set_info(&quot;unwind_exception&quot;, dont_gc_arguments);
1049         // note: no stubframe since we are about to leave the current
1050         //       activation and we are calling a leaf VM function only.
1051         generate_unwind_exception(sasm);
1052       }
1053       break;
1054 
1055     case access_field_patching_id:
1056       { StubFrame f(sasm, &quot;access_field_patching&quot;, dont_gc_arguments);
1057         // we should set up register map
1058         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));
1059       }
1060       break;
1061 
1062     case load_klass_patching_id:
1063       { StubFrame f(sasm, &quot;load_klass_patching&quot;, dont_gc_arguments);
1064         // we should set up register map
1065         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));
1066       }
1067       break;
1068 
1069     case load_mirror_patching_id:
1070       { StubFrame f(sasm, &quot;load_mirror_patching&quot;, dont_gc_arguments);
1071         // we should set up register map
1072         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));
1073       }
1074       break;
1075 
1076     case load_appendix_patching_id:
1077       { StubFrame f(sasm, &quot;load_appendix_patching&quot;, dont_gc_arguments);
1078         // we should set up register map
1079         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));
1080       }
1081       break;
1082 
1083     case handle_exception_nofpu_id:
1084     case handle_exception_id:
1085       { StubFrame f(sasm, &quot;handle_exception&quot;, dont_gc_arguments);
1086         oop_maps = generate_handle_exception(id, sasm);
1087       }
1088       break;
1089 
1090     case handle_exception_from_callee_id:
1091       { StubFrame f(sasm, &quot;handle_exception_from_callee&quot;, dont_gc_arguments);
1092         oop_maps = generate_handle_exception(id, sasm);
1093       }
1094       break;
1095 
1096     case throw_index_exception_id:
1097       { StubFrame f(sasm, &quot;index_range_check_failed&quot;, dont_gc_arguments);
1098         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);
1099       }
1100       break;
1101 
1102     case throw_array_store_exception_id:
1103       { StubFrame f(sasm, &quot;throw_array_store_exception&quot;, dont_gc_arguments);
1104         // tos + 0: link
1105         //     + 1: return address
1106         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);
1107       }
1108       break;
1109 
1110     case predicate_failed_trap_id:
1111       {
1112         StubFrame f(sasm, &quot;predicate_failed_trap&quot;, dont_gc_arguments);
1113 
1114         OopMap* map = save_live_registers(sasm);
1115 
1116         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));
1117         oop_maps = new OopMapSet();
1118         oop_maps-&gt;add_gc_map(call_offset, map);
1119         restore_live_registers(sasm);
1120         __ leave();
1121         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
1122         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
1123 
1124         __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_reexecution()));
1125       }
1126       break;
1127 
1128     case dtrace_object_alloc_id:
1129       { // c_rarg0: object
1130         StubFrame f(sasm, &quot;dtrace_object_alloc&quot;, dont_gc_arguments);
1131         save_live_registers(sasm);
1132 
1133         __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), c_rarg0);
1134 
1135         restore_live_registers(sasm);
1136       }
1137       break;
1138 
1139     default:
1140       { StubFrame f(sasm, &quot;unimplemented entry&quot;, dont_gc_arguments);
1141         __ mov(r0, (int)id);
1142         __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, unimplemented_entry), r0);
1143         __ should_not_reach_here();
1144       }
1145       break;
1146     }
1147   }
1148   return oop_maps;
1149 }
1150 
1151 #undef __
1152 
1153 const char *Runtime1::pd_name_for_address(address entry) { Unimplemented(); return 0; }
    </pre>
  </body>
</html>