<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="shenandoahEvacOOMHandler.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  29 #include &quot;gc/shared/gcArguments.hpp&quot;
  30 #include &quot;gc/shared/gcTimer.hpp&quot;
  31 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  32 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  33 #include &quot;gc/shared/memAllocator.hpp&quot;
  34 #include &quot;gc/shared/oopStorageSet.hpp&quot;
  35 #include &quot;gc/shared/plab.hpp&quot;
  36 
  37 #include &quot;gc/shenandoah/shenandoahBarrierSet.hpp&quot;
  38 #include &quot;gc/shenandoah/shenandoahClosures.inline.hpp&quot;
  39 #include &quot;gc/shenandoah/shenandoahCollectionSet.hpp&quot;
  40 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
  41 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
  42 #include &quot;gc/shenandoah/shenandoahConcurrentRoots.hpp&quot;
  43 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
  44 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
  45 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
  46 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
  47 #include &quot;gc/shenandoah/shenandoahHeapRegion.inline.hpp&quot;
  48 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;

  49 #include &quot;gc/shenandoah/shenandoahMarkCompact.hpp&quot;
  50 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
  51 #include &quot;gc/shenandoah/shenandoahMemoryPool.hpp&quot;
  52 #include &quot;gc/shenandoah/shenandoahMetrics.hpp&quot;
  53 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
  54 #include &quot;gc/shenandoah/shenandoahOopClosures.inline.hpp&quot;
  55 #include &quot;gc/shenandoah/shenandoahPacer.inline.hpp&quot;
  56 #include &quot;gc/shenandoah/shenandoahPadding.hpp&quot;
  57 #include &quot;gc/shenandoah/shenandoahParallelCleaning.inline.hpp&quot;
  58 #include &quot;gc/shenandoah/shenandoahRootProcessor.inline.hpp&quot;
  59 #include &quot;gc/shenandoah/shenandoahStringDedup.hpp&quot;
  60 #include &quot;gc/shenandoah/shenandoahTaskqueue.hpp&quot;
  61 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
  62 #include &quot;gc/shenandoah/shenandoahVerifier.hpp&quot;
  63 #include &quot;gc/shenandoah/shenandoahCodeRoots.hpp&quot;
  64 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
  65 #include &quot;gc/shenandoah/shenandoahWorkGroup.hpp&quot;
  66 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
  67 #include &quot;gc/shenandoah/mode/shenandoahIUMode.hpp&quot;
  68 #include &quot;gc/shenandoah/mode/shenandoahPassiveMode.hpp&quot;
</pre>
<hr />
<pre>
  93     shenandoah_assert_not_forwarded(p, obj);
  94   }
  95 }
  96 
  97 void ShenandoahAssertToSpaceClosure::do_oop(narrowOop* p) { do_oop_work(p); }
  98 void ShenandoahAssertToSpaceClosure::do_oop(oop* p)       { do_oop_work(p); }
  99 #endif
 100 
 101 class ShenandoahPretouchHeapTask : public AbstractGangTask {
 102 private:
 103   ShenandoahRegionIterator _regions;
 104   const size_t _page_size;
 105 public:
 106   ShenandoahPretouchHeapTask(size_t page_size) :
 107     AbstractGangTask(&quot;Shenandoah Pretouch Heap&quot;),
 108     _page_size(page_size) {}
 109 
 110   virtual void work(uint worker_id) {
 111     ShenandoahHeapRegion* r = _regions.next();
 112     while (r != NULL) {
<span class="line-modified"> 113       os::pretouch_memory(r-&gt;bottom(), r-&gt;end(), _page_size);</span>


 114       r = _regions.next();
 115     }
 116   }
 117 };
 118 
 119 class ShenandoahPretouchBitmapTask : public AbstractGangTask {
 120 private:
 121   ShenandoahRegionIterator _regions;
 122   char* _bitmap_base;
 123   const size_t _bitmap_size;
 124   const size_t _page_size;
 125 public:
 126   ShenandoahPretouchBitmapTask(char* bitmap_base, size_t bitmap_size, size_t page_size) :
 127     AbstractGangTask(&quot;Shenandoah Pretouch Bitmap&quot;),
 128     _bitmap_base(bitmap_base),
 129     _bitmap_size(bitmap_size),
 130     _page_size(page_size) {}
 131 
 132   virtual void work(uint worker_id) {
 133     ShenandoahHeapRegion* r = _regions.next();
 134     while (r != NULL) {
 135       size_t start = r-&gt;index()       * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 136       size_t end   = (r-&gt;index() + 1) * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 137       assert (end &lt;= _bitmap_size, &quot;end is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, end, _bitmap_size);
 138 
<span class="line-modified"> 139       os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);</span>


 140 
 141       r = _regions.next();
 142     }
 143   }
 144 };
 145 
 146 jint ShenandoahHeap::initialize() {
 147   //
 148   // Figure out heap sizing
 149   //
 150 
 151   size_t init_byte_size = InitialHeapSize;
 152   size_t min_byte_size  = MinHeapSize;
 153   size_t max_byte_size  = MaxHeapSize;
 154   size_t heap_alignment = HeapAlignment;
 155 
 156   size_t reg_size_bytes = ShenandoahHeapRegion::region_size_bytes();
 157 
<span class="line-removed"> 158   if (ShenandoahAlwaysPreTouch) {</span>
<span class="line-removed"> 159     // Enabled pre-touch means the entire heap is committed right away.</span>
<span class="line-removed"> 160     init_byte_size = max_byte_size;</span>
<span class="line-removed"> 161   }</span>
<span class="line-removed"> 162 </span>
 163   Universe::check_alignment(max_byte_size,  reg_size_bytes, &quot;Shenandoah heap&quot;);
 164   Universe::check_alignment(init_byte_size, reg_size_bytes, &quot;Shenandoah heap&quot;);
 165 
 166   _num_regions = ShenandoahHeapRegion::region_count();
 167 
 168   // Now we know the number of regions, initialize the heuristics.
 169   initialize_heuristics();
 170 
 171   size_t num_committed_regions = init_byte_size / reg_size_bytes;
 172   num_committed_regions = MIN2(num_committed_regions, _num_regions);
 173   assert(num_committed_regions &lt;= _num_regions, &quot;sanity&quot;);
 174   _initial_size = num_committed_regions * reg_size_bytes;
 175 
 176   size_t num_min_regions = min_byte_size / reg_size_bytes;
 177   num_min_regions = MIN2(num_min_regions, _num_regions);
 178   assert(num_min_regions &lt;= _num_regions, &quot;sanity&quot;);
 179   _minimum_size = num_min_regions * reg_size_bytes;
 180 
 181   _committed = _initial_size;
 182 
</pre>
<hr />
<pre>
 275   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 276   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 277   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 278   _aux_bitmap_region_special = aux_bitmap.special();
 279   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 280 
 281   //
 282   // Create regions and region sets
 283   //
 284   size_t region_align = align_up(sizeof(ShenandoahHeapRegion), SHENANDOAH_CACHE_LINE_SIZE);
 285   size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);
 286   region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());
 287 
 288   ReservedSpace region_storage(region_storage_size, region_page_size);
 289   MemTracker::record_virtual_memory_type(region_storage.base(), mtGC);
 290   if (!region_storage.special()) {
 291     os::commit_memory_or_exit(region_storage.base(), region_storage_size, region_page_size, false,
 292                               &quot;Cannot commit region memory&quot;);
 293   }
 294 



























 295   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 296   _free_set = new ShenandoahFreeSet(this, _num_regions);
<span class="line-removed"> 297   _collection_set = new ShenandoahCollectionSet(this, sh_rs.base(), sh_rs.size());</span>
 298 
 299   {
 300     ShenandoahHeapLocker locker(lock());
 301 
 302     for (size_t i = 0; i &lt; _num_regions; i++) {
 303       HeapWord* start = (HeapWord*)sh_rs.base() + ShenandoahHeapRegion::region_size_words() * i;
 304       bool is_committed = i &lt; num_committed_regions;
 305       void* loc = region_storage.base() + i * region_align;
 306 
 307       ShenandoahHeapRegion* r = new (loc) ShenandoahHeapRegion(start, i, is_committed);
 308       assert(is_aligned(r, SHENANDOAH_CACHE_LINE_SIZE), &quot;Sanity&quot;);
 309 
 310       _marking_context-&gt;initialize_top_at_mark_start(r);
 311       _regions[i] = r;
 312       assert(!collection_set()-&gt;is_in(i), &quot;New region should not be in collection set&quot;);
 313     }
 314 
 315     // Initialize to complete
 316     _marking_context-&gt;mark_complete();
 317 
 318     _free_set-&gt;rebuild();
 319   }
 320 
<span class="line-modified"> 321   if (ShenandoahAlwaysPreTouch) {</span>
<span class="line-removed"> 322     assert(!AlwaysPreTouch, &quot;Should have been overridden&quot;);</span>
<span class="line-removed"> 323 </span>
 324     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 325     // before initialize() below zeroes it with initializing thread. For any given region,
 326     // we touch the region and the corresponding bitmaps from the same thread.
 327     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 328 
<span class="line-modified"> 329     size_t pretouch_heap_page_size = heap_page_size;</span>
<span class="line-modified"> 330     size_t pretouch_bitmap_page_size = bitmap_page_size;</span>
 331 
 332 #ifdef LINUX
 333     // UseTransparentHugePages would madvise that backing memory can be coalesced into huge
 334     // pages. But, the kernel needs to know that every small page is used, in order to coalesce
 335     // them into huge one. Therefore, we need to pretouch with smaller pages.
 336     if (UseTransparentHugePages) {
<span class="line-modified"> 337       pretouch_heap_page_size = (size_t)os::vm_page_size();</span>
<span class="line-modified"> 338       pretouch_bitmap_page_size = (size_t)os::vm_page_size();</span>
 339     }
 340 #endif
 341 
 342     // OS memory managers may want to coalesce back-to-back pages. Make their jobs
 343     // simpler by pre-touching continuous spaces (heap and bitmap) separately.
 344 
<span class="line-modified"> 345     log_info(gc, init)(&quot;Pretouch bitmap: &quot; SIZE_FORMAT &quot; regions, &quot; SIZE_FORMAT &quot; bytes page&quot;,</span>
<span class="line-removed"> 346                        _num_regions, pretouch_bitmap_page_size);</span>
<span class="line-removed"> 347     ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, pretouch_bitmap_page_size);</span>
 348     _workers-&gt;run_task(&amp;bcl);
 349 
<span class="line-modified"> 350     log_info(gc, init)(&quot;Pretouch heap: &quot; SIZE_FORMAT &quot; regions, &quot; SIZE_FORMAT &quot; bytes page&quot;,</span>
<span class="line-removed"> 351                        _num_regions, pretouch_heap_page_size);</span>
<span class="line-removed"> 352     ShenandoahPretouchHeapTask hcl(pretouch_heap_page_size);</span>
 353     _workers-&gt;run_task(&amp;hcl);
 354   }
 355 
 356   //
 357   // Initialize the rest of GC subsystems
 358   //
 359 
 360   _liveness_cache = NEW_C_HEAP_ARRAY(ShenandoahLiveData*, _max_workers, mtGC);
 361   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 362     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(ShenandoahLiveData, _num_regions, mtGC);
 363     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(ShenandoahLiveData));
 364   }
 365 
 366   // There should probably be Shenandoah-specific options for these,
 367   // just as there are G1-specific options.
 368   {
 369     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 370     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 371     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 372   }
 373 
 374   _monitoring_support = new ShenandoahMonitoringSupport(this);
 375   _phase_timings = new ShenandoahPhaseTimings(max_workers());
 376   ShenandoahStringDedup::initialize();
 377   ShenandoahCodeRoots::initialize();
 378 
 379   if (ShenandoahPacing) {
 380     _pacer = new ShenandoahPacer(this);
 381     _pacer-&gt;setup_for_idle();
 382   } else {
 383     _pacer = NULL;
 384   }
 385 
 386   _control_thread = new ShenandoahControlThread();
 387 
<span class="line-modified"> 388   log_info(gc, init)(&quot;Initialize Shenandoah heap: &quot; SIZE_FORMAT &quot;%s initial, &quot; SIZE_FORMAT &quot;%s min, &quot; SIZE_FORMAT &quot;%s max&quot;,</span>
<span class="line-modified"> 389                      byte_size_in_proper_unit(_initial_size),  proper_unit_for_byte_size(_initial_size),</span>
<span class="line-removed"> 390                      byte_size_in_proper_unit(_minimum_size),  proper_unit_for_byte_size(_minimum_size),</span>
<span class="line-removed"> 391                      byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity())</span>
<span class="line-removed"> 392   );</span>
 393 
<span class="line-modified"> 394   log_info(gc, init)(&quot;Safepointing mechanism: thread-local poll&quot;);</span>
 395 
 396   return JNI_OK;
 397 }
 398 
 399 void ShenandoahHeap::initialize_heuristics() {
 400   if (ShenandoahGCMode != NULL) {
 401     if (strcmp(ShenandoahGCMode, &quot;satb&quot;) == 0) {
 402       _gc_mode = new ShenandoahSATBMode();
 403     } else if (strcmp(ShenandoahGCMode, &quot;iu&quot;) == 0) {
 404       _gc_mode = new ShenandoahIUMode();
 405     } else if (strcmp(ShenandoahGCMode, &quot;passive&quot;) == 0) {
 406       _gc_mode = new ShenandoahPassiveMode();
 407     } else {
 408       vm_exit_during_initialization(&quot;Unknown -XX:ShenandoahGCMode option&quot;);
 409     }
 410   } else {
 411     ShouldNotReachHere();
 412   }
 413   _gc_mode-&gt;initialize_flags();
 414   if (_gc_mode-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 415     vm_exit_during_initialization(
 416             err_msg(&quot;GC mode \&quot;%s\&quot; is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.&quot;,
 417                     _gc_mode-&gt;name()));
 418   }
 419   if (_gc_mode-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 420     vm_exit_during_initialization(
 421             err_msg(&quot;GC mode \&quot;%s\&quot; is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.&quot;,
 422                     _gc_mode-&gt;name()));
 423   }
<span class="line-removed"> 424   log_info(gc, init)(&quot;Shenandoah GC mode: %s&quot;,</span>
<span class="line-removed"> 425                      _gc_mode-&gt;name());</span>
 426 
 427   _heuristics = _gc_mode-&gt;initialize_heuristics();
 428 
 429   if (_heuristics-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 430     vm_exit_during_initialization(
 431             err_msg(&quot;Heuristics \&quot;%s\&quot; is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.&quot;,
 432                     _heuristics-&gt;name()));
 433   }
 434   if (_heuristics-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 435     vm_exit_during_initialization(
 436             err_msg(&quot;Heuristics \&quot;%s\&quot; is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.&quot;,
 437                     _heuristics-&gt;name()));
 438   }
<span class="line-removed"> 439   log_info(gc, init)(&quot;Shenandoah heuristics: %s&quot;,</span>
<span class="line-removed"> 440                      _heuristics-&gt;name());</span>
 441 }
 442 
 443 #ifdef _MSC_VER
 444 #pragma warning( push )
 445 #pragma warning( disable:4355 ) // &#39;this&#39; : used in base member initializer list
 446 #endif
 447 
 448 ShenandoahHeap::ShenandoahHeap(ShenandoahCollectorPolicy* policy) :
 449   CollectedHeap(),
 450   _initial_size(0),
 451   _used(0),
 452   _committed(0),
 453   _bytes_allocated_since_gc_start(0),
 454   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 455   _workers(NULL),
 456   _safepoint_workers(NULL),
 457   _heap_region_special(false),
 458   _num_regions(0),
 459   _regions(NULL),
 460   _update_refs_iterator(this),
</pre>
<hr />
<pre>
 469   _phase_timings(NULL),
 470   _monitoring_support(NULL),
 471   _memory_pool(NULL),
 472   _stw_memory_manager(&quot;Shenandoah Pauses&quot;, &quot;end of GC pause&quot;),
 473   _cycle_memory_manager(&quot;Shenandoah Cycles&quot;, &quot;end of GC cycle&quot;),
 474   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 475   _soft_ref_policy(),
 476   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 477   _ref_processor(NULL),
 478   _marking_context(NULL),
 479   _bitmap_size(0),
 480   _bitmap_regions_per_slice(0),
 481   _bitmap_bytes_per_slice(0),
 482   _bitmap_region_special(false),
 483   _aux_bitmap_region_special(false),
 484   _liveness_cache(NULL),
 485   _collection_set(NULL)
 486 {
 487   _heap = this;
 488 
<span class="line-removed"> 489   log_info(gc, init)(&quot;GC threads: &quot; UINT32_FORMAT &quot; parallel, &quot; UINT32_FORMAT &quot; concurrent&quot;, ParallelGCThreads, ConcGCThreads);</span>
<span class="line-removed"> 490 </span>
 491   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 492 
 493   _max_workers = MAX2(_max_workers, 1U);
 494   _workers = new ShenandoahWorkGang(&quot;Shenandoah GC Threads&quot;, _max_workers,
 495                             /* are_GC_task_threads */ true,
 496                             /* are_ConcurrentGC_threads */ true);
 497   if (_workers == NULL) {
 498     vm_exit_during_initialization(&quot;Failed necessary allocation.&quot;);
 499   } else {
 500     _workers-&gt;initialize_workers();
 501   }
 502 
 503   if (ParallelGCThreads &gt; 1) {
 504     _safepoint_workers = new ShenandoahWorkGang(&quot;Safepoint Cleanup Thread&quot;,
 505                                                 ParallelGCThreads,
 506                       /* are_GC_task_threads */ false,
 507                  /* are_ConcurrentGC_threads */ false);
 508     _safepoint_workers-&gt;initialize_workers();
 509   }
 510 }
</pre>
<hr />
<pre>
1813     // cleanup the weak oops in CLD and determinate nmethod&#39;s unloading state, so that we
1814     // can cleanup immediate garbage sooner.
1815     if (_concurrent_class_unloading) {
1816       // Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the
1817       // CLD&#39;s holder or evacuate it.
1818       ShenandoahIsCLDAliveClosure is_cld_alive;
1819       _cld_roots.cld_do(&amp;is_cld_alive, worker_id);
1820 
1821       // Applies ShenandoahIsNMethodAliveClosure to registered nmethods.
1822       // The closure calls nmethod-&gt;is_unloading(). The is_unloading
1823       // state is cached, therefore, during concurrent class unloading phase,
1824       // we will not touch the metadata of unloading nmethods
1825       ShenandoahIsNMethodAliveClosure is_nmethod_alive;
1826       _nmethod_itr.nmethods_do(&amp;is_nmethod_alive);
1827     }
1828   }
1829 };
1830 
1831 void ShenandoahHeap::op_weak_roots() {
1832   if (is_concurrent_weak_root_in_progress()) {

1833     {
<span class="line-modified">1834       // Concurrent weak root processing</span>
1835       ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);
1836       ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);
1837       workers()-&gt;run_task(&amp;task);
1838       if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1839         set_concurrent_weak_root_in_progress(false);
1840       }
1841     }
1842 
1843     // Perform handshake to flush out dead oops
1844     {
1845       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);
1846       ShenandoahRendezvousClosure cl;
1847       Handshake::execute(&amp;cl);
1848     }
1849   }
1850 }
1851 
1852 void ShenandoahHeap::op_class_unloading() {
1853   assert (is_concurrent_weak_root_in_progress() &amp;&amp;
1854           ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),
</pre>
<hr />
<pre>
2115   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), &quot;Why set the flag?&quot;);
2116   if (in_progress) {
2117     _concurrent_strong_root_in_progress.set();
2118   } else {
2119     _concurrent_strong_root_in_progress.unset();
2120   }
2121 }
2122 
2123 void ShenandoahHeap::set_concurrent_weak_root_in_progress(bool in_progress) {
2124   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), &quot;Why set the flag?&quot;);
2125   if (in_progress) {
2126     _concurrent_weak_root_in_progress.set();
2127   } else {
2128     _concurrent_weak_root_in_progress.unset();
2129   }
2130 }
2131 
2132 void ShenandoahHeap::ref_processing_init() {
2133   assert(_max_workers &gt; 0, &quot;Sanity&quot;);
2134 
<span class="line-removed">2135   bool mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);</span>
<span class="line-removed">2136   bool mt_discovery = _max_workers &gt; 1;</span>
<span class="line-removed">2137 </span>
2138   _ref_processor =
2139     new ReferenceProcessor(&amp;_subject_to_discovery,  // is_subject_to_discovery
<span class="line-modified">2140                            mt_processing,           // MT processing</span>
2141                            _max_workers,            // Degree of MT processing
<span class="line-modified">2142                            mt_discovery,            // MT discovery</span>
2143                            _max_workers,            // Degree of MT discovery
2144                            false,                   // Reference discovery is not atomic
2145                            NULL,                    // No closure, should be installed before use
2146                            true);                   // Scale worker threads
2147 
<span class="line-removed">2148   log_info(gc, init)(&quot;Reference processing: %s discovery, %s processing&quot;,</span>
<span class="line-removed">2149           mt_discovery ? &quot;parallel&quot; : &quot;serial&quot;,</span>
<span class="line-removed">2150           mt_processing ? &quot;parallel&quot; : &quot;serial&quot;);</span>
<span class="line-removed">2151 </span>
2152   shenandoah_assert_rp_isalive_not_installed();
2153 }
2154 
2155 GCTracer* ShenandoahHeap::tracer() {
2156   return shenandoah_policy()-&gt;tracer();
2157 }
2158 
2159 size_t ShenandoahHeap::tlab_used(Thread* thread) const {
2160   return _free_set-&gt;used();
2161 }
2162 
2163 bool ShenandoahHeap::try_cancel_gc() {
2164   while (true) {
2165     jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);
2166     if (prev == CANCELLABLE) return true;
2167     else if (prev == CANCELLED) return false;
2168     assert(ShenandoahSuspendibleWorkers, &quot;should not get here when not using suspendible workers&quot;);
2169     assert(prev == NOT_CANCELLED, &quot;must be NOT_CANCELLED&quot;);
2170     if (Thread::current()-&gt;is_Java_thread()) {
2171       // We need to provide a safepoint here, otherwise we might
</pre>
<hr />
<pre>
2643 }
2644 
2645 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
2646   shenandoah_assert_heaplocked();
2647 
2648   // Bitmaps in special regions do not need commits
2649   if (_bitmap_region_special) {
2650     return true;
2651   }
2652 
2653   if (is_bitmap_slice_committed(r, true)) {
2654     // Some other region from the group is already committed, meaning the bitmap
2655     // slice is already committed, we exit right away.
2656     return true;
2657   }
2658 
2659   // Commit the bitmap slice:
2660   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2661   size_t off = _bitmap_bytes_per_slice * slice;
2662   size_t len = _bitmap_bytes_per_slice;
<span class="line-modified">2663   if (!os::commit_memory((char*)_bitmap_region.start() + off, len, false)) {</span>


2664     return false;
2665   }





2666   return true;
2667 }
2668 
2669 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
2670   shenandoah_assert_heaplocked();
2671 
2672   // Bitmaps in special regions do not need uncommits
2673   if (_bitmap_region_special) {
2674     return true;
2675   }
2676 
2677   if (is_bitmap_slice_committed(r, true)) {
2678     // Some other region from the group is still committed, meaning the bitmap
2679     // slice is should stay committed, exit right away.
2680     return true;
2681   }
2682 
2683   // Uncommit the bitmap slice:
2684   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2685   size_t off = _bitmap_bytes_per_slice * slice;
</pre>
<hr />
<pre>
2991   _stw_memory_manager.add_pool(_memory_pool);
2992 }
2993 
2994 GrowableArray&lt;GCMemoryManager*&gt; ShenandoahHeap::memory_managers() {
2995   GrowableArray&lt;GCMemoryManager*&gt; memory_managers(2);
2996   memory_managers.append(&amp;_cycle_memory_manager);
2997   memory_managers.append(&amp;_stw_memory_manager);
2998   return memory_managers;
2999 }
3000 
3001 GrowableArray&lt;MemoryPool*&gt; ShenandoahHeap::memory_pools() {
3002   GrowableArray&lt;MemoryPool*&gt; memory_pools(1);
3003   memory_pools.append(_memory_pool);
3004   return memory_pools;
3005 }
3006 
3007 MemoryUsage ShenandoahHeap::memory_usage() {
3008   return _memory_pool-&gt;get_memory_usage();
3009 }
3010 
<span class="line-removed">3011 void ShenandoahHeap::enter_evacuation() {</span>
<span class="line-removed">3012   _oom_evac_handler.enter_evacuation();</span>
<span class="line-removed">3013 }</span>
<span class="line-removed">3014 </span>
<span class="line-removed">3015 void ShenandoahHeap::leave_evacuation() {</span>
<span class="line-removed">3016   _oom_evac_handler.leave_evacuation();</span>
<span class="line-removed">3017 }</span>
<span class="line-removed">3018 </span>
3019 ShenandoahRegionIterator::ShenandoahRegionIterator() :
3020   _heap(ShenandoahHeap::heap()),
3021   _index(0) {}
3022 
3023 ShenandoahRegionIterator::ShenandoahRegionIterator(ShenandoahHeap* heap) :
3024   _heap(heap),
3025   _index(0) {}
3026 
3027 void ShenandoahRegionIterator::reset() {
3028   _index = 0;
3029 }
3030 
3031 bool ShenandoahRegionIterator::has_next() const {
3032   return _index &lt; _heap-&gt;num_regions();
3033 }
3034 
3035 char ShenandoahHeap::gc_state() const {
3036   return _gc_state.raw_value();
3037 }
3038 
</pre>
</td>
<td>
<hr />
<pre>
  29 #include &quot;gc/shared/gcArguments.hpp&quot;
  30 #include &quot;gc/shared/gcTimer.hpp&quot;
  31 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  32 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  33 #include &quot;gc/shared/memAllocator.hpp&quot;
  34 #include &quot;gc/shared/oopStorageSet.hpp&quot;
  35 #include &quot;gc/shared/plab.hpp&quot;
  36 
  37 #include &quot;gc/shenandoah/shenandoahBarrierSet.hpp&quot;
  38 #include &quot;gc/shenandoah/shenandoahClosures.inline.hpp&quot;
  39 #include &quot;gc/shenandoah/shenandoahCollectionSet.hpp&quot;
  40 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
  41 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
  42 #include &quot;gc/shenandoah/shenandoahConcurrentRoots.hpp&quot;
  43 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
  44 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
  45 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
  46 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
  47 #include &quot;gc/shenandoah/shenandoahHeapRegion.inline.hpp&quot;
  48 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
<span class="line-added">  49 #include &quot;gc/shenandoah/shenandoahInitLogger.hpp&quot;</span>
  50 #include &quot;gc/shenandoah/shenandoahMarkCompact.hpp&quot;
  51 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
  52 #include &quot;gc/shenandoah/shenandoahMemoryPool.hpp&quot;
  53 #include &quot;gc/shenandoah/shenandoahMetrics.hpp&quot;
  54 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
  55 #include &quot;gc/shenandoah/shenandoahOopClosures.inline.hpp&quot;
  56 #include &quot;gc/shenandoah/shenandoahPacer.inline.hpp&quot;
  57 #include &quot;gc/shenandoah/shenandoahPadding.hpp&quot;
  58 #include &quot;gc/shenandoah/shenandoahParallelCleaning.inline.hpp&quot;
  59 #include &quot;gc/shenandoah/shenandoahRootProcessor.inline.hpp&quot;
  60 #include &quot;gc/shenandoah/shenandoahStringDedup.hpp&quot;
  61 #include &quot;gc/shenandoah/shenandoahTaskqueue.hpp&quot;
  62 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
  63 #include &quot;gc/shenandoah/shenandoahVerifier.hpp&quot;
  64 #include &quot;gc/shenandoah/shenandoahCodeRoots.hpp&quot;
  65 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
  66 #include &quot;gc/shenandoah/shenandoahWorkGroup.hpp&quot;
  67 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
  68 #include &quot;gc/shenandoah/mode/shenandoahIUMode.hpp&quot;
  69 #include &quot;gc/shenandoah/mode/shenandoahPassiveMode.hpp&quot;
</pre>
<hr />
<pre>
  94     shenandoah_assert_not_forwarded(p, obj);
  95   }
  96 }
  97 
  98 void ShenandoahAssertToSpaceClosure::do_oop(narrowOop* p) { do_oop_work(p); }
  99 void ShenandoahAssertToSpaceClosure::do_oop(oop* p)       { do_oop_work(p); }
 100 #endif
 101 
 102 class ShenandoahPretouchHeapTask : public AbstractGangTask {
 103 private:
 104   ShenandoahRegionIterator _regions;
 105   const size_t _page_size;
 106 public:
 107   ShenandoahPretouchHeapTask(size_t page_size) :
 108     AbstractGangTask(&quot;Shenandoah Pretouch Heap&quot;),
 109     _page_size(page_size) {}
 110 
 111   virtual void work(uint worker_id) {
 112     ShenandoahHeapRegion* r = _regions.next();
 113     while (r != NULL) {
<span class="line-modified"> 114       if (r-&gt;is_committed()) {</span>
<span class="line-added"> 115         os::pretouch_memory(r-&gt;bottom(), r-&gt;end(), _page_size);</span>
<span class="line-added"> 116       }</span>
 117       r = _regions.next();
 118     }
 119   }
 120 };
 121 
 122 class ShenandoahPretouchBitmapTask : public AbstractGangTask {
 123 private:
 124   ShenandoahRegionIterator _regions;
 125   char* _bitmap_base;
 126   const size_t _bitmap_size;
 127   const size_t _page_size;
 128 public:
 129   ShenandoahPretouchBitmapTask(char* bitmap_base, size_t bitmap_size, size_t page_size) :
 130     AbstractGangTask(&quot;Shenandoah Pretouch Bitmap&quot;),
 131     _bitmap_base(bitmap_base),
 132     _bitmap_size(bitmap_size),
 133     _page_size(page_size) {}
 134 
 135   virtual void work(uint worker_id) {
 136     ShenandoahHeapRegion* r = _regions.next();
 137     while (r != NULL) {
 138       size_t start = r-&gt;index()       * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 139       size_t end   = (r-&gt;index() + 1) * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 140       assert (end &lt;= _bitmap_size, &quot;end is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, end, _bitmap_size);
 141 
<span class="line-modified"> 142       if (r-&gt;is_committed()) {</span>
<span class="line-added"> 143         os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);</span>
<span class="line-added"> 144       }</span>
 145 
 146       r = _regions.next();
 147     }
 148   }
 149 };
 150 
 151 jint ShenandoahHeap::initialize() {
 152   //
 153   // Figure out heap sizing
 154   //
 155 
 156   size_t init_byte_size = InitialHeapSize;
 157   size_t min_byte_size  = MinHeapSize;
 158   size_t max_byte_size  = MaxHeapSize;
 159   size_t heap_alignment = HeapAlignment;
 160 
 161   size_t reg_size_bytes = ShenandoahHeapRegion::region_size_bytes();
 162 





 163   Universe::check_alignment(max_byte_size,  reg_size_bytes, &quot;Shenandoah heap&quot;);
 164   Universe::check_alignment(init_byte_size, reg_size_bytes, &quot;Shenandoah heap&quot;);
 165 
 166   _num_regions = ShenandoahHeapRegion::region_count();
 167 
 168   // Now we know the number of regions, initialize the heuristics.
 169   initialize_heuristics();
 170 
 171   size_t num_committed_regions = init_byte_size / reg_size_bytes;
 172   num_committed_regions = MIN2(num_committed_regions, _num_regions);
 173   assert(num_committed_regions &lt;= _num_regions, &quot;sanity&quot;);
 174   _initial_size = num_committed_regions * reg_size_bytes;
 175 
 176   size_t num_min_regions = min_byte_size / reg_size_bytes;
 177   num_min_regions = MIN2(num_min_regions, _num_regions);
 178   assert(num_min_regions &lt;= _num_regions, &quot;sanity&quot;);
 179   _minimum_size = num_min_regions * reg_size_bytes;
 180 
 181   _committed = _initial_size;
 182 
</pre>
<hr />
<pre>
 275   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 276   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 277   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 278   _aux_bitmap_region_special = aux_bitmap.special();
 279   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 280 
 281   //
 282   // Create regions and region sets
 283   //
 284   size_t region_align = align_up(sizeof(ShenandoahHeapRegion), SHENANDOAH_CACHE_LINE_SIZE);
 285   size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);
 286   region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());
 287 
 288   ReservedSpace region_storage(region_storage_size, region_page_size);
 289   MemTracker::record_virtual_memory_type(region_storage.base(), mtGC);
 290   if (!region_storage.special()) {
 291     os::commit_memory_or_exit(region_storage.base(), region_storage_size, region_page_size, false,
 292                               &quot;Cannot commit region memory&quot;);
 293   }
 294 
<span class="line-added"> 295   // Try to fit the collection set bitmap at lower addresses. This optimizes code generation for cset checks.</span>
<span class="line-added"> 296   // Go up until a sensible limit (subject to encoding constraints) and try to reserve the space there.</span>
<span class="line-added"> 297   // If not successful, bite a bullet and allocate at whatever address.</span>
<span class="line-added"> 298   {</span>
<span class="line-added"> 299     size_t cset_align = MAX2&lt;size_t&gt;(os::vm_page_size(), os::vm_allocation_granularity());</span>
<span class="line-added"> 300     size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) &gt;&gt; ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);</span>
<span class="line-added"> 301 </span>
<span class="line-added"> 302     uintptr_t min = round_up_power_of_2(cset_align);</span>
<span class="line-added"> 303     uintptr_t max = (1u &lt;&lt; 30u);</span>
<span class="line-added"> 304 </span>
<span class="line-added"> 305     for (uintptr_t addr = min; addr &lt;= max; addr &lt;&lt;= 1u) {</span>
<span class="line-added"> 306       char* req_addr = (char*)addr;</span>
<span class="line-added"> 307       assert(is_aligned(req_addr, cset_align), &quot;Should be aligned&quot;);</span>
<span class="line-added"> 308       ReservedSpace cset_rs(cset_size, cset_align, false, req_addr);</span>
<span class="line-added"> 309       if (cset_rs.is_reserved()) {</span>
<span class="line-added"> 310         assert(cset_rs.base() == req_addr, &quot;Allocated where requested: &quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT, p2i(cset_rs.base()), addr);</span>
<span class="line-added"> 311         _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());</span>
<span class="line-added"> 312         break;</span>
<span class="line-added"> 313       }</span>
<span class="line-added"> 314     }</span>
<span class="line-added"> 315 </span>
<span class="line-added"> 316     if (_collection_set == NULL) {</span>
<span class="line-added"> 317       ReservedSpace cset_rs(cset_size, cset_align, false);</span>
<span class="line-added"> 318       _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());</span>
<span class="line-added"> 319     }</span>
<span class="line-added"> 320   }</span>
<span class="line-added"> 321 </span>
 322   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 323   _free_set = new ShenandoahFreeSet(this, _num_regions);

 324 
 325   {
 326     ShenandoahHeapLocker locker(lock());
 327 
 328     for (size_t i = 0; i &lt; _num_regions; i++) {
 329       HeapWord* start = (HeapWord*)sh_rs.base() + ShenandoahHeapRegion::region_size_words() * i;
 330       bool is_committed = i &lt; num_committed_regions;
 331       void* loc = region_storage.base() + i * region_align;
 332 
 333       ShenandoahHeapRegion* r = new (loc) ShenandoahHeapRegion(start, i, is_committed);
 334       assert(is_aligned(r, SHENANDOAH_CACHE_LINE_SIZE), &quot;Sanity&quot;);
 335 
 336       _marking_context-&gt;initialize_top_at_mark_start(r);
 337       _regions[i] = r;
 338       assert(!collection_set()-&gt;is_in(i), &quot;New region should not be in collection set&quot;);
 339     }
 340 
 341     // Initialize to complete
 342     _marking_context-&gt;mark_complete();
 343 
 344     _free_set-&gt;rebuild();
 345   }
 346 
<span class="line-modified"> 347   if (AlwaysPreTouch) {</span>


 348     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 349     // before initialize() below zeroes it with initializing thread. For any given region,
 350     // we touch the region and the corresponding bitmaps from the same thread.
 351     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 352 
<span class="line-modified"> 353     _pretouch_heap_page_size = heap_page_size;</span>
<span class="line-modified"> 354     _pretouch_bitmap_page_size = bitmap_page_size;</span>
 355 
 356 #ifdef LINUX
 357     // UseTransparentHugePages would madvise that backing memory can be coalesced into huge
 358     // pages. But, the kernel needs to know that every small page is used, in order to coalesce
 359     // them into huge one. Therefore, we need to pretouch with smaller pages.
 360     if (UseTransparentHugePages) {
<span class="line-modified"> 361       _pretouch_heap_page_size = (size_t)os::vm_page_size();</span>
<span class="line-modified"> 362       _pretouch_bitmap_page_size = (size_t)os::vm_page_size();</span>
 363     }
 364 #endif
 365 
 366     // OS memory managers may want to coalesce back-to-back pages. Make their jobs
 367     // simpler by pre-touching continuous spaces (heap and bitmap) separately.
 368 
<span class="line-modified"> 369     ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, _pretouch_bitmap_page_size);</span>


 370     _workers-&gt;run_task(&amp;bcl);
 371 
<span class="line-modified"> 372     ShenandoahPretouchHeapTask hcl(_pretouch_heap_page_size);</span>


 373     _workers-&gt;run_task(&amp;hcl);
 374   }
 375 
 376   //
 377   // Initialize the rest of GC subsystems
 378   //
 379 
 380   _liveness_cache = NEW_C_HEAP_ARRAY(ShenandoahLiveData*, _max_workers, mtGC);
 381   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 382     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(ShenandoahLiveData, _num_regions, mtGC);
 383     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(ShenandoahLiveData));
 384   }
 385 
 386   // There should probably be Shenandoah-specific options for these,
 387   // just as there are G1-specific options.
 388   {
 389     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 390     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 391     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 392   }
 393 
 394   _monitoring_support = new ShenandoahMonitoringSupport(this);
 395   _phase_timings = new ShenandoahPhaseTimings(max_workers());
 396   ShenandoahStringDedup::initialize();
 397   ShenandoahCodeRoots::initialize();
 398 
 399   if (ShenandoahPacing) {
 400     _pacer = new ShenandoahPacer(this);
 401     _pacer-&gt;setup_for_idle();
 402   } else {
 403     _pacer = NULL;
 404   }
 405 
 406   _control_thread = new ShenandoahControlThread();
 407 
<span class="line-modified"> 408   _ref_proc_mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);</span>
<span class="line-modified"> 409   _ref_proc_mt_discovery = _max_workers &gt; 1;</span>



 410 
<span class="line-modified"> 411   ShenandoahInitLogger::print();</span>
 412 
 413   return JNI_OK;
 414 }
 415 
 416 void ShenandoahHeap::initialize_heuristics() {
 417   if (ShenandoahGCMode != NULL) {
 418     if (strcmp(ShenandoahGCMode, &quot;satb&quot;) == 0) {
 419       _gc_mode = new ShenandoahSATBMode();
 420     } else if (strcmp(ShenandoahGCMode, &quot;iu&quot;) == 0) {
 421       _gc_mode = new ShenandoahIUMode();
 422     } else if (strcmp(ShenandoahGCMode, &quot;passive&quot;) == 0) {
 423       _gc_mode = new ShenandoahPassiveMode();
 424     } else {
 425       vm_exit_during_initialization(&quot;Unknown -XX:ShenandoahGCMode option&quot;);
 426     }
 427   } else {
 428     ShouldNotReachHere();
 429   }
 430   _gc_mode-&gt;initialize_flags();
 431   if (_gc_mode-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 432     vm_exit_during_initialization(
 433             err_msg(&quot;GC mode \&quot;%s\&quot; is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.&quot;,
 434                     _gc_mode-&gt;name()));
 435   }
 436   if (_gc_mode-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 437     vm_exit_during_initialization(
 438             err_msg(&quot;GC mode \&quot;%s\&quot; is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.&quot;,
 439                     _gc_mode-&gt;name()));
 440   }


 441 
 442   _heuristics = _gc_mode-&gt;initialize_heuristics();
 443 
 444   if (_heuristics-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 445     vm_exit_during_initialization(
 446             err_msg(&quot;Heuristics \&quot;%s\&quot; is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.&quot;,
 447                     _heuristics-&gt;name()));
 448   }
 449   if (_heuristics-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 450     vm_exit_during_initialization(
 451             err_msg(&quot;Heuristics \&quot;%s\&quot; is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.&quot;,
 452                     _heuristics-&gt;name()));
 453   }


 454 }
 455 
 456 #ifdef _MSC_VER
 457 #pragma warning( push )
 458 #pragma warning( disable:4355 ) // &#39;this&#39; : used in base member initializer list
 459 #endif
 460 
 461 ShenandoahHeap::ShenandoahHeap(ShenandoahCollectorPolicy* policy) :
 462   CollectedHeap(),
 463   _initial_size(0),
 464   _used(0),
 465   _committed(0),
 466   _bytes_allocated_since_gc_start(0),
 467   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 468   _workers(NULL),
 469   _safepoint_workers(NULL),
 470   _heap_region_special(false),
 471   _num_regions(0),
 472   _regions(NULL),
 473   _update_refs_iterator(this),
</pre>
<hr />
<pre>
 482   _phase_timings(NULL),
 483   _monitoring_support(NULL),
 484   _memory_pool(NULL),
 485   _stw_memory_manager(&quot;Shenandoah Pauses&quot;, &quot;end of GC pause&quot;),
 486   _cycle_memory_manager(&quot;Shenandoah Cycles&quot;, &quot;end of GC cycle&quot;),
 487   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 488   _soft_ref_policy(),
 489   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 490   _ref_processor(NULL),
 491   _marking_context(NULL),
 492   _bitmap_size(0),
 493   _bitmap_regions_per_slice(0),
 494   _bitmap_bytes_per_slice(0),
 495   _bitmap_region_special(false),
 496   _aux_bitmap_region_special(false),
 497   _liveness_cache(NULL),
 498   _collection_set(NULL)
 499 {
 500   _heap = this;
 501 


 502   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 503 
 504   _max_workers = MAX2(_max_workers, 1U);
 505   _workers = new ShenandoahWorkGang(&quot;Shenandoah GC Threads&quot;, _max_workers,
 506                             /* are_GC_task_threads */ true,
 507                             /* are_ConcurrentGC_threads */ true);
 508   if (_workers == NULL) {
 509     vm_exit_during_initialization(&quot;Failed necessary allocation.&quot;);
 510   } else {
 511     _workers-&gt;initialize_workers();
 512   }
 513 
 514   if (ParallelGCThreads &gt; 1) {
 515     _safepoint_workers = new ShenandoahWorkGang(&quot;Safepoint Cleanup Thread&quot;,
 516                                                 ParallelGCThreads,
 517                       /* are_GC_task_threads */ false,
 518                  /* are_ConcurrentGC_threads */ false);
 519     _safepoint_workers-&gt;initialize_workers();
 520   }
 521 }
</pre>
<hr />
<pre>
1824     // cleanup the weak oops in CLD and determinate nmethod&#39;s unloading state, so that we
1825     // can cleanup immediate garbage sooner.
1826     if (_concurrent_class_unloading) {
1827       // Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the
1828       // CLD&#39;s holder or evacuate it.
1829       ShenandoahIsCLDAliveClosure is_cld_alive;
1830       _cld_roots.cld_do(&amp;is_cld_alive, worker_id);
1831 
1832       // Applies ShenandoahIsNMethodAliveClosure to registered nmethods.
1833       // The closure calls nmethod-&gt;is_unloading(). The is_unloading
1834       // state is cached, therefore, during concurrent class unloading phase,
1835       // we will not touch the metadata of unloading nmethods
1836       ShenandoahIsNMethodAliveClosure is_nmethod_alive;
1837       _nmethod_itr.nmethods_do(&amp;is_nmethod_alive);
1838     }
1839   }
1840 };
1841 
1842 void ShenandoahHeap::op_weak_roots() {
1843   if (is_concurrent_weak_root_in_progress()) {
<span class="line-added">1844     // Concurrent weak root processing</span>
1845     {
<span class="line-modified">1846       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);</span>
1847       ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);
1848       ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);
1849       workers()-&gt;run_task(&amp;task);
1850       if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1851         set_concurrent_weak_root_in_progress(false);
1852       }
1853     }
1854 
1855     // Perform handshake to flush out dead oops
1856     {
1857       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);
1858       ShenandoahRendezvousClosure cl;
1859       Handshake::execute(&amp;cl);
1860     }
1861   }
1862 }
1863 
1864 void ShenandoahHeap::op_class_unloading() {
1865   assert (is_concurrent_weak_root_in_progress() &amp;&amp;
1866           ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),
</pre>
<hr />
<pre>
2127   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), &quot;Why set the flag?&quot;);
2128   if (in_progress) {
2129     _concurrent_strong_root_in_progress.set();
2130   } else {
2131     _concurrent_strong_root_in_progress.unset();
2132   }
2133 }
2134 
2135 void ShenandoahHeap::set_concurrent_weak_root_in_progress(bool in_progress) {
2136   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), &quot;Why set the flag?&quot;);
2137   if (in_progress) {
2138     _concurrent_weak_root_in_progress.set();
2139   } else {
2140     _concurrent_weak_root_in_progress.unset();
2141   }
2142 }
2143 
2144 void ShenandoahHeap::ref_processing_init() {
2145   assert(_max_workers &gt; 0, &quot;Sanity&quot;);
2146 



2147   _ref_processor =
2148     new ReferenceProcessor(&amp;_subject_to_discovery,  // is_subject_to_discovery
<span class="line-modified">2149                            _ref_proc_mt_processing, // MT processing</span>
2150                            _max_workers,            // Degree of MT processing
<span class="line-modified">2151                            _ref_proc_mt_discovery,  // MT discovery</span>
2152                            _max_workers,            // Degree of MT discovery
2153                            false,                   // Reference discovery is not atomic
2154                            NULL,                    // No closure, should be installed before use
2155                            true);                   // Scale worker threads
2156 




2157   shenandoah_assert_rp_isalive_not_installed();
2158 }
2159 
2160 GCTracer* ShenandoahHeap::tracer() {
2161   return shenandoah_policy()-&gt;tracer();
2162 }
2163 
2164 size_t ShenandoahHeap::tlab_used(Thread* thread) const {
2165   return _free_set-&gt;used();
2166 }
2167 
2168 bool ShenandoahHeap::try_cancel_gc() {
2169   while (true) {
2170     jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);
2171     if (prev == CANCELLABLE) return true;
2172     else if (prev == CANCELLED) return false;
2173     assert(ShenandoahSuspendibleWorkers, &quot;should not get here when not using suspendible workers&quot;);
2174     assert(prev == NOT_CANCELLED, &quot;must be NOT_CANCELLED&quot;);
2175     if (Thread::current()-&gt;is_Java_thread()) {
2176       // We need to provide a safepoint here, otherwise we might
</pre>
<hr />
<pre>
2648 }
2649 
2650 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
2651   shenandoah_assert_heaplocked();
2652 
2653   // Bitmaps in special regions do not need commits
2654   if (_bitmap_region_special) {
2655     return true;
2656   }
2657 
2658   if (is_bitmap_slice_committed(r, true)) {
2659     // Some other region from the group is already committed, meaning the bitmap
2660     // slice is already committed, we exit right away.
2661     return true;
2662   }
2663 
2664   // Commit the bitmap slice:
2665   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2666   size_t off = _bitmap_bytes_per_slice * slice;
2667   size_t len = _bitmap_bytes_per_slice;
<span class="line-modified">2668   char* start = (char*) _bitmap_region.start() + off;</span>
<span class="line-added">2669 </span>
<span class="line-added">2670   if (!os::commit_memory(start, len, false)) {</span>
2671     return false;
2672   }
<span class="line-added">2673 </span>
<span class="line-added">2674   if (AlwaysPreTouch) {</span>
<span class="line-added">2675     os::pretouch_memory(start, start + len, _pretouch_bitmap_page_size);</span>
<span class="line-added">2676   }</span>
<span class="line-added">2677 </span>
2678   return true;
2679 }
2680 
2681 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
2682   shenandoah_assert_heaplocked();
2683 
2684   // Bitmaps in special regions do not need uncommits
2685   if (_bitmap_region_special) {
2686     return true;
2687   }
2688 
2689   if (is_bitmap_slice_committed(r, true)) {
2690     // Some other region from the group is still committed, meaning the bitmap
2691     // slice is should stay committed, exit right away.
2692     return true;
2693   }
2694 
2695   // Uncommit the bitmap slice:
2696   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2697   size_t off = _bitmap_bytes_per_slice * slice;
</pre>
<hr />
<pre>
3003   _stw_memory_manager.add_pool(_memory_pool);
3004 }
3005 
3006 GrowableArray&lt;GCMemoryManager*&gt; ShenandoahHeap::memory_managers() {
3007   GrowableArray&lt;GCMemoryManager*&gt; memory_managers(2);
3008   memory_managers.append(&amp;_cycle_memory_manager);
3009   memory_managers.append(&amp;_stw_memory_manager);
3010   return memory_managers;
3011 }
3012 
3013 GrowableArray&lt;MemoryPool*&gt; ShenandoahHeap::memory_pools() {
3014   GrowableArray&lt;MemoryPool*&gt; memory_pools(1);
3015   memory_pools.append(_memory_pool);
3016   return memory_pools;
3017 }
3018 
3019 MemoryUsage ShenandoahHeap::memory_usage() {
3020   return _memory_pool-&gt;get_memory_usage();
3021 }
3022 








3023 ShenandoahRegionIterator::ShenandoahRegionIterator() :
3024   _heap(ShenandoahHeap::heap()),
3025   _index(0) {}
3026 
3027 ShenandoahRegionIterator::ShenandoahRegionIterator(ShenandoahHeap* heap) :
3028   _heap(heap),
3029   _index(0) {}
3030 
3031 void ShenandoahRegionIterator::reset() {
3032   _index = 0;
3033 }
3034 
3035 bool ShenandoahRegionIterator::has_next() const {
3036   return _index &lt; _heap-&gt;num_regions();
3037 }
3038 
3039 char ShenandoahHeap::gc_state() const {
3040   return _gc_state.raw_value();
3041 }
3042 
</pre>
</td>
</tr>
</table>
<center><a href="shenandoahEvacOOMHandler.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>