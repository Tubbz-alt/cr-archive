<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/metaspaceShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="metaspaceClosure.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspaceShared.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  52 #include &quot;memory/universe.hpp&quot;
  53 #include &quot;oops/compressedOops.inline.hpp&quot;
  54 #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  55 #include &quot;oops/instanceMirrorKlass.hpp&quot;
  56 #include &quot;oops/instanceRefKlass.hpp&quot;
  57 #include &quot;oops/methodData.hpp&quot;
  58 #include &quot;oops/objArrayKlass.hpp&quot;
  59 #include &quot;oops/objArrayOop.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;oops/typeArrayKlass.hpp&quot;
  62 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  63 #include &quot;runtime/handles.inline.hpp&quot;
  64 #include &quot;runtime/os.hpp&quot;
  65 #include &quot;runtime/safepointVerifiers.hpp&quot;
  66 #include &quot;runtime/signature.hpp&quot;
  67 #include &quot;runtime/timerTrace.hpp&quot;
  68 #include &quot;runtime/vmThread.hpp&quot;
  69 #include &quot;runtime/vmOperations.hpp&quot;
  70 #include &quot;utilities/align.hpp&quot;
  71 #include &quot;utilities/bitMap.inline.hpp&quot;

  72 #include &quot;utilities/defaultStream.hpp&quot;
  73 #include &quot;utilities/hashtable.inline.hpp&quot;
  74 #if INCLUDE_G1GC
  75 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  76 #endif
  77 
  78 ReservedSpace MetaspaceShared::_shared_rs;
  79 VirtualSpace MetaspaceShared::_shared_vs;
  80 ReservedSpace MetaspaceShared::_symbol_rs;
  81 VirtualSpace MetaspaceShared::_symbol_vs;
  82 MetaspaceSharedStats MetaspaceShared::_stats;
  83 bool MetaspaceShared::_has_error_classes;
  84 bool MetaspaceShared::_archive_loading_failed = false;
  85 bool MetaspaceShared::_remapped_readwrite = false;
  86 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  87 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  88 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  89 intx MetaspaceShared::_relocation_delta;
  90 
  91 // The CDS archive is divided into the following regions:
</pre>
<hr />
<pre>
 174                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 175   if (strcmp(_name, failing_region) == 0) {
 176     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 177   }
 178 }
 179 
 180 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
 181   _rs = rs;
 182   _vs = vs;
 183   // Start with 0 committed bytes. The memory will be committed as needed by
 184   // MetaspaceShared::commit_to().
 185   if (!_vs-&gt;initialize(*_rs, 0)) {
 186     fatal(&quot;Unable to allocate memory for shared space&quot;);
 187   }
 188   _base = _top = _rs-&gt;base();
 189   _end = _rs-&gt;end();
 190 }
 191 
 192 void DumpRegion::pack(DumpRegion* next) {
 193   assert(!is_packed(), &quot;sanity&quot;);
<span class="line-modified"> 194   _end = (char*)align_up(_top, Metaspace::reserve_alignment());</span>
 195   _is_packed = true;
 196   if (next != NULL) {
 197     next-&gt;_rs = _rs;
 198     next-&gt;_vs = _vs;
 199     next-&gt;_base = next-&gt;_top = this-&gt;_end;
 200     next-&gt;_end = _rs-&gt;end();
 201   }
 202 }
 203 
 204 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);
 205 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 206 
 207 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
 208   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);
 209 }
 210 
 211 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 212   return &amp;_mc_region;
 213 }
 214 
</pre>
<hr />
<pre>
 220   return &amp;_ro_region;
 221 }
 222 
 223 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 224                                       ReservedSpace* rs) {
 225   current-&gt;pack(next);
 226 }
 227 
 228 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
 229   return _symbol_region.allocate(num_bytes);
 230 }
 231 
 232 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 233   return _mc_region.allocate(num_bytes);
 234 }
 235 
 236 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 237   return _ro_region.allocate(num_bytes);
 238 }
 239 
<span class="line-modified"> 240 // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:</span>
<span class="line-removed"> 241 // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,</span>
<span class="line-removed"> 242 //                                    while keeping the first range at offset 0 of this range.</span>
<span class="line-removed"> 243 // Metaspace::reserve_alignment()  -- so we can pass the region to</span>
<span class="line-removed"> 244 //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.</span>
<span class="line-removed"> 245 size_t MetaspaceShared::reserved_space_alignment() {</span>
<span class="line-removed"> 246   size_t os_align = os::vm_allocation_granularity();</span>
<span class="line-removed"> 247   size_t ms_align = Metaspace::reserve_alignment();</span>
<span class="line-removed"> 248   if (os_align &gt;= ms_align) {</span>
<span class="line-removed"> 249     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-removed"> 250     return os_align;</span>
<span class="line-removed"> 251   } else {</span>
<span class="line-removed"> 252     assert(ms_align % os_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-removed"> 253     return ms_align;</span>
<span class="line-removed"> 254   }</span>
<span class="line-removed"> 255 }</span>
 256 
<span class="line-modified"> 257 ReservedSpace MetaspaceShared::reserve_shared_space(size_t size, char* requested_address) {</span>
<span class="line-modified"> 258   return Metaspace::reserve_space(size, reserved_space_alignment(),</span>
<span class="line-modified"> 259                                   requested_address, requested_address != NULL);</span>









 260 }

 261 
 262 void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
 263   assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified"> 264   const size_t reserve_alignment = reserved_space_alignment();</span>





 265   char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
 266 
 267 #ifdef _LP64
<span class="line-modified"> 268   // On 64-bit VM, the heap and class space layout will be the same as if</span>
<span class="line-modified"> 269   // you&#39;re running in -Xshare:on mode:</span>
<span class="line-modified"> 270   //</span>
<span class="line-modified"> 271   //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="line-removed"> 272   //                              v</span>
<span class="line-removed"> 273   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-removed"> 274   // |    Heap    | Archive |     | MC | RW | RO |    class space     |</span>
<span class="line-removed"> 275   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-removed"> 276   // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="line-removed"> 277   //</span>
 278   const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
 279   const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
 280 #else
<span class="line-modified"> 281   // We don&#39;t support archives larger than 256MB on 32-bit due to limited virtual address space.</span>

 282   size_t cds_total = align_down(256*M, reserve_alignment);
 283 #endif
 284 

 285   bool use_requested_base = true;





 286   if (ArchiveRelocationMode == 1) {
 287     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
 288     use_requested_base = false;
 289   }
 290 
 291   // First try to reserve the space at the specified SharedBaseAddress.
 292   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
 293   if (use_requested_base) {
<span class="line-modified"> 294     _shared_rs = reserve_shared_space(cds_total, shared_base);</span>







 295   }
<span class="line-modified"> 296   if (_shared_rs.is_reserved()) {</span>
<span class="line-modified"> 297     assert(shared_base == 0 || _shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="line-modified"> 298   } else {</span>
<span class="line-modified"> 299     // Get a mmap region anywhere if the SharedBaseAddress fails.</span>
<span class="line-modified"> 300     _shared_rs = reserve_shared_space(cds_total);</span>









 301   }

 302   if (!_shared_rs.is_reserved()) {
 303     vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
 304                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
 305   }
 306 
 307 #ifdef _LP64
<span class="line-removed"> 308   // During dump time, we allocate 4GB (UnscaledClassSpaceMax) of space and split it up:</span>
<span class="line-removed"> 309   // + The upper 1 GB is used as the &quot;temporary compressed class space&quot; -- preload_classes()</span>
<span class="line-removed"> 310   //   will store Klasses into this space.</span>
<span class="line-removed"> 311   // + The lower 3 GB is used for the archive -- when preload_classes() is done,</span>
<span class="line-removed"> 312   //   ArchiveCompactor will copy the class metadata into this space, first the RW parts,</span>
<span class="line-removed"> 313   //   then the RO parts.</span>
<span class="line-removed"> 314 </span>
<span class="line-removed"> 315   size_t max_archive_size = align_down(cds_total * 3 / 4, reserve_alignment);</span>
<span class="line-removed"> 316   ReservedSpace tmp_class_space = _shared_rs.last_part(max_archive_size);</span>
<span class="line-removed"> 317   CompressedClassSpaceSize = align_down(tmp_class_space.size(), reserve_alignment);</span>
<span class="line-removed"> 318   _shared_rs = _shared_rs.first_part(max_archive_size);</span>
 319 
 320   if (UseCompressedClassPointers) {
<span class="line-modified"> 321     // Set up compress class pointers.</span>
<span class="line-modified"> 322     CompressedKlassPointers::set_base((address)_shared_rs.base());</span>
<span class="line-modified"> 323     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent</span>
<span class="line-modified"> 324     // with AOT.</span>
<span class="line-modified"> 325     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="line-modified"> 326     // Set the range of klass addresses to 4GB.</span>
<span class="line-modified"> 327     CompressedKlassPointers::set_range(cds_total);</span>









































 328     Metaspace::initialize_class_space(tmp_class_space);














 329   }
<span class="line-removed"> 330   log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="line-removed"> 331                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
 332 
<span class="line-removed"> 333   log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="line-removed"> 334                 CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
 335 #endif
 336 
 337   init_shared_dump_space(&amp;_mc_region);
 338   SharedBaseAddress = (size_t)_shared_rs.base();
 339   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 340                 _shared_rs.size(), p2i(_shared_rs.base()));
 341 
 342   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
 343   _symbol_rs = ReservedSpace(symbol_rs_size);
 344   if (!_symbol_rs.is_reserved()) {
 345     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,
 346                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));
 347   }
 348   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);
 349 }
 350 
 351 // Called by universe_post_init()
 352 void MetaspaceShared::post_initialize(TRAPS) {
 353   if (UseSharedSpaces) {
 354     int size = FileMapInfo::get_number_of_shared_paths();
</pre>
<hr />
<pre>
1143                                const char *name, size_t total_size);
1144   void relocate_to_default_base_address(CHeapBitMap* ptrmap);
1145 
1146 public:
1147 
1148   VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
1149   void doit();   // outline because gdb sucks
1150   bool allow_nested_vm_operations() const { return true; }
1151 }; // class VM_PopulateDumpSharedSpace
1152 
1153 class SortedSymbolClosure: public SymbolClosure {
1154   GrowableArray&lt;Symbol*&gt; _symbols;
1155   virtual void do_symbol(Symbol** sym) {
1156     assert((*sym)-&gt;is_permanent(), &quot;archived symbols must be permanent&quot;);
1157     _symbols.append(*sym);
1158   }
1159   static int compare_symbols_by_address(Symbol** a, Symbol** b) {
1160     if (a[0] &lt; b[0]) {
1161       return -1;
1162     } else if (a[0] == b[0]) {


1163       return 0;
1164     } else {
1165       return 1;
1166     }
1167   }
1168 
1169 public:
1170   SortedSymbolClosure() {
1171     SymbolTable::symbols_do(this);
1172     _symbols.sort(compare_symbols_by_address);
1173   }
1174   GrowableArray&lt;Symbol*&gt;* get_sorted_symbols() {
1175     return &amp;_symbols;
1176   }
1177 };
1178 
1179 // ArchiveCompactor --
1180 //
1181 // This class is the central piece of shared archive compaction -- all metaspace data are
1182 // initially allocated outside of the shared regions. ArchiveCompactor copies the
</pre>
<hr />
<pre>
2056 
2057 bool MetaspaceShared::is_in_trampoline_frame(address addr) {
2058   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {
2059     return true;
2060   }
2061   return false;
2062 }
2063 
2064 bool MetaspaceShared::is_shared_dynamic(void* p) {
2065   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;
2066       (p &gt;= _shared_metaspace_static_top)) {
2067     return true;
2068   } else {
2069     return false;
2070   }
2071 }
2072 
2073 void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
2074   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
2075   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;

2076   FileMapInfo* static_mapinfo = open_static_archive();
2077   FileMapInfo* dynamic_mapinfo = NULL;
2078 
2079   if (static_mapinfo != NULL) {
2080     dynamic_mapinfo = open_dynamic_archive();
2081 
2082     // First try to map at the requested address
2083     result = map_archives(static_mapinfo, dynamic_mapinfo, true);
2084     if (result == MAP_ARCHIVE_MMAP_FAILURE) {
2085       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen
2086       // by the OS.
2087       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);
2088       result = map_archives(static_mapinfo, dynamic_mapinfo, false);
2089     }
2090   }
2091 
2092   if (result == MAP_ARCHIVE_SUCCESS) {
2093     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());
2094     char* cds_base = static_mapinfo-&gt;mapped_base();
2095     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();
</pre>
<hr />
<pre>
2132   }
2133   if (Arguments::GetSharedDynamicArchivePath() == NULL) {
2134     return NULL;
2135   }
2136 
2137   FileMapInfo* mapinfo = new FileMapInfo(false);
2138   if (!mapinfo-&gt;initialize()) {
2139     delete(mapinfo);
2140     return NULL;
2141   }
2142   return mapinfo;
2143 }
2144 
2145 // use_requested_addr:
2146 //  true  = map at FileMapHeader::_requested_base_address
2147 //  false = map at an alternative address picked by OS.
2148 MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
2149                                                bool use_requested_addr) {
2150   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
2151       // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="line-modified">2152       // For debug builds, the check is done in FileMapInfo::map_regions for better test coverage.</span>

2153       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2154       return MAP_ARCHIVE_MMAP_FAILURE;
2155     });
2156 
2157   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
2158     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);
2159     return MAP_ARCHIVE_MMAP_FAILURE;
2160   };
2161 
2162   if (dynamic_mapinfo != NULL) {
2163     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
2164     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
2165     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
2166   }
2167 
<span class="line-modified">2168   ReservedSpace main_rs, archive_space_rs, class_space_rs;</span>
2169   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
2170   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="line-modified">2171                                                                  use_requested_addr, main_rs, archive_space_rs,</span>
2172                                                                  class_space_rs);
2173   if (mapped_base_address == NULL) {
2174     result = MAP_ARCHIVE_MMAP_FAILURE;
2175   } else {





















2176     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2177                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
2178     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2179                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());























2180     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
2181     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
2182                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
2183 
2184     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="line-modified">2185       // This is for simulating mmap failures at the requested address. In debug builds, we do it</span>
<span class="line-modified">2186       // here (after all archives have possibly been mapped), so we can thoroughly test the code for</span>
<span class="line-modified">2187       // failure handling (releasing all allocated resource, etc).</span>

2188       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2189       if (static_result == MAP_ARCHIVE_SUCCESS) {
2190         static_result = MAP_ARCHIVE_MMAP_FAILURE;
2191       }
2192       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2193         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;
2194       }
2195     });
2196 
2197     if (static_result == MAP_ARCHIVE_SUCCESS) {
2198       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2199         result = MAP_ARCHIVE_SUCCESS;
2200       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {
2201         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);
2202         // No need to retry mapping the dynamic archive again, as it will never succeed
2203         // (bad file, etc) -- just keep the base archive.
2204         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,
2205                                   dynamic_mapinfo-&gt;full_path());
2206         result = MAP_ARCHIVE_SUCCESS;
2207         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no
2208         // easy API to do that right now.
2209       } else {
2210         result = MAP_ARCHIVE_MMAP_FAILURE;
2211       }
2212     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {
2213       result = MAP_ARCHIVE_OTHER_FAILURE;
2214     } else {
2215       result = MAP_ARCHIVE_MMAP_FAILURE;
2216     }
2217   }
2218 
2219   if (result == MAP_ARCHIVE_SUCCESS) {
<span class="line-removed">2220     if (!main_rs.is_reserved() &amp;&amp; class_space_rs.is_reserved()) {</span>
<span class="line-removed">2221       MemTracker::record_virtual_memory_type((address)class_space_rs.base(), mtClass);</span>
<span class="line-removed">2222     }</span>
2223     SharedBaseAddress = (size_t)mapped_base_address;
2224     LP64_ONLY({
2225         if (Metaspace::using_class_space()) {
<span class="line-modified">2226           assert(class_space_rs.is_reserved(), &quot;must be&quot;);</span>
<span class="line-modified">2227           char* cds_base = static_mapinfo-&gt;mapped_base();</span>
<span class="line-modified">2228           Metaspace::allocate_metaspace_compressed_klass_ptrs(class_space_rs, NULL, (address)cds_base);</span>






2229           // map_heap_regions() compares the current narrow oop and klass encodings
2230           // with the archived ones, so it must be done after all encodings are determined.
2231           static_mapinfo-&gt;map_heap_regions();
<span class="line-removed">2232           CompressedKlassPointers::set_range(CompressedClassSpaceSize);</span>
2233         }
2234       });
2235   } else {
2236     unmap_archive(static_mapinfo);
2237     unmap_archive(dynamic_mapinfo);
<span class="line-modified">2238     release_reserved_spaces(main_rs, archive_space_rs, class_space_rs);</span>
2239   }
2240 
2241   return result;
2242 }
2243 






















































2244 char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
2245                                                           FileMapInfo* dynamic_mapinfo,
<span class="line-modified">2246                                                           bool use_requested_addr,</span>
<span class="line-removed">2247                                                           ReservedSpace&amp; main_rs,</span>
2248                                                           ReservedSpace&amp; archive_space_rs,
2249                                                           ReservedSpace&amp; class_space_rs) {
<span class="line-removed">2250   const bool use_klass_space = NOT_LP64(false) LP64_ONLY(Metaspace::using_class_space());</span>
<span class="line-removed">2251   const size_t class_space_size = NOT_LP64(0) LP64_ONLY(Metaspace::compressed_class_space_size());</span>
2252 
<span class="line-modified">2253   if (use_klass_space) {</span>
<span class="line-modified">2254     assert(class_space_size &gt; 0, &quot;CompressedClassSpaceSize must have been validated&quot;);</span>
<span class="line-removed">2255   }</span>
<span class="line-removed">2256   if (use_requested_addr &amp;&amp; !is_aligned(static_mapinfo-&gt;requested_base_address(), reserved_space_alignment())) {</span>
<span class="line-removed">2257     return NULL;</span>
<span class="line-removed">2258   }</span>
2259 
2260   // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="line-modified">2261   size_t base_offset = static_mapinfo-&gt;mapping_base_offset();</span>
<span class="line-modified">2262   size_t end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">2263   assert(base_offset == 0, &quot;must be&quot;);</span>
<span class="line-modified">2264   assert(is_aligned(end_offset,  os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">2265   assert(is_aligned(base_offset, os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">2266 </span>
<span class="line-modified">2267   // In case reserved_space_alignment() != os::vm_allocation_granularity()</span>
<span class="line-modified">2268   assert((size_t)os::vm_allocation_granularity() &lt;= reserved_space_alignment(), &quot;must be&quot;);</span>
<span class="line-modified">2269   end_offset = align_up(end_offset, reserved_space_alignment());</span>
<span class="line-modified">2270 </span>
<span class="line-modified">2271   size_t archive_space_size = end_offset - base_offset;</span>
<span class="line-removed">2272 </span>
<span class="line-removed">2273   // Special handling for Windows because it cannot mmap into a reserved space:</span>
<span class="line-removed">2274   //    use_requested_addr: We just map each region individually, and give up if any one of them fails.</span>
<span class="line-removed">2275   //   !use_requested_addr: We reserve the space first, and then os::read in all the regions (instead of mmap).</span>
<span class="line-removed">2276   //                        We&#39;re going to patch all the pointers anyway so there&#39;s no benefit for mmap.</span>
<span class="line-removed">2277 </span>
<span class="line-removed">2278   if (use_requested_addr) {</span>
<span class="line-removed">2279     char* archive_space_base = static_mapinfo-&gt;requested_base_address() + base_offset;</span>
<span class="line-removed">2280     char* archive_space_end  = archive_space_base + archive_space_size;</span>
<span class="line-removed">2281     if (!MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-removed">2282       archive_space_rs = reserve_shared_space(archive_space_size, archive_space_base);</span>
<span class="line-removed">2283       if (!archive_space_rs.is_reserved()) {</span>
<span class="line-removed">2284         return NULL;</span>
<span class="line-removed">2285       }</span>
<span class="line-removed">2286     }</span>
<span class="line-removed">2287     if (use_klass_space) {</span>
<span class="line-removed">2288       // Make sure we can map the klass space immediately following the archive_space space</span>
<span class="line-removed">2289       // Don&#39;t call reserve_shared_space here as that may try to enforce platform-specific</span>
<span class="line-removed">2290       // alignment rules which only apply to the archive base address</span>
<span class="line-removed">2291       char* class_space_base = archive_space_end;</span>
<span class="line-removed">2292       class_space_rs = ReservedSpace(class_space_size, reserved_space_alignment(),</span>
<span class="line-removed">2293                                      false /* large_pages */, class_space_base);</span>
<span class="line-removed">2294       if (!class_space_rs.is_reserved()) {</span>
<span class="line-removed">2295         return NULL;</span>
<span class="line-removed">2296       }</span>
<span class="line-removed">2297     }</span>
<span class="line-removed">2298     return static_mapinfo-&gt;requested_base_address();</span>
<span class="line-removed">2299   } else {</span>
<span class="line-removed">2300     if (use_klass_space) {</span>
<span class="line-removed">2301       main_rs = reserve_shared_space(archive_space_size + class_space_size);</span>
<span class="line-removed">2302       if (main_rs.is_reserved()) {</span>
<span class="line-removed">2303         archive_space_rs = main_rs.first_part(archive_space_size, reserved_space_alignment(), /*split=*/true);</span>
<span class="line-removed">2304         class_space_rs = main_rs.last_part(archive_space_size);</span>
<span class="line-removed">2305       }</span>
<span class="line-removed">2306     } else {</span>
<span class="line-removed">2307       main_rs = reserve_shared_space(archive_space_size);</span>
<span class="line-removed">2308       archive_space_rs = main_rs;</span>
2309     }







2310     if (archive_space_rs.is_reserved()) {


2311       return archive_space_rs.base();
<span class="line-removed">2312     } else {</span>
<span class="line-removed">2313       return NULL;</span>
2314     }








































2315   }































2316 }
2317 
<span class="line-modified">2318 void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; main_rs,</span>
<span class="line-removed">2319                                               ReservedSpace&amp; archive_space_rs,</span>
2320                                               ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2321   if (main_rs.is_reserved()) {</span>
<span class="line-modified">2322     assert(main_rs.contains(archive_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-modified">2323     assert(main_rs.contains(class_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-modified">2324     log_debug(cds)(&quot;Released shared space (archive+classes) &quot; INTPTR_FORMAT, p2i(main_rs.base()));</span>
<span class="line-modified">2325     main_rs.release();</span>
<span class="line-modified">2326   } else {</span>
<span class="line-modified">2327     if (archive_space_rs.is_reserved()) {</span>
<span class="line-removed">2328       log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-removed">2329       archive_space_rs.release();</span>
<span class="line-removed">2330     }</span>
<span class="line-removed">2331     if (class_space_rs.is_reserved()) {</span>
<span class="line-removed">2332       log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-removed">2333       class_space_rs.release();</span>
<span class="line-removed">2334     }</span>
2335   }
2336 }
2337 
2338 static int archive_regions[]  = {MetaspaceShared::mc,
2339                                  MetaspaceShared::rw,
2340                                  MetaspaceShared::ro};
2341 static int archive_regions_count  = 3;
2342 
2343 MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {
2344   assert(UseSharedSpaces, &quot;must be runtime&quot;);
2345   if (mapinfo == NULL) {
2346     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.
2347   }
2348 
2349   mapinfo-&gt;set_is_mapped(false);
2350 
2351   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {
2352     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT
2353                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());
2354     return MAP_ARCHIVE_OTHER_FAILURE;
</pre>
<hr />
<pre>
2459 }
2460 
2461 void MetaspaceShared::report_out_of_space(const char* name, size_t needed_bytes) {
2462   // This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.
2463   // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
2464   // or so.
2465   _mc_region.print_out_of_space_msg(name, needed_bytes);
2466   _rw_region.print_out_of_space_msg(name, needed_bytes);
2467   _ro_region.print_out_of_space_msg(name, needed_bytes);
2468 
2469   vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
2470                                 &quot;Please reduce the number of shared classes.&quot;);
2471 }
2472 
2473 // This is used to relocate the pointers so that the archive can be mapped at
2474 // Arguments::default_SharedBaseAddress() without runtime relocation.
2475 intx MetaspaceShared::final_delta() {
2476   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
2477        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
2478 }




























</pre>
</td>
<td>
<hr />
<pre>
  52 #include &quot;memory/universe.hpp&quot;
  53 #include &quot;oops/compressedOops.inline.hpp&quot;
  54 #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  55 #include &quot;oops/instanceMirrorKlass.hpp&quot;
  56 #include &quot;oops/instanceRefKlass.hpp&quot;
  57 #include &quot;oops/methodData.hpp&quot;
  58 #include &quot;oops/objArrayKlass.hpp&quot;
  59 #include &quot;oops/objArrayOop.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;oops/typeArrayKlass.hpp&quot;
  62 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  63 #include &quot;runtime/handles.inline.hpp&quot;
  64 #include &quot;runtime/os.hpp&quot;
  65 #include &quot;runtime/safepointVerifiers.hpp&quot;
  66 #include &quot;runtime/signature.hpp&quot;
  67 #include &quot;runtime/timerTrace.hpp&quot;
  68 #include &quot;runtime/vmThread.hpp&quot;
  69 #include &quot;runtime/vmOperations.hpp&quot;
  70 #include &quot;utilities/align.hpp&quot;
  71 #include &quot;utilities/bitMap.inline.hpp&quot;
<span class="line-added">  72 #include &quot;utilities/ostream.hpp&quot;</span>
  73 #include &quot;utilities/defaultStream.hpp&quot;
  74 #include &quot;utilities/hashtable.inline.hpp&quot;
  75 #if INCLUDE_G1GC
  76 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  77 #endif
  78 
  79 ReservedSpace MetaspaceShared::_shared_rs;
  80 VirtualSpace MetaspaceShared::_shared_vs;
  81 ReservedSpace MetaspaceShared::_symbol_rs;
  82 VirtualSpace MetaspaceShared::_symbol_vs;
  83 MetaspaceSharedStats MetaspaceShared::_stats;
  84 bool MetaspaceShared::_has_error_classes;
  85 bool MetaspaceShared::_archive_loading_failed = false;
  86 bool MetaspaceShared::_remapped_readwrite = false;
  87 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  88 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  89 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  90 intx MetaspaceShared::_relocation_delta;
  91 
  92 // The CDS archive is divided into the following regions:
</pre>
<hr />
<pre>
 175                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 176   if (strcmp(_name, failing_region) == 0) {
 177     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 178   }
 179 }
 180 
 181 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
 182   _rs = rs;
 183   _vs = vs;
 184   // Start with 0 committed bytes. The memory will be committed as needed by
 185   // MetaspaceShared::commit_to().
 186   if (!_vs-&gt;initialize(*_rs, 0)) {
 187     fatal(&quot;Unable to allocate memory for shared space&quot;);
 188   }
 189   _base = _top = _rs-&gt;base();
 190   _end = _rs-&gt;end();
 191 }
 192 
 193 void DumpRegion::pack(DumpRegion* next) {
 194   assert(!is_packed(), &quot;sanity&quot;);
<span class="line-modified"> 195   _end = (char*)align_up(_top, MetaspaceShared::reserved_space_alignment());</span>
 196   _is_packed = true;
 197   if (next != NULL) {
 198     next-&gt;_rs = _rs;
 199     next-&gt;_vs = _vs;
 200     next-&gt;_base = next-&gt;_top = this-&gt;_end;
 201     next-&gt;_end = _rs-&gt;end();
 202   }
 203 }
 204 
 205 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);
 206 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 207 
 208 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
 209   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);
 210 }
 211 
 212 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 213   return &amp;_mc_region;
 214 }
 215 
</pre>
<hr />
<pre>
 221   return &amp;_ro_region;
 222 }
 223 
 224 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 225                                       ReservedSpace* rs) {
 226   current-&gt;pack(next);
 227 }
 228 
 229 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
 230   return _symbol_region.allocate(num_bytes);
 231 }
 232 
 233 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 234   return _mc_region.allocate(num_bytes);
 235 }
 236 
 237 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 238   return _ro_region.allocate(num_bytes);
 239 }
 240 
<span class="line-modified"> 241 size_t MetaspaceShared::reserved_space_alignment() { return os::vm_allocation_granularity(); }</span>















 242 
<span class="line-modified"> 243 #ifdef _LP64</span>
<span class="line-modified"> 244 // Check SharedBaseAddress for validity. At this point, os::init() must</span>
<span class="line-modified"> 245 //  have been ran.</span>
<span class="line-added"> 246 static void check_SharedBaseAddress() {</span>
<span class="line-added"> 247   SharedBaseAddress = align_up(SharedBaseAddress,</span>
<span class="line-added"> 248                                MetaspaceShared::reserved_space_alignment());</span>
<span class="line-added"> 249   if (!CompressedKlassPointers::is_valid_base((address)SharedBaseAddress)) {</span>
<span class="line-added"> 250     log_warning(cds)(&quot;SharedBaseAddress=&quot; PTR_FORMAT &quot; is invalid for this &quot;</span>
<span class="line-added"> 251                      &quot;platform, option will be ignored.&quot;,</span>
<span class="line-added"> 252                      p2i((address)SharedBaseAddress));</span>
<span class="line-added"> 253     SharedBaseAddress = Arguments::default_SharedBaseAddress();</span>
<span class="line-added"> 254   }</span>
 255 }
<span class="line-added"> 256 #endif</span>
 257 
 258 void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
 259   assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified"> 260 </span>
<span class="line-added"> 261 #ifdef _LP64</span>
<span class="line-added"> 262   check_SharedBaseAddress();</span>
<span class="line-added"> 263 #endif</span>
<span class="line-added"> 264 </span>
<span class="line-added"> 265   const size_t reserve_alignment = MetaspaceShared::reserved_space_alignment();</span>
 266   char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
 267 
 268 #ifdef _LP64
<span class="line-modified"> 269   assert(CompressedKlassPointers::is_valid_base((address)shared_base), &quot;Sanity&quot;);</span>
<span class="line-modified"> 270   // On 64-bit VM we reserve a 4G range and, if UseCompressedClassPointers=1,</span>
<span class="line-modified"> 271   //  will use that to house both the archives and the ccs. See below for</span>
<span class="line-modified"> 272   //  details.</span>






 273   const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
 274   const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
 275 #else
<span class="line-modified"> 276   // We don&#39;t support archives larger than 256MB on 32-bit due to limited</span>
<span class="line-added"> 277   //  virtual address space.</span>
 278   size_t cds_total = align_down(256*M, reserve_alignment);
 279 #endif
 280 
<span class="line-added"> 281   // Whether to use SharedBaseAddress as attach address.</span>
 282   bool use_requested_base = true;
<span class="line-added"> 283 </span>
<span class="line-added"> 284   if (shared_base == NULL) {</span>
<span class="line-added"> 285     use_requested_base = false;</span>
<span class="line-added"> 286   }</span>
<span class="line-added"> 287 </span>
 288   if (ArchiveRelocationMode == 1) {
 289     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
 290     use_requested_base = false;
 291   }
 292 
 293   // First try to reserve the space at the specified SharedBaseAddress.
 294   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
 295   if (use_requested_base) {
<span class="line-modified"> 296     _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="line-added"> 297                                false /* large */, (char*)shared_base);</span>
<span class="line-added"> 298     if (_shared_rs.is_reserved()) {</span>
<span class="line-added"> 299       assert(_shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="line-added"> 300     } else {</span>
<span class="line-added"> 301       log_info(cds)(&quot;dumptime space reservation: failed to map at &quot;</span>
<span class="line-added"> 302                     &quot;SharedBaseAddress &quot; PTR_FORMAT, p2i(shared_base));</span>
<span class="line-added"> 303     }</span>
 304   }
<span class="line-modified"> 305   if (!_shared_rs.is_reserved()) {</span>
<span class="line-modified"> 306     // Get a reserved space anywhere if attaching at the SharedBaseAddress</span>
<span class="line-modified"> 307     //  fails:</span>
<span class="line-modified"> 308     if (UseCompressedClassPointers) {</span>
<span class="line-modified"> 309       // If we need to reserve class space as well, let the platform handle</span>
<span class="line-added"> 310       //  the reservation.</span>
<span class="line-added"> 311       LP64_ONLY(_shared_rs =</span>
<span class="line-added"> 312                 Metaspace::reserve_address_space_for_compressed_classes(cds_total);)</span>
<span class="line-added"> 313       NOT_LP64(ShouldNotReachHere();)</span>
<span class="line-added"> 314     } else {</span>
<span class="line-added"> 315       // anywhere is fine.</span>
<span class="line-added"> 316       _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="line-added"> 317                                  false /* large */, (char*)NULL);</span>
<span class="line-added"> 318     }</span>
 319   }
<span class="line-added"> 320 </span>
 321   if (!_shared_rs.is_reserved()) {
 322     vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
 323                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
 324   }
 325 
 326 #ifdef _LP64











 327 
 328   if (UseCompressedClassPointers) {
<span class="line-modified"> 329 </span>
<span class="line-modified"> 330     assert(CompressedKlassPointers::is_valid_base((address)_shared_rs.base()), &quot;Sanity&quot;);</span>
<span class="line-modified"> 331 </span>
<span class="line-modified"> 332     // On 64-bit VM, if UseCompressedClassPointers=1, the compressed class space</span>
<span class="line-modified"> 333     //  must be allocated near the cds such as that the compressed Klass pointer</span>
<span class="line-modified"> 334     //  encoding can be used to en/decode pointers from both cds and ccs. Since</span>
<span class="line-modified"> 335     //  Metaspace cannot do this (it knows nothing about cds), we do it for</span>
<span class="line-added"> 336     //  Metaspace here and pass it the space to use for ccs.</span>
<span class="line-added"> 337     //</span>
<span class="line-added"> 338     // We do this by reserving space for the ccs behind the archives. Note</span>
<span class="line-added"> 339     //  however that ccs follows a different alignment</span>
<span class="line-added"> 340     //  (Metaspace::reserve_alignment), so there may be a gap between ccs and</span>
<span class="line-added"> 341     //  cds.</span>
<span class="line-added"> 342     // We use a similar layout at runtime, see reserve_address_space_for_archives().</span>
<span class="line-added"> 343     //</span>
<span class="line-added"> 344     //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="line-added"> 345     //                              v</span>
<span class="line-added"> 346     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="line-added"> 347     // |    Heap    | Archive |     | MC | RW | RO | [gap]  |    class space  |</span>
<span class="line-added"> 348     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="line-added"> 349     // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="line-added"> 350     //</span>
<span class="line-added"> 351     // Note: ccs must follow the archives, and the archives must start at the</span>
<span class="line-added"> 352     //  encoding base. However, the exact placement of ccs does not matter as</span>
<span class="line-added"> 353     //  long as it it resides in the encoding range of CompressedKlassPointers</span>
<span class="line-added"> 354     //  and comes after the archive.</span>
<span class="line-added"> 355     //</span>
<span class="line-added"> 356     // We do this by splitting up the allocated 4G into 3G of archive space,</span>
<span class="line-added"> 357     //  followed by 1G for the ccs:</span>
<span class="line-added"> 358     // + The upper 1 GB is used as the &quot;temporary compressed class space&quot;</span>
<span class="line-added"> 359     //   -- preload_classes() will store Klasses into this space.</span>
<span class="line-added"> 360     // + The lower 3 GB is used for the archive -- when preload_classes()</span>
<span class="line-added"> 361     //   is done, ArchiveCompactor will copy the class metadata into this</span>
<span class="line-added"> 362     //   space, first the RW parts, then the RO parts.</span>
<span class="line-added"> 363 </span>
<span class="line-added"> 364     // Starting address of ccs must be aligned to Metaspace::reserve_alignment()...</span>
<span class="line-added"> 365     size_t class_space_size = align_down(_shared_rs.size() / 4, Metaspace::reserve_alignment());</span>
<span class="line-added"> 366     address class_space_start = (address)align_down(_shared_rs.end() - class_space_size, Metaspace::reserve_alignment());</span>
<span class="line-added"> 367     size_t archive_size = class_space_start - (address)_shared_rs.base();</span>
<span class="line-added"> 368 </span>
<span class="line-added"> 369     ReservedSpace tmp_class_space = _shared_rs.last_part(archive_size);</span>
<span class="line-added"> 370     _shared_rs = _shared_rs.first_part(archive_size);</span>
<span class="line-added"> 371 </span>
<span class="line-added"> 372     // ... as does the size of ccs.</span>
<span class="line-added"> 373     tmp_class_space = tmp_class_space.first_part(class_space_size);</span>
<span class="line-added"> 374     CompressedClassSpaceSize = class_space_size;</span>
<span class="line-added"> 375 </span>
<span class="line-added"> 376     // Let Metaspace initialize ccs</span>
 377     Metaspace::initialize_class_space(tmp_class_space);
<span class="line-added"> 378 </span>
<span class="line-added"> 379     // and set up CompressedKlassPointers encoding.</span>
<span class="line-added"> 380     CompressedKlassPointers::initialize((address)_shared_rs.base(), cds_total);</span>
<span class="line-added"> 381 </span>
<span class="line-added"> 382     log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="line-added"> 383                   p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
<span class="line-added"> 384 </span>
<span class="line-added"> 385     log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="line-added"> 386                   CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
<span class="line-added"> 387 </span>
<span class="line-added"> 388     assert(_shared_rs.end() == tmp_class_space.base() &amp;&amp;</span>
<span class="line-added"> 389            is_aligned(_shared_rs.base(), MetaspaceShared::reserved_space_alignment()) &amp;&amp;</span>
<span class="line-added"> 390            is_aligned(tmp_class_space.base(), Metaspace::reserve_alignment()) &amp;&amp;</span>
<span class="line-added"> 391            is_aligned(tmp_class_space.size(), Metaspace::reserve_alignment()), &quot;Sanity&quot;);</span>
 392   }


 393 


 394 #endif
 395 
 396   init_shared_dump_space(&amp;_mc_region);
 397   SharedBaseAddress = (size_t)_shared_rs.base();
 398   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 399                 _shared_rs.size(), p2i(_shared_rs.base()));
 400 
 401   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
 402   _symbol_rs = ReservedSpace(symbol_rs_size);
 403   if (!_symbol_rs.is_reserved()) {
 404     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,
 405                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));
 406   }
 407   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);
 408 }
 409 
 410 // Called by universe_post_init()
 411 void MetaspaceShared::post_initialize(TRAPS) {
 412   if (UseSharedSpaces) {
 413     int size = FileMapInfo::get_number_of_shared_paths();
</pre>
<hr />
<pre>
1202                                const char *name, size_t total_size);
1203   void relocate_to_default_base_address(CHeapBitMap* ptrmap);
1204 
1205 public:
1206 
1207   VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
1208   void doit();   // outline because gdb sucks
1209   bool allow_nested_vm_operations() const { return true; }
1210 }; // class VM_PopulateDumpSharedSpace
1211 
1212 class SortedSymbolClosure: public SymbolClosure {
1213   GrowableArray&lt;Symbol*&gt; _symbols;
1214   virtual void do_symbol(Symbol** sym) {
1215     assert((*sym)-&gt;is_permanent(), &quot;archived symbols must be permanent&quot;);
1216     _symbols.append(*sym);
1217   }
1218   static int compare_symbols_by_address(Symbol** a, Symbol** b) {
1219     if (a[0] &lt; b[0]) {
1220       return -1;
1221     } else if (a[0] == b[0]) {
<span class="line-added">1222       ResourceMark rm;</span>
<span class="line-added">1223       log_warning(cds)(&quot;Duplicated symbol %s unexpected&quot;, (*a)-&gt;as_C_string());</span>
1224       return 0;
1225     } else {
1226       return 1;
1227     }
1228   }
1229 
1230 public:
1231   SortedSymbolClosure() {
1232     SymbolTable::symbols_do(this);
1233     _symbols.sort(compare_symbols_by_address);
1234   }
1235   GrowableArray&lt;Symbol*&gt;* get_sorted_symbols() {
1236     return &amp;_symbols;
1237   }
1238 };
1239 
1240 // ArchiveCompactor --
1241 //
1242 // This class is the central piece of shared archive compaction -- all metaspace data are
1243 // initially allocated outside of the shared regions. ArchiveCompactor copies the
</pre>
<hr />
<pre>
2117 
2118 bool MetaspaceShared::is_in_trampoline_frame(address addr) {
2119   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {
2120     return true;
2121   }
2122   return false;
2123 }
2124 
2125 bool MetaspaceShared::is_shared_dynamic(void* p) {
2126   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;
2127       (p &gt;= _shared_metaspace_static_top)) {
2128     return true;
2129   } else {
2130     return false;
2131   }
2132 }
2133 
2134 void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
2135   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
2136   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
<span class="line-added">2137 </span>
2138   FileMapInfo* static_mapinfo = open_static_archive();
2139   FileMapInfo* dynamic_mapinfo = NULL;
2140 
2141   if (static_mapinfo != NULL) {
2142     dynamic_mapinfo = open_dynamic_archive();
2143 
2144     // First try to map at the requested address
2145     result = map_archives(static_mapinfo, dynamic_mapinfo, true);
2146     if (result == MAP_ARCHIVE_MMAP_FAILURE) {
2147       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen
2148       // by the OS.
2149       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);
2150       result = map_archives(static_mapinfo, dynamic_mapinfo, false);
2151     }
2152   }
2153 
2154   if (result == MAP_ARCHIVE_SUCCESS) {
2155     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());
2156     char* cds_base = static_mapinfo-&gt;mapped_base();
2157     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();
</pre>
<hr />
<pre>
2194   }
2195   if (Arguments::GetSharedDynamicArchivePath() == NULL) {
2196     return NULL;
2197   }
2198 
2199   FileMapInfo* mapinfo = new FileMapInfo(false);
2200   if (!mapinfo-&gt;initialize()) {
2201     delete(mapinfo);
2202     return NULL;
2203   }
2204   return mapinfo;
2205 }
2206 
2207 // use_requested_addr:
2208 //  true  = map at FileMapHeader::_requested_base_address
2209 //  false = map at an alternative address picked by OS.
2210 MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
2211                                                bool use_requested_addr) {
2212   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
2213       // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="line-modified">2214       // For debug builds, the check is done below, after reserving the space, for better test coverage</span>
<span class="line-added">2215       // (see comment below).</span>
2216       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2217       return MAP_ARCHIVE_MMAP_FAILURE;
2218     });
2219 
2220   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
2221     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);
2222     return MAP_ARCHIVE_MMAP_FAILURE;
2223   };
2224 
2225   if (dynamic_mapinfo != NULL) {
2226     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
2227     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
2228     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
2229   }
2230 
<span class="line-modified">2231   ReservedSpace archive_space_rs, class_space_rs;</span>
2232   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
2233   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="line-modified">2234                                                                  use_requested_addr, archive_space_rs,</span>
2235                                                                  class_space_rs);
2236   if (mapped_base_address == NULL) {
2237     result = MAP_ARCHIVE_MMAP_FAILURE;
2238   } else {
<span class="line-added">2239 </span>
<span class="line-added">2240 #ifdef ASSERT</span>
<span class="line-added">2241     // Some sanity checks after reserving address spaces for archives</span>
<span class="line-added">2242     //  and class space.</span>
<span class="line-added">2243     assert(archive_space_rs.is_reserved(), &quot;Sanity&quot;);</span>
<span class="line-added">2244     if (Metaspace::using_class_space()) {</span>
<span class="line-added">2245       // Class space must closely follow the archive space. Both spaces</span>
<span class="line-added">2246       //  must be aligned correctly.</span>
<span class="line-added">2247       assert(class_space_rs.is_reserved(),</span>
<span class="line-added">2248              &quot;A class space should have been reserved&quot;);</span>
<span class="line-added">2249       assert(class_space_rs.base() &gt;= archive_space_rs.end(),</span>
<span class="line-added">2250              &quot;class space should follow the cds archive space&quot;);</span>
<span class="line-added">2251       assert(is_aligned(archive_space_rs.base(),</span>
<span class="line-added">2252                         MetaspaceShared::reserved_space_alignment()),</span>
<span class="line-added">2253              &quot;Archive space misaligned&quot;);</span>
<span class="line-added">2254       assert(is_aligned(class_space_rs.base(),</span>
<span class="line-added">2255                         Metaspace::reserve_alignment()),</span>
<span class="line-added">2256              &quot;class space misaligned&quot;);</span>
<span class="line-added">2257     }</span>
<span class="line-added">2258 #endif // ASSERT</span>
<span class="line-added">2259 </span>
2260     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2261                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
2262     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2263                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());
<span class="line-added">2264 </span>
<span class="line-added">2265     if (MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-added">2266       // We have now reserved address space for the archives, and will map in</span>
<span class="line-added">2267       //  the archive files into this space.</span>
<span class="line-added">2268       //</span>
<span class="line-added">2269       // Special handling for Windows: on Windows we cannot map a file view</span>
<span class="line-added">2270       //  into an existing memory mapping. So, we unmap the address range we</span>
<span class="line-added">2271       //  just reserved again, which will make it available for mapping the</span>
<span class="line-added">2272       //  archives.</span>
<span class="line-added">2273       // Reserving this range has not been for naught however since it makes</span>
<span class="line-added">2274       //  us reasonably sure the address range is available.</span>
<span class="line-added">2275       //</span>
<span class="line-added">2276       // But still it may fail, since between unmapping the range and mapping</span>
<span class="line-added">2277       //  in the archive someone else may grab the address space. Therefore</span>
<span class="line-added">2278       //  there is a fallback in FileMap::map_region() where we just read in</span>
<span class="line-added">2279       //  the archive files sequentially instead of mapping it in. We couple</span>
<span class="line-added">2280       //  this with use_requested_addr, since we&#39;re going to patch all the</span>
<span class="line-added">2281       //  pointers anyway so there&#39;s no benefit to mmap.</span>
<span class="line-added">2282       if (use_requested_addr) {</span>
<span class="line-added">2283         log_info(cds)(&quot;Windows mmap workaround: releasing archive space.&quot;);</span>
<span class="line-added">2284         archive_space_rs.release();</span>
<span class="line-added">2285       }</span>
<span class="line-added">2286     }</span>
2287     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
2288     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
2289                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
2290 
2291     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="line-modified">2292       // This is for simulating mmap failures at the requested address. In</span>
<span class="line-modified">2293       //  debug builds, we do it here (after all archives have possibly been</span>
<span class="line-modified">2294       //  mapped), so we can thoroughly test the code for failure handling</span>
<span class="line-added">2295       //  (releasing all allocated resource, etc).</span>
2296       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2297       if (static_result == MAP_ARCHIVE_SUCCESS) {
2298         static_result = MAP_ARCHIVE_MMAP_FAILURE;
2299       }
2300       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2301         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;
2302       }
2303     });
2304 
2305     if (static_result == MAP_ARCHIVE_SUCCESS) {
2306       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2307         result = MAP_ARCHIVE_SUCCESS;
2308       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {
2309         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);
2310         // No need to retry mapping the dynamic archive again, as it will never succeed
2311         // (bad file, etc) -- just keep the base archive.
2312         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,
2313                                   dynamic_mapinfo-&gt;full_path());
2314         result = MAP_ARCHIVE_SUCCESS;
2315         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no
2316         // easy API to do that right now.
2317       } else {
2318         result = MAP_ARCHIVE_MMAP_FAILURE;
2319       }
2320     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {
2321       result = MAP_ARCHIVE_OTHER_FAILURE;
2322     } else {
2323       result = MAP_ARCHIVE_MMAP_FAILURE;
2324     }
2325   }
2326 
2327   if (result == MAP_ARCHIVE_SUCCESS) {



2328     SharedBaseAddress = (size_t)mapped_base_address;
2329     LP64_ONLY({
2330         if (Metaspace::using_class_space()) {
<span class="line-modified">2331           // Set up ccs in metaspace.</span>
<span class="line-modified">2332           Metaspace::initialize_class_space(class_space_rs);</span>
<span class="line-modified">2333 </span>
<span class="line-added">2334           // Set up compressed Klass pointer encoding: the encoding range must</span>
<span class="line-added">2335           //  cover both archive and class space.</span>
<span class="line-added">2336           address cds_base = (address)static_mapinfo-&gt;mapped_base();</span>
<span class="line-added">2337           address ccs_end = (address)class_space_rs.end();</span>
<span class="line-added">2338           CompressedKlassPointers::initialize(cds_base, ccs_end - cds_base);</span>
<span class="line-added">2339 </span>
2340           // map_heap_regions() compares the current narrow oop and klass encodings
2341           // with the archived ones, so it must be done after all encodings are determined.
2342           static_mapinfo-&gt;map_heap_regions();

2343         }
2344       });
2345   } else {
2346     unmap_archive(static_mapinfo);
2347     unmap_archive(dynamic_mapinfo);
<span class="line-modified">2348     release_reserved_spaces(archive_space_rs, class_space_rs);</span>
2349   }
2350 
2351   return result;
2352 }
2353 
<span class="line-added">2354 </span>
<span class="line-added">2355 // This will reserve two address spaces suitable to house Klass structures, one</span>
<span class="line-added">2356 //  for the cds archives (static archive and optionally dynamic archive) and</span>
<span class="line-added">2357 //  optionally one move for ccs.</span>
<span class="line-added">2358 //</span>
<span class="line-added">2359 // Since both spaces must fall within the compressed class pointer encoding</span>
<span class="line-added">2360 //  range, they are allocated close to each other.</span>
<span class="line-added">2361 //</span>
<span class="line-added">2362 // Space for archives will be reserved first, followed by a potential gap,</span>
<span class="line-added">2363 //  followed by the space for ccs:</span>
<span class="line-added">2364 //</span>
<span class="line-added">2365 // +-- Base address             A        B                     End</span>
<span class="line-added">2366 // |                            |        |                      |</span>
<span class="line-added">2367 // v                            v        v                      v</span>
<span class="line-added">2368 // +-------------+--------------+        +----------------------+</span>
<span class="line-added">2369 // | static arc  | [dyn. arch]  | [gap]  | compr. class space   |</span>
<span class="line-added">2370 // +-------------+--------------+        +----------------------+</span>
<span class="line-added">2371 //</span>
<span class="line-added">2372 // (The gap may result from different alignment requirements between metaspace</span>
<span class="line-added">2373 //  and CDS)</span>
<span class="line-added">2374 //</span>
<span class="line-added">2375 // If UseCompressedClassPointers is disabled, only one address space will be</span>
<span class="line-added">2376 //  reserved:</span>
<span class="line-added">2377 //</span>
<span class="line-added">2378 // +-- Base address             End</span>
<span class="line-added">2379 // |                            |</span>
<span class="line-added">2380 // v                            v</span>
<span class="line-added">2381 // +-------------+--------------+</span>
<span class="line-added">2382 // | static arc  | [dyn. arch]  |</span>
<span class="line-added">2383 // +-------------+--------------+</span>
<span class="line-added">2384 //</span>
<span class="line-added">2385 // Base address: If use_archive_base_addr address is true, the Base address is</span>
<span class="line-added">2386 //  determined by the address stored in the static archive. If</span>
<span class="line-added">2387 //  use_archive_base_addr address is false, this base address is determined</span>
<span class="line-added">2388 //  by the platform.</span>
<span class="line-added">2389 //</span>
<span class="line-added">2390 // If UseCompressedClassPointers=1, the range encompassing both spaces will be</span>
<span class="line-added">2391 //  suitable to en/decode narrow Klass pointers: the base will be valid for</span>
<span class="line-added">2392 //  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.</span>
<span class="line-added">2393 //</span>
<span class="line-added">2394 // Return:</span>
<span class="line-added">2395 //</span>
<span class="line-added">2396 // - On success:</span>
<span class="line-added">2397 //    - archive_space_rs will be reserved and large enough to host static and</span>
<span class="line-added">2398 //      if needed dynamic archive: [Base, A).</span>
<span class="line-added">2399 //      archive_space_rs.base and size will be aligned to CDS reserve</span>
<span class="line-added">2400 //      granularity.</span>
<span class="line-added">2401 //    - class_space_rs: If UseCompressedClassPointers=1, class_space_rs will</span>
<span class="line-added">2402 //      be reserved. Its start address will be aligned to metaspace reserve</span>
<span class="line-added">2403 //      alignment, which may differ from CDS alignment. It will follow the cds</span>
<span class="line-added">2404 //      archive space, close enough such that narrow class pointer encoding</span>
<span class="line-added">2405 //      covers both spaces.</span>
<span class="line-added">2406 //      If UseCompressedClassPointers=0, class_space_rs remains unreserved.</span>
<span class="line-added">2407 // - On error: NULL is returned and the spaces remain unreserved.</span>
2408 char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
2409                                                           FileMapInfo* dynamic_mapinfo,
<span class="line-modified">2410                                                           bool use_archive_base_addr,</span>

2411                                                           ReservedSpace&amp; archive_space_rs,
2412                                                           ReservedSpace&amp; class_space_rs) {


2413 
<span class="line-modified">2414   address const base_address = (address) (use_archive_base_addr ? static_mapinfo-&gt;requested_base_address() : NULL);</span>
<span class="line-modified">2415   const size_t archive_space_alignment = MetaspaceShared::reserved_space_alignment();</span>




2416 
2417   // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="line-modified">2418   assert(static_mapinfo-&gt;mapping_base_offset() == 0, &quot;Must be&quot;);</span>
<span class="line-modified">2419   size_t archive_end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">2420   size_t archive_space_size = align_up(archive_end_offset, archive_space_alignment);</span>
<span class="line-modified">2421 </span>
<span class="line-modified">2422   // If a base address is given, it must have valid alignment and be suitable as encoding base.</span>
<span class="line-modified">2423   if (base_address != NULL) {</span>
<span class="line-modified">2424     assert(is_aligned(base_address, archive_space_alignment),</span>
<span class="line-modified">2425            &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>
<span class="line-modified">2426     if (Metaspace::using_class_space()) {</span>
<span class="line-modified">2427       assert(CompressedKlassPointers::is_valid_base(base_address),</span>
<span class="line-modified">2428              &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>





































2429     }
<span class="line-added">2430   }</span>
<span class="line-added">2431 </span>
<span class="line-added">2432   if (!Metaspace::using_class_space()) {</span>
<span class="line-added">2433     // Get the simple case out of the way first:</span>
<span class="line-added">2434     // no compressed class space, simple allocation.</span>
<span class="line-added">2435     archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,</span>
<span class="line-added">2436                                      false /* bool large */, (char*)base_address);</span>
2437     if (archive_space_rs.is_reserved()) {
<span class="line-added">2438       assert(base_address == NULL ||</span>
<span class="line-added">2439              (address)archive_space_rs.base() == base_address, &quot;Sanity&quot;);</span>
2440       return archive_space_rs.base();


2441     }
<span class="line-added">2442     return NULL;</span>
<span class="line-added">2443   }</span>
<span class="line-added">2444 </span>
<span class="line-added">2445 #ifdef _LP64</span>
<span class="line-added">2446 </span>
<span class="line-added">2447   // Complex case: two spaces adjacent to each other, both to be addressable</span>
<span class="line-added">2448   //  with narrow class pointers.</span>
<span class="line-added">2449   // We reserve the whole range spanning both spaces, then split that range up.</span>
<span class="line-added">2450 </span>
<span class="line-added">2451   const size_t class_space_alignment = Metaspace::reserve_alignment();</span>
<span class="line-added">2452 </span>
<span class="line-added">2453   // To simplify matters, lets assume that metaspace alignment will always be</span>
<span class="line-added">2454   //  equal or a multiple of archive alignment.</span>
<span class="line-added">2455   assert(is_power_of_2(class_space_alignment) &amp;&amp;</span>
<span class="line-added">2456                        is_power_of_2(archive_space_alignment) &amp;&amp;</span>
<span class="line-added">2457                        class_space_alignment &gt;= archive_space_alignment,</span>
<span class="line-added">2458                        &quot;Sanity&quot;);</span>
<span class="line-added">2459 </span>
<span class="line-added">2460   const size_t class_space_size = CompressedClassSpaceSize;</span>
<span class="line-added">2461   assert(CompressedClassSpaceSize &gt; 0 &amp;&amp;</span>
<span class="line-added">2462          is_aligned(CompressedClassSpaceSize, class_space_alignment),</span>
<span class="line-added">2463          &quot;CompressedClassSpaceSize malformed: &quot;</span>
<span class="line-added">2464          SIZE_FORMAT, CompressedClassSpaceSize);</span>
<span class="line-added">2465 </span>
<span class="line-added">2466   const size_t ccs_begin_offset = align_up(archive_space_size,</span>
<span class="line-added">2467                                            class_space_alignment);</span>
<span class="line-added">2468   const size_t gap_size = ccs_begin_offset - archive_space_size;</span>
<span class="line-added">2469 </span>
<span class="line-added">2470   const size_t total_range_size =</span>
<span class="line-added">2471       align_up(archive_space_size + gap_size + class_space_size,</span>
<span class="line-added">2472                os::vm_allocation_granularity());</span>
<span class="line-added">2473 </span>
<span class="line-added">2474   ReservedSpace total_rs;</span>
<span class="line-added">2475   if (base_address != NULL) {</span>
<span class="line-added">2476     // Reserve at the given archive base address, or not at all.</span>
<span class="line-added">2477     total_rs = ReservedSpace(total_range_size, archive_space_alignment,</span>
<span class="line-added">2478                              false /* bool large */, (char*) base_address);</span>
<span class="line-added">2479   } else {</span>
<span class="line-added">2480     // Reserve at any address, but leave it up to the platform to choose a good one.</span>
<span class="line-added">2481     total_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size);</span>
2482   }
<span class="line-added">2483 </span>
<span class="line-added">2484   if (!total_rs.is_reserved()) {</span>
<span class="line-added">2485     return NULL;</span>
<span class="line-added">2486   }</span>
<span class="line-added">2487 </span>
<span class="line-added">2488   // Paranoid checks:</span>
<span class="line-added">2489   assert(base_address == NULL || (address)total_rs.base() == base_address,</span>
<span class="line-added">2490          &quot;Sanity (&quot; PTR_FORMAT &quot; vs &quot; PTR_FORMAT &quot;)&quot;, p2i(base_address), p2i(total_rs.base()));</span>
<span class="line-added">2491   assert(is_aligned(total_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2492   assert(total_rs.size() == total_range_size, &quot;Sanity&quot;);</span>
<span class="line-added">2493   assert(CompressedKlassPointers::is_valid_base((address)total_rs.base()), &quot;Sanity&quot;);</span>
<span class="line-added">2494 </span>
<span class="line-added">2495   // Now split up the space into ccs and cds archive. For simplicity, just leave</span>
<span class="line-added">2496   //  the gap reserved at the end of the archive space.</span>
<span class="line-added">2497   archive_space_rs = total_rs.first_part(ccs_begin_offset,</span>
<span class="line-added">2498                                          (size_t)os::vm_allocation_granularity(),</span>
<span class="line-added">2499                                          /*split=*/true);</span>
<span class="line-added">2500   class_space_rs = total_rs.last_part(ccs_begin_offset);</span>
<span class="line-added">2501 </span>
<span class="line-added">2502   assert(is_aligned(archive_space_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2503   assert(is_aligned(archive_space_rs.size(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2504   assert(is_aligned(class_space_rs.base(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2505   assert(is_aligned(class_space_rs.size(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2506 </span>
<span class="line-added">2507   return archive_space_rs.base();</span>
<span class="line-added">2508 </span>
<span class="line-added">2509 #else</span>
<span class="line-added">2510   ShouldNotReachHere();</span>
<span class="line-added">2511   return NULL;</span>
<span class="line-added">2512 #endif</span>
<span class="line-added">2513 </span>
2514 }
2515 
<span class="line-modified">2516 void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; archive_space_rs,</span>

2517                                               ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2518   if (archive_space_rs.is_reserved()) {</span>
<span class="line-modified">2519     log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-modified">2520     archive_space_rs.release();</span>
<span class="line-modified">2521   }</span>
<span class="line-modified">2522   if (class_space_rs.is_reserved()) {</span>
<span class="line-modified">2523     log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-modified">2524     class_space_rs.release();</span>







2525   }
2526 }
2527 
2528 static int archive_regions[]  = {MetaspaceShared::mc,
2529                                  MetaspaceShared::rw,
2530                                  MetaspaceShared::ro};
2531 static int archive_regions_count  = 3;
2532 
2533 MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {
2534   assert(UseSharedSpaces, &quot;must be runtime&quot;);
2535   if (mapinfo == NULL) {
2536     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.
2537   }
2538 
2539   mapinfo-&gt;set_is_mapped(false);
2540 
2541   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {
2542     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT
2543                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());
2544     return MAP_ARCHIVE_OTHER_FAILURE;
</pre>
<hr />
<pre>
2649 }
2650 
2651 void MetaspaceShared::report_out_of_space(const char* name, size_t needed_bytes) {
2652   // This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.
2653   // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
2654   // or so.
2655   _mc_region.print_out_of_space_msg(name, needed_bytes);
2656   _rw_region.print_out_of_space_msg(name, needed_bytes);
2657   _ro_region.print_out_of_space_msg(name, needed_bytes);
2658 
2659   vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
2660                                 &quot;Please reduce the number of shared classes.&quot;);
2661 }
2662 
2663 // This is used to relocate the pointers so that the archive can be mapped at
2664 // Arguments::default_SharedBaseAddress() without runtime relocation.
2665 intx MetaspaceShared::final_delta() {
2666   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
2667        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
2668 }
<span class="line-added">2669 </span>
<span class="line-added">2670 void MetaspaceShared::print_on(outputStream* st) {</span>
<span class="line-added">2671   if (UseSharedSpaces || DumpSharedSpaces) {</span>
<span class="line-added">2672     st-&gt;print(&quot;CDS archive(s) mapped at: &quot;);</span>
<span class="line-added">2673     address base;</span>
<span class="line-added">2674     address top;</span>
<span class="line-added">2675     if (UseSharedSpaces) { // Runtime</span>
<span class="line-added">2676       base = (address)MetaspaceObj::shared_metaspace_base();</span>
<span class="line-added">2677       address static_top = (address)_shared_metaspace_static_top;</span>
<span class="line-added">2678       top = (address)MetaspaceObj::shared_metaspace_top();</span>
<span class="line-added">2679       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(static_top), p2i(top));</span>
<span class="line-added">2680     } else if (DumpSharedSpaces) { // Dump Time</span>
<span class="line-added">2681       base = (address)_shared_rs.base();</span>
<span class="line-added">2682       top = (address)_shared_rs.end();</span>
<span class="line-added">2683       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(top));</span>
<span class="line-added">2684     }</span>
<span class="line-added">2685     st-&gt;print(&quot;size &quot; SIZE_FORMAT &quot;, &quot;, top - base);</span>
<span class="line-added">2686     st-&gt;print(&quot;SharedBaseAddress: &quot; PTR_FORMAT &quot;, ArchiveRelocationMode: %d.&quot;, SharedBaseAddress, (int)ArchiveRelocationMode);</span>
<span class="line-added">2687   } else {</span>
<span class="line-added">2688     st-&gt;print(&quot;CDS disabled.&quot;);</span>
<span class="line-added">2689   }</span>
<span class="line-added">2690   st-&gt;cr();</span>
<span class="line-added">2691 }</span>
<span class="line-added">2692 </span>
<span class="line-added">2693 </span>
<span class="line-added">2694 </span>
<span class="line-added">2695 </span>
<span class="line-added">2696 </span>
</pre>
</td>
</tr>
</table>
<center><a href="metaspaceClosure.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>