<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/share/memory/metaspace.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="filemap.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspace.hpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -971,206 +971,100 @@</span>
  bool Metaspace::_initialized = false;
  
  #define VIRTUALSPACEMULTIPLIER 2
  
  #ifdef _LP64
<span class="udiff-line-removed">- static const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- void Metaspace::set_narrow_klass_base_and_shift(ReservedSpace metaspace_rs, address cds_base) {</span>
<span class="udiff-line-removed">-   assert(!DumpSharedSpaces, &quot;narrow_klass is set by MetaspaceShared class.&quot;);</span>
<span class="udiff-line-removed">-   // Figure out the narrow_klass_base and the narrow_klass_shift.  The</span>
<span class="udiff-line-removed">-   // narrow_klass_base is the lower of the metaspace base and the cds base</span>
<span class="udiff-line-removed">-   // (if cds is enabled).  The narrow_klass_shift depends on the distance</span>
<span class="udiff-line-removed">-   // between the lower base and higher address.</span>
<span class="udiff-line-removed">-   address lower_base = (address)metaspace_rs.base();</span>
<span class="udiff-line-removed">-   address higher_address = (address)metaspace_rs.end();</span>
<span class="udiff-line-removed">-   if (cds_base != NULL) {</span>
<span class="udiff-line-removed">-     assert(UseSharedSpaces, &quot;must be&quot;);</span>
<span class="udiff-line-removed">-     lower_base = MIN2(lower_base, cds_base);</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     uint64_t klass_encoding_max = UnscaledClassSpaceMax &lt;&lt; LogKlassAlignmentInBytes;</span>
<span class="udiff-line-removed">-     // If compressed class space fits in lower 32G, we don&#39;t need a base.</span>
<span class="udiff-line-removed">-     if (higher_address &lt;= (address)klass_encoding_max) {</span>
<span class="udiff-line-removed">-       lower_base = 0; // Effectively lower base is zero.</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   CompressedKlassPointers::set_base(lower_base);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   // CDS uses LogKlassAlignmentInBytes for narrow_klass_shift. See</span>
<span class="udiff-line-removed">-   // MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() for</span>
<span class="udiff-line-removed">-   // how dump time narrow_klass_shift is set. Although, CDS can work</span>
<span class="udiff-line-removed">-   // with zero-shift mode also, to be consistent with AOT it uses</span>
<span class="udiff-line-removed">-   // LogKlassAlignmentInBytes for klass shift so archived java heap objects</span>
<span class="udiff-line-removed">-   // can be used at same time as AOT code.</span>
<span class="udiff-line-removed">-   if (!UseSharedSpaces</span>
<span class="udiff-line-removed">-       &amp;&amp; (uint64_t)(higher_address - lower_base) &lt;= UnscaledClassSpaceMax) {</span>
<span class="udiff-line-removed">-     CompressedKlassPointers::set_shift(0);</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">-   AOTLoader::set_narrow_klass_shift();</span>
<span class="udiff-line-removed">- }</span>
  
<span class="udiff-line-modified-removed">- // Try to allocate the metaspace at the requested addr.</span>
<span class="udiff-line-removed">- void Metaspace::allocate_metaspace_compressed_klass_ptrs(ReservedSpace metaspace_rs, char* requested_addr, address cds_base) {</span>
<span class="udiff-line-removed">-   assert(!DumpSharedSpaces, &quot;compress klass space is allocated by MetaspaceShared class.&quot;);</span>
<span class="udiff-line-removed">-   assert(using_class_space(), &quot;called improperly&quot;);</span>
<span class="udiff-line-removed">-   assert(UseCompressedClassPointers, &quot;Only use with CompressedKlassPtrs&quot;);</span>
<span class="udiff-line-removed">-   assert(compressed_class_space_size() &lt; KlassEncodingMetaspaceMax,</span>
<span class="udiff-line-removed">-          &quot;Metaspace size is too big&quot;);</span>
<span class="udiff-line-removed">-   assert_is_aligned(requested_addr, _reserve_alignment);</span>
<span class="udiff-line-removed">-   assert_is_aligned(cds_base, _reserve_alignment);</span>
<span class="udiff-line-removed">-   assert_is_aligned(compressed_class_space_size(), _reserve_alignment);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   if (metaspace_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-     // CDS should have already reserved the space.</span>
<span class="udiff-line-removed">-     assert(requested_addr == NULL, &quot;not used&quot;);</span>
<span class="udiff-line-removed">-     assert(cds_base != NULL, &quot;CDS should have already reserved the memory space&quot;);</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     assert(cds_base == NULL, &quot;must be&quot;);</span>
<span class="udiff-line-removed">-     metaspace_rs = reserve_space(compressed_class_space_size(),</span>
<span class="udiff-line-removed">-                                  _reserve_alignment, requested_addr,</span>
<span class="udiff-line-removed">-                                  false /* use_requested_addr */);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   if (!metaspace_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-     assert(cds_base == NULL, &quot;CDS should have already reserved the memory space&quot;);</span>
<span class="udiff-line-removed">-     // If no successful allocation then try to allocate the space anywhere.  If</span>
<span class="udiff-line-removed">-     // that fails then OOM doom.  At this point we cannot try allocating the</span>
<span class="udiff-line-removed">-     // metaspace as if UseCompressedClassPointers is off because too much</span>
<span class="udiff-line-removed">-     // initialization has happened that depends on UseCompressedClassPointers.</span>
<span class="udiff-line-removed">-     // So, UseCompressedClassPointers cannot be turned off at this point.</span>
<span class="udiff-line-removed">-     metaspace_rs = reserve_space(compressed_class_space_size(),</span>
<span class="udiff-line-removed">-                                  _reserve_alignment, NULL, false);</span>
<span class="udiff-line-removed">-     if (!metaspace_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-       vm_exit_during_initialization(err_msg(&quot;Could not allocate metaspace: &quot; SIZE_FORMAT &quot; bytes&quot;,</span>
<span class="udiff-line-removed">-                                             compressed_class_space_size()));</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   if (cds_base == NULL) {</span>
<span class="udiff-line-removed">-     // If we got here then the metaspace got allocated.</span>
<span class="udiff-line-removed">-     MemTracker::record_virtual_memory_type((address)metaspace_rs.base(), mtClass);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   set_narrow_klass_base_and_shift(metaspace_rs, cds_base);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   initialize_class_space(metaspace_rs);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   LogTarget(Trace, gc, metaspace) lt;</span>
<span class="udiff-line-removed">-   if (lt.is_enabled()) {</span>
<span class="udiff-line-removed">-     ResourceMark rm;</span>
<span class="udiff-line-removed">-     LogStream ls(lt);</span>
<span class="udiff-line-removed">-     print_compressed_class_space(&amp;ls, requested_addr);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- void Metaspace::print_compressed_class_space(outputStream* st, const char* requested_addr) {</span>
<span class="udiff-line-removed">-   st-&gt;print_cr(&quot;Narrow klass base: &quot; PTR_FORMAT &quot;, Narrow klass shift: %d&quot;,</span>
<span class="udiff-line-removed">-                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
<span class="udiff-line-modified-added">+ void Metaspace::print_compressed_class_space(outputStream* st) {</span>
    if (_class_space_list != NULL) {
      address base = (address)_class_space_list-&gt;current_virtual_space()-&gt;bottom();
<span class="udiff-line-modified-removed">-     st-&gt;print(&quot;Compressed class space size: &quot; SIZE_FORMAT &quot; Address: &quot; PTR_FORMAT,</span>
<span class="udiff-line-modified-removed">-                  compressed_class_space_size(), p2i(base));</span>
<span class="udiff-line-modified-removed">-     if (requested_addr != 0) {</span>
<span class="udiff-line-removed">-       st-&gt;print(&quot; Req Addr: &quot; PTR_FORMAT, p2i(requested_addr));</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-modified-added">+     address top = base + compressed_class_space_size();</span>
<span class="udiff-line-modified-added">+     st-&gt;print(&quot;Compressed class space mapped at: &quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;, size: &quot; SIZE_FORMAT,</span>
<span class="udiff-line-modified-added">+                p2i(base), p2i(top), top - base);</span>
      st-&gt;cr();
    }
  }
  
<span class="udiff-line-modified-removed">- // For UseCompressedClassPointers the class space is reserved above the top of</span>
<span class="udiff-line-removed">- // the Java heap.  The argument passed in is at the base of the compressed space.</span>
<span class="udiff-line-modified-added">+ // Given a prereserved space, use that to set up the compressed class space list.</span>
  void Metaspace::initialize_class_space(ReservedSpace rs) {
<span class="udiff-line-removed">-   // The reserved space size may be bigger because of alignment, esp with UseLargePages</span>
<span class="udiff-line-removed">-   assert(rs.size() &gt;= CompressedClassSpaceSize,</span>
<span class="udiff-line-removed">-          SIZE_FORMAT &quot; != &quot; SIZE_FORMAT, rs.size(), CompressedClassSpaceSize);</span>
    assert(using_class_space(), &quot;Must be using class space&quot;);
<span class="udiff-line-added">+   assert(_class_space_list == NULL &amp;&amp; _chunk_manager_class == NULL, &quot;Only call once&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   assert(rs.size() == CompressedClassSpaceSize, SIZE_FORMAT &quot; != &quot; SIZE_FORMAT,</span>
<span class="udiff-line-added">+          rs.size(), CompressedClassSpaceSize);</span>
<span class="udiff-line-added">+   assert(is_aligned(rs.base(), Metaspace::reserve_alignment()) &amp;&amp;</span>
<span class="udiff-line-added">+          is_aligned(rs.size(), Metaspace::reserve_alignment()),</span>
<span class="udiff-line-added">+          &quot;wrong alignment&quot;);</span>
<span class="udiff-line-added">+ </span>
    _class_space_list = new VirtualSpaceList(rs);
    _chunk_manager_class = new ChunkManager(true/*is_class*/);
  
<span class="udiff-line-added">+   // This does currently not work because rs may be the result of a split</span>
<span class="udiff-line-added">+   // operation and NMT seems not to be able to handle splits.</span>
<span class="udiff-line-added">+   // Will be fixed with JDK-8243535.</span>
<span class="udiff-line-added">+   // MemTracker::record_virtual_memory_type((address)rs.base(), mtClass);</span>
<span class="udiff-line-added">+ </span>
    if (!_class_space_list-&gt;initialization_succeeded()) {
      vm_exit_during_initialization(&quot;Failed to setup compressed class space virtual space list.&quot;);
    }
<span class="udiff-line-added">+ </span>
  }
  
<span class="udiff-line-modified-removed">- #endif // _LP64</span>
<span class="udiff-line-modified-added">+ // Reserve a range of memory at an address suitable for en/decoding narrow</span>
<span class="udiff-line-added">+ // Klass pointers (see: CompressedClassPointers::is_valid_base()).</span>
<span class="udiff-line-added">+ // The returned address shall both be suitable as a compressed class pointers</span>
<span class="udiff-line-added">+ //  base, and aligned to Metaspace::reserve_alignment (which is equal to or a</span>
<span class="udiff-line-added">+ //  multiple of allocation granularity).</span>
<span class="udiff-line-added">+ // On error, returns an unreserved space.</span>
<span class="udiff-line-added">+ ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size) {</span>
  
<span class="udiff-line-modified-removed">- #ifdef PREFERRED_METASPACE_ALIGNMENT</span>
<span class="udiff-line-modified-removed">- ReservedSpace Metaspace::reserve_preferred_space(size_t size, size_t alignment,</span>
<span class="udiff-line-removed">-                                                  bool large_pages, char *requested_addr,</span>
<span class="udiff-line-removed">-                                                  bool use_requested_addr) {</span>
<span class="udiff-line-removed">-   // Our compressed klass pointers may fit nicely into the lower 32 bits.</span>
<span class="udiff-line-removed">-   if (requested_addr != NULL &amp;&amp; (uint64_t)requested_addr + size &lt; 4*G) {</span>
<span class="udiff-line-removed">-     ReservedSpace rs(size, alignment, large_pages, requested_addr);</span>
<span class="udiff-line-removed">-     if (rs.is_reserved() || use_requested_addr) {</span>
<span class="udiff-line-removed">-       return rs;</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   struct SearchParams { uintptr_t limit; size_t increment; };</span>
<span class="udiff-line-modified-added">+ #ifdef AARCH64</span>
<span class="udiff-line-modified-added">+   const size_t alignment = Metaspace::reserve_alignment();</span>
  
    // AArch64: Try to align metaspace so that we can decode a compressed
    // klass with a single MOVK instruction. We can do this iff the
    // compressed class base is a multiple of 4G.
<span class="udiff-line-modified-removed">-   // Aix: Search for a place where we can find memory. If we need to load</span>
<span class="udiff-line-modified-removed">-   // the base, 4G alignment is helpful, too.</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   // Go faster above 32G as it is no longer possible to use a zero base.</span>
<span class="udiff-line-modified-removed">-   // AArch64: Additionally, ensure the lower LogKlassAlignmentInBytes</span>
<span class="udiff-line-modified-removed">-   // bits of the upper 32-bits of the address are zero so we can handle</span>
<span class="udiff-line-modified-removed">-   // a shift when decoding.</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   static const SearchParams search_params[] = {</span>
<span class="udiff-line-modified-removed">-     // Limit    Increment</span>
<span class="udiff-line-modified-removed">-     {  32*G,    AARCH64_ONLY(4*)G,                               },</span>
<span class="udiff-line-modified-removed">-     {  1024*G,  (4 AARCH64_ONLY(&lt;&lt; LogKlassAlignmentInBytes))*G  },</span>
<span class="udiff-line-modified-added">+   // Additionally, above 32G, ensure the lower LogKlassAlignmentInBytes bits</span>
<span class="udiff-line-modified-added">+   // of the upper 32-bits of the address are zero so we can handle a shift</span>
<span class="udiff-line-modified-added">+   // when decoding.</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+   static const struct {</span>
<span class="udiff-line-modified-added">+     address from;</span>
<span class="udiff-line-modified-added">+     address to;</span>
<span class="udiff-line-modified-added">+     size_t increment;</span>
<span class="udiff-line-modified-added">+   } search_ranges[] = {</span>
<span class="udiff-line-modified-added">+     {  (address)(4*G),   (address)(32*G),   4*G, },</span>
<span class="udiff-line-modified-added">+     {  (address)(32*G),  (address)(1024*G), (4 &lt;&lt; LogKlassAlignmentInBytes) * G },</span>
<span class="udiff-line-modified-added">+     {  NULL, NULL, 0 }</span>
    };
  
<span class="udiff-line-modified-removed">-   // Null requested_addr means allocate anywhere so ensure the search</span>
<span class="udiff-line-modified-removed">-   // begins from a non-null address.</span>
<span class="udiff-line-modified-removed">-   char *a = MAX2(requested_addr, (char *)search_params[0].increment);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   for (const SearchParams *p = search_params;</span>
<span class="udiff-line-modified-removed">-        p &lt; search_params + ARRAY_SIZE(search_params);</span>
<span class="udiff-line-modified-removed">-        ++p) {</span>
<span class="udiff-line-modified-removed">-     a = align_up(a, p-&gt;increment);</span>
<span class="udiff-line-removed">-     if (use_requested_addr &amp;&amp; a != requested_addr)</span>
<span class="udiff-line-removed">-       return ReservedSpace();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     for (; a &lt; (char *)p-&gt;limit; a += p-&gt;increment) {</span>
<span class="udiff-line-removed">-       ReservedSpace rs(size, alignment, large_pages, a);</span>
<span class="udiff-line-removed">-       if (rs.is_reserved() || use_requested_addr) {</span>
<span class="udiff-line-modified-added">+   for (int i = 0; search_ranges[i].from != NULL; i ++) {</span>
<span class="udiff-line-modified-added">+     address a = search_ranges[i].from;</span>
<span class="udiff-line-modified-added">+     assert(CompressedKlassPointers::is_valid_base(a), &quot;Sanity&quot;);</span>
<span class="udiff-line-modified-added">+     while (a &lt; search_ranges[i].to) {</span>
<span class="udiff-line-modified-added">+       ReservedSpace rs(size, Metaspace::reserve_alignment(),</span>
<span class="udiff-line-modified-added">+                        false /*large_pages*/, (char*)a);</span>
<span class="udiff-line-modified-added">+       if (rs.is_reserved()) {</span>
<span class="udiff-line-modified-added">+         assert(a == (address)rs.base(), &quot;Sanity&quot;);</span>
          return rs;
        }
<span class="udiff-line-added">+       a +=  search_ranges[i].increment;</span>
      }
    }
  
<span class="udiff-line-added">+   // Note: on AARCH64, if the code above does not find any good placement, we</span>
<span class="udiff-line-added">+   // have no recourse. We return an empty space and the VM will exit.</span>
    return ReservedSpace();
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- #endif // PREFERRED_METASPACE_ALIGNMENT</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- // Try to reserve a region for the metaspace at the requested address. Some</span>
<span class="udiff-line-removed">- // platforms have particular alignment requirements to allow efficient decode of</span>
<span class="udiff-line-removed">- // compressed class pointers in which case requested_addr is treated as hint for</span>
<span class="udiff-line-removed">- // where to start looking unless use_requested_addr is true.</span>
<span class="udiff-line-removed">- ReservedSpace Metaspace::reserve_space(size_t size, size_t alignment,</span>
<span class="udiff-line-removed">-                                        char* requested_addr, bool use_requested_addr) {</span>
<span class="udiff-line-removed">-   bool large_pages = false; // Don&#39;t use large pages for the class space.</span>
<span class="udiff-line-removed">-   assert(is_aligned(requested_addr, alignment), &quot;must be&quot;);</span>
<span class="udiff-line-removed">-   assert(requested_addr != NULL || !use_requested_addr,</span>
<span class="udiff-line-removed">-          &quot;cannot set use_requested_addr with NULL address&quot;);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- #ifdef PREFERRED_METASPACE_ALIGNMENT</span>
<span class="udiff-line-removed">-   return reserve_preferred_space(size, alignment, large_pages,</span>
<span class="udiff-line-removed">-                                  requested_addr, use_requested_addr);</span>
  #else
<span class="udiff-line-modified-removed">-   return ReservedSpace(size, alignment, large_pages, requested_addr);</span>
<span class="udiff-line-modified-removed">- #endif</span>
<span class="udiff-line-modified-added">+   // Default implementation: Just reserve anywhere.</span>
<span class="udiff-line-modified-added">+   return ReservedSpace(size, Metaspace::reserve_alignment(), false, (char*)NULL);</span>
<span class="udiff-line-added">+ #endif // AARCH64</span>
  }
  
<span class="udiff-line-added">+ #endif // _LP64</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ </span>
  void Metaspace::ergo_initialize() {
    if (DumpSharedSpaces) {
      // Using large pages when dumping the shared archive is currently not implemented.
      FLAG_SET_ERGO(UseLargePagesInMetaspace, false);
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1227,38 +1121,78 @@</span>
  }
  
  void Metaspace::global_initialize() {
    MetaspaceGC::initialize();
  
<span class="udiff-line-modified-removed">-   bool class_space_inited = false;</span>
<span class="udiff-line-modified-added">+   // If UseCompressedClassPointers=1, we have two cases:</span>
<span class="udiff-line-added">+   // a) if CDS is active (either dump time or runtime), it will create the ccs</span>
<span class="udiff-line-added">+   //    for us, initialize it and set up CompressedKlassPointers encoding.</span>
<span class="udiff-line-added">+   //    Class space will be reserved above the mapped archives.</span>
<span class="udiff-line-added">+   // b) if CDS is not active, we will create the ccs on our own. It will be</span>
<span class="udiff-line-added">+   //    placed above the java heap, since we assume it has been placed in low</span>
<span class="udiff-line-added">+   //    address regions. We may rethink this (see JDK-8244943). Failing that,</span>
<span class="udiff-line-added">+   //    it will be placed anywhere.</span>
<span class="udiff-line-added">+ </span>
  #if INCLUDE_CDS
<span class="udiff-line-added">+   // case (a)</span>
    if (DumpSharedSpaces) {
      MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();
<span class="udiff-line-removed">-     class_space_inited = true;</span>
    } else if (UseSharedSpaces) {
      // If any of the archived space fails to map, UseSharedSpaces
      // is reset to false.
      MetaspaceShared::initialize_runtime_shared_and_meta_spaces();
<span class="udiff-line-removed">-     class_space_inited = UseSharedSpaces;</span>
    }
  
    if (DynamicDumpSharedSpaces &amp;&amp; !UseSharedSpaces) {
      vm_exit_during_initialization(&quot;DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded&quot;, NULL);
    }
  #endif // INCLUDE_CDS
  
  #ifdef _LP64
<span class="udiff-line-modified-removed">-   if (using_class_space() &amp;&amp; !class_space_inited) {</span>
<span class="udiff-line-modified-removed">-     char* base;</span>
<span class="udiff-line-modified-removed">-     if (UseCompressedOops) {</span>
<span class="udiff-line-modified-removed">-       base = (char*)align_up(CompressedOops::end(), _reserve_alignment);</span>
<span class="udiff-line-modified-removed">-     } else {</span>
<span class="udiff-line-modified-removed">-       base = (char*)HeapBaseMinAddress;</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+   if (using_class_space() &amp;&amp; !class_space_is_initialized()) {</span>
<span class="udiff-line-modified-added">+     assert(!UseSharedSpaces &amp;&amp; !DumpSharedSpaces, &quot;CDS should be off at this point&quot;);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     // case (b)</span>
<span class="udiff-line-modified-added">+     ReservedSpace rs;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // If UseCompressedOops=1, java heap may have been placed in coops-friendly</span>
<span class="udiff-line-added">+     //  territory already (lower address regions), so we attempt to place ccs</span>
<span class="udiff-line-added">+     //  right above the java heap.</span>
<span class="udiff-line-added">+     // If UseCompressedOops=0, the heap has been placed anywhere - probably in</span>
<span class="udiff-line-added">+     //  high memory regions. In that case, try to place ccs at the lowest allowed</span>
<span class="udiff-line-added">+     //  mapping address.</span>
<span class="udiff-line-added">+     address base = UseCompressedOops ? CompressedOops::end() : (address)HeapBaseMinAddress;</span>
<span class="udiff-line-added">+     base = align_up(base, Metaspace::reserve_alignment());</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     const size_t size = align_up(CompressedClassSpaceSize, Metaspace::reserve_alignment());</span>
<span class="udiff-line-added">+     if (base != NULL) {</span>
<span class="udiff-line-added">+       if (CompressedKlassPointers::is_valid_base(base)) {</span>
<span class="udiff-line-added">+         rs = ReservedSpace(size, Metaspace::reserve_alignment(),</span>
<span class="udiff-line-added">+                            false /* large */, (char*)base);</span>
<span class="udiff-line-added">+       }</span>
      }
<span class="udiff-line-modified-removed">-     ReservedSpace dummy;</span>
<span class="udiff-line-modified-removed">-     allocate_metaspace_compressed_klass_ptrs(dummy, base, 0);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     // ...failing that, reserve anywhere, but let platform do optimized placement:</span>
<span class="udiff-line-added">+     if (!rs.is_reserved()) {</span>
<span class="udiff-line-added">+       rs = Metaspace::reserve_address_space_for_compressed_classes(size);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // ...failing that, give up.</span>
<span class="udiff-line-added">+     if (!rs.is_reserved()) {</span>
<span class="udiff-line-added">+       vm_exit_during_initialization(</span>
<span class="udiff-line-added">+           err_msg(&quot;Could not allocate compressed class space: &quot; SIZE_FORMAT &quot; bytes&quot;,</span>
<span class="udiff-line-added">+                    compressed_class_space_size()));</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Initialize space</span>
<span class="udiff-line-added">+     Metaspace::initialize_class_space(rs);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Set up compressed class pointer encoding.</span>
<span class="udiff-line-added">+     CompressedKlassPointers::initialize((address)rs.base(), rs.size());</span>
    }
<span class="udiff-line-added">+ </span>
  #endif
  
    // Initialize these before initializing the VirtualSpaceList
    _first_chunk_word_size = InitialBootClassLoaderMetaspaceSize / BytesPerWord;
    _first_chunk_word_size = align_word_size_up(_first_chunk_word_size);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1283,10 +1217,24 @@</span>
  
    _tracer = new MetaspaceTracer();
  
    _initialized = true;
  
<span class="udiff-line-added">+ #ifdef _LP64</span>
<span class="udiff-line-added">+   if (UseCompressedClassPointers) {</span>
<span class="udiff-line-added">+     // Note: &quot;cds&quot; would be a better fit but keep this for backward compatibility.</span>
<span class="udiff-line-added">+     LogTarget(Info, gc, metaspace) lt;</span>
<span class="udiff-line-added">+     if (lt.is_enabled()) {</span>
<span class="udiff-line-added">+       ResourceMark rm;</span>
<span class="udiff-line-added">+       LogStream ls(lt);</span>
<span class="udiff-line-added">+       CDS_ONLY(MetaspaceShared::print_on(&amp;ls);)</span>
<span class="udiff-line-added">+       Metaspace::print_compressed_class_space(&amp;ls);</span>
<span class="udiff-line-added">+       CompressedKlassPointers::print_mode(&amp;ls);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ #endif</span>
<span class="udiff-line-added">+ </span>
  }
  
  void Metaspace::post_initialize() {
    MetaspaceGC::post_initialize();
  }
</pre>
<center><a href="filemap.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspace.hpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>