<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/ppc/templateTable_ppc_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="templateInterpreterGenerator_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_ppc.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/ppc/templateTable_ppc_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
2501   if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2502     __ sldi(Rscratch, Rscratch, exact_log2(BytesPerInstWord)); // Volatile ? size of 1 instruction : 0.
2503   }
2504   __ ldx(Rbtable, Rbtable, Rflags);
2505 
2506   // Get the obj from stack.
2507   if (!is_static) {
2508     pop_and_check_object(Rclass_or_obj); // Kills R11_scratch1.
2509   } else {
2510     __ verify_oop(Rclass_or_obj);
2511   }
2512 
2513   if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2514     __ subf(Rbtable, Rscratch, Rbtable); // Point to volatile/non-volatile entry point.
2515   }
2516   __ mtctr(Rbtable);
2517   __ bctr();
2518 
2519 #ifdef ASSERT
2520   __ bind(LFlagInvalid);
<span class="line-modified">2521   __ stop(&quot;got invalid flag&quot;, 0x654);</span>
2522 #endif
2523 
2524   if (!is_static &amp;&amp; rc == may_not_rewrite) {
2525     // We reuse the code from is_static.  It&#39;s jumped to via the table above.
2526     return;
2527   }
2528 
2529 #ifdef ASSERT
2530   // __ bind(Lvtos);
2531   address pc_before_fence = __ pc();
2532   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2533   assert(__ pc() - pc_before_fence == (ptrdiff_t)BytesPerInstWord, &quot;must be single instruction&quot;);
2534   assert(branch_table[vtos] == 0, &quot;can&#39;t compute twice&quot;);
2535   branch_table[vtos] = __ pc(); // non-volatile_entry point
<span class="line-modified">2536   __ stop(&quot;vtos unexpected&quot;, 0x655);</span>
2537 #endif
2538 
2539   __ align(32, 28, 28); // Align load.
2540   // __ bind(Ldtos);
2541   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2542   assert(branch_table[dtos] == 0, &quot;can&#39;t compute twice&quot;);
2543   branch_table[dtos] = __ pc(); // non-volatile_entry point
2544   __ lfdx(F15_ftos, Rclass_or_obj, Roffset);
2545   __ push(dtos);
2546   if (!is_static &amp;&amp; rc == may_rewrite) {
2547     patch_bytecode(Bytecodes::_fast_dgetfield, Rbc, Rscratch);
2548   }
2549   {
2550     Label acquire_double;
2551     __ beq(CCR6, acquire_double); // Volatile?
2552     __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2553 
2554     __ bind(acquire_double);
2555     __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
2556     __ beq_predict_taken(CCR0, Lisync);
</pre>
<hr />
<pre>
2830 #ifdef ASSERT
2831   Label LFlagInvalid;
2832   __ cmpldi(CCR0, Rflags, number_of_states);
2833   __ bge(CCR0, LFlagInvalid);
2834 #endif
2835 
2836   // Load from branch table and dispatch (volatile case: one instruction ahead).
2837   __ sldi(Rflags, Rflags, LogBytesPerWord);
2838   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2839     __ cmpwi(CR_is_vol, Rscratch, 1);  // Volatile?
2840   }
2841   __ sldi(Rscratch, Rscratch, exact_log2(BytesPerInstWord)); // Volatile? size of instruction 1 : 0.
2842   __ ldx(Rbtable, Rbtable, Rflags);
2843 
2844   __ subf(Rbtable, Rscratch, Rbtable); // Point to volatile/non-volatile entry point.
2845   __ mtctr(Rbtable);
2846   __ bctr();
2847 
2848 #ifdef ASSERT
2849   __ bind(LFlagInvalid);
<span class="line-modified">2850   __ stop(&quot;got invalid flag&quot;, 0x656);</span>
2851 
2852   // __ bind(Lvtos);
2853   address pc_before_release = __ pc();
2854   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2855   assert(__ pc() - pc_before_release == (ptrdiff_t)BytesPerInstWord, &quot;must be single instruction&quot;);
2856   assert(branch_table[vtos] == 0, &quot;can&#39;t compute twice&quot;);
2857   branch_table[vtos] = __ pc(); // non-volatile_entry point
<span class="line-modified">2858   __ stop(&quot;vtos unexpected&quot;, 0x657);</span>
2859 #endif
2860 
2861   __ align(32, 28, 28); // Align pop.
2862   // __ bind(Ldtos);
2863   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2864   assert(branch_table[dtos] == 0, &quot;can&#39;t compute twice&quot;);
2865   branch_table[dtos] = __ pc(); // non-volatile_entry point
2866   __ pop(dtos);
2867   if (!is_static) {
2868     pop_and_check_object(Rclass_or_obj);  // Kills R11_scratch1.
2869   }
2870   __ stfdx(F15_ftos, Rclass_or_obj, Roffset);
2871   if (!is_static &amp;&amp; rc == may_rewrite) {
2872     patch_bytecode(Bytecodes::_fast_dputfield, Rbc, Rscratch, true, byte_no);
2873   }
2874   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2875     __ beq(CR_is_vol, Lvolatile); // Volatile?
2876   }
2877   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2878 
</pre>
</td>
<td>
<hr />
<pre>
2501   if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2502     __ sldi(Rscratch, Rscratch, exact_log2(BytesPerInstWord)); // Volatile ? size of 1 instruction : 0.
2503   }
2504   __ ldx(Rbtable, Rbtable, Rflags);
2505 
2506   // Get the obj from stack.
2507   if (!is_static) {
2508     pop_and_check_object(Rclass_or_obj); // Kills R11_scratch1.
2509   } else {
2510     __ verify_oop(Rclass_or_obj);
2511   }
2512 
2513   if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2514     __ subf(Rbtable, Rscratch, Rbtable); // Point to volatile/non-volatile entry point.
2515   }
2516   __ mtctr(Rbtable);
2517   __ bctr();
2518 
2519 #ifdef ASSERT
2520   __ bind(LFlagInvalid);
<span class="line-modified">2521   __ stop(&quot;got invalid flag&quot;);</span>
2522 #endif
2523 
2524   if (!is_static &amp;&amp; rc == may_not_rewrite) {
2525     // We reuse the code from is_static.  It&#39;s jumped to via the table above.
2526     return;
2527   }
2528 
2529 #ifdef ASSERT
2530   // __ bind(Lvtos);
2531   address pc_before_fence = __ pc();
2532   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2533   assert(__ pc() - pc_before_fence == (ptrdiff_t)BytesPerInstWord, &quot;must be single instruction&quot;);
2534   assert(branch_table[vtos] == 0, &quot;can&#39;t compute twice&quot;);
2535   branch_table[vtos] = __ pc(); // non-volatile_entry point
<span class="line-modified">2536   __ stop(&quot;vtos unexpected&quot;);</span>
2537 #endif
2538 
2539   __ align(32, 28, 28); // Align load.
2540   // __ bind(Ldtos);
2541   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2542   assert(branch_table[dtos] == 0, &quot;can&#39;t compute twice&quot;);
2543   branch_table[dtos] = __ pc(); // non-volatile_entry point
2544   __ lfdx(F15_ftos, Rclass_or_obj, Roffset);
2545   __ push(dtos);
2546   if (!is_static &amp;&amp; rc == may_rewrite) {
2547     patch_bytecode(Bytecodes::_fast_dgetfield, Rbc, Rscratch);
2548   }
2549   {
2550     Label acquire_double;
2551     __ beq(CCR6, acquire_double); // Volatile?
2552     __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2553 
2554     __ bind(acquire_double);
2555     __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
2556     __ beq_predict_taken(CCR0, Lisync);
</pre>
<hr />
<pre>
2830 #ifdef ASSERT
2831   Label LFlagInvalid;
2832   __ cmpldi(CCR0, Rflags, number_of_states);
2833   __ bge(CCR0, LFlagInvalid);
2834 #endif
2835 
2836   // Load from branch table and dispatch (volatile case: one instruction ahead).
2837   __ sldi(Rflags, Rflags, LogBytesPerWord);
2838   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2839     __ cmpwi(CR_is_vol, Rscratch, 1);  // Volatile?
2840   }
2841   __ sldi(Rscratch, Rscratch, exact_log2(BytesPerInstWord)); // Volatile? size of instruction 1 : 0.
2842   __ ldx(Rbtable, Rbtable, Rflags);
2843 
2844   __ subf(Rbtable, Rscratch, Rbtable); // Point to volatile/non-volatile entry point.
2845   __ mtctr(Rbtable);
2846   __ bctr();
2847 
2848 #ifdef ASSERT
2849   __ bind(LFlagInvalid);
<span class="line-modified">2850   __ stop(&quot;got invalid flag&quot;);</span>
2851 
2852   // __ bind(Lvtos);
2853   address pc_before_release = __ pc();
2854   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2855   assert(__ pc() - pc_before_release == (ptrdiff_t)BytesPerInstWord, &quot;must be single instruction&quot;);
2856   assert(branch_table[vtos] == 0, &quot;can&#39;t compute twice&quot;);
2857   branch_table[vtos] = __ pc(); // non-volatile_entry point
<span class="line-modified">2858   __ stop(&quot;vtos unexpected&quot;);</span>
2859 #endif
2860 
2861   __ align(32, 28, 28); // Align pop.
2862   // __ bind(Ldtos);
2863   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2864   assert(branch_table[dtos] == 0, &quot;can&#39;t compute twice&quot;);
2865   branch_table[dtos] = __ pc(); // non-volatile_entry point
2866   __ pop(dtos);
2867   if (!is_static) {
2868     pop_and_check_object(Rclass_or_obj);  // Kills R11_scratch1.
2869   }
2870   __ stfdx(F15_ftos, Rclass_or_obj, Roffset);
2871   if (!is_static &amp;&amp; rc == may_rewrite) {
2872     patch_bytecode(Bytecodes::_fast_dputfield, Rbc, Rscratch, true, byte_no);
2873   }
2874   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2875     __ beq(CR_is_vol, Lvolatile); // Volatile?
2876   }
2877   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2878 
</pre>
</td>
</tr>
</table>
<center><a href="templateInterpreterGenerator_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_ppc.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>