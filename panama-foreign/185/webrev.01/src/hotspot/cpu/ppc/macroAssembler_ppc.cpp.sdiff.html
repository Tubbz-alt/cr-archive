<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/ppc/macroAssembler_ppc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="interp_masm_ppc_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/ppc/macroAssembler_ppc.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 885   assert(tmp != R1_SP, &quot;must be distinct&quot;);
 886   ld(tmp, _abi(lr), R1_SP);
 887   mtlr(tmp);
 888   ld(tmp, _abi(cr), R1_SP);
 889   mtcr(tmp);
 890 }
 891 
 892 address MacroAssembler::get_PC_trash_LR(Register result) {
 893   Label L;
 894   bl(L);
 895   bind(L);
 896   address lr_pc = pc();
 897   mflr(result);
 898   return lr_pc;
 899 }
 900 
 901 void MacroAssembler::resize_frame(Register offset, Register tmp) {
 902 #ifdef ASSERT
 903   assert_different_registers(offset, tmp, R1_SP);
 904   andi_(tmp, offset, frame::alignment_in_bytes-1);
<span class="line-modified"> 905   asm_assert_eq(&quot;resize_frame: unaligned&quot;, 0x204);</span>
 906 #endif
 907 
 908   // tmp &lt;- *(SP)
 909   ld(tmp, _abi(callers_sp), R1_SP);
 910   // addr &lt;- SP + offset;
 911   // *(addr) &lt;- tmp;
 912   // SP &lt;- addr
 913   stdux(tmp, R1_SP, offset);
 914 }
 915 
 916 void MacroAssembler::resize_frame(int offset, Register tmp) {
 917   assert(is_simm(offset, 16), &quot;too big an offset&quot;);
 918   assert_different_registers(tmp, R1_SP);
 919   assert((offset &amp; (frame::alignment_in_bytes-1))==0, &quot;resize_frame: unaligned&quot;);
 920   // tmp &lt;- *(SP)
 921   ld(tmp, _abi(callers_sp), R1_SP);
 922   // addr &lt;- SP + offset;
 923   // *(addr) &lt;- tmp;
 924   // SP &lt;- addr
 925   stdu(tmp, offset, R1_SP);
 926 }
 927 
 928 void MacroAssembler::resize_frame_absolute(Register addr, Register tmp1, Register tmp2) {
 929   // (addr == tmp1) || (addr == tmp2) is allowed here!
 930   assert(tmp1 != tmp2, &quot;must be distinct&quot;);
 931 
 932   // compute offset w.r.t. current stack pointer
 933   // tmp_1 &lt;- addr - SP (!)
 934   subf(tmp1, R1_SP, addr);
 935 
 936   // atomically update SP keeping back link.
 937   resize_frame(tmp1/* offset */, tmp2/* tmp */);
 938 }
 939 
 940 void MacroAssembler::push_frame(Register bytes, Register tmp) {
 941 #ifdef ASSERT
 942   assert(bytes != R0, &quot;r0 not allowed here&quot;);
 943   andi_(R0, bytes, frame::alignment_in_bytes-1);
<span class="line-modified"> 944   asm_assert_eq(&quot;push_frame(Reg, Reg): unaligned&quot;, 0x203);</span>
 945 #endif
 946   neg(tmp, bytes);
 947   stdux(R1_SP, R1_SP, tmp);
 948 }
 949 
 950 // Push a frame of size `bytes&#39;.
 951 void MacroAssembler::push_frame(unsigned int bytes, Register tmp) {
 952   long offset = align_addr(bytes, frame::alignment_in_bytes);
 953   if (is_simm(-offset, 16)) {
 954     stdu(R1_SP, -offset, R1_SP);
 955   } else {
 956     load_const_optimized(tmp, -offset);
 957     stdux(R1_SP, R1_SP, tmp);
 958   }
 959 }
 960 
 961 // Push a frame of size `bytes&#39; plus abi_reg_args on top.
 962 void MacroAssembler::push_frame_reg_args(unsigned int bytes, Register tmp) {
 963   push_frame(bytes + frame::abi_reg_args_size, tmp);
 964 }
</pre>
<hr />
<pre>
2296 
2297   const Register new_top = t1;
2298   //verify_tlab(); not implemented
2299 
2300   ld(obj, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
2301   ld(R0, in_bytes(JavaThread::tlab_end_offset()), R16_thread);
2302   if (var_size_in_bytes == noreg) {
2303     addi(new_top, obj, con_size_in_bytes);
2304   } else {
2305     add(new_top, obj, var_size_in_bytes);
2306   }
2307   cmpld(CCR0, new_top, R0);
2308   bc_far_optimized(Assembler::bcondCRbiIs1, bi0(CCR0, Assembler::greater), slow_case);
2309 
2310 #ifdef ASSERT
2311   // make sure new free pointer is properly aligned
2312   {
2313     Label L;
2314     andi_(R0, new_top, MinObjAlignmentInBytesMask);
2315     beq(CCR0, L);
<span class="line-modified">2316     stop(&quot;updated TLAB free is not properly aligned&quot;, 0x934);</span>
2317     bind(L);
2318   }
2319 #endif // ASSERT
2320 
2321   // update the tlab top pointer
2322   std(new_top, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
2323   //verify_tlab(); not implemented
2324 }
2325 void MacroAssembler::incr_allocated_bytes(RegisterOrConstant size_in_bytes, Register t1, Register t2) {
2326   unimplemented(&quot;incr_allocated_bytes&quot;);
2327 }
2328 
2329 address MacroAssembler::emit_trampoline_stub(int destination_toc_offset,
2330                                              int insts_call_instruction_offset, Register Rtoc) {
2331   // Start the stub.
2332   address stub = start_a_stub(64);
2333   if (stub == NULL) { return NULL; } // CodeCache full: bail out
2334 
2335   // Create a trampoline stub relocation which relates this trampoline stub
2336   // with the call instruction at insts_call_instruction_offset in the
</pre>
<hr />
<pre>
2775   // We don&#39;t reload mark word. Will only be reset at safepoint.
2776   ld(R0, 0, owner_addr_Reg); // Load in transaction, conflicts need to be tracked.
2777   cmpdi(flag, R0, 0);
2778   beq(flag, DONE_LABEL);
2779 
2780   if (UseRTMXendForLockBusy) {
2781     tend_();
2782     b(L_decrement_retry);
2783   } else {
2784     tabort_();
2785   }
2786   bind(L_on_abort);
2787   const Register abort_status_Reg = tmpReg;
2788   mftexasr(abort_status_Reg);
2789   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2790     rtm_profiling(abort_status_Reg, /*temp*/ owner_addr_Reg, rtm_counters, method_data, profile_rtm);
2791     // Restore owner_addr_Reg
2792     ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);
2793 #ifdef ASSERT
2794     andi_(R0, mark_word, markWord::monitor_value);
<span class="line-modified">2795     asm_assert_ne(&quot;must be inflated&quot;, 0xa754); // Deflating only allowed at safepoint.</span>
2796 #endif
2797     addi(owner_addr_Reg, mark_word, owner_offset);
2798   }
2799   if (RTMRetryCount &gt; 0) {
2800     // Retry on lock abort if abort status is not permanent.
2801     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
2802   }
2803 
2804   // Appears unlocked - try to swing _owner from null to non-null.
2805   cmpxchgd(flag, /*current val*/ R0, (intptr_t)0, /*new val*/ R16_thread, owner_addr_Reg,
2806            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2807            MacroAssembler::cmpxchgx_hint_acquire_lock(), noreg, &amp;L_decrement_retry, true);
2808 
2809   if (RTMRetryCount &gt; 0) {
2810     // success done else retry
2811     b(DONE_LABEL);
2812     bind(L_decrement_retry);
2813     // Spin and retry if lock is busy.
2814     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, owner_addr_Reg, L_rtm_retry);
2815   } else {
</pre>
<hr />
<pre>
2912   // Try to CAS m-&gt;owner from NULL to current thread.
2913   addi(temp, displaced_header, ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value);
2914   cmpxchgd(/*flag=*/flag,
2915            /*current_value=*/current_header,
2916            /*compare_value=*/(intptr_t)0,
2917            /*exchange_value=*/R16_thread,
2918            /*where=*/temp,
2919            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2920            MacroAssembler::cmpxchgx_hint_acquire_lock());
2921 
2922   // Store a non-null value into the box.
2923   std(box, BasicLock::displaced_header_offset_in_bytes(), box);
2924 
2925 # ifdef ASSERT
2926   bne(flag, cont);
2927   // We have acquired the monitor, check some invariants.
2928   addi(/*monitor=*/temp, temp, -ObjectMonitor::owner_offset_in_bytes());
2929   // Invariant 1: _recursions should be 0.
2930   //assert(ObjectMonitor::recursions_size_in_bytes() == 8, &quot;unexpected size&quot;);
2931   asm_assert_mem8_is_zero(ObjectMonitor::recursions_offset_in_bytes(), temp,
<span class="line-modified">2932                             &quot;monitor-&gt;_recursions should be 0&quot;, -1);</span>
2933 # endif
2934 
2935 #if INCLUDE_RTM_OPT
2936   } // use_rtm()
2937 #endif
2938 
2939   bind(cont);
2940   // flag == EQ indicates success
2941   // flag == NE indicates failure
2942 }
2943 
2944 void MacroAssembler::compiler_fast_unlock_object(ConditionRegister flag, Register oop, Register box,
2945                                                  Register temp, Register displaced_header, Register current_header,
2946                                                  bool try_bias, bool use_rtm) {
2947   assert_different_registers(oop, box, temp, displaced_header, current_header);
2948   assert(flag != CCR0, &quot;bad condition register&quot;);
2949   Label cont;
2950   Label object_has_monitor;
2951 
2952   if (try_bias) {
</pre>
<hr />
<pre>
3041   // Armed page has poll_bit set.
3042   andi_(temp_reg, temp_reg, SafepointMechanism::poll_bit());
3043   bne(CCR0, slow_path);
3044 }
3045 
3046 void MacroAssembler::resolve_jobject(Register value, Register tmp1, Register tmp2, bool needs_frame) {
3047   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3048   bs-&gt;resolve_jobject(this, value, tmp1, tmp2, needs_frame);
3049 }
3050 
3051 // Values for last_Java_pc, and last_Java_sp must comply to the rules
3052 // in frame_ppc.hpp.
3053 void MacroAssembler::set_last_Java_frame(Register last_Java_sp, Register last_Java_pc) {
3054   // Always set last_Java_pc and flags first because once last_Java_sp
3055   // is visible has_last_Java_frame is true and users will look at the
3056   // rest of the fields. (Note: flags should always be zero before we
3057   // get here so doesn&#39;t need to be set.)
3058 
3059   // Verify that last_Java_pc was zeroed on return to Java
3060   asm_assert_mem8_is_zero(in_bytes(JavaThread::last_Java_pc_offset()), R16_thread,
<span class="line-modified">3061                           &quot;last_Java_pc not zeroed before leaving Java&quot;, 0x200);</span>
3062 
3063   // When returning from calling out from Java mode the frame anchor&#39;s
3064   // last_Java_pc will always be set to NULL. It is set here so that
3065   // if we are doing a call to native (not VM) that we capture the
3066   // known pc and don&#39;t have to rely on the native call having a
3067   // standard frame linkage where we can find the pc.
3068   if (last_Java_pc != noreg)
3069     std(last_Java_pc, in_bytes(JavaThread::last_Java_pc_offset()), R16_thread);
3070 
3071   // Set last_Java_sp last.
3072   std(last_Java_sp, in_bytes(JavaThread::last_Java_sp_offset()), R16_thread);
3073 }
3074 
3075 void MacroAssembler::reset_last_Java_frame(void) {
3076   asm_assert_mem8_isnot_zero(in_bytes(JavaThread::last_Java_sp_offset()),
<span class="line-modified">3077                              R16_thread, &quot;SP was not set, still zero&quot;, 0x202);</span>
3078 
3079   BLOCK_COMMENT(&quot;reset_last_Java_frame {&quot;);
3080   li(R0, 0);
3081 
3082   // _last_Java_sp = 0
3083   std(R0, in_bytes(JavaThread::last_Java_sp_offset()), R16_thread);
3084 
3085   // _last_Java_pc = 0
3086   std(R0, in_bytes(JavaThread::last_Java_pc_offset()), R16_thread);
3087   BLOCK_COMMENT(&quot;} reset_last_Java_frame&quot;);
3088 }
3089 
3090 void MacroAssembler::set_top_ijava_frame_at_SP_as_last_Java_frame(Register sp, Register tmp1) {
3091   assert_different_registers(sp, tmp1);
3092 
3093   // sp points to a TOP_IJAVA_FRAME, retrieve frame&#39;s PC via
3094   // TOP_IJAVA_FRAME_ABI.
3095   // FIXME: assert that we really have a TOP_IJAVA_FRAME here!
3096   address entry = pc();
3097   load_const_optimized(tmp1, entry);
</pre>
<hr />
<pre>
4310   addi(tmp3, xlen, 1);
4311   sldi(tmp, tmp3, LogBytesPerInt);
4312   stwx(carry, z, tmp);
4313   addic_(tmp3, tmp3, -1);
4314   blt(CCR0, L_done);
4315 
4316   srdi(carry, carry, 32);
4317   sldi(tmp, tmp3, LogBytesPerInt);
4318   stwx(carry, z, tmp);
4319   b(L_second_loop);
4320 
4321   // Next infrequent code is moved outside loops.
4322   bind(L_last_x);
4323 
4324   lwz(x_xstart, 0, x);
4325   b(L_third_loop_prologue);
4326 
4327   bind(L_done);
4328 }   // multiply_to_len
4329 
<span class="line-modified">4330 void MacroAssembler::asm_assert(bool check_equal, const char *msg, int id) {</span>
4331 #ifdef ASSERT
4332   Label ok;
4333   if (check_equal) {
4334     beq(CCR0, ok);
4335   } else {
4336     bne(CCR0, ok);
4337   }
<span class="line-modified">4338   stop(msg, id);</span>
4339   bind(ok);
4340 #endif
4341 }
4342 
4343 void MacroAssembler::asm_assert_mems_zero(bool check_equal, int size, int mem_offset,
<span class="line-modified">4344                                           Register mem_base, const char* msg, int id) {</span>
4345 #ifdef ASSERT
4346   switch (size) {
4347     case 4:
4348       lwz(R0, mem_offset, mem_base);
4349       cmpwi(CCR0, R0, 0);
4350       break;
4351     case 8:
4352       ld(R0, mem_offset, mem_base);
4353       cmpdi(CCR0, R0, 0);
4354       break;
4355     default:
4356       ShouldNotReachHere();
4357   }
<span class="line-modified">4358   asm_assert(check_equal, msg, id);</span>
4359 #endif // ASSERT
4360 }
4361 
4362 void MacroAssembler::verify_thread() {
4363   if (VerifyThread) {
4364     unimplemented(&quot;&#39;VerifyThread&#39; currently not implemented on PPC&quot;);
4365   }
4366 }
4367 
4368 void MacroAssembler::verify_coop(Register coop, const char* msg) {
4369   if (!VerifyOops) { return; }
4370   if (UseCompressedOops) { decode_heap_oop(coop); }
4371   verify_oop(coop, msg);
4372   if (UseCompressedOops) { encode_heap_oop(coop, coop); }
4373 }
4374 
4375 // READ: oop. KILL: R0. Volatile floats perhaps.
4376 void MacroAssembler::verify_oop(Register oop, const char* msg) {
4377   if (!VerifyOops) {
4378     return;
</pre>
<hr />
<pre>
4413   const Register tmp = R11; // Will be preserved.
4414   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
4415   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4416 
4417   ld(R4_ARG2, offs, base);
4418   save_LR_CR(tmp); // save in old frame
4419   push_frame_reg_args(nbytes_save, tmp);
4420   // load FunctionDescriptor** / entry_address *
4421   load_const_optimized(tmp, fd, R0);
4422   // load FunctionDescriptor* / entry_address
4423   ld(tmp, 0, tmp);
4424   load_const_optimized(R3_ARG1, (address)msg, R0);
4425   // Call destination for its side effect.
4426   call_c(tmp);
4427 
4428   pop_frame();
4429   restore_LR_CR(tmp);
4430   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
4431 }
4432 
<span class="line-removed">4433 const char* stop_types[] = {</span>
<span class="line-removed">4434   &quot;stop&quot;,</span>
<span class="line-removed">4435   &quot;untested&quot;,</span>
<span class="line-removed">4436   &quot;unimplemented&quot;,</span>
<span class="line-removed">4437   &quot;shouldnotreachhere&quot;</span>
<span class="line-removed">4438 };</span>
<span class="line-removed">4439 </span>
<span class="line-removed">4440 static void stop_on_request(int tp, const char* msg) {</span>
<span class="line-removed">4441   tty-&gt;print(&quot;PPC assembly code requires stop: (%s) %s\n&quot;, stop_types[tp%/*stop_end*/4], msg);</span>
<span class="line-removed">4442   guarantee(false, &quot;PPC assembly code requires stop: %s&quot;, msg);</span>
<span class="line-removed">4443 }</span>
<span class="line-removed">4444 </span>
4445 // Call a C-function that prints output.
<span class="line-modified">4446 void MacroAssembler::stop(int type, const char* msg, int id) {</span>
4447 #ifndef PRODUCT
<span class="line-modified">4448   block_comment(err_msg(&quot;stop: %s %s {&quot;, stop_types[type%stop_end], msg));</span>
4449 #else
4450   block_comment(&quot;stop {&quot;);
4451 #endif
4452 
<span class="line-modified">4453   // setup arguments</span>
<span class="line-modified">4454   load_const_optimized(R3_ARG1, type);</span>
<span class="line-modified">4455   load_const_optimized(R4_ARG2, (void *)msg, /*tmp=*/R0);</span>
<span class="line-modified">4456   call_VM_leaf(CAST_FROM_FN_PTR(address, stop_on_request), R3_ARG1, R4_ARG2);</span>
<span class="line-modified">4457   illtrap();</span>
<span class="line-modified">4458   emit_int32(id);</span>
4459   block_comment(&quot;} stop;&quot;);
4460 }
4461 
4462 #ifndef PRODUCT
4463 // Write pattern 0x0101010101010101 in memory region [low-before, high+after].
4464 // Val, addr are temp registers.
4465 // If low == addr, addr is killed.
4466 // High is preserved.
4467 void MacroAssembler::zap_from_to(Register low, int before, Register high, int after, Register val, Register addr) {
4468   if (!ZapMemory) return;
4469 
4470   assert_different_registers(low, val);
4471 
4472   BLOCK_COMMENT(&quot;zap memory region {&quot;);
4473   load_const_optimized(val, 0x0101010101010101);
4474   int size = before + after;
4475   if (low == high &amp;&amp; size &lt; 5 &amp;&amp; size &gt; 0) {
4476     int offset = -before*BytesPerWord;
4477     for (int i = 0; i &lt; size; ++i) {
4478       std(val, offset, low);
</pre>
</td>
<td>
<hr />
<pre>
 885   assert(tmp != R1_SP, &quot;must be distinct&quot;);
 886   ld(tmp, _abi(lr), R1_SP);
 887   mtlr(tmp);
 888   ld(tmp, _abi(cr), R1_SP);
 889   mtcr(tmp);
 890 }
 891 
 892 address MacroAssembler::get_PC_trash_LR(Register result) {
 893   Label L;
 894   bl(L);
 895   bind(L);
 896   address lr_pc = pc();
 897   mflr(result);
 898   return lr_pc;
 899 }
 900 
 901 void MacroAssembler::resize_frame(Register offset, Register tmp) {
 902 #ifdef ASSERT
 903   assert_different_registers(offset, tmp, R1_SP);
 904   andi_(tmp, offset, frame::alignment_in_bytes-1);
<span class="line-modified"> 905   asm_assert_eq(&quot;resize_frame: unaligned&quot;);</span>
 906 #endif
 907 
 908   // tmp &lt;- *(SP)
 909   ld(tmp, _abi(callers_sp), R1_SP);
 910   // addr &lt;- SP + offset;
 911   // *(addr) &lt;- tmp;
 912   // SP &lt;- addr
 913   stdux(tmp, R1_SP, offset);
 914 }
 915 
 916 void MacroAssembler::resize_frame(int offset, Register tmp) {
 917   assert(is_simm(offset, 16), &quot;too big an offset&quot;);
 918   assert_different_registers(tmp, R1_SP);
 919   assert((offset &amp; (frame::alignment_in_bytes-1))==0, &quot;resize_frame: unaligned&quot;);
 920   // tmp &lt;- *(SP)
 921   ld(tmp, _abi(callers_sp), R1_SP);
 922   // addr &lt;- SP + offset;
 923   // *(addr) &lt;- tmp;
 924   // SP &lt;- addr
 925   stdu(tmp, offset, R1_SP);
 926 }
 927 
 928 void MacroAssembler::resize_frame_absolute(Register addr, Register tmp1, Register tmp2) {
 929   // (addr == tmp1) || (addr == tmp2) is allowed here!
 930   assert(tmp1 != tmp2, &quot;must be distinct&quot;);
 931 
 932   // compute offset w.r.t. current stack pointer
 933   // tmp_1 &lt;- addr - SP (!)
 934   subf(tmp1, R1_SP, addr);
 935 
 936   // atomically update SP keeping back link.
 937   resize_frame(tmp1/* offset */, tmp2/* tmp */);
 938 }
 939 
 940 void MacroAssembler::push_frame(Register bytes, Register tmp) {
 941 #ifdef ASSERT
 942   assert(bytes != R0, &quot;r0 not allowed here&quot;);
 943   andi_(R0, bytes, frame::alignment_in_bytes-1);
<span class="line-modified"> 944   asm_assert_eq(&quot;push_frame(Reg, Reg): unaligned&quot;);</span>
 945 #endif
 946   neg(tmp, bytes);
 947   stdux(R1_SP, R1_SP, tmp);
 948 }
 949 
 950 // Push a frame of size `bytes&#39;.
 951 void MacroAssembler::push_frame(unsigned int bytes, Register tmp) {
 952   long offset = align_addr(bytes, frame::alignment_in_bytes);
 953   if (is_simm(-offset, 16)) {
 954     stdu(R1_SP, -offset, R1_SP);
 955   } else {
 956     load_const_optimized(tmp, -offset);
 957     stdux(R1_SP, R1_SP, tmp);
 958   }
 959 }
 960 
 961 // Push a frame of size `bytes&#39; plus abi_reg_args on top.
 962 void MacroAssembler::push_frame_reg_args(unsigned int bytes, Register tmp) {
 963   push_frame(bytes + frame::abi_reg_args_size, tmp);
 964 }
</pre>
<hr />
<pre>
2296 
2297   const Register new_top = t1;
2298   //verify_tlab(); not implemented
2299 
2300   ld(obj, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
2301   ld(R0, in_bytes(JavaThread::tlab_end_offset()), R16_thread);
2302   if (var_size_in_bytes == noreg) {
2303     addi(new_top, obj, con_size_in_bytes);
2304   } else {
2305     add(new_top, obj, var_size_in_bytes);
2306   }
2307   cmpld(CCR0, new_top, R0);
2308   bc_far_optimized(Assembler::bcondCRbiIs1, bi0(CCR0, Assembler::greater), slow_case);
2309 
2310 #ifdef ASSERT
2311   // make sure new free pointer is properly aligned
2312   {
2313     Label L;
2314     andi_(R0, new_top, MinObjAlignmentInBytesMask);
2315     beq(CCR0, L);
<span class="line-modified">2316     stop(&quot;updated TLAB free is not properly aligned&quot;);</span>
2317     bind(L);
2318   }
2319 #endif // ASSERT
2320 
2321   // update the tlab top pointer
2322   std(new_top, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
2323   //verify_tlab(); not implemented
2324 }
2325 void MacroAssembler::incr_allocated_bytes(RegisterOrConstant size_in_bytes, Register t1, Register t2) {
2326   unimplemented(&quot;incr_allocated_bytes&quot;);
2327 }
2328 
2329 address MacroAssembler::emit_trampoline_stub(int destination_toc_offset,
2330                                              int insts_call_instruction_offset, Register Rtoc) {
2331   // Start the stub.
2332   address stub = start_a_stub(64);
2333   if (stub == NULL) { return NULL; } // CodeCache full: bail out
2334 
2335   // Create a trampoline stub relocation which relates this trampoline stub
2336   // with the call instruction at insts_call_instruction_offset in the
</pre>
<hr />
<pre>
2775   // We don&#39;t reload mark word. Will only be reset at safepoint.
2776   ld(R0, 0, owner_addr_Reg); // Load in transaction, conflicts need to be tracked.
2777   cmpdi(flag, R0, 0);
2778   beq(flag, DONE_LABEL);
2779 
2780   if (UseRTMXendForLockBusy) {
2781     tend_();
2782     b(L_decrement_retry);
2783   } else {
2784     tabort_();
2785   }
2786   bind(L_on_abort);
2787   const Register abort_status_Reg = tmpReg;
2788   mftexasr(abort_status_Reg);
2789   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2790     rtm_profiling(abort_status_Reg, /*temp*/ owner_addr_Reg, rtm_counters, method_data, profile_rtm);
2791     // Restore owner_addr_Reg
2792     ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);
2793 #ifdef ASSERT
2794     andi_(R0, mark_word, markWord::monitor_value);
<span class="line-modified">2795     asm_assert_ne(&quot;must be inflated&quot;); // Deflating only allowed at safepoint.</span>
2796 #endif
2797     addi(owner_addr_Reg, mark_word, owner_offset);
2798   }
2799   if (RTMRetryCount &gt; 0) {
2800     // Retry on lock abort if abort status is not permanent.
2801     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
2802   }
2803 
2804   // Appears unlocked - try to swing _owner from null to non-null.
2805   cmpxchgd(flag, /*current val*/ R0, (intptr_t)0, /*new val*/ R16_thread, owner_addr_Reg,
2806            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2807            MacroAssembler::cmpxchgx_hint_acquire_lock(), noreg, &amp;L_decrement_retry, true);
2808 
2809   if (RTMRetryCount &gt; 0) {
2810     // success done else retry
2811     b(DONE_LABEL);
2812     bind(L_decrement_retry);
2813     // Spin and retry if lock is busy.
2814     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, owner_addr_Reg, L_rtm_retry);
2815   } else {
</pre>
<hr />
<pre>
2912   // Try to CAS m-&gt;owner from NULL to current thread.
2913   addi(temp, displaced_header, ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value);
2914   cmpxchgd(/*flag=*/flag,
2915            /*current_value=*/current_header,
2916            /*compare_value=*/(intptr_t)0,
2917            /*exchange_value=*/R16_thread,
2918            /*where=*/temp,
2919            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2920            MacroAssembler::cmpxchgx_hint_acquire_lock());
2921 
2922   // Store a non-null value into the box.
2923   std(box, BasicLock::displaced_header_offset_in_bytes(), box);
2924 
2925 # ifdef ASSERT
2926   bne(flag, cont);
2927   // We have acquired the monitor, check some invariants.
2928   addi(/*monitor=*/temp, temp, -ObjectMonitor::owner_offset_in_bytes());
2929   // Invariant 1: _recursions should be 0.
2930   //assert(ObjectMonitor::recursions_size_in_bytes() == 8, &quot;unexpected size&quot;);
2931   asm_assert_mem8_is_zero(ObjectMonitor::recursions_offset_in_bytes(), temp,
<span class="line-modified">2932                             &quot;monitor-&gt;_recursions should be 0&quot;);</span>
2933 # endif
2934 
2935 #if INCLUDE_RTM_OPT
2936   } // use_rtm()
2937 #endif
2938 
2939   bind(cont);
2940   // flag == EQ indicates success
2941   // flag == NE indicates failure
2942 }
2943 
2944 void MacroAssembler::compiler_fast_unlock_object(ConditionRegister flag, Register oop, Register box,
2945                                                  Register temp, Register displaced_header, Register current_header,
2946                                                  bool try_bias, bool use_rtm) {
2947   assert_different_registers(oop, box, temp, displaced_header, current_header);
2948   assert(flag != CCR0, &quot;bad condition register&quot;);
2949   Label cont;
2950   Label object_has_monitor;
2951 
2952   if (try_bias) {
</pre>
<hr />
<pre>
3041   // Armed page has poll_bit set.
3042   andi_(temp_reg, temp_reg, SafepointMechanism::poll_bit());
3043   bne(CCR0, slow_path);
3044 }
3045 
3046 void MacroAssembler::resolve_jobject(Register value, Register tmp1, Register tmp2, bool needs_frame) {
3047   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3048   bs-&gt;resolve_jobject(this, value, tmp1, tmp2, needs_frame);
3049 }
3050 
3051 // Values for last_Java_pc, and last_Java_sp must comply to the rules
3052 // in frame_ppc.hpp.
3053 void MacroAssembler::set_last_Java_frame(Register last_Java_sp, Register last_Java_pc) {
3054   // Always set last_Java_pc and flags first because once last_Java_sp
3055   // is visible has_last_Java_frame is true and users will look at the
3056   // rest of the fields. (Note: flags should always be zero before we
3057   // get here so doesn&#39;t need to be set.)
3058 
3059   // Verify that last_Java_pc was zeroed on return to Java
3060   asm_assert_mem8_is_zero(in_bytes(JavaThread::last_Java_pc_offset()), R16_thread,
<span class="line-modified">3061                           &quot;last_Java_pc not zeroed before leaving Java&quot;);</span>
3062 
3063   // When returning from calling out from Java mode the frame anchor&#39;s
3064   // last_Java_pc will always be set to NULL. It is set here so that
3065   // if we are doing a call to native (not VM) that we capture the
3066   // known pc and don&#39;t have to rely on the native call having a
3067   // standard frame linkage where we can find the pc.
3068   if (last_Java_pc != noreg)
3069     std(last_Java_pc, in_bytes(JavaThread::last_Java_pc_offset()), R16_thread);
3070 
3071   // Set last_Java_sp last.
3072   std(last_Java_sp, in_bytes(JavaThread::last_Java_sp_offset()), R16_thread);
3073 }
3074 
3075 void MacroAssembler::reset_last_Java_frame(void) {
3076   asm_assert_mem8_isnot_zero(in_bytes(JavaThread::last_Java_sp_offset()),
<span class="line-modified">3077                              R16_thread, &quot;SP was not set, still zero&quot;);</span>
3078 
3079   BLOCK_COMMENT(&quot;reset_last_Java_frame {&quot;);
3080   li(R0, 0);
3081 
3082   // _last_Java_sp = 0
3083   std(R0, in_bytes(JavaThread::last_Java_sp_offset()), R16_thread);
3084 
3085   // _last_Java_pc = 0
3086   std(R0, in_bytes(JavaThread::last_Java_pc_offset()), R16_thread);
3087   BLOCK_COMMENT(&quot;} reset_last_Java_frame&quot;);
3088 }
3089 
3090 void MacroAssembler::set_top_ijava_frame_at_SP_as_last_Java_frame(Register sp, Register tmp1) {
3091   assert_different_registers(sp, tmp1);
3092 
3093   // sp points to a TOP_IJAVA_FRAME, retrieve frame&#39;s PC via
3094   // TOP_IJAVA_FRAME_ABI.
3095   // FIXME: assert that we really have a TOP_IJAVA_FRAME here!
3096   address entry = pc();
3097   load_const_optimized(tmp1, entry);
</pre>
<hr />
<pre>
4310   addi(tmp3, xlen, 1);
4311   sldi(tmp, tmp3, LogBytesPerInt);
4312   stwx(carry, z, tmp);
4313   addic_(tmp3, tmp3, -1);
4314   blt(CCR0, L_done);
4315 
4316   srdi(carry, carry, 32);
4317   sldi(tmp, tmp3, LogBytesPerInt);
4318   stwx(carry, z, tmp);
4319   b(L_second_loop);
4320 
4321   // Next infrequent code is moved outside loops.
4322   bind(L_last_x);
4323 
4324   lwz(x_xstart, 0, x);
4325   b(L_third_loop_prologue);
4326 
4327   bind(L_done);
4328 }   // multiply_to_len
4329 
<span class="line-modified">4330 void MacroAssembler::asm_assert(bool check_equal, const char *msg) {</span>
4331 #ifdef ASSERT
4332   Label ok;
4333   if (check_equal) {
4334     beq(CCR0, ok);
4335   } else {
4336     bne(CCR0, ok);
4337   }
<span class="line-modified">4338   stop(msg);</span>
4339   bind(ok);
4340 #endif
4341 }
4342 
4343 void MacroAssembler::asm_assert_mems_zero(bool check_equal, int size, int mem_offset,
<span class="line-modified">4344                                           Register mem_base, const char* msg) {</span>
4345 #ifdef ASSERT
4346   switch (size) {
4347     case 4:
4348       lwz(R0, mem_offset, mem_base);
4349       cmpwi(CCR0, R0, 0);
4350       break;
4351     case 8:
4352       ld(R0, mem_offset, mem_base);
4353       cmpdi(CCR0, R0, 0);
4354       break;
4355     default:
4356       ShouldNotReachHere();
4357   }
<span class="line-modified">4358   asm_assert(check_equal, msg);</span>
4359 #endif // ASSERT
4360 }
4361 
4362 void MacroAssembler::verify_thread() {
4363   if (VerifyThread) {
4364     unimplemented(&quot;&#39;VerifyThread&#39; currently not implemented on PPC&quot;);
4365   }
4366 }
4367 
4368 void MacroAssembler::verify_coop(Register coop, const char* msg) {
4369   if (!VerifyOops) { return; }
4370   if (UseCompressedOops) { decode_heap_oop(coop); }
4371   verify_oop(coop, msg);
4372   if (UseCompressedOops) { encode_heap_oop(coop, coop); }
4373 }
4374 
4375 // READ: oop. KILL: R0. Volatile floats perhaps.
4376 void MacroAssembler::verify_oop(Register oop, const char* msg) {
4377   if (!VerifyOops) {
4378     return;
</pre>
<hr />
<pre>
4413   const Register tmp = R11; // Will be preserved.
4414   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
4415   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4416 
4417   ld(R4_ARG2, offs, base);
4418   save_LR_CR(tmp); // save in old frame
4419   push_frame_reg_args(nbytes_save, tmp);
4420   // load FunctionDescriptor** / entry_address *
4421   load_const_optimized(tmp, fd, R0);
4422   // load FunctionDescriptor* / entry_address
4423   ld(tmp, 0, tmp);
4424   load_const_optimized(R3_ARG1, (address)msg, R0);
4425   // Call destination for its side effect.
4426   call_c(tmp);
4427 
4428   pop_frame();
4429   restore_LR_CR(tmp);
4430   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
4431 }
4432 












4433 // Call a C-function that prints output.
<span class="line-modified">4434 void MacroAssembler::stop(int type, const char* msg) {</span>
4435 #ifndef PRODUCT
<span class="line-modified">4436   block_comment(err_msg(&quot;stop(type %d): %s {&quot;, type, msg));</span>
4437 #else
4438   block_comment(&quot;stop {&quot;);
4439 #endif
4440 
<span class="line-modified">4441   if (type != stop_shouldnotreachhere) {</span>
<span class="line-modified">4442     // Use R0 to pass msg. &quot;shouldnotreachhere&quot; preserves R0.</span>
<span class="line-modified">4443     load_const_optimized(R0, (void*)msg);</span>
<span class="line-modified">4444   }</span>
<span class="line-modified">4445   tdi_unchecked(traptoUnconditional, 0/*reg 0*/, type);</span>
<span class="line-modified">4446 </span>
4447   block_comment(&quot;} stop;&quot;);
4448 }
4449 
4450 #ifndef PRODUCT
4451 // Write pattern 0x0101010101010101 in memory region [low-before, high+after].
4452 // Val, addr are temp registers.
4453 // If low == addr, addr is killed.
4454 // High is preserved.
4455 void MacroAssembler::zap_from_to(Register low, int before, Register high, int after, Register val, Register addr) {
4456   if (!ZapMemory) return;
4457 
4458   assert_different_registers(low, val);
4459 
4460   BLOCK_COMMENT(&quot;zap memory region {&quot;);
4461   load_const_optimized(val, 0x0101010101010101);
4462   int size = before + after;
4463   if (low == high &amp;&amp; size &lt; 5 &amp;&amp; size &gt; 0) {
4464     int offset = -before*BytesPerWord;
4465     for (int i = 0; i &lt; size; ++i) {
4466       std(val, offset, low);
</pre>
</td>
</tr>
</table>
<center><a href="interp_masm_ppc_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>