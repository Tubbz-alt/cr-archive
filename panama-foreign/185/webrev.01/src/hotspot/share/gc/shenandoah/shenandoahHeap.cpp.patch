diff a/src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp b/src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp
--- a/src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp
+++ b/src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp
@@ -44,10 +44,11 @@
 #include "gc/shenandoah/shenandoahFreeSet.hpp"
 #include "gc/shenandoah/shenandoahPhaseTimings.hpp"
 #include "gc/shenandoah/shenandoahHeap.inline.hpp"
 #include "gc/shenandoah/shenandoahHeapRegion.inline.hpp"
 #include "gc/shenandoah/shenandoahHeapRegionSet.hpp"
+#include "gc/shenandoah/shenandoahInitLogger.hpp"
 #include "gc/shenandoah/shenandoahMarkCompact.hpp"
 #include "gc/shenandoah/shenandoahMarkingContext.inline.hpp"
 #include "gc/shenandoah/shenandoahMemoryPool.hpp"
 #include "gc/shenandoah/shenandoahMetrics.hpp"
 #include "gc/shenandoah/shenandoahMonitoringSupport.hpp"
@@ -108,11 +109,13 @@
     _page_size(page_size) {}
 
   virtual void work(uint worker_id) {
     ShenandoahHeapRegion* r = _regions.next();
     while (r != NULL) {
-      os::pretouch_memory(r->bottom(), r->end(), _page_size);
+      if (r->is_committed()) {
+        os::pretouch_memory(r->bottom(), r->end(), _page_size);
+      }
       r = _regions.next();
     }
   }
 };
 
@@ -134,11 +137,13 @@
     while (r != NULL) {
       size_t start = r->index()       * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
       size_t end   = (r->index() + 1) * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
       assert (end <= _bitmap_size, "end is sane: " SIZE_FORMAT " < " SIZE_FORMAT, end, _bitmap_size);
 
-      os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);
+      if (r->is_committed()) {
+        os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);
+      }
 
       r = _regions.next();
     }
   }
 };
@@ -153,15 +158,10 @@
   size_t max_byte_size  = MaxHeapSize;
   size_t heap_alignment = HeapAlignment;
 
   size_t reg_size_bytes = ShenandoahHeapRegion::region_size_bytes();
 
-  if (ShenandoahAlwaysPreTouch) {
-    // Enabled pre-touch means the entire heap is committed right away.
-    init_byte_size = max_byte_size;
-  }
-
   Universe::check_alignment(max_byte_size,  reg_size_bytes, "Shenandoah heap");
   Universe::check_alignment(init_byte_size, reg_size_bytes, "Shenandoah heap");
 
   _num_regions = ShenandoahHeapRegion::region_count();
 
@@ -290,13 +290,39 @@
   if (!region_storage.special()) {
     os::commit_memory_or_exit(region_storage.base(), region_storage_size, region_page_size, false,
                               "Cannot commit region memory");
   }
 
+  // Try to fit the collection set bitmap at lower addresses. This optimizes code generation for cset checks.
+  // Go up until a sensible limit (subject to encoding constraints) and try to reserve the space there.
+  // If not successful, bite a bullet and allocate at whatever address.
+  {
+    size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());
+    size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);
+
+    uintptr_t min = round_up_power_of_2(cset_align);
+    uintptr_t max = (1u << 30u);
+
+    for (uintptr_t addr = min; addr <= max; addr <<= 1u) {
+      char* req_addr = (char*)addr;
+      assert(is_aligned(req_addr, cset_align), "Should be aligned");
+      ReservedSpace cset_rs(cset_size, cset_align, false, req_addr);
+      if (cset_rs.is_reserved()) {
+        assert(cset_rs.base() == req_addr, "Allocated where requested: " PTR_FORMAT ", " PTR_FORMAT, p2i(cset_rs.base()), addr);
+        _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
+        break;
+      }
+    }
+
+    if (_collection_set == NULL) {
+      ReservedSpace cset_rs(cset_size, cset_align, false);
+      _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
+    }
+  }
+
   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
   _free_set = new ShenandoahFreeSet(this, _num_regions);
-  _collection_set = new ShenandoahCollectionSet(this, sh_rs.base(), sh_rs.size());
 
   {
     ShenandoahHeapLocker locker(lock());
 
     for (size_t i = 0; i < _num_regions; i++) {
@@ -316,42 +342,36 @@
     _marking_context->mark_complete();
 
     _free_set->rebuild();
   }
 
-  if (ShenandoahAlwaysPreTouch) {
-    assert(!AlwaysPreTouch, "Should have been overridden");
-
+  if (AlwaysPreTouch) {
     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
     // before initialize() below zeroes it with initializing thread. For any given region,
     // we touch the region and the corresponding bitmaps from the same thread.
     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 
-    size_t pretouch_heap_page_size = heap_page_size;
-    size_t pretouch_bitmap_page_size = bitmap_page_size;
+    _pretouch_heap_page_size = heap_page_size;
+    _pretouch_bitmap_page_size = bitmap_page_size;
 
 #ifdef LINUX
     // UseTransparentHugePages would madvise that backing memory can be coalesced into huge
     // pages. But, the kernel needs to know that every small page is used, in order to coalesce
     // them into huge one. Therefore, we need to pretouch with smaller pages.
     if (UseTransparentHugePages) {
-      pretouch_heap_page_size = (size_t)os::vm_page_size();
-      pretouch_bitmap_page_size = (size_t)os::vm_page_size();
+      _pretouch_heap_page_size = (size_t)os::vm_page_size();
+      _pretouch_bitmap_page_size = (size_t)os::vm_page_size();
     }
 #endif
 
     // OS memory managers may want to coalesce back-to-back pages. Make their jobs
     // simpler by pre-touching continuous spaces (heap and bitmap) separately.
 
-    log_info(gc, init)("Pretouch bitmap: " SIZE_FORMAT " regions, " SIZE_FORMAT " bytes page",
-                       _num_regions, pretouch_bitmap_page_size);
-    ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, pretouch_bitmap_page_size);
+    ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, _pretouch_bitmap_page_size);
     _workers->run_task(&bcl);
 
-    log_info(gc, init)("Pretouch heap: " SIZE_FORMAT " regions, " SIZE_FORMAT " bytes page",
-                       _num_regions, pretouch_heap_page_size);
-    ShenandoahPretouchHeapTask hcl(pretouch_heap_page_size);
+    ShenandoahPretouchHeapTask hcl(_pretouch_heap_page_size);
     _workers->run_task(&hcl);
   }
 
   //
   // Initialize the rest of GC subsystems
@@ -383,17 +403,14 @@
     _pacer = NULL;
   }
 
   _control_thread = new ShenandoahControlThread();
 
-  log_info(gc, init)("Initialize Shenandoah heap: " SIZE_FORMAT "%s initial, " SIZE_FORMAT "%s min, " SIZE_FORMAT "%s max",
-                     byte_size_in_proper_unit(_initial_size),  proper_unit_for_byte_size(_initial_size),
-                     byte_size_in_proper_unit(_minimum_size),  proper_unit_for_byte_size(_minimum_size),
-                     byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity())
-  );
+  _ref_proc_mt_processing = ParallelRefProcEnabled && (ParallelGCThreads > 1);
+  _ref_proc_mt_discovery = _max_workers > 1;
 
-  log_info(gc, init)("Safepointing mechanism: thread-local poll");
+  ShenandoahInitLogger::print();
 
   return JNI_OK;
 }
 
 void ShenandoahHeap::initialize_heuristics() {
@@ -419,12 +436,10 @@
   if (_gc_mode->is_experimental() && !UnlockExperimentalVMOptions) {
     vm_exit_during_initialization(
             err_msg("GC mode \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
                     _gc_mode->name()));
   }
-  log_info(gc, init)("Shenandoah GC mode: %s",
-                     _gc_mode->name());
 
   _heuristics = _gc_mode->initialize_heuristics();
 
   if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {
     vm_exit_during_initialization(
@@ -434,12 +449,10 @@
   if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {
     vm_exit_during_initialization(
             err_msg("Heuristics \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
                     _heuristics->name()));
   }
-  log_info(gc, init)("Shenandoah heuristics: %s",
-                     _heuristics->name());
 }
 
 #ifdef _MSC_VER
 #pragma warning( push )
 #pragma warning( disable:4355 ) // 'this' : used in base member initializer list
@@ -484,12 +497,10 @@
   _liveness_cache(NULL),
   _collection_set(NULL)
 {
   _heap = this;
 
-  log_info(gc, init)("GC threads: " UINT32_FORMAT " parallel, " UINT32_FORMAT " concurrent", ParallelGCThreads, ConcGCThreads);
-
   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 
   _max_workers = MAX2(_max_workers, 1U);
   _workers = new ShenandoahWorkGang("Shenandoah GC Threads", _max_workers,
                             /* are_GC_task_threads */ true,
@@ -1828,12 +1839,13 @@
   }
 };
 
 void ShenandoahHeap::op_weak_roots() {
   if (is_concurrent_weak_root_in_progress()) {
+    // Concurrent weak root processing
     {
-      // Concurrent weak root processing
+      ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);
       ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);
       ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);
       workers()->run_task(&task);
       if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
         set_concurrent_weak_root_in_progress(false);
@@ -2130,27 +2142,20 @@
 }
 
 void ShenandoahHeap::ref_processing_init() {
   assert(_max_workers > 0, "Sanity");
 
-  bool mt_processing = ParallelRefProcEnabled && (ParallelGCThreads > 1);
-  bool mt_discovery = _max_workers > 1;
-
   _ref_processor =
     new ReferenceProcessor(&_subject_to_discovery,  // is_subject_to_discovery
-                           mt_processing,           // MT processing
+                           _ref_proc_mt_processing, // MT processing
                            _max_workers,            // Degree of MT processing
-                           mt_discovery,            // MT discovery
+                           _ref_proc_mt_discovery,  // MT discovery
                            _max_workers,            // Degree of MT discovery
                            false,                   // Reference discovery is not atomic
                            NULL,                    // No closure, should be installed before use
                            true);                   // Scale worker threads
 
-  log_info(gc, init)("Reference processing: %s discovery, %s processing",
-          mt_discovery ? "parallel" : "serial",
-          mt_processing ? "parallel" : "serial");
-
   shenandoah_assert_rp_isalive_not_installed();
 }
 
 GCTracer* ShenandoahHeap::tracer() {
   return shenandoah_policy()->tracer();
@@ -2658,13 +2663,20 @@
 
   // Commit the bitmap slice:
   size_t slice = r->index() / _bitmap_regions_per_slice;
   size_t off = _bitmap_bytes_per_slice * slice;
   size_t len = _bitmap_bytes_per_slice;
-  if (!os::commit_memory((char*)_bitmap_region.start() + off, len, false)) {
+  char* start = (char*) _bitmap_region.start() + off;
+
+  if (!os::commit_memory(start, len, false)) {
     return false;
   }
+
+  if (AlwaysPreTouch) {
+    os::pretouch_memory(start, start + len, _pretouch_bitmap_page_size);
+  }
+
   return true;
 }
 
 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
   shenandoah_assert_heaplocked();
