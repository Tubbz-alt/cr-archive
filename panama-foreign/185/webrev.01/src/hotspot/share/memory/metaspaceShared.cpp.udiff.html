<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/share/memory/metaspaceShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="metaspace/virtualSpaceNode.hpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspaceShared.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -67,10 +67,11 @@</span>
  #include &quot;runtime/timerTrace.hpp&quot;
  #include &quot;runtime/vmThread.hpp&quot;
  #include &quot;runtime/vmOperations.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
  #include &quot;utilities/bitMap.inline.hpp&quot;
<span class="udiff-line-added">+ #include &quot;utilities/ostream.hpp&quot;</span>
  #include &quot;utilities/defaultStream.hpp&quot;
  #include &quot;utilities/hashtable.inline.hpp&quot;
  #if INCLUDE_G1GC
  #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  #endif
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -189,11 +190,11 @@</span>
    _end = _rs-&gt;end();
  }
  
  void DumpRegion::pack(DumpRegion* next) {
    assert(!is_packed(), &quot;sanity&quot;);
<span class="udiff-line-modified-removed">-   _end = (char*)align_up(_top, Metaspace::reserve_alignment());</span>
<span class="udiff-line-modified-added">+   _end = (char*)align_up(_top, MetaspaceShared::reserved_space_alignment());</span>
    _is_packed = true;
    if (next != NULL) {
      next-&gt;_rs = _rs;
      next-&gt;_vs = _vs;
      next-&gt;_base = next-&gt;_top = this-&gt;_end;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -235,105 +236,163 @@</span>
  
  char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
    return _ro_region.allocate(num_bytes);
  }
  
<span class="udiff-line-modified-removed">- // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:</span>
<span class="udiff-line-removed">- // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,</span>
<span class="udiff-line-removed">- //                                    while keeping the first range at offset 0 of this range.</span>
<span class="udiff-line-removed">- // Metaspace::reserve_alignment()  -- so we can pass the region to</span>
<span class="udiff-line-removed">- //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.</span>
<span class="udiff-line-removed">- size_t MetaspaceShared::reserved_space_alignment() {</span>
<span class="udiff-line-removed">-   size_t os_align = os::vm_allocation_granularity();</span>
<span class="udiff-line-removed">-   size_t ms_align = Metaspace::reserve_alignment();</span>
<span class="udiff-line-removed">-   if (os_align &gt;= ms_align) {</span>
<span class="udiff-line-removed">-     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);</span>
<span class="udiff-line-removed">-     return os_align;</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     assert(ms_align % os_align == 0, &quot;must be a multiple&quot;);</span>
<span class="udiff-line-removed">-     return ms_align;</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-modified-added">+ size_t MetaspaceShared::reserved_space_alignment() { return os::vm_allocation_granularity(); }</span>
  
<span class="udiff-line-modified-removed">- ReservedSpace MetaspaceShared::reserve_shared_space(size_t size, char* requested_address) {</span>
<span class="udiff-line-modified-removed">-   return Metaspace::reserve_space(size, reserved_space_alignment(),</span>
<span class="udiff-line-modified-removed">-                                   requested_address, requested_address != NULL);</span>
<span class="udiff-line-modified-added">+ #ifdef _LP64</span>
<span class="udiff-line-modified-added">+ // Check SharedBaseAddress for validity. At this point, os::init() must</span>
<span class="udiff-line-modified-added">+ //  have been ran.</span>
<span class="udiff-line-added">+ static void check_SharedBaseAddress() {</span>
<span class="udiff-line-added">+   SharedBaseAddress = align_up(SharedBaseAddress,</span>
<span class="udiff-line-added">+                                MetaspaceShared::reserved_space_alignment());</span>
<span class="udiff-line-added">+   if (!CompressedKlassPointers::is_valid_base((address)SharedBaseAddress)) {</span>
<span class="udiff-line-added">+     log_warning(cds)(&quot;SharedBaseAddress=&quot; PTR_FORMAT &quot; is invalid for this &quot;</span>
<span class="udiff-line-added">+                      &quot;platform, option will be ignored.&quot;,</span>
<span class="udiff-line-added">+                      p2i((address)SharedBaseAddress));</span>
<span class="udiff-line-added">+     SharedBaseAddress = Arguments::default_SharedBaseAddress();</span>
<span class="udiff-line-added">+   }</span>
  }
<span class="udiff-line-added">+ #endif</span>
  
  void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
    assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="udiff-line-modified-removed">-   const size_t reserve_alignment = reserved_space_alignment();</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+ #ifdef _LP64</span>
<span class="udiff-line-added">+   check_SharedBaseAddress();</span>
<span class="udiff-line-added">+ #endif</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   const size_t reserve_alignment = MetaspaceShared::reserved_space_alignment();</span>
    char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
  
  #ifdef _LP64
<span class="udiff-line-modified-removed">-   // On 64-bit VM, the heap and class space layout will be the same as if</span>
<span class="udiff-line-modified-removed">-   // you&#39;re running in -Xshare:on mode:</span>
<span class="udiff-line-modified-removed">-   //</span>
<span class="udiff-line-modified-removed">-   //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="udiff-line-removed">-   //                              v</span>
<span class="udiff-line-removed">-   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="udiff-line-removed">-   // |    Heap    | Archive |     | MC | RW | RO |    class space     |</span>
<span class="udiff-line-removed">-   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="udiff-line-removed">-   // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="udiff-line-removed">-   //</span>
<span class="udiff-line-modified-added">+   assert(CompressedKlassPointers::is_valid_base((address)shared_base), &quot;Sanity&quot;);</span>
<span class="udiff-line-modified-added">+   // On 64-bit VM we reserve a 4G range and, if UseCompressedClassPointers=1,</span>
<span class="udiff-line-modified-added">+   //  will use that to house both the archives and the ccs. See below for</span>
<span class="udiff-line-modified-added">+   //  details.</span>
    const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
    const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
  #else
<span class="udiff-line-modified-removed">-   // We don&#39;t support archives larger than 256MB on 32-bit due to limited virtual address space.</span>
<span class="udiff-line-modified-added">+   // We don&#39;t support archives larger than 256MB on 32-bit due to limited</span>
<span class="udiff-line-added">+   //  virtual address space.</span>
    size_t cds_total = align_down(256*M, reserve_alignment);
  #endif
  
<span class="udiff-line-added">+   // Whether to use SharedBaseAddress as attach address.</span>
    bool use_requested_base = true;
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (shared_base == NULL) {</span>
<span class="udiff-line-added">+     use_requested_base = false;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
    if (ArchiveRelocationMode == 1) {
      log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
      use_requested_base = false;
    }
  
    // First try to reserve the space at the specified SharedBaseAddress.
    assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
    if (use_requested_base) {
<span class="udiff-line-modified-removed">-     _shared_rs = reserve_shared_space(cds_total, shared_base);</span>
<span class="udiff-line-modified-added">+     _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="udiff-line-added">+                                false /* large */, (char*)shared_base);</span>
<span class="udiff-line-added">+     if (_shared_rs.is_reserved()) {</span>
<span class="udiff-line-added">+       assert(_shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="udiff-line-added">+     } else {</span>
<span class="udiff-line-added">+       log_info(cds)(&quot;dumptime space reservation: failed to map at &quot;</span>
<span class="udiff-line-added">+                     &quot;SharedBaseAddress &quot; PTR_FORMAT, p2i(shared_base));</span>
<span class="udiff-line-added">+     }</span>
    }
<span class="udiff-line-modified-removed">-   if (_shared_rs.is_reserved()) {</span>
<span class="udiff-line-modified-removed">-     assert(shared_base == 0 || _shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="udiff-line-modified-removed">-   } else {</span>
<span class="udiff-line-modified-removed">-     // Get a mmap region anywhere if the SharedBaseAddress fails.</span>
<span class="udiff-line-modified-removed">-     _shared_rs = reserve_shared_space(cds_total);</span>
<span class="udiff-line-modified-added">+   if (!_shared_rs.is_reserved()) {</span>
<span class="udiff-line-modified-added">+     // Get a reserved space anywhere if attaching at the SharedBaseAddress</span>
<span class="udiff-line-modified-added">+     //  fails:</span>
<span class="udiff-line-modified-added">+     if (UseCompressedClassPointers) {</span>
<span class="udiff-line-modified-added">+       // If we need to reserve class space as well, let the platform handle</span>
<span class="udiff-line-added">+       //  the reservation.</span>
<span class="udiff-line-added">+       LP64_ONLY(_shared_rs =</span>
<span class="udiff-line-added">+                 Metaspace::reserve_address_space_for_compressed_classes(cds_total);)</span>
<span class="udiff-line-added">+       NOT_LP64(ShouldNotReachHere();)</span>
<span class="udiff-line-added">+     } else {</span>
<span class="udiff-line-added">+       // anywhere is fine.</span>
<span class="udiff-line-added">+       _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="udiff-line-added">+                                  false /* large */, (char*)NULL);</span>
<span class="udiff-line-added">+     }</span>
    }
<span class="udiff-line-added">+ </span>
    if (!_shared_rs.is_reserved()) {
      vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
                                    err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
    }
  
  #ifdef _LP64
<span class="udiff-line-removed">-   // During dump time, we allocate 4GB (UnscaledClassSpaceMax) of space and split it up:</span>
<span class="udiff-line-removed">-   // + The upper 1 GB is used as the &quot;temporary compressed class space&quot; -- preload_classes()</span>
<span class="udiff-line-removed">-   //   will store Klasses into this space.</span>
<span class="udiff-line-removed">-   // + The lower 3 GB is used for the archive -- when preload_classes() is done,</span>
<span class="udiff-line-removed">-   //   ArchiveCompactor will copy the class metadata into this space, first the RW parts,</span>
<span class="udiff-line-removed">-   //   then the RO parts.</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   size_t max_archive_size = align_down(cds_total * 3 / 4, reserve_alignment);</span>
<span class="udiff-line-removed">-   ReservedSpace tmp_class_space = _shared_rs.last_part(max_archive_size);</span>
<span class="udiff-line-removed">-   CompressedClassSpaceSize = align_down(tmp_class_space.size(), reserve_alignment);</span>
<span class="udiff-line-removed">-   _shared_rs = _shared_rs.first_part(max_archive_size);</span>
  
    if (UseCompressedClassPointers) {
<span class="udiff-line-modified-removed">-     // Set up compress class pointers.</span>
<span class="udiff-line-modified-removed">-     CompressedKlassPointers::set_base((address)_shared_rs.base());</span>
<span class="udiff-line-modified-removed">-     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent</span>
<span class="udiff-line-modified-removed">-     // with AOT.</span>
<span class="udiff-line-modified-removed">-     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="udiff-line-modified-removed">-     // Set the range of klass addresses to 4GB.</span>
<span class="udiff-line-modified-removed">-     CompressedKlassPointers::set_range(cds_total);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     assert(CompressedKlassPointers::is_valid_base((address)_shared_rs.base()), &quot;Sanity&quot;);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     // On 64-bit VM, if UseCompressedClassPointers=1, the compressed class space</span>
<span class="udiff-line-modified-added">+     //  must be allocated near the cds such as that the compressed Klass pointer</span>
<span class="udiff-line-modified-added">+     //  encoding can be used to en/decode pointers from both cds and ccs. Since</span>
<span class="udiff-line-modified-added">+     //  Metaspace cannot do this (it knows nothing about cds), we do it for</span>
<span class="udiff-line-added">+     //  Metaspace here and pass it the space to use for ccs.</span>
<span class="udiff-line-added">+     //</span>
<span class="udiff-line-added">+     // We do this by reserving space for the ccs behind the archives. Note</span>
<span class="udiff-line-added">+     //  however that ccs follows a different alignment</span>
<span class="udiff-line-added">+     //  (Metaspace::reserve_alignment), so there may be a gap between ccs and</span>
<span class="udiff-line-added">+     //  cds.</span>
<span class="udiff-line-added">+     // We use a similar layout at runtime, see reserve_address_space_for_archives().</span>
<span class="udiff-line-added">+     //</span>
<span class="udiff-line-added">+     //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="udiff-line-added">+     //                              v</span>
<span class="udiff-line-added">+     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="udiff-line-added">+     // |    Heap    | Archive |     | MC | RW | RO | [gap]  |    class space  |</span>
<span class="udiff-line-added">+     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="udiff-line-added">+     // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="udiff-line-added">+     //</span>
<span class="udiff-line-added">+     // Note: ccs must follow the archives, and the archives must start at the</span>
<span class="udiff-line-added">+     //  encoding base. However, the exact placement of ccs does not matter as</span>
<span class="udiff-line-added">+     //  long as it it resides in the encoding range of CompressedKlassPointers</span>
<span class="udiff-line-added">+     //  and comes after the archive.</span>
<span class="udiff-line-added">+     //</span>
<span class="udiff-line-added">+     // We do this by splitting up the allocated 4G into 3G of archive space,</span>
<span class="udiff-line-added">+     //  followed by 1G for the ccs:</span>
<span class="udiff-line-added">+     // + The upper 1 GB is used as the &quot;temporary compressed class space&quot;</span>
<span class="udiff-line-added">+     //   -- preload_classes() will store Klasses into this space.</span>
<span class="udiff-line-added">+     // + The lower 3 GB is used for the archive -- when preload_classes()</span>
<span class="udiff-line-added">+     //   is done, ArchiveCompactor will copy the class metadata into this</span>
<span class="udiff-line-added">+     //   space, first the RW parts, then the RO parts.</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Starting address of ccs must be aligned to Metaspace::reserve_alignment()...</span>
<span class="udiff-line-added">+     size_t class_space_size = align_down(_shared_rs.size() / 4, Metaspace::reserve_alignment());</span>
<span class="udiff-line-added">+     address class_space_start = (address)align_down(_shared_rs.end() - class_space_size, Metaspace::reserve_alignment());</span>
<span class="udiff-line-added">+     size_t archive_size = class_space_start - (address)_shared_rs.base();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     ReservedSpace tmp_class_space = _shared_rs.last_part(archive_size);</span>
<span class="udiff-line-added">+     _shared_rs = _shared_rs.first_part(archive_size);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // ... as does the size of ccs.</span>
<span class="udiff-line-added">+     tmp_class_space = tmp_class_space.first_part(class_space_size);</span>
<span class="udiff-line-added">+     CompressedClassSpaceSize = class_space_size;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Let Metaspace initialize ccs</span>
      Metaspace::initialize_class_space(tmp_class_space);
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // and set up CompressedKlassPointers encoding.</span>
<span class="udiff-line-added">+     CompressedKlassPointers::initialize((address)_shared_rs.base(), cds_total);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="udiff-line-added">+                   p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="udiff-line-added">+                   CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     assert(_shared_rs.end() == tmp_class_space.base() &amp;&amp;</span>
<span class="udiff-line-added">+            is_aligned(_shared_rs.base(), MetaspaceShared::reserved_space_alignment()) &amp;&amp;</span>
<span class="udiff-line-added">+            is_aligned(tmp_class_space.base(), Metaspace::reserve_alignment()) &amp;&amp;</span>
<span class="udiff-line-added">+            is_aligned(tmp_class_space.size(), Metaspace::reserve_alignment()), &quot;Sanity&quot;);</span>
    }
<span class="udiff-line-removed">-   log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="udiff-line-removed">-                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
  
<span class="udiff-line-removed">-   log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="udiff-line-removed">-                 CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
  #endif
  
    init_shared_dump_space(&amp;_mc_region);
    SharedBaseAddress = (size_t)_shared_rs.base();
    log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2071,10 +2130,11 @@</span>
  }
  
  void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
    assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
    MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
<span class="udiff-line-added">+ </span>
    FileMapInfo* static_mapinfo = open_static_archive();
    FileMapInfo* dynamic_mapinfo = NULL;
  
    if (static_mapinfo != NULL) {
      dynamic_mapinfo = open_dynamic_archive();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2147,11 +2207,12 @@</span>
  //  false = map at an alternative address picked by OS.
  MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
                                                 bool use_requested_addr) {
    PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
        // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="udiff-line-modified-removed">-       // For debug builds, the check is done in FileMapInfo::map_regions for better test coverage.</span>
<span class="udiff-line-modified-added">+       // For debug builds, the check is done below, after reserving the space, for better test coverage</span>
<span class="udiff-line-added">+       // (see comment below).</span>
        log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
        return MAP_ARCHIVE_MMAP_FAILURE;
      });
  
    if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2163,30 +2224,75 @@</span>
      // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
      // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
      assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
    }
  
<span class="udiff-line-modified-removed">-   ReservedSpace main_rs, archive_space_rs, class_space_rs;</span>
<span class="udiff-line-modified-added">+   ReservedSpace archive_space_rs, class_space_rs;</span>
    MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
    char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="udiff-line-modified-removed">-                                                                  use_requested_addr, main_rs, archive_space_rs,</span>
<span class="udiff-line-modified-added">+                                                                  use_requested_addr, archive_space_rs,</span>
                                                                   class_space_rs);
    if (mapped_base_address == NULL) {
      result = MAP_ARCHIVE_MMAP_FAILURE;
    } else {
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ #ifdef ASSERT</span>
<span class="udiff-line-added">+     // Some sanity checks after reserving address spaces for archives</span>
<span class="udiff-line-added">+     //  and class space.</span>
<span class="udiff-line-added">+     assert(archive_space_rs.is_reserved(), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+     if (Metaspace::using_class_space()) {</span>
<span class="udiff-line-added">+       // Class space must closely follow the archive space. Both spaces</span>
<span class="udiff-line-added">+       //  must be aligned correctly.</span>
<span class="udiff-line-added">+       assert(class_space_rs.is_reserved(),</span>
<span class="udiff-line-added">+              &quot;A class space should have been reserved&quot;);</span>
<span class="udiff-line-added">+       assert(class_space_rs.base() &gt;= archive_space_rs.end(),</span>
<span class="udiff-line-added">+              &quot;class space should follow the cds archive space&quot;);</span>
<span class="udiff-line-added">+       assert(is_aligned(archive_space_rs.base(),</span>
<span class="udiff-line-added">+                         MetaspaceShared::reserved_space_alignment()),</span>
<span class="udiff-line-added">+              &quot;Archive space misaligned&quot;);</span>
<span class="udiff-line-added">+       assert(is_aligned(class_space_rs.base(),</span>
<span class="udiff-line-added">+                         Metaspace::reserve_alignment()),</span>
<span class="udiff-line-added">+              &quot;class space misaligned&quot;);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ #endif // ASSERT</span>
<span class="udiff-line-added">+ </span>
      log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
                     p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
      log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
                     p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     if (MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="udiff-line-added">+       // We have now reserved address space for the archives, and will map in</span>
<span class="udiff-line-added">+       //  the archive files into this space.</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // Special handling for Windows: on Windows we cannot map a file view</span>
<span class="udiff-line-added">+       //  into an existing memory mapping. So, we unmap the address range we</span>
<span class="udiff-line-added">+       //  just reserved again, which will make it available for mapping the</span>
<span class="udiff-line-added">+       //  archives.</span>
<span class="udiff-line-added">+       // Reserving this range has not been for naught however since it makes</span>
<span class="udiff-line-added">+       //  us reasonably sure the address range is available.</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // But still it may fail, since between unmapping the range and mapping</span>
<span class="udiff-line-added">+       //  in the archive someone else may grab the address space. Therefore</span>
<span class="udiff-line-added">+       //  there is a fallback in FileMap::map_region() where we just read in</span>
<span class="udiff-line-added">+       //  the archive files sequentially instead of mapping it in. We couple</span>
<span class="udiff-line-added">+       //  this with use_requested_addr, since we&#39;re going to patch all the</span>
<span class="udiff-line-added">+       //  pointers anyway so there&#39;s no benefit to mmap.</span>
<span class="udiff-line-added">+       if (use_requested_addr) {</span>
<span class="udiff-line-added">+         log_info(cds)(&quot;Windows mmap workaround: releasing archive space.&quot;);</span>
<span class="udiff-line-added">+         archive_space_rs.release();</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+     }</span>
      MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
      MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
                                       map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
  
      DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="udiff-line-modified-removed">-       // This is for simulating mmap failures at the requested address. In debug builds, we do it</span>
<span class="udiff-line-modified-removed">-       // here (after all archives have possibly been mapped), so we can thoroughly test the code for</span>
<span class="udiff-line-modified-removed">-       // failure handling (releasing all allocated resource, etc).</span>
<span class="udiff-line-modified-added">+       // This is for simulating mmap failures at the requested address. In</span>
<span class="udiff-line-modified-added">+       //  debug builds, we do it here (after all archives have possibly been</span>
<span class="udiff-line-modified-added">+       //  mapped), so we can thoroughly test the code for failure handling</span>
<span class="udiff-line-added">+       //  (releasing all allocated resource, etc).</span>
        log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
        if (static_result == MAP_ARCHIVE_SUCCESS) {
          static_result = MAP_ARCHIVE_MMAP_FAILURE;
        }
        if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2215,125 +2321,207 @@</span>
        result = MAP_ARCHIVE_MMAP_FAILURE;
      }
    }
  
    if (result == MAP_ARCHIVE_SUCCESS) {
<span class="udiff-line-removed">-     if (!main_rs.is_reserved() &amp;&amp; class_space_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-       MemTracker::record_virtual_memory_type((address)class_space_rs.base(), mtClass);</span>
<span class="udiff-line-removed">-     }</span>
      SharedBaseAddress = (size_t)mapped_base_address;
      LP64_ONLY({
          if (Metaspace::using_class_space()) {
<span class="udiff-line-modified-removed">-           assert(class_space_rs.is_reserved(), &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">-           char* cds_base = static_mapinfo-&gt;mapped_base();</span>
<span class="udiff-line-modified-removed">-           Metaspace::allocate_metaspace_compressed_klass_ptrs(class_space_rs, NULL, (address)cds_base);</span>
<span class="udiff-line-modified-added">+           // Set up ccs in metaspace.</span>
<span class="udiff-line-modified-added">+           Metaspace::initialize_class_space(class_space_rs);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+           // Set up compressed Klass pointer encoding: the encoding range must</span>
<span class="udiff-line-added">+           //  cover both archive and class space.</span>
<span class="udiff-line-added">+           address cds_base = (address)static_mapinfo-&gt;mapped_base();</span>
<span class="udiff-line-added">+           address ccs_end = (address)class_space_rs.end();</span>
<span class="udiff-line-added">+           CompressedKlassPointers::initialize(cds_base, ccs_end - cds_base);</span>
<span class="udiff-line-added">+ </span>
            // map_heap_regions() compares the current narrow oop and klass encodings
            // with the archived ones, so it must be done after all encodings are determined.
            static_mapinfo-&gt;map_heap_regions();
<span class="udiff-line-removed">-           CompressedKlassPointers::set_range(CompressedClassSpaceSize);</span>
          }
        });
    } else {
      unmap_archive(static_mapinfo);
      unmap_archive(dynamic_mapinfo);
<span class="udiff-line-modified-removed">-     release_reserved_spaces(main_rs, archive_space_rs, class_space_rs);</span>
<span class="udiff-line-modified-added">+     release_reserved_spaces(archive_space_rs, class_space_rs);</span>
    }
  
    return result;
  }
  
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ // This will reserve two address spaces suitable to house Klass structures, one</span>
<span class="udiff-line-added">+ //  for the cds archives (static archive and optionally dynamic archive) and</span>
<span class="udiff-line-added">+ //  optionally one move for ccs.</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // Since both spaces must fall within the compressed class pointer encoding</span>
<span class="udiff-line-added">+ //  range, they are allocated close to each other.</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // Space for archives will be reserved first, followed by a potential gap,</span>
<span class="udiff-line-added">+ //  followed by the space for ccs:</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // +-- Base address             A        B                     End</span>
<span class="udiff-line-added">+ // |                            |        |                      |</span>
<span class="udiff-line-added">+ // v                            v        v                      v</span>
<span class="udiff-line-added">+ // +-------------+--------------+        +----------------------+</span>
<span class="udiff-line-added">+ // | static arc  | [dyn. arch]  | [gap]  | compr. class space   |</span>
<span class="udiff-line-added">+ // +-------------+--------------+        +----------------------+</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // (The gap may result from different alignment requirements between metaspace</span>
<span class="udiff-line-added">+ //  and CDS)</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // If UseCompressedClassPointers is disabled, only one address space will be</span>
<span class="udiff-line-added">+ //  reserved:</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // +-- Base address             End</span>
<span class="udiff-line-added">+ // |                            |</span>
<span class="udiff-line-added">+ // v                            v</span>
<span class="udiff-line-added">+ // +-------------+--------------+</span>
<span class="udiff-line-added">+ // | static arc  | [dyn. arch]  |</span>
<span class="udiff-line-added">+ // +-------------+--------------+</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // Base address: If use_archive_base_addr address is true, the Base address is</span>
<span class="udiff-line-added">+ //  determined by the address stored in the static archive. If</span>
<span class="udiff-line-added">+ //  use_archive_base_addr address is false, this base address is determined</span>
<span class="udiff-line-added">+ //  by the platform.</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // If UseCompressedClassPointers=1, the range encompassing both spaces will be</span>
<span class="udiff-line-added">+ //  suitable to en/decode narrow Klass pointers: the base will be valid for</span>
<span class="udiff-line-added">+ //  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // Return:</span>
<span class="udiff-line-added">+ //</span>
<span class="udiff-line-added">+ // - On success:</span>
<span class="udiff-line-added">+ //    - archive_space_rs will be reserved and large enough to host static and</span>
<span class="udiff-line-added">+ //      if needed dynamic archive: [Base, A).</span>
<span class="udiff-line-added">+ //      archive_space_rs.base and size will be aligned to CDS reserve</span>
<span class="udiff-line-added">+ //      granularity.</span>
<span class="udiff-line-added">+ //    - class_space_rs: If UseCompressedClassPointers=1, class_space_rs will</span>
<span class="udiff-line-added">+ //      be reserved. Its start address will be aligned to metaspace reserve</span>
<span class="udiff-line-added">+ //      alignment, which may differ from CDS alignment. It will follow the cds</span>
<span class="udiff-line-added">+ //      archive space, close enough such that narrow class pointer encoding</span>
<span class="udiff-line-added">+ //      covers both spaces.</span>
<span class="udiff-line-added">+ //      If UseCompressedClassPointers=0, class_space_rs remains unreserved.</span>
<span class="udiff-line-added">+ // - On error: NULL is returned and the spaces remain unreserved.</span>
  char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
                                                            FileMapInfo* dynamic_mapinfo,
<span class="udiff-line-modified-removed">-                                                           bool use_requested_addr,</span>
<span class="udiff-line-removed">-                                                           ReservedSpace&amp; main_rs,</span>
<span class="udiff-line-modified-added">+                                                           bool use_archive_base_addr,</span>
                                                            ReservedSpace&amp; archive_space_rs,
                                                            ReservedSpace&amp; class_space_rs) {
<span class="udiff-line-removed">-   const bool use_klass_space = NOT_LP64(false) LP64_ONLY(Metaspace::using_class_space());</span>
<span class="udiff-line-removed">-   const size_t class_space_size = NOT_LP64(0) LP64_ONLY(Metaspace::compressed_class_space_size());</span>
  
<span class="udiff-line-modified-removed">-   if (use_klass_space) {</span>
<span class="udiff-line-modified-removed">-     assert(class_space_size &gt; 0, &quot;CompressedClassSpaceSize must have been validated&quot;);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">-   if (use_requested_addr &amp;&amp; !is_aligned(static_mapinfo-&gt;requested_base_address(), reserved_space_alignment())) {</span>
<span class="udiff-line-removed">-     return NULL;</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-modified-added">+   address const base_address = (address) (use_archive_base_addr ? static_mapinfo-&gt;requested_base_address() : NULL);</span>
<span class="udiff-line-modified-added">+   const size_t archive_space_alignment = MetaspaceShared::reserved_space_alignment();</span>
  
    // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="udiff-line-modified-removed">-   size_t base_offset = static_mapinfo-&gt;mapping_base_offset();</span>
<span class="udiff-line-modified-removed">-   size_t end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="udiff-line-modified-removed">-   assert(base_offset == 0, &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">-   assert(is_aligned(end_offset,  os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">-   assert(is_aligned(base_offset, os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   // In case reserved_space_alignment() != os::vm_allocation_granularity()</span>
<span class="udiff-line-modified-removed">-   assert((size_t)os::vm_allocation_granularity() &lt;= reserved_space_alignment(), &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">-   end_offset = align_up(end_offset, reserved_space_alignment());</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   size_t archive_space_size = end_offset - base_offset;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   // Special handling for Windows because it cannot mmap into a reserved space:</span>
<span class="udiff-line-removed">-   //    use_requested_addr: We just map each region individually, and give up if any one of them fails.</span>
<span class="udiff-line-removed">-   //   !use_requested_addr: We reserve the space first, and then os::read in all the regions (instead of mmap).</span>
<span class="udiff-line-removed">-   //                        We&#39;re going to patch all the pointers anyway so there&#39;s no benefit for mmap.</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   if (use_requested_addr) {</span>
<span class="udiff-line-removed">-     char* archive_space_base = static_mapinfo-&gt;requested_base_address() + base_offset;</span>
<span class="udiff-line-removed">-     char* archive_space_end  = archive_space_base + archive_space_size;</span>
<span class="udiff-line-removed">-     if (!MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="udiff-line-removed">-       archive_space_rs = reserve_shared_space(archive_space_size, archive_space_base);</span>
<span class="udiff-line-removed">-       if (!archive_space_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-         return NULL;</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     if (use_klass_space) {</span>
<span class="udiff-line-removed">-       // Make sure we can map the klass space immediately following the archive_space space</span>
<span class="udiff-line-removed">-       // Don&#39;t call reserve_shared_space here as that may try to enforce platform-specific</span>
<span class="udiff-line-removed">-       // alignment rules which only apply to the archive base address</span>
<span class="udiff-line-removed">-       char* class_space_base = archive_space_end;</span>
<span class="udiff-line-removed">-       class_space_rs = ReservedSpace(class_space_size, reserved_space_alignment(),</span>
<span class="udiff-line-removed">-                                      false /* large_pages */, class_space_base);</span>
<span class="udiff-line-removed">-       if (!class_space_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-         return NULL;</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     return static_mapinfo-&gt;requested_base_address();</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     if (use_klass_space) {</span>
<span class="udiff-line-removed">-       main_rs = reserve_shared_space(archive_space_size + class_space_size);</span>
<span class="udiff-line-removed">-       if (main_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-         archive_space_rs = main_rs.first_part(archive_space_size, reserved_space_alignment(), /*split=*/true);</span>
<span class="udiff-line-removed">-         class_space_rs = main_rs.last_part(archive_space_size);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-     } else {</span>
<span class="udiff-line-removed">-       main_rs = reserve_shared_space(archive_space_size);</span>
<span class="udiff-line-removed">-       archive_space_rs = main_rs;</span>
<span class="udiff-line-modified-added">+   assert(static_mapinfo-&gt;mapping_base_offset() == 0, &quot;Must be&quot;);</span>
<span class="udiff-line-modified-added">+   size_t archive_end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="udiff-line-modified-added">+   size_t archive_space_size = align_up(archive_end_offset, archive_space_alignment);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+   // If a base address is given, it must have valid alignment and be suitable as encoding base.</span>
<span class="udiff-line-modified-added">+   if (base_address != NULL) {</span>
<span class="udiff-line-modified-added">+     assert(is_aligned(base_address, archive_space_alignment),</span>
<span class="udiff-line-modified-added">+            &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>
<span class="udiff-line-modified-added">+     if (Metaspace::using_class_space()) {</span>
<span class="udiff-line-modified-added">+       assert(CompressedKlassPointers::is_valid_base(base_address),</span>
<span class="udiff-line-modified-added">+              &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>
      }
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (!Metaspace::using_class_space()) {</span>
<span class="udiff-line-added">+     // Get the simple case out of the way first:</span>
<span class="udiff-line-added">+     // no compressed class space, simple allocation.</span>
<span class="udiff-line-added">+     archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,</span>
<span class="udiff-line-added">+                                      false /* bool large */, (char*)base_address);</span>
      if (archive_space_rs.is_reserved()) {
<span class="udiff-line-added">+       assert(base_address == NULL ||</span>
<span class="udiff-line-added">+              (address)archive_space_rs.base() == base_address, &quot;Sanity&quot;);</span>
        return archive_space_rs.base();
<span class="udiff-line-removed">-     } else {</span>
<span class="udiff-line-removed">-       return NULL;</span>
      }
<span class="udiff-line-added">+     return NULL;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ #ifdef _LP64</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Complex case: two spaces adjacent to each other, both to be addressable</span>
<span class="udiff-line-added">+   //  with narrow class pointers.</span>
<span class="udiff-line-added">+   // We reserve the whole range spanning both spaces, then split that range up.</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   const size_t class_space_alignment = Metaspace::reserve_alignment();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // To simplify matters, lets assume that metaspace alignment will always be</span>
<span class="udiff-line-added">+   //  equal or a multiple of archive alignment.</span>
<span class="udiff-line-added">+   assert(is_power_of_2(class_space_alignment) &amp;&amp;</span>
<span class="udiff-line-added">+                        is_power_of_2(archive_space_alignment) &amp;&amp;</span>
<span class="udiff-line-added">+                        class_space_alignment &gt;= archive_space_alignment,</span>
<span class="udiff-line-added">+                        &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   const size_t class_space_size = CompressedClassSpaceSize;</span>
<span class="udiff-line-added">+   assert(CompressedClassSpaceSize &gt; 0 &amp;&amp;</span>
<span class="udiff-line-added">+          is_aligned(CompressedClassSpaceSize, class_space_alignment),</span>
<span class="udiff-line-added">+          &quot;CompressedClassSpaceSize malformed: &quot;</span>
<span class="udiff-line-added">+          SIZE_FORMAT, CompressedClassSpaceSize);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   const size_t ccs_begin_offset = align_up(archive_space_size,</span>
<span class="udiff-line-added">+                                            class_space_alignment);</span>
<span class="udiff-line-added">+   const size_t gap_size = ccs_begin_offset - archive_space_size;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   const size_t total_range_size =</span>
<span class="udiff-line-added">+       align_up(archive_space_size + gap_size + class_space_size,</span>
<span class="udiff-line-added">+                os::vm_allocation_granularity());</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   ReservedSpace total_rs;</span>
<span class="udiff-line-added">+   if (base_address != NULL) {</span>
<span class="udiff-line-added">+     // Reserve at the given archive base address, or not at all.</span>
<span class="udiff-line-added">+     total_rs = ReservedSpace(total_range_size, archive_space_alignment,</span>
<span class="udiff-line-added">+                              false /* bool large */, (char*) base_address);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     // Reserve at any address, but leave it up to the platform to choose a good one.</span>
<span class="udiff-line-added">+     total_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size);</span>
    }
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (!total_rs.is_reserved()) {</span>
<span class="udiff-line-added">+     return NULL;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Paranoid checks:</span>
<span class="udiff-line-added">+   assert(base_address == NULL || (address)total_rs.base() == base_address,</span>
<span class="udiff-line-added">+          &quot;Sanity (&quot; PTR_FORMAT &quot; vs &quot; PTR_FORMAT &quot;)&quot;, p2i(base_address), p2i(total_rs.base()));</span>
<span class="udiff-line-added">+   assert(is_aligned(total_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+   assert(total_rs.size() == total_range_size, &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+   assert(CompressedKlassPointers::is_valid_base((address)total_rs.base()), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Now split up the space into ccs and cds archive. For simplicity, just leave</span>
<span class="udiff-line-added">+   //  the gap reserved at the end of the archive space.</span>
<span class="udiff-line-added">+   archive_space_rs = total_rs.first_part(ccs_begin_offset,</span>
<span class="udiff-line-added">+                                          (size_t)os::vm_allocation_granularity(),</span>
<span class="udiff-line-added">+                                          /*split=*/true);</span>
<span class="udiff-line-added">+   class_space_rs = total_rs.last_part(ccs_begin_offset);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   assert(is_aligned(archive_space_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+   assert(is_aligned(archive_space_rs.size(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+   assert(is_aligned(class_space_rs.base(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+   assert(is_aligned(class_space_rs.size(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   return archive_space_rs.base();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ #else</span>
<span class="udiff-line-added">+   ShouldNotReachHere();</span>
<span class="udiff-line-added">+   return NULL;</span>
<span class="udiff-line-added">+ #endif</span>
<span class="udiff-line-added">+ </span>
  }
  
<span class="udiff-line-modified-removed">- void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; main_rs,</span>
<span class="udiff-line-removed">-                                               ReservedSpace&amp; archive_space_rs,</span>
<span class="udiff-line-modified-added">+ void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; archive_space_rs,</span>
                                                ReservedSpace&amp; class_space_rs) {
<span class="udiff-line-modified-removed">-   if (main_rs.is_reserved()) {</span>
<span class="udiff-line-modified-removed">-     assert(main_rs.contains(archive_space_rs.base()), &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">-     assert(main_rs.contains(class_space_rs.base()), &quot;must be&quot;);</span>
<span class="udiff-line-modified-removed">-     log_debug(cds)(&quot;Released shared space (archive+classes) &quot; INTPTR_FORMAT, p2i(main_rs.base()));</span>
<span class="udiff-line-modified-removed">-     main_rs.release();</span>
<span class="udiff-line-modified-removed">-   } else {</span>
<span class="udiff-line-modified-removed">-     if (archive_space_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-       log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="udiff-line-removed">-       archive_space_rs.release();</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     if (class_space_rs.is_reserved()) {</span>
<span class="udiff-line-removed">-       log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="udiff-line-removed">-       class_space_rs.release();</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-modified-added">+   if (archive_space_rs.is_reserved()) {</span>
<span class="udiff-line-modified-added">+     log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="udiff-line-modified-added">+     archive_space_rs.release();</span>
<span class="udiff-line-modified-added">+   }</span>
<span class="udiff-line-modified-added">+   if (class_space_rs.is_reserved()) {</span>
<span class="udiff-line-modified-added">+     log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="udiff-line-modified-added">+     class_space_rs.release();</span>
    }
  }
  
  static int archive_regions[]  = {MetaspaceShared::mc,
                                   MetaspaceShared::rw,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2474,5 +2662,33 @@</span>
  // Arguments::default_SharedBaseAddress() without runtime relocation.
  intx MetaspaceShared::final_delta() {
    return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
         - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
  }
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MetaspaceShared::print_on(outputStream* st) {</span>
<span class="udiff-line-added">+   if (UseSharedSpaces || DumpSharedSpaces) {</span>
<span class="udiff-line-added">+     st-&gt;print(&quot;CDS archive(s) mapped at: &quot;);</span>
<span class="udiff-line-added">+     address base;</span>
<span class="udiff-line-added">+     address top;</span>
<span class="udiff-line-added">+     if (UseSharedSpaces) { // Runtime</span>
<span class="udiff-line-added">+       base = (address)MetaspaceObj::shared_metaspace_base();</span>
<span class="udiff-line-added">+       address static_top = (address)_shared_metaspace_static_top;</span>
<span class="udiff-line-added">+       top = (address)MetaspaceObj::shared_metaspace_top();</span>
<span class="udiff-line-added">+       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(static_top), p2i(top));</span>
<span class="udiff-line-added">+     } else if (DumpSharedSpaces) { // Dump Time</span>
<span class="udiff-line-added">+       base = (address)_shared_rs.base();</span>
<span class="udiff-line-added">+       top = (address)_shared_rs.end();</span>
<span class="udiff-line-added">+       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(top));</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     st-&gt;print(&quot;size &quot; SIZE_FORMAT &quot;, &quot;, top - base);</span>
<span class="udiff-line-added">+     st-&gt;print(&quot;SharedBaseAddress: &quot; PTR_FORMAT &quot;, ArchiveRelocationMode: %d.&quot;, SharedBaseAddress, (int)ArchiveRelocationMode);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     st-&gt;print(&quot;CDS disabled.&quot;);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   st-&gt;cr();</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ </span>
</pre>
<center><a href="metaspace/virtualSpaceNode.hpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>