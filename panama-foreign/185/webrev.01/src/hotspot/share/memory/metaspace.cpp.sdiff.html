<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/metaspace.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="filemap.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspace.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 956 
 957 // Metaspace methods
 958 
 959 size_t Metaspace::_first_chunk_word_size = 0;
 960 size_t Metaspace::_first_class_chunk_word_size = 0;
 961 
 962 size_t Metaspace::_commit_alignment = 0;
 963 size_t Metaspace::_reserve_alignment = 0;
 964 
 965 VirtualSpaceList* Metaspace::_space_list = NULL;
 966 VirtualSpaceList* Metaspace::_class_space_list = NULL;
 967 
 968 ChunkManager* Metaspace::_chunk_manager_metadata = NULL;
 969 ChunkManager* Metaspace::_chunk_manager_class = NULL;
 970 
 971 bool Metaspace::_initialized = false;
 972 
 973 #define VIRTUALSPACEMULTIPLIER 2
 974 
 975 #ifdef _LP64
<span class="line-removed"> 976 static const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);</span>
<span class="line-removed"> 977 </span>
<span class="line-removed"> 978 void Metaspace::set_narrow_klass_base_and_shift(ReservedSpace metaspace_rs, address cds_base) {</span>
<span class="line-removed"> 979   assert(!DumpSharedSpaces, &quot;narrow_klass is set by MetaspaceShared class.&quot;);</span>
<span class="line-removed"> 980   // Figure out the narrow_klass_base and the narrow_klass_shift.  The</span>
<span class="line-removed"> 981   // narrow_klass_base is the lower of the metaspace base and the cds base</span>
<span class="line-removed"> 982   // (if cds is enabled).  The narrow_klass_shift depends on the distance</span>
<span class="line-removed"> 983   // between the lower base and higher address.</span>
<span class="line-removed"> 984   address lower_base = (address)metaspace_rs.base();</span>
<span class="line-removed"> 985   address higher_address = (address)metaspace_rs.end();</span>
<span class="line-removed"> 986   if (cds_base != NULL) {</span>
<span class="line-removed"> 987     assert(UseSharedSpaces, &quot;must be&quot;);</span>
<span class="line-removed"> 988     lower_base = MIN2(lower_base, cds_base);</span>
<span class="line-removed"> 989   } else {</span>
<span class="line-removed"> 990     uint64_t klass_encoding_max = UnscaledClassSpaceMax &lt;&lt; LogKlassAlignmentInBytes;</span>
<span class="line-removed"> 991     // If compressed class space fits in lower 32G, we don&#39;t need a base.</span>
<span class="line-removed"> 992     if (higher_address &lt;= (address)klass_encoding_max) {</span>
<span class="line-removed"> 993       lower_base = 0; // Effectively lower base is zero.</span>
<span class="line-removed"> 994     }</span>
<span class="line-removed"> 995   }</span>
<span class="line-removed"> 996 </span>
<span class="line-removed"> 997   CompressedKlassPointers::set_base(lower_base);</span>
<span class="line-removed"> 998 </span>
<span class="line-removed"> 999   // CDS uses LogKlassAlignmentInBytes for narrow_klass_shift. See</span>
<span class="line-removed">1000   // MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() for</span>
<span class="line-removed">1001   // how dump time narrow_klass_shift is set. Although, CDS can work</span>
<span class="line-removed">1002   // with zero-shift mode also, to be consistent with AOT it uses</span>
<span class="line-removed">1003   // LogKlassAlignmentInBytes for klass shift so archived java heap objects</span>
<span class="line-removed">1004   // can be used at same time as AOT code.</span>
<span class="line-removed">1005   if (!UseSharedSpaces</span>
<span class="line-removed">1006       &amp;&amp; (uint64_t)(higher_address - lower_base) &lt;= UnscaledClassSpaceMax) {</span>
<span class="line-removed">1007     CompressedKlassPointers::set_shift(0);</span>
<span class="line-removed">1008   } else {</span>
<span class="line-removed">1009     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="line-removed">1010   }</span>
<span class="line-removed">1011   AOTLoader::set_narrow_klass_shift();</span>
<span class="line-removed">1012 }</span>
1013 
<span class="line-modified">1014 // Try to allocate the metaspace at the requested addr.</span>
<span class="line-removed">1015 void Metaspace::allocate_metaspace_compressed_klass_ptrs(ReservedSpace metaspace_rs, char* requested_addr, address cds_base) {</span>
<span class="line-removed">1016   assert(!DumpSharedSpaces, &quot;compress klass space is allocated by MetaspaceShared class.&quot;);</span>
<span class="line-removed">1017   assert(using_class_space(), &quot;called improperly&quot;);</span>
<span class="line-removed">1018   assert(UseCompressedClassPointers, &quot;Only use with CompressedKlassPtrs&quot;);</span>
<span class="line-removed">1019   assert(compressed_class_space_size() &lt; KlassEncodingMetaspaceMax,</span>
<span class="line-removed">1020          &quot;Metaspace size is too big&quot;);</span>
<span class="line-removed">1021   assert_is_aligned(requested_addr, _reserve_alignment);</span>
<span class="line-removed">1022   assert_is_aligned(cds_base, _reserve_alignment);</span>
<span class="line-removed">1023   assert_is_aligned(compressed_class_space_size(), _reserve_alignment);</span>
<span class="line-removed">1024 </span>
<span class="line-removed">1025   if (metaspace_rs.is_reserved()) {</span>
<span class="line-removed">1026     // CDS should have already reserved the space.</span>
<span class="line-removed">1027     assert(requested_addr == NULL, &quot;not used&quot;);</span>
<span class="line-removed">1028     assert(cds_base != NULL, &quot;CDS should have already reserved the memory space&quot;);</span>
<span class="line-removed">1029   } else {</span>
<span class="line-removed">1030     assert(cds_base == NULL, &quot;must be&quot;);</span>
<span class="line-removed">1031     metaspace_rs = reserve_space(compressed_class_space_size(),</span>
<span class="line-removed">1032                                  _reserve_alignment, requested_addr,</span>
<span class="line-removed">1033                                  false /* use_requested_addr */);</span>
<span class="line-removed">1034   }</span>
<span class="line-removed">1035 </span>
<span class="line-removed">1036   if (!metaspace_rs.is_reserved()) {</span>
<span class="line-removed">1037     assert(cds_base == NULL, &quot;CDS should have already reserved the memory space&quot;);</span>
<span class="line-removed">1038     // If no successful allocation then try to allocate the space anywhere.  If</span>
<span class="line-removed">1039     // that fails then OOM doom.  At this point we cannot try allocating the</span>
<span class="line-removed">1040     // metaspace as if UseCompressedClassPointers is off because too much</span>
<span class="line-removed">1041     // initialization has happened that depends on UseCompressedClassPointers.</span>
<span class="line-removed">1042     // So, UseCompressedClassPointers cannot be turned off at this point.</span>
<span class="line-removed">1043     metaspace_rs = reserve_space(compressed_class_space_size(),</span>
<span class="line-removed">1044                                  _reserve_alignment, NULL, false);</span>
<span class="line-removed">1045     if (!metaspace_rs.is_reserved()) {</span>
<span class="line-removed">1046       vm_exit_during_initialization(err_msg(&quot;Could not allocate metaspace: &quot; SIZE_FORMAT &quot; bytes&quot;,</span>
<span class="line-removed">1047                                             compressed_class_space_size()));</span>
<span class="line-removed">1048     }</span>
<span class="line-removed">1049   }</span>
<span class="line-removed">1050 </span>
<span class="line-removed">1051   if (cds_base == NULL) {</span>
<span class="line-removed">1052     // If we got here then the metaspace got allocated.</span>
<span class="line-removed">1053     MemTracker::record_virtual_memory_type((address)metaspace_rs.base(), mtClass);</span>
<span class="line-removed">1054   }</span>
<span class="line-removed">1055 </span>
<span class="line-removed">1056   set_narrow_klass_base_and_shift(metaspace_rs, cds_base);</span>
<span class="line-removed">1057 </span>
<span class="line-removed">1058   initialize_class_space(metaspace_rs);</span>
<span class="line-removed">1059 </span>
<span class="line-removed">1060   LogTarget(Trace, gc, metaspace) lt;</span>
<span class="line-removed">1061   if (lt.is_enabled()) {</span>
<span class="line-removed">1062     ResourceMark rm;</span>
<span class="line-removed">1063     LogStream ls(lt);</span>
<span class="line-removed">1064     print_compressed_class_space(&amp;ls, requested_addr);</span>
<span class="line-removed">1065   }</span>
<span class="line-removed">1066 }</span>
<span class="line-removed">1067 </span>
<span class="line-removed">1068 void Metaspace::print_compressed_class_space(outputStream* st, const char* requested_addr) {</span>
<span class="line-removed">1069   st-&gt;print_cr(&quot;Narrow klass base: &quot; PTR_FORMAT &quot;, Narrow klass shift: %d&quot;,</span>
<span class="line-removed">1070                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
1071   if (_class_space_list != NULL) {
1072     address base = (address)_class_space_list-&gt;current_virtual_space()-&gt;bottom();
<span class="line-modified">1073     st-&gt;print(&quot;Compressed class space size: &quot; SIZE_FORMAT &quot; Address: &quot; PTR_FORMAT,</span>
<span class="line-modified">1074                  compressed_class_space_size(), p2i(base));</span>
<span class="line-modified">1075     if (requested_addr != 0) {</span>
<span class="line-removed">1076       st-&gt;print(&quot; Req Addr: &quot; PTR_FORMAT, p2i(requested_addr));</span>
<span class="line-removed">1077     }</span>
1078     st-&gt;cr();
1079   }
1080 }
1081 
<span class="line-modified">1082 // For UseCompressedClassPointers the class space is reserved above the top of</span>
<span class="line-removed">1083 // the Java heap.  The argument passed in is at the base of the compressed space.</span>
1084 void Metaspace::initialize_class_space(ReservedSpace rs) {
<span class="line-removed">1085   // The reserved space size may be bigger because of alignment, esp with UseLargePages</span>
<span class="line-removed">1086   assert(rs.size() &gt;= CompressedClassSpaceSize,</span>
<span class="line-removed">1087          SIZE_FORMAT &quot; != &quot; SIZE_FORMAT, rs.size(), CompressedClassSpaceSize);</span>
1088   assert(using_class_space(), &quot;Must be using class space&quot;);








1089   _class_space_list = new VirtualSpaceList(rs);
1090   _chunk_manager_class = new ChunkManager(true/*is_class*/);
1091 





1092   if (!_class_space_list-&gt;initialization_succeeded()) {
1093     vm_exit_during_initialization(&quot;Failed to setup compressed class space virtual space list.&quot;);
1094   }

1095 }
1096 
<span class="line-modified">1097 #endif // _LP64</span>






1098 
<span class="line-modified">1099 #ifdef PREFERRED_METASPACE_ALIGNMENT</span>
<span class="line-modified">1100 ReservedSpace Metaspace::reserve_preferred_space(size_t size, size_t alignment,</span>
<span class="line-removed">1101                                                  bool large_pages, char *requested_addr,</span>
<span class="line-removed">1102                                                  bool use_requested_addr) {</span>
<span class="line-removed">1103   // Our compressed klass pointers may fit nicely into the lower 32 bits.</span>
<span class="line-removed">1104   if (requested_addr != NULL &amp;&amp; (uint64_t)requested_addr + size &lt; 4*G) {</span>
<span class="line-removed">1105     ReservedSpace rs(size, alignment, large_pages, requested_addr);</span>
<span class="line-removed">1106     if (rs.is_reserved() || use_requested_addr) {</span>
<span class="line-removed">1107       return rs;</span>
<span class="line-removed">1108     }</span>
<span class="line-removed">1109   }</span>
<span class="line-removed">1110 </span>
<span class="line-removed">1111   struct SearchParams { uintptr_t limit; size_t increment; };</span>
1112 
1113   // AArch64: Try to align metaspace so that we can decode a compressed
1114   // klass with a single MOVK instruction. We can do this iff the
1115   // compressed class base is a multiple of 4G.
<span class="line-modified">1116   // Aix: Search for a place where we can find memory. If we need to load</span>
<span class="line-modified">1117   // the base, 4G alignment is helpful, too.</span>
<span class="line-modified">1118 </span>
<span class="line-modified">1119   // Go faster above 32G as it is no longer possible to use a zero base.</span>
<span class="line-modified">1120   // AArch64: Additionally, ensure the lower LogKlassAlignmentInBytes</span>
<span class="line-modified">1121   // bits of the upper 32-bits of the address are zero so we can handle</span>
<span class="line-modified">1122   // a shift when decoding.</span>
<span class="line-modified">1123 </span>
<span class="line-modified">1124   static const SearchParams search_params[] = {</span>
<span class="line-modified">1125     // Limit    Increment</span>
<span class="line-modified">1126     {  32*G,    AARCH64_ONLY(4*)G,                               },</span>
<span class="line-modified">1127     {  1024*G,  (4 AARCH64_ONLY(&lt;&lt; LogKlassAlignmentInBytes))*G  },</span>
1128   };
1129 
<span class="line-modified">1130   // Null requested_addr means allocate anywhere so ensure the search</span>
<span class="line-modified">1131   // begins from a non-null address.</span>
<span class="line-modified">1132   char *a = MAX2(requested_addr, (char *)search_params[0].increment);</span>
<span class="line-modified">1133 </span>
<span class="line-modified">1134   for (const SearchParams *p = search_params;</span>
<span class="line-modified">1135        p &lt; search_params + ARRAY_SIZE(search_params);</span>
<span class="line-modified">1136        ++p) {</span>
<span class="line-modified">1137     a = align_up(a, p-&gt;increment);</span>
<span class="line-removed">1138     if (use_requested_addr &amp;&amp; a != requested_addr)</span>
<span class="line-removed">1139       return ReservedSpace();</span>
<span class="line-removed">1140 </span>
<span class="line-removed">1141     for (; a &lt; (char *)p-&gt;limit; a += p-&gt;increment) {</span>
<span class="line-removed">1142       ReservedSpace rs(size, alignment, large_pages, a);</span>
<span class="line-removed">1143       if (rs.is_reserved() || use_requested_addr) {</span>
1144         return rs;
1145       }

1146     }
1147   }
1148 


1149   return ReservedSpace();
<span class="line-removed">1150 }</span>
<span class="line-removed">1151 #endif // PREFERRED_METASPACE_ALIGNMENT</span>
<span class="line-removed">1152 </span>
<span class="line-removed">1153 // Try to reserve a region for the metaspace at the requested address. Some</span>
<span class="line-removed">1154 // platforms have particular alignment requirements to allow efficient decode of</span>
<span class="line-removed">1155 // compressed class pointers in which case requested_addr is treated as hint for</span>
<span class="line-removed">1156 // where to start looking unless use_requested_addr is true.</span>
<span class="line-removed">1157 ReservedSpace Metaspace::reserve_space(size_t size, size_t alignment,</span>
<span class="line-removed">1158                                        char* requested_addr, bool use_requested_addr) {</span>
<span class="line-removed">1159   bool large_pages = false; // Don&#39;t use large pages for the class space.</span>
<span class="line-removed">1160   assert(is_aligned(requested_addr, alignment), &quot;must be&quot;);</span>
<span class="line-removed">1161   assert(requested_addr != NULL || !use_requested_addr,</span>
<span class="line-removed">1162          &quot;cannot set use_requested_addr with NULL address&quot;);</span>
<span class="line-removed">1163 </span>
<span class="line-removed">1164 #ifdef PREFERRED_METASPACE_ALIGNMENT</span>
<span class="line-removed">1165   return reserve_preferred_space(size, alignment, large_pages,</span>
<span class="line-removed">1166                                  requested_addr, use_requested_addr);</span>
1167 #else
<span class="line-modified">1168   return ReservedSpace(size, alignment, large_pages, requested_addr);</span>
<span class="line-modified">1169 #endif</span>

1170 }
1171 



1172 void Metaspace::ergo_initialize() {
1173   if (DumpSharedSpaces) {
1174     // Using large pages when dumping the shared archive is currently not implemented.
1175     FLAG_SET_ERGO(UseLargePagesInMetaspace, false);
1176   }
1177 
1178   size_t page_size = os::vm_page_size();
1179   if (UseLargePages &amp;&amp; UseLargePagesInMetaspace) {
1180     page_size = os::large_page_size();
1181   }
1182 
1183   _commit_alignment  = page_size;
1184   _reserve_alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
1185 
1186   // Do not use FLAG_SET_ERGO to update MaxMetaspaceSize, since this will
1187   // override if MaxMetaspaceSize was set on the command line or not.
1188   // This information is needed later to conform to the specification of the
1189   // java.lang.management.MemoryUsage API.
1190   //
1191   // Ideally, we would be able to set the default value of MaxMetaspaceSize in
</pre>
<hr />
<pre>
1212   if (UseCompressedClassPointers) {
1213     if ((min_metaspace_sz + CompressedClassSpaceSize) &gt;  MaxMetaspaceSize) {
1214       if (min_metaspace_sz &gt;= MaxMetaspaceSize) {
1215         vm_exit_during_initialization(&quot;MaxMetaspaceSize is too small.&quot;);
1216       } else {
1217         FLAG_SET_ERGO(CompressedClassSpaceSize,
1218                       MaxMetaspaceSize - min_metaspace_sz);
1219       }
1220     }
1221   } else if (min_metaspace_sz &gt;= MaxMetaspaceSize) {
1222     FLAG_SET_ERGO(InitialBootClassLoaderMetaspaceSize,
1223                   min_metaspace_sz);
1224   }
1225 
1226   set_compressed_class_space_size(CompressedClassSpaceSize);
1227 }
1228 
1229 void Metaspace::global_initialize() {
1230   MetaspaceGC::initialize();
1231 
<span class="line-modified">1232   bool class_space_inited = false;</span>








1233 #if INCLUDE_CDS

1234   if (DumpSharedSpaces) {
1235     MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();
<span class="line-removed">1236     class_space_inited = true;</span>
1237   } else if (UseSharedSpaces) {
1238     // If any of the archived space fails to map, UseSharedSpaces
1239     // is reset to false.
1240     MetaspaceShared::initialize_runtime_shared_and_meta_spaces();
<span class="line-removed">1241     class_space_inited = UseSharedSpaces;</span>
1242   }
1243 
1244   if (DynamicDumpSharedSpaces &amp;&amp; !UseSharedSpaces) {
1245     vm_exit_during_initialization(&quot;DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded&quot;, NULL);
1246   }
1247 #endif // INCLUDE_CDS
1248 
1249 #ifdef _LP64
<span class="line-modified">1250   if (using_class_space() &amp;&amp; !class_space_inited) {</span>
<span class="line-modified">1251     char* base;</span>
<span class="line-modified">1252     if (UseCompressedOops) {</span>
<span class="line-modified">1253       base = (char*)align_up(CompressedOops::end(), _reserve_alignment);</span>
<span class="line-modified">1254     } else {</span>
<span class="line-modified">1255       base = (char*)HeapBaseMinAddress;</span>
















1256     }
<span class="line-modified">1257     ReservedSpace dummy;</span>
<span class="line-modified">1258     allocate_metaspace_compressed_klass_ptrs(dummy, base, 0);</span>
















1259   }

1260 #endif
1261 
1262   // Initialize these before initializing the VirtualSpaceList
1263   _first_chunk_word_size = InitialBootClassLoaderMetaspaceSize / BytesPerWord;
1264   _first_chunk_word_size = align_word_size_up(_first_chunk_word_size);
1265   // Make the first class chunk bigger than a medium chunk so it&#39;s not put
1266   // on the medium chunk list.   The next chunk will be small and progress
1267   // from there.  This size calculated by -version.
1268   _first_class_chunk_word_size = MIN2((size_t)MediumChunk*6,
1269                                      (CompressedClassSpaceSize/BytesPerWord)*2);
1270   _first_class_chunk_word_size = align_word_size_up(_first_class_chunk_word_size);
1271   // Arbitrarily set the initial virtual space to a multiple
1272   // of the boot class loader size.
1273   size_t word_size = VIRTUALSPACEMULTIPLIER * _first_chunk_word_size;
1274   word_size = align_up(word_size, Metaspace::reserve_alignment_words());
1275 
1276   // Initialize the list of virtual spaces.
1277   _space_list = new VirtualSpaceList(word_size);
1278   _chunk_manager_metadata = new ChunkManager(false/*metaspace*/);
1279 
1280   if (!_space_list-&gt;initialization_succeeded()) {
1281     vm_exit_during_initialization(&quot;Unable to setup metadata virtual space list.&quot;, NULL);
1282   }
1283 
1284   _tracer = new MetaspaceTracer();
1285 
1286   _initialized = true;
1287 














1288 }
1289 
1290 void Metaspace::post_initialize() {
1291   MetaspaceGC::post_initialize();
1292 }
1293 
1294 void Metaspace::verify_global_initialization() {
1295   assert(space_list() != NULL, &quot;Metadata VirtualSpaceList has not been initialized&quot;);
1296   assert(chunk_manager_metadata() != NULL, &quot;Metadata ChunkManager has not been initialized&quot;);
1297 
1298   if (using_class_space()) {
1299     assert(class_space_list() != NULL, &quot;Class VirtualSpaceList has not been initialized&quot;);
1300     assert(chunk_manager_class() != NULL, &quot;Class ChunkManager has not been initialized&quot;);
1301   }
1302 }
1303 
1304 size_t Metaspace::align_word_size_up(size_t word_size) {
1305   size_t byte_size = word_size * wordSize;
1306   return ReservedSpace::allocation_align_size_up(byte_size) / wordSize;
1307 }
</pre>
</td>
<td>
<hr />
<pre>
 956 
 957 // Metaspace methods
 958 
 959 size_t Metaspace::_first_chunk_word_size = 0;
 960 size_t Metaspace::_first_class_chunk_word_size = 0;
 961 
 962 size_t Metaspace::_commit_alignment = 0;
 963 size_t Metaspace::_reserve_alignment = 0;
 964 
 965 VirtualSpaceList* Metaspace::_space_list = NULL;
 966 VirtualSpaceList* Metaspace::_class_space_list = NULL;
 967 
 968 ChunkManager* Metaspace::_chunk_manager_metadata = NULL;
 969 ChunkManager* Metaspace::_chunk_manager_class = NULL;
 970 
 971 bool Metaspace::_initialized = false;
 972 
 973 #define VIRTUALSPACEMULTIPLIER 2
 974 
 975 #ifdef _LP64





































 976 
<span class="line-modified"> 977 void Metaspace::print_compressed_class_space(outputStream* st) {</span>
























































 978   if (_class_space_list != NULL) {
 979     address base = (address)_class_space_list-&gt;current_virtual_space()-&gt;bottom();
<span class="line-modified"> 980     address top = base + compressed_class_space_size();</span>
<span class="line-modified"> 981     st-&gt;print(&quot;Compressed class space mapped at: &quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;, size: &quot; SIZE_FORMAT,</span>
<span class="line-modified"> 982                p2i(base), p2i(top), top - base);</span>


 983     st-&gt;cr();
 984   }
 985 }
 986 
<span class="line-modified"> 987 // Given a prereserved space, use that to set up the compressed class space list.</span>

 988 void Metaspace::initialize_class_space(ReservedSpace rs) {



 989   assert(using_class_space(), &quot;Must be using class space&quot;);
<span class="line-added"> 990   assert(_class_space_list == NULL &amp;&amp; _chunk_manager_class == NULL, &quot;Only call once&quot;);</span>
<span class="line-added"> 991 </span>
<span class="line-added"> 992   assert(rs.size() == CompressedClassSpaceSize, SIZE_FORMAT &quot; != &quot; SIZE_FORMAT,</span>
<span class="line-added"> 993          rs.size(), CompressedClassSpaceSize);</span>
<span class="line-added"> 994   assert(is_aligned(rs.base(), Metaspace::reserve_alignment()) &amp;&amp;</span>
<span class="line-added"> 995          is_aligned(rs.size(), Metaspace::reserve_alignment()),</span>
<span class="line-added"> 996          &quot;wrong alignment&quot;);</span>
<span class="line-added"> 997 </span>
 998   _class_space_list = new VirtualSpaceList(rs);
 999   _chunk_manager_class = new ChunkManager(true/*is_class*/);
1000 
<span class="line-added">1001   // This does currently not work because rs may be the result of a split</span>
<span class="line-added">1002   // operation and NMT seems not to be able to handle splits.</span>
<span class="line-added">1003   // Will be fixed with JDK-8243535.</span>
<span class="line-added">1004   // MemTracker::record_virtual_memory_type((address)rs.base(), mtClass);</span>
<span class="line-added">1005 </span>
1006   if (!_class_space_list-&gt;initialization_succeeded()) {
1007     vm_exit_during_initialization(&quot;Failed to setup compressed class space virtual space list.&quot;);
1008   }
<span class="line-added">1009 </span>
1010 }
1011 
<span class="line-modified">1012 // Reserve a range of memory at an address suitable for en/decoding narrow</span>
<span class="line-added">1013 // Klass pointers (see: CompressedClassPointers::is_valid_base()).</span>
<span class="line-added">1014 // The returned address shall both be suitable as a compressed class pointers</span>
<span class="line-added">1015 //  base, and aligned to Metaspace::reserve_alignment (which is equal to or a</span>
<span class="line-added">1016 //  multiple of allocation granularity).</span>
<span class="line-added">1017 // On error, returns an unreserved space.</span>
<span class="line-added">1018 ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size) {</span>
1019 
<span class="line-modified">1020 #ifdef AARCH64</span>
<span class="line-modified">1021   const size_t alignment = Metaspace::reserve_alignment();</span>











1022 
1023   // AArch64: Try to align metaspace so that we can decode a compressed
1024   // klass with a single MOVK instruction. We can do this iff the
1025   // compressed class base is a multiple of 4G.
<span class="line-modified">1026   // Additionally, above 32G, ensure the lower LogKlassAlignmentInBytes bits</span>
<span class="line-modified">1027   // of the upper 32-bits of the address are zero so we can handle a shift</span>
<span class="line-modified">1028   // when decoding.</span>
<span class="line-modified">1029 </span>
<span class="line-modified">1030   static const struct {</span>
<span class="line-modified">1031     address from;</span>
<span class="line-modified">1032     address to;</span>
<span class="line-modified">1033     size_t increment;</span>
<span class="line-modified">1034   } search_ranges[] = {</span>
<span class="line-modified">1035     {  (address)(4*G),   (address)(32*G),   4*G, },</span>
<span class="line-modified">1036     {  (address)(32*G),  (address)(1024*G), (4 &lt;&lt; LogKlassAlignmentInBytes) * G },</span>
<span class="line-modified">1037     {  NULL, NULL, 0 }</span>
1038   };
1039 
<span class="line-modified">1040   for (int i = 0; search_ranges[i].from != NULL; i ++) {</span>
<span class="line-modified">1041     address a = search_ranges[i].from;</span>
<span class="line-modified">1042     assert(CompressedKlassPointers::is_valid_base(a), &quot;Sanity&quot;);</span>
<span class="line-modified">1043     while (a &lt; search_ranges[i].to) {</span>
<span class="line-modified">1044       ReservedSpace rs(size, Metaspace::reserve_alignment(),</span>
<span class="line-modified">1045                        false /*large_pages*/, (char*)a);</span>
<span class="line-modified">1046       if (rs.is_reserved()) {</span>
<span class="line-modified">1047         assert(a == (address)rs.base(), &quot;Sanity&quot;);</span>






1048         return rs;
1049       }
<span class="line-added">1050       a +=  search_ranges[i].increment;</span>
1051     }
1052   }
1053 
<span class="line-added">1054   // Note: on AARCH64, if the code above does not find any good placement, we</span>
<span class="line-added">1055   // have no recourse. We return an empty space and the VM will exit.</span>
1056   return ReservedSpace();

















1057 #else
<span class="line-modified">1058   // Default implementation: Just reserve anywhere.</span>
<span class="line-modified">1059   return ReservedSpace(size, Metaspace::reserve_alignment(), false, (char*)NULL);</span>
<span class="line-added">1060 #endif // AARCH64</span>
1061 }
1062 
<span class="line-added">1063 #endif // _LP64</span>
<span class="line-added">1064 </span>
<span class="line-added">1065 </span>
1066 void Metaspace::ergo_initialize() {
1067   if (DumpSharedSpaces) {
1068     // Using large pages when dumping the shared archive is currently not implemented.
1069     FLAG_SET_ERGO(UseLargePagesInMetaspace, false);
1070   }
1071 
1072   size_t page_size = os::vm_page_size();
1073   if (UseLargePages &amp;&amp; UseLargePagesInMetaspace) {
1074     page_size = os::large_page_size();
1075   }
1076 
1077   _commit_alignment  = page_size;
1078   _reserve_alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
1079 
1080   // Do not use FLAG_SET_ERGO to update MaxMetaspaceSize, since this will
1081   // override if MaxMetaspaceSize was set on the command line or not.
1082   // This information is needed later to conform to the specification of the
1083   // java.lang.management.MemoryUsage API.
1084   //
1085   // Ideally, we would be able to set the default value of MaxMetaspaceSize in
</pre>
<hr />
<pre>
1106   if (UseCompressedClassPointers) {
1107     if ((min_metaspace_sz + CompressedClassSpaceSize) &gt;  MaxMetaspaceSize) {
1108       if (min_metaspace_sz &gt;= MaxMetaspaceSize) {
1109         vm_exit_during_initialization(&quot;MaxMetaspaceSize is too small.&quot;);
1110       } else {
1111         FLAG_SET_ERGO(CompressedClassSpaceSize,
1112                       MaxMetaspaceSize - min_metaspace_sz);
1113       }
1114     }
1115   } else if (min_metaspace_sz &gt;= MaxMetaspaceSize) {
1116     FLAG_SET_ERGO(InitialBootClassLoaderMetaspaceSize,
1117                   min_metaspace_sz);
1118   }
1119 
1120   set_compressed_class_space_size(CompressedClassSpaceSize);
1121 }
1122 
1123 void Metaspace::global_initialize() {
1124   MetaspaceGC::initialize();
1125 
<span class="line-modified">1126   // If UseCompressedClassPointers=1, we have two cases:</span>
<span class="line-added">1127   // a) if CDS is active (either dump time or runtime), it will create the ccs</span>
<span class="line-added">1128   //    for us, initialize it and set up CompressedKlassPointers encoding.</span>
<span class="line-added">1129   //    Class space will be reserved above the mapped archives.</span>
<span class="line-added">1130   // b) if CDS is not active, we will create the ccs on our own. It will be</span>
<span class="line-added">1131   //    placed above the java heap, since we assume it has been placed in low</span>
<span class="line-added">1132   //    address regions. We may rethink this (see JDK-8244943). Failing that,</span>
<span class="line-added">1133   //    it will be placed anywhere.</span>
<span class="line-added">1134 </span>
1135 #if INCLUDE_CDS
<span class="line-added">1136   // case (a)</span>
1137   if (DumpSharedSpaces) {
1138     MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();

1139   } else if (UseSharedSpaces) {
1140     // If any of the archived space fails to map, UseSharedSpaces
1141     // is reset to false.
1142     MetaspaceShared::initialize_runtime_shared_and_meta_spaces();

1143   }
1144 
1145   if (DynamicDumpSharedSpaces &amp;&amp; !UseSharedSpaces) {
1146     vm_exit_during_initialization(&quot;DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded&quot;, NULL);
1147   }
1148 #endif // INCLUDE_CDS
1149 
1150 #ifdef _LP64
<span class="line-modified">1151 </span>
<span class="line-modified">1152   if (using_class_space() &amp;&amp; !class_space_is_initialized()) {</span>
<span class="line-modified">1153     assert(!UseSharedSpaces &amp;&amp; !DumpSharedSpaces, &quot;CDS should be off at this point&quot;);</span>
<span class="line-modified">1154 </span>
<span class="line-modified">1155     // case (b)</span>
<span class="line-modified">1156     ReservedSpace rs;</span>
<span class="line-added">1157 </span>
<span class="line-added">1158     // If UseCompressedOops=1, java heap may have been placed in coops-friendly</span>
<span class="line-added">1159     //  territory already (lower address regions), so we attempt to place ccs</span>
<span class="line-added">1160     //  right above the java heap.</span>
<span class="line-added">1161     // If UseCompressedOops=0, the heap has been placed anywhere - probably in</span>
<span class="line-added">1162     //  high memory regions. In that case, try to place ccs at the lowest allowed</span>
<span class="line-added">1163     //  mapping address.</span>
<span class="line-added">1164     address base = UseCompressedOops ? CompressedOops::end() : (address)HeapBaseMinAddress;</span>
<span class="line-added">1165     base = align_up(base, Metaspace::reserve_alignment());</span>
<span class="line-added">1166 </span>
<span class="line-added">1167     const size_t size = align_up(CompressedClassSpaceSize, Metaspace::reserve_alignment());</span>
<span class="line-added">1168     if (base != NULL) {</span>
<span class="line-added">1169       if (CompressedKlassPointers::is_valid_base(base)) {</span>
<span class="line-added">1170         rs = ReservedSpace(size, Metaspace::reserve_alignment(),</span>
<span class="line-added">1171                            false /* large */, (char*)base);</span>
<span class="line-added">1172       }</span>
1173     }
<span class="line-modified">1174 </span>
<span class="line-modified">1175     // ...failing that, reserve anywhere, but let platform do optimized placement:</span>
<span class="line-added">1176     if (!rs.is_reserved()) {</span>
<span class="line-added">1177       rs = Metaspace::reserve_address_space_for_compressed_classes(size);</span>
<span class="line-added">1178     }</span>
<span class="line-added">1179 </span>
<span class="line-added">1180     // ...failing that, give up.</span>
<span class="line-added">1181     if (!rs.is_reserved()) {</span>
<span class="line-added">1182       vm_exit_during_initialization(</span>
<span class="line-added">1183           err_msg(&quot;Could not allocate compressed class space: &quot; SIZE_FORMAT &quot; bytes&quot;,</span>
<span class="line-added">1184                    compressed_class_space_size()));</span>
<span class="line-added">1185     }</span>
<span class="line-added">1186 </span>
<span class="line-added">1187     // Initialize space</span>
<span class="line-added">1188     Metaspace::initialize_class_space(rs);</span>
<span class="line-added">1189 </span>
<span class="line-added">1190     // Set up compressed class pointer encoding.</span>
<span class="line-added">1191     CompressedKlassPointers::initialize((address)rs.base(), rs.size());</span>
1192   }
<span class="line-added">1193 </span>
1194 #endif
1195 
1196   // Initialize these before initializing the VirtualSpaceList
1197   _first_chunk_word_size = InitialBootClassLoaderMetaspaceSize / BytesPerWord;
1198   _first_chunk_word_size = align_word_size_up(_first_chunk_word_size);
1199   // Make the first class chunk bigger than a medium chunk so it&#39;s not put
1200   // on the medium chunk list.   The next chunk will be small and progress
1201   // from there.  This size calculated by -version.
1202   _first_class_chunk_word_size = MIN2((size_t)MediumChunk*6,
1203                                      (CompressedClassSpaceSize/BytesPerWord)*2);
1204   _first_class_chunk_word_size = align_word_size_up(_first_class_chunk_word_size);
1205   // Arbitrarily set the initial virtual space to a multiple
1206   // of the boot class loader size.
1207   size_t word_size = VIRTUALSPACEMULTIPLIER * _first_chunk_word_size;
1208   word_size = align_up(word_size, Metaspace::reserve_alignment_words());
1209 
1210   // Initialize the list of virtual spaces.
1211   _space_list = new VirtualSpaceList(word_size);
1212   _chunk_manager_metadata = new ChunkManager(false/*metaspace*/);
1213 
1214   if (!_space_list-&gt;initialization_succeeded()) {
1215     vm_exit_during_initialization(&quot;Unable to setup metadata virtual space list.&quot;, NULL);
1216   }
1217 
1218   _tracer = new MetaspaceTracer();
1219 
1220   _initialized = true;
1221 
<span class="line-added">1222 #ifdef _LP64</span>
<span class="line-added">1223   if (UseCompressedClassPointers) {</span>
<span class="line-added">1224     // Note: &quot;cds&quot; would be a better fit but keep this for backward compatibility.</span>
<span class="line-added">1225     LogTarget(Info, gc, metaspace) lt;</span>
<span class="line-added">1226     if (lt.is_enabled()) {</span>
<span class="line-added">1227       ResourceMark rm;</span>
<span class="line-added">1228       LogStream ls(lt);</span>
<span class="line-added">1229       CDS_ONLY(MetaspaceShared::print_on(&amp;ls);)</span>
<span class="line-added">1230       Metaspace::print_compressed_class_space(&amp;ls);</span>
<span class="line-added">1231       CompressedKlassPointers::print_mode(&amp;ls);</span>
<span class="line-added">1232     }</span>
<span class="line-added">1233   }</span>
<span class="line-added">1234 #endif</span>
<span class="line-added">1235 </span>
1236 }
1237 
1238 void Metaspace::post_initialize() {
1239   MetaspaceGC::post_initialize();
1240 }
1241 
1242 void Metaspace::verify_global_initialization() {
1243   assert(space_list() != NULL, &quot;Metadata VirtualSpaceList has not been initialized&quot;);
1244   assert(chunk_manager_metadata() != NULL, &quot;Metadata ChunkManager has not been initialized&quot;);
1245 
1246   if (using_class_space()) {
1247     assert(class_space_list() != NULL, &quot;Class VirtualSpaceList has not been initialized&quot;);
1248     assert(chunk_manager_class() != NULL, &quot;Class ChunkManager has not been initialized&quot;);
1249   }
1250 }
1251 
1252 size_t Metaspace::align_word_size_up(size_t word_size) {
1253   size_t byte_size = word_size * wordSize;
1254   return ReservedSpace::allocation_align_size_up(byte_size) / wordSize;
1255 }
</pre>
</td>
</tr>
</table>
<center><a href="filemap.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspace.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>