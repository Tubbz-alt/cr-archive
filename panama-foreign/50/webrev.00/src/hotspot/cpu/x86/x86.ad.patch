diff a/src/hotspot/cpu/x86/x86.ad b/src/hotspot/cpu/x86/x86.ad
--- a/src/hotspot/cpu/x86/x86.ad
+++ b/src/hotspot/cpu/x86/x86.ad
@@ -1260,11 +1260,11 @@
       if (!UsePopCountInstruction) {
         return false;
       }
       break;
     case Op_PopCountVI:
-      if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {
+      if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {
         return false;
       }
       break;
     case Op_MulVI:
       if ((UseSSE < 4) && (UseAVX < 1)) { // only with SSE4_1 or AVX
@@ -1293,18 +1293,10 @@
     case Op_MulReductionVI:
       if (UseSSE < 4) { // requires at least SSE4
         return false;
       }
       break;
-    case Op_AddReductionVF:
-    case Op_AddReductionVD:
-    case Op_MulReductionVF:
-    case Op_MulReductionVD:
-      if (UseSSE < 1) { // requires at least SSE
-        return false;
-      }
-      break;
     case Op_SqrtVD:
     case Op_SqrtVF:
       if (UseAVX < 1) { // enabled for AVX only
         return false;
       }
@@ -1336,18 +1328,10 @@
     case Op_OnSpinWait:
       if (VM_Version::supports_on_spin_wait() == false) {
         return false;
       }
       break;
-    case Op_MulAddVS2VI:
-    case Op_RShiftVL:
-    case Op_AbsVD:
-    case Op_NegVD:
-      if (UseSSE < 2) {
-        return false;
-      }
-      break;
     case Op_MulVB:
     case Op_LShiftVB:
     case Op_RShiftVB:
     case Op_URShiftVB:
       if (UseSSE < 4) {
@@ -1379,10 +1363,28 @@
     case Op_RoundDoubleModeV:
       if (VM_Version::supports_avx() == false) {
         return false; // 128bit vroundpd is not available
       }
       break;
+#ifndef _LP64
+    case Op_AddReductionVF:
+    case Op_AddReductionVD:
+    case Op_MulReductionVF:
+    case Op_MulReductionVD:
+      if (UseSSE < 1) { // requires at least SSE
+        return false;
+      }
+      break;
+    case Op_MulAddVS2VI:
+    case Op_RShiftVL:
+    case Op_AbsVD:
+    case Op_NegVD:
+      if (UseSSE < 2) {
+        return false;
+      }
+      break;
+#endif // !LP64
   }
   return true;  // Match rules are supported by default.
 }
 
 //------------------------------------------------------------------------
@@ -3120,107 +3122,69 @@
 
 // ====================REPLICATE=======================================
 
 // Replicate byte scalar to be vector
 instruct ReplB_reg(vec dst, rRegI src) %{
-  predicate((n->as_Vector()->length() <= 32) ||
-            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
   match(Set dst (ReplicateB src));
   format %{ "replicateB $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
-      assert(VM_Version::supports_avx512bw(), "required");
+      assert(VM_Version::supports_avx512bw(), "required"); // 512-bit byte vectors assume AVX512BW
       int vlen_enc = vector_length_encoding(this);
       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
     } else {
       __ movdl($dst$$XMMRegister, $src$$Register);
       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
       if (vlen >= 16) {
         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
         if (vlen >= 32) {
-          assert(vlen == 32, "sanity"); // vlen == 64 && !AVX512BW is covered by ReplB_reg_leg
+          assert(vlen == 32, "sanity");
           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
         }
       }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct ReplB_reg_leg(legVec dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions
-  match(Set dst (ReplicateB src));
-  format %{ "replicateB $dst,$src" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 instruct ReplB_mem(vec dst, memory mem) %{
-  predicate((n->as_Vector()->length() <= 32 && VM_Version::supports_avx512vlbw()) || // AVX512VL for <512bit operands
-            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions
+  predicate(VM_Version::supports_avx2());
   match(Set dst (ReplicateB (LoadB mem)));
   format %{ "replicateB $dst,$mem" %}
   ins_encode %{
-    assert(UseAVX > 2, "required");
     int vector_len = vector_length_encoding(this);
     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplB_imm(vec dst, immI con) %{
-  predicate((n->as_Vector()->length() <= 32) ||
-            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
   match(Set dst (ReplicateB con));
   format %{ "replicateB $dst,$con" %}
   ins_encode %{
     uint vlen = vector_length(this);
     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
     if (vlen == 4) {
       __ movdl($dst$$XMMRegister, const_addr);
     } else {
       __ movq($dst$$XMMRegister, const_addr);
       if (vlen >= 16) {
-        if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
+        if (VM_Version::supports_avx2()) {
           int vlen_enc = vector_length_encoding(this);
-          __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+          __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
         } else {
+          assert(vlen == 16, "sanity");
           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-          if (vlen >= 32) {
-             assert(vlen == 32, "sanity");// vlen == 64 && !AVX512BW is covered by ReplB_imm_leg
-            __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-          }
         }
       }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct ReplB_imm_leg(legVec dst, immI con) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateB con));
-  format %{ "replicateB $dst,$con" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 // Replicate byte scalar zero to be vector
 instruct ReplB_zero(vec dst, immI0 zero) %{
   match(Set dst (ReplicateB zero));
   format %{ "replicateB $dst,$zero" %}
   ins_encode %{
@@ -3237,130 +3201,68 @@
 %}
 
 // ====================ReplicateS=======================================
 
 instruct ReplS_reg(vec dst, rRegI src) %{
-  predicate((n->as_Vector()->length() <= 16) ||
-            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
   match(Set dst (ReplicateS src));
   format %{ "replicateS $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
-      assert(VM_Version::supports_avx512bw(), "required");
+      assert(VM_Version::supports_avx512bw(), "required"); // 512-bit short vectors assume AVX512BW
       int vlen_enc = vector_length_encoding(this);
       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
     } else {
       __ movdl($dst$$XMMRegister, $src$$Register);
       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
       if (vlen >= 8) {
         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
         if (vlen >= 16) {
-          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_reg_leg
+          assert(vlen == 16, "sanity");
           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
         }
       }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct ReplS_reg_leg(legVec dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS src));
-  format %{ "replicateS $dst,$src" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 instruct ReplS_mem(vec dst, memory mem) %{
-  predicate((n->as_Vector()->length() >= 4  &&
-             n->as_Vector()->length() <= 16 && VM_Version::supports_avx()) ||
-            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
+  predicate(VM_Version::supports_avx2());
   match(Set dst (ReplicateS (LoadS mem)));
   format %{ "replicateS $dst,$mem" %}
   ins_encode %{
-    uint vlen = vector_length(this);
-    if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
-      assert(VM_Version::supports_avx512bw(), "required");
-      int vlen_enc = vector_length_encoding(this);
-      __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
-    } else {
-      __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-      if (vlen >= 8) {
-        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-        if (vlen >= 16) {
-          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_mem_leg
-          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-        }
-      }
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct ReplS_mem_leg(legVec dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "replicateS $dst,$mem" %}
-  ins_encode %{
-    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
+    int vlen_enc = vector_length_encoding(this);
+    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplS_imm(vec dst, immI con) %{
-  predicate((n->as_Vector()->length() <= 16) ||
-            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
   match(Set dst (ReplicateS con));
   format %{ "replicateS $dst,$con" %}
   ins_encode %{
     uint vlen = vector_length(this);
-    InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));
+    InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));
     if (vlen == 2) {
-      __ movdl($dst$$XMMRegister, constaddr);
+      __ movdl($dst$$XMMRegister, const_addr);
     } else {
-      __ movq($dst$$XMMRegister, constaddr);
-      if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for <512bit operands
-        assert(VM_Version::supports_avx512bw(), "required");
-        int vlen_enc = vector_length_encoding(this);
-        __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
-      } else {
-        __ movq($dst$$XMMRegister, constaddr);
-        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-        if (vlen >= 16) {
-          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_imm_leg
-          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+      __ movq($dst$$XMMRegister, const_addr);
+      if (vlen >= 8) {
+        if (VM_Version::supports_avx2()) {
+          int vlen_enc = vector_length_encoding(this);
+          __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+        } else {
+          assert(vlen == 8, "sanity");
+          __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
         }
       }
     }
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
-instruct ReplS_imm_leg(legVec dst, immI con) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS con));
-  format %{ "replicateS $dst,$con" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 instruct ReplS_zero(vec dst, immI0 zero) %{
   match(Set dst (ReplicateS zero));
   format %{ "replicateS $dst,$zero" %}
   ins_encode %{
     uint vlen = vector_length(this);
@@ -3395,48 +3297,42 @@
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplI_mem(vec dst, memory mem) %{
-  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
   match(Set dst (ReplicateI (LoadI mem)));
   format %{ "replicateI $dst,$mem" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen <= 4) {
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      __ movdl($dst$$XMMRegister, $mem$$Address);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vector_len = vector_length_encoding(this);
       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
-    } else {
-      assert(vlen == 8, "sanity");
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplI_imm(vec dst, immI con) %{
   match(Set dst (ReplicateI con));
   format %{ "replicateI $dst,$con" %}
   ins_encode %{
     uint vlen = vector_length(this);
-    InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));
-    if (vlen == 2) {
-      __ movq($dst$$XMMRegister, constaddr);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));
+    if (vlen <= 4) {
+      __ movq($dst$$XMMRegister, const_addr);
+      if (vlen == 4) {
+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      }
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vector_len = vector_length_encoding(this);
-      __ movq($dst$$XMMRegister, constaddr);
+      __ movq($dst$$XMMRegister, const_addr);
       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-    } else {
-      __ movq($dst$$XMMRegister, constaddr);
-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-      if (vlen >= 8) {
-        assert(vlen == 8, "sanity");
-        __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-      }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3542,18 +3438,14 @@
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 2) {
       __ movq($dst$$XMMRegister, $mem$$Address);
       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vlen_enc = vector_length_encoding(this);
       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
-    } else {
-      assert(vlen == 4, "sanity");
-      __ movq($dst$$XMMRegister, $mem$$Address);
-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3565,19 +3457,15 @@
     uint vlen = vector_length(this);
     InternalAddress const_addr = $constantaddress($con);
     if (vlen == 2) {
       __ movq($dst$$XMMRegister, const_addr);
       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vlen_enc = vector_length_encoding(this);
       __ movq($dst$$XMMRegister, const_addr);
       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
-    } else {
-      assert(vlen == 4, "sanity");
-      __ movq($dst$$XMMRegister, const_addr);
-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3603,37 +3491,34 @@
   format %{ "replicateF $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen <= 4) {
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+   } else if (VM_Version::supports_avx2()) {
       int vector_len = vector_length_encoding(this);
-      __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+      __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
     } else {
       assert(vlen == 8, "sanity");
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplF_mem(vec dst, memory mem) %{
-  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
   match(Set dst (ReplicateF (LoadF mem)));
   format %{ "replicateF $dst,$mem" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen <= 4) {
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      __ movdl($dst$$XMMRegister, $mem$$Address);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+    } else {
+      assert(VM_Version::supports_avx(), "sanity");
       int vector_len = vector_length_encoding(this);
       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
-    } else {
-      assert(vlen == 8, "sanity");
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3660,37 +3545,34 @@
   format %{ "replicateD $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 2) {
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    } else if (VM_Version::supports_avx2()) {
       int vector_len = vector_length_encoding(this);
-      __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+      __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
     } else {
       assert(vlen == 4, "sanity");
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplD_mem(vec dst, memory mem) %{
-  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
   match(Set dst (ReplicateD (LoadD mem)));
   format %{ "replicateD $dst,$mem" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 2) {
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      __ movq($dst$$XMMRegister, $mem$$Address);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);
+    } else {
+      assert(VM_Version::supports_avx(), "sanity");
       int vector_len = vector_length_encoding(this);
       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
-    } else {
-      assert(vlen == 4, "sanity");
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -5782,11 +5664,11 @@
 %}
 
 // --------------------------------- Vector Multiply Add Add ----------------------------------
 
 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
-  predicate(VM_Version::supports_vnni());
+  predicate(VM_Version::supports_avx512_vnni());
   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
   format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI" %}
   ins_encode %{
     assert(UseAVX > 2, "required");
     int vector_len = vector_length_encoding(this);
