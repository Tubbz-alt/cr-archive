<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="vm_version_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_64.ad.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/x86.ad</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
1280     case Op_AddReductionVL:
1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here
1282         return false;
1283       }
1284       break;
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
<span class="line-removed">1298     case Op_AddReductionVF:</span>
<span class="line-removed">1299     case Op_AddReductionVD:</span>
<span class="line-removed">1300     case Op_MulReductionVF:</span>
<span class="line-removed">1301     case Op_MulReductionVD:</span>
<span class="line-removed">1302       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-removed">1303         return false;</span>
<span class="line-removed">1304       }</span>
<span class="line-removed">1305       break;</span>
1306     case Op_SqrtVD:
1307     case Op_SqrtVF:
1308       if (UseAVX &lt; 1) { // enabled for AVX only
1309         return false;
1310       }
1311       break;
1312     case Op_CompareAndSwapL:
1313 #ifdef _LP64
1314     case Op_CompareAndSwapP:
1315 #endif
1316       if (!VM_Version::supports_cx8()) {
1317         return false;
1318       }
1319       break;
1320     case Op_CMoveVF:
1321     case Op_CMoveVD:
1322       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1323         return false;
1324       }
1325       break;
1326     case Op_StrIndexOf:
1327       if (!UseSSE42Intrinsics) {
1328         return false;
1329       }
1330       break;
1331     case Op_StrIndexOfChar:
1332       if (!UseSSE42Intrinsics) {
1333         return false;
1334       }
1335       break;
1336     case Op_OnSpinWait:
1337       if (VM_Version::supports_on_spin_wait() == false) {
1338         return false;
1339       }
1340       break;
<span class="line-removed">1341     case Op_MulAddVS2VI:</span>
<span class="line-removed">1342     case Op_RShiftVL:</span>
<span class="line-removed">1343     case Op_AbsVD:</span>
<span class="line-removed">1344     case Op_NegVD:</span>
<span class="line-removed">1345       if (UseSSE &lt; 2) {</span>
<span class="line-removed">1346         return false;</span>
<span class="line-removed">1347       }</span>
<span class="line-removed">1348       break;</span>
1349     case Op_MulVB:
1350     case Op_LShiftVB:
1351     case Op_RShiftVB:
1352     case Op_URShiftVB:
1353       if (UseSSE &lt; 4) {
1354         return false;
1355       }
1356       break;
1357 #ifdef _LP64
1358     case Op_MaxD:
1359     case Op_MaxF:
1360     case Op_MinD:
1361     case Op_MinF:
1362       if (UseAVX &lt; 1) { // enabled for AVX only
1363         return false;
1364       }
1365       break;
1366 #endif
1367     case Op_CacheWB:
1368     case Op_CacheWBPreSync:
1369     case Op_CacheWBPostSync:
1370       if (!VM_Version::supports_data_cache_line_flush()) {
1371         return false;
1372       }
1373       break;
1374     case Op_RoundDoubleMode:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379     case Op_RoundDoubleModeV:
1380       if (VM_Version::supports_avx() == false) {
1381         return false; // 128bit vroundpd is not available
1382       }
1383       break;


















1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
</pre>
<hr />
<pre>
3105   match(Set mem (StoreVector mem src));
3106   ins_cost(145);
3107   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3108   ins_encode %{
3109     switch (vector_length_in_bytes(this, $src)) {
3110       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3111       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3112       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3113       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3114       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3115       default: ShouldNotReachHere();
3116     }
3117   %}
3118   ins_pipe( pipe_slow );
3119 %}
3120 
3121 // ====================REPLICATE=======================================
3122 
3123 // Replicate byte scalar to be vector
3124 instruct ReplB_reg(vec dst, rRegI src) %{
<span class="line-removed">3125   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3126             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;); // vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_reg_leg</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
<span class="line-removed">3151 instruct ReplB_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3152   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions</span>
<span class="line-removed">3153   match(Set dst (ReplicateB src));</span>
<span class="line-removed">3154   format %{ &quot;replicateB $dst,$src&quot; %}</span>
<span class="line-removed">3155   ins_encode %{</span>
<span class="line-removed">3156     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3157     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3158     __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3159     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3160     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3161     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3162     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3163   %}</span>
<span class="line-removed">3164   ins_pipe( pipe_slow );</span>
<span class="line-removed">3165 %}</span>
<span class="line-removed">3166 </span>
3167 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3168   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32 &amp;&amp; VM_Version::supports_avx512vlbw()) || // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3169             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions</span>
3170   match(Set dst (ReplicateB (LoadB mem)));
3171   format %{ &quot;replicateB $dst,$mem&quot; %}
3172   ins_encode %{
<span class="line-removed">3173     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
3174     int vector_len = vector_length_encoding(this);
3175     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3176   %}
3177   ins_pipe( pipe_slow );
3178 %}
3179 
3180 instruct ReplB_imm(vec dst, immI con) %{
<span class="line-removed">3181   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3182             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3183   match(Set dst (ReplicateB con));
3184   format %{ &quot;replicateB $dst,$con&quot; %}
3185   ins_encode %{
3186     uint vlen = vector_length(this);
3187     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3188     if (vlen == 4) {
3189       __ movdl($dst$$XMMRegister, const_addr);
3190     } else {
3191       __ movq($dst$$XMMRegister, const_addr);
3192       if (vlen &gt;= 16) {
<span class="line-modified">3193         if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
3194           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3195           __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3196         } else {

3197           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-removed">3198           if (vlen &gt;= 32) {</span>
<span class="line-removed">3199              assert(vlen == 32, &quot;sanity&quot;);// vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_imm_leg</span>
<span class="line-removed">3200             __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3201           }</span>
3202         }
3203       }
3204     }
3205   %}
3206   ins_pipe( pipe_slow );
3207 %}
3208 
<span class="line-removed">3209 instruct ReplB_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3210   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3211   match(Set dst (ReplicateB con));</span>
<span class="line-removed">3212   format %{ &quot;replicateB $dst,$con&quot; %}</span>
<span class="line-removed">3213   ins_encode %{</span>
<span class="line-removed">3214     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));</span>
<span class="line-removed">3215     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3216     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3217     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3218   %}</span>
<span class="line-removed">3219   ins_pipe( pipe_slow );</span>
<span class="line-removed">3220 %}</span>
<span class="line-removed">3221 </span>
3222 // Replicate byte scalar zero to be vector
3223 instruct ReplB_zero(vec dst, immI0 zero) %{
3224   match(Set dst (ReplicateB zero));
3225   format %{ &quot;replicateB $dst,$zero&quot; %}
3226   ins_encode %{
3227     uint vlen = vector_length(this);
3228     if (vlen &lt;= 16) {
3229       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3230     } else {
3231       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3232       int vlen_enc = vector_length_encoding(this);
3233       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3234     }
3235   %}
3236   ins_pipe( fpu_reg_reg );
3237 %}
3238 
3239 // ====================ReplicateS=======================================
3240 
3241 instruct ReplS_reg(vec dst, rRegI src) %{
<span class="line-removed">3242   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3243             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3244   match(Set dst (ReplicateS src));
3245   format %{ &quot;replicateS $dst,$src&quot; %}
3246   ins_encode %{
3247     uint vlen = vector_length(this);
3248     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3249       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3250       int vlen_enc = vector_length_encoding(this);
3251       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3252     } else {
3253       __ movdl($dst$$XMMRegister, $src$$Register);
3254       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3255       if (vlen &gt;= 8) {
3256         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         if (vlen &gt;= 16) {
<span class="line-modified">3258           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_reg_leg</span>
3259           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3260         }
3261       }
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
<span class="line-removed">3267 instruct ReplS_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3268   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3269   match(Set dst (ReplicateS src));</span>
<span class="line-removed">3270   format %{ &quot;replicateS $dst,$src&quot; %}</span>
<span class="line-removed">3271   ins_encode %{</span>
<span class="line-removed">3272     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3273     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3274     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3275     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3276     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3277   %}</span>
<span class="line-removed">3278   ins_pipe( pipe_slow );</span>
<span class="line-removed">3279 %}</span>
<span class="line-removed">3280 </span>
3281 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3282   predicate((n-&gt;as_Vector()-&gt;length() &gt;= 4  &amp;&amp;</span>
<span class="line-removed">3283              n-&gt;as_Vector()-&gt;length() &lt;= 16 &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3284             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3285   match(Set dst (ReplicateS (LoadS mem)));
3286   format %{ &quot;replicateS $dst,$mem&quot; %}
3287   ins_encode %{
<span class="line-modified">3288     uint vlen = vector_length(this);</span>
<span class="line-modified">3289     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3290       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-removed">3291       int vlen_enc = vector_length_encoding(this);</span>
<span class="line-removed">3292       __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>
<span class="line-removed">3293     } else {</span>
<span class="line-removed">3294       __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3295       if (vlen &gt;= 8) {</span>
<span class="line-removed">3296         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3297         if (vlen &gt;= 16) {</span>
<span class="line-removed">3298           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_mem_leg</span>
<span class="line-removed">3299           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3300         }</span>
<span class="line-removed">3301       }</span>
<span class="line-removed">3302     }</span>
<span class="line-removed">3303   %}</span>
<span class="line-removed">3304   ins_pipe( pipe_slow );</span>
<span class="line-removed">3305 %}</span>
<span class="line-removed">3306 </span>
<span class="line-removed">3307 instruct ReplS_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3308   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3309   match(Set dst (ReplicateS (LoadS mem)));</span>
<span class="line-removed">3310   format %{ &quot;replicateS $dst,$mem&quot; %}</span>
<span class="line-removed">3311   ins_encode %{</span>
<span class="line-removed">3312     __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3313     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3314     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3315     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
3316   %}
3317   ins_pipe( pipe_slow );
3318 %}
3319 
3320 instruct ReplS_imm(vec dst, immI con) %{
<span class="line-removed">3321   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3322             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3323   match(Set dst (ReplicateS con));
3324   format %{ &quot;replicateS $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
<span class="line-modified">3327     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3328     if (vlen == 2) {
<span class="line-modified">3329       __ movdl($dst$$XMMRegister, constaddr);</span>
3330     } else {
<span class="line-modified">3331       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3332       if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for &lt;512bit operands</span>
<span class="line-modified">3333         assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-modified">3334         int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3335         __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3336       } else {</span>
<span class="line-modified">3337         __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3338         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3339         if (vlen &gt;= 16) {</span>
<span class="line-removed">3340           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_imm_leg</span>
<span class="line-removed">3341           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3342         }
3343       }
3344     }
3345   %}
3346   ins_pipe( fpu_reg_reg );
3347 %}
3348 
<span class="line-removed">3349 instruct ReplS_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3350   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3351   match(Set dst (ReplicateS con));</span>
<span class="line-removed">3352   format %{ &quot;replicateS $dst,$con&quot; %}</span>
<span class="line-removed">3353   ins_encode %{</span>
<span class="line-removed">3354     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));</span>
<span class="line-removed">3355     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3356     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3357     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3358   %}</span>
<span class="line-removed">3359   ins_pipe( pipe_slow );</span>
<span class="line-removed">3360 %}</span>
<span class="line-removed">3361 </span>
3362 instruct ReplS_zero(vec dst, immI0 zero) %{
3363   match(Set dst (ReplicateS zero));
3364   format %{ &quot;replicateS $dst,$zero&quot; %}
3365   ins_encode %{
3366     uint vlen = vector_length(this);
3367     if (vlen &lt;= 8) {
3368       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3369     } else {
3370       int vlen_enc = vector_length_encoding(this);
3371       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3372     }
3373   %}
3374   ins_pipe( fpu_reg_reg );
3375 %}
3376 
3377 // ====================ReplicateI=======================================
3378 
3379 instruct ReplI_reg(vec dst, rRegI src) %{
3380   match(Set dst (ReplicateI src));
3381   format %{ &quot;replicateI $dst,$src&quot; %}
3382   ins_encode %{
3383     uint vlen = vector_length(this);
3384     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3385       int vlen_enc = vector_length_encoding(this);
3386       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3387     } else {
3388       __ movdl($dst$$XMMRegister, $src$$Register);
3389       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3390       if (vlen &gt;= 8) {
3391         assert(vlen == 8, &quot;sanity&quot;);
3392         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3393       }
3394     }
3395   %}
3396   ins_pipe( pipe_slow );
3397 %}
3398 
3399 instruct ReplI_mem(vec dst, memory mem) %{
<span class="line-removed">3400   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3401   match(Set dst (ReplicateI (LoadI mem)));
3402   format %{ &quot;replicateI $dst,$mem&quot; %}
3403   ins_encode %{
3404     uint vlen = vector_length(this);
3405     if (vlen &lt;= 4) {
<span class="line-modified">3406       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3407     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3408       int vector_len = vector_length_encoding(this);
3409       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3410     } else {</span>
<span class="line-removed">3411       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3412       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3413       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3414     }
3415   %}
3416   ins_pipe( pipe_slow );
3417 %}
3418 
3419 instruct ReplI_imm(vec dst, immI con) %{
3420   match(Set dst (ReplicateI con));
3421   format %{ &quot;replicateI $dst,$con&quot; %}
3422   ins_encode %{
3423     uint vlen = vector_length(this);
<span class="line-modified">3424     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3425     if (vlen == 2) {</span>
<span class="line-modified">3426       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3427     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>




3428       int vector_len = vector_length_encoding(this);
<span class="line-modified">3429       __ movq($dst$$XMMRegister, constaddr);</span>
3430       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
<span class="line-removed">3431     } else {</span>
<span class="line-removed">3432       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-removed">3433       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3434       if (vlen &gt;= 8) {</span>
<span class="line-removed">3435         assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3436         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3437       }</span>
3438     }
3439   %}
3440   ins_pipe( pipe_slow );
3441 %}
3442 
3443 // Replicate integer (4 byte) scalar zero to be vector
3444 instruct ReplI_zero(vec dst, immI0 zero) %{
3445   match(Set dst (ReplicateI zero));
3446   format %{ &quot;replicateI $dst,$zero&quot; %}
3447   ins_encode %{
3448     uint vlen = vector_length(this);
3449     if (vlen &lt;= 4) {
3450       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3451     } else {
3452       int vlen_enc = vector_length_encoding(this);
3453       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3454     }
3455   %}
3456   ins_pipe( fpu_reg_reg );
3457 %}
</pre>
<hr />
<pre>
3527     } else {
3528       int vector_len = Assembler::AVX_512bit;
3529       __ movdl($dst$$XMMRegister, $src$$Register);
3530       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3531       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3532       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3533     }
3534   %}
3535   ins_pipe( pipe_slow );
3536 %}
3537 #endif // _LP64
3538 
3539 instruct ReplL_mem(vec dst, memory mem) %{
3540   match(Set dst (ReplicateL (LoadL mem)));
3541   format %{ &quot;replicateL $dst,$mem&quot; %}
3542   ins_encode %{
3543     uint vlen = vector_length(this);
3544     if (vlen == 2) {
3545       __ movq($dst$$XMMRegister, $mem$$Address);
3546       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3547     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3548       int vlen_enc = vector_length_encoding(this);
3549       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
<span class="line-removed">3550     } else {</span>
<span class="line-removed">3551       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3552       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-removed">3553       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3554       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3555     }
3556   %}
3557   ins_pipe( pipe_slow );
3558 %}
3559 
3560 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3561 instruct ReplL_imm(vec dst, immL con) %{
3562   match(Set dst (ReplicateL con));
3563   format %{ &quot;replicateL $dst,$con&quot; %}
3564   ins_encode %{
3565     uint vlen = vector_length(this);
3566     InternalAddress const_addr = $constantaddress($con);
3567     if (vlen == 2) {
3568       __ movq($dst$$XMMRegister, const_addr);
3569       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3570     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3571       int vlen_enc = vector_length_encoding(this);
3572       __ movq($dst$$XMMRegister, const_addr);
3573       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
<span class="line-removed">3574     } else {</span>
<span class="line-removed">3575       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3576       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-removed">3577       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3578       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3579     }
3580   %}
3581   ins_pipe( pipe_slow );
3582 %}
3583 
3584 instruct ReplL_zero(vec dst, immL0 zero) %{
3585   match(Set dst (ReplicateL zero));
3586   format %{ &quot;replicateL $dst,$zero&quot; %}
3587   ins_encode %{
3588     int vlen = vector_length(this);
3589     if (vlen == 2) {
3590       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3591     } else {
3592       int vlen_enc = vector_length_encoding(this);
3593       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3594     }
3595   %}
3596   ins_pipe( fpu_reg_reg );
3597 %}
3598 
3599 // ====================ReplicateF=======================================
3600 
3601 instruct ReplF_reg(vec dst, vlRegF src) %{
3602   match(Set dst (ReplicateF src));
3603   format %{ &quot;replicateF $dst,$src&quot; %}
3604   ins_encode %{
3605     uint vlen = vector_length(this);
3606     if (vlen &lt;= 4) {
3607       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3608     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3609       int vector_len = vector_length_encoding(this);
<span class="line-modified">3610       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3611     } else {
3612       assert(vlen == 8, &quot;sanity&quot;);
3613       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3614       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3615     }
3616   %}
3617   ins_pipe( pipe_slow );
3618 %}
3619 
3620 instruct ReplF_mem(vec dst, memory mem) %{
<span class="line-removed">3621   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3622   match(Set dst (ReplicateF (LoadF mem)));
3623   format %{ &quot;replicateF $dst,$mem&quot; %}
3624   ins_encode %{
3625     uint vlen = vector_length(this);
3626     if (vlen &lt;= 4) {
<span class="line-modified">3627       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3628     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3629       int vector_len = vector_length_encoding(this);
3630       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3631     } else {</span>
<span class="line-removed">3632       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3633       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3634       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3635     }
3636   %}
3637   ins_pipe( pipe_slow );
3638 %}
3639 
3640 instruct ReplF_zero(vec dst, immF0 zero) %{
3641   match(Set dst (ReplicateF zero));
3642   format %{ &quot;replicateF $dst,$zero&quot; %}
3643   ins_encode %{
3644     uint vlen = vector_length(this);
3645     if (vlen &lt;= 4) {
3646       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3647     } else {
3648       int vlen_enc = vector_length_encoding(this);
3649       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3650     }
3651   %}
3652   ins_pipe( fpu_reg_reg );
3653 %}
3654 
3655 // ====================ReplicateD=======================================
3656 
3657 // Replicate double (8 bytes) scalar to be vector
3658 instruct ReplD_reg(vec dst, vlRegD src) %{
3659   match(Set dst (ReplicateD src));
3660   format %{ &quot;replicateD $dst,$src&quot; %}
3661   ins_encode %{
3662     uint vlen = vector_length(this);
3663     if (vlen == 2) {
3664       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3665     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3666       int vector_len = vector_length_encoding(this);
<span class="line-modified">3667       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3668     } else {
3669       assert(vlen == 4, &quot;sanity&quot;);
3670       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3671       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3672     }
3673   %}
3674   ins_pipe( pipe_slow );
3675 %}
3676 
3677 instruct ReplD_mem(vec dst, memory mem) %{
<span class="line-removed">3678   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3679   match(Set dst (ReplicateD (LoadD mem)));
3680   format %{ &quot;replicateD $dst,$mem&quot; %}
3681   ins_encode %{
3682     uint vlen = vector_length(this);
3683     if (vlen == 2) {
<span class="line-modified">3684       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-modified">3685     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3686       int vector_len = vector_length_encoding(this);
3687       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3688     } else {</span>
<span class="line-removed">3689       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3690       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-removed">3691       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3692     }
3693   %}
3694   ins_pipe( pipe_slow );
3695 %}
3696 
3697 instruct ReplD_zero(vec dst, immD0 zero) %{
3698   match(Set dst (ReplicateD zero));
3699   format %{ &quot;replicateD $dst,$zero&quot; %}
3700   ins_encode %{
3701     uint vlen = vector_length(this);
3702     if (vlen == 2) {
3703       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3704     } else {
3705       int vlen_enc = vector_length_encoding(this);
3706       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3707     }
3708   %}
3709   ins_pipe( fpu_reg_reg );
3710 %}
3711 
</pre>
<hr />
<pre>
5767   ins_encode %{
5768     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5769   %}
5770   ins_pipe( pipe_slow );
5771 %}
5772 
5773 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5774   predicate(UseAVX &gt; 0);
5775   match(Set dst (MulAddVS2VI src1 src2));
5776   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5777   ins_encode %{
5778     int vector_len = vector_length_encoding(this);
5779     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5780   %}
5781   ins_pipe( pipe_slow );
5782 %}
5783 
5784 // --------------------------------- Vector Multiply Add Add ----------------------------------
5785 
5786 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5787   predicate(VM_Version::supports_vnni());</span>
5788   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5789   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5790   ins_encode %{
5791     assert(UseAVX &gt; 2, &quot;required&quot;);
5792     int vector_len = vector_length_encoding(this);
5793     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5794   %}
5795   ins_pipe( pipe_slow );
5796   ins_cost(10);
5797 %}
5798 
5799 // --------------------------------- PopCount --------------------------------------
5800 
5801 instruct vpopcountI(vec dst, vec src) %{
5802   match(Set dst (PopCountVI src));
5803   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5804   ins_encode %{
5805     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5806 
5807     int vector_len = vector_length_encoding(this);
</pre>
</td>
<td>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
1280     case Op_AddReductionVL:
1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here
1282         return false;
1283       }
1284       break;
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;








1298     case Op_SqrtVD:
1299     case Op_SqrtVF:
1300       if (UseAVX &lt; 1) { // enabled for AVX only
1301         return false;
1302       }
1303       break;
1304     case Op_CompareAndSwapL:
1305 #ifdef _LP64
1306     case Op_CompareAndSwapP:
1307 #endif
1308       if (!VM_Version::supports_cx8()) {
1309         return false;
1310       }
1311       break;
1312     case Op_CMoveVF:
1313     case Op_CMoveVD:
1314       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1315         return false;
1316       }
1317       break;
1318     case Op_StrIndexOf:
1319       if (!UseSSE42Intrinsics) {
1320         return false;
1321       }
1322       break;
1323     case Op_StrIndexOfChar:
1324       if (!UseSSE42Intrinsics) {
1325         return false;
1326       }
1327       break;
1328     case Op_OnSpinWait:
1329       if (VM_Version::supports_on_spin_wait() == false) {
1330         return false;
1331       }
1332       break;








1333     case Op_MulVB:
1334     case Op_LShiftVB:
1335     case Op_RShiftVB:
1336     case Op_URShiftVB:
1337       if (UseSSE &lt; 4) {
1338         return false;
1339       }
1340       break;
1341 #ifdef _LP64
1342     case Op_MaxD:
1343     case Op_MaxF:
1344     case Op_MinD:
1345     case Op_MinF:
1346       if (UseAVX &lt; 1) { // enabled for AVX only
1347         return false;
1348       }
1349       break;
1350 #endif
1351     case Op_CacheWB:
1352     case Op_CacheWBPreSync:
1353     case Op_CacheWBPostSync:
1354       if (!VM_Version::supports_data_cache_line_flush()) {
1355         return false;
1356       }
1357       break;
1358     case Op_RoundDoubleMode:
1359       if (UseSSE &lt; 4) {
1360         return false;
1361       }
1362       break;
1363     case Op_RoundDoubleModeV:
1364       if (VM_Version::supports_avx() == false) {
1365         return false; // 128bit vroundpd is not available
1366       }
1367       break;
<span class="line-added">1368 #ifndef _LP64</span>
<span class="line-added">1369     case Op_AddReductionVF:</span>
<span class="line-added">1370     case Op_AddReductionVD:</span>
<span class="line-added">1371     case Op_MulReductionVF:</span>
<span class="line-added">1372     case Op_MulReductionVD:</span>
<span class="line-added">1373       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-added">1374         return false;</span>
<span class="line-added">1375       }</span>
<span class="line-added">1376       break;</span>
<span class="line-added">1377     case Op_MulAddVS2VI:</span>
<span class="line-added">1378     case Op_RShiftVL:</span>
<span class="line-added">1379     case Op_AbsVD:</span>
<span class="line-added">1380     case Op_NegVD:</span>
<span class="line-added">1381       if (UseSSE &lt; 2) {</span>
<span class="line-added">1382         return false;</span>
<span class="line-added">1383       }</span>
<span class="line-added">1384       break;</span>
<span class="line-added">1385 #endif // !LP64</span>
1386   }
1387   return true;  // Match rules are supported by default.
1388 }
1389 
1390 //------------------------------------------------------------------------
1391 
1392 // Identify extra cases that we might want to provide match rules for vector nodes and
1393 // other intrinsics guarded with vector length (vlen) and element type (bt).
1394 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1395   if (!match_rule_supported(opcode)) {
1396     return false;
1397   }
1398   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1399   //   * SSE2 supports 128bit vectors for all types;
1400   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1401   //   * AVX2 supports 256bit vectors for all types;
1402   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1403   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1404   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1405   // And MaxVectorSize is taken into account as well.
</pre>
<hr />
<pre>
3107   match(Set mem (StoreVector mem src));
3108   ins_cost(145);
3109   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3110   ins_encode %{
3111     switch (vector_length_in_bytes(this, $src)) {
3112       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3113       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3114       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3115       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3116       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3117       default: ShouldNotReachHere();
3118     }
3119   %}
3120   ins_pipe( pipe_slow );
3121 %}
3122 
3123 // ====================REPLICATE=======================================
3124 
3125 // Replicate byte scalar to be vector
3126 instruct ReplB_reg(vec dst, rRegI src) %{


3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;);</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
















3151 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3152   predicate(VM_Version::supports_avx2());</span>

3153   match(Set dst (ReplicateB (LoadB mem)));
3154   format %{ &quot;replicateB $dst,$mem&quot; %}
3155   ins_encode %{

3156     int vector_len = vector_length_encoding(this);
3157     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3158   %}
3159   ins_pipe( pipe_slow );
3160 %}
3161 
3162 instruct ReplB_imm(vec dst, immI con) %{


3163   match(Set dst (ReplicateB con));
3164   format %{ &quot;replicateB $dst,$con&quot; %}
3165   ins_encode %{
3166     uint vlen = vector_length(this);
3167     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3168     if (vlen == 4) {
3169       __ movdl($dst$$XMMRegister, const_addr);
3170     } else {
3171       __ movq($dst$$XMMRegister, const_addr);
3172       if (vlen &gt;= 16) {
<span class="line-modified">3173         if (VM_Version::supports_avx2()) {</span>
3174           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3175           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3176         } else {
<span class="line-added">3177           assert(vlen == 16, &quot;sanity&quot;);</span>
3178           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);




3179         }
3180       }
3181     }
3182   %}
3183   ins_pipe( pipe_slow );
3184 %}
3185 













3186 // Replicate byte scalar zero to be vector
3187 instruct ReplB_zero(vec dst, immI0 zero) %{
3188   match(Set dst (ReplicateB zero));
3189   format %{ &quot;replicateB $dst,$zero&quot; %}
3190   ins_encode %{
3191     uint vlen = vector_length(this);
3192     if (vlen &lt;= 16) {
3193       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3194     } else {
3195       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3196       int vlen_enc = vector_length_encoding(this);
3197       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3198     }
3199   %}
3200   ins_pipe( fpu_reg_reg );
3201 %}
3202 
3203 // ====================ReplicateS=======================================
3204 
3205 instruct ReplS_reg(vec dst, rRegI src) %{


3206   match(Set dst (ReplicateS src));
3207   format %{ &quot;replicateS $dst,$src&quot; %}
3208   ins_encode %{
3209     uint vlen = vector_length(this);
3210     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3211       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW</span>
3212       int vlen_enc = vector_length_encoding(this);
3213       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3214     } else {
3215       __ movdl($dst$$XMMRegister, $src$$Register);
3216       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3217       if (vlen &gt;= 8) {
3218         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3219         if (vlen &gt;= 16) {
<span class="line-modified">3220           assert(vlen == 16, &quot;sanity&quot;);</span>
3221           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3222         }
3223       }
3224     }
3225   %}
3226   ins_pipe( pipe_slow );
3227 %}
3228 














3229 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3230   predicate(VM_Version::supports_avx2());</span>


3231   match(Set dst (ReplicateS (LoadS mem)));
3232   format %{ &quot;replicateS $dst,$mem&quot; %}
3233   ins_encode %{
<span class="line-modified">3234     int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3235     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>


























3236   %}
3237   ins_pipe( pipe_slow );
3238 %}
3239 
3240 instruct ReplS_imm(vec dst, immI con) %{


3241   match(Set dst (ReplicateS con));
3242   format %{ &quot;replicateS $dst,$con&quot; %}
3243   ins_encode %{
3244     uint vlen = vector_length(this);
<span class="line-modified">3245     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3246     if (vlen == 2) {
<span class="line-modified">3247       __ movdl($dst$$XMMRegister, const_addr);</span>
3248     } else {
<span class="line-modified">3249       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3250       if (vlen &gt;= 8) {</span>
<span class="line-modified">3251         if (VM_Version::supports_avx2()) {</span>
<span class="line-modified">3252           int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3253           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3254         } else {</span>
<span class="line-modified">3255           assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-modified">3256           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>



3257         }
3258       }
3259     }
3260   %}
3261   ins_pipe( fpu_reg_reg );
3262 %}
3263 













3264 instruct ReplS_zero(vec dst, immI0 zero) %{
3265   match(Set dst (ReplicateS zero));
3266   format %{ &quot;replicateS $dst,$zero&quot; %}
3267   ins_encode %{
3268     uint vlen = vector_length(this);
3269     if (vlen &lt;= 8) {
3270       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3271     } else {
3272       int vlen_enc = vector_length_encoding(this);
3273       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3274     }
3275   %}
3276   ins_pipe( fpu_reg_reg );
3277 %}
3278 
3279 // ====================ReplicateI=======================================
3280 
3281 instruct ReplI_reg(vec dst, rRegI src) %{
3282   match(Set dst (ReplicateI src));
3283   format %{ &quot;replicateI $dst,$src&quot; %}
3284   ins_encode %{
3285     uint vlen = vector_length(this);
3286     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3287       int vlen_enc = vector_length_encoding(this);
3288       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3289     } else {
3290       __ movdl($dst$$XMMRegister, $src$$Register);
3291       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3292       if (vlen &gt;= 8) {
3293         assert(vlen == 8, &quot;sanity&quot;);
3294         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3295       }
3296     }
3297   %}
3298   ins_pipe( pipe_slow );
3299 %}
3300 
3301 instruct ReplI_mem(vec dst, memory mem) %{

3302   match(Set dst (ReplicateI (LoadI mem)));
3303   format %{ &quot;replicateI $dst,$mem&quot; %}
3304   ins_encode %{
3305     uint vlen = vector_length(this);
3306     if (vlen &lt;= 4) {
<span class="line-modified">3307       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3308       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3309     } else {</span>
<span class="line-added">3310       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3311       int vector_len = vector_length_encoding(this);
3312       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);




3313     }
3314   %}
3315   ins_pipe( pipe_slow );
3316 %}
3317 
3318 instruct ReplI_imm(vec dst, immI con) %{
3319   match(Set dst (ReplicateI con));
3320   format %{ &quot;replicateI $dst,$con&quot; %}
3321   ins_encode %{
3322     uint vlen = vector_length(this);
<span class="line-modified">3323     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3324     if (vlen &lt;= 4) {</span>
<span class="line-modified">3325       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3326       if (vlen == 4) {</span>
<span class="line-added">3327         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-added">3328       }</span>
<span class="line-added">3329     } else {</span>
<span class="line-added">3330       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3331       int vector_len = vector_length_encoding(this);
<span class="line-modified">3332       __ movq($dst$$XMMRegister, const_addr);</span>
3333       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);







3334     }
3335   %}
3336   ins_pipe( pipe_slow );
3337 %}
3338 
3339 // Replicate integer (4 byte) scalar zero to be vector
3340 instruct ReplI_zero(vec dst, immI0 zero) %{
3341   match(Set dst (ReplicateI zero));
3342   format %{ &quot;replicateI $dst,$zero&quot; %}
3343   ins_encode %{
3344     uint vlen = vector_length(this);
3345     if (vlen &lt;= 4) {
3346       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3347     } else {
3348       int vlen_enc = vector_length_encoding(this);
3349       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3350     }
3351   %}
3352   ins_pipe( fpu_reg_reg );
3353 %}
</pre>
<hr />
<pre>
3423     } else {
3424       int vector_len = Assembler::AVX_512bit;
3425       __ movdl($dst$$XMMRegister, $src$$Register);
3426       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3427       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3428       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3429     }
3430   %}
3431   ins_pipe( pipe_slow );
3432 %}
3433 #endif // _LP64
3434 
3435 instruct ReplL_mem(vec dst, memory mem) %{
3436   match(Set dst (ReplicateL (LoadL mem)));
3437   format %{ &quot;replicateL $dst,$mem&quot; %}
3438   ins_encode %{
3439     uint vlen = vector_length(this);
3440     if (vlen == 2) {
3441       __ movq($dst$$XMMRegister, $mem$$Address);
3442       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3443     } else {</span>
<span class="line-added">3444       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3445       int vlen_enc = vector_length_encoding(this);
3446       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);





3447     }
3448   %}
3449   ins_pipe( pipe_slow );
3450 %}
3451 
3452 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3453 instruct ReplL_imm(vec dst, immL con) %{
3454   match(Set dst (ReplicateL con));
3455   format %{ &quot;replicateL $dst,$con&quot; %}
3456   ins_encode %{
3457     uint vlen = vector_length(this);
3458     InternalAddress const_addr = $constantaddress($con);
3459     if (vlen == 2) {
3460       __ movq($dst$$XMMRegister, const_addr);
3461       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3462     } else {</span>
<span class="line-added">3463       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3464       int vlen_enc = vector_length_encoding(this);
3465       __ movq($dst$$XMMRegister, const_addr);
3466       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);





3467     }
3468   %}
3469   ins_pipe( pipe_slow );
3470 %}
3471 
3472 instruct ReplL_zero(vec dst, immL0 zero) %{
3473   match(Set dst (ReplicateL zero));
3474   format %{ &quot;replicateL $dst,$zero&quot; %}
3475   ins_encode %{
3476     int vlen = vector_length(this);
3477     if (vlen == 2) {
3478       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3479     } else {
3480       int vlen_enc = vector_length_encoding(this);
3481       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3482     }
3483   %}
3484   ins_pipe( fpu_reg_reg );
3485 %}
3486 
3487 // ====================ReplicateF=======================================
3488 
3489 instruct ReplF_reg(vec dst, vlRegF src) %{
3490   match(Set dst (ReplicateF src));
3491   format %{ &quot;replicateF $dst,$src&quot; %}
3492   ins_encode %{
3493     uint vlen = vector_length(this);
3494     if (vlen &lt;= 4) {
3495       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3496    } else if (VM_Version::supports_avx2()) {</span>
3497       int vector_len = vector_length_encoding(this);
<span class="line-modified">3498       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3499     } else {
3500       assert(vlen == 8, &quot;sanity&quot;);
3501       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3502       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3503     }
3504   %}
3505   ins_pipe( pipe_slow );
3506 %}
3507 
3508 instruct ReplF_mem(vec dst, memory mem) %{

3509   match(Set dst (ReplicateF (LoadF mem)));
3510   format %{ &quot;replicateF $dst,$mem&quot; %}
3511   ins_encode %{
3512     uint vlen = vector_length(this);
3513     if (vlen &lt;= 4) {
<span class="line-modified">3514       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3515       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3516     } else {</span>
<span class="line-added">3517       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
3518       int vector_len = vector_length_encoding(this);
3519       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);




3520     }
3521   %}
3522   ins_pipe( pipe_slow );
3523 %}
3524 
3525 instruct ReplF_zero(vec dst, immF0 zero) %{
3526   match(Set dst (ReplicateF zero));
3527   format %{ &quot;replicateF $dst,$zero&quot; %}
3528   ins_encode %{
3529     uint vlen = vector_length(this);
3530     if (vlen &lt;= 4) {
3531       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3532     } else {
3533       int vlen_enc = vector_length_encoding(this);
3534       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3535     }
3536   %}
3537   ins_pipe( fpu_reg_reg );
3538 %}
3539 
3540 // ====================ReplicateD=======================================
3541 
3542 // Replicate double (8 bytes) scalar to be vector
3543 instruct ReplD_reg(vec dst, vlRegD src) %{
3544   match(Set dst (ReplicateD src));
3545   format %{ &quot;replicateD $dst,$src&quot; %}
3546   ins_encode %{
3547     uint vlen = vector_length(this);
3548     if (vlen == 2) {
3549       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3550     } else if (VM_Version::supports_avx2()) {</span>
3551       int vector_len = vector_length_encoding(this);
<span class="line-modified">3552       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3553     } else {
3554       assert(vlen == 4, &quot;sanity&quot;);
3555       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3556       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3557     }
3558   %}
3559   ins_pipe( pipe_slow );
3560 %}
3561 
3562 instruct ReplD_mem(vec dst, memory mem) %{

3563   match(Set dst (ReplicateD (LoadD mem)));
3564   format %{ &quot;replicateD $dst,$mem&quot; %}
3565   ins_encode %{
3566     uint vlen = vector_length(this);
3567     if (vlen == 2) {
<span class="line-modified">3568       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3569       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);</span>
<span class="line-added">3570     } else {</span>
<span class="line-added">3571       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
3572       int vector_len = vector_length_encoding(this);
3573       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);




3574     }
3575   %}
3576   ins_pipe( pipe_slow );
3577 %}
3578 
3579 instruct ReplD_zero(vec dst, immD0 zero) %{
3580   match(Set dst (ReplicateD zero));
3581   format %{ &quot;replicateD $dst,$zero&quot; %}
3582   ins_encode %{
3583     uint vlen = vector_length(this);
3584     if (vlen == 2) {
3585       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3586     } else {
3587       int vlen_enc = vector_length_encoding(this);
3588       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3589     }
3590   %}
3591   ins_pipe( fpu_reg_reg );
3592 %}
3593 
</pre>
<hr />
<pre>
5649   ins_encode %{
5650     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5651   %}
5652   ins_pipe( pipe_slow );
5653 %}
5654 
5655 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5656   predicate(UseAVX &gt; 0);
5657   match(Set dst (MulAddVS2VI src1 src2));
5658   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5659   ins_encode %{
5660     int vector_len = vector_length_encoding(this);
5661     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5662   %}
5663   ins_pipe( pipe_slow );
5664 %}
5665 
5666 // --------------------------------- Vector Multiply Add Add ----------------------------------
5667 
5668 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5669   predicate(VM_Version::supports_avx512_vnni());</span>
5670   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5671   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5672   ins_encode %{
5673     assert(UseAVX &gt; 2, &quot;required&quot;);
5674     int vector_len = vector_length_encoding(this);
5675     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5676   %}
5677   ins_pipe( pipe_slow );
5678   ins_cost(10);
5679 %}
5680 
5681 // --------------------------------- PopCount --------------------------------------
5682 
5683 instruct vpopcountI(vec dst, vec src) %{
5684   match(Set dst (PopCountVI src));
5685   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5686   ins_encode %{
5687     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5688 
5689     int vector_len = vector_length_encoding(this);
</pre>
</td>
</tr>
</table>
<center><a href="vm_version_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_64.ad.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>