<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/stubGenerator_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroAssembler_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/stubGenerator_x86_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
4428   __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4429   __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4430   __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4431   __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4432   __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4433   __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4434 }
4435 
4436   void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {
4437     __ movdqu(xmmdst, Address(key, offset));
4438     if (xmm_shuf_mask != NULL) {
4439       __ pshufb(xmmdst, xmm_shuf_mask);
4440     } else {
4441       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4442     }
4443     __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);
4444 
4445   }
4446 
4447 address generate_cipherBlockChaining_decryptVectorAESCrypt() {
<span class="line-modified">4448     assert(VM_Version::supports_vaes(), &quot;need AES instructions and misaligned SSE support&quot;);</span>
4449     __ align(CodeEntryAlignment);
4450     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
4451     address start = __ pc();
4452 
4453     const Register from = c_rarg0;  // source array address
4454     const Register to = c_rarg1;  // destination array address
4455     const Register key = c_rarg2;  // key array address
4456     const Register rvec = c_rarg3;  // r byte array initialized from initvector array address
4457     // and left with the results of the last encryption block
4458 #ifndef _WIN64
4459     const Register len_reg = c_rarg4;  // src len (must be multiple of blocksize 16)
4460 #else
4461     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
4462     const Register len_reg = r11;      // pick the volatile windows register
4463 #endif
4464 
4465     Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,
4466           Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;
4467 
4468     __ enter();
</pre>
<hr />
<pre>
5733 #endif
5734     __ push(tmp5);
5735 
5736     // Rename temps used throughout the code.
5737     const Register idx = tmp1;
5738     const Register nIdx = tmp2;
5739 
5740     __ xorl(idx, idx);
5741 
5742     // Start right shift from end of the array.
5743     // For example, if #iteration = 4 and newIdx = 1
5744     // then dest[4] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5745     // if #iteration = 4 and newIdx = 0
5746     // then dest[3] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5747     __ movl(idx, totalNumIter);
5748     __ movl(nIdx, idx);
5749     __ addl(nIdx, newIdx);
5750 
5751     // If vectorization is enabled, check if the number of iterations is at least 64
5752     // If not, then go to ShifTwo processing 2 iterations
<span class="line-modified">5753     if (VM_Version::supports_vbmi2()) {</span>
5754       __ cmpptr(totalNumIter, (AVX3Threshold/64));
5755       __ jcc(Assembler::less, ShiftTwo);
5756 
5757       if (AVX3Threshold &lt; 16 * 64) {
5758         __ cmpl(totalNumIter, 16);
5759         __ jcc(Assembler::less, ShiftTwo);
5760       }
5761       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5762       __ subl(idx, 16);
5763       __ subl(nIdx, 16);
5764       __ BIND(Shift512Loop);
5765       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);
5766       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5767       __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);
5768       __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);
5769       __ subl(nIdx, 16);
5770       __ subl(idx, 16);
5771       __ jcc(Assembler::greaterEqual, Shift512Loop);
5772       __ addl(idx, 16);
5773       __ addl(nIdx, 16);
</pre>
<hr />
<pre>
5857     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5858     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5859     // Save callee save registers.
5860     __ push(tmp3);
5861     __ push(tmp4);
5862 #endif
5863     __ push(tmp5);
5864 
5865     // Rename temps used throughout the code
5866     const Register idx = tmp1;
5867     const Register numIterTmp = tmp2;
5868 
5869     // Start idx from zero.
5870     __ xorl(idx, idx);
5871     // Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.
5872     __ lea(newArr, Address(newArr, newIdx, Address::times_4));
5873     __ movl(numIterTmp, totalNumIter);
5874 
5875     // If vectorization is enabled, check if the number of iterations is at least 64
5876     // If not, then go to ShiftTwo shifting two numbers at a time
<span class="line-modified">5877     if (VM_Version::supports_vbmi2()) {</span>
5878       __ cmpl(totalNumIter, (AVX3Threshold/64));
5879       __ jcc(Assembler::less, ShiftTwo);
5880 
5881       if (AVX3Threshold &lt; 16 * 64) {
5882         __ cmpl(totalNumIter, 16);
5883         __ jcc(Assembler::less, ShiftTwo);
5884       }
5885       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5886       __ subl(numIterTmp, 16);
5887       __ BIND(Shift512Loop);
5888       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5889       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);
5890       __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);
5891       __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);
5892       __ addl(idx, 16);
5893       __ subl(numIterTmp, 16);
5894       __ jcc(Assembler::greaterEqual, Shift512Loop);
5895       __ addl(numIterTmp, 16);
5896     }
5897     __ BIND(ShiftTwo);
</pre>
<hr />
<pre>
6449     StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(&quot;vector_short_to_byte_mask&quot;, 0x00ff00ff00ff00ff);
6450     StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(&quot;vector_byte_perm_mask&quot;);
6451     StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(&quot;vector_long_sign_mask&quot;, 0x8000000000000000);
6452 
6453     // support for verify_oop (must happen after universe_init)
6454     StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();
6455 
6456     // data cache line writeback
6457     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6458     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6459 
6460     // arraycopy stubs used by compilers
6461     generate_arraycopy_stubs();
6462 
6463     // don&#39;t bother generating these AES intrinsic stubs unless global flag is set
6464     if (UseAESIntrinsics) {
6465       StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  // needed by the others
6466       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6467       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6468       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
<span class="line-modified">6469       if (VM_Version::supports_vaes() &amp;&amp;  VM_Version::supports_avx512vl() &amp;&amp; VM_Version::supports_avx512dq() ) {</span>
6470         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();
6471         StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();
6472         StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();
6473       } else {
6474         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();
6475       }
6476     }
6477     if (UseAESCTRIntrinsics) {
<span class="line-modified">6478       if (VM_Version::supports_vaes() &amp;&amp; VM_Version::supports_avx512bw() &amp;&amp; VM_Version::supports_avx512vl()) {</span>
6479         StubRoutines::x86::_counter_mask_addr = counter_mask_addr();
6480         StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();
6481       } else {
6482         StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();
6483         StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();
6484       }
6485     }
6486 
6487     if (UseSHA1Intrinsics) {
6488       StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();
6489       StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();
6490       StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, &quot;sha1_implCompress&quot;);
6491       StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, &quot;sha1_implCompressMB&quot;);
6492     }
6493     if (UseSHA256Intrinsics) {
6494       StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;
6495       char* dst = (char*)StubRoutines::x86::_k256_W;
6496       char* src = (char*)StubRoutines::x86::_k256;
6497       for (int ii = 0; ii &lt; 16; ++ii) {
6498         memcpy(dst + 32 * ii,      src + 16 * ii, 16);
</pre>
<hr />
<pre>
6539                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6540                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6541     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6542                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6543                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6544 
6545     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
6546     if (bs_nm != NULL) {
6547       StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();
6548     }
6549 #ifdef COMPILER2
6550     if (UseMultiplyToLenIntrinsic) {
6551       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6552     }
6553     if (UseSquareToLenIntrinsic) {
6554       StubRoutines::_squareToLen = generate_squareToLen();
6555     }
6556     if (UseMulAddIntrinsic) {
6557       StubRoutines::_mulAdd = generate_mulAdd();
6558     }
<span class="line-modified">6559     if (VM_Version::supports_vbmi2()) {</span>
6560       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
6561       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
6562     }
6563 #ifndef _WINDOWS
6564     if (UseMontgomeryMultiplyIntrinsic) {
6565       StubRoutines::_montgomeryMultiply
6566         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
6567     }
6568     if (UseMontgomerySquareIntrinsic) {
6569       StubRoutines::_montgomerySquare
6570         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
6571     }
6572 #endif // WINDOWS
6573 #endif // COMPILER2
6574 
6575     if (UseVectorizedMismatchIntrinsic) {
6576       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
6577     }
6578   }
6579 
</pre>
</td>
<td>
<hr />
<pre>
4428   __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4429   __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4430   __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4431   __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4432   __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4433   __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4434 }
4435 
4436   void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {
4437     __ movdqu(xmmdst, Address(key, offset));
4438     if (xmm_shuf_mask != NULL) {
4439       __ pshufb(xmmdst, xmm_shuf_mask);
4440     } else {
4441       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4442     }
4443     __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);
4444 
4445   }
4446 
4447 address generate_cipherBlockChaining_decryptVectorAESCrypt() {
<span class="line-modified">4448     assert(VM_Version::supports_avx512_vaes(), &quot;need AES instructions and misaligned SSE support&quot;);</span>
4449     __ align(CodeEntryAlignment);
4450     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
4451     address start = __ pc();
4452 
4453     const Register from = c_rarg0;  // source array address
4454     const Register to = c_rarg1;  // destination array address
4455     const Register key = c_rarg2;  // key array address
4456     const Register rvec = c_rarg3;  // r byte array initialized from initvector array address
4457     // and left with the results of the last encryption block
4458 #ifndef _WIN64
4459     const Register len_reg = c_rarg4;  // src len (must be multiple of blocksize 16)
4460 #else
4461     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
4462     const Register len_reg = r11;      // pick the volatile windows register
4463 #endif
4464 
4465     Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,
4466           Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;
4467 
4468     __ enter();
</pre>
<hr />
<pre>
5733 #endif
5734     __ push(tmp5);
5735 
5736     // Rename temps used throughout the code.
5737     const Register idx = tmp1;
5738     const Register nIdx = tmp2;
5739 
5740     __ xorl(idx, idx);
5741 
5742     // Start right shift from end of the array.
5743     // For example, if #iteration = 4 and newIdx = 1
5744     // then dest[4] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5745     // if #iteration = 4 and newIdx = 0
5746     // then dest[3] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5747     __ movl(idx, totalNumIter);
5748     __ movl(nIdx, idx);
5749     __ addl(nIdx, newIdx);
5750 
5751     // If vectorization is enabled, check if the number of iterations is at least 64
5752     // If not, then go to ShifTwo processing 2 iterations
<span class="line-modified">5753     if (VM_Version::supports_avx512_vbmi2()) {</span>
5754       __ cmpptr(totalNumIter, (AVX3Threshold/64));
5755       __ jcc(Assembler::less, ShiftTwo);
5756 
5757       if (AVX3Threshold &lt; 16 * 64) {
5758         __ cmpl(totalNumIter, 16);
5759         __ jcc(Assembler::less, ShiftTwo);
5760       }
5761       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5762       __ subl(idx, 16);
5763       __ subl(nIdx, 16);
5764       __ BIND(Shift512Loop);
5765       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);
5766       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5767       __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);
5768       __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);
5769       __ subl(nIdx, 16);
5770       __ subl(idx, 16);
5771       __ jcc(Assembler::greaterEqual, Shift512Loop);
5772       __ addl(idx, 16);
5773       __ addl(nIdx, 16);
</pre>
<hr />
<pre>
5857     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5858     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5859     // Save callee save registers.
5860     __ push(tmp3);
5861     __ push(tmp4);
5862 #endif
5863     __ push(tmp5);
5864 
5865     // Rename temps used throughout the code
5866     const Register idx = tmp1;
5867     const Register numIterTmp = tmp2;
5868 
5869     // Start idx from zero.
5870     __ xorl(idx, idx);
5871     // Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.
5872     __ lea(newArr, Address(newArr, newIdx, Address::times_4));
5873     __ movl(numIterTmp, totalNumIter);
5874 
5875     // If vectorization is enabled, check if the number of iterations is at least 64
5876     // If not, then go to ShiftTwo shifting two numbers at a time
<span class="line-modified">5877     if (VM_Version::supports_avx512_vbmi2()) {</span>
5878       __ cmpl(totalNumIter, (AVX3Threshold/64));
5879       __ jcc(Assembler::less, ShiftTwo);
5880 
5881       if (AVX3Threshold &lt; 16 * 64) {
5882         __ cmpl(totalNumIter, 16);
5883         __ jcc(Assembler::less, ShiftTwo);
5884       }
5885       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5886       __ subl(numIterTmp, 16);
5887       __ BIND(Shift512Loop);
5888       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5889       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);
5890       __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);
5891       __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);
5892       __ addl(idx, 16);
5893       __ subl(numIterTmp, 16);
5894       __ jcc(Assembler::greaterEqual, Shift512Loop);
5895       __ addl(numIterTmp, 16);
5896     }
5897     __ BIND(ShiftTwo);
</pre>
<hr />
<pre>
6449     StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(&quot;vector_short_to_byte_mask&quot;, 0x00ff00ff00ff00ff);
6450     StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(&quot;vector_byte_perm_mask&quot;);
6451     StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(&quot;vector_long_sign_mask&quot;, 0x8000000000000000);
6452 
6453     // support for verify_oop (must happen after universe_init)
6454     StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();
6455 
6456     // data cache line writeback
6457     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6458     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6459 
6460     // arraycopy stubs used by compilers
6461     generate_arraycopy_stubs();
6462 
6463     // don&#39;t bother generating these AES intrinsic stubs unless global flag is set
6464     if (UseAESIntrinsics) {
6465       StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  // needed by the others
6466       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6467       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6468       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
<span class="line-modified">6469       if (VM_Version::supports_avx512_vaes() &amp;&amp;  VM_Version::supports_avx512vl() &amp;&amp; VM_Version::supports_avx512dq() ) {</span>
6470         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();
6471         StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();
6472         StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();
6473       } else {
6474         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();
6475       }
6476     }
6477     if (UseAESCTRIntrinsics) {
<span class="line-modified">6478       if (VM_Version::supports_avx512_vaes() &amp;&amp; VM_Version::supports_avx512bw() &amp;&amp; VM_Version::supports_avx512vl()) {</span>
6479         StubRoutines::x86::_counter_mask_addr = counter_mask_addr();
6480         StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();
6481       } else {
6482         StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();
6483         StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();
6484       }
6485     }
6486 
6487     if (UseSHA1Intrinsics) {
6488       StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();
6489       StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();
6490       StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, &quot;sha1_implCompress&quot;);
6491       StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, &quot;sha1_implCompressMB&quot;);
6492     }
6493     if (UseSHA256Intrinsics) {
6494       StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;
6495       char* dst = (char*)StubRoutines::x86::_k256_W;
6496       char* src = (char*)StubRoutines::x86::_k256;
6497       for (int ii = 0; ii &lt; 16; ++ii) {
6498         memcpy(dst + 32 * ii,      src + 16 * ii, 16);
</pre>
<hr />
<pre>
6539                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6540                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6541     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6542                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6543                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6544 
6545     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
6546     if (bs_nm != NULL) {
6547       StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();
6548     }
6549 #ifdef COMPILER2
6550     if (UseMultiplyToLenIntrinsic) {
6551       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6552     }
6553     if (UseSquareToLenIntrinsic) {
6554       StubRoutines::_squareToLen = generate_squareToLen();
6555     }
6556     if (UseMulAddIntrinsic) {
6557       StubRoutines::_mulAdd = generate_mulAdd();
6558     }
<span class="line-modified">6559     if (VM_Version::supports_avx512_vbmi2()) {</span>
6560       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
6561       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
6562     }
6563 #ifndef _WINDOWS
6564     if (UseMontgomeryMultiplyIntrinsic) {
6565       StubRoutines::_montgomeryMultiply
6566         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
6567     }
6568     if (UseMontgomerySquareIntrinsic) {
6569       StubRoutines::_montgomerySquare
6570         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
6571     }
6572 #endif // WINDOWS
6573 #endif // COMPILER2
6574 
6575     if (UseVectorizedMismatchIntrinsic) {
6576       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
6577     }
6578   }
6579 
</pre>
</td>
</tr>
</table>
<center><a href="macroAssembler_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>