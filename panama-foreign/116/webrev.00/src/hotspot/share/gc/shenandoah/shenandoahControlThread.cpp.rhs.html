<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 32 #include &quot;gc/shenandoah/shenandoahHeuristics.hpp&quot;
 33 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
 34 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
 35 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
 36 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
 37 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
 38 #include &quot;memory/iterator.hpp&quot;
 39 #include &quot;memory/universe.hpp&quot;
 40 #include &quot;runtime/atomic.hpp&quot;
 41 
 42 ShenandoahControlThread::ShenandoahControlThread() :
 43   ConcurrentGCThread(),
 44   _alloc_failure_waiters_lock(Mutex::leaf, &quot;ShenandoahAllocFailureGC_lock&quot;, true, Monitor::_safepoint_check_always),
 45   _gc_waiters_lock(Mutex::leaf, &quot;ShenandoahRequestedGC_lock&quot;, true, Monitor::_safepoint_check_always),
 46   _periodic_task(this),
 47   _requested_gc_cause(GCCause::_no_cause_specified),
 48   _degen_point(ShenandoahHeap::_degenerated_outside_cycle),
 49   _allocs_seen(0) {
 50 
 51   create_and_start(ShenandoahCriticalControlThreadPriority ? CriticalPriority : NearMaxPriority);
 52   _periodic_task.enroll();
 53   _periodic_satb_flush_task.enroll();
 54 }
 55 
 56 ShenandoahControlThread::~ShenandoahControlThread() {
 57   // This is here so that super is called.
 58 }
 59 
 60 void ShenandoahPeriodicTask::task() {
 61   _thread-&gt;handle_force_counters_update();
 62   _thread-&gt;handle_counters_update();
 63 }
 64 
 65 void ShenandoahPeriodicSATBFlushTask::task() {
 66   ShenandoahHeap::heap()-&gt;force_satb_flush_all_threads();
 67 }
 68 
 69 void ShenandoahControlThread::run_service() {
 70   ShenandoahHeap* heap = ShenandoahHeap::heap();
 71 
 72   GCMode default_mode = concurrent_normal;
 73   GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;
 74   int sleep = ShenandoahControlIntervalMin;
 75 
 76   double last_shrink_time = os::elapsedTime();
 77   double last_sleep_adjust_time = os::elapsedTime();
 78 
 79   // Shrink period avoids constantly polling regions for shrinking.
 80   // Having a period 10x lower than the delay would mean we hit the
 81   // shrinking with lag of less than 1/10-th of true delay.
 82   // ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.
 83   double shrink_period = (double)ShenandoahUncommitDelay / 1000 / 10;
 84 
 85   ShenandoahCollectorPolicy* policy = heap-&gt;shenandoah_policy();
 86   ShenandoahHeuristics* heuristics = heap-&gt;heuristics();
 87   while (!in_graceful_shutdown() &amp;&amp; !should_terminate()) {
 88     // Figure out if we have pending requests.
 89     bool alloc_failure_pending = _alloc_failure_gc.is_set();
 90     bool explicit_gc_requested = _gc_requested.is_set() &amp;&amp;  is_explicit_gc(_requested_gc_cause);
 91     bool implicit_gc_requested = _gc_requested.is_set() &amp;&amp; !is_explicit_gc(_requested_gc_cause);
 92 
 93     // This control loop iteration have seen this much allocations.
 94     size_t allocs_seen = Atomic::xchg(&amp;_allocs_seen, (size_t)0);
 95 
 96     // Choose which GC mode to run in. The block below should select a single mode.
 97     GCMode mode = none;
 98     GCCause::Cause cause = GCCause::_last_gc_cause;
 99     ShenandoahHeap::ShenandoahDegenPoint degen_point = ShenandoahHeap::_degenerated_unset;
100 
101     if (alloc_failure_pending) {
102       // Allocation failure takes precedence: we have to deal with it first thing
103       log_info(gc)(&quot;Trigger: Handle Allocation Failure&quot;);
104 
105       cause = GCCause::_allocation_failure;
106 
107       // Consume the degen point, and seed it with default value
108       degen_point = _degen_point;
109       _degen_point = ShenandoahHeap::_degenerated_outside_cycle;
110 
111       if (ShenandoahDegeneratedGC &amp;&amp; heuristics-&gt;should_degenerate_cycle()) {
112         heuristics-&gt;record_allocation_failure_gc();
113         policy-&gt;record_alloc_failure_to_degenerated(degen_point);
114         mode = stw_degenerated;
115       } else {
116         heuristics-&gt;record_allocation_failure_gc();
117         policy-&gt;record_alloc_failure_to_full();
118         mode = stw_full;
119       }
120 
121     } else if (explicit_gc_requested) {
122       cause = _requested_gc_cause;
123       log_info(gc)(&quot;Trigger: Explicit GC request (%s)&quot;, GCCause::to_string(cause));
124 
125       heuristics-&gt;record_requested_gc();
126 
127       if (ExplicitGCInvokesConcurrent) {
128         policy-&gt;record_explicit_to_concurrent();
129         mode = default_mode;
130         // Unload and clean up everything
131         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
132         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
133       } else {
134         policy-&gt;record_explicit_to_full();
135         mode = stw_full;
136       }
137     } else if (implicit_gc_requested) {
138       cause = _requested_gc_cause;
139       log_info(gc)(&quot;Trigger: Implicit GC request (%s)&quot;, GCCause::to_string(cause));
140 
141       heuristics-&gt;record_requested_gc();
142 
143       if (ShenandoahImplicitGCInvokesConcurrent) {
144         policy-&gt;record_implicit_to_concurrent();
145         mode = default_mode;
146 
147         // Unload and clean up everything
148         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
149         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
150       } else {
151         policy-&gt;record_implicit_to_full();
152         mode = stw_full;
153       }
154     } else {
155       // Potential normal cycle: ask heuristics if it wants to act
156       if (heuristics-&gt;should_start_gc()) {
157         mode = default_mode;
158         cause = default_cause;
159       }
160 
161       // Ask policy if this cycle wants to process references or unload classes
162       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
163       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
164     }
165 
166     // Blow all soft references on this cycle, if handling allocation failure,
167     // or we are requested to do so unconditionally.
168     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
169       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
170     }
171 
172     bool gc_requested = (mode != none);
173     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
174 
175     if (gc_requested) {
176       heap-&gt;reset_bytes_allocated_since_gc_start();
177 
178       // Use default constructor to snapshot the Metaspace state before GC.
179       metaspace::MetaspaceSizesSnapshot meta_sizes;
180 
181       // If GC was requested, we are sampling the counters even without actual triggers
182       // from allocation machinery. This captures GC phases more accurately.
183       set_forced_counters_update(true);
184 
185       // If GC was requested, we better dump freeset data for performance debugging
186       {
187         ShenandoahHeapLocker locker(heap-&gt;lock());
188         heap-&gt;free_set()-&gt;log_status();
189       }
190 
191       switch (mode) {
192         case concurrent_normal:
193           service_concurrent_normal_cycle(cause);
194           break;
195         case stw_degenerated:
196           service_stw_degenerated_cycle(cause, degen_point);
197           break;
198         case stw_full:
199           service_stw_full_cycle(cause);
200           break;
201         default:
202           ShouldNotReachHere();
203       }
204 
205       // If this was the requested GC cycle, notify waiters about it
206       if (explicit_gc_requested || implicit_gc_requested) {
207         notify_gc_waiters();
208       }
209 
210       // If this was the allocation failure GC cycle, notify waiters about it
211       if (alloc_failure_pending) {
212         notify_alloc_failure_waiters();
213       }
214 
215       // Report current free set state at the end of cycle, whether
216       // it is a normal completion, or the abort.
217       {
218         ShenandoahHeapLocker locker(heap-&gt;lock());
219         heap-&gt;free_set()-&gt;log_status();
220 
221         // Notify Universe about new heap usage. This has implications for
222         // global soft refs policy, and we better report it every time heap
223         // usage goes down.
224         Universe::update_heap_info_at_gc();
225       }
226 
227       // Disable forced counters update, and update counters one more time
228       // to capture the state at the end of GC session.
229       handle_force_counters_update();
230       set_forced_counters_update(false);
231 
232       // Retract forceful part of soft refs policy
233       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(false);
234 
235       // Clear metaspace oom flag, if current cycle unloaded classes
236       if (heap-&gt;unload_classes()) {
237         heuristics-&gt;clear_metaspace_oom();
238       }
239 
240       // Print Metaspace change following GC (if logging is enabled).
241       MetaspaceUtils::print_metaspace_change(meta_sizes);
242 
243       // GC is over, we are at idle now
244       if (ShenandoahPacing) {
245         heap-&gt;pacer()-&gt;setup_for_idle();
246       }
247     } else {
248       // Allow allocators to know we have seen this much regions
249       if (ShenandoahPacing &amp;&amp; (allocs_seen &gt; 0)) {
250         heap-&gt;pacer()-&gt;report_alloc(allocs_seen);
251       }
252     }
253 
254     double current = os::elapsedTime();
255 
256     if (ShenandoahUncommit &amp;&amp; (explicit_gc_requested || (current - last_shrink_time &gt; shrink_period))) {
257       // Try to uncommit enough stale regions. Explicit GC tries to uncommit everything.
258       // Regular paths uncommit only occasionally.
259       double shrink_before = explicit_gc_requested ?
260                              current :
261                              current - (ShenandoahUncommitDelay / 1000.0);
262       service_uncommit(shrink_before);
263       last_shrink_time = current;
264     }
265 
266     // Wait before performing the next action. If allocation happened during this wait,
267     // we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,
268     // back off exponentially.
269     if (_heap_changed.try_unset()) {
270       sleep = ShenandoahControlIntervalMin;
271     } else if ((current - last_sleep_adjust_time) * 1000 &gt; ShenandoahControlIntervalAdjustPeriod){
272       sleep = MIN2&lt;int&gt;(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));
273       last_sleep_adjust_time = current;
274     }
275     os::naked_short_sleep(sleep);
276   }
277 
278   // Wait for the actual stop(), can&#39;t leave run_service() earlier.
279   while (!should_terminate()) {
280     os::naked_short_sleep(ShenandoahControlIntervalMin);
281   }
282 }
283 
284 void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {
285   // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
286   // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
287   // If second allocation failure happens during Degenerated GC cycle (for example, when GC
288   // tries to evac something and no memory is available), cycle degrades to Full GC.
289   //
290   // There are also a shortcut through the normal cycle: immediate garbage shortcut, when
291   // heuristics says there are no regions to compact, and all the collection comes from immediately
292   // reclaimable regions.
293   //
294   // ................................................................................................
295   //
296   //                                    (immediate garbage shortcut)                Concurrent GC
297   //                             /-------------------------------------------\
298   //                             |                                           |
299   //                             |                                           |
300   //                             |                                           |
301   //                             |                                           v
302   // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
303   //                   |                    |                 |              ^
304   //                   | (af)               | (af)            | (af)         |
305   // ..................|....................|.................|..............|.......................
306   //                   |                    |                 |              |
307   //                   |                    |                 |              |      Degenerated GC
308   //                   v                    v                 v              |
309   //               STW Mark ----------&gt; STW Evac ----&gt; STW Update-Refs -----&gt;o
310   //                   |                    |                 |              ^
311   //                   | (af)               | (af)            | (af)         |
312   // ..................|....................|.................|..............|.......................
313   //                   |                    |                 |              |
314   //                   |                    v                 |              |      Full GC
315   //                   \-------------------&gt;o&lt;----------------/              |
316   //                                        |                                |
317   //                                        v                                |
318   //                                      Full GC  --------------------------/
319   //
320   ShenandoahHeap* heap = ShenandoahHeap::heap();
321 
322   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_outside_cycle)) return;
323 
324   GCIdMark gc_id_mark;
325   ShenandoahGCSession session(cause);
326 
327   TraceCollectorStats tcs(heap-&gt;monitoring_support()-&gt;concurrent_collection_counters());
328 
329   // Reset for upcoming marking
330   heap-&gt;entry_reset();
331 
332   // Start initial mark under STW
333   heap-&gt;vmop_entry_init_mark();
334 
335   // Continue concurrent mark
336   heap-&gt;entry_mark();
337   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_mark)) return;
338 
339   // If not cancelled, can try to concurrently pre-clean
340   heap-&gt;entry_preclean();
341 
342   // Complete marking under STW, and start evacuation
343   heap-&gt;vmop_entry_final_mark();
344 
<a name="1" id="anc1"></a><span class="line-modified">345   // Process weak roots that might still point to regions that would be broken by cleanup</span>
<span class="line-modified">346   heap-&gt;entry_weak_roots();</span>
347 
348   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
349   // the space. This would be the last action if there is nothing to evacuate.
350   heap-&gt;entry_cleanup();
351 
352   {
353     ShenandoahHeapLocker locker(heap-&gt;lock());
354     heap-&gt;free_set()-&gt;log_status();
355   }
356 
<a name="2" id="anc2"></a><span class="line-added">357   // Processing strong roots</span>
<span class="line-added">358   // This may be skipped if there is nothing to update/evacuate.</span>
<span class="line-added">359   // If so, strong_root_in_progress would be unset.</span>
<span class="line-added">360   if (heap-&gt;is_concurrent_strong_root_in_progress()) {</span>
<span class="line-added">361     heap-&gt;entry_strong_roots();</span>
<span class="line-added">362   }</span>
<span class="line-added">363 </span>
364   // Continue the cycle with evacuation and optional update-refs.
365   // This may be skipped if there is nothing to evacuate.
366   // If so, evac_in_progress would be unset by collection set preparation code.
367   if (heap-&gt;is_evacuation_in_progress()) {
368     // Concurrently evacuate
369     heap-&gt;entry_evac();
370     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
371 
372     // Perform update-refs phase.
373     heap-&gt;vmop_entry_init_updaterefs();
374     heap-&gt;entry_updaterefs();
375     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;
376 
377     heap-&gt;vmop_entry_final_updaterefs();
378 
379     // Update references freed up collection set, kick the cleanup to reclaim the space.
380     heap-&gt;entry_cleanup();
381   }
382 
383   // Cycle is complete
384   heap-&gt;heuristics()-&gt;record_success_concurrent();
385   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
386 }
387 
388 bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point) {
389   ShenandoahHeap* heap = ShenandoahHeap::heap();
390   if (heap-&gt;cancelled_gc()) {
391     assert (is_alloc_failure_gc() || in_graceful_shutdown(), &quot;Cancel GC either for alloc failure GC, or gracefully exiting&quot;);
392     if (!in_graceful_shutdown()) {
393       assert (_degen_point == ShenandoahHeap::_degenerated_outside_cycle,
394               &quot;Should not be set yet: %s&quot;, ShenandoahHeap::degen_point_to_string(_degen_point));
395       _degen_point = point;
396     }
397     return true;
398   }
399   return false;
400 }
401 
402 void ShenandoahControlThread::stop_service() {
403   // Nothing to do here.
404 }
405 
406 void ShenandoahControlThread::service_stw_full_cycle(GCCause::Cause cause) {
407   GCIdMark gc_id_mark;
408   ShenandoahGCSession session(cause);
409 
410   ShenandoahHeap* heap = ShenandoahHeap::heap();
411   heap-&gt;vmop_entry_full(cause);
412 
413   heap-&gt;heuristics()-&gt;record_success_full();
414   heap-&gt;shenandoah_policy()-&gt;record_success_full();
415 }
416 
417 void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point) {
418   assert (point != ShenandoahHeap::_degenerated_unset, &quot;Degenerated point should be set&quot;);
419 
420   GCIdMark gc_id_mark;
421   ShenandoahGCSession session(cause);
422 
423   ShenandoahHeap* heap = ShenandoahHeap::heap();
424   heap-&gt;vmop_degenerated(point);
425 
426   heap-&gt;heuristics()-&gt;record_success_degenerated();
427   heap-&gt;shenandoah_policy()-&gt;record_success_degenerated();
428 }
429 
430 void ShenandoahControlThread::service_uncommit(double shrink_before) {
431   ShenandoahHeap* heap = ShenandoahHeap::heap();
432 
433   // Determine if there is work to do. This avoids taking heap lock if there is
434   // no work available, avoids spamming logs with superfluous logging messages,
435   // and minimises the amount of work while locks are taken.
436 
437   if (heap-&gt;committed() &lt;= heap-&gt;min_capacity()) return;
438 
439   bool has_work = false;
440   for (size_t i = 0; i &lt; heap-&gt;num_regions(); i++) {
441     ShenandoahHeapRegion *r = heap-&gt;get_region(i);
442     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
443       has_work = true;
444       break;
445     }
446   }
447 
448   if (has_work) {
449     heap-&gt;entry_uncommit(shrink_before);
450   }
451 }
452 
453 bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {
454   return GCCause::is_user_requested_gc(cause) ||
455          GCCause::is_serviceability_requested_gc(cause);
456 }
457 
458 void ShenandoahControlThread::request_gc(GCCause::Cause cause) {
459   assert(GCCause::is_user_requested_gc(cause) ||
460          GCCause::is_serviceability_requested_gc(cause) ||
461          cause == GCCause::_metadata_GC_clear_soft_refs ||
462          cause == GCCause::_full_gc_alot ||
463          cause == GCCause::_wb_full_gc ||
464          cause == GCCause::_scavenge_alot,
465          &quot;only requested GCs here&quot;);
466 
467   if (is_explicit_gc(cause)) {
468     if (!DisableExplicitGC) {
469       handle_requested_gc(cause);
470     }
471   } else {
472     handle_requested_gc(cause);
473   }
474 }
475 
476 void ShenandoahControlThread::handle_requested_gc(GCCause::Cause cause) {
477   _requested_gc_cause = cause;
478   _gc_requested.set();
479   MonitorLocker ml(&amp;_gc_waiters_lock);
480   while (_gc_requested.is_set()) {
481     ml.wait();
482   }
483 }
484 
485 void ShenandoahControlThread::handle_alloc_failure(ShenandoahAllocRequest&amp; req) {
486   ShenandoahHeap* heap = ShenandoahHeap::heap();
487 
488   assert(current()-&gt;is_Java_thread(), &quot;expect Java thread here&quot;);
489 
490   if (try_set_alloc_failure_gc()) {
491     // Only report the first allocation failure
492     log_info(gc)(&quot;Failed to allocate %s, &quot; SIZE_FORMAT &quot;%s&quot;,
493                  req.type_string(),
494                  byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));
495 
496     // Now that alloc failure GC is scheduled, we can abort everything else
497     heap-&gt;cancel_gc(GCCause::_allocation_failure);
498   }
499 
500   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);
501   while (is_alloc_failure_gc()) {
502     ml.wait();
503   }
504 }
505 
506 void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {
507   ShenandoahHeap* heap = ShenandoahHeap::heap();
508 
509   if (try_set_alloc_failure_gc()) {
510     // Only report the first allocation failure
511     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s for evacuation&quot;,
512                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
513   }
514 
515   // Forcefully report allocation failure
516   heap-&gt;cancel_gc(GCCause::_shenandoah_allocation_failure_evac);
517 }
518 
519 void ShenandoahControlThread::notify_alloc_failure_waiters() {
520   _alloc_failure_gc.unset();
521   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);
522   ml.notify_all();
523 }
524 
525 bool ShenandoahControlThread::try_set_alloc_failure_gc() {
526   return _alloc_failure_gc.try_set();
527 }
528 
529 bool ShenandoahControlThread::is_alloc_failure_gc() {
530   return _alloc_failure_gc.is_set();
531 }
532 
533 void ShenandoahControlThread::notify_gc_waiters() {
534   _gc_requested.unset();
535   MonitorLocker ml(&amp;_gc_waiters_lock);
536   ml.notify_all();
537 }
538 
539 void ShenandoahControlThread::handle_counters_update() {
540   if (_do_counters_update.is_set()) {
541     _do_counters_update.unset();
542     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
543   }
544 }
545 
546 void ShenandoahControlThread::handle_force_counters_update() {
547   if (_force_counters_update.is_set()) {
548     _do_counters_update.unset(); // reset these too, we do update now!
549     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
550   }
551 }
552 
553 void ShenandoahControlThread::notify_heap_changed() {
554   // This is called from allocation path, and thus should be fast.
555 
556   // Update monitoring counters when we took a new region. This amortizes the
557   // update costs on slow path.
558   if (_do_counters_update.is_unset()) {
559     _do_counters_update.set();
560   }
561   // Notify that something had changed.
562   if (_heap_changed.is_unset()) {
563     _heap_changed.set();
564   }
565 }
566 
567 void ShenandoahControlThread::pacing_notify_alloc(size_t words) {
568   assert(ShenandoahPacing, &quot;should only call when pacing is enabled&quot;);
569   Atomic::add(&amp;_allocs_seen, words);
570 }
571 
572 void ShenandoahControlThread::set_forced_counters_update(bool value) {
573   _force_counters_update.set_cond(value);
574 }
575 
576 void ShenandoahControlThread::print() const {
577   print_on(tty);
578 }
579 
580 void ShenandoahControlThread::print_on(outputStream* st) const {
581   st-&gt;print(&quot;Shenandoah Concurrent Thread&quot;);
582   Thread::print_on(st);
583   st-&gt;cr();
584 }
585 
586 void ShenandoahControlThread::start() {
587   create_and_start();
588 }
589 
590 void ShenandoahControlThread::prepare_for_graceful_shutdown() {
591   _graceful_shutdown.set();
592 }
593 
594 bool ShenandoahControlThread::in_graceful_shutdown() {
595   return _graceful_shutdown.is_set();
596 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>