<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="c2/g1BarrierSetC2.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1126 
1127   collector.prepare_collection();
1128   collector.collect();
1129   collector.complete_collection();
1130 
1131   // Full collection was successfully completed.
1132   return true;
1133 }
1134 
1135 void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {
1136   // Currently, there is no facility in the do_full_collection(bool) API to notify
1137   // the caller that the collection did not succeed (e.g., because it was locked
1138   // out by the GC locker). So, right now, we&#39;ll ignore the return value.
1139   bool dummy = do_full_collection(true,                /* explicit_gc */
1140                                   clear_all_soft_refs);
1141 }
1142 
1143 void G1CollectedHeap::resize_heap_if_necessary() {
1144   assert_at_safepoint_on_vm_thread();
1145 
<span class="line-modified">1146   // Capacity, free and used after the GC counted as full regions to</span>
<span class="line-modified">1147   // include the waste in the following calculations.</span>
<span class="line-modified">1148   const size_t capacity_after_gc = capacity();</span>
<span class="line-modified">1149   const size_t used_after_gc = capacity_after_gc - unused_committed_regions_in_bytes();</span>
<span class="line-modified">1150 </span>
<span class="line-modified">1151   // This is enforced in arguments.cpp.</span>
<span class="line-modified">1152   assert(MinHeapFreeRatio &lt;= MaxHeapFreeRatio,</span>
<span class="line-modified">1153          &quot;otherwise the code below doesn&#39;t make sense&quot;);</span>
<span class="line-modified">1154 </span>
<span class="line-removed">1155   // We don&#39;t have floating point command-line arguments</span>
<span class="line-removed">1156   const double minimum_free_percentage = (double) MinHeapFreeRatio / 100.0;</span>
<span class="line-removed">1157   const double maximum_used_percentage = 1.0 - minimum_free_percentage;</span>
<span class="line-removed">1158   const double maximum_free_percentage = (double) MaxHeapFreeRatio / 100.0;</span>
<span class="line-removed">1159   const double minimum_used_percentage = 1.0 - maximum_free_percentage;</span>
<span class="line-removed">1160 </span>
<span class="line-removed">1161   // We have to be careful here as these two calculations can overflow</span>
<span class="line-removed">1162   // 32-bit size_t&#39;s.</span>
<span class="line-removed">1163   double used_after_gc_d = (double) used_after_gc;</span>
<span class="line-removed">1164   double minimum_desired_capacity_d = used_after_gc_d / maximum_used_percentage;</span>
<span class="line-removed">1165   double maximum_desired_capacity_d = used_after_gc_d / minimum_used_percentage;</span>
<span class="line-removed">1166 </span>
<span class="line-removed">1167   // Let&#39;s make sure that they are both under the max heap size, which</span>
<span class="line-removed">1168   // by default will make them fit into a size_t.</span>
<span class="line-removed">1169   double desired_capacity_upper_bound = (double) MaxHeapSize;</span>
<span class="line-removed">1170   minimum_desired_capacity_d = MIN2(minimum_desired_capacity_d,</span>
<span class="line-removed">1171                                     desired_capacity_upper_bound);</span>
<span class="line-removed">1172   maximum_desired_capacity_d = MIN2(maximum_desired_capacity_d,</span>
<span class="line-removed">1173                                     desired_capacity_upper_bound);</span>
<span class="line-removed">1174 </span>
<span class="line-removed">1175   // We can now safely turn them into size_t&#39;s.</span>
<span class="line-removed">1176   size_t minimum_desired_capacity = (size_t) minimum_desired_capacity_d;</span>
<span class="line-removed">1177   size_t maximum_desired_capacity = (size_t) maximum_desired_capacity_d;</span>
<span class="line-removed">1178 </span>
<span class="line-removed">1179   // This assert only makes sense here, before we adjust them</span>
<span class="line-removed">1180   // with respect to the min and max heap size.</span>
<span class="line-removed">1181   assert(minimum_desired_capacity &lt;= maximum_desired_capacity,</span>
<span class="line-removed">1182          &quot;minimum_desired_capacity = &quot; SIZE_FORMAT &quot;, &quot;</span>
<span class="line-removed">1183          &quot;maximum_desired_capacity = &quot; SIZE_FORMAT,</span>
<span class="line-removed">1184          minimum_desired_capacity, maximum_desired_capacity);</span>
<span class="line-removed">1185 </span>
<span class="line-removed">1186   // Should not be greater than the heap max size. No need to adjust</span>
<span class="line-removed">1187   // it with respect to the heap min size as it&#39;s a lower bound (i.e.,</span>
<span class="line-removed">1188   // we&#39;ll try to make the capacity larger than it, not smaller).</span>
<span class="line-removed">1189   minimum_desired_capacity = MIN2(minimum_desired_capacity, MaxHeapSize);</span>
<span class="line-removed">1190   // Should not be less than the heap min size. No need to adjust it</span>
<span class="line-removed">1191   // with respect to the heap max size as it&#39;s an upper bound (i.e.,</span>
<span class="line-removed">1192   // we&#39;ll try to make the capacity smaller than it, not greater).</span>
<span class="line-removed">1193   maximum_desired_capacity =  MAX2(maximum_desired_capacity, MinHeapSize);</span>
<span class="line-removed">1194 </span>
<span class="line-removed">1195   if (capacity_after_gc &lt; minimum_desired_capacity) {</span>
<span class="line-removed">1196     // Don&#39;t expand unless it&#39;s significant</span>
<span class="line-removed">1197     size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;</span>
<span class="line-removed">1198 </span>
<span class="line-removed">1199     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (capacity lower than min desired capacity). &quot;</span>
<span class="line-removed">1200                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;</span>
<span class="line-removed">1201                               &quot;min_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,</span>
<span class="line-removed">1202                               capacity_after_gc, used_after_gc, used(), minimum_desired_capacity, MinHeapFreeRatio);</span>
<span class="line-removed">1203 </span>
<span class="line-removed">1204     expand(expand_bytes, _workers);</span>
<span class="line-removed">1205 </span>
<span class="line-removed">1206     // No expansion, now see if we want to shrink</span>
<span class="line-removed">1207   } else if (capacity_after_gc &gt; maximum_desired_capacity) {</span>
<span class="line-removed">1208     // Capacity too large, compute shrinking size</span>
<span class="line-removed">1209     size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;</span>
<span class="line-removed">1210 </span>
<span class="line-removed">1211     log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (capacity higher than max desired capacity). &quot;</span>
<span class="line-removed">1212                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;</span>
<span class="line-removed">1213                               &quot;maximum_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,</span>
<span class="line-removed">1214                               capacity_after_gc, used_after_gc, used(), maximum_desired_capacity, MaxHeapFreeRatio);</span>
<span class="line-removed">1215 </span>
<span class="line-removed">1216     shrink(shrink_bytes);</span>
1217   }
1218 }
1219 
1220 HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,
1221                                                             bool do_gc,
1222                                                             bool clear_all_soft_refs,
1223                                                             bool expect_null_mutator_alloc_region,
1224                                                             bool* gc_succeeded) {
1225   *gc_succeeded = true;
1226   // Let&#39;s attempt the allocation first.
1227   HeapWord* result =
1228     attempt_allocation_at_safepoint(word_size,
1229                                     expect_null_mutator_alloc_region);
1230   if (result != NULL) {
1231     return result;
1232   }
1233 
1234   // In a G1 heap, we&#39;re supposed to keep allocation from failing by
1235   // incremental pauses.  Therefore, at least for now, we&#39;ll favor
1236   // expansion over collection.  (This might change in the future if we can
</pre>
<hr />
<pre>
1851   SuspendibleThreadSet::desynchronize();
1852 }
1853 
1854 void G1CollectedHeap::post_initialize() {
1855   CollectedHeap::post_initialize();
1856   ref_processing_init();
1857 }
1858 
1859 void G1CollectedHeap::ref_processing_init() {
1860   // Reference processing in G1 currently works as follows:
1861   //
1862   // * There are two reference processor instances. One is
1863   //   used to record and process discovered references
1864   //   during concurrent marking; the other is used to
1865   //   record and process references during STW pauses
1866   //   (both full and incremental).
1867   // * Both ref processors need to &#39;span&#39; the entire heap as
1868   //   the regions in the collection set may be dotted around.
1869   //
1870   // * For the concurrent marking ref processor:
<span class="line-modified">1871   //   * Reference discovery is enabled at initial marking.</span>
1872   //   * Reference discovery is disabled and the discovered
1873   //     references processed etc during remarking.
1874   //   * Reference discovery is MT (see below).
1875   //   * Reference discovery requires a barrier (see below).
1876   //   * Reference processing may or may not be MT
1877   //     (depending on the value of ParallelRefProcEnabled
1878   //     and ParallelGCThreads).
1879   //   * A full GC disables reference discovery by the CM
1880   //     ref processor and abandons any entries on it&#39;s
1881   //     discovered lists.
1882   //
1883   // * For the STW processor:
1884   //   * Non MT discovery is enabled at the start of a full GC.
1885   //   * Processing and enqueueing during a full GC is non-MT.
1886   //   * During a full GC, references are processed after marking.
1887   //
1888   //   * Discovery (may or may not be MT) is enabled at the start
1889   //     of an incremental evacuation pause.
1890   //   * References are processed near the end of a STW evacuation pause.
1891   //   * For both types of GC:
</pre>
<hr />
<pre>
2092       ResourceMark rm; /* For thread name. */                           \
2093       LogStream LOG_COLLECT_CONCURRENTLY_s(&amp;LOG_COLLECT_CONCURRENTLY_lt); \
2094       LOG_COLLECT_CONCURRENTLY_s.print(&quot;%s: Try Collect Concurrently (%s): &quot;, \
2095                                        Thread::current()-&gt;name(),       \
2096                                        GCCause::to_string(cause));      \
2097       LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \
2098     }                                                                   \
2099   } while (0)
2100 
2101 #define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \
2102   LOG_COLLECT_CONCURRENTLY(cause, &quot;complete %s&quot;, BOOL_TO_STR(result))
2103 
2104 bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,
2105                                                uint gc_counter,
2106                                                uint old_marking_started_before) {
2107   assert_heap_not_locked();
2108   assert(should_do_concurrent_full_gc(cause),
2109          &quot;Non-concurrent cause %s&quot;, GCCause::to_string(cause));
2110 
2111   for (uint i = 1; true; ++i) {
<span class="line-modified">2112     // Try to schedule an initial-mark evacuation pause that will</span>
2113     // start a concurrent cycle.
2114     LOG_COLLECT_CONCURRENTLY(cause, &quot;attempt %u&quot;, i);
2115     VM_G1TryInitiateConcMark op(gc_counter,
2116                                 cause,
2117                                 policy()-&gt;max_pause_time_ms());
2118     VMThread::execute(&amp;op);
2119 
2120     // Request is trivially finished.
2121     if (cause == GCCause::_g1_periodic_collection) {
2122       LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());
2123       return op.gc_succeeded();
2124     }
2125 
2126     // If VMOp skipped initiating concurrent marking cycle because
2127     // we&#39;re terminating, then we&#39;re done.
2128     if (op.terminating()) {
2129       LOG_COLLECT_CONCURRENTLY(cause, &quot;skipped: terminating&quot;);
2130       return false;
2131     }
2132 
</pre>
<hr />
<pre>
2161       // to start a concurrent cycle.
2162       if (old_marking_started_before != old_marking_started_after) {
2163         LOG_COLLECT_CONCURRENTLY(cause, &quot;ignoring STW full GC&quot;);
2164         old_marking_started_before = old_marking_started_after;
2165       }
2166     } else if (!GCCause::is_user_requested_gc(cause)) {
2167       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2168       // ensure that progress is made.
2169       //
2170       // Request is finished if any of
2171       // (1) the VMOp successfully performed a GC,
2172       // (2) a concurrent cycle was already in progress,
2173       // (3) whitebox is controlling concurrent cycles,
2174       // (4) a new cycle was started (by this thread or some other), or
2175       // (5) a Full GC was performed.
2176       // Cases (4) and (5) are detected together by a change to
2177       // _old_marking_cycles_started.
2178       //
2179       // Note that (1) does not imply (4).  If we&#39;re still in the mixed
2180       // phase of an earlier concurrent collection, the request to make the
<span class="line-modified">2181       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for</span>
2182       // both conditions we&#39;ll spin doing back-to-back collections.
2183       if (op.gc_succeeded() ||
2184           op.cycle_already_in_progress() ||
2185           op.whitebox_attached() ||
2186           (old_marking_started_before != old_marking_started_after)) {
2187         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2188         return true;
2189       }
2190     } else {                    // User-requested GC.
2191       // For a user-requested collection, we want to ensure that a complete
2192       // full collection has been performed before returning, but without
2193       // waiting for more than needed.
2194 
2195       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2196       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2197       // should do otherwise.  Trying again just does back to back GCs.
2198       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2199       // to meet the goal of ensuring a full collection was performed.
2200       assert(!op.gc_succeeded() ||
2201              (old_marking_started_before != old_marking_started_after),
</pre>
<hr />
<pre>
2659                        stats-&gt;regions_filled(), stats-&gt;direct_allocated(),
2660                        stats-&gt;failure_used(), stats-&gt;failure_waste());
2661 }
2662 
2663 void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {
2664   const G1HeapSummary&amp; heap_summary = create_g1_heap_summary();
2665   gc_tracer-&gt;report_gc_heap_summary(when, heap_summary);
2666 
2667   const MetaspaceSummary&amp; metaspace_summary = create_metaspace_summary();
2668   gc_tracer-&gt;report_metaspace_summary(when, metaspace_summary);
2669 }
2670 
2671 void G1CollectedHeap::gc_prologue(bool full) {
2672   assert(InlineCacheBuffer::is_empty(), &quot;should have cleaned up ICBuffer&quot;);
2673 
2674   // This summary needs to be printed before incrementing total collections.
2675   rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
2676 
2677   // Update common counters.
2678   increment_total_collections(full /* full gc */);
<span class="line-modified">2679   if (full || collector_state()-&gt;in_initial_mark_gc()) {</span>
2680     increment_old_marking_cycles_started();
2681   }
2682 
2683   // Fill TLAB&#39;s and such
2684   {
2685     Ticks start = Ticks::now();
2686     ensure_parsability(true);
2687     Tickspan dt = Ticks::now() - start;
2688     phase_times()-&gt;record_prepare_tlab_time_ms(dt.seconds() * MILLIUNITS);
2689   }
2690 
2691   if (!full) {
2692     // Flush dirty card queues to qset, so later phases don&#39;t need to account
2693     // for partially filled per-thread queues and such.  Not needed for full
2694     // collections, which ignore those logs.
2695     Ticks start = Ticks::now();
2696     G1BarrierSet::dirty_card_queue_set().concatenate_logs();
2697     Tickspan dt = Ticks::now() - start;
2698     phase_times()-&gt;record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);
2699   }
</pre>
<hr />
<pre>
2890 
2891   phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);
2892 }
2893 
2894 void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms) {
2895 
2896   _collection_set.finalize_initial_collection_set(target_pause_time_ms, &amp;_survivor);
2897   evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length() +
2898                                             collection_set()-&gt;optional_region_length());
2899 
2900   _cm-&gt;verify_no_collection_set_oops();
2901 
2902   if (_hr_printer.is_active()) {
2903     G1PrintCollectionSetClosure cl(&amp;_hr_printer);
2904     _collection_set.iterate(&amp;cl);
2905     _collection_set.iterate_optional(&amp;cl);
2906   }
2907 }
2908 
2909 G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {
<span class="line-modified">2910   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
2911     return G1HeapVerifier::G1VerifyConcurrentStart;
2912   } else if (collector_state()-&gt;in_young_only_phase()) {
2913     return G1HeapVerifier::G1VerifyYoungNormal;
2914   } else {
2915     return G1HeapVerifier::G1VerifyMixed;
2916   }
2917 }
2918 
2919 void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {
2920   if (VerifyRememberedSets) {
2921     log_info(gc, verify)(&quot;[Verifying RemSets before GC]&quot;);
2922     VerifyRegionRemSetClosure v_cl;
2923     heap_region_iterate(&amp;v_cl);
2924   }
2925   _verifier-&gt;verify_before_gc(type);
2926   _verifier-&gt;check_bitmaps(&quot;GC Start&quot;);
2927   verify_numa_regions(&quot;GC Start&quot;);
2928 }
2929 
2930 void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {
2931   if (VerifyRememberedSets) {
2932     log_info(gc, verify)(&quot;[Verifying RemSets after GC]&quot;);
2933     VerifyRegionRemSetClosure v_cl;
2934     heap_region_iterate(&amp;v_cl);
2935   }
2936   _verifier-&gt;verify_after_gc(type);
2937   _verifier-&gt;check_bitmaps(&quot;GC End&quot;);
2938   verify_numa_regions(&quot;GC End&quot;);
2939 }
2940 
2941 void G1CollectedHeap::expand_heap_after_young_collection(){
<span class="line-modified">2942   size_t expand_bytes = _heap_sizing_policy-&gt;expansion_amount();</span>
2943   if (expand_bytes &gt; 0) {
2944     // No need for an ergo logging here,
2945     // expansion_amount() does this when it returns a value &gt; 0.
2946     double expand_ms;
2947     if (!expand(expand_bytes, _workers, &amp;expand_ms)) {
2948       // We failed to expand the heap. Cannot do anything about it.
2949     }
2950     phase_times()-&gt;record_expand_heap_time(expand_ms);
2951   }
2952 }
2953 
2954 const char* G1CollectedHeap::young_gc_name() const {
<span class="line-modified">2955   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
2956     return &quot;Pause Young (Concurrent Start)&quot;;
2957   } else if (collector_state()-&gt;in_young_only_phase()) {
2958     if (collector_state()-&gt;in_young_gc_before_mixed()) {
2959       return &quot;Pause Young (Prepare Mixed)&quot;;
2960     } else {
2961       return &quot;Pause Young (Normal)&quot;;
2962     }
2963   } else {
2964     return &quot;Pause Young (Mixed)&quot;;
2965   }
2966 }
2967 
2968 bool G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {
2969   assert_at_safepoint_on_vm_thread();
2970   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
2971 
2972   if (GCLocker::check_active_before_gc()) {
2973     return false;
2974   }
2975 
</pre>
<hr />
<pre>
2988 void G1CollectedHeap::do_collection_pause_at_safepoint_helper(double target_pause_time_ms) {
2989   GCIdMark gc_id_mark;
2990 
2991   SvcGCMarker sgcm(SvcGCMarker::MINOR);
2992   ResourceMark rm;
2993 
2994   policy()-&gt;note_gc_start();
2995 
2996   _gc_timer_stw-&gt;register_gc_start();
2997   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());
2998 
2999   wait_for_root_region_scanning();
3000 
3001   print_heap_before_gc();
3002   print_heap_regions();
3003   trace_heap_before_gc(_gc_tracer_stw);
3004 
3005   _verifier-&gt;verify_region_sets_optional();
3006   _verifier-&gt;verify_dirty_young_regions();
3007 
<span class="line-modified">3008   // We should not be doing initial mark unless the conc mark thread is running</span>
3009   if (!_cm_thread-&gt;should_terminate()) {
<span class="line-modified">3010     // This call will decide whether this pause is an initial-mark</span>
<span class="line-modified">3011     // pause. If it is, in_initial_mark_gc() will return true</span>
3012     // for the duration of this pause.
3013     policy()-&gt;decide_on_conc_mark_initiation();
3014   }
3015 
<span class="line-modified">3016   // We do not allow initial-mark to be piggy-backed on a mixed GC.</span>
<span class="line-modified">3017   assert(!collector_state()-&gt;in_initial_mark_gc() ||</span>
3018          collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
3019   // We also do not allow mixed GCs during marking.
3020   assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
3021 
<span class="line-modified">3022   // Record whether this pause is an initial mark. When the current</span>
3023   // thread has completed its logging output and it&#39;s safe to signal
3024   // the CM thread, the flag&#39;s value in the policy has been reset.
<span class="line-modified">3025   bool should_start_conc_mark = collector_state()-&gt;in_initial_mark_gc();</span>
3026   if (should_start_conc_mark) {
3027     _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());
3028   }
3029 
3030   // Inner scope for scope based logging, timers, and stats collection
3031   {
3032     G1EvacuationInfo evacuation_info;
3033 
3034     _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
3035 
3036     GCTraceCPUTime tcpu;
3037 
3038     GCTraceTime(Info, gc) tm(young_gc_name(), NULL, gc_cause(), true);
3039 
3040     uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
3041                                                             workers()-&gt;active_workers(),
3042                                                             Threads::number_of_non_daemon_threads());
3043     active_workers = workers()-&gt;update_active_workers(active_workers);
3044     log_info(gc,task)(&quot;Using %u workers of %u for evacuation&quot;, active_workers, workers()-&gt;total_workers());
3045 
</pre>
<hr />
<pre>
3089                                                   collection_set()-&gt;optional_region_length());
3090         pre_evacuate_collection_set(evacuation_info, &amp;per_thread_states);
3091 
3092         // Actually do the work...
3093         evacuate_initial_collection_set(&amp;per_thread_states);
3094 
3095         if (_collection_set.optional_region_length() != 0) {
3096           evacuate_optional_collection_set(&amp;per_thread_states);
3097         }
3098         post_evacuate_collection_set(evacuation_info, &amp;rdcqs, &amp;per_thread_states);
3099 
3100         start_new_collection_set();
3101 
3102         _survivor_evac_stats.adjust_desired_plab_sz();
3103         _old_evac_stats.adjust_desired_plab_sz();
3104 
3105         if (should_start_conc_mark) {
3106           // We have to do this before we notify the CM threads that
3107           // they can start working to make sure that all the
3108           // appropriate initialization is done on the CM object.
<span class="line-modified">3109           concurrent_mark()-&gt;post_initial_mark();</span>
3110           // Note that we don&#39;t actually trigger the CM thread at
3111           // this point. We do that later when we&#39;re sure that
3112           // the current thread has completed its logging output.
3113         }
3114 
3115         allocate_dummy_regions();
3116 
3117         _allocator-&gt;init_mutator_alloc_regions();
3118 
3119         expand_heap_after_young_collection();
3120 
3121         double sample_end_time_sec = os::elapsedTime();
3122         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
3123         policy()-&gt;record_collection_pause_end(pause_time_ms);
3124       }
3125 
3126       verify_after_young_collection(verify_type);
3127 
3128       gc_epilogue(false);
3129     }
</pre>
<hr />
<pre>
3570                                               &amp;drain_queue,
3571                                               &amp;par_task_executor,
3572                                               pt);
3573   }
3574 
3575   _gc_tracer_stw-&gt;report_gc_reference_stats(stats);
3576 
3577   // We have completed copying any necessary live referent objects.
3578   assert(pss-&gt;queue_is_empty(), &quot;both queue and overflow should be empty&quot;);
3579 
3580   make_pending_list_reachable();
3581 
3582   assert(!rp-&gt;discovery_enabled(), &quot;Postcondition&quot;);
3583   rp-&gt;verify_no_references_recorded();
3584 
3585   double ref_proc_time = os::elapsedTime() - ref_proc_start;
3586   phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
3587 }
3588 
3589 void G1CollectedHeap::make_pending_list_reachable() {
<span class="line-modified">3590   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
3591     oop pll_head = Universe::reference_pending_list();
3592     if (pll_head != NULL) {
3593       // Any valid worker id is fine here as we are in the VM thread and single-threaded.
3594       _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
3595     }
3596   }
3597 }
3598 
3599 void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
3600   Ticks start = Ticks::now();
3601   per_thread_states-&gt;flush();
3602   phase_times()-&gt;record_or_add_time_secs(G1GCPhaseTimes::MergePSS, 0 /* worker_id */, (Ticks::now() - start).seconds());
3603 }
3604 
3605 class G1PrepareEvacuationTask : public AbstractGangTask {
3606   class G1PrepareRegionsClosure : public HeapRegionClosure {
3607     G1CollectedHeap* _g1h;
3608     G1PrepareEvacuationTask* _parent_task;
3609     size_t _worker_humongous_total;
3610     size_t _worker_humongous_candidates;
</pre>
<hr />
<pre>
3759     rem_set()-&gt;prepare_for_scan_heap_roots();
3760     phase_times()-&gt;record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);
3761   }
3762 
3763   {
3764     G1PrepareEvacuationTask g1_prep_task(this);
3765     Tickspan task_time = run_task(&amp;g1_prep_task);
3766 
3767     phase_times()-&gt;record_register_regions(task_time.seconds() * 1000.0,
3768                                            g1_prep_task.humongous_total(),
3769                                            g1_prep_task.humongous_candidates());
3770   }
3771 
3772   assert(_verifier-&gt;check_region_attr_table(), &quot;Inconsistency in the region attributes table.&quot;);
3773   _preserved_marks_set.assert_empty();
3774 
3775 #if COMPILER2_OR_JVMCI
3776   DerivedPointerTable::clear();
3777 #endif
3778 
<span class="line-modified">3779   // InitialMark needs claim bits to keep track of the marked-through CLDs.</span>
<span class="line-modified">3780   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-modified">3781     concurrent_mark()-&gt;pre_initial_mark();</span>
3782 
3783     double start_clear_claimed_marks = os::elapsedTime();
3784 
3785     ClassLoaderDataGraph::clear_claimed_marks();
3786 
3787     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3788     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3789   }
3790 
3791   // Should G1EvacuationFailureALot be in effect for this GC?
3792   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3793 }
3794 
3795 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3796 protected:
3797   G1CollectedHeap* _g1h;
3798   G1ParScanThreadStateSet* _per_thread_states;
3799   G1ScannerTasksQueueSet* _task_queues;
3800   TaskTerminator _terminator;
3801   uint _num_workers;
</pre>
<hr />
<pre>
4829     }
4830     _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4831     register_region_with_region_attr(new_alloc_region);
4832     _hr_printer.alloc(new_alloc_region);
4833     return new_alloc_region;
4834   }
4835   return NULL;
4836 }
4837 
4838 void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
4839                                              size_t allocated_bytes,
4840                                              G1HeapRegionAttr dest) {
4841   _bytes_used_during_gc += allocated_bytes;
4842   if (dest.is_old()) {
4843     old_set_add(alloc_region);
4844   } else {
4845     assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());
4846     _survivor.add_used_bytes(allocated_bytes);
4847   }
4848 
<span class="line-modified">4849   bool const during_im = collector_state()-&gt;in_initial_mark_gc();</span>
4850   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
4851     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
4852   }
4853   _hr_printer.retire(alloc_region);
4854 }
4855 
4856 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4857   bool expanded = false;
4858   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4859 
4860   if (index != G1_NO_HRM_INDEX) {
4861     if (expanded) {
4862       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
4863                                 HeapRegion::GrainWords * HeapWordSize);
4864     }
4865     return _hrm-&gt;allocate_free_regions_starting_at(index, 1);
4866   }
4867   return NULL;
4868 }
4869 
</pre>
</td>
<td>
<hr />
<pre>
1126 
1127   collector.prepare_collection();
1128   collector.collect();
1129   collector.complete_collection();
1130 
1131   // Full collection was successfully completed.
1132   return true;
1133 }
1134 
1135 void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {
1136   // Currently, there is no facility in the do_full_collection(bool) API to notify
1137   // the caller that the collection did not succeed (e.g., because it was locked
1138   // out by the GC locker). So, right now, we&#39;ll ignore the return value.
1139   bool dummy = do_full_collection(true,                /* explicit_gc */
1140                                   clear_all_soft_refs);
1141 }
1142 
1143 void G1CollectedHeap::resize_heap_if_necessary() {
1144   assert_at_safepoint_on_vm_thread();
1145 
<span class="line-modified">1146   bool should_expand;</span>
<span class="line-modified">1147   size_t resize_amount = _heap_sizing_policy-&gt;full_collection_resize_amount(should_expand);</span>
<span class="line-modified">1148 </span>
<span class="line-modified">1149   if (resize_amount == 0) {</span>
<span class="line-modified">1150     return;</span>
<span class="line-modified">1151   } else if (should_expand) {</span>
<span class="line-modified">1152     expand(resize_amount, _workers);</span>
<span class="line-modified">1153   } else {</span>
<span class="line-modified">1154     shrink(resize_amount);</span>






























































1155   }
1156 }
1157 
1158 HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,
1159                                                             bool do_gc,
1160                                                             bool clear_all_soft_refs,
1161                                                             bool expect_null_mutator_alloc_region,
1162                                                             bool* gc_succeeded) {
1163   *gc_succeeded = true;
1164   // Let&#39;s attempt the allocation first.
1165   HeapWord* result =
1166     attempt_allocation_at_safepoint(word_size,
1167                                     expect_null_mutator_alloc_region);
1168   if (result != NULL) {
1169     return result;
1170   }
1171 
1172   // In a G1 heap, we&#39;re supposed to keep allocation from failing by
1173   // incremental pauses.  Therefore, at least for now, we&#39;ll favor
1174   // expansion over collection.  (This might change in the future if we can
</pre>
<hr />
<pre>
1789   SuspendibleThreadSet::desynchronize();
1790 }
1791 
1792 void G1CollectedHeap::post_initialize() {
1793   CollectedHeap::post_initialize();
1794   ref_processing_init();
1795 }
1796 
1797 void G1CollectedHeap::ref_processing_init() {
1798   // Reference processing in G1 currently works as follows:
1799   //
1800   // * There are two reference processor instances. One is
1801   //   used to record and process discovered references
1802   //   during concurrent marking; the other is used to
1803   //   record and process references during STW pauses
1804   //   (both full and incremental).
1805   // * Both ref processors need to &#39;span&#39; the entire heap as
1806   //   the regions in the collection set may be dotted around.
1807   //
1808   // * For the concurrent marking ref processor:
<span class="line-modified">1809   //   * Reference discovery is enabled at concurrent start.</span>
1810   //   * Reference discovery is disabled and the discovered
1811   //     references processed etc during remarking.
1812   //   * Reference discovery is MT (see below).
1813   //   * Reference discovery requires a barrier (see below).
1814   //   * Reference processing may or may not be MT
1815   //     (depending on the value of ParallelRefProcEnabled
1816   //     and ParallelGCThreads).
1817   //   * A full GC disables reference discovery by the CM
1818   //     ref processor and abandons any entries on it&#39;s
1819   //     discovered lists.
1820   //
1821   // * For the STW processor:
1822   //   * Non MT discovery is enabled at the start of a full GC.
1823   //   * Processing and enqueueing during a full GC is non-MT.
1824   //   * During a full GC, references are processed after marking.
1825   //
1826   //   * Discovery (may or may not be MT) is enabled at the start
1827   //     of an incremental evacuation pause.
1828   //   * References are processed near the end of a STW evacuation pause.
1829   //   * For both types of GC:
</pre>
<hr />
<pre>
2030       ResourceMark rm; /* For thread name. */                           \
2031       LogStream LOG_COLLECT_CONCURRENTLY_s(&amp;LOG_COLLECT_CONCURRENTLY_lt); \
2032       LOG_COLLECT_CONCURRENTLY_s.print(&quot;%s: Try Collect Concurrently (%s): &quot;, \
2033                                        Thread::current()-&gt;name(),       \
2034                                        GCCause::to_string(cause));      \
2035       LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \
2036     }                                                                   \
2037   } while (0)
2038 
2039 #define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \
2040   LOG_COLLECT_CONCURRENTLY(cause, &quot;complete %s&quot;, BOOL_TO_STR(result))
2041 
2042 bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,
2043                                                uint gc_counter,
2044                                                uint old_marking_started_before) {
2045   assert_heap_not_locked();
2046   assert(should_do_concurrent_full_gc(cause),
2047          &quot;Non-concurrent cause %s&quot;, GCCause::to_string(cause));
2048 
2049   for (uint i = 1; true; ++i) {
<span class="line-modified">2050     // Try to schedule concurrent start evacuation pause that will</span>
2051     // start a concurrent cycle.
2052     LOG_COLLECT_CONCURRENTLY(cause, &quot;attempt %u&quot;, i);
2053     VM_G1TryInitiateConcMark op(gc_counter,
2054                                 cause,
2055                                 policy()-&gt;max_pause_time_ms());
2056     VMThread::execute(&amp;op);
2057 
2058     // Request is trivially finished.
2059     if (cause == GCCause::_g1_periodic_collection) {
2060       LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());
2061       return op.gc_succeeded();
2062     }
2063 
2064     // If VMOp skipped initiating concurrent marking cycle because
2065     // we&#39;re terminating, then we&#39;re done.
2066     if (op.terminating()) {
2067       LOG_COLLECT_CONCURRENTLY(cause, &quot;skipped: terminating&quot;);
2068       return false;
2069     }
2070 
</pre>
<hr />
<pre>
2099       // to start a concurrent cycle.
2100       if (old_marking_started_before != old_marking_started_after) {
2101         LOG_COLLECT_CONCURRENTLY(cause, &quot;ignoring STW full GC&quot;);
2102         old_marking_started_before = old_marking_started_after;
2103       }
2104     } else if (!GCCause::is_user_requested_gc(cause)) {
2105       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2106       // ensure that progress is made.
2107       //
2108       // Request is finished if any of
2109       // (1) the VMOp successfully performed a GC,
2110       // (2) a concurrent cycle was already in progress,
2111       // (3) whitebox is controlling concurrent cycles,
2112       // (4) a new cycle was started (by this thread or some other), or
2113       // (5) a Full GC was performed.
2114       // Cases (4) and (5) are detected together by a change to
2115       // _old_marking_cycles_started.
2116       //
2117       // Note that (1) does not imply (4).  If we&#39;re still in the mixed
2118       // phase of an earlier concurrent collection, the request to make the
<span class="line-modified">2119       // collection a concurrent start won&#39;t be honored.  If we don&#39;t check for</span>
2120       // both conditions we&#39;ll spin doing back-to-back collections.
2121       if (op.gc_succeeded() ||
2122           op.cycle_already_in_progress() ||
2123           op.whitebox_attached() ||
2124           (old_marking_started_before != old_marking_started_after)) {
2125         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2126         return true;
2127       }
2128     } else {                    // User-requested GC.
2129       // For a user-requested collection, we want to ensure that a complete
2130       // full collection has been performed before returning, but without
2131       // waiting for more than needed.
2132 
2133       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2134       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2135       // should do otherwise.  Trying again just does back to back GCs.
2136       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2137       // to meet the goal of ensuring a full collection was performed.
2138       assert(!op.gc_succeeded() ||
2139              (old_marking_started_before != old_marking_started_after),
</pre>
<hr />
<pre>
2597                        stats-&gt;regions_filled(), stats-&gt;direct_allocated(),
2598                        stats-&gt;failure_used(), stats-&gt;failure_waste());
2599 }
2600 
2601 void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {
2602   const G1HeapSummary&amp; heap_summary = create_g1_heap_summary();
2603   gc_tracer-&gt;report_gc_heap_summary(when, heap_summary);
2604 
2605   const MetaspaceSummary&amp; metaspace_summary = create_metaspace_summary();
2606   gc_tracer-&gt;report_metaspace_summary(when, metaspace_summary);
2607 }
2608 
2609 void G1CollectedHeap::gc_prologue(bool full) {
2610   assert(InlineCacheBuffer::is_empty(), &quot;should have cleaned up ICBuffer&quot;);
2611 
2612   // This summary needs to be printed before incrementing total collections.
2613   rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
2614 
2615   // Update common counters.
2616   increment_total_collections(full /* full gc */);
<span class="line-modified">2617   if (full || collector_state()-&gt;in_concurrent_start_gc()) {</span>
2618     increment_old_marking_cycles_started();
2619   }
2620 
2621   // Fill TLAB&#39;s and such
2622   {
2623     Ticks start = Ticks::now();
2624     ensure_parsability(true);
2625     Tickspan dt = Ticks::now() - start;
2626     phase_times()-&gt;record_prepare_tlab_time_ms(dt.seconds() * MILLIUNITS);
2627   }
2628 
2629   if (!full) {
2630     // Flush dirty card queues to qset, so later phases don&#39;t need to account
2631     // for partially filled per-thread queues and such.  Not needed for full
2632     // collections, which ignore those logs.
2633     Ticks start = Ticks::now();
2634     G1BarrierSet::dirty_card_queue_set().concatenate_logs();
2635     Tickspan dt = Ticks::now() - start;
2636     phase_times()-&gt;record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);
2637   }
</pre>
<hr />
<pre>
2828 
2829   phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);
2830 }
2831 
2832 void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms) {
2833 
2834   _collection_set.finalize_initial_collection_set(target_pause_time_ms, &amp;_survivor);
2835   evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length() +
2836                                             collection_set()-&gt;optional_region_length());
2837 
2838   _cm-&gt;verify_no_collection_set_oops();
2839 
2840   if (_hr_printer.is_active()) {
2841     G1PrintCollectionSetClosure cl(&amp;_hr_printer);
2842     _collection_set.iterate(&amp;cl);
2843     _collection_set.iterate_optional(&amp;cl);
2844   }
2845 }
2846 
2847 G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {
<span class="line-modified">2848   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
2849     return G1HeapVerifier::G1VerifyConcurrentStart;
2850   } else if (collector_state()-&gt;in_young_only_phase()) {
2851     return G1HeapVerifier::G1VerifyYoungNormal;
2852   } else {
2853     return G1HeapVerifier::G1VerifyMixed;
2854   }
2855 }
2856 
2857 void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {
2858   if (VerifyRememberedSets) {
2859     log_info(gc, verify)(&quot;[Verifying RemSets before GC]&quot;);
2860     VerifyRegionRemSetClosure v_cl;
2861     heap_region_iterate(&amp;v_cl);
2862   }
2863   _verifier-&gt;verify_before_gc(type);
2864   _verifier-&gt;check_bitmaps(&quot;GC Start&quot;);
2865   verify_numa_regions(&quot;GC Start&quot;);
2866 }
2867 
2868 void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {
2869   if (VerifyRememberedSets) {
2870     log_info(gc, verify)(&quot;[Verifying RemSets after GC]&quot;);
2871     VerifyRegionRemSetClosure v_cl;
2872     heap_region_iterate(&amp;v_cl);
2873   }
2874   _verifier-&gt;verify_after_gc(type);
2875   _verifier-&gt;check_bitmaps(&quot;GC End&quot;);
2876   verify_numa_regions(&quot;GC End&quot;);
2877 }
2878 
2879 void G1CollectedHeap::expand_heap_after_young_collection(){
<span class="line-modified">2880   size_t expand_bytes = _heap_sizing_policy-&gt;young_collection_expansion_amount();</span>
2881   if (expand_bytes &gt; 0) {
2882     // No need for an ergo logging here,
2883     // expansion_amount() does this when it returns a value &gt; 0.
2884     double expand_ms;
2885     if (!expand(expand_bytes, _workers, &amp;expand_ms)) {
2886       // We failed to expand the heap. Cannot do anything about it.
2887     }
2888     phase_times()-&gt;record_expand_heap_time(expand_ms);
2889   }
2890 }
2891 
2892 const char* G1CollectedHeap::young_gc_name() const {
<span class="line-modified">2893   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
2894     return &quot;Pause Young (Concurrent Start)&quot;;
2895   } else if (collector_state()-&gt;in_young_only_phase()) {
2896     if (collector_state()-&gt;in_young_gc_before_mixed()) {
2897       return &quot;Pause Young (Prepare Mixed)&quot;;
2898     } else {
2899       return &quot;Pause Young (Normal)&quot;;
2900     }
2901   } else {
2902     return &quot;Pause Young (Mixed)&quot;;
2903   }
2904 }
2905 
2906 bool G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {
2907   assert_at_safepoint_on_vm_thread();
2908   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
2909 
2910   if (GCLocker::check_active_before_gc()) {
2911     return false;
2912   }
2913 
</pre>
<hr />
<pre>
2926 void G1CollectedHeap::do_collection_pause_at_safepoint_helper(double target_pause_time_ms) {
2927   GCIdMark gc_id_mark;
2928 
2929   SvcGCMarker sgcm(SvcGCMarker::MINOR);
2930   ResourceMark rm;
2931 
2932   policy()-&gt;note_gc_start();
2933 
2934   _gc_timer_stw-&gt;register_gc_start();
2935   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());
2936 
2937   wait_for_root_region_scanning();
2938 
2939   print_heap_before_gc();
2940   print_heap_regions();
2941   trace_heap_before_gc(_gc_tracer_stw);
2942 
2943   _verifier-&gt;verify_region_sets_optional();
2944   _verifier-&gt;verify_dirty_young_regions();
2945 
<span class="line-modified">2946   // We should not be doing concurrent start unless the concurrent mark thread is running</span>
2947   if (!_cm_thread-&gt;should_terminate()) {
<span class="line-modified">2948     // This call will decide whether this pause is a concurrent start</span>
<span class="line-modified">2949     // pause. If it is, in_concurrent_start_gc() will return true</span>
2950     // for the duration of this pause.
2951     policy()-&gt;decide_on_conc_mark_initiation();
2952   }
2953 
<span class="line-modified">2954   // We do not allow concurrent start to be piggy-backed on a mixed GC.</span>
<span class="line-modified">2955   assert(!collector_state()-&gt;in_concurrent_start_gc() ||</span>
2956          collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
2957   // We also do not allow mixed GCs during marking.
2958   assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
2959 
<span class="line-modified">2960   // Record whether this pause is a concurrent start. When the current</span>
2961   // thread has completed its logging output and it&#39;s safe to signal
2962   // the CM thread, the flag&#39;s value in the policy has been reset.
<span class="line-modified">2963   bool should_start_conc_mark = collector_state()-&gt;in_concurrent_start_gc();</span>
2964   if (should_start_conc_mark) {
2965     _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());
2966   }
2967 
2968   // Inner scope for scope based logging, timers, and stats collection
2969   {
2970     G1EvacuationInfo evacuation_info;
2971 
2972     _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
2973 
2974     GCTraceCPUTime tcpu;
2975 
2976     GCTraceTime(Info, gc) tm(young_gc_name(), NULL, gc_cause(), true);
2977 
2978     uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
2979                                                             workers()-&gt;active_workers(),
2980                                                             Threads::number_of_non_daemon_threads());
2981     active_workers = workers()-&gt;update_active_workers(active_workers);
2982     log_info(gc,task)(&quot;Using %u workers of %u for evacuation&quot;, active_workers, workers()-&gt;total_workers());
2983 
</pre>
<hr />
<pre>
3027                                                   collection_set()-&gt;optional_region_length());
3028         pre_evacuate_collection_set(evacuation_info, &amp;per_thread_states);
3029 
3030         // Actually do the work...
3031         evacuate_initial_collection_set(&amp;per_thread_states);
3032 
3033         if (_collection_set.optional_region_length() != 0) {
3034           evacuate_optional_collection_set(&amp;per_thread_states);
3035         }
3036         post_evacuate_collection_set(evacuation_info, &amp;rdcqs, &amp;per_thread_states);
3037 
3038         start_new_collection_set();
3039 
3040         _survivor_evac_stats.adjust_desired_plab_sz();
3041         _old_evac_stats.adjust_desired_plab_sz();
3042 
3043         if (should_start_conc_mark) {
3044           // We have to do this before we notify the CM threads that
3045           // they can start working to make sure that all the
3046           // appropriate initialization is done on the CM object.
<span class="line-modified">3047           concurrent_mark()-&gt;post_concurrent_start();</span>
3048           // Note that we don&#39;t actually trigger the CM thread at
3049           // this point. We do that later when we&#39;re sure that
3050           // the current thread has completed its logging output.
3051         }
3052 
3053         allocate_dummy_regions();
3054 
3055         _allocator-&gt;init_mutator_alloc_regions();
3056 
3057         expand_heap_after_young_collection();
3058 
3059         double sample_end_time_sec = os::elapsedTime();
3060         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
3061         policy()-&gt;record_collection_pause_end(pause_time_ms);
3062       }
3063 
3064       verify_after_young_collection(verify_type);
3065 
3066       gc_epilogue(false);
3067     }
</pre>
<hr />
<pre>
3508                                               &amp;drain_queue,
3509                                               &amp;par_task_executor,
3510                                               pt);
3511   }
3512 
3513   _gc_tracer_stw-&gt;report_gc_reference_stats(stats);
3514 
3515   // We have completed copying any necessary live referent objects.
3516   assert(pss-&gt;queue_is_empty(), &quot;both queue and overflow should be empty&quot;);
3517 
3518   make_pending_list_reachable();
3519 
3520   assert(!rp-&gt;discovery_enabled(), &quot;Postcondition&quot;);
3521   rp-&gt;verify_no_references_recorded();
3522 
3523   double ref_proc_time = os::elapsedTime() - ref_proc_start;
3524   phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
3525 }
3526 
3527 void G1CollectedHeap::make_pending_list_reachable() {
<span class="line-modified">3528   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
3529     oop pll_head = Universe::reference_pending_list();
3530     if (pll_head != NULL) {
3531       // Any valid worker id is fine here as we are in the VM thread and single-threaded.
3532       _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
3533     }
3534   }
3535 }
3536 
3537 void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
3538   Ticks start = Ticks::now();
3539   per_thread_states-&gt;flush();
3540   phase_times()-&gt;record_or_add_time_secs(G1GCPhaseTimes::MergePSS, 0 /* worker_id */, (Ticks::now() - start).seconds());
3541 }
3542 
3543 class G1PrepareEvacuationTask : public AbstractGangTask {
3544   class G1PrepareRegionsClosure : public HeapRegionClosure {
3545     G1CollectedHeap* _g1h;
3546     G1PrepareEvacuationTask* _parent_task;
3547     size_t _worker_humongous_total;
3548     size_t _worker_humongous_candidates;
</pre>
<hr />
<pre>
3697     rem_set()-&gt;prepare_for_scan_heap_roots();
3698     phase_times()-&gt;record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);
3699   }
3700 
3701   {
3702     G1PrepareEvacuationTask g1_prep_task(this);
3703     Tickspan task_time = run_task(&amp;g1_prep_task);
3704 
3705     phase_times()-&gt;record_register_regions(task_time.seconds() * 1000.0,
3706                                            g1_prep_task.humongous_total(),
3707                                            g1_prep_task.humongous_candidates());
3708   }
3709 
3710   assert(_verifier-&gt;check_region_attr_table(), &quot;Inconsistency in the region attributes table.&quot;);
3711   _preserved_marks_set.assert_empty();
3712 
3713 #if COMPILER2_OR_JVMCI
3714   DerivedPointerTable::clear();
3715 #endif
3716 
<span class="line-modified">3717   // Concurrent start needs claim bits to keep track of the marked-through CLDs.</span>
<span class="line-modified">3718   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
<span class="line-modified">3719     concurrent_mark()-&gt;pre_concurrent_start();</span>
3720 
3721     double start_clear_claimed_marks = os::elapsedTime();
3722 
3723     ClassLoaderDataGraph::clear_claimed_marks();
3724 
3725     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3726     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3727   }
3728 
3729   // Should G1EvacuationFailureALot be in effect for this GC?
3730   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3731 }
3732 
3733 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3734 protected:
3735   G1CollectedHeap* _g1h;
3736   G1ParScanThreadStateSet* _per_thread_states;
3737   G1ScannerTasksQueueSet* _task_queues;
3738   TaskTerminator _terminator;
3739   uint _num_workers;
</pre>
<hr />
<pre>
4767     }
4768     _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4769     register_region_with_region_attr(new_alloc_region);
4770     _hr_printer.alloc(new_alloc_region);
4771     return new_alloc_region;
4772   }
4773   return NULL;
4774 }
4775 
4776 void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
4777                                              size_t allocated_bytes,
4778                                              G1HeapRegionAttr dest) {
4779   _bytes_used_during_gc += allocated_bytes;
4780   if (dest.is_old()) {
4781     old_set_add(alloc_region);
4782   } else {
4783     assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());
4784     _survivor.add_used_bytes(allocated_bytes);
4785   }
4786 
<span class="line-modified">4787   bool const during_im = collector_state()-&gt;in_concurrent_start_gc();</span>
4788   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
4789     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
4790   }
4791   _hr_printer.retire(alloc_region);
4792 }
4793 
4794 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4795   bool expanded = false;
4796   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4797 
4798   if (index != G1_NO_HRM_INDEX) {
4799     if (expanded) {
4800       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
4801                                 HeapRegion::GrainWords * HeapWordSize);
4802     }
4803     return _hrm-&gt;allocate_free_regions_starting_at(index, 1);
4804   }
4805   return NULL;
4806 }
4807 
</pre>
</td>
</tr>
</table>
<center><a href="c2/g1BarrierSetC2.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>