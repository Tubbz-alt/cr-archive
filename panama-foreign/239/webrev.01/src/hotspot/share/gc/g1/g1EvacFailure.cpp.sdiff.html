<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1EvacFailure.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1ConcurrentMarkThread.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1HeapSizingPolicy.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1EvacFailure.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 62       return;
 63     }
 64 
 65     if (HeapRegion::is_in_same_region(p, CompressedOops::decode(o))) {
 66       return;
 67     }
 68     size_t card_index = _ct-&gt;index_for(p);
 69     if (card_index != _last_enqueued_card) {
 70       _rdcq-&gt;enqueue(_ct-&gt;byte_for_index(card_index));
 71       _last_enqueued_card = card_index;
 72     }
 73   }
 74 };
 75 
 76 class RemoveSelfForwardPtrObjClosure: public ObjectClosure {
 77   G1CollectedHeap* _g1h;
 78   G1ConcurrentMark* _cm;
 79   HeapRegion* _hr;
 80   size_t _marked_bytes;
 81   UpdateLogBuffersDeferred* _log_buffer_cl;
<span class="line-modified"> 82   bool _during_initial_mark;</span>
 83   uint _worker_id;
 84   HeapWord* _last_forwarded_object_end;
 85 
 86 public:
 87   RemoveSelfForwardPtrObjClosure(HeapRegion* hr,
 88                                  UpdateLogBuffersDeferred* log_buffer_cl,
<span class="line-modified"> 89                                  bool during_initial_mark,</span>
 90                                  uint worker_id) :
 91     _g1h(G1CollectedHeap::heap()),
 92     _cm(_g1h-&gt;concurrent_mark()),
 93     _hr(hr),
 94     _marked_bytes(0),
 95     _log_buffer_cl(log_buffer_cl),
<span class="line-modified"> 96     _during_initial_mark(during_initial_mark),</span>
 97     _worker_id(worker_id),
 98     _last_forwarded_object_end(hr-&gt;bottom()) { }
 99 
100   size_t marked_bytes() { return _marked_bytes; }
101 
102   // Iterate over the live objects in the region to find self-forwarded objects
103   // that need to be kept live. We need to update the remembered sets of these
104   // objects. Further update the BOT and marks.
105   // We can coalesce and overwrite the remaining heap contents with dummy objects
106   // as they have either been dead or evacuated (which are unreferenced now, i.e.
107   // dead too) already.
108   void do_object(oop obj) {
109     HeapWord* obj_addr = cast_from_oop&lt;HeapWord*&gt;(obj);
110     assert(_hr-&gt;is_in(obj_addr), &quot;sanity&quot;);
111 
112     if (obj-&gt;is_forwarded() &amp;&amp; obj-&gt;forwardee() == obj) {
113       // The object failed to move.
114 
115       zap_dead_objects(_last_forwarded_object_end, obj_addr);
116       // We consider all objects that we find self-forwarded to be
117       // live. What we&#39;ll do is that we&#39;ll update the prev marking
118       // info so that they are all under PTAMS and explicitly marked.
119       if (!_cm-&gt;is_marked_in_prev_bitmap(obj)) {
120         _cm-&gt;mark_in_prev_bitmap(obj);
121       }
<span class="line-modified">122       if (_during_initial_mark) {</span>
123         // For the next marking info we&#39;ll only mark the
124         // self-forwarded objects explicitly if we are during
<span class="line-modified">125         // initial-mark (since, normally, we only mark objects pointed</span>
126         // to by roots if we succeed in copying them). By marking all
127         // self-forwarded objects we ensure that we mark any that are
128         // still pointed to be roots. During concurrent marking, and
<span class="line-modified">129         // after initial-mark, we don&#39;t need to mark any objects</span>
130         // explicitly and all objects in the CSet are considered
131         // (implicitly) live. So, we won&#39;t mark them explicitly and
132         // we&#39;ll leave them over NTAMS.
133         _cm-&gt;mark_in_next_bitmap(_worker_id, _hr, obj);
134       }
135       size_t obj_size = obj-&gt;size();
136 
137       _marked_bytes += (obj_size * HeapWordSize);
138       PreservedMarks::init_forwarded_mark(obj);
139 
140       // While we were processing RSet buffers during the collection,
141       // we actually didn&#39;t scan any cards on the collection set,
142       // since we didn&#39;t want to update remembered sets with entries
143       // that point into the collection set, given that live objects
144       // from the collection set are about to move and such entries
145       // will be stale very soon.
146       // This change also dealt with a reliability issue which
147       // involved scanning a card in the collection set and coming
148       // across an array that was being chunked and looking malformed.
149       // The problem is that, if evacuation fails, we might have
</pre>
<hr />
<pre>
194     zap_dead_objects(_last_forwarded_object_end, _hr-&gt;top());
195   }
196 };
197 
198 class RemoveSelfForwardPtrHRClosure: public HeapRegionClosure {
199   G1CollectedHeap* _g1h;
200   uint _worker_id;
201 
202   G1RedirtyCardsQueue _rdcq;
203   UpdateLogBuffersDeferred _log_buffer_cl;
204 
205 public:
206   RemoveSelfForwardPtrHRClosure(G1RedirtyCardsQueueSet* rdcqs, uint worker_id) :
207     _g1h(G1CollectedHeap::heap()),
208     _worker_id(worker_id),
209     _rdcq(rdcqs),
210     _log_buffer_cl(&amp;_rdcq) {
211   }
212 
213   size_t remove_self_forward_ptr_by_walking_hr(HeapRegion* hr,
<span class="line-modified">214                                                bool during_initial_mark) {</span>
215     RemoveSelfForwardPtrObjClosure rspc(hr,
216                                         &amp;_log_buffer_cl,
<span class="line-modified">217                                         during_initial_mark,</span>
218                                         _worker_id);
219     hr-&gt;object_iterate(&amp;rspc);
220     // Need to zap the remainder area of the processed region.
221     rspc.zap_remainder();
222 
223     return rspc.marked_bytes();
224   }
225 
226   bool do_heap_region(HeapRegion *hr) {
227     assert(!hr-&gt;is_pinned(), &quot;Unexpected pinned region at index %u&quot;, hr-&gt;hrm_index());
228     assert(hr-&gt;in_collection_set(), &quot;bad CS&quot;);
229 
230     if (hr-&gt;evacuation_failed()) {
231       hr-&gt;clear_index_in_opt_cset();
232 
<span class="line-modified">233       bool during_initial_mark = _g1h-&gt;collector_state()-&gt;in_initial_mark_gc();</span>
<span class="line-modified">234       bool during_conc_mark = _g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress();</span>
235 
<span class="line-modified">236       hr-&gt;note_self_forwarding_removal_start(during_initial_mark,</span>
<span class="line-modified">237                                                during_conc_mark);</span>
238       _g1h-&gt;verifier()-&gt;check_bitmaps(&quot;Self-Forwarding Ptr Removal&quot;, hr);
239 
240       hr-&gt;reset_bot();
241 
<span class="line-modified">242       size_t live_bytes = remove_self_forward_ptr_by_walking_hr(hr, during_initial_mark);</span>
243 
244       hr-&gt;rem_set()-&gt;clean_strong_code_roots(hr);
245       hr-&gt;rem_set()-&gt;clear_locked(true);
246 
247       hr-&gt;note_self_forwarding_removal_end(live_bytes);
248     }
249     return false;
250   }
251 };
252 
253 G1ParRemoveSelfForwardPtrsTask::G1ParRemoveSelfForwardPtrsTask(G1RedirtyCardsQueueSet* rdcqs) :
254   AbstractGangTask(&quot;G1 Remove Self-forwarding Pointers&quot;),
255   _g1h(G1CollectedHeap::heap()),
256   _rdcqs(rdcqs),
257   _hrclaimer(_g1h-&gt;workers()-&gt;active_workers()) { }
258 
259 void G1ParRemoveSelfForwardPtrsTask::work(uint worker_id) {
260   RemoveSelfForwardPtrHRClosure rsfp_cl(_rdcqs, worker_id);
261 
262   _g1h-&gt;collection_set_iterate_increment_from(&amp;rsfp_cl, &amp;_hrclaimer, worker_id);
</pre>
</td>
<td>
<hr />
<pre>
 62       return;
 63     }
 64 
 65     if (HeapRegion::is_in_same_region(p, CompressedOops::decode(o))) {
 66       return;
 67     }
 68     size_t card_index = _ct-&gt;index_for(p);
 69     if (card_index != _last_enqueued_card) {
 70       _rdcq-&gt;enqueue(_ct-&gt;byte_for_index(card_index));
 71       _last_enqueued_card = card_index;
 72     }
 73   }
 74 };
 75 
 76 class RemoveSelfForwardPtrObjClosure: public ObjectClosure {
 77   G1CollectedHeap* _g1h;
 78   G1ConcurrentMark* _cm;
 79   HeapRegion* _hr;
 80   size_t _marked_bytes;
 81   UpdateLogBuffersDeferred* _log_buffer_cl;
<span class="line-modified"> 82   bool _during_concurrent_start;</span>
 83   uint _worker_id;
 84   HeapWord* _last_forwarded_object_end;
 85 
 86 public:
 87   RemoveSelfForwardPtrObjClosure(HeapRegion* hr,
 88                                  UpdateLogBuffersDeferred* log_buffer_cl,
<span class="line-modified"> 89                                  bool during_concurrent_start,</span>
 90                                  uint worker_id) :
 91     _g1h(G1CollectedHeap::heap()),
 92     _cm(_g1h-&gt;concurrent_mark()),
 93     _hr(hr),
 94     _marked_bytes(0),
 95     _log_buffer_cl(log_buffer_cl),
<span class="line-modified"> 96     _during_concurrent_start(during_concurrent_start),</span>
 97     _worker_id(worker_id),
 98     _last_forwarded_object_end(hr-&gt;bottom()) { }
 99 
100   size_t marked_bytes() { return _marked_bytes; }
101 
102   // Iterate over the live objects in the region to find self-forwarded objects
103   // that need to be kept live. We need to update the remembered sets of these
104   // objects. Further update the BOT and marks.
105   // We can coalesce and overwrite the remaining heap contents with dummy objects
106   // as they have either been dead or evacuated (which are unreferenced now, i.e.
107   // dead too) already.
108   void do_object(oop obj) {
109     HeapWord* obj_addr = cast_from_oop&lt;HeapWord*&gt;(obj);
110     assert(_hr-&gt;is_in(obj_addr), &quot;sanity&quot;);
111 
112     if (obj-&gt;is_forwarded() &amp;&amp; obj-&gt;forwardee() == obj) {
113       // The object failed to move.
114 
115       zap_dead_objects(_last_forwarded_object_end, obj_addr);
116       // We consider all objects that we find self-forwarded to be
117       // live. What we&#39;ll do is that we&#39;ll update the prev marking
118       // info so that they are all under PTAMS and explicitly marked.
119       if (!_cm-&gt;is_marked_in_prev_bitmap(obj)) {
120         _cm-&gt;mark_in_prev_bitmap(obj);
121       }
<span class="line-modified">122       if (_during_concurrent_start) {</span>
123         // For the next marking info we&#39;ll only mark the
124         // self-forwarded objects explicitly if we are during
<span class="line-modified">125         // concurrent start (since, normally, we only mark objects pointed</span>
126         // to by roots if we succeed in copying them). By marking all
127         // self-forwarded objects we ensure that we mark any that are
128         // still pointed to be roots. During concurrent marking, and
<span class="line-modified">129         // after concurrent start, we don&#39;t need to mark any objects</span>
130         // explicitly and all objects in the CSet are considered
131         // (implicitly) live. So, we won&#39;t mark them explicitly and
132         // we&#39;ll leave them over NTAMS.
133         _cm-&gt;mark_in_next_bitmap(_worker_id, _hr, obj);
134       }
135       size_t obj_size = obj-&gt;size();
136 
137       _marked_bytes += (obj_size * HeapWordSize);
138       PreservedMarks::init_forwarded_mark(obj);
139 
140       // While we were processing RSet buffers during the collection,
141       // we actually didn&#39;t scan any cards on the collection set,
142       // since we didn&#39;t want to update remembered sets with entries
143       // that point into the collection set, given that live objects
144       // from the collection set are about to move and such entries
145       // will be stale very soon.
146       // This change also dealt with a reliability issue which
147       // involved scanning a card in the collection set and coming
148       // across an array that was being chunked and looking malformed.
149       // The problem is that, if evacuation fails, we might have
</pre>
<hr />
<pre>
194     zap_dead_objects(_last_forwarded_object_end, _hr-&gt;top());
195   }
196 };
197 
198 class RemoveSelfForwardPtrHRClosure: public HeapRegionClosure {
199   G1CollectedHeap* _g1h;
200   uint _worker_id;
201 
202   G1RedirtyCardsQueue _rdcq;
203   UpdateLogBuffersDeferred _log_buffer_cl;
204 
205 public:
206   RemoveSelfForwardPtrHRClosure(G1RedirtyCardsQueueSet* rdcqs, uint worker_id) :
207     _g1h(G1CollectedHeap::heap()),
208     _worker_id(worker_id),
209     _rdcq(rdcqs),
210     _log_buffer_cl(&amp;_rdcq) {
211   }
212 
213   size_t remove_self_forward_ptr_by_walking_hr(HeapRegion* hr,
<span class="line-modified">214                                                bool during_concurrent_start) {</span>
215     RemoveSelfForwardPtrObjClosure rspc(hr,
216                                         &amp;_log_buffer_cl,
<span class="line-modified">217                                         during_concurrent_start,</span>
218                                         _worker_id);
219     hr-&gt;object_iterate(&amp;rspc);
220     // Need to zap the remainder area of the processed region.
221     rspc.zap_remainder();
222 
223     return rspc.marked_bytes();
224   }
225 
226   bool do_heap_region(HeapRegion *hr) {
227     assert(!hr-&gt;is_pinned(), &quot;Unexpected pinned region at index %u&quot;, hr-&gt;hrm_index());
228     assert(hr-&gt;in_collection_set(), &quot;bad CS&quot;);
229 
230     if (hr-&gt;evacuation_failed()) {
231       hr-&gt;clear_index_in_opt_cset();
232 
<span class="line-modified">233       bool during_concurrent_start = _g1h-&gt;collector_state()-&gt;in_concurrent_start_gc();</span>
<span class="line-modified">234       bool during_concurrent_mark = _g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress();</span>
235 
<span class="line-modified">236       hr-&gt;note_self_forwarding_removal_start(during_concurrent_start,</span>
<span class="line-modified">237                                              during_concurrent_mark);</span>
238       _g1h-&gt;verifier()-&gt;check_bitmaps(&quot;Self-Forwarding Ptr Removal&quot;, hr);
239 
240       hr-&gt;reset_bot();
241 
<span class="line-modified">242       size_t live_bytes = remove_self_forward_ptr_by_walking_hr(hr, during_concurrent_start);</span>
243 
244       hr-&gt;rem_set()-&gt;clean_strong_code_roots(hr);
245       hr-&gt;rem_set()-&gt;clear_locked(true);
246 
247       hr-&gt;note_self_forwarding_removal_end(live_bytes);
248     }
249     return false;
250   }
251 };
252 
253 G1ParRemoveSelfForwardPtrsTask::G1ParRemoveSelfForwardPtrsTask(G1RedirtyCardsQueueSet* rdcqs) :
254   AbstractGangTask(&quot;G1 Remove Self-forwarding Pointers&quot;),
255   _g1h(G1CollectedHeap::heap()),
256   _rdcqs(rdcqs),
257   _hrclaimer(_g1h-&gt;workers()-&gt;active_workers()) { }
258 
259 void G1ParRemoveSelfForwardPtrsTask::work(uint worker_id) {
260   RemoveSelfForwardPtrHRClosure rsfp_cl(_rdcqs, worker_id);
261 
262   _g1h-&gt;collection_set_iterate_increment_from(&amp;rsfp_cl, &amp;_hrclaimer, worker_id);
</pre>
</td>
</tr>
</table>
<center><a href="g1ConcurrentMarkThread.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1HeapSizingPolicy.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>