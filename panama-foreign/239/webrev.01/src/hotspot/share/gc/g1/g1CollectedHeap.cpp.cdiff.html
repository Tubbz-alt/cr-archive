<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="c2/g1BarrierSetC2.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1141,81 ***</span>
  }
  
  void G1CollectedHeap::resize_heap_if_necessary() {
    assert_at_safepoint_on_vm_thread();
  
<span class="line-modified">!   // Capacity, free and used after the GC counted as full regions to</span>
<span class="line-modified">!   // include the waste in the following calculations.</span>
<span class="line-modified">!   const size_t capacity_after_gc = capacity();</span>
<span class="line-modified">!   const size_t used_after_gc = capacity_after_gc - unused_committed_regions_in_bytes();</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // This is enforced in arguments.cpp.</span>
<span class="line-modified">!   assert(MinHeapFreeRatio &lt;= MaxHeapFreeRatio,</span>
<span class="line-modified">!          &quot;otherwise the code below doesn&#39;t make sense&quot;);</span>
<span class="line-modified">! </span>
<span class="line-removed">-   // We don&#39;t have floating point command-line arguments</span>
<span class="line-removed">-   const double minimum_free_percentage = (double) MinHeapFreeRatio / 100.0;</span>
<span class="line-removed">-   const double maximum_used_percentage = 1.0 - minimum_free_percentage;</span>
<span class="line-removed">-   const double maximum_free_percentage = (double) MaxHeapFreeRatio / 100.0;</span>
<span class="line-removed">-   const double minimum_used_percentage = 1.0 - maximum_free_percentage;</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // We have to be careful here as these two calculations can overflow</span>
<span class="line-removed">-   // 32-bit size_t&#39;s.</span>
<span class="line-removed">-   double used_after_gc_d = (double) used_after_gc;</span>
<span class="line-removed">-   double minimum_desired_capacity_d = used_after_gc_d / maximum_used_percentage;</span>
<span class="line-removed">-   double maximum_desired_capacity_d = used_after_gc_d / minimum_used_percentage;</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // Let&#39;s make sure that they are both under the max heap size, which</span>
<span class="line-removed">-   // by default will make them fit into a size_t.</span>
<span class="line-removed">-   double desired_capacity_upper_bound = (double) MaxHeapSize;</span>
<span class="line-removed">-   minimum_desired_capacity_d = MIN2(minimum_desired_capacity_d,</span>
<span class="line-removed">-                                     desired_capacity_upper_bound);</span>
<span class="line-removed">-   maximum_desired_capacity_d = MIN2(maximum_desired_capacity_d,</span>
<span class="line-removed">-                                     desired_capacity_upper_bound);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // We can now safely turn them into size_t&#39;s.</span>
<span class="line-removed">-   size_t minimum_desired_capacity = (size_t) minimum_desired_capacity_d;</span>
<span class="line-removed">-   size_t maximum_desired_capacity = (size_t) maximum_desired_capacity_d;</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // This assert only makes sense here, before we adjust them</span>
<span class="line-removed">-   // with respect to the min and max heap size.</span>
<span class="line-removed">-   assert(minimum_desired_capacity &lt;= maximum_desired_capacity,</span>
<span class="line-removed">-          &quot;minimum_desired_capacity = &quot; SIZE_FORMAT &quot;, &quot;</span>
<span class="line-removed">-          &quot;maximum_desired_capacity = &quot; SIZE_FORMAT,</span>
<span class="line-removed">-          minimum_desired_capacity, maximum_desired_capacity);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // Should not be greater than the heap max size. No need to adjust</span>
<span class="line-removed">-   // it with respect to the heap min size as it&#39;s a lower bound (i.e.,</span>
<span class="line-removed">-   // we&#39;ll try to make the capacity larger than it, not smaller).</span>
<span class="line-removed">-   minimum_desired_capacity = MIN2(minimum_desired_capacity, MaxHeapSize);</span>
<span class="line-removed">-   // Should not be less than the heap min size. No need to adjust it</span>
<span class="line-removed">-   // with respect to the heap max size as it&#39;s an upper bound (i.e.,</span>
<span class="line-removed">-   // we&#39;ll try to make the capacity smaller than it, not greater).</span>
<span class="line-removed">-   maximum_desired_capacity =  MAX2(maximum_desired_capacity, MinHeapSize);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   if (capacity_after_gc &lt; minimum_desired_capacity) {</span>
<span class="line-removed">-     // Don&#39;t expand unless it&#39;s significant</span>
<span class="line-removed">-     size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;</span>
<span class="line-removed">- </span>
<span class="line-removed">-     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (capacity lower than min desired capacity). &quot;</span>
<span class="line-removed">-                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;</span>
<span class="line-removed">-                               &quot;min_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,</span>
<span class="line-removed">-                               capacity_after_gc, used_after_gc, used(), minimum_desired_capacity, MinHeapFreeRatio);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     expand(expand_bytes, _workers);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     // No expansion, now see if we want to shrink</span>
<span class="line-removed">-   } else if (capacity_after_gc &gt; maximum_desired_capacity) {</span>
<span class="line-removed">-     // Capacity too large, compute shrinking size</span>
<span class="line-removed">-     size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;</span>
<span class="line-removed">- </span>
<span class="line-removed">-     log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (capacity higher than max desired capacity). &quot;</span>
<span class="line-removed">-                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;</span>
<span class="line-removed">-                               &quot;maximum_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,</span>
<span class="line-removed">-                               capacity_after_gc, used_after_gc, used(), maximum_desired_capacity, MaxHeapFreeRatio);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     shrink(shrink_bytes);</span>
    }
  }
  
  HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,
                                                              bool do_gc,
<span class="line-new-header">--- 1141,19 ---</span>
  }
  
  void G1CollectedHeap::resize_heap_if_necessary() {
    assert_at_safepoint_on_vm_thread();
  
<span class="line-modified">!   bool should_expand;</span>
<span class="line-modified">!   size_t resize_amount = _heap_sizing_policy-&gt;full_collection_resize_amount(should_expand);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   if (resize_amount == 0) {</span>
<span class="line-modified">!     return;</span>
<span class="line-modified">!   } else if (should_expand) {</span>
<span class="line-modified">!     expand(resize_amount, _workers);</span>
<span class="line-modified">!   } else {</span>
<span class="line-modified">!     shrink(resize_amount);</span>
    }
  }
  
  HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,
                                                              bool do_gc,
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1866,11 ***</span>
    //   (both full and incremental).
    // * Both ref processors need to &#39;span&#39; the entire heap as
    //   the regions in the collection set may be dotted around.
    //
    // * For the concurrent marking ref processor:
<span class="line-modified">!   //   * Reference discovery is enabled at initial marking.</span>
    //   * Reference discovery is disabled and the discovered
    //     references processed etc during remarking.
    //   * Reference discovery is MT (see below).
    //   * Reference discovery requires a barrier (see below).
    //   * Reference processing may or may not be MT
<span class="line-new-header">--- 1804,11 ---</span>
    //   (both full and incremental).
    // * Both ref processors need to &#39;span&#39; the entire heap as
    //   the regions in the collection set may be dotted around.
    //
    // * For the concurrent marking ref processor:
<span class="line-modified">!   //   * Reference discovery is enabled at concurrent start.</span>
    //   * Reference discovery is disabled and the discovered
    //     references processed etc during remarking.
    //   * Reference discovery is MT (see below).
    //   * Reference discovery requires a barrier (see below).
    //   * Reference processing may or may not be MT
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2107,11 ***</span>
    assert_heap_not_locked();
    assert(should_do_concurrent_full_gc(cause),
           &quot;Non-concurrent cause %s&quot;, GCCause::to_string(cause));
  
    for (uint i = 1; true; ++i) {
<span class="line-modified">!     // Try to schedule an initial-mark evacuation pause that will</span>
      // start a concurrent cycle.
      LOG_COLLECT_CONCURRENTLY(cause, &quot;attempt %u&quot;, i);
      VM_G1TryInitiateConcMark op(gc_counter,
                                  cause,
                                  policy()-&gt;max_pause_time_ms());
<span class="line-new-header">--- 2045,11 ---</span>
    assert_heap_not_locked();
    assert(should_do_concurrent_full_gc(cause),
           &quot;Non-concurrent cause %s&quot;, GCCause::to_string(cause));
  
    for (uint i = 1; true; ++i) {
<span class="line-modified">!     // Try to schedule concurrent start evacuation pause that will</span>
      // start a concurrent cycle.
      LOG_COLLECT_CONCURRENTLY(cause, &quot;attempt %u&quot;, i);
      VM_G1TryInitiateConcMark op(gc_counter,
                                  cause,
                                  policy()-&gt;max_pause_time_ms());
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2176,11 ***</span>
        // Cases (4) and (5) are detected together by a change to
        // _old_marking_cycles_started.
        //
        // Note that (1) does not imply (4).  If we&#39;re still in the mixed
        // phase of an earlier concurrent collection, the request to make the
<span class="line-modified">!       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for</span>
        // both conditions we&#39;ll spin doing back-to-back collections.
        if (op.gc_succeeded() ||
            op.cycle_already_in_progress() ||
            op.whitebox_attached() ||
            (old_marking_started_before != old_marking_started_after)) {
<span class="line-new-header">--- 2114,11 ---</span>
        // Cases (4) and (5) are detected together by a change to
        // _old_marking_cycles_started.
        //
        // Note that (1) does not imply (4).  If we&#39;re still in the mixed
        // phase of an earlier concurrent collection, the request to make the
<span class="line-modified">!       // collection a concurrent start won&#39;t be honored.  If we don&#39;t check for</span>
        // both conditions we&#39;ll spin doing back-to-back collections.
        if (op.gc_succeeded() ||
            op.cycle_already_in_progress() ||
            op.whitebox_attached() ||
            (old_marking_started_before != old_marking_started_after)) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2674,11 ***</span>
    // This summary needs to be printed before incrementing total collections.
    rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
  
    // Update common counters.
    increment_total_collections(full /* full gc */);
<span class="line-modified">!   if (full || collector_state()-&gt;in_initial_mark_gc()) {</span>
      increment_old_marking_cycles_started();
    }
  
    // Fill TLAB&#39;s and such
    {
<span class="line-new-header">--- 2612,11 ---</span>
    // This summary needs to be printed before incrementing total collections.
    rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
  
    // Update common counters.
    increment_total_collections(full /* full gc */);
<span class="line-modified">!   if (full || collector_state()-&gt;in_concurrent_start_gc()) {</span>
      increment_old_marking_cycles_started();
    }
  
    // Fill TLAB&#39;s and such
    {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2905,11 ***</span>
      _collection_set.iterate_optional(&amp;cl);
    }
  }
  
  G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {
<span class="line-modified">!   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
      return G1HeapVerifier::G1VerifyConcurrentStart;
    } else if (collector_state()-&gt;in_young_only_phase()) {
      return G1HeapVerifier::G1VerifyYoungNormal;
    } else {
      return G1HeapVerifier::G1VerifyMixed;
<span class="line-new-header">--- 2843,11 ---</span>
      _collection_set.iterate_optional(&amp;cl);
    }
  }
  
  G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {
<span class="line-modified">!   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
      return G1HeapVerifier::G1VerifyConcurrentStart;
    } else if (collector_state()-&gt;in_young_only_phase()) {
      return G1HeapVerifier::G1VerifyYoungNormal;
    } else {
      return G1HeapVerifier::G1VerifyMixed;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2937,11 ***</span>
    _verifier-&gt;check_bitmaps(&quot;GC End&quot;);
    verify_numa_regions(&quot;GC End&quot;);
  }
  
  void G1CollectedHeap::expand_heap_after_young_collection(){
<span class="line-modified">!   size_t expand_bytes = _heap_sizing_policy-&gt;expansion_amount();</span>
    if (expand_bytes &gt; 0) {
      // No need for an ergo logging here,
      // expansion_amount() does this when it returns a value &gt; 0.
      double expand_ms;
      if (!expand(expand_bytes, _workers, &amp;expand_ms)) {
<span class="line-new-header">--- 2875,11 ---</span>
    _verifier-&gt;check_bitmaps(&quot;GC End&quot;);
    verify_numa_regions(&quot;GC End&quot;);
  }
  
  void G1CollectedHeap::expand_heap_after_young_collection(){
<span class="line-modified">!   size_t expand_bytes = _heap_sizing_policy-&gt;young_collection_expansion_amount();</span>
    if (expand_bytes &gt; 0) {
      // No need for an ergo logging here,
      // expansion_amount() does this when it returns a value &gt; 0.
      double expand_ms;
      if (!expand(expand_bytes, _workers, &amp;expand_ms)) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2950,11 ***</span>
      phase_times()-&gt;record_expand_heap_time(expand_ms);
    }
  }
  
  const char* G1CollectedHeap::young_gc_name() const {
<span class="line-modified">!   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
      return &quot;Pause Young (Concurrent Start)&quot;;
    } else if (collector_state()-&gt;in_young_only_phase()) {
      if (collector_state()-&gt;in_young_gc_before_mixed()) {
        return &quot;Pause Young (Prepare Mixed)&quot;;
      } else {
<span class="line-new-header">--- 2888,11 ---</span>
      phase_times()-&gt;record_expand_heap_time(expand_ms);
    }
  }
  
  const char* G1CollectedHeap::young_gc_name() const {
<span class="line-modified">!   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
      return &quot;Pause Young (Concurrent Start)&quot;;
    } else if (collector_state()-&gt;in_young_only_phase()) {
      if (collector_state()-&gt;in_young_gc_before_mixed()) {
        return &quot;Pause Young (Prepare Mixed)&quot;;
      } else {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3003,28 ***</span>
    trace_heap_before_gc(_gc_tracer_stw);
  
    _verifier-&gt;verify_region_sets_optional();
    _verifier-&gt;verify_dirty_young_regions();
  
<span class="line-modified">!   // We should not be doing initial mark unless the conc mark thread is running</span>
    if (!_cm_thread-&gt;should_terminate()) {
<span class="line-modified">!     // This call will decide whether this pause is an initial-mark</span>
<span class="line-modified">!     // pause. If it is, in_initial_mark_gc() will return true</span>
      // for the duration of this pause.
      policy()-&gt;decide_on_conc_mark_initiation();
    }
  
<span class="line-modified">!   // We do not allow initial-mark to be piggy-backed on a mixed GC.</span>
<span class="line-modified">!   assert(!collector_state()-&gt;in_initial_mark_gc() ||</span>
           collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
    // We also do not allow mixed GCs during marking.
    assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
  
<span class="line-modified">!   // Record whether this pause is an initial mark. When the current</span>
    // thread has completed its logging output and it&#39;s safe to signal
    // the CM thread, the flag&#39;s value in the policy has been reset.
<span class="line-modified">!   bool should_start_conc_mark = collector_state()-&gt;in_initial_mark_gc();</span>
    if (should_start_conc_mark) {
      _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());
    }
  
    // Inner scope for scope based logging, timers, and stats collection
<span class="line-new-header">--- 2941,28 ---</span>
    trace_heap_before_gc(_gc_tracer_stw);
  
    _verifier-&gt;verify_region_sets_optional();
    _verifier-&gt;verify_dirty_young_regions();
  
<span class="line-modified">!   // We should not be doing concurrent start unless the concurrent mark thread is running</span>
    if (!_cm_thread-&gt;should_terminate()) {
<span class="line-modified">!     // This call will decide whether this pause is a concurrent start</span>
<span class="line-modified">!     // pause. If it is, in_concurrent_start_gc() will return true</span>
      // for the duration of this pause.
      policy()-&gt;decide_on_conc_mark_initiation();
    }
  
<span class="line-modified">!   // We do not allow concurrent start to be piggy-backed on a mixed GC.</span>
<span class="line-modified">!   assert(!collector_state()-&gt;in_concurrent_start_gc() ||</span>
           collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
    // We also do not allow mixed GCs during marking.
    assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
  
<span class="line-modified">!   // Record whether this pause is a concurrent start. When the current</span>
    // thread has completed its logging output and it&#39;s safe to signal
    // the CM thread, the flag&#39;s value in the policy has been reset.
<span class="line-modified">!   bool should_start_conc_mark = collector_state()-&gt;in_concurrent_start_gc();</span>
    if (should_start_conc_mark) {
      _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());
    }
  
    // Inner scope for scope based logging, timers, and stats collection
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3104,11 ***</span>
  
          if (should_start_conc_mark) {
            // We have to do this before we notify the CM threads that
            // they can start working to make sure that all the
            // appropriate initialization is done on the CM object.
<span class="line-modified">!           concurrent_mark()-&gt;post_initial_mark();</span>
            // Note that we don&#39;t actually trigger the CM thread at
            // this point. We do that later when we&#39;re sure that
            // the current thread has completed its logging output.
          }
  
<span class="line-new-header">--- 3042,11 ---</span>
  
          if (should_start_conc_mark) {
            // We have to do this before we notify the CM threads that
            // they can start working to make sure that all the
            // appropriate initialization is done on the CM object.
<span class="line-modified">!           concurrent_mark()-&gt;post_concurrent_start();</span>
            // Note that we don&#39;t actually trigger the CM thread at
            // this point. We do that later when we&#39;re sure that
            // the current thread has completed its logging output.
          }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3585,11 ***</span>
    double ref_proc_time = os::elapsedTime() - ref_proc_start;
    phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
  }
  
  void G1CollectedHeap::make_pending_list_reachable() {
<span class="line-modified">!   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
      oop pll_head = Universe::reference_pending_list();
      if (pll_head != NULL) {
        // Any valid worker id is fine here as we are in the VM thread and single-threaded.
        _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
      }
<span class="line-new-header">--- 3523,11 ---</span>
    double ref_proc_time = os::elapsedTime() - ref_proc_start;
    phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
  }
  
  void G1CollectedHeap::make_pending_list_reachable() {
<span class="line-modified">!   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
      oop pll_head = Universe::reference_pending_list();
      if (pll_head != NULL) {
        // Any valid worker id is fine here as we are in the VM thread and single-threaded.
        _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
      }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3774,13 ***</span>
  
  #if COMPILER2_OR_JVMCI
    DerivedPointerTable::clear();
  #endif
  
<span class="line-modified">!   // InitialMark needs claim bits to keep track of the marked-through CLDs.</span>
<span class="line-modified">!   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-modified">!     concurrent_mark()-&gt;pre_initial_mark();</span>
  
      double start_clear_claimed_marks = os::elapsedTime();
  
      ClassLoaderDataGraph::clear_claimed_marks();
  
<span class="line-new-header">--- 3712,13 ---</span>
  
  #if COMPILER2_OR_JVMCI
    DerivedPointerTable::clear();
  #endif
  
<span class="line-modified">!   // Concurrent start needs claim bits to keep track of the marked-through CLDs.</span>
<span class="line-modified">!   if (collector_state()-&gt;in_concurrent_start_gc()) {</span>
<span class="line-modified">!     concurrent_mark()-&gt;pre_concurrent_start();</span>
  
      double start_clear_claimed_marks = os::elapsedTime();
  
      ClassLoaderDataGraph::clear_claimed_marks();
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 4844,11 ***</span>
    } else {
      assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());
      _survivor.add_used_bytes(allocated_bytes);
    }
  
<span class="line-modified">!   bool const during_im = collector_state()-&gt;in_initial_mark_gc();</span>
    if (during_im &amp;&amp; allocated_bytes &gt; 0) {
      _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
    }
    _hr_printer.retire(alloc_region);
  }
<span class="line-new-header">--- 4782,11 ---</span>
    } else {
      assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());
      _survivor.add_used_bytes(allocated_bytes);
    }
  
<span class="line-modified">!   bool const during_im = collector_state()-&gt;in_concurrent_start_gc();</span>
    if (during_im &amp;&amp; allocated_bytes &gt; 0) {
      _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
    }
    _hr_printer.retire(alloc_region);
  }
</pre>
<center><a href="c2/g1BarrierSetC2.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>