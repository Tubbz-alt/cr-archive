<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/arm/c1_CodeStubs_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/macroAssembler.inline.hpp&quot;
 27 #include &quot;c1/c1_CodeStubs.hpp&quot;
 28 #include &quot;c1/c1_FrameMap.hpp&quot;
 29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 31 #include &quot;c1/c1_Runtime1.hpp&quot;
 32 #include &quot;classfile/javaClasses.hpp&quot;
 33 #include &quot;memory/universe.hpp&quot;
 34 #include &quot;nativeInst_arm.hpp&quot;
 35 #include &quot;runtime/sharedRuntime.hpp&quot;
 36 #include &quot;utilities/macros.hpp&quot;
 37 #include &quot;vmreg_arm.inline.hpp&quot;
 38 
 39 #define __ ce-&gt;masm()-&gt;
 40 
 41 void CounterOverflowStub::emit_code(LIR_Assembler* ce) {
 42   __ bind(_entry);
 43   ce-&gt;store_parameter(_bci, 0);
 44   ce-&gt;store_parameter(_method-&gt;as_constant_ptr()-&gt;as_metadata(), 1);
 45   __ call(Runtime1::entry_for(Runtime1::counter_overflow_id), relocInfo::runtime_call_type);
 46   ce-&gt;add_call_info_here(_info);
 47   ce-&gt;verify_oop_map(_info);
 48 
 49   __ b(_continuation);
 50 }
 51 
 52 
 53 // TODO: ARM - is it possible to inline these stubs into the main code stream?
 54 
 55 
 56 RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)
 57   : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {
 58   assert(info != NULL, &quot;must have info&quot;);
 59   _info = new CodeEmitInfo(info);
 60 }
 61 
 62 RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)
 63   : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {
 64   assert(info != NULL, &quot;must have info&quot;);
 65   _info = new CodeEmitInfo(info);
 66 }
 67 
 68 void RangeCheckStub::emit_code(LIR_Assembler* ce) {
 69   __ bind(_entry);
 70 
 71   if (_info-&gt;deoptimize_on_exception()) {
 72     __ call(Runtime1::entry_for(Runtime1::predicate_failed_trap_id), relocInfo::runtime_call_type);
 73     ce-&gt;add_call_info_here(_info);
 74     ce-&gt;verify_oop_map(_info);
 75     debug_only(__ should_not_reach_here());
 76     return;
 77   }
 78   // Pass the array index on stack because all registers must be preserved
 79   ce-&gt;verify_reserved_argument_area_size(_throw_index_out_of_bounds_exception ? 1 : 2);
 80   if (_index-&gt;is_cpu_register()) {
 81     __ str_32(_index-&gt;as_register(), Address(SP));
 82   } else {
 83     __ mov_slow(Rtemp, _index-&gt;as_jint()); // Rtemp should be OK in C1
 84     __ str_32(Rtemp, Address(SP));
 85   }
 86 
 87   if (_throw_index_out_of_bounds_exception) {
 88     __ call(Runtime1::entry_for(Runtime1::throw_index_exception_id), relocInfo::runtime_call_type);
 89   } else {
 90     __ str(_array-&gt;as_pointer_register(), Address(SP, BytesPerWord)); // ??? Correct offset? Correct instruction?
 91     __ call(Runtime1::entry_for(Runtime1::throw_range_check_failed_id), relocInfo::runtime_call_type);
 92   }
 93   ce-&gt;add_call_info_here(_info);
 94   ce-&gt;verify_oop_map(_info);
 95   DEBUG_ONLY(STOP(&quot;RangeCheck&quot;);)
 96 }
 97 
 98 PredicateFailedStub::PredicateFailedStub(CodeEmitInfo* info) {
 99   _info = new CodeEmitInfo(info);
100 }
101 
102 void PredicateFailedStub::emit_code(LIR_Assembler* ce) {
103   __ bind(_entry);
104   __ call(Runtime1::entry_for(Runtime1::predicate_failed_trap_id), relocInfo::runtime_call_type);
105   ce-&gt;add_call_info_here(_info);
106   ce-&gt;verify_oop_map(_info);
107   debug_only(__ should_not_reach_here());
108 }
109 
110 void DivByZeroStub::emit_code(LIR_Assembler* ce) {
111   if (_offset != -1) {
112     ce-&gt;compilation()-&gt;implicit_exception_table()-&gt;append(_offset, __ offset());
113   }
114   __ bind(_entry);
115   __ call(Runtime1::entry_for(Runtime1::throw_div0_exception_id),
116           relocInfo::runtime_call_type);
117   ce-&gt;add_call_info_here(_info);
118   DEBUG_ONLY(STOP(&quot;DivByZero&quot;);)
119 }
120 
121 
122 // Implementation of NewInstanceStub
123 
124 NewInstanceStub::NewInstanceStub(LIR_Opr klass_reg, LIR_Opr result, ciInstanceKlass* klass, CodeEmitInfo* info, Runtime1::StubID stub_id) {
125   _result = result;
126   _klass = klass;
127   _klass_reg = klass_reg;
128   _info = new CodeEmitInfo(info);
129   assert(stub_id == Runtime1::new_instance_id                 ||
130          stub_id == Runtime1::fast_new_instance_id            ||
131          stub_id == Runtime1::fast_new_instance_init_check_id,
132          &quot;need new_instance id&quot;);
133   _stub_id   = stub_id;
134 }
135 
136 
137 void NewInstanceStub::emit_code(LIR_Assembler* ce) {
138   assert(_result-&gt;as_register() == R0, &quot;runtime call setup&quot;);
139   assert(_klass_reg-&gt;as_register() == R1, &quot;runtime call setup&quot;);
140   __ bind(_entry);
141   __ call(Runtime1::entry_for(_stub_id), relocInfo::runtime_call_type);
142   ce-&gt;add_call_info_here(_info);
143   ce-&gt;verify_oop_map(_info);
144   __ b(_continuation);
145 }
146 
147 
148 // Implementation of NewTypeArrayStub
149 
150 NewTypeArrayStub::NewTypeArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {
151   _klass_reg = klass_reg;
152   _length = length;
153   _result = result;
154   _info = new CodeEmitInfo(info);
155 }
156 
157 
158 void NewTypeArrayStub::emit_code(LIR_Assembler* ce) {
159   assert(_result-&gt;as_register() == R0, &quot;runtime call setup&quot;);
160   assert(_klass_reg-&gt;as_register() == R1, &quot;runtime call setup&quot;);
161   assert(_length-&gt;as_register() == R2, &quot;runtime call setup&quot;);
162   __ bind(_entry);
163   __ call(Runtime1::entry_for(Runtime1::new_type_array_id), relocInfo::runtime_call_type);
164   ce-&gt;add_call_info_here(_info);
165   ce-&gt;verify_oop_map(_info);
166   __ b(_continuation);
167 }
168 
169 
170 // Implementation of NewObjectArrayStub
171 
172 NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {
173   _klass_reg = klass_reg;
174   _result = result;
175   _length = length;
176   _info = new CodeEmitInfo(info);
177 }
178 
179 
180 void NewObjectArrayStub::emit_code(LIR_Assembler* ce) {
181   assert(_result-&gt;as_register() == R0, &quot;runtime call setup&quot;);
182   assert(_klass_reg-&gt;as_register() == R1, &quot;runtime call setup&quot;);
183   assert(_length-&gt;as_register() == R2, &quot;runtime call setup&quot;);
184   __ bind(_entry);
185   __ call(Runtime1::entry_for(Runtime1::new_object_array_id), relocInfo::runtime_call_type);
186   ce-&gt;add_call_info_here(_info);
187   ce-&gt;verify_oop_map(_info);
188   __ b(_continuation);
189 }
190 
191 
192 // Implementation of MonitorAccessStubs
193 
194 MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)
195 : MonitorAccessStub(obj_reg, lock_reg)
196 {
197   _info = new CodeEmitInfo(info);
198 }
199 
200 
201 void MonitorEnterStub::emit_code(LIR_Assembler* ce) {
202   __ bind(_entry);
203   const Register obj_reg = _obj_reg-&gt;as_pointer_register();
204   const Register lock_reg = _lock_reg-&gt;as_pointer_register();
205 
206   ce-&gt;verify_reserved_argument_area_size(2);
207   if (obj_reg &lt; lock_reg) {
208     __ stmia(SP, RegisterSet(obj_reg) | RegisterSet(lock_reg));
209   } else {
210     __ str(obj_reg, Address(SP));
211     __ str(lock_reg, Address(SP, BytesPerWord));
212   }
213 
214   Runtime1::StubID enter_id = ce-&gt;compilation()-&gt;has_fpu_code() ?
215                               Runtime1::monitorenter_id :
216                               Runtime1::monitorenter_nofpu_id;
217   __ call(Runtime1::entry_for(enter_id), relocInfo::runtime_call_type);
218   ce-&gt;add_call_info_here(_info);
219   ce-&gt;verify_oop_map(_info);
220   __ b(_continuation);
221 }
222 
223 
224 void MonitorExitStub::emit_code(LIR_Assembler* ce) {
225   __ bind(_entry);
226   if (_compute_lock) {
227     ce-&gt;monitor_address(_monitor_ix, _lock_reg);
228   }
229   const Register lock_reg = _lock_reg-&gt;as_pointer_register();
230 
231   ce-&gt;verify_reserved_argument_area_size(1);
232   __ str(lock_reg, Address(SP));
233 
234   // Non-blocking leaf routine - no call info needed
235   Runtime1::StubID exit_id = ce-&gt;compilation()-&gt;has_fpu_code() ?
236                              Runtime1::monitorexit_id :
237                              Runtime1::monitorexit_nofpu_id;
238   __ call(Runtime1::entry_for(exit_id), relocInfo::runtime_call_type);
239   __ b(_continuation);
240 }
241 
242 
243 // Call return is directly after patch word
244 int PatchingStub::_patch_info_offset = 0;
245 
246 void PatchingStub::align_patch_site(MacroAssembler* masm) {
247 #if 0
248   // TODO: investigate if we required to implement this
249     ShouldNotReachHere();
250 #endif
251 }
252 
253 void PatchingStub::emit_code(LIR_Assembler* ce) {
254   const int patchable_instruction_offset = 0;
255 
256   assert(NativeCall::instruction_size &lt;= _bytes_to_copy &amp;&amp; _bytes_to_copy &lt;= 0xFF,
257          &quot;not enough room for call&quot;);
258   assert((_bytes_to_copy &amp; 3) == 0, &quot;must copy a multiple of four bytes&quot;);
259   Label call_patch;
260   bool is_load = (_id == load_klass_id) || (_id == load_mirror_id) || (_id == load_appendix_id);
261 
262 
263   if (is_load &amp;&amp; !VM_Version::supports_movw()) {
264     address start = __ pc();
265 
266     // The following sequence duplicates code provided in MacroAssembler::patchable_mov_oop()
267     // without creating relocation info entry.
268 
269     assert((__ pc() - start) == patchable_instruction_offset, &quot;should be&quot;);
270     __ ldr(_obj, Address(PC));
271     // Extra nop to handle case of large offset of oop placeholder (see NativeMovConstReg::set_data).
272     __ nop();
273 
274 #ifdef ASSERT
275     for (int i = 0; i &lt; _bytes_to_copy; i++) {
276       assert(((address)_pc_start)[i] == start[i], &quot;should be the same code&quot;);
277     }
278 #endif // ASSERT
279   }
280 
281   address being_initialized_entry = __ pc();
282   if (CommentedAssembly) {
283     __ block_comment(&quot; patch template&quot;);
284   }
285   if (is_load) {
286     address start = __ pc();
287     if (_id == load_mirror_id || _id == load_appendix_id) {
288       __ patchable_mov_oop(_obj, (jobject)Universe::non_oop_word(), _index);
289     } else {
290       __ patchable_mov_metadata(_obj, (Metadata*)Universe::non_oop_word(), _index);
291     }
292 #ifdef ASSERT
293     for (int i = 0; i &lt; _bytes_to_copy; i++) {
294       assert(((address)_pc_start)[i] == start[i], &quot;should be the same code&quot;);
295     }
296 #endif // ASSERT
297   } else {
298     int* start = (int*)_pc_start;
299     int* end = start + (_bytes_to_copy / BytesPerInt);
300     while (start &lt; end) {
301       __ emit_int32(*start++);
302     }
303   }
304   address end_of_patch = __ pc();
305 
306   int bytes_to_skip = 0;
307   if (_id == load_mirror_id) {
308     int offset = __ offset();
309     if (CommentedAssembly) {
310       __ block_comment(&quot; being_initialized check&quot;);
311     }
312 
313     assert(_obj != noreg, &quot;must be a valid register&quot;);
314     // Rtemp should be OK in C1
315     __ ldr(Rtemp, Address(_obj, java_lang_Class::klass_offset()));
316     __ ldr(Rtemp, Address(Rtemp, InstanceKlass::init_thread_offset()));
317     __ cmp(Rtemp, Rthread);
318     __ b(call_patch, ne);
319     __ b(_patch_site_continuation);
320 
321     bytes_to_skip += __ offset() - offset;
322   }
323 
324   if (CommentedAssembly) {
325     __ block_comment(&quot;patch data - 3 high bytes of the word&quot;);
326   }
327   const int sizeof_patch_record = 4;
328   bytes_to_skip += sizeof_patch_record;
329   int being_initialized_entry_offset = __ pc() - being_initialized_entry + sizeof_patch_record;
330   __ emit_int32(0xff | being_initialized_entry_offset &lt;&lt; 8 | bytes_to_skip &lt;&lt; 16 | _bytes_to_copy &lt;&lt; 24);
331 
332   address patch_info_pc = __ pc();
333   assert(patch_info_pc - end_of_patch == bytes_to_skip, &quot;incorrect patch info&quot;);
334 
335   // runtime call will return here
336   Label call_return;
337   __ bind(call_return);
338   ce-&gt;add_call_info_here(_info);
339   assert(_patch_info_offset == (patch_info_pc - __ pc()), &quot;must not change&quot;);
340   __ b(_patch_site_entry);
341 
342   address entry = __ pc();
343   NativeGeneralJump::insert_unconditional((address)_pc_start, entry);
344   address target = NULL;
345   relocInfo::relocType reloc_type = relocInfo::none;
346   switch (_id) {
347     case access_field_id:  target = Runtime1::entry_for(Runtime1::access_field_patching_id); break;
348     case load_klass_id:    target = Runtime1::entry_for(Runtime1::load_klass_patching_id); reloc_type = relocInfo::metadata_type; break;
349     case load_mirror_id:   target = Runtime1::entry_for(Runtime1::load_mirror_patching_id); reloc_type = relocInfo::oop_type; break;
350     case load_appendix_id: target = Runtime1::entry_for(Runtime1::load_appendix_patching_id); reloc_type = relocInfo::oop_type; break;
351     default: ShouldNotReachHere();
352   }
353   __ bind(call_patch);
354 
355   if (CommentedAssembly) {
356     __ block_comment(&quot;patch entry point&quot;);
357   }
358 
359   // arrange for call to return just after patch word
360   __ adr(LR, call_return);
361   __ jump(target, relocInfo::runtime_call_type, Rtemp);
362 
363   if (is_load) {
364     CodeSection* cs = __ code_section();
365     address pc = (address)_pc_start;
366     RelocIterator iter(cs, pc, pc + 1);
367     relocInfo::change_reloc_info_for_address(&amp;iter, pc, reloc_type, relocInfo::none);
368   }
369 }
370 
371 void DeoptimizeStub::emit_code(LIR_Assembler* ce) {
372   __ bind(_entry);
373   __ mov_slow(Rtemp, _trap_request);
374   ce-&gt;verify_reserved_argument_area_size(1);
375   __ str(Rtemp, Address(SP));
376   __ call(Runtime1::entry_for(Runtime1::deoptimize_id), relocInfo::runtime_call_type);
377   ce-&gt;add_call_info_here(_info);
378   DEBUG_ONLY(__ should_not_reach_here());
379 }
380 
381 
382 void ImplicitNullCheckStub::emit_code(LIR_Assembler* ce) {
383   address a;
384   if (_info-&gt;deoptimize_on_exception()) {
385     // Deoptimize, do not throw the exception, because it is
386     // probably wrong to do it here.
387     a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);
388   } else {
389     a = Runtime1::entry_for(Runtime1::throw_null_pointer_exception_id);
390   }
391   ce-&gt;compilation()-&gt;implicit_exception_table()-&gt;append(_offset, __ offset());
392   __ bind(_entry);
393   __ call(a, relocInfo::runtime_call_type);
394   ce-&gt;add_call_info_here(_info);
395   ce-&gt;verify_oop_map(_info);
396   DEBUG_ONLY(STOP(&quot;ImplicitNullCheck&quot;);)
397 }
398 
399 
400 void SimpleExceptionStub::emit_code(LIR_Assembler* ce) {
401   __ bind(_entry);
402   // Pass the object on stack because all registers must be preserved
403   if (_obj-&gt;is_cpu_register()) {
404     ce-&gt;verify_reserved_argument_area_size(1);
405     __ str(_obj-&gt;as_pointer_register(), Address(SP));
406   } else {
407     assert(_obj-&gt;is_illegal(), &quot;should be&quot;);
408   }
409   __ call(Runtime1::entry_for(_stub), relocInfo::runtime_call_type);
410   ce-&gt;add_call_info_here(_info);
411   DEBUG_ONLY(STOP(&quot;SimpleException&quot;);)
412 }
413 
414 
415 void ArrayCopyStub::emit_code(LIR_Assembler* ce) {
416   __ bind(_entry);
417 
418   VMRegPair args[5];
419   BasicType signature[5] = { T_OBJECT, T_INT, T_OBJECT, T_INT, T_INT };
420   SharedRuntime::java_calling_convention(signature, args, 5, true);
421 
422   Register r[5];
423   r[0] = src()-&gt;as_pointer_register();
424   r[1] = src_pos()-&gt;as_register();
425   r[2] = dst()-&gt;as_pointer_register();
426   r[3] = dst_pos()-&gt;as_register();
427   r[4] = length()-&gt;as_register();
428 
429   for (int i = 0; i &lt; 5; i++) {
430     VMReg arg = args[i].first();
431     if (arg-&gt;is_stack()) {
432       __ str(r[i], Address(SP, arg-&gt;reg2stack() * VMRegImpl::stack_slot_size));
433     } else {
434       assert(r[i] == arg-&gt;as_Register(), &quot;Calling conventions must match&quot;);
435     }
436   }
437 
438   ce-&gt;emit_static_call_stub();
439   if (ce-&gt;compilation()-&gt;bailed_out()) {
440     return; // CodeCache is full
441   }
442   int ret_addr_offset = __ patchable_call(SharedRuntime::get_resolve_static_call_stub(), relocInfo::static_call_type);
443   assert(ret_addr_offset == __ offset(), &quot;embedded return address not allowed&quot;);
444   ce-&gt;add_call_info_here(info());
445   ce-&gt;verify_oop_map(info());
446   __ b(_continuation);
447 }
448 
449 #undef __
    </pre>
  </body>
</html>