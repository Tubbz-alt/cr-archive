<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../arm/c1_CodeStubs_arm.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1689     __ get_fpsr(r1);
1690     __ cbzw(r1, L_Okay);
1691     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l));
1692     __ bind(L_Okay);
1693   }
1694     break;
1695   case Bytecodes::_d2f:
1696     __ fcvtd(v0, v0);
1697     break;
1698   default:
1699     ShouldNotReachHere();
1700   }
1701 }
1702 
1703 void TemplateTable::lcmp()
1704 {
1705   transition(ltos, itos);
1706   Label done;
1707   __ pop_l(r1);
1708   __ cmp(r1, r0);
<span class="line-modified">1709   __ mov(r0, (u_int64_t)-1L);</span>
1710   __ br(Assembler::LT, done);
1711   // __ mov(r0, 1UL);
1712   // __ csel(r0, r0, zr, Assembler::NE);
1713   // and here is a faster way
1714   __ csinc(r0, zr, zr, Assembler::EQ);
1715   __ bind(done);
1716 }
1717 
1718 void TemplateTable::float_cmp(bool is_float, int unordered_result)
1719 {
1720   Label done;
1721   if (is_float) {
1722     // XXX get rid of pop here, use ... reg, mem32
1723     __ pop_f(v1);
1724     __ fcmps(v1, v0);
1725   } else {
1726     // XXX get rid of pop here, use ... reg, mem64
1727     __ pop_d(v1);
1728     __ fcmpd(v1, v0);
1729   }
1730   if (unordered_result &lt; 0) {
1731     // we want -1 for unordered or less than, 0 for equal and 1 for
1732     // greater than.
<span class="line-modified">1733     __ mov(r0, (u_int64_t)-1L);</span>
1734     // for FP LT tests less than or unordered
1735     __ br(Assembler::LT, done);
1736     // install 0 for EQ otherwise 1
1737     __ csinc(r0, zr, zr, Assembler::EQ);
1738   } else {
1739     // we want -1 for less than, 0 for equal and 1 for unordered or
1740     // greater than.
1741     __ mov(r0, 1L);
1742     // for FP HI tests greater than or unordered
1743     __ br(Assembler::HI, done);
1744     // install 0 for EQ otherwise ~0
1745     __ csinv(r0, zr, zr, Assembler::EQ);
1746 
1747   }
1748   __ bind(done);
1749 }
1750 
1751 void TemplateTable::branch(bool is_jsr, bool is_wide)
1752 {
1753   // We might be moving to a safepoint.  The thread which calls
</pre>
<hr />
<pre>
2958     case Bytecodes::_fast_dputfield: __ pop_d(); break;
2959     case Bytecodes::_fast_fputfield: __ pop_f(); break;
2960     case Bytecodes::_fast_lputfield: __ pop_l(r0); break;
2961     default: break;
2962     }
2963     __ bind(L2);
2964   }
2965 }
2966 
2967 void TemplateTable::fast_storefield(TosState state)
2968 {
2969   transition(state, vtos);
2970 
2971   ByteSize base = ConstantPoolCache::base_offset();
2972 
2973   jvmti_post_fast_field_mod();
2974 
2975   // access constant pool cache
2976   __ get_cache_and_index_at_bcp(r2, r1, 1);
2977 



2978   // test for volatile with r3
2979   __ ldrw(r3, Address(r2, in_bytes(base +
2980                                    ConstantPoolCacheEntry::flags_offset())));
2981 
2982   // replace index with field offset from cache entry
2983   __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));
2984 
2985   {
2986     Label notVolatile;
2987     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2988     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
2989     __ bind(notVolatile);
2990   }
2991 
2992   Label notVolatile;
2993 
2994   // Get object from stack
2995   pop_and_check_object(r2);
2996 
2997   // field address
</pre>
<hr />
<pre>
3050     __ lea(rscratch1, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
3051     __ ldrw(r2, Address(rscratch1));
3052     __ cbzw(r2, L1);
3053     // access constant pool cache entry
3054     __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);
3055     __ verify_oop(r0);
3056     __ push_ptr(r0);  // save object pointer before call_VM() clobbers it
3057     __ mov(c_rarg1, r0);
3058     // c_rarg1: object pointer copied above
3059     // c_rarg2: cache entry pointer
3060     __ call_VM(noreg,
3061                CAST_FROM_FN_PTR(address,
3062                                 InterpreterRuntime::post_field_access),
3063                c_rarg1, c_rarg2);
3064     __ pop_ptr(r0); // restore object pointer
3065     __ bind(L1);
3066   }
3067 
3068   // access constant pool cache
3069   __ get_cache_and_index_at_bcp(r2, r1, 1);




3070   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3071                                   ConstantPoolCacheEntry::f2_offset())));
3072   __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3073                                    ConstantPoolCacheEntry::flags_offset())));
3074 
3075   // r0: object
3076   __ verify_oop(r0);
3077   __ null_check(r0);
3078   const Address field(r0, r1);
3079 
3080   // 8179954: We need to make sure that the code generated for
3081   // volatile accesses forms a sequentially-consistent set of
3082   // operations when combined with STLR and LDAR.  Without a leading
3083   // membar it&#39;s possible for a simple Dekker test to fail if loads
3084   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3085   // the stores in one method and we interpret the loads in another.
3086   if (!is_c1_or_interpreter_only()) {
3087     Label notVolatile;
3088     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3089     __ membar(MacroAssembler::AnyAny);
</pre>
</td>
<td>
<hr />
<pre>
1689     __ get_fpsr(r1);
1690     __ cbzw(r1, L_Okay);
1691     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l));
1692     __ bind(L_Okay);
1693   }
1694     break;
1695   case Bytecodes::_d2f:
1696     __ fcvtd(v0, v0);
1697     break;
1698   default:
1699     ShouldNotReachHere();
1700   }
1701 }
1702 
1703 void TemplateTable::lcmp()
1704 {
1705   transition(ltos, itos);
1706   Label done;
1707   __ pop_l(r1);
1708   __ cmp(r1, r0);
<span class="line-modified">1709   __ mov(r0, (uint64_t)-1L);</span>
1710   __ br(Assembler::LT, done);
1711   // __ mov(r0, 1UL);
1712   // __ csel(r0, r0, zr, Assembler::NE);
1713   // and here is a faster way
1714   __ csinc(r0, zr, zr, Assembler::EQ);
1715   __ bind(done);
1716 }
1717 
1718 void TemplateTable::float_cmp(bool is_float, int unordered_result)
1719 {
1720   Label done;
1721   if (is_float) {
1722     // XXX get rid of pop here, use ... reg, mem32
1723     __ pop_f(v1);
1724     __ fcmps(v1, v0);
1725   } else {
1726     // XXX get rid of pop here, use ... reg, mem64
1727     __ pop_d(v1);
1728     __ fcmpd(v1, v0);
1729   }
1730   if (unordered_result &lt; 0) {
1731     // we want -1 for unordered or less than, 0 for equal and 1 for
1732     // greater than.
<span class="line-modified">1733     __ mov(r0, (uint64_t)-1L);</span>
1734     // for FP LT tests less than or unordered
1735     __ br(Assembler::LT, done);
1736     // install 0 for EQ otherwise 1
1737     __ csinc(r0, zr, zr, Assembler::EQ);
1738   } else {
1739     // we want -1 for less than, 0 for equal and 1 for unordered or
1740     // greater than.
1741     __ mov(r0, 1L);
1742     // for FP HI tests greater than or unordered
1743     __ br(Assembler::HI, done);
1744     // install 0 for EQ otherwise ~0
1745     __ csinv(r0, zr, zr, Assembler::EQ);
1746 
1747   }
1748   __ bind(done);
1749 }
1750 
1751 void TemplateTable::branch(bool is_jsr, bool is_wide)
1752 {
1753   // We might be moving to a safepoint.  The thread which calls
</pre>
<hr />
<pre>
2958     case Bytecodes::_fast_dputfield: __ pop_d(); break;
2959     case Bytecodes::_fast_fputfield: __ pop_f(); break;
2960     case Bytecodes::_fast_lputfield: __ pop_l(r0); break;
2961     default: break;
2962     }
2963     __ bind(L2);
2964   }
2965 }
2966 
2967 void TemplateTable::fast_storefield(TosState state)
2968 {
2969   transition(state, vtos);
2970 
2971   ByteSize base = ConstantPoolCache::base_offset();
2972 
2973   jvmti_post_fast_field_mod();
2974 
2975   // access constant pool cache
2976   __ get_cache_and_index_at_bcp(r2, r1, 1);
2977 
<span class="line-added">2978   // Must prevent reordering of the following cp cache loads with bytecode load</span>
<span class="line-added">2979   __ membar(MacroAssembler::LoadLoad);</span>
<span class="line-added">2980 </span>
2981   // test for volatile with r3
2982   __ ldrw(r3, Address(r2, in_bytes(base +
2983                                    ConstantPoolCacheEntry::flags_offset())));
2984 
2985   // replace index with field offset from cache entry
2986   __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));
2987 
2988   {
2989     Label notVolatile;
2990     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2991     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
2992     __ bind(notVolatile);
2993   }
2994 
2995   Label notVolatile;
2996 
2997   // Get object from stack
2998   pop_and_check_object(r2);
2999 
3000   // field address
</pre>
<hr />
<pre>
3053     __ lea(rscratch1, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
3054     __ ldrw(r2, Address(rscratch1));
3055     __ cbzw(r2, L1);
3056     // access constant pool cache entry
3057     __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);
3058     __ verify_oop(r0);
3059     __ push_ptr(r0);  // save object pointer before call_VM() clobbers it
3060     __ mov(c_rarg1, r0);
3061     // c_rarg1: object pointer copied above
3062     // c_rarg2: cache entry pointer
3063     __ call_VM(noreg,
3064                CAST_FROM_FN_PTR(address,
3065                                 InterpreterRuntime::post_field_access),
3066                c_rarg1, c_rarg2);
3067     __ pop_ptr(r0); // restore object pointer
3068     __ bind(L1);
3069   }
3070 
3071   // access constant pool cache
3072   __ get_cache_and_index_at_bcp(r2, r1, 1);
<span class="line-added">3073 </span>
<span class="line-added">3074   // Must prevent reordering of the following cp cache loads with bytecode load</span>
<span class="line-added">3075   __ membar(MacroAssembler::LoadLoad);</span>
<span class="line-added">3076 </span>
3077   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3078                                   ConstantPoolCacheEntry::f2_offset())));
3079   __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3080                                    ConstantPoolCacheEntry::flags_offset())));
3081 
3082   // r0: object
3083   __ verify_oop(r0);
3084   __ null_check(r0);
3085   const Address field(r0, r1);
3086 
3087   // 8179954: We need to make sure that the code generated for
3088   // volatile accesses forms a sequentially-consistent set of
3089   // operations when combined with STLR and LDAR.  Without a leading
3090   // membar it&#39;s possible for a simple Dekker test to fail if loads
3091   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3092   // the stores in one method and we interpret the loads in another.
3093   if (!is_c1_or_interpreter_only()) {
3094     Label notVolatile;
3095     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3096     __ membar(MacroAssembler::AnyAny);
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../arm/c1_CodeStubs_arm.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>