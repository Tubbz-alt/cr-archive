<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="immediate_aarch64.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  76   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  77     // Load register (literal)
  78     Instruction_aarch64::spatch(branch, 23, 5, offset);
  79   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  80     // Unconditional branch (immediate)
  81     Instruction_aarch64::spatch(branch, 25, 0, offset);
  82   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  83     // Conditional branch (immediate)
  84     Instruction_aarch64::spatch(branch, 23, 5, offset);
  85   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  86     // Compare &amp; branch (immediate)
  87     Instruction_aarch64::spatch(branch, 23, 5, offset);
  88   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  89     // Test &amp; branch (immediate)
  90     Instruction_aarch64::spatch(branch, 18, 5, offset);
  91   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  92     // PC-rel. addressing
  93     offset = target-branch;
  94     int shift = Instruction_aarch64::extract(insn, 31, 31);
  95     if (shift) {
<span class="line-modified">  96       u_int64_t dest = (u_int64_t)target;</span>
  97       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  98       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
  99       unsigned offset_lo = dest &amp; 0xfff;
 100       offset = adr_page - pc_page;
 101 
 102       // We handle 4 types of PC relative addressing
 103       //   1 - adrp    Rx, target_page
 104       //       ldr/str Ry, [Rx, #offset_in_page]
 105       //   2 - adrp    Rx, target_page
 106       //       add     Ry, Rx, #offset_in_page
 107       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 108       //       movk    Rx, #imm16&lt;&lt;32
 109       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 110       // In the first 3 cases we must check that Rx is the same in the adrp and the
 111       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 112       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 113       // to be followed by a random unrelated ldr/str, add or movk instruction.
 114       //
 115       unsigned insn2 = ((unsigned*)branch)[1];
 116       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
</pre>
<hr />
<pre>
 129         Instruction_aarch64::patch(branch + sizeof (unsigned),
 130                                    21, 10, offset_lo);
 131         instructions = 2;
 132       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 133                    Instruction_aarch64::extract(insn, 4, 0) ==
 134                      Instruction_aarch64::extract(insn2, 4, 0)) {
 135         // movk #imm16&lt;&lt;32
 136         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 137         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);
 138         long pc_page = (long)branch &gt;&gt; 12;
 139         long adr_page = (long)dest &gt;&gt; 12;
 140         offset = adr_page - pc_page;
 141         instructions = 2;
 142       }
 143     }
 144     int offset_lo = offset &amp; 3;
 145     offset &gt;&gt;= 2;
 146     Instruction_aarch64::spatch(branch, 23, 5, offset);
 147     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 148   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
<span class="line-modified"> 149     u_int64_t dest = (u_int64_t)target;</span>
 150     // Move wide constant
 151     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 152     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 154     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 157     instructions = 3;
 158   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 159              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 160     // nothing to do
 161     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 162   } else {
 163     ShouldNotReachHere();
 164   }
 165   return instructions * NativeInstruction::instruction_size;
 166 }
 167 
 168 int MacroAssembler::patch_oop(address insn_addr, address o) {
 169   int instructions;
</pre>
<hr />
<pre>
 255         return address(target_page + (byte_offset &lt;&lt; size));
 256       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 257                 Instruction_aarch64::extract(insn, 4, 0) ==
 258                         Instruction_aarch64::extract(insn2, 4, 0)) {
 259         // add (immediate)
 260         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 261         return address(target_page + byte_offset);
 262       } else {
 263         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 264                Instruction_aarch64::extract(insn, 4, 0) ==
 265                  Instruction_aarch64::extract(insn2, 4, 0)) {
 266           target_page = (target_page &amp; 0xffffffff) |
 267                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 268         }
 269         return (address)target_page;
 270       }
 271     } else {
 272       ShouldNotReachHere();
 273     }
 274   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
<span class="line-modified"> 275     u_int32_t *insns = (u_int32_t *)insn_addr;</span>
 276     // Move wide constant: movz, movk, movk.  See movptr().
 277     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 278     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
<span class="line-modified"> 279     return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))</span>
<span class="line-modified"> 280                    + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)</span>
<span class="line-modified"> 281                    + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));</span>
 282   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 283              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 284     return 0;
 285   } else {
 286     ShouldNotReachHere();
 287   }
 288   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 289 }
 290 
 291 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 292   ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 293   tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 294 }
 295 
 296 // Just like safepoint_poll, but use an acquiring load for thread-
 297 // local polling.
 298 //
 299 // We need an acquire here to ensure that any subsequent load of the
 300 // global SafepointSynchronize::_state flag is ordered after this load
 301 // of the local Thread::_polling page.  We don&#39;t want this poll to
</pre>
<hr />
<pre>
1474   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1475 }
1476 
1477 void MacroAssembler::null_check(Register reg, int offset) {
1478   if (needs_explicit_null_check(offset)) {
1479     // provoke OS NULL exception if reg = NULL by
1480     // accessing M[reg] w/o changing any registers
1481     // NOTE: this is plenty to provoke a segv
1482     ldr(zr, Address(reg));
1483   } else {
1484     // nothing to do, (later) access of M[reg + offset]
1485     // will provoke OS NULL exception if reg = NULL
1486   }
1487 }
1488 
1489 // MacroAssembler protected routines needed to implement
1490 // public methods
1491 
1492 void MacroAssembler::mov(Register r, Address dest) {
1493   code_section()-&gt;relocate(pc(), dest.rspec());
<span class="line-modified">1494   u_int64_t imm64 = (u_int64_t)dest.target();</span>
1495   movptr(r, imm64);
1496 }
1497 
1498 // Move a constant pointer into r.  In AArch64 mode the virtual
1499 // address space is 48 bits in size, so we only need three
1500 // instructions to create a patchable instruction sequence that can
1501 // reach anywhere.
1502 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1503 #ifndef PRODUCT
1504   {
1505     char buffer[64];
1506     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1507     block_comment(buffer);
1508   }
1509 #endif
1510   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1511   movz(r, imm64 &amp; 0xffff);
1512   imm64 &gt;&gt;= 16;
1513   movk(r, imm64 &amp; 0xffff, 16);
1514   imm64 &gt;&gt;= 16;
1515   movk(r, imm64 &amp; 0xffff, 32);
1516 }
1517 
1518 // Macro to mov replicated immediate to vector register.
1519 //  Vd will get the following values for different arrangements in T
1520 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1521 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1522 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1523 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1524 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1525 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1526 //   T1D/T2D: invalid
<span class="line-modified">1527 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {</span>
1528   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1529   if (T == T8B || T == T16B) {
1530     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1531     movi(Vd, T, imm32 &amp; 0xff, 0);
1532     return;
1533   }
<span class="line-modified">1534   u_int32_t nimm32 = ~imm32;</span>
1535   if (T == T4H || T == T8H) {
1536     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1537     imm32 &amp;= 0xffff;
1538     nimm32 &amp;= 0xffff;
1539   }
<span class="line-modified">1540   u_int32_t x = imm32;</span>
1541   int movi_cnt = 0;
1542   int movn_cnt = 0;
1543   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1544   x = nimm32;
1545   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1546   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1547   unsigned lsl = 0;
1548   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1549   if (movn_cnt &lt; movi_cnt)
1550     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1551   else
1552     movi(Vd, T, imm32 &amp; 0xff, lsl);
1553   imm32 &gt;&gt;= 8; lsl += 8;
1554   while (imm32) {
1555     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1556     if (movn_cnt &lt; movi_cnt)
1557       bici(Vd, T, imm32 &amp; 0xff, lsl);
1558     else
1559       orri(Vd, T, imm32 &amp; 0xff, lsl);
1560     lsl += 8; imm32 &gt;&gt;= 8;
1561   }
1562 }
1563 
<span class="line-modified">1564 void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)</span>
1565 {
1566 #ifndef PRODUCT
1567   {
1568     char buffer[64];
1569     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1570     block_comment(buffer);
1571   }
1572 #endif
1573   if (operand_valid_for_logical_immediate(false, imm64)) {
1574     orr(dst, zr, imm64);
1575   } else {
1576     // we can use a combination of MOVZ or MOVN with
1577     // MOVK to build up the constant
<span class="line-modified">1578     u_int64_t imm_h[4];</span>
1579     int zero_count = 0;
1580     int neg_count = 0;
1581     int i;
1582     for (i = 0; i &lt; 4; i++) {
1583       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1584       if (imm_h[i] == 0) {
1585         zero_count++;
1586       } else if (imm_h[i] == 0xffffL) {
1587         neg_count++;
1588       }
1589     }
1590     if (zero_count == 4) {
1591       // one MOVZ will do
1592       movz(dst, 0);
1593     } else if (neg_count == 4) {
1594       // one MOVN will do
1595       movn(dst, 0);
1596     } else if (zero_count == 3) {
1597       for (i = 0; i &lt; 4; i++) {
1598         if (imm_h[i] != 0L) {
<span class="line-modified">1599           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1600           break;
1601         }
1602       }
1603     } else if (neg_count == 3) {
1604       // one MOVN will do
1605       for (int i = 0; i &lt; 4; i++) {
1606         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1607           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1608           break;
1609         }
1610       }
1611     } else if (zero_count == 2) {
1612       // one MOVZ and one MOVK will do
1613       for (i = 0; i &lt; 3; i++) {
1614         if (imm_h[i] != 0L) {
<span class="line-modified">1615           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1616           i++;
1617           break;
1618         }
1619       }
1620       for (;i &lt; 4; i++) {
1621         if (imm_h[i] != 0L) {
<span class="line-modified">1622           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1623         }
1624       }
1625     } else if (neg_count == 2) {
1626       // one MOVN and one MOVK will do
1627       for (i = 0; i &lt; 4; i++) {
1628         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1629           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1630           i++;
1631           break;
1632         }
1633       }
1634       for (;i &lt; 4; i++) {
1635         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1636           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1637         }
1638       }
1639     } else if (zero_count == 1) {
1640       // one MOVZ and two MOVKs will do
1641       for (i = 0; i &lt; 4; i++) {
1642         if (imm_h[i] != 0L) {
<span class="line-modified">1643           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1644           i++;
1645           break;
1646         }
1647       }
1648       for (;i &lt; 4; i++) {
1649         if (imm_h[i] != 0x0L) {
<span class="line-modified">1650           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1651         }
1652       }
1653     } else if (neg_count == 1) {
1654       // one MOVN and two MOVKs will do
1655       for (i = 0; i &lt; 4; i++) {
1656         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1657           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1658           i++;
1659           break;
1660         }
1661       }
1662       for (;i &lt; 4; i++) {
1663         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1664           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1665         }
1666       }
1667     } else {
1668       // use a MOVZ and 3 MOVKs (makes it easier to debug)
<span class="line-modified">1669       movz(dst, (u_int32_t)imm_h[0], 0);</span>
1670       for (i = 1; i &lt; 4; i++) {
<span class="line-modified">1671         movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1672       }
1673     }
1674   }
1675 }
1676 
<span class="line-modified">1677 void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)</span>
1678 {
1679 #ifndef PRODUCT
1680     {
1681       char buffer[64];
1682       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1683       block_comment(buffer);
1684     }
1685 #endif
1686   if (operand_valid_for_logical_immediate(true, imm32)) {
1687     orrw(dst, zr, imm32);
1688   } else {
1689     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1690     // constant
<span class="line-modified">1691     u_int32_t imm_h[2];</span>
1692     imm_h[0] = imm32 &amp; 0xffff;
1693     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1694     if (imm_h[0] == 0) {
1695       movzw(dst, imm_h[1], 16);
1696     } else if (imm_h[0] == 0xffff) {
1697       movnw(dst, imm_h[1] ^ 0xffff, 16);
1698     } else if (imm_h[1] == 0) {
1699       movzw(dst, imm_h[0], 0);
1700     } else if (imm_h[1] == 0xffff) {
1701       movnw(dst, imm_h[0] ^ 0xffff, 0);
1702     } else {
1703       // use a MOVZ and MOVK (makes it easier to debug)
1704       movzw(dst, imm_h[0], 0);
1705       movkw(dst, imm_h[1], 16);
1706     }
1707   }
1708 }
1709 
1710 // Form an address from base + offset in Rd.  Rd may or may
1711 // not actually be used: you must use the Address that is returned.
</pre>
<hr />
<pre>
4818   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
4819     Label l;
4820     tbz(cnt, exact_log2(i), l);
4821     for (int j = 0; j &lt; i; j += 2) {
4822       stp(zr, zr, post(ptr, 16));
4823     }
4824     bind(l);
4825   }
4826   {
4827     Label l;
4828     tbz(cnt, 0, l);
4829     str(zr, Address(ptr));
4830     bind(l);
4831   }
4832   BLOCK_COMMENT(&quot;} zero_words&quot;);
4833 }
4834 
4835 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
4836 // cnt:          Immediate count in HeapWords.
4837 #define SmallArraySize (18 * BytesPerLong)
<span class="line-modified">4838 void MacroAssembler::zero_words(Register base, u_int64_t cnt)</span>
4839 {
4840   BLOCK_COMMENT(&quot;zero_words {&quot;);
4841   int i = cnt &amp; 1;  // store any odd word to start
4842   if (i) str(zr, Address(base));
4843 
4844   if (cnt &lt;= SmallArraySize / BytesPerLong) {
4845     for (; i &lt; (int)cnt; i += 2)
4846       stp(zr, zr, Address(base, i * wordSize));
4847   } else {
4848     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
4849     int remainder = cnt % (2 * unroll);
4850     for (; i &lt; remainder; i += 2)
4851       stp(zr, zr, Address(base, i * wordSize));
4852 
4853     Label loop;
4854     Register cnt_reg = rscratch1;
4855     Register loop_base = rscratch2;
4856     cnt = cnt - remainder;
4857     mov(cnt_reg, cnt);
4858     // adjust base and prebias by -2 * wordSize so we can pre-increment
</pre>
</td>
<td>
<hr />
<pre>
  76   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  77     // Load register (literal)
  78     Instruction_aarch64::spatch(branch, 23, 5, offset);
  79   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  80     // Unconditional branch (immediate)
  81     Instruction_aarch64::spatch(branch, 25, 0, offset);
  82   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  83     // Conditional branch (immediate)
  84     Instruction_aarch64::spatch(branch, 23, 5, offset);
  85   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  86     // Compare &amp; branch (immediate)
  87     Instruction_aarch64::spatch(branch, 23, 5, offset);
  88   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  89     // Test &amp; branch (immediate)
  90     Instruction_aarch64::spatch(branch, 18, 5, offset);
  91   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  92     // PC-rel. addressing
  93     offset = target-branch;
  94     int shift = Instruction_aarch64::extract(insn, 31, 31);
  95     if (shift) {
<span class="line-modified">  96       uint64_t dest = (uint64_t)target;</span>
  97       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  98       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
  99       unsigned offset_lo = dest &amp; 0xfff;
 100       offset = adr_page - pc_page;
 101 
 102       // We handle 4 types of PC relative addressing
 103       //   1 - adrp    Rx, target_page
 104       //       ldr/str Ry, [Rx, #offset_in_page]
 105       //   2 - adrp    Rx, target_page
 106       //       add     Ry, Rx, #offset_in_page
 107       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 108       //       movk    Rx, #imm16&lt;&lt;32
 109       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 110       // In the first 3 cases we must check that Rx is the same in the adrp and the
 111       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 112       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 113       // to be followed by a random unrelated ldr/str, add or movk instruction.
 114       //
 115       unsigned insn2 = ((unsigned*)branch)[1];
 116       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
</pre>
<hr />
<pre>
 129         Instruction_aarch64::patch(branch + sizeof (unsigned),
 130                                    21, 10, offset_lo);
 131         instructions = 2;
 132       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 133                    Instruction_aarch64::extract(insn, 4, 0) ==
 134                      Instruction_aarch64::extract(insn2, 4, 0)) {
 135         // movk #imm16&lt;&lt;32
 136         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 137         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);
 138         long pc_page = (long)branch &gt;&gt; 12;
 139         long adr_page = (long)dest &gt;&gt; 12;
 140         offset = adr_page - pc_page;
 141         instructions = 2;
 142       }
 143     }
 144     int offset_lo = offset &amp; 3;
 145     offset &gt;&gt;= 2;
 146     Instruction_aarch64::spatch(branch, 23, 5, offset);
 147     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 148   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
<span class="line-modified"> 149     uint64_t dest = (uint64_t)target;</span>
 150     // Move wide constant
 151     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 152     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 154     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 157     instructions = 3;
 158   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 159              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 160     // nothing to do
 161     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 162   } else {
 163     ShouldNotReachHere();
 164   }
 165   return instructions * NativeInstruction::instruction_size;
 166 }
 167 
 168 int MacroAssembler::patch_oop(address insn_addr, address o) {
 169   int instructions;
</pre>
<hr />
<pre>
 255         return address(target_page + (byte_offset &lt;&lt; size));
 256       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 257                 Instruction_aarch64::extract(insn, 4, 0) ==
 258                         Instruction_aarch64::extract(insn2, 4, 0)) {
 259         // add (immediate)
 260         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 261         return address(target_page + byte_offset);
 262       } else {
 263         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 264                Instruction_aarch64::extract(insn, 4, 0) ==
 265                  Instruction_aarch64::extract(insn2, 4, 0)) {
 266           target_page = (target_page &amp; 0xffffffff) |
 267                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 268         }
 269         return (address)target_page;
 270       }
 271     } else {
 272       ShouldNotReachHere();
 273     }
 274   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
<span class="line-modified"> 275     uint32_t *insns = (uint32_t *)insn_addr;</span>
 276     // Move wide constant: movz, movk, movk.  See movptr().
 277     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 278     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
<span class="line-modified"> 279     return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))</span>
<span class="line-modified"> 280                    + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)</span>
<span class="line-modified"> 281                    + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));</span>
 282   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 283              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 284     return 0;
 285   } else {
 286     ShouldNotReachHere();
 287   }
 288   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 289 }
 290 
 291 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 292   ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 293   tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 294 }
 295 
 296 // Just like safepoint_poll, but use an acquiring load for thread-
 297 // local polling.
 298 //
 299 // We need an acquire here to ensure that any subsequent load of the
 300 // global SafepointSynchronize::_state flag is ordered after this load
 301 // of the local Thread::_polling page.  We don&#39;t want this poll to
</pre>
<hr />
<pre>
1474   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1475 }
1476 
1477 void MacroAssembler::null_check(Register reg, int offset) {
1478   if (needs_explicit_null_check(offset)) {
1479     // provoke OS NULL exception if reg = NULL by
1480     // accessing M[reg] w/o changing any registers
1481     // NOTE: this is plenty to provoke a segv
1482     ldr(zr, Address(reg));
1483   } else {
1484     // nothing to do, (later) access of M[reg + offset]
1485     // will provoke OS NULL exception if reg = NULL
1486   }
1487 }
1488 
1489 // MacroAssembler protected routines needed to implement
1490 // public methods
1491 
1492 void MacroAssembler::mov(Register r, Address dest) {
1493   code_section()-&gt;relocate(pc(), dest.rspec());
<span class="line-modified">1494   uint64_t imm64 = (uint64_t)dest.target();</span>
1495   movptr(r, imm64);
1496 }
1497 
1498 // Move a constant pointer into r.  In AArch64 mode the virtual
1499 // address space is 48 bits in size, so we only need three
1500 // instructions to create a patchable instruction sequence that can
1501 // reach anywhere.
1502 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1503 #ifndef PRODUCT
1504   {
1505     char buffer[64];
1506     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1507     block_comment(buffer);
1508   }
1509 #endif
1510   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1511   movz(r, imm64 &amp; 0xffff);
1512   imm64 &gt;&gt;= 16;
1513   movk(r, imm64 &amp; 0xffff, 16);
1514   imm64 &gt;&gt;= 16;
1515   movk(r, imm64 &amp; 0xffff, 32);
1516 }
1517 
1518 // Macro to mov replicated immediate to vector register.
1519 //  Vd will get the following values for different arrangements in T
1520 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1521 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1522 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1523 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1524 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1525 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1526 //   T1D/T2D: invalid
<span class="line-modified">1527 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {</span>
1528   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1529   if (T == T8B || T == T16B) {
1530     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1531     movi(Vd, T, imm32 &amp; 0xff, 0);
1532     return;
1533   }
<span class="line-modified">1534   uint32_t nimm32 = ~imm32;</span>
1535   if (T == T4H || T == T8H) {
1536     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1537     imm32 &amp;= 0xffff;
1538     nimm32 &amp;= 0xffff;
1539   }
<span class="line-modified">1540   uint32_t x = imm32;</span>
1541   int movi_cnt = 0;
1542   int movn_cnt = 0;
1543   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1544   x = nimm32;
1545   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1546   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1547   unsigned lsl = 0;
1548   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1549   if (movn_cnt &lt; movi_cnt)
1550     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1551   else
1552     movi(Vd, T, imm32 &amp; 0xff, lsl);
1553   imm32 &gt;&gt;= 8; lsl += 8;
1554   while (imm32) {
1555     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1556     if (movn_cnt &lt; movi_cnt)
1557       bici(Vd, T, imm32 &amp; 0xff, lsl);
1558     else
1559       orri(Vd, T, imm32 &amp; 0xff, lsl);
1560     lsl += 8; imm32 &gt;&gt;= 8;
1561   }
1562 }
1563 
<span class="line-modified">1564 void MacroAssembler::mov_immediate64(Register dst, uint64_t imm64)</span>
1565 {
1566 #ifndef PRODUCT
1567   {
1568     char buffer[64];
1569     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1570     block_comment(buffer);
1571   }
1572 #endif
1573   if (operand_valid_for_logical_immediate(false, imm64)) {
1574     orr(dst, zr, imm64);
1575   } else {
1576     // we can use a combination of MOVZ or MOVN with
1577     // MOVK to build up the constant
<span class="line-modified">1578     uint64_t imm_h[4];</span>
1579     int zero_count = 0;
1580     int neg_count = 0;
1581     int i;
1582     for (i = 0; i &lt; 4; i++) {
1583       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1584       if (imm_h[i] == 0) {
1585         zero_count++;
1586       } else if (imm_h[i] == 0xffffL) {
1587         neg_count++;
1588       }
1589     }
1590     if (zero_count == 4) {
1591       // one MOVZ will do
1592       movz(dst, 0);
1593     } else if (neg_count == 4) {
1594       // one MOVN will do
1595       movn(dst, 0);
1596     } else if (zero_count == 3) {
1597       for (i = 0; i &lt; 4; i++) {
1598         if (imm_h[i] != 0L) {
<span class="line-modified">1599           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1600           break;
1601         }
1602       }
1603     } else if (neg_count == 3) {
1604       // one MOVN will do
1605       for (int i = 0; i &lt; 4; i++) {
1606         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1607           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1608           break;
1609         }
1610       }
1611     } else if (zero_count == 2) {
1612       // one MOVZ and one MOVK will do
1613       for (i = 0; i &lt; 3; i++) {
1614         if (imm_h[i] != 0L) {
<span class="line-modified">1615           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1616           i++;
1617           break;
1618         }
1619       }
1620       for (;i &lt; 4; i++) {
1621         if (imm_h[i] != 0L) {
<span class="line-modified">1622           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1623         }
1624       }
1625     } else if (neg_count == 2) {
1626       // one MOVN and one MOVK will do
1627       for (i = 0; i &lt; 4; i++) {
1628         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1629           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1630           i++;
1631           break;
1632         }
1633       }
1634       for (;i &lt; 4; i++) {
1635         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1636           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1637         }
1638       }
1639     } else if (zero_count == 1) {
1640       // one MOVZ and two MOVKs will do
1641       for (i = 0; i &lt; 4; i++) {
1642         if (imm_h[i] != 0L) {
<span class="line-modified">1643           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1644           i++;
1645           break;
1646         }
1647       }
1648       for (;i &lt; 4; i++) {
1649         if (imm_h[i] != 0x0L) {
<span class="line-modified">1650           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1651         }
1652       }
1653     } else if (neg_count == 1) {
1654       // one MOVN and two MOVKs will do
1655       for (i = 0; i &lt; 4; i++) {
1656         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1657           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1658           i++;
1659           break;
1660         }
1661       }
1662       for (;i &lt; 4; i++) {
1663         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1664           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1665         }
1666       }
1667     } else {
1668       // use a MOVZ and 3 MOVKs (makes it easier to debug)
<span class="line-modified">1669       movz(dst, (uint32_t)imm_h[0], 0);</span>
1670       for (i = 1; i &lt; 4; i++) {
<span class="line-modified">1671         movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1672       }
1673     }
1674   }
1675 }
1676 
<span class="line-modified">1677 void MacroAssembler::mov_immediate32(Register dst, uint32_t imm32)</span>
1678 {
1679 #ifndef PRODUCT
1680     {
1681       char buffer[64];
1682       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1683       block_comment(buffer);
1684     }
1685 #endif
1686   if (operand_valid_for_logical_immediate(true, imm32)) {
1687     orrw(dst, zr, imm32);
1688   } else {
1689     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1690     // constant
<span class="line-modified">1691     uint32_t imm_h[2];</span>
1692     imm_h[0] = imm32 &amp; 0xffff;
1693     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1694     if (imm_h[0] == 0) {
1695       movzw(dst, imm_h[1], 16);
1696     } else if (imm_h[0] == 0xffff) {
1697       movnw(dst, imm_h[1] ^ 0xffff, 16);
1698     } else if (imm_h[1] == 0) {
1699       movzw(dst, imm_h[0], 0);
1700     } else if (imm_h[1] == 0xffff) {
1701       movnw(dst, imm_h[0] ^ 0xffff, 0);
1702     } else {
1703       // use a MOVZ and MOVK (makes it easier to debug)
1704       movzw(dst, imm_h[0], 0);
1705       movkw(dst, imm_h[1], 16);
1706     }
1707   }
1708 }
1709 
1710 // Form an address from base + offset in Rd.  Rd may or may
1711 // not actually be used: you must use the Address that is returned.
</pre>
<hr />
<pre>
4818   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
4819     Label l;
4820     tbz(cnt, exact_log2(i), l);
4821     for (int j = 0; j &lt; i; j += 2) {
4822       stp(zr, zr, post(ptr, 16));
4823     }
4824     bind(l);
4825   }
4826   {
4827     Label l;
4828     tbz(cnt, 0, l);
4829     str(zr, Address(ptr));
4830     bind(l);
4831   }
4832   BLOCK_COMMENT(&quot;} zero_words&quot;);
4833 }
4834 
4835 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
4836 // cnt:          Immediate count in HeapWords.
4837 #define SmallArraySize (18 * BytesPerLong)
<span class="line-modified">4838 void MacroAssembler::zero_words(Register base, uint64_t cnt)</span>
4839 {
4840   BLOCK_COMMENT(&quot;zero_words {&quot;);
4841   int i = cnt &amp; 1;  // store any odd word to start
4842   if (i) str(zr, Address(base));
4843 
4844   if (cnt &lt;= SmallArraySize / BytesPerLong) {
4845     for (; i &lt; (int)cnt; i += 2)
4846       stp(zr, zr, Address(base, i * wordSize));
4847   } else {
4848     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
4849     int remainder = cnt % (2 * unroll);
4850     for (; i &lt; remainder; i += 2)
4851       stp(zr, zr, Address(base, i * wordSize));
4852 
4853     Label loop;
4854     Register cnt_reg = rscratch1;
4855     Register loop_base = rscratch2;
4856     cnt = cnt - remainder;
4857     mov(cnt_reg, cnt);
4858     // adjust base and prebias by -2 * wordSize so we can pre-increment
</pre>
</td>
</tr>
</table>
<center><a href="immediate_aarch64.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>