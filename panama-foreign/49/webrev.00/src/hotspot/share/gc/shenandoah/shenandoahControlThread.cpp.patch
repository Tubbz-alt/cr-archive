diff a/src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp b/src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp
--- a/src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp
+++ b/src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp
@@ -176,41 +176,40 @@
     assert (!gc_requested || cause != GCCause::_last_gc_cause, "GC cause should be set");
 
     if (gc_requested) {
       heap->reset_bytes_allocated_since_gc_start();
 
+      // Use default constructor to snapshot the Metaspace state before GC.
+      metaspace::MetaspaceSizesSnapshot meta_sizes;
+
       // If GC was requested, we are sampling the counters even without actual triggers
       // from allocation machinery. This captures GC phases more accurately.
       set_forced_counters_update(true);
 
       // If GC was requested, we better dump freeset data for performance debugging
       {
         ShenandoahHeapLocker locker(heap->lock());
         heap->free_set()->log_status();
       }
-    }
 
-    switch (mode) {
-      case none:
-        break;
-      case concurrent_traversal:
-        service_concurrent_traversal_cycle(cause);
-        break;
-      case concurrent_normal:
-        service_concurrent_normal_cycle(cause);
-        break;
-      case stw_degenerated:
-        service_stw_degenerated_cycle(cause, degen_point);
-        break;
-      case stw_full:
-        service_stw_full_cycle(cause);
-        break;
-      default:
-        ShouldNotReachHere();
-    }
+      switch (mode) {
+        case concurrent_traversal:
+          service_concurrent_traversal_cycle(cause);
+          break;
+        case concurrent_normal:
+          service_concurrent_normal_cycle(cause);
+          break;
+        case stw_degenerated:
+          service_stw_degenerated_cycle(cause, degen_point);
+          break;
+        case stw_full:
+          service_stw_full_cycle(cause);
+          break;
+        default:
+          ShouldNotReachHere();
+      }
 
-    if (gc_requested) {
       // If this was the requested GC cycle, notify waiters about it
       if (explicit_gc_requested || implicit_gc_requested) {
         notify_gc_waiters();
       }
 
@@ -242,10 +241,13 @@
       // Clear metaspace oom flag, if current cycle unloaded classes
       if (heap->unload_classes()) {
         heuristics->clear_metaspace_oom();
       }
 
+      // Print Metaspace change following GC (if logging is enabled).
+      MetaspaceUtils::print_metaspace_change(meta_sizes);
+
       // GC is over, we are at idle now
       if (ShenandoahPacing) {
         heap->pacer()->setup_for_idle();
       }
     } else {
@@ -314,23 +316,22 @@
   // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
   // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
   // If second allocation failure happens during Degenerated GC cycle (for example, when GC
   // tries to evac something and no memory is available), cycle degrades to Full GC.
   //
-  // There are also two shortcuts through the normal cycle: a) immediate garbage shortcut, when
+  // There are also a shortcut through the normal cycle: immediate garbage shortcut, when
   // heuristics says there are no regions to compact, and all the collection comes from immediately
-  // reclaimable regions; b) coalesced UR shortcut, when heuristics decides to coalesce UR with the
-  // mark from the next cycle.
+  // reclaimable regions.
   //
   // ................................................................................................
   //
   //                                    (immediate garbage shortcut)                Concurrent GC
   //                             /-------------------------------------------\
-  //                             |                       (coalesced UR)      v
-  //                             |                  /----------------------->o
-  //                             |                  |                        |
-  //                             |                  |                        v
+  //                             |                                           |
+  //                             |                                           |
+  //                             |                                           |
+  //                             |                                           v
   // [START] ----> Conc Mark ----o----> Conc Evac --o--> Conc Update-Refs ---o----> [END]
   //                   |                    |                 |              ^
   //                   | (af)               | (af)            | (af)         |
   // ..................|....................|.................|..............|.......................
   //                   |                    |                 |              |
@@ -390,26 +391,19 @@
   if (heap->is_evacuation_in_progress()) {
     // Concurrently evacuate
     heap->entry_evac();
     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
 
-    // Perform update-refs phase, if required. This phase can be skipped if heuristics
-    // decides to piggy-back the update-refs on the next marking cycle. On either path,
-    // we need to turn off evacuation: either in init-update-refs, or in final-evac.
-    if (heap->heuristics()->should_start_update_refs()) {
-      heap->vmop_entry_init_updaterefs();
-      heap->entry_updaterefs();
-      if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;
-
-      heap->vmop_entry_final_updaterefs();
+    // Perform update-refs phase.
+    heap->vmop_entry_init_updaterefs();
+    heap->entry_updaterefs();
+    if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;
 
-      // Update references freed up collection set, kick the cleanup to reclaim the space.
-      heap->entry_cleanup();
+    heap->vmop_entry_final_updaterefs();
 
-    } else {
-      heap->vmop_entry_final_evac();
-    }
+    // Update references freed up collection set, kick the cleanup to reclaim the space.
+    heap->entry_cleanup();
   }
 
   // Cycle is complete
   heap->heuristics()->record_success_concurrent();
   heap->shenandoah_policy()->record_success_concurrent();
