<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/arm/c1_LIRAssembler_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;c1/c1_Compilation.hpp&quot;
  28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  30 #include &quot;c1/c1_Runtime1.hpp&quot;
  31 #include &quot;c1/c1_ValueStack.hpp&quot;
  32 #include &quot;ci/ciArrayKlass.hpp&quot;
  33 #include &quot;ci/ciInstance.hpp&quot;
  34 #include &quot;gc/shared/collectedHeap.hpp&quot;
  35 #include &quot;memory/universe.hpp&quot;
  36 #include &quot;nativeInst_arm.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
  40 #include &quot;utilities/powerOfTwo.hpp&quot;
  41 #include &quot;vmreg_arm.inline.hpp&quot;
  42 
  43 #define __ _masm-&gt;
  44 
  45 // Note: Rtemp usage is this file should not impact C2 and should be
  46 // correct as long as it is not implicitly used in lower layers (the
  47 // arm [macro]assembler) and used with care in the other C1 specific
  48 // files.
  49 
  50 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
  51   ShouldNotCallThis(); // Not used on ARM
  52   return false;
  53 }
  54 
  55 
  56 LIR_Opr LIR_Assembler::receiverOpr() {
  57   // The first register in Java calling conventions
  58   return FrameMap::R0_oop_opr;
  59 }
  60 
  61 LIR_Opr LIR_Assembler::osrBufferPointer() {
  62   return FrameMap::as_pointer_opr(R0);
  63 }
  64 
  65 #ifndef PRODUCT
  66 void LIR_Assembler::verify_reserved_argument_area_size(int args_count) {
  67   assert(args_count * wordSize &lt;= frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space for arguments&quot;);
  68 }
  69 #endif // !PRODUCT
  70 
  71 void LIR_Assembler::store_parameter(jint c, int offset_from_sp_in_words) {
  72   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  73   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  74   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  75   __ mov_slow(Rtemp, c);
  76   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  77 }
  78 
  79 void LIR_Assembler::store_parameter(Metadata* m, int offset_from_sp_in_words) {
  80   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  81   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  82   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  83   __ mov_metadata(Rtemp, m);
  84   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  85 }
  86 
  87 //--------------fpu register translations-----------------------
  88 
  89 
  90 void LIR_Assembler::breakpoint() {
  91   __ breakpoint();
  92 }
  93 
  94 void LIR_Assembler::push(LIR_Opr opr) {
  95   Unimplemented();
  96 }
  97 
  98 void LIR_Assembler::pop(LIR_Opr opr) {
  99   Unimplemented();
 100 }
 101 
 102 //-------------------------------------------
 103 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 104   Register base = addr-&gt;base()-&gt;as_pointer_register();
 105 
 106 
 107   if (addr-&gt;index()-&gt;is_illegal() || addr-&gt;index()-&gt;is_constant()) {
 108     int offset = addr-&gt;disp();
 109     if (addr-&gt;index()-&gt;is_constant()) {
 110       offset += addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() &lt;&lt; addr-&gt;scale();
 111     }
 112 
 113     if ((offset &lt;= -4096) || (offset &gt;= 4096)) {
 114       BAILOUT_(&quot;offset not in range&quot;, Address(base));
 115     }
 116 
 117     return Address(base, offset);
 118 
 119   } else {
 120     assert(addr-&gt;disp() == 0, &quot;can&#39;t have both&quot;);
 121     int scale = addr-&gt;scale();
 122 
 123     assert(addr-&gt;index()-&gt;is_single_cpu(), &quot;should be&quot;);
 124     return scale &gt;= 0 ? Address(base, addr-&gt;index()-&gt;as_register(), lsl, scale) :
 125                         Address(base, addr-&gt;index()-&gt;as_register(), lsr, -scale);
 126   }
 127 }
 128 
 129 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 130   Address base = as_Address(addr);
 131   assert(base.index() == noreg, &quot;must be&quot;);
 132   if (base.disp() + BytesPerWord &gt;= 4096) { BAILOUT_(&quot;offset not in range&quot;, Address(base.base(),0)); }
 133   return Address(base.base(), base.disp() + BytesPerWord);
 134 }
 135 
 136 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 137   return as_Address(addr);
 138 }
 139 
 140 
 141 void LIR_Assembler::osr_entry() {
 142   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 143   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 144   ValueStack* entry_state = osr_entry-&gt;end()-&gt;state();
 145   int number_of_locks = entry_state-&gt;locks_size();
 146 
 147   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 148   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 149 
 150   assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 151   int monitor_offset = (method()-&gt;max_locals() + 2 * (number_of_locks - 1)) * BytesPerWord;
 152   for (int i = 0; i &lt; number_of_locks; i++) {
 153     int slot_offset = monitor_offset - (i * 2 * BytesPerWord);
 154     __ ldr(R1, Address(OSR_buf, slot_offset + 0*BytesPerWord));
 155     __ ldr(R2, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 156     __ str(R1, frame_map()-&gt;address_for_monitor_lock(i));
 157     __ str(R2, frame_map()-&gt;address_for_monitor_object(i));
 158   }
 159 }
 160 
 161 
 162 int LIR_Assembler::check_icache() {
 163   Register receiver = LIR_Assembler::receiverOpr()-&gt;as_register();
 164   int offset = __ offset();
 165   __ inline_cache_check(receiver, Ricklass);
 166   return offset;
 167 }
 168 
 169 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 170   ShouldNotReachHere(); // not implemented
 171 }
 172 
 173 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 174   jobject o = (jobject)Universe::non_oop_word();
 175   int index = __ oop_recorder()-&gt;allocate_oop_index(o);
 176 
 177   PatchingStub* patch = new PatchingStub(_masm, patching_id(info), index);
 178 
 179   __ patchable_mov_oop(reg, o, index);
 180   patching_epilog(patch, lir_patch_normal, reg, info);
 181 }
 182 
 183 
 184 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 185   Metadata* o = (Metadata*)Universe::non_oop_word();
 186   int index = __ oop_recorder()-&gt;allocate_metadata_index(o);
 187   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id, index);
 188 
 189   __ patchable_mov_metadata(reg, o, index);
 190   patching_epilog(patch, lir_patch_normal, reg, info);
 191 }
 192 
 193 
 194 int LIR_Assembler::initial_frame_size_in_bytes() const {
 195   // Subtracts two words to account for return address and link
 196   return frame_map()-&gt;framesize()*VMRegImpl::stack_slot_size - 2*wordSize;
 197 }
 198 
 199 
 200 int LIR_Assembler::emit_exception_handler() {
 201   // TODO: ARM
 202   __ nop(); // See comments in other ports
 203 
 204   address handler_base = __ start_a_stub(exception_handler_size());
 205   if (handler_base == NULL) {
 206     bailout(&quot;exception handler overflow&quot;);
 207     return -1;
 208   }
 209 
 210   int offset = code_offset();
 211 
 212   // check that there is really an exception
 213   __ verify_not_null_oop(Rexception_obj);
 214 
 215   __ call(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id), relocInfo::runtime_call_type);
 216   __ should_not_reach_here();
 217 
 218   assert(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 219   __ end_a_stub();
 220 
 221   return offset;
 222 }
 223 
 224 // Emit the code to remove the frame from the stack in the exception
 225 // unwind path.
 226 int LIR_Assembler::emit_unwind_handler() {
 227 #ifndef PRODUCT
 228   if (CommentedAssembly) {
 229     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 230   }
 231 #endif
 232 
 233   int offset = code_offset();
 234 
 235   // Fetch the exception from TLS and clear out exception related thread state
 236   Register zero = __ zero_register(Rtemp);
 237   __ ldr(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
 238   __ str(zero, Address(Rthread, JavaThread::exception_oop_offset()));
 239   __ str(zero, Address(Rthread, JavaThread::exception_pc_offset()));
 240 
 241   __ bind(_unwind_handler_entry);
 242   __ verify_not_null_oop(Rexception_obj);
 243 
 244   // Preform needed unlocking
 245   MonitorExitStub* stub = NULL;
 246   if (method()-&gt;is_synchronized()) {
 247     monitor_address(0, FrameMap::R0_opr);
 248     stub = new MonitorExitStub(FrameMap::R0_opr, true, 0);
 249     __ unlock_object(R2, R1, R0, Rtemp, *stub-&gt;entry());
 250     __ bind(*stub-&gt;continuation());
 251   }
 252 
 253   // remove the activation and dispatch to the unwind handler
 254   __ remove_frame(initial_frame_size_in_bytes()); // restores FP and LR
 255   __ jump(Runtime1::entry_for(Runtime1::unwind_exception_id), relocInfo::runtime_call_type, Rtemp);
 256 
 257   // Emit the slow path assembly
 258   if (stub != NULL) {
 259     stub-&gt;emit_code(this);
 260   }
 261 
 262   return offset;
 263 }
 264 
 265 
 266 int LIR_Assembler::emit_deopt_handler() {
 267   address handler_base = __ start_a_stub(deopt_handler_size());
 268   if (handler_base == NULL) {
 269     bailout(&quot;deopt handler overflow&quot;);
 270     return -1;
 271   }
 272 
 273   int offset = code_offset();
 274 
 275   __ mov_relative_address(LR, __ pc());
 276   __ push(LR); // stub expects LR to be saved
 277   __ jump(SharedRuntime::deopt_blob()-&gt;unpack(), relocInfo::runtime_call_type, noreg);
 278 
 279   assert(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 280   __ end_a_stub();
 281 
 282   return offset;
 283 }
 284 
 285 
 286 void LIR_Assembler::return_op(LIR_Opr result) {
 287   // Pop the frame before safepoint polling
 288   __ remove_frame(initial_frame_size_in_bytes());
 289   __ read_polling_page(Rtemp, relocInfo::poll_return_type);
 290   __ ret();
 291 }
 292 
 293 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 294   if (info != NULL) {
 295     add_debug_info_for_branch(info);
 296   }
 297   int offset = __ offset();
 298   __ read_polling_page(Rtemp, relocInfo::poll_type);
 299   return offset;
 300 }
 301 
 302 
 303 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 304   if (from_reg != to_reg) {
 305     __ mov(to_reg, from_reg);
 306   }
 307 }
 308 
 309 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 310   assert(src-&gt;is_constant() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 311   LIR_Const* c = src-&gt;as_constant_ptr();
 312 
 313   switch (c-&gt;type()) {
 314     case T_ADDRESS:
 315     case T_INT:
 316       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 317       __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint());
 318       break;
 319 
 320     case T_LONG:
 321       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 322       __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 323       __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 324       break;
 325 
 326     case T_OBJECT:
 327       if (patch_code == lir_patch_none) {
 328         __ mov_oop(dest-&gt;as_register(), c-&gt;as_jobject());
 329       } else {
 330         jobject2reg_with_patching(dest-&gt;as_register(), info);
 331       }
 332       break;
 333 
 334     case T_METADATA:
 335       if (patch_code == lir_patch_none) {
 336         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 337       } else {
 338         klass2reg_with_patching(dest-&gt;as_register(), info);
 339       }
 340       break;
 341 
 342     case T_FLOAT:
 343       if (dest-&gt;is_single_fpu()) {
 344         __ mov_float(dest-&gt;as_float_reg(), c-&gt;as_jfloat());
 345       } else {
 346         // Simple getters can return float constant directly into r0
 347         __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint_bits());
 348       }
 349       break;
 350 
 351     case T_DOUBLE:
 352       if (dest-&gt;is_double_fpu()) {
 353         __ mov_double(dest-&gt;as_double_reg(), c-&gt;as_jdouble());
 354       } else {
 355         // Simple getters can return double constant directly into r1r0
 356         __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo_bits());
 357         __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi_bits());
 358       }
 359       break;
 360 
 361     default:
 362       ShouldNotReachHere();
 363   }
 364 }
 365 
 366 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 367   assert(src-&gt;is_constant(), &quot;must be&quot;);
 368   assert(dest-&gt;is_stack(), &quot;must be&quot;);
 369   LIR_Const* c = src-&gt;as_constant_ptr();
 370 
 371   switch (c-&gt;type()) {
 372     case T_INT:  // fall through
 373     case T_FLOAT:
 374       __ mov_slow(Rtemp, c-&gt;as_jint_bits());
 375       __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 376       break;
 377 
 378     case T_ADDRESS:
 379       __ mov_slow(Rtemp, c-&gt;as_jint());
 380       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 381       break;
 382 
 383     case T_OBJECT:
 384       __ mov_oop(Rtemp, c-&gt;as_jobject());
 385       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 386       break;
 387 
 388     case T_LONG:  // fall through
 389     case T_DOUBLE:
 390       __ mov_slow(Rtemp, c-&gt;as_jint_lo_bits());
 391       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 392       if (c-&gt;as_jint_hi_bits() != c-&gt;as_jint_lo_bits()) {
 393         __ mov_slow(Rtemp, c-&gt;as_jint_hi_bits());
 394       }
 395       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 396       break;
 397 
 398     default:
 399       ShouldNotReachHere();
 400   }
 401 }
 402 
 403 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 404                               CodeEmitInfo* info, bool wide) {
 405   assert((src-&gt;as_constant_ptr()-&gt;type() == T_OBJECT &amp;&amp; src-&gt;as_constant_ptr()-&gt;as_jobject() == NULL),&quot;cannot handle otherwise&quot;);
 406   __ mov(Rtemp, 0);
 407 
 408   int null_check_offset = code_offset();
 409   __ str(Rtemp, as_Address(dest-&gt;as_address_ptr()));
 410 
 411   if (info != NULL) {
 412     assert(false, &quot;arm32 didn&#39;t support this before, investigate if bug&quot;);
 413     add_debug_info_for_null_check(null_check_offset, info);
 414   }
 415 }
 416 
 417 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 418   assert(src-&gt;is_register() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 419 
 420   if (src-&gt;is_single_cpu()) {
 421     if (dest-&gt;is_single_cpu()) {
 422       move_regs(src-&gt;as_register(), dest-&gt;as_register());
 423     } else if (dest-&gt;is_single_fpu()) {
 424       __ fmsr(dest-&gt;as_float_reg(), src-&gt;as_register());
 425     } else {
 426       ShouldNotReachHere();
 427     }
 428   } else if (src-&gt;is_double_cpu()) {
 429     if (dest-&gt;is_double_cpu()) {
 430       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 431     } else {
 432       __ fmdrr(dest-&gt;as_double_reg(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 433     }
 434   } else if (src-&gt;is_single_fpu()) {
 435     if (dest-&gt;is_single_fpu()) {
 436       __ mov_float(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 437     } else if (dest-&gt;is_single_cpu()) {
 438       __ mov_fpr2gpr_float(dest-&gt;as_register(), src-&gt;as_float_reg());
 439     } else {
 440       ShouldNotReachHere();
 441     }
 442   } else if (src-&gt;is_double_fpu()) {
 443     if (dest-&gt;is_double_fpu()) {
 444       __ mov_double(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 445     } else if (dest-&gt;is_double_cpu()) {
 446       __ fmrrd(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_double_reg());
 447     } else {
 448       ShouldNotReachHere();
 449     }
 450   } else {
 451     ShouldNotReachHere();
 452   }
 453 }
 454 
 455 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 456   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 457   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 458 
 459   Address addr = dest-&gt;is_single_word() ?
 460     frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()) :
 461     frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 462 
 463   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 464   if (src-&gt;is_single_fpu() || src-&gt;is_double_fpu()) {
 465     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 466   }
 467 
 468   if (src-&gt;is_single_cpu()) {
 469     switch (type) {
 470       case T_OBJECT:
 471       case T_ARRAY:    __ verify_oop(src-&gt;as_register());   // fall through
 472       case T_ADDRESS:
 473       case T_METADATA: __ str(src-&gt;as_register(), addr);    break;
 474       case T_FLOAT:    // used in intBitsToFloat intrinsic implementation, fall through
 475       case T_INT:      __ str_32(src-&gt;as_register(), addr); break;
 476       default:
 477         ShouldNotReachHere();
 478     }
 479   } else if (src-&gt;is_double_cpu()) {
 480     __ str(src-&gt;as_register_lo(), addr);
 481     __ str(src-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 482   } else if (src-&gt;is_single_fpu()) {
 483     __ str_float(src-&gt;as_float_reg(), addr);
 484   } else if (src-&gt;is_double_fpu()) {
 485     __ str_double(src-&gt;as_double_reg(), addr);
 486   } else {
 487     ShouldNotReachHere();
 488   }
 489 }
 490 
 491 
 492 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 493                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 494                             bool pop_fpu_stack, bool wide,
 495                             bool unaligned) {
 496   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 497   Register base_reg = to_addr-&gt;base()-&gt;as_pointer_register();
 498   const bool needs_patching = (patch_code != lir_patch_none);
 499 
 500   PatchingStub* patch = NULL;
 501   if (needs_patching) {
 502     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 503   }
 504 
 505   int null_check_offset = code_offset();
 506 
 507   switch (type) {
 508     case T_ARRAY:
 509     case T_OBJECT:
 510       if (UseCompressedOops &amp;&amp; !wide) {
 511         ShouldNotReachHere();
 512       } else {
 513         __ str(src-&gt;as_register(), as_Address(to_addr));
 514       }
 515       break;
 516 
 517     case T_ADDRESS:
 518       __ str(src-&gt;as_pointer_register(), as_Address(to_addr));
 519       break;
 520 
 521     case T_BYTE:
 522     case T_BOOLEAN:
 523       __ strb(src-&gt;as_register(), as_Address(to_addr));
 524       break;
 525 
 526     case T_CHAR:
 527     case T_SHORT:
 528       __ strh(src-&gt;as_register(), as_Address(to_addr));
 529       break;
 530 
 531     case T_INT:
 532 #ifdef __SOFTFP__
 533     case T_FLOAT:
 534 #endif // __SOFTFP__
 535       __ str_32(src-&gt;as_register(), as_Address(to_addr));
 536       break;
 537 
 538 
 539 #ifdef __SOFTFP__
 540     case T_DOUBLE:
 541 #endif // __SOFTFP__
 542     case T_LONG: {
 543       Register from_lo = src-&gt;as_register_lo();
 544       Register from_hi = src-&gt;as_register_hi();
 545       if (to_addr-&gt;index()-&gt;is_register()) {
 546         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 547         assert(to_addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 548         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 549         base_reg = Rtemp;
 550         __ str(from_lo, Address(Rtemp));
 551         if (patch != NULL) {
 552           __ nop(); // see comment before patching_epilog for 2nd str
 553           patching_epilog(patch, lir_patch_low, base_reg, info);
 554           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 555           patch_code = lir_patch_high;
 556         }
 557         __ str(from_hi, Address(Rtemp, BytesPerWord));
 558       } else if (base_reg == from_lo) {
 559         __ str(from_hi, as_Address_hi(to_addr));
 560         if (patch != NULL) {
 561           __ nop(); // see comment before patching_epilog for 2nd str
 562           patching_epilog(patch, lir_patch_high, base_reg, info);
 563           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 564           patch_code = lir_patch_low;
 565         }
 566         __ str(from_lo, as_Address_lo(to_addr));
 567       } else {
 568         __ str(from_lo, as_Address_lo(to_addr));
 569         if (patch != NULL) {
 570           __ nop(); // see comment before patching_epilog for 2nd str
 571           patching_epilog(patch, lir_patch_low, base_reg, info);
 572           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 573           patch_code = lir_patch_high;
 574         }
 575         __ str(from_hi, as_Address_hi(to_addr));
 576       }
 577       break;
 578     }
 579 
 580 #ifndef __SOFTFP__
 581     case T_FLOAT:
 582       if (to_addr-&gt;index()-&gt;is_register()) {
 583         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 584         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 585         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 586         __ fsts(src-&gt;as_float_reg(), Address(Rtemp, to_addr-&gt;disp()));
 587       } else {
 588         __ fsts(src-&gt;as_float_reg(), as_Address(to_addr));
 589       }
 590       break;
 591 
 592     case T_DOUBLE:
 593       if (to_addr-&gt;index()-&gt;is_register()) {
 594         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 595         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 596         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 597         __ fstd(src-&gt;as_double_reg(), Address(Rtemp, to_addr-&gt;disp()));
 598       } else {
 599         __ fstd(src-&gt;as_double_reg(), as_Address(to_addr));
 600       }
 601       break;
 602 #endif // __SOFTFP__
 603 
 604 
 605     default:
 606       ShouldNotReachHere();
 607   }
 608 
 609   if (info != NULL) {
 610     add_debug_info_for_null_check(null_check_offset, info);
 611   }
 612 
 613   if (patch != NULL) {
 614     // Offset embedded into LDR/STR instruction may appear not enough
 615     // to address a field. So, provide a space for one more instruction
 616     // that will deal with larger offsets.
 617     __ nop();
 618     patching_epilog(patch, patch_code, base_reg, info);
 619   }
 620 }
 621 
 622 
 623 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 624   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 625   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 626 
 627   Address addr = src-&gt;is_single_word() ?
 628     frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()) :
 629     frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 630 
 631   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 632   if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 633     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 634   }
 635 
 636   if (dest-&gt;is_single_cpu()) {
 637     switch (type) {
 638       case T_OBJECT:
 639       case T_ARRAY:
 640       case T_ADDRESS:
 641       case T_METADATA: __ ldr(dest-&gt;as_register(), addr); break;
 642       case T_FLOAT:    // used in floatToRawIntBits intrinsic implemenation
 643       case T_INT:      __ ldr_u32(dest-&gt;as_register(), addr); break;
 644       default:
 645         ShouldNotReachHere();
 646     }
 647     if ((type == T_OBJECT) || (type == T_ARRAY)) {
 648       __ verify_oop(dest-&gt;as_register());
 649     }
 650   } else if (dest-&gt;is_double_cpu()) {
 651     __ ldr(dest-&gt;as_register_lo(), addr);
 652     __ ldr(dest-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 653   } else if (dest-&gt;is_single_fpu()) {
 654     __ ldr_float(dest-&gt;as_float_reg(), addr);
 655   } else if (dest-&gt;is_double_fpu()) {
 656     __ ldr_double(dest-&gt;as_double_reg(), addr);
 657   } else {
 658     ShouldNotReachHere();
 659   }
 660 }
 661 
 662 
 663 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 664   if (src-&gt;is_single_stack()) {
 665     switch (src-&gt;type()) {
 666       case T_OBJECT:
 667       case T_ARRAY:
 668       case T_ADDRESS:
 669       case T_METADATA:
 670         __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 671         __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 672         break;
 673 
 674       case T_INT:
 675       case T_FLOAT:
 676         __ ldr_u32(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 677         __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 678         break;
 679 
 680       default:
 681         ShouldNotReachHere();
 682     }
 683   } else {
 684     assert(src-&gt;is_double_stack(), &quot;must be&quot;);
 685     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 686     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 687     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 688     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 689   }
 690 }
 691 
 692 
 693 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type,
 694                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 695                             bool wide, bool unaligned) {
 696   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
 697   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 698   LIR_Address* addr = src-&gt;as_address_ptr();
 699 
 700   Register base_reg = addr-&gt;base()-&gt;as_pointer_register();
 701 
 702   PatchingStub* patch = NULL;
 703   if (patch_code != lir_patch_none) {
 704     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 705   }
 706   if (info != NULL) {
 707     add_debug_info_for_null_check_here(info);
 708   }
 709 
 710   switch (type) {
 711     case T_OBJECT:  // fall through
 712     case T_ARRAY:
 713       if (UseCompressedOops &amp;&amp; !wide) {
 714         __ ldr_u32(dest-&gt;as_register(), as_Address(addr));
 715       } else {
 716         __ ldr(dest-&gt;as_register(), as_Address(addr));
 717       }
 718       break;
 719 
 720     case T_ADDRESS:
 721       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
 722         __ ldr_u32(dest-&gt;as_pointer_register(), as_Address(addr));
 723       } else {
 724         __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 725       }
 726       break;
 727 
 728     case T_INT:
 729 #ifdef __SOFTFP__
 730     case T_FLOAT:
 731 #endif // __SOFTFP__
 732       __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 733       break;
 734 
 735     case T_BOOLEAN:
 736       __ ldrb(dest-&gt;as_register(), as_Address(addr));
 737       break;
 738 
 739     case T_BYTE:
 740       __ ldrsb(dest-&gt;as_register(), as_Address(addr));
 741       break;
 742 
 743     case T_CHAR:
 744       __ ldrh(dest-&gt;as_register(), as_Address(addr));
 745       break;
 746 
 747     case T_SHORT:
 748       __ ldrsh(dest-&gt;as_register(), as_Address(addr));
 749       break;
 750 
 751 
 752 #ifdef __SOFTFP__
 753     case T_DOUBLE:
 754 #endif // __SOFTFP__
 755     case T_LONG: {
 756       Register to_lo = dest-&gt;as_register_lo();
 757       Register to_hi = dest-&gt;as_register_hi();
 758       if (addr-&gt;index()-&gt;is_register()) {
 759         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 760         assert(addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 761         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 762         base_reg = Rtemp;
 763         __ ldr(to_lo, Address(Rtemp));
 764         if (patch != NULL) {
 765           __ nop(); // see comment before patching_epilog for 2nd ldr
 766           patching_epilog(patch, lir_patch_low, base_reg, info);
 767           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 768           patch_code = lir_patch_high;
 769         }
 770         __ ldr(to_hi, Address(Rtemp, BytesPerWord));
 771       } else if (base_reg == to_lo) {
 772         __ ldr(to_hi, as_Address_hi(addr));
 773         if (patch != NULL) {
 774           __ nop(); // see comment before patching_epilog for 2nd ldr
 775           patching_epilog(patch, lir_patch_high, base_reg, info);
 776           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 777           patch_code = lir_patch_low;
 778         }
 779         __ ldr(to_lo, as_Address_lo(addr));
 780       } else {
 781         __ ldr(to_lo, as_Address_lo(addr));
 782         if (patch != NULL) {
 783           __ nop(); // see comment before patching_epilog for 2nd ldr
 784           patching_epilog(patch, lir_patch_low, base_reg, info);
 785           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 786           patch_code = lir_patch_high;
 787         }
 788         __ ldr(to_hi, as_Address_hi(addr));
 789       }
 790       break;
 791     }
 792 
 793 #ifndef __SOFTFP__
 794     case T_FLOAT:
 795       if (addr-&gt;index()-&gt;is_register()) {
 796         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 797         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 798         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 799         __ flds(dest-&gt;as_float_reg(), Address(Rtemp, addr-&gt;disp()));
 800       } else {
 801         __ flds(dest-&gt;as_float_reg(), as_Address(addr));
 802       }
 803       break;
 804 
 805     case T_DOUBLE:
 806       if (addr-&gt;index()-&gt;is_register()) {
 807         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 808         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 809         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 810         __ fldd(dest-&gt;as_double_reg(), Address(Rtemp, addr-&gt;disp()));
 811       } else {
 812         __ fldd(dest-&gt;as_double_reg(), as_Address(addr));
 813       }
 814       break;
 815 #endif // __SOFTFP__
 816 
 817 
 818     default:
 819       ShouldNotReachHere();
 820   }
 821 
 822   if (patch != NULL) {
 823     // Offset embedded into LDR/STR instruction may appear not enough
 824     // to address a field. So, provide a space for one more instruction
 825     // that will deal with larger offsets.
 826     __ nop();
 827     patching_epilog(patch, patch_code, base_reg, info);
 828   }
 829 
 830 }
 831 
 832 
 833 void LIR_Assembler::emit_op3(LIR_Op3* op) {
 834   bool is_32 = op-&gt;result_opr()-&gt;is_single_cpu();
 835 
 836   if (op-&gt;code() == lir_idiv &amp;&amp; op-&gt;in_opr2()-&gt;is_constant() &amp;&amp; is_32) {
 837     int c = op-&gt;in_opr2()-&gt;as_constant_ptr()-&gt;as_jint();
 838     assert(is_power_of_2(c), &quot;non power-of-2 constant should be put in a register&quot;);
 839 
 840     Register left = op-&gt;in_opr1()-&gt;as_register();
 841     Register dest = op-&gt;result_opr()-&gt;as_register();
 842     if (c == 1) {
 843       __ mov(dest, left);
 844     } else if (c == 2) {
 845       __ add_32(dest, left, AsmOperand(left, lsr, 31));
 846       __ asr_32(dest, dest, 1);
 847     } else if (c != (int) 0x80000000) {
 848       int power = log2_intptr(c);
 849       __ asr_32(Rtemp, left, 31);
 850       __ add_32(dest, left, AsmOperand(Rtemp, lsr, 32-power)); // dest = left + (left &lt; 0 ? 2^power - 1 : 0);
 851       __ asr_32(dest, dest, power);                            // dest = dest &gt;&gt;&gt; power;
 852     } else {
 853       // x/0x80000000 is a special case, since dividend is a power of two, but is negative.
 854       // The only possible result values are 0 and 1, with 1 only for dividend == divisor == 0x80000000.
 855       __ cmp_32(left, c);
 856       __ mov(dest, 0, ne);
 857       __ mov(dest, 1, eq);
 858     }
 859   } else {
 860     assert(op-&gt;code() == lir_idiv || op-&gt;code() == lir_irem, &quot;unexpected op3&quot;);
 861     __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::runtime_call_type);
 862     add_debug_info_for_div0_here(op-&gt;info());
 863   }
 864 }
 865 
 866 
 867 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
 868 #ifdef ASSERT
 869   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
 870   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
 871   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
 872   assert(op-&gt;info() == NULL, &quot;CodeEmitInfo?&quot;);
 873 #endif // ASSERT
 874 
 875 #ifdef __SOFTFP__
 876   assert (op-&gt;code() != lir_cond_float_branch, &quot;this should be impossible&quot;);
 877 #else
 878   if (op-&gt;code() == lir_cond_float_branch) {
 879     __ fmstat();
 880     __ b(*(op-&gt;ublock()-&gt;label()), vs);
 881   }
 882 #endif // __SOFTFP__
 883 
 884   AsmCondition acond = al;
 885   switch (op-&gt;cond()) {
 886     case lir_cond_equal:        acond = eq; break;
 887     case lir_cond_notEqual:     acond = ne; break;
 888     case lir_cond_less:         acond = lt; break;
 889     case lir_cond_lessEqual:    acond = le; break;
 890     case lir_cond_greaterEqual: acond = ge; break;
 891     case lir_cond_greater:      acond = gt; break;
 892     case lir_cond_aboveEqual:   acond = hs; break;
 893     case lir_cond_belowEqual:   acond = ls; break;
 894     default: assert(op-&gt;cond() == lir_cond_always, &quot;must be&quot;);
 895   }
 896   __ b(*(op-&gt;label()), acond);
 897 }
 898 
 899 
 900 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
 901   LIR_Opr src  = op-&gt;in_opr();
 902   LIR_Opr dest = op-&gt;result_opr();
 903 
 904   switch (op-&gt;bytecode()) {
 905     case Bytecodes::_i2l:
 906       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 907       __ mov(dest-&gt;as_register_hi(), AsmOperand(src-&gt;as_register(), asr, 31));
 908       break;
 909     case Bytecodes::_l2i:
 910       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 911       break;
 912     case Bytecodes::_i2b:
 913       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 8);
 914       break;
 915     case Bytecodes::_i2s:
 916       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 917       break;
 918     case Bytecodes::_i2c:
 919       __ zero_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 920       break;
 921     case Bytecodes::_f2d:
 922       __ convert_f2d(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
 923       break;
 924     case Bytecodes::_d2f:
 925       __ convert_d2f(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
 926       break;
 927     case Bytecodes::_i2f:
 928       __ fmsr(Stemp, src-&gt;as_register());
 929       __ fsitos(dest-&gt;as_float_reg(), Stemp);
 930       break;
 931     case Bytecodes::_i2d:
 932       __ fmsr(Stemp, src-&gt;as_register());
 933       __ fsitod(dest-&gt;as_double_reg(), Stemp);
 934       break;
 935     case Bytecodes::_f2i:
 936       __ ftosizs(Stemp, src-&gt;as_float_reg());
 937       __ fmrs(dest-&gt;as_register(), Stemp);
 938       break;
 939     case Bytecodes::_d2i:
 940       __ ftosizd(Stemp, src-&gt;as_double_reg());
 941       __ fmrs(dest-&gt;as_register(), Stemp);
 942       break;
 943     default:
 944       ShouldNotReachHere();
 945   }
 946 }
 947 
 948 
 949 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
 950   if (op-&gt;init_check()) {
 951     Register tmp = op-&gt;tmp1()-&gt;as_register();
 952     __ ldrb(tmp, Address(op-&gt;klass()-&gt;as_register(), InstanceKlass::init_state_offset()));
 953     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
 954     __ cmp(tmp, InstanceKlass::fully_initialized);
 955     __ b(*op-&gt;stub()-&gt;entry(), ne);
 956   }
 957   __ allocate_object(op-&gt;obj()-&gt;as_register(),
 958                      op-&gt;tmp1()-&gt;as_register(),
 959                      op-&gt;tmp2()-&gt;as_register(),
 960                      op-&gt;tmp3()-&gt;as_register(),
 961                      op-&gt;header_size(),
 962                      op-&gt;object_size(),
 963                      op-&gt;klass()-&gt;as_register(),
 964                      *op-&gt;stub()-&gt;entry());
 965   __ bind(*op-&gt;stub()-&gt;continuation());
 966 }
 967 
 968 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
 969   if (UseSlowPath ||
 970       (!UseFastNewObjectArray &amp;&amp; (op-&gt;type() == T_OBJECT || op-&gt;type() == T_ARRAY)) ||
 971       (!UseFastNewTypeArray   &amp;&amp; (op-&gt;type() != T_OBJECT &amp;&amp; op-&gt;type() != T_ARRAY))) {
 972     __ b(*op-&gt;stub()-&gt;entry());
 973   } else {
 974     __ allocate_array(op-&gt;obj()-&gt;as_register(),
 975                       op-&gt;len()-&gt;as_register(),
 976                       op-&gt;tmp1()-&gt;as_register(),
 977                       op-&gt;tmp2()-&gt;as_register(),
 978                       op-&gt;tmp3()-&gt;as_register(),
 979                       arrayOopDesc::header_size(op-&gt;type()),
 980                       type2aelembytes(op-&gt;type()),
 981                       op-&gt;klass()-&gt;as_register(),
 982                       *op-&gt;stub()-&gt;entry());
 983   }
 984   __ bind(*op-&gt;stub()-&gt;continuation());
 985 }
 986 
 987 void LIR_Assembler::type_profile_helper(Register mdo, int mdo_offset_bias,
 988                                         ciMethodData *md, ciProfileData *data,
 989                                         Register recv, Register tmp1, Label* update_done) {
 990   assert_different_registers(mdo, recv, tmp1);
 991   uint i;
 992   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
 993     Label next_test;
 994     // See if the receiver is receiver[n].
 995     Address receiver_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
 996                           mdo_offset_bias);
 997     __ ldr(tmp1, receiver_addr);
 998     __ verify_klass_ptr(tmp1);
 999     __ cmp(recv, tmp1);
1000     __ b(next_test, ne);
1001     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1002                       mdo_offset_bias);
1003     __ ldr(tmp1, data_addr);
1004     __ add(tmp1, tmp1, DataLayout::counter_increment);
1005     __ str(tmp1, data_addr);
1006     __ b(*update_done);
1007     __ bind(next_test);
1008   }
1009 
1010   // Didn&#39;t find receiver; find next empty slot and fill it in
1011   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
1012     Label next_test;
1013     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
1014                       mdo_offset_bias);
1015     __ ldr(tmp1, recv_addr);
1016     __ cbnz(tmp1, next_test);
1017     __ str(recv, recv_addr);
1018     __ mov(tmp1, DataLayout::counter_increment);
1019     __ str(tmp1, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1020                          mdo_offset_bias));
1021     __ b(*update_done);
1022     __ bind(next_test);
1023   }
1024 }
1025 
1026 void LIR_Assembler::setup_md_access(ciMethod* method, int bci,
1027                                     ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias) {
1028   md = method-&gt;method_data_or_null();
1029   assert(md != NULL, &quot;Sanity&quot;);
1030   data = md-&gt;bci_to_data(bci);
1031   assert(data != NULL,       &quot;need data for checkcast&quot;);
1032   assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1033   if (md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()) + data-&gt;size_in_bytes() &gt;= 4096) {
1034     // The offset is large so bias the mdo by the base of the slot so
1035     // that the ldr can use an immediate offset to reference the slots of the data
1036     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, DataLayout::header_offset());
1037   }
1038 }
1039 
1040 // On 32-bit ARM, code before this helper should test obj for null (ZF should be set if obj is null).
1041 void LIR_Assembler::typecheck_profile_helper1(ciMethod* method, int bci,
1042                                               ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias,
1043                                               Register obj, Register mdo, Register data_val, Label* obj_is_null) {
1044   assert(method != NULL, &quot;Should have method&quot;);
1045   assert_different_registers(obj, mdo, data_val);
1046   setup_md_access(method, bci, md, data, mdo_offset_bias);
1047   Label not_null;
1048   __ b(not_null, ne);
1049   __ mov_metadata(mdo, md-&gt;constant_encoding());
1050   if (mdo_offset_bias &gt; 0) {
1051     __ mov_slow(data_val, mdo_offset_bias);
1052     __ add(mdo, mdo, data_val);
1053   }
1054   Address flags_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()) - mdo_offset_bias);
1055   __ ldrb(data_val, flags_addr);
1056   __ orr(data_val, data_val, (uint)BitData::null_seen_byte_constant());
1057   __ strb(data_val, flags_addr);
1058   __ b(*obj_is_null);
1059   __ bind(not_null);
1060 }
1061 
1062 void LIR_Assembler::typecheck_profile_helper2(ciMethodData* md, ciProfileData* data, int mdo_offset_bias,
1063                                               Register mdo, Register recv, Register value, Register tmp1,
1064                                               Label* profile_cast_success, Label* profile_cast_failure,
1065                                               Label* success, Label* failure) {
1066   assert_different_registers(mdo, value, tmp1);
1067   __ bind(*profile_cast_success);
1068   __ mov_metadata(mdo, md-&gt;constant_encoding());
1069   if (mdo_offset_bias &gt; 0) {
1070     __ mov_slow(tmp1, mdo_offset_bias);
1071     __ add(mdo, mdo, tmp1);
1072   }
1073   __ load_klass(recv, value);
1074   type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, success);
1075   __ b(*success);
1076   // Cast failure case
1077   __ bind(*profile_cast_failure);
1078   __ mov_metadata(mdo, md-&gt;constant_encoding());
1079   if (mdo_offset_bias &gt; 0) {
1080     __ mov_slow(tmp1, mdo_offset_bias);
1081     __ add(mdo, mdo, tmp1);
1082   }
1083   Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
1084   __ ldr(tmp1, data_addr);
1085   __ sub(tmp1, tmp1, DataLayout::counter_increment);
1086   __ str(tmp1, data_addr);
1087   __ b(*failure);
1088 }
1089 
1090 // Sets `res` to true, if `cond` holds.
1091 static void set_instanceof_result(MacroAssembler* _masm, Register res, AsmCondition cond) {
1092   __ mov(res, 1, cond);
1093 }
1094 
1095 
1096 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1097   // TODO: ARM - can be more effective with one more register
1098   switch (op-&gt;code()) {
1099     case lir_store_check: {
1100       CodeStub* stub = op-&gt;stub();
1101       Register value = op-&gt;object()-&gt;as_register();
1102       Register array = op-&gt;array()-&gt;as_register();
1103       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1104       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1105       assert_different_registers(klass_RInfo, k_RInfo, Rtemp);
1106       if (op-&gt;should_profile()) {
1107         assert_different_registers(value, klass_RInfo, k_RInfo, Rtemp);
1108       }
1109 
1110       // check if it needs to be profiled
1111       ciMethodData* md;
1112       ciProfileData* data;
1113       int mdo_offset_bias = 0;
1114       Label profile_cast_success, profile_cast_failure, done;
1115       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1116       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
1117 
1118       if (op-&gt;should_profile()) {
1119         __ cmp(value, 0);
1120         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, value, k_RInfo, Rtemp, &amp;done);
1121       } else {
1122         __ cbz(value, done);
1123       }
1124       assert_different_registers(k_RInfo, value);
1125       add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1126       __ load_klass(k_RInfo, array);
1127       __ load_klass(klass_RInfo, value);
1128       __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1129       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1130       // check for immediate positive hit
1131       __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1132       __ cmp(klass_RInfo, k_RInfo);
1133       __ cond_cmp(Rtemp, k_RInfo, ne);
1134       __ b(*success_target, eq);
1135       // check for immediate negative hit
1136       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1137       __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1138       __ b(*failure_target, ne);
1139       // slow case
1140       assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1141       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1142       __ cbz(R0, *failure_target);
1143       if (op-&gt;should_profile()) {
1144         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1145         if (mdo == value) {
1146           mdo = k_RInfo;
1147           recv = klass_RInfo;
1148         }
1149         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, value, tmp1,
1150                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1151                                   &amp;done, stub-&gt;entry());
1152       }
1153       __ bind(done);
1154       break;
1155     }
1156 
1157     case lir_checkcast: {
1158       CodeStub* stub = op-&gt;stub();
1159       Register obj = op-&gt;object()-&gt;as_register();
1160       Register res = op-&gt;result_opr()-&gt;as_register();
1161       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1162       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1163       ciKlass* k = op-&gt;klass();
1164       assert_different_registers(res, k_RInfo, klass_RInfo, Rtemp);
1165 
1166       if (stub-&gt;is_simple_exception_stub()) {
1167       // TODO: ARM - Late binding is used to prevent confusion of register allocator
1168       assert(stub-&gt;is_exception_throw_stub(), &quot;must be&quot;);
1169       ((SimpleExceptionStub*)stub)-&gt;set_obj(op-&gt;result_opr());
1170       }
1171       ciMethodData* md;
1172       ciProfileData* data;
1173       int mdo_offset_bias = 0;
1174 
1175       Label done;
1176 
1177       Label profile_cast_failure, profile_cast_success;
1178       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : op-&gt;stub()-&gt;entry();
1179       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1180 
1181 
1182       __ movs(res, obj);
1183       if (op-&gt;should_profile()) {
1184         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1185       } else {
1186         __ b(done, eq);
1187       }
1188       if (k-&gt;is_loaded()) {
1189         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1190       } else if (k_RInfo != obj) {
1191         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1192         __ movs(res, obj);
1193       } else {
1194         // Patching doesn&#39;t update &quot;res&quot; register after GC, so do patching first
1195         klass2reg_with_patching(Rtemp, op-&gt;info_for_patch());
1196         __ movs(res, obj);
1197         __ mov(k_RInfo, Rtemp);
1198       }
1199       __ load_klass(klass_RInfo, res, ne);
1200 
1201       if (op-&gt;fast_check()) {
1202         __ cmp(klass_RInfo, k_RInfo, ne);
1203         __ b(*failure_target, ne);
1204       } else if (k-&gt;is_loaded()) {
1205         __ b(*success_target, eq);
1206         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1207         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1208           __ cmp(Rtemp, k_RInfo);
1209           __ b(*failure_target, ne);
1210         } else {
1211           __ cmp(klass_RInfo, k_RInfo);
1212           __ cmp(Rtemp, k_RInfo, ne);
1213           __ b(*success_target, eq);
1214           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1215           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1216           __ cbz(R0, *failure_target);
1217         }
1218       } else {
1219         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1220         __ b(*success_target, eq);
1221         // check for immediate positive hit
1222         __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1223         __ cmp(klass_RInfo, k_RInfo);
1224         __ cmp(Rtemp, k_RInfo, ne);
1225         __ b(*success_target, eq);
1226         // check for immediate negative hit
1227         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1228         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1229         __ b(*failure_target, ne);
1230         // slow case
1231         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1232         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1233         __ cbz(R0, *failure_target);
1234       }
1235 
1236       if (op-&gt;should_profile()) {
1237         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1238         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1239                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1240                                   &amp;done, stub-&gt;entry());
1241       }
1242       __ bind(done);
1243       break;
1244     }
1245 
1246     case lir_instanceof: {
1247       Register obj = op-&gt;object()-&gt;as_register();
1248       Register res = op-&gt;result_opr()-&gt;as_register();
1249       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1250       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1251       ciKlass* k = op-&gt;klass();
1252       assert_different_registers(res, klass_RInfo, k_RInfo, Rtemp);
1253 
1254       ciMethodData* md;
1255       ciProfileData* data;
1256       int mdo_offset_bias = 0;
1257 
1258       Label done;
1259 
1260       Label profile_cast_failure, profile_cast_success;
1261       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : &amp;done;
1262       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1263 
1264       __ movs(res, obj);
1265 
1266       if (op-&gt;should_profile()) {
1267         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1268       } else {
1269         __ b(done, eq);
1270       }
1271 
1272       if (k-&gt;is_loaded()) {
1273         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1274       } else {
1275         op-&gt;info_for_patch()-&gt;add_register_oop(FrameMap::as_oop_opr(res));
1276         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1277       }
1278       __ load_klass(klass_RInfo, res);
1279 
1280       if (!op-&gt;should_profile()) {
1281         __ mov(res, 0);
1282       }
1283 
1284       if (op-&gt;fast_check()) {
1285         __ cmp(klass_RInfo, k_RInfo);
1286         if (!op-&gt;should_profile()) {
1287           set_instanceof_result(_masm, res, eq);
1288         } else {
1289           __ b(profile_cast_failure, ne);
1290         }
1291       } else if (k-&gt;is_loaded()) {
1292         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1293         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1294           __ cmp(Rtemp, k_RInfo);
1295           if (!op-&gt;should_profile()) {
1296             set_instanceof_result(_masm, res, eq);
1297           } else {
1298             __ b(profile_cast_failure, ne);
1299           }
1300         } else {
1301           __ cmp(klass_RInfo, k_RInfo);
1302           __ cond_cmp(Rtemp, k_RInfo, ne);
1303           if (!op-&gt;should_profile()) {
1304             set_instanceof_result(_masm, res, eq);
1305           }
1306           __ b(*success_target, eq);
1307           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1308           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1309           if (!op-&gt;should_profile()) {
1310             move_regs(R0, res);
1311           } else {
1312             __ cbz(R0, *failure_target);
1313           }
1314         }
1315       } else {
1316         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1317         // check for immediate positive hit
1318         __ cmp(klass_RInfo, k_RInfo);
1319         if (!op-&gt;should_profile()) {
1320           __ ldr(res, Address(klass_RInfo, Rtemp), ne);
1321           __ cond_cmp(res, k_RInfo, ne);
1322           set_instanceof_result(_masm, res, eq);
1323         } else {
1324           __ ldr(Rtemp, Address(klass_RInfo, Rtemp), ne);
1325           __ cond_cmp(Rtemp, k_RInfo, ne);
1326         }
1327         __ b(*success_target, eq);
1328         // check for immediate negative hit
1329         if (op-&gt;should_profile()) {
1330           __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1331         }
1332         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1333         if (!op-&gt;should_profile()) {
1334           __ mov(res, 0, ne);
1335         }
1336         __ b(*failure_target, ne);
1337         // slow case
1338         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1339         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1340         if (!op-&gt;should_profile()) {
1341           move_regs(R0, res);
1342         }
1343         if (op-&gt;should_profile()) {
1344           __ cbz(R0, *failure_target);
1345         }
1346       }
1347 
1348       if (op-&gt;should_profile()) {
1349         Label done_ok, done_failure;
1350         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1351         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1352                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1353                                   &amp;done_ok, &amp;done_failure);
1354         __ bind(done_failure);
1355         __ mov(res, 0);
1356         __ b(done);
1357         __ bind(done_ok);
1358         __ mov(res, 1);
1359       }
1360       __ bind(done);
1361       break;
1362     }
1363     default:
1364       ShouldNotReachHere();
1365   }
1366 }
1367 
1368 
1369 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1370   //   if (*addr == cmpval) {
1371   //     *addr = newval;
1372   //     dest = 1;
1373   //   } else {
1374   //     dest = 0;
1375   //   }
1376   // FIXME: membar_release
1377   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
1378   Register addr = op-&gt;addr()-&gt;is_register() ?
1379     op-&gt;addr()-&gt;as_pointer_register() :
1380     op-&gt;addr()-&gt;as_address_ptr()-&gt;base()-&gt;as_pointer_register();
1381   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;disp() == 0, &quot;unexpected disp&quot;);
1382   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;unexpected index&quot;);
1383   if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj) {
1384     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1385     Register newval = op-&gt;new_value()-&gt;as_register();
1386     Register dest = op-&gt;result_opr()-&gt;as_register();
1387     assert_different_registers(dest, addr, cmpval, newval, Rtemp);
1388 
1389     __ atomic_cas_bool(cmpval, newval, addr, 0, Rtemp); // Rtemp free by default at C1 LIR layer
1390     __ mov(dest, 1, eq);
1391     __ mov(dest, 0, ne);
1392   } else if (op-&gt;code() == lir_cas_long) {
1393     assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1394     Register cmp_value_lo = op-&gt;cmp_value()-&gt;as_register_lo();
1395     Register cmp_value_hi = op-&gt;cmp_value()-&gt;as_register_hi();
1396     Register new_value_lo = op-&gt;new_value()-&gt;as_register_lo();
1397     Register new_value_hi = op-&gt;new_value()-&gt;as_register_hi();
1398     Register dest = op-&gt;result_opr()-&gt;as_register();
1399     Register tmp_lo = op-&gt;tmp1()-&gt;as_register_lo();
1400     Register tmp_hi = op-&gt;tmp1()-&gt;as_register_hi();
1401 
1402     assert_different_registers(tmp_lo, tmp_hi, cmp_value_lo, cmp_value_hi, dest, new_value_lo, new_value_hi, addr);
1403     assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1404     assert(new_value_hi-&gt;encoding() == new_value_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1405     assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1406     assert((new_value_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1407     __ atomic_cas64(tmp_lo, tmp_hi, dest, cmp_value_lo, cmp_value_hi,
1408                     new_value_lo, new_value_hi, addr, 0);
1409   } else {
1410     Unimplemented();
1411   }
1412   // FIXME: is full membar really needed instead of just membar_acquire?
1413   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1414 }
1415 
1416 
1417 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1418   AsmCondition acond = al;
1419   AsmCondition ncond = nv;
1420   if (opr1 != opr2) {
1421     switch (condition) {
1422       case lir_cond_equal:        acond = eq; ncond = ne; break;
1423       case lir_cond_notEqual:     acond = ne; ncond = eq; break;
1424       case lir_cond_less:         acond = lt; ncond = ge; break;
1425       case lir_cond_lessEqual:    acond = le; ncond = gt; break;
1426       case lir_cond_greaterEqual: acond = ge; ncond = lt; break;
1427       case lir_cond_greater:      acond = gt; ncond = le; break;
1428       case lir_cond_aboveEqual:   acond = hs; ncond = lo; break;
1429       case lir_cond_belowEqual:   acond = ls; ncond = hi; break;
1430       default: ShouldNotReachHere();
1431     }
1432   }
1433 
1434   for (;;) {                         // two iterations only
1435     if (opr1 == result) {
1436       // do nothing
1437     } else if (opr1-&gt;is_single_cpu()) {
1438       __ mov(result-&gt;as_register(), opr1-&gt;as_register(), acond);
1439     } else if (opr1-&gt;is_double_cpu()) {
1440       __ long_move(result-&gt;as_register_lo(), result-&gt;as_register_hi(),
1441                    opr1-&gt;as_register_lo(), opr1-&gt;as_register_hi(), acond);
1442     } else if (opr1-&gt;is_single_stack()) {
1443       __ ldr(result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr1-&gt;single_stack_ix()), acond);
1444     } else if (opr1-&gt;is_double_stack()) {
1445       __ ldr(result-&gt;as_register_lo(),
1446              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), lo_word_offset_in_bytes), acond);
1447       __ ldr(result-&gt;as_register_hi(),
1448              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), hi_word_offset_in_bytes), acond);
1449     } else if (opr1-&gt;is_illegal()) {
1450       // do nothing: this part of the cmove has been optimized away in the peephole optimizer
1451     } else {
1452       assert(opr1-&gt;is_constant(), &quot;must be&quot;);
1453       LIR_Const* c = opr1-&gt;as_constant_ptr();
1454 
1455       switch (c-&gt;type()) {
1456         case T_INT:
1457           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1458           break;
1459         case T_LONG:
1460           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1461           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1462           break;
1463         case T_OBJECT:
1464           __ mov_oop(result-&gt;as_register(), c-&gt;as_jobject(), 0, acond);
1465           break;
1466         case T_FLOAT:
1467 #ifdef __SOFTFP__
1468           // not generated now.
1469           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1470 #else
1471           __ mov_float(result-&gt;as_float_reg(), c-&gt;as_jfloat(), acond);
1472 #endif // __SOFTFP__
1473           break;
1474         case T_DOUBLE:
1475 #ifdef __SOFTFP__
1476           // not generated now.
1477           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1478           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1479 #else
1480           __ mov_double(result-&gt;as_double_reg(), c-&gt;as_jdouble(), acond);
1481 #endif // __SOFTFP__
1482           break;
1483         default:
1484           ShouldNotReachHere();
1485       }
1486     }
1487 
1488     // Negate the condition and repeat the algorithm with the second operand
1489     if (opr1 == opr2) { break; }
1490     opr1 = opr2;
1491     acond = ncond;
1492   }
1493 }
1494 
1495 #ifdef ASSERT
1496 static int reg_size(LIR_Opr op) {
1497   switch (op-&gt;type()) {
1498   case T_FLOAT:
1499   case T_INT:      return BytesPerInt;
1500   case T_LONG:
1501   case T_DOUBLE:   return BytesPerLong;
1502   case T_OBJECT:
1503   case T_ARRAY:
1504   case T_METADATA: return BytesPerWord;
1505   case T_ADDRESS:
1506   case T_ILLEGAL:  // fall through
1507   default: ShouldNotReachHere(); return -1;
1508   }
1509 }
1510 #endif
1511 
1512 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1513   assert(info == NULL, &quot;unused on this code path&quot;);
1514   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1515 
1516   if (right-&gt;is_address()) {
1517     // special case for adding shifted/extended register
1518     const Register res = dest-&gt;as_pointer_register();
1519     const Register lreg = left-&gt;as_pointer_register();
1520     const LIR_Address* addr = right-&gt;as_address_ptr();
1521 
1522     assert(addr-&gt;base()-&gt;as_pointer_register() == lreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1523 
1524     int scale = addr-&gt;scale();
1525     AsmShift shift = lsl;
1526 
1527 
1528     assert(reg_size(addr-&gt;base()) == reg_size(addr-&gt;index()), &quot;should be&quot;);
1529     assert(reg_size(addr-&gt;base()) == reg_size(dest), &quot;should be&quot;);
1530     assert(reg_size(dest) == wordSize, &quot;should be&quot;);
1531 
1532     AsmOperand operand(addr-&gt;index()-&gt;as_pointer_register(), shift, scale);
1533     switch (code) {
1534       case lir_add: __ add(res, lreg, operand); break;
1535       case lir_sub: __ sub(res, lreg, operand); break;
1536       default: ShouldNotReachHere();
1537     }
1538 
1539   } else if (left-&gt;is_address()) {
1540     assert(code == lir_sub &amp;&amp; right-&gt;is_single_cpu(), &quot;special case used by strength_reduce_multiply()&quot;);
1541     const LIR_Address* addr = left-&gt;as_address_ptr();
1542     const Register res = dest-&gt;as_register();
1543     const Register rreg = right-&gt;as_register();
1544     assert(addr-&gt;base()-&gt;as_register() == rreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1545     __ rsb(res, rreg, AsmOperand(addr-&gt;index()-&gt;as_register(), lsl, addr-&gt;scale()));
1546 
1547   } else if (dest-&gt;is_single_cpu()) {
1548     assert(left-&gt;is_single_cpu(), &quot;unexpected left operand&quot;);
1549 
1550     const Register res = dest-&gt;as_register();
1551     const Register lreg = left-&gt;as_register();
1552 
1553     if (right-&gt;is_single_cpu()) {
1554       const Register rreg = right-&gt;as_register();
1555       switch (code) {
1556         case lir_add: __ add_32(res, lreg, rreg); break;
1557         case lir_sub: __ sub_32(res, lreg, rreg); break;
1558         case lir_mul: __ mul_32(res, lreg, rreg); break;
1559         default: ShouldNotReachHere();
1560       }
1561     } else {
1562       assert(right-&gt;is_constant(), &quot;must be&quot;);
1563       const jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
1564       if (!Assembler::is_arith_imm_in_range(c)) {
1565         BAILOUT(&quot;illegal arithmetic operand&quot;);
1566       }
1567       switch (code) {
1568         case lir_add: __ add_32(res, lreg, c); break;
1569         case lir_sub: __ sub_32(res, lreg, c); break;
1570         default: ShouldNotReachHere();
1571       }
1572     }
1573 
1574   } else if (dest-&gt;is_double_cpu()) {
1575     Register res_lo = dest-&gt;as_register_lo();
1576     Register res_hi = dest-&gt;as_register_hi();
1577     Register lreg_lo = left-&gt;as_register_lo();
1578     Register lreg_hi = left-&gt;as_register_hi();
1579     if (right-&gt;is_double_cpu()) {
1580       Register rreg_lo = right-&gt;as_register_lo();
1581       Register rreg_hi = right-&gt;as_register_hi();
1582       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1583         res_lo = Rtemp;
1584       }
1585       switch (code) {
1586         case lir_add:
1587           __ adds(res_lo, lreg_lo, rreg_lo);
1588           __ adc(res_hi, lreg_hi, rreg_hi);
1589           break;
1590         case lir_sub:
1591           __ subs(res_lo, lreg_lo, rreg_lo);
1592           __ sbc(res_hi, lreg_hi, rreg_hi);
1593           break;
1594         default:
1595           ShouldNotReachHere();
1596       }
1597     } else {
1598       assert(right-&gt;is_constant(), &quot;must be&quot;);
1599       assert((right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32) == 0, &quot;out of range&quot;);
1600       const jint c = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1601       if (res_lo == lreg_hi) {
1602         res_lo = Rtemp;
1603       }
1604       switch (code) {
1605         case lir_add:
1606           __ adds(res_lo, lreg_lo, c);
1607           __ adc(res_hi, lreg_hi, 0);
1608           break;
1609         case lir_sub:
1610           __ subs(res_lo, lreg_lo, c);
1611           __ sbc(res_hi, lreg_hi, 0);
1612           break;
1613         default:
1614           ShouldNotReachHere();
1615       }
1616     }
1617     move_regs(res_lo, dest-&gt;as_register_lo());
1618 
1619   } else if (dest-&gt;is_single_fpu()) {
1620     assert(left-&gt;is_single_fpu(), &quot;must be&quot;);
1621     assert(right-&gt;is_single_fpu(), &quot;must be&quot;);
1622     const FloatRegister res = dest-&gt;as_float_reg();
1623     const FloatRegister lreg = left-&gt;as_float_reg();
1624     const FloatRegister rreg = right-&gt;as_float_reg();
1625     switch (code) {
1626       case lir_add: __ add_float(res, lreg, rreg); break;
1627       case lir_sub: __ sub_float(res, lreg, rreg); break;
1628       case lir_mul_strictfp: // fall through
1629       case lir_mul: __ mul_float(res, lreg, rreg); break;
1630       case lir_div_strictfp: // fall through
1631       case lir_div: __ div_float(res, lreg, rreg); break;
1632       default: ShouldNotReachHere();
1633     }
1634   } else if (dest-&gt;is_double_fpu()) {
1635     assert(left-&gt;is_double_fpu(), &quot;must be&quot;);
1636     assert(right-&gt;is_double_fpu(), &quot;must be&quot;);
1637     const FloatRegister res = dest-&gt;as_double_reg();
1638     const FloatRegister lreg = left-&gt;as_double_reg();
1639     const FloatRegister rreg = right-&gt;as_double_reg();
1640     switch (code) {
1641       case lir_add: __ add_double(res, lreg, rreg); break;
1642       case lir_sub: __ sub_double(res, lreg, rreg); break;
1643       case lir_mul_strictfp: // fall through
1644       case lir_mul: __ mul_double(res, lreg, rreg); break;
1645       case lir_div_strictfp: // fall through
1646       case lir_div: __ div_double(res, lreg, rreg); break;
1647       default: ShouldNotReachHere();
1648     }
1649   } else {
1650     ShouldNotReachHere();
1651   }
1652 }
1653 
1654 
1655 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1656   switch (code) {
1657     case lir_abs:
1658       __ abs_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1659       break;
1660     case lir_sqrt:
1661       __ sqrt_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1662       break;
1663     default:
1664       ShouldNotReachHere();
1665   }
1666 }
1667 
1668 
1669 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {
1670   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1671   assert(left-&gt;is_register(), &quot;wrong items state&quot;);
1672 
1673   if (dest-&gt;is_single_cpu()) {
1674 
1675     const Register res = dest-&gt;as_register();
1676     const Register lreg = left-&gt;as_register();
1677 
1678     if (right-&gt;is_single_cpu()) {
1679       const Register rreg = right-&gt;as_register();
1680       switch (code) {
1681         case lir_logic_and: __ and_32(res, lreg, rreg); break;
1682         case lir_logic_or:  __ orr_32(res, lreg, rreg); break;
1683         case lir_logic_xor: __ eor_32(res, lreg, rreg); break;
1684         default: ShouldNotReachHere();
1685       }
1686     } else {
1687       assert(right-&gt;is_constant(), &quot;must be&quot;);
1688       const uint c = (uint)right-&gt;as_constant_ptr()-&gt;as_jint();
1689       switch (code) {
1690         case lir_logic_and: __ and_32(res, lreg, c); break;
1691         case lir_logic_or:  __ orr_32(res, lreg, c); break;
1692         case lir_logic_xor: __ eor_32(res, lreg, c); break;
1693         default: ShouldNotReachHere();
1694       }
1695     }
1696   } else {
1697     assert(dest-&gt;is_double_cpu(), &quot;should be&quot;);
1698     Register res_lo = dest-&gt;as_register_lo();
1699 
1700     assert (dest-&gt;type() == T_LONG, &quot;unexpected result type&quot;);
1701     assert (left-&gt;type() == T_LONG, &quot;unexpected left type&quot;);
1702     assert (right-&gt;type() == T_LONG, &quot;unexpected right type&quot;);
1703 
1704     const Register res_hi = dest-&gt;as_register_hi();
1705     const Register lreg_lo = left-&gt;as_register_lo();
1706     const Register lreg_hi = left-&gt;as_register_hi();
1707 
1708     if (right-&gt;is_register()) {
1709       const Register rreg_lo = right-&gt;as_register_lo();
1710       const Register rreg_hi = right-&gt;as_register_hi();
1711       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1712         res_lo = Rtemp; // Temp register helps to avoid overlap between result and input
1713       }
1714       switch (code) {
1715         case lir_logic_and:
1716           __ andr(res_lo, lreg_lo, rreg_lo);
1717           __ andr(res_hi, lreg_hi, rreg_hi);
1718           break;
1719         case lir_logic_or:
1720           __ orr(res_lo, lreg_lo, rreg_lo);
1721           __ orr(res_hi, lreg_hi, rreg_hi);
1722           break;
1723         case lir_logic_xor:
1724           __ eor(res_lo, lreg_lo, rreg_lo);
1725           __ eor(res_hi, lreg_hi, rreg_hi);
1726           break;
1727         default:
1728           ShouldNotReachHere();
1729       }
1730       move_regs(res_lo, dest-&gt;as_register_lo());
1731     } else {
1732       assert(right-&gt;is_constant(), &quot;must be&quot;);
1733       const jint c_lo = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1734       const jint c_hi = (jint) (right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32);
1735       // Case for logic_or from do_ClassIDIntrinsic()
1736       if (c_hi == 0 &amp;&amp; AsmOperand::is_rotated_imm(c_lo)) {
1737         switch (code) {
1738           case lir_logic_and:
1739             __ andr(res_lo, lreg_lo, c_lo);
1740             __ mov(res_hi, 0);
1741             break;
1742           case lir_logic_or:
1743             __ orr(res_lo, lreg_lo, c_lo);
1744             break;
1745           case lir_logic_xor:
1746             __ eor(res_lo, lreg_lo, c_lo);
1747             break;
1748         default:
1749           ShouldNotReachHere();
1750         }
1751       } else if (code == lir_logic_and &amp;&amp;
1752                  c_hi == -1 &amp;&amp;
1753                  (AsmOperand::is_rotated_imm(c_lo) ||
1754                   AsmOperand::is_rotated_imm(~c_lo))) {
1755         // Another case which handles logic_and from do_ClassIDIntrinsic()
1756         if (AsmOperand::is_rotated_imm(c_lo)) {
1757           __ andr(res_lo, lreg_lo, c_lo);
1758         } else {
1759           __ bic(res_lo, lreg_lo, ~c_lo);
1760         }
1761         if (res_hi != lreg_hi) {
1762           __ mov(res_hi, lreg_hi);
1763         }
1764       } else {
1765         BAILOUT(&quot;64 bit constant cannot be inlined&quot;);
1766       }
1767     }
1768   }
1769 }
1770 
1771 
1772 
1773 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1774   if (opr1-&gt;is_single_cpu()) {
1775     if (opr2-&gt;is_constant()) {
1776       switch (opr2-&gt;as_constant_ptr()-&gt;type()) {
1777         case T_INT: {
1778           const jint c = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1779           if (Assembler::is_arith_imm_in_range(c)) {
1780             __ cmp_32(opr1-&gt;as_register(), c);
1781           } else if (Assembler::is_arith_imm_in_range(-c)) {
1782             __ cmn_32(opr1-&gt;as_register(), -c);
1783           } else {
1784             // This can happen when compiling lookupswitch
1785             __ mov_slow(Rtemp, c);
1786             __ cmp_32(opr1-&gt;as_register(), Rtemp);
1787           }
1788           break;
1789         }
1790         case T_OBJECT:
1791           assert(opr2-&gt;as_constant_ptr()-&gt;as_jobject() == NULL, &quot;cannot handle otherwise&quot;);
1792           __ cmp(opr1-&gt;as_register(), 0);
1793           break;
1794         case T_METADATA:
1795           assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;Only equality tests&quot;);
1796           assert(opr2-&gt;as_constant_ptr()-&gt;as_metadata() == NULL, &quot;cannot handle otherwise&quot;);
1797           __ cmp(opr1-&gt;as_register(), 0);
1798           break;
1799         default:
1800           ShouldNotReachHere();
1801       }
1802     } else if (opr2-&gt;is_single_cpu()) {
1803       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {
1804         assert(opr2-&gt;type() == T_OBJECT || opr2-&gt;type() == T_ARRAY, &quot;incompatibe type&quot;);
1805         __ cmpoop(opr1-&gt;as_register(), opr2-&gt;as_register());
1806       } else if (opr1-&gt;type() == T_METADATA || opr1-&gt;type() == T_ADDRESS) {
1807         assert(opr2-&gt;type() == T_METADATA || opr2-&gt;type() == T_ADDRESS, &quot;incompatibe type&quot;);
1808         __ cmp(opr1-&gt;as_register(), opr2-&gt;as_register());
1809       } else {
1810         assert(opr2-&gt;type() != T_OBJECT &amp;&amp; opr2-&gt;type() != T_ARRAY &amp;&amp; opr2-&gt;type() != T_METADATA &amp;&amp; opr2-&gt;type() != T_ADDRESS, &quot;incompatibe type&quot;);
1811         __ cmp_32(opr1-&gt;as_register(), opr2-&gt;as_register());
1812       }
1813     } else {
1814       ShouldNotReachHere();
1815     }
1816   } else if (opr1-&gt;is_double_cpu()) {
1817     Register xlo = opr1-&gt;as_register_lo();
1818     Register xhi = opr1-&gt;as_register_hi();
1819     if (opr2-&gt;is_constant() &amp;&amp; opr2-&gt;as_jlong() == 0) {
1820       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;cannot handle otherwise&quot;);
1821       __ orrs(Rtemp, xlo, xhi);
1822     } else if (opr2-&gt;is_register()) {
1823       Register ylo = opr2-&gt;as_register_lo();
1824       Register yhi = opr2-&gt;as_register_hi();
1825       if (condition == lir_cond_equal || condition == lir_cond_notEqual) {
1826         __ teq(xhi, yhi);
1827         __ teq(xlo, ylo, eq);
1828       } else {
1829         __ subs(xlo, xlo, ylo);
1830         __ sbcs(xhi, xhi, yhi);
1831       }
1832     } else {
1833       ShouldNotReachHere();
1834     }
1835   } else if (opr1-&gt;is_single_fpu()) {
1836     if (opr2-&gt;is_constant()) {
1837       assert(opr2-&gt;as_jfloat() == 0.0f, &quot;cannot handle otherwise&quot;);
1838       __ cmp_zero_float(opr1-&gt;as_float_reg());
1839     } else {
1840       __ cmp_float(opr1-&gt;as_float_reg(), opr2-&gt;as_float_reg());
1841     }
1842   } else if (opr1-&gt;is_double_fpu()) {
1843     if (opr2-&gt;is_constant()) {
1844       assert(opr2-&gt;as_jdouble() == 0.0, &quot;cannot handle otherwise&quot;);
1845       __ cmp_zero_double(opr1-&gt;as_double_reg());
1846     } else {
1847       __ cmp_double(opr1-&gt;as_double_reg(), opr2-&gt;as_double_reg());
1848     }
1849   } else {
1850     ShouldNotReachHere();
1851   }
1852 }
1853 
1854 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
1855   const Register res = dst-&gt;as_register();
1856   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
1857     comp_op(lir_cond_unknown, left, right, op);
1858     __ fmstat();
1859     if (code == lir_ucmp_fd2i) {  // unordered is less
1860       __ mvn(res, 0, lt);
1861       __ mov(res, 1, ge);
1862     } else {                      // unordered is greater
1863       __ mov(res, 1, cs);
1864       __ mvn(res, 0, cc);
1865     }
1866     __ mov(res, 0, eq);
1867 
1868   } else {
1869     assert(code == lir_cmp_l2i, &quot;must be&quot;);
1870 
1871     Label done;
1872     const Register xlo = left-&gt;as_register_lo();
1873     const Register xhi = left-&gt;as_register_hi();
1874     const Register ylo = right-&gt;as_register_lo();
1875     const Register yhi = right-&gt;as_register_hi();
1876     __ cmp(xhi, yhi);
1877     __ mov(res, 1, gt);
1878     __ mvn(res, 0, lt);
1879     __ b(done, ne);
1880     __ subs(res, xlo, ylo);
1881     __ mov(res, 1, hi);
1882     __ mvn(res, 0, lo);
1883     __ bind(done);
1884   }
1885 }
1886 
1887 
1888 void LIR_Assembler::align_call(LIR_Code code) {
1889   // Not needed
1890 }
1891 
1892 
1893 void LIR_Assembler::call(LIR_OpJavaCall *op, relocInfo::relocType rtype) {
1894   int ret_addr_offset = __ patchable_call(op-&gt;addr(), rtype);
1895   assert(ret_addr_offset == __ offset(), &quot;embedded return address not allowed&quot;);
1896   add_call_info_here(op-&gt;info());
1897 }
1898 
1899 
1900 void LIR_Assembler::ic_call(LIR_OpJavaCall *op) {
1901   bool near_range = __ cache_fully_reachable();
1902   address oop_address = pc();
1903 
1904   bool use_movw = VM_Version::supports_movw();
1905 
1906   // Ricklass may contain something that is not a metadata pointer so
1907   // mov_metadata can&#39;t be used
1908   InlinedAddress value((address)Universe::non_oop_word());
1909   InlinedAddress addr(op-&gt;addr());
1910   if (use_movw) {
1911     __ movw(Ricklass, ((unsigned int)Universe::non_oop_word()) &amp; 0xffff);
1912     __ movt(Ricklass, ((unsigned int)Universe::non_oop_word()) &gt;&gt; 16);
1913   } else {
1914     // No movw/movt, must be load a pc relative value but no
1915     // relocation so no metadata table to load from.
1916     // Use a b instruction rather than a bl, inline constant after the
1917     // branch, use a PC relative ldr to load the constant, arrange for
1918     // the call to return after the constant(s).
1919     __ ldr_literal(Ricklass, value);
1920   }
1921   __ relocate(virtual_call_Relocation::spec(oop_address));
1922   if (near_range &amp;&amp; use_movw) {
1923     __ bl(op-&gt;addr());
1924   } else {
1925     Label call_return;
1926     __ adr(LR, call_return);
1927     if (near_range) {
1928       __ b(op-&gt;addr());
1929     } else {
1930       __ indirect_jump(addr, Rtemp);
1931       __ bind_literal(addr);
1932     }
1933     if (!use_movw) {
1934       __ bind_literal(value);
1935     }
1936     __ bind(call_return);
1937   }
1938   add_call_info(code_offset(), op-&gt;info());
1939 }
1940 
1941 
1942 /* Currently, vtable-dispatch is only enabled for sparc platforms */
1943 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
1944   ShouldNotReachHere();
1945 }
1946 
1947 void LIR_Assembler::emit_static_call_stub() {
1948   address call_pc = __ pc();
1949   address stub = __ start_a_stub(call_stub_size());
1950   if (stub == NULL) {
1951     BAILOUT(&quot;static call stub overflow&quot;);
1952   }
1953 
1954   DEBUG_ONLY(int offset = code_offset();)
1955 
1956   InlinedMetadata metadata_literal(NULL);
1957   __ relocate(static_stub_Relocation::spec(call_pc));
1958   // If not a single instruction, NativeMovConstReg::next_instruction_address()
1959   // must jump over the whole following ldr_literal.
1960   // (See CompiledStaticCall::set_to_interpreted())
1961 #ifdef ASSERT
1962   address ldr_site = __ pc();
1963 #endif
1964   __ ldr_literal(Rmethod, metadata_literal);
1965   assert(nativeMovConstReg_at(ldr_site)-&gt;next_instruction_address() == __ pc(), &quot;Fix ldr_literal or its parsing&quot;);
1966   bool near_range = __ cache_fully_reachable();
1967   InlinedAddress dest((address)-1);
1968   if (near_range) {
1969     address branch_site = __ pc();
1970     __ b(branch_site); // b to self maps to special NativeJump -1 destination
1971   } else {
1972     __ indirect_jump(dest, Rtemp);
1973   }
1974   __ bind_literal(metadata_literal); // includes spec_for_immediate reloc
1975   if (!near_range) {
1976     __ bind_literal(dest); // special NativeJump -1 destination
1977   }
1978 
1979   assert(code_offset() - offset &lt;= call_stub_size(), &quot;overflow&quot;);
1980   __ end_a_stub();
1981 }
1982 
1983 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
1984   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
1985   assert(exceptionPC-&gt;as_register()  == Rexception_pc, &quot;must match&quot;);
1986   info-&gt;add_register_oop(exceptionOop);
1987 
1988   Runtime1::StubID handle_id = compilation()-&gt;has_fpu_code() ?
1989                                Runtime1::handle_exception_id :
1990                                Runtime1::handle_exception_nofpu_id;
1991   Label return_address;
1992   __ adr(Rexception_pc, return_address);
1993   __ call(Runtime1::entry_for(handle_id), relocInfo::runtime_call_type);
1994   __ bind(return_address);
1995   add_call_info_here(info);  // for exception handler
1996 }
1997 
1998 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
1999   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
2000   __ b(_unwind_handler_entry);
2001 }
2002 
2003 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2004   AsmShift shift = lsl;
2005   switch (code) {
2006     case lir_shl:  shift = lsl; break;
2007     case lir_shr:  shift = asr; break;
2008     case lir_ushr: shift = lsr; break;
2009     default: ShouldNotReachHere();
2010   }
2011 
2012   if (dest-&gt;is_single_cpu()) {
2013     __ andr(Rtemp, count-&gt;as_register(), 31);
2014     __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, Rtemp));
2015   } else if (dest-&gt;is_double_cpu()) {
2016     Register dest_lo = dest-&gt;as_register_lo();
2017     Register dest_hi = dest-&gt;as_register_hi();
2018     Register src_lo  = left-&gt;as_register_lo();
2019     Register src_hi  = left-&gt;as_register_hi();
2020     Register Rcount  = count-&gt;as_register();
2021     // Resolve possible register conflicts
2022     if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2023       dest_hi = Rtemp;
2024     } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2025       dest_lo = Rtemp;
2026     } else if (dest_lo == src_lo &amp;&amp; dest_hi == src_hi) {
2027       dest_lo = Rtemp;
2028     } else if (dest_lo == Rcount || dest_hi == Rcount) {
2029       Rcount = Rtemp;
2030     }
2031     __ andr(Rcount, count-&gt;as_register(), 63);
2032     __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, Rcount);
2033     move_regs(dest_lo, dest-&gt;as_register_lo());
2034     move_regs(dest_hi, dest-&gt;as_register_hi());
2035   } else {
2036     ShouldNotReachHere();
2037   }
2038 }
2039 
2040 
2041 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2042   AsmShift shift = lsl;
2043   switch (code) {
2044     case lir_shl:  shift = lsl; break;
2045     case lir_shr:  shift = asr; break;
2046     case lir_ushr: shift = lsr; break;
2047     default: ShouldNotReachHere();
2048   }
2049 
2050   if (dest-&gt;is_single_cpu()) {
2051     count &amp;= 31;
2052     if (count != 0) {
2053       __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, count));
2054     } else {
2055       move_regs(left-&gt;as_register(), dest-&gt;as_register());
2056     }
2057   } else if (dest-&gt;is_double_cpu()) {
2058     count &amp;= 63;
2059     if (count != 0) {
2060       Register dest_lo = dest-&gt;as_register_lo();
2061       Register dest_hi = dest-&gt;as_register_hi();
2062       Register src_lo  = left-&gt;as_register_lo();
2063       Register src_hi  = left-&gt;as_register_hi();
2064       // Resolve possible register conflicts
2065       if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2066         dest_hi = Rtemp;
2067       } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2068         dest_lo = Rtemp;
2069       }
2070       __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, count);
2071       move_regs(dest_lo, dest-&gt;as_register_lo());
2072       move_regs(dest_hi, dest-&gt;as_register_hi());
2073     } else {
2074       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(),
2075                    left-&gt;as_register_lo(), left-&gt;as_register_hi());
2076     }
2077   } else {
2078     ShouldNotReachHere();
2079   }
2080 }
2081 
2082 
2083 // Saves 4 given registers in reserved argument area.
2084 void LIR_Assembler::save_in_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2085   verify_reserved_argument_area_size(4);
2086   __ stmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4));
2087 }
2088 
2089 // Restores 4 given registers from reserved argument area.
2090 void LIR_Assembler::restore_from_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2091   __ ldmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4), no_writeback);
2092 }
2093 
2094 
2095 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2096   ciArrayKlass* default_type = op-&gt;expected_type();
2097   Register src = op-&gt;src()-&gt;as_register();
2098   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2099   Register dst = op-&gt;dst()-&gt;as_register();
2100   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2101   Register length  = op-&gt;length()-&gt;as_register();
2102   Register tmp = op-&gt;tmp()-&gt;as_register();
2103   Register tmp2 = Rtemp;
2104 
2105   assert(src == R0 &amp;&amp; src_pos == R1 &amp;&amp; dst == R2 &amp;&amp; dst_pos == R3, &quot;code assumption&quot;);
2106 
2107   __ resolve(ACCESS_READ, src);
2108   __ resolve(ACCESS_WRITE, dst);
2109 
2110   CodeStub* stub = op-&gt;stub();
2111 
2112   int flags = op-&gt;flags();
2113   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2114   if (basic_type == T_ARRAY) basic_type = T_OBJECT;
2115 
2116   // If we don&#39;t know anything or it&#39;s an object array, just go through the generic arraycopy
2117   if (default_type == NULL) {
2118 
2119     // save arguments, because they will be killed by a runtime call
2120     save_in_reserved_area(R0, R1, R2, R3);
2121 
2122     // pass length argument on SP[0]
2123     __ str(length, Address(SP, -2*wordSize, pre_indexed));  // 2 words for a proper stack alignment
2124 
2125     address copyfunc_addr = StubRoutines::generic_arraycopy();
2126     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2127 #ifndef PRODUCT
2128     if (PrintC1Statistics) {
2129       __ inc_counter((address)&amp;Runtime1::_generic_arraycopystub_cnt, tmp, tmp2);
2130     }
2131 #endif // !PRODUCT
2132     // the stub is in the code cache so close enough
2133     __ call(copyfunc_addr, relocInfo::runtime_call_type);
2134 
2135     __ add(SP, SP, 2*wordSize);
2136 
2137     __ cbz_32(R0, *stub-&gt;continuation());
2138 
2139     __ mvn_32(tmp, R0);
2140     restore_from_reserved_area(R0, R1, R2, R3);  // load saved arguments in slow case only
2141     __ sub_32(length, length, tmp);
2142     __ add_32(src_pos, src_pos, tmp);
2143     __ add_32(dst_pos, dst_pos, tmp);
2144 
2145     __ b(*stub-&gt;entry());
2146 
2147     __ bind(*stub-&gt;continuation());
2148     return;
2149   }
2150 
2151   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(),
2152          &quot;must be true at this point&quot;);
2153   int elem_size = type2aelembytes(basic_type);
2154   int shift = exact_log2(elem_size);
2155 
2156   // Check for NULL
2157   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2158     if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2159       __ cmp(src, 0);
2160       __ cond_cmp(dst, 0, ne);  // make one instruction shorter if both checks are needed
2161       __ b(*stub-&gt;entry(), eq);
2162     } else {
2163       __ cbz(src, *stub-&gt;entry());
2164     }
2165   } else if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2166     __ cbz(dst, *stub-&gt;entry());
2167   }
2168 
2169   // If the compiler was not able to prove that exact type of the source or the destination
2170   // of the arraycopy is an array type, check at runtime if the source or the destination is
2171   // an instance type.
2172   if (flags &amp; LIR_OpArrayCopy::type_check) {
2173     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2174       __ load_klass(tmp, dst);
2175       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2176       __ mov_slow(tmp, Klass::_lh_neutral_value);
2177       __ cmp_32(tmp2, tmp);
2178       __ b(*stub-&gt;entry(), ge);
2179     }
2180 
2181     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2182       __ load_klass(tmp, src);
2183       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2184       __ mov_slow(tmp, Klass::_lh_neutral_value);
2185       __ cmp_32(tmp2, tmp);
2186       __ b(*stub-&gt;entry(), ge);
2187     }
2188   }
2189 
2190   // Check if negative
2191   const int all_positive_checks = LIR_OpArrayCopy::src_pos_positive_check |
2192                                   LIR_OpArrayCopy::dst_pos_positive_check |
2193                                   LIR_OpArrayCopy::length_positive_check;
2194   switch (flags &amp; all_positive_checks) {
2195     case LIR_OpArrayCopy::src_pos_positive_check:
2196       __ branch_if_negative_32(src_pos, *stub-&gt;entry());
2197       break;
2198     case LIR_OpArrayCopy::dst_pos_positive_check:
2199       __ branch_if_negative_32(dst_pos, *stub-&gt;entry());
2200       break;
2201     case LIR_OpArrayCopy::length_positive_check:
2202       __ branch_if_negative_32(length, *stub-&gt;entry());
2203       break;
2204     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::dst_pos_positive_check:
2205       __ branch_if_any_negative_32(src_pos, dst_pos, tmp, *stub-&gt;entry());
2206       break;
2207     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2208       __ branch_if_any_negative_32(src_pos, length, tmp, *stub-&gt;entry());
2209       break;
2210     case LIR_OpArrayCopy::dst_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2211       __ branch_if_any_negative_32(dst_pos, length, tmp, *stub-&gt;entry());
2212       break;
2213     case all_positive_checks:
2214       __ branch_if_any_negative_32(src_pos, dst_pos, length, tmp, *stub-&gt;entry());
2215       break;
2216     default:
2217       assert((flags &amp; all_positive_checks) == 0, &quot;the last option&quot;);
2218   }
2219 
2220   // Range checks
2221   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2222     __ ldr_s32(tmp2, Address(src, arrayOopDesc::length_offset_in_bytes()));
2223     __ add_32(tmp, src_pos, length);
2224     __ cmp_32(tmp, tmp2);
2225     __ b(*stub-&gt;entry(), hi);
2226   }
2227   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2228     __ ldr_s32(tmp2, Address(dst, arrayOopDesc::length_offset_in_bytes()));
2229     __ add_32(tmp, dst_pos, length);
2230     __ cmp_32(tmp, tmp2);
2231     __ b(*stub-&gt;entry(), hi);
2232   }
2233 
2234   // Check if src and dst are of the same type
2235   if (flags &amp; LIR_OpArrayCopy::type_check) {
2236     // We don&#39;t know the array types are compatible
2237     if (basic_type != T_OBJECT) {
2238       // Simple test for basic type arrays
2239       if (UseCompressedClassPointers) {
2240         // We don&#39;t need decode because we just need to compare
2241         __ ldr_u32(tmp, Address(src, oopDesc::klass_offset_in_bytes()));
2242         __ ldr_u32(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));
2243         __ cmp_32(tmp, tmp2);
2244       } else {
2245         __ load_klass(tmp, src);
2246         __ load_klass(tmp2, dst);
2247         __ cmp(tmp, tmp2);
2248       }
2249       __ b(*stub-&gt;entry(), ne);
2250     } else {
2251       // For object arrays, if src is a sub class of dst then we can
2252       // safely do the copy.
2253       Label cont, slow;
2254 
2255       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2256 
2257       __ load_klass(tmp, src);
2258       __ load_klass(tmp2, dst);
2259 
2260       // We are at a call so all live registers are saved before we
2261       // get here
2262       assert_different_registers(tmp, tmp2, R6, altFP_7_11);
2263 
2264       __ check_klass_subtype_fast_path(tmp, tmp2, R6, altFP_7_11, &amp;cont, copyfunc_addr == NULL ? stub-&gt;entry() : &amp;slow, NULL);
2265 
2266       __ mov(R6, R0);
2267       __ mov(altFP_7_11, R1);
2268       __ mov(R0, tmp);
2269       __ mov(R1, tmp2);
2270       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type); // does not blow any registers except R0, LR and Rtemp
2271       __ cmp_32(R0, 0);
2272       __ mov(R0, R6);
2273       __ mov(R1, altFP_7_11);
2274 
2275       if (copyfunc_addr != NULL) { // use stub if available
2276         // src is not a sub class of dst so we have to do a
2277         // per-element check.
2278 
2279         __ b(cont, ne);
2280 
2281         __ bind(slow);
2282 
2283         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2284         if ((flags &amp; mask) != mask) {
2285           // Check that at least both of them object arrays.
2286           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2287 
2288           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2289             __ load_klass(tmp, src);
2290           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2291             __ load_klass(tmp, dst);
2292           }
2293           int lh_offset = in_bytes(Klass::layout_helper_offset());
2294 
2295           __ ldr_u32(tmp2, Address(tmp, lh_offset));
2296 
2297           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2298           __ mov_slow(tmp, objArray_lh);
2299           __ cmp_32(tmp, tmp2);
2300           __ b(*stub-&gt;entry(), ne);
2301         }
2302 
2303         save_in_reserved_area(R0, R1, R2, R3);
2304 
2305         Register src_ptr = R0;
2306         Register dst_ptr = R1;
2307         Register len     = R2;
2308         Register chk_off = R3;
2309         Register super_k = tmp;
2310 
2311         __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2312         __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2313 
2314         __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2315         __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2316         __ load_klass(tmp, dst);
2317 
2318         int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2319         int sco_offset = in_bytes(Klass::super_check_offset_offset());
2320 
2321         __ ldr(super_k, Address(tmp, ek_offset));
2322 
2323         __ mov(len, length);
2324         __ ldr_u32(chk_off, Address(super_k, sco_offset));
2325         __ push(super_k);
2326 
2327         __ call(copyfunc_addr, relocInfo::runtime_call_type);
2328 
2329 #ifndef PRODUCT
2330         if (PrintC1Statistics) {
2331           Label failed;
2332           __ cbnz_32(R0, failed);
2333           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_cnt, tmp, tmp2);
2334           __ bind(failed);
2335         }
2336 #endif // PRODUCT
2337 
2338         __ add(SP, SP, wordSize);  // Drop super_k argument
2339 
2340         __ cbz_32(R0, *stub-&gt;continuation());
2341         __ mvn_32(tmp, R0);
2342 
2343         // load saved arguments in slow case only
2344         restore_from_reserved_area(R0, R1, R2, R3);
2345 
2346         __ sub_32(length, length, tmp);
2347         __ add_32(src_pos, src_pos, tmp);
2348         __ add_32(dst_pos, dst_pos, tmp);
2349 
2350 #ifndef PRODUCT
2351         if (PrintC1Statistics) {
2352           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt, tmp, tmp2);
2353         }
2354 #endif
2355 
2356         __ b(*stub-&gt;entry());
2357 
2358         __ bind(cont);
2359       } else {
2360         __ b(*stub-&gt;entry(), eq);
2361         __ bind(cont);
2362       }
2363     }
2364   }
2365 
2366 #ifndef PRODUCT
2367   if (PrintC1Statistics) {
2368     address counter = Runtime1::arraycopy_count_address(basic_type);
2369     __ inc_counter(counter, tmp, tmp2);
2370   }
2371 #endif // !PRODUCT
2372 
2373   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2374   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2375   const char *name;
2376   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2377 
2378   Register src_ptr = R0;
2379   Register dst_ptr = R1;
2380   Register len     = R2;
2381 
2382   __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2383   __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2384 
2385   __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2386   __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2387 
2388   __ mov(len, length);
2389 
2390   __ call(entry, relocInfo::runtime_call_type);
2391 
2392   __ bind(*stub-&gt;continuation());
2393 }
2394 
2395 #ifdef ASSERT
2396  // emit run-time assertion
2397 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
2398   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
2399 
2400   if (op-&gt;in_opr1()-&gt;is_valid()) {
2401     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
2402     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
2403   } else {
2404     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
2405     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
2406   }
2407 
2408   Label ok;
2409   if (op-&gt;condition() != lir_cond_always) {
2410     AsmCondition acond = al;
2411     switch (op-&gt;condition()) {
2412       case lir_cond_equal:        acond = eq; break;
2413       case lir_cond_notEqual:     acond = ne; break;
2414       case lir_cond_less:         acond = lt; break;
2415       case lir_cond_lessEqual:    acond = le; break;
2416       case lir_cond_greaterEqual: acond = ge; break;
2417       case lir_cond_greater:      acond = gt; break;
2418       case lir_cond_aboveEqual:   acond = hs; break;
2419       case lir_cond_belowEqual:   acond = ls; break;
2420       default:                    ShouldNotReachHere();
2421     }
2422     __ b(ok, acond);
2423   }
2424   if (op-&gt;halt()) {
2425     const char* str = __ code_string(op-&gt;msg());
2426     __ stop(str);
2427   } else {
2428     breakpoint();
2429   }
2430   __ bind(ok);
2431 }
2432 #endif // ASSERT
2433 
2434 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2435   fatal(&quot;CRC32 intrinsic is not implemented on this platform&quot;);
2436 }
2437 
2438 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2439   Register obj = op-&gt;obj_opr()-&gt;as_pointer_register();
2440   Register hdr = op-&gt;hdr_opr()-&gt;as_pointer_register();
2441   Register lock = op-&gt;lock_opr()-&gt;as_pointer_register();
2442   Register tmp = op-&gt;scratch_opr()-&gt;is_illegal() ? noreg :
2443                  op-&gt;scratch_opr()-&gt;as_pointer_register();
2444 
2445   if (!UseFastLocking) {
2446     __ b(*op-&gt;stub()-&gt;entry());
2447   } else if (op-&gt;code() == lir_lock) {
2448     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2449     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2450     int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2451     if (op-&gt;info() != NULL) {
2452       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2453     }
2454   } else if (op-&gt;code() == lir_unlock) {
2455     __ unlock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2456   } else {
2457     ShouldNotReachHere();
2458   }
2459   __ bind(*op-&gt;stub()-&gt;continuation());
2460 }
2461 
2462 
2463 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2464   ciMethod* method = op-&gt;profiled_method();
2465   int bci          = op-&gt;profiled_bci();
2466   ciMethod* callee = op-&gt;profiled_callee();
2467 
2468   // Update counter for all call types
2469   ciMethodData* md = method-&gt;method_data_or_null();
2470   assert(md != NULL, &quot;Sanity&quot;);
2471   ciProfileData* data = md-&gt;bci_to_data(bci);
2472   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2473   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2474   Register mdo  = op-&gt;mdo()-&gt;as_register();
2475   assert(op-&gt;tmp1()-&gt;is_register(), &quot;tmp1 must be allocated&quot;);
2476   Register tmp1 = op-&gt;tmp1()-&gt;as_pointer_register();
2477   assert_different_registers(mdo, tmp1);
2478   __ mov_metadata(mdo, md-&gt;constant_encoding());
2479   int mdo_offset_bias = 0;
2480   int max_offset = 4096;
2481   if (md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) + data-&gt;size_in_bytes() &gt;= max_offset) {
2482     // The offset is large so bias the mdo by the base of the slot so
2483     // that the ldr can use an immediate offset to reference the slots of the data
2484     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, CounterData::count_offset());
2485     __ mov_slow(tmp1, mdo_offset_bias);
2486     __ add(mdo, mdo, tmp1);
2487   }
2488 
2489   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2490   // Perform additional virtual call profiling for invokevirtual and
2491   // invokeinterface bytecodes
2492   if (op-&gt;should_profile_receiver_type()) {
2493     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2494     Register recv = op-&gt;recv()-&gt;as_register();
2495     assert_different_registers(mdo, tmp1, recv);
2496     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2497     ciKlass* known_klass = op-&gt;known_holder();
2498     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2499       // We know the type that will be seen at this call site; we can
2500       // statically update the MethodData* rather than needing to do
2501       // dynamic tests on the receiver type
2502 
2503       // NOTE: we should probably put a lock around this search to
2504       // avoid collisions by concurrent compilations
2505       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2506       uint i;
2507       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2508         ciKlass* receiver = vc_data-&gt;receiver(i);
2509         if (known_klass-&gt;equals(receiver)) {
2510           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data,
2511                                                          VirtualCallData::receiver_count_offset(i)) -
2512                             mdo_offset_bias);
2513           __ ldr(tmp1, data_addr);
2514           __ add(tmp1, tmp1, DataLayout::counter_increment);
2515           __ str(tmp1, data_addr);
2516           return;
2517         }
2518       }
2519 
2520       // Receiver type not found in profile data; select an empty slot
2521 
2522       // Note that this is less efficient than it should be because it
2523       // always does a write to the receiver part of the
2524       // VirtualCallData rather than just the first time
2525       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2526         ciKlass* receiver = vc_data-&gt;receiver(i);
2527         if (receiver == NULL) {
2528           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)) -
2529                             mdo_offset_bias);
2530           __ mov_metadata(tmp1, known_klass-&gt;constant_encoding());
2531           __ str(tmp1, recv_addr);
2532           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)) -
2533                             mdo_offset_bias);
2534           __ ldr(tmp1, data_addr);
2535           __ add(tmp1, tmp1, DataLayout::counter_increment);
2536           __ str(tmp1, data_addr);
2537           return;
2538         }
2539       }
2540     } else {
2541       __ load_klass(recv, recv);
2542       Label update_done;
2543       type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, &amp;update_done);
2544       // Receiver did not match any saved receiver and there is no empty row for it.
2545       // Increment total counter to indicate polymorphic case.
2546       __ ldr(tmp1, counter_addr);
2547       __ add(tmp1, tmp1, DataLayout::counter_increment);
2548       __ str(tmp1, counter_addr);
2549 
2550       __ bind(update_done);
2551     }
2552   } else {
2553     // Static call
2554     __ ldr(tmp1, counter_addr);
2555     __ add(tmp1, tmp1, DataLayout::counter_increment);
2556     __ str(tmp1, counter_addr);
2557   }
2558 }
2559 
2560 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2561   fatal(&quot;Type profiling not implemented on this platform&quot;);
2562 }
2563 
2564 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2565   Unimplemented();
2566 }
2567 
2568 
2569 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2570   Address mon_addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
2571   __ add_slow(dst-&gt;as_pointer_register(), mon_addr.base(), mon_addr.disp());
2572 }
2573 
2574 
2575 void LIR_Assembler::align_backward_branch_target() {
2576   // Some ARM processors do better with 8-byte branch target alignment
2577   __ align(8);
2578 }
2579 
2580 
2581 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
2582   // tmp must be unused
2583   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2584 
2585   if (left-&gt;is_single_cpu()) {
2586     assert (dest-&gt;type() == T_INT, &quot;unexpected result type&quot;);
2587     assert (left-&gt;type() == T_INT, &quot;unexpected left type&quot;);
2588     __ neg_32(dest-&gt;as_register(), left-&gt;as_register());
2589   } else if (left-&gt;is_double_cpu()) {
2590     Register dest_lo = dest-&gt;as_register_lo();
2591     Register dest_hi = dest-&gt;as_register_hi();
2592     Register src_lo = left-&gt;as_register_lo();
2593     Register src_hi = left-&gt;as_register_hi();
2594     if (dest_lo == src_hi) {
2595       dest_lo = Rtemp;
2596     }
2597     __ rsbs(dest_lo, src_lo, 0);
2598     __ rsc(dest_hi, src_hi, 0);
2599     move_regs(dest_lo, dest-&gt;as_register_lo());
2600   } else if (left-&gt;is_single_fpu()) {
2601     __ neg_float(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2602   } else if (left-&gt;is_double_fpu()) {
2603     __ neg_double(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2604   } else {
2605     ShouldNotReachHere();
2606   }
2607 }
2608 
2609 
2610 void LIR_Assembler::leal(LIR_Opr addr_opr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
2611   assert(patch_code == lir_patch_none, &quot;Patch code not supported&quot;);
2612   LIR_Address* addr = addr_opr-&gt;as_address_ptr();
2613   if (addr-&gt;index()-&gt;is_illegal()) {
2614     jint c = addr-&gt;disp();
2615     if (!Assembler::is_arith_imm_in_range(c)) {
2616       BAILOUT(&quot;illegal arithmetic operand&quot;);
2617     }
2618     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(), c);
2619   } else {
2620     assert(addr-&gt;disp() == 0, &quot;cannot handle otherwise&quot;);
2621     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(),
2622            AsmOperand(addr-&gt;index()-&gt;as_pointer_register(), lsl, addr-&gt;scale()));
2623   }
2624 }
2625 
2626 
2627 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2628   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2629   __ call(dest);
2630   if (info != NULL) {
2631     add_call_info_here(info);
2632   }
2633 }
2634 
2635 
2636 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2637   assert(src-&gt;is_double_cpu() &amp;&amp; dest-&gt;is_address() ||
2638          src-&gt;is_address() &amp;&amp; dest-&gt;is_double_cpu(),
2639          &quot;Simple move_op is called for all other cases&quot;);
2640 
2641   int null_check_offset;
2642   if (dest-&gt;is_address()) {
2643     // Store
2644     const LIR_Address* addr = dest-&gt;as_address_ptr();
2645     const Register src_lo = src-&gt;as_register_lo();
2646     const Register src_hi = src-&gt;as_register_hi();
2647     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2648 
2649     if (src_lo &lt; src_hi) {
2650       null_check_offset = __ offset();
2651       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(src_hi));
2652     } else {
2653       assert(src_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2654       __ mov(Rtemp, src_hi);
2655       null_check_offset = __ offset();
2656       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(Rtemp));
2657     }
2658   } else {
2659     // Load
2660     const LIR_Address* addr = src-&gt;as_address_ptr();
2661     const Register dest_lo = dest-&gt;as_register_lo();
2662     const Register dest_hi = dest-&gt;as_register_hi();
2663     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2664 
2665     null_check_offset = __ offset();
2666     if (dest_lo &lt; dest_hi) {
2667       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(dest_hi));
2668     } else {
2669       assert(dest_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2670       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(Rtemp));
2671       __ mov(dest_hi, Rtemp);
2672     }
2673   }
2674 
2675   if (info != NULL) {
2676     add_debug_info_for_null_check(null_check_offset, info);
2677   }
2678 }
2679 
2680 
2681 void LIR_Assembler::membar() {
2682   __ membar(MacroAssembler::StoreLoad, Rtemp);
2683 }
2684 
2685 void LIR_Assembler::membar_acquire() {
2686   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
2687 }
2688 
2689 void LIR_Assembler::membar_release() {
2690   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2691 }
2692 
2693 void LIR_Assembler::membar_loadload() {
2694   __ membar(MacroAssembler::LoadLoad, Rtemp);
2695 }
2696 
2697 void LIR_Assembler::membar_storestore() {
2698   __ membar(MacroAssembler::StoreStore, Rtemp);
2699 }
2700 
2701 void LIR_Assembler::membar_loadstore() {
2702   __ membar(MacroAssembler::LoadStore, Rtemp);
2703 }
2704 
2705 void LIR_Assembler::membar_storeload() {
2706   __ membar(MacroAssembler::StoreLoad, Rtemp);
2707 }
2708 
2709 void LIR_Assembler::on_spin_wait() {
2710   Unimplemented();
2711 }
2712 
2713 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
2714   // Not used on ARM
2715   Unimplemented();
2716 }
2717 
2718 void LIR_Assembler::peephole(LIR_List* lir) {
2719   LIR_OpList* inst = lir-&gt;instructions_list();
2720   const int inst_length = inst-&gt;length();
2721   for (int i = 0; i &lt; inst_length; i++) {
2722     LIR_Op* op = inst-&gt;at(i);
2723     switch (op-&gt;code()) {
2724       case lir_cmp: {
2725         // Replace:
2726         //   cmp rX, y
2727         //   cmove [EQ] y, z, rX
2728         // with
2729         //   cmp rX, y
2730         //   cmove [EQ] illegalOpr, z, rX
2731         //
2732         // or
2733         //   cmp rX, y
2734         //   cmove [NE] z, y, rX
2735         // with
2736         //   cmp rX, y
2737         //   cmove [NE] z, illegalOpr, rX
2738         //
2739         // moves from illegalOpr should be removed when converting LIR to native assembly
2740 
2741         LIR_Op2* cmp = op-&gt;as_Op2();
2742         assert(cmp != NULL, &quot;cmp LIR instruction is not an op2&quot;);
2743 
2744         if (i + 1 &lt; inst_length) {
2745           LIR_Op2* cmove = inst-&gt;at(i + 1)-&gt;as_Op2();
2746           if (cmove != NULL &amp;&amp; cmove-&gt;code() == lir_cmove) {
2747             LIR_Opr cmove_res = cmove-&gt;result_opr();
2748             bool res_is_op1 = cmove_res == cmp-&gt;in_opr1();
2749             bool res_is_op2 = cmove_res == cmp-&gt;in_opr2();
2750             LIR_Opr cmp_res, cmp_arg;
2751             if (res_is_op1) {
2752               cmp_res = cmp-&gt;in_opr1();
2753               cmp_arg = cmp-&gt;in_opr2();
2754             } else if (res_is_op2) {
2755               cmp_res = cmp-&gt;in_opr2();
2756               cmp_arg = cmp-&gt;in_opr1();
2757             } else {
2758               cmp_res = LIR_OprFact::illegalOpr;
2759               cmp_arg = LIR_OprFact::illegalOpr;
2760             }
2761 
2762             if (cmp_res != LIR_OprFact::illegalOpr) {
2763               LIR_Condition cond = cmove-&gt;condition();
2764               if (cond == lir_cond_equal &amp;&amp; cmove-&gt;in_opr1() == cmp_arg) {
2765                 cmove-&gt;set_in_opr1(LIR_OprFact::illegalOpr);
2766               } else if (cond == lir_cond_notEqual &amp;&amp; cmove-&gt;in_opr2() == cmp_arg) {
2767                 cmove-&gt;set_in_opr2(LIR_OprFact::illegalOpr);
2768               }
2769             }
2770           }
2771         }
2772         break;
2773       }
2774 
2775       default:
2776         break;
2777     }
2778   }
2779 }
2780 
2781 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
2782   assert(src-&gt;is_address(), &quot;sanity&quot;);
2783   Address addr = as_Address(src-&gt;as_address_ptr());
2784 
2785   if (code == lir_xchg) {
2786   } else {
2787     assert (!data-&gt;is_oop(), &quot;xadd for oops&quot;);
2788   }
2789 
2790   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2791 
2792   Label retry;
2793   __ bind(retry);
2794 
2795   if (data-&gt;type() == T_INT || data-&gt;is_oop()) {
2796     Register dst = dest-&gt;as_register();
2797     Register new_val = noreg;
2798     __ ldrex(dst, addr);
2799     if (code == lir_xadd) {
2800       Register tmp_reg = tmp-&gt;as_register();
2801       if (data-&gt;is_constant()) {
2802         assert_different_registers(dst, tmp_reg);
2803         __ add_32(tmp_reg, dst, data-&gt;as_constant_ptr()-&gt;as_jint());
2804       } else {
2805         assert_different_registers(dst, tmp_reg, data-&gt;as_register());
2806         __ add_32(tmp_reg, dst, data-&gt;as_register());
2807       }
2808       new_val = tmp_reg;
2809     } else {
2810       if (UseCompressedOops &amp;&amp; data-&gt;is_oop()) {
2811         new_val = tmp-&gt;as_pointer_register();
2812       } else {
2813         new_val = data-&gt;as_register();
2814       }
2815       assert_different_registers(dst, new_val);
2816     }
2817     __ strex(Rtemp, new_val, addr);
2818 
2819   } else if (data-&gt;type() == T_LONG) {
2820     Register dst_lo = dest-&gt;as_register_lo();
2821     Register new_val_lo = noreg;
2822     Register dst_hi = dest-&gt;as_register_hi();
2823 
2824     assert(dst_hi-&gt;encoding() == dst_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2825     assert((dst_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2826 
2827     __ bind(retry);
2828     __ ldrexd(dst_lo, addr);
2829     if (code == lir_xadd) {
2830       Register tmp_lo = tmp-&gt;as_register_lo();
2831       Register tmp_hi = tmp-&gt;as_register_hi();
2832 
2833       assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2834       assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2835 
2836       if (data-&gt;is_constant()) {
2837         jlong c = data-&gt;as_constant_ptr()-&gt;as_jlong();
2838         assert((jlong)((jint)c) == c, &quot;overflow&quot;);
2839         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi);
2840         __ adds(tmp_lo, dst_lo, (jint)c);
2841         __ adc(tmp_hi, dst_hi, 0);
2842       } else {
2843         Register new_val_lo = data-&gt;as_register_lo();
2844         Register new_val_hi = data-&gt;as_register_hi();
2845         __ adds(tmp_lo, dst_lo, new_val_lo);
2846         __ adc(tmp_hi, dst_hi, new_val_hi);
2847         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi, new_val_lo, new_val_hi);
2848       }
2849       new_val_lo = tmp_lo;
2850     } else {
2851       new_val_lo = data-&gt;as_register_lo();
2852       Register new_val_hi = data-&gt;as_register_hi();
2853 
2854       assert_different_registers(dst_lo, dst_hi, new_val_lo, new_val_hi);
2855       assert(new_val_hi-&gt;encoding() == new_val_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2856       assert((new_val_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2857     }
2858     __ strexd(Rtemp, new_val_lo, addr);
2859   } else {
2860     ShouldNotReachHere();
2861   }
2862 
2863   __ cbnz_32(Rtemp, retry);
2864   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
2865 
2866 }
2867 
2868 #undef __
    </pre>
  </body>
</html>