<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 //
   2 // Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
   3 // DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4 //
   5 // This code is free software; you can redistribute it and/or modify it
   6 // under the terms of the GNU General Public License version 2 only, as
   7 // published by the Free Software Foundation.
   8 //
   9 // This code is distributed in the hope that it will be useful, but WITHOUT
  10 // ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11 // FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12 // version 2 for more details (a copy is included in the LICENSE file that
  13 // accompanied this code).
  14 //
  15 // You should have received a copy of the GNU General Public License version
  16 // 2 along with this work; if not, write to the Free Software Foundation,
  17 // Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18 //
  19 // Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20 // or visit www.oracle.com if you need additional information or have any
  21 // questions.
  22 //
  23 //
  24 
  25 // X86 Common Architecture Description File
  26 
  27 //----------REGISTER DEFINITION BLOCK------------------------------------------
  28 // This information is used by the matcher and the register allocator to
  29 // describe individual registers and classes of registers within the target
  30 // archtecture.
  31 
  32 register %{
  33 //----------Architecture Description Register Definitions----------------------
  34 // General Registers
  35 // &quot;reg_def&quot;  name ( register save type, C convention save type,
  36 //                   ideal register type, encoding );
  37 // Register Save Types:
  38 //
  39 // NS  = No-Save:       The register allocator assumes that these registers
  40 //                      can be used without saving upon entry to the method, &amp;
  41 //                      that they do not need to be saved at call sites.
  42 //
  43 // SOC = Save-On-Call:  The register allocator assumes that these registers
  44 //                      can be used without saving upon entry to the method,
  45 //                      but that they must be saved at call sites.
  46 //
  47 // SOE = Save-On-Entry: The register allocator assumes that these registers
  48 //                      must be saved before using them upon entry to the
  49 //                      method, but they do not need to be saved at call
  50 //                      sites.
  51 //
  52 // AS  = Always-Save:   The register allocator assumes that these registers
  53 //                      must be saved before using them upon entry to the
  54 //                      method, &amp; that they must be saved at call sites.
  55 //
  56 // Ideal Register Type is used to determine how to save &amp; restore a
  57 // register.  Op_RegI will get spilled with LoadI/StoreI, Op_RegP will get
  58 // spilled with LoadP/StoreP.  If the register supports both, use Op_RegI.
  59 //
  60 // The encoding number is the actual bit-pattern placed into the opcodes.
  61 
  62 // XMM registers.  512-bit registers or 8 words each, labeled (a)-p.
  63 // Word a in each register holds a Float, words ab hold a Double.
  64 // The whole registers are used in SSE4.2 version intrinsics,
  65 // array copy stubs and superword operations (see UseSSE42Intrinsics,
  66 // UseXMMForArrayCopy and UseSuperword flags).
  67 // For pre EVEX enabled architectures:
  68 //      XMM8-XMM15 must be encoded with REX (VEX for UseAVX)
  69 // For EVEX enabled architectures:
  70 //      XMM8-XMM31 must be encoded with REX (EVEX for UseAVX).
  71 //
  72 // Linux ABI:   No register preserved across function calls
  73 //              XMM0-XMM7 might hold parameters
  74 // Windows ABI: XMM6-XMM31 preserved across function calls
  75 //              XMM0-XMM3 might hold parameters
  76 
  77 reg_def XMM0 ( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg());
  78 reg_def XMM0b( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(1));
  79 reg_def XMM0c( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(2));
  80 reg_def XMM0d( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(3));
  81 reg_def XMM0e( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(4));
  82 reg_def XMM0f( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(5));
  83 reg_def XMM0g( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(6));
  84 reg_def XMM0h( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(7));
  85 reg_def XMM0i( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(8));
  86 reg_def XMM0j( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(9));
  87 reg_def XMM0k( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(10));
  88 reg_def XMM0l( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(11));
  89 reg_def XMM0m( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(12));
  90 reg_def XMM0n( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(13));
  91 reg_def XMM0o( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(14));
  92 reg_def XMM0p( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(15));
  93 
  94 reg_def XMM1 ( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg());
  95 reg_def XMM1b( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(1));
  96 reg_def XMM1c( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(2));
  97 reg_def XMM1d( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(3));
  98 reg_def XMM1e( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(4));
  99 reg_def XMM1f( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(5));
 100 reg_def XMM1g( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(6));
 101 reg_def XMM1h( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(7));
 102 reg_def XMM1i( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(8));
 103 reg_def XMM1j( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(9));
 104 reg_def XMM1k( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(10));
 105 reg_def XMM1l( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(11));
 106 reg_def XMM1m( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(12));
 107 reg_def XMM1n( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(13));
 108 reg_def XMM1o( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(14));
 109 reg_def XMM1p( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(15));
 110 
 111 reg_def XMM2 ( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg());
 112 reg_def XMM2b( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(1));
 113 reg_def XMM2c( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(2));
 114 reg_def XMM2d( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(3));
 115 reg_def XMM2e( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(4));
 116 reg_def XMM2f( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(5));
 117 reg_def XMM2g( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(6));
 118 reg_def XMM2h( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(7));
 119 reg_def XMM2i( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(8));
 120 reg_def XMM2j( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(9));
 121 reg_def XMM2k( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(10));
 122 reg_def XMM2l( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(11));
 123 reg_def XMM2m( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(12));
 124 reg_def XMM2n( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(13));
 125 reg_def XMM2o( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(14));
 126 reg_def XMM2p( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(15));
 127 
 128 reg_def XMM3 ( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg());
 129 reg_def XMM3b( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(1));
 130 reg_def XMM3c( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(2));
 131 reg_def XMM3d( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(3));
 132 reg_def XMM3e( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(4));
 133 reg_def XMM3f( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(5));
 134 reg_def XMM3g( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(6));
 135 reg_def XMM3h( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(7));
 136 reg_def XMM3i( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(8));
 137 reg_def XMM3j( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(9));
 138 reg_def XMM3k( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(10));
 139 reg_def XMM3l( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(11));
 140 reg_def XMM3m( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(12));
 141 reg_def XMM3n( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(13));
 142 reg_def XMM3o( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(14));
 143 reg_def XMM3p( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(15));
 144 
 145 reg_def XMM4 ( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg());
 146 reg_def XMM4b( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(1));
 147 reg_def XMM4c( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(2));
 148 reg_def XMM4d( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(3));
 149 reg_def XMM4e( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(4));
 150 reg_def XMM4f( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(5));
 151 reg_def XMM4g( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(6));
 152 reg_def XMM4h( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(7));
 153 reg_def XMM4i( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(8));
 154 reg_def XMM4j( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(9));
 155 reg_def XMM4k( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(10));
 156 reg_def XMM4l( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(11));
 157 reg_def XMM4m( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(12));
 158 reg_def XMM4n( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(13));
 159 reg_def XMM4o( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(14));
 160 reg_def XMM4p( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(15));
 161 
 162 reg_def XMM5 ( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg());
 163 reg_def XMM5b( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(1));
 164 reg_def XMM5c( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(2));
 165 reg_def XMM5d( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(3));
 166 reg_def XMM5e( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(4));
 167 reg_def XMM5f( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(5));
 168 reg_def XMM5g( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(6));
 169 reg_def XMM5h( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(7));
 170 reg_def XMM5i( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(8));
 171 reg_def XMM5j( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(9));
 172 reg_def XMM5k( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(10));
 173 reg_def XMM5l( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(11));
 174 reg_def XMM5m( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(12));
 175 reg_def XMM5n( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(13));
 176 reg_def XMM5o( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(14));
 177 reg_def XMM5p( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(15));
 178 
 179 reg_def XMM6 ( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg());
 180 reg_def XMM6b( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(1));
 181 reg_def XMM6c( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(2));
 182 reg_def XMM6d( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(3));
 183 reg_def XMM6e( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(4));
 184 reg_def XMM6f( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(5));
 185 reg_def XMM6g( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(6));
 186 reg_def XMM6h( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(7));
 187 reg_def XMM6i( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(8));
 188 reg_def XMM6j( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(9));
 189 reg_def XMM6k( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(10));
 190 reg_def XMM6l( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(11));
 191 reg_def XMM6m( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(12));
 192 reg_def XMM6n( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(13));
 193 reg_def XMM6o( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(14));
 194 reg_def XMM6p( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(15));
 195 
 196 reg_def XMM7 ( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg());
 197 reg_def XMM7b( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(1));
 198 reg_def XMM7c( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(2));
 199 reg_def XMM7d( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(3));
 200 reg_def XMM7e( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(4));
 201 reg_def XMM7f( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(5));
 202 reg_def XMM7g( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(6));
 203 reg_def XMM7h( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(7));
 204 reg_def XMM7i( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(8));
 205 reg_def XMM7j( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(9));
 206 reg_def XMM7k( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(10));
 207 reg_def XMM7l( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(11));
 208 reg_def XMM7m( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(12));
 209 reg_def XMM7n( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(13));
 210 reg_def XMM7o( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(14));
 211 reg_def XMM7p( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(15));
 212 
 213 #ifdef _LP64
 214 
 215 reg_def XMM8 ( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg());
 216 reg_def XMM8b( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(1));
 217 reg_def XMM8c( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(2));
 218 reg_def XMM8d( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(3));
 219 reg_def XMM8e( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(4));
 220 reg_def XMM8f( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(5));
 221 reg_def XMM8g( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(6));
 222 reg_def XMM8h( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(7));
 223 reg_def XMM8i( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(8));
 224 reg_def XMM8j( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(9));
 225 reg_def XMM8k( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(10));
 226 reg_def XMM8l( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(11));
 227 reg_def XMM8m( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(12));
 228 reg_def XMM8n( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(13));
 229 reg_def XMM8o( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(14));
 230 reg_def XMM8p( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(15));
 231 
 232 reg_def XMM9 ( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg());
 233 reg_def XMM9b( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(1));
 234 reg_def XMM9c( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(2));
 235 reg_def XMM9d( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(3));
 236 reg_def XMM9e( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(4));
 237 reg_def XMM9f( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(5));
 238 reg_def XMM9g( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(6));
 239 reg_def XMM9h( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(7));
 240 reg_def XMM9i( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(8));
 241 reg_def XMM9j( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(9));
 242 reg_def XMM9k( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(10));
 243 reg_def XMM9l( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(11));
 244 reg_def XMM9m( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(12));
 245 reg_def XMM9n( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(13));
 246 reg_def XMM9o( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(14));
 247 reg_def XMM9p( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(15));
 248 
 249 reg_def XMM10 ( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg());
 250 reg_def XMM10b( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(1));
 251 reg_def XMM10c( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(2));
 252 reg_def XMM10d( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(3));
 253 reg_def XMM10e( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(4));
 254 reg_def XMM10f( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(5));
 255 reg_def XMM10g( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(6));
 256 reg_def XMM10h( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(7));
 257 reg_def XMM10i( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(8));
 258 reg_def XMM10j( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(9));
 259 reg_def XMM10k( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(10));
 260 reg_def XMM10l( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(11));
 261 reg_def XMM10m( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(12));
 262 reg_def XMM10n( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(13));
 263 reg_def XMM10o( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(14));
 264 reg_def XMM10p( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(15));
 265 
 266 reg_def XMM11 ( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg());
 267 reg_def XMM11b( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(1));
 268 reg_def XMM11c( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(2));
 269 reg_def XMM11d( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(3));
 270 reg_def XMM11e( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(4));
 271 reg_def XMM11f( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(5));
 272 reg_def XMM11g( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(6));
 273 reg_def XMM11h( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(7));
 274 reg_def XMM11i( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(8));
 275 reg_def XMM11j( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(9));
 276 reg_def XMM11k( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(10));
 277 reg_def XMM11l( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(11));
 278 reg_def XMM11m( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(12));
 279 reg_def XMM11n( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(13));
 280 reg_def XMM11o( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(14));
 281 reg_def XMM11p( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(15));
 282 
 283 reg_def XMM12 ( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg());
 284 reg_def XMM12b( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(1));
 285 reg_def XMM12c( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(2));
 286 reg_def XMM12d( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(3));
 287 reg_def XMM12e( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(4));
 288 reg_def XMM12f( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(5));
 289 reg_def XMM12g( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(6));
 290 reg_def XMM12h( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(7));
 291 reg_def XMM12i( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(8));
 292 reg_def XMM12j( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(9));
 293 reg_def XMM12k( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(10));
 294 reg_def XMM12l( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(11));
 295 reg_def XMM12m( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(12));
 296 reg_def XMM12n( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(13));
 297 reg_def XMM12o( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(14));
 298 reg_def XMM12p( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(15));
 299 
 300 reg_def XMM13 ( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg());
 301 reg_def XMM13b( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(1));
 302 reg_def XMM13c( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(2));
 303 reg_def XMM13d( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(3));
 304 reg_def XMM13e( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(4));
 305 reg_def XMM13f( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(5));
 306 reg_def XMM13g( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(6));
 307 reg_def XMM13h( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(7));
 308 reg_def XMM13i( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(8));
 309 reg_def XMM13j( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(9));
 310 reg_def XMM13k( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(10));
 311 reg_def XMM13l( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(11));
 312 reg_def XMM13m( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(12));
 313 reg_def XMM13n( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(13));
 314 reg_def XMM13o( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(14));
 315 reg_def XMM13p( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(15));
 316 
 317 reg_def XMM14 ( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg());
 318 reg_def XMM14b( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(1));
 319 reg_def XMM14c( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(2));
 320 reg_def XMM14d( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(3));
 321 reg_def XMM14e( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(4));
 322 reg_def XMM14f( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(5));
 323 reg_def XMM14g( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(6));
 324 reg_def XMM14h( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(7));
 325 reg_def XMM14i( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(8));
 326 reg_def XMM14j( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(9));
 327 reg_def XMM14k( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(10));
 328 reg_def XMM14l( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(11));
 329 reg_def XMM14m( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(12));
 330 reg_def XMM14n( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(13));
 331 reg_def XMM14o( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(14));
 332 reg_def XMM14p( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(15));
 333 
 334 reg_def XMM15 ( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg());
 335 reg_def XMM15b( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(1));
 336 reg_def XMM15c( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(2));
 337 reg_def XMM15d( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(3));
 338 reg_def XMM15e( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(4));
 339 reg_def XMM15f( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(5));
 340 reg_def XMM15g( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(6));
 341 reg_def XMM15h( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(7));
 342 reg_def XMM15i( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(8));
 343 reg_def XMM15j( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(9));
 344 reg_def XMM15k( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(10));
 345 reg_def XMM15l( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(11));
 346 reg_def XMM15m( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(12));
 347 reg_def XMM15n( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(13));
 348 reg_def XMM15o( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(14));
 349 reg_def XMM15p( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(15));
 350 
 351 reg_def XMM16 ( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg());
 352 reg_def XMM16b( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(1));
 353 reg_def XMM16c( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(2));
 354 reg_def XMM16d( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(3));
 355 reg_def XMM16e( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(4));
 356 reg_def XMM16f( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(5));
 357 reg_def XMM16g( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(6));
 358 reg_def XMM16h( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(7));
 359 reg_def XMM16i( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(8));
 360 reg_def XMM16j( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(9));
 361 reg_def XMM16k( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(10));
 362 reg_def XMM16l( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(11));
 363 reg_def XMM16m( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(12));
 364 reg_def XMM16n( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(13));
 365 reg_def XMM16o( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(14));
 366 reg_def XMM16p( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(15));
 367 
 368 reg_def XMM17 ( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg());
 369 reg_def XMM17b( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(1));
 370 reg_def XMM17c( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(2));
 371 reg_def XMM17d( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(3));
 372 reg_def XMM17e( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(4));
 373 reg_def XMM17f( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(5));
 374 reg_def XMM17g( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(6));
 375 reg_def XMM17h( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(7));
 376 reg_def XMM17i( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(8));
 377 reg_def XMM17j( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(9));
 378 reg_def XMM17k( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(10));
 379 reg_def XMM17l( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(11));
 380 reg_def XMM17m( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(12));
 381 reg_def XMM17n( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(13));
 382 reg_def XMM17o( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(14));
 383 reg_def XMM17p( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(15));
 384 
 385 reg_def XMM18 ( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg());
 386 reg_def XMM18b( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(1));
 387 reg_def XMM18c( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(2));
 388 reg_def XMM18d( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(3));
 389 reg_def XMM18e( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(4));
 390 reg_def XMM18f( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(5));
 391 reg_def XMM18g( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(6));
 392 reg_def XMM18h( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(7));
 393 reg_def XMM18i( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(8));
 394 reg_def XMM18j( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(9));
 395 reg_def XMM18k( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(10));
 396 reg_def XMM18l( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(11));
 397 reg_def XMM18m( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(12));
 398 reg_def XMM18n( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(13));
 399 reg_def XMM18o( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(14));
 400 reg_def XMM18p( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(15));
 401 
 402 reg_def XMM19 ( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg());
 403 reg_def XMM19b( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(1));
 404 reg_def XMM19c( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(2));
 405 reg_def XMM19d( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(3));
 406 reg_def XMM19e( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(4));
 407 reg_def XMM19f( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(5));
 408 reg_def XMM19g( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(6));
 409 reg_def XMM19h( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(7));
 410 reg_def XMM19i( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(8));
 411 reg_def XMM19j( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(9));
 412 reg_def XMM19k( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(10));
 413 reg_def XMM19l( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(11));
 414 reg_def XMM19m( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(12));
 415 reg_def XMM19n( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(13));
 416 reg_def XMM19o( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(14));
 417 reg_def XMM19p( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(15));
 418 
 419 reg_def XMM20 ( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg());
 420 reg_def XMM20b( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(1));
 421 reg_def XMM20c( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(2));
 422 reg_def XMM20d( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(3));
 423 reg_def XMM20e( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(4));
 424 reg_def XMM20f( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(5));
 425 reg_def XMM20g( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(6));
 426 reg_def XMM20h( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(7));
 427 reg_def XMM20i( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(8));
 428 reg_def XMM20j( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(9));
 429 reg_def XMM20k( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(10));
 430 reg_def XMM20l( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(11));
 431 reg_def XMM20m( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(12));
 432 reg_def XMM20n( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(13));
 433 reg_def XMM20o( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(14));
 434 reg_def XMM20p( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(15));
 435 
 436 reg_def XMM21 ( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg());
 437 reg_def XMM21b( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(1));
 438 reg_def XMM21c( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(2));
 439 reg_def XMM21d( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(3));
 440 reg_def XMM21e( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(4));
 441 reg_def XMM21f( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(5));
 442 reg_def XMM21g( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(6));
 443 reg_def XMM21h( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(7));
 444 reg_def XMM21i( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(8));
 445 reg_def XMM21j( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(9));
 446 reg_def XMM21k( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(10));
 447 reg_def XMM21l( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(11));
 448 reg_def XMM21m( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(12));
 449 reg_def XMM21n( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(13));
 450 reg_def XMM21o( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(14));
 451 reg_def XMM21p( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(15));
 452 
 453 reg_def XMM22 ( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg());
 454 reg_def XMM22b( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(1));
 455 reg_def XMM22c( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(2));
 456 reg_def XMM22d( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(3));
 457 reg_def XMM22e( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(4));
 458 reg_def XMM22f( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(5));
 459 reg_def XMM22g( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(6));
 460 reg_def XMM22h( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(7));
 461 reg_def XMM22i( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(8));
 462 reg_def XMM22j( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(9));
 463 reg_def XMM22k( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(10));
 464 reg_def XMM22l( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(11));
 465 reg_def XMM22m( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(12));
 466 reg_def XMM22n( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(13));
 467 reg_def XMM22o( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(14));
 468 reg_def XMM22p( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(15));
 469 
 470 reg_def XMM23 ( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg());
 471 reg_def XMM23b( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(1));
 472 reg_def XMM23c( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(2));
 473 reg_def XMM23d( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(3));
 474 reg_def XMM23e( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(4));
 475 reg_def XMM23f( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(5));
 476 reg_def XMM23g( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(6));
 477 reg_def XMM23h( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(7));
 478 reg_def XMM23i( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(8));
 479 reg_def XMM23j( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(9));
 480 reg_def XMM23k( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(10));
 481 reg_def XMM23l( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(11));
 482 reg_def XMM23m( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(12));
 483 reg_def XMM23n( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(13));
 484 reg_def XMM23o( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(14));
 485 reg_def XMM23p( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(15));
 486 
 487 reg_def XMM24 ( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg());
 488 reg_def XMM24b( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(1));
 489 reg_def XMM24c( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(2));
 490 reg_def XMM24d( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(3));
 491 reg_def XMM24e( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(4));
 492 reg_def XMM24f( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(5));
 493 reg_def XMM24g( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(6));
 494 reg_def XMM24h( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(7));
 495 reg_def XMM24i( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(8));
 496 reg_def XMM24j( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(9));
 497 reg_def XMM24k( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(10));
 498 reg_def XMM24l( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(11));
 499 reg_def XMM24m( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(12));
 500 reg_def XMM24n( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(13));
 501 reg_def XMM24o( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(14));
 502 reg_def XMM24p( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(15));
 503 
 504 reg_def XMM25 ( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg());
 505 reg_def XMM25b( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(1));
 506 reg_def XMM25c( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(2));
 507 reg_def XMM25d( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(3));
 508 reg_def XMM25e( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(4));
 509 reg_def XMM25f( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(5));
 510 reg_def XMM25g( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(6));
 511 reg_def XMM25h( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(7));
 512 reg_def XMM25i( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(8));
 513 reg_def XMM25j( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(9));
 514 reg_def XMM25k( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(10));
 515 reg_def XMM25l( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(11));
 516 reg_def XMM25m( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(12));
 517 reg_def XMM25n( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(13));
 518 reg_def XMM25o( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(14));
 519 reg_def XMM25p( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(15));
 520 
 521 reg_def XMM26 ( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg());
 522 reg_def XMM26b( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(1));
 523 reg_def XMM26c( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(2));
 524 reg_def XMM26d( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(3));
 525 reg_def XMM26e( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(4));
 526 reg_def XMM26f( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(5));
 527 reg_def XMM26g( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(6));
 528 reg_def XMM26h( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(7));
 529 reg_def XMM26i( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(8));
 530 reg_def XMM26j( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(9));
 531 reg_def XMM26k( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(10));
 532 reg_def XMM26l( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(11));
 533 reg_def XMM26m( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(12));
 534 reg_def XMM26n( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(13));
 535 reg_def XMM26o( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(14));
 536 reg_def XMM26p( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(15));
 537 
 538 reg_def XMM27 ( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg());
 539 reg_def XMM27b( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(1));
 540 reg_def XMM27c( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(2));
 541 reg_def XMM27d( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(3));
 542 reg_def XMM27e( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(4));
 543 reg_def XMM27f( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(5));
 544 reg_def XMM27g( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(6));
 545 reg_def XMM27h( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(7));
 546 reg_def XMM27i( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(8));
 547 reg_def XMM27j( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(9));
 548 reg_def XMM27k( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(10));
 549 reg_def XMM27l( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(11));
 550 reg_def XMM27m( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(12));
 551 reg_def XMM27n( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(13));
 552 reg_def XMM27o( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(14));
 553 reg_def XMM27p( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(15));
 554 
 555 reg_def XMM28 ( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg());
 556 reg_def XMM28b( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(1));
 557 reg_def XMM28c( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(2));
 558 reg_def XMM28d( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(3));
 559 reg_def XMM28e( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(4));
 560 reg_def XMM28f( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(5));
 561 reg_def XMM28g( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(6));
 562 reg_def XMM28h( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(7));
 563 reg_def XMM28i( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(8));
 564 reg_def XMM28j( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(9));
 565 reg_def XMM28k( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(10));
 566 reg_def XMM28l( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(11));
 567 reg_def XMM28m( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(12));
 568 reg_def XMM28n( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(13));
 569 reg_def XMM28o( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(14));
 570 reg_def XMM28p( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(15));
 571 
 572 reg_def XMM29 ( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg());
 573 reg_def XMM29b( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(1));
 574 reg_def XMM29c( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(2));
 575 reg_def XMM29d( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(3));
 576 reg_def XMM29e( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(4));
 577 reg_def XMM29f( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(5));
 578 reg_def XMM29g( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(6));
 579 reg_def XMM29h( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(7));
 580 reg_def XMM29i( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(8));
 581 reg_def XMM29j( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(9));
 582 reg_def XMM29k( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(10));
 583 reg_def XMM29l( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(11));
 584 reg_def XMM29m( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(12));
 585 reg_def XMM29n( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(13));
 586 reg_def XMM29o( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(14));
 587 reg_def XMM29p( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(15));
 588 
 589 reg_def XMM30 ( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg());
 590 reg_def XMM30b( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(1));
 591 reg_def XMM30c( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(2));
 592 reg_def XMM30d( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(3));
 593 reg_def XMM30e( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(4));
 594 reg_def XMM30f( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(5));
 595 reg_def XMM30g( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(6));
 596 reg_def XMM30h( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(7));
 597 reg_def XMM30i( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(8));
 598 reg_def XMM30j( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(9));
 599 reg_def XMM30k( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(10));
 600 reg_def XMM30l( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(11));
 601 reg_def XMM30m( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(12));
 602 reg_def XMM30n( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(13));
 603 reg_def XMM30o( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(14));
 604 reg_def XMM30p( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(15));
 605 
 606 reg_def XMM31 ( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg());
 607 reg_def XMM31b( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(1));
 608 reg_def XMM31c( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(2));
 609 reg_def XMM31d( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(3));
 610 reg_def XMM31e( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(4));
 611 reg_def XMM31f( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(5));
 612 reg_def XMM31g( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(6));
 613 reg_def XMM31h( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(7));
 614 reg_def XMM31i( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(8));
 615 reg_def XMM31j( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(9));
 616 reg_def XMM31k( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(10));
 617 reg_def XMM31l( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(11));
 618 reg_def XMM31m( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(12));
 619 reg_def XMM31n( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(13));
 620 reg_def XMM31o( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(14));
 621 reg_def XMM31p( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(15));
 622 
 623 #endif // _LP64
 624 
 625 #ifdef _LP64
 626 reg_def RFLAGS(SOC, SOC, 0, 16, VMRegImpl::Bad());
 627 #else
 628 reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());
 629 #endif // _LP64
 630 
 631 alloc_class chunk1(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
 632                    XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
 633                    XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
 634                    XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
 635                    XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
 636                    XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
 637                    XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
 638                    XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
 639 #ifdef _LP64
 640                   ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
 641                    XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
 642                    XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
 643                    XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
 644                    XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
 645                    XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
 646                    XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
 647                    XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
 648                   ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
 649                    XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
 650                    XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
 651                    XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
 652                    XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
 653                    XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
 654                    XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
 655                    XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
 656                    XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
 657                    XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
 658                    XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
 659                    XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
 660                    XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
 661                    XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
 662                    XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
 663                    XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
 664 #endif
 665                       );
 666 
 667 // flags allocation class should be last.
 668 alloc_class chunk2(RFLAGS);
 669 
 670 // Singleton class for condition codes
 671 reg_class int_flags(RFLAGS);
 672 
 673 // Class for pre evex float registers
 674 reg_class float_reg_legacy(XMM0,
 675                     XMM1,
 676                     XMM2,
 677                     XMM3,
 678                     XMM4,
 679                     XMM5,
 680                     XMM6,
 681                     XMM7
 682 #ifdef _LP64
 683                    ,XMM8,
 684                     XMM9,
 685                     XMM10,
 686                     XMM11,
 687                     XMM12,
 688                     XMM13,
 689                     XMM14,
 690                     XMM15
 691 #endif
 692                     );
 693 
 694 // Class for evex float registers
 695 reg_class float_reg_evex(XMM0,
 696                     XMM1,
 697                     XMM2,
 698                     XMM3,
 699                     XMM4,
 700                     XMM5,
 701                     XMM6,
 702                     XMM7
 703 #ifdef _LP64
 704                    ,XMM8,
 705                     XMM9,
 706                     XMM10,
 707                     XMM11,
 708                     XMM12,
 709                     XMM13,
 710                     XMM14,
 711                     XMM15,
 712                     XMM16,
 713                     XMM17,
 714                     XMM18,
 715                     XMM19,
 716                     XMM20,
 717                     XMM21,
 718                     XMM22,
 719                     XMM23,
 720                     XMM24,
 721                     XMM25,
 722                     XMM26,
 723                     XMM27,
 724                     XMM28,
 725                     XMM29,
 726                     XMM30,
 727                     XMM31
 728 #endif
 729                     );
 730 
 731 reg_class_dynamic float_reg(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() %} );
 732 reg_class_dynamic float_reg_vl(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 733 
 734 // Class for pre evex double registers
 735 reg_class double_reg_legacy(XMM0,  XMM0b,
 736                      XMM1,  XMM1b,
 737                      XMM2,  XMM2b,
 738                      XMM3,  XMM3b,
 739                      XMM4,  XMM4b,
 740                      XMM5,  XMM5b,
 741                      XMM6,  XMM6b,
 742                      XMM7,  XMM7b
 743 #ifdef _LP64
 744                     ,XMM8,  XMM8b,
 745                      XMM9,  XMM9b,
 746                      XMM10, XMM10b,
 747                      XMM11, XMM11b,
 748                      XMM12, XMM12b,
 749                      XMM13, XMM13b,
 750                      XMM14, XMM14b,
 751                      XMM15, XMM15b
 752 #endif
 753                      );
 754 
 755 // Class for evex double registers
 756 reg_class double_reg_evex(XMM0,  XMM0b,
 757                      XMM1,  XMM1b,
 758                      XMM2,  XMM2b,
 759                      XMM3,  XMM3b,
 760                      XMM4,  XMM4b,
 761                      XMM5,  XMM5b,
 762                      XMM6,  XMM6b,
 763                      XMM7,  XMM7b
 764 #ifdef _LP64
 765                     ,XMM8,  XMM8b,
 766                      XMM9,  XMM9b,
 767                      XMM10, XMM10b,
 768                      XMM11, XMM11b,
 769                      XMM12, XMM12b,
 770                      XMM13, XMM13b,
 771                      XMM14, XMM14b,
 772                      XMM15, XMM15b,
 773                      XMM16, XMM16b,
 774                      XMM17, XMM17b,
 775                      XMM18, XMM18b,
 776                      XMM19, XMM19b,
 777                      XMM20, XMM20b,
 778                      XMM21, XMM21b,
 779                      XMM22, XMM22b,
 780                      XMM23, XMM23b,
 781                      XMM24, XMM24b,
 782                      XMM25, XMM25b,
 783                      XMM26, XMM26b,
 784                      XMM27, XMM27b,
 785                      XMM28, XMM28b,
 786                      XMM29, XMM29b,
 787                      XMM30, XMM30b,
 788                      XMM31, XMM31b
 789 #endif
 790                      );
 791 
 792 reg_class_dynamic double_reg(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() %} );
 793 reg_class_dynamic double_reg_vl(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 794 
 795 // Class for pre evex 32bit vector registers
 796 reg_class vectors_reg_legacy(XMM0,
 797                       XMM1,
 798                       XMM2,
 799                       XMM3,
 800                       XMM4,
 801                       XMM5,
 802                       XMM6,
 803                       XMM7
 804 #ifdef _LP64
 805                      ,XMM8,
 806                       XMM9,
 807                       XMM10,
 808                       XMM11,
 809                       XMM12,
 810                       XMM13,
 811                       XMM14,
 812                       XMM15
 813 #endif
 814                       );
 815 
 816 // Class for evex 32bit vector registers
 817 reg_class vectors_reg_evex(XMM0,
 818                       XMM1,
 819                       XMM2,
 820                       XMM3,
 821                       XMM4,
 822                       XMM5,
 823                       XMM6,
 824                       XMM7
 825 #ifdef _LP64
 826                      ,XMM8,
 827                       XMM9,
 828                       XMM10,
 829                       XMM11,
 830                       XMM12,
 831                       XMM13,
 832                       XMM14,
 833                       XMM15,
 834                       XMM16,
 835                       XMM17,
 836                       XMM18,
 837                       XMM19,
 838                       XMM20,
 839                       XMM21,
 840                       XMM22,
 841                       XMM23,
 842                       XMM24,
 843                       XMM25,
 844                       XMM26,
 845                       XMM27,
 846                       XMM28,
 847                       XMM29,
 848                       XMM30,
 849                       XMM31
 850 #endif
 851                       );
 852 
 853 reg_class_dynamic vectors_reg(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_evex() %} );
 854 reg_class_dynamic vectors_reg_vlbwdq(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 855 
 856 // Class for all 64bit vector registers
 857 reg_class vectord_reg_legacy(XMM0,  XMM0b,
 858                       XMM1,  XMM1b,
 859                       XMM2,  XMM2b,
 860                       XMM3,  XMM3b,
 861                       XMM4,  XMM4b,
 862                       XMM5,  XMM5b,
 863                       XMM6,  XMM6b,
 864                       XMM7,  XMM7b
 865 #ifdef _LP64
 866                      ,XMM8,  XMM8b,
 867                       XMM9,  XMM9b,
 868                       XMM10, XMM10b,
 869                       XMM11, XMM11b,
 870                       XMM12, XMM12b,
 871                       XMM13, XMM13b,
 872                       XMM14, XMM14b,
 873                       XMM15, XMM15b
 874 #endif
 875                       );
 876 
 877 // Class for all 64bit vector registers
 878 reg_class vectord_reg_evex(XMM0,  XMM0b,
 879                       XMM1,  XMM1b,
 880                       XMM2,  XMM2b,
 881                       XMM3,  XMM3b,
 882                       XMM4,  XMM4b,
 883                       XMM5,  XMM5b,
 884                       XMM6,  XMM6b,
 885                       XMM7,  XMM7b
 886 #ifdef _LP64
 887                      ,XMM8,  XMM8b,
 888                       XMM9,  XMM9b,
 889                       XMM10, XMM10b,
 890                       XMM11, XMM11b,
 891                       XMM12, XMM12b,
 892                       XMM13, XMM13b,
 893                       XMM14, XMM14b,
 894                       XMM15, XMM15b,
 895                       XMM16, XMM16b,
 896                       XMM17, XMM17b,
 897                       XMM18, XMM18b,
 898                       XMM19, XMM19b,
 899                       XMM20, XMM20b,
 900                       XMM21, XMM21b,
 901                       XMM22, XMM22b,
 902                       XMM23, XMM23b,
 903                       XMM24, XMM24b,
 904                       XMM25, XMM25b,
 905                       XMM26, XMM26b,
 906                       XMM27, XMM27b,
 907                       XMM28, XMM28b,
 908                       XMM29, XMM29b,
 909                       XMM30, XMM30b,
 910                       XMM31, XMM31b
 911 #endif
 912                       );
 913 
 914 reg_class_dynamic vectord_reg(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_evex() %} );
 915 reg_class_dynamic vectord_reg_vlbwdq(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 916 
 917 // Class for all 128bit vector registers
 918 reg_class vectorx_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,
 919                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 920                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 921                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 922                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 923                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 924                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 925                       XMM7,  XMM7b,  XMM7c,  XMM7d
 926 #ifdef _LP64
 927                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 928                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 929                       XMM10, XMM10b, XMM10c, XMM10d,
 930                       XMM11, XMM11b, XMM11c, XMM11d,
 931                       XMM12, XMM12b, XMM12c, XMM12d,
 932                       XMM13, XMM13b, XMM13c, XMM13d,
 933                       XMM14, XMM14b, XMM14c, XMM14d,
 934                       XMM15, XMM15b, XMM15c, XMM15d
 935 #endif
 936                       );
 937 
 938 // Class for all 128bit vector registers
 939 reg_class vectorx_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,
 940                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 941                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 942                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 943                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 944                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 945                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 946                       XMM7,  XMM7b,  XMM7c,  XMM7d
 947 #ifdef _LP64
 948                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 949                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 950                       XMM10, XMM10b, XMM10c, XMM10d,
 951                       XMM11, XMM11b, XMM11c, XMM11d,
 952                       XMM12, XMM12b, XMM12c, XMM12d,
 953                       XMM13, XMM13b, XMM13c, XMM13d,
 954                       XMM14, XMM14b, XMM14c, XMM14d,
 955                       XMM15, XMM15b, XMM15c, XMM15d,
 956                       XMM16, XMM16b, XMM16c, XMM16d,
 957                       XMM17, XMM17b, XMM17c, XMM17d,
 958                       XMM18, XMM18b, XMM18c, XMM18d,
 959                       XMM19, XMM19b, XMM19c, XMM19d,
 960                       XMM20, XMM20b, XMM20c, XMM20d,
 961                       XMM21, XMM21b, XMM21c, XMM21d,
 962                       XMM22, XMM22b, XMM22c, XMM22d,
 963                       XMM23, XMM23b, XMM23c, XMM23d,
 964                       XMM24, XMM24b, XMM24c, XMM24d,
 965                       XMM25, XMM25b, XMM25c, XMM25d,
 966                       XMM26, XMM26b, XMM26c, XMM26d,
 967                       XMM27, XMM27b, XMM27c, XMM27d,
 968                       XMM28, XMM28b, XMM28c, XMM28d,
 969                       XMM29, XMM29b, XMM29c, XMM29d,
 970                       XMM30, XMM30b, XMM30c, XMM30d,
 971                       XMM31, XMM31b, XMM31c, XMM31d
 972 #endif
 973                       );
 974 
 975 reg_class_dynamic vectorx_reg(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_evex() %} );
 976 reg_class_dynamic vectorx_reg_vlbwdq(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 977 
 978 // Class for all 256bit vector registers
 979 reg_class vectory_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
 980                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
 981                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
 982                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
 983                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
 984                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
 985                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
 986                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
 987 #ifdef _LP64
 988                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
 989                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
 990                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
 991                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
 992                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
 993                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
 994                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
 995                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h
 996 #endif
 997                       );
 998 
 999 // Class for all 256bit vector registers
1000 reg_class vectory_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
1001                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
1002                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
1003                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
1004                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
1005                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
1006                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
1007                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
1008 #ifdef _LP64
1009                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
1010                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
1011                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
1012                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
1013                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
1014                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
1015                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
1016                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h,
1017                       XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h,
1018                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h,
1019                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h,
1020                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h,
1021                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h,
1022                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h,
1023                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h,
1024                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h,
1025                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h,
1026                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h,
1027                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h,
1028                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h,
1029                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h,
1030                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h,
1031                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h,
1032                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h
1033 #endif
1034                       );
1035 
1036 reg_class_dynamic vectory_reg(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_evex() %} );
1037 reg_class_dynamic vectory_reg_vlbwdq(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
1038 
1039 // Class for all 512bit vector registers
1040 reg_class vectorz_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1041                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1042                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1043                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1044                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1045                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1046                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1047                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1048 #ifdef _LP64
1049                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1050                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1051                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1052                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1053                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1054                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1055                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1056                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1057                      ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
1058                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
1059                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
1060                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
1061                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
1062                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
1063                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
1064                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
1065                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
1066                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
1067                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
1068                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
1069                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
1070                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
1071                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
1072                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
1073 #endif
1074                       );
1075 
1076 // Class for restricted 512bit vector registers
1077 reg_class vectorz_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1078                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1079                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1080                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1081                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1082                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1083                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1084                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1085 #ifdef _LP64
1086                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1087                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1088                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1089                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1090                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1091                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1092                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1093                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1094 #endif
1095                       );
1096 
1097 reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
1098 reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
1099 
1100 %}
1101 
1102 
1103 //----------SOURCE BLOCK-------------------------------------------------------
1104 // This is a block of C++ code which provides values, functions, and
1105 // definitions necessary in the rest of the architecture description
1106 
1107 source_hpp %{
1108 // Header information of the source block.
1109 // Method declarations/definitions which are used outside
1110 // the ad-scope can conveniently be defined here.
1111 //
1112 // To keep related declarations/definitions/uses close together,
1113 // we switch between source %{ }% and source_hpp %{ }% freely as needed.
1114 
1115 class NativeJump;
1116 
1117 class CallStubImpl {
1118 
1119   //--------------------------------------------------------------
1120   //---&lt;  Used for optimization in Compile::shorten_branches  &gt;---
1121   //--------------------------------------------------------------
1122 
1123  public:
1124   // Size of call trampoline stub.
1125   static uint size_call_trampoline() {
1126     return 0; // no call trampolines on this platform
1127   }
1128 
1129   // number of relocations needed by a call trampoline stub
1130   static uint reloc_call_trampoline() {
1131     return 0; // no call trampolines on this platform
1132   }
1133 };
1134 
1135 class HandlerImpl {
1136 
1137  public:
1138 
1139   static int emit_exception_handler(CodeBuffer &amp;cbuf);
1140   static int emit_deopt_handler(CodeBuffer&amp; cbuf);
1141 
1142   static uint size_exception_handler() {
1143     // NativeCall instruction size is the same as NativeJump.
1144     // exception handler starts out as jump and can be patched to
1145     // a call be deoptimization.  (4932387)
1146     // Note that this value is also credited (in output.cpp) to
1147     // the size of the code section.
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
1168 %} // end source_hpp
1169 
1170 source %{
1171 
1172 #include &quot;opto/addnode.hpp&quot;
1173 
1174 // Emit exception handler code.
1175 // Stuff framesize into a register and call a VM stub routine.
1176 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1177 
1178   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1179   // That&#39;s why we must use the macroassembler to generate a handler.
1180   MacroAssembler _masm(&amp;cbuf);
1181   address base = __ start_a_stub(size_exception_handler());
1182   if (base == NULL) {
1183     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1184     return 0;  // CodeBuffer::expand failed
1185   }
1186   int offset = __ offset();
1187   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1188   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1189   __ end_a_stub();
1190   return offset;
1191 }
1192 
1193 // Emit deopt handler code.
1194 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1195 
1196   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1197   // That&#39;s why we must use the macroassembler to generate a handler.
1198   MacroAssembler _masm(&amp;cbuf);
1199   address base = __ start_a_stub(size_deopt_handler());
1200   if (base == NULL) {
1201     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1202     return 0;  // CodeBuffer::expand failed
1203   }
1204   int offset = __ offset();
1205 
1206 #ifdef _LP64
1207   address the_pc = (address) __ pc();
1208   Label next;
1209   // push a &quot;the_pc&quot; on the stack without destroying any registers
1210   // as they all may be live.
1211 
1212   // push address of &quot;next&quot;
1213   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1214   __ bind(next);
1215   // adjust it so it matches &quot;the_pc&quot;
1216   __ subptr(Address(rsp, 0), __ offset() - offset);
1217 #else
1218   InternalAddress here(__ pc());
1219   __ pushptr(here.addr());
1220 #endif
1221 
1222   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
1223   assert(__ offset() - offset &lt;= (int) size_deopt_handler(), &quot;overflow %d&quot;, (__ offset() - offset));
1224   __ end_a_stub();
1225   return offset;
1226 }
1227 
1228 
1229 //=============================================================================
1230 
1231   // Float masks come from different places depending on platform.
1232 #ifdef _LP64
1233   static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }
1234   static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }
1235   static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }
1236   static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }
1237 #else
1238   static address float_signmask()  { return (address)float_signmask_pool; }
1239   static address float_signflip()  { return (address)float_signflip_pool; }
1240   static address double_signmask() { return (address)double_signmask_pool; }
1241   static address double_signflip() { return (address)double_signflip_pool; }
1242 #endif
1243   static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
1244   static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
1265       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
1280     case Op_AddReductionVL:
1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here
1282         return false;
1283       }
1284       break;
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
1298     case Op_SqrtVD:
1299     case Op_SqrtVF:
1300       if (UseAVX &lt; 1) { // enabled for AVX only
1301         return false;
1302       }
1303       break;
1304     case Op_CompareAndSwapL:
1305 #ifdef _LP64
1306     case Op_CompareAndSwapP:
1307 #endif
1308       if (!VM_Version::supports_cx8()) {
1309         return false;
1310       }
1311       break;
1312     case Op_CMoveVF:
1313     case Op_CMoveVD:
1314       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1315         return false;
1316       }
1317       break;
1318     case Op_StrIndexOf:
1319       if (!UseSSE42Intrinsics) {
1320         return false;
1321       }
1322       break;
1323     case Op_StrIndexOfChar:
1324       if (!UseSSE42Intrinsics) {
1325         return false;
1326       }
1327       break;
1328     case Op_OnSpinWait:
1329       if (VM_Version::supports_on_spin_wait() == false) {
1330         return false;
1331       }
1332       break;
1333     case Op_MulVB:
1334     case Op_LShiftVB:
1335     case Op_RShiftVB:
1336     case Op_URShiftVB:
1337       if (UseSSE &lt; 4) {
1338         return false;
1339       }
1340       break;
1341 #ifdef _LP64
1342     case Op_MaxD:
1343     case Op_MaxF:
1344     case Op_MinD:
1345     case Op_MinF:
1346       if (UseAVX &lt; 1) { // enabled for AVX only
1347         return false;
1348       }
1349       break;
1350 #endif
1351     case Op_CacheWB:
1352     case Op_CacheWBPreSync:
1353     case Op_CacheWBPostSync:
1354       if (!VM_Version::supports_data_cache_line_flush()) {
1355         return false;
1356       }
1357       break;
1358     case Op_RoundDoubleMode:
1359       if (UseSSE &lt; 4) {
1360         return false;
1361       }
1362       break;
1363     case Op_RoundDoubleModeV:
1364       if (VM_Version::supports_avx() == false) {
1365         return false; // 128bit vroundpd is not available
1366       }
1367       break;
1368 #ifndef _LP64
1369     case Op_AddReductionVF:
1370     case Op_AddReductionVD:
1371     case Op_MulReductionVF:
1372     case Op_MulReductionVD:
1373       if (UseSSE &lt; 1) { // requires at least SSE
1374         return false;
1375       }
1376       break;
1377     case Op_MulAddVS2VI:
1378     case Op_RShiftVL:
1379     case Op_AbsVD:
1380     case Op_NegVD:
1381       if (UseSSE &lt; 2) {
1382         return false;
1383       }
1384       break;
1385 #endif // !LP64
1386   }
1387   return true;  // Match rules are supported by default.
1388 }
1389 
1390 //------------------------------------------------------------------------
1391 
1392 // Identify extra cases that we might want to provide match rules for vector nodes and
1393 // other intrinsics guarded with vector length (vlen) and element type (bt).
1394 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1395   if (!match_rule_supported(opcode)) {
1396     return false;
1397   }
1398   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1399   //   * SSE2 supports 128bit vectors for all types;
1400   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1401   //   * AVX2 supports 256bit vectors for all types;
1402   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1403   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1404   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1405   // And MaxVectorSize is taken into account as well.
1406   if (!vector_size_supported(bt, vlen)) {
1407     return false;
1408   }
1409   // Special cases which require vector length follow:
1410   //   * implementation limitations
1411   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1412   //   * 128bit vroundpd instruction is present only in AVX1
1413   switch (opcode) {
1414     case Op_AbsVF:
1415     case Op_NegVF:
1416       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1417         return false; // 512bit vandps and vxorps are not available
1418       }
1419       break;
1420     case Op_AbsVD:
1421     case Op_NegVD:
1422       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1423         return false; // 512bit vandpd and vxorpd are not available
1424       }
1425       break;
1426     case Op_CMoveVF:
1427       if (vlen != 8) {
1428         return false; // implementation limitation (only vcmov8F_reg is present)
1429       }
1430       break;
1431     case Op_CMoveVD:
1432       if (vlen != 4) {
1433         return false; // implementation limitation (only vcmov4D_reg is present)
1434       }
1435       break;
1436   }
1437   return true;  // Per default match rules are supported.
1438 }
1439 
1440 // x86 supports generic vector operands: vec and legVec.
1441 const bool Matcher::supports_generic_vector_operands = true;
1442 
1443 MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1444   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1445   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1446   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1447       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1448     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1449     return new legVecZOper();
1450   }
1451   if (legacy) {
1452     switch (ideal_reg) {
1453       case Op_VecS: return new legVecSOper();
1454       case Op_VecD: return new legVecDOper();
1455       case Op_VecX: return new legVecXOper();
1456       case Op_VecY: return new legVecYOper();
1457       case Op_VecZ: return new legVecZOper();
1458     }
1459   } else {
1460     switch (ideal_reg) {
1461       case Op_VecS: return new vecSOper();
1462       case Op_VecD: return new vecDOper();
1463       case Op_VecX: return new vecXOper();
1464       case Op_VecY: return new vecYOper();
1465       case Op_VecZ: return new vecZOper();
1466     }
1467   }
1468   ShouldNotReachHere();
1469   return NULL;
1470 }
1471 
1472 bool Matcher::is_generic_reg2reg_move(MachNode* m) {
1473   switch (m-&gt;rule()) {
1474     case MoveVec2Leg_rule:
1475     case MoveLeg2Vec_rule:
1476       return true;
1477     default:
1478       return false;
1479   }
1480 }
1481 
1482 bool Matcher::is_generic_vector(MachOper* opnd) {
1483   switch (opnd-&gt;opcode()) {
1484     case VEC:
1485     case LEGVEC:
1486       return true;
1487     default:
1488       return false;
1489   }
1490 }
1491 
1492 //------------------------------------------------------------------------
1493 
1494 const bool Matcher::has_predicated_vectors(void) {
1495   bool ret_value = false;
1496   if (UseAVX &gt; 2) {
1497     ret_value = VM_Version::supports_avx512vl();
1498   }
1499 
1500   return ret_value;
1501 }
1502 
1503 const int Matcher::float_pressure(int default_pressure_threshold) {
1504   int float_pressure_threshold = default_pressure_threshold;
1505 #ifdef _LP64
1506   if (UseAVX &gt; 2) {
1507     // Increase pressure threshold on machines with AVX3 which have
1508     // 2x more XMM registers.
1509     float_pressure_threshold = default_pressure_threshold * 2;
1510   }
1511 #endif
1512   return float_pressure_threshold;
1513 }
1514 
1515 // Max vector size in bytes. 0 if not supported.
1516 const int Matcher::vector_width_in_bytes(BasicType bt) {
1517   assert(is_java_primitive(bt), &quot;only primitive type vectors&quot;);
1518   if (UseSSE &lt; 2) return 0;
1519   // SSE2 supports 128bit vectors for all types.
1520   // AVX2 supports 256bit vectors for all types.
1521   // AVX2/EVEX supports 512bit vectors for all types.
1522   int size = (UseAVX &gt; 1) ? (1 &lt;&lt; UseAVX) * 8 : 16;
1523   // AVX1 supports 256bit vectors only for FLOAT and DOUBLE.
1524   if (UseAVX &gt; 0 &amp;&amp; (bt == T_FLOAT || bt == T_DOUBLE))
1525     size = (UseAVX &gt; 2) ? 64 : 32;
1526   if (UseAVX &gt; 2 &amp;&amp; (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))
1527     size = (VM_Version::supports_avx512bw()) ? 64 : 32;
1528   // Use flag to limit vector size.
1529   size = MIN2(size,(int)MaxVectorSize);
1530   // Minimum 2 values in vector (or 4 for bytes).
1531   switch (bt) {
1532   case T_DOUBLE:
1533   case T_LONG:
1534     if (size &lt; 16) return 0;
1535     break;
1536   case T_FLOAT:
1537   case T_INT:
1538     if (size &lt; 8) return 0;
1539     break;
1540   case T_BOOLEAN:
1541     if (size &lt; 4) return 0;
1542     break;
1543   case T_CHAR:
1544     if (size &lt; 4) return 0;
1545     break;
1546   case T_BYTE:
1547     if (size &lt; 4) return 0;
1548     break;
1549   case T_SHORT:
1550     if (size &lt; 4) return 0;
1551     break;
1552   default:
1553     ShouldNotReachHere();
1554   }
1555   return size;
1556 }
1557 
1558 // Limits on vector size (number of elements) loaded into vector.
1559 const int Matcher::max_vector_size(const BasicType bt) {
1560   return vector_width_in_bytes(bt)/type2aelembytes(bt);
1561 }
1562 const int Matcher::min_vector_size(const BasicType bt) {
1563   int max_size = max_vector_size(bt);
1564   // Min size which can be loaded into vector is 4 bytes.
1565   int size = (type2aelembytes(bt) == 1) ? 4 : 2;
1566   return MIN2(size,max_size);
1567 }
1568 
1569 // Vector ideal reg corresponding to specified size in bytes
1570 const uint Matcher::vector_ideal_reg(int size) {
1571   assert(MaxVectorSize &gt;= size, &quot;&quot;);
1572   switch(size) {
1573     case  4: return Op_VecS;
1574     case  8: return Op_VecD;
1575     case 16: return Op_VecX;
1576     case 32: return Op_VecY;
1577     case 64: return Op_VecZ;
1578   }
1579   ShouldNotReachHere();
1580   return 0;
1581 }
1582 
1583 // Only lowest bits of xmm reg are used for vector shift count.
1584 const uint Matcher::vector_shift_count_ideal_reg(int size) {
1585   return Op_VecS;
1586 }
1587 
1588 // x86 supports misaligned vectors store/load.
1589 const bool Matcher::misaligned_vectors_ok() {
1590   return true;
1591 }
1592 
1593 // x86 AES instructions are compatible with SunJCE expanded
1594 // keys, hence we do not need to pass the original key to stubs
1595 const bool Matcher::pass_original_key_for_aes() {
1596   return false;
1597 }
1598 
1599 
1600 const bool Matcher::convi2l_type_required = true;
1601 
1602 // Check for shift by small constant as well
1603 static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1604   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
1605       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
1606       // Are there other uses besides address expressions?
1607       !matcher-&gt;is_visited(shift)) {
1608     address_visited.set(shift-&gt;_idx); // Flag as address_visited
1609     mstack.push(shift-&gt;in(2), Matcher::Visit);
1610     Node *conv = shift-&gt;in(1);
1611 #ifdef _LP64
1612     // Allow Matcher to match the rule which bypass
1613     // ConvI2L operation for an array index on LP64
1614     // if the index value is positive.
1615     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1616         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1617         // Are there other uses besides address expressions?
1618         !matcher-&gt;is_visited(conv)) {
1619       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1620       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1621     } else
1622 #endif
1623       mstack.push(conv, Matcher::Pre_Visit);
1624     return true;
1625   }
1626   return false;
1627 }
1628 
1629 // Should the Matcher clone shifts on addressing modes, expecting them
1630 // to be subsumed into complex addressing expressions or compute them
1631 // into registers?
1632 bool Matcher::clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1633   Node *off = m-&gt;in(AddPNode::Offset);
1634   if (off-&gt;is_Con()) {
1635     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1636     Node *adr = m-&gt;in(AddPNode::Address);
1637 
1638     // Intel can handle 2 adds in addressing mode
1639     // AtomicAdd is not an addressing expression.
1640     // Cheap to find it by looking for screwy base.
1641     if (adr-&gt;is_AddP() &amp;&amp;
1642         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1643         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1644         // Are there other uses besides address expressions?
1645         !is_visited(adr)) {
1646       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1647       Node *shift = adr-&gt;in(AddPNode::Offset);
1648       if (!clone_shift(shift, this, mstack, address_visited)) {
1649         mstack.push(shift, Pre_Visit);
1650       }
1651       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1652       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
1653     } else {
1654       mstack.push(adr, Pre_Visit);
1655     }
1656 
1657     // Clone X+offset as it also folds into most addressing expressions
1658     mstack.push(off, Visit);
1659     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1660     return true;
1661   } else if (clone_shift(off, this, mstack, address_visited)) {
1662     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1663     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1664     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1665     return true;
1666   }
1667   return false;
1668 }
1669 
1670 void Compile::reshape_address(AddPNode* addp) {
1671 }
1672 
1673 static inline uint vector_length(const MachNode* n) {
1674   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1675   return vt-&gt;length();
1676 }
1677 
1678 static inline uint vector_length_in_bytes(const MachNode* n) {
1679   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1680   return vt-&gt;length_in_bytes();
1681 }
1682 
1683 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1684   uint def_idx = use-&gt;operand_index(opnd);
1685   Node* def = use-&gt;in(def_idx);
1686   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1687 }
1688 
1689 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1690   switch(vector_length_in_bytes(n)) {
1691     case  4: // fall-through
1692     case  8: // fall-through
1693     case 16: return Assembler::AVX_128bit;
1694     case 32: return Assembler::AVX_256bit;
1695     case 64: return Assembler::AVX_512bit;
1696 
1697     default: {
1698       ShouldNotReachHere();
1699       return Assembler::AVX_NoVec;
1700     }
1701   }
1702 }
1703 
1704 // Helper methods for MachSpillCopyNode::implementation().
1705 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1706                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1707   // In 64-bit VM size calculation is very complex. Emitting instructions
1708   // into scratch buffer is used to get size in 64-bit VM.
1709   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1710   assert(ireg == Op_VecS || // 32bit vector
1711          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1712          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1713          &quot;no non-adjacent vector moves&quot; );
1714   if (cbuf) {
1715     MacroAssembler _masm(cbuf);
1716     int offset = __ offset();
1717     switch (ireg) {
1718     case Op_VecS: // copy whole register
1719     case Op_VecD:
1720     case Op_VecX:
1721 #ifndef _LP64
1722       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1723 #else
1724       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1725         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1726       } else {
1727         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1728      }
1729 #endif
1730       break;
1731     case Op_VecY:
1732 #ifndef _LP64
1733       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1734 #else
1735       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1736         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1737       } else {
1738         __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1739      }
1740 #endif
1741       break;
1742     case Op_VecZ:
1743       __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);
1744       break;
1745     default:
1746       ShouldNotReachHere();
1747     }
1748     int size = __ offset() - offset;
1749 #ifdef ASSERT
1750     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1751     assert(!do_size || size == 4, &quot;incorrect size calculattion&quot;);
1752 #endif
1753     return size;
1754 #ifndef PRODUCT
1755   } else if (!do_size) {
1756     switch (ireg) {
1757     case Op_VecS:
1758     case Op_VecD:
1759     case Op_VecX:
1760       st-&gt;print(&quot;movdqu  %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1761       break;
1762     case Op_VecY:
1763     case Op_VecZ:
1764       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1765       break;
1766     default:
1767       ShouldNotReachHere();
1768     }
1769 #endif
1770   }
1771   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1772   return (UseAVX &gt; 2) ? 6 : 4;
1773 }
1774 
1775 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1776                      int stack_offset, int reg, uint ireg, outputStream* st) {
1777   // In 64-bit VM size calculation is very complex. Emitting instructions
1778   // into scratch buffer is used to get size in 64-bit VM.
1779   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1780   if (cbuf) {
1781     MacroAssembler _masm(cbuf);
1782     int offset = __ offset();
1783     if (is_load) {
1784       switch (ireg) {
1785       case Op_VecS:
1786         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1787         break;
1788       case Op_VecD:
1789         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1790         break;
1791       case Op_VecX:
1792 #ifndef _LP64
1793         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1794 #else
1795         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1796           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1797         } else {
1798           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1799           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1800         }
1801 #endif
1802         break;
1803       case Op_VecY:
1804 #ifndef _LP64
1805         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1806 #else
1807         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1808           __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1809         } else {
1810           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1811           __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1812         }
1813 #endif
1814         break;
1815       case Op_VecZ:
1816         __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);
1817         break;
1818       default:
1819         ShouldNotReachHere();
1820       }
1821     } else { // store
1822       switch (ireg) {
1823       case Op_VecS:
1824         __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1825         break;
1826       case Op_VecD:
1827         __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1828         break;
1829       case Op_VecX:
1830 #ifndef _LP64
1831         __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1832 #else
1833         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1834           __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1835         }
1836         else {
1837           __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1838         }
1839 #endif
1840         break;
1841       case Op_VecY:
1842 #ifndef _LP64
1843         __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1844 #else
1845         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1846           __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1847         }
1848         else {
1849           __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1850         }
1851 #endif
1852         break;
1853       case Op_VecZ:
1854         __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1855         break;
1856       default:
1857         ShouldNotReachHere();
1858       }
1859     }
1860     int size = __ offset() - offset;
1861 #ifdef ASSERT
1862     int offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt; 0x80) ? 1 : (UseAVX &gt; 2) ? 6 : 4);
1863     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1864     assert(!do_size || size == (5+offset_size), &quot;incorrect size calculattion&quot;);
1865 #endif
1866     return size;
1867 #ifndef PRODUCT
1868   } else if (!do_size) {
1869     if (is_load) {
1870       switch (ireg) {
1871       case Op_VecS:
1872         st-&gt;print(&quot;movd    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1873         break;
1874       case Op_VecD:
1875         st-&gt;print(&quot;movq    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1876         break;
1877        case Op_VecX:
1878         st-&gt;print(&quot;movdqu  %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1879         break;
1880       case Op_VecY:
1881       case Op_VecZ:
1882         st-&gt;print(&quot;vmovdqu %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1883         break;
1884       default:
1885         ShouldNotReachHere();
1886       }
1887     } else { // store
1888       switch (ireg) {
1889       case Op_VecS:
1890         st-&gt;print(&quot;movd    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1891         break;
1892       case Op_VecD:
1893         st-&gt;print(&quot;movq    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1894         break;
1895        case Op_VecX:
1896         st-&gt;print(&quot;movdqu  [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1897         break;
1898       case Op_VecY:
1899       case Op_VecZ:
1900         st-&gt;print(&quot;vmovdqu [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1901         break;
1902       default:
1903         ShouldNotReachHere();
1904       }
1905     }
1906 #endif
1907   }
1908   bool is_single_byte = false;
1909   int vec_len = 0;
1910   if ((UseAVX &gt; 2) &amp;&amp; (stack_offset != 0)) {
1911     int tuple_type = Assembler::EVEX_FVM;
1912     int input_size = Assembler::EVEX_32bit;
1913     switch (ireg) {
1914     case Op_VecS:
1915       tuple_type = Assembler::EVEX_T1S;
1916       break;
1917     case Op_VecD:
1918       tuple_type = Assembler::EVEX_T1S;
1919       input_size = Assembler::EVEX_64bit;
1920       break;
1921     case Op_VecX:
1922       break;
1923     case Op_VecY:
1924       vec_len = 1;
1925       break;
1926     case Op_VecZ:
1927       vec_len = 2;
1928       break;
1929     }
1930     is_single_byte = Assembler::query_compressed_disp_byte(stack_offset, true, vec_len, tuple_type, input_size, 0);
1931   }
1932   int offset_size = 0;
1933   int size = 5;
1934   if (UseAVX &gt; 2 ) {
1935     if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len == 2)) {
1936       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
1937       size += 2; // Need an additional two bytes for EVEX encoding
1938     } else if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len &lt; 2)) {
1939       offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
1940     } else {
1941       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
1942       size += 2; // Need an additional two bytes for EVEX encodding
1943     }
1944   } else {
1945     offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
1946   }
1947   // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1948   return size+offset_size;
1949 }
1950 
1951 static inline jint replicate4_imm(int con, int width) {
1952   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 32bit.
1953   assert(width == 1 || width == 2, &quot;only byte or short types here&quot;);
1954   int bit_width = width * 8;
1955   jint val = con;
1956   val &amp;= (1 &lt;&lt; bit_width) - 1;  // mask off sign bits
1957   while(bit_width &lt; 32) {
1958     val |= (val &lt;&lt; bit_width);
1959     bit_width &lt;&lt;= 1;
1960   }
1961   return val;
1962 }
1963 
1964 static inline jlong replicate8_imm(int con, int width) {
1965   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
1966   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
1967   int bit_width = width * 8;
1968   jlong val = con;
1969   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
1970   while(bit_width &lt; 64) {
1971     val |= (val &lt;&lt; bit_width);
1972     bit_width &lt;&lt;= 1;
1973   }
1974   return val;
1975 }
1976 
1977 #ifndef PRODUCT
1978   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
1979     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
1980   }
1981 #endif
1982 
1983   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
1984     MacroAssembler _masm(&amp;cbuf);
1985     __ nop(_count);
1986   }
1987 
1988   uint MachNopNode::size(PhaseRegAlloc*) const {
1989     return _count;
1990   }
1991 
1992 #ifndef PRODUCT
1993   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
1994     st-&gt;print(&quot;# breakpoint&quot;);
1995   }
1996 #endif
1997 
1998   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
1999     MacroAssembler _masm(&amp;cbuf);
2000     __ int3();
2001   }
2002 
2003   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2004     return MachNode::size(ra_);
2005   }
2006 
2007 %}
2008 
2009 encode %{
2010 
2011   enc_class call_epilog %{
2012     if (VerifyStackAtCalls) {
2013       // Check that stack depth is unchanged: find majik cookie on stack
2014       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
2015       MacroAssembler _masm(&amp;cbuf);
2016       Label L;
2017       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2018       __ jccb(Assembler::equal, L);
2019       // Die if stack mismatch
2020       __ int3();
2021       __ bind(L);
2022     }
2023   %}
2024 
2025 %}
2026 
2027 
2028 //----------OPERANDS-----------------------------------------------------------
2029 // Operand definitions must precede instruction definitions for correct parsing
2030 // in the ADLC because operands constitute user defined types which are used in
2031 // instruction definitions.
2032 
2033 // Vectors
2034 
2035 // Dummy generic vector class. Should be used for all vector operands.
2036 // Replaced with vec[SDXYZ] during post-selection pass.
2037 operand vec() %{
2038   constraint(ALLOC_IN_RC(dynamic));
2039   match(VecX);
2040   match(VecY);
2041   match(VecZ);
2042   match(VecS);
2043   match(VecD);
2044 
2045   format %{ %}
2046   interface(REG_INTER);
2047 %}
2048 
2049 // Dummy generic legacy vector class. Should be used for all legacy vector operands.
2050 // Replaced with legVec[SDXYZ] during post-selection cleanup.
2051 // Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
2052 // runtime code generation via reg_class_dynamic.
2053 operand legVec() %{
2054   constraint(ALLOC_IN_RC(dynamic));
2055   match(VecX);
2056   match(VecY);
2057   match(VecZ);
2058   match(VecS);
2059   match(VecD);
2060 
2061   format %{ %}
2062   interface(REG_INTER);
2063 %}
2064 
2065 // Replaces vec during post-selection cleanup. See above.
2066 operand vecS() %{
2067   constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
2068   match(VecS);
2069 
2070   format %{ %}
2071   interface(REG_INTER);
2072 %}
2073 
2074 // Replaces legVec during post-selection cleanup. See above.
2075 operand legVecS() %{
2076   constraint(ALLOC_IN_RC(vectors_reg_legacy));
2077   match(VecS);
2078 
2079   format %{ %}
2080   interface(REG_INTER);
2081 %}
2082 
2083 // Replaces vec during post-selection cleanup. See above.
2084 operand vecD() %{
2085   constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
2086   match(VecD);
2087 
2088   format %{ %}
2089   interface(REG_INTER);
2090 %}
2091 
2092 // Replaces legVec during post-selection cleanup. See above.
2093 operand legVecD() %{
2094   constraint(ALLOC_IN_RC(vectord_reg_legacy));
2095   match(VecD);
2096 
2097   format %{ %}
2098   interface(REG_INTER);
2099 %}
2100 
2101 // Replaces vec during post-selection cleanup. See above.
2102 operand vecX() %{
2103   constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
2104   match(VecX);
2105 
2106   format %{ %}
2107   interface(REG_INTER);
2108 %}
2109 
2110 // Replaces legVec during post-selection cleanup. See above.
2111 operand legVecX() %{
2112   constraint(ALLOC_IN_RC(vectorx_reg_legacy));
2113   match(VecX);
2114 
2115   format %{ %}
2116   interface(REG_INTER);
2117 %}
2118 
2119 // Replaces vec during post-selection cleanup. See above.
2120 operand vecY() %{
2121   constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
2122   match(VecY);
2123 
2124   format %{ %}
2125   interface(REG_INTER);
2126 %}
2127 
2128 // Replaces legVec during post-selection cleanup. See above.
2129 operand legVecY() %{
2130   constraint(ALLOC_IN_RC(vectory_reg_legacy));
2131   match(VecY);
2132 
2133   format %{ %}
2134   interface(REG_INTER);
2135 %}
2136 
2137 // Replaces vec during post-selection cleanup. See above.
2138 operand vecZ() %{
2139   constraint(ALLOC_IN_RC(vectorz_reg));
2140   match(VecZ);
2141 
2142   format %{ %}
2143   interface(REG_INTER);
2144 %}
2145 
2146 // Replaces legVec during post-selection cleanup. See above.
2147 operand legVecZ() %{
2148   constraint(ALLOC_IN_RC(vectorz_reg_legacy));
2149   match(VecZ);
2150 
2151   format %{ %}
2152   interface(REG_INTER);
2153 %}
2154 
2155 // Comparison Code for FP conditional move
2156 operand cmpOp_vcmppd() %{
2157   match(Bool);
2158 
2159   predicate(n-&gt;as_Bool()-&gt;_test._test != BoolTest::overflow &amp;&amp;
2160             n-&gt;as_Bool()-&gt;_test._test != BoolTest::no_overflow);
2161   format %{ &quot;&quot; %}
2162   interface(COND_INTER) %{
2163     equal        (0x0, &quot;eq&quot;);
2164     less         (0x1, &quot;lt&quot;);
2165     less_equal   (0x2, &quot;le&quot;);
2166     not_equal    (0xC, &quot;ne&quot;);
2167     greater_equal(0xD, &quot;ge&quot;);
2168     greater      (0xE, &quot;gt&quot;);
2169     //TODO cannot compile (adlc breaks) without two next lines with error:
2170     // x86_64.ad(13987) Syntax Error: :In operand cmpOp_vcmppd: Do not support this encode constant: &#39; %{
2171     // equal&#39; for overflow.
2172     overflow     (0x20, &quot;o&quot;);  // not really supported by the instruction
2173     no_overflow  (0x21, &quot;no&quot;); // not really supported by the instruction
2174   %}
2175 %}
2176 
2177 
2178 // INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)
2179 
2180 // ============================================================================
2181 
2182 instruct ShouldNotReachHere() %{
2183   match(Halt);
2184   format %{ &quot;ud2\t# ShouldNotReachHere&quot; %}
2185   ins_encode %{
2186     __ stop(_halt_reason);
2187   %}
2188   ins_pipe(pipe_slow);
2189 %}
2190 
2191 // =================================EVEX special===============================
2192 
2193 instruct setMask(rRegI dst, rRegI src) %{
2194   predicate(Matcher::has_predicated_vectors());
2195   match(Set dst (SetVectMaskI  src));
2196   effect(TEMP dst);
2197   format %{ &quot;setvectmask   $dst, $src&quot; %}
2198   ins_encode %{
2199     __ setvectmask($dst$$Register, $src$$Register);
2200   %}
2201   ins_pipe(pipe_slow);
2202 %}
2203 
2204 // ============================================================================
2205 
2206 instruct addF_reg(regF dst, regF src) %{
2207   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2208   match(Set dst (AddF dst src));
2209 
2210   format %{ &quot;addss   $dst, $src&quot; %}
2211   ins_cost(150);
2212   ins_encode %{
2213     __ addss($dst$$XMMRegister, $src$$XMMRegister);
2214   %}
2215   ins_pipe(pipe_slow);
2216 %}
2217 
2218 instruct addF_mem(regF dst, memory src) %{
2219   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2220   match(Set dst (AddF dst (LoadF src)));
2221 
2222   format %{ &quot;addss   $dst, $src&quot; %}
2223   ins_cost(150);
2224   ins_encode %{
2225     __ addss($dst$$XMMRegister, $src$$Address);
2226   %}
2227   ins_pipe(pipe_slow);
2228 %}
2229 
2230 instruct addF_imm(regF dst, immF con) %{
2231   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2232   match(Set dst (AddF dst con));
2233   format %{ &quot;addss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2234   ins_cost(150);
2235   ins_encode %{
2236     __ addss($dst$$XMMRegister, $constantaddress($con));
2237   %}
2238   ins_pipe(pipe_slow);
2239 %}
2240 
2241 instruct addF_reg_reg(regF dst, regF src1, regF src2) %{
2242   predicate(UseAVX &gt; 0);
2243   match(Set dst (AddF src1 src2));
2244 
2245   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2246   ins_cost(150);
2247   ins_encode %{
2248     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2249   %}
2250   ins_pipe(pipe_slow);
2251 %}
2252 
2253 instruct addF_reg_mem(regF dst, regF src1, memory src2) %{
2254   predicate(UseAVX &gt; 0);
2255   match(Set dst (AddF src1 (LoadF src2)));
2256 
2257   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2258   ins_cost(150);
2259   ins_encode %{
2260     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2261   %}
2262   ins_pipe(pipe_slow);
2263 %}
2264 
2265 instruct addF_reg_imm(regF dst, regF src, immF con) %{
2266   predicate(UseAVX &gt; 0);
2267   match(Set dst (AddF src con));
2268 
2269   format %{ &quot;vaddss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2270   ins_cost(150);
2271   ins_encode %{
2272     __ vaddss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2273   %}
2274   ins_pipe(pipe_slow);
2275 %}
2276 
2277 instruct addD_reg(regD dst, regD src) %{
2278   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2279   match(Set dst (AddD dst src));
2280 
2281   format %{ &quot;addsd   $dst, $src&quot; %}
2282   ins_cost(150);
2283   ins_encode %{
2284     __ addsd($dst$$XMMRegister, $src$$XMMRegister);
2285   %}
2286   ins_pipe(pipe_slow);
2287 %}
2288 
2289 instruct addD_mem(regD dst, memory src) %{
2290   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2291   match(Set dst (AddD dst (LoadD src)));
2292 
2293   format %{ &quot;addsd   $dst, $src&quot; %}
2294   ins_cost(150);
2295   ins_encode %{
2296     __ addsd($dst$$XMMRegister, $src$$Address);
2297   %}
2298   ins_pipe(pipe_slow);
2299 %}
2300 
2301 instruct addD_imm(regD dst, immD con) %{
2302   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2303   match(Set dst (AddD dst con));
2304   format %{ &quot;addsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2305   ins_cost(150);
2306   ins_encode %{
2307     __ addsd($dst$$XMMRegister, $constantaddress($con));
2308   %}
2309   ins_pipe(pipe_slow);
2310 %}
2311 
2312 instruct addD_reg_reg(regD dst, regD src1, regD src2) %{
2313   predicate(UseAVX &gt; 0);
2314   match(Set dst (AddD src1 src2));
2315 
2316   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2317   ins_cost(150);
2318   ins_encode %{
2319     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2320   %}
2321   ins_pipe(pipe_slow);
2322 %}
2323 
2324 instruct addD_reg_mem(regD dst, regD src1, memory src2) %{
2325   predicate(UseAVX &gt; 0);
2326   match(Set dst (AddD src1 (LoadD src2)));
2327 
2328   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2329   ins_cost(150);
2330   ins_encode %{
2331     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2332   %}
2333   ins_pipe(pipe_slow);
2334 %}
2335 
2336 instruct addD_reg_imm(regD dst, regD src, immD con) %{
2337   predicate(UseAVX &gt; 0);
2338   match(Set dst (AddD src con));
2339 
2340   format %{ &quot;vaddsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2341   ins_cost(150);
2342   ins_encode %{
2343     __ vaddsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2344   %}
2345   ins_pipe(pipe_slow);
2346 %}
2347 
2348 instruct subF_reg(regF dst, regF src) %{
2349   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2350   match(Set dst (SubF dst src));
2351 
2352   format %{ &quot;subss   $dst, $src&quot; %}
2353   ins_cost(150);
2354   ins_encode %{
2355     __ subss($dst$$XMMRegister, $src$$XMMRegister);
2356   %}
2357   ins_pipe(pipe_slow);
2358 %}
2359 
2360 instruct subF_mem(regF dst, memory src) %{
2361   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2362   match(Set dst (SubF dst (LoadF src)));
2363 
2364   format %{ &quot;subss   $dst, $src&quot; %}
2365   ins_cost(150);
2366   ins_encode %{
2367     __ subss($dst$$XMMRegister, $src$$Address);
2368   %}
2369   ins_pipe(pipe_slow);
2370 %}
2371 
2372 instruct subF_imm(regF dst, immF con) %{
2373   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2374   match(Set dst (SubF dst con));
2375   format %{ &quot;subss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2376   ins_cost(150);
2377   ins_encode %{
2378     __ subss($dst$$XMMRegister, $constantaddress($con));
2379   %}
2380   ins_pipe(pipe_slow);
2381 %}
2382 
2383 instruct subF_reg_reg(regF dst, regF src1, regF src2) %{
2384   predicate(UseAVX &gt; 0);
2385   match(Set dst (SubF src1 src2));
2386 
2387   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2388   ins_cost(150);
2389   ins_encode %{
2390     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2391   %}
2392   ins_pipe(pipe_slow);
2393 %}
2394 
2395 instruct subF_reg_mem(regF dst, regF src1, memory src2) %{
2396   predicate(UseAVX &gt; 0);
2397   match(Set dst (SubF src1 (LoadF src2)));
2398 
2399   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2400   ins_cost(150);
2401   ins_encode %{
2402     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2403   %}
2404   ins_pipe(pipe_slow);
2405 %}
2406 
2407 instruct subF_reg_imm(regF dst, regF src, immF con) %{
2408   predicate(UseAVX &gt; 0);
2409   match(Set dst (SubF src con));
2410 
2411   format %{ &quot;vsubss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2412   ins_cost(150);
2413   ins_encode %{
2414     __ vsubss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2415   %}
2416   ins_pipe(pipe_slow);
2417 %}
2418 
2419 instruct subD_reg(regD dst, regD src) %{
2420   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2421   match(Set dst (SubD dst src));
2422 
2423   format %{ &quot;subsd   $dst, $src&quot; %}
2424   ins_cost(150);
2425   ins_encode %{
2426     __ subsd($dst$$XMMRegister, $src$$XMMRegister);
2427   %}
2428   ins_pipe(pipe_slow);
2429 %}
2430 
2431 instruct subD_mem(regD dst, memory src) %{
2432   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2433   match(Set dst (SubD dst (LoadD src)));
2434 
2435   format %{ &quot;subsd   $dst, $src&quot; %}
2436   ins_cost(150);
2437   ins_encode %{
2438     __ subsd($dst$$XMMRegister, $src$$Address);
2439   %}
2440   ins_pipe(pipe_slow);
2441 %}
2442 
2443 instruct subD_imm(regD dst, immD con) %{
2444   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2445   match(Set dst (SubD dst con));
2446   format %{ &quot;subsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2447   ins_cost(150);
2448   ins_encode %{
2449     __ subsd($dst$$XMMRegister, $constantaddress($con));
2450   %}
2451   ins_pipe(pipe_slow);
2452 %}
2453 
2454 instruct subD_reg_reg(regD dst, regD src1, regD src2) %{
2455   predicate(UseAVX &gt; 0);
2456   match(Set dst (SubD src1 src2));
2457 
2458   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2459   ins_cost(150);
2460   ins_encode %{
2461     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2462   %}
2463   ins_pipe(pipe_slow);
2464 %}
2465 
2466 instruct subD_reg_mem(regD dst, regD src1, memory src2) %{
2467   predicate(UseAVX &gt; 0);
2468   match(Set dst (SubD src1 (LoadD src2)));
2469 
2470   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2471   ins_cost(150);
2472   ins_encode %{
2473     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2474   %}
2475   ins_pipe(pipe_slow);
2476 %}
2477 
2478 instruct subD_reg_imm(regD dst, regD src, immD con) %{
2479   predicate(UseAVX &gt; 0);
2480   match(Set dst (SubD src con));
2481 
2482   format %{ &quot;vsubsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2483   ins_cost(150);
2484   ins_encode %{
2485     __ vsubsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2486   %}
2487   ins_pipe(pipe_slow);
2488 %}
2489 
2490 instruct mulF_reg(regF dst, regF src) %{
2491   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2492   match(Set dst (MulF dst src));
2493 
2494   format %{ &quot;mulss   $dst, $src&quot; %}
2495   ins_cost(150);
2496   ins_encode %{
2497     __ mulss($dst$$XMMRegister, $src$$XMMRegister);
2498   %}
2499   ins_pipe(pipe_slow);
2500 %}
2501 
2502 instruct mulF_mem(regF dst, memory src) %{
2503   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2504   match(Set dst (MulF dst (LoadF src)));
2505 
2506   format %{ &quot;mulss   $dst, $src&quot; %}
2507   ins_cost(150);
2508   ins_encode %{
2509     __ mulss($dst$$XMMRegister, $src$$Address);
2510   %}
2511   ins_pipe(pipe_slow);
2512 %}
2513 
2514 instruct mulF_imm(regF dst, immF con) %{
2515   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2516   match(Set dst (MulF dst con));
2517   format %{ &quot;mulss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2518   ins_cost(150);
2519   ins_encode %{
2520     __ mulss($dst$$XMMRegister, $constantaddress($con));
2521   %}
2522   ins_pipe(pipe_slow);
2523 %}
2524 
2525 instruct mulF_reg_reg(regF dst, regF src1, regF src2) %{
2526   predicate(UseAVX &gt; 0);
2527   match(Set dst (MulF src1 src2));
2528 
2529   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2530   ins_cost(150);
2531   ins_encode %{
2532     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2533   %}
2534   ins_pipe(pipe_slow);
2535 %}
2536 
2537 instruct mulF_reg_mem(regF dst, regF src1, memory src2) %{
2538   predicate(UseAVX &gt; 0);
2539   match(Set dst (MulF src1 (LoadF src2)));
2540 
2541   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2542   ins_cost(150);
2543   ins_encode %{
2544     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2545   %}
2546   ins_pipe(pipe_slow);
2547 %}
2548 
2549 instruct mulF_reg_imm(regF dst, regF src, immF con) %{
2550   predicate(UseAVX &gt; 0);
2551   match(Set dst (MulF src con));
2552 
2553   format %{ &quot;vmulss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2554   ins_cost(150);
2555   ins_encode %{
2556     __ vmulss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2557   %}
2558   ins_pipe(pipe_slow);
2559 %}
2560 
2561 instruct mulD_reg(regD dst, regD src) %{
2562   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2563   match(Set dst (MulD dst src));
2564 
2565   format %{ &quot;mulsd   $dst, $src&quot; %}
2566   ins_cost(150);
2567   ins_encode %{
2568     __ mulsd($dst$$XMMRegister, $src$$XMMRegister);
2569   %}
2570   ins_pipe(pipe_slow);
2571 %}
2572 
2573 instruct mulD_mem(regD dst, memory src) %{
2574   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2575   match(Set dst (MulD dst (LoadD src)));
2576 
2577   format %{ &quot;mulsd   $dst, $src&quot; %}
2578   ins_cost(150);
2579   ins_encode %{
2580     __ mulsd($dst$$XMMRegister, $src$$Address);
2581   %}
2582   ins_pipe(pipe_slow);
2583 %}
2584 
2585 instruct mulD_imm(regD dst, immD con) %{
2586   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2587   match(Set dst (MulD dst con));
2588   format %{ &quot;mulsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2589   ins_cost(150);
2590   ins_encode %{
2591     __ mulsd($dst$$XMMRegister, $constantaddress($con));
2592   %}
2593   ins_pipe(pipe_slow);
2594 %}
2595 
2596 instruct mulD_reg_reg(regD dst, regD src1, regD src2) %{
2597   predicate(UseAVX &gt; 0);
2598   match(Set dst (MulD src1 src2));
2599 
2600   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2601   ins_cost(150);
2602   ins_encode %{
2603     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2604   %}
2605   ins_pipe(pipe_slow);
2606 %}
2607 
2608 instruct mulD_reg_mem(regD dst, regD src1, memory src2) %{
2609   predicate(UseAVX &gt; 0);
2610   match(Set dst (MulD src1 (LoadD src2)));
2611 
2612   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2613   ins_cost(150);
2614   ins_encode %{
2615     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2616   %}
2617   ins_pipe(pipe_slow);
2618 %}
2619 
2620 instruct mulD_reg_imm(regD dst, regD src, immD con) %{
2621   predicate(UseAVX &gt; 0);
2622   match(Set dst (MulD src con));
2623 
2624   format %{ &quot;vmulsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2625   ins_cost(150);
2626   ins_encode %{
2627     __ vmulsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2628   %}
2629   ins_pipe(pipe_slow);
2630 %}
2631 
2632 instruct divF_reg(regF dst, regF src) %{
2633   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2634   match(Set dst (DivF dst src));
2635 
2636   format %{ &quot;divss   $dst, $src&quot; %}
2637   ins_cost(150);
2638   ins_encode %{
2639     __ divss($dst$$XMMRegister, $src$$XMMRegister);
2640   %}
2641   ins_pipe(pipe_slow);
2642 %}
2643 
2644 instruct divF_mem(regF dst, memory src) %{
2645   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2646   match(Set dst (DivF dst (LoadF src)));
2647 
2648   format %{ &quot;divss   $dst, $src&quot; %}
2649   ins_cost(150);
2650   ins_encode %{
2651     __ divss($dst$$XMMRegister, $src$$Address);
2652   %}
2653   ins_pipe(pipe_slow);
2654 %}
2655 
2656 instruct divF_imm(regF dst, immF con) %{
2657   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2658   match(Set dst (DivF dst con));
2659   format %{ &quot;divss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2660   ins_cost(150);
2661   ins_encode %{
2662     __ divss($dst$$XMMRegister, $constantaddress($con));
2663   %}
2664   ins_pipe(pipe_slow);
2665 %}
2666 
2667 instruct divF_reg_reg(regF dst, regF src1, regF src2) %{
2668   predicate(UseAVX &gt; 0);
2669   match(Set dst (DivF src1 src2));
2670 
2671   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2672   ins_cost(150);
2673   ins_encode %{
2674     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2675   %}
2676   ins_pipe(pipe_slow);
2677 %}
2678 
2679 instruct divF_reg_mem(regF dst, regF src1, memory src2) %{
2680   predicate(UseAVX &gt; 0);
2681   match(Set dst (DivF src1 (LoadF src2)));
2682 
2683   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2684   ins_cost(150);
2685   ins_encode %{
2686     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2687   %}
2688   ins_pipe(pipe_slow);
2689 %}
2690 
2691 instruct divF_reg_imm(regF dst, regF src, immF con) %{
2692   predicate(UseAVX &gt; 0);
2693   match(Set dst (DivF src con));
2694 
2695   format %{ &quot;vdivss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2696   ins_cost(150);
2697   ins_encode %{
2698     __ vdivss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2699   %}
2700   ins_pipe(pipe_slow);
2701 %}
2702 
2703 instruct divD_reg(regD dst, regD src) %{
2704   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2705   match(Set dst (DivD dst src));
2706 
2707   format %{ &quot;divsd   $dst, $src&quot; %}
2708   ins_cost(150);
2709   ins_encode %{
2710     __ divsd($dst$$XMMRegister, $src$$XMMRegister);
2711   %}
2712   ins_pipe(pipe_slow);
2713 %}
2714 
2715 instruct divD_mem(regD dst, memory src) %{
2716   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2717   match(Set dst (DivD dst (LoadD src)));
2718 
2719   format %{ &quot;divsd   $dst, $src&quot; %}
2720   ins_cost(150);
2721   ins_encode %{
2722     __ divsd($dst$$XMMRegister, $src$$Address);
2723   %}
2724   ins_pipe(pipe_slow);
2725 %}
2726 
2727 instruct divD_imm(regD dst, immD con) %{
2728   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2729   match(Set dst (DivD dst con));
2730   format %{ &quot;divsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2731   ins_cost(150);
2732   ins_encode %{
2733     __ divsd($dst$$XMMRegister, $constantaddress($con));
2734   %}
2735   ins_pipe(pipe_slow);
2736 %}
2737 
2738 instruct divD_reg_reg(regD dst, regD src1, regD src2) %{
2739   predicate(UseAVX &gt; 0);
2740   match(Set dst (DivD src1 src2));
2741 
2742   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2743   ins_cost(150);
2744   ins_encode %{
2745     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2746   %}
2747   ins_pipe(pipe_slow);
2748 %}
2749 
2750 instruct divD_reg_mem(regD dst, regD src1, memory src2) %{
2751   predicate(UseAVX &gt; 0);
2752   match(Set dst (DivD src1 (LoadD src2)));
2753 
2754   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2755   ins_cost(150);
2756   ins_encode %{
2757     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2758   %}
2759   ins_pipe(pipe_slow);
2760 %}
2761 
2762 instruct divD_reg_imm(regD dst, regD src, immD con) %{
2763   predicate(UseAVX &gt; 0);
2764   match(Set dst (DivD src con));
2765 
2766   format %{ &quot;vdivsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2767   ins_cost(150);
2768   ins_encode %{
2769     __ vdivsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2770   %}
2771   ins_pipe(pipe_slow);
2772 %}
2773 
2774 instruct absF_reg(regF dst) %{
2775   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2776   match(Set dst (AbsF dst));
2777   ins_cost(150);
2778   format %{ &quot;andps   $dst, [0x7fffffff]\t# abs float by sign masking&quot; %}
2779   ins_encode %{
2780     __ andps($dst$$XMMRegister, ExternalAddress(float_signmask()));
2781   %}
2782   ins_pipe(pipe_slow);
2783 %}
2784 
2785 instruct absF_reg_reg(vlRegF dst, vlRegF src) %{
2786   predicate(UseAVX &gt; 0);
2787   match(Set dst (AbsF src));
2788   ins_cost(150);
2789   format %{ &quot;vandps  $dst, $src, [0x7fffffff]\t# abs float by sign masking&quot; %}
2790   ins_encode %{
2791     int vector_len = 0;
2792     __ vandps($dst$$XMMRegister, $src$$XMMRegister,
2793               ExternalAddress(float_signmask()), vector_len);
2794   %}
2795   ins_pipe(pipe_slow);
2796 %}
2797 
2798 instruct absD_reg(regD dst) %{
2799   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2800   match(Set dst (AbsD dst));
2801   ins_cost(150);
2802   format %{ &quot;andpd   $dst, [0x7fffffffffffffff]\t&quot;
2803             &quot;# abs double by sign masking&quot; %}
2804   ins_encode %{
2805     __ andpd($dst$$XMMRegister, ExternalAddress(double_signmask()));
2806   %}
2807   ins_pipe(pipe_slow);
2808 %}
2809 
2810 instruct absD_reg_reg(vlRegD dst, vlRegD src) %{
2811   predicate(UseAVX &gt; 0);
2812   match(Set dst (AbsD src));
2813   ins_cost(150);
2814   format %{ &quot;vandpd  $dst, $src, [0x7fffffffffffffff]\t&quot;
2815             &quot;# abs double by sign masking&quot; %}
2816   ins_encode %{
2817     int vector_len = 0;
2818     __ vandpd($dst$$XMMRegister, $src$$XMMRegister,
2819               ExternalAddress(double_signmask()), vector_len);
2820   %}
2821   ins_pipe(pipe_slow);
2822 %}
2823 
2824 instruct negF_reg(regF dst) %{
2825   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2826   match(Set dst (NegF dst));
2827   ins_cost(150);
2828   format %{ &quot;xorps   $dst, [0x80000000]\t# neg float by sign flipping&quot; %}
2829   ins_encode %{
2830     __ xorps($dst$$XMMRegister, ExternalAddress(float_signflip()));
2831   %}
2832   ins_pipe(pipe_slow);
2833 %}
2834 
2835 instruct negF_reg_reg(vlRegF dst, vlRegF src) %{
2836   predicate(UseAVX &gt; 0);
2837   match(Set dst (NegF src));
2838   ins_cost(150);
2839   format %{ &quot;vnegatess  $dst, $src, [0x80000000]\t# neg float by sign flipping&quot; %}
2840   ins_encode %{
2841     __ vnegatess($dst$$XMMRegister, $src$$XMMRegister,
2842                  ExternalAddress(float_signflip()));
2843   %}
2844   ins_pipe(pipe_slow);
2845 %}
2846 
2847 instruct negD_reg(regD dst) %{
2848   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2849   match(Set dst (NegD dst));
2850   ins_cost(150);
2851   format %{ &quot;xorpd   $dst, [0x8000000000000000]\t&quot;
2852             &quot;# neg double by sign flipping&quot; %}
2853   ins_encode %{
2854     __ xorpd($dst$$XMMRegister, ExternalAddress(double_signflip()));
2855   %}
2856   ins_pipe(pipe_slow);
2857 %}
2858 
2859 instruct negD_reg_reg(vlRegD dst, vlRegD src) %{
2860   predicate(UseAVX &gt; 0);
2861   match(Set dst (NegD src));
2862   ins_cost(150);
2863   format %{ &quot;vnegatesd  $dst, $src, [0x8000000000000000]\t&quot;
2864             &quot;# neg double by sign flipping&quot; %}
2865   ins_encode %{
2866     __ vnegatesd($dst$$XMMRegister, $src$$XMMRegister,
2867                  ExternalAddress(double_signflip()));
2868   %}
2869   ins_pipe(pipe_slow);
2870 %}
2871 
2872 instruct sqrtF_reg(regF dst, regF src) %{
2873   predicate(UseSSE&gt;=1);
2874   match(Set dst (SqrtF src));
2875 
2876   format %{ &quot;sqrtss  $dst, $src&quot; %}
2877   ins_cost(150);
2878   ins_encode %{
2879     __ sqrtss($dst$$XMMRegister, $src$$XMMRegister);
2880   %}
2881   ins_pipe(pipe_slow);
2882 %}
2883 
2884 instruct sqrtF_mem(regF dst, memory src) %{
2885   predicate(UseSSE&gt;=1);
2886   match(Set dst (SqrtF (LoadF src)));
2887 
2888   format %{ &quot;sqrtss  $dst, $src&quot; %}
2889   ins_cost(150);
2890   ins_encode %{
2891     __ sqrtss($dst$$XMMRegister, $src$$Address);
2892   %}
2893   ins_pipe(pipe_slow);
2894 %}
2895 
2896 instruct sqrtF_imm(regF dst, immF con) %{
2897   predicate(UseSSE&gt;=1);
2898   match(Set dst (SqrtF con));
2899 
2900   format %{ &quot;sqrtss  $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2901   ins_cost(150);
2902   ins_encode %{
2903     __ sqrtss($dst$$XMMRegister, $constantaddress($con));
2904   %}
2905   ins_pipe(pipe_slow);
2906 %}
2907 
2908 instruct sqrtD_reg(regD dst, regD src) %{
2909   predicate(UseSSE&gt;=2);
2910   match(Set dst (SqrtD src));
2911 
2912   format %{ &quot;sqrtsd  $dst, $src&quot; %}
2913   ins_cost(150);
2914   ins_encode %{
2915     __ sqrtsd($dst$$XMMRegister, $src$$XMMRegister);
2916   %}
2917   ins_pipe(pipe_slow);
2918 %}
2919 
2920 instruct sqrtD_mem(regD dst, memory src) %{
2921   predicate(UseSSE&gt;=2);
2922   match(Set dst (SqrtD (LoadD src)));
2923 
2924   format %{ &quot;sqrtsd  $dst, $src&quot; %}
2925   ins_cost(150);
2926   ins_encode %{
2927     __ sqrtsd($dst$$XMMRegister, $src$$Address);
2928   %}
2929   ins_pipe(pipe_slow);
2930 %}
2931 
2932 instruct sqrtD_imm(regD dst, immD con) %{
2933   predicate(UseSSE&gt;=2);
2934   match(Set dst (SqrtD con));
2935   format %{ &quot;sqrtsd  $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2936   ins_cost(150);
2937   ins_encode %{
2938     __ sqrtsd($dst$$XMMRegister, $constantaddress($con));
2939   %}
2940   ins_pipe(pipe_slow);
2941 %}
2942 
2943 
2944 #ifdef _LP64
2945 instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
2946   match(Set dst (RoundDoubleMode src rmode));
2947   format %{ &quot;roundsd $dst,$src&quot; %}
2948   ins_cost(150);
2949   ins_encode %{
2950     assert(UseSSE &gt;= 4, &quot;required&quot;);
2951     __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
2952   %}
2953   ins_pipe(pipe_slow);
2954 %}
2955 
2956 instruct roundD_mem(legRegD dst, memory src, immU8 rmode) %{
2957   match(Set dst (RoundDoubleMode (LoadD src) rmode));
2958   format %{ &quot;roundsd $dst,$src&quot; %}
2959   ins_cost(150);
2960   ins_encode %{
2961     assert(UseSSE &gt;= 4, &quot;required&quot;);
2962     __ roundsd($dst$$XMMRegister, $src$$Address, $rmode$$constant);
2963   %}
2964   ins_pipe(pipe_slow);
2965 %}
2966 
2967 instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{
2968   match(Set dst (RoundDoubleMode con rmode));
2969   effect(TEMP scratch_reg);
2970   format %{ &quot;roundsd $dst,[$constantaddress]\t# load from constant table: double=$con&quot; %}
2971   ins_cost(150);
2972   ins_encode %{
2973     assert(UseSSE &gt;= 4, &quot;required&quot;);
2974     __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);
2975   %}
2976   ins_pipe(pipe_slow);
2977 %}
2978 
2979 instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
2980   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
2981   match(Set dst (RoundDoubleModeV src rmode));
2982   format %{ &quot;vroundpd $dst,$src,$rmode\t! round packedD&quot; %}
2983   ins_encode %{
2984     assert(UseAVX &gt; 0, &quot;required&quot;);
2985     int vector_len = vector_length_encoding(this);
2986     __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vector_len);
2987   %}
2988   ins_pipe( pipe_slow );
2989 %}
2990 
2991 instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
2992   predicate(n-&gt;as_Vector()-&gt;length() == 8);
2993   match(Set dst (RoundDoubleModeV src rmode));
2994   format %{ &quot;vrndscalepd $dst,$src,$rmode\t! round packed8D&quot; %}
2995   ins_encode %{
2996     assert(UseAVX &gt; 2, &quot;required&quot;);
2997     __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
2998   %}
2999   ins_pipe( pipe_slow );
3000 %}
3001 
3002 instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
3003   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3004   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3005   format %{ &quot;vroundpd $dst, $mem, $rmode\t! round packedD&quot; %}
3006   ins_encode %{
3007     assert(UseAVX &gt; 0, &quot;required&quot;);
3008     int vector_len = vector_length_encoding(this);
3009     __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vector_len);
3010   %}
3011   ins_pipe( pipe_slow );
3012 %}
3013 
3014 instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
3015   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3016   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3017   format %{ &quot;vrndscalepd $dst,$mem,$rmode\t! round packed8D&quot; %}
3018   ins_encode %{
3019     assert(UseAVX &gt; 2, &quot;required&quot;);
3020     __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
3021   %}
3022   ins_pipe( pipe_slow );
3023 %}
3024 #endif // _LP64
3025 
3026 instruct onspinwait() %{
3027   match(OnSpinWait);
3028   ins_cost(200);
3029 
3030   format %{
3031     $$template
3032     $$emit$$&quot;pause\t! membar_onspinwait&quot;
3033   %}
3034   ins_encode %{
3035     __ pause();
3036   %}
3037   ins_pipe(pipe_slow);
3038 %}
3039 
3040 // a * b + c
3041 instruct fmaD_reg(regD a, regD b, regD c) %{
3042   predicate(UseFMA);
3043   match(Set c (FmaD  c (Binary a b)));
3044   format %{ &quot;fmasd $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3045   ins_cost(150);
3046   ins_encode %{
3047     __ fmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3048   %}
3049   ins_pipe( pipe_slow );
3050 %}
3051 
3052 // a * b + c
3053 instruct fmaF_reg(regF a, regF b, regF c) %{
3054   predicate(UseFMA);
3055   match(Set c (FmaF  c (Binary a b)));
3056   format %{ &quot;fmass $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3057   ins_cost(150);
3058   ins_encode %{
3059     __ fmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3060   %}
3061   ins_pipe( pipe_slow );
3062 %}
3063 
3064 // ====================VECTOR INSTRUCTIONS=====================================
3065 
3066 // Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
3067 instruct MoveVec2Leg(legVec dst, vec src) %{
3068   match(Set dst src);
3069   format %{ &quot;&quot; %}
3070   ins_encode %{
3071     ShouldNotReachHere();
3072   %}
3073   ins_pipe( fpu_reg_reg );
3074 %}
3075 
3076 instruct MoveLeg2Vec(vec dst, legVec src) %{
3077   match(Set dst src);
3078   format %{ &quot;&quot; %}
3079   ins_encode %{
3080     ShouldNotReachHere();
3081   %}
3082   ins_pipe( fpu_reg_reg );
3083 %}
3084 
3085 // ============================================================================
3086 
3087 // Load vectors
3088 instruct loadV(vec dst, memory mem) %{
3089   match(Set dst (LoadVector mem));
3090   ins_cost(125);
3091   format %{ &quot;load_vector $dst,$mem&quot; %}
3092   ins_encode %{
3093     switch (vector_length_in_bytes(this)) {
3094       case  4: __ movdl    ($dst$$XMMRegister, $mem$$Address); break;
3095       case  8: __ movq     ($dst$$XMMRegister, $mem$$Address); break;
3096       case 16: __ movdqu   ($dst$$XMMRegister, $mem$$Address); break;
3097       case 32: __ vmovdqu  ($dst$$XMMRegister, $mem$$Address); break;
3098       case 64: __ evmovdqul($dst$$XMMRegister, $mem$$Address, Assembler::AVX_512bit); break;
3099       default: ShouldNotReachHere();
3100     }
3101   %}
3102   ins_pipe( pipe_slow );
3103 %}
3104 
3105 // Store vectors generic operand pattern.
3106 instruct storeV(memory mem, vec src) %{
3107   match(Set mem (StoreVector mem src));
3108   ins_cost(145);
3109   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3110   ins_encode %{
3111     switch (vector_length_in_bytes(this, $src)) {
3112       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3113       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3114       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3115       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3116       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3117       default: ShouldNotReachHere();
3118     }
3119   %}
3120   ins_pipe( pipe_slow );
3121 %}
3122 
3123 // ====================REPLICATE=======================================
3124 
3125 // Replicate byte scalar to be vector
3126 instruct ReplB_reg(vec dst, rRegI src) %{
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
3142           assert(vlen == 32, &quot;sanity&quot;);
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
3151 instruct ReplB_mem(vec dst, memory mem) %{
3152   predicate(VM_Version::supports_avx2());
3153   match(Set dst (ReplicateB (LoadB mem)));
3154   format %{ &quot;replicateB $dst,$mem&quot; %}
3155   ins_encode %{
3156     int vector_len = vector_length_encoding(this);
3157     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3158   %}
3159   ins_pipe( pipe_slow );
3160 %}
3161 
3162 instruct ReplB_imm(vec dst, immI con) %{
3163   match(Set dst (ReplicateB con));
3164   format %{ &quot;replicateB $dst,$con&quot; %}
3165   ins_encode %{
3166     uint vlen = vector_length(this);
3167     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3168     if (vlen == 4) {
3169       __ movdl($dst$$XMMRegister, const_addr);
3170     } else {
3171       __ movq($dst$$XMMRegister, const_addr);
3172       if (vlen &gt;= 16) {
3173         if (VM_Version::supports_avx2()) {
3174           int vlen_enc = vector_length_encoding(this);
3175           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3176         } else {
3177           assert(vlen == 16, &quot;sanity&quot;);
3178           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3179         }
3180       }
3181     }
3182   %}
3183   ins_pipe( pipe_slow );
3184 %}
3185 
3186 // Replicate byte scalar zero to be vector
3187 instruct ReplB_zero(vec dst, immI0 zero) %{
3188   match(Set dst (ReplicateB zero));
3189   format %{ &quot;replicateB $dst,$zero&quot; %}
3190   ins_encode %{
3191     uint vlen = vector_length(this);
3192     if (vlen &lt;= 16) {
3193       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3194     } else {
3195       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3196       int vlen_enc = vector_length_encoding(this);
3197       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3198     }
3199   %}
3200   ins_pipe( fpu_reg_reg );
3201 %}
3202 
3203 // ====================ReplicateS=======================================
3204 
3205 instruct ReplS_reg(vec dst, rRegI src) %{
3206   match(Set dst (ReplicateS src));
3207   format %{ &quot;replicateS $dst,$src&quot; %}
3208   ins_encode %{
3209     uint vlen = vector_length(this);
3210     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3211       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW
3212       int vlen_enc = vector_length_encoding(this);
3213       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3214     } else {
3215       __ movdl($dst$$XMMRegister, $src$$Register);
3216       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3217       if (vlen &gt;= 8) {
3218         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3219         if (vlen &gt;= 16) {
3220           assert(vlen == 16, &quot;sanity&quot;);
3221           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3222         }
3223       }
3224     }
3225   %}
3226   ins_pipe( pipe_slow );
3227 %}
3228 
3229 instruct ReplS_mem(vec dst, memory mem) %{
3230   predicate(VM_Version::supports_avx2());
3231   match(Set dst (ReplicateS (LoadS mem)));
3232   format %{ &quot;replicateS $dst,$mem&quot; %}
3233   ins_encode %{
3234     int vlen_enc = vector_length_encoding(this);
3235     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
3236   %}
3237   ins_pipe( pipe_slow );
3238 %}
3239 
3240 instruct ReplS_imm(vec dst, immI con) %{
3241   match(Set dst (ReplicateS con));
3242   format %{ &quot;replicateS $dst,$con&quot; %}
3243   ins_encode %{
3244     uint vlen = vector_length(this);
3245     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));
3246     if (vlen == 2) {
3247       __ movdl($dst$$XMMRegister, const_addr);
3248     } else {
3249       __ movq($dst$$XMMRegister, const_addr);
3250       if (vlen &gt;= 8) {
3251         if (VM_Version::supports_avx2()) {
3252           int vlen_enc = vector_length_encoding(this);
3253           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3254         } else {
3255           assert(vlen == 8, &quot;sanity&quot;);
3256           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         }
3258       }
3259     }
3260   %}
3261   ins_pipe( fpu_reg_reg );
3262 %}
3263 
3264 instruct ReplS_zero(vec dst, immI0 zero) %{
3265   match(Set dst (ReplicateS zero));
3266   format %{ &quot;replicateS $dst,$zero&quot; %}
3267   ins_encode %{
3268     uint vlen = vector_length(this);
3269     if (vlen &lt;= 8) {
3270       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3271     } else {
3272       int vlen_enc = vector_length_encoding(this);
3273       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3274     }
3275   %}
3276   ins_pipe( fpu_reg_reg );
3277 %}
3278 
3279 // ====================ReplicateI=======================================
3280 
3281 instruct ReplI_reg(vec dst, rRegI src) %{
3282   match(Set dst (ReplicateI src));
3283   format %{ &quot;replicateI $dst,$src&quot; %}
3284   ins_encode %{
3285     uint vlen = vector_length(this);
3286     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3287       int vlen_enc = vector_length_encoding(this);
3288       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3289     } else {
3290       __ movdl($dst$$XMMRegister, $src$$Register);
3291       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3292       if (vlen &gt;= 8) {
3293         assert(vlen == 8, &quot;sanity&quot;);
3294         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3295       }
3296     }
3297   %}
3298   ins_pipe( pipe_slow );
3299 %}
3300 
3301 instruct ReplI_mem(vec dst, memory mem) %{
3302   match(Set dst (ReplicateI (LoadI mem)));
3303   format %{ &quot;replicateI $dst,$mem&quot; %}
3304   ins_encode %{
3305     uint vlen = vector_length(this);
3306     if (vlen &lt;= 4) {
3307       __ movdl($dst$$XMMRegister, $mem$$Address);
3308       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3309     } else {
3310       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3311       int vector_len = vector_length_encoding(this);
3312       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
3313     }
3314   %}
3315   ins_pipe( pipe_slow );
3316 %}
3317 
3318 instruct ReplI_imm(vec dst, immI con) %{
3319   match(Set dst (ReplicateI con));
3320   format %{ &quot;replicateI $dst,$con&quot; %}
3321   ins_encode %{
3322     uint vlen = vector_length(this);
3323     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));
3324     if (vlen &lt;= 4) {
3325       __ movq($dst$$XMMRegister, const_addr);
3326       if (vlen == 4) {
3327         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3328       }
3329     } else {
3330       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3331       int vector_len = vector_length_encoding(this);
3332       __ movq($dst$$XMMRegister, const_addr);
3333       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3334     }
3335   %}
3336   ins_pipe( pipe_slow );
3337 %}
3338 
3339 // Replicate integer (4 byte) scalar zero to be vector
3340 instruct ReplI_zero(vec dst, immI0 zero) %{
3341   match(Set dst (ReplicateI zero));
3342   format %{ &quot;replicateI $dst,$zero&quot; %}
3343   ins_encode %{
3344     uint vlen = vector_length(this);
3345     if (vlen &lt;= 4) {
3346       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3347     } else {
3348       int vlen_enc = vector_length_encoding(this);
3349       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3350     }
3351   %}
3352   ins_pipe( fpu_reg_reg );
3353 %}
3354 
3355 // ====================ReplicateL=======================================
3356 
3357 #ifdef _LP64
3358 // Replicate long (8 byte) scalar to be vector
3359 instruct ReplL_reg(vec dst, rRegL src) %{
3360   match(Set dst (ReplicateL src));
3361   format %{ &quot;replicateL $dst,$src&quot; %}
3362   ins_encode %{
3363     uint vlen = vector_length(this);
3364     if (vlen == 2) {
3365       __ movdq($dst$$XMMRegister, $src$$Register);
3366       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3367     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3368       int vlen_enc = vector_length_encoding(this);
3369       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3370     } else {
3371       assert(vlen == 4, &quot;sanity&quot;);
3372       __ movdq($dst$$XMMRegister, $src$$Register);
3373       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3374       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3375     }
3376   %}
3377   ins_pipe( pipe_slow );
3378 %}
3379 #else // _LP64
3380 // Replicate long (8 byte) scalar to be vector
3381 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3382   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3383   match(Set dst (ReplicateL src));
3384   effect(TEMP dst, USE src, TEMP tmp);
3385   format %{ &quot;replicateL $dst,$src&quot; %}
3386   ins_encode %{
3387     uint vlen = vector_length(this);
3388     if (vlen == 2) {
3389       __ movdl($dst$$XMMRegister, $src$$Register);
3390       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3391       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3392       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3393     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3394       int vector_len = Assembler::AVX_256bit;
3395       __ movdl($dst$$XMMRegister, $src$$Register);
3396       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3397       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3398       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3399     } else {
3400       __ movdl($dst$$XMMRegister, $src$$Register);
3401       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3402       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3403       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3404       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3405     }
3406   %}
3407   ins_pipe( pipe_slow );
3408 %}
3409 
3410 instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{
3411   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3412   match(Set dst (ReplicateL src));
3413   effect(TEMP dst, USE src, TEMP tmp);
3414   format %{ &quot;replicateL $dst,$src&quot; %}
3415   ins_encode %{
3416     if (VM_Version::supports_avx512vl()) {
3417       __ movdl($dst$$XMMRegister, $src$$Register);
3418       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3419       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3420       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3421       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3422       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3423     } else {
3424       int vector_len = Assembler::AVX_512bit;
3425       __ movdl($dst$$XMMRegister, $src$$Register);
3426       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3427       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3428       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3429     }
3430   %}
3431   ins_pipe( pipe_slow );
3432 %}
3433 #endif // _LP64
3434 
3435 instruct ReplL_mem(vec dst, memory mem) %{
3436   match(Set dst (ReplicateL (LoadL mem)));
3437   format %{ &quot;replicateL $dst,$mem&quot; %}
3438   ins_encode %{
3439     uint vlen = vector_length(this);
3440     if (vlen == 2) {
3441       __ movq($dst$$XMMRegister, $mem$$Address);
3442       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3443     } else {
3444       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3445       int vlen_enc = vector_length_encoding(this);
3446       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
3447     }
3448   %}
3449   ins_pipe( pipe_slow );
3450 %}
3451 
3452 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3453 instruct ReplL_imm(vec dst, immL con) %{
3454   match(Set dst (ReplicateL con));
3455   format %{ &quot;replicateL $dst,$con&quot; %}
3456   ins_encode %{
3457     uint vlen = vector_length(this);
3458     InternalAddress const_addr = $constantaddress($con);
3459     if (vlen == 2) {
3460       __ movq($dst$$XMMRegister, const_addr);
3461       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3462     } else {
3463       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3464       int vlen_enc = vector_length_encoding(this);
3465       __ movq($dst$$XMMRegister, const_addr);
3466       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3467     }
3468   %}
3469   ins_pipe( pipe_slow );
3470 %}
3471 
3472 instruct ReplL_zero(vec dst, immL0 zero) %{
3473   match(Set dst (ReplicateL zero));
3474   format %{ &quot;replicateL $dst,$zero&quot; %}
3475   ins_encode %{
3476     int vlen = vector_length(this);
3477     if (vlen == 2) {
3478       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3479     } else {
3480       int vlen_enc = vector_length_encoding(this);
3481       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3482     }
3483   %}
3484   ins_pipe( fpu_reg_reg );
3485 %}
3486 
3487 // ====================ReplicateF=======================================
3488 
3489 instruct ReplF_reg(vec dst, vlRegF src) %{
3490   match(Set dst (ReplicateF src));
3491   format %{ &quot;replicateF $dst,$src&quot; %}
3492   ins_encode %{
3493     uint vlen = vector_length(this);
3494     if (vlen &lt;= 4) {
3495       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3496    } else if (VM_Version::supports_avx2()) {
3497       int vector_len = vector_length_encoding(this);
3498       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
3499     } else {
3500       assert(vlen == 8, &quot;sanity&quot;);
3501       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3502       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3503     }
3504   %}
3505   ins_pipe( pipe_slow );
3506 %}
3507 
3508 instruct ReplF_mem(vec dst, memory mem) %{
3509   match(Set dst (ReplicateF (LoadF mem)));
3510   format %{ &quot;replicateF $dst,$mem&quot; %}
3511   ins_encode %{
3512     uint vlen = vector_length(this);
3513     if (vlen &lt;= 4) {
3514       __ movdl($dst$$XMMRegister, $mem$$Address);
3515       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3516     } else {
3517       assert(VM_Version::supports_avx(), &quot;sanity&quot;);
3518       int vector_len = vector_length_encoding(this);
3519       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
3520     }
3521   %}
3522   ins_pipe( pipe_slow );
3523 %}
3524 
3525 instruct ReplF_zero(vec dst, immF0 zero) %{
3526   match(Set dst (ReplicateF zero));
3527   format %{ &quot;replicateF $dst,$zero&quot; %}
3528   ins_encode %{
3529     uint vlen = vector_length(this);
3530     if (vlen &lt;= 4) {
3531       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3532     } else {
3533       int vlen_enc = vector_length_encoding(this);
3534       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3535     }
3536   %}
3537   ins_pipe( fpu_reg_reg );
3538 %}
3539 
3540 // ====================ReplicateD=======================================
3541 
3542 // Replicate double (8 bytes) scalar to be vector
3543 instruct ReplD_reg(vec dst, vlRegD src) %{
3544   match(Set dst (ReplicateD src));
3545   format %{ &quot;replicateD $dst,$src&quot; %}
3546   ins_encode %{
3547     uint vlen = vector_length(this);
3548     if (vlen == 2) {
3549       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3550     } else if (VM_Version::supports_avx2()) {
3551       int vector_len = vector_length_encoding(this);
3552       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
3553     } else {
3554       assert(vlen == 4, &quot;sanity&quot;);
3555       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3556       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3557     }
3558   %}
3559   ins_pipe( pipe_slow );
3560 %}
3561 
3562 instruct ReplD_mem(vec dst, memory mem) %{
3563   match(Set dst (ReplicateD (LoadD mem)));
3564   format %{ &quot;replicateD $dst,$mem&quot; %}
3565   ins_encode %{
3566     uint vlen = vector_length(this);
3567     if (vlen == 2) {
3568       __ movq($dst$$XMMRegister, $mem$$Address);
3569       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);
3570     } else {
3571       assert(VM_Version::supports_avx(), &quot;sanity&quot;);
3572       int vector_len = vector_length_encoding(this);
3573       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
3574     }
3575   %}
3576   ins_pipe( pipe_slow );
3577 %}
3578 
3579 instruct ReplD_zero(vec dst, immD0 zero) %{
3580   match(Set dst (ReplicateD zero));
3581   format %{ &quot;replicateD $dst,$zero&quot; %}
3582   ins_encode %{
3583     uint vlen = vector_length(this);
3584     if (vlen == 2) {
3585       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3586     } else {
3587       int vlen_enc = vector_length_encoding(this);
3588       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3589     }
3590   %}
3591   ins_pipe( fpu_reg_reg );
3592 %}
3593 
3594 // ====================REDUCTION ARITHMETIC=======================================
3595 
3596 // =======================AddReductionVI==========================================
3597 
3598 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3599   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3600   match(Set dst (AddReductionVI src1 src2));
3601   effect(TEMP tmp, TEMP tmp2);
3602   format %{ &quot;vector_add2I_reduction $dst,$src1,$src2&quot; %}
3603   ins_encode %{
3604     if (UseAVX &gt; 2) {
3605       int vector_len = Assembler::AVX_128bit;
3606       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3607       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3608       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3609       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3610       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3611     } else if (VM_Version::supports_avxonly()) {
3612       int vector_len = Assembler::AVX_128bit;
3613       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
3614       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3615       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
3616       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3617     } else {
3618       assert(UseSSE &gt; 2, &quot;required&quot;);
3619       __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);
3620       __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);
3621       __ movdl($tmp$$XMMRegister, $src1$$Register);
3622       __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);
3623       __ movdl($dst$$Register, $tmp$$XMMRegister);
3624     }
3625   %}
3626   ins_pipe( pipe_slow );
3627 %}
3628 
3629 instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3630   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3631   match(Set dst (AddReductionVI src1 src2));
3632   effect(TEMP tmp, TEMP tmp2);
3633   format %{ &quot;vector_add4I_reduction $dst,$src1,$src2&quot; %}
3634   ins_encode %{
3635     if (UseAVX &gt; 2) {
3636       int vector_len = Assembler::AVX_128bit;
3637       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
3638       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3639       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3640       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3641       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3642       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3643       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3644     } else if (VM_Version::supports_avxonly()) {
3645       int vector_len = Assembler::AVX_128bit;
3646       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
3647       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
3648       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3649       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
3650       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3651     } else {
3652       assert(UseSSE &gt; 2, &quot;required&quot;);
3653       __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);
3654       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
3655       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
3656       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3657       __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);
3658       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3659     }
3660   %}
3661   ins_pipe( pipe_slow );
3662 %}
3663 
3664 instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3665   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
3666   match(Set dst (AddReductionVI src1 src2));
3667   effect(TEMP tmp, TEMP tmp2);
3668   format %{ &quot;vector_add8I_reduction $dst,$src1,$src2&quot; %}
3669   ins_encode %{
3670     if (UseAVX &gt; 2) {
3671       int vector_len = Assembler::AVX_128bit;
3672       __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
3673       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
3674       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
3675       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3676       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3677       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3678       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3679       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3680       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3681     } else {
3682       assert(UseAVX &gt; 0, &quot;&quot;);
3683       int vector_len = Assembler::AVX_256bit;
3684       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
3685       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3686       __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);
3687       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3688       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3689       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3690       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3691     }
3692   %}
3693   ins_pipe( pipe_slow );
3694 %}
3695 
3696 instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
3697   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
3698   match(Set dst (AddReductionVI src1 src2));
3699   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
3700   format %{ &quot;vector_add16I_reduction $dst,$src1,$src2&quot; %}
3701   ins_encode %{
3702     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
3703     __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
3704     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
3705     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
3706     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
3707     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3708     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3709     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3710     __ movdl($tmp2$$XMMRegister, $src1$$Register);
3711     __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3712     __ movdl($dst$$Register, $tmp2$$XMMRegister);
3713   %}
3714   ins_pipe( pipe_slow );
3715 %}
3716 
3717 // =======================AddReductionVL==========================================
3718 
3719 #ifdef _LP64
3720 instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
3721   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3722   match(Set dst (AddReductionVL src1 src2));
3723   effect(TEMP tmp, TEMP tmp2);
3724   format %{ &quot;vector_add2L_reduction $dst,$src1,$src2&quot; %}
3725   ins_encode %{
3726     assert(UseAVX &gt; 2, &quot;required&quot;);
3727     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
3728     __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
3729     __ movdq($tmp2$$XMMRegister, $src1$$Register);
3730     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3731     __ movdq($dst$$Register, $tmp2$$XMMRegister);
3732   %}
3733   ins_pipe( pipe_slow );
3734 %}
3735 
3736 instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
3737   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3738   match(Set dst (AddReductionVL src1 src2));
3739   effect(TEMP tmp, TEMP tmp2);
3740   format %{ &quot;vector_add4L_reduction $dst,$src1,$src2&quot; %}
3741   ins_encode %{
3742     assert(UseAVX &gt; 2, &quot;required&quot;);
3743     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
3744     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
3745     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
3746     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3747     __ movdq($tmp$$XMMRegister, $src1$$Register);
3748     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3749     __ movdq($dst$$Register, $tmp2$$XMMRegister);
3750   %}
3751   ins_pipe( pipe_slow );
3752 %}
3753 
3754 instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
3755   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
3756   match(Set dst (AddReductionVL src1 src2));
3757   effect(TEMP tmp, TEMP tmp2);
3758   format %{ &quot;vector_addL_reduction $dst,$src1,$src2&quot; %}
3759   ins_encode %{
3760     assert(UseAVX &gt; 2, &quot;required&quot;);
3761     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
3762     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
3763     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
3764     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3765     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
3766     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3767     __ movdq($tmp$$XMMRegister, $src1$$Register);
3768     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3769     __ movdq($dst$$Register, $tmp2$$XMMRegister);
3770   %}
3771   ins_pipe( pipe_slow );
3772 %}
3773 #endif // _LP64
3774 
3775 // =======================AddReductionVF==========================================
3776 
3777 instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{
3778   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3779   match(Set dst (AddReductionVF dst src2));
3780   effect(TEMP dst, TEMP tmp);
3781   format %{ &quot;vector_add2F_reduction $dst,$dst,$src2&quot; %}
3782   ins_encode %{
3783     if (UseAVX &gt; 0) {
3784       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3785       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
3786       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3787     } else {
3788       assert(UseSSE &gt; 0, &quot;required&quot;);
3789       __ addss($dst$$XMMRegister, $src2$$XMMRegister);
3790       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
3791       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
3792     }
3793   %}
3794   ins_pipe( pipe_slow );
3795 %}
3796 
3797 instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{
3798   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3799   match(Set dst (AddReductionVF dst src2));
3800   effect(TEMP dst, TEMP tmp);
3801   format %{ &quot;vector_add4F_reduction $dst,$dst,$src2&quot; %}
3802   ins_encode %{
3803     if (UseAVX &gt; 0) {
3804       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3805       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
3806       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3807       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
3808       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3809       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
3810       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3811     } else {
3812       assert(UseSSE &gt; 0, &quot;required&quot;);
3813       __ addss($dst$$XMMRegister, $src2$$XMMRegister);
3814       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
3815       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
3816       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
3817       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
3818       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
3819       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
3820     }
3821   %}
3822   ins_pipe( pipe_slow );
3823 %}
3824 
3825 
3826 instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
3827   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
3828   match(Set dst (AddReductionVF dst src2));
3829   effect(TEMP tmp, TEMP dst, TEMP tmp2);
3830   format %{ &quot;vector_add8F_reduction $dst,$dst,$src2&quot; %}
3831   ins_encode %{
3832     assert(UseAVX &gt; 0, &quot;required&quot;);
3833     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3834     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
3835     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3836     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
3837     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3838     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
3839     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3840     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
3841     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3842     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
3843     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3844     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
3845     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3846     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
3847     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3848   %}
3849   ins_pipe( pipe_slow );
3850 %}
3851 
3852 instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
3853   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
3854   match(Set dst (AddReductionVF dst src2));
3855   effect(TEMP tmp, TEMP dst, TEMP tmp2);
3856   format %{ &quot;vector_add16F_reduction $dst,$dst,$src2&quot; %}
3857   ins_encode %{
3858     assert(UseAVX &gt; 2, &quot;required&quot;);
3859     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3860     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
3861     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3862     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
3863     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3864     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
3865     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3866     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3867     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3868     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
3869     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3870     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
3871     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3872     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
3873     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3874     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
3875     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3876     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
3877     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3878     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
3879     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3880     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
3881     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3882     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
3883     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3884     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
3885     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3886     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
3887     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3888     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
3889     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3890   %}
3891   ins_pipe( pipe_slow );
3892 %}
3893 
3894 // =======================AddReductionVD==========================================
3895 
3896 instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{
3897   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3898   match(Set dst (AddReductionVD dst src2));
3899   effect(TEMP tmp, TEMP dst);
3900   format %{ &quot;vector_add2D_reduction  $dst,$src2&quot; %}
3901   ins_encode %{
3902     if (UseAVX &gt; 0) {
3903       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3904       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
3905       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3906     } else {
3907       assert(UseSSE &gt; 0, &quot;required&quot;);
3908       __ addsd($dst$$XMMRegister, $src2$$XMMRegister);
3909       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
3910       __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);
3911     }
3912   %}
3913   ins_pipe( pipe_slow );
3914 %}
3915 
3916 instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
3917   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3918   match(Set dst (AddReductionVD dst src2));
3919   effect(TEMP tmp, TEMP dst, TEMP tmp2);
3920   format %{ &quot;vector_add4D_reduction $dst,$dst,$src2&quot; %}
3921   ins_encode %{
3922     assert(UseAVX &gt; 0, &quot;required&quot;);
3923     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3924     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
3925     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3926     __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3927     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3928     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
3929     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3930   %}
3931   ins_pipe( pipe_slow );
3932 %}
3933 
3934 instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
3935   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
3936   match(Set dst (AddReductionVD dst src2));
3937   effect(TEMP tmp, TEMP dst, TEMP tmp2);
3938   format %{ &quot;vector_add8D_reduction $dst,$dst,$src2&quot; %}
3939   ins_encode %{
3940     assert(UseAVX &gt; 2, &quot;required&quot;);
3941     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
3942     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
3943     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3944     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3945     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3946     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
3947     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3948     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
3949     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3950     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
3951     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3952     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
3953     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
3954     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
3955     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
3956   %}
3957   ins_pipe( pipe_slow );
3958 %}
3959 
3960 // =======================MulReductionVI==========================================
3961 
3962 instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3963   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3964   match(Set dst (MulReductionVI src1 src2));
3965   effect(TEMP tmp, TEMP tmp2);
3966   format %{ &quot;vector_mul2I_reduction $dst,$src1,$src2&quot; %}
3967   ins_encode %{
3968     if (UseAVX &gt; 0) {
3969       int vector_len = Assembler::AVX_128bit;
3970       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3971       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3972       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3973       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3974       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3975     } else {
3976       assert(UseSSE &gt; 3, &quot;required&quot;);
3977       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3978       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
3979       __ movdl($tmp$$XMMRegister, $src1$$Register);
3980       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
3981       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3982     }
3983   %}
3984   ins_pipe( pipe_slow );
3985 %}
3986 
3987 instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3988   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3989   match(Set dst (MulReductionVI src1 src2));
3990   effect(TEMP tmp, TEMP tmp2);
3991   format %{ &quot;vector_mul4I_reduction $dst,$src1,$src2&quot; %}
3992   ins_encode %{
3993     if (UseAVX &gt; 0) {
3994       int vector_len = Assembler::AVX_128bit;
3995       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
3996       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3997       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3998       __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3999       __ movdl($tmp2$$XMMRegister, $src1$$Register);
4000       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4001       __ movdl($dst$$Register, $tmp2$$XMMRegister);
4002     } else {
4003       assert(UseSSE &gt; 3, &quot;required&quot;);
4004       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
4005       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
4006       __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);
4007       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
4008       __ movdl($tmp$$XMMRegister, $src1$$Register);
4009       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
4010       __ movdl($dst$$Register, $tmp2$$XMMRegister);
4011     }
4012   %}
4013   ins_pipe( pipe_slow );
4014 %}
4015 
4016 instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
4017   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4018   match(Set dst (MulReductionVI src1 src2));
4019   effect(TEMP tmp, TEMP tmp2);
4020   format %{ &quot;vector_mul8I_reduction $dst,$src1,$src2&quot; %}
4021   ins_encode %{
4022     assert(UseAVX &gt; 1, &quot;required&quot;);
4023     int vector_len = Assembler::AVX_128bit;
4024     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
4025     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
4026     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
4027     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4028     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
4029     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4030     __ movdl($tmp2$$XMMRegister, $src1$$Register);
4031     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4032     __ movdl($dst$$Register, $tmp2$$XMMRegister);
4033   %}
4034   ins_pipe( pipe_slow );
4035 %}
4036 
4037 instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
4038   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
4039   match(Set dst (MulReductionVI src1 src2));
4040   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
4041   format %{ &quot;vector_mul16I_reduction $dst,$src1,$src2&quot; %}
4042   ins_encode %{
4043     assert(UseAVX &gt; 2, &quot;required&quot;);
4044     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
4045     __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
4046     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
4047     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
4048     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
4049     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4050     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
4051     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4052     __ movdl($tmp2$$XMMRegister, $src1$$Register);
4053     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4054     __ movdl($dst$$Register, $tmp2$$XMMRegister);
4055   %}
4056   ins_pipe( pipe_slow );
4057 %}
4058 
4059 // =======================MulReductionVL==========================================
4060 
4061 #ifdef _LP64
4062 instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
4063   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4064   match(Set dst (MulReductionVL src1 src2));
4065   effect(TEMP tmp, TEMP tmp2);
4066   format %{ &quot;vector_mul2L_reduction $dst,$src1,$src2&quot; %}
4067   ins_encode %{
4068     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);
4069     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
4070     __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
4071     __ movdq($tmp2$$XMMRegister, $src1$$Register);
4072     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4073     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4074   %}
4075   ins_pipe( pipe_slow );
4076 %}
4077 
4078 instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
4079   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4080   match(Set dst (MulReductionVL src1 src2));
4081   effect(TEMP tmp, TEMP tmp2);
4082   format %{ &quot;vector_mul4L_reduction $dst,$src1,$src2&quot; %}
4083   ins_encode %{
4084     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);
4085     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
4086     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
4087     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4088     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4089     __ movdq($tmp$$XMMRegister, $src1$$Register);
4090     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4091     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4092   %}
4093   ins_pipe( pipe_slow );
4094 %}
4095 
4096 instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
4097   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4098   match(Set dst (MulReductionVL src1 src2));
4099   effect(TEMP tmp, TEMP tmp2);
4100   format %{ &quot;vector_mul8L_reduction $dst,$src1,$src2&quot; %}
4101   ins_encode %{
4102     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);
4103     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4104     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
4105     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
4106     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4107     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4108     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4109     __ movdq($tmp$$XMMRegister, $src1$$Register);
4110     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4111     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4112   %}
4113   ins_pipe( pipe_slow );
4114 %}
4115 #endif
4116 
4117 // =======================MulReductionVF==========================================
4118 
4119 instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{
4120   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4121   match(Set dst (MulReductionVF dst src2));
4122   effect(TEMP dst, TEMP tmp);
4123   format %{ &quot;vector_mul2F_reduction $dst,$dst,$src2&quot; %}
4124   ins_encode %{
4125     if (UseAVX &gt; 0) {
4126       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4127       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4128       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4129     } else {
4130       assert(UseSSE &gt; 0, &quot;required&quot;);
4131       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
4132       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4133       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4134     }
4135   %}
4136   ins_pipe( pipe_slow );
4137 %}
4138 
4139 instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{
4140   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4141   match(Set dst (MulReductionVF dst src2));
4142   effect(TEMP dst, TEMP tmp);
4143   format %{ &quot;vector_mul4F_reduction $dst,$dst,$src2&quot; %}
4144   ins_encode %{
4145     if (UseAVX &gt; 0) {
4146       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4147       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4148       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4149       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4150       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4151       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4152       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4153     } else {
4154       assert(UseSSE &gt; 0, &quot;required&quot;);
4155       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
4156       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4157       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4158       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4159       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4160       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4161       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4162     }
4163   %}
4164   ins_pipe( pipe_slow );
4165 %}
4166 
4167 instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
4168   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4169   match(Set dst (MulReductionVF dst src2));
4170   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4171   format %{ &quot;vector_mul8F_reduction $dst,$dst,$src2&quot; %}
4172   ins_encode %{
4173     assert(UseAVX &gt; 0, &quot;required&quot;);
4174     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4175     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4176     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4177     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4178     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4179     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4180     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4181     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4182     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4183     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4184     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4185     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4186     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4187     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4188     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4189   %}
4190   ins_pipe( pipe_slow );
4191 %}
4192 
4193 instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
4194   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
4195   match(Set dst (MulReductionVF dst src2));
4196   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4197   format %{ &quot;vector_mul16F_reduction $dst,$dst,$src2&quot; %}
4198   ins_encode %{
4199     assert(UseAVX &gt; 2, &quot;required&quot;);
4200     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4201     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4202     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4203     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4204     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4205     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4206     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4207     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4208     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4209     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4210     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4211     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4212     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4213     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4214     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4215     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
4216     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4217     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4218     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4219     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4220     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4221     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4222     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4223     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
4224     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4225     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4226     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4227     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4228     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4229     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4230     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4231   %}
4232   ins_pipe( pipe_slow );
4233 %}
4234 
4235 // =======================MulReductionVD==========================================
4236 
4237 instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{
4238   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4239   match(Set dst (MulReductionVD dst src2));
4240   effect(TEMP dst, TEMP tmp);
4241   format %{ &quot;vector_mul2D_reduction $dst,$dst,$src2&quot; %}
4242   ins_encode %{
4243     if (UseAVX &gt; 0) {
4244       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4245       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4246       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4247     } else {
4248       assert(UseSSE &gt; 0, &quot;required&quot;);
4249       __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);
4250       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4251       __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);
4252     }
4253   %}
4254   ins_pipe( pipe_slow );
4255 %}
4256 
4257 
4258 instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
4259   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 2
4260   match(Set dst (MulReductionVD dst src2));
4261   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4262   format %{ &quot;vector_mul4D_reduction  $dst,$dst,$src2&quot; %}
4263   ins_encode %{
4264     assert(UseAVX &gt; 0, &quot;required&quot;);
4265     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4266     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4267     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4268     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4269     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4270     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4271     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4272   %}
4273   ins_pipe( pipe_slow );
4274 %}
4275 
4276 instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
4277   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 2
4278   match(Set dst (MulReductionVD dst src2));
4279   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4280   format %{ &quot;vector_mul8D_reduction $dst,$dst,$src2&quot; %}
4281   ins_encode %{
4282     assert(UseAVX &gt; 0, &quot;required&quot;);
4283     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4284     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4285     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4286     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4287     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4288     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4289     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4290     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
4291     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4292     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4293     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4294     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
4295     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4296     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4297     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4298   %}
4299   ins_pipe( pipe_slow );
4300 %}
4301 
4302 // ====================VECTOR ARITHMETIC=======================================
4303 
4304 // --------------------------------- ADD --------------------------------------
4305 
4306 // Bytes vector add
4307 instruct vaddB(vec dst, vec src) %{
4308   predicate(UseAVX == 0);
4309   match(Set dst (AddVB dst src));
4310   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
4311   ins_encode %{
4312     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
4313   %}
4314   ins_pipe( pipe_slow );
4315 %}
4316 
4317 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
4318   predicate(UseAVX &gt; 0);
4319   match(Set dst (AddVB src1 src2));
4320   format %{ &quot;vpaddb  $dst,$src1,$src2\t! add packedB&quot; %}
4321   ins_encode %{
4322     int vector_len = vector_length_encoding(this);
4323     __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4324   %}
4325   ins_pipe( pipe_slow );
4326 %}
4327 
4328 instruct vaddB_mem(vec dst, vec src, memory mem) %{
4329   predicate(UseAVX &gt; 0);
4330   match(Set dst (AddVB src (LoadVector mem)));
4331   format %{ &quot;vpaddb  $dst,$src,$mem\t! add packedB&quot; %}
4332   ins_encode %{
4333     int vector_len = vector_length_encoding(this);
4334     __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4335   %}
4336   ins_pipe( pipe_slow );
4337 %}
4338 
4339 // Shorts/Chars vector add
4340 instruct vaddS(vec dst, vec src) %{
4341   predicate(UseAVX == 0);
4342   match(Set dst (AddVS dst src));
4343   format %{ &quot;paddw   $dst,$src\t! add packedS&quot; %}
4344   ins_encode %{
4345     __ paddw($dst$$XMMRegister, $src$$XMMRegister);
4346   %}
4347   ins_pipe( pipe_slow );
4348 %}
4349 
4350 instruct vaddS_reg(vec dst, vec src1, vec src2) %{
4351   predicate(UseAVX &gt; 0);
4352   match(Set dst (AddVS src1 src2));
4353   format %{ &quot;vpaddw  $dst,$src1,$src2\t! add packedS&quot; %}
4354   ins_encode %{
4355     int vector_len = vector_length_encoding(this);
4356     __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4357   %}
4358   ins_pipe( pipe_slow );
4359 %}
4360 
4361 instruct vaddS_mem(vec dst, vec src, memory mem) %{
4362   predicate(UseAVX &gt; 0);
4363   match(Set dst (AddVS src (LoadVector mem)));
4364   format %{ &quot;vpaddw  $dst,$src,$mem\t! add packedS&quot; %}
4365   ins_encode %{
4366     int vector_len = vector_length_encoding(this);
4367     __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4368   %}
4369   ins_pipe( pipe_slow );
4370 %}
4371 
4372 // Integers vector add
4373 instruct vaddI(vec dst, vec src) %{
4374   predicate(UseAVX == 0);
4375   match(Set dst (AddVI dst src));
4376   format %{ &quot;paddd   $dst,$src\t! add packedI&quot; %}
4377   ins_encode %{
4378     __ paddd($dst$$XMMRegister, $src$$XMMRegister);
4379   %}
4380   ins_pipe( pipe_slow );
4381 %}
4382 
4383 instruct vaddI_reg(vec dst, vec src1, vec src2) %{
4384   predicate(UseAVX &gt; 0);
4385   match(Set dst (AddVI src1 src2));
4386   format %{ &quot;vpaddd  $dst,$src1,$src2\t! add packedI&quot; %}
4387   ins_encode %{
4388     int vector_len = vector_length_encoding(this);
4389     __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4390   %}
4391   ins_pipe( pipe_slow );
4392 %}
4393 
4394 
4395 instruct vaddI_mem(vec dst, vec src, memory mem) %{
4396   predicate(UseAVX &gt; 0);
4397   match(Set dst (AddVI src (LoadVector mem)));
4398   format %{ &quot;vpaddd  $dst,$src,$mem\t! add packedI&quot; %}
4399   ins_encode %{
4400     int vector_len = vector_length_encoding(this);
4401     __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4402   %}
4403   ins_pipe( pipe_slow );
4404 %}
4405 
4406 // Longs vector add
4407 instruct vaddL(vec dst, vec src) %{
4408   predicate(UseAVX == 0);
4409   match(Set dst (AddVL dst src));
4410   format %{ &quot;paddq   $dst,$src\t! add packedL&quot; %}
4411   ins_encode %{
4412     __ paddq($dst$$XMMRegister, $src$$XMMRegister);
4413   %}
4414   ins_pipe( pipe_slow );
4415 %}
4416 
4417 instruct vaddL_reg(vec dst, vec src1, vec src2) %{
4418   predicate(UseAVX &gt; 0);
4419   match(Set dst (AddVL src1 src2));
4420   format %{ &quot;vpaddq  $dst,$src1,$src2\t! add packedL&quot; %}
4421   ins_encode %{
4422     int vector_len = vector_length_encoding(this);
4423     __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4424   %}
4425   ins_pipe( pipe_slow );
4426 %}
4427 
4428 instruct vaddL_mem(vec dst, vec src, memory mem) %{
4429   predicate(UseAVX &gt; 0);
4430   match(Set dst (AddVL src (LoadVector mem)));
4431   format %{ &quot;vpaddq  $dst,$src,$mem\t! add packedL&quot; %}
4432   ins_encode %{
4433     int vector_len = vector_length_encoding(this);
4434     __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4435   %}
4436   ins_pipe( pipe_slow );
4437 %}
4438 
4439 // Floats vector add
4440 instruct vaddF(vec dst, vec src) %{
4441   predicate(UseAVX == 0);
4442   match(Set dst (AddVF dst src));
4443   format %{ &quot;addps   $dst,$src\t! add packedF&quot; %}
4444   ins_encode %{
4445     __ addps($dst$$XMMRegister, $src$$XMMRegister);
4446   %}
4447   ins_pipe( pipe_slow );
4448 %}
4449 
4450 instruct vaddF_reg(vec dst, vec src1, vec src2) %{
4451   predicate(UseAVX &gt; 0);
4452   match(Set dst (AddVF src1 src2));
4453   format %{ &quot;vaddps  $dst,$src1,$src2\t! add packedF&quot; %}
4454   ins_encode %{
4455     int vector_len = vector_length_encoding(this);
4456     __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4457   %}
4458   ins_pipe( pipe_slow );
4459 %}
4460 
4461 instruct vaddF_mem(vec dst, vec src, memory mem) %{
4462   predicate(UseAVX &gt; 0);
4463   match(Set dst (AddVF src (LoadVector mem)));
4464   format %{ &quot;vaddps  $dst,$src,$mem\t! add packedF&quot; %}
4465   ins_encode %{
4466     int vector_len = vector_length_encoding(this);
4467     __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4468   %}
4469   ins_pipe( pipe_slow );
4470 %}
4471 
4472 // Doubles vector add
4473 instruct vaddD(vec dst, vec src) %{
4474   predicate(UseAVX == 0);
4475   match(Set dst (AddVD dst src));
4476   format %{ &quot;addpd   $dst,$src\t! add packedD&quot; %}
4477   ins_encode %{
4478     __ addpd($dst$$XMMRegister, $src$$XMMRegister);
4479   %}
4480   ins_pipe( pipe_slow );
4481 %}
4482 
4483 instruct vaddD_reg(vec dst, vec src1, vec src2) %{
4484   predicate(UseAVX &gt; 0);
4485   match(Set dst (AddVD src1 src2));
4486   format %{ &quot;vaddpd  $dst,$src1,$src2\t! add packedD&quot; %}
4487   ins_encode %{
4488     int vector_len = vector_length_encoding(this);
4489     __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4490   %}
4491   ins_pipe( pipe_slow );
4492 %}
4493 
4494 instruct vaddD_mem(vec dst, vec src, memory mem) %{
4495   predicate(UseAVX &gt; 0);
4496   match(Set dst (AddVD src (LoadVector mem)));
4497   format %{ &quot;vaddpd  $dst,$src,$mem\t! add packedD&quot; %}
4498   ins_encode %{
4499     int vector_len = vector_length_encoding(this);
4500     __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4501   %}
4502   ins_pipe( pipe_slow );
4503 %}
4504 
4505 // --------------------------------- SUB --------------------------------------
4506 
4507 // Bytes vector sub
4508 instruct vsubB(vec dst, vec src) %{
4509   predicate(UseAVX == 0);
4510   match(Set dst (SubVB dst src));
4511   format %{ &quot;psubb   $dst,$src\t! sub packedB&quot; %}
4512   ins_encode %{
4513     __ psubb($dst$$XMMRegister, $src$$XMMRegister);
4514   %}
4515   ins_pipe( pipe_slow );
4516 %}
4517 
4518 instruct vsubB_reg(vec dst, vec src1, vec src2) %{
4519   predicate(UseAVX &gt; 0);
4520   match(Set dst (SubVB src1 src2));
4521   format %{ &quot;vpsubb  $dst,$src1,$src2\t! sub packedB&quot; %}
4522   ins_encode %{
4523     int vector_len = vector_length_encoding(this);
4524     __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4525   %}
4526   ins_pipe( pipe_slow );
4527 %}
4528 
4529 instruct vsubB_mem(vec dst, vec src, memory mem) %{
4530   predicate(UseAVX &gt; 0);
4531   match(Set dst (SubVB src (LoadVector mem)));
4532   format %{ &quot;vpsubb  $dst,$src,$mem\t! sub packedB&quot; %}
4533   ins_encode %{
4534     int vector_len = vector_length_encoding(this);
4535     __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4536   %}
4537   ins_pipe( pipe_slow );
4538 %}
4539 
4540 // Shorts/Chars vector sub
4541 instruct vsubS(vec dst, vec src) %{
4542   predicate(UseAVX == 0);
4543   match(Set dst (SubVS dst src));
4544   format %{ &quot;psubw   $dst,$src\t! sub packedS&quot; %}
4545   ins_encode %{
4546     __ psubw($dst$$XMMRegister, $src$$XMMRegister);
4547   %}
4548   ins_pipe( pipe_slow );
4549 %}
4550 
4551 
4552 instruct vsubS_reg(vec dst, vec src1, vec src2) %{
4553   predicate(UseAVX &gt; 0);
4554   match(Set dst (SubVS src1 src2));
4555   format %{ &quot;vpsubw  $dst,$src1,$src2\t! sub packedS&quot; %}
4556   ins_encode %{
4557     int vector_len = vector_length_encoding(this);
4558     __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4559   %}
4560   ins_pipe( pipe_slow );
4561 %}
4562 
4563 instruct vsubS_mem(vec dst, vec src, memory mem) %{
4564   predicate(UseAVX &gt; 0);
4565   match(Set dst (SubVS src (LoadVector mem)));
4566   format %{ &quot;vpsubw  $dst,$src,$mem\t! sub packedS&quot; %}
4567   ins_encode %{
4568     int vector_len = vector_length_encoding(this);
4569     __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4570   %}
4571   ins_pipe( pipe_slow );
4572 %}
4573 
4574 // Integers vector sub
4575 instruct vsubI(vec dst, vec src) %{
4576   predicate(UseAVX == 0);
4577   match(Set dst (SubVI dst src));
4578   format %{ &quot;psubd   $dst,$src\t! sub packedI&quot; %}
4579   ins_encode %{
4580     __ psubd($dst$$XMMRegister, $src$$XMMRegister);
4581   %}
4582   ins_pipe( pipe_slow );
4583 %}
4584 
4585 instruct vsubI_reg(vec dst, vec src1, vec src2) %{
4586   predicate(UseAVX &gt; 0);
4587   match(Set dst (SubVI src1 src2));
4588   format %{ &quot;vpsubd  $dst,$src1,$src2\t! sub packedI&quot; %}
4589   ins_encode %{
4590     int vector_len = vector_length_encoding(this);
4591     __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4592   %}
4593   ins_pipe( pipe_slow );
4594 %}
4595 
4596 instruct vsubI_mem(vec dst, vec src, memory mem) %{
4597   predicate(UseAVX &gt; 0);
4598   match(Set dst (SubVI src (LoadVector mem)));
4599   format %{ &quot;vpsubd  $dst,$src,$mem\t! sub packedI&quot; %}
4600   ins_encode %{
4601     int vector_len = vector_length_encoding(this);
4602     __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4603   %}
4604   ins_pipe( pipe_slow );
4605 %}
4606 
4607 // Longs vector sub
4608 instruct vsubL(vec dst, vec src) %{
4609   predicate(UseAVX == 0);
4610   match(Set dst (SubVL dst src));
4611   format %{ &quot;psubq   $dst,$src\t! sub packedL&quot; %}
4612   ins_encode %{
4613     __ psubq($dst$$XMMRegister, $src$$XMMRegister);
4614   %}
4615   ins_pipe( pipe_slow );
4616 %}
4617 
4618 instruct vsubL_reg(vec dst, vec src1, vec src2) %{
4619   predicate(UseAVX &gt; 0);
4620   match(Set dst (SubVL src1 src2));
4621   format %{ &quot;vpsubq  $dst,$src1,$src2\t! sub packedL&quot; %}
4622   ins_encode %{
4623     int vector_len = vector_length_encoding(this);
4624     __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4625   %}
4626   ins_pipe( pipe_slow );
4627 %}
4628 
4629 
4630 instruct vsubL_mem(vec dst, vec src, memory mem) %{
4631   predicate(UseAVX &gt; 0);
4632   match(Set dst (SubVL src (LoadVector mem)));
4633   format %{ &quot;vpsubq  $dst,$src,$mem\t! sub packedL&quot; %}
4634   ins_encode %{
4635     int vector_len = vector_length_encoding(this);
4636     __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4637   %}
4638   ins_pipe( pipe_slow );
4639 %}
4640 
4641 // Floats vector sub
4642 instruct vsubF(vec dst, vec src) %{
4643   predicate(UseAVX == 0);
4644   match(Set dst (SubVF dst src));
4645   format %{ &quot;subps   $dst,$src\t! sub packedF&quot; %}
4646   ins_encode %{
4647     __ subps($dst$$XMMRegister, $src$$XMMRegister);
4648   %}
4649   ins_pipe( pipe_slow );
4650 %}
4651 
4652 instruct vsubF_reg(vec dst, vec src1, vec src2) %{
4653   predicate(UseAVX &gt; 0);
4654   match(Set dst (SubVF src1 src2));
4655   format %{ &quot;vsubps  $dst,$src1,$src2\t! sub packedF&quot; %}
4656   ins_encode %{
4657     int vector_len = vector_length_encoding(this);
4658     __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4659   %}
4660   ins_pipe( pipe_slow );
4661 %}
4662 
4663 instruct vsubF_mem(vec dst, vec src, memory mem) %{
4664   predicate(UseAVX &gt; 0);
4665   match(Set dst (SubVF src (LoadVector mem)));
4666   format %{ &quot;vsubps  $dst,$src,$mem\t! sub packedF&quot; %}
4667   ins_encode %{
4668     int vector_len = vector_length_encoding(this);
4669     __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4670   %}
4671   ins_pipe( pipe_slow );
4672 %}
4673 
4674 // Doubles vector sub
4675 instruct vsubD(vec dst, vec src) %{
4676   predicate(UseAVX == 0);
4677   match(Set dst (SubVD dst src));
4678   format %{ &quot;subpd   $dst,$src\t! sub packedD&quot; %}
4679   ins_encode %{
4680     __ subpd($dst$$XMMRegister, $src$$XMMRegister);
4681   %}
4682   ins_pipe( pipe_slow );
4683 %}
4684 
4685 instruct vsubD_reg(vec dst, vec src1, vec src2) %{
4686   predicate(UseAVX &gt; 0);
4687   match(Set dst (SubVD src1 src2));
4688   format %{ &quot;vsubpd  $dst,$src1,$src2\t! sub packedD&quot; %}
4689   ins_encode %{
4690     int vector_len = vector_length_encoding(this);
4691     __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4692   %}
4693   ins_pipe( pipe_slow );
4694 %}
4695 
4696 instruct vsubD_mem(vec dst, vec src, memory mem) %{
4697   predicate(UseAVX &gt; 0);
4698   match(Set dst (SubVD src (LoadVector mem)));
4699   format %{ &quot;vsubpd  $dst,$src,$mem\t! sub packedD&quot; %}
4700   ins_encode %{
4701     int vector_len = vector_length_encoding(this);
4702     __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4703   %}
4704   ins_pipe( pipe_slow );
4705 %}
4706 
4707 // --------------------------------- MUL --------------------------------------
4708 
4709 // Byte vector mul
4710 instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4711   predicate(n-&gt;as_Vector()-&gt;length() == 4 ||
4712             n-&gt;as_Vector()-&gt;length() == 8);
4713   match(Set dst (MulVB src1 src2));
4714   effect(TEMP dst, TEMP tmp, TEMP scratch);
4715   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4716   ins_encode %{
4717     assert(UseSSE &gt; 3, &quot;required&quot;);
4718     __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);
4719     __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);
4720     __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);
4721     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4722     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4723     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4724   %}
4725   ins_pipe( pipe_slow );
4726 %}
4727 
4728 instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4729   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4730   match(Set dst (MulVB src1 src2));
4731   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4732   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4733   ins_encode %{
4734     assert(UseSSE &gt; 3, &quot;required&quot;);
4735     __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);
4736     __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);
4737     __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);
4738     __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);
4739     __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);
4740     __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);
4741     __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);
4742     __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);
4743     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4744     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
4745     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
4746     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
4747   %}
4748   ins_pipe( pipe_slow );
4749 %}
4750 
4751 instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4752   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
4753   match(Set dst (MulVB src1 src2));
4754   effect(TEMP dst, TEMP tmp, TEMP scratch);
4755   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4756   ins_encode %{
4757   int vector_len = Assembler::AVX_256bit;
4758     __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vector_len);
4759     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4760     __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vector_len);
4761     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4762     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
4763     __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);
4764     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);
4765   %}
4766   ins_pipe( pipe_slow );
4767 %}
4768 
4769 instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4770   predicate(n-&gt;as_Vector()-&gt;length() == 32);
4771   match(Set dst (MulVB src1 src2));
4772   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4773   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4774   ins_encode %{
4775     assert(UseAVX &gt; 1, &quot;required&quot;);
4776     int vector_len = Assembler::AVX_256bit;
4777     __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4778     __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);
4779     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4780     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4781     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4782     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4783     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4784     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4785     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4786     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4787     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4788     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4789     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4790     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
4791   %}
4792   ins_pipe( pipe_slow );
4793 %}
4794 
4795 instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4796   predicate(n-&gt;as_Vector()-&gt;length() == 64);
4797   match(Set dst (MulVB src1 src2));
4798   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4799   format %{&quot;vector_mulB $dst,$src1,$src2\n\t&quot; %}
4800   ins_encode %{
4801     assert(UseAVX &gt; 2, &quot;required&quot;);
4802     int vector_len = Assembler::AVX_512bit;
4803     __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4804     __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);
4805     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4806     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4807     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4808     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4809     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4810     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4811     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4812     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4813     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4814     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4815     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4816     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
4817     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4818   %}
4819   ins_pipe( pipe_slow );
4820 %}
4821 
4822 // Shorts/Chars vector mul
4823 instruct vmulS(vec dst, vec src) %{
4824   predicate(UseAVX == 0);
4825   match(Set dst (MulVS dst src));
4826   format %{ &quot;pmullw $dst,$src\t! mul packedS&quot; %}
4827   ins_encode %{
4828     __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
4829   %}
4830   ins_pipe( pipe_slow );
4831 %}
4832 
4833 instruct vmulS_reg(vec dst, vec src1, vec src2) %{
4834   predicate(UseAVX &gt; 0);
4835   match(Set dst (MulVS src1 src2));
4836   format %{ &quot;vpmullw $dst,$src1,$src2\t! mul packedS&quot; %}
4837   ins_encode %{
4838     int vector_len = vector_length_encoding(this);
4839     __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4840   %}
4841   ins_pipe( pipe_slow );
4842 %}
4843 
4844 instruct vmulS_mem(vec dst, vec src, memory mem) %{
4845   predicate(UseAVX &gt; 0);
4846   match(Set dst (MulVS src (LoadVector mem)));
4847   format %{ &quot;vpmullw $dst,$src,$mem\t! mul packedS&quot; %}
4848   ins_encode %{
4849     int vector_len = vector_length_encoding(this);
4850     __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4851   %}
4852   ins_pipe( pipe_slow );
4853 %}
4854 
4855 // Integers vector mul
4856 instruct vmulI(vec dst, vec src) %{
4857   predicate(UseAVX == 0);
4858   match(Set dst (MulVI dst src));
4859   format %{ &quot;pmulld  $dst,$src\t! mul packedI&quot; %}
4860   ins_encode %{
4861     assert(UseSSE &gt; 3, &quot;required&quot;);
4862     __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
4863   %}
4864   ins_pipe( pipe_slow );
4865 %}
4866 
4867 instruct vmulI_reg(vec dst, vec src1, vec src2) %{
4868   predicate(UseAVX &gt; 0);
4869   match(Set dst (MulVI src1 src2));
4870   format %{ &quot;vpmulld $dst,$src1,$src2\t! mul packedI&quot; %}
4871   ins_encode %{
4872     int vector_len = vector_length_encoding(this);
4873     __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4874   %}
4875   ins_pipe( pipe_slow );
4876 %}
4877 
4878 instruct vmulI_mem(vec dst, vec src, memory mem) %{
4879   predicate(UseAVX &gt; 0);
4880   match(Set dst (MulVI src (LoadVector mem)));
4881   format %{ &quot;vpmulld $dst,$src,$mem\t! mul packedI&quot; %}
4882   ins_encode %{
4883     int vector_len = vector_length_encoding(this);
4884     __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4885   %}
4886   ins_pipe( pipe_slow );
4887 %}
4888 
4889 // Longs vector mul
4890 instruct vmulL_reg(vec dst, vec src1, vec src2) %{
4891   match(Set dst (MulVL src1 src2));
4892   format %{ &quot;vpmullq $dst,$src1,$src2\t! mul packedL&quot; %}
4893   ins_encode %{
4894     assert(UseAVX &gt; 2, &quot;required&quot;);
4895     int vector_len = vector_length_encoding(this);
4896     __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4897   %}
4898   ins_pipe( pipe_slow );
4899 %}
4900 
4901 instruct vmulL_mem(vec dst, vec src, memory mem) %{
4902   match(Set dst (MulVL src (LoadVector mem)));
4903   format %{ &quot;vpmullq $dst,$src,$mem\t! mul packedL&quot; %}
4904   ins_encode %{
4905     assert(UseAVX &gt; 2, &quot;required&quot;);
4906     int vector_len = vector_length_encoding(this);
4907     __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4908   %}
4909   ins_pipe( pipe_slow );
4910 %}
4911 
4912 // Floats vector mul
4913 instruct vmulF(vec dst, vec src) %{
4914   predicate(UseAVX == 0);
4915   match(Set dst (MulVF dst src));
4916   format %{ &quot;mulps   $dst,$src\t! mul packedF&quot; %}
4917   ins_encode %{
4918     __ mulps($dst$$XMMRegister, $src$$XMMRegister);
4919   %}
4920   ins_pipe( pipe_slow );
4921 %}
4922 
4923 instruct vmulF_reg(vec dst, vec src1, vec src2) %{
4924   predicate(UseAVX &gt; 0);
4925   match(Set dst (MulVF src1 src2));
4926   format %{ &quot;vmulps  $dst,$src1,$src2\t! mul packedF&quot; %}
4927   ins_encode %{
4928     int vector_len = vector_length_encoding(this);
4929     __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4930   %}
4931   ins_pipe( pipe_slow );
4932 %}
4933 
4934 instruct vmulF_mem(vec dst, vec src, memory mem) %{
4935   predicate(UseAVX &gt; 0);
4936   match(Set dst (MulVF src (LoadVector mem)));
4937   format %{ &quot;vmulps  $dst,$src,$mem\t! mul packedF&quot; %}
4938   ins_encode %{
4939     int vector_len = vector_length_encoding(this);
4940     __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4941   %}
4942   ins_pipe( pipe_slow );
4943 %}
4944 
4945 // Doubles vector mul
4946 instruct vmulD(vec dst, vec src) %{
4947   predicate(UseAVX == 0);
4948   match(Set dst (MulVD dst src));
4949   format %{ &quot;mulpd   $dst,$src\t! mul packedD&quot; %}
4950   ins_encode %{
4951     __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
4952   %}
4953   ins_pipe( pipe_slow );
4954 %}
4955 
4956 instruct vmulD_reg(vec dst, vec src1, vec src2) %{
4957   predicate(UseAVX &gt; 0);
4958   match(Set dst (MulVD src1 src2));
4959   format %{ &quot;vmulpd  $dst,$src1,$src2\t! mul packedD&quot; %}
4960   ins_encode %{
4961     int vector_len = vector_length_encoding(this);
4962     __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4963   %}
4964   ins_pipe( pipe_slow );
4965 %}
4966 
4967 instruct vmulD_mem(vec dst, vec src, memory mem) %{
4968   predicate(UseAVX &gt; 0);
4969   match(Set dst (MulVD src (LoadVector mem)));
4970   format %{ &quot;vmulpd  $dst,$src,$mem\t! mul packedD&quot; %}
4971   ins_encode %{
4972     int vector_len = vector_length_encoding(this);
4973     __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4974   %}
4975   ins_pipe( pipe_slow );
4976 %}
4977 
4978 instruct vcmov8F_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
4979   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 8);
4980   match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
4981   effect(TEMP dst, USE src1, USE src2);
4982   format %{ &quot;cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t&quot;
4983             &quot;blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t&quot;
4984          %}
4985   ins_encode %{
4986     int vector_len = 1;
4987     int cond = (Assembler::Condition)($copnd$$cmpcode);
4988     __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
4989     __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
4990   %}
4991   ins_pipe( pipe_slow );
4992 %}
4993 
4994 instruct vcmov4D_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
4995   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 4);
4996   match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
4997   effect(TEMP dst, USE src1, USE src2);
4998   format %{ &quot;cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t&quot;
4999             &quot;blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t&quot;
5000          %}
5001   ins_encode %{
5002     int vector_len = 1;
5003     int cond = (Assembler::Condition)($copnd$$cmpcode);
5004     __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
5005     __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
5006   %}
5007   ins_pipe( pipe_slow );
5008 %}
5009 
5010 // --------------------------------- DIV --------------------------------------
5011 
5012 // Floats vector div
5013 instruct vdivF(vec dst, vec src) %{
5014   predicate(UseAVX == 0);
5015   match(Set dst (DivVF dst src));
5016   format %{ &quot;divps   $dst,$src\t! div packedF&quot; %}
5017   ins_encode %{
5018     __ divps($dst$$XMMRegister, $src$$XMMRegister);
5019   %}
5020   ins_pipe( pipe_slow );
5021 %}
5022 
5023 instruct vdivF_reg(vec dst, vec src1, vec src2) %{
5024   predicate(UseAVX &gt; 0);
5025   match(Set dst (DivVF src1 src2));
5026   format %{ &quot;vdivps  $dst,$src1,$src2\t! div packedF&quot; %}
5027   ins_encode %{
5028     int vector_len = vector_length_encoding(this);
5029     __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5030   %}
5031   ins_pipe( pipe_slow );
5032 %}
5033 
5034 instruct vdivF_mem(vec dst, vec src, memory mem) %{
5035   predicate(UseAVX &gt; 0);
5036   match(Set dst (DivVF src (LoadVector mem)));
5037   format %{ &quot;vdivps  $dst,$src,$mem\t! div packedF&quot; %}
5038   ins_encode %{
5039     int vector_len = vector_length_encoding(this);
5040     __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5041   %}
5042   ins_pipe( pipe_slow );
5043 %}
5044 
5045 // Doubles vector div
5046 instruct vdivD(vec dst, vec src) %{
5047   predicate(UseAVX == 0);
5048   match(Set dst (DivVD dst src));
5049   format %{ &quot;divpd   $dst,$src\t! div packedD&quot; %}
5050   ins_encode %{
5051     __ divpd($dst$$XMMRegister, $src$$XMMRegister);
5052   %}
5053   ins_pipe( pipe_slow );
5054 %}
5055 
5056 instruct vdivD_reg(vec dst, vec src1, vec src2) %{
5057   predicate(UseAVX &gt; 0);
5058   match(Set dst (DivVD src1 src2));
5059   format %{ &quot;vdivpd  $dst,$src1,$src2\t! div packedD&quot; %}
5060   ins_encode %{
5061     int vector_len = vector_length_encoding(this);
5062     __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5063   %}
5064   ins_pipe( pipe_slow );
5065 %}
5066 
5067 instruct vdivD_mem(vec dst, vec src, memory mem) %{
5068   predicate(UseAVX &gt; 0);
5069   match(Set dst (DivVD src (LoadVector mem)));
5070   format %{ &quot;vdivpd  $dst,$src,$mem\t! div packedD&quot; %}
5071   ins_encode %{
5072     int vector_len = vector_length_encoding(this);
5073     __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5074   %}
5075   ins_pipe( pipe_slow );
5076 %}
5077 
5078 // --------------------------------- Sqrt --------------------------------------
5079 
5080 instruct vsqrtF_reg(vec dst, vec src) %{
5081   match(Set dst (SqrtVF src));
5082   format %{ &quot;vsqrtps  $dst,$src\t! sqrt packedF&quot; %}
5083   ins_encode %{
5084     assert(UseAVX &gt; 0, &quot;required&quot;);
5085     int vector_len = vector_length_encoding(this);
5086     __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5087   %}
5088   ins_pipe( pipe_slow );
5089 %}
5090 
5091 instruct vsqrtF_mem(vec dst, memory mem) %{
5092   match(Set dst (SqrtVF (LoadVector mem)));
5093   format %{ &quot;vsqrtps  $dst,$mem\t! sqrt packedF&quot; %}
5094   ins_encode %{
5095     assert(UseAVX &gt; 0, &quot;required&quot;);
5096     int vector_len = vector_length_encoding(this);
5097     __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
5098   %}
5099   ins_pipe( pipe_slow );
5100 %}
5101 
5102 // Floating point vector sqrt
5103 instruct vsqrtD_reg(vec dst, vec src) %{
5104   match(Set dst (SqrtVD src));
5105   format %{ &quot;vsqrtpd  $dst,$src\t! sqrt packedD&quot; %}
5106   ins_encode %{
5107     assert(UseAVX &gt; 0, &quot;required&quot;);
5108     int vector_len = vector_length_encoding(this);
5109     __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5110   %}
5111   ins_pipe( pipe_slow );
5112 %}
5113 
5114 instruct vsqrtD_mem(vec dst, memory mem) %{
5115   match(Set dst (SqrtVD (LoadVector mem)));
5116   format %{ &quot;vsqrtpd  $dst,$mem\t! sqrt packedD&quot; %}
5117   ins_encode %{
5118     assert(UseAVX &gt; 0, &quot;required&quot;);
5119     int vector_len = vector_length_encoding(this);
5120     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
5121   %}
5122   ins_pipe( pipe_slow );
5123 %}
5124 
5125 // ------------------------------ Shift ---------------------------------------
5126 
5127 // Left and right shift count vectors are the same on x86
5128 // (only lowest bits of xmm reg are used for count).
5129 instruct vshiftcnt(vec dst, rRegI cnt) %{
5130   match(Set dst (LShiftCntV cnt));
5131   match(Set dst (RShiftCntV cnt));
5132   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
5133   ins_encode %{
5134     __ movdl($dst$$XMMRegister, $cnt$$Register);
5135   %}
5136   ins_pipe( pipe_slow );
5137 %}
5138 
5139 // Byte vector shift
5140 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5141   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
5142   match(Set dst (LShiftVB src shift));
5143   match(Set dst (RShiftVB src shift));
5144   match(Set dst (URShiftVB src shift));
5145   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
5146   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5147   ins_encode %{
5148     assert(UseSSE &gt; 3, &quot;required&quot;);
5149     int opcode = this-&gt;ideal_Opcode();
5150     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
5151     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
5152     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5153     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
5154     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
5155   %}
5156   ins_pipe( pipe_slow );
5157 %}
5158 
5159 instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
5160   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
5161   match(Set dst (LShiftVB src shift));
5162   match(Set dst (RShiftVB src shift));
5163   match(Set dst (URShiftVB src shift));
5164   effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);
5165   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5166   ins_encode %{
5167     assert(UseSSE &gt; 3, &quot;required&quot;);
5168     int opcode = this-&gt;ideal_Opcode();
5169 
5170     __ vextendbw(opcode, $tmp1$$XMMRegister, $src$$XMMRegister);
5171     __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
5172     __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
5173     __ vextendbw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
5174     __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
5175     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5176     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
5177     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
5178     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
5179   %}
5180   ins_pipe( pipe_slow );
5181 %}
5182 
5183 instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5184   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
5185   match(Set dst (LShiftVB src shift));
5186   match(Set dst (RShiftVB src shift));
5187   match(Set dst (URShiftVB src shift));
5188   effect(TEMP dst, TEMP tmp, TEMP scratch);
5189   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5190   ins_encode %{
5191     int opcode = this-&gt;ideal_Opcode();
5192     int vector_len = Assembler::AVX_256bit;
5193     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister, vector_len);
5194     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5195     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5196     __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
5197     __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
5198   %}
5199   ins_pipe( pipe_slow );
5200 %}
5201 
5202 instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5203   predicate(n-&gt;as_Vector()-&gt;length() == 32);
5204   match(Set dst (LShiftVB src shift));
5205   match(Set dst (RShiftVB src shift));
5206   match(Set dst (URShiftVB src shift));
5207   effect(TEMP dst, TEMP tmp, TEMP scratch);
5208   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5209   ins_encode %{
5210     assert(UseAVX &gt; 1, &quot;required&quot;);
5211     int opcode = this-&gt;ideal_Opcode();
5212     int vector_len = Assembler::AVX_256bit;
5213     __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
5214     __ vextendbw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
5215     __ vextendbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, vector_len);
5216     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5217     __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vector_len);
5218     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5219     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5220     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5221     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
5222   %}
5223   ins_pipe( pipe_slow );
5224 %}
5225 
5226 instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
5227   predicate(n-&gt;as_Vector()-&gt;length() == 64);
5228   match(Set dst (LShiftVB src shift));
5229   match(Set dst (RShiftVB src shift));
5230   match(Set dst (URShiftVB src shift));
5231   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
5232   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5233   ins_encode %{
5234     assert(UseAVX &gt; 2, &quot;required&quot;);
5235     int opcode = this-&gt;ideal_Opcode();
5236     int vector_len = Assembler::AVX_512bit;
5237     __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
5238     __ vextendbw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
5239     __ vextendbw(opcode, $tmp2$$XMMRegister, $src$$XMMRegister, vector_len);
5240     __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vector_len);
5241     __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vector_len);
5242     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5243     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5244     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5245     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5246     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
5247     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
5248     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5249   %}
5250   ins_pipe( pipe_slow );
5251 %}
5252 
5253 // Shorts vector logical right shift produces incorrect Java result
5254 // for negative data because java code convert short value into int with
5255 // sign extension before a shift. But char vectors are fine since chars are
5256 // unsigned values.
5257 // Shorts/Chars vector left shift
5258 instruct vshiftS(vec dst, vec src, vec shift) %{
5259   match(Set dst (LShiftVS src shift));
5260   match(Set dst (RShiftVS src shift));
5261   match(Set dst (URShiftVS src shift));
5262   effect(TEMP dst, USE src, USE shift);
5263   format %{ &quot;vshiftw  $dst,$src,$shift\t! shift packedS&quot; %}
5264   ins_encode %{
5265     int opcode = this-&gt;ideal_Opcode();
5266     if (UseAVX &gt; 0) {
5267       int vlen_enc = vector_length_encoding(this);
5268       __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
5269     } else {
5270       int vlen = vector_length(this);
5271       if (vlen == 2) {
5272         __ movflt($dst$$XMMRegister, $src$$XMMRegister);
5273         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5274       } else if (vlen == 4) {
5275         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
5276         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5277       } else {
5278         assert (vlen == 8, &quot;sanity&quot;);
5279         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5280         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5281       }
5282     }
5283   %}
5284   ins_pipe( pipe_slow );
5285 %}
5286 
5287 // Integers vector left shift
5288 instruct vshiftI(vec dst, vec src, vec shift) %{
5289   match(Set dst (LShiftVI src shift));
5290   match(Set dst (RShiftVI src shift));
5291   match(Set dst (URShiftVI src shift));
5292   effect(TEMP dst, USE src, USE shift);
5293   format %{ &quot;vshiftd  $dst,$src,$shift\t! shift packedI&quot; %}
5294   ins_encode %{
5295     int opcode = this-&gt;ideal_Opcode();
5296     if (UseAVX &gt; 0) {
5297       int vector_len = vector_length_encoding(this);
5298       __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5299     } else {
5300       int vlen = vector_length(this);
5301       if (vlen == 2) {
5302         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
5303         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5304       } else {
5305         assert(vlen == 4, &quot;sanity&quot;);
5306         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5307         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5308       }
5309     }
5310   %}
5311   ins_pipe( pipe_slow );
5312 %}
5313 
5314 // Longs vector shift
5315 instruct vshiftL(vec dst, vec src, vec shift) %{
5316   match(Set dst (LShiftVL src shift));
5317   match(Set dst (URShiftVL src shift));
5318   effect(TEMP dst, USE src, USE shift);
5319   format %{ &quot;vshiftq  $dst,$src,$shift\t! shift packedL&quot; %}
5320   ins_encode %{
5321     int opcode = this-&gt;ideal_Opcode();
5322     if (UseAVX &gt; 0) {
5323       int vector_len = vector_length_encoding(this);
5324       __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5325     } else {
5326       assert(vector_length(this) == 2, &quot;&quot;);
5327       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5328       __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5329     }
5330   %}
5331   ins_pipe( pipe_slow );
5332 %}
5333 
5334 // -------------------ArithmeticRightShift -----------------------------------
5335 // Long vector arithmetic right shift
5336 instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5337   predicate(UseAVX &lt;= 2);
5338   match(Set dst (RShiftVL src shift));
5339   effect(TEMP dst, TEMP tmp, TEMP scratch);
5340   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5341   ins_encode %{
5342     uint vlen = vector_length(this);
5343     if (vlen == 2) {
5344       assert(UseSSE &gt;= 2, &quot;required&quot;);
5345       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5346       __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
5347       __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5348       __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
5349       __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
5350       __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
5351     } else {
5352       assert(vlen == 4, &quot;sanity&quot;);
5353       assert(UseAVX &gt; 1, &quot;required&quot;);
5354       int vector_len = Assembler::AVX_256bit;
5355       __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5356       __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5357       __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5358       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5359       __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5360     }
5361   %}
5362   ins_pipe( pipe_slow );
5363 %}
5364 
5365 instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
5366   predicate(UseAVX &gt; 2);
5367   match(Set dst (RShiftVL src shift));
5368   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5369   ins_encode %{
5370     int vector_len = vector_length_encoding(this);
5371     __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5372   %}
5373   ins_pipe( pipe_slow );
5374 %}
5375 
5376 // --------------------------------- AND --------------------------------------
5377 
5378 instruct vand(vec dst, vec src) %{
5379   predicate(UseAVX == 0);
5380   match(Set dst (AndV dst src));
5381   format %{ &quot;pand    $dst,$src\t! and vectors&quot; %}
5382   ins_encode %{
5383     __ pand($dst$$XMMRegister, $src$$XMMRegister);
5384   %}
5385   ins_pipe( pipe_slow );
5386 %}
5387 
5388 instruct vand_reg(vec dst, vec src1, vec src2) %{
5389   predicate(UseAVX &gt; 0);
5390   match(Set dst (AndV src1 src2));
5391   format %{ &quot;vpand   $dst,$src1,$src2\t! and vectors&quot; %}
5392   ins_encode %{
5393     int vector_len = vector_length_encoding(this);
5394     __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5395   %}
5396   ins_pipe( pipe_slow );
5397 %}
5398 
5399 instruct vand_mem(vec dst, vec src, memory mem) %{
5400   predicate(UseAVX &gt; 0);
5401   match(Set dst (AndV src (LoadVector mem)));
5402   format %{ &quot;vpand   $dst,$src,$mem\t! and vectors&quot; %}
5403   ins_encode %{
5404     int vector_len = vector_length_encoding(this);
5405     __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5406   %}
5407   ins_pipe( pipe_slow );
5408 %}
5409 
5410 // --------------------------------- OR ---------------------------------------
5411 
5412 instruct vor(vec dst, vec src) %{
5413   predicate(UseAVX == 0);
5414   match(Set dst (OrV dst src));
5415   format %{ &quot;por     $dst,$src\t! or vectors&quot; %}
5416   ins_encode %{
5417     __ por($dst$$XMMRegister, $src$$XMMRegister);
5418   %}
5419   ins_pipe( pipe_slow );
5420 %}
5421 
5422 instruct vor_reg(vec dst, vec src1, vec src2) %{
5423   predicate(UseAVX &gt; 0);
5424   match(Set dst (OrV src1 src2));
5425   format %{ &quot;vpor    $dst,$src1,$src2\t! or vectors&quot; %}
5426   ins_encode %{
5427     int vector_len = vector_length_encoding(this);
5428     __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5429   %}
5430   ins_pipe( pipe_slow );
5431 %}
5432 
5433 instruct vor_mem(vec dst, vec src, memory mem) %{
5434   predicate(UseAVX &gt; 0);
5435   match(Set dst (OrV src (LoadVector mem)));
5436   format %{ &quot;vpor    $dst,$src,$mem\t! or vectors&quot; %}
5437   ins_encode %{
5438     int vector_len = vector_length_encoding(this);
5439     __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5440   %}
5441   ins_pipe( pipe_slow );
5442 %}
5443 
5444 // --------------------------------- XOR --------------------------------------
5445 
5446 instruct vxor(vec dst, vec src) %{
5447   predicate(UseAVX == 0);
5448   match(Set dst (XorV dst src));
5449   format %{ &quot;pxor    $dst,$src\t! xor vectors&quot; %}
5450   ins_encode %{
5451     __ pxor($dst$$XMMRegister, $src$$XMMRegister);
5452   %}
5453   ins_pipe( pipe_slow );
5454 %}
5455 
5456 instruct vxor_reg(vec dst, vec src1, vec src2) %{
5457   predicate(UseAVX &gt; 0);
5458   match(Set dst (XorV src1 src2));
5459   format %{ &quot;vpxor   $dst,$src1,$src2\t! xor vectors&quot; %}
5460   ins_encode %{
5461     int vector_len = vector_length_encoding(this);
5462     __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5463   %}
5464   ins_pipe( pipe_slow );
5465 %}
5466 
5467 instruct vxor_mem(vec dst, vec src, memory mem) %{
5468   predicate(UseAVX &gt; 0);
5469   match(Set dst (XorV src (LoadVector mem)));
5470   format %{ &quot;vpxor   $dst,$src,$mem\t! xor vectors&quot; %}
5471   ins_encode %{
5472     int vector_len = vector_length_encoding(this);
5473     __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5474   %}
5475   ins_pipe( pipe_slow );
5476 %}
5477 
5478 // --------------------------------- ABS --------------------------------------
5479 // a = |a|
5480 instruct vabsB_reg(vec dst, vec src) %{
5481   match(Set dst (AbsVB  src));
5482   format %{ &quot;vabsb $dst,$src\t# $dst = |$src| abs packedB&quot; %}
5483   ins_encode %{
5484     uint vlen = vector_length(this);
5485     if (vlen &lt;= 16) {
5486       __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
5487     } else {
5488       int vlen_enc = vector_length_encoding(this);
5489       __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5490     }
5491   %}
5492   ins_pipe( pipe_slow );
5493 %}
5494 
5495 instruct vabsS_reg(vec dst, vec src) %{
5496   match(Set dst (AbsVS  src));
5497   format %{ &quot;vabsw $dst,$src\t# $dst = |$src| abs packedS&quot; %}
5498   ins_encode %{
5499     uint vlen = vector_length(this);
5500     if (vlen &lt;= 8) {
5501       __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
5502     } else {
5503       int vlen_enc = vector_length_encoding(this);
5504       __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5505     }
5506   %}
5507   ins_pipe( pipe_slow );
5508 %}
5509 
5510 instruct vabsI_reg(vec dst, vec src) %{
5511   match(Set dst (AbsVI  src));
5512   format %{ &quot;pabsd $dst,$src\t# $dst = |$src| abs packedI&quot; %}
5513   ins_encode %{
5514     uint vlen = vector_length(this);
5515     if (vlen &lt;= 4) {
5516       __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
5517     } else {
5518       int vlen_enc = vector_length_encoding(this);
5519       __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5520     }
5521   %}
5522   ins_pipe( pipe_slow );
5523 %}
5524 
5525 instruct vabsL_reg(vec dst, vec src) %{
5526   match(Set dst (AbsVL  src));
5527   format %{ &quot;evpabsq $dst,$src\t# $dst = |$src| abs packedL&quot; %}
5528   ins_encode %{
5529     assert(UseAVX &gt; 2, &quot;required&quot;);
5530     int vector_len = vector_length_encoding(this);
5531     __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5532   %}
5533   ins_pipe( pipe_slow );
5534 %}
5535 
5536 // --------------------------------- ABSNEG --------------------------------------
5537 
5538 instruct vabsnegF(vec dst, vec src, rRegI scratch) %{
5539   predicate(n-&gt;as_Vector()-&gt;length() != 4); // handled by 1-operand instruction vabsneg4F
5540   match(Set dst (AbsVF src));
5541   match(Set dst (NegVF src));
5542   effect(TEMP scratch);
5543   format %{ &quot;vabsnegf $dst,$src,[mask]\t# absneg packedF&quot; %}
5544   ins_cost(150);
5545   ins_encode %{
5546     int opcode = this-&gt;ideal_Opcode();
5547     int vlen = vector_length(this);
5548     if (vlen == 2) {
5549       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5550     } else {
5551       assert(vlen == 8 || vlen == 16, &quot;required&quot;);
5552       int vlen_enc = vector_length_encoding(this);
5553       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5554     }
5555   %}
5556   ins_pipe( pipe_slow );
5557 %}
5558 
5559 instruct vabsneg4F(vec dst, rRegI scratch) %{
5560   predicate(n-&gt;as_Vector()-&gt;length() == 4);
5561   match(Set dst (AbsVF dst));
5562   match(Set dst (NegVF dst));
5563   effect(TEMP scratch);
5564   format %{ &quot;vabsnegf $dst,[mask]\t# absneg packed4F&quot; %}
5565   ins_cost(150);
5566   ins_encode %{
5567     int opcode = this-&gt;ideal_Opcode();
5568     __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);
5569   %}
5570   ins_pipe( pipe_slow );
5571 %}
5572 
5573 instruct vabsnegD(vec dst, vec src, rRegI scratch) %{
5574   match(Set dst (AbsVD  src));
5575   match(Set dst (NegVD  src));
5576   effect(TEMP scratch);
5577   format %{ &quot;vabsnegd $dst,$src,[mask]\t# absneg packedD&quot; %}
5578   ins_encode %{
5579     int opcode = this-&gt;ideal_Opcode();
5580     uint vlen = vector_length(this);
5581     if (vlen == 2) {
5582       assert(UseSSE &gt;= 2, &quot;required&quot;);
5583       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5584     } else {
5585       int vlen_enc = vector_length_encoding(this);
5586       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5587     }
5588   %}
5589   ins_pipe( pipe_slow );
5590 %}
5591 
5592 // --------------------------------- FMA --------------------------------------
5593 // a * b + c
5594 
5595 instruct vfmaF_reg(vec a, vec b, vec c) %{
5596   match(Set c (FmaVF  c (Binary a b)));
5597   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5598   ins_cost(150);
5599   ins_encode %{
5600     assert(UseFMA, &quot;not enabled&quot;);
5601     int vector_len = vector_length_encoding(this);
5602     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5603   %}
5604   ins_pipe( pipe_slow );
5605 %}
5606 
5607 instruct vfmaF_mem(vec a, memory b, vec c) %{
5608   match(Set c (FmaVF  c (Binary a (LoadVector b))));
5609   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5610   ins_cost(150);
5611   ins_encode %{
5612     assert(UseFMA, &quot;not enabled&quot;);
5613     int vector_len = vector_length_encoding(this);
5614     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5615   %}
5616   ins_pipe( pipe_slow );
5617 %}
5618 
5619 instruct vfmaD_reg(vec a, vec b, vec c) %{
5620   match(Set c (FmaVD  c (Binary a b)));
5621   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5622   ins_cost(150);
5623   ins_encode %{
5624     assert(UseFMA, &quot;not enabled&quot;);
5625     int vector_len = vector_length_encoding(this);
5626     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5627   %}
5628   ins_pipe( pipe_slow );
5629 %}
5630 
5631 instruct vfmaD_mem(vec a, memory b, vec c) %{
5632   match(Set c (FmaVD  c (Binary a (LoadVector b))));
5633   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5634   ins_cost(150);
5635   ins_encode %{
5636     assert(UseFMA, &quot;not enabled&quot;);
5637     int vector_len = vector_length_encoding(this);
5638     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5639   %}
5640   ins_pipe( pipe_slow );
5641 %}
5642 
5643 // --------------------------------- Vector Multiply Add --------------------------------------
5644 
5645 instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
5646   predicate(UseAVX == 0);
5647   match(Set dst (MulAddVS2VI dst src1));
5648   format %{ &quot;pmaddwd $dst,$dst,$src1\t! muladd packedStoI&quot; %}
5649   ins_encode %{
5650     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5651   %}
5652   ins_pipe( pipe_slow );
5653 %}
5654 
5655 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5656   predicate(UseAVX &gt; 0);
5657   match(Set dst (MulAddVS2VI src1 src2));
5658   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5659   ins_encode %{
5660     int vector_len = vector_length_encoding(this);
5661     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5662   %}
5663   ins_pipe( pipe_slow );
5664 %}
5665 
5666 // --------------------------------- Vector Multiply Add Add ----------------------------------
5667 
5668 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
5669   predicate(VM_Version::supports_avx512_vnni());
5670   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5671   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5672   ins_encode %{
5673     assert(UseAVX &gt; 2, &quot;required&quot;);
5674     int vector_len = vector_length_encoding(this);
5675     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5676   %}
5677   ins_pipe( pipe_slow );
5678   ins_cost(10);
5679 %}
5680 
5681 // --------------------------------- PopCount --------------------------------------
5682 
5683 instruct vpopcountI(vec dst, vec src) %{
5684   match(Set dst (PopCountVI src));
5685   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5686   ins_encode %{
5687     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5688 
5689     int vector_len = vector_length_encoding(this);
5690     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5691   %}
5692   ins_pipe( pipe_slow );
5693 %}
    </pre>
  </body>
</html>