<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="interp_masm_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/macroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1067 void MacroAssembler::reserved_stack_check() {
1068     // testing if reserved zone needs to be enabled
1069     Label no_reserved_zone_enabling;
1070     Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
1071     NOT_LP64(get_thread(rsi);)
1072 
1073     cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));
1074     jcc(Assembler::below, no_reserved_zone_enabling);
1075 
1076     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
1077     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
1078     should_not_reach_here();
1079 
1080     bind(no_reserved_zone_enabling);
1081 }
1082 
1083 int MacroAssembler::biased_locking_enter(Register lock_reg,
1084                                          Register obj_reg,
1085                                          Register swap_reg,
1086                                          Register tmp_reg,

1087                                          bool swap_reg_contains_mark,
1088                                          Label&amp; done,
1089                                          Label* slow_case,
1090                                          BiasedLockingCounters* counters) {
1091   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1092   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
1093   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
1094   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
1095   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
1096   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
1097   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
1098 
1099   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
1100     counters = BiasedLocking::counters();
1101   }
1102   // Biased locking
1103   // See whether the lock is currently biased toward our thread and
1104   // whether the epoch is still valid
1105   // Note that the runtime guarantees sufficient alignment of JavaThread
1106   // pointers to allow age to be placed into low bits
</pre>
<hr />
<pre>
1111     null_check_offset = offset();
1112     movptr(swap_reg, mark_addr);
1113   }
1114   movptr(tmp_reg, swap_reg);
1115   andptr(tmp_reg, markWord::biased_lock_mask_in_place);
1116   cmpptr(tmp_reg, markWord::biased_lock_pattern);
1117   jcc(Assembler::notEqual, cas_label);
1118   // The bias pattern is present in the object&#39;s header. Need to check
1119   // whether the bias owner and the epoch are both still current.
1120 #ifndef _LP64
1121   // Note that because there is no current thread register on x86_32 we
1122   // need to store off the mark word we read out of the object to
1123   // avoid reloading it and needing to recheck invariants below. This
1124   // store is unfortunate but it makes the overall code shorter and
1125   // simpler.
1126   movptr(saved_mark_addr, swap_reg);
1127 #endif
1128   if (swap_reg_contains_mark) {
1129     null_check_offset = offset();
1130   }
<span class="line-modified">1131   load_prototype_header(tmp_reg, obj_reg);</span>
1132 #ifdef _LP64
1133   orptr(tmp_reg, r15_thread);
1134   xorptr(tmp_reg, swap_reg);
1135   Register header_reg = tmp_reg;
1136 #else
1137   xorptr(tmp_reg, swap_reg);
1138   get_thread(swap_reg);
1139   xorptr(swap_reg, tmp_reg);
1140   Register header_reg = swap_reg;
1141 #endif
1142   andptr(header_reg, ~((int) markWord::age_mask_in_place));
1143   if (counters != NULL) {
1144     cond_inc32(Assembler::zero,
1145                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
1146   }
1147   jcc(Assembler::equal, done);
1148 
1149   Label try_revoke_bias;
1150   Label try_rebias;
1151 
</pre>
<hr />
<pre>
1197   // interpreter runtime in the slow case.
1198   if (counters != NULL) {
1199     cond_inc32(Assembler::zero,
1200                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
1201   }
1202   if (slow_case != NULL) {
1203     jcc(Assembler::notZero, *slow_case);
1204   }
1205   jmp(done);
1206 
1207   bind(try_rebias);
1208   // At this point we know the epoch has expired, meaning that the
1209   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
1210   // circumstances _only_, we are allowed to use the current header&#39;s
1211   // value as the comparison value when doing the cas to acquire the
1212   // bias in the current epoch. In other words, we allow transfer of
1213   // the bias from one thread to another directly in this situation.
1214   //
1215   // FIXME: due to a lack of registers we currently blow away the age
1216   // bits in this situation. Should attempt to preserve them.
<span class="line-modified">1217   load_prototype_header(tmp_reg, obj_reg);</span>
1218 #ifdef _LP64
1219   orptr(tmp_reg, r15_thread);
1220 #else
1221   get_thread(swap_reg);
1222   orptr(tmp_reg, swap_reg);
1223   movptr(swap_reg, saved_mark_addr);
1224 #endif
1225   lock();
1226   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1227   // If the biasing toward our thread failed, then another thread
1228   // succeeded in biasing it toward itself and we need to revoke that
1229   // bias. The revocation will occur in the runtime in the slow case.
1230   if (counters != NULL) {
1231     cond_inc32(Assembler::zero,
1232                ExternalAddress((address) counters-&gt;rebiased_lock_entry_count_addr()));
1233   }
1234   if (slow_case != NULL) {
1235     jcc(Assembler::notZero, *slow_case);
1236   }
1237   jmp(done);
1238 
1239   bind(try_revoke_bias);
1240   // The prototype mark in the klass doesn&#39;t have the bias bit set any
1241   // more, indicating that objects of this data type are not supposed
1242   // to be biased any more. We are going to try to reset the mark of
1243   // this object to the prototype value and fall through to the
1244   // CAS-based locking scheme. Note that if our CAS fails, it means
1245   // that another thread raced us for the privilege of revoking the
1246   // bias of this particular object, so it&#39;s okay to continue in the
1247   // normal locking code.
1248   //
1249   // FIXME: due to a lack of registers we currently blow away the age
1250   // bits in this situation. Should attempt to preserve them.
1251   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
<span class="line-modified">1252   load_prototype_header(tmp_reg, obj_reg);</span>
1253   lock();
1254   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1255   // Fall through to the normal CAS-based lock, because no matter what
1256   // the result of the above CAS, some thread must have succeeded in
1257   // removing the bias bit from the object&#39;s header.
1258   if (counters != NULL) {
1259     cond_inc32(Assembler::zero,
1260                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
1261   }
1262 
1263   bind(cas_label);
1264 
1265   return null_check_offset;
1266 }
1267 
1268 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
1269   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1270 
1271   // Check for biased locking unlock case, which is a no-op
1272   // Note: we do not have to check the thread ID for two reasons.
</pre>
<hr />
<pre>
1494                                   bool     check_exceptions) {
1495   // determine java_thread register
1496   if (!java_thread-&gt;is_valid()) {
1497 #ifdef _LP64
1498     java_thread = r15_thread;
1499 #else
1500     java_thread = rdi;
1501     get_thread(java_thread);
1502 #endif // LP64
1503   }
1504   // determine last_java_sp register
1505   if (!last_java_sp-&gt;is_valid()) {
1506     last_java_sp = rsp;
1507   }
1508   // debugging support
1509   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
1510   LP64_ONLY(assert(java_thread == r15_thread, &quot;unexpected register&quot;));
1511 #ifdef ASSERT
1512   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
1513   // r12 is the heapbase.
<span class="line-modified">1514   LP64_ONLY(if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);)</span>
1515 #endif // ASSERT
1516 
1517   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
1518   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
1519 
1520   // push java thread (becomes first argument of C function)
1521 
1522   NOT_LP64(push(java_thread); number_of_arguments++);
1523   LP64_ONLY(mov(c_rarg0, r15_thread));
1524 
1525   // set last Java frame before call
1526   assert(last_java_sp != rbp, &quot;can&#39;t use ebp/rbp&quot;);
1527 
1528   // Only interpreter should have to set fp
1529   set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);
1530 
1531   // do the call, remove parameters
1532   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);
1533 
1534   // restore the thread (cannot use the pushed argument since arguments
</pre>
<hr />
<pre>
4306 
4307 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
4308   // get mirror
4309   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
4310   load_method_holder(mirror, method);
4311   movptr(mirror, Address(mirror, mirror_offset));
4312   resolve_oop_handle(mirror, tmp);
4313 }
4314 
4315 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
4316   load_method_holder(rresult, rmethod);
4317   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
4318 }
4319 
4320 void MacroAssembler::load_method_holder(Register holder, Register method) {
4321   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
4322   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
4323   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
4324 }
4325 
<span class="line-modified">4326 void MacroAssembler::load_klass(Register dst, Register src) {</span>


4327 #ifdef _LP64
4328   if (UseCompressedClassPointers) {
4329     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-modified">4330     decode_klass_not_null(dst);</span>
4331   } else
4332 #endif
4333     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4334 }
4335 
<span class="line-modified">4336 void MacroAssembler::load_prototype_header(Register dst, Register src) {</span>
<span class="line-modified">4337   load_klass(dst, src);</span>
4338   movptr(dst, Address(dst, Klass::prototype_header_offset()));
4339 }
4340 
<span class="line-modified">4341 void MacroAssembler::store_klass(Register dst, Register src) {</span>


4342 #ifdef _LP64
4343   if (UseCompressedClassPointers) {
<span class="line-modified">4344     encode_klass_not_null(src);</span>
4345     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4346   } else
4347 #endif
4348     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4349 }
4350 
4351 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
4352                                     Register tmp1, Register thread_tmp) {
4353   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4354   decorators = AccessInternal::decorator_fixup(decorators);
4355   bool as_raw = (decorators &amp; AS_RAW) != 0;
4356   if (as_raw) {
4357     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4358   } else {
4359     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4360   }
4361 }
4362 
4363 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
4364                                      Register tmp1, Register tmp2) {
</pre>
<hr />
<pre>
4538     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
4539     if (LogMinObjAlignmentInBytes == Address::times_8) {
4540       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
4541     } else {
4542       if (dst != src) {
4543         movq(dst, src);
4544       }
4545       shlq(dst, LogMinObjAlignmentInBytes);
4546       if (CompressedOops::base() != NULL) {
4547         addq(dst, r12_heapbase);
4548       }
4549     }
4550   } else {
4551     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
4552     if (dst != src) {
4553       movq(dst, src);
4554     }
4555   }
4556 }
4557 
<span class="line-modified">4558 void MacroAssembler::encode_klass_not_null(Register r) {</span>

4559   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4560     // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.</span>
<span class="line-modified">4561     assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);</span>
<span class="line-removed">4562     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-removed">4563     subq(r, r12_heapbase);</span>
4564   }
4565   if (CompressedKlassPointers::shift() != 0) {
4566     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4567     shrq(r, LogKlassAlignmentInBytes);
4568   }
<span class="line-removed">4569   if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-removed">4570     reinit_heapbase();</span>
<span class="line-removed">4571   }</span>
<span class="line-removed">4572 }</span>
<span class="line-removed">4573 </span>
<span class="line-removed">4574 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {</span>
<span class="line-removed">4575   if (dst == src) {</span>
<span class="line-removed">4576     encode_klass_not_null(src);</span>
<span class="line-removed">4577   } else {</span>
<span class="line-removed">4578     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-removed">4579       mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-removed">4580       negq(dst);</span>
<span class="line-removed">4581       addq(dst, src);</span>
<span class="line-removed">4582     } else {</span>
<span class="line-removed">4583       movptr(dst, src);</span>
<span class="line-removed">4584     }</span>
<span class="line-removed">4585     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-removed">4586       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-removed">4587       shrq(dst, LogKlassAlignmentInBytes);</span>
<span class="line-removed">4588     }</span>
<span class="line-removed">4589   }</span>
4590 }
4591 
<span class="line-modified">4592 // Function instr_size_for_decode_klass_not_null() counts the instructions</span>
<span class="line-modified">4593 // generated by decode_klass_not_null(register r) and reinit_heapbase(),</span>
<span class="line-removed">4594 // when (Universe::heap() != NULL).  Hence, if the instructions they</span>
<span class="line-removed">4595 // generate change, then this method needs to be updated.</span>
<span class="line-removed">4596 int MacroAssembler::instr_size_for_decode_klass_not_null() {</span>
<span class="line-removed">4597   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);</span>
4598   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4599     // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).</span>
<span class="line-modified">4600     return (CompressedKlassPointers::shift() == 0 ? 20 : 24);</span>
4601   } else {
<span class="line-modified">4602     // longest load decode klass function, mov64, leaq</span>
<span class="line-modified">4603     return 16;</span>



4604   }
4605 }
4606 
4607 // !!! If the instructions that get generated here change then function
4608 // instr_size_for_decode_klass_not_null() needs to get updated.
<span class="line-modified">4609 void  MacroAssembler::decode_klass_not_null(Register r) {</span>

4610   // Note: it will change flags
<span class="line-modified">4611   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);</span>
<span class="line-removed">4612   assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);</span>
4613   // Cannot assert, unverified entry point counts instructions (see .ad file)
4614   // vtableStubs also counts instructions in pd_code_size_limit.
4615   // Also do not verify_oop as this is called by verify_oop.
4616   if (CompressedKlassPointers::shift() != 0) {
4617     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4618     shlq(r, LogKlassAlignmentInBytes);
4619   }
<span class="line-removed">4620   // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.</span>
4621   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4622     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4623     addq(r, r12_heapbase);</span>
<span class="line-removed">4624     reinit_heapbase();</span>
4625   }
4626 }
4627 
<span class="line-modified">4628 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {</span>

4629   // Note: it will change flags
4630   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
<span class="line-modified">4631   if (dst == src) {</span>
<span class="line-modified">4632     decode_klass_not_null(dst);</span>







4633   } else {
<span class="line-modified">4634     // Cannot assert, unverified entry point counts instructions (see .ad file)</span>
<span class="line-modified">4635     // vtableStubs also counts instructions in pd_code_size_limit.</span>
<span class="line-modified">4636     // Also do not verify_oop as this is called by verify_oop.</span>
<span class="line-modified">4637     mov64(dst, (int64_t)CompressedKlassPointers::base());</span>

4638     if (CompressedKlassPointers::shift() != 0) {
4639       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4640       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
4641       leaq(dst, Address(dst, src, Address::times_8, 0));
4642     } else {
4643       addq(dst, src);
4644     }
4645   }
4646 }
4647 
4648 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4649   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
4650   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4651   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4652   int oop_index = oop_recorder()-&gt;find_index(obj);
4653   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4654   mov_narrow_oop(dst, oop_index, rspec);
4655 }
4656 
4657 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
</pre>
<hr />
<pre>
4697   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
4698 }
4699 
4700 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
4701   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4702   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4703   int klass_index = oop_recorder()-&gt;find_index(k);
4704   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
4705   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
4706 }
4707 
4708 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
4709   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4710   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4711   int klass_index = oop_recorder()-&gt;find_index(k);
4712   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
4713   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
4714 }
4715 
4716 void MacroAssembler::reinit_heapbase() {
<span class="line-modified">4717   if (UseCompressedOops || UseCompressedClassPointers) {</span>
4718     if (Universe::heap() != NULL) {
4719       if (CompressedOops::base() == NULL) {
4720         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
4721       } else {
4722         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
4723       }
4724     } else {
4725       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
4726     }
4727   }
4728 }
4729 
4730 #endif // _LP64
4731 
4732 // C2 compiled method&#39;s prolog code.
4733 void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {
4734 
4735   // WARNING: Initial instruction MUST be 5 bytes or longer so that
4736   // NativeJump::patch_verified_entry will be able to patch out the entry
4737   // code safely. The push to verify stack depth is ok at 5 bytes,
</pre>
</td>
<td>
<hr />
<pre>
1067 void MacroAssembler::reserved_stack_check() {
1068     // testing if reserved zone needs to be enabled
1069     Label no_reserved_zone_enabling;
1070     Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
1071     NOT_LP64(get_thread(rsi);)
1072 
1073     cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));
1074     jcc(Assembler::below, no_reserved_zone_enabling);
1075 
1076     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
1077     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
1078     should_not_reach_here();
1079 
1080     bind(no_reserved_zone_enabling);
1081 }
1082 
1083 int MacroAssembler::biased_locking_enter(Register lock_reg,
1084                                          Register obj_reg,
1085                                          Register swap_reg,
1086                                          Register tmp_reg,
<span class="line-added">1087                                          Register tmp_reg2,</span>
1088                                          bool swap_reg_contains_mark,
1089                                          Label&amp; done,
1090                                          Label* slow_case,
1091                                          BiasedLockingCounters* counters) {
1092   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1093   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
1094   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
1095   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
1096   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
1097   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
1098   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
1099 
1100   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
1101     counters = BiasedLocking::counters();
1102   }
1103   // Biased locking
1104   // See whether the lock is currently biased toward our thread and
1105   // whether the epoch is still valid
1106   // Note that the runtime guarantees sufficient alignment of JavaThread
1107   // pointers to allow age to be placed into low bits
</pre>
<hr />
<pre>
1112     null_check_offset = offset();
1113     movptr(swap_reg, mark_addr);
1114   }
1115   movptr(tmp_reg, swap_reg);
1116   andptr(tmp_reg, markWord::biased_lock_mask_in_place);
1117   cmpptr(tmp_reg, markWord::biased_lock_pattern);
1118   jcc(Assembler::notEqual, cas_label);
1119   // The bias pattern is present in the object&#39;s header. Need to check
1120   // whether the bias owner and the epoch are both still current.
1121 #ifndef _LP64
1122   // Note that because there is no current thread register on x86_32 we
1123   // need to store off the mark word we read out of the object to
1124   // avoid reloading it and needing to recheck invariants below. This
1125   // store is unfortunate but it makes the overall code shorter and
1126   // simpler.
1127   movptr(saved_mark_addr, swap_reg);
1128 #endif
1129   if (swap_reg_contains_mark) {
1130     null_check_offset = offset();
1131   }
<span class="line-modified">1132   load_prototype_header(tmp_reg, obj_reg, tmp_reg2);</span>
1133 #ifdef _LP64
1134   orptr(tmp_reg, r15_thread);
1135   xorptr(tmp_reg, swap_reg);
1136   Register header_reg = tmp_reg;
1137 #else
1138   xorptr(tmp_reg, swap_reg);
1139   get_thread(swap_reg);
1140   xorptr(swap_reg, tmp_reg);
1141   Register header_reg = swap_reg;
1142 #endif
1143   andptr(header_reg, ~((int) markWord::age_mask_in_place));
1144   if (counters != NULL) {
1145     cond_inc32(Assembler::zero,
1146                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
1147   }
1148   jcc(Assembler::equal, done);
1149 
1150   Label try_revoke_bias;
1151   Label try_rebias;
1152 
</pre>
<hr />
<pre>
1198   // interpreter runtime in the slow case.
1199   if (counters != NULL) {
1200     cond_inc32(Assembler::zero,
1201                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
1202   }
1203   if (slow_case != NULL) {
1204     jcc(Assembler::notZero, *slow_case);
1205   }
1206   jmp(done);
1207 
1208   bind(try_rebias);
1209   // At this point we know the epoch has expired, meaning that the
1210   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
1211   // circumstances _only_, we are allowed to use the current header&#39;s
1212   // value as the comparison value when doing the cas to acquire the
1213   // bias in the current epoch. In other words, we allow transfer of
1214   // the bias from one thread to another directly in this situation.
1215   //
1216   // FIXME: due to a lack of registers we currently blow away the age
1217   // bits in this situation. Should attempt to preserve them.
<span class="line-modified">1218   load_prototype_header(tmp_reg, obj_reg, tmp_reg2);</span>
1219 #ifdef _LP64
1220   orptr(tmp_reg, r15_thread);
1221 #else
1222   get_thread(swap_reg);
1223   orptr(tmp_reg, swap_reg);
1224   movptr(swap_reg, saved_mark_addr);
1225 #endif
1226   lock();
1227   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1228   // If the biasing toward our thread failed, then another thread
1229   // succeeded in biasing it toward itself and we need to revoke that
1230   // bias. The revocation will occur in the runtime in the slow case.
1231   if (counters != NULL) {
1232     cond_inc32(Assembler::zero,
1233                ExternalAddress((address) counters-&gt;rebiased_lock_entry_count_addr()));
1234   }
1235   if (slow_case != NULL) {
1236     jcc(Assembler::notZero, *slow_case);
1237   }
1238   jmp(done);
1239 
1240   bind(try_revoke_bias);
1241   // The prototype mark in the klass doesn&#39;t have the bias bit set any
1242   // more, indicating that objects of this data type are not supposed
1243   // to be biased any more. We are going to try to reset the mark of
1244   // this object to the prototype value and fall through to the
1245   // CAS-based locking scheme. Note that if our CAS fails, it means
1246   // that another thread raced us for the privilege of revoking the
1247   // bias of this particular object, so it&#39;s okay to continue in the
1248   // normal locking code.
1249   //
1250   // FIXME: due to a lack of registers we currently blow away the age
1251   // bits in this situation. Should attempt to preserve them.
1252   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
<span class="line-modified">1253   load_prototype_header(tmp_reg, obj_reg, tmp_reg2);</span>
1254   lock();
1255   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1256   // Fall through to the normal CAS-based lock, because no matter what
1257   // the result of the above CAS, some thread must have succeeded in
1258   // removing the bias bit from the object&#39;s header.
1259   if (counters != NULL) {
1260     cond_inc32(Assembler::zero,
1261                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
1262   }
1263 
1264   bind(cas_label);
1265 
1266   return null_check_offset;
1267 }
1268 
1269 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
1270   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1271 
1272   // Check for biased locking unlock case, which is a no-op
1273   // Note: we do not have to check the thread ID for two reasons.
</pre>
<hr />
<pre>
1495                                   bool     check_exceptions) {
1496   // determine java_thread register
1497   if (!java_thread-&gt;is_valid()) {
1498 #ifdef _LP64
1499     java_thread = r15_thread;
1500 #else
1501     java_thread = rdi;
1502     get_thread(java_thread);
1503 #endif // LP64
1504   }
1505   // determine last_java_sp register
1506   if (!last_java_sp-&gt;is_valid()) {
1507     last_java_sp = rsp;
1508   }
1509   // debugging support
1510   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
1511   LP64_ONLY(assert(java_thread == r15_thread, &quot;unexpected register&quot;));
1512 #ifdef ASSERT
1513   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
1514   // r12 is the heapbase.
<span class="line-modified">1515   LP64_ONLY(if (UseCompressedOops &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);)</span>
1516 #endif // ASSERT
1517 
1518   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
1519   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
1520 
1521   // push java thread (becomes first argument of C function)
1522 
1523   NOT_LP64(push(java_thread); number_of_arguments++);
1524   LP64_ONLY(mov(c_rarg0, r15_thread));
1525 
1526   // set last Java frame before call
1527   assert(last_java_sp != rbp, &quot;can&#39;t use ebp/rbp&quot;);
1528 
1529   // Only interpreter should have to set fp
1530   set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);
1531 
1532   // do the call, remove parameters
1533   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);
1534 
1535   // restore the thread (cannot use the pushed argument since arguments
</pre>
<hr />
<pre>
4307 
4308 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
4309   // get mirror
4310   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
4311   load_method_holder(mirror, method);
4312   movptr(mirror, Address(mirror, mirror_offset));
4313   resolve_oop_handle(mirror, tmp);
4314 }
4315 
4316 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
4317   load_method_holder(rresult, rmethod);
4318   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
4319 }
4320 
4321 void MacroAssembler::load_method_holder(Register holder, Register method) {
4322   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
4323   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
4324   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
4325 }
4326 
<span class="line-modified">4327 void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {</span>
<span class="line-added">4328   assert_different_registers(src, tmp);</span>
<span class="line-added">4329   assert_different_registers(dst, tmp);</span>
4330 #ifdef _LP64
4331   if (UseCompressedClassPointers) {
4332     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-modified">4333     decode_klass_not_null(dst, tmp);</span>
4334   } else
4335 #endif
4336     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4337 }
4338 
<span class="line-modified">4339 void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {</span>
<span class="line-modified">4340   load_klass(dst, src, tmp);</span>
4341   movptr(dst, Address(dst, Klass::prototype_header_offset()));
4342 }
4343 
<span class="line-modified">4344 void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {</span>
<span class="line-added">4345   assert_different_registers(src, tmp);</span>
<span class="line-added">4346   assert_different_registers(dst, tmp);</span>
4347 #ifdef _LP64
4348   if (UseCompressedClassPointers) {
<span class="line-modified">4349     encode_klass_not_null(src, tmp);</span>
4350     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4351   } else
4352 #endif
4353     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4354 }
4355 
4356 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
4357                                     Register tmp1, Register thread_tmp) {
4358   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4359   decorators = AccessInternal::decorator_fixup(decorators);
4360   bool as_raw = (decorators &amp; AS_RAW) != 0;
4361   if (as_raw) {
4362     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4363   } else {
4364     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4365   }
4366 }
4367 
4368 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
4369                                      Register tmp1, Register tmp2) {
</pre>
<hr />
<pre>
4543     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
4544     if (LogMinObjAlignmentInBytes == Address::times_8) {
4545       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
4546     } else {
4547       if (dst != src) {
4548         movq(dst, src);
4549       }
4550       shlq(dst, LogMinObjAlignmentInBytes);
4551       if (CompressedOops::base() != NULL) {
4552         addq(dst, r12_heapbase);
4553       }
4554     }
4555   } else {
4556     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
4557     if (dst != src) {
4558       movq(dst, src);
4559     }
4560   }
4561 }
4562 
<span class="line-modified">4563 void MacroAssembler::encode_klass_not_null(Register r, Register tmp) {</span>
<span class="line-added">4564   assert_different_registers(r, tmp);</span>
4565   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4566     mov64(tmp, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4567     subq(r, tmp);</span>


4568   }
4569   if (CompressedKlassPointers::shift() != 0) {
4570     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4571     shrq(r, LogKlassAlignmentInBytes);
4572   }





















4573 }
4574 
<span class="line-modified">4575 void MacroAssembler::encode_and_move_klass_not_null(Register dst, Register src) {</span>
<span class="line-modified">4576   assert_different_registers(src, dst);</span>




4577   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4578     mov64(dst, -(int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4579     addq(dst, src);</span>
4580   } else {
<span class="line-modified">4581     movptr(dst, src);</span>
<span class="line-modified">4582   }</span>
<span class="line-added">4583   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-added">4584     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-added">4585     shrq(dst, LogKlassAlignmentInBytes);</span>
4586   }
4587 }
4588 
4589 // !!! If the instructions that get generated here change then function
4590 // instr_size_for_decode_klass_not_null() needs to get updated.
<span class="line-modified">4591 void  MacroAssembler::decode_klass_not_null(Register r, Register tmp) {</span>
<span class="line-added">4592   assert_different_registers(r, tmp);</span>
4593   // Note: it will change flags
<span class="line-modified">4594   assert(UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);</span>

4595   // Cannot assert, unverified entry point counts instructions (see .ad file)
4596   // vtableStubs also counts instructions in pd_code_size_limit.
4597   // Also do not verify_oop as this is called by verify_oop.
4598   if (CompressedKlassPointers::shift() != 0) {
4599     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4600     shlq(r, LogKlassAlignmentInBytes);
4601   }

4602   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4603     mov64(tmp, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4604     addq(r, tmp);</span>

4605   }
4606 }
4607 
<span class="line-modified">4608 void  MacroAssembler::decode_and_move_klass_not_null(Register dst, Register src) {</span>
<span class="line-added">4609   assert_different_registers(src, dst);</span>
4610   // Note: it will change flags
4611   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
<span class="line-modified">4612   // Cannot assert, unverified entry point counts instructions (see .ad file)</span>
<span class="line-modified">4613   // vtableStubs also counts instructions in pd_code_size_limit.</span>
<span class="line-added">4614   // Also do not verify_oop as this is called by verify_oop.</span>
<span class="line-added">4615 </span>
<span class="line-added">4616   if (CompressedKlassPointers::base() == NULL &amp;&amp;</span>
<span class="line-added">4617       CompressedKlassPointers::shift() == 0) {</span>
<span class="line-added">4618     // The best case scenario is that there is no base or shift. Then it is already</span>
<span class="line-added">4619     // a pointer that needs nothing but a register rename.</span>
<span class="line-added">4620     movl(dst, src);</span>
4621   } else {
<span class="line-modified">4622     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified">4623       mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4624     } else {</span>
<span class="line-modified">4625       xorq(dst, dst);</span>
<span class="line-added">4626     }</span>
4627     if (CompressedKlassPointers::shift() != 0) {
4628       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4629       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
4630       leaq(dst, Address(dst, src, Address::times_8, 0));
4631     } else {
4632       addq(dst, src);
4633     }
4634   }
4635 }
4636 
4637 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4638   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
4639   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4640   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4641   int oop_index = oop_recorder()-&gt;find_index(obj);
4642   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4643   mov_narrow_oop(dst, oop_index, rspec);
4644 }
4645 
4646 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
</pre>
<hr />
<pre>
4686   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
4687 }
4688 
4689 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
4690   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4691   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4692   int klass_index = oop_recorder()-&gt;find_index(k);
4693   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
4694   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
4695 }
4696 
4697 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
4698   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4699   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4700   int klass_index = oop_recorder()-&gt;find_index(k);
4701   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
4702   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
4703 }
4704 
4705 void MacroAssembler::reinit_heapbase() {
<span class="line-modified">4706   if (UseCompressedOops) {</span>
4707     if (Universe::heap() != NULL) {
4708       if (CompressedOops::base() == NULL) {
4709         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
4710       } else {
4711         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
4712       }
4713     } else {
4714       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
4715     }
4716   }
4717 }
4718 
4719 #endif // _LP64
4720 
4721 // C2 compiled method&#39;s prolog code.
4722 void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {
4723 
4724   // WARNING: Initial instruction MUST be 5 bytes or longer so that
4725   // NativeJump::patch_verified_entry will be able to patch out the entry
4726   // code safely. The push to verify stack depth is ok at 5 bytes,
</pre>
</td>
</tr>
</table>
<center><a href="interp_masm_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>