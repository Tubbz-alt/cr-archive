<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 938   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
 939 
 940   // -------------------------------------------------------------------------
 941   // Generate a C2I adapter.  On entry we know rbx holds the Method* during calls
 942   // to the interpreter.  The args start out packed in the compiled layout.  They
 943   // need to be unpacked into the interpreter layout.  This will almost always
 944   // require some stack space.  We grow the current (compiled) stack, then repack
 945   // the args.  We  finally end in a jump to the generic interpreter entry point.
 946   // On exit from the interpreter, the interpreter will restore our SP (lest the
 947   // compiled code, which relys solely on SP and not RBP, get sick).
 948 
 949   address c2i_unverified_entry = __ pc();
 950   Label skip_fixup;
 951   Label ok;
 952 
 953   Register holder = rax;
 954   Register receiver = j_rarg0;
 955   Register temp = rbx;
 956 
 957   {
<span class="line-modified"> 958     __ load_klass(temp, receiver);</span>
 959     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
 960     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
 961     __ jcc(Assembler::equal, ok);
 962     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 963 
 964     __ bind(ok);
 965     // Method might have been compiled since the call site was patched to
 966     // interpreted if that is the case treat it as a miss so we can get
 967     // the call site corrected.
 968     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 969     __ jcc(Assembler::equal, skip_fixup);
 970     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 971   }
 972 
 973   address c2i_entry = __ pc();
 974 
 975   // Class initialization barrier for static methods
 976   address c2i_no_clinit_check_entry = NULL;
 977   if (VM_Version::supports_fast_class_init_checks()) {
 978     Label L_skip_barrier;
</pre>
<hr />
<pre>
2122   // stack properly aligned.
2123   stack_slots = align_up(stack_slots, StackAlignmentInSlots);
2124 
2125   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
2126 
2127   // First thing make an ic check to see if we should even be here
2128 
2129   // We are free to use all registers as temps without saving them and
2130   // restoring them except rbp. rbp is the only callee save register
2131   // as far as the interpreter and the compiler(s) are concerned.
2132 
2133 
2134   const Register ic_reg = rax;
2135   const Register receiver = j_rarg0;
2136 
2137   Label hit;
2138   Label exception_pending;
2139 
2140   assert_different_registers(ic_reg, receiver, rscratch1);
2141   __ verify_oop(receiver);
<span class="line-modified">2142   __ load_klass(rscratch1, receiver);</span>
2143   __ cmpq(ic_reg, rscratch1);
2144   __ jcc(Assembler::equal, hit);
2145 
2146   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
2147 
2148   // Verified entry point must be aligned
2149   __ align(8);
2150 
2151   __ bind(hit);
2152 
2153   int vep_offset = ((intptr_t)__ pc()) - start;
2154 
2155   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {
2156     Label L_skip_barrier;
2157     Register klass = r10;
2158     __ mov_metadata(klass, method-&gt;method_holder()); // InstanceKlass*
2159     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
2160 
2161     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
2162 
</pre>
<hr />
<pre>
2466   Label lock_done;
2467 
2468   if (method-&gt;is_synchronized()) {
2469     assert(!is_critical_native, &quot;unhandled&quot;);
2470 
2471 
2472     const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();
2473 
2474     // Get the handle (the 2nd argument)
2475     __ mov(oop_handle_reg, c_rarg1);
2476 
2477     // Get address of the box
2478 
2479     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2480 
2481     // Load the oop from the handle
2482     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2483 
2484     __ resolve(IS_NOT_NULL, obj_reg);
2485     if (UseBiasedLocking) {
<span class="line-modified">2486       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, false, lock_done, &amp;slow_path_lock);</span>
2487     }
2488 
2489     // Load immediate 1 into swap_reg %rax
2490     __ movl(swap_reg, 1);
2491 
2492     // Load (object-&gt;mark() | 1) into swap_reg %rax
2493     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2494 
2495     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2496     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2497 
2498     // src -&gt; dest iff dest == rax else rax &lt;- dest
2499     __ lock();
2500     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2501     __ jcc(Assembler::equal, lock_done);
2502 
2503     // Hmm should this move to the slow path code area???
2504 
2505     // Test if the oopMark is an obvious stack pointer, i.e.,
2506     //  1) (mark &amp; 3) == 0, and
</pre>
</td>
<td>
<hr />
<pre>
 938   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
 939 
 940   // -------------------------------------------------------------------------
 941   // Generate a C2I adapter.  On entry we know rbx holds the Method* during calls
 942   // to the interpreter.  The args start out packed in the compiled layout.  They
 943   // need to be unpacked into the interpreter layout.  This will almost always
 944   // require some stack space.  We grow the current (compiled) stack, then repack
 945   // the args.  We  finally end in a jump to the generic interpreter entry point.
 946   // On exit from the interpreter, the interpreter will restore our SP (lest the
 947   // compiled code, which relys solely on SP and not RBP, get sick).
 948 
 949   address c2i_unverified_entry = __ pc();
 950   Label skip_fixup;
 951   Label ok;
 952 
 953   Register holder = rax;
 954   Register receiver = j_rarg0;
 955   Register temp = rbx;
 956 
 957   {
<span class="line-modified"> 958     __ load_klass(temp, receiver, rscratch1);</span>
 959     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
 960     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
 961     __ jcc(Assembler::equal, ok);
 962     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 963 
 964     __ bind(ok);
 965     // Method might have been compiled since the call site was patched to
 966     // interpreted if that is the case treat it as a miss so we can get
 967     // the call site corrected.
 968     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 969     __ jcc(Assembler::equal, skip_fixup);
 970     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 971   }
 972 
 973   address c2i_entry = __ pc();
 974 
 975   // Class initialization barrier for static methods
 976   address c2i_no_clinit_check_entry = NULL;
 977   if (VM_Version::supports_fast_class_init_checks()) {
 978     Label L_skip_barrier;
</pre>
<hr />
<pre>
2122   // stack properly aligned.
2123   stack_slots = align_up(stack_slots, StackAlignmentInSlots);
2124 
2125   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
2126 
2127   // First thing make an ic check to see if we should even be here
2128 
2129   // We are free to use all registers as temps without saving them and
2130   // restoring them except rbp. rbp is the only callee save register
2131   // as far as the interpreter and the compiler(s) are concerned.
2132 
2133 
2134   const Register ic_reg = rax;
2135   const Register receiver = j_rarg0;
2136 
2137   Label hit;
2138   Label exception_pending;
2139 
2140   assert_different_registers(ic_reg, receiver, rscratch1);
2141   __ verify_oop(receiver);
<span class="line-modified">2142   __ load_klass(rscratch1, receiver, rscratch2);</span>
2143   __ cmpq(ic_reg, rscratch1);
2144   __ jcc(Assembler::equal, hit);
2145 
2146   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
2147 
2148   // Verified entry point must be aligned
2149   __ align(8);
2150 
2151   __ bind(hit);
2152 
2153   int vep_offset = ((intptr_t)__ pc()) - start;
2154 
2155   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {
2156     Label L_skip_barrier;
2157     Register klass = r10;
2158     __ mov_metadata(klass, method-&gt;method_holder()); // InstanceKlass*
2159     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
2160 
2161     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
2162 
</pre>
<hr />
<pre>
2466   Label lock_done;
2467 
2468   if (method-&gt;is_synchronized()) {
2469     assert(!is_critical_native, &quot;unhandled&quot;);
2470 
2471 
2472     const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();
2473 
2474     // Get the handle (the 2nd argument)
2475     __ mov(oop_handle_reg, c_rarg1);
2476 
2477     // Get address of the box
2478 
2479     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2480 
2481     // Load the oop from the handle
2482     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2483 
2484     __ resolve(IS_NOT_NULL, obj_reg);
2485     if (UseBiasedLocking) {
<span class="line-modified">2486       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &amp;slow_path_lock);</span>
2487     }
2488 
2489     // Load immediate 1 into swap_reg %rax
2490     __ movl(swap_reg, 1);
2491 
2492     // Load (object-&gt;mark() | 1) into swap_reg %rax
2493     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2494 
2495     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2496     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2497 
2498     // src -&gt; dest iff dest == rax else rax &lt;- dest
2499     __ lock();
2500     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2501     __ jcc(Assembler::equal, lock_done);
2502 
2503     // Hmm should this move to the slow path code area???
2504 
2505     // Test if the oopMark is an obvious stack pointer, i.e.,
2506     //  1) (mark &amp; 3) == 0, and
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>