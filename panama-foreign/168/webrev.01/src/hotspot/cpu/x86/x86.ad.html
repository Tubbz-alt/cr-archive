<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 //
   2 // Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
   3 // DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4 //
   5 // This code is free software; you can redistribute it and/or modify it
   6 // under the terms of the GNU General Public License version 2 only, as
   7 // published by the Free Software Foundation.
   8 //
   9 // This code is distributed in the hope that it will be useful, but WITHOUT
  10 // ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11 // FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12 // version 2 for more details (a copy is included in the LICENSE file that
  13 // accompanied this code).
  14 //
  15 // You should have received a copy of the GNU General Public License version
  16 // 2 along with this work; if not, write to the Free Software Foundation,
  17 // Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18 //
  19 // Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20 // or visit www.oracle.com if you need additional information or have any
  21 // questions.
  22 //
  23 //
  24 
  25 // X86 Common Architecture Description File
  26 
  27 //----------REGISTER DEFINITION BLOCK------------------------------------------
  28 // This information is used by the matcher and the register allocator to
  29 // describe individual registers and classes of registers within the target
  30 // archtecture.
  31 
  32 register %{
  33 //----------Architecture Description Register Definitions----------------------
  34 // General Registers
  35 // &quot;reg_def&quot;  name ( register save type, C convention save type,
  36 //                   ideal register type, encoding );
  37 // Register Save Types:
  38 //
  39 // NS  = No-Save:       The register allocator assumes that these registers
  40 //                      can be used without saving upon entry to the method, &amp;
  41 //                      that they do not need to be saved at call sites.
  42 //
  43 // SOC = Save-On-Call:  The register allocator assumes that these registers
  44 //                      can be used without saving upon entry to the method,
  45 //                      but that they must be saved at call sites.
  46 //
  47 // SOE = Save-On-Entry: The register allocator assumes that these registers
  48 //                      must be saved before using them upon entry to the
  49 //                      method, but they do not need to be saved at call
  50 //                      sites.
  51 //
  52 // AS  = Always-Save:   The register allocator assumes that these registers
  53 //                      must be saved before using them upon entry to the
  54 //                      method, &amp; that they must be saved at call sites.
  55 //
  56 // Ideal Register Type is used to determine how to save &amp; restore a
  57 // register.  Op_RegI will get spilled with LoadI/StoreI, Op_RegP will get
  58 // spilled with LoadP/StoreP.  If the register supports both, use Op_RegI.
  59 //
  60 // The encoding number is the actual bit-pattern placed into the opcodes.
  61 
  62 // XMM registers.  512-bit registers or 8 words each, labeled (a)-p.
  63 // Word a in each register holds a Float, words ab hold a Double.
  64 // The whole registers are used in SSE4.2 version intrinsics,
  65 // array copy stubs and superword operations (see UseSSE42Intrinsics,
  66 // UseXMMForArrayCopy and UseSuperword flags).
  67 // For pre EVEX enabled architectures:
  68 //      XMM8-XMM15 must be encoded with REX (VEX for UseAVX)
  69 // For EVEX enabled architectures:
  70 //      XMM8-XMM31 must be encoded with REX (EVEX for UseAVX).
  71 //
  72 // Linux ABI:   No register preserved across function calls
  73 //              XMM0-XMM7 might hold parameters
  74 // Windows ABI: XMM6-XMM31 preserved across function calls
  75 //              XMM0-XMM3 might hold parameters
  76 
  77 reg_def XMM0 ( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg());
  78 reg_def XMM0b( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(1));
  79 reg_def XMM0c( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(2));
  80 reg_def XMM0d( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(3));
  81 reg_def XMM0e( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(4));
  82 reg_def XMM0f( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(5));
  83 reg_def XMM0g( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(6));
  84 reg_def XMM0h( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(7));
  85 reg_def XMM0i( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(8));
  86 reg_def XMM0j( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(9));
  87 reg_def XMM0k( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(10));
  88 reg_def XMM0l( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(11));
  89 reg_def XMM0m( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(12));
  90 reg_def XMM0n( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(13));
  91 reg_def XMM0o( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(14));
  92 reg_def XMM0p( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(15));
  93 
  94 reg_def XMM1 ( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg());
  95 reg_def XMM1b( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(1));
  96 reg_def XMM1c( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(2));
  97 reg_def XMM1d( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(3));
  98 reg_def XMM1e( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(4));
  99 reg_def XMM1f( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(5));
 100 reg_def XMM1g( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(6));
 101 reg_def XMM1h( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(7));
 102 reg_def XMM1i( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(8));
 103 reg_def XMM1j( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(9));
 104 reg_def XMM1k( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(10));
 105 reg_def XMM1l( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(11));
 106 reg_def XMM1m( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(12));
 107 reg_def XMM1n( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(13));
 108 reg_def XMM1o( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(14));
 109 reg_def XMM1p( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(15));
 110 
 111 reg_def XMM2 ( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg());
 112 reg_def XMM2b( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(1));
 113 reg_def XMM2c( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(2));
 114 reg_def XMM2d( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(3));
 115 reg_def XMM2e( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(4));
 116 reg_def XMM2f( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(5));
 117 reg_def XMM2g( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(6));
 118 reg_def XMM2h( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(7));
 119 reg_def XMM2i( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(8));
 120 reg_def XMM2j( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(9));
 121 reg_def XMM2k( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(10));
 122 reg_def XMM2l( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(11));
 123 reg_def XMM2m( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(12));
 124 reg_def XMM2n( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(13));
 125 reg_def XMM2o( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(14));
 126 reg_def XMM2p( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(15));
 127 
 128 reg_def XMM3 ( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg());
 129 reg_def XMM3b( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(1));
 130 reg_def XMM3c( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(2));
 131 reg_def XMM3d( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(3));
 132 reg_def XMM3e( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(4));
 133 reg_def XMM3f( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(5));
 134 reg_def XMM3g( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(6));
 135 reg_def XMM3h( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(7));
 136 reg_def XMM3i( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(8));
 137 reg_def XMM3j( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(9));
 138 reg_def XMM3k( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(10));
 139 reg_def XMM3l( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(11));
 140 reg_def XMM3m( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(12));
 141 reg_def XMM3n( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(13));
 142 reg_def XMM3o( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(14));
 143 reg_def XMM3p( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(15));
 144 
 145 reg_def XMM4 ( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg());
 146 reg_def XMM4b( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(1));
 147 reg_def XMM4c( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(2));
 148 reg_def XMM4d( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(3));
 149 reg_def XMM4e( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(4));
 150 reg_def XMM4f( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(5));
 151 reg_def XMM4g( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(6));
 152 reg_def XMM4h( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(7));
 153 reg_def XMM4i( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(8));
 154 reg_def XMM4j( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(9));
 155 reg_def XMM4k( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(10));
 156 reg_def XMM4l( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(11));
 157 reg_def XMM4m( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(12));
 158 reg_def XMM4n( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(13));
 159 reg_def XMM4o( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(14));
 160 reg_def XMM4p( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(15));
 161 
 162 reg_def XMM5 ( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg());
 163 reg_def XMM5b( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(1));
 164 reg_def XMM5c( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(2));
 165 reg_def XMM5d( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(3));
 166 reg_def XMM5e( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(4));
 167 reg_def XMM5f( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(5));
 168 reg_def XMM5g( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(6));
 169 reg_def XMM5h( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(7));
 170 reg_def XMM5i( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(8));
 171 reg_def XMM5j( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(9));
 172 reg_def XMM5k( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(10));
 173 reg_def XMM5l( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(11));
 174 reg_def XMM5m( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(12));
 175 reg_def XMM5n( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(13));
 176 reg_def XMM5o( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(14));
 177 reg_def XMM5p( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(15));
 178 
 179 reg_def XMM6 ( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg());
 180 reg_def XMM6b( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(1));
 181 reg_def XMM6c( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(2));
 182 reg_def XMM6d( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(3));
 183 reg_def XMM6e( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(4));
 184 reg_def XMM6f( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(5));
 185 reg_def XMM6g( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(6));
 186 reg_def XMM6h( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(7));
 187 reg_def XMM6i( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(8));
 188 reg_def XMM6j( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(9));
 189 reg_def XMM6k( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(10));
 190 reg_def XMM6l( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(11));
 191 reg_def XMM6m( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(12));
 192 reg_def XMM6n( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(13));
 193 reg_def XMM6o( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(14));
 194 reg_def XMM6p( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(15));
 195 
 196 reg_def XMM7 ( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg());
 197 reg_def XMM7b( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(1));
 198 reg_def XMM7c( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(2));
 199 reg_def XMM7d( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(3));
 200 reg_def XMM7e( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(4));
 201 reg_def XMM7f( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(5));
 202 reg_def XMM7g( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(6));
 203 reg_def XMM7h( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(7));
 204 reg_def XMM7i( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(8));
 205 reg_def XMM7j( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(9));
 206 reg_def XMM7k( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(10));
 207 reg_def XMM7l( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(11));
 208 reg_def XMM7m( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(12));
 209 reg_def XMM7n( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(13));
 210 reg_def XMM7o( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(14));
 211 reg_def XMM7p( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(15));
 212 
 213 #ifdef _LP64
 214 
 215 reg_def XMM8 ( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg());
 216 reg_def XMM8b( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(1));
 217 reg_def XMM8c( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(2));
 218 reg_def XMM8d( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(3));
 219 reg_def XMM8e( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(4));
 220 reg_def XMM8f( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(5));
 221 reg_def XMM8g( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(6));
 222 reg_def XMM8h( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(7));
 223 reg_def XMM8i( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(8));
 224 reg_def XMM8j( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(9));
 225 reg_def XMM8k( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(10));
 226 reg_def XMM8l( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(11));
 227 reg_def XMM8m( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(12));
 228 reg_def XMM8n( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(13));
 229 reg_def XMM8o( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(14));
 230 reg_def XMM8p( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(15));
 231 
 232 reg_def XMM9 ( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg());
 233 reg_def XMM9b( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(1));
 234 reg_def XMM9c( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(2));
 235 reg_def XMM9d( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(3));
 236 reg_def XMM9e( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(4));
 237 reg_def XMM9f( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(5));
 238 reg_def XMM9g( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(6));
 239 reg_def XMM9h( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(7));
 240 reg_def XMM9i( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(8));
 241 reg_def XMM9j( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(9));
 242 reg_def XMM9k( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(10));
 243 reg_def XMM9l( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(11));
 244 reg_def XMM9m( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(12));
 245 reg_def XMM9n( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(13));
 246 reg_def XMM9o( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(14));
 247 reg_def XMM9p( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(15));
 248 
 249 reg_def XMM10 ( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg());
 250 reg_def XMM10b( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(1));
 251 reg_def XMM10c( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(2));
 252 reg_def XMM10d( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(3));
 253 reg_def XMM10e( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(4));
 254 reg_def XMM10f( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(5));
 255 reg_def XMM10g( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(6));
 256 reg_def XMM10h( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(7));
 257 reg_def XMM10i( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(8));
 258 reg_def XMM10j( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(9));
 259 reg_def XMM10k( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(10));
 260 reg_def XMM10l( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(11));
 261 reg_def XMM10m( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(12));
 262 reg_def XMM10n( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(13));
 263 reg_def XMM10o( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(14));
 264 reg_def XMM10p( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(15));
 265 
 266 reg_def XMM11 ( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg());
 267 reg_def XMM11b( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(1));
 268 reg_def XMM11c( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(2));
 269 reg_def XMM11d( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(3));
 270 reg_def XMM11e( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(4));
 271 reg_def XMM11f( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(5));
 272 reg_def XMM11g( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(6));
 273 reg_def XMM11h( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(7));
 274 reg_def XMM11i( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(8));
 275 reg_def XMM11j( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(9));
 276 reg_def XMM11k( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(10));
 277 reg_def XMM11l( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(11));
 278 reg_def XMM11m( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(12));
 279 reg_def XMM11n( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(13));
 280 reg_def XMM11o( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(14));
 281 reg_def XMM11p( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(15));
 282 
 283 reg_def XMM12 ( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg());
 284 reg_def XMM12b( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(1));
 285 reg_def XMM12c( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(2));
 286 reg_def XMM12d( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(3));
 287 reg_def XMM12e( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(4));
 288 reg_def XMM12f( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(5));
 289 reg_def XMM12g( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(6));
 290 reg_def XMM12h( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(7));
 291 reg_def XMM12i( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(8));
 292 reg_def XMM12j( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(9));
 293 reg_def XMM12k( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(10));
 294 reg_def XMM12l( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(11));
 295 reg_def XMM12m( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(12));
 296 reg_def XMM12n( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(13));
 297 reg_def XMM12o( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(14));
 298 reg_def XMM12p( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(15));
 299 
 300 reg_def XMM13 ( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg());
 301 reg_def XMM13b( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(1));
 302 reg_def XMM13c( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(2));
 303 reg_def XMM13d( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(3));
 304 reg_def XMM13e( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(4));
 305 reg_def XMM13f( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(5));
 306 reg_def XMM13g( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(6));
 307 reg_def XMM13h( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(7));
 308 reg_def XMM13i( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(8));
 309 reg_def XMM13j( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(9));
 310 reg_def XMM13k( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(10));
 311 reg_def XMM13l( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(11));
 312 reg_def XMM13m( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(12));
 313 reg_def XMM13n( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(13));
 314 reg_def XMM13o( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(14));
 315 reg_def XMM13p( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(15));
 316 
 317 reg_def XMM14 ( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg());
 318 reg_def XMM14b( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(1));
 319 reg_def XMM14c( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(2));
 320 reg_def XMM14d( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(3));
 321 reg_def XMM14e( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(4));
 322 reg_def XMM14f( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(5));
 323 reg_def XMM14g( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(6));
 324 reg_def XMM14h( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(7));
 325 reg_def XMM14i( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(8));
 326 reg_def XMM14j( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(9));
 327 reg_def XMM14k( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(10));
 328 reg_def XMM14l( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(11));
 329 reg_def XMM14m( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(12));
 330 reg_def XMM14n( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(13));
 331 reg_def XMM14o( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(14));
 332 reg_def XMM14p( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(15));
 333 
 334 reg_def XMM15 ( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg());
 335 reg_def XMM15b( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(1));
 336 reg_def XMM15c( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(2));
 337 reg_def XMM15d( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(3));
 338 reg_def XMM15e( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(4));
 339 reg_def XMM15f( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(5));
 340 reg_def XMM15g( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(6));
 341 reg_def XMM15h( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(7));
 342 reg_def XMM15i( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(8));
 343 reg_def XMM15j( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(9));
 344 reg_def XMM15k( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(10));
 345 reg_def XMM15l( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(11));
 346 reg_def XMM15m( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(12));
 347 reg_def XMM15n( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(13));
 348 reg_def XMM15o( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(14));
 349 reg_def XMM15p( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(15));
 350 
 351 reg_def XMM16 ( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg());
 352 reg_def XMM16b( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(1));
 353 reg_def XMM16c( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(2));
 354 reg_def XMM16d( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(3));
 355 reg_def XMM16e( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(4));
 356 reg_def XMM16f( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(5));
 357 reg_def XMM16g( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(6));
 358 reg_def XMM16h( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(7));
 359 reg_def XMM16i( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(8));
 360 reg_def XMM16j( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(9));
 361 reg_def XMM16k( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(10));
 362 reg_def XMM16l( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(11));
 363 reg_def XMM16m( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(12));
 364 reg_def XMM16n( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(13));
 365 reg_def XMM16o( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(14));
 366 reg_def XMM16p( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(15));
 367 
 368 reg_def XMM17 ( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg());
 369 reg_def XMM17b( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(1));
 370 reg_def XMM17c( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(2));
 371 reg_def XMM17d( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(3));
 372 reg_def XMM17e( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(4));
 373 reg_def XMM17f( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(5));
 374 reg_def XMM17g( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(6));
 375 reg_def XMM17h( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(7));
 376 reg_def XMM17i( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(8));
 377 reg_def XMM17j( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(9));
 378 reg_def XMM17k( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(10));
 379 reg_def XMM17l( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(11));
 380 reg_def XMM17m( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(12));
 381 reg_def XMM17n( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(13));
 382 reg_def XMM17o( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(14));
 383 reg_def XMM17p( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(15));
 384 
 385 reg_def XMM18 ( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg());
 386 reg_def XMM18b( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(1));
 387 reg_def XMM18c( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(2));
 388 reg_def XMM18d( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(3));
 389 reg_def XMM18e( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(4));
 390 reg_def XMM18f( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(5));
 391 reg_def XMM18g( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(6));
 392 reg_def XMM18h( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(7));
 393 reg_def XMM18i( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(8));
 394 reg_def XMM18j( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(9));
 395 reg_def XMM18k( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(10));
 396 reg_def XMM18l( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(11));
 397 reg_def XMM18m( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(12));
 398 reg_def XMM18n( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(13));
 399 reg_def XMM18o( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(14));
 400 reg_def XMM18p( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(15));
 401 
 402 reg_def XMM19 ( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg());
 403 reg_def XMM19b( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(1));
 404 reg_def XMM19c( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(2));
 405 reg_def XMM19d( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(3));
 406 reg_def XMM19e( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(4));
 407 reg_def XMM19f( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(5));
 408 reg_def XMM19g( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(6));
 409 reg_def XMM19h( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(7));
 410 reg_def XMM19i( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(8));
 411 reg_def XMM19j( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(9));
 412 reg_def XMM19k( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(10));
 413 reg_def XMM19l( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(11));
 414 reg_def XMM19m( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(12));
 415 reg_def XMM19n( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(13));
 416 reg_def XMM19o( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(14));
 417 reg_def XMM19p( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(15));
 418 
 419 reg_def XMM20 ( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg());
 420 reg_def XMM20b( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(1));
 421 reg_def XMM20c( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(2));
 422 reg_def XMM20d( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(3));
 423 reg_def XMM20e( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(4));
 424 reg_def XMM20f( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(5));
 425 reg_def XMM20g( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(6));
 426 reg_def XMM20h( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(7));
 427 reg_def XMM20i( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(8));
 428 reg_def XMM20j( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(9));
 429 reg_def XMM20k( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(10));
 430 reg_def XMM20l( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(11));
 431 reg_def XMM20m( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(12));
 432 reg_def XMM20n( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(13));
 433 reg_def XMM20o( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(14));
 434 reg_def XMM20p( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(15));
 435 
 436 reg_def XMM21 ( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg());
 437 reg_def XMM21b( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(1));
 438 reg_def XMM21c( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(2));
 439 reg_def XMM21d( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(3));
 440 reg_def XMM21e( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(4));
 441 reg_def XMM21f( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(5));
 442 reg_def XMM21g( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(6));
 443 reg_def XMM21h( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(7));
 444 reg_def XMM21i( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(8));
 445 reg_def XMM21j( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(9));
 446 reg_def XMM21k( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(10));
 447 reg_def XMM21l( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(11));
 448 reg_def XMM21m( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(12));
 449 reg_def XMM21n( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(13));
 450 reg_def XMM21o( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(14));
 451 reg_def XMM21p( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(15));
 452 
 453 reg_def XMM22 ( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg());
 454 reg_def XMM22b( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(1));
 455 reg_def XMM22c( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(2));
 456 reg_def XMM22d( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(3));
 457 reg_def XMM22e( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(4));
 458 reg_def XMM22f( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(5));
 459 reg_def XMM22g( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(6));
 460 reg_def XMM22h( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(7));
 461 reg_def XMM22i( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(8));
 462 reg_def XMM22j( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(9));
 463 reg_def XMM22k( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(10));
 464 reg_def XMM22l( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(11));
 465 reg_def XMM22m( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(12));
 466 reg_def XMM22n( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(13));
 467 reg_def XMM22o( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(14));
 468 reg_def XMM22p( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(15));
 469 
 470 reg_def XMM23 ( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg());
 471 reg_def XMM23b( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(1));
 472 reg_def XMM23c( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(2));
 473 reg_def XMM23d( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(3));
 474 reg_def XMM23e( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(4));
 475 reg_def XMM23f( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(5));
 476 reg_def XMM23g( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(6));
 477 reg_def XMM23h( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(7));
 478 reg_def XMM23i( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(8));
 479 reg_def XMM23j( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(9));
 480 reg_def XMM23k( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(10));
 481 reg_def XMM23l( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(11));
 482 reg_def XMM23m( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(12));
 483 reg_def XMM23n( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(13));
 484 reg_def XMM23o( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(14));
 485 reg_def XMM23p( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(15));
 486 
 487 reg_def XMM24 ( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg());
 488 reg_def XMM24b( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(1));
 489 reg_def XMM24c( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(2));
 490 reg_def XMM24d( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(3));
 491 reg_def XMM24e( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(4));
 492 reg_def XMM24f( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(5));
 493 reg_def XMM24g( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(6));
 494 reg_def XMM24h( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(7));
 495 reg_def XMM24i( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(8));
 496 reg_def XMM24j( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(9));
 497 reg_def XMM24k( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(10));
 498 reg_def XMM24l( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(11));
 499 reg_def XMM24m( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(12));
 500 reg_def XMM24n( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(13));
 501 reg_def XMM24o( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(14));
 502 reg_def XMM24p( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(15));
 503 
 504 reg_def XMM25 ( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg());
 505 reg_def XMM25b( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(1));
 506 reg_def XMM25c( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(2));
 507 reg_def XMM25d( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(3));
 508 reg_def XMM25e( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(4));
 509 reg_def XMM25f( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(5));
 510 reg_def XMM25g( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(6));
 511 reg_def XMM25h( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(7));
 512 reg_def XMM25i( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(8));
 513 reg_def XMM25j( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(9));
 514 reg_def XMM25k( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(10));
 515 reg_def XMM25l( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(11));
 516 reg_def XMM25m( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(12));
 517 reg_def XMM25n( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(13));
 518 reg_def XMM25o( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(14));
 519 reg_def XMM25p( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(15));
 520 
 521 reg_def XMM26 ( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg());
 522 reg_def XMM26b( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(1));
 523 reg_def XMM26c( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(2));
 524 reg_def XMM26d( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(3));
 525 reg_def XMM26e( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(4));
 526 reg_def XMM26f( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(5));
 527 reg_def XMM26g( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(6));
 528 reg_def XMM26h( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(7));
 529 reg_def XMM26i( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(8));
 530 reg_def XMM26j( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(9));
 531 reg_def XMM26k( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(10));
 532 reg_def XMM26l( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(11));
 533 reg_def XMM26m( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(12));
 534 reg_def XMM26n( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(13));
 535 reg_def XMM26o( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(14));
 536 reg_def XMM26p( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(15));
 537 
 538 reg_def XMM27 ( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg());
 539 reg_def XMM27b( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(1));
 540 reg_def XMM27c( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(2));
 541 reg_def XMM27d( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(3));
 542 reg_def XMM27e( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(4));
 543 reg_def XMM27f( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(5));
 544 reg_def XMM27g( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(6));
 545 reg_def XMM27h( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(7));
 546 reg_def XMM27i( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(8));
 547 reg_def XMM27j( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(9));
 548 reg_def XMM27k( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(10));
 549 reg_def XMM27l( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(11));
 550 reg_def XMM27m( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(12));
 551 reg_def XMM27n( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(13));
 552 reg_def XMM27o( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(14));
 553 reg_def XMM27p( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(15));
 554 
 555 reg_def XMM28 ( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg());
 556 reg_def XMM28b( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(1));
 557 reg_def XMM28c( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(2));
 558 reg_def XMM28d( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(3));
 559 reg_def XMM28e( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(4));
 560 reg_def XMM28f( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(5));
 561 reg_def XMM28g( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(6));
 562 reg_def XMM28h( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(7));
 563 reg_def XMM28i( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(8));
 564 reg_def XMM28j( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(9));
 565 reg_def XMM28k( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(10));
 566 reg_def XMM28l( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(11));
 567 reg_def XMM28m( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(12));
 568 reg_def XMM28n( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(13));
 569 reg_def XMM28o( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(14));
 570 reg_def XMM28p( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(15));
 571 
 572 reg_def XMM29 ( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg());
 573 reg_def XMM29b( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(1));
 574 reg_def XMM29c( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(2));
 575 reg_def XMM29d( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(3));
 576 reg_def XMM29e( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(4));
 577 reg_def XMM29f( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(5));
 578 reg_def XMM29g( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(6));
 579 reg_def XMM29h( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(7));
 580 reg_def XMM29i( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(8));
 581 reg_def XMM29j( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(9));
 582 reg_def XMM29k( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(10));
 583 reg_def XMM29l( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(11));
 584 reg_def XMM29m( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(12));
 585 reg_def XMM29n( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(13));
 586 reg_def XMM29o( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(14));
 587 reg_def XMM29p( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(15));
 588 
 589 reg_def XMM30 ( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg());
 590 reg_def XMM30b( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(1));
 591 reg_def XMM30c( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(2));
 592 reg_def XMM30d( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(3));
 593 reg_def XMM30e( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(4));
 594 reg_def XMM30f( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(5));
 595 reg_def XMM30g( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(6));
 596 reg_def XMM30h( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(7));
 597 reg_def XMM30i( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(8));
 598 reg_def XMM30j( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(9));
 599 reg_def XMM30k( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(10));
 600 reg_def XMM30l( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(11));
 601 reg_def XMM30m( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(12));
 602 reg_def XMM30n( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(13));
 603 reg_def XMM30o( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(14));
 604 reg_def XMM30p( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(15));
 605 
 606 reg_def XMM31 ( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg());
 607 reg_def XMM31b( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(1));
 608 reg_def XMM31c( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(2));
 609 reg_def XMM31d( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(3));
 610 reg_def XMM31e( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(4));
 611 reg_def XMM31f( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(5));
 612 reg_def XMM31g( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(6));
 613 reg_def XMM31h( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(7));
 614 reg_def XMM31i( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(8));
 615 reg_def XMM31j( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(9));
 616 reg_def XMM31k( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(10));
 617 reg_def XMM31l( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(11));
 618 reg_def XMM31m( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(12));
 619 reg_def XMM31n( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(13));
 620 reg_def XMM31o( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(14));
 621 reg_def XMM31p( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(15));
 622 
 623 #endif // _LP64
 624 
 625 #ifdef _LP64
 626 reg_def RFLAGS(SOC, SOC, 0, 16, VMRegImpl::Bad());
 627 #else
 628 reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());
 629 #endif // _LP64
 630 
 631 alloc_class chunk1(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
 632                    XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
 633                    XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
 634                    XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
 635                    XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
 636                    XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
 637                    XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
 638                    XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
 639 #ifdef _LP64
 640                   ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
 641                    XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
 642                    XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
 643                    XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
 644                    XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
 645                    XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
 646                    XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
 647                    XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
 648                   ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
 649                    XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
 650                    XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
 651                    XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
 652                    XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
 653                    XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
 654                    XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
 655                    XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
 656                    XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
 657                    XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
 658                    XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
 659                    XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
 660                    XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
 661                    XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
 662                    XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
 663                    XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
 664 #endif
 665                       );
 666 
 667 // flags allocation class should be last.
 668 alloc_class chunk2(RFLAGS);
 669 
 670 // Singleton class for condition codes
 671 reg_class int_flags(RFLAGS);
 672 
 673 // Class for pre evex float registers
 674 reg_class float_reg_legacy(XMM0,
 675                     XMM1,
 676                     XMM2,
 677                     XMM3,
 678                     XMM4,
 679                     XMM5,
 680                     XMM6,
 681                     XMM7
 682 #ifdef _LP64
 683                    ,XMM8,
 684                     XMM9,
 685                     XMM10,
 686                     XMM11,
 687                     XMM12,
 688                     XMM13,
 689                     XMM14,
 690                     XMM15
 691 #endif
 692                     );
 693 
 694 // Class for evex float registers
 695 reg_class float_reg_evex(XMM0,
 696                     XMM1,
 697                     XMM2,
 698                     XMM3,
 699                     XMM4,
 700                     XMM5,
 701                     XMM6,
 702                     XMM7
 703 #ifdef _LP64
 704                    ,XMM8,
 705                     XMM9,
 706                     XMM10,
 707                     XMM11,
 708                     XMM12,
 709                     XMM13,
 710                     XMM14,
 711                     XMM15,
 712                     XMM16,
 713                     XMM17,
 714                     XMM18,
 715                     XMM19,
 716                     XMM20,
 717                     XMM21,
 718                     XMM22,
 719                     XMM23,
 720                     XMM24,
 721                     XMM25,
 722                     XMM26,
 723                     XMM27,
 724                     XMM28,
 725                     XMM29,
 726                     XMM30,
 727                     XMM31
 728 #endif
 729                     );
 730 
 731 reg_class_dynamic float_reg(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() %} );
 732 reg_class_dynamic float_reg_vl(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 733 
 734 // Class for pre evex double registers
 735 reg_class double_reg_legacy(XMM0,  XMM0b,
 736                      XMM1,  XMM1b,
 737                      XMM2,  XMM2b,
 738                      XMM3,  XMM3b,
 739                      XMM4,  XMM4b,
 740                      XMM5,  XMM5b,
 741                      XMM6,  XMM6b,
 742                      XMM7,  XMM7b
 743 #ifdef _LP64
 744                     ,XMM8,  XMM8b,
 745                      XMM9,  XMM9b,
 746                      XMM10, XMM10b,
 747                      XMM11, XMM11b,
 748                      XMM12, XMM12b,
 749                      XMM13, XMM13b,
 750                      XMM14, XMM14b,
 751                      XMM15, XMM15b
 752 #endif
 753                      );
 754 
 755 // Class for evex double registers
 756 reg_class double_reg_evex(XMM0,  XMM0b,
 757                      XMM1,  XMM1b,
 758                      XMM2,  XMM2b,
 759                      XMM3,  XMM3b,
 760                      XMM4,  XMM4b,
 761                      XMM5,  XMM5b,
 762                      XMM6,  XMM6b,
 763                      XMM7,  XMM7b
 764 #ifdef _LP64
 765                     ,XMM8,  XMM8b,
 766                      XMM9,  XMM9b,
 767                      XMM10, XMM10b,
 768                      XMM11, XMM11b,
 769                      XMM12, XMM12b,
 770                      XMM13, XMM13b,
 771                      XMM14, XMM14b,
 772                      XMM15, XMM15b,
 773                      XMM16, XMM16b,
 774                      XMM17, XMM17b,
 775                      XMM18, XMM18b,
 776                      XMM19, XMM19b,
 777                      XMM20, XMM20b,
 778                      XMM21, XMM21b,
 779                      XMM22, XMM22b,
 780                      XMM23, XMM23b,
 781                      XMM24, XMM24b,
 782                      XMM25, XMM25b,
 783                      XMM26, XMM26b,
 784                      XMM27, XMM27b,
 785                      XMM28, XMM28b,
 786                      XMM29, XMM29b,
 787                      XMM30, XMM30b,
 788                      XMM31, XMM31b
 789 #endif
 790                      );
 791 
 792 reg_class_dynamic double_reg(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() %} );
 793 reg_class_dynamic double_reg_vl(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 794 
 795 // Class for pre evex 32bit vector registers
 796 reg_class vectors_reg_legacy(XMM0,
 797                       XMM1,
 798                       XMM2,
 799                       XMM3,
 800                       XMM4,
 801                       XMM5,
 802                       XMM6,
 803                       XMM7
 804 #ifdef _LP64
 805                      ,XMM8,
 806                       XMM9,
 807                       XMM10,
 808                       XMM11,
 809                       XMM12,
 810                       XMM13,
 811                       XMM14,
 812                       XMM15
 813 #endif
 814                       );
 815 
 816 // Class for evex 32bit vector registers
 817 reg_class vectors_reg_evex(XMM0,
 818                       XMM1,
 819                       XMM2,
 820                       XMM3,
 821                       XMM4,
 822                       XMM5,
 823                       XMM6,
 824                       XMM7
 825 #ifdef _LP64
 826                      ,XMM8,
 827                       XMM9,
 828                       XMM10,
 829                       XMM11,
 830                       XMM12,
 831                       XMM13,
 832                       XMM14,
 833                       XMM15,
 834                       XMM16,
 835                       XMM17,
 836                       XMM18,
 837                       XMM19,
 838                       XMM20,
 839                       XMM21,
 840                       XMM22,
 841                       XMM23,
 842                       XMM24,
 843                       XMM25,
 844                       XMM26,
 845                       XMM27,
 846                       XMM28,
 847                       XMM29,
 848                       XMM30,
 849                       XMM31
 850 #endif
 851                       );
 852 
 853 reg_class_dynamic vectors_reg(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_evex() %} );
 854 reg_class_dynamic vectors_reg_vlbwdq(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 855 
 856 // Class for all 64bit vector registers
 857 reg_class vectord_reg_legacy(XMM0,  XMM0b,
 858                       XMM1,  XMM1b,
 859                       XMM2,  XMM2b,
 860                       XMM3,  XMM3b,
 861                       XMM4,  XMM4b,
 862                       XMM5,  XMM5b,
 863                       XMM6,  XMM6b,
 864                       XMM7,  XMM7b
 865 #ifdef _LP64
 866                      ,XMM8,  XMM8b,
 867                       XMM9,  XMM9b,
 868                       XMM10, XMM10b,
 869                       XMM11, XMM11b,
 870                       XMM12, XMM12b,
 871                       XMM13, XMM13b,
 872                       XMM14, XMM14b,
 873                       XMM15, XMM15b
 874 #endif
 875                       );
 876 
 877 // Class for all 64bit vector registers
 878 reg_class vectord_reg_evex(XMM0,  XMM0b,
 879                       XMM1,  XMM1b,
 880                       XMM2,  XMM2b,
 881                       XMM3,  XMM3b,
 882                       XMM4,  XMM4b,
 883                       XMM5,  XMM5b,
 884                       XMM6,  XMM6b,
 885                       XMM7,  XMM7b
 886 #ifdef _LP64
 887                      ,XMM8,  XMM8b,
 888                       XMM9,  XMM9b,
 889                       XMM10, XMM10b,
 890                       XMM11, XMM11b,
 891                       XMM12, XMM12b,
 892                       XMM13, XMM13b,
 893                       XMM14, XMM14b,
 894                       XMM15, XMM15b,
 895                       XMM16, XMM16b,
 896                       XMM17, XMM17b,
 897                       XMM18, XMM18b,
 898                       XMM19, XMM19b,
 899                       XMM20, XMM20b,
 900                       XMM21, XMM21b,
 901                       XMM22, XMM22b,
 902                       XMM23, XMM23b,
 903                       XMM24, XMM24b,
 904                       XMM25, XMM25b,
 905                       XMM26, XMM26b,
 906                       XMM27, XMM27b,
 907                       XMM28, XMM28b,
 908                       XMM29, XMM29b,
 909                       XMM30, XMM30b,
 910                       XMM31, XMM31b
 911 #endif
 912                       );
 913 
 914 reg_class_dynamic vectord_reg(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_evex() %} );
 915 reg_class_dynamic vectord_reg_vlbwdq(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 916 
 917 // Class for all 128bit vector registers
 918 reg_class vectorx_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,
 919                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 920                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 921                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 922                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 923                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 924                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 925                       XMM7,  XMM7b,  XMM7c,  XMM7d
 926 #ifdef _LP64
 927                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 928                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 929                       XMM10, XMM10b, XMM10c, XMM10d,
 930                       XMM11, XMM11b, XMM11c, XMM11d,
 931                       XMM12, XMM12b, XMM12c, XMM12d,
 932                       XMM13, XMM13b, XMM13c, XMM13d,
 933                       XMM14, XMM14b, XMM14c, XMM14d,
 934                       XMM15, XMM15b, XMM15c, XMM15d
 935 #endif
 936                       );
 937 
 938 // Class for all 128bit vector registers
 939 reg_class vectorx_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,
 940                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 941                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 942                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 943                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 944                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 945                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 946                       XMM7,  XMM7b,  XMM7c,  XMM7d
 947 #ifdef _LP64
 948                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 949                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 950                       XMM10, XMM10b, XMM10c, XMM10d,
 951                       XMM11, XMM11b, XMM11c, XMM11d,
 952                       XMM12, XMM12b, XMM12c, XMM12d,
 953                       XMM13, XMM13b, XMM13c, XMM13d,
 954                       XMM14, XMM14b, XMM14c, XMM14d,
 955                       XMM15, XMM15b, XMM15c, XMM15d,
 956                       XMM16, XMM16b, XMM16c, XMM16d,
 957                       XMM17, XMM17b, XMM17c, XMM17d,
 958                       XMM18, XMM18b, XMM18c, XMM18d,
 959                       XMM19, XMM19b, XMM19c, XMM19d,
 960                       XMM20, XMM20b, XMM20c, XMM20d,
 961                       XMM21, XMM21b, XMM21c, XMM21d,
 962                       XMM22, XMM22b, XMM22c, XMM22d,
 963                       XMM23, XMM23b, XMM23c, XMM23d,
 964                       XMM24, XMM24b, XMM24c, XMM24d,
 965                       XMM25, XMM25b, XMM25c, XMM25d,
 966                       XMM26, XMM26b, XMM26c, XMM26d,
 967                       XMM27, XMM27b, XMM27c, XMM27d,
 968                       XMM28, XMM28b, XMM28c, XMM28d,
 969                       XMM29, XMM29b, XMM29c, XMM29d,
 970                       XMM30, XMM30b, XMM30c, XMM30d,
 971                       XMM31, XMM31b, XMM31c, XMM31d
 972 #endif
 973                       );
 974 
 975 reg_class_dynamic vectorx_reg(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_evex() %} );
 976 reg_class_dynamic vectorx_reg_vlbwdq(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 977 
 978 // Class for all 256bit vector registers
 979 reg_class vectory_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
 980                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
 981                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
 982                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
 983                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
 984                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
 985                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
 986                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
 987 #ifdef _LP64
 988                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
 989                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
 990                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
 991                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
 992                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
 993                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
 994                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
 995                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h
 996 #endif
 997                       );
 998 
 999 // Class for all 256bit vector registers
1000 reg_class vectory_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
1001                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
1002                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
1003                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
1004                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
1005                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
1006                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
1007                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
1008 #ifdef _LP64
1009                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
1010                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
1011                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
1012                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
1013                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
1014                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
1015                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
1016                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h,
1017                       XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h,
1018                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h,
1019                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h,
1020                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h,
1021                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h,
1022                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h,
1023                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h,
1024                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h,
1025                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h,
1026                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h,
1027                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h,
1028                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h,
1029                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h,
1030                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h,
1031                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h,
1032                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h
1033 #endif
1034                       );
1035 
1036 reg_class_dynamic vectory_reg(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_evex() %} );
1037 reg_class_dynamic vectory_reg_vlbwdq(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
1038 
1039 // Class for all 512bit vector registers
1040 reg_class vectorz_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1041                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1042                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1043                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1044                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1045                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1046                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1047                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1048 #ifdef _LP64
1049                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1050                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1051                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1052                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1053                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1054                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1055                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1056                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1057                      ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
1058                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
1059                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
1060                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
1061                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
1062                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
1063                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
1064                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
1065                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
1066                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
1067                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
1068                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
1069                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
1070                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
1071                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
1072                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
1073 #endif
1074                       );
1075 
1076 // Class for restricted 512bit vector registers
1077 reg_class vectorz_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1078                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1079                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1080                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1081                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1082                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1083                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1084                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1085 #ifdef _LP64
1086                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1087                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1088                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1089                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1090                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1091                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1092                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1093                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1094 #endif
1095                       );
1096 
1097 reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
1098 reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
1099 
1100 %}
1101 
1102 
1103 //----------SOURCE BLOCK-------------------------------------------------------
1104 // This is a block of C++ code which provides values, functions, and
1105 // definitions necessary in the rest of the architecture description
1106 
1107 source_hpp %{
1108 // Header information of the source block.
1109 // Method declarations/definitions which are used outside
1110 // the ad-scope can conveniently be defined here.
1111 //
1112 // To keep related declarations/definitions/uses close together,
1113 // we switch between source %{ }% and source_hpp %{ }% freely as needed.
1114 
1115 class NativeJump;
1116 
1117 class CallStubImpl {
1118 
1119   //--------------------------------------------------------------
1120   //---&lt;  Used for optimization in Compile::shorten_branches  &gt;---
1121   //--------------------------------------------------------------
1122 
1123  public:
1124   // Size of call trampoline stub.
1125   static uint size_call_trampoline() {
1126     return 0; // no call trampolines on this platform
1127   }
1128 
1129   // number of relocations needed by a call trampoline stub
1130   static uint reloc_call_trampoline() {
1131     return 0; // no call trampolines on this platform
1132   }
1133 };
1134 
1135 class HandlerImpl {
1136 
1137  public:
1138 
1139   static int emit_exception_handler(CodeBuffer &amp;cbuf);
1140   static int emit_deopt_handler(CodeBuffer&amp; cbuf);
1141 
1142   static uint size_exception_handler() {
1143     // NativeCall instruction size is the same as NativeJump.
1144     // exception handler starts out as jump and can be patched to
1145     // a call be deoptimization.  (4932387)
1146     // Note that this value is also credited (in output.cpp) to
1147     // the size of the code section.
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
1168 class Node::PD {
1169 public:
1170   enum NodeFlags {
1171     Flag_intel_jcc_erratum = Node::_last_flag &lt;&lt; 1,
1172     _last_flag             = Flag_intel_jcc_erratum
1173   };
1174 };
1175 
1176 %} // end source_hpp
1177 
1178 source %{
1179 
1180 #include &quot;opto/addnode.hpp&quot;
1181 #include &quot;c2_intelJccErratum_x86.hpp&quot;
1182 
1183 void PhaseOutput::pd_perform_mach_node_analysis() {
1184   if (VM_Version::has_intel_jcc_erratum()) {
1185     int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C-&gt;cfg(), C-&gt;regalloc());
1186     _buf_sizes._code += extra_padding;
1187   }
1188 }
1189 
1190 int MachNode::pd_alignment_required() const {
1191   PhaseOutput* output = Compile::current()-&gt;output();
1192   Block* block = output-&gt;block();
1193   int index = output-&gt;index();
1194   if (VM_Version::has_intel_jcc_erratum() &amp;&amp; IntelJccErratum::is_jcc_erratum_branch(block, this, index)) {
1195     // Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.
1196     return IntelJccErratum::largest_jcc_size() + 1;
1197   } else {
1198     return 1;
1199   }
1200 }
1201 
1202 int MachNode::compute_padding(int current_offset) const {
1203   if (flags() &amp; Node::PD::Flag_intel_jcc_erratum) {
1204     Compile* C = Compile::current();
1205     PhaseOutput* output = C-&gt;output();
1206     Block* block = output-&gt;block();
1207     int index = output-&gt;index();
1208     return IntelJccErratum::compute_padding(current_offset, this, block, index, C-&gt;regalloc());
1209   } else {
1210     return 0;
1211   }
1212 }
1213 
1214 // Emit exception handler code.
1215 // Stuff framesize into a register and call a VM stub routine.
1216 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1217 
1218   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1219   // That&#39;s why we must use the macroassembler to generate a handler.
1220   C2_MacroAssembler _masm(&amp;cbuf);
1221   address base = __ start_a_stub(size_exception_handler());
1222   if (base == NULL) {
1223     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1224     return 0;  // CodeBuffer::expand failed
1225   }
1226   int offset = __ offset();
1227   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1228   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1229   __ end_a_stub();
1230   return offset;
1231 }
1232 
1233 // Emit deopt handler code.
1234 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1235 
1236   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1237   // That&#39;s why we must use the macroassembler to generate a handler.
1238   C2_MacroAssembler _masm(&amp;cbuf);
1239   address base = __ start_a_stub(size_deopt_handler());
1240   if (base == NULL) {
1241     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1242     return 0;  // CodeBuffer::expand failed
1243   }
1244   int offset = __ offset();
1245 
1246 #ifdef _LP64
1247   address the_pc = (address) __ pc();
1248   Label next;
1249   // push a &quot;the_pc&quot; on the stack without destroying any registers
1250   // as they all may be live.
1251 
1252   // push address of &quot;next&quot;
1253   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1254   __ bind(next);
1255   // adjust it so it matches &quot;the_pc&quot;
1256   __ subptr(Address(rsp, 0), __ offset() - offset);
1257 #else
1258   InternalAddress here(__ pc());
1259   __ pushptr(here.addr());
1260 #endif
1261 
1262   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
1263   assert(__ offset() - offset &lt;= (int) size_deopt_handler(), &quot;overflow %d&quot;, (__ offset() - offset));
1264   __ end_a_stub();
1265   return offset;
1266 }
1267 
1268 
1269 //=============================================================================
1270 
1271   // Float masks come from different places depending on platform.
1272 #ifdef _LP64
1273   static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }
1274   static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }
1275   static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }
1276   static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }
1277 #else
1278   static address float_signmask()  { return (address)float_signmask_pool; }
1279   static address float_signflip()  { return (address)float_signflip_pool; }
1280   static address double_signmask() { return (address)double_signmask_pool; }
1281   static address double_signflip() { return (address)double_signflip_pool; }
1282 #endif
1283   static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
1284   static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
1285   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1286 
1287 //=============================================================================
1288 const bool Matcher::match_rule_supported(int opcode) {
1289   if (!has_match_rule(opcode)) {
1290     return false; // no match rule present
1291   }
1292   switch (opcode) {
1293     case Op_AbsVL:
1294       if (UseAVX &lt; 3) {
1295         return false;
1296       }
1297       break;
1298     case Op_PopCountI:
1299     case Op_PopCountL:
1300       if (!UsePopCountInstruction) {
1301         return false;
1302       }
1303       break;
1304     case Op_PopCountVI:
1305       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {
1306         return false;
1307       }
1308       break;
1309     case Op_MulVI:
1310       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1311         return false;
1312       }
1313       break;
1314     case Op_MulVL:
1315     case Op_MulReductionVL:
1316       if (VM_Version::supports_avx512dq() == false) {
1317         return false;
1318       }
1319       break;
1320     case Op_AbsVB:
1321     case Op_AbsVS:
1322     case Op_AbsVI:
1323     case Op_AddReductionVI:
1324     case Op_AndReductionV:
1325     case Op_OrReductionV:
1326     case Op_XorReductionV:
1327       if (UseSSE &lt; 3) { // requires at least SSSE3
1328         return false;
1329       }
1330       break;
1331     case Op_MulReductionVI:
1332       if (UseSSE &lt; 4) { // requires at least SSE4
1333         return false;
1334       }
1335       break;
1336     case Op_SqrtVD:
1337     case Op_SqrtVF:
1338       if (UseAVX &lt; 1) { // enabled for AVX only
1339         return false;
1340       }
1341       break;
1342     case Op_CompareAndSwapL:
1343 #ifdef _LP64
1344     case Op_CompareAndSwapP:
1345 #endif
1346       if (!VM_Version::supports_cx8()) {
1347         return false;
1348       }
1349       break;
1350     case Op_CMoveVF:
1351     case Op_CMoveVD:
1352       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1353         return false;
1354       }
1355       break;
1356     case Op_StrIndexOf:
1357       if (!UseSSE42Intrinsics) {
1358         return false;
1359       }
1360       break;
1361     case Op_StrIndexOfChar:
1362       if (!UseSSE42Intrinsics) {
1363         return false;
1364       }
1365       break;
1366     case Op_OnSpinWait:
1367       if (VM_Version::supports_on_spin_wait() == false) {
1368         return false;
1369       }
1370       break;
1371     case Op_MulVB:
1372     case Op_LShiftVB:
1373     case Op_RShiftVB:
1374     case Op_URShiftVB:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379 #ifdef _LP64
1380     case Op_MaxD:
1381     case Op_MaxF:
1382     case Op_MinD:
1383     case Op_MinF:
1384       if (UseAVX &lt; 1) { // enabled for AVX only
1385         return false;
1386       }
1387       break;
1388 #endif
1389     case Op_CacheWB:
1390     case Op_CacheWBPreSync:
1391     case Op_CacheWBPostSync:
1392       if (!VM_Version::supports_data_cache_line_flush()) {
1393         return false;
1394       }
1395       break;
1396     case Op_RoundDoubleMode:
1397       if (UseSSE &lt; 4) {
1398         return false;
1399       }
1400       break;
1401     case Op_RoundDoubleModeV:
1402       if (VM_Version::supports_avx() == false) {
1403         return false; // 128bit vroundpd is not available
1404       }
1405       break;
1406     case Op_MacroLogicV:
1407       if (UseAVX &lt; 3 || !UseVectorMacroLogic) {
1408         return false;
1409       }
1410       break;
1411 #ifndef _LP64
1412     case Op_AddReductionVF:
1413     case Op_AddReductionVD:
1414     case Op_MulReductionVF:
1415     case Op_MulReductionVD:
1416       if (UseSSE &lt; 1) { // requires at least SSE
1417         return false;
1418       }
1419       break;
1420     case Op_MulAddVS2VI:
1421     case Op_RShiftVL:
1422     case Op_AbsVD:
1423     case Op_NegVD:
1424       if (UseSSE &lt; 2) {
1425         return false;
1426       }
1427       break;
1428 #endif // !LP64
1429   }
1430   return true;  // Match rules are supported by default.
1431 }
1432 
1433 //------------------------------------------------------------------------
1434 
1435 // Identify extra cases that we might want to provide match rules for vector nodes and
1436 // other intrinsics guarded with vector length (vlen) and element type (bt).
1437 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1438   if (!match_rule_supported(opcode)) {
1439     return false;
1440   }
1441   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1442   //   * SSE2 supports 128bit vectors for all types;
1443   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1444   //   * AVX2 supports 256bit vectors for all types;
1445   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1446   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1447   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1448   // And MaxVectorSize is taken into account as well.
1449   if (!vector_size_supported(bt, vlen)) {
1450     return false;
1451   }
1452   // Special cases which require vector length follow:
1453   //   * implementation limitations
1454   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1455   //   * 128bit vroundpd instruction is present only in AVX1
1456   int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;
1457   switch (opcode) {
1458     case Op_AbsVF:
1459     case Op_NegVF:
1460       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1461         return false; // 512bit vandps and vxorps are not available
1462       }
1463       break;
1464     case Op_AbsVD:
1465     case Op_NegVD:
1466       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1467         return false; // 512bit vandpd and vxorpd are not available
1468       }
1469       break;
1470     case Op_CMoveVF:
1471       if (vlen != 8) {
1472         return false; // implementation limitation (only vcmov8F_reg is present)
1473       }
1474       break;
1475     case Op_MacroLogicV:
1476       if (!VM_Version::supports_evex() ||
1477           ((size_in_bits != 512) &amp;&amp; !VM_Version::supports_avx512vl())) {
1478         return false;
1479       }
1480       break;
1481     case Op_CMoveVD:
1482       if (vlen != 4) {
1483         return false; // implementation limitation (only vcmov4D_reg is present)
1484       }
1485       break;
1486   }
1487   return true;  // Per default match rules are supported.
1488 }
1489 
1490 // x86 supports generic vector operands: vec and legVec.
1491 const bool Matcher::supports_generic_vector_operands = true;
1492 
1493 MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1494   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1495   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1496   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1497       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1498     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1499     return new legVecZOper();
1500   }
1501   if (legacy) {
1502     switch (ideal_reg) {
1503       case Op_VecS: return new legVecSOper();
1504       case Op_VecD: return new legVecDOper();
1505       case Op_VecX: return new legVecXOper();
1506       case Op_VecY: return new legVecYOper();
1507       case Op_VecZ: return new legVecZOper();
1508     }
1509   } else {
1510     switch (ideal_reg) {
1511       case Op_VecS: return new vecSOper();
1512       case Op_VecD: return new vecDOper();
1513       case Op_VecX: return new vecXOper();
1514       case Op_VecY: return new vecYOper();
1515       case Op_VecZ: return new vecZOper();
1516     }
1517   }
1518   ShouldNotReachHere();
1519   return NULL;
1520 }
1521 
1522 bool Matcher::is_generic_reg2reg_move(MachNode* m) {
1523   switch (m-&gt;rule()) {
1524     case MoveVec2Leg_rule:
1525     case MoveLeg2Vec_rule:
1526       return true;
1527     default:
1528       return false;
1529   }
1530 }
1531 
1532 bool Matcher::is_generic_vector(MachOper* opnd) {
1533   switch (opnd-&gt;opcode()) {
1534     case VEC:
1535     case LEGVEC:
1536       return true;
1537     default:
1538       return false;
1539   }
1540 }
1541 
1542 //------------------------------------------------------------------------
1543 
1544 const bool Matcher::has_predicated_vectors(void) {
1545   bool ret_value = false;
1546   if (UseAVX &gt; 2) {
1547     ret_value = VM_Version::supports_avx512vl();
1548   }
1549 
1550   return ret_value;
1551 }
1552 
1553 const int Matcher::float_pressure(int default_pressure_threshold) {
1554   int float_pressure_threshold = default_pressure_threshold;
1555 #ifdef _LP64
1556   if (UseAVX &gt; 2) {
1557     // Increase pressure threshold on machines with AVX3 which have
1558     // 2x more XMM registers.
1559     float_pressure_threshold = default_pressure_threshold * 2;
1560   }
1561 #endif
1562   return float_pressure_threshold;
1563 }
1564 
1565 // Max vector size in bytes. 0 if not supported.
1566 const int Matcher::vector_width_in_bytes(BasicType bt) {
1567   assert(is_java_primitive(bt), &quot;only primitive type vectors&quot;);
1568   if (UseSSE &lt; 2) return 0;
1569   // SSE2 supports 128bit vectors for all types.
1570   // AVX2 supports 256bit vectors for all types.
1571   // AVX2/EVEX supports 512bit vectors for all types.
1572   int size = (UseAVX &gt; 1) ? (1 &lt;&lt; UseAVX) * 8 : 16;
1573   // AVX1 supports 256bit vectors only for FLOAT and DOUBLE.
1574   if (UseAVX &gt; 0 &amp;&amp; (bt == T_FLOAT || bt == T_DOUBLE))
1575     size = (UseAVX &gt; 2) ? 64 : 32;
1576   if (UseAVX &gt; 2 &amp;&amp; (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))
1577     size = (VM_Version::supports_avx512bw()) ? 64 : 32;
1578   // Use flag to limit vector size.
1579   size = MIN2(size,(int)MaxVectorSize);
1580   // Minimum 2 values in vector (or 4 for bytes).
1581   switch (bt) {
1582   case T_DOUBLE:
1583   case T_LONG:
1584     if (size &lt; 16) return 0;
1585     break;
1586   case T_FLOAT:
1587   case T_INT:
1588     if (size &lt; 8) return 0;
1589     break;
1590   case T_BOOLEAN:
1591     if (size &lt; 4) return 0;
1592     break;
1593   case T_CHAR:
1594     if (size &lt; 4) return 0;
1595     break;
1596   case T_BYTE:
1597     if (size &lt; 4) return 0;
1598     break;
1599   case T_SHORT:
1600     if (size &lt; 4) return 0;
1601     break;
1602   default:
1603     ShouldNotReachHere();
1604   }
1605   return size;
1606 }
1607 
1608 // Limits on vector size (number of elements) loaded into vector.
1609 const int Matcher::max_vector_size(const BasicType bt) {
1610   return vector_width_in_bytes(bt)/type2aelembytes(bt);
1611 }
1612 const int Matcher::min_vector_size(const BasicType bt) {
1613   int max_size = max_vector_size(bt);
1614   // Min size which can be loaded into vector is 4 bytes.
1615   int size = (type2aelembytes(bt) == 1) ? 4 : 2;
1616   return MIN2(size,max_size);
1617 }
1618 
1619 // Vector ideal reg corresponding to specified size in bytes
1620 const uint Matcher::vector_ideal_reg(int size) {
1621   assert(MaxVectorSize &gt;= size, &quot;&quot;);
1622   switch(size) {
1623     case  4: return Op_VecS;
1624     case  8: return Op_VecD;
1625     case 16: return Op_VecX;
1626     case 32: return Op_VecY;
1627     case 64: return Op_VecZ;
1628   }
1629   ShouldNotReachHere();
1630   return 0;
1631 }
1632 
1633 // x86 supports misaligned vectors store/load.
1634 const bool Matcher::misaligned_vectors_ok() {
1635   return true;
1636 }
1637 
1638 // x86 AES instructions are compatible with SunJCE expanded
1639 // keys, hence we do not need to pass the original key to stubs
1640 const bool Matcher::pass_original_key_for_aes() {
1641   return false;
1642 }
1643 
1644 
1645 const bool Matcher::convi2l_type_required = true;
1646 
1647 // Check for shift by small constant as well
1648 static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1649   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
1650       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
1651       // Are there other uses besides address expressions?
1652       !matcher-&gt;is_visited(shift)) {
1653     address_visited.set(shift-&gt;_idx); // Flag as address_visited
1654     mstack.push(shift-&gt;in(2), Matcher::Visit);
1655     Node *conv = shift-&gt;in(1);
1656 #ifdef _LP64
1657     // Allow Matcher to match the rule which bypass
1658     // ConvI2L operation for an array index on LP64
1659     // if the index value is positive.
1660     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1661         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1662         // Are there other uses besides address expressions?
1663         !matcher-&gt;is_visited(conv)) {
1664       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1665       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1666     } else
1667 #endif
1668       mstack.push(conv, Matcher::Pre_Visit);
1669     return true;
1670   }
1671   return false;
1672 }
1673 
1674 // This function identifies sub-graphs in which a &#39;load&#39; node is
1675 // input to two different nodes, and such that it can be matched
1676 // with BMI instructions like blsi, blsr, etc.
1677 // Example : for b = -a[i] &amp; a[i] can be matched to blsi r32, m32.
1678 // The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*
1679 // refers to the same node.
1680 //
1681 // Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)
1682 // This is a temporary solution until we make DAGs expressible in ADL.
1683 template&lt;typename ConType&gt;
1684 class FusedPatternMatcher {
1685   Node* _op1_node;
1686   Node* _mop_node;
1687   int _con_op;
1688 
1689   static int match_next(Node* n, int next_op, int next_op_idx) {
1690     if (n-&gt;in(1) == NULL || n-&gt;in(2) == NULL) {
1691       return -1;
1692     }
1693 
1694     if (next_op_idx == -1) { // n is commutative, try rotations
1695       if (n-&gt;in(1)-&gt;Opcode() == next_op) {
1696         return 1;
1697       } else if (n-&gt;in(2)-&gt;Opcode() == next_op) {
1698         return 2;
1699       }
1700     } else {
1701       assert(next_op_idx &gt; 0 &amp;&amp; next_op_idx &lt;= 2, &quot;Bad argument index&quot;);
1702       if (n-&gt;in(next_op_idx)-&gt;Opcode() == next_op) {
1703         return next_op_idx;
1704       }
1705     }
1706     return -1;
1707   }
1708 
1709  public:
1710   FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :
1711     _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }
1712 
1713   bool match(int op1, int op1_op2_idx,  // op1 and the index of the op1-&gt;op2 edge, -1 if op1 is commutative
1714              int op2, int op2_con_idx,  // op2 and the index of the op2-&gt;con edge, -1 if op2 is commutative
1715              typename ConType::NativeType con_value) {
1716     if (_op1_node-&gt;Opcode() != op1) {
1717       return false;
1718     }
1719     if (_mop_node-&gt;outcnt() &gt; 2) {
1720       return false;
1721     }
1722     op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);
1723     if (op1_op2_idx == -1) {
1724       return false;
1725     }
1726     // Memory operation must be the other edge
1727     int op1_mop_idx = (op1_op2_idx &amp; 1) + 1;
1728 
1729     // Check that the mop node is really what we want
1730     if (_op1_node-&gt;in(op1_mop_idx) == _mop_node) {
1731       Node* op2_node = _op1_node-&gt;in(op1_op2_idx);
1732       if (op2_node-&gt;outcnt() &gt; 1) {
1733         return false;
1734       }
1735       assert(op2_node-&gt;Opcode() == op2, &quot;Should be&quot;);
1736       op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);
1737       if (op2_con_idx == -1) {
1738         return false;
1739       }
1740       // Memory operation must be the other edge
1741       int op2_mop_idx = (op2_con_idx &amp; 1) + 1;
1742       // Check that the memory operation is the same node
1743       if (op2_node-&gt;in(op2_mop_idx) == _mop_node) {
1744         // Now check the constant
1745         const Type* con_type = op2_node-&gt;in(op2_con_idx)-&gt;bottom_type();
1746         if (con_type != Type::TOP &amp;&amp; ConType::as_self(con_type)-&gt;get_con() == con_value) {
1747           return true;
1748         }
1749       }
1750     }
1751     return false;
1752   }
1753 };
1754 
1755 static bool is_bmi_pattern(Node* n, Node* m) {
1756   assert(UseBMI1Instructions, &quot;sanity&quot;);
1757   if (n != NULL &amp;&amp; m != NULL) {
1758     if (m-&gt;Opcode() == Op_LoadI) {
1759       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
1760       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
1761              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
1762              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
1763     } else if (m-&gt;Opcode() == Op_LoadL) {
1764       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
1765       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
1766              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
1767              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
1768     }
1769   }
1770   return false;
1771 }
1772 
1773 // Should the matcher clone input &#39;m&#39; of node &#39;n&#39;?
1774 bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack&amp; mstack) {
1775   // If &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone the input &#39;m&#39;.
1776   if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
1777     mstack.push(m, Visit);
1778     return true;
1779   }
1780   return false;
1781 }
1782 
1783 // Should the Matcher clone shifts on addressing modes, expecting them
1784 // to be subsumed into complex addressing expressions or compute them
1785 // into registers?
1786 bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1787   Node *off = m-&gt;in(AddPNode::Offset);
1788   if (off-&gt;is_Con()) {
1789     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1790     Node *adr = m-&gt;in(AddPNode::Address);
1791 
1792     // Intel can handle 2 adds in addressing mode
1793     // AtomicAdd is not an addressing expression.
1794     // Cheap to find it by looking for screwy base.
1795     if (adr-&gt;is_AddP() &amp;&amp;
1796         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1797         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1798         // Are there other uses besides address expressions?
1799         !is_visited(adr)) {
1800       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1801       Node *shift = adr-&gt;in(AddPNode::Offset);
1802       if (!clone_shift(shift, this, mstack, address_visited)) {
1803         mstack.push(shift, Pre_Visit);
1804       }
1805       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1806       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
1807     } else {
1808       mstack.push(adr, Pre_Visit);
1809     }
1810 
1811     // Clone X+offset as it also folds into most addressing expressions
1812     mstack.push(off, Visit);
1813     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1814     return true;
1815   } else if (clone_shift(off, this, mstack, address_visited)) {
1816     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1817     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1818     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1819     return true;
1820   }
1821   return false;
1822 }
1823 
1824 void Compile::reshape_address(AddPNode* addp) {
1825 }
1826 
1827 static inline uint vector_length(const MachNode* n) {
1828   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1829   return vt-&gt;length();
1830 }
1831 
1832 static inline uint vector_length(const MachNode* use, MachOper* opnd) {
1833   uint def_idx = use-&gt;operand_index(opnd);
1834   Node* def = use-&gt;in(def_idx);
1835   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length();
1836 }
1837 
1838 static inline uint vector_length_in_bytes(const MachNode* n) {
1839   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1840   return vt-&gt;length_in_bytes();
1841 }
1842 
1843 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1844   uint def_idx = use-&gt;operand_index(opnd);
1845   Node* def = use-&gt;in(def_idx);
1846   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1847 }
1848 
1849 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1850   switch(vector_length_in_bytes(n)) {
1851     case  4: // fall-through
1852     case  8: // fall-through
1853     case 16: return Assembler::AVX_128bit;
1854     case 32: return Assembler::AVX_256bit;
1855     case 64: return Assembler::AVX_512bit;
1856 
1857     default: {
1858       ShouldNotReachHere();
1859       return Assembler::AVX_NoVec;
1860     }
1861   }
1862 }
1863 
1864 // Helper methods for MachSpillCopyNode::implementation().
1865 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1866                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1867   // In 64-bit VM size calculation is very complex. Emitting instructions
1868   // into scratch buffer is used to get size in 64-bit VM.
1869   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1870   assert(ireg == Op_VecS || // 32bit vector
1871          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1872          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1873          &quot;no non-adjacent vector moves&quot; );
1874   if (cbuf) {
1875     C2_MacroAssembler _masm(cbuf);
1876     int offset = __ offset();
1877     switch (ireg) {
1878     case Op_VecS: // copy whole register
1879     case Op_VecD:
1880     case Op_VecX:
1881 #ifndef _LP64
1882       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1883 #else
1884       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1885         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1886       } else {
1887         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1888      }
1889 #endif
1890       break;
1891     case Op_VecY:
1892 #ifndef _LP64
1893       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1894 #else
1895       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1896         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1897       } else {
1898         __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1899      }
1900 #endif
1901       break;
1902     case Op_VecZ:
1903       __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);
1904       break;
1905     default:
1906       ShouldNotReachHere();
1907     }
1908     int size = __ offset() - offset;
1909 #ifdef ASSERT
1910     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1911     assert(!do_size || size == 4, &quot;incorrect size calculattion&quot;);
1912 #endif
1913     return size;
1914 #ifndef PRODUCT
1915   } else if (!do_size) {
1916     switch (ireg) {
1917     case Op_VecS:
1918     case Op_VecD:
1919     case Op_VecX:
1920       st-&gt;print(&quot;movdqu  %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1921       break;
1922     case Op_VecY:
1923     case Op_VecZ:
1924       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1925       break;
1926     default:
1927       ShouldNotReachHere();
1928     }
1929 #endif
1930   }
1931   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1932   return (UseAVX &gt; 2) ? 6 : 4;
1933 }
1934 
1935 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1936                      int stack_offset, int reg, uint ireg, outputStream* st) {
1937   // In 64-bit VM size calculation is very complex. Emitting instructions
1938   // into scratch buffer is used to get size in 64-bit VM.
1939   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1940   if (cbuf) {
1941     C2_MacroAssembler _masm(cbuf);
1942     int offset = __ offset();
1943     if (is_load) {
1944       switch (ireg) {
1945       case Op_VecS:
1946         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1947         break;
1948       case Op_VecD:
1949         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1950         break;
1951       case Op_VecX:
1952 #ifndef _LP64
1953         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1954 #else
1955         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1956           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1957         } else {
1958           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1959           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1960         }
1961 #endif
1962         break;
1963       case Op_VecY:
1964 #ifndef _LP64
1965         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1966 #else
1967         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1968           __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1969         } else {
1970           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1971           __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1972         }
1973 #endif
1974         break;
1975       case Op_VecZ:
1976         __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);
1977         break;
1978       default:
1979         ShouldNotReachHere();
1980       }
1981     } else { // store
1982       switch (ireg) {
1983       case Op_VecS:
1984         __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1985         break;
1986       case Op_VecD:
1987         __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1988         break;
1989       case Op_VecX:
1990 #ifndef _LP64
1991         __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1992 #else
1993         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1994           __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1995         }
1996         else {
1997           __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1998         }
1999 #endif
2000         break;
2001       case Op_VecY:
2002 #ifndef _LP64
2003         __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
2004 #else
2005         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
2006           __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
2007         }
2008         else {
2009           __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
2010         }
2011 #endif
2012         break;
2013       case Op_VecZ:
2014         __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);
2015         break;
2016       default:
2017         ShouldNotReachHere();
2018       }
2019     }
2020     int size = __ offset() - offset;
2021 #ifdef ASSERT
2022     int offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt; 0x80) ? 1 : (UseAVX &gt; 2) ? 6 : 4);
2023     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
2024     assert(!do_size || size == (5+offset_size), &quot;incorrect size calculattion&quot;);
2025 #endif
2026     return size;
2027 #ifndef PRODUCT
2028   } else if (!do_size) {
2029     if (is_load) {
2030       switch (ireg) {
2031       case Op_VecS:
2032         st-&gt;print(&quot;movd    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2033         break;
2034       case Op_VecD:
2035         st-&gt;print(&quot;movq    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2036         break;
2037        case Op_VecX:
2038         st-&gt;print(&quot;movdqu  %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2039         break;
2040       case Op_VecY:
2041       case Op_VecZ:
2042         st-&gt;print(&quot;vmovdqu %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
2043         break;
2044       default:
2045         ShouldNotReachHere();
2046       }
2047     } else { // store
2048       switch (ireg) {
2049       case Op_VecS:
2050         st-&gt;print(&quot;movd    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2051         break;
2052       case Op_VecD:
2053         st-&gt;print(&quot;movq    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2054         break;
2055        case Op_VecX:
2056         st-&gt;print(&quot;movdqu  [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2057         break;
2058       case Op_VecY:
2059       case Op_VecZ:
2060         st-&gt;print(&quot;vmovdqu [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
2061         break;
2062       default:
2063         ShouldNotReachHere();
2064       }
2065     }
2066 #endif
2067   }
2068   bool is_single_byte = false;
2069   int vec_len = 0;
2070   if ((UseAVX &gt; 2) &amp;&amp; (stack_offset != 0)) {
2071     int tuple_type = Assembler::EVEX_FVM;
2072     int input_size = Assembler::EVEX_32bit;
2073     switch (ireg) {
2074     case Op_VecS:
2075       tuple_type = Assembler::EVEX_T1S;
2076       break;
2077     case Op_VecD:
2078       tuple_type = Assembler::EVEX_T1S;
2079       input_size = Assembler::EVEX_64bit;
2080       break;
2081     case Op_VecX:
2082       break;
2083     case Op_VecY:
2084       vec_len = 1;
2085       break;
2086     case Op_VecZ:
2087       vec_len = 2;
2088       break;
2089     }
2090     is_single_byte = Assembler::query_compressed_disp_byte(stack_offset, true, vec_len, tuple_type, input_size, 0);
2091   }
2092   int offset_size = 0;
2093   int size = 5;
2094   if (UseAVX &gt; 2 ) {
2095     if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len == 2)) {
2096       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
2097       size += 2; // Need an additional two bytes for EVEX encoding
2098     } else if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len &lt; 2)) {
2099       offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
2100     } else {
2101       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
2102       size += 2; // Need an additional two bytes for EVEX encodding
2103     }
2104   } else {
2105     offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
2106   }
2107   // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
2108   return size+offset_size;
2109 }
2110 
2111 static inline jint replicate4_imm(int con, int width) {
2112   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 32bit.
2113   assert(width == 1 || width == 2, &quot;only byte or short types here&quot;);
2114   int bit_width = width * 8;
2115   jint val = con;
2116   val &amp;= (1 &lt;&lt; bit_width) - 1;  // mask off sign bits
2117   while(bit_width &lt; 32) {
2118     val |= (val &lt;&lt; bit_width);
2119     bit_width &lt;&lt;= 1;
2120   }
2121   return val;
2122 }
2123 
2124 static inline jlong replicate8_imm(int con, int width) {
2125   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
2126   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
2127   int bit_width = width * 8;
2128   jlong val = con;
2129   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
2130   while(bit_width &lt; 64) {
2131     val |= (val &lt;&lt; bit_width);
2132     bit_width &lt;&lt;= 1;
2133   }
2134   return val;
2135 }
2136 
2137 #ifndef PRODUCT
2138   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
2139     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
2140   }
2141 #endif
2142 
2143   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
2144     C2_MacroAssembler _masm(&amp;cbuf);
2145     __ nop(_count);
2146   }
2147 
2148   uint MachNopNode::size(PhaseRegAlloc*) const {
2149     return _count;
2150   }
2151 
2152 #ifndef PRODUCT
2153   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
2154     st-&gt;print(&quot;# breakpoint&quot;);
2155   }
2156 #endif
2157 
2158   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
2159     C2_MacroAssembler _masm(&amp;cbuf);
2160     __ int3();
2161   }
2162 
2163   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2164     return MachNode::size(ra_);
2165   }
2166 
2167 %}
2168 
2169 encode %{
2170 
2171   enc_class call_epilog %{
2172     if (VerifyStackAtCalls) {
2173       // Check that stack depth is unchanged: find majik cookie on stack
2174       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
2175       C2_MacroAssembler _masm(&amp;cbuf);
2176       Label L;
2177       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2178       __ jccb(Assembler::equal, L);
2179       // Die if stack mismatch
2180       __ int3();
2181       __ bind(L);
2182     }
2183   %}
2184 
2185 %}
2186 
2187 
2188 //----------OPERANDS-----------------------------------------------------------
2189 // Operand definitions must precede instruction definitions for correct parsing
2190 // in the ADLC because operands constitute user defined types which are used in
2191 // instruction definitions.
2192 
2193 // Vectors
2194 
2195 // Dummy generic vector class. Should be used for all vector operands.
2196 // Replaced with vec[SDXYZ] during post-selection pass.
2197 operand vec() %{
2198   constraint(ALLOC_IN_RC(dynamic));
2199   match(VecX);
2200   match(VecY);
2201   match(VecZ);
2202   match(VecS);
2203   match(VecD);
2204 
2205   format %{ %}
2206   interface(REG_INTER);
2207 %}
2208 
2209 // Dummy generic legacy vector class. Should be used for all legacy vector operands.
2210 // Replaced with legVec[SDXYZ] during post-selection cleanup.
2211 // Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
2212 // runtime code generation via reg_class_dynamic.
2213 operand legVec() %{
2214   constraint(ALLOC_IN_RC(dynamic));
2215   match(VecX);
2216   match(VecY);
2217   match(VecZ);
2218   match(VecS);
2219   match(VecD);
2220 
2221   format %{ %}
2222   interface(REG_INTER);
2223 %}
2224 
2225 // Replaces vec during post-selection cleanup. See above.
2226 operand vecS() %{
2227   constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
2228   match(VecS);
2229 
2230   format %{ %}
2231   interface(REG_INTER);
2232 %}
2233 
2234 // Replaces legVec during post-selection cleanup. See above.
2235 operand legVecS() %{
2236   constraint(ALLOC_IN_RC(vectors_reg_legacy));
2237   match(VecS);
2238 
2239   format %{ %}
2240   interface(REG_INTER);
2241 %}
2242 
2243 // Replaces vec during post-selection cleanup. See above.
2244 operand vecD() %{
2245   constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
2246   match(VecD);
2247 
2248   format %{ %}
2249   interface(REG_INTER);
2250 %}
2251 
2252 // Replaces legVec during post-selection cleanup. See above.
2253 operand legVecD() %{
2254   constraint(ALLOC_IN_RC(vectord_reg_legacy));
2255   match(VecD);
2256 
2257   format %{ %}
2258   interface(REG_INTER);
2259 %}
2260 
2261 // Replaces vec during post-selection cleanup. See above.
2262 operand vecX() %{
2263   constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
2264   match(VecX);
2265 
2266   format %{ %}
2267   interface(REG_INTER);
2268 %}
2269 
2270 // Replaces legVec during post-selection cleanup. See above.
2271 operand legVecX() %{
2272   constraint(ALLOC_IN_RC(vectorx_reg_legacy));
2273   match(VecX);
2274 
2275   format %{ %}
2276   interface(REG_INTER);
2277 %}
2278 
2279 // Replaces vec during post-selection cleanup. See above.
2280 operand vecY() %{
2281   constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
2282   match(VecY);
2283 
2284   format %{ %}
2285   interface(REG_INTER);
2286 %}
2287 
2288 // Replaces legVec during post-selection cleanup. See above.
2289 operand legVecY() %{
2290   constraint(ALLOC_IN_RC(vectory_reg_legacy));
2291   match(VecY);
2292 
2293   format %{ %}
2294   interface(REG_INTER);
2295 %}
2296 
2297 // Replaces vec during post-selection cleanup. See above.
2298 operand vecZ() %{
2299   constraint(ALLOC_IN_RC(vectorz_reg));
2300   match(VecZ);
2301 
2302   format %{ %}
2303   interface(REG_INTER);
2304 %}
2305 
2306 // Replaces legVec during post-selection cleanup. See above.
2307 operand legVecZ() %{
2308   constraint(ALLOC_IN_RC(vectorz_reg_legacy));
2309   match(VecZ);
2310 
2311   format %{ %}
2312   interface(REG_INTER);
2313 %}
2314 
2315 // Comparison Code for FP conditional move
2316 operand cmpOp_vcmppd() %{
2317   match(Bool);
2318 
2319   predicate(n-&gt;as_Bool()-&gt;_test._test != BoolTest::overflow &amp;&amp;
2320             n-&gt;as_Bool()-&gt;_test._test != BoolTest::no_overflow);
2321   format %{ &quot;&quot; %}
2322   interface(COND_INTER) %{
2323     equal        (0x0, &quot;eq&quot;);
2324     less         (0x1, &quot;lt&quot;);
2325     less_equal   (0x2, &quot;le&quot;);
2326     not_equal    (0xC, &quot;ne&quot;);
2327     greater_equal(0xD, &quot;ge&quot;);
2328     greater      (0xE, &quot;gt&quot;);
2329     //TODO cannot compile (adlc breaks) without two next lines with error:
2330     // x86_64.ad(13987) Syntax Error: :In operand cmpOp_vcmppd: Do not support this encode constant: &#39; %{
2331     // equal&#39; for overflow.
2332     overflow     (0x20, &quot;o&quot;);  // not really supported by the instruction
2333     no_overflow  (0x21, &quot;no&quot;); // not really supported by the instruction
2334   %}
2335 %}
2336 
2337 
2338 // INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)
2339 
2340 // ============================================================================
2341 
2342 instruct ShouldNotReachHere() %{
2343   match(Halt);
2344   format %{ &quot;stop\t# ShouldNotReachHere&quot; %}
2345   ins_encode %{
2346     if (is_reachable()) {
2347       __ stop(_halt_reason);
2348     }
2349   %}
2350   ins_pipe(pipe_slow);
2351 %}
2352 
2353 // =================================EVEX special===============================
2354 
2355 instruct setMask(rRegI dst, rRegI src) %{
2356   predicate(Matcher::has_predicated_vectors());
2357   match(Set dst (SetVectMaskI  src));
2358   effect(TEMP dst);
2359   format %{ &quot;setvectmask   $dst, $src&quot; %}
2360   ins_encode %{
2361     __ setvectmask($dst$$Register, $src$$Register);
2362   %}
2363   ins_pipe(pipe_slow);
2364 %}
2365 
2366 // ============================================================================
2367 
2368 instruct addF_reg(regF dst, regF src) %{
2369   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2370   match(Set dst (AddF dst src));
2371 
2372   format %{ &quot;addss   $dst, $src&quot; %}
2373   ins_cost(150);
2374   ins_encode %{
2375     __ addss($dst$$XMMRegister, $src$$XMMRegister);
2376   %}
2377   ins_pipe(pipe_slow);
2378 %}
2379 
2380 instruct addF_mem(regF dst, memory src) %{
2381   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2382   match(Set dst (AddF dst (LoadF src)));
2383 
2384   format %{ &quot;addss   $dst, $src&quot; %}
2385   ins_cost(150);
2386   ins_encode %{
2387     __ addss($dst$$XMMRegister, $src$$Address);
2388   %}
2389   ins_pipe(pipe_slow);
2390 %}
2391 
2392 instruct addF_imm(regF dst, immF con) %{
2393   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2394   match(Set dst (AddF dst con));
2395   format %{ &quot;addss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2396   ins_cost(150);
2397   ins_encode %{
2398     __ addss($dst$$XMMRegister, $constantaddress($con));
2399   %}
2400   ins_pipe(pipe_slow);
2401 %}
2402 
2403 instruct addF_reg_reg(regF dst, regF src1, regF src2) %{
2404   predicate(UseAVX &gt; 0);
2405   match(Set dst (AddF src1 src2));
2406 
2407   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2408   ins_cost(150);
2409   ins_encode %{
2410     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2411   %}
2412   ins_pipe(pipe_slow);
2413 %}
2414 
2415 instruct addF_reg_mem(regF dst, regF src1, memory src2) %{
2416   predicate(UseAVX &gt; 0);
2417   match(Set dst (AddF src1 (LoadF src2)));
2418 
2419   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2420   ins_cost(150);
2421   ins_encode %{
2422     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2423   %}
2424   ins_pipe(pipe_slow);
2425 %}
2426 
2427 instruct addF_reg_imm(regF dst, regF src, immF con) %{
2428   predicate(UseAVX &gt; 0);
2429   match(Set dst (AddF src con));
2430 
2431   format %{ &quot;vaddss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2432   ins_cost(150);
2433   ins_encode %{
2434     __ vaddss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2435   %}
2436   ins_pipe(pipe_slow);
2437 %}
2438 
2439 instruct addD_reg(regD dst, regD src) %{
2440   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2441   match(Set dst (AddD dst src));
2442 
2443   format %{ &quot;addsd   $dst, $src&quot; %}
2444   ins_cost(150);
2445   ins_encode %{
2446     __ addsd($dst$$XMMRegister, $src$$XMMRegister);
2447   %}
2448   ins_pipe(pipe_slow);
2449 %}
2450 
2451 instruct addD_mem(regD dst, memory src) %{
2452   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2453   match(Set dst (AddD dst (LoadD src)));
2454 
2455   format %{ &quot;addsd   $dst, $src&quot; %}
2456   ins_cost(150);
2457   ins_encode %{
2458     __ addsd($dst$$XMMRegister, $src$$Address);
2459   %}
2460   ins_pipe(pipe_slow);
2461 %}
2462 
2463 instruct addD_imm(regD dst, immD con) %{
2464   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2465   match(Set dst (AddD dst con));
2466   format %{ &quot;addsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2467   ins_cost(150);
2468   ins_encode %{
2469     __ addsd($dst$$XMMRegister, $constantaddress($con));
2470   %}
2471   ins_pipe(pipe_slow);
2472 %}
2473 
2474 instruct addD_reg_reg(regD dst, regD src1, regD src2) %{
2475   predicate(UseAVX &gt; 0);
2476   match(Set dst (AddD src1 src2));
2477 
2478   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2479   ins_cost(150);
2480   ins_encode %{
2481     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2482   %}
2483   ins_pipe(pipe_slow);
2484 %}
2485 
2486 instruct addD_reg_mem(regD dst, regD src1, memory src2) %{
2487   predicate(UseAVX &gt; 0);
2488   match(Set dst (AddD src1 (LoadD src2)));
2489 
2490   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2491   ins_cost(150);
2492   ins_encode %{
2493     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2494   %}
2495   ins_pipe(pipe_slow);
2496 %}
2497 
2498 instruct addD_reg_imm(regD dst, regD src, immD con) %{
2499   predicate(UseAVX &gt; 0);
2500   match(Set dst (AddD src con));
2501 
2502   format %{ &quot;vaddsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2503   ins_cost(150);
2504   ins_encode %{
2505     __ vaddsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2506   %}
2507   ins_pipe(pipe_slow);
2508 %}
2509 
2510 instruct subF_reg(regF dst, regF src) %{
2511   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2512   match(Set dst (SubF dst src));
2513 
2514   format %{ &quot;subss   $dst, $src&quot; %}
2515   ins_cost(150);
2516   ins_encode %{
2517     __ subss($dst$$XMMRegister, $src$$XMMRegister);
2518   %}
2519   ins_pipe(pipe_slow);
2520 %}
2521 
2522 instruct subF_mem(regF dst, memory src) %{
2523   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2524   match(Set dst (SubF dst (LoadF src)));
2525 
2526   format %{ &quot;subss   $dst, $src&quot; %}
2527   ins_cost(150);
2528   ins_encode %{
2529     __ subss($dst$$XMMRegister, $src$$Address);
2530   %}
2531   ins_pipe(pipe_slow);
2532 %}
2533 
2534 instruct subF_imm(regF dst, immF con) %{
2535   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2536   match(Set dst (SubF dst con));
2537   format %{ &quot;subss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2538   ins_cost(150);
2539   ins_encode %{
2540     __ subss($dst$$XMMRegister, $constantaddress($con));
2541   %}
2542   ins_pipe(pipe_slow);
2543 %}
2544 
2545 instruct subF_reg_reg(regF dst, regF src1, regF src2) %{
2546   predicate(UseAVX &gt; 0);
2547   match(Set dst (SubF src1 src2));
2548 
2549   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2550   ins_cost(150);
2551   ins_encode %{
2552     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2553   %}
2554   ins_pipe(pipe_slow);
2555 %}
2556 
2557 instruct subF_reg_mem(regF dst, regF src1, memory src2) %{
2558   predicate(UseAVX &gt; 0);
2559   match(Set dst (SubF src1 (LoadF src2)));
2560 
2561   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2562   ins_cost(150);
2563   ins_encode %{
2564     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2565   %}
2566   ins_pipe(pipe_slow);
2567 %}
2568 
2569 instruct subF_reg_imm(regF dst, regF src, immF con) %{
2570   predicate(UseAVX &gt; 0);
2571   match(Set dst (SubF src con));
2572 
2573   format %{ &quot;vsubss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2574   ins_cost(150);
2575   ins_encode %{
2576     __ vsubss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2577   %}
2578   ins_pipe(pipe_slow);
2579 %}
2580 
2581 instruct subD_reg(regD dst, regD src) %{
2582   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2583   match(Set dst (SubD dst src));
2584 
2585   format %{ &quot;subsd   $dst, $src&quot; %}
2586   ins_cost(150);
2587   ins_encode %{
2588     __ subsd($dst$$XMMRegister, $src$$XMMRegister);
2589   %}
2590   ins_pipe(pipe_slow);
2591 %}
2592 
2593 instruct subD_mem(regD dst, memory src) %{
2594   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2595   match(Set dst (SubD dst (LoadD src)));
2596 
2597   format %{ &quot;subsd   $dst, $src&quot; %}
2598   ins_cost(150);
2599   ins_encode %{
2600     __ subsd($dst$$XMMRegister, $src$$Address);
2601   %}
2602   ins_pipe(pipe_slow);
2603 %}
2604 
2605 instruct subD_imm(regD dst, immD con) %{
2606   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2607   match(Set dst (SubD dst con));
2608   format %{ &quot;subsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2609   ins_cost(150);
2610   ins_encode %{
2611     __ subsd($dst$$XMMRegister, $constantaddress($con));
2612   %}
2613   ins_pipe(pipe_slow);
2614 %}
2615 
2616 instruct subD_reg_reg(regD dst, regD src1, regD src2) %{
2617   predicate(UseAVX &gt; 0);
2618   match(Set dst (SubD src1 src2));
2619 
2620   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2621   ins_cost(150);
2622   ins_encode %{
2623     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2624   %}
2625   ins_pipe(pipe_slow);
2626 %}
2627 
2628 instruct subD_reg_mem(regD dst, regD src1, memory src2) %{
2629   predicate(UseAVX &gt; 0);
2630   match(Set dst (SubD src1 (LoadD src2)));
2631 
2632   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2633   ins_cost(150);
2634   ins_encode %{
2635     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2636   %}
2637   ins_pipe(pipe_slow);
2638 %}
2639 
2640 instruct subD_reg_imm(regD dst, regD src, immD con) %{
2641   predicate(UseAVX &gt; 0);
2642   match(Set dst (SubD src con));
2643 
2644   format %{ &quot;vsubsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2645   ins_cost(150);
2646   ins_encode %{
2647     __ vsubsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2648   %}
2649   ins_pipe(pipe_slow);
2650 %}
2651 
2652 instruct mulF_reg(regF dst, regF src) %{
2653   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2654   match(Set dst (MulF dst src));
2655 
2656   format %{ &quot;mulss   $dst, $src&quot; %}
2657   ins_cost(150);
2658   ins_encode %{
2659     __ mulss($dst$$XMMRegister, $src$$XMMRegister);
2660   %}
2661   ins_pipe(pipe_slow);
2662 %}
2663 
2664 instruct mulF_mem(regF dst, memory src) %{
2665   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2666   match(Set dst (MulF dst (LoadF src)));
2667 
2668   format %{ &quot;mulss   $dst, $src&quot; %}
2669   ins_cost(150);
2670   ins_encode %{
2671     __ mulss($dst$$XMMRegister, $src$$Address);
2672   %}
2673   ins_pipe(pipe_slow);
2674 %}
2675 
2676 instruct mulF_imm(regF dst, immF con) %{
2677   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2678   match(Set dst (MulF dst con));
2679   format %{ &quot;mulss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2680   ins_cost(150);
2681   ins_encode %{
2682     __ mulss($dst$$XMMRegister, $constantaddress($con));
2683   %}
2684   ins_pipe(pipe_slow);
2685 %}
2686 
2687 instruct mulF_reg_reg(regF dst, regF src1, regF src2) %{
2688   predicate(UseAVX &gt; 0);
2689   match(Set dst (MulF src1 src2));
2690 
2691   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2692   ins_cost(150);
2693   ins_encode %{
2694     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2695   %}
2696   ins_pipe(pipe_slow);
2697 %}
2698 
2699 instruct mulF_reg_mem(regF dst, regF src1, memory src2) %{
2700   predicate(UseAVX &gt; 0);
2701   match(Set dst (MulF src1 (LoadF src2)));
2702 
2703   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2704   ins_cost(150);
2705   ins_encode %{
2706     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2707   %}
2708   ins_pipe(pipe_slow);
2709 %}
2710 
2711 instruct mulF_reg_imm(regF dst, regF src, immF con) %{
2712   predicate(UseAVX &gt; 0);
2713   match(Set dst (MulF src con));
2714 
2715   format %{ &quot;vmulss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2716   ins_cost(150);
2717   ins_encode %{
2718     __ vmulss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2719   %}
2720   ins_pipe(pipe_slow);
2721 %}
2722 
2723 instruct mulD_reg(regD dst, regD src) %{
2724   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2725   match(Set dst (MulD dst src));
2726 
2727   format %{ &quot;mulsd   $dst, $src&quot; %}
2728   ins_cost(150);
2729   ins_encode %{
2730     __ mulsd($dst$$XMMRegister, $src$$XMMRegister);
2731   %}
2732   ins_pipe(pipe_slow);
2733 %}
2734 
2735 instruct mulD_mem(regD dst, memory src) %{
2736   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2737   match(Set dst (MulD dst (LoadD src)));
2738 
2739   format %{ &quot;mulsd   $dst, $src&quot; %}
2740   ins_cost(150);
2741   ins_encode %{
2742     __ mulsd($dst$$XMMRegister, $src$$Address);
2743   %}
2744   ins_pipe(pipe_slow);
2745 %}
2746 
2747 instruct mulD_imm(regD dst, immD con) %{
2748   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2749   match(Set dst (MulD dst con));
2750   format %{ &quot;mulsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2751   ins_cost(150);
2752   ins_encode %{
2753     __ mulsd($dst$$XMMRegister, $constantaddress($con));
2754   %}
2755   ins_pipe(pipe_slow);
2756 %}
2757 
2758 instruct mulD_reg_reg(regD dst, regD src1, regD src2) %{
2759   predicate(UseAVX &gt; 0);
2760   match(Set dst (MulD src1 src2));
2761 
2762   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2763   ins_cost(150);
2764   ins_encode %{
2765     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2766   %}
2767   ins_pipe(pipe_slow);
2768 %}
2769 
2770 instruct mulD_reg_mem(regD dst, regD src1, memory src2) %{
2771   predicate(UseAVX &gt; 0);
2772   match(Set dst (MulD src1 (LoadD src2)));
2773 
2774   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2775   ins_cost(150);
2776   ins_encode %{
2777     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2778   %}
2779   ins_pipe(pipe_slow);
2780 %}
2781 
2782 instruct mulD_reg_imm(regD dst, regD src, immD con) %{
2783   predicate(UseAVX &gt; 0);
2784   match(Set dst (MulD src con));
2785 
2786   format %{ &quot;vmulsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2787   ins_cost(150);
2788   ins_encode %{
2789     __ vmulsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2790   %}
2791   ins_pipe(pipe_slow);
2792 %}
2793 
2794 instruct divF_reg(regF dst, regF src) %{
2795   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2796   match(Set dst (DivF dst src));
2797 
2798   format %{ &quot;divss   $dst, $src&quot; %}
2799   ins_cost(150);
2800   ins_encode %{
2801     __ divss($dst$$XMMRegister, $src$$XMMRegister);
2802   %}
2803   ins_pipe(pipe_slow);
2804 %}
2805 
2806 instruct divF_mem(regF dst, memory src) %{
2807   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2808   match(Set dst (DivF dst (LoadF src)));
2809 
2810   format %{ &quot;divss   $dst, $src&quot; %}
2811   ins_cost(150);
2812   ins_encode %{
2813     __ divss($dst$$XMMRegister, $src$$Address);
2814   %}
2815   ins_pipe(pipe_slow);
2816 %}
2817 
2818 instruct divF_imm(regF dst, immF con) %{
2819   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2820   match(Set dst (DivF dst con));
2821   format %{ &quot;divss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2822   ins_cost(150);
2823   ins_encode %{
2824     __ divss($dst$$XMMRegister, $constantaddress($con));
2825   %}
2826   ins_pipe(pipe_slow);
2827 %}
2828 
2829 instruct divF_reg_reg(regF dst, regF src1, regF src2) %{
2830   predicate(UseAVX &gt; 0);
2831   match(Set dst (DivF src1 src2));
2832 
2833   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2834   ins_cost(150);
2835   ins_encode %{
2836     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2837   %}
2838   ins_pipe(pipe_slow);
2839 %}
2840 
2841 instruct divF_reg_mem(regF dst, regF src1, memory src2) %{
2842   predicate(UseAVX &gt; 0);
2843   match(Set dst (DivF src1 (LoadF src2)));
2844 
2845   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2846   ins_cost(150);
2847   ins_encode %{
2848     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2849   %}
2850   ins_pipe(pipe_slow);
2851 %}
2852 
2853 instruct divF_reg_imm(regF dst, regF src, immF con) %{
2854   predicate(UseAVX &gt; 0);
2855   match(Set dst (DivF src con));
2856 
2857   format %{ &quot;vdivss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2858   ins_cost(150);
2859   ins_encode %{
2860     __ vdivss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2861   %}
2862   ins_pipe(pipe_slow);
2863 %}
2864 
2865 instruct divD_reg(regD dst, regD src) %{
2866   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2867   match(Set dst (DivD dst src));
2868 
2869   format %{ &quot;divsd   $dst, $src&quot; %}
2870   ins_cost(150);
2871   ins_encode %{
2872     __ divsd($dst$$XMMRegister, $src$$XMMRegister);
2873   %}
2874   ins_pipe(pipe_slow);
2875 %}
2876 
2877 instruct divD_mem(regD dst, memory src) %{
2878   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2879   match(Set dst (DivD dst (LoadD src)));
2880 
2881   format %{ &quot;divsd   $dst, $src&quot; %}
2882   ins_cost(150);
2883   ins_encode %{
2884     __ divsd($dst$$XMMRegister, $src$$Address);
2885   %}
2886   ins_pipe(pipe_slow);
2887 %}
2888 
2889 instruct divD_imm(regD dst, immD con) %{
2890   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2891   match(Set dst (DivD dst con));
2892   format %{ &quot;divsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2893   ins_cost(150);
2894   ins_encode %{
2895     __ divsd($dst$$XMMRegister, $constantaddress($con));
2896   %}
2897   ins_pipe(pipe_slow);
2898 %}
2899 
2900 instruct divD_reg_reg(regD dst, regD src1, regD src2) %{
2901   predicate(UseAVX &gt; 0);
2902   match(Set dst (DivD src1 src2));
2903 
2904   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2905   ins_cost(150);
2906   ins_encode %{
2907     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2908   %}
2909   ins_pipe(pipe_slow);
2910 %}
2911 
2912 instruct divD_reg_mem(regD dst, regD src1, memory src2) %{
2913   predicate(UseAVX &gt; 0);
2914   match(Set dst (DivD src1 (LoadD src2)));
2915 
2916   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2917   ins_cost(150);
2918   ins_encode %{
2919     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2920   %}
2921   ins_pipe(pipe_slow);
2922 %}
2923 
2924 instruct divD_reg_imm(regD dst, regD src, immD con) %{
2925   predicate(UseAVX &gt; 0);
2926   match(Set dst (DivD src con));
2927 
2928   format %{ &quot;vdivsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2929   ins_cost(150);
2930   ins_encode %{
2931     __ vdivsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2932   %}
2933   ins_pipe(pipe_slow);
2934 %}
2935 
2936 instruct absF_reg(regF dst) %{
2937   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2938   match(Set dst (AbsF dst));
2939   ins_cost(150);
2940   format %{ &quot;andps   $dst, [0x7fffffff]\t# abs float by sign masking&quot; %}
2941   ins_encode %{
2942     __ andps($dst$$XMMRegister, ExternalAddress(float_signmask()));
2943   %}
2944   ins_pipe(pipe_slow);
2945 %}
2946 
2947 instruct absF_reg_reg(vlRegF dst, vlRegF src) %{
2948   predicate(UseAVX &gt; 0);
2949   match(Set dst (AbsF src));
2950   ins_cost(150);
2951   format %{ &quot;vandps  $dst, $src, [0x7fffffff]\t# abs float by sign masking&quot; %}
2952   ins_encode %{
2953     int vector_len = 0;
2954     __ vandps($dst$$XMMRegister, $src$$XMMRegister,
2955               ExternalAddress(float_signmask()), vector_len);
2956   %}
2957   ins_pipe(pipe_slow);
2958 %}
2959 
2960 instruct absD_reg(regD dst) %{
2961   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2962   match(Set dst (AbsD dst));
2963   ins_cost(150);
2964   format %{ &quot;andpd   $dst, [0x7fffffffffffffff]\t&quot;
2965             &quot;# abs double by sign masking&quot; %}
2966   ins_encode %{
2967     __ andpd($dst$$XMMRegister, ExternalAddress(double_signmask()));
2968   %}
2969   ins_pipe(pipe_slow);
2970 %}
2971 
2972 instruct absD_reg_reg(vlRegD dst, vlRegD src) %{
2973   predicate(UseAVX &gt; 0);
2974   match(Set dst (AbsD src));
2975   ins_cost(150);
2976   format %{ &quot;vandpd  $dst, $src, [0x7fffffffffffffff]\t&quot;
2977             &quot;# abs double by sign masking&quot; %}
2978   ins_encode %{
2979     int vector_len = 0;
2980     __ vandpd($dst$$XMMRegister, $src$$XMMRegister,
2981               ExternalAddress(double_signmask()), vector_len);
2982   %}
2983   ins_pipe(pipe_slow);
2984 %}
2985 
2986 instruct negF_reg(regF dst) %{
2987   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2988   match(Set dst (NegF dst));
2989   ins_cost(150);
2990   format %{ &quot;xorps   $dst, [0x80000000]\t# neg float by sign flipping&quot; %}
2991   ins_encode %{
2992     __ xorps($dst$$XMMRegister, ExternalAddress(float_signflip()));
2993   %}
2994   ins_pipe(pipe_slow);
2995 %}
2996 
2997 instruct negF_reg_reg(vlRegF dst, vlRegF src) %{
2998   predicate(UseAVX &gt; 0);
2999   match(Set dst (NegF src));
3000   ins_cost(150);
3001   format %{ &quot;vnegatess  $dst, $src, [0x80000000]\t# neg float by sign flipping&quot; %}
3002   ins_encode %{
3003     __ vnegatess($dst$$XMMRegister, $src$$XMMRegister,
3004                  ExternalAddress(float_signflip()));
3005   %}
3006   ins_pipe(pipe_slow);
3007 %}
3008 
3009 instruct negD_reg(regD dst) %{
3010   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
3011   match(Set dst (NegD dst));
3012   ins_cost(150);
3013   format %{ &quot;xorpd   $dst, [0x8000000000000000]\t&quot;
3014             &quot;# neg double by sign flipping&quot; %}
3015   ins_encode %{
3016     __ xorpd($dst$$XMMRegister, ExternalAddress(double_signflip()));
3017   %}
3018   ins_pipe(pipe_slow);
3019 %}
3020 
3021 instruct negD_reg_reg(vlRegD dst, vlRegD src) %{
3022   predicate(UseAVX &gt; 0);
3023   match(Set dst (NegD src));
3024   ins_cost(150);
3025   format %{ &quot;vnegatesd  $dst, $src, [0x8000000000000000]\t&quot;
3026             &quot;# neg double by sign flipping&quot; %}
3027   ins_encode %{
3028     __ vnegatesd($dst$$XMMRegister, $src$$XMMRegister,
3029                  ExternalAddress(double_signflip()));
3030   %}
3031   ins_pipe(pipe_slow);
3032 %}
3033 
3034 instruct sqrtF_reg(regF dst, regF src) %{
3035   predicate(UseSSE&gt;=1);
3036   match(Set dst (SqrtF src));
3037 
3038   format %{ &quot;sqrtss  $dst, $src&quot; %}
3039   ins_cost(150);
3040   ins_encode %{
3041     __ sqrtss($dst$$XMMRegister, $src$$XMMRegister);
3042   %}
3043   ins_pipe(pipe_slow);
3044 %}
3045 
3046 instruct sqrtF_mem(regF dst, memory src) %{
3047   predicate(UseSSE&gt;=1);
3048   match(Set dst (SqrtF (LoadF src)));
3049 
3050   format %{ &quot;sqrtss  $dst, $src&quot; %}
3051   ins_cost(150);
3052   ins_encode %{
3053     __ sqrtss($dst$$XMMRegister, $src$$Address);
3054   %}
3055   ins_pipe(pipe_slow);
3056 %}
3057 
3058 instruct sqrtF_imm(regF dst, immF con) %{
3059   predicate(UseSSE&gt;=1);
3060   match(Set dst (SqrtF con));
3061 
3062   format %{ &quot;sqrtss  $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
3063   ins_cost(150);
3064   ins_encode %{
3065     __ sqrtss($dst$$XMMRegister, $constantaddress($con));
3066   %}
3067   ins_pipe(pipe_slow);
3068 %}
3069 
3070 instruct sqrtD_reg(regD dst, regD src) %{
3071   predicate(UseSSE&gt;=2);
3072   match(Set dst (SqrtD src));
3073 
3074   format %{ &quot;sqrtsd  $dst, $src&quot; %}
3075   ins_cost(150);
3076   ins_encode %{
3077     __ sqrtsd($dst$$XMMRegister, $src$$XMMRegister);
3078   %}
3079   ins_pipe(pipe_slow);
3080 %}
3081 
3082 instruct sqrtD_mem(regD dst, memory src) %{
3083   predicate(UseSSE&gt;=2);
3084   match(Set dst (SqrtD (LoadD src)));
3085 
3086   format %{ &quot;sqrtsd  $dst, $src&quot; %}
3087   ins_cost(150);
3088   ins_encode %{
3089     __ sqrtsd($dst$$XMMRegister, $src$$Address);
3090   %}
3091   ins_pipe(pipe_slow);
3092 %}
3093 
3094 instruct sqrtD_imm(regD dst, immD con) %{
3095   predicate(UseSSE&gt;=2);
3096   match(Set dst (SqrtD con));
3097   format %{ &quot;sqrtsd  $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
3098   ins_cost(150);
3099   ins_encode %{
3100     __ sqrtsd($dst$$XMMRegister, $constantaddress($con));
3101   %}
3102   ins_pipe(pipe_slow);
3103 %}
3104 
3105 
3106 #ifdef _LP64
3107 instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
3108   match(Set dst (RoundDoubleMode src rmode));
3109   format %{ &quot;roundsd $dst,$src&quot; %}
3110   ins_cost(150);
3111   ins_encode %{
3112     assert(UseSSE &gt;= 4, &quot;required&quot;);
3113     __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
3114   %}
3115   ins_pipe(pipe_slow);
3116 %}
3117 
3118 instruct roundD_mem(legRegD dst, memory src, immU8 rmode) %{
3119   match(Set dst (RoundDoubleMode (LoadD src) rmode));
3120   format %{ &quot;roundsd $dst,$src&quot; %}
3121   ins_cost(150);
3122   ins_encode %{
3123     assert(UseSSE &gt;= 4, &quot;required&quot;);
3124     __ roundsd($dst$$XMMRegister, $src$$Address, $rmode$$constant);
3125   %}
3126   ins_pipe(pipe_slow);
3127 %}
3128 
3129 instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{
3130   match(Set dst (RoundDoubleMode con rmode));
3131   effect(TEMP scratch_reg);
3132   format %{ &quot;roundsd $dst,[$constantaddress]\t# load from constant table: double=$con&quot; %}
3133   ins_cost(150);
3134   ins_encode %{
3135     assert(UseSSE &gt;= 4, &quot;required&quot;);
3136     __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);
3137   %}
3138   ins_pipe(pipe_slow);
3139 %}
3140 
3141 instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
3142   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3143   match(Set dst (RoundDoubleModeV src rmode));
3144   format %{ &quot;vroundpd $dst,$src,$rmode\t! round packedD&quot; %}
3145   ins_encode %{
3146     assert(UseAVX &gt; 0, &quot;required&quot;);
3147     int vector_len = vector_length_encoding(this);
3148     __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vector_len);
3149   %}
3150   ins_pipe( pipe_slow );
3151 %}
3152 
3153 instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
3154   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3155   match(Set dst (RoundDoubleModeV src rmode));
3156   format %{ &quot;vrndscalepd $dst,$src,$rmode\t! round packed8D&quot; %}
3157   ins_encode %{
3158     assert(UseAVX &gt; 2, &quot;required&quot;);
3159     __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
3160   %}
3161   ins_pipe( pipe_slow );
3162 %}
3163 
3164 instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
3165   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3166   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3167   format %{ &quot;vroundpd $dst, $mem, $rmode\t! round packedD&quot; %}
3168   ins_encode %{
3169     assert(UseAVX &gt; 0, &quot;required&quot;);
3170     int vector_len = vector_length_encoding(this);
3171     __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vector_len);
3172   %}
3173   ins_pipe( pipe_slow );
3174 %}
3175 
3176 instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
3177   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3178   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3179   format %{ &quot;vrndscalepd $dst,$mem,$rmode\t! round packed8D&quot; %}
3180   ins_encode %{
3181     assert(UseAVX &gt; 2, &quot;required&quot;);
3182     __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
3183   %}
3184   ins_pipe( pipe_slow );
3185 %}
3186 #endif // _LP64
3187 
3188 instruct onspinwait() %{
3189   match(OnSpinWait);
3190   ins_cost(200);
3191 
3192   format %{
3193     $$template
3194     $$emit$$&quot;pause\t! membar_onspinwait&quot;
3195   %}
3196   ins_encode %{
3197     __ pause();
3198   %}
3199   ins_pipe(pipe_slow);
3200 %}
3201 
3202 // a * b + c
3203 instruct fmaD_reg(regD a, regD b, regD c) %{
3204   predicate(UseFMA);
3205   match(Set c (FmaD  c (Binary a b)));
3206   format %{ &quot;fmasd $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3207   ins_cost(150);
3208   ins_encode %{
3209     __ fmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3210   %}
3211   ins_pipe( pipe_slow );
3212 %}
3213 
3214 // a * b + c
3215 instruct fmaF_reg(regF a, regF b, regF c) %{
3216   predicate(UseFMA);
3217   match(Set c (FmaF  c (Binary a b)));
3218   format %{ &quot;fmass $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3219   ins_cost(150);
3220   ins_encode %{
3221     __ fmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3222   %}
3223   ins_pipe( pipe_slow );
3224 %}
3225 
3226 // ====================VECTOR INSTRUCTIONS=====================================
3227 
3228 // Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
3229 instruct MoveVec2Leg(legVec dst, vec src) %{
3230   match(Set dst src);
3231   format %{ &quot;&quot; %}
3232   ins_encode %{
3233     ShouldNotReachHere();
3234   %}
3235   ins_pipe( fpu_reg_reg );
3236 %}
3237 
3238 instruct MoveLeg2Vec(vec dst, legVec src) %{
3239   match(Set dst src);
3240   format %{ &quot;&quot; %}
3241   ins_encode %{
3242     ShouldNotReachHere();
3243   %}
3244   ins_pipe( fpu_reg_reg );
3245 %}
3246 
3247 // ============================================================================
3248 
3249 // Load vectors
3250 instruct loadV(vec dst, memory mem) %{
3251   match(Set dst (LoadVector mem));
3252   ins_cost(125);
3253   format %{ &quot;load_vector $dst,$mem&quot; %}
3254   ins_encode %{
3255     switch (vector_length_in_bytes(this)) {
3256       case  4: __ movdl    ($dst$$XMMRegister, $mem$$Address); break;
3257       case  8: __ movq     ($dst$$XMMRegister, $mem$$Address); break;
3258       case 16: __ movdqu   ($dst$$XMMRegister, $mem$$Address); break;
3259       case 32: __ vmovdqu  ($dst$$XMMRegister, $mem$$Address); break;
3260       case 64: __ evmovdqul($dst$$XMMRegister, $mem$$Address, Assembler::AVX_512bit); break;
3261       default: ShouldNotReachHere();
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
3267 // Store vectors generic operand pattern.
3268 instruct storeV(memory mem, vec src) %{
3269   match(Set mem (StoreVector mem src));
3270   ins_cost(145);
3271   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3272   ins_encode %{
3273     switch (vector_length_in_bytes(this, $src)) {
3274       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3275       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3276       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3277       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3278       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3279       default: ShouldNotReachHere();
3280     }
3281   %}
3282   ins_pipe( pipe_slow );
3283 %}
3284 
3285 // ====================REPLICATE=======================================
3286 
3287 // Replicate byte scalar to be vector
3288 instruct ReplB_reg(vec dst, rRegI src) %{
3289   match(Set dst (ReplicateB src));
3290   format %{ &quot;replicateB $dst,$src&quot; %}
3291   ins_encode %{
3292     uint vlen = vector_length(this);
3293     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3294       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW
3295       int vlen_enc = vector_length_encoding(this);
3296       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3297     } else {
3298       __ movdl($dst$$XMMRegister, $src$$Register);
3299       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3300       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3301       if (vlen &gt;= 16) {
3302         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3303         if (vlen &gt;= 32) {
3304           assert(vlen == 32, &quot;sanity&quot;);
3305           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3306         }
3307       }
3308     }
3309   %}
3310   ins_pipe( pipe_slow );
3311 %}
3312 
3313 instruct ReplB_mem(vec dst, memory mem) %{
3314   predicate(VM_Version::supports_avx2());
3315   match(Set dst (ReplicateB (LoadB mem)));
3316   format %{ &quot;replicateB $dst,$mem&quot; %}
3317   ins_encode %{
3318     int vector_len = vector_length_encoding(this);
3319     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3320   %}
3321   ins_pipe( pipe_slow );
3322 %}
3323 
3324 instruct ReplB_imm(vec dst, immI con) %{
3325   match(Set dst (ReplicateB con));
3326   format %{ &quot;replicateB $dst,$con&quot; %}
3327   ins_encode %{
3328     uint vlen = vector_length(this);
3329     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3330     if (vlen == 4) {
3331       __ movdl($dst$$XMMRegister, const_addr);
3332     } else {
3333       __ movq($dst$$XMMRegister, const_addr);
3334       if (vlen &gt;= 16) {
3335         if (VM_Version::supports_avx2()) {
3336           int vlen_enc = vector_length_encoding(this);
3337           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3338         } else {
3339           assert(vlen == 16, &quot;sanity&quot;);
3340           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3341         }
3342       }
3343     }
3344   %}
3345   ins_pipe( pipe_slow );
3346 %}
3347 
3348 // Replicate byte scalar zero to be vector
3349 instruct ReplB_zero(vec dst, immI0 zero) %{
3350   match(Set dst (ReplicateB zero));
3351   format %{ &quot;replicateB $dst,$zero&quot; %}
3352   ins_encode %{
3353     uint vlen = vector_length(this);
3354     if (vlen &lt;= 16) {
3355       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3356     } else {
3357       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3358       int vlen_enc = vector_length_encoding(this);
3359       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3360     }
3361   %}
3362   ins_pipe( fpu_reg_reg );
3363 %}
3364 
3365 // ====================ReplicateS=======================================
3366 
3367 instruct ReplS_reg(vec dst, rRegI src) %{
3368   match(Set dst (ReplicateS src));
3369   format %{ &quot;replicateS $dst,$src&quot; %}
3370   ins_encode %{
3371     uint vlen = vector_length(this);
3372     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3373       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW
3374       int vlen_enc = vector_length_encoding(this);
3375       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3376     } else {
3377       __ movdl($dst$$XMMRegister, $src$$Register);
3378       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3379       if (vlen &gt;= 8) {
3380         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3381         if (vlen &gt;= 16) {
3382           assert(vlen == 16, &quot;sanity&quot;);
3383           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3384         }
3385       }
3386     }
3387   %}
3388   ins_pipe( pipe_slow );
3389 %}
3390 
3391 instruct ReplS_mem(vec dst, memory mem) %{
3392   predicate(VM_Version::supports_avx2());
3393   match(Set dst (ReplicateS (LoadS mem)));
3394   format %{ &quot;replicateS $dst,$mem&quot; %}
3395   ins_encode %{
3396     int vlen_enc = vector_length_encoding(this);
3397     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
3398   %}
3399   ins_pipe( pipe_slow );
3400 %}
3401 
3402 instruct ReplS_imm(vec dst, immI con) %{
3403   match(Set dst (ReplicateS con));
3404   format %{ &quot;replicateS $dst,$con&quot; %}
3405   ins_encode %{
3406     uint vlen = vector_length(this);
3407     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));
3408     if (vlen == 2) {
3409       __ movdl($dst$$XMMRegister, const_addr);
3410     } else {
3411       __ movq($dst$$XMMRegister, const_addr);
3412       if (vlen &gt;= 8) {
3413         if (VM_Version::supports_avx2()) {
3414           int vlen_enc = vector_length_encoding(this);
3415           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3416         } else {
3417           assert(vlen == 8, &quot;sanity&quot;);
3418           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3419         }
3420       }
3421     }
3422   %}
3423   ins_pipe( fpu_reg_reg );
3424 %}
3425 
3426 instruct ReplS_zero(vec dst, immI0 zero) %{
3427   match(Set dst (ReplicateS zero));
3428   format %{ &quot;replicateS $dst,$zero&quot; %}
3429   ins_encode %{
3430     uint vlen = vector_length(this);
3431     if (vlen &lt;= 8) {
3432       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3433     } else {
3434       int vlen_enc = vector_length_encoding(this);
3435       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3436     }
3437   %}
3438   ins_pipe( fpu_reg_reg );
3439 %}
3440 
3441 // ====================ReplicateI=======================================
3442 
3443 instruct ReplI_reg(vec dst, rRegI src) %{
3444   match(Set dst (ReplicateI src));
3445   format %{ &quot;replicateI $dst,$src&quot; %}
3446   ins_encode %{
3447     uint vlen = vector_length(this);
3448     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3449       int vlen_enc = vector_length_encoding(this);
3450       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3451     } else {
3452       __ movdl($dst$$XMMRegister, $src$$Register);
3453       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3454       if (vlen &gt;= 8) {
3455         assert(vlen == 8, &quot;sanity&quot;);
3456         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3457       }
3458     }
3459   %}
3460   ins_pipe( pipe_slow );
3461 %}
3462 
3463 instruct ReplI_mem(vec dst, memory mem) %{
3464   match(Set dst (ReplicateI (LoadI mem)));
3465   format %{ &quot;replicateI $dst,$mem&quot; %}
3466   ins_encode %{
3467     uint vlen = vector_length(this);
3468     if (vlen &lt;= 4) {
3469       __ movdl($dst$$XMMRegister, $mem$$Address);
3470       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3471     } else {
3472       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3473       int vector_len = vector_length_encoding(this);
3474       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
3475     }
3476   %}
3477   ins_pipe( pipe_slow );
3478 %}
3479 
3480 instruct ReplI_imm(vec dst, immI con) %{
3481   match(Set dst (ReplicateI con));
3482   format %{ &quot;replicateI $dst,$con&quot; %}
3483   ins_encode %{
3484     uint vlen = vector_length(this);
3485     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));
3486     if (vlen &lt;= 4) {
3487       __ movq($dst$$XMMRegister, const_addr);
3488       if (vlen == 4) {
3489         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3490       }
3491     } else {
3492       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3493       int vector_len = vector_length_encoding(this);
3494       __ movq($dst$$XMMRegister, const_addr);
3495       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3496     }
3497   %}
3498   ins_pipe( pipe_slow );
3499 %}
3500 
3501 // Replicate integer (4 byte) scalar zero to be vector
3502 instruct ReplI_zero(vec dst, immI0 zero) %{
3503   match(Set dst (ReplicateI zero));
3504   format %{ &quot;replicateI $dst,$zero&quot; %}
3505   ins_encode %{
3506     uint vlen = vector_length(this);
3507     if (vlen &lt;= 4) {
3508       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3509     } else {
3510       int vlen_enc = vector_length_encoding(this);
3511       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3512     }
3513   %}
3514   ins_pipe( fpu_reg_reg );
3515 %}
3516 
3517 instruct ReplI_M1(vec dst, immI_M1 con) %{
3518   predicate(UseAVX &gt; 0);
3519   match(Set dst (ReplicateB con));
3520   match(Set dst (ReplicateS con));
3521   match(Set dst (ReplicateI con));
3522   effect(TEMP dst);
3523   format %{ &quot;vallones $dst&quot; %}
3524   ins_encode %{
3525     int vector_len = vector_length_encoding(this);
3526     __ vallones($dst$$XMMRegister, vector_len);
3527   %}
3528   ins_pipe( pipe_slow );
3529 %}
3530 
3531 // ====================ReplicateL=======================================
3532 
3533 #ifdef _LP64
3534 // Replicate long (8 byte) scalar to be vector
3535 instruct ReplL_reg(vec dst, rRegL src) %{
3536   match(Set dst (ReplicateL src));
3537   format %{ &quot;replicateL $dst,$src&quot; %}
3538   ins_encode %{
3539     uint vlen = vector_length(this);
3540     if (vlen == 2) {
3541       __ movdq($dst$$XMMRegister, $src$$Register);
3542       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3543     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3544       int vlen_enc = vector_length_encoding(this);
3545       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3546     } else {
3547       assert(vlen == 4, &quot;sanity&quot;);
3548       __ movdq($dst$$XMMRegister, $src$$Register);
3549       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3550       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3551     }
3552   %}
3553   ins_pipe( pipe_slow );
3554 %}
3555 #else // _LP64
3556 // Replicate long (8 byte) scalar to be vector
3557 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3558   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3559   match(Set dst (ReplicateL src));
3560   effect(TEMP dst, USE src, TEMP tmp);
3561   format %{ &quot;replicateL $dst,$src&quot; %}
3562   ins_encode %{
3563     uint vlen = vector_length(this);
3564     if (vlen == 2) {
3565       __ movdl($dst$$XMMRegister, $src$$Register);
3566       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3567       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3568       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3569     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3570       int vector_len = Assembler::AVX_256bit;
3571       __ movdl($dst$$XMMRegister, $src$$Register);
3572       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3573       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3574       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3575     } else {
3576       __ movdl($dst$$XMMRegister, $src$$Register);
3577       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3578       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3579       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3580       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3581     }
3582   %}
3583   ins_pipe( pipe_slow );
3584 %}
3585 
3586 instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{
3587   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3588   match(Set dst (ReplicateL src));
3589   effect(TEMP dst, USE src, TEMP tmp);
3590   format %{ &quot;replicateL $dst,$src&quot; %}
3591   ins_encode %{
3592     if (VM_Version::supports_avx512vl()) {
3593       __ movdl($dst$$XMMRegister, $src$$Register);
3594       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3595       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3596       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3597       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3598       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3599     } else {
3600       int vector_len = Assembler::AVX_512bit;
3601       __ movdl($dst$$XMMRegister, $src$$Register);
3602       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3603       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3604       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3605     }
3606   %}
3607   ins_pipe( pipe_slow );
3608 %}
3609 #endif // _LP64
3610 
3611 instruct ReplL_mem(vec dst, memory mem) %{
3612   match(Set dst (ReplicateL (LoadL mem)));
3613   format %{ &quot;replicateL $dst,$mem&quot; %}
3614   ins_encode %{
3615     uint vlen = vector_length(this);
3616     if (vlen == 2) {
3617       __ movq($dst$$XMMRegister, $mem$$Address);
3618       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3619     } else {
3620       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3621       int vlen_enc = vector_length_encoding(this);
3622       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
3623     }
3624   %}
3625   ins_pipe( pipe_slow );
3626 %}
3627 
3628 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3629 instruct ReplL_imm(vec dst, immL con) %{
3630   match(Set dst (ReplicateL con));
3631   format %{ &quot;replicateL $dst,$con&quot; %}
3632   ins_encode %{
3633     uint vlen = vector_length(this);
3634     InternalAddress const_addr = $constantaddress($con);
3635     if (vlen == 2) {
3636       __ movq($dst$$XMMRegister, const_addr);
3637       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3638     } else {
3639       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);
3640       int vlen_enc = vector_length_encoding(this);
3641       __ movq($dst$$XMMRegister, const_addr);
3642       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3643     }
3644   %}
3645   ins_pipe( pipe_slow );
3646 %}
3647 
3648 instruct ReplL_zero(vec dst, immL0 zero) %{
3649   match(Set dst (ReplicateL zero));
3650   format %{ &quot;replicateL $dst,$zero&quot; %}
3651   ins_encode %{
3652     int vlen = vector_length(this);
3653     if (vlen == 2) {
3654       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3655     } else {
3656       int vlen_enc = vector_length_encoding(this);
3657       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3658     }
3659   %}
3660   ins_pipe( fpu_reg_reg );
3661 %}
3662 
3663 instruct ReplL_M1(vec dst, immL_M1 con) %{
3664   predicate(UseAVX &gt; 0);
3665   match(Set dst (ReplicateL con));
3666   effect(TEMP dst);
3667   format %{ &quot;vallones $dst&quot; %}
3668   ins_encode %{
3669     int vector_len = vector_length_encoding(this);
3670     __ vallones($dst$$XMMRegister, vector_len);
3671   %}
3672   ins_pipe( pipe_slow );
3673 %}
3674 
3675 // ====================ReplicateF=======================================
3676 
3677 instruct ReplF_reg(vec dst, vlRegF src) %{
3678   match(Set dst (ReplicateF src));
3679   format %{ &quot;replicateF $dst,$src&quot; %}
3680   ins_encode %{
3681     uint vlen = vector_length(this);
3682     if (vlen &lt;= 4) {
3683       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3684    } else if (VM_Version::supports_avx2()) {
3685       int vector_len = vector_length_encoding(this);
3686       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
3687     } else {
3688       assert(vlen == 8, &quot;sanity&quot;);
3689       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3690       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3691     }
3692   %}
3693   ins_pipe( pipe_slow );
3694 %}
3695 
3696 instruct ReplF_mem(vec dst, memory mem) %{
3697   match(Set dst (ReplicateF (LoadF mem)));
3698   format %{ &quot;replicateF $dst,$mem&quot; %}
3699   ins_encode %{
3700     uint vlen = vector_length(this);
3701     if (vlen &lt;= 4) {
3702       __ movdl($dst$$XMMRegister, $mem$$Address);
3703       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3704     } else {
3705       assert(VM_Version::supports_avx(), &quot;sanity&quot;);
3706       int vector_len = vector_length_encoding(this);
3707       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
3708     }
3709   %}
3710   ins_pipe( pipe_slow );
3711 %}
3712 
3713 instruct ReplF_zero(vec dst, immF0 zero) %{
3714   match(Set dst (ReplicateF zero));
3715   format %{ &quot;replicateF $dst,$zero&quot; %}
3716   ins_encode %{
3717     uint vlen = vector_length(this);
3718     if (vlen &lt;= 4) {
3719       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3720     } else {
3721       int vlen_enc = vector_length_encoding(this);
3722       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3723     }
3724   %}
3725   ins_pipe( fpu_reg_reg );
3726 %}
3727 
3728 // ====================ReplicateD=======================================
3729 
3730 // Replicate double (8 bytes) scalar to be vector
3731 instruct ReplD_reg(vec dst, vlRegD src) %{
3732   match(Set dst (ReplicateD src));
3733   format %{ &quot;replicateD $dst,$src&quot; %}
3734   ins_encode %{
3735     uint vlen = vector_length(this);
3736     if (vlen == 2) {
3737       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3738     } else if (VM_Version::supports_avx2()) {
3739       int vector_len = vector_length_encoding(this);
3740       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
3741     } else {
3742       assert(vlen == 4, &quot;sanity&quot;);
3743       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3744       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3745     }
3746   %}
3747   ins_pipe( pipe_slow );
3748 %}
3749 
3750 instruct ReplD_mem(vec dst, memory mem) %{
3751   match(Set dst (ReplicateD (LoadD mem)));
3752   format %{ &quot;replicateD $dst,$mem&quot; %}
3753   ins_encode %{
3754     uint vlen = vector_length(this);
3755     if (vlen == 2) {
3756       __ movq($dst$$XMMRegister, $mem$$Address);
3757       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);
3758     } else {
3759       assert(VM_Version::supports_avx(), &quot;sanity&quot;);
3760       int vector_len = vector_length_encoding(this);
3761       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
3762     }
3763   %}
3764   ins_pipe( pipe_slow );
3765 %}
3766 
3767 instruct ReplD_zero(vec dst, immD0 zero) %{
3768   match(Set dst (ReplicateD zero));
3769   format %{ &quot;replicateD $dst,$zero&quot; %}
3770   ins_encode %{
3771     uint vlen = vector_length(this);
3772     if (vlen == 2) {
3773       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3774     } else {
3775       int vlen_enc = vector_length_encoding(this);
3776       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3777     }
3778   %}
3779   ins_pipe( fpu_reg_reg );
3780 %}
3781 
3782 // ====================REDUCTION ARITHMETIC=======================================
3783 // =======================Int Reduction==========================================
3784 
3785 instruct reductionI(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{
3786   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;
3787             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 16);
3788   match(Set dst (AddReductionVI src1 src2));
3789   match(Set dst (MulReductionVI src1 src2));
3790   match(Set dst (AndReductionV  src1 src2));
3791   match(Set dst ( OrReductionV  src1 src2));
3792   match(Set dst (XorReductionV  src1 src2));
3793   effect(TEMP vtmp1, TEMP vtmp2);
3794   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3795   ins_encode %{
3796     int opcode = this-&gt;ideal_Opcode();
3797     int vlen = vector_length(this, $src2);
3798     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3799   %}
3800   ins_pipe( pipe_slow );
3801 %}
3802 
3803 instruct reduction16I(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
3804   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;
3805             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);
3806   match(Set dst (AddReductionVI src1 src2));
3807   match(Set dst (MulReductionVI src1 src2));
3808   match(Set dst (AndReductionV  src1 src2));
3809   match(Set dst ( OrReductionV  src1 src2));
3810   match(Set dst (XorReductionV  src1 src2));
3811   effect(TEMP vtmp1, TEMP vtmp2);
3812   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3813   ins_encode %{
3814     int opcode = this-&gt;ideal_Opcode();
3815     int vlen = vector_length(this, $src2);
3816     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3817   %}
3818   ins_pipe( pipe_slow );
3819 %}
3820 
3821 // =======================Long Reduction==========================================
3822 
3823 #ifdef _LP64
3824 instruct reductionL(rRegL dst, rRegL src1, vec src2, vec vtmp1, vec vtmp2) %{
3825   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;
3826             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 8);
3827   match(Set dst (AddReductionVL src1 src2));
3828   match(Set dst (MulReductionVL src1 src2));
3829   match(Set dst (AndReductionV  src1 src2));
3830   match(Set dst ( OrReductionV  src1 src2));
3831   match(Set dst (XorReductionV  src1 src2));
3832   effect(TEMP vtmp1, TEMP vtmp2);
3833   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3834   ins_encode %{
3835     int opcode = this-&gt;ideal_Opcode();
3836     int vlen = vector_length(this, $src2);
3837     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3838   %}
3839   ins_pipe( pipe_slow );
3840 %}
3841 
3842 instruct reduction8L(rRegL dst, rRegL src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
3843   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;
3844             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);
3845   match(Set dst (AddReductionVL src1 src2));
3846   match(Set dst (MulReductionVL src1 src2));
3847   match(Set dst (AndReductionV  src1 src2));
3848   match(Set dst ( OrReductionV  src1 src2));
3849   match(Set dst (XorReductionV  src1 src2));
3850   effect(TEMP vtmp1, TEMP vtmp2);
3851   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3852   ins_encode %{
3853     int opcode = this-&gt;ideal_Opcode();
3854     int vlen = vector_length(this, $src2);
3855     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3856   %}
3857   ins_pipe( pipe_slow );
3858 %}
3859 #endif // _LP64
3860 
3861 // =======================Float Reduction==========================================
3862 
3863 instruct reductionF128(regF dst, vec src, vec vtmp) %{
3864   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt;= 4);
3865   match(Set dst (AddReductionVF dst src));
3866   match(Set dst (MulReductionVF dst src));
3867   effect(TEMP dst, TEMP vtmp);
3868   format %{ &quot;vector_reduction_fp  $dst,$src ; using $vtmp as TEMP&quot; %}
3869   ins_encode %{
3870     int opcode = this-&gt;ideal_Opcode();
3871     int vlen = vector_length(this, $src);
3872     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
3873   %}
3874   ins_pipe( pipe_slow );
3875 %}
3876 
3877 instruct reduction8F(regF dst, vec src, vec vtmp1, vec vtmp2) %{
3878   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);
3879   match(Set dst (AddReductionVF dst src));
3880   match(Set dst (MulReductionVF dst src));
3881   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3882   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3883   ins_encode %{
3884     int opcode = this-&gt;ideal_Opcode();
3885     int vlen = vector_length(this, $src);
3886     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3887   %}
3888   ins_pipe( pipe_slow );
3889 %}
3890 
3891 instruct reduction16F(regF dst, legVec src, legVec vtmp1, legVec vtmp2) %{
3892   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);
3893   match(Set dst (AddReductionVF dst src));
3894   match(Set dst (MulReductionVF dst src));
3895   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3896   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3897   ins_encode %{
3898     int opcode = this-&gt;ideal_Opcode();
3899     int vlen = vector_length(this, $src);
3900     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3901   %}
3902   ins_pipe( pipe_slow );
3903 %}
3904 
3905 // =======================Double Reduction==========================================
3906 
3907 instruct reduction2D(regD dst, vec src, vec vtmp) %{
3908   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2);
3909   match(Set dst (AddReductionVD dst src));
3910   match(Set dst (MulReductionVD dst src));
3911   effect(TEMP dst, TEMP vtmp);
3912   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp as TEMP&quot; %}
3913   ins_encode %{
3914     int opcode = this-&gt;ideal_Opcode();
3915     int vlen = vector_length(this, $src);
3916     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
3917   %}
3918   ins_pipe( pipe_slow );
3919 %}
3920 
3921 instruct reduction4D(regD dst, vec src, vec vtmp1, vec vtmp2) %{
3922   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4);
3923   match(Set dst (AddReductionVD dst src));
3924   match(Set dst (MulReductionVD dst src));
3925   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3926   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3927   ins_encode %{
3928     int opcode = this-&gt;ideal_Opcode();
3929     int vlen = vector_length(this, $src);
3930     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3931   %}
3932   ins_pipe( pipe_slow );
3933 %}
3934 
3935 instruct reduction8D(regD dst, legVec src, legVec vtmp1, legVec vtmp2) %{
3936   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);
3937   match(Set dst (AddReductionVD dst src));
3938   match(Set dst (MulReductionVD dst src));
3939   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
3940   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}
3941   ins_encode %{
3942     int opcode = this-&gt;ideal_Opcode();
3943     int vlen = vector_length(this, $src);
3944     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
3945   %}
3946   ins_pipe( pipe_slow );
3947 %}
3948 
3949 // ====================VECTOR ARITHMETIC=======================================
3950 
3951 // --------------------------------- ADD --------------------------------------
3952 
3953 // Bytes vector add
3954 instruct vaddB(vec dst, vec src) %{
3955   predicate(UseAVX == 0);
3956   match(Set dst (AddVB dst src));
3957   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
3958   ins_encode %{
3959     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
3960   %}
3961   ins_pipe( pipe_slow );
3962 %}
3963 
3964 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
3965   predicate(UseAVX &gt; 0);
3966   match(Set dst (AddVB src1 src2));
3967   format %{ &quot;vpaddb  $dst,$src1,$src2\t! add packedB&quot; %}
3968   ins_encode %{
3969     int vector_len = vector_length_encoding(this);
3970     __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
3971   %}
3972   ins_pipe( pipe_slow );
3973 %}
3974 
3975 instruct vaddB_mem(vec dst, vec src, memory mem) %{
3976   predicate(UseAVX &gt; 0);
3977   match(Set dst (AddVB src (LoadVector mem)));
3978   format %{ &quot;vpaddb  $dst,$src,$mem\t! add packedB&quot; %}
3979   ins_encode %{
3980     int vector_len = vector_length_encoding(this);
3981     __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
3982   %}
3983   ins_pipe( pipe_slow );
3984 %}
3985 
3986 // Shorts/Chars vector add
3987 instruct vaddS(vec dst, vec src) %{
3988   predicate(UseAVX == 0);
3989   match(Set dst (AddVS dst src));
3990   format %{ &quot;paddw   $dst,$src\t! add packedS&quot; %}
3991   ins_encode %{
3992     __ paddw($dst$$XMMRegister, $src$$XMMRegister);
3993   %}
3994   ins_pipe( pipe_slow );
3995 %}
3996 
3997 instruct vaddS_reg(vec dst, vec src1, vec src2) %{
3998   predicate(UseAVX &gt; 0);
3999   match(Set dst (AddVS src1 src2));
4000   format %{ &quot;vpaddw  $dst,$src1,$src2\t! add packedS&quot; %}
4001   ins_encode %{
4002     int vector_len = vector_length_encoding(this);
4003     __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4004   %}
4005   ins_pipe( pipe_slow );
4006 %}
4007 
4008 instruct vaddS_mem(vec dst, vec src, memory mem) %{
4009   predicate(UseAVX &gt; 0);
4010   match(Set dst (AddVS src (LoadVector mem)));
4011   format %{ &quot;vpaddw  $dst,$src,$mem\t! add packedS&quot; %}
4012   ins_encode %{
4013     int vector_len = vector_length_encoding(this);
4014     __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4015   %}
4016   ins_pipe( pipe_slow );
4017 %}
4018 
4019 // Integers vector add
4020 instruct vaddI(vec dst, vec src) %{
4021   predicate(UseAVX == 0);
4022   match(Set dst (AddVI dst src));
4023   format %{ &quot;paddd   $dst,$src\t! add packedI&quot; %}
4024   ins_encode %{
4025     __ paddd($dst$$XMMRegister, $src$$XMMRegister);
4026   %}
4027   ins_pipe( pipe_slow );
4028 %}
4029 
4030 instruct vaddI_reg(vec dst, vec src1, vec src2) %{
4031   predicate(UseAVX &gt; 0);
4032   match(Set dst (AddVI src1 src2));
4033   format %{ &quot;vpaddd  $dst,$src1,$src2\t! add packedI&quot; %}
4034   ins_encode %{
4035     int vector_len = vector_length_encoding(this);
4036     __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4037   %}
4038   ins_pipe( pipe_slow );
4039 %}
4040 
4041 
4042 instruct vaddI_mem(vec dst, vec src, memory mem) %{
4043   predicate(UseAVX &gt; 0);
4044   match(Set dst (AddVI src (LoadVector mem)));
4045   format %{ &quot;vpaddd  $dst,$src,$mem\t! add packedI&quot; %}
4046   ins_encode %{
4047     int vector_len = vector_length_encoding(this);
4048     __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4049   %}
4050   ins_pipe( pipe_slow );
4051 %}
4052 
4053 // Longs vector add
4054 instruct vaddL(vec dst, vec src) %{
4055   predicate(UseAVX == 0);
4056   match(Set dst (AddVL dst src));
4057   format %{ &quot;paddq   $dst,$src\t! add packedL&quot; %}
4058   ins_encode %{
4059     __ paddq($dst$$XMMRegister, $src$$XMMRegister);
4060   %}
4061   ins_pipe( pipe_slow );
4062 %}
4063 
4064 instruct vaddL_reg(vec dst, vec src1, vec src2) %{
4065   predicate(UseAVX &gt; 0);
4066   match(Set dst (AddVL src1 src2));
4067   format %{ &quot;vpaddq  $dst,$src1,$src2\t! add packedL&quot; %}
4068   ins_encode %{
4069     int vector_len = vector_length_encoding(this);
4070     __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4071   %}
4072   ins_pipe( pipe_slow );
4073 %}
4074 
4075 instruct vaddL_mem(vec dst, vec src, memory mem) %{
4076   predicate(UseAVX &gt; 0);
4077   match(Set dst (AddVL src (LoadVector mem)));
4078   format %{ &quot;vpaddq  $dst,$src,$mem\t! add packedL&quot; %}
4079   ins_encode %{
4080     int vector_len = vector_length_encoding(this);
4081     __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4082   %}
4083   ins_pipe( pipe_slow );
4084 %}
4085 
4086 // Floats vector add
4087 instruct vaddF(vec dst, vec src) %{
4088   predicate(UseAVX == 0);
4089   match(Set dst (AddVF dst src));
4090   format %{ &quot;addps   $dst,$src\t! add packedF&quot; %}
4091   ins_encode %{
4092     __ addps($dst$$XMMRegister, $src$$XMMRegister);
4093   %}
4094   ins_pipe( pipe_slow );
4095 %}
4096 
4097 instruct vaddF_reg(vec dst, vec src1, vec src2) %{
4098   predicate(UseAVX &gt; 0);
4099   match(Set dst (AddVF src1 src2));
4100   format %{ &quot;vaddps  $dst,$src1,$src2\t! add packedF&quot; %}
4101   ins_encode %{
4102     int vector_len = vector_length_encoding(this);
4103     __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4104   %}
4105   ins_pipe( pipe_slow );
4106 %}
4107 
4108 instruct vaddF_mem(vec dst, vec src, memory mem) %{
4109   predicate(UseAVX &gt; 0);
4110   match(Set dst (AddVF src (LoadVector mem)));
4111   format %{ &quot;vaddps  $dst,$src,$mem\t! add packedF&quot; %}
4112   ins_encode %{
4113     int vector_len = vector_length_encoding(this);
4114     __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4115   %}
4116   ins_pipe( pipe_slow );
4117 %}
4118 
4119 // Doubles vector add
4120 instruct vaddD(vec dst, vec src) %{
4121   predicate(UseAVX == 0);
4122   match(Set dst (AddVD dst src));
4123   format %{ &quot;addpd   $dst,$src\t! add packedD&quot; %}
4124   ins_encode %{
4125     __ addpd($dst$$XMMRegister, $src$$XMMRegister);
4126   %}
4127   ins_pipe( pipe_slow );
4128 %}
4129 
4130 instruct vaddD_reg(vec dst, vec src1, vec src2) %{
4131   predicate(UseAVX &gt; 0);
4132   match(Set dst (AddVD src1 src2));
4133   format %{ &quot;vaddpd  $dst,$src1,$src2\t! add packedD&quot; %}
4134   ins_encode %{
4135     int vector_len = vector_length_encoding(this);
4136     __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4137   %}
4138   ins_pipe( pipe_slow );
4139 %}
4140 
4141 instruct vaddD_mem(vec dst, vec src, memory mem) %{
4142   predicate(UseAVX &gt; 0);
4143   match(Set dst (AddVD src (LoadVector mem)));
4144   format %{ &quot;vaddpd  $dst,$src,$mem\t! add packedD&quot; %}
4145   ins_encode %{
4146     int vector_len = vector_length_encoding(this);
4147     __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4148   %}
4149   ins_pipe( pipe_slow );
4150 %}
4151 
4152 // --------------------------------- SUB --------------------------------------
4153 
4154 // Bytes vector sub
4155 instruct vsubB(vec dst, vec src) %{
4156   predicate(UseAVX == 0);
4157   match(Set dst (SubVB dst src));
4158   format %{ &quot;psubb   $dst,$src\t! sub packedB&quot; %}
4159   ins_encode %{
4160     __ psubb($dst$$XMMRegister, $src$$XMMRegister);
4161   %}
4162   ins_pipe( pipe_slow );
4163 %}
4164 
4165 instruct vsubB_reg(vec dst, vec src1, vec src2) %{
4166   predicate(UseAVX &gt; 0);
4167   match(Set dst (SubVB src1 src2));
4168   format %{ &quot;vpsubb  $dst,$src1,$src2\t! sub packedB&quot; %}
4169   ins_encode %{
4170     int vector_len = vector_length_encoding(this);
4171     __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4172   %}
4173   ins_pipe( pipe_slow );
4174 %}
4175 
4176 instruct vsubB_mem(vec dst, vec src, memory mem) %{
4177   predicate(UseAVX &gt; 0);
4178   match(Set dst (SubVB src (LoadVector mem)));
4179   format %{ &quot;vpsubb  $dst,$src,$mem\t! sub packedB&quot; %}
4180   ins_encode %{
4181     int vector_len = vector_length_encoding(this);
4182     __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4183   %}
4184   ins_pipe( pipe_slow );
4185 %}
4186 
4187 // Shorts/Chars vector sub
4188 instruct vsubS(vec dst, vec src) %{
4189   predicate(UseAVX == 0);
4190   match(Set dst (SubVS dst src));
4191   format %{ &quot;psubw   $dst,$src\t! sub packedS&quot; %}
4192   ins_encode %{
4193     __ psubw($dst$$XMMRegister, $src$$XMMRegister);
4194   %}
4195   ins_pipe( pipe_slow );
4196 %}
4197 
4198 
4199 instruct vsubS_reg(vec dst, vec src1, vec src2) %{
4200   predicate(UseAVX &gt; 0);
4201   match(Set dst (SubVS src1 src2));
4202   format %{ &quot;vpsubw  $dst,$src1,$src2\t! sub packedS&quot; %}
4203   ins_encode %{
4204     int vector_len = vector_length_encoding(this);
4205     __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4206   %}
4207   ins_pipe( pipe_slow );
4208 %}
4209 
4210 instruct vsubS_mem(vec dst, vec src, memory mem) %{
4211   predicate(UseAVX &gt; 0);
4212   match(Set dst (SubVS src (LoadVector mem)));
4213   format %{ &quot;vpsubw  $dst,$src,$mem\t! sub packedS&quot; %}
4214   ins_encode %{
4215     int vector_len = vector_length_encoding(this);
4216     __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4217   %}
4218   ins_pipe( pipe_slow );
4219 %}
4220 
4221 // Integers vector sub
4222 instruct vsubI(vec dst, vec src) %{
4223   predicate(UseAVX == 0);
4224   match(Set dst (SubVI dst src));
4225   format %{ &quot;psubd   $dst,$src\t! sub packedI&quot; %}
4226   ins_encode %{
4227     __ psubd($dst$$XMMRegister, $src$$XMMRegister);
4228   %}
4229   ins_pipe( pipe_slow );
4230 %}
4231 
4232 instruct vsubI_reg(vec dst, vec src1, vec src2) %{
4233   predicate(UseAVX &gt; 0);
4234   match(Set dst (SubVI src1 src2));
4235   format %{ &quot;vpsubd  $dst,$src1,$src2\t! sub packedI&quot; %}
4236   ins_encode %{
4237     int vector_len = vector_length_encoding(this);
4238     __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4239   %}
4240   ins_pipe( pipe_slow );
4241 %}
4242 
4243 instruct vsubI_mem(vec dst, vec src, memory mem) %{
4244   predicate(UseAVX &gt; 0);
4245   match(Set dst (SubVI src (LoadVector mem)));
4246   format %{ &quot;vpsubd  $dst,$src,$mem\t! sub packedI&quot; %}
4247   ins_encode %{
4248     int vector_len = vector_length_encoding(this);
4249     __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4250   %}
4251   ins_pipe( pipe_slow );
4252 %}
4253 
4254 // Longs vector sub
4255 instruct vsubL(vec dst, vec src) %{
4256   predicate(UseAVX == 0);
4257   match(Set dst (SubVL dst src));
4258   format %{ &quot;psubq   $dst,$src\t! sub packedL&quot; %}
4259   ins_encode %{
4260     __ psubq($dst$$XMMRegister, $src$$XMMRegister);
4261   %}
4262   ins_pipe( pipe_slow );
4263 %}
4264 
4265 instruct vsubL_reg(vec dst, vec src1, vec src2) %{
4266   predicate(UseAVX &gt; 0);
4267   match(Set dst (SubVL src1 src2));
4268   format %{ &quot;vpsubq  $dst,$src1,$src2\t! sub packedL&quot; %}
4269   ins_encode %{
4270     int vector_len = vector_length_encoding(this);
4271     __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4272   %}
4273   ins_pipe( pipe_slow );
4274 %}
4275 
4276 
4277 instruct vsubL_mem(vec dst, vec src, memory mem) %{
4278   predicate(UseAVX &gt; 0);
4279   match(Set dst (SubVL src (LoadVector mem)));
4280   format %{ &quot;vpsubq  $dst,$src,$mem\t! sub packedL&quot; %}
4281   ins_encode %{
4282     int vector_len = vector_length_encoding(this);
4283     __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4284   %}
4285   ins_pipe( pipe_slow );
4286 %}
4287 
4288 // Floats vector sub
4289 instruct vsubF(vec dst, vec src) %{
4290   predicate(UseAVX == 0);
4291   match(Set dst (SubVF dst src));
4292   format %{ &quot;subps   $dst,$src\t! sub packedF&quot; %}
4293   ins_encode %{
4294     __ subps($dst$$XMMRegister, $src$$XMMRegister);
4295   %}
4296   ins_pipe( pipe_slow );
4297 %}
4298 
4299 instruct vsubF_reg(vec dst, vec src1, vec src2) %{
4300   predicate(UseAVX &gt; 0);
4301   match(Set dst (SubVF src1 src2));
4302   format %{ &quot;vsubps  $dst,$src1,$src2\t! sub packedF&quot; %}
4303   ins_encode %{
4304     int vector_len = vector_length_encoding(this);
4305     __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4306   %}
4307   ins_pipe( pipe_slow );
4308 %}
4309 
4310 instruct vsubF_mem(vec dst, vec src, memory mem) %{
4311   predicate(UseAVX &gt; 0);
4312   match(Set dst (SubVF src (LoadVector mem)));
4313   format %{ &quot;vsubps  $dst,$src,$mem\t! sub packedF&quot; %}
4314   ins_encode %{
4315     int vector_len = vector_length_encoding(this);
4316     __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4317   %}
4318   ins_pipe( pipe_slow );
4319 %}
4320 
4321 // Doubles vector sub
4322 instruct vsubD(vec dst, vec src) %{
4323   predicate(UseAVX == 0);
4324   match(Set dst (SubVD dst src));
4325   format %{ &quot;subpd   $dst,$src\t! sub packedD&quot; %}
4326   ins_encode %{
4327     __ subpd($dst$$XMMRegister, $src$$XMMRegister);
4328   %}
4329   ins_pipe( pipe_slow );
4330 %}
4331 
4332 instruct vsubD_reg(vec dst, vec src1, vec src2) %{
4333   predicate(UseAVX &gt; 0);
4334   match(Set dst (SubVD src1 src2));
4335   format %{ &quot;vsubpd  $dst,$src1,$src2\t! sub packedD&quot; %}
4336   ins_encode %{
4337     int vector_len = vector_length_encoding(this);
4338     __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4339   %}
4340   ins_pipe( pipe_slow );
4341 %}
4342 
4343 instruct vsubD_mem(vec dst, vec src, memory mem) %{
4344   predicate(UseAVX &gt; 0);
4345   match(Set dst (SubVD src (LoadVector mem)));
4346   format %{ &quot;vsubpd  $dst,$src,$mem\t! sub packedD&quot; %}
4347   ins_encode %{
4348     int vector_len = vector_length_encoding(this);
4349     __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4350   %}
4351   ins_pipe( pipe_slow );
4352 %}
4353 
4354 // --------------------------------- MUL --------------------------------------
4355 
4356 // Byte vector mul
4357 instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4358   predicate(n-&gt;as_Vector()-&gt;length() == 4 ||
4359             n-&gt;as_Vector()-&gt;length() == 8);
4360   match(Set dst (MulVB src1 src2));
4361   effect(TEMP dst, TEMP tmp, TEMP scratch);
4362   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4363   ins_encode %{
4364     assert(UseSSE &gt; 3, &quot;required&quot;);
4365     __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);
4366     __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);
4367     __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);
4368     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4369     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4370     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4371   %}
4372   ins_pipe( pipe_slow );
4373 %}
4374 
4375 instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4376   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4377   match(Set dst (MulVB src1 src2));
4378   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4379   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4380   ins_encode %{
4381     assert(UseSSE &gt; 3, &quot;required&quot;);
4382     __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);
4383     __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);
4384     __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);
4385     __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);
4386     __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);
4387     __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);
4388     __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);
4389     __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);
4390     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4391     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
4392     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
4393     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
4394   %}
4395   ins_pipe( pipe_slow );
4396 %}
4397 
4398 instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4399   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
4400   match(Set dst (MulVB src1 src2));
4401   effect(TEMP dst, TEMP tmp, TEMP scratch);
4402   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4403   ins_encode %{
4404   int vector_len = Assembler::AVX_256bit;
4405     __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vector_len);
4406     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4407     __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vector_len);
4408     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4409     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
4410     __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);
4411     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);
4412   %}
4413   ins_pipe( pipe_slow );
4414 %}
4415 
4416 instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4417   predicate(n-&gt;as_Vector()-&gt;length() == 32);
4418   match(Set dst (MulVB src1 src2));
4419   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4420   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4421   ins_encode %{
4422     assert(UseAVX &gt; 1, &quot;required&quot;);
4423     int vector_len = Assembler::AVX_256bit;
4424     __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4425     __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);
4426     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4427     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4428     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4429     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4430     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4431     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4432     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4433     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4434     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4435     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4436     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4437     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
4438   %}
4439   ins_pipe( pipe_slow );
4440 %}
4441 
4442 instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4443   predicate(n-&gt;as_Vector()-&gt;length() == 64);
4444   match(Set dst (MulVB src1 src2));
4445   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4446   format %{&quot;vector_mulB $dst,$src1,$src2\n\t&quot; %}
4447   ins_encode %{
4448     assert(UseAVX &gt; 2, &quot;required&quot;);
4449     int vector_len = Assembler::AVX_512bit;
4450     __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);
4451     __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);
4452     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4453     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4454     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4455     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
4456     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
4457     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4458     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4459     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4460     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4461     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4462     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4463     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
4464     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4465   %}
4466   ins_pipe( pipe_slow );
4467 %}
4468 
4469 // Shorts/Chars vector mul
4470 instruct vmulS(vec dst, vec src) %{
4471   predicate(UseAVX == 0);
4472   match(Set dst (MulVS dst src));
4473   format %{ &quot;pmullw $dst,$src\t! mul packedS&quot; %}
4474   ins_encode %{
4475     __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
4476   %}
4477   ins_pipe( pipe_slow );
4478 %}
4479 
4480 instruct vmulS_reg(vec dst, vec src1, vec src2) %{
4481   predicate(UseAVX &gt; 0);
4482   match(Set dst (MulVS src1 src2));
4483   format %{ &quot;vpmullw $dst,$src1,$src2\t! mul packedS&quot; %}
4484   ins_encode %{
4485     int vector_len = vector_length_encoding(this);
4486     __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4487   %}
4488   ins_pipe( pipe_slow );
4489 %}
4490 
4491 instruct vmulS_mem(vec dst, vec src, memory mem) %{
4492   predicate(UseAVX &gt; 0);
4493   match(Set dst (MulVS src (LoadVector mem)));
4494   format %{ &quot;vpmullw $dst,$src,$mem\t! mul packedS&quot; %}
4495   ins_encode %{
4496     int vector_len = vector_length_encoding(this);
4497     __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4498   %}
4499   ins_pipe( pipe_slow );
4500 %}
4501 
4502 // Integers vector mul
4503 instruct vmulI(vec dst, vec src) %{
4504   predicate(UseAVX == 0);
4505   match(Set dst (MulVI dst src));
4506   format %{ &quot;pmulld  $dst,$src\t! mul packedI&quot; %}
4507   ins_encode %{
4508     assert(UseSSE &gt; 3, &quot;required&quot;);
4509     __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
4510   %}
4511   ins_pipe( pipe_slow );
4512 %}
4513 
4514 instruct vmulI_reg(vec dst, vec src1, vec src2) %{
4515   predicate(UseAVX &gt; 0);
4516   match(Set dst (MulVI src1 src2));
4517   format %{ &quot;vpmulld $dst,$src1,$src2\t! mul packedI&quot; %}
4518   ins_encode %{
4519     int vector_len = vector_length_encoding(this);
4520     __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4521   %}
4522   ins_pipe( pipe_slow );
4523 %}
4524 
4525 instruct vmulI_mem(vec dst, vec src, memory mem) %{
4526   predicate(UseAVX &gt; 0);
4527   match(Set dst (MulVI src (LoadVector mem)));
4528   format %{ &quot;vpmulld $dst,$src,$mem\t! mul packedI&quot; %}
4529   ins_encode %{
4530     int vector_len = vector_length_encoding(this);
4531     __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4532   %}
4533   ins_pipe( pipe_slow );
4534 %}
4535 
4536 // Longs vector mul
4537 instruct vmulL_reg(vec dst, vec src1, vec src2) %{
4538   match(Set dst (MulVL src1 src2));
4539   format %{ &quot;vpmullq $dst,$src1,$src2\t! mul packedL&quot; %}
4540   ins_encode %{
4541     assert(UseAVX &gt; 2, &quot;required&quot;);
4542     int vector_len = vector_length_encoding(this);
4543     __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4544   %}
4545   ins_pipe( pipe_slow );
4546 %}
4547 
4548 instruct vmulL_mem(vec dst, vec src, memory mem) %{
4549   match(Set dst (MulVL src (LoadVector mem)));
4550   format %{ &quot;vpmullq $dst,$src,$mem\t! mul packedL&quot; %}
4551   ins_encode %{
4552     assert(UseAVX &gt; 2, &quot;required&quot;);
4553     int vector_len = vector_length_encoding(this);
4554     __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4555   %}
4556   ins_pipe( pipe_slow );
4557 %}
4558 
4559 // Floats vector mul
4560 instruct vmulF(vec dst, vec src) %{
4561   predicate(UseAVX == 0);
4562   match(Set dst (MulVF dst src));
4563   format %{ &quot;mulps   $dst,$src\t! mul packedF&quot; %}
4564   ins_encode %{
4565     __ mulps($dst$$XMMRegister, $src$$XMMRegister);
4566   %}
4567   ins_pipe( pipe_slow );
4568 %}
4569 
4570 instruct vmulF_reg(vec dst, vec src1, vec src2) %{
4571   predicate(UseAVX &gt; 0);
4572   match(Set dst (MulVF src1 src2));
4573   format %{ &quot;vmulps  $dst,$src1,$src2\t! mul packedF&quot; %}
4574   ins_encode %{
4575     int vector_len = vector_length_encoding(this);
4576     __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4577   %}
4578   ins_pipe( pipe_slow );
4579 %}
4580 
4581 instruct vmulF_mem(vec dst, vec src, memory mem) %{
4582   predicate(UseAVX &gt; 0);
4583   match(Set dst (MulVF src (LoadVector mem)));
4584   format %{ &quot;vmulps  $dst,$src,$mem\t! mul packedF&quot; %}
4585   ins_encode %{
4586     int vector_len = vector_length_encoding(this);
4587     __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4588   %}
4589   ins_pipe( pipe_slow );
4590 %}
4591 
4592 // Doubles vector mul
4593 instruct vmulD(vec dst, vec src) %{
4594   predicate(UseAVX == 0);
4595   match(Set dst (MulVD dst src));
4596   format %{ &quot;mulpd   $dst,$src\t! mul packedD&quot; %}
4597   ins_encode %{
4598     __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
4599   %}
4600   ins_pipe( pipe_slow );
4601 %}
4602 
4603 instruct vmulD_reg(vec dst, vec src1, vec src2) %{
4604   predicate(UseAVX &gt; 0);
4605   match(Set dst (MulVD src1 src2));
4606   format %{ &quot;vmulpd  $dst,$src1,$src2\t! mul packedD&quot; %}
4607   ins_encode %{
4608     int vector_len = vector_length_encoding(this);
4609     __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4610   %}
4611   ins_pipe( pipe_slow );
4612 %}
4613 
4614 instruct vmulD_mem(vec dst, vec src, memory mem) %{
4615   predicate(UseAVX &gt; 0);
4616   match(Set dst (MulVD src (LoadVector mem)));
4617   format %{ &quot;vmulpd  $dst,$src,$mem\t! mul packedD&quot; %}
4618   ins_encode %{
4619     int vector_len = vector_length_encoding(this);
4620     __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4621   %}
4622   ins_pipe( pipe_slow );
4623 %}
4624 
4625 instruct vcmov8F_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
4626   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 8);
4627   match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
4628   effect(TEMP dst, USE src1, USE src2);
4629   format %{ &quot;cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t&quot;
4630             &quot;blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t&quot;
4631          %}
4632   ins_encode %{
4633     int vector_len = 1;
4634     int cond = (Assembler::Condition)($copnd$$cmpcode);
4635     __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
4636     __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
4637   %}
4638   ins_pipe( pipe_slow );
4639 %}
4640 
4641 instruct vcmov4D_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
4642   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 4);
4643   match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
4644   effect(TEMP dst, USE src1, USE src2);
4645   format %{ &quot;cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t&quot;
4646             &quot;blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t&quot;
4647          %}
4648   ins_encode %{
4649     int vector_len = 1;
4650     int cond = (Assembler::Condition)($copnd$$cmpcode);
4651     __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
4652     __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
4653   %}
4654   ins_pipe( pipe_slow );
4655 %}
4656 
4657 // --------------------------------- DIV --------------------------------------
4658 
4659 // Floats vector div
4660 instruct vdivF(vec dst, vec src) %{
4661   predicate(UseAVX == 0);
4662   match(Set dst (DivVF dst src));
4663   format %{ &quot;divps   $dst,$src\t! div packedF&quot; %}
4664   ins_encode %{
4665     __ divps($dst$$XMMRegister, $src$$XMMRegister);
4666   %}
4667   ins_pipe( pipe_slow );
4668 %}
4669 
4670 instruct vdivF_reg(vec dst, vec src1, vec src2) %{
4671   predicate(UseAVX &gt; 0);
4672   match(Set dst (DivVF src1 src2));
4673   format %{ &quot;vdivps  $dst,$src1,$src2\t! div packedF&quot; %}
4674   ins_encode %{
4675     int vector_len = vector_length_encoding(this);
4676     __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4677   %}
4678   ins_pipe( pipe_slow );
4679 %}
4680 
4681 instruct vdivF_mem(vec dst, vec src, memory mem) %{
4682   predicate(UseAVX &gt; 0);
4683   match(Set dst (DivVF src (LoadVector mem)));
4684   format %{ &quot;vdivps  $dst,$src,$mem\t! div packedF&quot; %}
4685   ins_encode %{
4686     int vector_len = vector_length_encoding(this);
4687     __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4688   %}
4689   ins_pipe( pipe_slow );
4690 %}
4691 
4692 // Doubles vector div
4693 instruct vdivD(vec dst, vec src) %{
4694   predicate(UseAVX == 0);
4695   match(Set dst (DivVD dst src));
4696   format %{ &quot;divpd   $dst,$src\t! div packedD&quot; %}
4697   ins_encode %{
4698     __ divpd($dst$$XMMRegister, $src$$XMMRegister);
4699   %}
4700   ins_pipe( pipe_slow );
4701 %}
4702 
4703 instruct vdivD_reg(vec dst, vec src1, vec src2) %{
4704   predicate(UseAVX &gt; 0);
4705   match(Set dst (DivVD src1 src2));
4706   format %{ &quot;vdivpd  $dst,$src1,$src2\t! div packedD&quot; %}
4707   ins_encode %{
4708     int vector_len = vector_length_encoding(this);
4709     __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4710   %}
4711   ins_pipe( pipe_slow );
4712 %}
4713 
4714 instruct vdivD_mem(vec dst, vec src, memory mem) %{
4715   predicate(UseAVX &gt; 0);
4716   match(Set dst (DivVD src (LoadVector mem)));
4717   format %{ &quot;vdivpd  $dst,$src,$mem\t! div packedD&quot; %}
4718   ins_encode %{
4719     int vector_len = vector_length_encoding(this);
4720     __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4721   %}
4722   ins_pipe( pipe_slow );
4723 %}
4724 
4725 // --------------------------------- Sqrt --------------------------------------
4726 
4727 instruct vsqrtF_reg(vec dst, vec src) %{
4728   match(Set dst (SqrtVF src));
4729   format %{ &quot;vsqrtps  $dst,$src\t! sqrt packedF&quot; %}
4730   ins_encode %{
4731     assert(UseAVX &gt; 0, &quot;required&quot;);
4732     int vector_len = vector_length_encoding(this);
4733     __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
4734   %}
4735   ins_pipe( pipe_slow );
4736 %}
4737 
4738 instruct vsqrtF_mem(vec dst, memory mem) %{
4739   match(Set dst (SqrtVF (LoadVector mem)));
4740   format %{ &quot;vsqrtps  $dst,$mem\t! sqrt packedF&quot; %}
4741   ins_encode %{
4742     assert(UseAVX &gt; 0, &quot;required&quot;);
4743     int vector_len = vector_length_encoding(this);
4744     __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
4745   %}
4746   ins_pipe( pipe_slow );
4747 %}
4748 
4749 // Floating point vector sqrt
4750 instruct vsqrtD_reg(vec dst, vec src) %{
4751   match(Set dst (SqrtVD src));
4752   format %{ &quot;vsqrtpd  $dst,$src\t! sqrt packedD&quot; %}
4753   ins_encode %{
4754     assert(UseAVX &gt; 0, &quot;required&quot;);
4755     int vector_len = vector_length_encoding(this);
4756     __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
4757   %}
4758   ins_pipe( pipe_slow );
4759 %}
4760 
4761 instruct vsqrtD_mem(vec dst, memory mem) %{
4762   match(Set dst (SqrtVD (LoadVector mem)));
4763   format %{ &quot;vsqrtpd  $dst,$mem\t! sqrt packedD&quot; %}
4764   ins_encode %{
4765     assert(UseAVX &gt; 0, &quot;required&quot;);
4766     int vector_len = vector_length_encoding(this);
4767     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
4768   %}
4769   ins_pipe( pipe_slow );
4770 %}
4771 
4772 // ------------------------------ Shift ---------------------------------------
4773 
4774 // Left and right shift count vectors are the same on x86
4775 // (only lowest bits of xmm reg are used for count).
4776 instruct vshiftcnt(vec dst, rRegI cnt) %{
4777   match(Set dst (LShiftCntV cnt));
4778   match(Set dst (RShiftCntV cnt));
4779   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
4780   ins_encode %{
4781     __ movdl($dst$$XMMRegister, $cnt$$Register);
4782   %}
4783   ins_pipe( pipe_slow );
4784 %}
4785 
4786 // Byte vector shift
4787 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4788   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
4789   match(Set dst (LShiftVB src shift));
4790   match(Set dst (RShiftVB src shift));
4791   match(Set dst (URShiftVB src shift));
4792   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
4793   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4794   ins_encode %{
4795     assert(UseSSE &gt; 3, &quot;required&quot;);
4796     int opcode = this-&gt;ideal_Opcode();
4797     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
4798     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
4799     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4800     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4801     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4802   %}
4803   ins_pipe( pipe_slow );
4804 %}
4805 
4806 instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
4807   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4808   match(Set dst (LShiftVB src shift));
4809   match(Set dst (RShiftVB src shift));
4810   match(Set dst (URShiftVB src shift));
4811   effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);
4812   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4813   ins_encode %{
4814     assert(UseSSE &gt; 3, &quot;required&quot;);
4815     int opcode = this-&gt;ideal_Opcode();
4816 
4817     __ vextendbw(opcode, $tmp1$$XMMRegister, $src$$XMMRegister);
4818     __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
4819     __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
4820     __ vextendbw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
4821     __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
4822     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4823     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
4824     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
4825     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
4826   %}
4827   ins_pipe( pipe_slow );
4828 %}
4829 
4830 instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4831   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
4832   match(Set dst (LShiftVB src shift));
4833   match(Set dst (RShiftVB src shift));
4834   match(Set dst (URShiftVB src shift));
4835   effect(TEMP dst, TEMP tmp, TEMP scratch);
4836   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4837   ins_encode %{
4838     int opcode = this-&gt;ideal_Opcode();
4839     int vector_len = Assembler::AVX_256bit;
4840     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister, vector_len);
4841     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
4842     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
4843     __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
4844     __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
4845   %}
4846   ins_pipe( pipe_slow );
4847 %}
4848 
4849 instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4850   predicate(n-&gt;as_Vector()-&gt;length() == 32);
4851   match(Set dst (LShiftVB src shift));
4852   match(Set dst (RShiftVB src shift));
4853   match(Set dst (URShiftVB src shift));
4854   effect(TEMP dst, TEMP tmp, TEMP scratch);
4855   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4856   ins_encode %{
4857     assert(UseAVX &gt; 1, &quot;required&quot;);
4858     int opcode = this-&gt;ideal_Opcode();
4859     int vector_len = Assembler::AVX_256bit;
4860     __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
4861     __ vextendbw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
4862     __ vextendbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, vector_len);
4863     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
4864     __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vector_len);
4865     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
4866     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
4867     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
4868     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
4869   %}
4870   ins_pipe( pipe_slow );
4871 %}
4872 
4873 instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
4874   predicate(n-&gt;as_Vector()-&gt;length() == 64);
4875   match(Set dst (LShiftVB src shift));
4876   match(Set dst (RShiftVB src shift));
4877   match(Set dst (URShiftVB src shift));
4878   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4879   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
4880   ins_encode %{
4881     assert(UseAVX &gt; 2, &quot;required&quot;);
4882     int opcode = this-&gt;ideal_Opcode();
4883     int vector_len = Assembler::AVX_512bit;
4884     __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
4885     __ vextendbw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
4886     __ vextendbw(opcode, $tmp2$$XMMRegister, $src$$XMMRegister, vector_len);
4887     __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vector_len);
4888     __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vector_len);
4889     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4890     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
4891     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
4892     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4893     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4894     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
4895     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
4896   %}
4897   ins_pipe( pipe_slow );
4898 %}
4899 
4900 // Shorts vector logical right shift produces incorrect Java result
4901 // for negative data because java code convert short value into int with
4902 // sign extension before a shift. But char vectors are fine since chars are
4903 // unsigned values.
4904 // Shorts/Chars vector left shift
4905 instruct vshiftS(vec dst, vec src, vec shift) %{
4906   match(Set dst (LShiftVS src shift));
4907   match(Set dst (RShiftVS src shift));
4908   match(Set dst (URShiftVS src shift));
4909   effect(TEMP dst, USE src, USE shift);
4910   format %{ &quot;vshiftw  $dst,$src,$shift\t! shift packedS&quot; %}
4911   ins_encode %{
4912     int opcode = this-&gt;ideal_Opcode();
4913     if (UseAVX &gt; 0) {
4914       int vlen_enc = vector_length_encoding(this);
4915       __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
4916     } else {
4917       int vlen = vector_length(this);
4918       if (vlen == 2) {
4919         __ movflt($dst$$XMMRegister, $src$$XMMRegister);
4920         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4921       } else if (vlen == 4) {
4922         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
4923         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4924       } else {
4925         assert (vlen == 8, &quot;sanity&quot;);
4926         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4927         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4928       }
4929     }
4930   %}
4931   ins_pipe( pipe_slow );
4932 %}
4933 
4934 // Integers vector left shift
4935 instruct vshiftI(vec dst, vec src, vec shift) %{
4936   match(Set dst (LShiftVI src shift));
4937   match(Set dst (RShiftVI src shift));
4938   match(Set dst (URShiftVI src shift));
4939   effect(TEMP dst, USE src, USE shift);
4940   format %{ &quot;vshiftd  $dst,$src,$shift\t! shift packedI&quot; %}
4941   ins_encode %{
4942     int opcode = this-&gt;ideal_Opcode();
4943     if (UseAVX &gt; 0) {
4944       int vector_len = vector_length_encoding(this);
4945       __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
4946     } else {
4947       int vlen = vector_length(this);
4948       if (vlen == 2) {
4949         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
4950         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4951       } else {
4952         assert(vlen == 4, &quot;sanity&quot;);
4953         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4954         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4955       }
4956     }
4957   %}
4958   ins_pipe( pipe_slow );
4959 %}
4960 
4961 // Longs vector shift
4962 instruct vshiftL(vec dst, vec src, vec shift) %{
4963   match(Set dst (LShiftVL src shift));
4964   match(Set dst (URShiftVL src shift));
4965   effect(TEMP dst, USE src, USE shift);
4966   format %{ &quot;vshiftq  $dst,$src,$shift\t! shift packedL&quot; %}
4967   ins_encode %{
4968     int opcode = this-&gt;ideal_Opcode();
4969     if (UseAVX &gt; 0) {
4970       int vector_len = vector_length_encoding(this);
4971       __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
4972     } else {
4973       assert(vector_length(this) == 2, &quot;&quot;);
4974       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4975       __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
4976     }
4977   %}
4978   ins_pipe( pipe_slow );
4979 %}
4980 
4981 // -------------------ArithmeticRightShift -----------------------------------
4982 // Long vector arithmetic right shift
4983 instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
4984   predicate(UseAVX &lt;= 2);
4985   match(Set dst (RShiftVL src shift));
4986   effect(TEMP dst, TEMP tmp, TEMP scratch);
4987   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
4988   ins_encode %{
4989     uint vlen = vector_length(this);
4990     if (vlen == 2) {
4991       assert(UseSSE &gt;= 2, &quot;required&quot;);
4992       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
4993       __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
4994       __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
4995       __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
4996       __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
4997       __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
4998     } else {
4999       assert(vlen == 4, &quot;sanity&quot;);
5000       assert(UseAVX &gt; 1, &quot;required&quot;);
5001       int vector_len = Assembler::AVX_256bit;
5002       __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5003       __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5004       __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5005       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5006       __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5007     }
5008   %}
5009   ins_pipe( pipe_slow );
5010 %}
5011 
5012 instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
5013   predicate(UseAVX &gt; 2);
5014   match(Set dst (RShiftVL src shift));
5015   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5016   ins_encode %{
5017     int vector_len = vector_length_encoding(this);
5018     __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5019   %}
5020   ins_pipe( pipe_slow );
5021 %}
5022 
5023 // --------------------------------- AND --------------------------------------
5024 
5025 instruct vand(vec dst, vec src) %{
5026   predicate(UseAVX == 0);
5027   match(Set dst (AndV dst src));
5028   format %{ &quot;pand    $dst,$src\t! and vectors&quot; %}
5029   ins_encode %{
5030     __ pand($dst$$XMMRegister, $src$$XMMRegister);
5031   %}
5032   ins_pipe( pipe_slow );
5033 %}
5034 
5035 instruct vand_reg(vec dst, vec src1, vec src2) %{
5036   predicate(UseAVX &gt; 0);
5037   match(Set dst (AndV src1 src2));
5038   format %{ &quot;vpand   $dst,$src1,$src2\t! and vectors&quot; %}
5039   ins_encode %{
5040     int vector_len = vector_length_encoding(this);
5041     __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5042   %}
5043   ins_pipe( pipe_slow );
5044 %}
5045 
5046 instruct vand_mem(vec dst, vec src, memory mem) %{
5047   predicate(UseAVX &gt; 0);
5048   match(Set dst (AndV src (LoadVector mem)));
5049   format %{ &quot;vpand   $dst,$src,$mem\t! and vectors&quot; %}
5050   ins_encode %{
5051     int vector_len = vector_length_encoding(this);
5052     __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5053   %}
5054   ins_pipe( pipe_slow );
5055 %}
5056 
5057 // --------------------------------- OR ---------------------------------------
5058 
5059 instruct vor(vec dst, vec src) %{
5060   predicate(UseAVX == 0);
5061   match(Set dst (OrV dst src));
5062   format %{ &quot;por     $dst,$src\t! or vectors&quot; %}
5063   ins_encode %{
5064     __ por($dst$$XMMRegister, $src$$XMMRegister);
5065   %}
5066   ins_pipe( pipe_slow );
5067 %}
5068 
5069 instruct vor_reg(vec dst, vec src1, vec src2) %{
5070   predicate(UseAVX &gt; 0);
5071   match(Set dst (OrV src1 src2));
5072   format %{ &quot;vpor    $dst,$src1,$src2\t! or vectors&quot; %}
5073   ins_encode %{
5074     int vector_len = vector_length_encoding(this);
5075     __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5076   %}
5077   ins_pipe( pipe_slow );
5078 %}
5079 
5080 instruct vor_mem(vec dst, vec src, memory mem) %{
5081   predicate(UseAVX &gt; 0);
5082   match(Set dst (OrV src (LoadVector mem)));
5083   format %{ &quot;vpor    $dst,$src,$mem\t! or vectors&quot; %}
5084   ins_encode %{
5085     int vector_len = vector_length_encoding(this);
5086     __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5087   %}
5088   ins_pipe( pipe_slow );
5089 %}
5090 
5091 // --------------------------------- XOR --------------------------------------
5092 
5093 instruct vxor(vec dst, vec src) %{
5094   predicate(UseAVX == 0);
5095   match(Set dst (XorV dst src));
5096   format %{ &quot;pxor    $dst,$src\t! xor vectors&quot; %}
5097   ins_encode %{
5098     __ pxor($dst$$XMMRegister, $src$$XMMRegister);
5099   %}
5100   ins_pipe( pipe_slow );
5101 %}
5102 
5103 instruct vxor_reg(vec dst, vec src1, vec src2) %{
5104   predicate(UseAVX &gt; 0);
5105   match(Set dst (XorV src1 src2));
5106   format %{ &quot;vpxor   $dst,$src1,$src2\t! xor vectors&quot; %}
5107   ins_encode %{
5108     int vector_len = vector_length_encoding(this);
5109     __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5110   %}
5111   ins_pipe( pipe_slow );
5112 %}
5113 
5114 instruct vxor_mem(vec dst, vec src, memory mem) %{
5115   predicate(UseAVX &gt; 0);
5116   match(Set dst (XorV src (LoadVector mem)));
5117   format %{ &quot;vpxor   $dst,$src,$mem\t! xor vectors&quot; %}
5118   ins_encode %{
5119     int vector_len = vector_length_encoding(this);
5120     __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5121   %}
5122   ins_pipe( pipe_slow );
5123 %}
5124 
5125 // --------------------------------- ABS --------------------------------------
5126 // a = |a|
5127 instruct vabsB_reg(vec dst, vec src) %{
5128   match(Set dst (AbsVB  src));
5129   format %{ &quot;vabsb $dst,$src\t# $dst = |$src| abs packedB&quot; %}
5130   ins_encode %{
5131     uint vlen = vector_length(this);
5132     if (vlen &lt;= 16) {
5133       __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
5134     } else {
5135       int vlen_enc = vector_length_encoding(this);
5136       __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5137     }
5138   %}
5139   ins_pipe( pipe_slow );
5140 %}
5141 
5142 instruct vabsS_reg(vec dst, vec src) %{
5143   match(Set dst (AbsVS  src));
5144   format %{ &quot;vabsw $dst,$src\t# $dst = |$src| abs packedS&quot; %}
5145   ins_encode %{
5146     uint vlen = vector_length(this);
5147     if (vlen &lt;= 8) {
5148       __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
5149     } else {
5150       int vlen_enc = vector_length_encoding(this);
5151       __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5152     }
5153   %}
5154   ins_pipe( pipe_slow );
5155 %}
5156 
5157 instruct vabsI_reg(vec dst, vec src) %{
5158   match(Set dst (AbsVI  src));
5159   format %{ &quot;pabsd $dst,$src\t# $dst = |$src| abs packedI&quot; %}
5160   ins_encode %{
5161     uint vlen = vector_length(this);
5162     if (vlen &lt;= 4) {
5163       __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
5164     } else {
5165       int vlen_enc = vector_length_encoding(this);
5166       __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5167     }
5168   %}
5169   ins_pipe( pipe_slow );
5170 %}
5171 
5172 instruct vabsL_reg(vec dst, vec src) %{
5173   match(Set dst (AbsVL  src));
5174   format %{ &quot;evpabsq $dst,$src\t# $dst = |$src| abs packedL&quot; %}
5175   ins_encode %{
5176     assert(UseAVX &gt; 2, &quot;required&quot;);
5177     int vector_len = vector_length_encoding(this);
5178     __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5179   %}
5180   ins_pipe( pipe_slow );
5181 %}
5182 
5183 // --------------------------------- ABSNEG --------------------------------------
5184 
5185 instruct vabsnegF(vec dst, vec src, rRegI scratch) %{
5186   predicate(n-&gt;as_Vector()-&gt;length() != 4); // handled by 1-operand instruction vabsneg4F
5187   match(Set dst (AbsVF src));
5188   match(Set dst (NegVF src));
5189   effect(TEMP scratch);
5190   format %{ &quot;vabsnegf $dst,$src,[mask]\t# absneg packedF&quot; %}
5191   ins_cost(150);
5192   ins_encode %{
5193     int opcode = this-&gt;ideal_Opcode();
5194     int vlen = vector_length(this);
5195     if (vlen == 2) {
5196       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5197     } else {
5198       assert(vlen == 8 || vlen == 16, &quot;required&quot;);
5199       int vlen_enc = vector_length_encoding(this);
5200       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5201     }
5202   %}
5203   ins_pipe( pipe_slow );
5204 %}
5205 
5206 instruct vabsneg4F(vec dst, rRegI scratch) %{
5207   predicate(n-&gt;as_Vector()-&gt;length() == 4);
5208   match(Set dst (AbsVF dst));
5209   match(Set dst (NegVF dst));
5210   effect(TEMP scratch);
5211   format %{ &quot;vabsnegf $dst,[mask]\t# absneg packed4F&quot; %}
5212   ins_cost(150);
5213   ins_encode %{
5214     int opcode = this-&gt;ideal_Opcode();
5215     __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);
5216   %}
5217   ins_pipe( pipe_slow );
5218 %}
5219 
5220 instruct vabsnegD(vec dst, vec src, rRegI scratch) %{
5221   match(Set dst (AbsVD  src));
5222   match(Set dst (NegVD  src));
5223   effect(TEMP scratch);
5224   format %{ &quot;vabsnegd $dst,$src,[mask]\t# absneg packedD&quot; %}
5225   ins_encode %{
5226     int opcode = this-&gt;ideal_Opcode();
5227     uint vlen = vector_length(this);
5228     if (vlen == 2) {
5229       assert(UseSSE &gt;= 2, &quot;required&quot;);
5230       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5231     } else {
5232       int vlen_enc = vector_length_encoding(this);
5233       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5234     }
5235   %}
5236   ins_pipe( pipe_slow );
5237 %}
5238 
5239 // --------------------------------- FMA --------------------------------------
5240 // a * b + c
5241 
5242 instruct vfmaF_reg(vec a, vec b, vec c) %{
5243   match(Set c (FmaVF  c (Binary a b)));
5244   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5245   ins_cost(150);
5246   ins_encode %{
5247     assert(UseFMA, &quot;not enabled&quot;);
5248     int vector_len = vector_length_encoding(this);
5249     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5250   %}
5251   ins_pipe( pipe_slow );
5252 %}
5253 
5254 instruct vfmaF_mem(vec a, memory b, vec c) %{
5255   match(Set c (FmaVF  c (Binary a (LoadVector b))));
5256   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5257   ins_cost(150);
5258   ins_encode %{
5259     assert(UseFMA, &quot;not enabled&quot;);
5260     int vector_len = vector_length_encoding(this);
5261     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5262   %}
5263   ins_pipe( pipe_slow );
5264 %}
5265 
5266 instruct vfmaD_reg(vec a, vec b, vec c) %{
5267   match(Set c (FmaVD  c (Binary a b)));
5268   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5269   ins_cost(150);
5270   ins_encode %{
5271     assert(UseFMA, &quot;not enabled&quot;);
5272     int vector_len = vector_length_encoding(this);
5273     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5274   %}
5275   ins_pipe( pipe_slow );
5276 %}
5277 
5278 instruct vfmaD_mem(vec a, memory b, vec c) %{
5279   match(Set c (FmaVD  c (Binary a (LoadVector b))));
5280   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5281   ins_cost(150);
5282   ins_encode %{
5283     assert(UseFMA, &quot;not enabled&quot;);
5284     int vector_len = vector_length_encoding(this);
5285     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5286   %}
5287   ins_pipe( pipe_slow );
5288 %}
5289 
5290 // --------------------------------- Vector Multiply Add --------------------------------------
5291 
5292 instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
5293   predicate(UseAVX == 0);
5294   match(Set dst (MulAddVS2VI dst src1));
5295   format %{ &quot;pmaddwd $dst,$dst,$src1\t! muladd packedStoI&quot; %}
5296   ins_encode %{
5297     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5298   %}
5299   ins_pipe( pipe_slow );
5300 %}
5301 
5302 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5303   predicate(UseAVX &gt; 0);
5304   match(Set dst (MulAddVS2VI src1 src2));
5305   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5306   ins_encode %{
5307     int vector_len = vector_length_encoding(this);
5308     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5309   %}
5310   ins_pipe( pipe_slow );
5311 %}
5312 
5313 // --------------------------------- Vector Multiply Add Add ----------------------------------
5314 
5315 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
5316   predicate(VM_Version::supports_avx512_vnni());
5317   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5318   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5319   ins_encode %{
5320     assert(UseAVX &gt; 2, &quot;required&quot;);
5321     int vector_len = vector_length_encoding(this);
5322     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5323   %}
5324   ins_pipe( pipe_slow );
5325   ins_cost(10);
5326 %}
5327 
5328 // --------------------------------- PopCount --------------------------------------
5329 
5330 instruct vpopcountI(vec dst, vec src) %{
5331   match(Set dst (PopCountVI src));
5332   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5333   ins_encode %{
5334     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5335 
5336     int vector_len = vector_length_encoding(this);
5337     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5338   %}
5339   ins_pipe( pipe_slow );
5340 %}
5341 
5342 // --------------------------------- Bitwise Ternary Logic ----------------------------------
5343 
5344 instruct vpternlog(vec dst, vec src2, vec src3, immU8 func) %{
5345   match(Set dst (MacroLogicV (Binary dst src2) (Binary src3 func)));
5346   effect(TEMP dst);
5347   format %{ &quot;vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic&quot; %}
5348   ins_encode %{
5349     int vector_len = vector_length_encoding(this);
5350     __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$XMMRegister, vector_len);
5351   %}
5352   ins_pipe( pipe_slow );
5353 %}
5354 
5355 instruct vpternlog_mem(vec dst, vec src2, memory src3, immU8 func) %{
5356   match(Set dst (MacroLogicV (Binary dst src2) (Binary (LoadVector src3) func)));
5357   effect(TEMP dst);
5358   format %{ &quot;vpternlogd $dst,$src2,$src3,$func\t! vector ternary logic&quot; %}
5359   ins_encode %{
5360     int vector_len = vector_length_encoding(this);
5361     __ vpternlogd($dst$$XMMRegister, $func$$constant, $src2$$XMMRegister, $src3$$Address, vector_len);
5362   %}
5363   ins_pipe( pipe_slow );
5364 %}
    </pre>
  </body>
</html>