<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2016, 2019, Red Hat, Inc. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
 31 #include &quot;logging/logStream.hpp&quot;
 32 #include &quot;runtime/orderAccess.hpp&quot;
 33 
 34 ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :
 35   _heap(heap),
 36   _mutator_free_bitmap(max_regions, mtGC),
 37   _collector_free_bitmap(max_regions, mtGC),
 38   _max(max_regions)
 39 {
 40   clear_internal();
 41 }
 42 
 43 void ShenandoahFreeSet::increase_used(size_t num_bytes) {
 44   shenandoah_assert_heaplocked();
 45   _used += num_bytes;
 46 
 47   assert(_used &lt;= _capacity, &quot;must not use more than we have: used: &quot; SIZE_FORMAT
 48          &quot;, capacity: &quot; SIZE_FORMAT &quot;, num_bytes: &quot; SIZE_FORMAT, _used, _capacity, num_bytes);
 49 }
 50 
 51 bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {
 52   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 53           idx, _max, _mutator_leftmost, _mutator_rightmost);
 54   return _mutator_free_bitmap.at(idx);
 55 }
 56 
 57 bool ShenandoahFreeSet::is_collector_free(size_t idx) const {
 58   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 59           idx, _max, _collector_leftmost, _collector_rightmost);
 60   return _collector_free_bitmap.at(idx);
 61 }
 62 
 63 HeapWord* ShenandoahFreeSet::allocate_single(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 64   // Scan the bitmap looking for a first fit.
 65   //
 66   // Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,
 67   // we would find the region to allocate at right away.
 68   //
 69   // Allocations are biased: new application allocs go to beginning of the heap, and GC allocs
 70   // go to the end. This makes application allocation faster, because we would clear lots
 71   // of regions from the beginning most of the time.
 72   //
 73   // Free set maintains mutator and collector views, and normally they allocate in their views only,
 74   // unless we special cases for stealing and mixed allocations.
 75 
 76   switch (req.type()) {
 77     case ShenandoahAllocRequest::_alloc_tlab:
 78     case ShenandoahAllocRequest::_alloc_shared: {
 79 
 80       // Try to allocate in the mutator view
 81       for (size_t idx = _mutator_leftmost; idx &lt;= _mutator_rightmost; idx++) {
 82         if (is_mutator_free(idx)) {
 83           HeapWord* result = try_allocate_in(_heap-&gt;get_region(idx), req, in_new_region);
 84           if (result != NULL) {
 85             return result;
 86           }
 87         }
 88       }
 89 
 90       // There is no recovery. Mutator does not touch collector view at all.
 91       break;
 92     }
 93     case ShenandoahAllocRequest::_alloc_gclab:
 94     case ShenandoahAllocRequest::_alloc_shared_gc: {
 95       // size_t is unsigned, need to dodge underflow when _leftmost = 0
 96 
 97       // Fast-path: try to allocate in the collector view first
 98       for (size_t c = _collector_rightmost + 1; c &gt; _collector_leftmost; c--) {
 99         size_t idx = c - 1;
100         if (is_collector_free(idx)) {
101           HeapWord* result = try_allocate_in(_heap-&gt;get_region(idx), req, in_new_region);
102           if (result != NULL) {
103             return result;
104           }
105         }
106       }
107 
108       // No dice. Can we borrow space from mutator view?
109       if (!ShenandoahEvacReserveOverflow) {
110         return NULL;
111       }
112 
113       // Try to steal the empty region from the mutator view
114       for (size_t c = _mutator_rightmost + 1; c &gt; _mutator_leftmost; c--) {
115         size_t idx = c - 1;
116         if (is_mutator_free(idx)) {
117           ShenandoahHeapRegion* r = _heap-&gt;get_region(idx);
118           if (can_allocate_from(r)) {
119             flip_to_gc(r);
120             HeapWord *result = try_allocate_in(r, req, in_new_region);
121             if (result != NULL) {
122               return result;
123             }
124           }
125         }
126       }
127 
128       // Try to mix the allocation into the mutator view:
129       if (ShenandoahAllowMixedAllocs) {
130         for (size_t c = _mutator_rightmost + 1; c &gt; _mutator_leftmost; c--) {
131           size_t idx = c - 1;
132           if (is_mutator_free(idx)) {
133             HeapWord* result = try_allocate_in(_heap-&gt;get_region(idx), req, in_new_region);
134             if (result != NULL) {
135               return result;
136             }
137           }
138         }
139       }
140       break;
141     }
142     default:
143       ShouldNotReachHere();
144   }
145 
146   return NULL;
147 }
148 
149 HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
150   assert (!has_no_alloc_capacity(r), &quot;Performance: should avoid full regions on this path: &quot; SIZE_FORMAT, r-&gt;index());
151 
<a name="1" id="anc1"></a><span class="line-modified">152   if (_heap-&gt;is_concurrent_weak_root_in_progress() &amp;&amp;</span>
153       r-&gt;is_trash()) {
154     return NULL;
155   }
156 
157   try_recycle_trashed(r);
158 
159   in_new_region = r-&gt;is_empty();
160 
161   HeapWord* result = NULL;
162   size_t size = req.size();
163 
164   if (ShenandoahElasticTLAB &amp;&amp; req.is_lab_alloc()) {
165     size_t free = align_down(r-&gt;free() &gt;&gt; LogHeapWordSize, MinObjAlignment);
166     if (size &gt; free) {
167       size = free;
168     }
169     if (size &gt;= req.min_size()) {
170       result = r-&gt;allocate(size, req.type());
171       assert (result != NULL, &quot;Allocation must succeed: free &quot; SIZE_FORMAT &quot;, actual &quot; SIZE_FORMAT, free, size);
172     }
173   } else {
174     result = r-&gt;allocate(size, req.type());
175   }
176 
177   if (result != NULL) {
178     // Allocation successful, bump stats:
179     if (req.is_mutator_alloc()) {
180       increase_used(size * HeapWordSize);
181     }
182 
183     // Record actual allocation size
184     req.set_actual_size(size);
185 
186     if (req.is_gc_alloc()) {
187       r-&gt;set_update_watermark(r-&gt;top());
188     }
189   }
190 
191   if (result == NULL || has_no_alloc_capacity(r)) {
192     // Region cannot afford this or future allocations. Retire it.
193     //
194     // While this seems a bit harsh, especially in the case when this large allocation does not
195     // fit, but the next small one would, we are risking to inflate scan times when lots of
196     // almost-full regions precede the fully-empty region where we want allocate the entire TLAB.
197     // TODO: Record first fully-empty region, and use that for large allocations
198 
199     // Record the remainder as allocation waste
200     if (req.is_mutator_alloc()) {
201       size_t waste = r-&gt;free();
202       if (waste &gt; 0) {
203         increase_used(waste);
204         _heap-&gt;notify_mutator_alloc_words(waste &gt;&gt; LogHeapWordSize, true);
205       }
206     }
207 
208     size_t num = r-&gt;index();
209     _collector_free_bitmap.clear_bit(num);
210     _mutator_free_bitmap.clear_bit(num);
211     // Touched the bounds? Need to update:
212     if (touches_bounds(num)) {
213       adjust_bounds();
214     }
215     assert_bounds();
216   }
217   return result;
218 }
219 
220 bool ShenandoahFreeSet::touches_bounds(size_t num) const {
221   return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;
222 }
223 
224 void ShenandoahFreeSet::recompute_bounds() {
225   // Reset to the most pessimistic case:
226   _mutator_rightmost = _max - 1;
227   _mutator_leftmost = 0;
228   _collector_rightmost = _max - 1;
229   _collector_leftmost = 0;
230 
231   // ...and adjust from there
232   adjust_bounds();
233 }
234 
235 void ShenandoahFreeSet::adjust_bounds() {
236   // Rewind both mutator bounds until the next bit.
237   while (_mutator_leftmost &lt; _max &amp;&amp; !is_mutator_free(_mutator_leftmost)) {
238     _mutator_leftmost++;
239   }
240   while (_mutator_rightmost &gt; 0 &amp;&amp; !is_mutator_free(_mutator_rightmost)) {
241     _mutator_rightmost--;
242   }
243   // Rewind both collector bounds until the next bit.
244   while (_collector_leftmost &lt; _max &amp;&amp; !is_collector_free(_collector_leftmost)) {
245     _collector_leftmost++;
246   }
247   while (_collector_rightmost &gt; 0 &amp;&amp; !is_collector_free(_collector_rightmost)) {
248     _collector_rightmost--;
249   }
250 }
251 
252 HeapWord* ShenandoahFreeSet::allocate_contiguous(ShenandoahAllocRequest&amp; req) {
253   shenandoah_assert_heaplocked();
254 
255   size_t words_size = req.size();
256   size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);
257 
258   // No regions left to satisfy allocation, bye.
259   if (num &gt; mutator_count()) {
260     return NULL;
261   }
262 
263   // Find the continuous interval of $num regions, starting from $beg and ending in $end,
264   // inclusive. Contiguous allocations are biased to the beginning.
265 
266   size_t beg = _mutator_leftmost;
267   size_t end = beg;
268 
269   while (true) {
270     if (end &gt;= _max) {
271       // Hit the end, goodbye
272       return NULL;
273     }
274 
275     // If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.
276     // If region is not completely free, the current [beg; end] is useless, and we may fast-forward.
277     if (!is_mutator_free(end) || !can_allocate_from(_heap-&gt;get_region(end))) {
278       end++;
279       beg = end;
280       continue;
281     }
282 
283     if ((end - beg + 1) == num) {
284       // found the match
285       break;
286     }
287 
288     end++;
289   };
290 
291   size_t remainder = words_size &amp; ShenandoahHeapRegion::region_size_words_mask();
292 
293   // Initialize regions:
294   for (size_t i = beg; i &lt;= end; i++) {
295     ShenandoahHeapRegion* r = _heap-&gt;get_region(i);
296     try_recycle_trashed(r);
297 
298     assert(i == beg || _heap-&gt;get_region(i - 1)-&gt;index() + 1 == r-&gt;index(), &quot;Should be contiguous&quot;);
299     assert(r-&gt;is_empty(), &quot;Should be empty&quot;);
300 
301     if (i == beg) {
302       r-&gt;make_humongous_start();
303     } else {
304       r-&gt;make_humongous_cont();
305     }
306 
307     // Trailing region may be non-full, record the remainder there
308     size_t used_words;
309     if ((i == end) &amp;&amp; (remainder != 0)) {
310       used_words = remainder;
311     } else {
312       used_words = ShenandoahHeapRegion::region_size_words();
313     }
314 
315     r-&gt;set_top(r-&gt;bottom() + used_words);
316 
317     _mutator_free_bitmap.clear_bit(r-&gt;index());
318   }
319 
320   // While individual regions report their true use, all humongous regions are
321   // marked used in the free set.
322   increase_used(ShenandoahHeapRegion::region_size_bytes() * num);
323 
324   if (remainder != 0) {
325     // Record this remainder as allocation waste
326     _heap-&gt;notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);
327   }
328 
329   // Allocated at left/rightmost? Move the bounds appropriately.
330   if (beg == _mutator_leftmost || end == _mutator_rightmost) {
331     adjust_bounds();
332   }
333   assert_bounds();
334 
335   req.set_actual_size(words_size);
336   return _heap-&gt;get_region(beg)-&gt;bottom();
337 }
338 
339 bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {
<a name="2" id="anc2"></a><span class="line-modified">340   return r-&gt;is_empty() || (r-&gt;is_trash() &amp;&amp; !_heap-&gt;is_concurrent_weak_root_in_progress());</span>
341 }
342 
343 size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {
344   if (r-&gt;is_trash()) {
345     // This would be recycled on allocation path
346     return ShenandoahHeapRegion::region_size_bytes();
347   } else {
348     return r-&gt;free();
349   }
350 }
351 
352 bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {
353   return alloc_capacity(r) == 0;
354 }
355 
356 void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion *r) {
357   if (r-&gt;is_trash()) {
358     _heap-&gt;decrease_used(r-&gt;used());
359     r-&gt;recycle();
360   }
361 }
362 
363 void ShenandoahFreeSet::recycle_trash() {
364   // lock is not reentrable, check we don&#39;t have it
365   shenandoah_assert_not_heaplocked();
366 
367   for (size_t i = 0; i &lt; _heap-&gt;num_regions(); i++) {
368     ShenandoahHeapRegion* r = _heap-&gt;get_region(i);
369     if (r-&gt;is_trash()) {
370       ShenandoahHeapLocker locker(_heap-&gt;lock());
371       try_recycle_trashed(r);
372     }
373     SpinPause(); // allow allocators to take the lock
374   }
375 }
376 
377 void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {
378   size_t idx = r-&gt;index();
379 
380   assert(_mutator_free_bitmap.at(idx), &quot;Should be in mutator view&quot;);
381   assert(can_allocate_from(r), &quot;Should not be allocated&quot;);
382 
383   _mutator_free_bitmap.clear_bit(idx);
384   _collector_free_bitmap.set_bit(idx);
385   _collector_leftmost = MIN2(idx, _collector_leftmost);
386   _collector_rightmost = MAX2(idx, _collector_rightmost);
387 
388   _capacity -= alloc_capacity(r);
389 
390   if (touches_bounds(idx)) {
391     adjust_bounds();
392   }
393   assert_bounds();
394 }
395 
396 void ShenandoahFreeSet::clear() {
397   shenandoah_assert_heaplocked();
398   clear_internal();
399 }
400 
401 void ShenandoahFreeSet::clear_internal() {
402   _mutator_free_bitmap.clear();
403   _collector_free_bitmap.clear();
404   _mutator_leftmost = _max;
405   _mutator_rightmost = 0;
406   _collector_leftmost = _max;
407   _collector_rightmost = 0;
408   _capacity = 0;
409   _used = 0;
410 }
411 
412 void ShenandoahFreeSet::rebuild() {
413   shenandoah_assert_heaplocked();
414   clear();
415 
416   for (size_t idx = 0; idx &lt; _heap-&gt;num_regions(); idx++) {
417     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
418     if (region-&gt;is_alloc_allowed() || region-&gt;is_trash()) {
419       assert(!region-&gt;is_cset(), &quot;Shouldn&#39;t be adding those to the free set&quot;);
420 
421       // Do not add regions that would surely fail allocation
422       if (has_no_alloc_capacity(region)) continue;
423 
424       _capacity += alloc_capacity(region);
425       assert(_used &lt;= _capacity, &quot;must not use more than we have&quot;);
426 
427       assert(!is_mutator_free(idx), &quot;We are about to add it, it shouldn&#39;t be there already&quot;);
428       _mutator_free_bitmap.set_bit(idx);
429     }
430   }
431 
432   // Evac reserve: reserve trailing space for evacuations
433   size_t to_reserve = _heap-&gt;max_capacity() / 100 * ShenandoahEvacReserve;
434   size_t reserved = 0;
435 
436   for (size_t idx = _heap-&gt;num_regions() - 1; idx &gt; 0; idx--) {
437     if (reserved &gt;= to_reserve) break;
438 
439     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
440     if (_mutator_free_bitmap.at(idx) &amp;&amp; can_allocate_from(region)) {
441       _mutator_free_bitmap.clear_bit(idx);
442       _collector_free_bitmap.set_bit(idx);
443       size_t ac = alloc_capacity(region);
444       _capacity -= ac;
445       reserved += ac;
446     }
447   }
448 
449   recompute_bounds();
450   assert_bounds();
451 }
452 
453 void ShenandoahFreeSet::log_status() {
454   shenandoah_assert_heaplocked();
455 
456   LogTarget(Info, gc, ergo) lt;
457   if (lt.is_enabled()) {
458     ResourceMark rm;
459     LogStream ls(lt);
460 
461     {
462       size_t last_idx = 0;
463       size_t max = 0;
464       size_t max_contig = 0;
465       size_t empty_contig = 0;
466 
467       size_t total_used = 0;
468       size_t total_free = 0;
469       size_t total_free_ext = 0;
470 
471       for (size_t idx = _mutator_leftmost; idx &lt;= _mutator_rightmost; idx++) {
472         if (is_mutator_free(idx)) {
473           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
474           size_t free = alloc_capacity(r);
475 
476           max = MAX2(max, free);
477 
478           if (r-&gt;is_empty()) {
479             total_free_ext += free;
480             if (last_idx + 1 == idx) {
481               empty_contig++;
482             } else {
483               empty_contig = 1;
484             }
485           } else {
486             empty_contig = 0;
487           }
488 
489           total_used += r-&gt;used();
490           total_free += free;
491 
492           max_contig = MAX2(max_contig, empty_contig);
493           last_idx = idx;
494         }
495       }
496 
497       size_t max_humongous = max_contig * ShenandoahHeapRegion::region_size_bytes();
498       size_t free = capacity() - used();
499 
500       ls.print(&quot;Free: &quot; SIZE_FORMAT &quot;%s, Max: &quot; SIZE_FORMAT &quot;%s regular, &quot; SIZE_FORMAT &quot;%s humongous, &quot;,
501                byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),
502                byte_size_in_proper_unit(max),           proper_unit_for_byte_size(max),
503                byte_size_in_proper_unit(max_humongous), proper_unit_for_byte_size(max_humongous)
504       );
505 
506       ls.print(&quot;Frag: &quot;);
507       size_t frag_ext;
508       if (total_free_ext &gt; 0) {
509         frag_ext = 100 - (100 * max_humongous / total_free_ext);
510       } else {
511         frag_ext = 0;
512       }
513       ls.print(SIZE_FORMAT &quot;%% external, &quot;, frag_ext);
514 
515       size_t frag_int;
516       if (mutator_count() &gt; 0) {
517         frag_int = (100 * (total_used / mutator_count()) / ShenandoahHeapRegion::region_size_bytes());
518       } else {
519         frag_int = 0;
520       }
521       ls.print(SIZE_FORMAT &quot;%% internal; &quot;, frag_int);
522     }
523 
524     {
525       size_t max = 0;
526       size_t total_free = 0;
527 
528       for (size_t idx = _collector_leftmost; idx &lt;= _collector_rightmost; idx++) {
529         if (is_collector_free(idx)) {
530           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
531           size_t free = alloc_capacity(r);
532           max = MAX2(max, free);
533           total_free += free;
534         }
535       }
536 
537       ls.print_cr(&quot;Reserve: &quot; SIZE_FORMAT &quot;%s, Max: &quot; SIZE_FORMAT &quot;%s&quot;,
538                   byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),
539                   byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));
540     }
541   }
542 }
543 
544 HeapWord* ShenandoahFreeSet::allocate(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
545   shenandoah_assert_heaplocked();
546   assert_bounds();
547 
548   if (req.size() &gt; ShenandoahHeapRegion::humongous_threshold_words()) {
549     switch (req.type()) {
550       case ShenandoahAllocRequest::_alloc_shared:
551       case ShenandoahAllocRequest::_alloc_shared_gc:
552         in_new_region = true;
553         return allocate_contiguous(req);
554       case ShenandoahAllocRequest::_alloc_gclab:
555       case ShenandoahAllocRequest::_alloc_tlab:
556         in_new_region = false;
557         assert(false, &quot;Trying to allocate TLAB larger than the humongous threshold: &quot; SIZE_FORMAT &quot; &gt; &quot; SIZE_FORMAT,
558                req.size(), ShenandoahHeapRegion::humongous_threshold_words());
559         return NULL;
560       default:
561         ShouldNotReachHere();
562         return NULL;
563     }
564   } else {
565     return allocate_single(req, in_new_region);
566   }
567 }
568 
569 size_t ShenandoahFreeSet::unsafe_peek_free() const {
570   // Deliberately not locked, this method is unsafe when free set is modified.
571 
572   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
573     if (index &lt; _max &amp;&amp; is_mutator_free(index)) {
574       ShenandoahHeapRegion* r = _heap-&gt;get_region(index);
575       if (r-&gt;free() &gt;= MinTLABSize) {
576         return r-&gt;free();
577       }
578     }
579   }
580 
581   // It appears that no regions left
582   return 0;
583 }
584 
585 void ShenandoahFreeSet::print_on(outputStream* out) const {
586   out-&gt;print_cr(&quot;Mutator Free Set: &quot; SIZE_FORMAT &quot;&quot;, mutator_count());
587   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
588     if (is_mutator_free(index)) {
589       _heap-&gt;get_region(index)-&gt;print_on(out);
590     }
591   }
592   out-&gt;print_cr(&quot;Collector Free Set: &quot; SIZE_FORMAT &quot;&quot;, collector_count());
593   for (size_t index = _collector_leftmost; index &lt;= _collector_rightmost; index++) {
594     if (is_collector_free(index)) {
595       _heap-&gt;get_region(index)-&gt;print_on(out);
596     }
597   }
598 }
599 
600 /*
601  * Internal fragmentation metric: describes how fragmented the heap regions are.
602  *
603  * It is derived as:
604  *
605  *               sum(used[i]^2, i=0..k)
606  *   IF = 1 - ------------------------------
607  *              C * sum(used[i], i=0..k)
608  *
609  * ...where k is the number of regions in computation, C is the region capacity, and
610  * used[i] is the used space in the region.
611  *
612  * The non-linearity causes IF to be lower for the cases where the same total heap
613  * used is densely packed. For example:
614  *   a) Heap is completely full  =&gt; IF = 0
615  *   b) Heap is half full, first 50% regions are completely full =&gt; IF = 0
616  *   c) Heap is half full, each region is 50% full =&gt; IF = 1/2
617  *   d) Heap is quarter full, first 50% regions are completely full =&gt; IF = 0
618  *   e) Heap is quarter full, each region is 25% full =&gt; IF = 3/4
619  *   f) Heap has one small object per each region =&gt; IF =~ 1
620  */
621 double ShenandoahFreeSet::internal_fragmentation() {
622   double squared = 0;
623   double linear = 0;
624   int count = 0;
625 
626   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
627     if (is_mutator_free(index)) {
628       ShenandoahHeapRegion* r = _heap-&gt;get_region(index);
629       size_t used = r-&gt;used();
630       squared += used * used;
631       linear += used;
632       count++;
633     }
634   }
635 
636   if (count &gt; 0) {
637     double s = squared / (ShenandoahHeapRegion::region_size_bytes() * linear);
638     return 1 - s;
639   } else {
640     return 0;
641   }
642 }
643 
644 /*
645  * External fragmentation metric: describes how fragmented the heap is.
646  *
647  * It is derived as:
648  *
649  *   EF = 1 - largest_contiguous_free / total_free
650  *
651  * For example:
652  *   a) Heap is completely empty =&gt; EF = 0
653  *   b) Heap is completely full =&gt; EF = 0
654  *   c) Heap is first-half full =&gt; EF = 1/2
655  *   d) Heap is half full, full and empty regions interleave =&gt; EF =~ 1
656  */
657 double ShenandoahFreeSet::external_fragmentation() {
658   size_t last_idx = 0;
659   size_t max_contig = 0;
660   size_t empty_contig = 0;
661 
662   size_t free = 0;
663 
664   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
665     if (is_mutator_free(index)) {
666       ShenandoahHeapRegion* r = _heap-&gt;get_region(index);
667       if (r-&gt;is_empty()) {
668         free += ShenandoahHeapRegion::region_size_bytes();
669         if (last_idx + 1 == index) {
670           empty_contig++;
671         } else {
672           empty_contig = 1;
673         }
674       } else {
675         empty_contig = 0;
676       }
677 
678       max_contig = MAX2(max_contig, empty_contig);
679       last_idx = index;
680     }
681   }
682 
683   if (free &gt; 0) {
684     return 1 - (1.0 * max_contig * ShenandoahHeapRegion::region_size_bytes() / free);
685   } else {
686     return 0;
687   }
688 }
689 
690 #ifdef ASSERT
691 void ShenandoahFreeSet::assert_bounds() const {
692   // Performance invariants. Failing these would not break the free set, but performance
693   // would suffer.
694   assert (_mutator_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_leftmost,  _max);
695   assert (_mutator_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_rightmost, _max);
696 
697   assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _mutator_leftmost);
698   assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _mutator_rightmost);
699 
700   size_t beg_off = _mutator_free_bitmap.get_next_one_offset(0);
701   size_t end_off = _mutator_free_bitmap.get_next_one_offset(_mutator_rightmost + 1);
702   assert (beg_off &gt;= _mutator_leftmost, &quot;free regions before the leftmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT, beg_off, _mutator_leftmost);
703   assert (end_off == _max,      &quot;free regions past the rightmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT,  end_off, _mutator_rightmost);
704 
705   assert (_collector_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_leftmost,  _max);
706   assert (_collector_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_rightmost, _max);
707 
708   assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _collector_leftmost);
709   assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _collector_rightmost);
710 
711   beg_off = _collector_free_bitmap.get_next_one_offset(0);
712   end_off = _collector_free_bitmap.get_next_one_offset(_collector_rightmost + 1);
713   assert (beg_off &gt;= _collector_leftmost, &quot;free regions before the leftmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT, beg_off, _collector_leftmost);
714   assert (end_off == _max,      &quot;free regions past the rightmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT,  end_off, _collector_rightmost);
715 }
716 #endif
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>