<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1ConcurrentRefineThread.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1DirtyCardQueue.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1BufferNodeList.hpp&quot;
 27 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;
 28 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;

 29 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
 30 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 31 #include &quot;gc/g1/g1FreeIdSet.hpp&quot;
 32 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
 33 #include &quot;gc/g1/g1RemSet.hpp&quot;
 34 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
 35 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 36 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
 37 #include &quot;memory/iterator.hpp&quot;
 38 #include &quot;runtime/atomic.hpp&quot;


 39 #include &quot;runtime/os.hpp&quot;
 40 #include &quot;runtime/safepoint.hpp&quot;
 41 #include &quot;runtime/thread.inline.hpp&quot;
 42 #include &quot;runtime/threadSMR.hpp&quot;
 43 #include &quot;utilities/globalCounter.inline.hpp&quot;
 44 #include &quot;utilities/macros.hpp&quot;
 45 #include &quot;utilities/quickSort.hpp&quot;

 46 
 47 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 48   // Dirty card queues are always active, so we create them with their
 49   // active field set to true.
<span class="line-modified"> 50   PtrQueue(qset, true /* active */)</span>

 51 { }
 52 
 53 G1DirtyCardQueue::~G1DirtyCardQueue() {
 54   flush();












 55 }
 56 
 57 void G1DirtyCardQueue::handle_completed_buffer() {
 58   assert(!is_empty(), &quot;precondition&quot;);

 59   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
 60   allocate_buffer();
<span class="line-modified"> 61   dirty_card_qset()-&gt;handle_completed_buffer(node);</span>
 62 }
 63 
 64 // Assumed to be zero by concurrent threads.
 65 static uint par_ids_start() { return 0; }
 66 
 67 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 68   PtrQueueSet(allocator),
 69   _primary_refinement_thread(NULL),
 70   _num_cards(0),
 71   _completed(),
 72   _paused(),
 73   _free_ids(par_ids_start(), num_par_ids()),
 74   _process_cards_threshold(ProcessCardsThresholdNever),
 75   _max_cards(MaxCardsUnlimited),
 76   _padded_max_cards(MaxCardsUnlimited),
<span class="line-modified"> 77   _mutator_refined_cards_counters(NEW_C_HEAP_ARRAY(size_t, num_par_ids(), mtGC))</span>
 78 {
<span class="line-removed"> 79   ::memset(_mutator_refined_cards_counters, 0, num_par_ids() * sizeof(size_t));</span>
 80   _all_active = true;
 81 }
 82 
 83 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
 84   abandon_completed_buffers();
<span class="line-removed"> 85   FREE_C_HEAP_ARRAY(size_t, _mutator_refined_cards_counters);</span>
 86 }
 87 
 88 // Determines how many mutator threads can process the buffers in parallel.
 89 uint G1DirtyCardQueueSet::num_par_ids() {
 90   return (uint)os::initial_active_processor_count();
 91 }
 92 
<span class="line-removed"> 93 size_t G1DirtyCardQueueSet::total_mutator_refined_cards() const {</span>
<span class="line-removed"> 94   size_t sum = 0;</span>
<span class="line-removed"> 95   for (uint i = 0; i &lt; num_par_ids(); ++i) {</span>
<span class="line-removed"> 96     sum += _mutator_refined_cards_counters[i];</span>
<span class="line-removed"> 97   }</span>
<span class="line-removed"> 98   return sum;</span>
<span class="line-removed"> 99 }</span>
<span class="line-removed">100 </span>
101 void G1DirtyCardQueueSet::handle_zero_index_for_thread(Thread* t) {
102   G1ThreadLocalData::dirty_card_queue(t).handle_zero_index();
103 }
104 
105 #ifdef ASSERT
106 G1DirtyCardQueueSet::Queue::~Queue() {
107   assert(_head == NULL, &quot;precondition&quot;);
108   assert(_tail == NULL, &quot;precondition&quot;);
109 }
110 #endif // ASSERT
111 
112 BufferNode* G1DirtyCardQueueSet::Queue::top() const {
113   return Atomic::load(&amp;_head);
114 }
115 
116 // An append operation atomically exchanges the new tail with the queue tail.
117 // It then sets the &quot;next&quot; value of the old tail to the head of the list being
118 // appended; it is an invariant that the old tail&#39;s &quot;next&quot; value is NULL.
119 // But if the old tail is NULL then the queue was empty.  In this case the
120 // head of the list being appended is instead stored in the queue head; it is
</pre>
<hr />
<pre>
405   if (from._head != NULL) {
406     Atomic::add(&amp;_num_cards, from._entry_count);
407     _completed.append(*from._head, *from._tail);
408   }
409 }
410 
411 G1BufferNodeList G1DirtyCardQueueSet::take_all_completed_buffers() {
412   enqueue_all_paused_buffers();
413   verify_num_cards();
414   HeadTail buffers = _completed.take_all();
415   size_t num_cards = Atomic::load(&amp;_num_cards);
416   Atomic::store(&amp;_num_cards, size_t(0));
417   return G1BufferNodeList(buffers._head, buffers._tail, num_cards);
418 }
419 
420 class G1RefineBufferedCards : public StackObj {
421   BufferNode* const _node;
422   CardTable::CardValue** const _node_buffer;
423   const size_t _node_buffer_size;
424   const uint _worker_id;
<span class="line-modified">425   size_t* _total_refined_cards;</span>
426   G1RemSet* const _g1rs;
427 
428   static inline int compare_card(const CardTable::CardValue* p1,
429                                  const CardTable::CardValue* p2) {
430     return p2 - p1;
431   }
432 
433   // Sorts the cards from start_index to _node_buffer_size in *decreasing*
434   // address order. Tests showed that this order is preferable to not sorting
435   // or increasing address order.
436   void sort_cards(size_t start_index) {
437     QuickSort::sort(&amp;_node_buffer[start_index],
438                     _node_buffer_size - start_index,
439                     compare_card,
440                     false);
441   }
442 
443   // Returns the index to the first clean card in the buffer.
444   size_t clean_cards() {
445     const size_t start = _node-&gt;index();
</pre>
<hr />
<pre>
455     assert(src &lt;= dst, &quot;invariant&quot;);
456     for ( ; src &lt; dst; ++src) {
457       // Search low to high for a card to keep.
458       if (_g1rs-&gt;clean_card_before_refine(src)) {
459         // Found keeper.  Search high to low for a card to discard.
460         while (src &lt; --dst) {
461           if (!_g1rs-&gt;clean_card_before_refine(dst)) {
462             *dst = *src;         // Replace discard with keeper.
463             break;
464           }
465         }
466         // If discard search failed (src == dst), the outer loop will also end.
467       }
468     }
469 
470     // dst points to the first retained clean card, or the end of the buffer
471     // if all the cards were discarded.
472     const size_t first_clean = dst - _node_buffer;
473     assert(first_clean &gt;= start &amp;&amp; first_clean &lt;= _node_buffer_size, &quot;invariant&quot;);
474     // Discarded cards are considered as refined.
<span class="line-modified">475     *_total_refined_cards += first_clean - start;</span>

476     return first_clean;
477   }
478 
479   bool refine_cleaned_cards(size_t start_index) {
480     bool result = true;
481     size_t i = start_index;
482     for ( ; i &lt; _node_buffer_size; ++i) {
483       if (SuspendibleThreadSet::should_yield()) {
484         redirty_unrefined_cards(i);
485         result = false;
486         break;
487       }
488       _g1rs-&gt;refine_card_concurrently(_node_buffer[i], _worker_id);
489     }
490     _node-&gt;set_index(i);
<span class="line-modified">491     *_total_refined_cards += i - start_index;</span>
492     return result;
493   }
494 
495   void redirty_unrefined_cards(size_t start) {
496     for ( ; start &lt; _node_buffer_size; ++start) {
497       *_node_buffer[start] = G1CardTable::dirty_card_val();
498     }
499   }
500 
501 public:
502   G1RefineBufferedCards(BufferNode* node,
503                         size_t node_buffer_size,
504                         uint worker_id,
<span class="line-modified">505                         size_t* total_refined_cards) :</span>
506     _node(node),
507     _node_buffer(reinterpret_cast&lt;CardTable::CardValue**&gt;(BufferNode::make_buffer_from_node(node))),
508     _node_buffer_size(node_buffer_size),
509     _worker_id(worker_id),
<span class="line-modified">510     _total_refined_cards(total_refined_cards),</span>
511     _g1rs(G1CollectedHeap::heap()-&gt;rem_set()) {}
512 
513   bool refine() {
514     size_t first_clean_index = clean_cards();
515     if (first_clean_index == _node_buffer_size) {
516       _node-&gt;set_index(first_clean_index);
517       return true;
518     }
519     // This fence serves two purposes. First, the cards must be cleaned
520     // before processing the contents. Second, we can&#39;t proceed with
521     // processing a region until after the read of the region&#39;s top in
522     // collect_and_clean_cards(), for synchronization with possibly concurrent
523     // humongous object allocation (see comment at the StoreStore fence before
524     // setting the regions&#39; tops in humongous allocation path).
525     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
526     // wrto each other. We need both set, in any order, to proceed.
527     OrderAccess::fence();
528     sort_cards(first_clean_index);
529     return refine_cleaned_cards(first_clean_index);
530   }
531 };
532 
533 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
534                                         uint worker_id,
<span class="line-modified">535                                         size_t* total_refined_cards) {</span>

536   G1RefineBufferedCards buffered_cards(node,
537                                        buffer_size(),
538                                        worker_id,
<span class="line-modified">539                                        total_refined_cards);</span>
<span class="line-modified">540   return buffered_cards.refine();</span>


541 }
542 
543 void G1DirtyCardQueueSet::handle_refined_buffer(BufferNode* node,
544                                                 bool fully_processed) {
545   if (fully_processed) {
546     assert(node-&gt;index() == buffer_size(),
547            &quot;Buffer not fully consumed: index: &quot; SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,
548            node-&gt;index(), buffer_size());
549     deallocate_buffer(node);
550   } else {
551     assert(node-&gt;index() &lt; buffer_size(), &quot;Buffer fully consumed.&quot;);
552     // Buffer incompletely processed because there is a pending safepoint.
553     // Record partially processed buffer, to be finished later.
554     record_paused_buffer(node);
555   }
556 }
557 
<span class="line-modified">558 void G1DirtyCardQueueSet::handle_completed_buffer(BufferNode* new_node) {</span>

559   enqueue_completed_buffer(new_node);
560 
561   // No need for mutator refinement if number of cards is below limit.
562   if (Atomic::load(&amp;_num_cards) &lt;= Atomic::load(&amp;_padded_max_cards)) {
563     return;
564   }
565 
566   // Only Java threads perform mutator refinement.
567   if (!Thread::current()-&gt;is_Java_thread()) {
568     return;
569   }
570 
571   BufferNode* node = get_completed_buffer();
572   if (node == NULL) return;     // Didn&#39;t get a buffer to process.
573 
574   // Refine cards in buffer.
575 
576   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
<span class="line-modified">577   uint counter_index = worker_id - par_ids_start();</span>
<span class="line-removed">578   size_t* counter = &amp;_mutator_refined_cards_counters[counter_index];</span>
<span class="line-removed">579   bool fully_processed = refine_buffer(node, worker_id, counter);</span>
580   _free_ids.release_par_id(worker_id); // release the id
581 
582   // Deal with buffer after releasing id, to let another thread use id.
583   handle_refined_buffer(node, fully_processed);
584 }
585 
586 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
587                                                                size_t stop_at,
<span class="line-modified">588                                                                size_t* total_refined_cards) {</span>
589   // Not enough cards to trigger processing.
590   if (Atomic::load(&amp;_num_cards) &lt;= stop_at) return false;
591 
592   BufferNode* node = get_completed_buffer();
593   if (node == NULL) return false; // Didn&#39;t get a buffer to process.
594 
<span class="line-modified">595   bool fully_processed = refine_buffer(node, worker_id, total_refined_cards);</span>
596   handle_refined_buffer(node, fully_processed);
597   return true;
598 }
599 
600 void G1DirtyCardQueueSet::abandon_logs() {
601   assert_at_safepoint();
602   abandon_completed_buffers();

603 
604   // Since abandon is done only at safepoints, we can safely manipulate
605   // these queues.
606   struct AbandonThreadLogClosure : public ThreadClosure {
607     virtual void do_thread(Thread* t) {
<span class="line-modified">608       G1ThreadLocalData::dirty_card_queue(t).reset();</span>


609     }
610   } closure;
611   Threads::threads_do(&amp;closure);
612 
613   G1BarrierSet::shared_dirty_card_queue().reset();
614 }
615 
616 void G1DirtyCardQueueSet::concatenate_logs() {
617   // Iterate over all the threads, if we find a partial log add it to
618   // the global list of logs.  Temporarily turn off the limit on the number
619   // of outstanding buffers.
620   assert_at_safepoint();
621   size_t old_limit = max_cards();
622   set_max_cards(MaxCardsUnlimited);
623 
624   struct ConcatenateThreadLogClosure : public ThreadClosure {
625     virtual void do_thread(Thread* t) {
626       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
627       if (!dcq.is_empty()) {
628         dcq.flush();
629       }
630     }
631   } closure;
632   Threads::threads_do(&amp;closure);
633 
634   G1BarrierSet::shared_dirty_card_queue().flush();
635   enqueue_all_paused_buffers();
636   verify_num_cards();
637   set_max_cards(old_limit);
638 }
639 


































640 size_t G1DirtyCardQueueSet::max_cards() const {
641   return _max_cards;
642 }
643 
644 void G1DirtyCardQueueSet::set_max_cards(size_t value) {
645   _max_cards = value;
646   Atomic::store(&amp;_padded_max_cards, value);
647 }
648 
649 void G1DirtyCardQueueSet::set_max_cards_padding(size_t padding) {
650   // Compute sum, clipping to max.
651   size_t limit = _max_cards + padding;
652   if (limit &lt; padding) {        // Check for overflow.
653     limit = MaxCardsUnlimited;
654   }
655   Atomic::store(&amp;_padded_max_cards, limit);
656 }
657 
658 void G1DirtyCardQueueSet::discard_max_cards_padding() {
659   // Being racy here is okay, since all threads store the same value.
</pre>
</td>
<td>
<hr />
<pre>
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1BufferNodeList.hpp&quot;
 27 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;
 28 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
<span class="line-added"> 29 #include &quot;gc/g1/g1ConcurrentRefineStats.hpp&quot;</span>
 30 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
 31 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 32 #include &quot;gc/g1/g1FreeIdSet.hpp&quot;
 33 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
 34 #include &quot;gc/g1/g1RemSet.hpp&quot;
 35 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
 36 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 37 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
 38 #include &quot;memory/iterator.hpp&quot;
 39 #include &quot;runtime/atomic.hpp&quot;
<span class="line-added"> 40 #include &quot;runtime/mutex.hpp&quot;</span>
<span class="line-added"> 41 #include &quot;runtime/mutexLocker.hpp&quot;</span>
 42 #include &quot;runtime/os.hpp&quot;
 43 #include &quot;runtime/safepoint.hpp&quot;
 44 #include &quot;runtime/thread.inline.hpp&quot;
 45 #include &quot;runtime/threadSMR.hpp&quot;
 46 #include &quot;utilities/globalCounter.inline.hpp&quot;
 47 #include &quot;utilities/macros.hpp&quot;
 48 #include &quot;utilities/quickSort.hpp&quot;
<span class="line-added"> 49 #include &quot;utilities/ticks.hpp&quot;</span>
 50 
 51 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 52   // Dirty card queues are always active, so we create them with their
 53   // active field set to true.
<span class="line-modified"> 54   PtrQueue(qset, true /* active */),</span>
<span class="line-added"> 55   _refinement_stats(new G1ConcurrentRefineStats())</span>
 56 { }
 57 
 58 G1DirtyCardQueue::~G1DirtyCardQueue() {
 59   flush();
<span class="line-added"> 60   delete _refinement_stats;</span>
<span class="line-added"> 61 }</span>
<span class="line-added"> 62 </span>
<span class="line-added"> 63 void G1DirtyCardQueue::flush() {</span>
<span class="line-added"> 64   _refinement_stats-&gt;inc_dirtied_cards(size());</span>
<span class="line-added"> 65   flush_impl();</span>
<span class="line-added"> 66 }</span>
<span class="line-added"> 67 </span>
<span class="line-added"> 68 void G1DirtyCardQueue::on_thread_detach() {</span>
<span class="line-added"> 69   assert(this == &amp;G1ThreadLocalData::dirty_card_queue(Thread::current()), &quot;precondition&quot;);</span>
<span class="line-added"> 70   flush();</span>
<span class="line-added"> 71   dirty_card_qset()-&gt;record_detached_refinement_stats(_refinement_stats);</span>
 72 }
 73 
 74 void G1DirtyCardQueue::handle_completed_buffer() {
 75   assert(!is_empty(), &quot;precondition&quot;);
<span class="line-added"> 76   _refinement_stats-&gt;inc_dirtied_cards(size());</span>
 77   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
 78   allocate_buffer();
<span class="line-modified"> 79   dirty_card_qset()-&gt;handle_completed_buffer(node, _refinement_stats);</span>
 80 }
 81 
 82 // Assumed to be zero by concurrent threads.
 83 static uint par_ids_start() { return 0; }
 84 
 85 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 86   PtrQueueSet(allocator),
 87   _primary_refinement_thread(NULL),
 88   _num_cards(0),
 89   _completed(),
 90   _paused(),
 91   _free_ids(par_ids_start(), num_par_ids()),
 92   _process_cards_threshold(ProcessCardsThresholdNever),
 93   _max_cards(MaxCardsUnlimited),
 94   _padded_max_cards(MaxCardsUnlimited),
<span class="line-modified"> 95   _detached_refinement_stats()</span>
 96 {

 97   _all_active = true;
 98 }
 99 
100 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
101   abandon_completed_buffers();

102 }
103 
104 // Determines how many mutator threads can process the buffers in parallel.
105 uint G1DirtyCardQueueSet::num_par_ids() {
106   return (uint)os::initial_active_processor_count();
107 }
108 








109 void G1DirtyCardQueueSet::handle_zero_index_for_thread(Thread* t) {
110   G1ThreadLocalData::dirty_card_queue(t).handle_zero_index();
111 }
112 
113 #ifdef ASSERT
114 G1DirtyCardQueueSet::Queue::~Queue() {
115   assert(_head == NULL, &quot;precondition&quot;);
116   assert(_tail == NULL, &quot;precondition&quot;);
117 }
118 #endif // ASSERT
119 
120 BufferNode* G1DirtyCardQueueSet::Queue::top() const {
121   return Atomic::load(&amp;_head);
122 }
123 
124 // An append operation atomically exchanges the new tail with the queue tail.
125 // It then sets the &quot;next&quot; value of the old tail to the head of the list being
126 // appended; it is an invariant that the old tail&#39;s &quot;next&quot; value is NULL.
127 // But if the old tail is NULL then the queue was empty.  In this case the
128 // head of the list being appended is instead stored in the queue head; it is
</pre>
<hr />
<pre>
413   if (from._head != NULL) {
414     Atomic::add(&amp;_num_cards, from._entry_count);
415     _completed.append(*from._head, *from._tail);
416   }
417 }
418 
419 G1BufferNodeList G1DirtyCardQueueSet::take_all_completed_buffers() {
420   enqueue_all_paused_buffers();
421   verify_num_cards();
422   HeadTail buffers = _completed.take_all();
423   size_t num_cards = Atomic::load(&amp;_num_cards);
424   Atomic::store(&amp;_num_cards, size_t(0));
425   return G1BufferNodeList(buffers._head, buffers._tail, num_cards);
426 }
427 
428 class G1RefineBufferedCards : public StackObj {
429   BufferNode* const _node;
430   CardTable::CardValue** const _node_buffer;
431   const size_t _node_buffer_size;
432   const uint _worker_id;
<span class="line-modified">433   G1ConcurrentRefineStats* _stats;</span>
434   G1RemSet* const _g1rs;
435 
436   static inline int compare_card(const CardTable::CardValue* p1,
437                                  const CardTable::CardValue* p2) {
438     return p2 - p1;
439   }
440 
441   // Sorts the cards from start_index to _node_buffer_size in *decreasing*
442   // address order. Tests showed that this order is preferable to not sorting
443   // or increasing address order.
444   void sort_cards(size_t start_index) {
445     QuickSort::sort(&amp;_node_buffer[start_index],
446                     _node_buffer_size - start_index,
447                     compare_card,
448                     false);
449   }
450 
451   // Returns the index to the first clean card in the buffer.
452   size_t clean_cards() {
453     const size_t start = _node-&gt;index();
</pre>
<hr />
<pre>
463     assert(src &lt;= dst, &quot;invariant&quot;);
464     for ( ; src &lt; dst; ++src) {
465       // Search low to high for a card to keep.
466       if (_g1rs-&gt;clean_card_before_refine(src)) {
467         // Found keeper.  Search high to low for a card to discard.
468         while (src &lt; --dst) {
469           if (!_g1rs-&gt;clean_card_before_refine(dst)) {
470             *dst = *src;         // Replace discard with keeper.
471             break;
472           }
473         }
474         // If discard search failed (src == dst), the outer loop will also end.
475       }
476     }
477 
478     // dst points to the first retained clean card, or the end of the buffer
479     // if all the cards were discarded.
480     const size_t first_clean = dst - _node_buffer;
481     assert(first_clean &gt;= start &amp;&amp; first_clean &lt;= _node_buffer_size, &quot;invariant&quot;);
482     // Discarded cards are considered as refined.
<span class="line-modified">483     _stats-&gt;inc_refined_cards(first_clean - start);</span>
<span class="line-added">484     _stats-&gt;inc_precleaned_cards(first_clean - start);</span>
485     return first_clean;
486   }
487 
488   bool refine_cleaned_cards(size_t start_index) {
489     bool result = true;
490     size_t i = start_index;
491     for ( ; i &lt; _node_buffer_size; ++i) {
492       if (SuspendibleThreadSet::should_yield()) {
493         redirty_unrefined_cards(i);
494         result = false;
495         break;
496       }
497       _g1rs-&gt;refine_card_concurrently(_node_buffer[i], _worker_id);
498     }
499     _node-&gt;set_index(i);
<span class="line-modified">500     _stats-&gt;inc_refined_cards(i - start_index);</span>
501     return result;
502   }
503 
504   void redirty_unrefined_cards(size_t start) {
505     for ( ; start &lt; _node_buffer_size; ++start) {
506       *_node_buffer[start] = G1CardTable::dirty_card_val();
507     }
508   }
509 
510 public:
511   G1RefineBufferedCards(BufferNode* node,
512                         size_t node_buffer_size,
513                         uint worker_id,
<span class="line-modified">514                         G1ConcurrentRefineStats* stats) :</span>
515     _node(node),
516     _node_buffer(reinterpret_cast&lt;CardTable::CardValue**&gt;(BufferNode::make_buffer_from_node(node))),
517     _node_buffer_size(node_buffer_size),
518     _worker_id(worker_id),
<span class="line-modified">519     _stats(stats),</span>
520     _g1rs(G1CollectedHeap::heap()-&gt;rem_set()) {}
521 
522   bool refine() {
523     size_t first_clean_index = clean_cards();
524     if (first_clean_index == _node_buffer_size) {
525       _node-&gt;set_index(first_clean_index);
526       return true;
527     }
528     // This fence serves two purposes. First, the cards must be cleaned
529     // before processing the contents. Second, we can&#39;t proceed with
530     // processing a region until after the read of the region&#39;s top in
531     // collect_and_clean_cards(), for synchronization with possibly concurrent
532     // humongous object allocation (see comment at the StoreStore fence before
533     // setting the regions&#39; tops in humongous allocation path).
534     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
535     // wrto each other. We need both set, in any order, to proceed.
536     OrderAccess::fence();
537     sort_cards(first_clean_index);
538     return refine_cleaned_cards(first_clean_index);
539   }
540 };
541 
542 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
543                                         uint worker_id,
<span class="line-modified">544                                         G1ConcurrentRefineStats* stats) {</span>
<span class="line-added">545   Ticks start_time = Ticks::now();</span>
546   G1RefineBufferedCards buffered_cards(node,
547                                        buffer_size(),
548                                        worker_id,
<span class="line-modified">549                                        stats);</span>
<span class="line-modified">550   bool result = buffered_cards.refine();</span>
<span class="line-added">551   stats-&gt;inc_refinement_time(Ticks::now() - start_time);</span>
<span class="line-added">552   return result;</span>
553 }
554 
555 void G1DirtyCardQueueSet::handle_refined_buffer(BufferNode* node,
556                                                 bool fully_processed) {
557   if (fully_processed) {
558     assert(node-&gt;index() == buffer_size(),
559            &quot;Buffer not fully consumed: index: &quot; SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,
560            node-&gt;index(), buffer_size());
561     deallocate_buffer(node);
562   } else {
563     assert(node-&gt;index() &lt; buffer_size(), &quot;Buffer fully consumed.&quot;);
564     // Buffer incompletely processed because there is a pending safepoint.
565     // Record partially processed buffer, to be finished later.
566     record_paused_buffer(node);
567   }
568 }
569 
<span class="line-modified">570 void G1DirtyCardQueueSet::handle_completed_buffer(BufferNode* new_node,</span>
<span class="line-added">571                                                   G1ConcurrentRefineStats* stats) {</span>
572   enqueue_completed_buffer(new_node);
573 
574   // No need for mutator refinement if number of cards is below limit.
575   if (Atomic::load(&amp;_num_cards) &lt;= Atomic::load(&amp;_padded_max_cards)) {
576     return;
577   }
578 
579   // Only Java threads perform mutator refinement.
580   if (!Thread::current()-&gt;is_Java_thread()) {
581     return;
582   }
583 
584   BufferNode* node = get_completed_buffer();
585   if (node == NULL) return;     // Didn&#39;t get a buffer to process.
586 
587   // Refine cards in buffer.
588 
589   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
<span class="line-modified">590   bool fully_processed = refine_buffer(node, worker_id, stats);</span>


591   _free_ids.release_par_id(worker_id); // release the id
592 
593   // Deal with buffer after releasing id, to let another thread use id.
594   handle_refined_buffer(node, fully_processed);
595 }
596 
597 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
598                                                                size_t stop_at,
<span class="line-modified">599                                                                G1ConcurrentRefineStats* stats) {</span>
600   // Not enough cards to trigger processing.
601   if (Atomic::load(&amp;_num_cards) &lt;= stop_at) return false;
602 
603   BufferNode* node = get_completed_buffer();
604   if (node == NULL) return false; // Didn&#39;t get a buffer to process.
605 
<span class="line-modified">606   bool fully_processed = refine_buffer(node, worker_id, stats);</span>
607   handle_refined_buffer(node, fully_processed);
608   return true;
609 }
610 
611 void G1DirtyCardQueueSet::abandon_logs() {
612   assert_at_safepoint();
613   abandon_completed_buffers();
<span class="line-added">614   _detached_refinement_stats.reset();</span>
615 
616   // Since abandon is done only at safepoints, we can safely manipulate
617   // these queues.
618   struct AbandonThreadLogClosure : public ThreadClosure {
619     virtual void do_thread(Thread* t) {
<span class="line-modified">620       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);</span>
<span class="line-added">621       dcq.reset();</span>
<span class="line-added">622       dcq.refinement_stats()-&gt;reset();</span>
623     }
624   } closure;
625   Threads::threads_do(&amp;closure);
626 
627   G1BarrierSet::shared_dirty_card_queue().reset();
628 }
629 
630 void G1DirtyCardQueueSet::concatenate_logs() {
631   // Iterate over all the threads, if we find a partial log add it to
632   // the global list of logs.  Temporarily turn off the limit on the number
633   // of outstanding buffers.
634   assert_at_safepoint();
635   size_t old_limit = max_cards();
636   set_max_cards(MaxCardsUnlimited);
637 
638   struct ConcatenateThreadLogClosure : public ThreadClosure {
639     virtual void do_thread(Thread* t) {
640       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
641       if (!dcq.is_empty()) {
642         dcq.flush();
643       }
644     }
645   } closure;
646   Threads::threads_do(&amp;closure);
647 
648   G1BarrierSet::shared_dirty_card_queue().flush();
649   enqueue_all_paused_buffers();
650   verify_num_cards();
651   set_max_cards(old_limit);
652 }
653 
<span class="line-added">654 G1ConcurrentRefineStats G1DirtyCardQueueSet::get_and_reset_refinement_stats() {</span>
<span class="line-added">655   assert_at_safepoint();</span>
<span class="line-added">656 </span>
<span class="line-added">657   // Since we&#39;re at a safepoint, there aren&#39;t any races with recording of</span>
<span class="line-added">658   // detached refinement stats.  In particular, there&#39;s no risk of double</span>
<span class="line-added">659   // counting a thread that detaches after we&#39;ve examined it but before</span>
<span class="line-added">660   // we&#39;ve processed the detached stats.</span>
<span class="line-added">661 </span>
<span class="line-added">662   // Collect and reset stats for attached threads.</span>
<span class="line-added">663   struct CollectStats : public ThreadClosure {</span>
<span class="line-added">664     G1ConcurrentRefineStats _total_stats;</span>
<span class="line-added">665     virtual void do_thread(Thread* t) {</span>
<span class="line-added">666       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);</span>
<span class="line-added">667       G1ConcurrentRefineStats&amp; stats = *dcq.refinement_stats();</span>
<span class="line-added">668       _total_stats += stats;</span>
<span class="line-added">669       stats.reset();</span>
<span class="line-added">670     }</span>
<span class="line-added">671   } closure;</span>
<span class="line-added">672   Threads::threads_do(&amp;closure);</span>
<span class="line-added">673 </span>
<span class="line-added">674   // Collect and reset stats from detached threads.</span>
<span class="line-added">675   MutexLocker ml(G1DetachedRefinementStats_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-added">676   closure._total_stats += _detached_refinement_stats;</span>
<span class="line-added">677   _detached_refinement_stats.reset();</span>
<span class="line-added">678 </span>
<span class="line-added">679   return closure._total_stats;</span>
<span class="line-added">680 }</span>
<span class="line-added">681 </span>
<span class="line-added">682 void G1DirtyCardQueueSet::record_detached_refinement_stats(G1ConcurrentRefineStats* stats) {</span>
<span class="line-added">683   MutexLocker ml(G1DetachedRefinementStats_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-added">684   _detached_refinement_stats += *stats;</span>
<span class="line-added">685   stats-&gt;reset();</span>
<span class="line-added">686 }</span>
<span class="line-added">687 </span>
688 size_t G1DirtyCardQueueSet::max_cards() const {
689   return _max_cards;
690 }
691 
692 void G1DirtyCardQueueSet::set_max_cards(size_t value) {
693   _max_cards = value;
694   Atomic::store(&amp;_padded_max_cards, value);
695 }
696 
697 void G1DirtyCardQueueSet::set_max_cards_padding(size_t padding) {
698   // Compute sum, clipping to max.
699   size_t limit = _max_cards + padding;
700   if (limit &lt; padding) {        // Check for overflow.
701     limit = MaxCardsUnlimited;
702   }
703   Atomic::store(&amp;_padded_max_cards, limit);
704 }
705 
706 void G1DirtyCardQueueSet::discard_max_cards_padding() {
707   // Being racy here is okay, since all threads store the same value.
</pre>
</td>
</tr>
</table>
<center><a href="g1ConcurrentRefineThread.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1DirtyCardQueue.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>