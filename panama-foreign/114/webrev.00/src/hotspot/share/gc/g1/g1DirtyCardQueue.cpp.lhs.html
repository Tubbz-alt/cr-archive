<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1BufferNodeList.hpp&quot;
 27 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;
 28 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
<a name="1" id="anc1"></a>
 29 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
 30 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 31 #include &quot;gc/g1/g1FreeIdSet.hpp&quot;
 32 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
 33 #include &quot;gc/g1/g1RemSet.hpp&quot;
 34 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
 35 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 36 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
 37 #include &quot;memory/iterator.hpp&quot;
 38 #include &quot;runtime/atomic.hpp&quot;
<a name="2" id="anc2"></a>

 39 #include &quot;runtime/os.hpp&quot;
 40 #include &quot;runtime/safepoint.hpp&quot;
 41 #include &quot;runtime/thread.inline.hpp&quot;
 42 #include &quot;runtime/threadSMR.hpp&quot;
 43 #include &quot;utilities/globalCounter.inline.hpp&quot;
 44 #include &quot;utilities/macros.hpp&quot;
 45 #include &quot;utilities/quickSort.hpp&quot;
<a name="3" id="anc3"></a>
 46 
 47 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 48   // Dirty card queues are always active, so we create them with their
 49   // active field set to true.
<a name="4" id="anc4"></a><span class="line-modified"> 50   PtrQueue(qset, true /* active */)</span>

 51 { }
 52 
 53 G1DirtyCardQueue::~G1DirtyCardQueue() {
 54   flush();
<a name="5" id="anc5"></a>











 55 }
 56 
 57 void G1DirtyCardQueue::handle_completed_buffer() {
 58   assert(!is_empty(), &quot;precondition&quot;);
<a name="6" id="anc6"></a>
 59   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
 60   allocate_buffer();
<a name="7" id="anc7"></a><span class="line-modified"> 61   dirty_card_qset()-&gt;handle_completed_buffer(node);</span>
 62 }
 63 
 64 // Assumed to be zero by concurrent threads.
 65 static uint par_ids_start() { return 0; }
 66 
 67 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 68   PtrQueueSet(allocator),
 69   _primary_refinement_thread(NULL),
 70   _num_cards(0),
 71   _completed(),
 72   _paused(),
 73   _free_ids(par_ids_start(), num_par_ids()),
 74   _process_cards_threshold(ProcessCardsThresholdNever),
 75   _max_cards(MaxCardsUnlimited),
 76   _padded_max_cards(MaxCardsUnlimited),
<a name="8" id="anc8"></a><span class="line-modified"> 77   _mutator_refined_cards_counters(NEW_C_HEAP_ARRAY(size_t, num_par_ids(), mtGC))</span>
 78 {
<a name="9" id="anc9"></a><span class="line-removed"> 79   ::memset(_mutator_refined_cards_counters, 0, num_par_ids() * sizeof(size_t));</span>
 80   _all_active = true;
 81 }
 82 
 83 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
 84   abandon_completed_buffers();
<a name="10" id="anc10"></a><span class="line-removed"> 85   FREE_C_HEAP_ARRAY(size_t, _mutator_refined_cards_counters);</span>
 86 }
 87 
 88 // Determines how many mutator threads can process the buffers in parallel.
 89 uint G1DirtyCardQueueSet::num_par_ids() {
 90   return (uint)os::initial_active_processor_count();
 91 }
 92 
<a name="11" id="anc11"></a><span class="line-removed"> 93 size_t G1DirtyCardQueueSet::total_mutator_refined_cards() const {</span>
<span class="line-removed"> 94   size_t sum = 0;</span>
<span class="line-removed"> 95   for (uint i = 0; i &lt; num_par_ids(); ++i) {</span>
<span class="line-removed"> 96     sum += _mutator_refined_cards_counters[i];</span>
<span class="line-removed"> 97   }</span>
<span class="line-removed"> 98   return sum;</span>
<span class="line-removed"> 99 }</span>
<span class="line-removed">100 </span>
101 void G1DirtyCardQueueSet::handle_zero_index_for_thread(Thread* t) {
102   G1ThreadLocalData::dirty_card_queue(t).handle_zero_index();
103 }
104 
105 #ifdef ASSERT
106 G1DirtyCardQueueSet::Queue::~Queue() {
107   assert(_head == NULL, &quot;precondition&quot;);
108   assert(_tail == NULL, &quot;precondition&quot;);
109 }
110 #endif // ASSERT
111 
112 BufferNode* G1DirtyCardQueueSet::Queue::top() const {
113   return Atomic::load(&amp;_head);
114 }
115 
116 // An append operation atomically exchanges the new tail with the queue tail.
117 // It then sets the &quot;next&quot; value of the old tail to the head of the list being
118 // appended; it is an invariant that the old tail&#39;s &quot;next&quot; value is NULL.
119 // But if the old tail is NULL then the queue was empty.  In this case the
120 // head of the list being appended is instead stored in the queue head; it is
121 // an invariant that the queue head is NULL in this case.
122 //
123 // This means there is a period between the exchange and the old tail update
124 // where the queue sequence is split into two parts, the list from the queue
125 // head to the old tail, and the list being appended.  If there are concurrent
126 // push/append operations, each may introduce another such segment.  But they
127 // all eventually get resolved by their respective updates of their old tail&#39;s
128 // &quot;next&quot; value.  This also means that pop operations must handle a buffer
129 // with a NULL &quot;next&quot; value specially.
130 //
131 // A push operation is just a degenerate append, where the buffer being pushed
132 // is both the head and the tail of the list being appended.
133 void G1DirtyCardQueueSet::Queue::append(BufferNode&amp; first, BufferNode&amp; last) {
134   assert(last.next() == NULL, &quot;precondition&quot;);
135   BufferNode* old_tail = Atomic::xchg(&amp;_tail, &amp;last);
136   if (old_tail == NULL) {       // Was empty.
137     Atomic::store(&amp;_head, &amp;first);
138   } else {
139     assert(old_tail-&gt;next() == NULL, &quot;invariant&quot;);
140     old_tail-&gt;set_next(&amp;first);
141   }
142 }
143 
144 BufferNode* G1DirtyCardQueueSet::Queue::pop() {
145   Thread* current_thread = Thread::current();
146   while (true) {
147     // Use a critical section per iteration, rather than over the whole
148     // operation.  We&#39;re not guaranteed to make progress.  Lingering in one
149     // CS could lead to excessive allocation of buffers, because the CS
150     // blocks return of released buffers to the free list for reuse.
151     GlobalCounter::CriticalSection cs(current_thread);
152 
153     BufferNode* result = Atomic::load_acquire(&amp;_head);
154     if (result == NULL) return NULL; // Queue is empty.
155 
156     BufferNode* next = Atomic::load_acquire(BufferNode::next_ptr(*result));
157     if (next != NULL) {
158       // The &quot;usual&quot; lock-free pop from the head of a singly linked list.
159       if (result == Atomic::cmpxchg(&amp;_head, result, next)) {
160         // Former head successfully taken; it is not the last.
161         assert(Atomic::load(&amp;_tail) != result, &quot;invariant&quot;);
162         assert(result-&gt;next() != NULL, &quot;invariant&quot;);
163         result-&gt;set_next(NULL);
164         return result;
165       }
166       // Lost the race; try again.
167       continue;
168     }
169 
170     // next is NULL.  This case is handled differently from the &quot;usual&quot;
171     // lock-free pop from the head of a singly linked list.
172 
173     // If _tail == result then result is the only element in the list. We can
174     // remove it from the list by first setting _tail to NULL and then setting
175     // _head to NULL, the order being important.  We set _tail with cmpxchg in
176     // case of a concurrent push/append/pop also changing _tail.  If we win
177     // then we&#39;ve claimed result.
178     if (Atomic::cmpxchg(&amp;_tail, result, (BufferNode*)NULL) == result) {
179       assert(result-&gt;next() == NULL, &quot;invariant&quot;);
180       // Now that we&#39;ve claimed result, also set _head to NULL.  But we must
181       // be careful of a concurrent push/append after we NULLed _tail, since
182       // it may have already performed its list-was-empty update of _head,
183       // which we must not overwrite.
184       Atomic::cmpxchg(&amp;_head, result, (BufferNode*)NULL);
185       return result;
186     }
187 
188     // If _head != result then we lost the race to take result; try again.
189     if (result != Atomic::load_acquire(&amp;_head)) {
190       continue;
191     }
192 
193     // An in-progress concurrent operation interfered with taking the head
194     // element when it was the only element.  A concurrent pop may have won
195     // the race to clear the tail but not yet cleared the head. Alternatively,
196     // a concurrent push/append may have changed the tail but not yet linked
197     // result-&gt;next().  We cannot take result in either case.  We don&#39;t just
198     // try again, because we could spin for a long time waiting for that
199     // concurrent operation to finish.  In the first case, returning NULL is
200     // fine; we lost the race for the only element to another thread.  We
201     // also return NULL for the second case, and let the caller cope.
202     return NULL;
203   }
204 }
205 
206 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {
207   assert_at_safepoint();
208   HeadTail result(Atomic::load(&amp;_head), Atomic::load(&amp;_tail));
209   Atomic::store(&amp;_head, (BufferNode*)NULL);
210   Atomic::store(&amp;_tail, (BufferNode*)NULL);
211   return result;
212 }
213 
214 void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {
215   assert(cbn != NULL, &quot;precondition&quot;);
216   // Increment _num_cards before adding to queue, so queue removal doesn&#39;t
217   // need to deal with _num_cards possibly going negative.
218   size_t new_num_cards = Atomic::add(&amp;_num_cards, buffer_size() - cbn-&gt;index());
219   _completed.push(*cbn);
220   if ((new_num_cards &gt; process_cards_threshold()) &amp;&amp;
221       (_primary_refinement_thread != NULL)) {
222     _primary_refinement_thread-&gt;activate();
223   }
224 }
225 
226 BufferNode* G1DirtyCardQueueSet::get_completed_buffer() {
227   BufferNode* result = _completed.pop();
228   if (result == NULL) {         // Unlikely if no paused buffers.
229     enqueue_previous_paused_buffers();
230     result = _completed.pop();
231     if (result == NULL) return NULL;
232   }
233   Atomic::sub(&amp;_num_cards, buffer_size() - result-&gt;index());
234   return result;
235 }
236 
237 #ifdef ASSERT
238 void G1DirtyCardQueueSet::verify_num_cards() const {
239   size_t actual = 0;
240   BufferNode* cur = _completed.top();
241   for ( ; cur != NULL; cur = cur-&gt;next()) {
242     actual += buffer_size() - cur-&gt;index();
243   }
244   assert(actual == Atomic::load(&amp;_num_cards),
245          &quot;Num entries in completed buffers should be &quot; SIZE_FORMAT &quot; but are &quot; SIZE_FORMAT,
246          Atomic::load(&amp;_num_cards), actual);
247 }
248 #endif // ASSERT
249 
250 G1DirtyCardQueueSet::PausedBuffers::PausedList::PausedList() :
251   _head(NULL), _tail(NULL),
252   _safepoint_id(SafepointSynchronize::safepoint_id())
253 {}
254 
255 #ifdef ASSERT
256 G1DirtyCardQueueSet::PausedBuffers::PausedList::~PausedList() {
257   assert(Atomic::load(&amp;_head) == NULL, &quot;precondition&quot;);
258   assert(_tail == NULL, &quot;precondition&quot;);
259 }
260 #endif // ASSERT
261 
262 bool G1DirtyCardQueueSet::PausedBuffers::PausedList::is_next() const {
263   assert_not_at_safepoint();
264   return _safepoint_id == SafepointSynchronize::safepoint_id();
265 }
266 
267 void G1DirtyCardQueueSet::PausedBuffers::PausedList::add(BufferNode* node) {
268   assert_not_at_safepoint();
269   assert(is_next(), &quot;precondition&quot;);
270   BufferNode* old_head = Atomic::xchg(&amp;_head, node);
271   if (old_head == NULL) {
272     assert(_tail == NULL, &quot;invariant&quot;);
273     _tail = node;
274   } else {
275     node-&gt;set_next(old_head);
276   }
277 }
278 
279 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::PausedList::take() {
280   BufferNode* head = Atomic::load(&amp;_head);
281   BufferNode* tail = _tail;
282   Atomic::store(&amp;_head, (BufferNode*)NULL);
283   _tail = NULL;
284   return HeadTail(head, tail);
285 }
286 
287 G1DirtyCardQueueSet::PausedBuffers::PausedBuffers() : _plist(NULL) {}
288 
289 #ifdef ASSERT
290 G1DirtyCardQueueSet::PausedBuffers::~PausedBuffers() {
291   assert(Atomic::load(&amp;_plist) == NULL, &quot;invariant&quot;);
292 }
293 #endif // ASSERT
294 
295 void G1DirtyCardQueueSet::PausedBuffers::add(BufferNode* node) {
296   assert_not_at_safepoint();
297   PausedList* plist = Atomic::load_acquire(&amp;_plist);
298   if (plist == NULL) {
299     // Try to install a new next list.
300     plist = new PausedList();
301     PausedList* old_plist = Atomic::cmpxchg(&amp;_plist, (PausedList*)NULL, plist);
302     if (old_plist != NULL) {
303       // Some other thread installed a new next list.  Use it instead.
304       delete plist;
305       plist = old_plist;
306     }
307   }
308   assert(plist-&gt;is_next(), &quot;invariant&quot;);
309   plist-&gt;add(node);
310 }
311 
312 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_previous() {
313   assert_not_at_safepoint();
314   PausedList* previous;
315   {
316     // Deal with plist in a critical section, to prevent it from being
317     // deleted out from under us by a concurrent take_previous().
318     GlobalCounter::CriticalSection cs(Thread::current());
319     previous = Atomic::load_acquire(&amp;_plist);
320     if ((previous == NULL) ||   // Nothing to take.
321         previous-&gt;is_next() ||  // Not from a previous safepoint.
322         // Some other thread stole it.
323         (Atomic::cmpxchg(&amp;_plist, previous, (PausedList*)NULL) != previous)) {
324       return HeadTail();
325     }
326   }
327   // We now own previous.
328   HeadTail result = previous-&gt;take();
329   // There might be other threads examining previous (in concurrent
330   // take_previous()).  Synchronize to wait until any such threads are
331   // done with such examination before deleting.
332   GlobalCounter::write_synchronize();
333   delete previous;
334   return result;
335 }
336 
337 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_all() {
338   assert_at_safepoint();
339   HeadTail result;
340   PausedList* plist = Atomic::load(&amp;_plist);
341   if (plist != NULL) {
342     Atomic::store(&amp;_plist, (PausedList*)NULL);
343     result = plist-&gt;take();
344     delete plist;
345   }
346   return result;
347 }
348 
349 void G1DirtyCardQueueSet::record_paused_buffer(BufferNode* node) {
350   assert_not_at_safepoint();
351   assert(node-&gt;next() == NULL, &quot;precondition&quot;);
352   // Ensure there aren&#39;t any paused buffers from a previous safepoint.
353   enqueue_previous_paused_buffers();
354   // Cards for paused buffers are included in count, to contribute to
355   // notification checking after the coming safepoint if it doesn&#39;t GC.
356   // Note that this means the queue&#39;s _num_cards differs from the number
357   // of cards in the queued buffers when there are paused buffers.
358   Atomic::add(&amp;_num_cards, buffer_size() - node-&gt;index());
359   _paused.add(node);
360 }
361 
362 void G1DirtyCardQueueSet::enqueue_paused_buffers_aux(const HeadTail&amp; paused) {
363   if (paused._head != NULL) {
364     assert(paused._tail != NULL, &quot;invariant&quot;);
365     // Cards from paused buffers are already recorded in the queue count.
366     _completed.append(*paused._head, *paused._tail);
367   }
368 }
369 
370 void G1DirtyCardQueueSet::enqueue_previous_paused_buffers() {
371   assert_not_at_safepoint();
372   enqueue_paused_buffers_aux(_paused.take_previous());
373 }
374 
375 void G1DirtyCardQueueSet::enqueue_all_paused_buffers() {
376   assert_at_safepoint();
377   enqueue_paused_buffers_aux(_paused.take_all());
378 }
379 
380 void G1DirtyCardQueueSet::abandon_completed_buffers() {
381   enqueue_all_paused_buffers();
382   verify_num_cards();
383   G1BufferNodeList list = take_all_completed_buffers();
384   BufferNode* buffers_to_delete = list._head;
385   while (buffers_to_delete != NULL) {
386     BufferNode* bn = buffers_to_delete;
387     buffers_to_delete = bn-&gt;next();
388     bn-&gt;set_next(NULL);
389     deallocate_buffer(bn);
390   }
391 }
392 
393 void G1DirtyCardQueueSet::notify_if_necessary() {
394   if ((_primary_refinement_thread != NULL) &amp;&amp;
395       (num_cards() &gt; process_cards_threshold())) {
396     _primary_refinement_thread-&gt;activate();
397   }
398 }
399 
400 // Merge lists of buffers. The source queue set is emptied as a
401 // result. The queue sets must share the same allocator.
402 void G1DirtyCardQueueSet::merge_bufferlists(G1RedirtyCardsQueueSet* src) {
403   assert(allocator() == src-&gt;allocator(), &quot;precondition&quot;);
404   const G1BufferNodeList from = src-&gt;take_all_completed_buffers();
405   if (from._head != NULL) {
406     Atomic::add(&amp;_num_cards, from._entry_count);
407     _completed.append(*from._head, *from._tail);
408   }
409 }
410 
411 G1BufferNodeList G1DirtyCardQueueSet::take_all_completed_buffers() {
412   enqueue_all_paused_buffers();
413   verify_num_cards();
414   HeadTail buffers = _completed.take_all();
415   size_t num_cards = Atomic::load(&amp;_num_cards);
416   Atomic::store(&amp;_num_cards, size_t(0));
417   return G1BufferNodeList(buffers._head, buffers._tail, num_cards);
418 }
419 
420 class G1RefineBufferedCards : public StackObj {
421   BufferNode* const _node;
422   CardTable::CardValue** const _node_buffer;
423   const size_t _node_buffer_size;
424   const uint _worker_id;
<a name="12" id="anc12"></a><span class="line-modified">425   size_t* _total_refined_cards;</span>
426   G1RemSet* const _g1rs;
427 
428   static inline int compare_card(const CardTable::CardValue* p1,
429                                  const CardTable::CardValue* p2) {
430     return p2 - p1;
431   }
432 
433   // Sorts the cards from start_index to _node_buffer_size in *decreasing*
434   // address order. Tests showed that this order is preferable to not sorting
435   // or increasing address order.
436   void sort_cards(size_t start_index) {
437     QuickSort::sort(&amp;_node_buffer[start_index],
438                     _node_buffer_size - start_index,
439                     compare_card,
440                     false);
441   }
442 
443   // Returns the index to the first clean card in the buffer.
444   size_t clean_cards() {
445     const size_t start = _node-&gt;index();
446     assert(start &lt;= _node_buffer_size, &quot;invariant&quot;);
447 
448     // Two-fingered compaction algorithm similar to the filtering mechanism in
449     // SATBMarkQueue. The main difference is that clean_card_before_refine()
450     // could change the buffer element in-place.
451     // We don&#39;t check for SuspendibleThreadSet::should_yield(), because
452     // cleaning and redirtying the cards is fast.
453     CardTable::CardValue** src = &amp;_node_buffer[start];
454     CardTable::CardValue** dst = &amp;_node_buffer[_node_buffer_size];
455     assert(src &lt;= dst, &quot;invariant&quot;);
456     for ( ; src &lt; dst; ++src) {
457       // Search low to high for a card to keep.
458       if (_g1rs-&gt;clean_card_before_refine(src)) {
459         // Found keeper.  Search high to low for a card to discard.
460         while (src &lt; --dst) {
461           if (!_g1rs-&gt;clean_card_before_refine(dst)) {
462             *dst = *src;         // Replace discard with keeper.
463             break;
464           }
465         }
466         // If discard search failed (src == dst), the outer loop will also end.
467       }
468     }
469 
470     // dst points to the first retained clean card, or the end of the buffer
471     // if all the cards were discarded.
472     const size_t first_clean = dst - _node_buffer;
473     assert(first_clean &gt;= start &amp;&amp; first_clean &lt;= _node_buffer_size, &quot;invariant&quot;);
474     // Discarded cards are considered as refined.
<a name="13" id="anc13"></a><span class="line-modified">475     *_total_refined_cards += first_clean - start;</span>

476     return first_clean;
477   }
478 
479   bool refine_cleaned_cards(size_t start_index) {
480     bool result = true;
481     size_t i = start_index;
482     for ( ; i &lt; _node_buffer_size; ++i) {
483       if (SuspendibleThreadSet::should_yield()) {
484         redirty_unrefined_cards(i);
485         result = false;
486         break;
487       }
488       _g1rs-&gt;refine_card_concurrently(_node_buffer[i], _worker_id);
489     }
490     _node-&gt;set_index(i);
<a name="14" id="anc14"></a><span class="line-modified">491     *_total_refined_cards += i - start_index;</span>
492     return result;
493   }
494 
495   void redirty_unrefined_cards(size_t start) {
496     for ( ; start &lt; _node_buffer_size; ++start) {
497       *_node_buffer[start] = G1CardTable::dirty_card_val();
498     }
499   }
500 
501 public:
502   G1RefineBufferedCards(BufferNode* node,
503                         size_t node_buffer_size,
504                         uint worker_id,
<a name="15" id="anc15"></a><span class="line-modified">505                         size_t* total_refined_cards) :</span>
506     _node(node),
507     _node_buffer(reinterpret_cast&lt;CardTable::CardValue**&gt;(BufferNode::make_buffer_from_node(node))),
508     _node_buffer_size(node_buffer_size),
509     _worker_id(worker_id),
<a name="16" id="anc16"></a><span class="line-modified">510     _total_refined_cards(total_refined_cards),</span>
511     _g1rs(G1CollectedHeap::heap()-&gt;rem_set()) {}
512 
513   bool refine() {
514     size_t first_clean_index = clean_cards();
515     if (first_clean_index == _node_buffer_size) {
516       _node-&gt;set_index(first_clean_index);
517       return true;
518     }
519     // This fence serves two purposes. First, the cards must be cleaned
520     // before processing the contents. Second, we can&#39;t proceed with
521     // processing a region until after the read of the region&#39;s top in
522     // collect_and_clean_cards(), for synchronization with possibly concurrent
523     // humongous object allocation (see comment at the StoreStore fence before
524     // setting the regions&#39; tops in humongous allocation path).
525     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
526     // wrto each other. We need both set, in any order, to proceed.
527     OrderAccess::fence();
528     sort_cards(first_clean_index);
529     return refine_cleaned_cards(first_clean_index);
530   }
531 };
532 
533 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
534                                         uint worker_id,
<a name="17" id="anc17"></a><span class="line-modified">535                                         size_t* total_refined_cards) {</span>

536   G1RefineBufferedCards buffered_cards(node,
537                                        buffer_size(),
538                                        worker_id,
<a name="18" id="anc18"></a><span class="line-modified">539                                        total_refined_cards);</span>
<span class="line-modified">540   return buffered_cards.refine();</span>


541 }
542 
543 void G1DirtyCardQueueSet::handle_refined_buffer(BufferNode* node,
544                                                 bool fully_processed) {
545   if (fully_processed) {
546     assert(node-&gt;index() == buffer_size(),
547            &quot;Buffer not fully consumed: index: &quot; SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,
548            node-&gt;index(), buffer_size());
549     deallocate_buffer(node);
550   } else {
551     assert(node-&gt;index() &lt; buffer_size(), &quot;Buffer fully consumed.&quot;);
552     // Buffer incompletely processed because there is a pending safepoint.
553     // Record partially processed buffer, to be finished later.
554     record_paused_buffer(node);
555   }
556 }
557 
<a name="19" id="anc19"></a><span class="line-modified">558 void G1DirtyCardQueueSet::handle_completed_buffer(BufferNode* new_node) {</span>

559   enqueue_completed_buffer(new_node);
560 
561   // No need for mutator refinement if number of cards is below limit.
562   if (Atomic::load(&amp;_num_cards) &lt;= Atomic::load(&amp;_padded_max_cards)) {
563     return;
564   }
565 
566   // Only Java threads perform mutator refinement.
567   if (!Thread::current()-&gt;is_Java_thread()) {
568     return;
569   }
570 
571   BufferNode* node = get_completed_buffer();
572   if (node == NULL) return;     // Didn&#39;t get a buffer to process.
573 
574   // Refine cards in buffer.
575 
576   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
<a name="20" id="anc20"></a><span class="line-modified">577   uint counter_index = worker_id - par_ids_start();</span>
<span class="line-removed">578   size_t* counter = &amp;_mutator_refined_cards_counters[counter_index];</span>
<span class="line-removed">579   bool fully_processed = refine_buffer(node, worker_id, counter);</span>
580   _free_ids.release_par_id(worker_id); // release the id
581 
582   // Deal with buffer after releasing id, to let another thread use id.
583   handle_refined_buffer(node, fully_processed);
584 }
585 
586 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
587                                                                size_t stop_at,
<a name="21" id="anc21"></a><span class="line-modified">588                                                                size_t* total_refined_cards) {</span>
589   // Not enough cards to trigger processing.
590   if (Atomic::load(&amp;_num_cards) &lt;= stop_at) return false;
591 
592   BufferNode* node = get_completed_buffer();
593   if (node == NULL) return false; // Didn&#39;t get a buffer to process.
594 
<a name="22" id="anc22"></a><span class="line-modified">595   bool fully_processed = refine_buffer(node, worker_id, total_refined_cards);</span>
596   handle_refined_buffer(node, fully_processed);
597   return true;
598 }
599 
600 void G1DirtyCardQueueSet::abandon_logs() {
601   assert_at_safepoint();
602   abandon_completed_buffers();
<a name="23" id="anc23"></a>
603 
604   // Since abandon is done only at safepoints, we can safely manipulate
605   // these queues.
606   struct AbandonThreadLogClosure : public ThreadClosure {
607     virtual void do_thread(Thread* t) {
<a name="24" id="anc24"></a><span class="line-modified">608       G1ThreadLocalData::dirty_card_queue(t).reset();</span>


609     }
610   } closure;
611   Threads::threads_do(&amp;closure);
612 
613   G1BarrierSet::shared_dirty_card_queue().reset();
614 }
615 
616 void G1DirtyCardQueueSet::concatenate_logs() {
617   // Iterate over all the threads, if we find a partial log add it to
618   // the global list of logs.  Temporarily turn off the limit on the number
619   // of outstanding buffers.
620   assert_at_safepoint();
621   size_t old_limit = max_cards();
622   set_max_cards(MaxCardsUnlimited);
623 
624   struct ConcatenateThreadLogClosure : public ThreadClosure {
625     virtual void do_thread(Thread* t) {
626       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
627       if (!dcq.is_empty()) {
628         dcq.flush();
629       }
630     }
631   } closure;
632   Threads::threads_do(&amp;closure);
633 
634   G1BarrierSet::shared_dirty_card_queue().flush();
635   enqueue_all_paused_buffers();
636   verify_num_cards();
637   set_max_cards(old_limit);
638 }
639 
<a name="25" id="anc25"></a>

































640 size_t G1DirtyCardQueueSet::max_cards() const {
641   return _max_cards;
642 }
643 
644 void G1DirtyCardQueueSet::set_max_cards(size_t value) {
645   _max_cards = value;
646   Atomic::store(&amp;_padded_max_cards, value);
647 }
648 
649 void G1DirtyCardQueueSet::set_max_cards_padding(size_t padding) {
650   // Compute sum, clipping to max.
651   size_t limit = _max_cards + padding;
652   if (limit &lt; padding) {        // Check for overflow.
653     limit = MaxCardsUnlimited;
654   }
655   Atomic::store(&amp;_padded_max_cards, limit);
656 }
657 
658 void G1DirtyCardQueueSet::discard_max_cards_padding() {
659   // Being racy here is okay, since all threads store the same value.
660   if (_max_cards != Atomic::load(&amp;_padded_max_cards)) {
661     Atomic::store(&amp;_padded_max_cards, _max_cards);
662   }
663 }
<a name="26" id="anc26"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="26" type="hidden" />
</body>
</html>