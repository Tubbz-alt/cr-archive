<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/gc/g1/heapRegion.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;code/nmethod.hpp&quot;
 27 #include &quot;gc/g1/g1BlockOffsetTable.inline.hpp&quot;
 28 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 29 #include &quot;gc/g1/g1CollectionSet.hpp&quot;
 30 #include &quot;gc/g1/g1HeapRegionTraceType.hpp&quot;
 31 #include &quot;gc/g1/g1NUMA.hpp&quot;
 32 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
 33 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
 34 #include &quot;gc/g1/heapRegionBounds.inline.hpp&quot;
 35 #include &quot;gc/g1/heapRegionManager.inline.hpp&quot;
 36 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 37 #include &quot;gc/g1/heapRegionTracer.hpp&quot;
 38 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
 39 #include &quot;logging/log.hpp&quot;
 40 #include &quot;logging/logStream.hpp&quot;
 41 #include &quot;memory/iterator.inline.hpp&quot;
 42 #include &quot;memory/resourceArea.hpp&quot;
 43 #include &quot;oops/access.inline.hpp&quot;
 44 #include &quot;oops/compressedOops.inline.hpp&quot;
 45 #include &quot;oops/oop.inline.hpp&quot;
 46 #include &quot;utilities/powerOfTwo.hpp&quot;
 47 
 48 int    HeapRegion::LogOfHRGrainBytes = 0;
 49 int    HeapRegion::LogOfHRGrainWords = 0;
 50 int    HeapRegion::LogCardsPerRegion = 0;
 51 size_t HeapRegion::GrainBytes        = 0;
 52 size_t HeapRegion::GrainWords        = 0;
 53 size_t HeapRegion::CardsPerRegion    = 0;
 54 
 55 size_t HeapRegion::max_region_size() {
 56   return HeapRegionBounds::max_size();
 57 }
 58 
 59 size_t HeapRegion::min_region_size_in_words() {
 60   return HeapRegionBounds::min_size() &gt;&gt; LogHeapWordSize;
 61 }
 62 
 63 void HeapRegion::setup_heap_region_size(size_t max_heap_size) {
 64   size_t region_size = G1HeapRegionSize;
 65   // G1HeapRegionSize = 0 means decide ergonomically.
 66   if (region_size == 0) {
 67     region_size = MAX2(max_heap_size / HeapRegionBounds::target_number(),
 68                        HeapRegionBounds::min_size());
 69   }
 70 
 71   // Make sure region size is a power of 2. Rounding up since this
 72   // is beneficial in most cases.
 73   region_size = round_up_power_of_2(region_size);
 74 
 75   // Now make sure that we don&#39;t go over or under our limits.
 76   region_size = clamp(region_size, HeapRegionBounds::min_size(), HeapRegionBounds::max_size());
 77 
 78   // Calculate the log for the region size.
 79   int region_size_log = exact_log2_long((jlong)region_size);
 80 
 81   // Now, set up the globals.
 82   guarantee(LogOfHRGrainBytes == 0, &quot;we should only set it once&quot;);
 83   LogOfHRGrainBytes = region_size_log;
 84 
 85   guarantee(LogOfHRGrainWords == 0, &quot;we should only set it once&quot;);
 86   LogOfHRGrainWords = LogOfHRGrainBytes - LogHeapWordSize;
 87 
 88   guarantee(GrainBytes == 0, &quot;we should only set it once&quot;);
 89   // The cast to int is safe, given that we&#39;ve bounded region_size by
 90   // MIN_REGION_SIZE and MAX_REGION_SIZE.
 91   GrainBytes = region_size;
 92   log_info(gc, heap)(&quot;Heap region size: &quot; SIZE_FORMAT &quot;M&quot;, GrainBytes / M);
 93 
 94   guarantee(GrainWords == 0, &quot;we should only set it once&quot;);
 95   GrainWords = GrainBytes &gt;&gt; LogHeapWordSize;
 96   guarantee((size_t) 1 &lt;&lt; LogOfHRGrainWords == GrainWords, &quot;sanity&quot;);
 97 
 98   guarantee(CardsPerRegion == 0, &quot;we should only set it once&quot;);
 99   CardsPerRegion = GrainBytes &gt;&gt; G1CardTable::card_shift;
100 
101   LogCardsPerRegion = log2_long((jlong) CardsPerRegion);
102 
103   if (G1HeapRegionSize != GrainBytes) {
104     FLAG_SET_ERGO(G1HeapRegionSize, GrainBytes);
105   }
106 }
107 
108 void HeapRegion::handle_evacuation_failure() {
109   uninstall_surv_rate_group();
110   clear_young_index_in_cset();
111   set_evacuation_failed(false);
112   set_old();
113 }
114 
115 void HeapRegion::unlink_from_list() {
116   set_next(NULL);
117   set_prev(NULL);
118   set_containing_set(NULL);
119 }
120 
121 void HeapRegion::hr_clear(bool clear_space) {
122   assert(_humongous_start_region == NULL,
123          &quot;we should have already filtered out humongous regions&quot;);
124   assert(!in_collection_set(),
125          &quot;Should not clear heap region %u in the collection set&quot;, hrm_index());
126 
127   clear_young_index_in_cset();
128   clear_index_in_opt_cset();
129   uninstall_surv_rate_group();
130   set_free();
131   reset_pre_dummy_top();
132 
133   rem_set()-&gt;clear_locked();
134 
135   zero_marked_bytes();
136 
137   init_top_at_mark_start();
138   if (clear_space) clear(SpaceDecorator::Mangle);
139 
140   _evacuation_failed = false;
141   _gc_efficiency = 0.0;
142 }
143 
144 void HeapRegion::clear_cardtable() {
145   G1CardTable* ct = G1CollectedHeap::heap()-&gt;card_table();
146   ct-&gt;clear(MemRegion(bottom(), end()));
147 }
148 
149 void HeapRegion::calc_gc_efficiency() {
150   // GC efficiency is the ratio of how much space would be
151   // reclaimed over how long we predict it would take to reclaim it.
152   G1Policy* policy = G1CollectedHeap::heap()-&gt;policy();
153 
154   // Retrieve a prediction of the elapsed time for this region for
155   // a mixed gc because the region will only be evacuated during a
156   // mixed gc.
157   double region_elapsed_time_ms = policy-&gt;predict_region_total_time_ms(this, false /* for_young_gc */);
158   _gc_efficiency = (double) reclaimable_bytes() / region_elapsed_time_ms;
159 }
160 
161 void HeapRegion::set_free() {
162   report_region_type_change(G1HeapRegionTraceType::Free);
163   _type.set_free();
164 }
165 
166 void HeapRegion::set_eden() {
167   report_region_type_change(G1HeapRegionTraceType::Eden);
168   _type.set_eden();
169 }
170 
171 void HeapRegion::set_eden_pre_gc() {
172   report_region_type_change(G1HeapRegionTraceType::Eden);
173   _type.set_eden_pre_gc();
174 }
175 
176 void HeapRegion::set_survivor() {
177   report_region_type_change(G1HeapRegionTraceType::Survivor);
178   _type.set_survivor();
179 }
180 
181 void HeapRegion::move_to_old() {
182   if (_type.relabel_as_old()) {
183     report_region_type_change(G1HeapRegionTraceType::Old);
184   }
185 }
186 
187 void HeapRegion::set_old() {
188   report_region_type_change(G1HeapRegionTraceType::Old);
189   _type.set_old();
190 }
191 
192 void HeapRegion::set_open_archive() {
193   report_region_type_change(G1HeapRegionTraceType::OpenArchive);
194   _type.set_open_archive();
195 }
196 
197 void HeapRegion::set_closed_archive() {
198   report_region_type_change(G1HeapRegionTraceType::ClosedArchive);
199   _type.set_closed_archive();
200 }
201 
202 void HeapRegion::set_starts_humongous(HeapWord* obj_top, size_t fill_size) {
203   assert(!is_humongous(), &quot;sanity / pre-condition&quot;);
204   assert(top() == bottom(), &quot;should be empty&quot;);
205 
206   report_region_type_change(G1HeapRegionTraceType::StartsHumongous);
207   _type.set_starts_humongous();
208   _humongous_start_region = this;
209 
210   _bot_part.set_for_starts_humongous(obj_top, fill_size);
211 }
212 
213 void HeapRegion::set_continues_humongous(HeapRegion* first_hr) {
214   assert(!is_humongous(), &quot;sanity / pre-condition&quot;);
215   assert(top() == bottom(), &quot;should be empty&quot;);
216   assert(first_hr-&gt;is_starts_humongous(), &quot;pre-condition&quot;);
217 
218   report_region_type_change(G1HeapRegionTraceType::ContinuesHumongous);
219   _type.set_continues_humongous();
220   _humongous_start_region = first_hr;
221 
222   _bot_part.set_object_can_span(true);
223 }
224 
225 void HeapRegion::clear_humongous() {
226   assert(is_humongous(), &quot;pre-condition&quot;);
227 
228   assert(capacity() == HeapRegion::GrainBytes, &quot;pre-condition&quot;);
229   _humongous_start_region = NULL;
230 
231   _bot_part.set_object_can_span(false);
232 }
233 
234 HeapRegion::HeapRegion(uint hrm_index,
235                        G1BlockOffsetTable* bot,
236                        MemRegion mr) :
237   _bottom(mr.start()),
238   _end(mr.end()),
239   _top(NULL),
240   _compaction_top(NULL),
241   _bot_part(bot, this),
242   _par_alloc_lock(Mutex::leaf, &quot;HeapRegion par alloc lock&quot;, true),
243   _pre_dummy_top(NULL),
244   _rem_set(NULL),
245   _hrm_index(hrm_index),
246   _type(),
247   _humongous_start_region(NULL),
248   _evacuation_failed(false),
249   _index_in_opt_cset(InvalidCSetIndex),
250   _next(NULL), _prev(NULL),
251 #ifdef ASSERT
252   _containing_set(NULL),
253 #endif
254   _prev_top_at_mark_start(NULL), _next_top_at_mark_start(NULL),
255   _prev_marked_bytes(0), _next_marked_bytes(0),
256   _young_index_in_cset(-1),
257   _surv_rate_group(NULL), _age_index(G1SurvRateGroup::InvalidAgeIndex), _gc_efficiency(0.0),
258   _node_index(G1NUMA::UnknownNodeIndex)
259 {
260   assert(Universe::on_page_boundary(mr.start()) &amp;&amp; Universe::on_page_boundary(mr.end()),
261          &quot;invalid space boundaries&quot;);
262 
263   _rem_set = new HeapRegionRemSet(bot, this);
264   initialize();
265 }
266 
267 void HeapRegion::initialize(bool clear_space, bool mangle_space) {
268   assert(_rem_set-&gt;is_empty(), &quot;Remembered set must be empty&quot;);
269 
270   if (clear_space) {
271     clear(mangle_space);
272   }
273 
274   set_top(bottom());
275   set_compaction_top(bottom());
276   reset_bot();
277 
278   hr_clear(false /*clear_space*/);
279 }
280 
281 void HeapRegion::report_region_type_change(G1HeapRegionTraceType::Type to) {
282   HeapRegionTracer::send_region_type_change(_hrm_index,
283                                             get_trace_type(),
284                                             to,
285                                             (uintptr_t)bottom(),
286                                             used());
287 }
288 
289 void HeapRegion::note_self_forwarding_removal_start(bool during_initial_mark,
290                                                     bool during_conc_mark) {
291   // We always recreate the prev marking info and we&#39;ll explicitly
292   // mark all objects we find to be self-forwarded on the prev
293   // bitmap. So all objects need to be below PTAMS.
294   _prev_marked_bytes = 0;
295 
296   if (during_initial_mark) {
297     // During initial-mark, we&#39;ll also explicitly mark all objects
298     // we find to be self-forwarded on the next bitmap. So all
299     // objects need to be below NTAMS.
300     _next_top_at_mark_start = top();
301     _next_marked_bytes = 0;
302   } else if (during_conc_mark) {
303     // During concurrent mark, all objects in the CSet (including
304     // the ones we find to be self-forwarded) are implicitly live.
305     // So all objects need to be above NTAMS.
306     _next_top_at_mark_start = bottom();
307     _next_marked_bytes = 0;
308   }
309 }
310 
311 void HeapRegion::note_self_forwarding_removal_end(size_t marked_bytes) {
312   assert(marked_bytes &lt;= used(),
313          &quot;marked: &quot; SIZE_FORMAT &quot; used: &quot; SIZE_FORMAT, marked_bytes, used());
314   _prev_top_at_mark_start = top();
315   _prev_marked_bytes = marked_bytes;
316 }
317 
318 // Code roots support
319 
320 void HeapRegion::add_strong_code_root(nmethod* nm) {
321   HeapRegionRemSet* hrrs = rem_set();
322   hrrs-&gt;add_strong_code_root(nm);
323 }
324 
325 void HeapRegion::add_strong_code_root_locked(nmethod* nm) {
326   assert_locked_or_safepoint(CodeCache_lock);
327   HeapRegionRemSet* hrrs = rem_set();
328   hrrs-&gt;add_strong_code_root_locked(nm);
329 }
330 
331 void HeapRegion::remove_strong_code_root(nmethod* nm) {
332   HeapRegionRemSet* hrrs = rem_set();
333   hrrs-&gt;remove_strong_code_root(nm);
334 }
335 
336 void HeapRegion::strong_code_roots_do(CodeBlobClosure* blk) const {
337   HeapRegionRemSet* hrrs = rem_set();
338   hrrs-&gt;strong_code_roots_do(blk);
339 }
340 
341 class VerifyStrongCodeRootOopClosure: public OopClosure {
342   const HeapRegion* _hr;
343   bool _failures;
344   bool _has_oops_in_region;
345 
346   template &lt;class T&gt; void do_oop_work(T* p) {
347     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
348     if (!CompressedOops::is_null(heap_oop)) {
349       oop obj = CompressedOops::decode_not_null(heap_oop);
350 
351       // Note: not all the oops embedded in the nmethod are in the
352       // current region. We only look at those which are.
353       if (_hr-&gt;is_in(obj)) {
354         // Object is in the region. Check that its less than top
355         if (_hr-&gt;top() &lt;= cast_from_oop&lt;HeapWord*&gt;(obj)) {
356           // Object is above top
357           log_error(gc, verify)(&quot;Object &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT &quot; is above top &quot;,
358                                 p2i(obj), HR_FORMAT_PARAMS(_hr));
359           _failures = true;
360           return;
361         }
362         // Nmethod has at least one oop in the current region
363         _has_oops_in_region = true;
364       }
365     }
366   }
367 
368 public:
369   VerifyStrongCodeRootOopClosure(const HeapRegion* hr):
370     _hr(hr), _failures(false), _has_oops_in_region(false) {}
371 
372   void do_oop(narrowOop* p) { do_oop_work(p); }
373   void do_oop(oop* p)       { do_oop_work(p); }
374 
375   bool failures()           { return _failures; }
376   bool has_oops_in_region() { return _has_oops_in_region; }
377 };
378 
379 class VerifyStrongCodeRootCodeBlobClosure: public CodeBlobClosure {
380   const HeapRegion* _hr;
381   bool _failures;
382 public:
383   VerifyStrongCodeRootCodeBlobClosure(const HeapRegion* hr) :
384     _hr(hr), _failures(false) {}
385 
386   void do_code_blob(CodeBlob* cb) {
387     nmethod* nm = (cb == NULL) ? NULL : cb-&gt;as_compiled_method()-&gt;as_nmethod_or_null();
388     if (nm != NULL) {
389       // Verify that the nemthod is live
390       if (!nm-&gt;is_alive()) {
391         log_error(gc, verify)(&quot;region [&quot; PTR_FORMAT &quot;,&quot; PTR_FORMAT &quot;] has dead nmethod &quot; PTR_FORMAT &quot; in its strong code roots&quot;,
392                               p2i(_hr-&gt;bottom()), p2i(_hr-&gt;end()), p2i(nm));
393         _failures = true;
394       } else {
395         VerifyStrongCodeRootOopClosure oop_cl(_hr);
396         nm-&gt;oops_do(&amp;oop_cl);
397         if (!oop_cl.has_oops_in_region()) {
398           log_error(gc, verify)(&quot;region [&quot; PTR_FORMAT &quot;,&quot; PTR_FORMAT &quot;] has nmethod &quot; PTR_FORMAT &quot; in its strong code roots with no pointers into region&quot;,
399                                 p2i(_hr-&gt;bottom()), p2i(_hr-&gt;end()), p2i(nm));
400           _failures = true;
401         } else if (oop_cl.failures()) {
402           log_error(gc, verify)(&quot;region [&quot; PTR_FORMAT &quot;,&quot; PTR_FORMAT &quot;] has other failures for nmethod &quot; PTR_FORMAT,
403                                 p2i(_hr-&gt;bottom()), p2i(_hr-&gt;end()), p2i(nm));
404           _failures = true;
405         }
406       }
407     }
408   }
409 
410   bool failures()       { return _failures; }
411 };
412 
413 void HeapRegion::verify_strong_code_roots(VerifyOption vo, bool* failures) const {
414   if (!G1VerifyHeapRegionCodeRoots) {
415     // We&#39;re not verifying code roots.
416     return;
417   }
418   if (vo == VerifyOption_G1UseFullMarking) {
419     // Marking verification during a full GC is performed after class
420     // unloading, code cache unloading, etc so the strong code roots
421     // attached to each heap region are in an inconsistent state. They won&#39;t
422     // be consistent until the strong code roots are rebuilt after the
423     // actual GC. Skip verifying the strong code roots in this particular
424     // time.
425     assert(VerifyDuringGC, &quot;only way to get here&quot;);
426     return;
427   }
428 
429   HeapRegionRemSet* hrrs = rem_set();
430   size_t strong_code_roots_length = hrrs-&gt;strong_code_roots_list_length();
431 
432   // if this region is empty then there should be no entries
433   // on its strong code root list
434   if (is_empty()) {
435     if (strong_code_roots_length &gt; 0) {
436       log_error(gc, verify)(&quot;region &quot; HR_FORMAT &quot; is empty but has &quot; SIZE_FORMAT &quot; code root entries&quot;,
437                             HR_FORMAT_PARAMS(this), strong_code_roots_length);
438       *failures = true;
439     }
440     return;
441   }
442 
443   if (is_continues_humongous()) {
444     if (strong_code_roots_length &gt; 0) {
445       log_error(gc, verify)(&quot;region &quot; HR_FORMAT &quot; is a continuation of a humongous region but has &quot; SIZE_FORMAT &quot; code root entries&quot;,
446                             HR_FORMAT_PARAMS(this), strong_code_roots_length);
447       *failures = true;
448     }
449     return;
450   }
451 
452   VerifyStrongCodeRootCodeBlobClosure cb_cl(this);
453   strong_code_roots_do(&amp;cb_cl);
454 
455   if (cb_cl.failures()) {
456     *failures = true;
457   }
458 }
459 
460 void HeapRegion::print() const { print_on(tty); }
461 
462 void HeapRegion::print_on(outputStream* st) const {
463   st-&gt;print(&quot;|%4u&quot;, this-&gt;_hrm_index);
464   st-&gt;print(&quot;|&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT,
465             p2i(bottom()), p2i(top()), p2i(end()));
466   st-&gt;print(&quot;|%3d%%&quot;, (int) ((double) used() * 100 / capacity()));
467   st-&gt;print(&quot;|%2s&quot;, get_short_type_str());
468   if (in_collection_set()) {
469     st-&gt;print(&quot;|CS&quot;);
470   } else {
471     st-&gt;print(&quot;|  &quot;);
472   }
473   st-&gt;print(&quot;|TAMS &quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;| %s &quot;,
474                p2i(prev_top_at_mark_start()), p2i(next_top_at_mark_start()), rem_set()-&gt;get_state_str());
475   if (UseNUMA) {
476     G1NUMA* numa = G1NUMA::numa();
477     if (node_index() &lt; numa-&gt;num_active_nodes()) {
478       st-&gt;print(&quot;|%d&quot;, numa-&gt;numa_id(node_index()));
479     } else {
480       st-&gt;print(&quot;|-&quot;);
481     }
482   }
483   st-&gt;print_cr(&quot;&quot;);
484 }
485 
486 class G1VerificationClosure : public BasicOopIterateClosure {
487 protected:
488   G1CollectedHeap* _g1h;
489   G1CardTable *_ct;
490   oop _containing_obj;
491   bool _failures;
492   int _n_failures;
493   VerifyOption _vo;
494 public:
495   // _vo == UsePrevMarking -&gt; use &quot;prev&quot; marking information,
496   // _vo == UseNextMarking -&gt; use &quot;next&quot; marking information,
497   // _vo == UseFullMarking -&gt; use &quot;next&quot; marking bitmap but no TAMS.
498   G1VerificationClosure(G1CollectedHeap* g1h, VerifyOption vo) :
499     _g1h(g1h), _ct(g1h-&gt;card_table()),
500     _containing_obj(NULL), _failures(false), _n_failures(0), _vo(vo) {
501   }
502 
503   void set_containing_obj(oop obj) {
504     _containing_obj = obj;
505   }
506 
507   bool failures() { return _failures; }
508   int n_failures() { return _n_failures; }
509 
510   void print_object(outputStream* out, oop obj) {
511 #ifdef PRODUCT
512     Klass* k = obj-&gt;klass();
513     const char* class_name = k-&gt;external_name();
514     out-&gt;print_cr(&quot;class name %s&quot;, class_name);
515 #else // PRODUCT
516     obj-&gt;print_on(out);
517 #endif // PRODUCT
518   }
519 
520   // This closure provides its own oop verification code.
521   debug_only(virtual bool should_verify_oops() { return false; })
522 };
523 
524 class VerifyLiveClosure : public G1VerificationClosure {
525 public:
526   VerifyLiveClosure(G1CollectedHeap* g1h, VerifyOption vo) : G1VerificationClosure(g1h, vo) {}
527   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
528   virtual void do_oop(oop* p) { do_oop_work(p); }
529 
530   template &lt;class T&gt;
531   void do_oop_work(T* p) {
532     assert(_containing_obj != NULL, &quot;Precondition&quot;);
533     assert(!_g1h-&gt;is_obj_dead_cond(_containing_obj, _vo),
534       &quot;Precondition&quot;);
535     verify_liveness(p);
536   }
537 
538   template &lt;class T&gt;
539   void verify_liveness(T* p) {
540     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
541     Log(gc, verify) log;
542     if (!CompressedOops::is_null(heap_oop)) {
543       oop obj = CompressedOops::decode_not_null(heap_oop);
544       bool failed = false;
545       if (!_g1h-&gt;is_in(obj) || _g1h-&gt;is_obj_dead_cond(obj, _vo)) {
546         MutexLocker x(ParGCRareEvent_lock,
547           Mutex::_no_safepoint_check_flag);
548 
549         if (!_failures) {
550           log.error(&quot;----------&quot;);
551         }
552         ResourceMark rm;
553         if (!_g1h-&gt;is_in(obj)) {
554           HeapRegion* from = _g1h-&gt;heap_region_containing((HeapWord*)p);
555           log.error(&quot;Field &quot; PTR_FORMAT &quot; of live obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
556                     p2i(p), p2i(_containing_obj), HR_FORMAT_PARAMS(from));
557           LogStream ls(log.error());
558           print_object(&amp;ls, _containing_obj);
559           HeapRegion* const to = _g1h-&gt;heap_region_containing(obj);
560           log.error(&quot;points to obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT &quot; remset %s&quot;,
561                     p2i(obj), HR_FORMAT_PARAMS(to), to-&gt;rem_set()-&gt;get_state_str());
562         } else {
563           HeapRegion* from = _g1h-&gt;heap_region_containing((HeapWord*)p);
564           HeapRegion* to = _g1h-&gt;heap_region_containing(obj);
565           log.error(&quot;Field &quot; PTR_FORMAT &quot; of live obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
566                     p2i(p), p2i(_containing_obj), HR_FORMAT_PARAMS(from));
567           LogStream ls(log.error());
568           print_object(&amp;ls, _containing_obj);
569           log.error(&quot;points to dead obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
570                     p2i(obj), HR_FORMAT_PARAMS(to));
571           print_object(&amp;ls, obj);
572         }
573         log.error(&quot;----------&quot;);
574         _failures = true;
575         failed = true;
576         _n_failures++;
577       }
578     }
579   }
580 };
581 
582 class VerifyRemSetClosure : public G1VerificationClosure {
583 public:
584   VerifyRemSetClosure(G1CollectedHeap* g1h, VerifyOption vo) : G1VerificationClosure(g1h, vo) {}
585   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
586   virtual void do_oop(oop* p) { do_oop_work(p); }
587 
588   template &lt;class T&gt;
589   void do_oop_work(T* p) {
590     assert(_containing_obj != NULL, &quot;Precondition&quot;);
591     assert(!_g1h-&gt;is_obj_dead_cond(_containing_obj, _vo),
592       &quot;Precondition&quot;);
593     verify_remembered_set(p);
594   }
595 
596   template &lt;class T&gt;
597   void verify_remembered_set(T* p) {
598     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
599     Log(gc, verify) log;
600     if (!CompressedOops::is_null(heap_oop)) {
601       oop obj = CompressedOops::decode_not_null(heap_oop);
602       HeapRegion* from = _g1h-&gt;heap_region_containing((HeapWord*)p);
603       HeapRegion* to = _g1h-&gt;heap_region_containing(obj);
604       if (from != NULL &amp;&amp; to != NULL &amp;&amp;
605         from != to &amp;&amp;
606         !to-&gt;is_pinned() &amp;&amp;
607         to-&gt;rem_set()-&gt;is_complete()) {
608         jbyte cv_obj = *_ct-&gt;byte_for_const(_containing_obj);
609         jbyte cv_field = *_ct-&gt;byte_for_const(p);
610         const jbyte dirty = G1CardTable::dirty_card_val();
611 
612         bool is_bad = !(from-&gt;is_young()
613           || to-&gt;rem_set()-&gt;contains_reference(p)
614           || (_containing_obj-&gt;is_objArray() ?
615                 cv_field == dirty :
616                 cv_obj == dirty || cv_field == dirty));
617         if (is_bad) {
618           MutexLocker x(ParGCRareEvent_lock,
619             Mutex::_no_safepoint_check_flag);
620 
621           if (!_failures) {
622             log.error(&quot;----------&quot;);
623           }
624           log.error(&quot;Missing rem set entry:&quot;);
625           log.error(&quot;Field &quot; PTR_FORMAT &quot; of obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
626                     p2i(p), p2i(_containing_obj), HR_FORMAT_PARAMS(from));
627           ResourceMark rm;
628           LogStream ls(log.error());
629           _containing_obj-&gt;print_on(&amp;ls);
630           log.error(&quot;points to obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT &quot; remset %s&quot;,
631                     p2i(obj), HR_FORMAT_PARAMS(to), to-&gt;rem_set()-&gt;get_state_str());
632           if (oopDesc::is_oop(obj)) {
633             obj-&gt;print_on(&amp;ls);
634           }
635           log.error(&quot;Obj head CTE = %d, field CTE = %d.&quot;, cv_obj, cv_field);
636           log.error(&quot;----------&quot;);
637           _failures = true;
638           _n_failures++;
639         }
640       }
641     }
642   }
643 };
644 
645 // Closure that applies the given two closures in sequence.
646 class G1Mux2Closure : public BasicOopIterateClosure {
647   OopClosure* _c1;
648   OopClosure* _c2;
649 public:
650   G1Mux2Closure(OopClosure *c1, OopClosure *c2) { _c1 = c1; _c2 = c2; }
651   template &lt;class T&gt; inline void do_oop_work(T* p) {
652     // Apply first closure; then apply the second.
653     _c1-&gt;do_oop(p);
654     _c2-&gt;do_oop(p);
655   }
656   virtual inline void do_oop(oop* p) { do_oop_work(p); }
657   virtual inline void do_oop(narrowOop* p) { do_oop_work(p); }
658 
659   // This closure provides its own oop verification code.
660   debug_only(virtual bool should_verify_oops() { return false; })
661 };
662 
663 void HeapRegion::verify(VerifyOption vo,
664                         bool* failures) const {
665   G1CollectedHeap* g1h = G1CollectedHeap::heap();
666   *failures = false;
667   HeapWord* p = bottom();
668   HeapWord* prev_p = NULL;
669   VerifyLiveClosure vl_cl(g1h, vo);
670   VerifyRemSetClosure vr_cl(g1h, vo);
671   bool is_region_humongous = is_humongous();
672   size_t object_num = 0;
673   while (p &lt; top()) {
674     oop obj = oop(p);
675     size_t obj_size = block_size(p);
676     object_num += 1;
677 
678     if (!g1h-&gt;is_obj_dead_cond(obj, this, vo)) {
679       if (oopDesc::is_oop(obj)) {
680         Klass* klass = obj-&gt;klass();
681         bool is_metaspace_object = Metaspace::contains(klass);
682         if (!is_metaspace_object) {
683           log_error(gc, verify)(&quot;klass &quot; PTR_FORMAT &quot; of object &quot; PTR_FORMAT &quot; &quot;
684                                 &quot;not metadata&quot;, p2i(klass), p2i(obj));
685           *failures = true;
686           return;
687         } else if (!klass-&gt;is_klass()) {
688           log_error(gc, verify)(&quot;klass &quot; PTR_FORMAT &quot; of object &quot; PTR_FORMAT &quot; &quot;
689                                 &quot;not a klass&quot;, p2i(klass), p2i(obj));
690           *failures = true;
691           return;
692         } else {
693           vl_cl.set_containing_obj(obj);
694           if (!g1h-&gt;collector_state()-&gt;in_full_gc() || G1VerifyRSetsDuringFullGC) {
695             // verify liveness and rem_set
696             vr_cl.set_containing_obj(obj);
697             G1Mux2Closure mux(&amp;vl_cl, &amp;vr_cl);
698             obj-&gt;oop_iterate(&amp;mux);
699 
700             if (vr_cl.failures()) {
701               *failures = true;
702             }
703             if (G1MaxVerifyFailures &gt;= 0 &amp;&amp;
704               vr_cl.n_failures() &gt;= G1MaxVerifyFailures) {
705               return;
706             }
707           } else {
708             // verify only liveness
709             obj-&gt;oop_iterate(&amp;vl_cl);
710           }
711           if (vl_cl.failures()) {
712             *failures = true;
713           }
714           if (G1MaxVerifyFailures &gt;= 0 &amp;&amp;
715               vl_cl.n_failures() &gt;= G1MaxVerifyFailures) {
716             return;
717           }
718         }
719       } else {
720         log_error(gc, verify)(PTR_FORMAT &quot; not an oop&quot;, p2i(obj));
721         *failures = true;
722         return;
723       }
724     }
725     prev_p = p;
726     p += obj_size;
727   }
728 
729   if (!is_young() &amp;&amp; !is_empty()) {
730     _bot_part.verify();
731   }
732 
733   if (is_region_humongous) {
734     oop obj = oop(this-&gt;humongous_start_region()-&gt;bottom());
735     if (cast_from_oop&lt;HeapWord*&gt;(obj) &gt; bottom() || cast_from_oop&lt;HeapWord*&gt;(obj) + obj-&gt;size() &lt; bottom()) {
736       log_error(gc, verify)(&quot;this humongous region is not part of its&#39; humongous object &quot; PTR_FORMAT, p2i(obj));
737       *failures = true;
738       return;
739     }
740   }
741 
742   if (!is_region_humongous &amp;&amp; p != top()) {
743     log_error(gc, verify)(&quot;end of last object &quot; PTR_FORMAT &quot; &quot;
744                           &quot;does not match top &quot; PTR_FORMAT, p2i(p), p2i(top()));
745     *failures = true;
746     return;
747   }
748 
749   HeapWord* the_end = end();
750   // Do some extra BOT consistency checking for addresses in the
751   // range [top, end). BOT look-ups in this range should yield
752   // top. No point in doing that if top == end (there&#39;s nothing there).
753   if (p &lt; the_end) {
754     // Look up top
755     HeapWord* addr_1 = p;
756     HeapWord* b_start_1 = block_start_const(addr_1);
757     if (b_start_1 != p) {
758       log_error(gc, verify)(&quot;BOT look up for top: &quot; PTR_FORMAT &quot; &quot;
759                             &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
760                             p2i(addr_1), p2i(b_start_1), p2i(p));
761       *failures = true;
762       return;
763     }
764 
765     // Look up top + 1
766     HeapWord* addr_2 = p + 1;
767     if (addr_2 &lt; the_end) {
768       HeapWord* b_start_2 = block_start_const(addr_2);
769       if (b_start_2 != p) {
770         log_error(gc, verify)(&quot;BOT look up for top + 1: &quot; PTR_FORMAT &quot; &quot;
771                               &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
772                               p2i(addr_2), p2i(b_start_2), p2i(p));
773         *failures = true;
774         return;
775       }
776     }
777 
778     // Look up an address between top and end
779     size_t diff = pointer_delta(the_end, p) / 2;
780     HeapWord* addr_3 = p + diff;
781     if (addr_3 &lt; the_end) {
782       HeapWord* b_start_3 = block_start_const(addr_3);
783       if (b_start_3 != p) {
784         log_error(gc, verify)(&quot;BOT look up for top + diff: &quot; PTR_FORMAT &quot; &quot;
785                               &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
786                               p2i(addr_3), p2i(b_start_3), p2i(p));
787         *failures = true;
788         return;
789       }
790     }
791 
792     // Look up end - 1
793     HeapWord* addr_4 = the_end - 1;
794     HeapWord* b_start_4 = block_start_const(addr_4);
795     if (b_start_4 != p) {
796       log_error(gc, verify)(&quot;BOT look up for end - 1: &quot; PTR_FORMAT &quot; &quot;
797                             &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
798                             p2i(addr_4), p2i(b_start_4), p2i(p));
799       *failures = true;
800       return;
801     }
802   }
803 
804   verify_strong_code_roots(vo, failures);
805 }
806 
807 void HeapRegion::verify() const {
808   bool dummy = false;
809   verify(VerifyOption_G1UsePrevMarking, /* failures */ &amp;dummy);
810 }
811 
812 void HeapRegion::verify_rem_set(VerifyOption vo, bool* failures) const {
813   G1CollectedHeap* g1h = G1CollectedHeap::heap();
814   *failures = false;
815   HeapWord* p = bottom();
816   HeapWord* prev_p = NULL;
817   VerifyRemSetClosure vr_cl(g1h, vo);
818   while (p &lt; top()) {
819     oop obj = oop(p);
820     size_t obj_size = block_size(p);
821 
822     if (!g1h-&gt;is_obj_dead_cond(obj, this, vo)) {
823       if (oopDesc::is_oop(obj)) {
824         vr_cl.set_containing_obj(obj);
825         obj-&gt;oop_iterate(&amp;vr_cl);
826 
827         if (vr_cl.failures()) {
828           *failures = true;
829         }
830         if (G1MaxVerifyFailures &gt;= 0 &amp;&amp;
831           vr_cl.n_failures() &gt;= G1MaxVerifyFailures) {
832           return;
833         }
834       } else {
835         log_error(gc, verify)(PTR_FORMAT &quot; not an oop&quot;, p2i(obj));
836         *failures = true;
837         return;
838       }
839     }
840 
841     prev_p = p;
842     p += obj_size;
843   }
844 }
845 
846 void HeapRegion::verify_rem_set() const {
847   bool failures = false;
848   verify_rem_set(VerifyOption_G1UsePrevMarking, &amp;failures);
849   guarantee(!failures, &quot;HeapRegion RemSet verification failed&quot;);
850 }
851 
852 void HeapRegion::clear(bool mangle_space) {
853   set_top(bottom());
854   set_compaction_top(bottom());
855 
856   if (ZapUnusedHeapArea &amp;&amp; mangle_space) {
857     mangle_unused_area();
858   }
859   reset_bot();
860 }
861 
862 #ifndef PRODUCT
863 void HeapRegion::mangle_unused_area() {
864   SpaceMangler::mangle_region(MemRegion(top(), end()));
865 }
866 #endif
867 
868 HeapWord* HeapRegion::initialize_threshold() {
869   return _bot_part.initialize_threshold();
870 }
871 
872 HeapWord* HeapRegion::cross_threshold(HeapWord* start, HeapWord* end) {
873   _bot_part.alloc_block(start, end);
874   return _bot_part.threshold();
875 }
876 
877 void HeapRegion::object_iterate(ObjectClosure* blk) {
878   HeapWord* p = bottom();
879   while (p &lt; top()) {
880     if (block_is_obj(p)) {
881       blk-&gt;do_object(oop(p));
882     }
883     p += block_size(p);
884   }
885 }
    </pre>
  </body>
</html>