<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/metaspaceShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="metaspace/virtualSpaceNode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspaceShared.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  52 #include &quot;memory/universe.hpp&quot;
  53 #include &quot;oops/compressedOops.inline.hpp&quot;
  54 #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  55 #include &quot;oops/instanceMirrorKlass.hpp&quot;
  56 #include &quot;oops/instanceRefKlass.hpp&quot;
  57 #include &quot;oops/methodData.hpp&quot;
  58 #include &quot;oops/objArrayKlass.hpp&quot;
  59 #include &quot;oops/objArrayOop.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;oops/typeArrayKlass.hpp&quot;
  62 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  63 #include &quot;runtime/handles.inline.hpp&quot;
  64 #include &quot;runtime/os.hpp&quot;
  65 #include &quot;runtime/safepointVerifiers.hpp&quot;
  66 #include &quot;runtime/signature.hpp&quot;
  67 #include &quot;runtime/timerTrace.hpp&quot;
  68 #include &quot;runtime/vmThread.hpp&quot;
  69 #include &quot;runtime/vmOperations.hpp&quot;
  70 #include &quot;utilities/align.hpp&quot;
  71 #include &quot;utilities/bitMap.inline.hpp&quot;

  72 #include &quot;utilities/defaultStream.hpp&quot;
  73 #include &quot;utilities/hashtable.inline.hpp&quot;
  74 #if INCLUDE_G1GC
  75 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  76 #endif
  77 
  78 ReservedSpace MetaspaceShared::_shared_rs;
  79 VirtualSpace MetaspaceShared::_shared_vs;
  80 ReservedSpace MetaspaceShared::_symbol_rs;
  81 VirtualSpace MetaspaceShared::_symbol_vs;
  82 MetaspaceSharedStats MetaspaceShared::_stats;
  83 bool MetaspaceShared::_has_error_classes;
  84 bool MetaspaceShared::_archive_loading_failed = false;
  85 bool MetaspaceShared::_remapped_readwrite = false;
  86 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  87 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  88 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  89 intx MetaspaceShared::_relocation_delta;
  90 
  91 // The CDS archive is divided into the following regions:
</pre>
<hr />
<pre>
 174                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 175   if (strcmp(_name, failing_region) == 0) {
 176     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 177   }
 178 }
 179 
 180 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
 181   _rs = rs;
 182   _vs = vs;
 183   // Start with 0 committed bytes. The memory will be committed as needed by
 184   // MetaspaceShared::commit_to().
 185   if (!_vs-&gt;initialize(*_rs, 0)) {
 186     fatal(&quot;Unable to allocate memory for shared space&quot;);
 187   }
 188   _base = _top = _rs-&gt;base();
 189   _end = _rs-&gt;end();
 190 }
 191 
 192 void DumpRegion::pack(DumpRegion* next) {
 193   assert(!is_packed(), &quot;sanity&quot;);
<span class="line-modified"> 194   _end = (char*)align_up(_top, Metaspace::reserve_alignment());</span>
 195   _is_packed = true;
 196   if (next != NULL) {
 197     next-&gt;_rs = _rs;
 198     next-&gt;_vs = _vs;
 199     next-&gt;_base = next-&gt;_top = this-&gt;_end;
 200     next-&gt;_end = _rs-&gt;end();
 201   }
 202 }
 203 
 204 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);
 205 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 206 
 207 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
 208   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);
 209 }
 210 
 211 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 212   return &amp;_mc_region;
 213 }
 214 
</pre>
<hr />
<pre>
 220   return &amp;_ro_region;
 221 }
 222 
 223 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 224                                       ReservedSpace* rs) {
 225   current-&gt;pack(next);
 226 }
 227 
 228 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
 229   return _symbol_region.allocate(num_bytes);
 230 }
 231 
 232 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 233   return _mc_region.allocate(num_bytes);
 234 }
 235 
 236 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 237   return _ro_region.allocate(num_bytes);
 238 }
 239 
<span class="line-modified"> 240 // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:</span>
<span class="line-removed"> 241 // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,</span>
<span class="line-removed"> 242 //                                    while keeping the first range at offset 0 of this range.</span>
<span class="line-removed"> 243 // Metaspace::reserve_alignment()  -- so we can pass the region to</span>
<span class="line-removed"> 244 //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.</span>
<span class="line-removed"> 245 size_t MetaspaceShared::reserved_space_alignment() {</span>
<span class="line-removed"> 246   size_t os_align = os::vm_allocation_granularity();</span>
<span class="line-removed"> 247   size_t ms_align = Metaspace::reserve_alignment();</span>
<span class="line-removed"> 248   if (os_align &gt;= ms_align) {</span>
<span class="line-removed"> 249     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-removed"> 250     return os_align;</span>
<span class="line-removed"> 251   } else {</span>
<span class="line-removed"> 252     assert(ms_align % os_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-removed"> 253     return ms_align;</span>
<span class="line-removed"> 254   }</span>
<span class="line-removed"> 255 }</span>
 256 
<span class="line-modified"> 257 ReservedSpace MetaspaceShared::reserve_shared_space(size_t size, char* requested_address) {</span>
<span class="line-modified"> 258   return Metaspace::reserve_space(size, reserved_space_alignment(),</span>
<span class="line-modified"> 259                                   requested_address, requested_address != NULL);</span>









 260 }

 261 
 262 void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
 263   assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified"> 264   const size_t reserve_alignment = reserved_space_alignment();</span>





 265   char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
 266 
 267 #ifdef _LP64
<span class="line-modified"> 268   // On 64-bit VM, the heap and class space layout will be the same as if</span>
<span class="line-modified"> 269   // you&#39;re running in -Xshare:on mode:</span>
<span class="line-modified"> 270   //</span>
<span class="line-modified"> 271   //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="line-removed"> 272   //                              v</span>
<span class="line-removed"> 273   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-removed"> 274   // |    Heap    | Archive |     | MC | RW | RO |    class space     |</span>
<span class="line-removed"> 275   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-removed"> 276   // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="line-removed"> 277   //</span>
 278   const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
 279   const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
 280 #else
<span class="line-modified"> 281   // We don&#39;t support archives larger than 256MB on 32-bit due to limited virtual address space.</span>

 282   size_t cds_total = align_down(256*M, reserve_alignment);
 283 #endif
 284 

 285   bool use_requested_base = true;





 286   if (ArchiveRelocationMode == 1) {
 287     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
 288     use_requested_base = false;
 289   }
 290 
 291   // First try to reserve the space at the specified SharedBaseAddress.
 292   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
 293   if (use_requested_base) {
<span class="line-modified"> 294     _shared_rs = reserve_shared_space(cds_total, shared_base);</span>







 295   }
<span class="line-modified"> 296   if (_shared_rs.is_reserved()) {</span>
<span class="line-modified"> 297     assert(shared_base == 0 || _shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="line-modified"> 298   } else {</span>
<span class="line-modified"> 299     // Get a mmap region anywhere if the SharedBaseAddress fails.</span>
<span class="line-modified"> 300     _shared_rs = reserve_shared_space(cds_total);</span>









 301   }

 302   if (!_shared_rs.is_reserved()) {
 303     vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
 304                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
 305   }
 306 
 307 #ifdef _LP64
<span class="line-removed"> 308   // During dump time, we allocate 4GB (UnscaledClassSpaceMax) of space and split it up:</span>
<span class="line-removed"> 309   // + The upper 1 GB is used as the &quot;temporary compressed class space&quot; -- preload_classes()</span>
<span class="line-removed"> 310   //   will store Klasses into this space.</span>
<span class="line-removed"> 311   // + The lower 3 GB is used for the archive -- when preload_classes() is done,</span>
<span class="line-removed"> 312   //   ArchiveCompactor will copy the class metadata into this space, first the RW parts,</span>
<span class="line-removed"> 313   //   then the RO parts.</span>
<span class="line-removed"> 314 </span>
<span class="line-removed"> 315   size_t max_archive_size = align_down(cds_total * 3 / 4, reserve_alignment);</span>
<span class="line-removed"> 316   ReservedSpace tmp_class_space = _shared_rs.last_part(max_archive_size);</span>
<span class="line-removed"> 317   CompressedClassSpaceSize = align_down(tmp_class_space.size(), reserve_alignment);</span>
<span class="line-removed"> 318   _shared_rs = _shared_rs.first_part(max_archive_size);</span>
 319 
 320   if (UseCompressedClassPointers) {
<span class="line-modified"> 321     // Set up compress class pointers.</span>
<span class="line-modified"> 322     CompressedKlassPointers::set_base((address)_shared_rs.base());</span>
<span class="line-modified"> 323     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent</span>
<span class="line-modified"> 324     // with AOT.</span>
<span class="line-modified"> 325     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="line-modified"> 326     // Set the range of klass addresses to 4GB.</span>
<span class="line-modified"> 327     CompressedKlassPointers::set_range(cds_total);</span>









































 328     Metaspace::initialize_class_space(tmp_class_space);














 329   }
<span class="line-removed"> 330   log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="line-removed"> 331                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
 332 
<span class="line-removed"> 333   log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="line-removed"> 334                 CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
 335 #endif
 336 
 337   init_shared_dump_space(&amp;_mc_region);
 338   SharedBaseAddress = (size_t)_shared_rs.base();
 339   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 340                 _shared_rs.size(), p2i(_shared_rs.base()));
 341 
 342   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
 343   _symbol_rs = ReservedSpace(symbol_rs_size);
 344   if (!_symbol_rs.is_reserved()) {
 345     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,
 346                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));
 347   }
 348   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);
 349 }
 350 
 351 // Called by universe_post_init()
 352 void MetaspaceShared::post_initialize(TRAPS) {
 353   if (UseSharedSpaces) {
 354     int size = FileMapInfo::get_number_of_shared_paths();
</pre>
<hr />
<pre>
2056 
2057 bool MetaspaceShared::is_in_trampoline_frame(address addr) {
2058   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {
2059     return true;
2060   }
2061   return false;
2062 }
2063 
2064 bool MetaspaceShared::is_shared_dynamic(void* p) {
2065   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;
2066       (p &gt;= _shared_metaspace_static_top)) {
2067     return true;
2068   } else {
2069     return false;
2070   }
2071 }
2072 
2073 void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
2074   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
2075   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;

2076   FileMapInfo* static_mapinfo = open_static_archive();
2077   FileMapInfo* dynamic_mapinfo = NULL;
2078 
2079   if (static_mapinfo != NULL) {
2080     dynamic_mapinfo = open_dynamic_archive();
2081 
2082     // First try to map at the requested address
2083     result = map_archives(static_mapinfo, dynamic_mapinfo, true);
2084     if (result == MAP_ARCHIVE_MMAP_FAILURE) {
2085       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen
2086       // by the OS.
2087       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);
2088       result = map_archives(static_mapinfo, dynamic_mapinfo, false);
2089     }
2090   }
2091 
2092   if (result == MAP_ARCHIVE_SUCCESS) {
2093     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());
2094     char* cds_base = static_mapinfo-&gt;mapped_base();
2095     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();
</pre>
<hr />
<pre>
2132   }
2133   if (Arguments::GetSharedDynamicArchivePath() == NULL) {
2134     return NULL;
2135   }
2136 
2137   FileMapInfo* mapinfo = new FileMapInfo(false);
2138   if (!mapinfo-&gt;initialize()) {
2139     delete(mapinfo);
2140     return NULL;
2141   }
2142   return mapinfo;
2143 }
2144 
2145 // use_requested_addr:
2146 //  true  = map at FileMapHeader::_requested_base_address
2147 //  false = map at an alternative address picked by OS.
2148 MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
2149                                                bool use_requested_addr) {
2150   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
2151       // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="line-modified">2152       // For debug builds, the check is done in FileMapInfo::map_regions for better test coverage.</span>

2153       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2154       return MAP_ARCHIVE_MMAP_FAILURE;
2155     });
2156 
2157   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
2158     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);
2159     return MAP_ARCHIVE_MMAP_FAILURE;
2160   };
2161 
2162   if (dynamic_mapinfo != NULL) {
2163     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
2164     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
2165     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
2166   }
2167 
<span class="line-modified">2168   ReservedSpace main_rs, archive_space_rs, class_space_rs;</span>
2169   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
2170   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="line-modified">2171                                                                  use_requested_addr, main_rs, archive_space_rs,</span>
2172                                                                  class_space_rs);
2173   if (mapped_base_address == NULL) {
2174     result = MAP_ARCHIVE_MMAP_FAILURE;
2175   } else {





















2176     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2177                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
2178     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2179                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());























2180     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
2181     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
2182                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
2183 
2184     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="line-modified">2185       // This is for simulating mmap failures at the requested address. In debug builds, we do it</span>
<span class="line-modified">2186       // here (after all archives have possibly been mapped), so we can thoroughly test the code for</span>
<span class="line-modified">2187       // failure handling (releasing all allocated resource, etc).</span>

2188       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2189       if (static_result == MAP_ARCHIVE_SUCCESS) {
2190         static_result = MAP_ARCHIVE_MMAP_FAILURE;
2191       }
2192       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2193         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;
2194       }
2195     });
2196 
2197     if (static_result == MAP_ARCHIVE_SUCCESS) {
2198       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2199         result = MAP_ARCHIVE_SUCCESS;
2200       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {
2201         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);
2202         // No need to retry mapping the dynamic archive again, as it will never succeed
2203         // (bad file, etc) -- just keep the base archive.
2204         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,
2205                                   dynamic_mapinfo-&gt;full_path());
2206         result = MAP_ARCHIVE_SUCCESS;
2207         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no
2208         // easy API to do that right now.
2209       } else {
2210         result = MAP_ARCHIVE_MMAP_FAILURE;
2211       }
2212     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {
2213       result = MAP_ARCHIVE_OTHER_FAILURE;
2214     } else {
2215       result = MAP_ARCHIVE_MMAP_FAILURE;
2216     }
2217   }
2218 
2219   if (result == MAP_ARCHIVE_SUCCESS) {
<span class="line-removed">2220     if (!main_rs.is_reserved() &amp;&amp; class_space_rs.is_reserved()) {</span>
<span class="line-removed">2221       MemTracker::record_virtual_memory_type((address)class_space_rs.base(), mtClass);</span>
<span class="line-removed">2222     }</span>
2223     SharedBaseAddress = (size_t)mapped_base_address;
2224     LP64_ONLY({
2225         if (Metaspace::using_class_space()) {
<span class="line-modified">2226           assert(class_space_rs.is_reserved(), &quot;must be&quot;);</span>
<span class="line-modified">2227           char* cds_base = static_mapinfo-&gt;mapped_base();</span>
<span class="line-modified">2228           Metaspace::allocate_metaspace_compressed_klass_ptrs(class_space_rs, NULL, (address)cds_base);</span>






2229           // map_heap_regions() compares the current narrow oop and klass encodings
2230           // with the archived ones, so it must be done after all encodings are determined.
2231           static_mapinfo-&gt;map_heap_regions();
<span class="line-removed">2232           CompressedKlassPointers::set_range(CompressedClassSpaceSize);</span>
2233         }
2234       });
2235   } else {
2236     unmap_archive(static_mapinfo);
2237     unmap_archive(dynamic_mapinfo);
<span class="line-modified">2238     release_reserved_spaces(main_rs, archive_space_rs, class_space_rs);</span>
2239   }
2240 
2241   return result;
2242 }
2243 






















































2244 char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
2245                                                           FileMapInfo* dynamic_mapinfo,
<span class="line-modified">2246                                                           bool use_requested_addr,</span>
<span class="line-removed">2247                                                           ReservedSpace&amp; main_rs,</span>
2248                                                           ReservedSpace&amp; archive_space_rs,
2249                                                           ReservedSpace&amp; class_space_rs) {
<span class="line-removed">2250   const bool use_klass_space = NOT_LP64(false) LP64_ONLY(Metaspace::using_class_space());</span>
<span class="line-removed">2251   const size_t class_space_size = NOT_LP64(0) LP64_ONLY(Metaspace::compressed_class_space_size());</span>
2252 
<span class="line-modified">2253   if (use_klass_space) {</span>
<span class="line-modified">2254     assert(class_space_size &gt; 0, &quot;CompressedClassSpaceSize must have been validated&quot;);</span>
<span class="line-removed">2255   }</span>
<span class="line-removed">2256   if (use_requested_addr &amp;&amp; !is_aligned(static_mapinfo-&gt;requested_base_address(), reserved_space_alignment())) {</span>
<span class="line-removed">2257     return NULL;</span>
<span class="line-removed">2258   }</span>
2259 
2260   // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="line-modified">2261   size_t base_offset = static_mapinfo-&gt;mapping_base_offset();</span>
<span class="line-modified">2262   size_t end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">2263   assert(base_offset == 0, &quot;must be&quot;);</span>
<span class="line-modified">2264   assert(is_aligned(end_offset,  os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">2265   assert(is_aligned(base_offset, os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">2266 </span>
<span class="line-modified">2267   // In case reserved_space_alignment() != os::vm_allocation_granularity()</span>
<span class="line-modified">2268   assert((size_t)os::vm_allocation_granularity() &lt;= reserved_space_alignment(), &quot;must be&quot;);</span>
<span class="line-modified">2269   end_offset = align_up(end_offset, reserved_space_alignment());</span>
<span class="line-modified">2270 </span>
<span class="line-modified">2271   size_t archive_space_size = end_offset - base_offset;</span>
<span class="line-removed">2272 </span>
<span class="line-removed">2273   // Special handling for Windows because it cannot mmap into a reserved space:</span>
<span class="line-removed">2274   //    use_requested_addr: We just map each region individually, and give up if any one of them fails.</span>
<span class="line-removed">2275   //   !use_requested_addr: We reserve the space first, and then os::read in all the regions (instead of mmap).</span>
<span class="line-removed">2276   //                        We&#39;re going to patch all the pointers anyway so there&#39;s no benefit for mmap.</span>
<span class="line-removed">2277 </span>
<span class="line-removed">2278   if (use_requested_addr) {</span>
<span class="line-removed">2279     char* archive_space_base = static_mapinfo-&gt;requested_base_address() + base_offset;</span>
<span class="line-removed">2280     char* archive_space_end  = archive_space_base + archive_space_size;</span>
<span class="line-removed">2281     if (!MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-removed">2282       archive_space_rs = reserve_shared_space(archive_space_size, archive_space_base);</span>
<span class="line-removed">2283       if (!archive_space_rs.is_reserved()) {</span>
<span class="line-removed">2284         return NULL;</span>
<span class="line-removed">2285       }</span>
<span class="line-removed">2286     }</span>
<span class="line-removed">2287     if (use_klass_space) {</span>
<span class="line-removed">2288       // Make sure we can map the klass space immediately following the archive_space space</span>
<span class="line-removed">2289       // Don&#39;t call reserve_shared_space here as that may try to enforce platform-specific</span>
<span class="line-removed">2290       // alignment rules which only apply to the archive base address</span>
<span class="line-removed">2291       char* class_space_base = archive_space_end;</span>
<span class="line-removed">2292       class_space_rs = ReservedSpace(class_space_size, reserved_space_alignment(),</span>
<span class="line-removed">2293                                      false /* large_pages */, class_space_base);</span>
<span class="line-removed">2294       if (!class_space_rs.is_reserved()) {</span>
<span class="line-removed">2295         return NULL;</span>
<span class="line-removed">2296       }</span>
<span class="line-removed">2297     }</span>
<span class="line-removed">2298     return static_mapinfo-&gt;requested_base_address();</span>
<span class="line-removed">2299   } else {</span>
<span class="line-removed">2300     if (use_klass_space) {</span>
<span class="line-removed">2301       main_rs = reserve_shared_space(archive_space_size + class_space_size);</span>
<span class="line-removed">2302       if (main_rs.is_reserved()) {</span>
<span class="line-removed">2303         archive_space_rs = main_rs.first_part(archive_space_size, reserved_space_alignment(), /*split=*/true);</span>
<span class="line-removed">2304         class_space_rs = main_rs.last_part(archive_space_size);</span>
<span class="line-removed">2305       }</span>
<span class="line-removed">2306     } else {</span>
<span class="line-removed">2307       main_rs = reserve_shared_space(archive_space_size);</span>
<span class="line-removed">2308       archive_space_rs = main_rs;</span>
2309     }







2310     if (archive_space_rs.is_reserved()) {


2311       return archive_space_rs.base();
<span class="line-removed">2312     } else {</span>
<span class="line-removed">2313       return NULL;</span>
2314     }








































2315   }































2316 }
2317 
<span class="line-modified">2318 void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; main_rs,</span>
<span class="line-removed">2319                                               ReservedSpace&amp; archive_space_rs,</span>
2320                                               ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2321   if (main_rs.is_reserved()) {</span>
<span class="line-modified">2322     assert(main_rs.contains(archive_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-modified">2323     assert(main_rs.contains(class_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-modified">2324     log_debug(cds)(&quot;Released shared space (archive+classes) &quot; INTPTR_FORMAT, p2i(main_rs.base()));</span>
<span class="line-modified">2325     main_rs.release();</span>
<span class="line-modified">2326   } else {</span>
<span class="line-modified">2327     if (archive_space_rs.is_reserved()) {</span>
<span class="line-removed">2328       log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-removed">2329       archive_space_rs.release();</span>
<span class="line-removed">2330     }</span>
<span class="line-removed">2331     if (class_space_rs.is_reserved()) {</span>
<span class="line-removed">2332       log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-removed">2333       class_space_rs.release();</span>
<span class="line-removed">2334     }</span>
2335   }
2336 }
2337 
2338 static int archive_regions[]  = {MetaspaceShared::mc,
2339                                  MetaspaceShared::rw,
2340                                  MetaspaceShared::ro};
2341 static int archive_regions_count  = 3;
2342 
2343 MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {
2344   assert(UseSharedSpaces, &quot;must be runtime&quot;);
2345   if (mapinfo == NULL) {
2346     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.
2347   }
2348 
2349   mapinfo-&gt;set_is_mapped(false);
2350 
2351   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {
2352     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT
2353                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());
2354     return MAP_ARCHIVE_OTHER_FAILURE;
</pre>
<hr />
<pre>
2459 }
2460 
2461 void MetaspaceShared::report_out_of_space(const char* name, size_t needed_bytes) {
2462   // This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.
2463   // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
2464   // or so.
2465   _mc_region.print_out_of_space_msg(name, needed_bytes);
2466   _rw_region.print_out_of_space_msg(name, needed_bytes);
2467   _ro_region.print_out_of_space_msg(name, needed_bytes);
2468 
2469   vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
2470                                 &quot;Please reduce the number of shared classes.&quot;);
2471 }
2472 
2473 // This is used to relocate the pointers so that the archive can be mapped at
2474 // Arguments::default_SharedBaseAddress() without runtime relocation.
2475 intx MetaspaceShared::final_delta() {
2476   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
2477        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
2478 }




























</pre>
</td>
<td>
<hr />
<pre>
  52 #include &quot;memory/universe.hpp&quot;
  53 #include &quot;oops/compressedOops.inline.hpp&quot;
  54 #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  55 #include &quot;oops/instanceMirrorKlass.hpp&quot;
  56 #include &quot;oops/instanceRefKlass.hpp&quot;
  57 #include &quot;oops/methodData.hpp&quot;
  58 #include &quot;oops/objArrayKlass.hpp&quot;
  59 #include &quot;oops/objArrayOop.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;oops/typeArrayKlass.hpp&quot;
  62 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  63 #include &quot;runtime/handles.inline.hpp&quot;
  64 #include &quot;runtime/os.hpp&quot;
  65 #include &quot;runtime/safepointVerifiers.hpp&quot;
  66 #include &quot;runtime/signature.hpp&quot;
  67 #include &quot;runtime/timerTrace.hpp&quot;
  68 #include &quot;runtime/vmThread.hpp&quot;
  69 #include &quot;runtime/vmOperations.hpp&quot;
  70 #include &quot;utilities/align.hpp&quot;
  71 #include &quot;utilities/bitMap.inline.hpp&quot;
<span class="line-added">  72 #include &quot;utilities/ostream.hpp&quot;</span>
  73 #include &quot;utilities/defaultStream.hpp&quot;
  74 #include &quot;utilities/hashtable.inline.hpp&quot;
  75 #if INCLUDE_G1GC
  76 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  77 #endif
  78 
  79 ReservedSpace MetaspaceShared::_shared_rs;
  80 VirtualSpace MetaspaceShared::_shared_vs;
  81 ReservedSpace MetaspaceShared::_symbol_rs;
  82 VirtualSpace MetaspaceShared::_symbol_vs;
  83 MetaspaceSharedStats MetaspaceShared::_stats;
  84 bool MetaspaceShared::_has_error_classes;
  85 bool MetaspaceShared::_archive_loading_failed = false;
  86 bool MetaspaceShared::_remapped_readwrite = false;
  87 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  88 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  89 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  90 intx MetaspaceShared::_relocation_delta;
  91 
  92 // The CDS archive is divided into the following regions:
</pre>
<hr />
<pre>
 175                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 176   if (strcmp(_name, failing_region) == 0) {
 177     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 178   }
 179 }
 180 
 181 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
 182   _rs = rs;
 183   _vs = vs;
 184   // Start with 0 committed bytes. The memory will be committed as needed by
 185   // MetaspaceShared::commit_to().
 186   if (!_vs-&gt;initialize(*_rs, 0)) {
 187     fatal(&quot;Unable to allocate memory for shared space&quot;);
 188   }
 189   _base = _top = _rs-&gt;base();
 190   _end = _rs-&gt;end();
 191 }
 192 
 193 void DumpRegion::pack(DumpRegion* next) {
 194   assert(!is_packed(), &quot;sanity&quot;);
<span class="line-modified"> 195   _end = (char*)align_up(_top, MetaspaceShared::reserved_space_alignment());</span>
 196   _is_packed = true;
 197   if (next != NULL) {
 198     next-&gt;_rs = _rs;
 199     next-&gt;_vs = _vs;
 200     next-&gt;_base = next-&gt;_top = this-&gt;_end;
 201     next-&gt;_end = _rs-&gt;end();
 202   }
 203 }
 204 
 205 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);
 206 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 207 
 208 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
 209   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);
 210 }
 211 
 212 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 213   return &amp;_mc_region;
 214 }
 215 
</pre>
<hr />
<pre>
 221   return &amp;_ro_region;
 222 }
 223 
 224 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 225                                       ReservedSpace* rs) {
 226   current-&gt;pack(next);
 227 }
 228 
 229 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
 230   return _symbol_region.allocate(num_bytes);
 231 }
 232 
 233 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 234   return _mc_region.allocate(num_bytes);
 235 }
 236 
 237 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 238   return _ro_region.allocate(num_bytes);
 239 }
 240 
<span class="line-modified"> 241 size_t MetaspaceShared::reserved_space_alignment() { return os::vm_allocation_granularity(); }</span>















 242 
<span class="line-modified"> 243 #ifdef _LP64</span>
<span class="line-modified"> 244 // Check SharedBaseAddress for validity. At this point, os::init() must</span>
<span class="line-modified"> 245 //  have been ran.</span>
<span class="line-added"> 246 static void check_SharedBaseAddress() {</span>
<span class="line-added"> 247   SharedBaseAddress = align_up(SharedBaseAddress,</span>
<span class="line-added"> 248                                MetaspaceShared::reserved_space_alignment());</span>
<span class="line-added"> 249   if (!CompressedKlassPointers::is_valid_base((address)SharedBaseAddress)) {</span>
<span class="line-added"> 250     log_warning(cds)(&quot;SharedBaseAddress=&quot; PTR_FORMAT &quot; is invalid for this &quot;</span>
<span class="line-added"> 251                      &quot;platform, option will be ignored.&quot;,</span>
<span class="line-added"> 252                      p2i((address)SharedBaseAddress));</span>
<span class="line-added"> 253     SharedBaseAddress = Arguments::default_SharedBaseAddress();</span>
<span class="line-added"> 254   }</span>
 255 }
<span class="line-added"> 256 #endif</span>
 257 
 258 void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
 259   assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified"> 260 </span>
<span class="line-added"> 261 #ifdef _LP64</span>
<span class="line-added"> 262   check_SharedBaseAddress();</span>
<span class="line-added"> 263 #endif</span>
<span class="line-added"> 264 </span>
<span class="line-added"> 265   const size_t reserve_alignment = MetaspaceShared::reserved_space_alignment();</span>
 266   char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
 267 
 268 #ifdef _LP64
<span class="line-modified"> 269   assert(CompressedKlassPointers::is_valid_base((address)shared_base), &quot;Sanity&quot;);</span>
<span class="line-modified"> 270   // On 64-bit VM we reserve a 4G range and, if UseCompressedClassPointers=1,</span>
<span class="line-modified"> 271   //  will use that to house both the archives and the ccs. See below for</span>
<span class="line-modified"> 272   //  details.</span>






 273   const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
 274   const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
 275 #else
<span class="line-modified"> 276   // We don&#39;t support archives larger than 256MB on 32-bit due to limited</span>
<span class="line-added"> 277   //  virtual address space.</span>
 278   size_t cds_total = align_down(256*M, reserve_alignment);
 279 #endif
 280 
<span class="line-added"> 281   // Whether to use SharedBaseAddress as attach address.</span>
 282   bool use_requested_base = true;
<span class="line-added"> 283 </span>
<span class="line-added"> 284   if (shared_base == NULL) {</span>
<span class="line-added"> 285     use_requested_base = false;</span>
<span class="line-added"> 286   }</span>
<span class="line-added"> 287 </span>
 288   if (ArchiveRelocationMode == 1) {
 289     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
 290     use_requested_base = false;
 291   }
 292 
 293   // First try to reserve the space at the specified SharedBaseAddress.
 294   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
 295   if (use_requested_base) {
<span class="line-modified"> 296     _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="line-added"> 297                                false /* large */, (char*)shared_base);</span>
<span class="line-added"> 298     if (_shared_rs.is_reserved()) {</span>
<span class="line-added"> 299       assert(_shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="line-added"> 300     } else {</span>
<span class="line-added"> 301       log_info(cds)(&quot;dumptime space reservation: failed to map at &quot;</span>
<span class="line-added"> 302                     &quot;SharedBaseAddress &quot; PTR_FORMAT, p2i(shared_base));</span>
<span class="line-added"> 303     }</span>
 304   }
<span class="line-modified"> 305   if (!_shared_rs.is_reserved()) {</span>
<span class="line-modified"> 306     // Get a reserved space anywhere if attaching at the SharedBaseAddress</span>
<span class="line-modified"> 307     //  fails:</span>
<span class="line-modified"> 308     if (UseCompressedClassPointers) {</span>
<span class="line-modified"> 309       // If we need to reserve class space as well, let the platform handle</span>
<span class="line-added"> 310       //  the reservation.</span>
<span class="line-added"> 311       LP64_ONLY(_shared_rs =</span>
<span class="line-added"> 312                 Metaspace::reserve_address_space_for_compressed_classes(cds_total);)</span>
<span class="line-added"> 313       NOT_LP64(ShouldNotReachHere();)</span>
<span class="line-added"> 314     } else {</span>
<span class="line-added"> 315       // anywhere is fine.</span>
<span class="line-added"> 316       _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="line-added"> 317                                  false /* large */, (char*)NULL);</span>
<span class="line-added"> 318     }</span>
 319   }
<span class="line-added"> 320 </span>
 321   if (!_shared_rs.is_reserved()) {
 322     vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
 323                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
 324   }
 325 
 326 #ifdef _LP64











 327 
 328   if (UseCompressedClassPointers) {
<span class="line-modified"> 329 </span>
<span class="line-modified"> 330     assert(CompressedKlassPointers::is_valid_base((address)_shared_rs.base()), &quot;Sanity&quot;);</span>
<span class="line-modified"> 331 </span>
<span class="line-modified"> 332     // On 64-bit VM, if UseCompressedClassPointers=1, the compressed class space</span>
<span class="line-modified"> 333     //  must be allocated near the cds such as that the compressed Klass pointer</span>
<span class="line-modified"> 334     //  encoding can be used to en/decode pointers from both cds and ccs. Since</span>
<span class="line-modified"> 335     //  Metaspace cannot do this (it knows nothing about cds), we do it for</span>
<span class="line-added"> 336     //  Metaspace here and pass it the space to use for ccs.</span>
<span class="line-added"> 337     //</span>
<span class="line-added"> 338     // We do this by reserving space for the ccs behind the archives. Note</span>
<span class="line-added"> 339     //  however that ccs follows a different alignment</span>
<span class="line-added"> 340     //  (Metaspace::reserve_alignment), so there may be a gap between ccs and</span>
<span class="line-added"> 341     //  cds.</span>
<span class="line-added"> 342     // We use a similar layout at runtime, see reserve_address_space_for_archives().</span>
<span class="line-added"> 343     //</span>
<span class="line-added"> 344     //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="line-added"> 345     //                              v</span>
<span class="line-added"> 346     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="line-added"> 347     // |    Heap    | Archive |     | MC | RW | RO | [gap]  |    class space  |</span>
<span class="line-added"> 348     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="line-added"> 349     // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="line-added"> 350     //</span>
<span class="line-added"> 351     // Note: ccs must follow the archives, and the archives must start at the</span>
<span class="line-added"> 352     //  encoding base. However, the exact placement of ccs does not matter as</span>
<span class="line-added"> 353     //  long as it it resides in the encoding range of CompressedKlassPointers</span>
<span class="line-added"> 354     //  and comes after the archive.</span>
<span class="line-added"> 355     //</span>
<span class="line-added"> 356     // We do this by splitting up the allocated 4G into 3G of archive space,</span>
<span class="line-added"> 357     //  followed by 1G for the ccs:</span>
<span class="line-added"> 358     // + The upper 1 GB is used as the &quot;temporary compressed class space&quot;</span>
<span class="line-added"> 359     //   -- preload_classes() will store Klasses into this space.</span>
<span class="line-added"> 360     // + The lower 3 GB is used for the archive -- when preload_classes()</span>
<span class="line-added"> 361     //   is done, ArchiveCompactor will copy the class metadata into this</span>
<span class="line-added"> 362     //   space, first the RW parts, then the RO parts.</span>
<span class="line-added"> 363 </span>
<span class="line-added"> 364     // Starting address of ccs must be aligned to Metaspace::reserve_alignment()...</span>
<span class="line-added"> 365     size_t class_space_size = align_down(_shared_rs.size() / 4, Metaspace::reserve_alignment());</span>
<span class="line-added"> 366     address class_space_start = (address)align_down(_shared_rs.end() - class_space_size, Metaspace::reserve_alignment());</span>
<span class="line-added"> 367     size_t archive_size = class_space_start - (address)_shared_rs.base();</span>
<span class="line-added"> 368 </span>
<span class="line-added"> 369     ReservedSpace tmp_class_space = _shared_rs.last_part(archive_size);</span>
<span class="line-added"> 370     _shared_rs = _shared_rs.first_part(archive_size);</span>
<span class="line-added"> 371 </span>
<span class="line-added"> 372     // ... as does the size of ccs.</span>
<span class="line-added"> 373     tmp_class_space = tmp_class_space.first_part(class_space_size);</span>
<span class="line-added"> 374     CompressedClassSpaceSize = class_space_size;</span>
<span class="line-added"> 375 </span>
<span class="line-added"> 376     // Let Metaspace initialize ccs</span>
 377     Metaspace::initialize_class_space(tmp_class_space);
<span class="line-added"> 378 </span>
<span class="line-added"> 379     // and set up CompressedKlassPointers encoding.</span>
<span class="line-added"> 380     CompressedKlassPointers::initialize((address)_shared_rs.base(), cds_total);</span>
<span class="line-added"> 381 </span>
<span class="line-added"> 382     log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="line-added"> 383                   p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
<span class="line-added"> 384 </span>
<span class="line-added"> 385     log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="line-added"> 386                   CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
<span class="line-added"> 387 </span>
<span class="line-added"> 388     assert(_shared_rs.end() == tmp_class_space.base() &amp;&amp;</span>
<span class="line-added"> 389            is_aligned(_shared_rs.base(), MetaspaceShared::reserved_space_alignment()) &amp;&amp;</span>
<span class="line-added"> 390            is_aligned(tmp_class_space.base(), Metaspace::reserve_alignment()) &amp;&amp;</span>
<span class="line-added"> 391            is_aligned(tmp_class_space.size(), Metaspace::reserve_alignment()), &quot;Sanity&quot;);</span>
 392   }


 393 


 394 #endif
 395 
 396   init_shared_dump_space(&amp;_mc_region);
 397   SharedBaseAddress = (size_t)_shared_rs.base();
 398   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 399                 _shared_rs.size(), p2i(_shared_rs.base()));
 400 
 401   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
 402   _symbol_rs = ReservedSpace(symbol_rs_size);
 403   if (!_symbol_rs.is_reserved()) {
 404     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,
 405                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));
 406   }
 407   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);
 408 }
 409 
 410 // Called by universe_post_init()
 411 void MetaspaceShared::post_initialize(TRAPS) {
 412   if (UseSharedSpaces) {
 413     int size = FileMapInfo::get_number_of_shared_paths();
</pre>
<hr />
<pre>
2115 
2116 bool MetaspaceShared::is_in_trampoline_frame(address addr) {
2117   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {
2118     return true;
2119   }
2120   return false;
2121 }
2122 
2123 bool MetaspaceShared::is_shared_dynamic(void* p) {
2124   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;
2125       (p &gt;= _shared_metaspace_static_top)) {
2126     return true;
2127   } else {
2128     return false;
2129   }
2130 }
2131 
2132 void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
2133   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
2134   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
<span class="line-added">2135 </span>
2136   FileMapInfo* static_mapinfo = open_static_archive();
2137   FileMapInfo* dynamic_mapinfo = NULL;
2138 
2139   if (static_mapinfo != NULL) {
2140     dynamic_mapinfo = open_dynamic_archive();
2141 
2142     // First try to map at the requested address
2143     result = map_archives(static_mapinfo, dynamic_mapinfo, true);
2144     if (result == MAP_ARCHIVE_MMAP_FAILURE) {
2145       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen
2146       // by the OS.
2147       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);
2148       result = map_archives(static_mapinfo, dynamic_mapinfo, false);
2149     }
2150   }
2151 
2152   if (result == MAP_ARCHIVE_SUCCESS) {
2153     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());
2154     char* cds_base = static_mapinfo-&gt;mapped_base();
2155     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();
</pre>
<hr />
<pre>
2192   }
2193   if (Arguments::GetSharedDynamicArchivePath() == NULL) {
2194     return NULL;
2195   }
2196 
2197   FileMapInfo* mapinfo = new FileMapInfo(false);
2198   if (!mapinfo-&gt;initialize()) {
2199     delete(mapinfo);
2200     return NULL;
2201   }
2202   return mapinfo;
2203 }
2204 
2205 // use_requested_addr:
2206 //  true  = map at FileMapHeader::_requested_base_address
2207 //  false = map at an alternative address picked by OS.
2208 MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
2209                                                bool use_requested_addr) {
2210   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
2211       // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="line-modified">2212       // For debug builds, the check is done below, after reserving the space, for better test coverage</span>
<span class="line-added">2213       // (see comment below).</span>
2214       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2215       return MAP_ARCHIVE_MMAP_FAILURE;
2216     });
2217 
2218   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
2219     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);
2220     return MAP_ARCHIVE_MMAP_FAILURE;
2221   };
2222 
2223   if (dynamic_mapinfo != NULL) {
2224     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
2225     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
2226     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
2227   }
2228 
<span class="line-modified">2229   ReservedSpace archive_space_rs, class_space_rs;</span>
2230   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
2231   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="line-modified">2232                                                                  use_requested_addr, archive_space_rs,</span>
2233                                                                  class_space_rs);
2234   if (mapped_base_address == NULL) {
2235     result = MAP_ARCHIVE_MMAP_FAILURE;
2236   } else {
<span class="line-added">2237 </span>
<span class="line-added">2238 #ifdef ASSERT</span>
<span class="line-added">2239     // Some sanity checks after reserving address spaces for archives</span>
<span class="line-added">2240     //  and class space.</span>
<span class="line-added">2241     assert(archive_space_rs.is_reserved(), &quot;Sanity&quot;);</span>
<span class="line-added">2242     if (Metaspace::using_class_space()) {</span>
<span class="line-added">2243       // Class space must closely follow the archive space. Both spaces</span>
<span class="line-added">2244       //  must be aligned correctly.</span>
<span class="line-added">2245       assert(class_space_rs.is_reserved(),</span>
<span class="line-added">2246              &quot;A class space should have been reserved&quot;);</span>
<span class="line-added">2247       assert(class_space_rs.base() &gt;= archive_space_rs.end(),</span>
<span class="line-added">2248              &quot;class space should follow the cds archive space&quot;);</span>
<span class="line-added">2249       assert(is_aligned(archive_space_rs.base(),</span>
<span class="line-added">2250                         MetaspaceShared::reserved_space_alignment()),</span>
<span class="line-added">2251              &quot;Archive space misaligned&quot;);</span>
<span class="line-added">2252       assert(is_aligned(class_space_rs.base(),</span>
<span class="line-added">2253                         Metaspace::reserve_alignment()),</span>
<span class="line-added">2254              &quot;class space misaligned&quot;);</span>
<span class="line-added">2255     }</span>
<span class="line-added">2256 #endif // ASSERT</span>
<span class="line-added">2257 </span>
2258     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2259                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
2260     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2261                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());
<span class="line-added">2262 </span>
<span class="line-added">2263     if (MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-added">2264       // We have now reserved address space for the archives, and will map in</span>
<span class="line-added">2265       //  the archive files into this space.</span>
<span class="line-added">2266       //</span>
<span class="line-added">2267       // Special handling for Windows: on Windows we cannot map a file view</span>
<span class="line-added">2268       //  into an existing memory mapping. So, we unmap the address range we</span>
<span class="line-added">2269       //  just reserved again, which will make it available for mapping the</span>
<span class="line-added">2270       //  archives.</span>
<span class="line-added">2271       // Reserving this range has not been for naught however since it makes</span>
<span class="line-added">2272       //  us reasonably sure the address range is available.</span>
<span class="line-added">2273       //</span>
<span class="line-added">2274       // But still it may fail, since between unmapping the range and mapping</span>
<span class="line-added">2275       //  in the archive someone else may grab the address space. Therefore</span>
<span class="line-added">2276       //  there is a fallback in FileMap::map_region() where we just read in</span>
<span class="line-added">2277       //  the archive files sequentially instead of mapping it in. We couple</span>
<span class="line-added">2278       //  this with use_requested_addr, since we&#39;re going to patch all the</span>
<span class="line-added">2279       //  pointers anyway so there&#39;s no benefit to mmap.</span>
<span class="line-added">2280       if (use_requested_addr) {</span>
<span class="line-added">2281         log_info(cds)(&quot;Windows mmap workaround: releasing archive space.&quot;);</span>
<span class="line-added">2282         archive_space_rs.release();</span>
<span class="line-added">2283       }</span>
<span class="line-added">2284     }</span>
2285     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
2286     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
2287                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
2288 
2289     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="line-modified">2290       // This is for simulating mmap failures at the requested address. In</span>
<span class="line-modified">2291       //  debug builds, we do it here (after all archives have possibly been</span>
<span class="line-modified">2292       //  mapped), so we can thoroughly test the code for failure handling</span>
<span class="line-added">2293       //  (releasing all allocated resource, etc).</span>
2294       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2295       if (static_result == MAP_ARCHIVE_SUCCESS) {
2296         static_result = MAP_ARCHIVE_MMAP_FAILURE;
2297       }
2298       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2299         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;
2300       }
2301     });
2302 
2303     if (static_result == MAP_ARCHIVE_SUCCESS) {
2304       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2305         result = MAP_ARCHIVE_SUCCESS;
2306       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {
2307         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);
2308         // No need to retry mapping the dynamic archive again, as it will never succeed
2309         // (bad file, etc) -- just keep the base archive.
2310         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,
2311                                   dynamic_mapinfo-&gt;full_path());
2312         result = MAP_ARCHIVE_SUCCESS;
2313         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no
2314         // easy API to do that right now.
2315       } else {
2316         result = MAP_ARCHIVE_MMAP_FAILURE;
2317       }
2318     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {
2319       result = MAP_ARCHIVE_OTHER_FAILURE;
2320     } else {
2321       result = MAP_ARCHIVE_MMAP_FAILURE;
2322     }
2323   }
2324 
2325   if (result == MAP_ARCHIVE_SUCCESS) {



2326     SharedBaseAddress = (size_t)mapped_base_address;
2327     LP64_ONLY({
2328         if (Metaspace::using_class_space()) {
<span class="line-modified">2329           // Set up ccs in metaspace.</span>
<span class="line-modified">2330           Metaspace::initialize_class_space(class_space_rs);</span>
<span class="line-modified">2331 </span>
<span class="line-added">2332           // Set up compressed Klass pointer encoding: the encoding range must</span>
<span class="line-added">2333           //  cover both archive and class space.</span>
<span class="line-added">2334           address cds_base = (address)static_mapinfo-&gt;mapped_base();</span>
<span class="line-added">2335           address ccs_end = (address)class_space_rs.end();</span>
<span class="line-added">2336           CompressedKlassPointers::initialize(cds_base, ccs_end - cds_base);</span>
<span class="line-added">2337 </span>
2338           // map_heap_regions() compares the current narrow oop and klass encodings
2339           // with the archived ones, so it must be done after all encodings are determined.
2340           static_mapinfo-&gt;map_heap_regions();

2341         }
2342       });
2343   } else {
2344     unmap_archive(static_mapinfo);
2345     unmap_archive(dynamic_mapinfo);
<span class="line-modified">2346     release_reserved_spaces(archive_space_rs, class_space_rs);</span>
2347   }
2348 
2349   return result;
2350 }
2351 
<span class="line-added">2352 </span>
<span class="line-added">2353 // This will reserve two address spaces suitable to house Klass structures, one</span>
<span class="line-added">2354 //  for the cds archives (static archive and optionally dynamic archive) and</span>
<span class="line-added">2355 //  optionally one move for ccs.</span>
<span class="line-added">2356 //</span>
<span class="line-added">2357 // Since both spaces must fall within the compressed class pointer encoding</span>
<span class="line-added">2358 //  range, they are allocated close to each other.</span>
<span class="line-added">2359 //</span>
<span class="line-added">2360 // Space for archives will be reserved first, followed by a potential gap,</span>
<span class="line-added">2361 //  followed by the space for ccs:</span>
<span class="line-added">2362 //</span>
<span class="line-added">2363 // +-- Base address             A        B                     End</span>
<span class="line-added">2364 // |                            |        |                      |</span>
<span class="line-added">2365 // v                            v        v                      v</span>
<span class="line-added">2366 // +-------------+--------------+        +----------------------+</span>
<span class="line-added">2367 // | static arc  | [dyn. arch]  | [gap]  | compr. class space   |</span>
<span class="line-added">2368 // +-------------+--------------+        +----------------------+</span>
<span class="line-added">2369 //</span>
<span class="line-added">2370 // (The gap may result from different alignment requirements between metaspace</span>
<span class="line-added">2371 //  and CDS)</span>
<span class="line-added">2372 //</span>
<span class="line-added">2373 // If UseCompressedClassPointers is disabled, only one address space will be</span>
<span class="line-added">2374 //  reserved:</span>
<span class="line-added">2375 //</span>
<span class="line-added">2376 // +-- Base address             End</span>
<span class="line-added">2377 // |                            |</span>
<span class="line-added">2378 // v                            v</span>
<span class="line-added">2379 // +-------------+--------------+</span>
<span class="line-added">2380 // | static arc  | [dyn. arch]  |</span>
<span class="line-added">2381 // +-------------+--------------+</span>
<span class="line-added">2382 //</span>
<span class="line-added">2383 // Base address: If use_archive_base_addr address is true, the Base address is</span>
<span class="line-added">2384 //  determined by the address stored in the static archive. If</span>
<span class="line-added">2385 //  use_archive_base_addr address is false, this base address is determined</span>
<span class="line-added">2386 //  by the platform.</span>
<span class="line-added">2387 //</span>
<span class="line-added">2388 // If UseCompressedClassPointers=1, the range encompassing both spaces will be</span>
<span class="line-added">2389 //  suitable to en/decode narrow Klass pointers: the base will be valid for</span>
<span class="line-added">2390 //  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.</span>
<span class="line-added">2391 //</span>
<span class="line-added">2392 // Return:</span>
<span class="line-added">2393 //</span>
<span class="line-added">2394 // - On success:</span>
<span class="line-added">2395 //    - archive_space_rs will be reserved and large enough to host static and</span>
<span class="line-added">2396 //      if needed dynamic archive: [Base, A).</span>
<span class="line-added">2397 //      archive_space_rs.base and size will be aligned to CDS reserve</span>
<span class="line-added">2398 //      granularity.</span>
<span class="line-added">2399 //    - class_space_rs: If UseCompressedClassPointers=1, class_space_rs will</span>
<span class="line-added">2400 //      be reserved. Its start address will be aligned to metaspace reserve</span>
<span class="line-added">2401 //      alignment, which may differ from CDS alignment. It will follow the cds</span>
<span class="line-added">2402 //      archive space, close enough such that narrow class pointer encoding</span>
<span class="line-added">2403 //      covers both spaces.</span>
<span class="line-added">2404 //      If UseCompressedClassPointers=0, class_space_rs remains unreserved.</span>
<span class="line-added">2405 // - On error: NULL is returned and the spaces remain unreserved.</span>
2406 char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
2407                                                           FileMapInfo* dynamic_mapinfo,
<span class="line-modified">2408                                                           bool use_archive_base_addr,</span>

2409                                                           ReservedSpace&amp; archive_space_rs,
2410                                                           ReservedSpace&amp; class_space_rs) {


2411 
<span class="line-modified">2412   address const base_address = (address) (use_archive_base_addr ? static_mapinfo-&gt;requested_base_address() : NULL);</span>
<span class="line-modified">2413   const size_t archive_space_alignment = MetaspaceShared::reserved_space_alignment();</span>




2414 
2415   // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="line-modified">2416   assert(static_mapinfo-&gt;mapping_base_offset() == 0, &quot;Must be&quot;);</span>
<span class="line-modified">2417   size_t archive_end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">2418   size_t archive_space_size = align_up(archive_end_offset, archive_space_alignment);</span>
<span class="line-modified">2419 </span>
<span class="line-modified">2420   // If a base address is given, it must have valid alignment and be suitable as encoding base.</span>
<span class="line-modified">2421   if (base_address != NULL) {</span>
<span class="line-modified">2422     assert(is_aligned(base_address, archive_space_alignment),</span>
<span class="line-modified">2423            &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>
<span class="line-modified">2424     if (Metaspace::using_class_space()) {</span>
<span class="line-modified">2425       assert(CompressedKlassPointers::is_valid_base(base_address),</span>
<span class="line-modified">2426              &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>





































2427     }
<span class="line-added">2428   }</span>
<span class="line-added">2429 </span>
<span class="line-added">2430   if (!Metaspace::using_class_space()) {</span>
<span class="line-added">2431     // Get the simple case out of the way first:</span>
<span class="line-added">2432     // no compressed class space, simple allocation.</span>
<span class="line-added">2433     archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,</span>
<span class="line-added">2434                                      false /* bool large */, (char*)base_address);</span>
2435     if (archive_space_rs.is_reserved()) {
<span class="line-added">2436       assert(base_address == NULL ||</span>
<span class="line-added">2437              (address)archive_space_rs.base() == base_address, &quot;Sanity&quot;);</span>
2438       return archive_space_rs.base();


2439     }
<span class="line-added">2440     return NULL;</span>
<span class="line-added">2441   }</span>
<span class="line-added">2442 </span>
<span class="line-added">2443 #ifdef _LP64</span>
<span class="line-added">2444 </span>
<span class="line-added">2445   // Complex case: two spaces adjacent to each other, both to be addressable</span>
<span class="line-added">2446   //  with narrow class pointers.</span>
<span class="line-added">2447   // We reserve the whole range spanning both spaces, then split that range up.</span>
<span class="line-added">2448 </span>
<span class="line-added">2449   const size_t class_space_alignment = Metaspace::reserve_alignment();</span>
<span class="line-added">2450 </span>
<span class="line-added">2451   // To simplify matters, lets assume that metaspace alignment will always be</span>
<span class="line-added">2452   //  equal or a multiple of archive alignment.</span>
<span class="line-added">2453   assert(is_power_of_2(class_space_alignment) &amp;&amp;</span>
<span class="line-added">2454                        is_power_of_2(archive_space_alignment) &amp;&amp;</span>
<span class="line-added">2455                        class_space_alignment &gt;= archive_space_alignment,</span>
<span class="line-added">2456                        &quot;Sanity&quot;);</span>
<span class="line-added">2457 </span>
<span class="line-added">2458   const size_t class_space_size = CompressedClassSpaceSize;</span>
<span class="line-added">2459   assert(CompressedClassSpaceSize &gt; 0 &amp;&amp;</span>
<span class="line-added">2460          is_aligned(CompressedClassSpaceSize, class_space_alignment),</span>
<span class="line-added">2461          &quot;CompressedClassSpaceSize malformed: &quot;</span>
<span class="line-added">2462          SIZE_FORMAT, CompressedClassSpaceSize);</span>
<span class="line-added">2463 </span>
<span class="line-added">2464   const size_t ccs_begin_offset = align_up(archive_space_size,</span>
<span class="line-added">2465                                            class_space_alignment);</span>
<span class="line-added">2466   const size_t gap_size = ccs_begin_offset - archive_space_size;</span>
<span class="line-added">2467 </span>
<span class="line-added">2468   const size_t total_range_size =</span>
<span class="line-added">2469       align_up(archive_space_size + gap_size + class_space_size,</span>
<span class="line-added">2470                os::vm_allocation_granularity());</span>
<span class="line-added">2471 </span>
<span class="line-added">2472   ReservedSpace total_rs;</span>
<span class="line-added">2473   if (base_address != NULL) {</span>
<span class="line-added">2474     // Reserve at the given archive base address, or not at all.</span>
<span class="line-added">2475     total_rs = ReservedSpace(total_range_size, archive_space_alignment,</span>
<span class="line-added">2476                              false /* bool large */, (char*) base_address);</span>
<span class="line-added">2477   } else {</span>
<span class="line-added">2478     // Reserve at any address, but leave it up to the platform to choose a good one.</span>
<span class="line-added">2479     total_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size);</span>
2480   }
<span class="line-added">2481 </span>
<span class="line-added">2482   if (!total_rs.is_reserved()) {</span>
<span class="line-added">2483     return NULL;</span>
<span class="line-added">2484   }</span>
<span class="line-added">2485 </span>
<span class="line-added">2486   // Paranoid checks:</span>
<span class="line-added">2487   assert(base_address == NULL || (address)total_rs.base() == base_address,</span>
<span class="line-added">2488          &quot;Sanity (&quot; PTR_FORMAT &quot; vs &quot; PTR_FORMAT &quot;)&quot;, p2i(base_address), p2i(total_rs.base()));</span>
<span class="line-added">2489   assert(is_aligned(total_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2490   assert(total_rs.size() == total_range_size, &quot;Sanity&quot;);</span>
<span class="line-added">2491   assert(CompressedKlassPointers::is_valid_base((address)total_rs.base()), &quot;Sanity&quot;);</span>
<span class="line-added">2492 </span>
<span class="line-added">2493   // Now split up the space into ccs and cds archive. For simplicity, just leave</span>
<span class="line-added">2494   //  the gap reserved at the end of the archive space.</span>
<span class="line-added">2495   archive_space_rs = total_rs.first_part(ccs_begin_offset,</span>
<span class="line-added">2496                                          (size_t)os::vm_allocation_granularity(),</span>
<span class="line-added">2497                                          /*split=*/true);</span>
<span class="line-added">2498   class_space_rs = total_rs.last_part(ccs_begin_offset);</span>
<span class="line-added">2499 </span>
<span class="line-added">2500   assert(is_aligned(archive_space_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2501   assert(is_aligned(archive_space_rs.size(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2502   assert(is_aligned(class_space_rs.base(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2503   assert(is_aligned(class_space_rs.size(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2504 </span>
<span class="line-added">2505   return archive_space_rs.base();</span>
<span class="line-added">2506 </span>
<span class="line-added">2507 #else</span>
<span class="line-added">2508   ShouldNotReachHere();</span>
<span class="line-added">2509   return NULL;</span>
<span class="line-added">2510 #endif</span>
<span class="line-added">2511 </span>
2512 }
2513 
<span class="line-modified">2514 void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; archive_space_rs,</span>

2515                                               ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2516   if (archive_space_rs.is_reserved()) {</span>
<span class="line-modified">2517     log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-modified">2518     archive_space_rs.release();</span>
<span class="line-modified">2519   }</span>
<span class="line-modified">2520   if (class_space_rs.is_reserved()) {</span>
<span class="line-modified">2521     log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-modified">2522     class_space_rs.release();</span>







2523   }
2524 }
2525 
2526 static int archive_regions[]  = {MetaspaceShared::mc,
2527                                  MetaspaceShared::rw,
2528                                  MetaspaceShared::ro};
2529 static int archive_regions_count  = 3;
2530 
2531 MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {
2532   assert(UseSharedSpaces, &quot;must be runtime&quot;);
2533   if (mapinfo == NULL) {
2534     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.
2535   }
2536 
2537   mapinfo-&gt;set_is_mapped(false);
2538 
2539   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {
2540     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT
2541                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());
2542     return MAP_ARCHIVE_OTHER_FAILURE;
</pre>
<hr />
<pre>
2647 }
2648 
2649 void MetaspaceShared::report_out_of_space(const char* name, size_t needed_bytes) {
2650   // This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.
2651   // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
2652   // or so.
2653   _mc_region.print_out_of_space_msg(name, needed_bytes);
2654   _rw_region.print_out_of_space_msg(name, needed_bytes);
2655   _ro_region.print_out_of_space_msg(name, needed_bytes);
2656 
2657   vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
2658                                 &quot;Please reduce the number of shared classes.&quot;);
2659 }
2660 
2661 // This is used to relocate the pointers so that the archive can be mapped at
2662 // Arguments::default_SharedBaseAddress() without runtime relocation.
2663 intx MetaspaceShared::final_delta() {
2664   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
2665        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
2666 }
<span class="line-added">2667 </span>
<span class="line-added">2668 void MetaspaceShared::print_on(outputStream* st) {</span>
<span class="line-added">2669   if (UseSharedSpaces || DumpSharedSpaces) {</span>
<span class="line-added">2670     st-&gt;print(&quot;CDS archive(s) mapped at: &quot;);</span>
<span class="line-added">2671     address base;</span>
<span class="line-added">2672     address top;</span>
<span class="line-added">2673     if (UseSharedSpaces) { // Runtime</span>
<span class="line-added">2674       base = (address)MetaspaceObj::shared_metaspace_base();</span>
<span class="line-added">2675       address static_top = (address)_shared_metaspace_static_top;</span>
<span class="line-added">2676       top = (address)MetaspaceObj::shared_metaspace_top();</span>
<span class="line-added">2677       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(static_top), p2i(top));</span>
<span class="line-added">2678     } else if (DumpSharedSpaces) { // Dump Time</span>
<span class="line-added">2679       base = (address)_shared_rs.base();</span>
<span class="line-added">2680       top = (address)_shared_rs.end();</span>
<span class="line-added">2681       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(top));</span>
<span class="line-added">2682     }</span>
<span class="line-added">2683     st-&gt;print(&quot;size &quot; SIZE_FORMAT &quot;, &quot;, top - base);</span>
<span class="line-added">2684     st-&gt;print(&quot;SharedBaseAddress: &quot; PTR_FORMAT &quot;, ArchiveRelocationMode: %d.&quot;, SharedBaseAddress, (int)ArchiveRelocationMode);</span>
<span class="line-added">2685   } else {</span>
<span class="line-added">2686     st-&gt;print(&quot;CDS disabled.&quot;);</span>
<span class="line-added">2687   }</span>
<span class="line-added">2688   st-&gt;cr();</span>
<span class="line-added">2689 }</span>
<span class="line-added">2690 </span>
<span class="line-added">2691 </span>
<span class="line-added">2692 </span>
<span class="line-added">2693 </span>
<span class="line-added">2694 </span>
</pre>
</td>
</tr>
</table>
<center><a href="metaspace/virtualSpaceNode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>