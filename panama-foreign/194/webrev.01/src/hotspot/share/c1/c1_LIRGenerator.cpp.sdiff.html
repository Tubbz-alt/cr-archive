<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ci/ciField.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRGenerator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 461 void LIRGenerator::klass2reg_with_patching(LIR_Opr r, ciMetadata* obj, CodeEmitInfo* info, bool need_resolve) {
 462   /* C2 relies on constant pool entries being resolved (ciTypeFlow), so if TieredCompilation
 463    * is active and the class hasn&#39;t yet been resolved we need to emit a patch that resolves
 464    * the class. */
 465   if ((TieredCompilation &amp;&amp; need_resolve) || !obj-&gt;is_loaded() || PatchALot) {
 466     assert(info != NULL, &quot;info must be set if class is not loaded&quot;);
 467     __ klass2reg_patch(NULL, r, info);
 468   } else {
 469     // no patching needed
 470     __ metadata2reg(obj-&gt;constant_encoding(), r);
 471   }
 472 }
 473 
 474 
 475 void LIRGenerator::array_range_check(LIR_Opr array, LIR_Opr index,
 476                                     CodeEmitInfo* null_check_info, CodeEmitInfo* range_check_info) {
 477   CodeStub* stub = new RangeCheckStub(range_check_info, index, array);
 478   if (index-&gt;is_constant()) {
 479     cmp_mem_int(lir_cond_belowEqual, array, arrayOopDesc::length_offset_in_bytes(),
 480                 index-&gt;as_jint(), null_check_info);
<span class="line-modified"> 481     __ branch(lir_cond_belowEqual, T_INT, stub); // forward branch</span>
 482   } else {
 483     cmp_reg_mem(lir_cond_aboveEqual, index, array,
 484                 arrayOopDesc::length_offset_in_bytes(), T_INT, null_check_info);
<span class="line-modified"> 485     __ branch(lir_cond_aboveEqual, T_INT, stub); // forward branch</span>
 486   }
 487 }
 488 
 489 
 490 void LIRGenerator::nio_range_check(LIR_Opr buffer, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {
 491   CodeStub* stub = new RangeCheckStub(info, index);
 492   if (index-&gt;is_constant()) {
 493     cmp_mem_int(lir_cond_belowEqual, buffer, java_nio_Buffer::limit_offset(), index-&gt;as_jint(), info);
<span class="line-modified"> 494     __ branch(lir_cond_belowEqual, T_INT, stub); // forward branch</span>
 495   } else {
 496     cmp_reg_mem(lir_cond_aboveEqual, index, buffer,
 497                 java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified"> 498     __ branch(lir_cond_aboveEqual, T_INT, stub); // forward branch</span>
 499   }
 500   __ move(index, result);
 501 }
 502 
 503 
 504 
 505 void LIRGenerator::arithmetic_op(Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, bool is_strictfp, LIR_Opr tmp_op, CodeEmitInfo* info) {
 506   LIR_Opr result_op = result;
 507   LIR_Opr left_op   = left;
 508   LIR_Opr right_op  = right;
 509 
 510   if (TwoOperandLIRForm &amp;&amp; left_op != result_op) {
 511     assert(right_op != result_op, &quot;malformed&quot;);
 512     __ move(left_op, result_op);
 513     left_op = result_op;
 514   }
 515 
 516   switch(code) {
 517     case Bytecodes::_dadd:
 518     case Bytecodes::_fadd:
</pre>
<hr />
<pre>
 669 #endif
 670 
 671 void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {
 672   klass2reg_with_patching(klass_reg, klass, info, is_unresolved);
 673   // If klass is not loaded we do not know if the klass has finalizers:
 674   if (UseFastNewInstance &amp;&amp; klass-&gt;is_loaded()
 675       &amp;&amp; !Klass::layout_helper_needs_slow_path(klass-&gt;layout_helper())) {
 676 
 677     Runtime1::StubID stub_id = klass-&gt;is_initialized() ? Runtime1::fast_new_instance_id : Runtime1::fast_new_instance_init_check_id;
 678 
 679     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, stub_id);
 680 
 681     assert(klass-&gt;is_loaded(), &quot;must be loaded&quot;);
 682     // allocate space for instance
 683     assert(klass-&gt;size_helper() &gt;= 0, &quot;illegal instance size&quot;);
 684     const int instance_size = align_object_size(klass-&gt;size_helper());
 685     __ allocate_object(dst, scratch1, scratch2, scratch3, scratch4,
 686                        oopDesc::header_size(), instance_size, klass_reg, !klass-&gt;is_initialized(), slow_path);
 687   } else {
 688     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);
<span class="line-modified"> 689     __ branch(lir_cond_always, T_ILLEGAL, slow_path);</span>
 690     __ branch_destination(slow_path-&gt;continuation());
 691   }
 692 }
 693 
 694 
 695 static bool is_constant_zero(Instruction* inst) {
 696   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 697   if (c) {
 698     return (c-&gt;value() == 0);
 699   }
 700   return false;
 701 }
 702 
 703 
 704 static bool positive_constant(Instruction* inst) {
 705   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 706   if (c) {
 707     return (c-&gt;value() &gt;= 0);
 708   }
 709   return false;
</pre>
<hr />
<pre>
1203     args-&gt;append(meth);
1204     call_runtime(&amp;signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, NULL);
1205   }
1206 
1207   if (x-&gt;type()-&gt;is_void()) {
1208     __ return_op(LIR_OprFact::illegalOpr);
1209   } else {
1210     LIR_Opr reg = result_register_for(x-&gt;type(), /*callee=*/true);
1211     LIRItem result(x-&gt;result(), this);
1212 
1213     result.load_item_force(reg);
1214     __ return_op(result.result());
1215   }
1216   set_no_result(x);
1217 }
1218 
1219 // Examble: ref.get()
1220 // Combination of LoadField and g1 pre-write barrier
1221 void LIRGenerator::do_Reference_get(Intrinsic* x) {
1222 
<span class="line-modified">1223   const int referent_offset = java_lang_ref_Reference::referent_offset;</span>
<span class="line-removed">1224   guarantee(referent_offset &gt; 0, &quot;referent offset not initialized&quot;);</span>
1225 
1226   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1227 
1228   LIRItem reference(x-&gt;argument_at(0), this);
1229   reference.load_item();
1230 
1231   // need to perform the null check on the reference objecy
1232   CodeEmitInfo* info = NULL;
1233   if (x-&gt;needs_null_check()) {
1234     info = state_for(x);
1235   }
1236 
1237   LIR_Opr result = rlock_result(x, T_OBJECT);
1238   access_load_at(IN_HEAP | ON_WEAK_OOP_REF, T_OBJECT,
1239                  reference, LIR_OprFact::intConst(referent_offset), result);
1240 }
1241 
1242 // Example: clazz.isInstance(object)
1243 void LIRGenerator::do_isInstance(Intrinsic* x) {
1244   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
</pre>
<hr />
<pre>
1290   __ move_wide(new LIR_Address(temp, in_bytes(Klass::java_mirror_offset()), T_ADDRESS), temp);
1291   // mirror = ((OopHandle)mirror)-&gt;resolve();
1292   access_load(IN_NATIVE, T_OBJECT,
1293               LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), result);
1294 }
1295 
1296 // java.lang.Class::isPrimitive()
1297 void LIRGenerator::do_isPrimitive(Intrinsic* x) {
1298   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1299 
1300   LIRItem rcvr(x-&gt;argument_at(0), this);
1301   rcvr.load_item();
1302   LIR_Opr temp = new_register(T_METADATA);
1303   LIR_Opr result = rlock_result(x);
1304 
1305   CodeEmitInfo* info = NULL;
1306   if (x-&gt;needs_null_check()) {
1307     info = state_for(x);
1308   }
1309 
<span class="line-modified">1310   __ move(new LIR_Address(rcvr.result(), java_lang_Class::klass_offset_in_bytes(), T_ADDRESS), temp, info);</span>
1311   __ cmp(lir_cond_notEqual, temp, LIR_OprFact::metadataConst(0));
1312   __ cmove(lir_cond_notEqual, LIR_OprFact::intConst(0), LIR_OprFact::intConst(1), result, T_BOOLEAN);
1313 }
1314 
1315 
1316 // Example: Thread.currentThread()
1317 void LIRGenerator::do_currentThread(Intrinsic* x) {
1318   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
1319   LIR_Opr reg = rlock_result(x);
1320   __ move_wide(new LIR_Address(getThreadPointer(), in_bytes(JavaThread::threadObj_offset()), T_OBJECT), reg);
1321 }
1322 
1323 
1324 void LIRGenerator::do_RegisterFinalizer(Intrinsic* x) {
1325   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1326   LIRItem receiver(x-&gt;argument_at(0), this);
1327 
1328   receiver.load_item();
1329   BasicTypeList signature;
1330   signature.append(T_OBJECT); // receiver
</pre>
<hr />
<pre>
1575   if (needs_store_check || x-&gt;check_boolean()) {
1576     value.load_item();
1577   } else {
1578     value.load_for_store(x-&gt;elt_type());
1579   }
1580 
1581   set_no_result(x);
1582 
1583   // the CodeEmitInfo must be duplicated for each different
1584   // LIR-instruction because spilling can occur anywhere between two
1585   // instructions and so the debug information must be different
1586   CodeEmitInfo* range_check_info = state_for(x);
1587   CodeEmitInfo* null_check_info = NULL;
1588   if (x-&gt;needs_null_check()) {
1589     null_check_info = new CodeEmitInfo(range_check_info);
1590   }
1591 
1592   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1593     if (use_length) {
1594       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">1595       __ branch(lir_cond_belowEqual, T_INT, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1596     } else {
1597       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1598       // range_check also does the null check
1599       null_check_info = NULL;
1600     }
1601   }
1602 
1603   if (GenerateArrayStoreCheck &amp;&amp; needs_store_check) {
1604     CodeEmitInfo* store_check_info = new CodeEmitInfo(range_check_info);
1605     array_store_check(value.result(), array.result(), store_check_info, x-&gt;profiled_method(), x-&gt;profiled_bci());
1606   }
1607 
1608   DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1609   if (x-&gt;check_boolean()) {
1610     decorators |= C1_MASK_BOOLEAN;
1611   }
1612 
1613   access_store_at(decorators, x-&gt;elt_type(), array, index.result(), value.result(),
1614                   NULL, null_check_info);
1615 }
</pre>
<hr />
<pre>
1764 //------------------------java.nio.Buffer.checkIndex------------------------
1765 
1766 // int java.nio.Buffer.checkIndex(int)
1767 void LIRGenerator::do_NIOCheckIndex(Intrinsic* x) {
1768   // NOTE: by the time we are in checkIndex() we are guaranteed that
1769   // the buffer is non-null (because checkIndex is package-private and
1770   // only called from within other methods in the buffer).
1771   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
1772   LIRItem buf  (x-&gt;argument_at(0), this);
1773   LIRItem index(x-&gt;argument_at(1), this);
1774   buf.load_item();
1775   index.load_item();
1776 
1777   LIR_Opr result = rlock_result(x);
1778   if (GenerateRangeChecks) {
1779     CodeEmitInfo* info = state_for(x);
1780     CodeStub* stub = new RangeCheckStub(info, index.result());
1781     LIR_Opr buf_obj = access_resolve(IS_NOT_NULL | ACCESS_READ, buf.result());
1782     if (index.result()-&gt;is_constant()) {
1783       cmp_mem_int(lir_cond_belowEqual, buf_obj, java_nio_Buffer::limit_offset(), index.result()-&gt;as_jint(), info);
<span class="line-modified">1784       __ branch(lir_cond_belowEqual, T_INT, stub);</span>
1785     } else {
1786       cmp_reg_mem(lir_cond_aboveEqual, index.result(), buf_obj,
1787                   java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified">1788       __ branch(lir_cond_aboveEqual, T_INT, stub);</span>
1789     }
1790     __ move(index.result(), result);
1791   } else {
1792     // Just load the index into the result register
1793     __ move(index.result(), result);
1794   }
1795 }
1796 
1797 
1798 //------------------------array access--------------------------------------
1799 
1800 
1801 void LIRGenerator::do_ArrayLength(ArrayLength* x) {
1802   LIRItem array(x-&gt;array(), this);
1803   array.load_item();
1804   LIR_Opr reg = rlock_result(x);
1805 
1806   CodeEmitInfo* info = NULL;
1807   if (x-&gt;needs_null_check()) {
1808     NullCheck* nc = x-&gt;explicit_null_check();
</pre>
<hr />
<pre>
1842   }
1843 
1844   CodeEmitInfo* range_check_info = state_for(x);
1845   CodeEmitInfo* null_check_info = NULL;
1846   if (x-&gt;needs_null_check()) {
1847     NullCheck* nc = x-&gt;explicit_null_check();
1848     if (nc != NULL) {
1849       null_check_info = state_for(nc);
1850     } else {
1851       null_check_info = range_check_info;
1852     }
1853     if (StressLoopInvariantCodeMotion &amp;&amp; null_check_info-&gt;deoptimize_on_exception()) {
1854       LIR_Opr obj = new_register(T_OBJECT);
1855       __ move(LIR_OprFact::oopConst(NULL), obj);
1856       __ null_check(obj, new CodeEmitInfo(null_check_info));
1857     }
1858   }
1859 
1860   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1861     if (StressLoopInvariantCodeMotion &amp;&amp; range_check_info-&gt;deoptimize_on_exception()) {
<span class="line-modified">1862       __ branch(lir_cond_always, T_ILLEGAL, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1863     } else if (use_length) {
1864       // TODO: use a (modified) version of array_range_check that does not require a
1865       //       constant length to be loaded to a register
1866       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">1867       __ branch(lir_cond_belowEqual, T_INT, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1868     } else {
1869       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1870       // The range check performs the null check, so clear it out for the load
1871       null_check_info = NULL;
1872     }
1873   }
1874 
1875   DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1876 
1877   LIR_Opr result = rlock_result(x, x-&gt;elt_type());
1878   access_load_at(decorators, x-&gt;elt_type(),
1879                  array, index.result(), result,
1880                  NULL, null_check_info);
1881 }
1882 
1883 
1884 void LIRGenerator::do_NullCheck(NullCheck* x) {
1885   if (x-&gt;can_trap()) {
1886     LIRItem value(x-&gt;obj(), this);
1887     value.load_item();
</pre>
<hr />
<pre>
2223 
2224   LIR_Opr result;
2225   if (x-&gt;is_add()) {
2226     result = access_atomic_add_at(decorators, type, src, off, value);
2227   } else {
2228     result = access_atomic_xchg_at(decorators, type, src, off, value);
2229   }
2230   set_result(x, result);
2231 }
2232 
2233 void LIRGenerator::do_SwitchRanges(SwitchRangeArray* x, LIR_Opr value, BlockBegin* default_sux) {
2234   int lng = x-&gt;length();
2235 
2236   for (int i = 0; i &lt; lng; i++) {
2237     C1SwitchRange* one_range = x-&gt;at(i);
2238     int low_key = one_range-&gt;low_key();
2239     int high_key = one_range-&gt;high_key();
2240     BlockBegin* dest = one_range-&gt;sux();
2241     if (low_key == high_key) {
2242       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2243       __ branch(lir_cond_equal, T_INT, dest);</span>
2244     } else if (high_key - low_key == 1) {
2245       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2246       __ branch(lir_cond_equal, T_INT, dest);</span>
2247       __ cmp(lir_cond_equal, value, high_key);
<span class="line-modified">2248       __ branch(lir_cond_equal, T_INT, dest);</span>
2249     } else {
2250       LabelObj* L = new LabelObj();
2251       __ cmp(lir_cond_less, value, low_key);
<span class="line-modified">2252       __ branch(lir_cond_less, T_INT, L-&gt;label());</span>
2253       __ cmp(lir_cond_lessEqual, value, high_key);
<span class="line-modified">2254       __ branch(lir_cond_lessEqual, T_INT, dest);</span>
2255       __ branch_destination(L-&gt;label());
2256     }
2257   }
2258   __ jump(default_sux);
2259 }
2260 
2261 
2262 SwitchRangeArray* LIRGenerator::create_lookup_ranges(TableSwitch* x) {
2263   SwitchRangeList* res = new SwitchRangeList();
2264   int len = x-&gt;length();
2265   if (len &gt; 0) {
2266     BlockBegin* sux = x-&gt;sux_at(0);
2267     int key = x-&gt;lo_key();
2268     BlockBegin* default_sux = x-&gt;default_sux();
2269     C1SwitchRange* range = new C1SwitchRange(key, sux);
2270     for (int i = 0; i &lt; len; i++, key++) {
2271       BlockBegin* new_sux = x-&gt;sux_at(i);
2272       if (sux == new_sux) {
2273         // still in same range
2274         range-&gt;set_high_key(key);
</pre>
<hr />
<pre>
2354       __ cmp(lir_cond_equal, value, i + lo_key);
2355       __ move(data_offset_reg, tmp_reg);
2356       __ cmove(lir_cond_equal,
2357                LIR_OprFact::intptrConst(count_offset),
2358                tmp_reg,
2359                data_offset_reg, T_INT);
2360     }
2361 
2362     LIR_Opr data_reg = new_pointer_register();
2363     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2364     __ move(data_addr, data_reg);
2365     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2366     __ move(data_reg, data_addr);
2367   }
2368 
2369   if (UseTableRanges) {
2370     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2371   } else {
2372     for (int i = 0; i &lt; len; i++) {
2373       __ cmp(lir_cond_equal, value, i + lo_key);
<span class="line-modified">2374       __ branch(lir_cond_equal, T_INT, x-&gt;sux_at(i));</span>
2375     }
2376     __ jump(x-&gt;default_sux());
2377   }
2378 }
2379 
2380 
2381 void LIRGenerator::do_LookupSwitch(LookupSwitch* x) {
2382   LIRItem tag(x-&gt;tag(), this);
2383   tag.load_item();
2384   set_no_result(x);
2385 
2386   if (x-&gt;is_safepoint()) {
2387     __ safepoint(safepoint_poll_register(), state_for(x, x-&gt;state_before()));
2388   }
2389 
2390   // move values into phi locations
2391   move_to_phi(x-&gt;state());
2392 
2393   LIR_Opr value = tag.result();
2394   int len = x-&gt;length();
</pre>
<hr />
<pre>
2413       __ move(data_offset_reg, tmp_reg);
2414       __ cmove(lir_cond_equal,
2415                LIR_OprFact::intptrConst(count_offset),
2416                tmp_reg,
2417                data_offset_reg, T_INT);
2418     }
2419 
2420     LIR_Opr data_reg = new_pointer_register();
2421     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2422     __ move(data_addr, data_reg);
2423     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2424     __ move(data_reg, data_addr);
2425   }
2426 
2427   if (UseTableRanges) {
2428     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2429   } else {
2430     int len = x-&gt;length();
2431     for (int i = 0; i &lt; len; i++) {
2432       __ cmp(lir_cond_equal, value, x-&gt;key_at(i));
<span class="line-modified">2433       __ branch(lir_cond_equal, T_INT, x-&gt;sux_at(i));</span>
2434     }
2435     __ jump(x-&gt;default_sux());
2436   }
2437 }
2438 
2439 
2440 void LIRGenerator::do_Goto(Goto* x) {
2441   set_no_result(x);
2442 
2443   if (block()-&gt;next()-&gt;as_OsrEntry()) {
2444     // need to free up storage used for OSR entry point
2445     LIR_Opr osrBuffer = block()-&gt;next()-&gt;operand();
2446     BasicTypeList signature;
2447     signature.append(NOT_LP64(T_INT) LP64_ONLY(T_LONG)); // pass a pointer to osrBuffer
2448     CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
2449     __ move(osrBuffer, cc-&gt;args()-&gt;at(0));
2450     __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_end),
2451                          getThreadTemp(), LIR_OprFact::illegalOpr, cc-&gt;args());
2452   }
2453 
</pre>
<hr />
<pre>
2937   LIRItem t_val(x-&gt;tval(), this);
2938   LIRItem f_val(x-&gt;fval(), this);
2939   t_val.dont_load_item();
2940   f_val.dont_load_item();
2941   LIR_Opr reg = rlock_result(x);
2942 
2943   __ cmp(lir_cond(x-&gt;cond()), left.result(), right.result());
2944   __ cmove(lir_cond(x-&gt;cond()), t_val.result(), f_val.result(), reg, as_BasicType(x-&gt;x()-&gt;type()));
2945 }
2946 
2947 #ifdef JFR_HAVE_INTRINSICS
2948 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
2949   CodeEmitInfo* info = state_for(x);
2950   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
2951 
2952   assert(info != NULL, &quot;must have info&quot;);
2953   LIRItem arg(x-&gt;argument_at(0), this);
2954 
2955   arg.load_item();
2956   LIR_Opr klass = new_register(T_METADATA);
<span class="line-modified">2957   __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset_in_bytes(), T_ADDRESS), klass, info);</span>
2958   LIR_Opr id = new_register(T_LONG);
2959   ByteSize offset = KLASS_TRACE_ID_OFFSET;
2960   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
2961 
2962   __ move(trace_id_addr, id);
2963   __ logical_or(id, LIR_OprFact::longConst(0x01l), id);
2964   __ store(id, trace_id_addr);
2965 
2966 #ifdef TRACE_ID_META_BITS
2967   __ logical_and(id, LIR_OprFact::longConst(~TRACE_ID_META_BITS), id);
2968 #endif
2969 #ifdef TRACE_ID_SHIFT
2970   __ unsigned_shift_right(id, TRACE_ID_SHIFT, id);
2971 #endif
2972 
2973   __ move(id, rlock_result(x));
2974 }
2975 
2976 void LIRGenerator::do_getEventWriter(Intrinsic* x) {
2977   LabelObj* L_end = new LabelObj();
2978 


2979   LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),
2980                                            in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),
<span class="line-modified">2981                                            T_OBJECT);</span>
2982   LIR_Opr result = rlock_result(x);
<span class="line-modified">2983   __ move_wide(jobj_addr, result);</span>
<span class="line-modified">2984   __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(NULL));</span>
<span class="line-modified">2985   __ branch(lir_cond_equal, T_OBJECT, L_end-&gt;label());</span>


2986 
<span class="line-removed">2987   LIR_Opr jobj = new_register(T_OBJECT);</span>
<span class="line-removed">2988   __ move(result, jobj);</span>
2989   access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);
2990 
2991   __ branch_destination(L_end-&gt;label());
2992 }
2993 
2994 #endif
2995 
2996 
2997 void LIRGenerator::do_RuntimeCall(address routine, Intrinsic* x) {
2998   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
2999   // Enforce computation of _reserved_argument_area_size which is required on some platforms.
3000   BasicTypeList signature;
3001   CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
3002   LIR_Opr reg = result_register_for(x-&gt;type());
3003   __ call_runtime_leaf(routine, getThreadTemp(),
3004                        reg, new LIR_OprList());
3005   LIR_Opr result = rlock_result(x);
3006   __ move(reg, result);
3007 }
3008 
</pre>
<hr />
<pre>
3326   }
3327   increment_event_counter_impl(info, info-&gt;scope()-&gt;method(), step, right_n_bits(freq_log), bci, backedge, true);
3328 }
3329 
3330 void LIRGenerator::decrement_age(CodeEmitInfo* info) {
3331   ciMethod* method = info-&gt;scope()-&gt;method();
3332   MethodCounters* mc_adr = method-&gt;ensure_method_counters();
3333   if (mc_adr != NULL) {
3334     LIR_Opr mc = new_pointer_register();
3335     __ move(LIR_OprFact::intptrConst(mc_adr), mc);
3336     int offset = in_bytes(MethodCounters::nmethod_age_offset());
3337     LIR_Address* counter = new LIR_Address(mc, offset, T_INT);
3338     LIR_Opr result = new_register(T_INT);
3339     __ load(counter, result);
3340     __ sub(result, LIR_OprFact::intConst(1), result);
3341     __ store(result, counter);
3342     // DeoptimizeStub will reexecute from the current state in code info.
3343     CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,
3344                                          Deoptimization::Action_make_not_entrant);
3345     __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));
<span class="line-modified">3346     __ branch(lir_cond_lessEqual, T_INT, deopt);</span>
3347   }
3348 }
3349 
3350 
3351 void LIRGenerator::increment_event_counter_impl(CodeEmitInfo* info,
3352                                                 ciMethod *method, LIR_Opr step, int frequency,
3353                                                 int bci, bool backedge, bool notify) {
3354   assert(frequency == 0 || is_power_of_2(frequency + 1), &quot;Frequency must be x^2 - 1 or 0&quot;);
3355   int level = _compilation-&gt;env()-&gt;comp_level();
3356   assert(level &gt; CompLevel_simple, &quot;Shouldn&#39;t be here&quot;);
3357 
3358   int offset = -1;
3359   LIR_Opr counter_holder = NULL;
3360   if (level == CompLevel_limited_profile) {
3361     MethodCounters* counters_adr = method-&gt;ensure_method_counters();
3362     if (counters_adr == NULL) {
3363       bailout(&quot;method counters allocation failed&quot;);
3364       return;
3365     }
3366     counter_holder = new_pointer_register();
</pre>
<hr />
<pre>
3373                                  MethodData::invocation_counter_offset());
3374     ciMethodData* md = method-&gt;method_data_or_null();
3375     assert(md != NULL, &quot;Sanity&quot;);
3376     __ metadata2reg(md-&gt;constant_encoding(), counter_holder);
3377   } else {
3378     ShouldNotReachHere();
3379   }
3380   LIR_Address* counter = new LIR_Address(counter_holder, offset, T_INT);
3381   LIR_Opr result = new_register(T_INT);
3382   __ load(counter, result);
3383   __ add(result, step, result);
3384   __ store(result, counter);
3385   if (notify &amp;&amp; (!backedge || UseOnStackReplacement)) {
3386     LIR_Opr meth = LIR_OprFact::metadataConst(method-&gt;constant_encoding());
3387     // The bci for info can point to cmp for if&#39;s we want the if bci
3388     CodeStub* overflow = new CounterOverflowStub(info, bci, meth);
3389     int freq = frequency &lt;&lt; InvocationCounter::count_shift;
3390     if (freq == 0) {
3391       if (!step-&gt;is_constant()) {
3392         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
<span class="line-modified">3393         __ branch(lir_cond_notEqual, T_ILLEGAL, overflow);</span>
3394       } else {
<span class="line-modified">3395         __ branch(lir_cond_always, T_ILLEGAL, overflow);</span>
3396       }
3397     } else {
3398       LIR_Opr mask = load_immediate(freq, T_INT);
3399       if (!step-&gt;is_constant()) {
3400         // If step is 0, make sure the overflow check below always fails
3401         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
3402         __ cmove(lir_cond_notEqual, result, LIR_OprFact::intConst(InvocationCounter::count_increment), result, T_INT);
3403       }
3404       __ logical_and(result, mask, result);
3405       __ cmp(lir_cond_equal, result, LIR_OprFact::intConst(0));
<span class="line-modified">3406       __ branch(lir_cond_equal, T_INT, overflow);</span>
3407     }
3408     __ branch_destination(overflow-&gt;continuation());
3409   }
3410 }
3411 
3412 void LIRGenerator::do_RuntimeCall(RuntimeCall* x) {
3413   LIR_OprList* args = new LIR_OprList(x-&gt;number_of_arguments());
3414   BasicTypeList* signature = new BasicTypeList(x-&gt;number_of_arguments());
3415 
3416   if (x-&gt;pass_thread()) {
3417     signature-&gt;append(LP64_ONLY(T_LONG) NOT_LP64(T_INT));    // thread
3418     args-&gt;append(getThreadPointer());
3419   }
3420 
3421   for (int i = 0; i &lt; x-&gt;number_of_arguments(); i++) {
3422     Value a = x-&gt;argument_at(i);
3423     LIRItem* item = new LIRItem(a, this);
3424     item-&gt;load_item();
3425     args-&gt;append(item-&gt;result());
3426     signature-&gt;append(as_BasicType(a-&gt;type()));
</pre>
<hr />
<pre>
3500     ValueTag tag = x-&gt;x()-&gt;type()-&gt;tag();
3501     If::Condition cond = x-&gt;cond();
3502     LIRItem xitem(x-&gt;x(), this);
3503     LIRItem yitem(x-&gt;y(), this);
3504     LIRItem* xin = &amp;xitem;
3505     LIRItem* yin = &amp;yitem;
3506 
3507     assert(tag == intTag, &quot;Only integer deoptimizations are valid!&quot;);
3508 
3509     xin-&gt;load_item();
3510     yin-&gt;dont_load_item();
3511     set_no_result(x);
3512 
3513     LIR_Opr left = xin-&gt;result();
3514     LIR_Opr right = yin-&gt;result();
3515 
3516     CodeEmitInfo *info = state_for(x, x-&gt;state());
3517     CodeStub* stub = new PredicateFailedStub(info);
3518 
3519     __ cmp(lir_cond(cond), left, right);
<span class="line-modified">3520     __ branch(lir_cond(cond), right-&gt;type(), stub);</span>
3521   }
3522 }
3523 
3524 
3525 LIR_Opr LIRGenerator::call_runtime(Value arg1, address entry, ValueType* result_type, CodeEmitInfo* info) {
3526   LIRItemList args(1);
3527   LIRItem value(arg1, this);
3528   args.append(&amp;value);
3529   BasicTypeList signature;
3530   signature.append(as_BasicType(arg1-&gt;type()));
3531 
3532   return call_runtime(&amp;signature, &amp;args, entry, result_type, info);
3533 }
3534 
3535 
3536 LIR_Opr LIRGenerator::call_runtime(Value arg1, Value arg2, address entry, ValueType* result_type, CodeEmitInfo* info) {
3537   LIRItemList args(2);
3538   LIRItem value1(arg1, this);
3539   LIRItem value2(arg2, this);
3540   args.append(&amp;value1);
</pre>
</td>
<td>
<hr />
<pre>
 461 void LIRGenerator::klass2reg_with_patching(LIR_Opr r, ciMetadata* obj, CodeEmitInfo* info, bool need_resolve) {
 462   /* C2 relies on constant pool entries being resolved (ciTypeFlow), so if TieredCompilation
 463    * is active and the class hasn&#39;t yet been resolved we need to emit a patch that resolves
 464    * the class. */
 465   if ((TieredCompilation &amp;&amp; need_resolve) || !obj-&gt;is_loaded() || PatchALot) {
 466     assert(info != NULL, &quot;info must be set if class is not loaded&quot;);
 467     __ klass2reg_patch(NULL, r, info);
 468   } else {
 469     // no patching needed
 470     __ metadata2reg(obj-&gt;constant_encoding(), r);
 471   }
 472 }
 473 
 474 
 475 void LIRGenerator::array_range_check(LIR_Opr array, LIR_Opr index,
 476                                     CodeEmitInfo* null_check_info, CodeEmitInfo* range_check_info) {
 477   CodeStub* stub = new RangeCheckStub(range_check_info, index, array);
 478   if (index-&gt;is_constant()) {
 479     cmp_mem_int(lir_cond_belowEqual, array, arrayOopDesc::length_offset_in_bytes(),
 480                 index-&gt;as_jint(), null_check_info);
<span class="line-modified"> 481     __ branch(lir_cond_belowEqual, stub); // forward branch</span>
 482   } else {
 483     cmp_reg_mem(lir_cond_aboveEqual, index, array,
 484                 arrayOopDesc::length_offset_in_bytes(), T_INT, null_check_info);
<span class="line-modified"> 485     __ branch(lir_cond_aboveEqual, stub); // forward branch</span>
 486   }
 487 }
 488 
 489 
 490 void LIRGenerator::nio_range_check(LIR_Opr buffer, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {
 491   CodeStub* stub = new RangeCheckStub(info, index);
 492   if (index-&gt;is_constant()) {
 493     cmp_mem_int(lir_cond_belowEqual, buffer, java_nio_Buffer::limit_offset(), index-&gt;as_jint(), info);
<span class="line-modified"> 494     __ branch(lir_cond_belowEqual, stub); // forward branch</span>
 495   } else {
 496     cmp_reg_mem(lir_cond_aboveEqual, index, buffer,
 497                 java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified"> 498     __ branch(lir_cond_aboveEqual, stub); // forward branch</span>
 499   }
 500   __ move(index, result);
 501 }
 502 
 503 
 504 
 505 void LIRGenerator::arithmetic_op(Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, bool is_strictfp, LIR_Opr tmp_op, CodeEmitInfo* info) {
 506   LIR_Opr result_op = result;
 507   LIR_Opr left_op   = left;
 508   LIR_Opr right_op  = right;
 509 
 510   if (TwoOperandLIRForm &amp;&amp; left_op != result_op) {
 511     assert(right_op != result_op, &quot;malformed&quot;);
 512     __ move(left_op, result_op);
 513     left_op = result_op;
 514   }
 515 
 516   switch(code) {
 517     case Bytecodes::_dadd:
 518     case Bytecodes::_fadd:
</pre>
<hr />
<pre>
 669 #endif
 670 
 671 void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {
 672   klass2reg_with_patching(klass_reg, klass, info, is_unresolved);
 673   // If klass is not loaded we do not know if the klass has finalizers:
 674   if (UseFastNewInstance &amp;&amp; klass-&gt;is_loaded()
 675       &amp;&amp; !Klass::layout_helper_needs_slow_path(klass-&gt;layout_helper())) {
 676 
 677     Runtime1::StubID stub_id = klass-&gt;is_initialized() ? Runtime1::fast_new_instance_id : Runtime1::fast_new_instance_init_check_id;
 678 
 679     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, stub_id);
 680 
 681     assert(klass-&gt;is_loaded(), &quot;must be loaded&quot;);
 682     // allocate space for instance
 683     assert(klass-&gt;size_helper() &gt;= 0, &quot;illegal instance size&quot;);
 684     const int instance_size = align_object_size(klass-&gt;size_helper());
 685     __ allocate_object(dst, scratch1, scratch2, scratch3, scratch4,
 686                        oopDesc::header_size(), instance_size, klass_reg, !klass-&gt;is_initialized(), slow_path);
 687   } else {
 688     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);
<span class="line-modified"> 689     __ branch(lir_cond_always, slow_path);</span>
 690     __ branch_destination(slow_path-&gt;continuation());
 691   }
 692 }
 693 
 694 
 695 static bool is_constant_zero(Instruction* inst) {
 696   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 697   if (c) {
 698     return (c-&gt;value() == 0);
 699   }
 700   return false;
 701 }
 702 
 703 
 704 static bool positive_constant(Instruction* inst) {
 705   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 706   if (c) {
 707     return (c-&gt;value() &gt;= 0);
 708   }
 709   return false;
</pre>
<hr />
<pre>
1203     args-&gt;append(meth);
1204     call_runtime(&amp;signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, NULL);
1205   }
1206 
1207   if (x-&gt;type()-&gt;is_void()) {
1208     __ return_op(LIR_OprFact::illegalOpr);
1209   } else {
1210     LIR_Opr reg = result_register_for(x-&gt;type(), /*callee=*/true);
1211     LIRItem result(x-&gt;result(), this);
1212 
1213     result.load_item_force(reg);
1214     __ return_op(result.result());
1215   }
1216   set_no_result(x);
1217 }
1218 
1219 // Examble: ref.get()
1220 // Combination of LoadField and g1 pre-write barrier
1221 void LIRGenerator::do_Reference_get(Intrinsic* x) {
1222 
<span class="line-modified">1223   const int referent_offset = java_lang_ref_Reference::referent_offset();</span>

1224 
1225   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1226 
1227   LIRItem reference(x-&gt;argument_at(0), this);
1228   reference.load_item();
1229 
1230   // need to perform the null check on the reference objecy
1231   CodeEmitInfo* info = NULL;
1232   if (x-&gt;needs_null_check()) {
1233     info = state_for(x);
1234   }
1235 
1236   LIR_Opr result = rlock_result(x, T_OBJECT);
1237   access_load_at(IN_HEAP | ON_WEAK_OOP_REF, T_OBJECT,
1238                  reference, LIR_OprFact::intConst(referent_offset), result);
1239 }
1240 
1241 // Example: clazz.isInstance(object)
1242 void LIRGenerator::do_isInstance(Intrinsic* x) {
1243   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
</pre>
<hr />
<pre>
1289   __ move_wide(new LIR_Address(temp, in_bytes(Klass::java_mirror_offset()), T_ADDRESS), temp);
1290   // mirror = ((OopHandle)mirror)-&gt;resolve();
1291   access_load(IN_NATIVE, T_OBJECT,
1292               LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), result);
1293 }
1294 
1295 // java.lang.Class::isPrimitive()
1296 void LIRGenerator::do_isPrimitive(Intrinsic* x) {
1297   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1298 
1299   LIRItem rcvr(x-&gt;argument_at(0), this);
1300   rcvr.load_item();
1301   LIR_Opr temp = new_register(T_METADATA);
1302   LIR_Opr result = rlock_result(x);
1303 
1304   CodeEmitInfo* info = NULL;
1305   if (x-&gt;needs_null_check()) {
1306     info = state_for(x);
1307   }
1308 
<span class="line-modified">1309   __ move(new LIR_Address(rcvr.result(), java_lang_Class::klass_offset(), T_ADDRESS), temp, info);</span>
1310   __ cmp(lir_cond_notEqual, temp, LIR_OprFact::metadataConst(0));
1311   __ cmove(lir_cond_notEqual, LIR_OprFact::intConst(0), LIR_OprFact::intConst(1), result, T_BOOLEAN);
1312 }
1313 
1314 
1315 // Example: Thread.currentThread()
1316 void LIRGenerator::do_currentThread(Intrinsic* x) {
1317   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
1318   LIR_Opr reg = rlock_result(x);
1319   __ move_wide(new LIR_Address(getThreadPointer(), in_bytes(JavaThread::threadObj_offset()), T_OBJECT), reg);
1320 }
1321 
1322 
1323 void LIRGenerator::do_RegisterFinalizer(Intrinsic* x) {
1324   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1325   LIRItem receiver(x-&gt;argument_at(0), this);
1326 
1327   receiver.load_item();
1328   BasicTypeList signature;
1329   signature.append(T_OBJECT); // receiver
</pre>
<hr />
<pre>
1574   if (needs_store_check || x-&gt;check_boolean()) {
1575     value.load_item();
1576   } else {
1577     value.load_for_store(x-&gt;elt_type());
1578   }
1579 
1580   set_no_result(x);
1581 
1582   // the CodeEmitInfo must be duplicated for each different
1583   // LIR-instruction because spilling can occur anywhere between two
1584   // instructions and so the debug information must be different
1585   CodeEmitInfo* range_check_info = state_for(x);
1586   CodeEmitInfo* null_check_info = NULL;
1587   if (x-&gt;needs_null_check()) {
1588     null_check_info = new CodeEmitInfo(range_check_info);
1589   }
1590 
1591   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1592     if (use_length) {
1593       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">1594       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1595     } else {
1596       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1597       // range_check also does the null check
1598       null_check_info = NULL;
1599     }
1600   }
1601 
1602   if (GenerateArrayStoreCheck &amp;&amp; needs_store_check) {
1603     CodeEmitInfo* store_check_info = new CodeEmitInfo(range_check_info);
1604     array_store_check(value.result(), array.result(), store_check_info, x-&gt;profiled_method(), x-&gt;profiled_bci());
1605   }
1606 
1607   DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1608   if (x-&gt;check_boolean()) {
1609     decorators |= C1_MASK_BOOLEAN;
1610   }
1611 
1612   access_store_at(decorators, x-&gt;elt_type(), array, index.result(), value.result(),
1613                   NULL, null_check_info);
1614 }
</pre>
<hr />
<pre>
1763 //------------------------java.nio.Buffer.checkIndex------------------------
1764 
1765 // int java.nio.Buffer.checkIndex(int)
1766 void LIRGenerator::do_NIOCheckIndex(Intrinsic* x) {
1767   // NOTE: by the time we are in checkIndex() we are guaranteed that
1768   // the buffer is non-null (because checkIndex is package-private and
1769   // only called from within other methods in the buffer).
1770   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
1771   LIRItem buf  (x-&gt;argument_at(0), this);
1772   LIRItem index(x-&gt;argument_at(1), this);
1773   buf.load_item();
1774   index.load_item();
1775 
1776   LIR_Opr result = rlock_result(x);
1777   if (GenerateRangeChecks) {
1778     CodeEmitInfo* info = state_for(x);
1779     CodeStub* stub = new RangeCheckStub(info, index.result());
1780     LIR_Opr buf_obj = access_resolve(IS_NOT_NULL | ACCESS_READ, buf.result());
1781     if (index.result()-&gt;is_constant()) {
1782       cmp_mem_int(lir_cond_belowEqual, buf_obj, java_nio_Buffer::limit_offset(), index.result()-&gt;as_jint(), info);
<span class="line-modified">1783       __ branch(lir_cond_belowEqual, stub);</span>
1784     } else {
1785       cmp_reg_mem(lir_cond_aboveEqual, index.result(), buf_obj,
1786                   java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified">1787       __ branch(lir_cond_aboveEqual, stub);</span>
1788     }
1789     __ move(index.result(), result);
1790   } else {
1791     // Just load the index into the result register
1792     __ move(index.result(), result);
1793   }
1794 }
1795 
1796 
1797 //------------------------array access--------------------------------------
1798 
1799 
1800 void LIRGenerator::do_ArrayLength(ArrayLength* x) {
1801   LIRItem array(x-&gt;array(), this);
1802   array.load_item();
1803   LIR_Opr reg = rlock_result(x);
1804 
1805   CodeEmitInfo* info = NULL;
1806   if (x-&gt;needs_null_check()) {
1807     NullCheck* nc = x-&gt;explicit_null_check();
</pre>
<hr />
<pre>
1841   }
1842 
1843   CodeEmitInfo* range_check_info = state_for(x);
1844   CodeEmitInfo* null_check_info = NULL;
1845   if (x-&gt;needs_null_check()) {
1846     NullCheck* nc = x-&gt;explicit_null_check();
1847     if (nc != NULL) {
1848       null_check_info = state_for(nc);
1849     } else {
1850       null_check_info = range_check_info;
1851     }
1852     if (StressLoopInvariantCodeMotion &amp;&amp; null_check_info-&gt;deoptimize_on_exception()) {
1853       LIR_Opr obj = new_register(T_OBJECT);
1854       __ move(LIR_OprFact::oopConst(NULL), obj);
1855       __ null_check(obj, new CodeEmitInfo(null_check_info));
1856     }
1857   }
1858 
1859   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1860     if (StressLoopInvariantCodeMotion &amp;&amp; range_check_info-&gt;deoptimize_on_exception()) {
<span class="line-modified">1861       __ branch(lir_cond_always, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1862     } else if (use_length) {
1863       // TODO: use a (modified) version of array_range_check that does not require a
1864       //       constant length to be loaded to a register
1865       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">1866       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1867     } else {
1868       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1869       // The range check performs the null check, so clear it out for the load
1870       null_check_info = NULL;
1871     }
1872   }
1873 
1874   DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1875 
1876   LIR_Opr result = rlock_result(x, x-&gt;elt_type());
1877   access_load_at(decorators, x-&gt;elt_type(),
1878                  array, index.result(), result,
1879                  NULL, null_check_info);
1880 }
1881 
1882 
1883 void LIRGenerator::do_NullCheck(NullCheck* x) {
1884   if (x-&gt;can_trap()) {
1885     LIRItem value(x-&gt;obj(), this);
1886     value.load_item();
</pre>
<hr />
<pre>
2222 
2223   LIR_Opr result;
2224   if (x-&gt;is_add()) {
2225     result = access_atomic_add_at(decorators, type, src, off, value);
2226   } else {
2227     result = access_atomic_xchg_at(decorators, type, src, off, value);
2228   }
2229   set_result(x, result);
2230 }
2231 
2232 void LIRGenerator::do_SwitchRanges(SwitchRangeArray* x, LIR_Opr value, BlockBegin* default_sux) {
2233   int lng = x-&gt;length();
2234 
2235   for (int i = 0; i &lt; lng; i++) {
2236     C1SwitchRange* one_range = x-&gt;at(i);
2237     int low_key = one_range-&gt;low_key();
2238     int high_key = one_range-&gt;high_key();
2239     BlockBegin* dest = one_range-&gt;sux();
2240     if (low_key == high_key) {
2241       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2242       __ branch(lir_cond_equal, dest);</span>
2243     } else if (high_key - low_key == 1) {
2244       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2245       __ branch(lir_cond_equal, dest);</span>
2246       __ cmp(lir_cond_equal, value, high_key);
<span class="line-modified">2247       __ branch(lir_cond_equal, dest);</span>
2248     } else {
2249       LabelObj* L = new LabelObj();
2250       __ cmp(lir_cond_less, value, low_key);
<span class="line-modified">2251       __ branch(lir_cond_less, L-&gt;label());</span>
2252       __ cmp(lir_cond_lessEqual, value, high_key);
<span class="line-modified">2253       __ branch(lir_cond_lessEqual, dest);</span>
2254       __ branch_destination(L-&gt;label());
2255     }
2256   }
2257   __ jump(default_sux);
2258 }
2259 
2260 
2261 SwitchRangeArray* LIRGenerator::create_lookup_ranges(TableSwitch* x) {
2262   SwitchRangeList* res = new SwitchRangeList();
2263   int len = x-&gt;length();
2264   if (len &gt; 0) {
2265     BlockBegin* sux = x-&gt;sux_at(0);
2266     int key = x-&gt;lo_key();
2267     BlockBegin* default_sux = x-&gt;default_sux();
2268     C1SwitchRange* range = new C1SwitchRange(key, sux);
2269     for (int i = 0; i &lt; len; i++, key++) {
2270       BlockBegin* new_sux = x-&gt;sux_at(i);
2271       if (sux == new_sux) {
2272         // still in same range
2273         range-&gt;set_high_key(key);
</pre>
<hr />
<pre>
2353       __ cmp(lir_cond_equal, value, i + lo_key);
2354       __ move(data_offset_reg, tmp_reg);
2355       __ cmove(lir_cond_equal,
2356                LIR_OprFact::intptrConst(count_offset),
2357                tmp_reg,
2358                data_offset_reg, T_INT);
2359     }
2360 
2361     LIR_Opr data_reg = new_pointer_register();
2362     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2363     __ move(data_addr, data_reg);
2364     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2365     __ move(data_reg, data_addr);
2366   }
2367 
2368   if (UseTableRanges) {
2369     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2370   } else {
2371     for (int i = 0; i &lt; len; i++) {
2372       __ cmp(lir_cond_equal, value, i + lo_key);
<span class="line-modified">2373       __ branch(lir_cond_equal, x-&gt;sux_at(i));</span>
2374     }
2375     __ jump(x-&gt;default_sux());
2376   }
2377 }
2378 
2379 
2380 void LIRGenerator::do_LookupSwitch(LookupSwitch* x) {
2381   LIRItem tag(x-&gt;tag(), this);
2382   tag.load_item();
2383   set_no_result(x);
2384 
2385   if (x-&gt;is_safepoint()) {
2386     __ safepoint(safepoint_poll_register(), state_for(x, x-&gt;state_before()));
2387   }
2388 
2389   // move values into phi locations
2390   move_to_phi(x-&gt;state());
2391 
2392   LIR_Opr value = tag.result();
2393   int len = x-&gt;length();
</pre>
<hr />
<pre>
2412       __ move(data_offset_reg, tmp_reg);
2413       __ cmove(lir_cond_equal,
2414                LIR_OprFact::intptrConst(count_offset),
2415                tmp_reg,
2416                data_offset_reg, T_INT);
2417     }
2418 
2419     LIR_Opr data_reg = new_pointer_register();
2420     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2421     __ move(data_addr, data_reg);
2422     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2423     __ move(data_reg, data_addr);
2424   }
2425 
2426   if (UseTableRanges) {
2427     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2428   } else {
2429     int len = x-&gt;length();
2430     for (int i = 0; i &lt; len; i++) {
2431       __ cmp(lir_cond_equal, value, x-&gt;key_at(i));
<span class="line-modified">2432       __ branch(lir_cond_equal, x-&gt;sux_at(i));</span>
2433     }
2434     __ jump(x-&gt;default_sux());
2435   }
2436 }
2437 
2438 
2439 void LIRGenerator::do_Goto(Goto* x) {
2440   set_no_result(x);
2441 
2442   if (block()-&gt;next()-&gt;as_OsrEntry()) {
2443     // need to free up storage used for OSR entry point
2444     LIR_Opr osrBuffer = block()-&gt;next()-&gt;operand();
2445     BasicTypeList signature;
2446     signature.append(NOT_LP64(T_INT) LP64_ONLY(T_LONG)); // pass a pointer to osrBuffer
2447     CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
2448     __ move(osrBuffer, cc-&gt;args()-&gt;at(0));
2449     __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_end),
2450                          getThreadTemp(), LIR_OprFact::illegalOpr, cc-&gt;args());
2451   }
2452 
</pre>
<hr />
<pre>
2936   LIRItem t_val(x-&gt;tval(), this);
2937   LIRItem f_val(x-&gt;fval(), this);
2938   t_val.dont_load_item();
2939   f_val.dont_load_item();
2940   LIR_Opr reg = rlock_result(x);
2941 
2942   __ cmp(lir_cond(x-&gt;cond()), left.result(), right.result());
2943   __ cmove(lir_cond(x-&gt;cond()), t_val.result(), f_val.result(), reg, as_BasicType(x-&gt;x()-&gt;type()));
2944 }
2945 
2946 #ifdef JFR_HAVE_INTRINSICS
2947 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
2948   CodeEmitInfo* info = state_for(x);
2949   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
2950 
2951   assert(info != NULL, &quot;must have info&quot;);
2952   LIRItem arg(x-&gt;argument_at(0), this);
2953 
2954   arg.load_item();
2955   LIR_Opr klass = new_register(T_METADATA);
<span class="line-modified">2956   __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);</span>
2957   LIR_Opr id = new_register(T_LONG);
2958   ByteSize offset = KLASS_TRACE_ID_OFFSET;
2959   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
2960 
2961   __ move(trace_id_addr, id);
2962   __ logical_or(id, LIR_OprFact::longConst(0x01l), id);
2963   __ store(id, trace_id_addr);
2964 
2965 #ifdef TRACE_ID_META_BITS
2966   __ logical_and(id, LIR_OprFact::longConst(~TRACE_ID_META_BITS), id);
2967 #endif
2968 #ifdef TRACE_ID_SHIFT
2969   __ unsigned_shift_right(id, TRACE_ID_SHIFT, id);
2970 #endif
2971 
2972   __ move(id, rlock_result(x));
2973 }
2974 
2975 void LIRGenerator::do_getEventWriter(Intrinsic* x) {
2976   LabelObj* L_end = new LabelObj();
2977 
<span class="line-added">2978   // FIXME T_ADDRESS should actually be T_METADATA but it can&#39;t because the</span>
<span class="line-added">2979   // meaning of these two is mixed up (see JDK-8026837).</span>
2980   LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),
2981                                            in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),
<span class="line-modified">2982                                            T_ADDRESS);</span>
2983   LIR_Opr result = rlock_result(x);
<span class="line-modified">2984   __ move(LIR_OprFact::oopConst(NULL), result);</span>
<span class="line-modified">2985   LIR_Opr jobj = new_register(T_METADATA);</span>
<span class="line-modified">2986   __ move_wide(jobj_addr, jobj);</span>
<span class="line-added">2987   __ cmp(lir_cond_equal, jobj, LIR_OprFact::metadataConst(0));</span>
<span class="line-added">2988   __ branch(lir_cond_equal, L_end-&gt;label());</span>
2989 


2990   access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);
2991 
2992   __ branch_destination(L_end-&gt;label());
2993 }
2994 
2995 #endif
2996 
2997 
2998 void LIRGenerator::do_RuntimeCall(address routine, Intrinsic* x) {
2999   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
3000   // Enforce computation of _reserved_argument_area_size which is required on some platforms.
3001   BasicTypeList signature;
3002   CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
3003   LIR_Opr reg = result_register_for(x-&gt;type());
3004   __ call_runtime_leaf(routine, getThreadTemp(),
3005                        reg, new LIR_OprList());
3006   LIR_Opr result = rlock_result(x);
3007   __ move(reg, result);
3008 }
3009 
</pre>
<hr />
<pre>
3327   }
3328   increment_event_counter_impl(info, info-&gt;scope()-&gt;method(), step, right_n_bits(freq_log), bci, backedge, true);
3329 }
3330 
3331 void LIRGenerator::decrement_age(CodeEmitInfo* info) {
3332   ciMethod* method = info-&gt;scope()-&gt;method();
3333   MethodCounters* mc_adr = method-&gt;ensure_method_counters();
3334   if (mc_adr != NULL) {
3335     LIR_Opr mc = new_pointer_register();
3336     __ move(LIR_OprFact::intptrConst(mc_adr), mc);
3337     int offset = in_bytes(MethodCounters::nmethod_age_offset());
3338     LIR_Address* counter = new LIR_Address(mc, offset, T_INT);
3339     LIR_Opr result = new_register(T_INT);
3340     __ load(counter, result);
3341     __ sub(result, LIR_OprFact::intConst(1), result);
3342     __ store(result, counter);
3343     // DeoptimizeStub will reexecute from the current state in code info.
3344     CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,
3345                                          Deoptimization::Action_make_not_entrant);
3346     __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));
<span class="line-modified">3347     __ branch(lir_cond_lessEqual, deopt);</span>
3348   }
3349 }
3350 
3351 
3352 void LIRGenerator::increment_event_counter_impl(CodeEmitInfo* info,
3353                                                 ciMethod *method, LIR_Opr step, int frequency,
3354                                                 int bci, bool backedge, bool notify) {
3355   assert(frequency == 0 || is_power_of_2(frequency + 1), &quot;Frequency must be x^2 - 1 or 0&quot;);
3356   int level = _compilation-&gt;env()-&gt;comp_level();
3357   assert(level &gt; CompLevel_simple, &quot;Shouldn&#39;t be here&quot;);
3358 
3359   int offset = -1;
3360   LIR_Opr counter_holder = NULL;
3361   if (level == CompLevel_limited_profile) {
3362     MethodCounters* counters_adr = method-&gt;ensure_method_counters();
3363     if (counters_adr == NULL) {
3364       bailout(&quot;method counters allocation failed&quot;);
3365       return;
3366     }
3367     counter_holder = new_pointer_register();
</pre>
<hr />
<pre>
3374                                  MethodData::invocation_counter_offset());
3375     ciMethodData* md = method-&gt;method_data_or_null();
3376     assert(md != NULL, &quot;Sanity&quot;);
3377     __ metadata2reg(md-&gt;constant_encoding(), counter_holder);
3378   } else {
3379     ShouldNotReachHere();
3380   }
3381   LIR_Address* counter = new LIR_Address(counter_holder, offset, T_INT);
3382   LIR_Opr result = new_register(T_INT);
3383   __ load(counter, result);
3384   __ add(result, step, result);
3385   __ store(result, counter);
3386   if (notify &amp;&amp; (!backedge || UseOnStackReplacement)) {
3387     LIR_Opr meth = LIR_OprFact::metadataConst(method-&gt;constant_encoding());
3388     // The bci for info can point to cmp for if&#39;s we want the if bci
3389     CodeStub* overflow = new CounterOverflowStub(info, bci, meth);
3390     int freq = frequency &lt;&lt; InvocationCounter::count_shift;
3391     if (freq == 0) {
3392       if (!step-&gt;is_constant()) {
3393         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
<span class="line-modified">3394         __ branch(lir_cond_notEqual, overflow);</span>
3395       } else {
<span class="line-modified">3396         __ branch(lir_cond_always, overflow);</span>
3397       }
3398     } else {
3399       LIR_Opr mask = load_immediate(freq, T_INT);
3400       if (!step-&gt;is_constant()) {
3401         // If step is 0, make sure the overflow check below always fails
3402         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
3403         __ cmove(lir_cond_notEqual, result, LIR_OprFact::intConst(InvocationCounter::count_increment), result, T_INT);
3404       }
3405       __ logical_and(result, mask, result);
3406       __ cmp(lir_cond_equal, result, LIR_OprFact::intConst(0));
<span class="line-modified">3407       __ branch(lir_cond_equal, overflow);</span>
3408     }
3409     __ branch_destination(overflow-&gt;continuation());
3410   }
3411 }
3412 
3413 void LIRGenerator::do_RuntimeCall(RuntimeCall* x) {
3414   LIR_OprList* args = new LIR_OprList(x-&gt;number_of_arguments());
3415   BasicTypeList* signature = new BasicTypeList(x-&gt;number_of_arguments());
3416 
3417   if (x-&gt;pass_thread()) {
3418     signature-&gt;append(LP64_ONLY(T_LONG) NOT_LP64(T_INT));    // thread
3419     args-&gt;append(getThreadPointer());
3420   }
3421 
3422   for (int i = 0; i &lt; x-&gt;number_of_arguments(); i++) {
3423     Value a = x-&gt;argument_at(i);
3424     LIRItem* item = new LIRItem(a, this);
3425     item-&gt;load_item();
3426     args-&gt;append(item-&gt;result());
3427     signature-&gt;append(as_BasicType(a-&gt;type()));
</pre>
<hr />
<pre>
3501     ValueTag tag = x-&gt;x()-&gt;type()-&gt;tag();
3502     If::Condition cond = x-&gt;cond();
3503     LIRItem xitem(x-&gt;x(), this);
3504     LIRItem yitem(x-&gt;y(), this);
3505     LIRItem* xin = &amp;xitem;
3506     LIRItem* yin = &amp;yitem;
3507 
3508     assert(tag == intTag, &quot;Only integer deoptimizations are valid!&quot;);
3509 
3510     xin-&gt;load_item();
3511     yin-&gt;dont_load_item();
3512     set_no_result(x);
3513 
3514     LIR_Opr left = xin-&gt;result();
3515     LIR_Opr right = yin-&gt;result();
3516 
3517     CodeEmitInfo *info = state_for(x, x-&gt;state());
3518     CodeStub* stub = new PredicateFailedStub(info);
3519 
3520     __ cmp(lir_cond(cond), left, right);
<span class="line-modified">3521     __ branch(lir_cond(cond), stub);</span>
3522   }
3523 }
3524 
3525 
3526 LIR_Opr LIRGenerator::call_runtime(Value arg1, address entry, ValueType* result_type, CodeEmitInfo* info) {
3527   LIRItemList args(1);
3528   LIRItem value(arg1, this);
3529   args.append(&amp;value);
3530   BasicTypeList signature;
3531   signature.append(as_BasicType(arg1-&gt;type()));
3532 
3533   return call_runtime(&amp;signature, &amp;args, entry, result_type, info);
3534 }
3535 
3536 
3537 LIR_Opr LIRGenerator::call_runtime(Value arg1, Value arg2, address entry, ValueType* result_type, CodeEmitInfo* info) {
3538   LIRItemList args(2);
3539   LIRItem value1(arg1, this);
3540   LIRItem value2(arg2, this);
3541   args.append(&amp;value1);
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ci/ciField.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>