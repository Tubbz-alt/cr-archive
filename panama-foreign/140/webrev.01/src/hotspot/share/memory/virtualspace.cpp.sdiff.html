<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/virtualspace.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="metaspaceTracer.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="virtualspace.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/virtualspace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  49   size_t alignment;
  50   if (large_pages &amp;&amp; has_preferred_page_size) {
  51     alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
  52     // ReservedSpace initialization requires size to be aligned to the given
  53     // alignment. Align the size up.
  54     size = align_up(size, alignment);
  55   } else {
  56     // Don&#39;t force the alignment to be large page aligned,
  57     // since that will waste memory.
  58     alignment = os::vm_allocation_granularity();
  59   }
  60   initialize(size, alignment, large_pages, NULL, false);
  61 }
  62 
  63 ReservedSpace::ReservedSpace(size_t size, size_t alignment,
  64                              bool large,
  65                              char* requested_address) : _fd_for_heap(-1) {
  66   initialize(size, alignment, large, requested_address, false);
  67 }
  68 
<span class="line-removed">  69 ReservedSpace::ReservedSpace(size_t size, size_t alignment,</span>
<span class="line-removed">  70                              bool large,</span>
<span class="line-removed">  71                              bool executable) : _fd_for_heap(-1) {</span>
<span class="line-removed">  72   initialize(size, alignment, large, NULL, executable);</span>
<span class="line-removed">  73 }</span>
<span class="line-removed">  74 </span>
  75 ReservedSpace::ReservedSpace(char* base, size_t size, size_t alignment,
  76                              bool special, bool executable) : _fd_for_heap(-1) {
  77   assert((size % os::vm_allocation_granularity()) == 0,
  78          &quot;size not allocation aligned&quot;);
  79   _base = base;
  80   _size = size;
  81   _alignment = alignment;
  82   _noaccess_prefix = 0;
  83   _special = special;
  84   _executable = executable;
  85 }
  86 
  87 // Helper method
  88 static void unmap_or_release_memory(char* base, size_t size, bool is_file_mapped) {
  89   if (is_file_mapped) {
  90     if (!os::unmap_memory(base, size)) {
  91       fatal(&quot;os::unmap_memory failed&quot;);
  92     }
  93   } else if (!os::release_memory(base, size)) {
  94     fatal(&quot;os::release_memory failed&quot;);
</pre>
<hr />
<pre>
 629   assert(markWord::encode_pointer_as_mark(&amp;_base[size]).decode_pointer() == &amp;_base[size],
 630          &quot;area must be distinguishable from marks for mark-sweep&quot;);
 631 
 632   if (base() != NULL) {
 633     MemTracker::record_virtual_memory_type((address)base(), mtJavaHeap);
 634   }
 635 
 636   if (_fd_for_heap != -1) {
 637     os::close(_fd_for_heap);
 638   }
 639 }
 640 
 641 MemRegion ReservedHeapSpace::region() const {
 642   return MemRegion((HeapWord*)base(), (HeapWord*)end());
 643 }
 644 
 645 // Reserve space for code segment.  Same as Java heap only we mark this as
 646 // executable.
 647 ReservedCodeSpace::ReservedCodeSpace(size_t r_size,
 648                                      size_t rs_align,
<span class="line-modified"> 649                                      bool large) :</span>
<span class="line-modified"> 650   ReservedSpace(r_size, rs_align, large, /*executable*/ true) {</span>
 651   MemTracker::record_virtual_memory_type((address)base(), mtCode);
 652 }
 653 
 654 // VirtualSpace
 655 
 656 VirtualSpace::VirtualSpace() {
 657   _low_boundary           = NULL;
 658   _high_boundary          = NULL;
 659   _low                    = NULL;
 660   _high                   = NULL;
 661   _lower_high             = NULL;
 662   _middle_high            = NULL;
 663   _upper_high             = NULL;
 664   _lower_high_boundary    = NULL;
 665   _middle_high_boundary   = NULL;
 666   _upper_high_boundary    = NULL;
 667   _lower_alignment        = 0;
 668   _middle_alignment       = 0;
 669   _upper_alignment        = 0;
 670   _special                = false;
</pre>
<hr />
<pre>
1110 
1111     if (rs.special()) {
1112       small_page_write(rs.base(), size);
1113     }
1114 
1115     release_memory_for_test(rs);
1116   }
1117 
1118   static void test_reserved_space3(size_t size, size_t alignment, bool maybe_large) {
1119     if (size &lt; alignment) {
1120       // Tests might set -XX:LargePageSizeInBytes=&lt;small pages&gt; and cause unexpected input arguments for this test.
1121       assert((size_t)os::vm_page_size() == os::large_page_size(), &quot;Test needs further refinement&quot;);
1122       return;
1123     }
1124 
1125     assert(is_aligned(size, os::vm_allocation_granularity()), &quot;Must be at least AG aligned&quot;);
1126     assert(is_aligned(size, alignment), &quot;Must be at least aligned against alignment&quot;);
1127 
1128     bool large = maybe_large &amp;&amp; UseLargePages &amp;&amp; size &gt;= os::large_page_size();
1129 
<span class="line-modified">1130     ReservedSpace rs(size, alignment, large, false);</span>
1131 
1132     assert(rs.base() != NULL, &quot;Must be&quot;);
1133     assert(rs.size() == size, &quot;Must be&quot;);
1134 
1135     if (rs.special()) {
1136       small_page_write(rs.base(), size);
1137     }
1138 
1139     release_memory_for_test(rs);
1140   }
1141 
1142 
1143   static void test_reserved_space1() {
1144     size_t size = 2 * 1024 * 1024;
1145     size_t ag   = os::vm_allocation_granularity();
1146 
1147     test_reserved_space1(size,      ag);
1148     test_reserved_space1(size * 2,  ag);
1149     test_reserved_space1(size * 10, ag);
1150   }
</pre>
<hr />
<pre>
1238 
1239 
1240 class TestVirtualSpace : AllStatic {
1241   enum TestLargePages {
1242     Default,
1243     Disable,
1244     Reserve,
1245     Commit
1246   };
1247 
1248   static ReservedSpace reserve_memory(size_t reserve_size_aligned, TestLargePages mode) {
1249     switch(mode) {
1250     default:
1251     case Default:
1252     case Reserve:
1253       return ReservedSpace(reserve_size_aligned);
1254     case Disable:
1255     case Commit:
1256       return ReservedSpace(reserve_size_aligned,
1257                            os::vm_allocation_granularity(),
<span class="line-modified">1258                            /* large */ false, /* exec */ false);</span>
1259     }
1260   }
1261 
1262   static bool initialize_virtual_space(VirtualSpace&amp; vs, ReservedSpace rs, TestLargePages mode) {
1263     switch(mode) {
1264     default:
1265     case Default:
1266     case Reserve:
1267       return vs.initialize(rs, 0);
1268     case Disable:
1269       return vs.initialize_with_granularity(rs, 0, os::vm_page_size());
1270     case Commit:
1271       return vs.initialize_with_granularity(rs, 0, os::page_size_for_region_unaligned(rs.size(), 1));
1272     }
1273   }
1274 
1275  public:
1276   static void test_virtual_space_actual_committed_space(size_t reserve_size, size_t commit_size,
1277                                                         TestLargePages mode = Default) {
1278     size_t granularity = os::vm_allocation_granularity();
</pre>
<hr />
<pre>
1293     } else {
1294       assert_ge(vs.actual_committed_size(), commit_size);
1295       // Approximate the commit granularity.
1296       // Make sure that we don&#39;t commit using large pages
1297       // if large pages has been disabled for this VirtualSpace.
1298       size_t commit_granularity = (mode == Disable || !UseLargePages) ?
1299                                    os::vm_page_size() : os::large_page_size();
1300       assert_lt(vs.actual_committed_size(), commit_size + commit_granularity);
1301     }
1302 
1303     reserved.release();
1304   }
1305 
1306   static void test_virtual_space_actual_committed_space_one_large_page() {
1307     if (!UseLargePages) {
1308       return;
1309     }
1310 
1311     size_t large_page_size = os::large_page_size();
1312 
<span class="line-modified">1313     ReservedSpace reserved(large_page_size, large_page_size, true, false);</span>
1314 
1315     assert(reserved.is_reserved(), &quot;Must be&quot;);
1316 
1317     VirtualSpace vs;
1318     bool initialized = vs.initialize(reserved, 0);
1319     assert(initialized, &quot;Failed to initialize VirtualSpace&quot;);
1320 
1321     vs.expand_by(large_page_size, false);
1322 
1323     assert_equals(vs.actual_committed_size(), large_page_size);
1324 
1325     reserved.release();
1326   }
1327 
1328   static void test_virtual_space_actual_committed_space() {
1329     test_virtual_space_actual_committed_space(4 * K, 0);
1330     test_virtual_space_actual_committed_space(4 * K, 4 * K);
1331     test_virtual_space_actual_committed_space(8 * K, 0);
1332     test_virtual_space_actual_committed_space(8 * K, 4 * K);
1333     test_virtual_space_actual_committed_space(8 * K, 8 * K);
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  49   size_t alignment;
  50   if (large_pages &amp;&amp; has_preferred_page_size) {
  51     alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
  52     // ReservedSpace initialization requires size to be aligned to the given
  53     // alignment. Align the size up.
  54     size = align_up(size, alignment);
  55   } else {
  56     // Don&#39;t force the alignment to be large page aligned,
  57     // since that will waste memory.
  58     alignment = os::vm_allocation_granularity();
  59   }
  60   initialize(size, alignment, large_pages, NULL, false);
  61 }
  62 
  63 ReservedSpace::ReservedSpace(size_t size, size_t alignment,
  64                              bool large,
  65                              char* requested_address) : _fd_for_heap(-1) {
  66   initialize(size, alignment, large, requested_address, false);
  67 }
  68 






  69 ReservedSpace::ReservedSpace(char* base, size_t size, size_t alignment,
  70                              bool special, bool executable) : _fd_for_heap(-1) {
  71   assert((size % os::vm_allocation_granularity()) == 0,
  72          &quot;size not allocation aligned&quot;);
  73   _base = base;
  74   _size = size;
  75   _alignment = alignment;
  76   _noaccess_prefix = 0;
  77   _special = special;
  78   _executable = executable;
  79 }
  80 
  81 // Helper method
  82 static void unmap_or_release_memory(char* base, size_t size, bool is_file_mapped) {
  83   if (is_file_mapped) {
  84     if (!os::unmap_memory(base, size)) {
  85       fatal(&quot;os::unmap_memory failed&quot;);
  86     }
  87   } else if (!os::release_memory(base, size)) {
  88     fatal(&quot;os::release_memory failed&quot;);
</pre>
<hr />
<pre>
 623   assert(markWord::encode_pointer_as_mark(&amp;_base[size]).decode_pointer() == &amp;_base[size],
 624          &quot;area must be distinguishable from marks for mark-sweep&quot;);
 625 
 626   if (base() != NULL) {
 627     MemTracker::record_virtual_memory_type((address)base(), mtJavaHeap);
 628   }
 629 
 630   if (_fd_for_heap != -1) {
 631     os::close(_fd_for_heap);
 632   }
 633 }
 634 
 635 MemRegion ReservedHeapSpace::region() const {
 636   return MemRegion((HeapWord*)base(), (HeapWord*)end());
 637 }
 638 
 639 // Reserve space for code segment.  Same as Java heap only we mark this as
 640 // executable.
 641 ReservedCodeSpace::ReservedCodeSpace(size_t r_size,
 642                                      size_t rs_align,
<span class="line-modified"> 643                                      bool large) : ReservedSpace() {</span>
<span class="line-modified"> 644   initialize(r_size, rs_align, large, /*requested address*/ NULL, /*executable*/ true);</span>
 645   MemTracker::record_virtual_memory_type((address)base(), mtCode);
 646 }
 647 
 648 // VirtualSpace
 649 
 650 VirtualSpace::VirtualSpace() {
 651   _low_boundary           = NULL;
 652   _high_boundary          = NULL;
 653   _low                    = NULL;
 654   _high                   = NULL;
 655   _lower_high             = NULL;
 656   _middle_high            = NULL;
 657   _upper_high             = NULL;
 658   _lower_high_boundary    = NULL;
 659   _middle_high_boundary   = NULL;
 660   _upper_high_boundary    = NULL;
 661   _lower_alignment        = 0;
 662   _middle_alignment       = 0;
 663   _upper_alignment        = 0;
 664   _special                = false;
</pre>
<hr />
<pre>
1104 
1105     if (rs.special()) {
1106       small_page_write(rs.base(), size);
1107     }
1108 
1109     release_memory_for_test(rs);
1110   }
1111 
1112   static void test_reserved_space3(size_t size, size_t alignment, bool maybe_large) {
1113     if (size &lt; alignment) {
1114       // Tests might set -XX:LargePageSizeInBytes=&lt;small pages&gt; and cause unexpected input arguments for this test.
1115       assert((size_t)os::vm_page_size() == os::large_page_size(), &quot;Test needs further refinement&quot;);
1116       return;
1117     }
1118 
1119     assert(is_aligned(size, os::vm_allocation_granularity()), &quot;Must be at least AG aligned&quot;);
1120     assert(is_aligned(size, alignment), &quot;Must be at least aligned against alignment&quot;);
1121 
1122     bool large = maybe_large &amp;&amp; UseLargePages &amp;&amp; size &gt;= os::large_page_size();
1123 
<span class="line-modified">1124     ReservedSpace rs(size, alignment, large);</span>
1125 
1126     assert(rs.base() != NULL, &quot;Must be&quot;);
1127     assert(rs.size() == size, &quot;Must be&quot;);
1128 
1129     if (rs.special()) {
1130       small_page_write(rs.base(), size);
1131     }
1132 
1133     release_memory_for_test(rs);
1134   }
1135 
1136 
1137   static void test_reserved_space1() {
1138     size_t size = 2 * 1024 * 1024;
1139     size_t ag   = os::vm_allocation_granularity();
1140 
1141     test_reserved_space1(size,      ag);
1142     test_reserved_space1(size * 2,  ag);
1143     test_reserved_space1(size * 10, ag);
1144   }
</pre>
<hr />
<pre>
1232 
1233 
1234 class TestVirtualSpace : AllStatic {
1235   enum TestLargePages {
1236     Default,
1237     Disable,
1238     Reserve,
1239     Commit
1240   };
1241 
1242   static ReservedSpace reserve_memory(size_t reserve_size_aligned, TestLargePages mode) {
1243     switch(mode) {
1244     default:
1245     case Default:
1246     case Reserve:
1247       return ReservedSpace(reserve_size_aligned);
1248     case Disable:
1249     case Commit:
1250       return ReservedSpace(reserve_size_aligned,
1251                            os::vm_allocation_granularity(),
<span class="line-modified">1252                            /* large */ false);</span>
1253     }
1254   }
1255 
1256   static bool initialize_virtual_space(VirtualSpace&amp; vs, ReservedSpace rs, TestLargePages mode) {
1257     switch(mode) {
1258     default:
1259     case Default:
1260     case Reserve:
1261       return vs.initialize(rs, 0);
1262     case Disable:
1263       return vs.initialize_with_granularity(rs, 0, os::vm_page_size());
1264     case Commit:
1265       return vs.initialize_with_granularity(rs, 0, os::page_size_for_region_unaligned(rs.size(), 1));
1266     }
1267   }
1268 
1269  public:
1270   static void test_virtual_space_actual_committed_space(size_t reserve_size, size_t commit_size,
1271                                                         TestLargePages mode = Default) {
1272     size_t granularity = os::vm_allocation_granularity();
</pre>
<hr />
<pre>
1287     } else {
1288       assert_ge(vs.actual_committed_size(), commit_size);
1289       // Approximate the commit granularity.
1290       // Make sure that we don&#39;t commit using large pages
1291       // if large pages has been disabled for this VirtualSpace.
1292       size_t commit_granularity = (mode == Disable || !UseLargePages) ?
1293                                    os::vm_page_size() : os::large_page_size();
1294       assert_lt(vs.actual_committed_size(), commit_size + commit_granularity);
1295     }
1296 
1297     reserved.release();
1298   }
1299 
1300   static void test_virtual_space_actual_committed_space_one_large_page() {
1301     if (!UseLargePages) {
1302       return;
1303     }
1304 
1305     size_t large_page_size = os::large_page_size();
1306 
<span class="line-modified">1307     ReservedSpace reserved(large_page_size, large_page_size, true);</span>
1308 
1309     assert(reserved.is_reserved(), &quot;Must be&quot;);
1310 
1311     VirtualSpace vs;
1312     bool initialized = vs.initialize(reserved, 0);
1313     assert(initialized, &quot;Failed to initialize VirtualSpace&quot;);
1314 
1315     vs.expand_by(large_page_size, false);
1316 
1317     assert_equals(vs.actual_committed_size(), large_page_size);
1318 
1319     reserved.release();
1320   }
1321 
1322   static void test_virtual_space_actual_committed_space() {
1323     test_virtual_space_actual_committed_space(4 * K, 0);
1324     test_virtual_space_actual_committed_space(4 * K, 4 * K);
1325     test_virtual_space_actual_committed_space(8 * K, 0);
1326     test_virtual_space_actual_committed_space(8 * K, 4 * K);
1327     test_virtual_space_actual_committed_space(8 * K, 8 * K);
</pre>
</td>
</tr>
</table>
<center><a href="metaspaceTracer.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="virtualspace.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>