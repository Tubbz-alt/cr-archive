<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/aarch64/gc/shared/barrierSetNMethod_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;code/codeCache.hpp&quot;
 27 #include &quot;code/nativeInst.hpp&quot;
 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 29 #include &quot;logging/log.hpp&quot;
 30 #include &quot;memory/resourceArea.hpp&quot;
 31 #include &quot;runtime/sharedRuntime.hpp&quot;
 32 #include &quot;runtime/thread.hpp&quot;
 33 #include &quot;utilities/align.hpp&quot;
 34 #include &quot;utilities/debug.hpp&quot;
 35 
 36 class NativeNMethodBarrier: public NativeInstruction {
 37   address instruction_address() const { return addr_at(0); }
 38 
 39   int *guard_addr() {
 40     return reinterpret_cast&lt;int*&gt;(instruction_address() + 10 * 4);
 41   }
 42 
 43 public:
 44   int get_value() {
 45     return Atomic::load_acquire(guard_addr());
 46   }
 47 
 48   void set_value(int value) {
 49     Atomic::release_store(guard_addr(), value);
 50   }
 51 
 52   void verify() const;
 53 };
 54 
 55 // Store the instruction bitmask, bits and name for checking the barrier.
 56 struct CheckInsn {
 57   uint32_t mask;
 58   uint32_t bits;
 59   const char *name;
 60 };
 61 
 62 static const struct CheckInsn barrierInsn[] = {
 63   { 0xff000000, 0x18000000, &quot;ldr (literal)&quot; },
 64   { 0xfffff0ff, 0xd50330bf, &quot;dmb&quot; },
 65   { 0xffc00000, 0xb9400000, &quot;ldr&quot;},
 66   { 0x7f20001f, 0x6b00001f, &quot;cmp&quot;},
 67   { 0xff00001f, 0x54000000, &quot;b.eq&quot;},
 68   { 0xff800000, 0xd2800000, &quot;mov&quot;},
 69   { 0xff800000, 0xf2800000, &quot;movk&quot;},
 70   { 0xff800000, 0xf2800000, &quot;movk&quot;},
 71   { 0xfffffc1f, 0xd63f0000, &quot;blr&quot;},
 72   { 0xfc000000, 0x14000000, &quot;b&quot;}
 73 };
 74 
 75 // The encodings must match the instructions emitted by
 76 // BarrierSetAssembler::nmethod_entry_barrier. The matching ignores the specific
 77 // register numbers and immediate values in the encoding.
 78 void NativeNMethodBarrier::verify() const {
 79   intptr_t addr = (intptr_t) instruction_address();
 80   for(unsigned int i = 0; i &lt; sizeof(barrierInsn)/sizeof(struct CheckInsn); i++ ) {
 81     uint32_t inst = *((uint32_t*) addr);
 82     if ((inst &amp; barrierInsn[i].mask) != barrierInsn[i].bits) {
 83       tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Code: 0x%x&quot;, addr, inst);
 84       fatal(&quot;not an %s instruction.&quot;, barrierInsn[i].name);
 85     }
 86     addr +=4;
 87   }
 88 }
 89 
 90 
 91 /* We&#39;re called from an nmethod when we need to deoptimize it. We do
 92    this by throwing away the nmethod&#39;s frame and jumping to the
 93    ic_miss stub. This looks like there has been an IC miss at the
 94    entry of the nmethod, so we resolve the call, which will fall back
 95    to the interpreter if the nmethod has been unloaded. */
 96 void BarrierSetNMethod::deoptimize(nmethod* nm, address* return_address_ptr) {
 97 
 98   typedef struct {
 99     intptr_t *sp; intptr_t *fp; address lr; address pc;
100   } frame_pointers_t;
101 
102   frame_pointers_t *new_frame = (frame_pointers_t *)(return_address_ptr - 5);
103 
104   JavaThread *thread = (JavaThread*)Thread::current();
105   RegisterMap reg_map(thread, false);
106   frame frame = thread-&gt;last_frame();
107 
108   assert(frame.is_compiled_frame() || frame.is_native_frame(), &quot;must be&quot;);
109   assert(frame.cb() == nm, &quot;must be&quot;);
110   frame = frame.sender(&amp;reg_map);
111 
112   LogTarget(Trace, nmethod, barrier) out;
113   if (out.is_enabled()) {
114     Thread* thread = Thread::current();
115     assert(thread-&gt;is_Java_thread(), &quot;must be JavaThread&quot;);
116     JavaThread* jth = (JavaThread*) thread;
117     ResourceMark mark;
118     log_trace(nmethod, barrier)(&quot;deoptimize(nmethod: %s(%p), return_addr: %p, osr: %d, thread: %p(%s), making rsp: %p) -&gt; %p&quot;,
119                                 nm-&gt;method()-&gt;name_and_sig_as_C_string(),
120                                 nm, *(address *) return_address_ptr, nm-&gt;is_osr_method(), jth,
121                                 jth-&gt;get_thread_name(), frame.sp(), nm-&gt;verified_entry_point());
122   }
123 
124   new_frame-&gt;sp = frame.sp();
125   new_frame-&gt;fp = frame.fp();
126   new_frame-&gt;lr = frame.pc();
127   new_frame-&gt;pc = SharedRuntime::get_handle_wrong_method_stub();
128 }
129 
130 // This is the offset of the entry barrier from where the frame is completed.
131 // If any code changes between the end of the verified entry where the entry
132 // barrier resides, and the completion of the frame, then
133 // NativeNMethodCmpBarrier::verify() will immediately complain when it does
134 // not find the expected native instruction at this offset, which needs updating.
135 // Note that this offset is invariant of PreserveFramePointer.
136 
137 static const int entry_barrier_offset = -4 * 11;
138 
139 static NativeNMethodBarrier* native_nmethod_barrier(nmethod* nm) {
140   address barrier_address = nm-&gt;code_begin() + nm-&gt;frame_complete_offset() + entry_barrier_offset;
141   NativeNMethodBarrier* barrier = reinterpret_cast&lt;NativeNMethodBarrier*&gt;(barrier_address);
142   debug_only(barrier-&gt;verify());
143   return barrier;
144 }
145 
146 void BarrierSetNMethod::disarm(nmethod* nm) {
147   if (!supports_entry_barrier(nm)) {
148     return;
149   }
150 
151   // Disarms the nmethod guard emitted by BarrierSetAssembler::nmethod_entry_barrier.
152   // Symmetric &quot;LDR; DMB ISHLD&quot; is in the nmethod barrier.
153   NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);
154 
155   barrier-&gt;set_value(disarmed_value());
156 }
157 
158 bool BarrierSetNMethod::is_armed(nmethod* nm) {
159   if (!supports_entry_barrier(nm)) {
160     return false;
161   }
162 
163   NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);
164   return barrier-&gt;get_value() != disarmed_value();
165 }
    </pre>
  </body>
</html>