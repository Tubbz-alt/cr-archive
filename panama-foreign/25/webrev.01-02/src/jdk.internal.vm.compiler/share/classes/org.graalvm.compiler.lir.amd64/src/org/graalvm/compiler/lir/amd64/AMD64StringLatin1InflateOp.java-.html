<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64StringLatin1InflateOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.amd64.AMD64.k1;
 28 import static jdk.vm.ci.amd64.AMD64.k2;
 29 import static jdk.vm.ci.amd64.AMD64.rdi;
 30 import static jdk.vm.ci.amd64.AMD64.rdx;
 31 import static jdk.vm.ci.amd64.AMD64.rsi;
 32 import static jdk.vm.ci.code.ValueUtil.asRegister;
 33 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 34 
 35 import java.util.EnumSet;
 36 
 37 import org.graalvm.compiler.asm.Label;
 38 import org.graalvm.compiler.asm.amd64.AMD64Address;
 39 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 40 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 41 import org.graalvm.compiler.core.common.LIRKind;
 42 import org.graalvm.compiler.lir.LIRInstructionClass;
 43 import org.graalvm.compiler.lir.Opcode;
 44 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 45 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 46 
 47 import jdk.vm.ci.amd64.AMD64;
 48 import jdk.vm.ci.amd64.AMD64.CPUFeature;
 49 import jdk.vm.ci.amd64.AMD64Kind;
 50 import jdk.vm.ci.code.Register;
 51 import jdk.vm.ci.code.TargetDescription;
 52 import jdk.vm.ci.meta.Value;
 53 
 54 @Opcode(&quot;AMD64_STRING_INFLATE&quot;)
 55 public final class AMD64StringLatin1InflateOp extends AMD64LIRInstruction {
 56     public static final LIRInstructionClass&lt;AMD64StringLatin1InflateOp&gt; TYPE = LIRInstructionClass.create(AMD64StringLatin1InflateOp.class);
 57 
 58     @Use({REG}) private Value rsrc;
 59     @Use({REG}) private Value rdst;
 60     @Use({REG}) private Value rlen;
 61 
 62     @Temp({REG}) private Value rsrcTemp;
 63     @Temp({REG}) private Value rdstTemp;
 64     @Temp({REG}) private Value rlenTemp;
 65 
 66     @Temp({REG}) private Value vtmp1;
 67     @Temp({REG}) private Value rtmp2;
 68 
 69     public AMD64StringLatin1InflateOp(LIRGeneratorTool tool, Value src, Value dst, Value len) {
 70         super(TYPE);
 71 
 72         assert asRegister(src).equals(rsi);
 73         assert asRegister(dst).equals(rdi);
 74         assert asRegister(len).equals(rdx);
 75 
 76         rsrcTemp = rsrc = src;
 77         rdstTemp = rdst = dst;
 78         rlenTemp = rlen = len;
 79 
 80         vtmp1 = useAVX512ForStringInflateCompress(tool.target()) ? tool.newVariable(LIRKind.value(AMD64Kind.V512_BYTE)) : tool.newVariable(LIRKind.value(AMD64Kind.V128_BYTE));
 81         rtmp2 = tool.newVariable(LIRKind.value(AMD64Kind.DWORD));
 82     }
 83 
 84     @Override
 85     public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 86         Register src = asRegister(rsrc);
 87         Register dst = asRegister(rdst);
 88         Register len = asRegister(rlen);
 89 
 90         Register tmp1 = asRegister(vtmp1);
 91         Register tmp2 = asRegister(rtmp2);
 92 
 93         byteArrayInflate(masm, src, dst, len, tmp1, tmp2);
 94     }
 95 
 96     public static boolean useAVX512ForStringInflateCompress(TargetDescription target) {
 97         EnumSet&lt;CPUFeature&gt; features = ((AMD64) target.arch).getFeatures();
 98         return features.contains(AMD64.CPUFeature.AVX512BW) &amp;&amp;
 99                         features.contains(AMD64.CPUFeature.AVX512VL) &amp;&amp;
100                         features.contains(AMD64.CPUFeature.BMI2);
101     }
102 
103     /**
104      * Inflate a Latin1 string using a byte[] array representation into a UTF16 string using a
105      * char[] array representation.
106      *
107      * @param masm the assembler
108      * @param src (rsi) the start address of source byte[] to be inflated
109      * @param dst (rdi) the start address of destination char[] array
110      * @param len (rdx) the length
111      * @param vtmp (xmm) temporary xmm register
112      * @param tmp (gpr) temporary gpr register
113      */
114     private static void byteArrayInflate(AMD64MacroAssembler masm, Register src, Register dst, Register len, Register vtmp, Register tmp) {
115         assert vtmp.getRegisterCategory().equals(AMD64.XMM);
116 
117         Label labelDone = new Label();
118         Label labelBelowThreshold = new Label();
119 
120         assert src.number != dst.number &amp;&amp; src.number != len.number &amp;&amp; src.number != tmp.number;
121         assert dst.number != len.number &amp;&amp; dst.number != tmp.number;
122         assert len.number != tmp.number;
123 
124         if (useAVX512ForStringInflateCompress(masm.target)) {
125             // If the length of the string is less than 16, we chose not to use the
126             // AVX512 instructions.
127             masm.testl(len, -16);
128             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelBelowThreshold);
129 
130             Label labelAvx512Tail = new Label();
131             // Test for suitable number chunks with respect to the size of the vector
132             // operation, mask off remaining number of chars (bytes) to inflate (such
133             // that &#39;len&#39; will always hold the number of bytes left to inflate) after
134             // committing to the vector loop.
135             // Adjust vector pointers to upper address bounds and inverse loop index.
136             // This will keep the loop condition simple.
137             //
138             // NOTE: The above idiom/pattern is used in all the loops below.
139 
140             masm.movl(tmp, len);
141             masm.andl(tmp, -32);     // The vector count (in chars).
142             masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelAvx512Tail);
143             masm.andl(len, 32 - 1);  // The tail count (in chars).
144 
145             masm.leaq(src, new AMD64Address(src, tmp, AMD64Address.Scale.Times1));
146             masm.leaq(dst, new AMD64Address(dst, tmp, AMD64Address.Scale.Times2));
147             masm.negq(tmp);
148 
149             Label labelAvx512Loop = new Label();
150             // Inflate 32 chars per iteration, reading 256-bit compact vectors
151             // and writing 512-bit inflated ditto.
152             masm.bind(labelAvx512Loop);
153             masm.evpmovzxbw(vtmp, new AMD64Address(src, tmp, AMD64Address.Scale.Times1));
154             masm.evmovdqu16(new AMD64Address(dst, tmp, AMD64Address.Scale.Times2), vtmp);
155             masm.addq(tmp, 32);
156             masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelAvx512Loop);
157 
158             masm.bind(labelAvx512Tail);
159             // All done if the tail count is zero.
160             masm.testl(len, len);
161             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelDone);
162 
163             masm.kmovq(k2, k1);      // Save k1
164 
165             // Compute (1 &lt;&lt; N) - 1 = ~(~0 &lt;&lt; N), where N is the remaining number
166             // of characters to process.
167             masm.movl(tmp, -1);
168             masm.shlxl(tmp, tmp, len);
169             masm.notl(tmp);
170 
171             masm.kmovd(k1, tmp);
172             masm.evpmovzxbw(vtmp, k1, new AMD64Address(src));
173             masm.evmovdqu16(new AMD64Address(dst), k1, vtmp);
174             masm.kmovq(k1, k2);      // Restore k1
175             masm.jmp(labelDone);
176         }
177 
178         if (masm.supports(AMD64.CPUFeature.SSE4_1)) {
179 
180             Label labelSSETail = new Label();
181 
182             if (masm.supports(AMD64.CPUFeature.AVX2)) {
183 
184                 Label labelAvx2Tail = new Label();
185 
186                 masm.movl(tmp, len);
187                 masm.andl(tmp, -16);
188                 masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelAvx2Tail);
189                 masm.andl(len, 16 - 1);
190 
191                 masm.leaq(src, new AMD64Address(src, tmp, AMD64Address.Scale.Times1));
192                 masm.leaq(dst, new AMD64Address(dst, tmp, AMD64Address.Scale.Times2));
193                 masm.negq(tmp);
194 
195                 Label labelAvx2Loop = new Label();
196                 // Inflate 16 bytes (chars) per iteration, reading 128-bit compact vectors
197                 // and writing 256-bit inflated ditto.
198                 masm.bind(labelAvx2Loop);
199                 masm.vpmovzxbw(vtmp, new AMD64Address(src, tmp, AMD64Address.Scale.Times1));
200                 masm.vmovdqu(new AMD64Address(dst, tmp, AMD64Address.Scale.Times2), vtmp);
201                 masm.addq(tmp, 16);
202                 masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelAvx2Loop);
203 
204                 masm.bind(labelBelowThreshold);
205                 masm.bind(labelAvx2Tail);
206 
207                 masm.movl(tmp, len);
208                 masm.andl(tmp, -8);
209                 masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelSSETail);
210                 masm.andl(len, 8 - 1);
211 
212                 // Inflate another 8 bytes before final tail copy.
213                 masm.pmovzxbw(vtmp, new AMD64Address(src));
214                 masm.movdqu(new AMD64Address(dst), vtmp);
215                 masm.addq(src, 8);
216                 masm.addq(dst, 16);
217 
218                 // Fall-through to labelSSETail.
219             } else {
220                 // When there is no AVX2 support available, we use AVX/SSE support to
221                 // inflate into maximum 128-bits per operation.
222 
223                 masm.movl(tmp, len);
224                 masm.andl(tmp, -8);
225                 masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelSSETail);
226                 masm.andl(len, 8 - 1);
227 
228                 masm.leaq(src, new AMD64Address(src, tmp, AMD64Address.Scale.Times1));
229                 masm.leaq(dst, new AMD64Address(dst, tmp, AMD64Address.Scale.Times2));
230                 masm.negq(tmp);
231 
232                 Label labelSSECopy8Loop = new Label();
233                 // Inflate 8 bytes (chars) per iteration, reading 64-bit compact vectors
234                 // and writing 128-bit inflated ditto.
235                 masm.bind(labelSSECopy8Loop);
236                 masm.pmovzxbw(vtmp, new AMD64Address(src, tmp, AMD64Address.Scale.Times1));
237                 masm.movdqu(new AMD64Address(dst, tmp, AMD64Address.Scale.Times2), vtmp);
238                 masm.addq(tmp, 8);
239                 masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelSSECopy8Loop);
240 
241                 // Fall-through to labelSSETail.
242             }
243 
244             Label labelCopyChars = new Label();
245 
246             masm.bind(labelSSETail);
247             masm.cmpl(len, 4);
248             masm.jccb(AMD64Assembler.ConditionFlag.Less, labelCopyChars);
249 
250             masm.movdl(vtmp, new AMD64Address(src));
251             masm.pmovzxbw(vtmp, vtmp);
252             masm.movq(new AMD64Address(dst), vtmp);
253             masm.subq(len, 4);
254             masm.addq(src, 4);
255             masm.addq(dst, 8);
256 
257             masm.bind(labelCopyChars);
258         }
259 
260         // Inflate any remaining characters (bytes) using a vanilla implementation.
261         masm.testl(len, len);
262         masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelDone);
263         masm.leaq(src, new AMD64Address(src, len, AMD64Address.Scale.Times1));
264         masm.leaq(dst, new AMD64Address(dst, len, AMD64Address.Scale.Times2));
265         masm.negq(len);
266 
267         Label labelCopyCharsLoop = new Label();
268         // Inflate a single byte (char) per iteration.
269         masm.bind(labelCopyCharsLoop);
270         masm.movzbl(tmp, new AMD64Address(src, len, AMD64Address.Scale.Times1));
271         masm.movw(new AMD64Address(dst, len, AMD64Address.Scale.Times2), tmp);
272         masm.incrementq(len, 1);
273         masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelCopyCharsLoop);
274 
275         masm.bind(labelDone);
276     }
277 
278     @Override
279     public boolean needsClearUpperVectorRegisters() {
280         return true;
281     }
282 }
    </pre>
  </body>
</html>