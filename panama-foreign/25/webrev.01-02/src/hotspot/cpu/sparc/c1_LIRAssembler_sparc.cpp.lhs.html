<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/sparc/c1_LIRAssembler_sparc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;c1/c1_Compilation.hpp&quot;
  28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  30 #include &quot;c1/c1_Runtime1.hpp&quot;
  31 #include &quot;c1/c1_ValueStack.hpp&quot;
  32 #include &quot;ci/ciArrayKlass.hpp&quot;
  33 #include &quot;ci/ciInstance.hpp&quot;
  34 #include &quot;gc/shared/collectedHeap.hpp&quot;
  35 #include &quot;memory/universe.hpp&quot;
  36 #include &quot;nativeInst_sparc.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  40 #include &quot;runtime/jniHandles.inline.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.inline.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
<a name="1" id="anc1"></a>
  43 
  44 #define __ _masm-&gt;
  45 
  46 
  47 //------------------------------------------------------------
  48 
  49 
  50 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
  51   if (opr-&gt;is_constant()) {
  52     LIR_Const* constant = opr-&gt;as_constant_ptr();
  53     switch (constant-&gt;type()) {
  54       case T_INT: {
  55         jint value = constant-&gt;as_jint();
  56         return Assembler::is_simm13(value);
  57       }
  58 
  59       default:
  60         return false;
  61     }
  62   }
  63   return false;
  64 }
  65 
  66 
  67 bool LIR_Assembler::is_single_instruction(LIR_Op* op) {
  68   switch (op-&gt;code()) {
  69     case lir_null_check:
  70     return true;
  71 
  72 
  73     case lir_add:
  74     case lir_ushr:
  75     case lir_shr:
  76     case lir_shl:
  77       // integer shifts and adds are always one instruction
  78       return op-&gt;result_opr()-&gt;is_single_cpu();
  79 
  80 
  81     case lir_move: {
  82       LIR_Op1* op1 = op-&gt;as_Op1();
  83       LIR_Opr src = op1-&gt;in_opr();
  84       LIR_Opr dst = op1-&gt;result_opr();
  85 
  86       if (src == dst) {
  87         NEEDS_CLEANUP;
  88         // this works around a problem where moves with the same src and dst
  89         // end up in the delay slot and then the assembler swallows the mov
  90         // since it has no effect and then it complains because the delay slot
  91         // is empty.  returning false stops the optimizer from putting this in
  92         // the delay slot
  93         return false;
  94       }
  95 
  96       // don&#39;t put moves involving oops into the delay slot since the VerifyOops code
  97       // will make it much larger than a single instruction.
  98       if (VerifyOops) {
  99         return false;
 100       }
 101 
 102       if (src-&gt;is_double_cpu() || dst-&gt;is_double_cpu() || op1-&gt;patch_code() != lir_patch_none ||
 103           ((src-&gt;is_double_fpu() || dst-&gt;is_double_fpu()) &amp;&amp; op1-&gt;move_kind() != lir_move_normal)) {
 104         return false;
 105       }
 106 
 107       if (UseCompressedOops) {
 108         if (dst-&gt;is_address() &amp;&amp; !dst-&gt;is_stack() &amp;&amp; is_reference_type(dst-&gt;type())) return false;
 109         if (src-&gt;is_address() &amp;&amp; !src-&gt;is_stack() &amp;&amp; is_reference_type(src-&gt;type())) return false;
 110       }
 111 
 112       if (UseCompressedClassPointers) {
 113         if (src-&gt;is_address() &amp;&amp; !src-&gt;is_stack() &amp;&amp; src-&gt;type() == T_ADDRESS &amp;&amp;
 114             src-&gt;as_address_ptr()-&gt;disp() == oopDesc::klass_offset_in_bytes()) return false;
 115       }
 116 
 117       if (dst-&gt;is_register()) {
 118         if (src-&gt;is_address() &amp;&amp; Assembler::is_simm13(src-&gt;as_address_ptr()-&gt;disp())) {
 119           return !PatchALot;
 120         } else if (src-&gt;is_single_stack()) {
 121           return true;
 122         }
 123       }
 124 
 125       if (src-&gt;is_register()) {
 126         if (dst-&gt;is_address() &amp;&amp; Assembler::is_simm13(dst-&gt;as_address_ptr()-&gt;disp())) {
 127           return !PatchALot;
 128         } else if (dst-&gt;is_single_stack()) {
 129           return true;
 130         }
 131       }
 132 
 133       if (dst-&gt;is_register() &amp;&amp;
 134           ((src-&gt;is_register() &amp;&amp; src-&gt;is_single_word() &amp;&amp; src-&gt;is_same_type(dst)) ||
 135            (src-&gt;is_constant() &amp;&amp; LIR_Assembler::is_small_constant(op-&gt;as_Op1()-&gt;in_opr())))) {
 136         return true;
 137       }
 138 
 139       return false;
 140     }
 141 
 142     default:
 143       return false;
 144   }
 145   ShouldNotReachHere();
 146 }
 147 
 148 
 149 LIR_Opr LIR_Assembler::receiverOpr() {
 150   return FrameMap::O0_oop_opr;
 151 }
 152 
 153 
 154 LIR_Opr LIR_Assembler::osrBufferPointer() {
 155   return FrameMap::I0_opr;
 156 }
 157 
 158 
 159 int LIR_Assembler::initial_frame_size_in_bytes() const {
 160   return in_bytes(frame_map()-&gt;framesize_in_bytes());
 161 }
 162 
 163 
 164 // inline cache check: the inline cached class is in G5_inline_cache_reg(G5);
 165 // we fetch the class of the receiver (O0) and compare it with the cached class.
 166 // If they do not match we jump to slow case.
 167 int LIR_Assembler::check_icache() {
 168   int offset = __ offset();
 169   __ inline_cache_check(O0, G5_inline_cache_reg);
 170   return offset;
 171 }
 172 
 173 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 174   ShouldNotReachHere(); // not implemented
 175 }
 176 
 177 void LIR_Assembler::osr_entry() {
 178   // On-stack-replacement entry sequence (interpreter frame layout described in interpreter_sparc.cpp):
 179   //
 180   //   1. Create a new compiled activation.
 181   //   2. Initialize local variables in the compiled activation.  The expression stack must be empty
 182   //      at the osr_bci; it is not initialized.
 183   //   3. Jump to the continuation address in compiled code to resume execution.
 184 
 185   // OSR entry point
 186   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 187   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 188   ValueStack* entry_state = osr_entry-&gt;end()-&gt;state();
 189   int number_of_locks = entry_state-&gt;locks_size();
 190 
 191   // Create a frame for the compiled activation.
 192   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 193 
 194   // OSR buffer is
 195   //
 196   // locals[nlocals-1..0]
 197   // monitors[number_of_locks-1..0]
 198   //
 199   // locals is a direct copy of the interpreter frame so in the osr buffer
 200   // so first slot in the local array is the last local from the interpreter
 201   // and last slot is local[0] (receiver) from the interpreter
 202   //
 203   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 204   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 205   // in the interpreter frame (the method lock if a sync method)
 206 
 207   // Initialize monitors in the compiled activation.
 208   //   I0: pointer to osr buffer
 209   //
 210   // All other registers are dead at this point and the locals will be
 211   // copied into place by code emitted in the IR.
 212 
 213   Register OSR_buf = osrBufferPointer()-&gt;as_register();
 214   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 215     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 216       (2 * BytesPerWord) * (number_of_locks - 1);
 217     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 218     // the OSR buffer using 2 word entries: first the lock and then
 219     // the oop.
 220     for (int i = 0; i &lt; number_of_locks; i++) {
 221       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 222 #ifdef ASSERT
 223       // verify the interpreter&#39;s monitor has a non-null object
 224       {
 225         Label L;
 226         __ ld_ptr(OSR_buf, slot_offset + 1*BytesPerWord, O7);
 227         __ cmp_and_br_short(O7, G0, Assembler::notEqual, Assembler::pt, L);
 228         __ stop(&quot;locked object is NULL&quot;);
 229         __ bind(L);
 230       }
 231 #endif // ASSERT
 232       // Copy the lock field into the compiled activation.
 233       __ ld_ptr(OSR_buf, slot_offset + 0, O7);
 234       __ st_ptr(O7, frame_map()-&gt;address_for_monitor_lock(i));
 235       __ ld_ptr(OSR_buf, slot_offset + 1*BytesPerWord, O7);
 236       __ st_ptr(O7, frame_map()-&gt;address_for_monitor_object(i));
 237     }
 238   }
 239 }
 240 
 241 
 242 // --------------------------------------------------------------------------------------------
 243 
 244 void LIR_Assembler::monitorexit(LIR_Opr obj_opr, LIR_Opr lock_opr, Register hdr, int monitor_no) {
 245   if (!GenerateSynchronizationCode) return;
 246 
 247   Register obj_reg = obj_opr-&gt;as_register();
 248   Register lock_reg = lock_opr-&gt;as_register();
 249 
 250   Address mon_addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
 251   Register reg = mon_addr.base();
 252   int offset = mon_addr.disp();
 253   // compute pointer to BasicLock
 254   if (mon_addr.is_simm13()) {
 255     __ add(reg, offset, lock_reg);
 256   }
 257   else {
 258     __ set(offset, lock_reg);
 259     __ add(reg, lock_reg, lock_reg);
 260   }
 261   // unlock object
 262   MonitorAccessStub* slow_case = new MonitorExitStub(lock_opr, UseFastLocking, monitor_no);
 263   // _slow_case_stubs-&gt;append(slow_case);
 264   // temporary fix: must be created after exceptionhandler, therefore as call stub
 265   _slow_case_stubs-&gt;append(slow_case);
 266   if (UseFastLocking) {
 267     // try inlined fast unlocking first, revert to slow locking if it fails
 268     // note: lock_reg points to the displaced header since the displaced header offset is 0!
 269     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
 270     __ unlock_object(hdr, obj_reg, lock_reg, *slow_case-&gt;entry());
 271   } else {
 272     // always do slow unlocking
 273     // note: the slow unlocking code could be inlined here, however if we use
 274     //       slow unlocking, speed doesn&#39;t matter anyway and this solution is
 275     //       simpler and requires less duplicated code - additionally, the
 276     //       slow unlocking code is the same in either case which simplifies
 277     //       debugging
 278     __ br(Assembler::always, false, Assembler::pt, *slow_case-&gt;entry());
 279     __ delayed()-&gt;nop();
 280   }
 281   // done
 282   __ bind(*slow_case-&gt;continuation());
 283 }
 284 
 285 
 286 int LIR_Assembler::emit_exception_handler() {
 287   // if the last instruction is a call (typically to do a throw which
 288   // is coming at the end after block reordering) the return address
 289   // must still point into the code area in order to avoid assertion
 290   // failures when searching for the corresponding bci =&gt; add a nop
 291   // (was bug 5/14/1999 - gri)
 292   __ nop();
 293 
 294   // generate code for exception handler
 295   ciMethod* method = compilation()-&gt;method();
 296 
 297   address handler_base = __ start_a_stub(exception_handler_size());
 298 
 299   if (handler_base == NULL) {
 300     // not enough space left for the handler
 301     bailout(&quot;exception handler overflow&quot;);
 302     return -1;
 303   }
 304 
 305   int offset = code_offset();
 306 
 307   __ call(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id), relocInfo::runtime_call_type);
 308   __ delayed()-&gt;nop();
 309   __ should_not_reach_here();
 310   guarantee(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 311   __ end_a_stub();
 312 
 313   return offset;
 314 }
 315 
 316 
 317 // Emit the code to remove the frame from the stack in the exception
 318 // unwind path.
 319 int LIR_Assembler::emit_unwind_handler() {
 320 #ifndef PRODUCT
 321   if (CommentedAssembly) {
 322     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 323   }
 324 #endif
 325 
 326   int offset = code_offset();
 327 
 328   // Fetch the exception from TLS and clear out exception related thread state
 329   __ ld_ptr(G2_thread, in_bytes(JavaThread::exception_oop_offset()), O0);
 330   __ st_ptr(G0, G2_thread, in_bytes(JavaThread::exception_oop_offset()));
 331   __ st_ptr(G0, G2_thread, in_bytes(JavaThread::exception_pc_offset()));
 332 
 333   __ bind(_unwind_handler_entry);
 334   __ verify_not_null_oop(O0);
 335   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 336     __ mov(O0, I0);  // Preserve the exception
 337   }
 338 
 339   // Preform needed unlocking
 340   MonitorExitStub* stub = NULL;
 341   if (method()-&gt;is_synchronized()) {
 342     monitor_address(0, FrameMap::I1_opr);
 343     stub = new MonitorExitStub(FrameMap::I1_opr, true, 0);
 344     __ unlock_object(I3, I2, I1, *stub-&gt;entry());
 345     __ bind(*stub-&gt;continuation());
 346   }
 347 
 348   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 349     __ mov(G2_thread, O0);
 350     __ save_thread(I1); // need to preserve thread in G2 across
 351                         // runtime call
 352     metadata2reg(method()-&gt;constant_encoding(), O1);
 353     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), relocInfo::runtime_call_type);
 354     __ delayed()-&gt;nop();
 355     __ restore_thread(I1);
 356   }
 357 
 358   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 359     __ mov(I0, O0);  // Restore the exception
 360   }
 361 
 362   // dispatch to the unwind logic
 363   __ call(Runtime1::entry_for(Runtime1::unwind_exception_id), relocInfo::runtime_call_type);
 364   __ delayed()-&gt;nop();
 365 
 366   // Emit the slow path assembly
 367   if (stub != NULL) {
 368     stub-&gt;emit_code(this);
 369   }
 370 
 371   return offset;
 372 }
 373 
 374 
 375 int LIR_Assembler::emit_deopt_handler() {
 376   // if the last instruction is a call (typically to do a throw which
 377   // is coming at the end after block reordering) the return address
 378   // must still point into the code area in order to avoid assertion
 379   // failures when searching for the corresponding bci =&gt; add a nop
 380   // (was bug 5/14/1999 - gri)
 381   __ nop();
 382 
 383   // generate code for deopt handler
 384   ciMethod* method = compilation()-&gt;method();
 385   address handler_base = __ start_a_stub(deopt_handler_size());
 386   if (handler_base == NULL) {
 387     // not enough space left for the handler
 388     bailout(&quot;deopt handler overflow&quot;);
 389     return -1;
 390   }
 391 
 392   int offset = code_offset();
 393   AddressLiteral deopt_blob(SharedRuntime::deopt_blob()-&gt;unpack());
 394   __ JUMP(deopt_blob, G3_scratch, 0); // sethi;jmp
 395   __ delayed()-&gt;nop();
 396   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 397   __ end_a_stub();
 398 
 399   return offset;
 400 }
 401 
 402 
 403 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 404   if (o == NULL) {
 405     __ set(NULL_WORD, reg);
 406   } else {
 407 #ifdef ASSERT
 408     {
 409       ThreadInVMfromNative tiv(JavaThread::current());
 410       assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(o)), &quot;should be real oop&quot;);
 411     }
 412 #endif
 413     int oop_index = __ oop_recorder()-&gt;find_index(o);
 414     RelocationHolder rspec = oop_Relocation::spec(oop_index);
 415     __ set(NULL_WORD, reg, rspec); // Will be set when the nmethod is created
 416   }
 417 }
 418 
 419 
 420 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo *info) {
 421   // Allocate a new index in table to hold the object once it&#39;s been patched
 422   int oop_index = __ oop_recorder()-&gt;allocate_oop_index(NULL);
 423   PatchingStub* patch = new PatchingStub(_masm, patching_id(info), oop_index);
 424 
 425   AddressLiteral addrlit(NULL, oop_Relocation::spec(oop_index));
 426   assert(addrlit.rspec().type() == relocInfo::oop_type, &quot;must be an oop reloc&quot;);
 427   // It may not seem necessary to use a sethi/add pair to load a NULL into dest, but the
 428   // NULL will be dynamically patched later and the patched value may be large.  We must
 429   // therefore generate the sethi/add as a placeholders
 430   __ patchable_set(addrlit, reg);
 431 
 432   patching_epilog(patch, lir_patch_normal, reg, info);
 433 }
 434 
 435 
 436 void LIR_Assembler::metadata2reg(Metadata* o, Register reg) {
 437   __ set_metadata_constant(o, reg);
 438 }
 439 
 440 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo *info) {
 441   // Allocate a new index in table to hold the klass once it&#39;s been patched
 442   int index = __ oop_recorder()-&gt;allocate_metadata_index(NULL);
 443   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id, index);
 444   AddressLiteral addrlit(NULL, metadata_Relocation::spec(index));
 445   assert(addrlit.rspec().type() == relocInfo::metadata_type, &quot;must be an metadata reloc&quot;);
 446   // It may not seem necessary to use a sethi/add pair to load a NULL into dest, but the
 447   // NULL will be dynamically patched later and the patched value may be large.  We must
 448   // therefore generate the sethi/add as a placeholders
 449   __ patchable_set(addrlit, reg);
 450 
 451   patching_epilog(patch, lir_patch_normal, reg, info);
 452 }
 453 
 454 void LIR_Assembler::emit_op3(LIR_Op3* op) {
 455   switch (op-&gt;code()) {
 456     case lir_idiv:
 457     case lir_irem:  // Both idiv &amp; irem are handled after the switch (below).
 458       break;
 459     case lir_fmaf:
 460       __ fmadd(FloatRegisterImpl::S,
 461                op-&gt;in_opr1()-&gt;as_float_reg(),
 462                op-&gt;in_opr2()-&gt;as_float_reg(),
 463                op-&gt;in_opr3()-&gt;as_float_reg(),
 464                op-&gt;result_opr()-&gt;as_float_reg());
 465       return;
 466     case lir_fmad:
 467       __ fmadd(FloatRegisterImpl::D,
 468                op-&gt;in_opr1()-&gt;as_double_reg(),
 469                op-&gt;in_opr2()-&gt;as_double_reg(),
 470                op-&gt;in_opr3()-&gt;as_double_reg(),
 471                op-&gt;result_opr()-&gt;as_double_reg());
 472       return;
 473     default:
 474       ShouldNotReachHere();
 475       break;
 476   }
 477 
 478   // Handle idiv &amp; irem:
 479 
 480   Register Rdividend = op-&gt;in_opr1()-&gt;as_register();
 481   Register Rdivisor  = noreg;
 482   Register Rscratch  = op-&gt;in_opr3()-&gt;as_register();
 483   Register Rresult   = op-&gt;result_opr()-&gt;as_register();
 484   int divisor = -1;
 485 
 486   if (op-&gt;in_opr2()-&gt;is_register()) {
 487     Rdivisor = op-&gt;in_opr2()-&gt;as_register();
 488   } else {
 489     divisor = op-&gt;in_opr2()-&gt;as_constant_ptr()-&gt;as_jint();
 490     assert(Assembler::is_simm13(divisor), &quot;can only handle simm13&quot;);
 491   }
 492 
 493   assert(Rdividend != Rscratch, &quot;&quot;);
 494   assert(Rdivisor  != Rscratch, &quot;&quot;);
 495   assert(op-&gt;code() == lir_idiv || op-&gt;code() == lir_irem, &quot;Must be irem or idiv&quot;);
 496 
 497   if (Rdivisor == noreg &amp;&amp; is_power_of_2(divisor)) {
 498     // convert division by a power of two into some shifts and logical operations
 499     if (op-&gt;code() == lir_idiv) {
 500       if (divisor == 2) {
 501         __ srl(Rdividend, 31, Rscratch);
 502       } else {
 503         __ sra(Rdividend, 31, Rscratch);
 504         __ and3(Rscratch, divisor - 1, Rscratch);
 505       }
 506       __ add(Rdividend, Rscratch, Rscratch);
 507       __ sra(Rscratch, log2_int(divisor), Rresult);
 508       return;
 509     } else {
 510       if (divisor == 2) {
 511         __ srl(Rdividend, 31, Rscratch);
 512       } else {
 513         __ sra(Rdividend, 31, Rscratch);
 514         __ and3(Rscratch, divisor - 1,Rscratch);
 515       }
 516       __ add(Rdividend, Rscratch, Rscratch);
 517       __ andn(Rscratch, divisor - 1,Rscratch);
 518       __ sub(Rdividend, Rscratch, Rresult);
 519       return;
 520     }
 521   }
 522 
 523   __ sra(Rdividend, 31, Rscratch);
 524   __ wry(Rscratch);
 525 
 526   add_debug_info_for_div0_here(op-&gt;info());
 527 
 528   if (Rdivisor != noreg) {
 529     __ sdivcc(Rdividend, Rdivisor, (op-&gt;code() == lir_idiv ? Rresult : Rscratch));
 530   } else {
 531     assert(Assembler::is_simm13(divisor), &quot;can only handle simm13&quot;);
 532     __ sdivcc(Rdividend, divisor, (op-&gt;code() == lir_idiv ? Rresult : Rscratch));
 533   }
 534 
 535   Label skip;
 536   __ br(Assembler::overflowSet, true, Assembler::pn, skip);
 537   __ delayed()-&gt;Assembler::sethi(0x80000000, (op-&gt;code() == lir_idiv ? Rresult : Rscratch));
 538   __ bind(skip);
 539 
 540   if (op-&gt;code() == lir_irem) {
 541     if (Rdivisor != noreg) {
 542       __ smul(Rscratch, Rdivisor, Rscratch);
 543     } else {
 544       __ smul(Rscratch, divisor, Rscratch);
 545     }
 546     __ sub(Rdividend, Rscratch, Rresult);
 547   }
 548 }
 549 
 550 
 551 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
 552 #ifdef ASSERT
 553   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
 554   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
 555   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
 556 #endif
 557   assert(op-&gt;info() == NULL, &quot;shouldn&#39;t have CodeEmitInfo&quot;);
 558 
 559   if (op-&gt;cond() == lir_cond_always) {
 560     __ br(Assembler::always, false, Assembler::pt, *(op-&gt;label()));
 561   } else if (op-&gt;code() == lir_cond_float_branch) {
 562     assert(op-&gt;ublock() != NULL, &quot;must have unordered successor&quot;);
 563     bool is_unordered = (op-&gt;ublock() == op-&gt;block());
 564     Assembler::Condition acond;
 565     switch (op-&gt;cond()) {
 566       case lir_cond_equal:         acond = Assembler::f_equal;    break;
 567       case lir_cond_notEqual:      acond = Assembler::f_notEqual; break;
 568       case lir_cond_less:          acond = (is_unordered ? Assembler::f_unorderedOrLess          : Assembler::f_less);           break;
 569       case lir_cond_greater:       acond = (is_unordered ? Assembler::f_unorderedOrGreater       : Assembler::f_greater);        break;
 570       case lir_cond_lessEqual:     acond = (is_unordered ? Assembler::f_unorderedOrLessOrEqual   : Assembler::f_lessOrEqual);    break;
 571       case lir_cond_greaterEqual:  acond = (is_unordered ? Assembler::f_unorderedOrGreaterOrEqual: Assembler::f_greaterOrEqual); break;
 572       default :                         ShouldNotReachHere();
 573     }
 574     __ fb( acond, false, Assembler::pn, *(op-&gt;label()));
 575   } else {
 576     assert (op-&gt;code() == lir_branch, &quot;just checking&quot;);
 577 
 578     Assembler::Condition acond;
 579     switch (op-&gt;cond()) {
 580       case lir_cond_equal:        acond = Assembler::equal;                break;
 581       case lir_cond_notEqual:     acond = Assembler::notEqual;             break;
 582       case lir_cond_less:         acond = Assembler::less;                 break;
 583       case lir_cond_lessEqual:    acond = Assembler::lessEqual;            break;
 584       case lir_cond_greaterEqual: acond = Assembler::greaterEqual;         break;
 585       case lir_cond_greater:      acond = Assembler::greater;              break;
 586       case lir_cond_aboveEqual:   acond = Assembler::greaterEqualUnsigned; break;
 587       case lir_cond_belowEqual:   acond = Assembler::lessEqualUnsigned;    break;
 588       default:                         ShouldNotReachHere();
 589     };
 590 
 591     // sparc has different condition codes for testing 32-bit
 592     // vs. 64-bit values.  We could always test xcc is we could
 593     // guarantee that 32-bit loads always sign extended but that isn&#39;t
 594     // true and since sign extension isn&#39;t free, it would impose a
 595     // slight cost.
 596     if  (op-&gt;type() == T_INT) {
 597       __ br(acond, false, Assembler::pn, *(op-&gt;label()));
 598     } else
 599       __ brx(acond, false, Assembler::pn, *(op-&gt;label()));
 600   }
 601   // The peephole pass fills the delay slot
 602 }
 603 
 604 
 605 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
 606   Bytecodes::Code code = op-&gt;bytecode();
 607   LIR_Opr dst = op-&gt;result_opr();
 608 
 609   switch(code) {
 610     case Bytecodes::_i2l: {
 611       Register rlo  = dst-&gt;as_register_lo();
 612       Register rhi  = dst-&gt;as_register_hi();
 613       Register rval = op-&gt;in_opr()-&gt;as_register();
 614       __ sra(rval, 0, rlo);
 615       break;
 616     }
 617     case Bytecodes::_i2d:
 618     case Bytecodes::_i2f: {
 619       bool is_double = (code == Bytecodes::_i2d);
 620       FloatRegister rdst = is_double ? dst-&gt;as_double_reg() : dst-&gt;as_float_reg();
 621       FloatRegisterImpl::Width w = is_double ? FloatRegisterImpl::D : FloatRegisterImpl::S;
 622       FloatRegister rsrc = op-&gt;in_opr()-&gt;as_float_reg();
 623       if (rsrc != rdst) {
 624         __ fmov(FloatRegisterImpl::S, rsrc, rdst);
 625       }
 626       __ fitof(w, rdst, rdst);
 627       break;
 628     }
 629     case Bytecodes::_f2i:{
 630       FloatRegister rsrc = op-&gt;in_opr()-&gt;as_float_reg();
 631       Address       addr = frame_map()-&gt;address_for_slot(dst-&gt;single_stack_ix());
 632       Label L;
 633       // result must be 0 if value is NaN; test by comparing value to itself
 634       __ fcmp(FloatRegisterImpl::S, Assembler::fcc0, rsrc, rsrc);
 635       __ fb(Assembler::f_unordered, true, Assembler::pn, L);
 636       __ delayed()-&gt;st(G0, addr); // annuled if contents of rsrc is not NaN
 637       __ ftoi(FloatRegisterImpl::S, rsrc, rsrc);
 638       // move integer result from float register to int register
 639       __ stf(FloatRegisterImpl::S, rsrc, addr.base(), addr.disp());
 640       __ bind (L);
 641       break;
 642     }
 643     case Bytecodes::_l2i: {
 644       Register rlo  = op-&gt;in_opr()-&gt;as_register_lo();
 645       Register rhi  = op-&gt;in_opr()-&gt;as_register_hi();
 646       Register rdst = dst-&gt;as_register();
 647       __ sra(rlo, 0, rdst);
 648       break;
 649     }
 650     case Bytecodes::_d2f:
 651     case Bytecodes::_f2d: {
 652       bool is_double = (code == Bytecodes::_f2d);
 653       assert((!is_double &amp;&amp; dst-&gt;is_single_fpu()) || (is_double &amp;&amp; dst-&gt;is_double_fpu()), &quot;check&quot;);
 654       LIR_Opr val = op-&gt;in_opr();
 655       FloatRegister rval = (code == Bytecodes::_d2f) ? val-&gt;as_double_reg() : val-&gt;as_float_reg();
 656       FloatRegister rdst = is_double ? dst-&gt;as_double_reg() : dst-&gt;as_float_reg();
 657       FloatRegisterImpl::Width vw = is_double ? FloatRegisterImpl::S : FloatRegisterImpl::D;
 658       FloatRegisterImpl::Width dw = is_double ? FloatRegisterImpl::D : FloatRegisterImpl::S;
 659       __ ftof(vw, dw, rval, rdst);
 660       break;
 661     }
 662     case Bytecodes::_i2s:
 663     case Bytecodes::_i2b: {
 664       Register rval = op-&gt;in_opr()-&gt;as_register();
 665       Register rdst = dst-&gt;as_register();
 666       int shift = (code == Bytecodes::_i2b) ? (BitsPerInt - T_BYTE_aelem_bytes * BitsPerByte) : (BitsPerInt - BitsPerShort);
 667       __ sll (rval, shift, rdst);
 668       __ sra (rdst, shift, rdst);
 669       break;
 670     }
 671     case Bytecodes::_i2c: {
 672       Register rval = op-&gt;in_opr()-&gt;as_register();
 673       Register rdst = dst-&gt;as_register();
 674       int shift = BitsPerInt - T_CHAR_aelem_bytes * BitsPerByte;
 675       __ sll (rval, shift, rdst);
 676       __ srl (rdst, shift, rdst);
 677       break;
 678     }
 679 
 680     default: ShouldNotReachHere();
 681   }
 682 }
 683 
 684 
 685 void LIR_Assembler::align_call(LIR_Code) {
 686   // do nothing since all instructions are word aligned on sparc
 687 }
 688 
 689 
 690 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
 691   __ call(op-&gt;addr(), rtype);
 692   // The peephole pass fills the delay slot, add_call_info is done in
 693   // LIR_Assembler::emit_delay.
 694 }
 695 
 696 
 697 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
 698   __ ic_call(op-&gt;addr(), false);
 699   // The peephole pass fills the delay slot, add_call_info is done in
 700   // LIR_Assembler::emit_delay.
 701 }
 702 
 703 
 704 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
 705   add_debug_info_for_null_check_here(op-&gt;info());
 706   __ load_klass(O0, G3_scratch);
 707   if (Assembler::is_simm13(op-&gt;vtable_offset())) {
 708     __ ld_ptr(G3_scratch, op-&gt;vtable_offset(), G5_method);
 709   } else {
 710     // This will generate 2 instructions
 711     __ set(op-&gt;vtable_offset(), G5_method);
 712     // ld_ptr, set_hi, set
 713     __ ld_ptr(G3_scratch, G5_method, G5_method);
 714   }
 715   __ ld_ptr(G5_method, Method::from_compiled_offset(), G3_scratch);
 716   __ callr(G3_scratch, G0);
 717   // the peephole pass fills the delay slot
 718 }
 719 
 720 int LIR_Assembler::store(LIR_Opr from_reg, Register base, int offset, BasicType type, bool wide, bool unaligned) {
 721   int store_offset;
 722   if (!Assembler::is_simm13(offset + (type == T_LONG) ? wordSize : 0)) {
 723     assert(base != O7, &quot;destroying register&quot;);
 724     assert(!unaligned, &quot;can&#39;t handle this&quot;);
 725     // for offsets larger than a simm13 we setup the offset in O7
 726     __ set(offset, O7);
 727     store_offset = store(from_reg, base, O7, type, wide);
 728   } else {
 729     if (is_reference_type(type)) {
 730       __ verify_oop(from_reg-&gt;as_register());
 731     }
 732     store_offset = code_offset();
 733     switch (type) {
 734       case T_BOOLEAN: // fall through
 735       case T_BYTE  : __ stb(from_reg-&gt;as_register(), base, offset); break;
 736       case T_CHAR  : __ sth(from_reg-&gt;as_register(), base, offset); break;
 737       case T_SHORT : __ sth(from_reg-&gt;as_register(), base, offset); break;
 738       case T_INT   : __ stw(from_reg-&gt;as_register(), base, offset); break;
 739       case T_LONG  :
 740         if (unaligned || PatchALot) {
 741           // Don&#39;t use O7 here because it may be equal to &#39;base&#39; (see LIR_Assembler::reg2mem)
 742           assert(G3_scratch != base, &quot;can&#39;t handle this&quot;);
 743           assert(G3_scratch != from_reg-&gt;as_register_lo(), &quot;can&#39;t handle this&quot;);
 744           __ srax(from_reg-&gt;as_register_lo(), 32, G3_scratch);
 745           __ stw(from_reg-&gt;as_register_lo(), base, offset + lo_word_offset_in_bytes);
 746           __ stw(G3_scratch,                 base, offset + hi_word_offset_in_bytes);
 747         } else {
 748           __ stx(from_reg-&gt;as_register_lo(), base, offset);
 749         }
 750         break;
 751       case T_ADDRESS:
 752       case T_METADATA:
 753         __ st_ptr(from_reg-&gt;as_register(), base, offset);
 754         break;
 755       case T_ARRAY : // fall through
 756       case T_OBJECT:
 757         {
 758           if (UseCompressedOops &amp;&amp; !wide) {
 759             __ encode_heap_oop(from_reg-&gt;as_register(), G3_scratch);
 760             store_offset = code_offset();
 761             __ stw(G3_scratch, base, offset);
 762           } else {
 763             __ st_ptr(from_reg-&gt;as_register(), base, offset);
 764           }
 765           break;
 766         }
 767 
 768       case T_FLOAT : __ stf(FloatRegisterImpl::S, from_reg-&gt;as_float_reg(), base, offset); break;
 769       case T_DOUBLE:
 770         {
 771           FloatRegister reg = from_reg-&gt;as_double_reg();
 772           // split unaligned stores
 773           if (unaligned || PatchALot) {
 774             assert(Assembler::is_simm13(offset + 4), &quot;must be&quot;);
 775             __ stf(FloatRegisterImpl::S, reg-&gt;successor(), base, offset + 4);
 776             __ stf(FloatRegisterImpl::S, reg,              base, offset);
 777           } else {
 778             __ stf(FloatRegisterImpl::D, reg, base, offset);
 779           }
 780           break;
 781         }
 782       default      : ShouldNotReachHere();
 783     }
 784   }
 785   return store_offset;
 786 }
 787 
 788 
 789 int LIR_Assembler::store(LIR_Opr from_reg, Register base, Register disp, BasicType type, bool wide) {
 790   if (is_reference_type(type)) {
 791     __ verify_oop(from_reg-&gt;as_register());
 792   }
 793   int store_offset = code_offset();
 794   switch (type) {
 795     case T_BOOLEAN: // fall through
 796     case T_BYTE  : __ stb(from_reg-&gt;as_register(), base, disp); break;
 797     case T_CHAR  : __ sth(from_reg-&gt;as_register(), base, disp); break;
 798     case T_SHORT : __ sth(from_reg-&gt;as_register(), base, disp); break;
 799     case T_INT   : __ stw(from_reg-&gt;as_register(), base, disp); break;
 800     case T_LONG  :
 801       __ stx(from_reg-&gt;as_register_lo(), base, disp);
 802       break;
 803     case T_ADDRESS:
 804       __ st_ptr(from_reg-&gt;as_register(), base, disp);
 805       break;
 806     case T_ARRAY : // fall through
 807     case T_OBJECT:
 808       {
 809         if (UseCompressedOops &amp;&amp; !wide) {
 810           __ encode_heap_oop(from_reg-&gt;as_register(), G3_scratch);
 811           store_offset = code_offset();
 812           __ stw(G3_scratch, base, disp);
 813         } else {
 814           __ st_ptr(from_reg-&gt;as_register(), base, disp);
 815         }
 816         break;
 817       }
 818     case T_FLOAT : __ stf(FloatRegisterImpl::S, from_reg-&gt;as_float_reg(), base, disp); break;
 819     case T_DOUBLE: __ stf(FloatRegisterImpl::D, from_reg-&gt;as_double_reg(), base, disp); break;
 820     default      : ShouldNotReachHere();
 821   }
 822   return store_offset;
 823 }
 824 
 825 
 826 int LIR_Assembler::load(Register base, int offset, LIR_Opr to_reg, BasicType type, bool wide, bool unaligned) {
 827   int load_offset;
 828   if (!Assembler::is_simm13(offset + (type == T_LONG) ? wordSize : 0)) {
 829     assert(base != O7, &quot;destroying register&quot;);
 830     assert(!unaligned, &quot;can&#39;t handle this&quot;);
 831     // for offsets larger than a simm13 we setup the offset in O7
 832     __ set(offset, O7);
 833     load_offset = load(base, O7, to_reg, type, wide);
 834   } else {
 835     load_offset = code_offset();
 836     switch(type) {
 837       case T_BOOLEAN: // fall through
 838       case T_BYTE  : __ ldsb(base, offset, to_reg-&gt;as_register()); break;
 839       case T_CHAR  : __ lduh(base, offset, to_reg-&gt;as_register()); break;
 840       case T_SHORT : __ ldsh(base, offset, to_reg-&gt;as_register()); break;
 841       case T_INT   : __ ld(base, offset, to_reg-&gt;as_register()); break;
 842       case T_LONG  :
 843         if (!unaligned &amp;&amp; !PatchALot) {
 844           __ ldx(base, offset, to_reg-&gt;as_register_lo());
 845         } else {
 846           assert(base != to_reg-&gt;as_register_lo(), &quot;can&#39;t handle this&quot;);
 847           assert(O7 != to_reg-&gt;as_register_lo(), &quot;can&#39;t handle this&quot;);
 848           __ ld(base, offset + hi_word_offset_in_bytes, to_reg-&gt;as_register_lo());
 849           __ lduw(base, offset + lo_word_offset_in_bytes, O7); // in case O7 is base or offset, use it last
 850           __ sllx(to_reg-&gt;as_register_lo(), 32, to_reg-&gt;as_register_lo());
 851           __ or3(to_reg-&gt;as_register_lo(), O7, to_reg-&gt;as_register_lo());
 852         }
 853         break;
 854       case T_METADATA:  __ ld_ptr(base, offset, to_reg-&gt;as_register()); break;
 855       case T_ADDRESS:
 856         if (offset == oopDesc::klass_offset_in_bytes() &amp;&amp; UseCompressedClassPointers) {
 857           __ lduw(base, offset, to_reg-&gt;as_register());
 858           __ decode_klass_not_null(to_reg-&gt;as_register());
 859         } else
 860         {
 861           __ ld_ptr(base, offset, to_reg-&gt;as_register());
 862         }
 863         break;
 864       case T_ARRAY : // fall through
 865       case T_OBJECT:
 866         {
 867           if (UseCompressedOops &amp;&amp; !wide) {
 868             __ lduw(base, offset, to_reg-&gt;as_register());
 869             __ decode_heap_oop(to_reg-&gt;as_register());
 870           } else {
 871             __ ld_ptr(base, offset, to_reg-&gt;as_register());
 872           }
 873           break;
 874         }
 875       case T_FLOAT:  __ ldf(FloatRegisterImpl::S, base, offset, to_reg-&gt;as_float_reg()); break;
 876       case T_DOUBLE:
 877         {
 878           FloatRegister reg = to_reg-&gt;as_double_reg();
 879           // split unaligned loads
 880           if (unaligned || PatchALot) {
 881             __ ldf(FloatRegisterImpl::S, base, offset + 4, reg-&gt;successor());
 882             __ ldf(FloatRegisterImpl::S, base, offset,     reg);
 883           } else {
 884             __ ldf(FloatRegisterImpl::D, base, offset, to_reg-&gt;as_double_reg());
 885           }
 886           break;
 887         }
 888       default      : ShouldNotReachHere();
 889     }
 890     if (is_reference_type(type)) {
 891       __ verify_oop(to_reg-&gt;as_register());
 892     }
 893   }
 894   return load_offset;
 895 }
 896 
 897 
 898 int LIR_Assembler::load(Register base, Register disp, LIR_Opr to_reg, BasicType type, bool wide) {
 899   int load_offset = code_offset();
 900   switch(type) {
 901     case T_BOOLEAN: // fall through
 902     case T_BYTE  :  __ ldsb(base, disp, to_reg-&gt;as_register()); break;
 903     case T_CHAR  :  __ lduh(base, disp, to_reg-&gt;as_register()); break;
 904     case T_SHORT :  __ ldsh(base, disp, to_reg-&gt;as_register()); break;
 905     case T_INT   :  __ ld(base, disp, to_reg-&gt;as_register()); break;
 906     case T_ADDRESS: __ ld_ptr(base, disp, to_reg-&gt;as_register()); break;
 907     case T_ARRAY : // fall through
 908     case T_OBJECT:
 909       {
 910           if (UseCompressedOops &amp;&amp; !wide) {
 911             __ lduw(base, disp, to_reg-&gt;as_register());
 912             __ decode_heap_oop(to_reg-&gt;as_register());
 913           } else {
 914             __ ld_ptr(base, disp, to_reg-&gt;as_register());
 915           }
 916           break;
 917       }
 918     case T_FLOAT:  __ ldf(FloatRegisterImpl::S, base, disp, to_reg-&gt;as_float_reg()); break;
 919     case T_DOUBLE: __ ldf(FloatRegisterImpl::D, base, disp, to_reg-&gt;as_double_reg()); break;
 920     case T_LONG  :
 921       __ ldx(base, disp, to_reg-&gt;as_register_lo());
 922       break;
 923     default      : ShouldNotReachHere();
 924   }
 925   if (is_reference_type(type)) {
 926     __ verify_oop(to_reg-&gt;as_register());
 927   }
 928   return load_offset;
 929 }
 930 
 931 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 932   LIR_Const* c = src-&gt;as_constant_ptr();
 933   switch (c-&gt;type()) {
 934     case T_INT:
 935     case T_FLOAT: {
 936       Register src_reg = O7;
 937       int value = c-&gt;as_jint_bits();
 938       if (value == 0) {
 939         src_reg = G0;
 940       } else {
 941         __ set(value, O7);
 942       }
 943       Address addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 944       __ stw(src_reg, addr.base(), addr.disp());
 945       break;
 946     }
 947     case T_ADDRESS: {
 948       Register src_reg = O7;
 949       int value = c-&gt;as_jint_bits();
 950       if (value == 0) {
 951         src_reg = G0;
 952       } else {
 953         __ set(value, O7);
 954       }
 955       Address addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 956       __ st_ptr(src_reg, addr.base(), addr.disp());
 957       break;
 958     }
 959     case T_OBJECT: {
 960       Register src_reg = O7;
 961       jobject2reg(c-&gt;as_jobject(), src_reg);
 962       Address addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 963       __ st_ptr(src_reg, addr.base(), addr.disp());
 964       break;
 965     }
 966     case T_LONG:
 967     case T_DOUBLE: {
 968       Address addr = frame_map()-&gt;address_for_double_slot(dest-&gt;double_stack_ix());
 969 
 970       Register tmp = O7;
 971       int value_lo = c-&gt;as_jint_lo_bits();
 972       if (value_lo == 0) {
 973         tmp = G0;
 974       } else {
 975         __ set(value_lo, O7);
 976       }
 977       __ stw(tmp, addr.base(), addr.disp() + lo_word_offset_in_bytes);
 978       int value_hi = c-&gt;as_jint_hi_bits();
 979       if (value_hi == 0) {
 980         tmp = G0;
 981       } else {
 982         __ set(value_hi, O7);
 983       }
 984       __ stw(tmp, addr.base(), addr.disp() + hi_word_offset_in_bytes);
 985       break;
 986     }
 987     default:
 988       Unimplemented();
 989   }
 990 }
 991 
 992 
 993 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 994   LIR_Const* c = src-&gt;as_constant_ptr();
 995   LIR_Address* addr     = dest-&gt;as_address_ptr();
 996   Register base = addr-&gt;base()-&gt;as_pointer_register();
 997   int offset = -1;
 998 
 999   switch (c-&gt;type()) {
1000     case T_FLOAT: type = T_INT; // Float constants are stored by int store instructions.
1001     case T_INT:
1002     case T_ADDRESS: {
1003       LIR_Opr tmp = FrameMap::O7_opr;
1004       int value = c-&gt;as_jint_bits();
1005       if (value == 0) {
1006         tmp = FrameMap::G0_opr;
1007       } else if (Assembler::is_simm13(value)) {
1008         __ set(value, O7);
1009       }
1010       if (addr-&gt;index()-&gt;is_valid()) {
1011         assert(addr-&gt;disp() == 0, &quot;must be zero&quot;);
1012         offset = store(tmp, base, addr-&gt;index()-&gt;as_pointer_register(), type, wide);
1013       } else {
1014         assert(Assembler::is_simm13(addr-&gt;disp()), &quot;can&#39;t handle larger addresses&quot;);
1015         offset = store(tmp, base, addr-&gt;disp(), type, wide, false);
1016       }
1017       break;
1018     }
1019     case T_LONG:
1020     case T_DOUBLE: {
1021       assert(!addr-&gt;index()-&gt;is_valid(), &quot;can&#39;t handle reg reg address here&quot;);
1022       assert(Assembler::is_simm13(addr-&gt;disp()) &amp;&amp;
1023              Assembler::is_simm13(addr-&gt;disp() + 4), &quot;can&#39;t handle larger addresses&quot;);
1024 
1025       LIR_Opr tmp = FrameMap::O7_opr;
1026       int value_lo = c-&gt;as_jint_lo_bits();
1027       if (value_lo == 0) {
1028         tmp = FrameMap::G0_opr;
1029       } else {
1030         __ set(value_lo, O7);
1031       }
1032       offset = store(tmp, base, addr-&gt;disp() + lo_word_offset_in_bytes, T_INT, wide, false);
1033       int value_hi = c-&gt;as_jint_hi_bits();
1034       if (value_hi == 0) {
1035         tmp = FrameMap::G0_opr;
1036       } else {
1037         __ set(value_hi, O7);
1038       }
1039       store(tmp, base, addr-&gt;disp() + hi_word_offset_in_bytes, T_INT, wide, false);
1040       break;
1041     }
1042     case T_OBJECT: {
1043       jobject obj = c-&gt;as_jobject();
1044       LIR_Opr tmp;
1045       if (obj == NULL) {
1046         tmp = FrameMap::G0_opr;
1047       } else {
1048         tmp = FrameMap::O7_opr;
1049         jobject2reg(c-&gt;as_jobject(), O7);
1050       }
1051       // handle either reg+reg or reg+disp address
1052       if (addr-&gt;index()-&gt;is_valid()) {
1053         assert(addr-&gt;disp() == 0, &quot;must be zero&quot;);
1054         offset = store(tmp, base, addr-&gt;index()-&gt;as_pointer_register(), type, wide);
1055       } else {
1056         assert(Assembler::is_simm13(addr-&gt;disp()), &quot;can&#39;t handle larger addresses&quot;);
1057         offset = store(tmp, base, addr-&gt;disp(), type, wide, false);
1058       }
1059 
1060       break;
1061     }
1062     default:
1063       Unimplemented();
1064   }
1065   if (info != NULL) {
1066     assert(offset != -1, &quot;offset should&#39;ve been set&quot;);
1067     add_debug_info_for_null_check(offset, info);
1068   }
1069 }
1070 
1071 
1072 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
1073   LIR_Const* c = src-&gt;as_constant_ptr();
1074   LIR_Opr to_reg = dest;
1075 
1076   switch (c-&gt;type()) {
1077     case T_INT:
1078     case T_ADDRESS:
1079       {
1080         jint con = c-&gt;as_jint();
1081         if (to_reg-&gt;is_single_cpu()) {
1082           assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
1083           __ set(con, to_reg-&gt;as_register());
1084         } else {
1085           ShouldNotReachHere();
1086           assert(to_reg-&gt;is_single_fpu(), &quot;wrong register kind&quot;);
1087 
1088           __ set(con, O7);
1089           Address temp_slot(SP, (frame::register_save_words * wordSize) + STACK_BIAS);
1090           __ st(O7, temp_slot);
1091           __ ldf(FloatRegisterImpl::S, temp_slot, to_reg-&gt;as_float_reg());
1092         }
1093       }
1094       break;
1095 
1096     case T_LONG:
1097       {
1098         jlong con = c-&gt;as_jlong();
1099 
1100         if (to_reg-&gt;is_double_cpu()) {
1101           __ set(con,  to_reg-&gt;as_register_lo());
1102         } else if (to_reg-&gt;is_single_cpu()) {
1103           __ set(con, to_reg-&gt;as_register());
1104         } else {
1105           ShouldNotReachHere();
1106           assert(to_reg-&gt;is_double_fpu(), &quot;wrong register kind&quot;);
1107           Address temp_slot_lo(SP, ((frame::register_save_words  ) * wordSize) + STACK_BIAS);
1108           Address temp_slot_hi(SP, ((frame::register_save_words) * wordSize) + (longSize/2) + STACK_BIAS);
1109           __ set(low(con),  O7);
1110           __ st(O7, temp_slot_lo);
1111           __ set(high(con), O7);
1112           __ st(O7, temp_slot_hi);
1113           __ ldf(FloatRegisterImpl::D, temp_slot_lo, to_reg-&gt;as_double_reg());
1114         }
1115       }
1116       break;
1117 
1118     case T_OBJECT:
1119       {
1120         if (patch_code == lir_patch_none) {
1121           jobject2reg(c-&gt;as_jobject(), to_reg-&gt;as_register());
1122         } else {
1123           jobject2reg_with_patching(to_reg-&gt;as_register(), info);
1124         }
1125       }
1126       break;
1127 
1128     case T_METADATA:
1129       {
1130         if (patch_code == lir_patch_none) {
1131           metadata2reg(c-&gt;as_metadata(), to_reg-&gt;as_register());
1132         } else {
1133           klass2reg_with_patching(to_reg-&gt;as_register(), info);
1134         }
1135       }
1136       break;
1137 
1138     case T_FLOAT:
1139       {
1140         address const_addr = __ float_constant(c-&gt;as_jfloat());
1141         if (const_addr == NULL) {
1142           bailout(&quot;const section overflow&quot;);
1143           break;
1144         }
1145         RelocationHolder rspec = internal_word_Relocation::spec(const_addr);
1146         AddressLiteral const_addrlit(const_addr, rspec);
1147         if (to_reg-&gt;is_single_fpu()) {
1148           __ patchable_sethi(const_addrlit, O7);
1149           __ relocate(rspec);
1150           __ ldf(FloatRegisterImpl::S, O7, const_addrlit.low10(), to_reg-&gt;as_float_reg());
1151 
1152         } else {
1153           assert(to_reg-&gt;is_single_cpu(), &quot;Must be a cpu register.&quot;);
1154 
1155           __ set(const_addrlit, O7);
1156           __ ld(O7, 0, to_reg-&gt;as_register());
1157         }
1158       }
1159       break;
1160 
1161     case T_DOUBLE:
1162       {
1163         address const_addr = __ double_constant(c-&gt;as_jdouble());
1164         if (const_addr == NULL) {
1165           bailout(&quot;const section overflow&quot;);
1166           break;
1167         }
1168         RelocationHolder rspec = internal_word_Relocation::spec(const_addr);
1169 
1170         if (to_reg-&gt;is_double_fpu()) {
1171           AddressLiteral const_addrlit(const_addr, rspec);
1172           __ patchable_sethi(const_addrlit, O7);
1173           __ relocate(rspec);
1174           __ ldf (FloatRegisterImpl::D, O7, const_addrlit.low10(), to_reg-&gt;as_double_reg());
1175         } else {
1176           assert(to_reg-&gt;is_double_cpu(), &quot;Must be a long register.&quot;);
1177           __ set(jlong_cast(c-&gt;as_jdouble()), to_reg-&gt;as_register_lo());
1178         }
1179 
1180       }
1181       break;
1182 
1183     default:
1184       ShouldNotReachHere();
1185   }
1186 }
1187 
1188 Address LIR_Assembler::as_Address(LIR_Address* addr) {
1189   Register reg = addr-&gt;base()-&gt;as_pointer_register();
1190   LIR_Opr index = addr-&gt;index();
1191   if (index-&gt;is_illegal()) {
1192     return Address(reg, addr-&gt;disp());
1193   } else {
1194     assert (addr-&gt;disp() == 0, &quot;unsupported address mode&quot;);
1195     return Address(reg, index-&gt;as_pointer_register());
1196   }
1197 }
1198 
1199 
1200 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
1201   switch (type) {
1202     case T_INT:
1203     case T_FLOAT: {
1204       Register tmp = O7;
1205       Address from = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1206       Address to   = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
1207       __ lduw(from.base(), from.disp(), tmp);
1208       __ stw(tmp, to.base(), to.disp());
1209       break;
1210     }
1211     case T_ADDRESS:
1212     case T_OBJECT: {
1213       Register tmp = O7;
1214       Address from = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1215       Address to   = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
1216       __ ld_ptr(from.base(), from.disp(), tmp);
1217       __ st_ptr(tmp, to.base(), to.disp());
1218       break;
1219     }
1220     case T_LONG:
1221     case T_DOUBLE: {
1222       Register tmp = O7;
1223       Address from = frame_map()-&gt;address_for_double_slot(src-&gt;double_stack_ix());
1224       Address to   = frame_map()-&gt;address_for_double_slot(dest-&gt;double_stack_ix());
1225       __ lduw(from.base(), from.disp(), tmp);
1226       __ stw(tmp, to.base(), to.disp());
1227       __ lduw(from.base(), from.disp() + 4, tmp);
1228       __ stw(tmp, to.base(), to.disp() + 4);
1229       break;
1230     }
1231 
1232     default:
1233       ShouldNotReachHere();
1234   }
1235 }
1236 
1237 
1238 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
1239   Address base = as_Address(addr);
1240   return Address(base.base(), base.disp() + hi_word_offset_in_bytes);
1241 }
1242 
1243 
1244 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
1245   Address base = as_Address(addr);
1246   return Address(base.base(), base.disp() + lo_word_offset_in_bytes);
1247 }
1248 
1249 
1250 void LIR_Assembler::mem2reg(LIR_Opr src_opr, LIR_Opr dest, BasicType type,
1251                             LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool unaligned) {
1252 
1253   assert(type != T_METADATA, &quot;load of metadata ptr not supported&quot;);
1254   LIR_Address* addr = src_opr-&gt;as_address_ptr();
1255   LIR_Opr to_reg = dest;
1256 
1257   Register src = addr-&gt;base()-&gt;as_pointer_register();
1258   Register disp_reg = noreg;
1259   int disp_value = addr-&gt;disp();
1260   bool needs_patching = (patch_code != lir_patch_none);
1261 
1262   if (addr-&gt;base()-&gt;type() == T_OBJECT) {
1263     __ verify_oop(src);
1264   }
1265 
1266   PatchingStub* patch = NULL;
1267   if (needs_patching) {
1268     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1269     assert(!to_reg-&gt;is_double_cpu() ||
1270            patch_code == lir_patch_none ||
1271            patch_code == lir_patch_normal, &quot;patching doesn&#39;t match register&quot;);
1272   }
1273 
1274   if (addr-&gt;index()-&gt;is_illegal()) {
1275     if (!Assembler::is_simm13(disp_value) &amp;&amp; (!unaligned || Assembler::is_simm13(disp_value + 4))) {
1276       if (needs_patching) {
1277         __ patchable_set(0, O7);
1278       } else {
1279         __ set(disp_value, O7);
1280       }
1281       disp_reg = O7;
1282     }
1283   } else if (unaligned || PatchALot) {
1284     __ add(src, addr-&gt;index()-&gt;as_pointer_register(), O7);
1285     src = O7;
1286   } else {
1287     disp_reg = addr-&gt;index()-&gt;as_pointer_register();
1288     assert(disp_value == 0, &quot;can&#39;t handle 3 operand addresses&quot;);
1289   }
1290 
1291   // remember the offset of the load.  The patching_epilog must be done
1292   // before the call to add_debug_info, otherwise the PcDescs don&#39;t get
1293   // entered in increasing order.
1294   int offset = code_offset();
1295 
1296   assert(disp_reg != noreg || Assembler::is_simm13(disp_value), &quot;should have set this up&quot;);
1297   if (disp_reg == noreg) {
1298     offset = load(src, disp_value, to_reg, type, wide, unaligned);
1299   } else {
1300     assert(!unaligned, &quot;can&#39;t handle this&quot;);
1301     offset = load(src, disp_reg, to_reg, type, wide);
1302   }
1303 
1304   if (patch != NULL) {
1305     patching_epilog(patch, patch_code, src, info);
1306   }
1307   if (info != NULL) add_debug_info_for_null_check(offset, info);
1308 }
1309 
1310 
1311 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
1312   Address addr;
1313   if (src-&gt;is_single_word()) {
1314     addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1315   } else if (src-&gt;is_double_word())  {
1316     addr = frame_map()-&gt;address_for_double_slot(src-&gt;double_stack_ix());
1317   }
1318 
1319   bool unaligned = (addr.disp() - STACK_BIAS) % 8 != 0;
1320   load(addr.base(), addr.disp(), dest, dest-&gt;type(), true /*wide*/, unaligned);
1321 }
1322 
1323 
1324 void LIR_Assembler::reg2stack(LIR_Opr from_reg, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
1325   Address addr;
1326   if (dest-&gt;is_single_word()) {
1327     addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
1328   } else if (dest-&gt;is_double_word())  {
1329     addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
1330   }
1331   bool unaligned = (addr.disp() - STACK_BIAS) % 8 != 0;
1332   store(from_reg, addr.base(), addr.disp(), from_reg-&gt;type(), true /*wide*/, unaligned);
1333 }
1334 
1335 
1336 void LIR_Assembler::reg2reg(LIR_Opr from_reg, LIR_Opr to_reg) {
1337   if (from_reg-&gt;is_float_kind() &amp;&amp; to_reg-&gt;is_float_kind()) {
1338     if (from_reg-&gt;is_double_fpu()) {
1339       // double to double moves
1340       assert(to_reg-&gt;is_double_fpu(), &quot;should match&quot;);
1341       __ fmov(FloatRegisterImpl::D, from_reg-&gt;as_double_reg(), to_reg-&gt;as_double_reg());
1342     } else {
1343       // float to float moves
1344       assert(to_reg-&gt;is_single_fpu(), &quot;should match&quot;);
1345       __ fmov(FloatRegisterImpl::S, from_reg-&gt;as_float_reg(), to_reg-&gt;as_float_reg());
1346     }
1347   } else if (!from_reg-&gt;is_float_kind() &amp;&amp; !to_reg-&gt;is_float_kind()) {
1348     if (from_reg-&gt;is_double_cpu()) {
1349       __ mov(from_reg-&gt;as_pointer_register(), to_reg-&gt;as_pointer_register());
1350     } else if (to_reg-&gt;is_double_cpu()) {
1351       // int to int moves
1352       __ mov(from_reg-&gt;as_register(), to_reg-&gt;as_register_lo());
1353     } else {
1354       // int to int moves
1355       __ mov(from_reg-&gt;as_register(), to_reg-&gt;as_register());
1356     }
1357   } else {
1358     ShouldNotReachHere();
1359   }
1360   if (is_reference_type(to_reg-&gt;type())) {
1361     __ verify_oop(to_reg-&gt;as_register());
1362   }
1363 }
1364 
1365 void LIR_Assembler::reg2mem(LIR_Opr from_reg, LIR_Opr dest, BasicType type,
1366                             LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack,
1367                             bool wide, bool unaligned) {
1368   assert(type != T_METADATA, &quot;store of metadata ptr not supported&quot;);
1369   LIR_Address* addr = dest-&gt;as_address_ptr();
1370 
1371   Register src = addr-&gt;base()-&gt;as_pointer_register();
1372   Register disp_reg = noreg;
1373   int disp_value = addr-&gt;disp();
1374   bool needs_patching = (patch_code != lir_patch_none);
1375 
1376   if (addr-&gt;base()-&gt;is_oop_register()) {
1377     __ verify_oop(src);
1378   }
1379 
1380   PatchingStub* patch = NULL;
1381   if (needs_patching) {
1382     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1383     assert(!from_reg-&gt;is_double_cpu() ||
1384            patch_code == lir_patch_none ||
1385            patch_code == lir_patch_normal, &quot;patching doesn&#39;t match register&quot;);
1386   }
1387 
1388   if (addr-&gt;index()-&gt;is_illegal()) {
1389     if (!Assembler::is_simm13(disp_value) &amp;&amp; (!unaligned || Assembler::is_simm13(disp_value + 4))) {
1390       if (needs_patching) {
1391         __ patchable_set(0, O7);
1392       } else {
1393         __ set(disp_value, O7);
1394       }
1395       disp_reg = O7;
1396     }
1397   } else if (unaligned || PatchALot) {
1398     __ add(src, addr-&gt;index()-&gt;as_pointer_register(), O7);
1399     src = O7;
1400   } else {
1401     disp_reg = addr-&gt;index()-&gt;as_pointer_register();
1402     assert(disp_value == 0, &quot;can&#39;t handle 3 operand addresses&quot;);
1403   }
1404 
1405   // remember the offset of the store.  The patching_epilog must be done
1406   // before the call to add_debug_info_for_null_check, otherwise the PcDescs don&#39;t get
1407   // entered in increasing order.
1408   int offset;
1409 
1410   assert(disp_reg != noreg || Assembler::is_simm13(disp_value), &quot;should have set this up&quot;);
1411   if (disp_reg == noreg) {
1412     offset = store(from_reg, src, disp_value, type, wide, unaligned);
1413   } else {
1414     assert(!unaligned, &quot;can&#39;t handle this&quot;);
1415     offset = store(from_reg, src, disp_reg, type, wide);
1416   }
1417 
1418   if (patch != NULL) {
1419     patching_epilog(patch, patch_code, src, info);
1420   }
1421 
1422   if (info != NULL) add_debug_info_for_null_check(offset, info);
1423 }
1424 
1425 
1426 void LIR_Assembler::return_op(LIR_Opr result) {
1427   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
1428     __ reserved_stack_check();
1429   }
1430   if (SafepointMechanism::uses_thread_local_poll()) {
1431     __ ld_ptr(Address(G2_thread, Thread::polling_page_offset()), L0);
1432   } else {
1433     __ set((intptr_t)os::get_polling_page(), L0);
1434   }
1435   __ relocate(relocInfo::poll_return_type);
1436   __ ld_ptr(L0, 0, G0);
1437   __ ret();
1438   __ delayed()-&gt;restore();
1439 }
1440 
1441 
1442 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
1443   if (SafepointMechanism::uses_thread_local_poll()) {
1444     __ ld_ptr(Address(G2_thread, Thread::polling_page_offset()), tmp-&gt;as_register());
1445   } else {
1446     __ set((intptr_t)os::get_polling_page(), tmp-&gt;as_register());
1447   }
1448   if (info != NULL) {
1449     add_debug_info_for_branch(info);
1450   }
1451   int offset = __ offset();
1452 
1453   __ relocate(relocInfo::poll_type);
1454   __ ld_ptr(tmp-&gt;as_register(), 0, G0);
1455   return offset;
1456 }
1457 
1458 
1459 void LIR_Assembler::emit_static_call_stub() {
1460   address call_pc = __ pc();
1461   address stub = __ start_a_stub(call_stub_size());
1462   if (stub == NULL) {
1463     bailout(&quot;static call stub overflow&quot;);
1464     return;
1465   }
1466 
1467   int start = __ offset();
1468   __ relocate(static_stub_Relocation::spec(call_pc));
1469 
1470   __ set_metadata(NULL, G5);
1471   // must be set to -1 at code generation time
1472   AddressLiteral addrlit(-1);
1473   __ jump_to(addrlit, G3);
1474   __ delayed()-&gt;nop();
1475 
1476   assert(__ offset() - start &lt;= call_stub_size(), &quot;stub too big&quot;);
1477   __ end_a_stub();
1478 }
1479 
1480 
1481 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1482   if (opr1-&gt;is_single_fpu()) {
1483     __ fcmp(FloatRegisterImpl::S, Assembler::fcc0, opr1-&gt;as_float_reg(), opr2-&gt;as_float_reg());
1484   } else if (opr1-&gt;is_double_fpu()) {
1485     __ fcmp(FloatRegisterImpl::D, Assembler::fcc0, opr1-&gt;as_double_reg(), opr2-&gt;as_double_reg());
1486   } else if (opr1-&gt;is_single_cpu()) {
1487     if (opr2-&gt;is_constant()) {
1488       switch (opr2-&gt;as_constant_ptr()-&gt;type()) {
1489         case T_INT:
1490           { jint con = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1491             if (Assembler::is_simm13(con)) {
1492               __ cmp(opr1-&gt;as_register(), con);
1493             } else {
1494               __ set(con, O7);
1495               __ cmp(opr1-&gt;as_register(), O7);
1496             }
1497           }
1498           break;
1499 
1500         case T_OBJECT:
1501           // there are only equal/notequal comparisions on objects
1502           { jobject con = opr2-&gt;as_constant_ptr()-&gt;as_jobject();
1503             if (con == NULL) {
1504               __ cmp(opr1-&gt;as_register(), 0);
1505             } else {
1506               jobject2reg(con, O7);
1507               __ cmp(opr1-&gt;as_register(), O7);
1508             }
1509           }
1510           break;
1511 
1512         case T_METADATA:
1513           // We only need, for now, comparison with NULL for metadata.
1514           { assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;oops&quot;);
1515             Metadata* m = opr2-&gt;as_constant_ptr()-&gt;as_metadata();
1516             if (m == NULL) {
1517               __ cmp(opr1-&gt;as_register(), 0);
1518             } else {
1519               ShouldNotReachHere();
1520             }
1521           }
1522           break;
1523 
1524         default:
1525           ShouldNotReachHere();
1526           break;
1527       }
1528     } else {
1529       if (opr2-&gt;is_address()) {
1530         LIR_Address * addr = opr2-&gt;as_address_ptr();
1531         BasicType type = addr-&gt;type();
1532         if ( type == T_OBJECT ) __ ld_ptr(as_Address(addr), O7);
1533         else                    __ ld(as_Address(addr), O7);
1534         __ cmp(opr1-&gt;as_register(), O7);
1535       } else {
1536         __ cmp(opr1-&gt;as_register(), opr2-&gt;as_register());
1537       }
1538     }
1539   } else if (opr1-&gt;is_double_cpu()) {
1540     Register xlo = opr1-&gt;as_register_lo();
1541     Register xhi = opr1-&gt;as_register_hi();
1542     if (opr2-&gt;is_constant() &amp;&amp; opr2-&gt;as_jlong() == 0) {
1543       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;only handles these cases&quot;);
1544       __ orcc(xhi, G0, G0);
1545     } else if (opr2-&gt;is_register()) {
1546       Register ylo = opr2-&gt;as_register_lo();
1547       Register yhi = opr2-&gt;as_register_hi();
1548       __ cmp(xlo, ylo);
1549     } else {
1550       ShouldNotReachHere();
1551     }
1552   } else if (opr1-&gt;is_address()) {
1553     LIR_Address * addr = opr1-&gt;as_address_ptr();
1554     BasicType type = addr-&gt;type();
1555     assert (opr2-&gt;is_constant(), &quot;Checking&quot;);
1556     if ( type == T_OBJECT ) __ ld_ptr(as_Address(addr), O7);
1557     else                    __ ld(as_Address(addr), O7);
1558     __ cmp(O7, opr2-&gt;as_constant_ptr()-&gt;as_jint());
1559   } else {
1560     ShouldNotReachHere();
1561   }
1562 }
1563 
1564 
1565 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op){
1566   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
1567     bool is_unordered_less = (code == lir_ucmp_fd2i);
1568     if (left-&gt;is_single_fpu()) {
1569       __ float_cmp(true, is_unordered_less ? -1 : 1, left-&gt;as_float_reg(), right-&gt;as_float_reg(), dst-&gt;as_register());
1570     } else if (left-&gt;is_double_fpu()) {
1571       __ float_cmp(false, is_unordered_less ? -1 : 1, left-&gt;as_double_reg(), right-&gt;as_double_reg(), dst-&gt;as_register());
1572     } else {
1573       ShouldNotReachHere();
1574     }
1575   } else if (code == lir_cmp_l2i) {
1576     __ lcmp(left-&gt;as_register_lo(), right-&gt;as_register_lo(), dst-&gt;as_register());
1577   } else {
1578     ShouldNotReachHere();
1579   }
1580 }
1581 
1582 
1583 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1584   Assembler::Condition acond;
1585   switch (condition) {
1586     case lir_cond_equal:        acond = Assembler::equal;        break;
1587     case lir_cond_notEqual:     acond = Assembler::notEqual;     break;
1588     case lir_cond_less:         acond = Assembler::less;         break;
1589     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    break;
1590     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; break;
1591     case lir_cond_greater:      acond = Assembler::greater;      break;
1592     case lir_cond_aboveEqual:   acond = Assembler::greaterEqualUnsigned;      break;
1593     case lir_cond_belowEqual:   acond = Assembler::lessEqualUnsigned;      break;
1594     default:                         ShouldNotReachHere();
1595   };
1596 
1597   if (opr1-&gt;is_constant() &amp;&amp; opr1-&gt;type() == T_INT) {
1598     Register dest = result-&gt;as_register();
1599     // load up first part of constant before branch
1600     // and do the rest in the delay slot.
1601     if (!Assembler::is_simm13(opr1-&gt;as_jint())) {
1602       __ sethi(opr1-&gt;as_jint(), dest);
1603     }
1604   } else if (opr1-&gt;is_constant()) {
1605     const2reg(opr1, result, lir_patch_none, NULL);
1606   } else if (opr1-&gt;is_register()) {
1607     reg2reg(opr1, result);
1608   } else if (opr1-&gt;is_stack()) {
1609     stack2reg(opr1, result, result-&gt;type());
1610   } else {
1611     ShouldNotReachHere();
1612   }
1613   Label skip;
1614     if  (type == T_INT) {
1615       __ br(acond, false, Assembler::pt, skip);
1616     } else {
1617       __ brx(acond, false, Assembler::pt, skip); // checks icc on 32bit and xcc on 64bit
1618     }
1619   if (opr1-&gt;is_constant() &amp;&amp; opr1-&gt;type() == T_INT) {
1620     Register dest = result-&gt;as_register();
1621     if (Assembler::is_simm13(opr1-&gt;as_jint())) {
1622       __ delayed()-&gt;or3(G0, opr1-&gt;as_jint(), dest);
1623     } else {
1624       // the sethi has been done above, so just put in the low 10 bits
1625       __ delayed()-&gt;or3(dest, opr1-&gt;as_jint() &amp; 0x3ff, dest);
1626     }
1627   } else {
1628     // can&#39;t do anything useful in the delay slot
1629     __ delayed()-&gt;nop();
1630   }
1631   if (opr2-&gt;is_constant()) {
1632     const2reg(opr2, result, lir_patch_none, NULL);
1633   } else if (opr2-&gt;is_register()) {
1634     reg2reg(opr2, result);
1635   } else if (opr2-&gt;is_stack()) {
1636     stack2reg(opr2, result, result-&gt;type());
1637   } else {
1638     ShouldNotReachHere();
1639   }
1640   __ bind(skip);
1641 }
1642 
1643 
1644 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1645   assert(info == NULL, &quot;unused on this code path&quot;);
1646   assert(left-&gt;is_register(), &quot;wrong items state&quot;);
1647   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1648 
1649   if (right-&gt;is_register()) {
1650     if (dest-&gt;is_float_kind()) {
1651 
1652       FloatRegister lreg, rreg, res;
1653       FloatRegisterImpl::Width w;
1654       if (right-&gt;is_single_fpu()) {
1655         w = FloatRegisterImpl::S;
1656         lreg = left-&gt;as_float_reg();
1657         rreg = right-&gt;as_float_reg();
1658         res  = dest-&gt;as_float_reg();
1659       } else {
1660         w = FloatRegisterImpl::D;
1661         lreg = left-&gt;as_double_reg();
1662         rreg = right-&gt;as_double_reg();
1663         res  = dest-&gt;as_double_reg();
1664       }
1665 
1666       switch (code) {
1667         case lir_add: __ fadd(w, lreg, rreg, res); break;
1668         case lir_sub: __ fsub(w, lreg, rreg, res); break;
1669         case lir_mul: // fall through
1670         case lir_mul_strictfp: __ fmul(w, lreg, rreg, res); break;
1671         case lir_div: // fall through
1672         case lir_div_strictfp: __ fdiv(w, lreg, rreg, res); break;
1673         default: ShouldNotReachHere();
1674       }
1675 
1676     } else if (dest-&gt;is_double_cpu()) {
1677       Register dst_lo = dest-&gt;as_register_lo();
1678       Register op1_lo = left-&gt;as_pointer_register();
1679       Register op2_lo = right-&gt;as_pointer_register();
1680 
1681       switch (code) {
1682         case lir_add:
1683           __ add(op1_lo, op2_lo, dst_lo);
1684           break;
1685 
1686         case lir_sub:
1687           __ sub(op1_lo, op2_lo, dst_lo);
1688           break;
1689 
1690         default: ShouldNotReachHere();
1691       }
1692     } else {
1693       assert (right-&gt;is_single_cpu(), &quot;Just Checking&quot;);
1694 
1695       Register lreg = left-&gt;as_register();
1696       Register res  = dest-&gt;as_register();
1697       Register rreg = right-&gt;as_register();
1698       switch (code) {
1699         case lir_add:  __ add  (lreg, rreg, res); break;
1700         case lir_sub:  __ sub  (lreg, rreg, res); break;
1701         case lir_mul:  __ mulx (lreg, rreg, res); break;
1702         default: ShouldNotReachHere();
1703       }
1704     }
1705   } else {
1706     assert (right-&gt;is_constant(), &quot;must be constant&quot;);
1707 
1708     if (dest-&gt;is_single_cpu()) {
1709       Register lreg = left-&gt;as_register();
1710       Register res  = dest-&gt;as_register();
1711       int    simm13 = right-&gt;as_constant_ptr()-&gt;as_jint();
1712 
1713       switch (code) {
1714         case lir_add:  __ add  (lreg, simm13, res); break;
1715         case lir_sub:  __ sub  (lreg, simm13, res); break;
1716         case lir_mul:  __ mulx (lreg, simm13, res); break;
1717         default: ShouldNotReachHere();
1718       }
1719     } else {
1720       Register lreg = left-&gt;as_pointer_register();
1721       Register res  = dest-&gt;as_register_lo();
1722       long con = right-&gt;as_constant_ptr()-&gt;as_jlong();
1723       assert(Assembler::is_simm13(con), &quot;must be simm13&quot;);
1724 
1725       switch (code) {
1726         case lir_add:  __ add  (lreg, (int)con, res); break;
1727         case lir_sub:  __ sub  (lreg, (int)con, res); break;
1728         case lir_mul:  __ mulx (lreg, (int)con, res); break;
1729         default: ShouldNotReachHere();
1730       }
1731     }
1732   }
1733 }
1734 
1735 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr thread, LIR_Opr dest, LIR_Op* op) {
1736   switch (code) {
1737     case lir_tan: {
1738       assert(thread-&gt;is_valid(), &quot;preserve the thread object for performance reasons&quot;);
1739       assert(dest-&gt;as_double_reg() == F0, &quot;the result will be in f0/f1&quot;);
1740       break;
1741     }
1742     case lir_sqrt: {
1743       assert(!thread-&gt;is_valid(), &quot;there is no need for a thread_reg for dsqrt&quot;);
1744       FloatRegister src_reg = value-&gt;as_double_reg();
1745       FloatRegister dst_reg = dest-&gt;as_double_reg();
1746       __ fsqrt(FloatRegisterImpl::D, src_reg, dst_reg);
1747       break;
1748     }
1749     case lir_abs: {
1750       assert(!thread-&gt;is_valid(), &quot;there is no need for a thread_reg for fabs&quot;);
1751       FloatRegister src_reg = value-&gt;as_double_reg();
1752       FloatRegister dst_reg = dest-&gt;as_double_reg();
1753       __ fabs(FloatRegisterImpl::D, src_reg, dst_reg);
1754       break;
1755     }
1756     default: {
1757       ShouldNotReachHere();
1758       break;
1759     }
1760   }
1761 }
1762 
1763 
1764 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {
1765   if (right-&gt;is_constant()) {
1766     if (dest-&gt;is_single_cpu()) {
1767       int simm13 = right-&gt;as_constant_ptr()-&gt;as_jint();
1768       switch (code) {
1769         case lir_logic_and:   __ and3 (left-&gt;as_register(), simm13, dest-&gt;as_register()); break;
1770         case lir_logic_or:    __ or3  (left-&gt;as_register(), simm13, dest-&gt;as_register()); break;
1771         case lir_logic_xor:   __ xor3 (left-&gt;as_register(), simm13, dest-&gt;as_register()); break;
1772         default: ShouldNotReachHere();
1773       }
1774     } else {
1775       long c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1776       assert(c == (int)c &amp;&amp; Assembler::is_simm13(c), &quot;out of range&quot;);
1777       int simm13 = (int)c;
1778       switch (code) {
1779         case lir_logic_and:
1780           __ and3 (left-&gt;as_register_lo(), simm13, dest-&gt;as_register_lo());
1781           break;
1782 
1783         case lir_logic_or:
1784           __ or3 (left-&gt;as_register_lo(), simm13, dest-&gt;as_register_lo());
1785           break;
1786 
1787         case lir_logic_xor:
1788           __ xor3 (left-&gt;as_register_lo(), simm13, dest-&gt;as_register_lo());
1789           break;
1790 
1791         default: ShouldNotReachHere();
1792       }
1793     }
1794   } else {
1795     assert(right-&gt;is_register(), &quot;right should be in register&quot;);
1796 
1797     if (dest-&gt;is_single_cpu()) {
1798       switch (code) {
1799         case lir_logic_and:   __ and3 (left-&gt;as_register(), right-&gt;as_register(), dest-&gt;as_register()); break;
1800         case lir_logic_or:    __ or3  (left-&gt;as_register(), right-&gt;as_register(), dest-&gt;as_register()); break;
1801         case lir_logic_xor:   __ xor3 (left-&gt;as_register(), right-&gt;as_register(), dest-&gt;as_register()); break;
1802         default: ShouldNotReachHere();
1803       }
1804     } else {
1805       Register l = (left-&gt;is_single_cpu() &amp;&amp; left-&gt;is_oop_register()) ? left-&gt;as_register() :
1806                                                                         left-&gt;as_register_lo();
1807       Register r = (right-&gt;is_single_cpu() &amp;&amp; right-&gt;is_oop_register()) ? right-&gt;as_register() :
1808                                                                           right-&gt;as_register_lo();
1809 
1810       switch (code) {
1811         case lir_logic_and: __ and3 (l, r, dest-&gt;as_register_lo()); break;
1812         case lir_logic_or:  __ or3  (l, r, dest-&gt;as_register_lo()); break;
1813         case lir_logic_xor: __ xor3 (l, r, dest-&gt;as_register_lo()); break;
1814         default: ShouldNotReachHere();
1815       }
1816     }
1817   }
1818 }
1819 
1820 
1821 int LIR_Assembler::shift_amount(BasicType t) {
1822   int elem_size = type2aelembytes(t);
1823   switch (elem_size) {
1824     case 1 : return 0;
1825     case 2 : return 1;
1826     case 4 : return 2;
1827     case 8 : return 3;
1828   }
1829   ShouldNotReachHere();
1830   return -1;
1831 }
1832 
1833 
1834 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
1835   assert(exceptionOop-&gt;as_register() == Oexception, &quot;should match&quot;);
1836   assert(exceptionPC-&gt;as_register() == Oissuing_pc, &quot;should match&quot;);
1837 
1838   info-&gt;add_register_oop(exceptionOop);
1839 
1840   // reuse the debug info from the safepoint poll for the throw op itself
1841   address pc_for_athrow  = __ pc();
1842   int pc_for_athrow_offset = __ offset();
1843   RelocationHolder rspec = internal_word_Relocation::spec(pc_for_athrow);
1844   __ set(pc_for_athrow, Oissuing_pc, rspec);
1845   add_call_info(pc_for_athrow_offset, info); // for exception handler
1846 
1847   __ call(Runtime1::entry_for(Runtime1::handle_exception_id), relocInfo::runtime_call_type);
1848   __ delayed()-&gt;nop();
1849 }
1850 
1851 
1852 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
1853   assert(exceptionOop-&gt;as_register() == Oexception, &quot;should match&quot;);
1854 
1855   __ br(Assembler::always, false, Assembler::pt, _unwind_handler_entry);
1856   __ delayed()-&gt;nop();
1857 }
1858 
1859 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
1860   Register src = op-&gt;src()-&gt;as_register();
1861   Register dst = op-&gt;dst()-&gt;as_register();
1862   Register src_pos = op-&gt;src_pos()-&gt;as_register();
1863   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
1864   Register length  = op-&gt;length()-&gt;as_register();
1865   Register tmp = op-&gt;tmp()-&gt;as_register();
1866   Register tmp2 = O7;
1867 
1868   int flags = op-&gt;flags();
1869   ciArrayKlass* default_type = op-&gt;expected_type();
1870   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
1871   if (basic_type == T_ARRAY) basic_type = T_OBJECT;
1872 
1873   // higher 32bits must be null
1874   __ sra(dst_pos, 0, dst_pos);
1875   __ sra(src_pos, 0, src_pos);
1876   __ sra(length, 0, length);
1877 
1878   // set up the arraycopy stub information
1879   ArrayCopyStub* stub = op-&gt;stub();
1880 
1881   // always do stub if no type information is available.  it&#39;s ok if
1882   // the known type isn&#39;t loaded since the code sanity checks
1883   // in debug mode and the type isn&#39;t required when we know the exact type
1884   // also check that the type is an array type.
1885   if (op-&gt;expected_type() == NULL) {
1886     __ mov(src,     O0);
1887     __ mov(src_pos, O1);
1888     __ mov(dst,     O2);
1889     __ mov(dst_pos, O3);
1890     __ mov(length,  O4);
1891     address copyfunc_addr = StubRoutines::generic_arraycopy();
1892     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
1893 
1894 #ifndef PRODUCT
1895     if (PrintC1Statistics) {
1896       address counter = (address)&amp;Runtime1::_generic_arraycopystub_cnt;
1897       __ inc_counter(counter, G1, G3);
1898     }
1899 #endif
1900     __ call_VM_leaf(tmp, copyfunc_addr);
1901 
1902     __ xor3(O0, -1, tmp);
1903     __ sub(length, tmp, length);
1904     __ add(src_pos, tmp, src_pos);
1905     __ cmp_zero_and_br(Assembler::less, O0, *stub-&gt;entry());
1906     __ delayed()-&gt;add(dst_pos, tmp, dst_pos);
1907     __ bind(*stub-&gt;continuation());
1908     return;
1909   }
1910 
1911   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass(), &quot;must be true at this point&quot;);
1912 
1913   // make sure src and dst are non-null and load array length
1914   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
1915     __ tst(src);
1916     __ brx(Assembler::equal, false, Assembler::pn, *stub-&gt;entry());
1917     __ delayed()-&gt;nop();
1918   }
1919 
1920   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
1921     __ tst(dst);
1922     __ brx(Assembler::equal, false, Assembler::pn, *stub-&gt;entry());
1923     __ delayed()-&gt;nop();
1924   }
1925 
1926   // If the compiler was not able to prove that exact type of the source or the destination
1927   // of the arraycopy is an array type, check at runtime if the source or the destination is
1928   // an instance type.
1929   if (flags &amp; LIR_OpArrayCopy::type_check) {
1930     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
1931       __ load_klass(dst, tmp);
1932       __ lduw(tmp, in_bytes(Klass::layout_helper_offset()), tmp2);
1933       __ cmp(tmp2, Klass::_lh_neutral_value);
1934       __ br(Assembler::greaterEqual, false, Assembler::pn, *stub-&gt;entry());
1935       __ delayed()-&gt;nop();
1936     }
1937 
1938     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
1939       __ load_klass(src, tmp);
1940       __ lduw(tmp, in_bytes(Klass::layout_helper_offset()), tmp2);
1941       __ cmp(tmp2, Klass::_lh_neutral_value);
1942       __ br(Assembler::greaterEqual, false, Assembler::pn, *stub-&gt;entry());
1943       __ delayed()-&gt;nop();
1944     }
1945   }
1946 
1947   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
1948     // test src_pos register
1949     __ cmp_zero_and_br(Assembler::less, src_pos, *stub-&gt;entry());
1950     __ delayed()-&gt;nop();
1951   }
1952 
1953   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
1954     // test dst_pos register
1955     __ cmp_zero_and_br(Assembler::less, dst_pos, *stub-&gt;entry());
1956     __ delayed()-&gt;nop();
1957   }
1958 
1959   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
1960     // make sure length isn&#39;t negative
1961     __ cmp_zero_and_br(Assembler::less, length, *stub-&gt;entry());
1962     __ delayed()-&gt;nop();
1963   }
1964 
1965   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
1966     __ ld(src, arrayOopDesc::length_offset_in_bytes(), tmp2);
1967     __ add(length, src_pos, tmp);
1968     __ cmp(tmp2, tmp);
1969     __ br(Assembler::carrySet, false, Assembler::pn, *stub-&gt;entry());
1970     __ delayed()-&gt;nop();
1971   }
1972 
1973   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
1974     __ ld(dst, arrayOopDesc::length_offset_in_bytes(), tmp2);
1975     __ add(length, dst_pos, tmp);
1976     __ cmp(tmp2, tmp);
1977     __ br(Assembler::carrySet, false, Assembler::pn, *stub-&gt;entry());
1978     __ delayed()-&gt;nop();
1979   }
1980 
1981   int shift = shift_amount(basic_type);
1982 
1983   if (flags &amp; LIR_OpArrayCopy::type_check) {
1984     // We don&#39;t know the array types are compatible
1985     if (basic_type != T_OBJECT) {
1986       // Simple test for basic type arrays
1987       if (UseCompressedClassPointers) {
1988         // We don&#39;t need decode because we just need to compare
1989         __ lduw(src, oopDesc::klass_offset_in_bytes(), tmp);
1990         __ lduw(dst, oopDesc::klass_offset_in_bytes(), tmp2);
1991         __ cmp(tmp, tmp2);
1992         __ br(Assembler::notEqual, false, Assembler::pt, *stub-&gt;entry());
1993       } else {
1994         __ ld_ptr(src, oopDesc::klass_offset_in_bytes(), tmp);
1995         __ ld_ptr(dst, oopDesc::klass_offset_in_bytes(), tmp2);
1996         __ cmp(tmp, tmp2);
1997         __ brx(Assembler::notEqual, false, Assembler::pt, *stub-&gt;entry());
1998       }
1999       __ delayed()-&gt;nop();
2000     } else {
2001       // For object arrays, if src is a sub class of dst then we can
2002       // safely do the copy.
2003       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2004 
2005       Label cont, slow;
2006       assert_different_registers(tmp, tmp2, G3, G1);
2007 
2008       __ load_klass(src, G3);
2009       __ load_klass(dst, G1);
2010 
2011       __ check_klass_subtype_fast_path(G3, G1, tmp, tmp2, &amp;cont, copyfunc_addr == NULL ? stub-&gt;entry() : &amp;slow, NULL);
2012 
2013       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
2014       __ delayed()-&gt;nop();
2015 
2016       __ cmp(G3, 0);
2017       if (copyfunc_addr != NULL) { // use stub if available
2018         // src is not a sub class of dst so we have to do a
2019         // per-element check.
2020         __ br(Assembler::notEqual, false, Assembler::pt, cont);
2021         __ delayed()-&gt;nop();
2022 
2023         __ bind(slow);
2024 
2025         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2026         if ((flags &amp; mask) != mask) {
2027           // Check that at least both of them object arrays.
2028           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2029 
2030           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2031             __ load_klass(src, tmp);
2032           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2033             __ load_klass(dst, tmp);
2034           }
2035           int lh_offset = in_bytes(Klass::layout_helper_offset());
2036 
2037           __ lduw(tmp, lh_offset, tmp2);
2038 
2039           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2040           __ set(objArray_lh, tmp);
2041           __ cmp(tmp, tmp2);
2042           __ br(Assembler::notEqual, false, Assembler::pt,  *stub-&gt;entry());
2043           __ delayed()-&gt;nop();
2044         }
2045 
2046         Register src_ptr = O0;
2047         Register dst_ptr = O1;
2048         Register len     = O2;
2049         Register chk_off = O3;
2050         Register super_k = O4;
2051 
2052         __ add(src, arrayOopDesc::base_offset_in_bytes(basic_type), src_ptr);
2053         if (shift == 0) {
2054           __ add(src_ptr, src_pos, src_ptr);
2055         } else {
2056           __ sll(src_pos, shift, tmp);
2057           __ add(src_ptr, tmp, src_ptr);
2058         }
2059 
2060         __ add(dst, arrayOopDesc::base_offset_in_bytes(basic_type), dst_ptr);
2061         if (shift == 0) {
2062           __ add(dst_ptr, dst_pos, dst_ptr);
2063         } else {
2064           __ sll(dst_pos, shift, tmp);
2065           __ add(dst_ptr, tmp, dst_ptr);
2066         }
2067         __ mov(length, len);
2068         __ load_klass(dst, tmp);
2069 
2070         int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2071         __ ld_ptr(tmp, ek_offset, super_k);
2072 
2073         int sco_offset = in_bytes(Klass::super_check_offset_offset());
2074         __ lduw(super_k, sco_offset, chk_off);
2075 
2076         __ call_VM_leaf(tmp, copyfunc_addr);
2077 
2078 #ifndef PRODUCT
2079         if (PrintC1Statistics) {
2080           Label failed;
2081           __ br_notnull_short(O0, Assembler::pn, failed);
2082           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_cnt, G1, G3);
2083           __ bind(failed);
2084         }
2085 #endif
2086 
2087         __ br_null(O0, false, Assembler::pt,  *stub-&gt;continuation());
2088         __ delayed()-&gt;xor3(O0, -1, tmp);
2089 
2090 #ifndef PRODUCT
2091         if (PrintC1Statistics) {
2092           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt, G1, G3);
2093         }
2094 #endif
2095 
2096         __ sub(length, tmp, length);
2097         __ add(src_pos, tmp, src_pos);
2098         __ br(Assembler::always, false, Assembler::pt, *stub-&gt;entry());
2099         __ delayed()-&gt;add(dst_pos, tmp, dst_pos);
2100 
2101         __ bind(cont);
2102       } else {
2103         __ br(Assembler::equal, false, Assembler::pn, *stub-&gt;entry());
2104         __ delayed()-&gt;nop();
2105         __ bind(cont);
2106       }
2107     }
2108   }
2109 
2110 #ifdef ASSERT
2111   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
2112     // Sanity check the known type with the incoming class.  For the
2113     // primitive case the types must match exactly with src.klass and
2114     // dst.klass each exactly matching the default type.  For the
2115     // object array case, if no type check is needed then either the
2116     // dst type is exactly the expected type and the src type is a
2117     // subtype which we can&#39;t check or src is the same array as dst
2118     // but not necessarily exactly of type default_type.
2119     Label known_ok, halt;
2120     metadata2reg(op-&gt;expected_type()-&gt;constant_encoding(), tmp);
2121     if (UseCompressedClassPointers) {
2122       // tmp holds the default type. It currently comes uncompressed after the
2123       // load of a constant, so encode it.
2124       __ encode_klass_not_null(tmp);
2125       // load the raw value of the dst klass, since we will be comparing
2126       // uncompressed values directly.
2127       __ lduw(dst, oopDesc::klass_offset_in_bytes(), tmp2);
2128       if (basic_type != T_OBJECT) {
2129         __ cmp(tmp, tmp2);
2130         __ br(Assembler::notEqual, false, Assembler::pn, halt);
2131         // load the raw value of the src klass.
2132         __ delayed()-&gt;lduw(src, oopDesc::klass_offset_in_bytes(), tmp2);
2133         __ cmp_and_br_short(tmp, tmp2, Assembler::equal, Assembler::pn, known_ok);
2134       } else {
2135         __ cmp(tmp, tmp2);
2136         __ br(Assembler::equal, false, Assembler::pn, known_ok);
2137         __ delayed()-&gt;cmp(src, dst);
2138         __ brx(Assembler::equal, false, Assembler::pn, known_ok);
2139         __ delayed()-&gt;nop();
2140       }
2141     } else {
2142       __ ld_ptr(dst, oopDesc::klass_offset_in_bytes(), tmp2);
2143       if (basic_type != T_OBJECT) {
2144         __ cmp(tmp, tmp2);
2145         __ brx(Assembler::notEqual, false, Assembler::pn, halt);
2146         __ delayed()-&gt;ld_ptr(src, oopDesc::klass_offset_in_bytes(), tmp2);
2147         __ cmp_and_brx_short(tmp, tmp2, Assembler::equal, Assembler::pn, known_ok);
2148       } else {
2149         __ cmp(tmp, tmp2);
2150         __ brx(Assembler::equal, false, Assembler::pn, known_ok);
2151         __ delayed()-&gt;cmp(src, dst);
2152         __ brx(Assembler::equal, false, Assembler::pn, known_ok);
2153         __ delayed()-&gt;nop();
2154       }
2155     }
2156     __ bind(halt);
2157     __ stop(&quot;incorrect type information in arraycopy&quot;);
2158     __ bind(known_ok);
2159   }
2160 #endif
2161 
2162 #ifndef PRODUCT
2163   if (PrintC1Statistics) {
2164     address counter = Runtime1::arraycopy_count_address(basic_type);
2165     __ inc_counter(counter, G1, G3);
2166   }
2167 #endif
2168 
2169   Register src_ptr = O0;
2170   Register dst_ptr = O1;
2171   Register len     = O2;
2172 
2173   __ add(src, arrayOopDesc::base_offset_in_bytes(basic_type), src_ptr);
2174   if (shift == 0) {
2175     __ add(src_ptr, src_pos, src_ptr);
2176   } else {
2177     __ sll(src_pos, shift, tmp);
2178     __ add(src_ptr, tmp, src_ptr);
2179   }
2180 
2181   __ add(dst, arrayOopDesc::base_offset_in_bytes(basic_type), dst_ptr);
2182   if (shift == 0) {
2183     __ add(dst_ptr, dst_pos, dst_ptr);
2184   } else {
2185     __ sll(dst_pos, shift, tmp);
2186     __ add(dst_ptr, tmp, dst_ptr);
2187   }
2188 
2189   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2190   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2191   const char *name;
2192   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2193 
2194   // arraycopy stubs takes a length in number of elements, so don&#39;t scale it.
2195   __ mov(length, len);
2196   __ call_VM_leaf(tmp, entry);
2197 
2198   __ bind(*stub-&gt;continuation());
2199 }
2200 
2201 
2202 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2203   if (dest-&gt;is_single_cpu()) {
2204     if (left-&gt;type() == T_OBJECT) {
2205       switch (code) {
2206         case lir_shl:  __ sllx  (left-&gt;as_register(), count-&gt;as_register(), dest-&gt;as_register()); break;
2207         case lir_shr:  __ srax  (left-&gt;as_register(), count-&gt;as_register(), dest-&gt;as_register()); break;
2208         case lir_ushr: __ srl   (left-&gt;as_register(), count-&gt;as_register(), dest-&gt;as_register()); break;
2209         default: ShouldNotReachHere();
2210       }
2211     } else
2212       switch (code) {
2213         case lir_shl:  __ sll   (left-&gt;as_register(), count-&gt;as_register(), dest-&gt;as_register()); break;
2214         case lir_shr:  __ sra   (left-&gt;as_register(), count-&gt;as_register(), dest-&gt;as_register()); break;
2215         case lir_ushr: __ srl   (left-&gt;as_register(), count-&gt;as_register(), dest-&gt;as_register()); break;
2216         default: ShouldNotReachHere();
2217       }
2218   } else {
2219     switch (code) {
2220       case lir_shl:  __ sllx  (left-&gt;as_register_lo(), count-&gt;as_register(), dest-&gt;as_register_lo()); break;
2221       case lir_shr:  __ srax  (left-&gt;as_register_lo(), count-&gt;as_register(), dest-&gt;as_register_lo()); break;
2222       case lir_ushr: __ srlx  (left-&gt;as_register_lo(), count-&gt;as_register(), dest-&gt;as_register_lo()); break;
2223       default: ShouldNotReachHere();
2224     }
2225   }
2226 }
2227 
2228 
2229 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2230   if (left-&gt;type() == T_OBJECT) {
2231     count = count &amp; 63;  // shouldn&#39;t shift by more than sizeof(intptr_t)
2232     Register l = left-&gt;as_register();
2233     Register d = dest-&gt;as_register_lo();
2234     switch (code) {
2235       case lir_shl:  __ sllx  (l, count, d); break;
2236       case lir_shr:  __ srax  (l, count, d); break;
2237       case lir_ushr: __ srlx  (l, count, d); break;
2238       default: ShouldNotReachHere();
2239     }
2240     return;
2241   }
2242 
2243   if (dest-&gt;is_single_cpu()) {
2244     count = count &amp; 0x1F; // Java spec
2245     switch (code) {
2246       case lir_shl:  __ sll   (left-&gt;as_register(), count, dest-&gt;as_register()); break;
2247       case lir_shr:  __ sra   (left-&gt;as_register(), count, dest-&gt;as_register()); break;
2248       case lir_ushr: __ srl   (left-&gt;as_register(), count, dest-&gt;as_register()); break;
2249       default: ShouldNotReachHere();
2250     }
2251   } else if (dest-&gt;is_double_cpu()) {
2252     count = count &amp; 63; // Java spec
2253     switch (code) {
2254       case lir_shl:  __ sllx  (left-&gt;as_pointer_register(), count, dest-&gt;as_pointer_register()); break;
2255       case lir_shr:  __ srax  (left-&gt;as_pointer_register(), count, dest-&gt;as_pointer_register()); break;
2256       case lir_ushr: __ srlx  (left-&gt;as_pointer_register(), count, dest-&gt;as_pointer_register()); break;
2257       default: ShouldNotReachHere();
2258     }
2259   } else {
2260     ShouldNotReachHere();
2261   }
2262 }
2263 
2264 
2265 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
2266   assert(op-&gt;tmp1()-&gt;as_register()  == G1 &amp;&amp;
2267          op-&gt;tmp2()-&gt;as_register()  == G3 &amp;&amp;
2268          op-&gt;tmp3()-&gt;as_register()  == G4 &amp;&amp;
2269          op-&gt;obj()-&gt;as_register()   == O0 &amp;&amp;
2270          op-&gt;klass()-&gt;as_register() == G5, &quot;must be&quot;);
2271   if (op-&gt;init_check()) {
2272     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
2273     __ ldub(op-&gt;klass()-&gt;as_register(),
2274           in_bytes(InstanceKlass::init_state_offset()),
2275           op-&gt;tmp1()-&gt;as_register());
2276     __ cmp(op-&gt;tmp1()-&gt;as_register(), InstanceKlass::fully_initialized);
2277     __ br(Assembler::notEqual, false, Assembler::pn, *op-&gt;stub()-&gt;entry());
2278     __ delayed()-&gt;nop();
2279   }
2280   __ allocate_object(op-&gt;obj()-&gt;as_register(),
2281                      op-&gt;tmp1()-&gt;as_register(),
2282                      op-&gt;tmp2()-&gt;as_register(),
2283                      op-&gt;tmp3()-&gt;as_register(),
2284                      op-&gt;header_size(),
2285                      op-&gt;object_size(),
2286                      op-&gt;klass()-&gt;as_register(),
2287                      *op-&gt;stub()-&gt;entry());
2288   __ bind(*op-&gt;stub()-&gt;continuation());
2289   __ verify_oop(op-&gt;obj()-&gt;as_register());
2290 }
2291 
2292 
2293 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
2294   assert(op-&gt;tmp1()-&gt;as_register()  == G1 &amp;&amp;
2295          op-&gt;tmp2()-&gt;as_register()  == G3 &amp;&amp;
2296          op-&gt;tmp3()-&gt;as_register()  == G4 &amp;&amp;
2297          op-&gt;tmp4()-&gt;as_register()  == O1 &amp;&amp;
2298          op-&gt;klass()-&gt;as_register() == G5, &quot;must be&quot;);
2299 
2300   __ signx(op-&gt;len()-&gt;as_register());
2301   if (UseSlowPath ||
2302       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
2303       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
2304     __ br(Assembler::always, false, Assembler::pt, *op-&gt;stub()-&gt;entry());
2305     __ delayed()-&gt;nop();
2306   } else {
2307     __ allocate_array(op-&gt;obj()-&gt;as_register(),
2308                       op-&gt;len()-&gt;as_register(),
2309                       op-&gt;tmp1()-&gt;as_register(),
2310                       op-&gt;tmp2()-&gt;as_register(),
2311                       op-&gt;tmp3()-&gt;as_register(),
2312                       arrayOopDesc::header_size(op-&gt;type()),
2313                       type2aelembytes(op-&gt;type()),
2314                       op-&gt;klass()-&gt;as_register(),
2315                       *op-&gt;stub()-&gt;entry());
2316   }
2317   __ bind(*op-&gt;stub()-&gt;continuation());
2318 }
2319 
2320 
2321 void LIR_Assembler::type_profile_helper(Register mdo, int mdo_offset_bias,
2322                                         ciMethodData *md, ciProfileData *data,
2323                                         Register recv, Register tmp1, Label* update_done) {
2324   uint i;
2325   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2326     Label next_test;
2327     // See if the receiver is receiver[n].
2328     Address receiver_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
2329                           mdo_offset_bias);
2330     __ ld_ptr(receiver_addr, tmp1);
2331     __ verify_klass_ptr(tmp1);
2332     __ cmp_and_brx_short(recv, tmp1, Assembler::notEqual, Assembler::pt, next_test);
2333     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
2334                       mdo_offset_bias);
2335     __ ld_ptr(data_addr, tmp1);
2336     __ add(tmp1, DataLayout::counter_increment, tmp1);
2337     __ st_ptr(tmp1, data_addr);
2338     __ ba(*update_done);
2339     __ delayed()-&gt;nop();
2340     __ bind(next_test);
2341   }
2342 
2343   // Didn&#39;t find receiver; find next empty slot and fill it in
2344   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2345     Label next_test;
2346     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
2347                       mdo_offset_bias);
2348     __ ld_ptr(recv_addr, tmp1);
2349     __ br_notnull_short(tmp1, Assembler::pt, next_test);
2350     __ st_ptr(recv, recv_addr);
2351     __ set(DataLayout::counter_increment, tmp1);
2352     __ st_ptr(tmp1, mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
2353               mdo_offset_bias);
2354     __ ba(*update_done);
2355     __ delayed()-&gt;nop();
2356     __ bind(next_test);
2357   }
2358 }
2359 
2360 
2361 void LIR_Assembler::setup_md_access(ciMethod* method, int bci,
2362                                     ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias) {
2363   md = method-&gt;method_data_or_null();
2364   assert(md != NULL, &quot;Sanity&quot;);
2365   data = md-&gt;bci_to_data(bci);
2366   assert(data != NULL,       &quot;need data for checkcast&quot;);
2367   assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
2368   if (!Assembler::is_simm13(md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()) + data-&gt;size_in_bytes())) {
2369     // The offset is large so bias the mdo by the base of the slot so
2370     // that the ld can use simm13s to reference the slots of the data
2371     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, DataLayout::header_offset());
2372   }
2373 }
2374 
2375 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
2376   // we always need a stub for the failure case.
2377   CodeStub* stub = op-&gt;stub();
2378   Register obj = op-&gt;object()-&gt;as_register();
2379   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
2380   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
2381   Register dst = op-&gt;result_opr()-&gt;as_register();
2382   Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
2383   ciKlass* k = op-&gt;klass();
2384 
2385 
2386   if (obj == k_RInfo) {
2387     k_RInfo = klass_RInfo;
2388     klass_RInfo = obj;
2389   }
2390 
2391   ciMethodData* md;
2392   ciProfileData* data;
2393   int mdo_offset_bias = 0;
2394   if (op-&gt;should_profile()) {
2395     ciMethod* method = op-&gt;profiled_method();
2396     assert(method != NULL, &quot;Should have method&quot;);
2397     setup_md_access(method, op-&gt;profiled_bci(), md, data, mdo_offset_bias);
2398 
2399     Label not_null;
2400     __ br_notnull_short(obj, Assembler::pn, not_null);
2401     Register mdo      = k_RInfo;
2402     Register data_val = Rtmp1;
2403     metadata2reg(md-&gt;constant_encoding(), mdo);
2404     if (mdo_offset_bias &gt; 0) {
2405       __ set(mdo_offset_bias, data_val);
2406       __ add(mdo, data_val, mdo);
2407     }
2408     Address flags_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()) - mdo_offset_bias);
2409     __ ldub(flags_addr, data_val);
2410     __ or3(data_val, BitData::null_seen_byte_constant(), data_val);
2411     __ stb(data_val, flags_addr);
2412     __ ba(*obj_is_null);
2413     __ delayed()-&gt;nop();
2414     __ bind(not_null);
2415   } else {
2416     __ br_null(obj, false, Assembler::pn, *obj_is_null);
2417     __ delayed()-&gt;nop();
2418   }
2419 
2420   Label profile_cast_failure, profile_cast_success;
2421   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
2422   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
2423 
2424   // patching may screw with our temporaries on sparc,
2425   // so let&#39;s do it before loading the class
2426   if (k-&gt;is_loaded()) {
2427     metadata2reg(k-&gt;constant_encoding(), k_RInfo);
2428   } else {
2429     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
2430   }
2431   assert(obj != k_RInfo, &quot;must be different&quot;);
2432 
2433   // get object class
2434   // not a safepoint as obj null check happens earlier
2435   __ load_klass(obj, klass_RInfo);
2436   if (op-&gt;fast_check()) {
2437     assert_different_registers(klass_RInfo, k_RInfo);
2438     __ cmp(k_RInfo, klass_RInfo);
2439     __ brx(Assembler::notEqual, false, Assembler::pt, *failure_target);
2440     __ delayed()-&gt;nop();
2441   } else {
2442     bool need_slow_path = true;
2443     if (k-&gt;is_loaded()) {
2444       if ((int) k-&gt;super_check_offset() != in_bytes(Klass::secondary_super_cache_offset()))
2445         need_slow_path = false;
2446       // perform the fast part of the checking logic
2447       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, noreg,
2448                                        (need_slow_path ? success_target : NULL),
2449                                        failure_target, NULL,
2450                                        RegisterOrConstant(k-&gt;super_check_offset()));
2451     } else {
2452       // perform the fast part of the checking logic
2453       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, O7, success_target,
2454                                        failure_target, NULL);
2455     }
2456     if (need_slow_path) {
2457       // call out-of-line instance of __ check_klass_subtype_slow_path(...):
2458       assert(klass_RInfo == G3 &amp;&amp; k_RInfo == G1, &quot;incorrect call setup&quot;);
2459       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
2460       __ delayed()-&gt;nop();
2461       __ cmp(G3, 0);
2462       __ br(Assembler::equal, false, Assembler::pn, *failure_target);
2463       __ delayed()-&gt;nop();
2464       // Fall through to success case
2465     }
2466   }
2467 
2468   if (op-&gt;should_profile()) {
2469     Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtmp1;
2470     assert_different_registers(obj, mdo, recv, tmp1);
2471     __ bind(profile_cast_success);
2472     metadata2reg(md-&gt;constant_encoding(), mdo);
2473     if (mdo_offset_bias &gt; 0) {
2474       __ set(mdo_offset_bias, tmp1);
2475       __ add(mdo, tmp1, mdo);
2476     }
2477     __ load_klass(obj, recv);
2478     type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, success);
2479     // Jump over the failure case
2480     __ ba(*success);
2481     __ delayed()-&gt;nop();
2482     // Cast failure case
2483     __ bind(profile_cast_failure);
2484     metadata2reg(md-&gt;constant_encoding(), mdo);
2485     if (mdo_offset_bias &gt; 0) {
2486       __ set(mdo_offset_bias, tmp1);
2487       __ add(mdo, tmp1, mdo);
2488     }
2489     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2490     __ ld_ptr(data_addr, tmp1);
2491     __ sub(tmp1, DataLayout::counter_increment, tmp1);
2492     __ st_ptr(tmp1, data_addr);
2493     __ ba(*failure);
2494     __ delayed()-&gt;nop();
2495   }
2496   __ ba(*success);
2497   __ delayed()-&gt;nop();
2498 }
2499 
2500 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
2501   LIR_Code code = op-&gt;code();
2502   if (code == lir_store_check) {
2503     Register value = op-&gt;object()-&gt;as_register();
2504     Register array = op-&gt;array()-&gt;as_register();
2505     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
2506     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
2507     Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
2508 
2509     __ verify_oop(value);
2510     CodeStub* stub = op-&gt;stub();
2511     // check if it needs to be profiled
2512     ciMethodData* md;
2513     ciProfileData* data;
2514     int mdo_offset_bias = 0;
2515     if (op-&gt;should_profile()) {
2516       ciMethod* method = op-&gt;profiled_method();
2517       assert(method != NULL, &quot;Should have method&quot;);
2518       setup_md_access(method, op-&gt;profiled_bci(), md, data, mdo_offset_bias);
2519     }
2520     Label profile_cast_success, profile_cast_failure, done;
2521     Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
2522     Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
2523 
2524     if (op-&gt;should_profile()) {
2525       Label not_null;
2526       __ br_notnull_short(value, Assembler::pn, not_null);
2527       Register mdo      = k_RInfo;
2528       Register data_val = Rtmp1;
2529       metadata2reg(md-&gt;constant_encoding(), mdo);
2530       if (mdo_offset_bias &gt; 0) {
2531         __ set(mdo_offset_bias, data_val);
2532         __ add(mdo, data_val, mdo);
2533       }
2534       Address flags_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()) - mdo_offset_bias);
2535       __ ldub(flags_addr, data_val);
2536       __ or3(data_val, BitData::null_seen_byte_constant(), data_val);
2537       __ stb(data_val, flags_addr);
2538       __ ba_short(done);
2539       __ bind(not_null);
2540     } else {
2541       __ br_null_short(value, Assembler::pn, done);
2542     }
2543     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
2544     __ load_klass(array, k_RInfo);
2545     __ load_klass(value, klass_RInfo);
2546 
2547     // get instance klass
2548     __ ld_ptr(Address(k_RInfo, ObjArrayKlass::element_klass_offset()), k_RInfo);
2549     // perform the fast part of the checking logic
2550     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, O7, success_target, failure_target, NULL);
2551 
2552     // call out-of-line instance of __ check_klass_subtype_slow_path(...):
2553     assert(klass_RInfo == G3 &amp;&amp; k_RInfo == G1, &quot;incorrect call setup&quot;);
2554     __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
2555     __ delayed()-&gt;nop();
2556     __ cmp(G3, 0);
2557     __ br(Assembler::equal, false, Assembler::pn, *failure_target);
2558     __ delayed()-&gt;nop();
2559     // fall through to the success case
2560 
2561     if (op-&gt;should_profile()) {
2562       Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtmp1;
2563       assert_different_registers(value, mdo, recv, tmp1);
2564       __ bind(profile_cast_success);
2565       metadata2reg(md-&gt;constant_encoding(), mdo);
2566       if (mdo_offset_bias &gt; 0) {
2567         __ set(mdo_offset_bias, tmp1);
2568         __ add(mdo, tmp1, mdo);
2569       }
2570       __ load_klass(value, recv);
2571       type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, &amp;done);
2572       __ ba_short(done);
2573       // Cast failure case
2574       __ bind(profile_cast_failure);
2575       metadata2reg(md-&gt;constant_encoding(), mdo);
2576       if (mdo_offset_bias &gt; 0) {
2577         __ set(mdo_offset_bias, tmp1);
2578         __ add(mdo, tmp1, mdo);
2579       }
2580       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2581       __ ld_ptr(data_addr, tmp1);
2582       __ sub(tmp1, DataLayout::counter_increment, tmp1);
2583       __ st_ptr(tmp1, data_addr);
2584       __ ba(*stub-&gt;entry());
2585       __ delayed()-&gt;nop();
2586     }
2587     __ bind(done);
2588   } else if (code == lir_checkcast) {
2589     Register obj = op-&gt;object()-&gt;as_register();
2590     Register dst = op-&gt;result_opr()-&gt;as_register();
2591     Label success;
2592     emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
2593     __ bind(success);
2594     __ mov(obj, dst);
2595   } else if (code == lir_instanceof) {
2596     Register obj = op-&gt;object()-&gt;as_register();
2597     Register dst = op-&gt;result_opr()-&gt;as_register();
2598     Label success, failure, done;
2599     emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
2600     __ bind(failure);
2601     __ set(0, dst);
2602     __ ba_short(done);
2603     __ bind(success);
2604     __ set(1, dst);
2605     __ bind(done);
2606   } else {
2607     ShouldNotReachHere();
2608   }
2609 
2610 }
2611 
2612 
2613 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
2614   if (op-&gt;code() == lir_cas_long) {
2615     assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
2616     Register addr = op-&gt;addr()-&gt;as_pointer_register();
2617     Register cmp_value_lo = op-&gt;cmp_value()-&gt;as_register_lo();
2618     Register cmp_value_hi = op-&gt;cmp_value()-&gt;as_register_hi();
2619     Register new_value_lo = op-&gt;new_value()-&gt;as_register_lo();
2620     Register new_value_hi = op-&gt;new_value()-&gt;as_register_hi();
2621     Register t1 = op-&gt;tmp1()-&gt;as_register();
2622     Register t2 = op-&gt;tmp2()-&gt;as_register();
2623     __ mov(cmp_value_lo, t1);
2624     __ mov(new_value_lo, t2);
2625     // perform the compare and swap operation
2626     __ casx(addr, t1, t2);
2627     // generate condition code - if the swap succeeded, t2 (&quot;new value&quot; reg) was
2628     // overwritten with the original value in &quot;addr&quot; and will be equal to t1.
2629     __ cmp(t1, t2);
2630   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj) {
2631     Register addr = op-&gt;addr()-&gt;as_pointer_register();
2632     Register cmp_value = op-&gt;cmp_value()-&gt;as_register();
2633     Register new_value = op-&gt;new_value()-&gt;as_register();
2634     Register t1 = op-&gt;tmp1()-&gt;as_register();
2635     Register t2 = op-&gt;tmp2()-&gt;as_register();
2636     __ mov(cmp_value, t1);
2637     __ mov(new_value, t2);
2638     if (op-&gt;code() == lir_cas_obj) {
2639       if (UseCompressedOops) {
2640         __ encode_heap_oop(t1);
2641         __ encode_heap_oop(t2);
2642         __ cas(addr, t1, t2);
2643       } else {
2644         __ cas_ptr(addr, t1, t2);
2645       }
2646     } else {
2647       __ cas(addr, t1, t2);
2648     }
2649     __ cmp(t1, t2);
2650   } else {
2651     Unimplemented();
2652   }
2653 }
2654 
2655 void LIR_Assembler::breakpoint() {
2656   __ breakpoint_trap();
2657 }
2658 
2659 
2660 void LIR_Assembler::push(LIR_Opr opr) {
2661   Unimplemented();
2662 }
2663 
2664 
2665 void LIR_Assembler::pop(LIR_Opr opr) {
2666   Unimplemented();
2667 }
2668 
2669 
2670 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst_opr) {
2671   Address mon_addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
2672   Register dst = dst_opr-&gt;as_register();
2673   Register reg = mon_addr.base();
2674   int offset = mon_addr.disp();
2675   // compute pointer to BasicLock
2676   if (mon_addr.is_simm13()) {
2677     __ add(reg, offset, dst);
2678   } else {
2679     __ set(offset, dst);
2680     __ add(dst, reg, dst);
2681   }
2682 }
2683 
2684 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2685   assert(op-&gt;crc()-&gt;is_single_cpu(),  &quot;crc must be register&quot;);
2686   assert(op-&gt;val()-&gt;is_single_cpu(),  &quot;byte value must be register&quot;);
2687   assert(op-&gt;result_opr()-&gt;is_single_cpu(), &quot;result must be register&quot;);
2688   Register crc = op-&gt;crc()-&gt;as_register();
2689   Register val = op-&gt;val()-&gt;as_register();
2690   Register table = op-&gt;result_opr()-&gt;as_register();
2691   Register res   = op-&gt;result_opr()-&gt;as_register();
2692 
2693   assert_different_registers(val, crc, table);
2694 
2695   __ set(ExternalAddress(StubRoutines::crc_table_addr()), table);
2696   __ not1(crc);
2697   __ clruwu(crc);
2698   __ update_byte_crc32(crc, val, table);
2699   __ not1(crc);
2700 
2701   __ mov(crc, res);
2702 }
2703 
2704 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2705   Register obj = op-&gt;obj_opr()-&gt;as_register();
2706   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
2707   Register lock = op-&gt;lock_opr()-&gt;as_register();
2708 
2709   // obj may not be an oop
2710   if (op-&gt;code() == lir_lock) {
2711     MonitorEnterStub* stub = (MonitorEnterStub*)op-&gt;stub();
2712     if (UseFastLocking) {
2713       assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2714       // add debug info for NullPointerException only if one is possible
2715       if (op-&gt;info() != NULL) {
2716         add_debug_info_for_null_check_here(op-&gt;info());
2717       }
2718       __ lock_object(hdr, obj, lock, op-&gt;scratch_opr()-&gt;as_register(), *op-&gt;stub()-&gt;entry());
2719     } else {
2720       // always do slow locking
2721       // note: the slow locking code could be inlined here, however if we use
2722       //       slow locking, speed doesn&#39;t matter anyway and this solution is
2723       //       simpler and requires less duplicated code - additionally, the
2724       //       slow locking code is the same in either case which simplifies
2725       //       debugging
2726       __ br(Assembler::always, false, Assembler::pt, *op-&gt;stub()-&gt;entry());
2727       __ delayed()-&gt;nop();
2728     }
2729   } else {
2730     assert (op-&gt;code() == lir_unlock, &quot;Invalid code, expected lir_unlock&quot;);
2731     if (UseFastLocking) {
2732       assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2733       __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
2734     } else {
2735       // always do slow unlocking
2736       // note: the slow unlocking code could be inlined here, however if we use
2737       //       slow unlocking, speed doesn&#39;t matter anyway and this solution is
2738       //       simpler and requires less duplicated code - additionally, the
2739       //       slow unlocking code is the same in either case which simplifies
2740       //       debugging
2741       __ br(Assembler::always, false, Assembler::pt, *op-&gt;stub()-&gt;entry());
2742       __ delayed()-&gt;nop();
2743     }
2744   }
2745   __ bind(*op-&gt;stub()-&gt;continuation());
2746 }
2747 
2748 
2749 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2750   ciMethod* method = op-&gt;profiled_method();
2751   int bci          = op-&gt;profiled_bci();
2752   ciMethod* callee = op-&gt;profiled_callee();
2753 
2754   // Update counter for all call types
2755   ciMethodData* md = method-&gt;method_data_or_null();
2756   assert(md != NULL, &quot;Sanity&quot;);
2757   ciProfileData* data = md-&gt;bci_to_data(bci);
2758   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2759   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2760   Register mdo  = op-&gt;mdo()-&gt;as_register();
2761   assert(op-&gt;tmp1()-&gt;is_double_cpu(), &quot;tmp1 must be allocated&quot;);
2762   Register tmp1 = op-&gt;tmp1()-&gt;as_register_lo();
2763   metadata2reg(md-&gt;constant_encoding(), mdo);
2764   int mdo_offset_bias = 0;
2765   if (!Assembler::is_simm13(md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) +
2766                             data-&gt;size_in_bytes())) {
2767     // The offset is large so bias the mdo by the base of the slot so
2768     // that the ld can use simm13s to reference the slots of the data
2769     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, CounterData::count_offset());
2770     __ set(mdo_offset_bias, O7);
2771     __ add(mdo, O7, mdo);
2772   }
2773 
2774   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2775   // Perform additional virtual call profiling for invokevirtual and
2776   // invokeinterface bytecodes
2777   if (op-&gt;should_profile_receiver_type()) {
2778     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2779     Register recv = op-&gt;recv()-&gt;as_register();
2780     assert_different_registers(mdo, tmp1, recv);
2781     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2782     ciKlass* known_klass = op-&gt;known_holder();
2783     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2784       // We know the type that will be seen at this call site; we can
2785       // statically update the MethodData* rather than needing to do
2786       // dynamic tests on the receiver type
2787 
2788       // NOTE: we should probably put a lock around this search to
2789       // avoid collisions by concurrent compilations
2790       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2791       uint i;
2792       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2793         ciKlass* receiver = vc_data-&gt;receiver(i);
2794         if (known_klass-&gt;equals(receiver)) {
2795           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data,
2796                                                          VirtualCallData::receiver_count_offset(i)) -
2797                             mdo_offset_bias);
2798           __ ld_ptr(data_addr, tmp1);
2799           __ add(tmp1, DataLayout::counter_increment, tmp1);
2800           __ st_ptr(tmp1, data_addr);
2801           return;
2802         }
2803       }
2804 
2805       // Receiver type not found in profile data; select an empty slot
2806 
2807       // Note that this is less efficient than it should be because it
2808       // always does a write to the receiver part of the
2809       // VirtualCallData rather than just the first time
2810       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2811         ciKlass* receiver = vc_data-&gt;receiver(i);
2812         if (receiver == NULL) {
2813           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)) -
2814                             mdo_offset_bias);
2815           metadata2reg(known_klass-&gt;constant_encoding(), tmp1);
2816           __ st_ptr(tmp1, recv_addr);
2817           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)) -
2818                             mdo_offset_bias);
2819           __ ld_ptr(data_addr, tmp1);
2820           __ add(tmp1, DataLayout::counter_increment, tmp1);
2821           __ st_ptr(tmp1, data_addr);
2822           return;
2823         }
2824       }
2825     } else {
2826       __ load_klass(recv, recv);
2827       Label update_done;
2828       type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, &amp;update_done);
2829       // Receiver did not match any saved receiver and there is no empty row for it.
2830       // Increment total counter to indicate polymorphic case.
2831       __ ld_ptr(counter_addr, tmp1);
2832       __ add(tmp1, DataLayout::counter_increment, tmp1);
2833       __ st_ptr(tmp1, counter_addr);
2834 
2835       __ bind(update_done);
2836     }
2837   } else {
2838     // Static call
2839     __ ld_ptr(counter_addr, tmp1);
2840     __ add(tmp1, DataLayout::counter_increment, tmp1);
2841     __ st_ptr(tmp1, counter_addr);
2842   }
2843 }
2844 
2845 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2846   Register obj = op-&gt;obj()-&gt;as_register();
2847   Register tmp1 = op-&gt;tmp()-&gt;as_pointer_register();
2848   Register tmp2 = G1;
2849   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
2850   ciKlass* exact_klass = op-&gt;exact_klass();
2851   intptr_t current_klass = op-&gt;current_klass();
2852   bool not_null = op-&gt;not_null();
2853   bool no_conflict = op-&gt;no_conflict();
2854 
2855   Label update, next, none;
2856 
2857   bool do_null = !not_null;
2858   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
2859   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
2860 
2861   assert(do_null || do_update, &quot;why are we here?&quot;);
2862   assert(!TypeEntries::was_null_seen(current_klass) || do_update, &quot;why are we here?&quot;);
2863 
2864   __ verify_oop(obj);
2865 
2866   if (tmp1 != obj) {
2867     __ mov(obj, tmp1);
2868   }
2869   if (do_null) {
2870     __ br_notnull_short(tmp1, Assembler::pt, update);
2871     if (!TypeEntries::was_null_seen(current_klass)) {
2872       __ ld_ptr(mdo_addr, tmp1);
2873       __ or3(tmp1, TypeEntries::null_seen, tmp1);
2874       __ st_ptr(tmp1, mdo_addr);
2875     }
2876     if (do_update) {
2877       __ ba(next);
2878       __ delayed()-&gt;nop();
2879     }
2880 #ifdef ASSERT
2881   } else {
2882     __ br_notnull_short(tmp1, Assembler::pt, update);
2883     __ stop(&quot;unexpect null obj&quot;);
2884 #endif
2885   }
2886 
2887   __ bind(update);
2888 
2889   if (do_update) {
2890 #ifdef ASSERT
2891     if (exact_klass != NULL) {
2892       Label ok;
2893       __ load_klass(tmp1, tmp1);
2894       metadata2reg(exact_klass-&gt;constant_encoding(), tmp2);
2895       __ cmp_and_br_short(tmp1, tmp2, Assembler::equal, Assembler::pt, ok);
2896       __ stop(&quot;exact klass and actual klass differ&quot;);
2897       __ bind(ok);
2898     }
2899 #endif
2900 
2901     Label do_update;
2902     __ ld_ptr(mdo_addr, tmp2);
2903 
2904     if (!no_conflict) {
2905       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
2906         if (exact_klass != NULL) {
2907           metadata2reg(exact_klass-&gt;constant_encoding(), tmp1);
2908         } else {
2909           __ load_klass(tmp1, tmp1);
2910         }
2911 
2912         __ xor3(tmp1, tmp2, tmp1);
2913         __ btst(TypeEntries::type_klass_mask, tmp1);
2914         // klass seen before, nothing to do. The unknown bit may have been
2915         // set already but no need to check.
2916         __ brx(Assembler::zero, false, Assembler::pt, next);
2917         __ delayed()-&gt;
2918 
2919            btst(TypeEntries::type_unknown, tmp1);
2920         // already unknown. Nothing to do anymore.
2921         __ brx(Assembler::notZero, false, Assembler::pt, next);
2922 
2923         if (TypeEntries::is_type_none(current_klass)) {
2924           __ delayed()-&gt;btst(TypeEntries::type_mask, tmp2);
2925           __ brx(Assembler::zero, true, Assembler::pt, do_update);
2926           // first time here. Set profile type.
2927           __ delayed()-&gt;or3(tmp2, tmp1, tmp2);
2928         } else {
2929           __ delayed()-&gt;nop();
2930         }
2931       } else {
2932         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
2933                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;conflict only&quot;);
2934 
2935         __ btst(TypeEntries::type_unknown, tmp2);
2936         // already unknown. Nothing to do anymore.
2937         __ brx(Assembler::notZero, false, Assembler::pt, next);
2938         __ delayed()-&gt;nop();
2939       }
2940 
2941       // different than before. Cannot keep accurate profile.
2942       __ or3(tmp2, TypeEntries::type_unknown, tmp2);
2943     } else {
2944       // There&#39;s a single possible klass at this profile point
2945       assert(exact_klass != NULL, &quot;should be&quot;);
2946       if (TypeEntries::is_type_none(current_klass)) {
2947         metadata2reg(exact_klass-&gt;constant_encoding(), tmp1);
2948         __ xor3(tmp1, tmp2, tmp1);
2949         __ btst(TypeEntries::type_klass_mask, tmp1);
2950         __ brx(Assembler::zero, false, Assembler::pt, next);
2951 #ifdef ASSERT
2952 
2953         {
2954           Label ok;
2955           __ delayed()-&gt;btst(TypeEntries::type_mask, tmp2);
2956           __ brx(Assembler::zero, true, Assembler::pt, ok);
2957           __ delayed()-&gt;nop();
2958 
2959           __ stop(&quot;unexpected profiling mismatch&quot;);
2960           __ bind(ok);
2961         }
2962         // first time here. Set profile type.
2963         __ or3(tmp2, tmp1, tmp2);
2964 #else
2965         // first time here. Set profile type.
2966         __ delayed()-&gt;or3(tmp2, tmp1, tmp2);
2967 #endif
2968 
2969       } else {
2970         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
2971                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;inconsistent&quot;);
2972 
2973         // already unknown. Nothing to do anymore.
2974         __ btst(TypeEntries::type_unknown, tmp2);
2975         __ brx(Assembler::notZero, false, Assembler::pt, next);
2976         __ delayed()-&gt;or3(tmp2, TypeEntries::type_unknown, tmp2);
2977       }
2978     }
2979 
2980     __ bind(do_update);
2981     __ st_ptr(tmp2, mdo_addr);
2982 
2983     __ bind(next);
2984   }
2985 }
2986 
2987 void LIR_Assembler::align_backward_branch_target() {
2988   __ align(OptoLoopAlignment);
2989 }
2990 
2991 
2992 void LIR_Assembler::emit_delay(LIR_OpDelay* op) {
2993   // make sure we are expecting a delay
2994   // this has the side effect of clearing the delay state
2995   // so we can use _masm instead of _masm-&gt;delayed() to do the
2996   // code generation.
2997   __ delayed();
2998 
2999   // make sure we only emit one instruction
3000   int offset = code_offset();
3001   op-&gt;delay_op()-&gt;emit_code(this);
3002 #ifdef ASSERT
3003   if (code_offset() - offset != NativeInstruction::nop_instruction_size) {
3004     op-&gt;delay_op()-&gt;print();
3005   }
3006   assert(code_offset() - offset == NativeInstruction::nop_instruction_size,
3007          &quot;only one instruction can go in a delay slot&quot;);
3008 #endif
3009 
3010   // we may also be emitting the call info for the instruction
3011   // which we are the delay slot of.
3012   CodeEmitInfo* call_info = op-&gt;call_info();
3013   if (call_info) {
3014     add_call_info(code_offset(), call_info);
3015   }
3016 
3017   if (VerifyStackAtCalls) {
3018     _masm-&gt;sub(FP, SP, O7);
3019     _masm-&gt;cmp(O7, initial_frame_size_in_bytes());
3020     _masm-&gt;trap(Assembler::notEqual, Assembler::ptr_cc, G0, ST_RESERVED_FOR_USER_0+2 );
3021   }
3022 }
3023 
3024 
3025 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
3026   // tmp must be unused
3027   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
3028   assert(left-&gt;is_register(), &quot;can only handle registers&quot;);
3029 
3030   if (left-&gt;is_single_cpu()) {
3031     __ neg(left-&gt;as_register(), dest-&gt;as_register());
3032   } else if (left-&gt;is_single_fpu()) {
3033     __ fneg(FloatRegisterImpl::S, left-&gt;as_float_reg(), dest-&gt;as_float_reg());
3034   } else if (left-&gt;is_double_fpu()) {
3035     __ fneg(FloatRegisterImpl::D, left-&gt;as_double_reg(), dest-&gt;as_double_reg());
3036   } else {
3037     assert (left-&gt;is_double_cpu(), &quot;Must be a long&quot;);
3038     Register Rlow = left-&gt;as_register_lo();
3039     Register Rhi = left-&gt;as_register_hi();
3040     __ sub(G0, Rlow, dest-&gt;as_register_lo());
3041   }
3042 }
3043 
3044 void LIR_Assembler::rt_call(LIR_Opr result, address dest,
3045                             const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
3046 
3047   // if tmp is invalid, then the function being called doesn&#39;t destroy the thread
3048   if (tmp-&gt;is_valid()) {
3049     __ save_thread(tmp-&gt;as_pointer_register());
3050   }
3051   __ call(dest, relocInfo::runtime_call_type);
3052   __ delayed()-&gt;nop();
3053   if (info != NULL) {
3054     add_call_info_here(info);
3055   }
3056   if (tmp-&gt;is_valid()) {
3057     __ restore_thread(tmp-&gt;as_pointer_register());
3058   }
3059 
3060 #ifdef ASSERT
3061   __ verify_thread();
3062 #endif // ASSERT
3063 }
3064 
3065 
3066 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
3067   ShouldNotReachHere();
3068 
3069   NEEDS_CLEANUP;
3070   if (type == T_LONG) {
3071     LIR_Address* mem_addr = dest-&gt;is_address() ? dest-&gt;as_address_ptr() : src-&gt;as_address_ptr();
3072 
3073     // (extended to allow indexed as well as constant displaced for JSR-166)
3074     Register idx = noreg; // contains either constant offset or index
3075 
3076     int disp = mem_addr-&gt;disp();
3077     if (mem_addr-&gt;index() == LIR_OprFact::illegalOpr) {
3078       if (!Assembler::is_simm13(disp)) {
3079         idx = O7;
3080         __ set(disp, idx);
3081       }
3082     } else {
3083       assert(disp == 0, &quot;not both indexed and disp&quot;);
3084       idx = mem_addr-&gt;index()-&gt;as_register();
3085     }
3086 
3087     int null_check_offset = -1;
3088 
3089     Register base = mem_addr-&gt;base()-&gt;as_register();
3090     if (src-&gt;is_register() &amp;&amp; dest-&gt;is_address()) {
3091       // G4 is high half, G5 is low half
3092       // clear the top bits of G5, and scale up G4
3093       __ srl (src-&gt;as_register_lo(),  0, G5);
3094       __ sllx(src-&gt;as_register_hi(), 32, G4);
3095       // combine the two halves into the 64 bits of G4
3096       __ or3(G4, G5, G4);
3097       null_check_offset = __ offset();
3098       if (idx == noreg) {
3099         __ stx(G4, base, disp);
3100       } else {
3101         __ stx(G4, base, idx);
3102       }
3103     } else if (src-&gt;is_address() &amp;&amp; dest-&gt;is_register()) {
3104       null_check_offset = __ offset();
3105       if (idx == noreg) {
3106         __ ldx(base, disp, G5);
3107       } else {
3108         __ ldx(base, idx, G5);
3109       }
3110       __ srax(G5, 32, dest-&gt;as_register_hi()); // fetch the high half into hi
3111       __ mov (G5, dest-&gt;as_register_lo());     // copy low half into lo
3112     } else {
3113       Unimplemented();
3114     }
3115     if (info != NULL) {
3116       add_debug_info_for_null_check(null_check_offset, info);
3117     }
3118 
3119   } else {
3120     // use normal move for all other volatiles since they don&#39;t need
3121     // special handling to remain atomic.
3122     move_op(src, dest, type, lir_patch_none, info, false, false, false);
3123   }
3124 }
3125 
3126 void LIR_Assembler::membar() {
3127   // only StoreLoad membars are ever explicitly needed on sparcs in TSO mode
3128   __ membar( Assembler::Membar_mask_bits(Assembler::StoreLoad) );
3129 }
3130 
3131 void LIR_Assembler::membar_acquire() {
3132   // no-op on TSO
3133 }
3134 
3135 void LIR_Assembler::membar_release() {
3136   // no-op on TSO
3137 }
3138 
3139 void LIR_Assembler::membar_loadload() {
3140   // no-op
3141   //__ membar(Assembler::Membar_mask_bits(Assembler::loadload));
3142 }
3143 
3144 void LIR_Assembler::membar_storestore() {
3145   // no-op
3146   //__ membar(Assembler::Membar_mask_bits(Assembler::storestore));
3147 }
3148 
3149 void LIR_Assembler::membar_loadstore() {
3150   // no-op
3151   //__ membar(Assembler::Membar_mask_bits(Assembler::loadstore));
3152 }
3153 
3154 void LIR_Assembler::membar_storeload() {
3155   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
3156 }
3157 
3158 void LIR_Assembler::on_spin_wait() {
3159   Unimplemented();
3160 }
3161 
3162 // Pack two sequential registers containing 32 bit values
3163 // into a single 64 bit register.
3164 // src and src-&gt;successor() are packed into dst
3165 // src and dst may be the same register.
3166 // Note: src is destroyed
3167 void LIR_Assembler::pack64(LIR_Opr src, LIR_Opr dst) {
3168   Register rs = src-&gt;as_register();
3169   Register rd = dst-&gt;as_register_lo();
3170   __ sllx(rs, 32, rs);
3171   __ srl(rs-&gt;successor(), 0, rs-&gt;successor());
3172   __ or3(rs, rs-&gt;successor(), rd);
3173 }
3174 
3175 // Unpack a 64 bit value in a register into
3176 // two sequential registers.
3177 // src is unpacked into dst and dst-&gt;successor()
3178 void LIR_Assembler::unpack64(LIR_Opr src, LIR_Opr dst) {
3179   Register rs = src-&gt;as_register_lo();
3180   Register rd = dst-&gt;as_register_hi();
3181   assert_different_registers(rs, rd, rd-&gt;successor());
3182   __ srlx(rs, 32, rd);
3183   __ srl (rs,  0, rd-&gt;successor());
3184 }
3185 
3186 void LIR_Assembler::leal(LIR_Opr addr_opr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
3187   const LIR_Address* addr = addr_opr-&gt;as_address_ptr();
3188   assert(addr-&gt;scale() == LIR_Address::times_1, &quot;can&#39;t handle complex addresses yet&quot;);
3189   const Register dest_reg = dest-&gt;as_pointer_register();
3190   const Register base_reg = addr-&gt;base()-&gt;as_pointer_register();
3191 
3192   if (patch_code != lir_patch_none) {
3193     PatchingStub* patch = new PatchingStub(_masm, PatchingStub::access_field_id);
3194     assert(addr-&gt;disp() != 0, &quot;must have&quot;);
3195     assert(base_reg != G3_scratch, &quot;invariant&quot;);
3196     __ patchable_set(0, G3_scratch);
3197     patching_epilog(patch, patch_code, base_reg, info);
3198     assert(dest_reg != G3_scratch, &quot;invariant&quot;);
3199     if (addr-&gt;index()-&gt;is_valid()) {
3200       const Register index_reg = addr-&gt;index()-&gt;as_pointer_register();
3201       assert(index_reg != G3_scratch, &quot;invariant&quot;);
3202       __ add(index_reg, G3_scratch, G3_scratch);
3203     }
3204     __ add(base_reg, G3_scratch, dest_reg);
3205   } else {
3206     if (Assembler::is_simm13(addr-&gt;disp())) {
3207       if (addr-&gt;index()-&gt;is_valid()) {
3208         const Register index_reg = addr-&gt;index()-&gt;as_pointer_register();
3209         assert(index_reg != G3_scratch, &quot;invariant&quot;);
3210         __ add(base_reg, addr-&gt;disp(), G3_scratch);
3211         __ add(index_reg, G3_scratch, dest_reg);
3212       } else {
3213         __ add(base_reg, addr-&gt;disp(), dest_reg);
3214       }
3215     } else {
3216       __ set(addr-&gt;disp(), G3_scratch);
3217       if (addr-&gt;index()-&gt;is_valid()) {
3218         const Register index_reg = addr-&gt;index()-&gt;as_pointer_register();
3219         assert(index_reg != G3_scratch, &quot;invariant&quot;);
3220         __ add(index_reg, G3_scratch, G3_scratch);
3221       }
3222       __ add(base_reg, G3_scratch, dest_reg);
3223     }
3224   }
3225 }
3226 
3227 
3228 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
3229   assert(result_reg-&gt;is_register(), &quot;check&quot;);
3230   __ mov(G2_thread, result_reg-&gt;as_register());
3231 }
3232 
3233 #ifdef ASSERT
3234 // emit run-time assertion
3235 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
3236   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
3237 
3238   if (op-&gt;in_opr1()-&gt;is_valid()) {
3239     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
3240     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
3241   } else {
3242     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
3243     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
3244   }
3245 
3246   Label ok;
3247   if (op-&gt;condition() != lir_cond_always) {
3248     Assembler::Condition acond;
3249     switch (op-&gt;condition()) {
3250       case lir_cond_equal:        acond = Assembler::equal;                break;
3251       case lir_cond_notEqual:     acond = Assembler::notEqual;             break;
3252       case lir_cond_less:         acond = Assembler::less;                 break;
3253       case lir_cond_lessEqual:    acond = Assembler::lessEqual;            break;
3254       case lir_cond_greaterEqual: acond = Assembler::greaterEqual;         break;
3255       case lir_cond_greater:      acond = Assembler::greater;              break;
3256       case lir_cond_aboveEqual:   acond = Assembler::greaterEqualUnsigned; break;
3257       case lir_cond_belowEqual:   acond = Assembler::lessEqualUnsigned;    break;
3258       default:                         ShouldNotReachHere();
3259     };
3260     __ br(acond, false, Assembler::pt, ok);
3261     __ delayed()-&gt;nop();
3262   }
3263   if (op-&gt;halt()) {
3264     const char* str = __ code_string(op-&gt;msg());
3265     __ stop(str);
3266   } else {
3267     breakpoint();
3268   }
3269   __ bind(ok);
3270 }
3271 #endif
3272 
3273 void LIR_Assembler::peephole(LIR_List* lir) {
3274   LIR_OpList* inst = lir-&gt;instructions_list();
3275   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3276     LIR_Op* op = inst-&gt;at(i);
3277     switch (op-&gt;code()) {
3278       case lir_cond_float_branch:
3279       case lir_branch: {
3280         LIR_OpBranch* branch = op-&gt;as_OpBranch();
3281         assert(branch-&gt;info() == NULL, &quot;shouldn&#39;t be state on branches anymore&quot;);
3282         LIR_Op* delay_op = NULL;
3283         // we&#39;d like to be able to pull following instructions into
3284         // this slot but we don&#39;t know enough to do it safely yet so
3285         // only optimize block to block control flow.
3286         if (LIRFillDelaySlots &amp;&amp; branch-&gt;block()) {
3287           LIR_Op* prev = inst-&gt;at(i - 1);
3288           if (prev &amp;&amp; LIR_Assembler::is_single_instruction(prev) &amp;&amp; prev-&gt;info() == NULL) {
3289             // swap previous instruction into delay slot
3290             inst-&gt;at_put(i - 1, op);
3291             inst-&gt;at_put(i, new LIR_OpDelay(prev, op-&gt;info()));
3292 #ifndef PRODUCT
3293             if (LIRTracePeephole) {
3294               tty-&gt;print_cr(&quot;delayed&quot;);
3295               inst-&gt;at(i - 1)-&gt;print();
3296               inst-&gt;at(i)-&gt;print();
3297               tty-&gt;cr();
3298             }
3299 #endif
3300             continue;
3301           }
3302         }
3303 
3304         if (!delay_op) {
3305           delay_op = new LIR_OpDelay(new LIR_Op0(lir_nop), NULL);
3306         }
3307         inst-&gt;insert_before(i + 1, delay_op);
3308         break;
3309       }
3310       case lir_static_call:
3311       case lir_virtual_call:
3312       case lir_icvirtual_call:
3313       case lir_optvirtual_call:
3314       case lir_dynamic_call: {
3315         LIR_Op* prev = inst-&gt;at(i - 1);
3316         if (LIRFillDelaySlots &amp;&amp; prev &amp;&amp; prev-&gt;code() == lir_move &amp;&amp; prev-&gt;info() == NULL &amp;&amp;
3317             (op-&gt;code() != lir_virtual_call ||
3318              !prev-&gt;result_opr()-&gt;is_single_cpu() ||
3319              prev-&gt;result_opr()-&gt;as_register() != O0) &amp;&amp;
3320             LIR_Assembler::is_single_instruction(prev)) {
3321           // Only moves without info can be put into the delay slot.
3322           // Also don&#39;t allow the setup of the receiver in the delay
3323           // slot for vtable calls.
3324           inst-&gt;at_put(i - 1, op);
3325           inst-&gt;at_put(i, new LIR_OpDelay(prev, op-&gt;info()));
3326 #ifndef PRODUCT
3327           if (LIRTracePeephole) {
3328             tty-&gt;print_cr(&quot;delayed&quot;);
3329             inst-&gt;at(i - 1)-&gt;print();
3330             inst-&gt;at(i)-&gt;print();
3331             tty-&gt;cr();
3332           }
3333 #endif
3334         } else {
3335           LIR_Op* delay_op = new LIR_OpDelay(new LIR_Op0(lir_nop), op-&gt;as_OpJavaCall()-&gt;info());
3336           inst-&gt;insert_before(i + 1, delay_op);
3337           i++;
3338         }
3339         break;
3340       }
3341     }
3342   }
3343 }
3344 
3345 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
3346   LIR_Address* addr = src-&gt;as_address_ptr();
3347 
3348   assert(data == dest, &quot;swap uses only 2 operands&quot;);
3349   assert (code == lir_xchg, &quot;no xadd on sparc&quot;);
3350 
3351   if (data-&gt;type() == T_INT) {
3352     __ swap(as_Address(addr), data-&gt;as_register());
3353   } else if (data-&gt;is_oop()) {
3354     Register obj = data-&gt;as_register();
3355     Register narrow = tmp-&gt;as_register();
3356     assert(UseCompressedOops, &quot;swap is 32bit only&quot;);
3357     __ encode_heap_oop(obj, narrow);
3358     __ swap(as_Address(addr), narrow);
3359     __ decode_heap_oop(narrow, obj);
3360   } else {
3361     ShouldNotReachHere();
3362   }
3363 }
3364 
3365 #undef __
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>