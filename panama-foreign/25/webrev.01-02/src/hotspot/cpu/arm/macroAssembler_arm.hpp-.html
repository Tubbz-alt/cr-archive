<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/arm/macroAssembler_arm.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef CPU_ARM_MACROASSEMBLER_ARM_HPP
  26 #define CPU_ARM_MACROASSEMBLER_ARM_HPP
  27 
  28 #include &quot;code/relocInfo.hpp&quot;
  29 
  30 class BiasedLockingCounters;
  31 
  32 // Introduced AddressLiteral and its subclasses to ease portability from
  33 // x86 and avoid relocation issues
  34 class AddressLiteral {
  35   RelocationHolder _rspec;
  36   // Typically we use AddressLiterals we want to use their rval
  37   // However in some situations we want the lval (effect address) of the item.
  38   // We provide a special factory for making those lvals.
  39   bool _is_lval;
  40 
  41   address          _target;
  42 
  43  private:
  44   static relocInfo::relocType reloc_for_target(address target) {
  45     // Used for ExternalAddress or when the type is not specified
  46     // Sometimes ExternalAddress is used for values which aren&#39;t
  47     // exactly addresses, like the card table base.
  48     // external_word_type can&#39;t be used for values in the first page
  49     // so just skip the reloc in that case.
  50     return external_word_Relocation::can_be_relocated(target) ? relocInfo::external_word_type : relocInfo::none;
  51   }
  52 
  53   void set_rspec(relocInfo::relocType rtype);
  54 
  55  protected:
  56   // creation
  57   AddressLiteral()
  58     : _is_lval(false),
  59       _target(NULL)
  60   {}
  61 
  62   public:
  63 
  64   AddressLiteral(address target, relocInfo::relocType rtype) {
  65     _is_lval = false;
  66     _target = target;
  67     set_rspec(rtype);
  68   }
  69 
  70   AddressLiteral(address target, RelocationHolder const&amp; rspec)
  71     : _rspec(rspec),
  72       _is_lval(false),
  73       _target(target)
  74   {}
  75 
  76   AddressLiteral(address target) {
  77     _is_lval = false;
  78     _target = target;
  79     set_rspec(reloc_for_target(target));
  80   }
  81 
  82   AddressLiteral addr() {
  83     AddressLiteral ret = *this;
  84     ret._is_lval = true;
  85     return ret;
  86   }
  87 
  88  private:
  89 
  90   address target() { return _target; }
  91   bool is_lval() { return _is_lval; }
  92 
  93   relocInfo::relocType reloc() const { return _rspec.type(); }
  94   const RelocationHolder&amp; rspec() const { return _rspec; }
  95 
  96   friend class Assembler;
  97   friend class MacroAssembler;
  98   friend class Address;
  99   friend class LIR_Assembler;
 100   friend class InlinedAddress;
 101 };
 102 
 103 class ExternalAddress: public AddressLiteral {
 104 
 105   public:
 106 
 107   ExternalAddress(address target) : AddressLiteral(target) {}
 108 
 109 };
 110 
 111 class InternalAddress: public AddressLiteral {
 112 
 113   public:
 114 
 115   InternalAddress(address target) : AddressLiteral(target, relocInfo::internal_word_type) {}
 116 
 117 };
 118 
 119 // Inlined constants, for use with ldr_literal / bind_literal
 120 // Note: InlinedInteger not supported (use move_slow(Register,int[,cond]))
 121 class InlinedLiteral: StackObj {
 122  public:
 123   Label label; // need to be public for direct access with &amp;
 124   InlinedLiteral() {
 125   }
 126 };
 127 
 128 class InlinedMetadata: public InlinedLiteral {
 129  private:
 130   Metadata *_data;
 131 
 132  public:
 133   InlinedMetadata(Metadata *data): InlinedLiteral() {
 134     _data = data;
 135   }
 136   Metadata *data() { return _data; }
 137 };
 138 
 139 // Currently unused
 140 // class InlinedOop: public InlinedLiteral {
 141 //  private:
 142 //   jobject _jobject;
 143 //
 144 //  public:
 145 //   InlinedOop(jobject target): InlinedLiteral() {
 146 //     _jobject = target;
 147 //   }
 148 //   jobject jobject() { return _jobject; }
 149 // };
 150 
 151 class InlinedAddress: public InlinedLiteral {
 152  private:
 153   AddressLiteral _literal;
 154 
 155  public:
 156 
 157   InlinedAddress(jobject object): InlinedLiteral(), _literal((address)object, relocInfo::oop_type) {
 158     ShouldNotReachHere(); // use mov_oop (or implement InlinedOop)
 159   }
 160 
 161   InlinedAddress(Metadata *data): InlinedLiteral(), _literal((address)data, relocInfo::metadata_type) {
 162     ShouldNotReachHere(); // use InlinedMetadata or mov_metadata
 163   }
 164 
 165   InlinedAddress(address target, const RelocationHolder &amp;rspec): InlinedLiteral(), _literal(target, rspec) {
 166     assert(rspec.type() != relocInfo::oop_type, &quot;Do not use InlinedAddress for oops&quot;);
 167     assert(rspec.type() != relocInfo::metadata_type, &quot;Do not use InlinedAddress for metadatas&quot;);
 168   }
 169 
 170   InlinedAddress(address target, relocInfo::relocType rtype): InlinedLiteral(), _literal(target, rtype) {
 171     assert(rtype != relocInfo::oop_type, &quot;Do not use InlinedAddress for oops&quot;);
 172     assert(rtype != relocInfo::metadata_type, &quot;Do not use InlinedAddress for metadatas&quot;);
 173   }
 174 
 175   // Note: default is relocInfo::none for InlinedAddress
 176   InlinedAddress(address target): InlinedLiteral(), _literal(target, relocInfo::none) {
 177   }
 178 
 179   address target() { return _literal.target(); }
 180 
 181   const RelocationHolder&amp; rspec() const { return _literal.rspec(); }
 182 };
 183 
 184 class InlinedString: public InlinedLiteral {
 185  private:
 186   const char* _msg;
 187 
 188  public:
 189   InlinedString(const char* msg): InlinedLiteral() {
 190     _msg = msg;
 191   }
 192   const char* msg() { return _msg; }
 193 };
 194 
 195 class MacroAssembler: public Assembler {
 196 protected:
 197 
 198   // Support for VM calls
 199   //
 200 
 201   // This is the base routine called by the different versions of call_VM_leaf.
 202   void call_VM_leaf_helper(address entry_point, int number_of_arguments);
 203 
 204   // This is the base routine called by the different versions of call_VM. The interpreter
 205   // may customize this version by overriding it for its purposes (e.g., to save/restore
 206   // additional registers when doing a VM call).
 207   virtual void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions);
 208 public:
 209 
 210   MacroAssembler(CodeBuffer* code) : Assembler(code) {}
 211 
 212   // These routines should emit JVMTI PopFrame and ForceEarlyReturn handling code.
 213   // The implementation is only non-empty for the InterpreterMacroAssembler,
 214   // as only the interpreter handles PopFrame and ForceEarlyReturn requests.
 215   virtual void check_and_handle_popframe() {}
 216   virtual void check_and_handle_earlyret() {}
 217 
 218   // By default, we do not need relocation information for non
 219   // patchable absolute addresses. However, when needed by some
 220   // extensions, ignore_non_patchable_relocations can be modified,
 221   // returning false to preserve all relocation information.
 222   inline bool ignore_non_patchable_relocations() { return true; }
 223 
 224   // Initially added to the Assembler interface as a pure virtual:
 225   //   RegisterConstant delayed_value(..)
 226   // for:
 227   //   6812678 macro assembler needs delayed binding of a few constants (for 6655638)
 228   // this was subsequently modified to its present name and return type
 229   virtual RegisterOrConstant delayed_value_impl(intptr_t* delayed_value_addr, Register tmp, int offset);
 230 
 231 
 232   void align(int modulus);
 233 
 234   // Support for VM calls
 235   //
 236   // It is imperative that all calls into the VM are handled via the call_VM methods.
 237   // They make sure that the stack linkage is setup correctly. call_VM&#39;s correspond
 238   // to ENTRY/ENTRY_X entry points while call_VM_leaf&#39;s correspond to LEAF entry points.
 239 
 240   void call_VM(Register oop_result, address entry_point, bool check_exceptions = true);
 241   void call_VM(Register oop_result, address entry_point, Register arg_1, bool check_exceptions = true);
 242   void call_VM(Register oop_result, address entry_point, Register arg_1, Register arg_2, bool check_exceptions = true);
 243   void call_VM(Register oop_result, address entry_point, Register arg_1, Register arg_2, Register arg_3, bool check_exceptions = true);
 244 
 245   // The following methods are required by templateTable.cpp,
 246   // but not used on ARM.
 247   void call_VM(Register oop_result, Register last_java_sp, address entry_point, int number_of_arguments = 0, bool check_exceptions = true);
 248   void call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, bool check_exceptions = true);
 249   void call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, Register arg_2, bool check_exceptions = true);
 250   void call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, Register arg_2, Register arg_3, bool check_exceptions = true);
 251 
 252   // Note: The super_call_VM calls are not used on ARM
 253 
 254   // Raw call, without saving/restoring registers, exception handling, etc.
 255   // Mainly used from various stubs.
 256   // Note: if &#39;save_R9_if_scratched&#39; is true, call_VM may on some
 257   // platforms save values on the stack. Set it to false (and handle
 258   // R9 in the callers) if the top of the stack must not be modified
 259   // by call_VM.
 260   void call_VM(address entry_point, bool save_R9_if_scratched);
 261 
 262   void call_VM_leaf(address entry_point);
 263   void call_VM_leaf(address entry_point, Register arg_1);
 264   void call_VM_leaf(address entry_point, Register arg_1, Register arg_2);
 265   void call_VM_leaf(address entry_point, Register arg_1, Register arg_2, Register arg_3);
 266   void call_VM_leaf(address entry_point, Register arg_1, Register arg_2, Register arg_3, Register arg_4);
 267 
 268   void get_vm_result(Register oop_result, Register tmp);
 269   void get_vm_result_2(Register metadata_result, Register tmp);
 270 
 271   // Always sets/resets sp, which default to SP if (last_sp == noreg)
 272   // Optionally sets/resets fp (use noreg to avoid setting it)
 273   // Optionally sets/resets pc depending on save_last_java_pc flag
 274   // Note: when saving PC, set_last_Java_frame returns PC&#39;s offset in the code section
 275   //       (for oop_maps offset computation)
 276   int set_last_Java_frame(Register last_sp, Register last_fp, bool save_last_java_pc, Register tmp);
 277   void reset_last_Java_frame(Register tmp);
 278   // status set in set_last_Java_frame for reset_last_Java_frame
 279   bool _fp_saved;
 280   bool _pc_saved;
 281 
 282 #ifdef PRODUCT
 283 #define BLOCK_COMMENT(str) /* nothing */
 284 #define STOP(error) __ stop(error)
 285 #else
 286 #define BLOCK_COMMENT(str) __ block_comment(str)
 287 #define STOP(error) __ block_comment(error); __ stop(error)
 288 #endif
 289 
 290   void lookup_virtual_method(Register recv_klass,
 291                              Register vtable_index,
 292                              Register method_result);
 293 
 294   // Test sub_klass against super_klass, with fast and slow paths.
 295 
 296   // The fast path produces a tri-state answer: yes / no / maybe-slow.
 297   // One of the three labels can be NULL, meaning take the fall-through.
 298   // No registers are killed, except temp_regs.
 299   void check_klass_subtype_fast_path(Register sub_klass,
 300                                      Register super_klass,
 301                                      Register temp_reg,
 302                                      Register temp_reg2,
 303                                      Label* L_success,
 304                                      Label* L_failure,
 305                                      Label* L_slow_path);
 306 
 307   // The rest of the type check; must be wired to a corresponding fast path.
 308   // It does not repeat the fast path logic, so don&#39;t use it standalone.
 309   // temp_reg3 can be noreg, if no temps are available.
 310   // Updates the sub&#39;s secondary super cache as necessary.
 311   // If set_cond_codes:
 312   // - condition codes will be Z on success, NZ on failure.
 313   // - temp_reg will be 0 on success, non-0 on failure
 314   void check_klass_subtype_slow_path(Register sub_klass,
 315                                      Register super_klass,
 316                                      Register temp_reg,
 317                                      Register temp_reg2,
 318                                      Register temp_reg3, // auto assigned if noreg
 319                                      Label* L_success,
 320                                      Label* L_failure,
 321                                      bool set_cond_codes = false);
 322 
 323   // Simplified, combined version, good for typical uses.
 324   // temp_reg3 can be noreg, if no temps are available. It is used only on slow path.
 325   // Falls through on failure.
 326   void check_klass_subtype(Register sub_klass,
 327                            Register super_klass,
 328                            Register temp_reg,
 329                            Register temp_reg2,
 330                            Register temp_reg3, // auto assigned on slow path if noreg
 331                            Label&amp; L_success);
 332 
 333   // Returns address of receiver parameter, using tmp as base register. tmp and params_count can be the same.
 334   Address receiver_argument_address(Register params_base, Register params_count, Register tmp);
 335 
 336   void _verify_oop(Register reg, const char* s, const char* file, int line);
 337   void _verify_oop_addr(Address addr, const char * s, const char* file, int line);
 338 
 339   // TODO: verify method and klass metadata (compare against vptr?)
 340   void _verify_method_ptr(Register reg, const char * msg, const char * file, int line) {}
 341   void _verify_klass_ptr(Register reg, const char * msg, const char * file, int line) {}
 342 
 343 #define verify_oop(reg) _verify_oop(reg, &quot;broken oop &quot; #reg, __FILE__, __LINE__)
 344 #define verify_oop_addr(addr) _verify_oop_addr(addr, &quot;broken oop &quot;, __FILE__, __LINE__)
 345 #define verify_method_ptr(reg) _verify_method_ptr(reg, &quot;broken method &quot; #reg, __FILE__, __LINE__)
 346 #define verify_klass_ptr(reg) _verify_klass_ptr(reg, &quot;broken klass &quot; #reg, __FILE__, __LINE__)
 347 
 348   void null_check(Register reg, Register tmp, int offset = -1);
 349   inline void null_check(Register reg) { null_check(reg, noreg, -1); } // for C1 lir_null_check
 350 
 351   // Puts address of allocated object into register `obj` and end of allocated object into register `obj_end`.
 352   void eden_allocate(Register obj, Register obj_end, Register tmp1, Register tmp2,
 353                      RegisterOrConstant size_expression, Label&amp; slow_case);
 354   void tlab_allocate(Register obj, Register obj_end, Register tmp1,
 355                      RegisterOrConstant size_expression, Label&amp; slow_case);
 356 
 357   void zero_memory(Register start, Register end, Register tmp);
 358 
 359   static bool needs_explicit_null_check(intptr_t offset);
 360   static bool uses_implicit_null_check(void* address);
 361 
 362   void arm_stack_overflow_check(int frame_size_in_bytes, Register tmp);
 363   void arm_stack_overflow_check(Register Rsize, Register tmp);
 364 
 365   void bang_stack_with_offset(int offset) {
 366     ShouldNotReachHere();
 367   }
 368 
 369   // Biased locking support
 370   // lock_reg and obj_reg must be loaded up with the appropriate values.
 371   // swap_reg must be supplied.
 372   // tmp_reg must be supplied.
 373   // Done label is branched to with condition code EQ set if the lock is
 374   // biased and we acquired it. Slow case label is branched to with
 375   // condition code NE set if the lock is biased but we failed to acquire
 376   // it. Otherwise fall through.
 377   // Returns offset of first potentially-faulting instruction for null
 378   // check info (currently consumed only by C1). If
 379   // swap_reg_contains_mark is true then returns -1 as it is assumed
 380   // the calling code has already passed any potential faults.
 381   // Notes:
 382   // - swap_reg and tmp_reg are scratched
 383   // - Rtemp was (implicitly) scratched and can now be specified as the tmp2
 384   int biased_locking_enter(Register obj_reg, Register swap_reg, Register tmp_reg,
 385                            bool swap_reg_contains_mark,
 386                            Register tmp2,
 387                            Label&amp; done, Label&amp; slow_case,
 388                            BiasedLockingCounters* counters = NULL);
 389   void biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done);
 390 
 391   // Building block for CAS cases of biased locking: makes CAS and records statistics.
 392   // Optional slow_case label is used to transfer control if CAS fails. Otherwise leaves condition codes set.
 393   void biased_locking_enter_with_cas(Register obj_reg, Register old_mark_reg, Register new_mark_reg,
 394                                      Register tmp, Label&amp; slow_case, int* counter_addr);
 395 
 396   void resolve_jobject(Register value, Register tmp1, Register tmp2);
 397 
 398   void nop() {
 399     mov(R0, R0);
 400   }
 401 
 402   void push(Register rd, AsmCondition cond = al) {
 403     assert(rd != SP, &quot;unpredictable instruction&quot;);
 404     str(rd, Address(SP, -wordSize, pre_indexed), cond);
 405   }
 406 
 407   void push(RegisterSet reg_set, AsmCondition cond = al) {
 408     assert(!reg_set.contains(SP), &quot;unpredictable instruction&quot;);
 409     stmdb(SP, reg_set, writeback, cond);
 410   }
 411 
 412   void pop(Register rd, AsmCondition cond = al) {
 413     assert(rd != SP, &quot;unpredictable instruction&quot;);
 414     ldr(rd, Address(SP, wordSize, post_indexed), cond);
 415   }
 416 
 417   void pop(RegisterSet reg_set, AsmCondition cond = al) {
 418     assert(!reg_set.contains(SP), &quot;unpredictable instruction&quot;);
 419     ldmia(SP, reg_set, writeback, cond);
 420   }
 421 
 422   void fpushd(FloatRegister fd, AsmCondition cond = al) {
 423     fstmdbd(SP, FloatRegisterSet(fd), writeback, cond);
 424   }
 425 
 426   void fpushs(FloatRegister fd, AsmCondition cond = al) {
 427     fstmdbs(SP, FloatRegisterSet(fd), writeback, cond);
 428   }
 429 
 430   void fpopd(FloatRegister fd, AsmCondition cond = al) {
 431     fldmiad(SP, FloatRegisterSet(fd), writeback, cond);
 432   }
 433 
 434   void fpops(FloatRegister fd, AsmCondition cond = al) {
 435     fldmias(SP, FloatRegisterSet(fd), writeback, cond);
 436   }
 437 
 438   void fpush(FloatRegisterSet reg_set) {
 439     fstmdbd(SP, reg_set, writeback);
 440   }
 441 
 442   void fpop(FloatRegisterSet reg_set) {
 443     fldmiad(SP, reg_set, writeback);
 444   }
 445 
 446   void fpush_hardfp(FloatRegisterSet reg_set) {
 447 #ifndef __SOFTFP__
 448     fpush(reg_set);
 449 #endif
 450   }
 451 
 452   void fpop_hardfp(FloatRegisterSet reg_set) {
 453 #ifndef __SOFTFP__
 454     fpop(reg_set);
 455 #endif
 456   }
 457 
 458   // Order access primitives
 459   enum Membar_mask_bits {
 460     StoreStore = 1 &lt;&lt; 3,
 461     LoadStore  = 1 &lt;&lt; 2,
 462     StoreLoad  = 1 &lt;&lt; 1,
 463     LoadLoad   = 1 &lt;&lt; 0
 464   };
 465 
 466   void membar(Membar_mask_bits mask,
 467               Register tmp,
 468               bool preserve_flags = true,
 469               Register load_tgt = noreg);
 470 
 471   void breakpoint(AsmCondition cond = al);
 472   void stop(const char* msg);
 473   // prints msg and continues
 474   void warn(const char* msg);
 475   void unimplemented(const char* what = &quot;&quot;);
 476   void should_not_reach_here()                   { stop(&quot;should not reach here&quot;); }
 477   static void debug(const char* msg, const intx* registers);
 478 
 479   // Create a walkable frame to help tracking down who called this code.
 480   // Returns the frame size in words.
 481   int should_not_call_this() {
 482     raw_push(FP, LR);
 483     should_not_reach_here();
 484     flush();
 485     return 2; // frame_size_in_words (FP+LR)
 486   }
 487 
 488   int save_all_registers();
 489   void restore_all_registers();
 490   int save_caller_save_registers();
 491   void restore_caller_save_registers();
 492 
 493   void add_rc(Register dst, Register arg1, RegisterOrConstant arg2);
 494 
 495   // add_slow and mov_slow are used to manipulate offsets larger than 1024,
 496   // these functions are not expected to handle all possible constants,
 497   // only those that can really occur during compilation
 498   void add_slow(Register rd, Register rn, int c);
 499   void sub_slow(Register rd, Register rn, int c);
 500 
 501 
 502   void mov_slow(Register rd, intptr_t c, AsmCondition cond = al);
 503   void mov_slow(Register rd, const char *string);
 504   void mov_slow(Register rd, address addr);
 505 
 506   void patchable_mov_oop(Register rd, jobject o, int oop_index) {
 507     mov_oop(rd, o, oop_index);
 508   }
 509   void mov_oop(Register rd, jobject o, int index = 0, AsmCondition cond = al);
 510 
 511   void patchable_mov_metadata(Register rd, Metadata* o, int index) {
 512     mov_metadata(rd, o, index);
 513   }
 514   void mov_metadata(Register rd, Metadata* o, int index = 0);
 515 
 516   void mov_float(FloatRegister fd, jfloat c, AsmCondition cond = al);
 517   void mov_double(FloatRegister fd, jdouble c, AsmCondition cond = al);
 518 
 519 
 520   // Note: this variant of mov_address assumes the address moves with
 521   // the code. Do *not* implement it with non-relocated instructions,
 522   // unless PC-relative.
 523   void mov_relative_address(Register rd, address addr, AsmCondition cond = al) {
 524     int offset = addr - pc() - 8;
 525     assert((offset &amp; 3) == 0, &quot;bad alignment&quot;);
 526     if (offset &gt;= 0) {
 527       assert(AsmOperand::is_rotated_imm(offset), &quot;addr too far&quot;);
 528       add(rd, PC, offset, cond);
 529     } else {
 530       assert(AsmOperand::is_rotated_imm(-offset), &quot;addr too far&quot;);
 531       sub(rd, PC, -offset, cond);
 532     }
 533   }
 534 
 535   // Runtime address that may vary from one execution to another.
 536   // Warning: do not implement as a PC relative address.
 537   void mov_address(Register rd, address addr) {
 538     mov_address(rd, addr, RelocationHolder::none);
 539   }
 540 
 541   // rspec can be RelocationHolder::none (for ignored symbolic Relocation).
 542   // In that case, the address is absolute and the generated code need
 543   // not be relocable.
 544   void mov_address(Register rd, address addr, RelocationHolder const&amp; rspec) {
 545     assert(rspec.type() != relocInfo::runtime_call_type, &quot;do not use mov_address for runtime calls&quot;);
 546     assert(rspec.type() != relocInfo::static_call_type, &quot;do not use mov_address for relocable calls&quot;);
 547     if (rspec.type() == relocInfo::none) {
 548       // absolute address, relocation not needed
 549       mov_slow(rd, (intptr_t)addr);
 550       return;
 551     }
 552     if (VM_Version::supports_movw()) {
 553       relocate(rspec);
 554       int c = (int)addr;
 555       movw(rd, c &amp; 0xffff);
 556       if ((unsigned int)c &gt;&gt; 16) {
 557         movt(rd, (unsigned int)c &gt;&gt; 16);
 558       }
 559       return;
 560     }
 561     Label skip_literal;
 562     InlinedAddress addr_literal(addr, rspec);
 563     ldr_literal(rd, addr_literal);
 564     b(skip_literal);
 565     bind_literal(addr_literal);
 566     bind(skip_literal);
 567   }
 568 
 569   // Note: Do not define mov_address for a Label
 570   //
 571   // Load from addresses potentially within the code are now handled
 572   // InlinedLiteral subclasses (to allow more flexibility on how the
 573   // ldr_literal is performed).
 574 
 575   void ldr_literal(Register rd, InlinedAddress&amp; L) {
 576     assert(L.rspec().type() != relocInfo::runtime_call_type, &quot;avoid ldr_literal for calls&quot;);
 577     assert(L.rspec().type() != relocInfo::static_call_type, &quot;avoid ldr_literal for calls&quot;);
 578     relocate(L.rspec());
 579     ldr(rd, Address(PC, target(L.label) - pc() - 8));
 580   }
 581 
 582   void ldr_literal(Register rd, InlinedString&amp; L) {
 583     const char* msg = L.msg();
 584     if (code()-&gt;consts()-&gt;contains((address)msg)) {
 585       // string address moves with the code
 586       ldr(rd, Address(PC, ((address)msg) - pc() - 8));
 587       return;
 588     }
 589     // Warning: use external strings with care. They are not relocated
 590     // if the code moves. If needed, use code_string to move them
 591     // to the consts section.
 592     ldr(rd, Address(PC, target(L.label) - pc() - 8));
 593   }
 594 
 595   void ldr_literal(Register rd, InlinedMetadata&amp; L) {
 596     // relocation done in the bind_literal for metadatas
 597     ldr(rd, Address(PC, target(L.label) - pc() - 8));
 598   }
 599 
 600   void bind_literal(InlinedAddress&amp; L) {
 601     bind(L.label);
 602     assert(L.rspec().type() != relocInfo::metadata_type, &quot;Must use InlinedMetadata&quot;);
 603     // We currently do not use oop &#39;bound&#39; literals.
 604     // If the code evolves and the following assert is triggered,
 605     // we need to implement InlinedOop (see InlinedMetadata).
 606     assert(L.rspec().type() != relocInfo::oop_type, &quot;Inlined oops not supported&quot;);
 607     // Note: relocation is handled by relocate calls in ldr_literal
 608     AbstractAssembler::emit_address((address)L.target());
 609   }
 610 
 611   void bind_literal(InlinedString&amp; L) {
 612     const char* msg = L.msg();
 613     if (code()-&gt;consts()-&gt;contains((address)msg)) {
 614       // The Label should not be used; avoid binding it
 615       // to detect errors.
 616       return;
 617     }
 618     bind(L.label);
 619     AbstractAssembler::emit_address((address)L.msg());
 620   }
 621 
 622   void bind_literal(InlinedMetadata&amp; L) {
 623     bind(L.label);
 624     relocate(metadata_Relocation::spec_for_immediate());
 625     AbstractAssembler::emit_address((address)L.data());
 626   }
 627 
 628   void resolve_oop_handle(Register result);
 629   void load_mirror(Register mirror, Register method, Register tmp);
 630 
 631 #define ARM_INSTR_1(common_mnemonic, arm32_mnemonic, arg_type) \
 632   void common_mnemonic(arg_type arg) { \
 633       arm32_mnemonic(arg); \
 634   }
 635 
 636 #define ARM_INSTR_2(common_mnemonic, arm32_mnemonic, arg1_type, arg2_type) \
 637   void common_mnemonic(arg1_type arg1, arg2_type arg2) { \
 638       arm32_mnemonic(arg1, arg2); \
 639   }
 640 
 641 #define ARM_INSTR_3(common_mnemonic, arm32_mnemonic, arg1_type, arg2_type, arg3_type) \
 642   void common_mnemonic(arg1_type arg1, arg2_type arg2, arg3_type arg3) { \
 643       arm32_mnemonic(arg1, arg2, arg3); \
 644   }
 645 
 646   ARM_INSTR_1(jump, bx,  Register)
 647   ARM_INSTR_1(call, blx, Register)
 648 
 649   ARM_INSTR_2(cbz_32,  cbz,  Register, Label&amp;)
 650   ARM_INSTR_2(cbnz_32, cbnz, Register, Label&amp;)
 651 
 652   ARM_INSTR_2(ldr_u32, ldr,  Register, Address)
 653   ARM_INSTR_2(ldr_s32, ldr,  Register, Address)
 654   ARM_INSTR_2(str_32,  str,  Register, Address)
 655 
 656   ARM_INSTR_2(mvn_32,  mvn,  Register, Register)
 657   ARM_INSTR_2(cmp_32,  cmp,  Register, Register)
 658   ARM_INSTR_2(neg_32,  neg,  Register, Register)
 659   ARM_INSTR_2(clz_32,  clz,  Register, Register)
 660   ARM_INSTR_2(rbit_32, rbit, Register, Register)
 661 
 662   ARM_INSTR_2(cmp_32,  cmp,  Register, int)
 663   ARM_INSTR_2(cmn_32,  cmn,  Register, int)
 664 
 665   ARM_INSTR_3(add_32,  add,  Register, Register, Register)
 666   ARM_INSTR_3(sub_32,  sub,  Register, Register, Register)
 667   ARM_INSTR_3(subs_32, subs, Register, Register, Register)
 668   ARM_INSTR_3(mul_32,  mul,  Register, Register, Register)
 669   ARM_INSTR_3(and_32,  andr, Register, Register, Register)
 670   ARM_INSTR_3(orr_32,  orr,  Register, Register, Register)
 671   ARM_INSTR_3(eor_32,  eor,  Register, Register, Register)
 672 
 673   ARM_INSTR_3(add_32,  add,  Register, Register, AsmOperand)
 674   ARM_INSTR_3(sub_32,  sub,  Register, Register, AsmOperand)
 675   ARM_INSTR_3(orr_32,  orr,  Register, Register, AsmOperand)
 676   ARM_INSTR_3(eor_32,  eor,  Register, Register, AsmOperand)
 677   ARM_INSTR_3(and_32,  andr, Register, Register, AsmOperand)
 678 
 679 
 680   ARM_INSTR_3(add_32,  add,  Register, Register, int)
 681   ARM_INSTR_3(adds_32, adds, Register, Register, int)
 682   ARM_INSTR_3(sub_32,  sub,  Register, Register, int)
 683   ARM_INSTR_3(subs_32, subs, Register, Register, int)
 684 
 685   ARM_INSTR_2(tst_32,  tst,  Register, unsigned int)
 686   ARM_INSTR_2(tst_32,  tst,  Register, AsmOperand)
 687 
 688   ARM_INSTR_3(and_32,  andr, Register, Register, uint)
 689   ARM_INSTR_3(orr_32,  orr,  Register, Register, uint)
 690   ARM_INSTR_3(eor_32,  eor,  Register, Register, uint)
 691 
 692   ARM_INSTR_1(cmp_zero_float,  fcmpzs, FloatRegister)
 693   ARM_INSTR_1(cmp_zero_double, fcmpzd, FloatRegister)
 694 
 695   ARM_INSTR_2(ldr_float,   flds,   FloatRegister, Address)
 696   ARM_INSTR_2(str_float,   fsts,   FloatRegister, Address)
 697   ARM_INSTR_2(mov_float,   fcpys,  FloatRegister, FloatRegister)
 698   ARM_INSTR_2(neg_float,   fnegs,  FloatRegister, FloatRegister)
 699   ARM_INSTR_2(abs_float,   fabss,  FloatRegister, FloatRegister)
 700   ARM_INSTR_2(sqrt_float,  fsqrts, FloatRegister, FloatRegister)
 701   ARM_INSTR_2(cmp_float,   fcmps,  FloatRegister, FloatRegister)
 702 
 703   ARM_INSTR_3(add_float,   fadds,  FloatRegister, FloatRegister, FloatRegister)
 704   ARM_INSTR_3(sub_float,   fsubs,  FloatRegister, FloatRegister, FloatRegister)
 705   ARM_INSTR_3(mul_float,   fmuls,  FloatRegister, FloatRegister, FloatRegister)
 706   ARM_INSTR_3(div_float,   fdivs,  FloatRegister, FloatRegister, FloatRegister)
 707 
 708   ARM_INSTR_2(ldr_double,  fldd,   FloatRegister, Address)
 709   ARM_INSTR_2(str_double,  fstd,   FloatRegister, Address)
 710   ARM_INSTR_2(mov_double,  fcpyd,  FloatRegister, FloatRegister)
 711   ARM_INSTR_2(neg_double,  fnegd,  FloatRegister, FloatRegister)
 712   ARM_INSTR_2(cmp_double,  fcmpd,  FloatRegister, FloatRegister)
 713   ARM_INSTR_2(abs_double,  fabsd,  FloatRegister, FloatRegister)
 714   ARM_INSTR_2(sqrt_double, fsqrtd, FloatRegister, FloatRegister)
 715 
 716   ARM_INSTR_3(add_double,  faddd,  FloatRegister, FloatRegister, FloatRegister)
 717   ARM_INSTR_3(sub_double,  fsubd,  FloatRegister, FloatRegister, FloatRegister)
 718   ARM_INSTR_3(mul_double,  fmuld,  FloatRegister, FloatRegister, FloatRegister)
 719   ARM_INSTR_3(div_double,  fdivd,  FloatRegister, FloatRegister, FloatRegister)
 720 
 721   ARM_INSTR_2(convert_f2d, fcvtds, FloatRegister, FloatRegister)
 722   ARM_INSTR_2(convert_d2f, fcvtsd, FloatRegister, FloatRegister)
 723 
 724   ARM_INSTR_2(mov_fpr2gpr_float, fmrs, Register, FloatRegister)
 725 
 726 #undef ARM_INSTR_1
 727 #undef ARM_INSTR_2
 728 #undef ARM_INSTR_3
 729 
 730 
 731 
 732   void tbz(Register rt, int bit, Label&amp; L) {
 733     assert(0 &lt;= bit &amp;&amp; bit &lt; BitsPerWord, &quot;bit number is out of range&quot;);
 734     tst(rt, 1 &lt;&lt; bit);
 735     b(L, eq);
 736   }
 737 
 738   void tbnz(Register rt, int bit, Label&amp; L) {
 739     assert(0 &lt;= bit &amp;&amp; bit &lt; BitsPerWord, &quot;bit number is out of range&quot;);
 740     tst(rt, 1 &lt;&lt; bit);
 741     b(L, ne);
 742   }
 743 
 744   void cbz(Register rt, Label&amp; L) {
 745     cmp(rt, 0);
 746     b(L, eq);
 747   }
 748 
 749   void cbz(Register rt, address target) {
 750     cmp(rt, 0);
 751     b(target, eq);
 752   }
 753 
 754   void cbnz(Register rt, Label&amp; L) {
 755     cmp(rt, 0);
 756     b(L, ne);
 757   }
 758 
 759   void ret(Register dst = LR) {
 760     bx(dst);
 761   }
 762 
 763 
 764   Register zero_register(Register tmp) {
 765     mov(tmp, 0);
 766     return tmp;
 767   }
 768 
 769   void logical_shift_left(Register dst, Register src, int shift) {
 770     mov(dst, AsmOperand(src, lsl, shift));
 771   }
 772 
 773   void logical_shift_left_32(Register dst, Register src, int shift) {
 774     mov(dst, AsmOperand(src, lsl, shift));
 775   }
 776 
 777   void logical_shift_right(Register dst, Register src, int shift) {
 778     mov(dst, AsmOperand(src, lsr, shift));
 779   }
 780 
 781   void arith_shift_right(Register dst, Register src, int shift) {
 782     mov(dst, AsmOperand(src, asr, shift));
 783   }
 784 
 785   void asr_32(Register dst, Register src, int shift) {
 786     mov(dst, AsmOperand(src, asr, shift));
 787   }
 788 
 789   // If &lt;cond&gt; holds, compares r1 and r2. Otherwise, flags are set so that &lt;cond&gt; does not hold.
 790   void cond_cmp(Register r1, Register r2, AsmCondition cond) {
 791     cmp(r1, r2, cond);
 792   }
 793 
 794   // If &lt;cond&gt; holds, compares r and imm. Otherwise, flags are set so that &lt;cond&gt; does not hold.
 795   void cond_cmp(Register r, int imm, AsmCondition cond) {
 796     cmp(r, imm, cond);
 797   }
 798 
 799   void align_reg(Register dst, Register src, int align) {
 800     assert (is_power_of_2(align), &quot;should be&quot;);
 801     bic(dst, src, align-1);
 802   }
 803 
 804   void prefetch_read(Address addr) {
 805     pld(addr);
 806   }
 807 
 808   void raw_push(Register r1, Register r2) {
 809     assert(r1-&gt;encoding() &lt; r2-&gt;encoding(), &quot;should be ordered&quot;);
 810     push(RegisterSet(r1) | RegisterSet(r2));
 811   }
 812 
 813   void raw_pop(Register r1, Register r2) {
 814     assert(r1-&gt;encoding() &lt; r2-&gt;encoding(), &quot;should be ordered&quot;);
 815     pop(RegisterSet(r1) | RegisterSet(r2));
 816   }
 817 
 818   void raw_push(Register r1, Register r2, Register r3) {
 819     assert(r1-&gt;encoding() &lt; r2-&gt;encoding() &amp;&amp; r2-&gt;encoding() &lt; r3-&gt;encoding(), &quot;should be ordered&quot;);
 820     push(RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3));
 821   }
 822 
 823   void raw_pop(Register r1, Register r2, Register r3) {
 824     assert(r1-&gt;encoding() &lt; r2-&gt;encoding() &amp;&amp; r2-&gt;encoding() &lt; r3-&gt;encoding(), &quot;should be ordered&quot;);
 825     pop(RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3));
 826   }
 827 
 828   // Restores registers r1 and r2 previously saved by raw_push(r1, r2, ret_addr) and returns by ret_addr. Clobbers LR.
 829   void raw_pop_and_ret(Register r1, Register r2) {
 830     raw_pop(r1, r2, PC);
 831   }
 832 
 833   void indirect_jump(Address addr, Register scratch) {
 834     ldr(PC, addr);
 835   }
 836 
 837   void indirect_jump(InlinedAddress&amp; literal, Register scratch) {
 838     ldr_literal(PC, literal);
 839   }
 840 
 841   void neg(Register dst, Register src) {
 842     rsb(dst, src, 0);
 843   }
 844 
 845   void branch_if_negative_32(Register r, Label&amp; L) {
 846     // TODO: This function and branch_if_any_negative_32 could possibly
 847     // be revised after the aarch64 removal.
 848     // tbnz is not used instead of tst &amp; b.mi because destination may be out of tbnz range (+-32KB)
 849     // since these methods are used in LIR_Assembler::emit_arraycopy() to jump to stub entry.
 850     tst_32(r, r);
 851     b(L, mi);
 852   }
 853 
 854   void branch_if_any_negative_32(Register r1, Register r2, Register tmp, Label&amp; L) {
 855     orrs(tmp, r1, r2);
 856     b(L, mi);
 857   }
 858 
 859   void branch_if_any_negative_32(Register r1, Register r2, Register r3, Register tmp, Label&amp; L) {
 860     orr_32(tmp, r1, r2);
 861     orrs(tmp, tmp, r3);
 862     b(L, mi);
 863   }
 864 
 865   void add_ptr_scaled_int32(Register dst, Register r1, Register r2, int shift) {
 866       add(dst, r1, AsmOperand(r2, lsl, shift));
 867   }
 868 
 869   void sub_ptr_scaled_int32(Register dst, Register r1, Register r2, int shift) {
 870     sub(dst, r1, AsmOperand(r2, lsl, shift));
 871   }
 872 
 873   // C &#39;boolean&#39; to Java boolean: x == 0 ? 0 : 1
 874   void c2bool(Register x);
 875 
 876     // klass oop manipulations if compressed
 877 
 878   void load_klass(Register dst_klass, Register src_oop, AsmCondition cond = al);
 879 
 880   void store_klass(Register src_klass, Register dst_oop);
 881 
 882 
 883     // oop manipulations
 884 
 885   void load_heap_oop(Register dst, Address src, Register tmp1 = noreg, Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);
 886   void store_heap_oop(Address obj, Register new_val, Register tmp1 = noreg, Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);
 887   void store_heap_oop_null(Address obj, Register new_val, Register tmp1 = noreg, Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);
 888 
 889   void access_load_at(BasicType type, DecoratorSet decorators, Address src, Register dst, Register tmp1, Register tmp2, Register tmp3);
 890   void access_store_at(BasicType type, DecoratorSet decorators, Address obj, Register new_val, Register tmp1, Register tmp2, Register tmp3, bool is_null);
 891 
 892   // Resolves obj for access. Result is placed in the same register.
 893   // All other registers are preserved.
 894   void resolve(DecoratorSet decorators, Register obj);
 895 
 896 
 897   void ldr_global_ptr(Register reg, address address_of_global);
 898   void ldr_global_s32(Register reg, address address_of_global);
 899   void ldrb_global(Register reg, address address_of_global);
 900 
 901   // address_placeholder_instruction is invalid instruction and is used
 902   // as placeholder in code for address of label
 903   enum { address_placeholder_instruction = 0xFFFFFFFF };
 904 
 905   void emit_address(Label&amp; L) {
 906     assert(!L.is_bound(), &quot;otherwise address will not be patched&quot;);
 907     target(L);       // creates relocation which will be patched later
 908 
 909     assert ((offset() &amp; (wordSize-1)) == 0, &quot;should be aligned by word size&quot;);
 910 
 911     AbstractAssembler::emit_address((address)address_placeholder_instruction);
 912   }
 913 
 914   void b(address target, AsmCondition cond = al) {
 915     Assembler::b(target, cond);                 \
 916   }
 917   void b(Label&amp; L, AsmCondition cond = al) {
 918     // internal jumps
 919     Assembler::b(target(L), cond);
 920   }
 921 
 922   void bl(address target, AsmCondition cond = al) {
 923     Assembler::bl(target, cond);
 924   }
 925   void bl(Label&amp; L, AsmCondition cond = al) {
 926     // internal calls
 927     Assembler::bl(target(L), cond);
 928   }
 929 
 930   void adr(Register dest, Label&amp; L, AsmCondition cond = al) {
 931     int delta = target(L) - pc() - 8;
 932     if (delta &gt;= 0) {
 933       add(dest, PC, delta, cond);
 934     } else {
 935       sub(dest, PC, -delta, cond);
 936     }
 937   }
 938 
 939   // Variable-length jump and calls. We now distinguish only the
 940   // patchable case from the other cases. Patchable must be
 941   // distinguised from relocable. Relocable means the generated code
 942   // containing the jump/call may move. Patchable means that the
 943   // targeted address may be changed later.
 944 
 945   // Non patchable versions.
 946   // - used only for relocInfo::runtime_call_type and relocInfo::none
 947   // - may use relative or absolute format (do not use relocInfo::none
 948   //   if the generated code may move)
 949   // - the implementation takes into account switch to THUMB mode if the
 950   //   destination is a THUMB address
 951   // - the implementation supports far targets
 952   //
 953   // To reduce regression risk, scratch still defaults to noreg on
 954   // arm32. This results in patchable instructions. However, if
 955   // patching really matters, the call sites should be modified and
 956   // use patchable_call or patchable_jump. If patching is not required
 957   // and if a register can be cloberred, it should be explicitly
 958   // specified to allow future optimizations.
 959   void jump(address target,
 960             relocInfo::relocType rtype = relocInfo::runtime_call_type,
 961             Register scratch = noreg, AsmCondition cond = al);
 962 
 963   void call(address target,
 964             RelocationHolder rspec, AsmCondition cond = al);
 965 
 966   void call(address target,
 967             relocInfo::relocType rtype = relocInfo::runtime_call_type,
 968             AsmCondition cond = al) {
 969     call(target, Relocation::spec_simple(rtype), cond);
 970   }
 971 
 972   void jump(AddressLiteral dest) {
 973     jump(dest.target(), dest.reloc());
 974   }
 975   void jump(address dest, relocInfo::relocType rtype, AsmCondition cond) {
 976     jump(dest, rtype, Rtemp, cond);
 977   }
 978 
 979   void call(AddressLiteral dest) {
 980     call(dest.target(), dest.reloc());
 981   }
 982 
 983   // Patchable version:
 984   // - set_destination can be used to atomically change the target
 985   //
 986   // The targets for patchable_jump and patchable_call must be in the
 987   // code cache.
 988   // [ including possible extensions of the code cache, like AOT code ]
 989   //
 990   // To reduce regression risk, scratch still defaults to noreg on
 991   // arm32. If a register can be cloberred, it should be explicitly
 992   // specified to allow future optimizations.
 993   void patchable_jump(address target,
 994                       relocInfo::relocType rtype = relocInfo::runtime_call_type,
 995                       Register scratch = noreg, AsmCondition cond = al
 996                       );
 997 
 998   // patchable_call may scratch Rtemp
 999   int patchable_call(address target,
1000                      RelocationHolder const&amp; rspec,
1001                      bool c2 = false);
1002 
1003   int patchable_call(address target,
1004                      relocInfo::relocType rtype,
1005                      bool c2 = false) {
1006     return patchable_call(target, Relocation::spec_simple(rtype), c2);
1007   }
1008 
1009 
1010   static bool _reachable_from_cache(address target);
1011   static bool _cache_fully_reachable();
1012   bool cache_fully_reachable();
1013   bool reachable_from_cache(address target);
1014 
1015   void zero_extend(Register rd, Register rn, int bits);
1016   void sign_extend(Register rd, Register rn, int bits);
1017 
1018   inline void zap_high_non_significant_bits(Register r) {
1019   }
1020 
1021   void cmpoop(Register obj1, Register obj2);
1022 
1023   void long_move(Register rd_lo, Register rd_hi,
1024                  Register rn_lo, Register rn_hi,
1025                  AsmCondition cond = al);
1026   void long_shift(Register rd_lo, Register rd_hi,
1027                   Register rn_lo, Register rn_hi,
1028                   AsmShift shift, Register count);
1029   void long_shift(Register rd_lo, Register rd_hi,
1030                   Register rn_lo, Register rn_hi,
1031                   AsmShift shift, int count);
1032 
1033   void atomic_cas(Register tmpreg1, Register tmpreg2, Register oldval, Register newval, Register base, int offset);
1034   void atomic_cas_bool(Register oldval, Register newval, Register base, int offset, Register tmpreg);
1035   void atomic_cas64(Register temp_lo, Register temp_hi, Register temp_result, Register oldval_lo, Register oldval_hi, Register newval_lo, Register newval_hi, Register base, int offset);
1036 
1037   void cas_for_lock_acquire(Register oldval, Register newval, Register base, Register tmp, Label &amp;slow_case, bool allow_fallthrough_on_failure = false, bool one_shot = false);
1038   void cas_for_lock_release(Register oldval, Register newval, Register base, Register tmp, Label &amp;slow_case, bool allow_fallthrough_on_failure = false, bool one_shot = false);
1039 
1040 #ifndef PRODUCT
1041   // Preserves flags and all registers.
1042   // On SMP the updated value might not be visible to external observers without a sychronization barrier
1043   void cond_atomic_inc32(AsmCondition cond, int* counter_addr);
1044 #endif // !PRODUCT
1045 
1046   // unconditional non-atomic increment
1047   void inc_counter(address counter_addr, Register tmpreg1, Register tmpreg2);
1048   void inc_counter(int* counter_addr, Register tmpreg1, Register tmpreg2) {
1049     inc_counter((address) counter_addr, tmpreg1, tmpreg2);
1050   }
1051 
1052   void pd_patch_instruction(address branch, address target, const char* file, int line);
1053 
1054   // Loading and storing values by size and signed-ness;
1055   // size must not exceed wordSize (i.e. 8-byte values are not supported on 32-bit ARM);
1056   // each of these calls generates exactly one load or store instruction,
1057   // so src can be pre- or post-indexed address.
1058   // 32-bit ARM variants also support conditional execution
1059   void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, AsmCondition cond = al);
1060   void store_sized_value(Register src, Address dst, size_t size_in_bytes, AsmCondition cond = al);
1061 
1062   void lookup_interface_method(Register recv_klass,
1063                                Register intf_klass,
1064                                RegisterOrConstant itable_index,
1065                                Register method_result,
1066                                Register temp_reg1,
1067                                Register temp_reg2,
1068                                Label&amp; L_no_such_interface);
1069 
1070   // Compare char[] arrays aligned to 4 bytes.
1071   void char_arrays_equals(Register ary1, Register ary2,
1072                           Register limit, Register result,
1073                           Register chr1, Register chr2, Label&amp; Ldone);
1074 
1075 
1076   void floating_cmp(Register dst);
1077 
1078   // improved x86 portability (minimizing source code changes)
1079 
1080   void ldr_literal(Register rd, AddressLiteral addr) {
1081     relocate(addr.rspec());
1082     ldr(rd, Address(PC, addr.target() - pc() - 8));
1083   }
1084 
1085   void lea(Register Rd, AddressLiteral addr) {
1086     // Never dereferenced, as on x86 (lval status ignored)
1087     mov_address(Rd, addr.target(), addr.rspec());
1088   }
1089 
1090   void restore_default_fp_mode();
1091 
1092 #ifdef COMPILER2
1093   void fast_lock(Register obj, Register box, Register scratch, Register scratch2, Register scratch3 = noreg);
1094   void fast_unlock(Register obj, Register box, Register scratch, Register scratch2);
1095 #endif
1096 
1097   void safepoint_poll(Register tmp1, Label&amp; slow_path);
1098   void get_polling_page(Register dest);
1099   void read_polling_page(Register dest, relocInfo::relocType rtype);
1100 };
1101 
1102 
1103 // The purpose of this class is to build several code fragments of the same size
1104 // in order to allow fast table branch.
1105 
1106 class FixedSizeCodeBlock {
1107 public:
1108   FixedSizeCodeBlock(MacroAssembler* masm, int size_in_instrs, bool enabled);
1109   ~FixedSizeCodeBlock();
1110 
1111 private:
1112   MacroAssembler* _masm;
1113   address _start;
1114   int _size_in_instrs;
1115   bool _enabled;
1116 };
1117 
1118 
1119 #endif // CPU_ARM_MACROASSEMBLER_ARM_HPP
    </pre>
  </body>
</html>