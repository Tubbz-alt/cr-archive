<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.code.ValueUtil.asRegister;
 28 import static jdk.vm.ci.code.ValueUtil.isRegister;
 29 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
 30 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.CONST;
 31 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.ILLEGAL;
 32 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 33 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
 34 
 35 import java.util.Objects;
 36 
 37 import org.graalvm.compiler.asm.Label;
 38 import org.graalvm.compiler.asm.amd64.AMD64Address;
 39 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
 40 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 41 import org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64RMOp;
 42 import org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag;
 43 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp;
 44 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMIOp;
 45 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMOp;
 46 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRVMOp;
 47 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
 48 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 49 import org.graalvm.compiler.asm.amd64.AVXKind;
 50 import org.graalvm.compiler.core.common.LIRKind;
 51 import org.graalvm.compiler.core.common.NumUtil;
 52 import org.graalvm.compiler.lir.ConstantValue;
 53 import org.graalvm.compiler.lir.LIRInstructionClass;
 54 import org.graalvm.compiler.lir.Opcode;
 55 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 56 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 57 
 58 import jdk.vm.ci.amd64.AMD64;
 59 import jdk.vm.ci.amd64.AMD64.CPUFeature;
 60 import jdk.vm.ci.amd64.AMD64Kind;
 61 import jdk.vm.ci.code.Register;
 62 import jdk.vm.ci.meta.JavaConstant;
 63 import jdk.vm.ci.meta.JavaKind;
 64 import jdk.vm.ci.meta.Value;
 65 
 66 /**
 67  */
 68 @Opcode(&quot;AMD64_ARRAY_INDEX_OF&quot;)
 69 public final class AMD64ArrayIndexOfOp extends AMD64LIRInstruction {
 70     public static final LIRInstructionClass&lt;AMD64ArrayIndexOfOp&gt; TYPE = LIRInstructionClass.create(AMD64ArrayIndexOfOp.class);
 71 
 72     private final JavaKind valueKind;
 73     private final int nValues;
 74     private final boolean findTwoConsecutive;
 75     private final AMD64Kind vectorKind;
 76     private final int arrayBaseOffset;
 77     private final Scale arrayIndexScale;
 78 
 79     @Def({REG}) protected Value resultValue;
 80     @Alive({REG}) protected Value arrayPtrValue;
 81     @Alive({REG}) protected Value arrayLengthValue;
 82     @Use({REG}) protected Value fromIndexValue;
 83     @Alive({REG, STACK, CONST}) protected Value searchValue1;
 84     @Alive({REG, STACK, CONST, ILLEGAL}) protected Value searchValue2;
 85     @Alive({REG, STACK, CONST, ILLEGAL}) protected Value searchValue3;
 86     @Alive({REG, STACK, CONST, ILLEGAL}) protected Value searchValue4;
 87     @Temp({REG}) protected Value comparisonResult1;
 88     @Temp({REG, ILLEGAL}) protected Value comparisonResult2;
 89     @Temp({REG, ILLEGAL}) protected Value vectorCompareVal1;
 90     @Temp({REG, ILLEGAL}) protected Value vectorCompareVal2;
 91     @Temp({REG, ILLEGAL}) protected Value vectorCompareVal3;
 92     @Temp({REG, ILLEGAL}) protected Value vectorCompareVal4;
 93     @Temp({REG, ILLEGAL}) protected Value vectorArray1;
 94     @Temp({REG, ILLEGAL}) protected Value vectorArray2;
 95     @Temp({REG, ILLEGAL}) protected Value vectorArray3;
 96     @Temp({REG, ILLEGAL}) protected Value vectorArray4;
 97 
 98     public AMD64ArrayIndexOfOp(JavaKind arrayKind, JavaKind valueKind, boolean findTwoConsecutive, int maxVectorSize, LIRGeneratorTool tool,
 99                     Value result, Value arrayPtr, Value arrayLength, Value fromIndex, Value... searchValues) {
100         super(TYPE);
101         this.valueKind = valueKind;
102         this.arrayBaseOffset = tool.getProviders().getMetaAccess().getArrayBaseOffset(arrayKind);
103         this.arrayIndexScale = Objects.requireNonNull(Scale.fromInt(tool.getProviders().getMetaAccess().getArrayIndexScale(valueKind)));
104         this.findTwoConsecutive = findTwoConsecutive;
105         assert 0 &lt; searchValues.length &amp;&amp; searchValues.length &lt;= 4;
106         assert byteMode(valueKind) || charMode(valueKind);
107         assert supports(tool, CPUFeature.SSE2) || supports(tool, CPUFeature.AVX) || supportsAVX2(tool);
108         nValues = searchValues.length;
109         assert !findTwoConsecutive || nValues == 1;
110         resultValue = result;
111         arrayPtrValue = arrayPtr;
112         arrayLengthValue = arrayLength;
113         fromIndexValue = fromIndex;
114         searchValue1 = searchValues[0];
115         searchValue2 = nValues &gt; 1 ? searchValues[1] : Value.ILLEGAL;
116         searchValue3 = nValues &gt; 2 ? searchValues[2] : Value.ILLEGAL;
117         searchValue4 = nValues &gt; 3 ? searchValues[3] : Value.ILLEGAL;
118         comparisonResult1 = tool.newVariable(LIRKind.value(tool.target().arch.getWordKind()));
119         comparisonResult2 = findTwoConsecutive ? tool.newVariable(LIRKind.value(tool.target().arch.getWordKind())) : Value.ILLEGAL;
120         vectorKind = supportsAVX2(tool) &amp;&amp; (maxVectorSize &lt; 0 || maxVectorSize &gt;= 32) ? byteMode(valueKind) ? AMD64Kind.V256_BYTE : AMD64Kind.V256_WORD
121                         : byteMode(valueKind) ? AMD64Kind.V128_BYTE : AMD64Kind.V128_WORD;
122         vectorCompareVal1 = tool.newVariable(LIRKind.value(vectorKind));
123         vectorCompareVal2 = nValues &gt; 1 ? tool.newVariable(LIRKind.value(vectorKind)) : Value.ILLEGAL;
124         vectorCompareVal3 = nValues &gt; 2 ? tool.newVariable(LIRKind.value(vectorKind)) : Value.ILLEGAL;
125         vectorCompareVal4 = nValues &gt; 3 ? tool.newVariable(LIRKind.value(vectorKind)) : Value.ILLEGAL;
126         vectorArray1 = tool.newVariable(LIRKind.value(vectorKind));
127         vectorArray2 = tool.newVariable(LIRKind.value(vectorKind));
128         vectorArray3 = tool.newVariable(LIRKind.value(vectorKind));
129         vectorArray4 = tool.newVariable(LIRKind.value(vectorKind));
130     }
131 
132     private static boolean byteMode(JavaKind kind) {
133         return kind == JavaKind.Byte;
134     }
135 
136     private static boolean charMode(JavaKind kind) {
137         return kind == JavaKind.Char;
138     }
139 
140     private JavaKind getComparisonKind() {
141         return findTwoConsecutive ? (byteMode(valueKind) ? JavaKind.Char : JavaKind.Int) : valueKind;
142     }
143 
144     private AVXKind.AVXSize getVectorSize() {
145         return AVXKind.getDataSize(vectorKind);
146     }
147 
148     @Override
149     public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler asm) {
150         int nVectors = nValues == 1 ? 4 : nValues == 2 ? 2 : 1;
151         Register arrayPtr = asRegister(arrayPtrValue);
152         Register arrayLength = asRegister(arrayLengthValue);
153         Register fromIndex = asRegister(fromIndexValue);
154         Register index = asRegister(resultValue);
155         Value[] searchValue = {
156                         nValues &gt; 0 ? searchValue1 : null,
157                         nValues &gt; 1 ? searchValue2 : null,
158                         nValues &gt; 2 ? searchValue3 : null,
159                         nValues &gt; 3 ? searchValue4 : null,
160         };
161         Register[] vecCmp = {
162                         nValues &gt; 0 ? asRegister(vectorCompareVal1) : null,
163                         nValues &gt; 1 ? asRegister(vectorCompareVal2) : null,
164                         nValues &gt; 2 ? asRegister(vectorCompareVal3) : null,
165                         nValues &gt; 3 ? asRegister(vectorCompareVal4) : null,
166         };
167         Register[] vecArray = {
168                         asRegister(vectorArray1),
169                         asRegister(vectorArray2),
170                         asRegister(vectorArray3),
171                         asRegister(vectorArray4),
172         };
173         Register[] cmpResult = {
174                         asRegister(comparisonResult1),
175                         findTwoConsecutive ? asRegister(comparisonResult2) : null,
176         };
177         Label ret = new Label();
178 
179         Label bulkVectorLoop = new Label();
180         Label singleVectorLoop = new Label();
181         Label[] vectorFound = {
182                         new Label(),
183                         new Label(),
184                         new Label(),
185                         new Label(),
186         };
187         Label runVectorized = new Label();
188         Label elementWiseLoop = new Label();
189         Label elementWiseFound = new Label();
190         Label elementWiseNotFound = new Label();
191         Label skipBulkVectorLoop = new Label();
192         int vectorSize = getVectorSize().getBytes() / valueKind.getByteCount();
193         int bulkSize = vectorSize * nVectors;
194         JavaKind vectorCompareKind = valueKind;
195         if (findTwoConsecutive) {
196             bulkSize /= 2;
197             vectorCompareKind = byteMode(valueKind) ? JavaKind.Char : JavaKind.Int;
198         }
199         // index = fromIndex + vectorSize (+1 if findTwoConsecutive)
200         // important: this must be the first register manipulation, since fromIndex is
201         // annotated with @Use
202         asm.leaq(index, new AMD64Address(fromIndex, vectorSize + (findTwoConsecutive ? 1 : 0)));
203 
204         // check if vector vector load is in bounds
205         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.LessEqual, runVectorized, true);
206 
207         // search range is smaller than vector size, do element-wise comparison
208 
209         // index = fromIndex (+ 1 if findTwoConsecutive)
210         asm.subq(index, vectorSize);
211         // check if enough array slots remain
212         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.GreaterEqual, elementWiseNotFound, true);
213         // compare one-by-one
214         asm.bind(elementWiseLoop);
215         // check for match
216         OperandSize cmpSize = getOpSize(getComparisonKind());
217         // address = findTwoConsecutive ? array[index - 1] : array[index]
218         AMD64Address arrayAddr = new AMD64Address(arrayPtr, index, arrayIndexScale, arrayBaseOffset - (findTwoConsecutive ? valueKind.getByteCount() : 0));
219         boolean valuesOnStack = searchValuesOnStack(searchValue);
220         if (valuesOnStack) {
221             (cmpSize == OperandSize.BYTE ? AMD64RMOp.MOVB : AMD64RMOp.MOV).emit(asm, cmpSize, cmpResult[0], arrayAddr);
222             for (int i = 0; i &lt; nValues; i++) {
223                 if (isConstant(searchValue[i])) {
224                     int imm = asConstant(searchValue[i]).asInt();
225                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, cmpResult[0], imm);
226                 } else if (isStackSlot(searchValue[i])) {
227                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], (AMD64Address) crb.asAddress(searchValue[i]));
228                 } else {
229                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], asRegister(searchValue[i]));
230                 }
231                 // TODO (yz) the preceding cmp instruction may be fused with the following jcc
232                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
233             }
234         } else {
235             for (int i = 0; i &lt; nValues; i++) {
236                 if (isConstant(searchValue[i])) {
237                     int imm = asConstant(searchValue[i]).asInt();
238                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, arrayAddr, imm);
239                 } else {
240                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, asRegister(searchValue[i]), arrayAddr);
241                 }
242                 // TODO (yz) the preceding cmp instruction may be fused with the following jcc
243                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
244             }
245         }
246         // adjust index
247         asm.incrementq(index, 1);
248         // continue loop
249         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Less, elementWiseLoop, true);
250 
251         asm.bind(elementWiseNotFound);
252         asm.xorq(index, index);
253 
254         if (findTwoConsecutive) {
255             asm.bind(elementWiseFound);
256             asm.decrementq(index, 1);
257         } else {
258             asm.decrementq(index, 1);
259             asm.bind(elementWiseFound);
260         }
261         asm.jmp(ret);
262 
263         // vectorized implementation
264         asm.bind(runVectorized);
265 
266         // move search values to vectors
267         for (int i = 0; i &lt; nValues; i++) {
268             // fill comparison vector with copies of the search value
269             broadcastSearchValue(crb, asm, vecCmp[i], searchValue[i], cmpResult[0], vecArray[0]);
270         }
271 
272         // do one unaligned vector comparison pass and adjust alignment afterwards
273         emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, false);
274 
275         // adjust index to vector size alignment
276         asm.leaq(cmpResult[0], new AMD64Address(arrayPtr, arrayBaseOffset));
277         if (charMode(valueKind)) {
278             asm.shrq(cmpResult[0], 1);
279         }
280         asm.addq(index, cmpResult[0]);
281         // adjust to next lower multiple of vector size
282         asm.andq(index, ~(vectorSize - 1));
283         asm.subq(index, cmpResult[0]);
284         // add bulk size
285         asm.addq(index, bulkSize);
286 
287         // check if there are enough array slots remaining for the bulk loop
288         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Greater, skipBulkVectorLoop, true);
289 
290         emitAlign(crb, asm);
291         asm.bind(bulkVectorLoop);
292         // memory-aligned bulk comparison
293         emitVectorCompare(asm, vectorCompareKind, nVectors, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, !findTwoConsecutive);
294         // adjust index
295         asm.addq(index, bulkSize);
296         // check if there are enough array slots remaining for the bulk loop
297         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.LessEqual, bulkVectorLoop, true);
298 
299         asm.bind(skipBulkVectorLoop);
300         if ((findTwoConsecutive &amp;&amp; nVectors == 2) || nVectors == 1) {
301             // do last load from end of array
302             asm.movq(index, arrayLength);
303             // compare
304             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
305         } else {
306             // remove bulk offset
307             asm.subq(index, bulkSize);
308             emitAlign(crb, asm);
309             // same loop as bulkVectorLoop, with only one vector
310             asm.bind(singleVectorLoop);
311             // add vector size
312             asm.addq(index, vectorSize);
313             // check if vector load is in bounds
314             asm.cmpq(index, arrayLength);
315             // if load would be over bounds, set the load to the end of the array
316             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, index, arrayLength);
317             // compare
318             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
319             // check if there are enough array slots remaining for the loop
320             asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Less, singleVectorLoop, true);
321         }
322 
323         asm.movl(index, -1);
324         asm.jmpb(ret);
325 
326         if (findTwoConsecutive) {
327             Label vectorFound2Done = new Label();
328 
329             // vectorFound[0] and vectorFound[2] behave like the single-char case
330             asm.bind(vectorFound[2]);
331             // add static offset
332             asm.subq(index, getResultIndexDelta(2));
333             asm.jmpb(vectorFound2Done);
334 
335             asm.bind(vectorFound[0]);
336             // add static offset
337             asm.subq(index, getResultIndexDelta(0));
338             asm.bind(vectorFound2Done);
339             // find offset
340             asm.bsfq(cmpResult[0], cmpResult[0]);
341             if (charMode(valueKind)) {
342                 // convert byte offset to chars
343                 asm.shrl(cmpResult[0], 1);
344             }
345             // add offset to index
346             asm.addq(index, cmpResult[0]);
347             asm.jmpb(ret);
348 
349             Label minResult = new Label();
350             Label minResultDone = new Label();
351 
352             // in vectorFound[1] and vectorFound[3], we have to check the results 0 and 2 as well
353             if (nVectors &gt; 2) {
354                 asm.bind(vectorFound[3]);
355                 // add offset
356                 asm.subq(index, getResultIndexDelta(3));
357                 asm.jmpb(minResult);
358             }
359 
360             asm.bind(vectorFound[1]);
361             // add offset
362             asm.subq(index, getResultIndexDelta(1));
363 
364             asm.bind(minResult);
365             // find offset 0
366             asm.bsfq(cmpResult[1], cmpResult[1]);
367             // check if second result is also a match
368             asm.testqAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.Zero, minResultDone, true);
369             // find offset 1
370             asm.bsfq(cmpResult[0], cmpResult[0]);
371             asm.addq(cmpResult[0], valueKind.getByteCount());
372             // if first result is greater than second, replace it with the second result
373             asm.cmpq(cmpResult[1], cmpResult[0]);
374             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, cmpResult[1], cmpResult[0]);
375             asm.bind(minResultDone);
376             if (charMode(valueKind)) {
377                 // convert byte offset to chars
378                 asm.shrl(cmpResult[1], 1);
379             }
380             // add offset to index
381             asm.addq(index, cmpResult[1]);
382         } else {
383             Label end = new Label();
384             for (int i = 0; i &lt; nVectors; i++) {
385                 asm.bind(vectorFound[i]);
386                 // add static offset
387                 asm.subq(index, getResultIndexDelta(i));
388                 if (i &lt; nVectors - 1) {
389                     asm.jmpb(end);
390                 }
391             }
392             asm.bind(end);
393             // find offset
394             asm.bsfq(cmpResult[0], cmpResult[0]);
395             if (charMode(valueKind)) {
396                 // convert byte offset to chars
397                 asm.shrl(cmpResult[0], 1);
398             }
399             // add offset to index
400             asm.addq(index, cmpResult[0]);
401         }
402         asm.bind(ret);
403     }
404 
405     private boolean searchValuesOnStack(Value[] searchValue) {
406         for (int i = 0; i &lt; nValues; i++) {
407             if (isStackSlot(searchValue[i])) {
408                 return true;
409             }
410         }
411         return false;
412     }
413 
414     private int getResultIndexDelta(int i) {
415         return (((findTwoConsecutive ? i / 2 : i) + 1) * (getVectorSize().getBytes() / valueKind.getByteCount())) + (findTwoConsecutive ? (i &amp; 1) : 0);
416     }
417 
418     private int getVectorOffset(int i) {
419         return arrayBaseOffset - getResultIndexDelta(i) * valueKind.getByteCount();
420     }
421 
422     private void broadcastSearchValue(CompilationResultBuilder crb, AMD64MacroAssembler asm, Register dst, Value srcVal, Register tmpReg, Register tmpVector) {
423         Register src = asRegOrTmpReg(crb, asm, srcVal, tmpReg);
424         if (asm.supports(CPUFeature.AVX)) {
425             VexMoveOp.VMOVD.emit(asm, AVXKind.AVXSize.DWORD, dst, src);
426         } else {
427             asm.movdl(dst, src);
428         }
429         emitBroadcast(asm, getComparisonKind(), dst, tmpVector, getVectorSize());
430     }
431 
432     private static boolean isConstant(Value val) {
433         assert !(val instanceof ConstantValue) || ((ConstantValue) val).isJavaConstant();
434         return val instanceof ConstantValue;
435     }
436 
437     private static JavaConstant asConstant(Value val) {
438         return ((ConstantValue) val).getJavaConstant();
439     }
440 
441     private static Register asRegOrTmpReg(CompilationResultBuilder crb, AMD64MacroAssembler asm, Value val, Register tmpReg) {
442         if (isRegister(val)) {
443             return asRegister(val);
444         } else if (isStackSlot(val)) {
445             asm.movl(tmpReg, (AMD64Address) crb.asAddress(val));
446             return tmpReg;
447         } else {
448             assert isConstant(val);
449             asm.movl(tmpReg, asConstant(val).asInt());
450             return tmpReg;
451         }
452     }
453 
454     private static void emitAlign(CompilationResultBuilder crb, AMD64MacroAssembler asm) {
455         asm.align(crb.target.wordSize * 2);
456     }
457 
458     /**
459      * Fills {@code vecDst} with copies of its lowest byte, word or dword.
460      */
461     private static void emitBroadcast(AMD64MacroAssembler asm, JavaKind kind, Register vecDst, Register vecTmp, AVXKind.AVXSize vectorSize) {
462         switch (kind) {
463             case Byte:
464                 if (asm.supports(CPUFeature.AVX2)) {
465                     VexRMOp.VPBROADCASTB.emit(asm, vectorSize, vecDst, vecDst);
466                 } else if (asm.supports(CPUFeature.AVX)) {
467                     VexRVMOp.VPXOR.emit(asm, vectorSize, vecTmp, vecTmp, vecTmp);
468                     VexRVMOp.VPSHUFB.emit(asm, vectorSize, vecDst, vecDst, vecTmp);
469                 } else if (asm.supports(CPUFeature.SSSE3)) {
470                     asm.pxor(vecTmp, vecTmp);
471                     asm.pshufb(vecDst, vecTmp);
472                 } else { // SSE2
473                     asm.punpcklbw(vecDst, vecDst);
474                     asm.punpcklbw(vecDst, vecDst);
475                     asm.pshufd(vecDst, vecDst, 0);
476                 }
477                 break;
478             case Short:
479             case Char:
480                 if (asm.supports(CPUFeature.AVX2)) {
481                     VexRMOp.VPBROADCASTW.emit(asm, vectorSize, vecDst, vecDst);
482                 } else if (asm.supports(CPUFeature.AVX)) {
483                     VexRMIOp.VPSHUFLW.emit(asm, vectorSize, vecDst, vecDst, 0);
484                     VexRMIOp.VPSHUFD.emit(asm, vectorSize, vecDst, vecDst, 0);
485                 } else { // SSE
486                     asm.pshuflw(vecDst, vecDst, 0);
487                     asm.pshufd(vecDst, vecDst, 0);
488                 }
489                 break;
490             case Int:
491                 if (asm.supports(CPUFeature.AVX2)) {
492                     VexRMOp.VPBROADCASTD.emit(asm, vectorSize, vecDst, vecDst);
493                 } else if (asm.supports(CPUFeature.AVX)) {
494                     VexRMIOp.VPSHUFD.emit(asm, vectorSize, vecDst, vecDst, 0);
495                 } else { // SSE
496                     asm.pshufd(vecDst, vecDst, 0);
497                 }
498                 break;
499             default:
500                 throw new UnsupportedOperationException();
501         }
502     }
503 
504     private void emitVectorCompare(AMD64MacroAssembler asm,
505                     JavaKind kind,
506                     int nVectors,
507                     Register arrayPtr,
508                     Register index,
509                     Register[] vecCmp,
510                     Register[] vecArray,
511                     Register[] cmpResult,
512                     Label[] vectorFound,
513                     boolean shortJmp,
514                     boolean alignedLoad) {
515         // load array contents into vectors
516         for (int i = 0; i &lt; nVectors; i++) {
517             int base = i * nValues;
518             for (int j = 0; j &lt; nValues; j++) {
519                 emitArrayLoad(asm, getVectorSize(), vecArray[base + j], arrayPtr, index, getVectorOffset(nVectors - (i + 1)), alignedLoad);
520             }
521         }
522         // compare all loaded bytes to the search value.
523         // matching bytes are set to 0xff, non-matching bytes are set to 0x00.
524         if (!findTwoConsecutive) {
525             for (int i = 0; i &lt; nVectors; i++) {
526                 int base = i * nValues;
527                 for (int j = 0; j &lt; nValues; j++) {
528                     emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[base + j], vecCmp[j]);
529                     if ((j &amp; 1) == 1) {
530                         emitPOR(asm, getVectorSize(), vecArray[base + j - 1], vecArray[base + j]);
531                     }
532                 }
533                 if (nValues &gt; 2) {
534                     emitPOR(asm, getVectorSize(), vecArray[base], vecArray[base + 2]);
535                 }
536                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[base]);
537                 asm.testlAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.NotZero, vectorFound[nVectors - (i + 1)], shortJmp);
538             }
539         } else {
540             for (int i = 0; i &lt; nVectors; i += 2) {
541                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i], vecCmp[0]);
542                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i + 1], vecCmp[0]);
543                 emitMOVMSK(asm, getVectorSize(), cmpResult[1], vecArray[i]);
544                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[i + 1]);
545                 asm.testlAndJcc(cmpResult[1], cmpResult[1], ConditionFlag.NotZero, vectorFound[nVectors - (i + 1)], shortJmp);
546                 asm.testlAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.NotZero, vectorFound[nVectors - (i + 2)], shortJmp);
547             }
548         }
549     }
550 
551     private void emitArrayLoad(AMD64MacroAssembler asm, AVXKind.AVXSize vectorSize, Register vecDst, Register arrayPtr, Register index, int offset, boolean alignedLoad) {
552         AMD64Address src = new AMD64Address(arrayPtr, index, arrayIndexScale, offset);
553         if (asm.supports(CPUFeature.AVX)) {
554             VexMoveOp loadOp = alignedLoad ? VexMoveOp.VMOVDQA32 : VexMoveOp.VMOVDQU32;
555             loadOp.emit(asm, vectorSize, vecDst, src);
556         } else {
557             // SSE
558             asm.movdqu(vecDst, src);
559         }
560     }
561 
562     /**
563      * Compares all packed bytes/words/dwords in {@code vecArray} to {@code vecCmp}. Matching values
564      * are set to all ones (0xff, 0xffff, ...), non-matching values are set to zero.
565      */
566     private static void emitVectorCompareInst(AMD64MacroAssembler asm, JavaKind kind, AVXKind.AVXSize vectorSize, Register vecArray, Register vecCmp) {
567         switch (kind) {
568             case Byte:
569                 if (asm.supports(CPUFeature.AVX)) {
570                     VexRVMOp.VPCMPEQB.emit(asm, vectorSize, vecArray, vecCmp, vecArray);
571                 } else { // SSE
572                     asm.pcmpeqb(vecArray, vecCmp);
573                 }
574                 break;
575             case Short:
576             case Char:
577                 if (asm.supports(CPUFeature.AVX)) {
578                     VexRVMOp.VPCMPEQW.emit(asm, vectorSize, vecArray, vecCmp, vecArray);
579                 } else { // SSE
580                     asm.pcmpeqw(vecArray, vecCmp);
581                 }
582                 break;
583             case Int:
584                 if (asm.supports(CPUFeature.AVX)) {
585                     VexRVMOp.VPCMPEQD.emit(asm, vectorSize, vecArray, vecCmp, vecArray);
586                 } else { // SSE
587                     asm.pcmpeqd(vecArray, vecCmp);
588                 }
589                 break;
590             default:
591                 throw new UnsupportedOperationException();
592         }
593     }
594 
595     private static void emitPOR(AMD64MacroAssembler asm, AVXKind.AVXSize vectorSize, Register dst, Register vecSrc) {
596         if (asm.supports(CPUFeature.AVX)) {
597             VexRVMOp.VPOR.emit(asm, vectorSize, dst, dst, vecSrc);
598         } else {
599             // SSE
600             asm.por(dst, vecSrc);
601         }
602     }
603 
604     private static void emitMOVMSK(AMD64MacroAssembler asm, AVXKind.AVXSize vectorSize, Register dst, Register vecSrc) {
605         if (asm.supports(CPUFeature.AVX)) {
606             VexRMOp.VPMOVMSKB.emit(asm, vectorSize, dst, vecSrc);
607         } else {
608             // SSE
609             asm.pmovmskb(dst, vecSrc);
610         }
611     }
612 
613     private static OperandSize getOpSize(JavaKind kind) {
614         switch (kind) {
615             case Byte:
616                 return OperandSize.BYTE;
617             case Short:
618             case Char:
619                 return OperandSize.WORD;
620             case Int:
621                 return OperandSize.DWORD;
622             default:
623                 return OperandSize.QWORD;
624         }
625     }
626 
627     private static boolean supportsAVX2(LIRGeneratorTool tool) {
628         return supports(tool, CPUFeature.AVX2);
629     }
630 
631     private static boolean supports(LIRGeneratorTool tool, CPUFeature cpuFeature) {
632         return ((AMD64) tool.target().arch).getFeatures().contains(cpuFeature);
633     }
634 
635     @Override
636     public boolean needsClearUpperVectorRegisters() {
637         return true;
638     }
639 }
    </pre>
  </body>
</html>