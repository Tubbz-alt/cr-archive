<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64ArrayEqualsOp.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64CCall.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.code.ValueUtil.asRegister;
 28 import static jdk.vm.ci.code.ValueUtil.isRegister;
 29 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
 30 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.CONST;
 31 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.ILLEGAL;
 32 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 33 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
 34 
 35 import java.util.Objects;
 36 
 37 import org.graalvm.compiler.asm.Label;
 38 import org.graalvm.compiler.asm.amd64.AMD64Address;
 39 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
 40 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 41 import org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64RMOp;

 42 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp;
 43 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMIOp;
 44 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMOp;
 45 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRVMOp;
 46 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
 47 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 48 import org.graalvm.compiler.asm.amd64.AVXKind;
 49 import org.graalvm.compiler.core.common.LIRKind;
 50 import org.graalvm.compiler.core.common.NumUtil;
 51 import org.graalvm.compiler.lir.ConstantValue;
 52 import org.graalvm.compiler.lir.LIRInstructionClass;
 53 import org.graalvm.compiler.lir.Opcode;
 54 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 55 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 56 
 57 import jdk.vm.ci.amd64.AMD64;
 58 import jdk.vm.ci.amd64.AMD64.CPUFeature;
 59 import jdk.vm.ci.amd64.AMD64Kind;
 60 import jdk.vm.ci.code.Register;
 61 import jdk.vm.ci.meta.JavaConstant;
</pre>
<hr />
<pre>
184                         new Label(),
185         };
186         Label runVectorized = new Label();
187         Label elementWiseLoop = new Label();
188         Label elementWiseFound = new Label();
189         Label elementWiseNotFound = new Label();
190         Label skipBulkVectorLoop = new Label();
191         int vectorSize = getVectorSize().getBytes() / valueKind.getByteCount();
192         int bulkSize = vectorSize * nVectors;
193         JavaKind vectorCompareKind = valueKind;
194         if (findTwoConsecutive) {
195             bulkSize /= 2;
196             vectorCompareKind = byteMode(valueKind) ? JavaKind.Char : JavaKind.Int;
197         }
198         // index = fromIndex + vectorSize (+1 if findTwoConsecutive)
199         // important: this must be the first register manipulation, since fromIndex is
200         // annotated with @Use
201         asm.leaq(index, new AMD64Address(fromIndex, vectorSize + (findTwoConsecutive ? 1 : 0)));
202 
203         // check if vector vector load is in bounds
<span class="line-modified">204         asm.cmpq(index, arrayLength);</span>
<span class="line-removed">205         asm.jccb(AMD64Assembler.ConditionFlag.LessEqual, runVectorized);</span>
206 
207         // search range is smaller than vector size, do element-wise comparison
208 
209         // index = fromIndex (+ 1 if findTwoConsecutive)
210         asm.subq(index, vectorSize);
211         // check if enough array slots remain
<span class="line-modified">212         asm.cmpq(index, arrayLength);</span>
<span class="line-removed">213         asm.jccb(AMD64Assembler.ConditionFlag.GreaterEqual, elementWiseNotFound);</span>
214         // compare one-by-one
215         asm.bind(elementWiseLoop);
216         // check for match
217         OperandSize cmpSize = getOpSize(getComparisonKind());
218         // address = findTwoConsecutive ? array[index - 1] : array[index]
219         AMD64Address arrayAddr = new AMD64Address(arrayPtr, index, arrayIndexScale, arrayBaseOffset - (findTwoConsecutive ? valueKind.getByteCount() : 0));
220         boolean valuesOnStack = searchValuesOnStack(searchValue);
221         if (valuesOnStack) {
222             (cmpSize == OperandSize.BYTE ? AMD64RMOp.MOVB : AMD64RMOp.MOV).emit(asm, cmpSize, cmpResult[0], arrayAddr);
223             for (int i = 0; i &lt; nValues; i++) {
224                 if (isConstant(searchValue[i])) {
225                     int imm = asConstant(searchValue[i]).asInt();
226                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, cmpResult[0], imm);
227                 } else if (isStackSlot(searchValue[i])) {
228                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], (AMD64Address) crb.asAddress(searchValue[i]));
229                 } else {
230                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], asRegister(searchValue[i]));
231                 }

232                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
233             }
234         } else {
235             for (int i = 0; i &lt; nValues; i++) {
236                 if (isConstant(searchValue[i])) {
237                     int imm = asConstant(searchValue[i]).asInt();
238                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, arrayAddr, imm);
239                 } else {
240                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, asRegister(searchValue[i]), arrayAddr);
241                 }

242                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
243             }
244         }
245         // adjust index
246         asm.incrementq(index, 1);
247         // continue loop
<span class="line-modified">248         asm.cmpq(index, arrayLength);</span>
<span class="line-removed">249         asm.jccb(AMD64Assembler.ConditionFlag.Less, elementWiseLoop);</span>
250 
251         asm.bind(elementWiseNotFound);
252         asm.xorq(index, index);
253 
254         if (findTwoConsecutive) {
255             asm.bind(elementWiseFound);
256             asm.decrementq(index, 1);
257         } else {
258             asm.decrementq(index, 1);
259             asm.bind(elementWiseFound);
260         }
261         asm.jmp(ret);
262 
263         // vectorized implementation
264         asm.bind(runVectorized);
265 
266         // move search values to vectors
267         for (int i = 0; i &lt; nValues; i++) {
268             // fill comparison vector with copies of the search value
269             broadcastSearchValue(crb, asm, vecCmp[i], searchValue[i], cmpResult[0], vecArray[0]);
270         }
271 
272         // do one unaligned vector comparison pass and adjust alignment afterwards
273         emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, false);
274 
275         // adjust index to vector size alignment
276         asm.leaq(cmpResult[0], new AMD64Address(arrayPtr, arrayBaseOffset));
277         if (charMode(valueKind)) {
278             asm.shrq(cmpResult[0], 1);
279         }
280         asm.addq(index, cmpResult[0]);
281         // adjust to next lower multiple of vector size
282         asm.andq(index, ~(vectorSize - 1));
283         asm.subq(index, cmpResult[0]);
284         // add bulk size
285         asm.addq(index, bulkSize);
286 
287         // check if there are enough array slots remaining for the bulk loop
<span class="line-modified">288         asm.cmpq(index, arrayLength);</span>
<span class="line-removed">289         asm.jccb(AMD64Assembler.ConditionFlag.Greater, skipBulkVectorLoop);</span>
290 
291         emitAlign(crb, asm);
292         asm.bind(bulkVectorLoop);
293         // memory-aligned bulk comparison
294         emitVectorCompare(asm, vectorCompareKind, nVectors, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, !findTwoConsecutive);
295         // adjust index
296         asm.addq(index, bulkSize);
297         // check if there are enough array slots remaining for the bulk loop
<span class="line-modified">298         asm.cmpq(index, arrayLength);</span>
<span class="line-removed">299         asm.jccb(AMD64Assembler.ConditionFlag.LessEqual, bulkVectorLoop);</span>
300 
301         asm.bind(skipBulkVectorLoop);
302         if ((findTwoConsecutive &amp;&amp; nVectors == 2) || nVectors == 1) {
303             // do last load from end of array
304             asm.movq(index, arrayLength);
305             // compare
306             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
307         } else {
308             // remove bulk offset
309             asm.subq(index, bulkSize);
310             emitAlign(crb, asm);
311             // same loop as bulkVectorLoop, with only one vector
312             asm.bind(singleVectorLoop);
313             // add vector size
314             asm.addq(index, vectorSize);
315             // check if vector load is in bounds
316             asm.cmpq(index, arrayLength);
317             // if load would be over bounds, set the load to the end of the array
318             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, index, arrayLength);
319             // compare
320             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
321             // check if there are enough array slots remaining for the loop
<span class="line-modified">322             asm.cmpq(index, arrayLength);</span>
<span class="line-removed">323             asm.jccb(AMD64Assembler.ConditionFlag.Less, singleVectorLoop);</span>
324         }
325 
326         asm.movl(index, -1);
327         asm.jmpb(ret);
328 
329         if (findTwoConsecutive) {
330             Label vectorFound2Done = new Label();
331 
332             // vectorFound[0] and vectorFound[2] behave like the single-char case
333             asm.bind(vectorFound[2]);
334             // add static offset
335             asm.subq(index, getResultIndexDelta(2));
336             asm.jmpb(vectorFound2Done);
337 
338             asm.bind(vectorFound[0]);
339             // add static offset
340             asm.subq(index, getResultIndexDelta(0));
341             asm.bind(vectorFound2Done);
342             // find offset
343             asm.bsfq(cmpResult[0], cmpResult[0]);
</pre>
<hr />
<pre>
351 
352             Label minResult = new Label();
353             Label minResultDone = new Label();
354 
355             // in vectorFound[1] and vectorFound[3], we have to check the results 0 and 2 as well
356             if (nVectors &gt; 2) {
357                 asm.bind(vectorFound[3]);
358                 // add offset
359                 asm.subq(index, getResultIndexDelta(3));
360                 asm.jmpb(minResult);
361             }
362 
363             asm.bind(vectorFound[1]);
364             // add offset
365             asm.subq(index, getResultIndexDelta(1));
366 
367             asm.bind(minResult);
368             // find offset 0
369             asm.bsfq(cmpResult[1], cmpResult[1]);
370             // check if second result is also a match
<span class="line-modified">371             asm.testq(cmpResult[0], cmpResult[0]);</span>
<span class="line-removed">372             asm.jccb(AMD64Assembler.ConditionFlag.Zero, minResultDone);</span>
373             // find offset 1
374             asm.bsfq(cmpResult[0], cmpResult[0]);
375             asm.addq(cmpResult[0], valueKind.getByteCount());
376             // if first result is greater than second, replace it with the second result
377             asm.cmpq(cmpResult[1], cmpResult[0]);
378             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, cmpResult[1], cmpResult[0]);
379             asm.bind(minResultDone);
380             if (charMode(valueKind)) {
381                 // convert byte offset to chars
382                 asm.shrl(cmpResult[1], 1);
383             }
384             // add offset to index
385             asm.addq(index, cmpResult[1]);
386         } else {
387             Label end = new Label();
388             for (int i = 0; i &lt; nVectors; i++) {
389                 asm.bind(vectorFound[i]);
390                 // add static offset
391                 asm.subq(index, getResultIndexDelta(i));
392                 if (i &lt; nVectors - 1) {
</pre>
<hr />
<pre>
521             int base = i * nValues;
522             for (int j = 0; j &lt; nValues; j++) {
523                 emitArrayLoad(asm, getVectorSize(), vecArray[base + j], arrayPtr, index, getVectorOffset(nVectors - (i + 1)), alignedLoad);
524             }
525         }
526         // compare all loaded bytes to the search value.
527         // matching bytes are set to 0xff, non-matching bytes are set to 0x00.
528         if (!findTwoConsecutive) {
529             for (int i = 0; i &lt; nVectors; i++) {
530                 int base = i * nValues;
531                 for (int j = 0; j &lt; nValues; j++) {
532                     emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[base + j], vecCmp[j]);
533                     if ((j &amp; 1) == 1) {
534                         emitPOR(asm, getVectorSize(), vecArray[base + j - 1], vecArray[base + j]);
535                     }
536                 }
537                 if (nValues &gt; 2) {
538                     emitPOR(asm, getVectorSize(), vecArray[base], vecArray[base + 2]);
539                 }
540                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[base]);
<span class="line-modified">541                 emitJnz(asm, cmpResult[0], vectorFound[nVectors - (i + 1)], shortJmp);</span>
542             }
543         } else {
544             for (int i = 0; i &lt; nVectors; i += 2) {
545                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i], vecCmp[0]);
546                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i + 1], vecCmp[0]);
547                 emitMOVMSK(asm, getVectorSize(), cmpResult[1], vecArray[i]);
548                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[i + 1]);
<span class="line-modified">549                 emitJnz(asm, cmpResult[1], vectorFound[nVectors - (i + 1)], shortJmp);</span>
<span class="line-modified">550                 emitJnz(asm, cmpResult[0], vectorFound[nVectors - (i + 2)], shortJmp);</span>
551             }
552         }
553     }
554 
<span class="line-removed">555     private static void emitJnz(AMD64MacroAssembler asm, Register cond, Label tgt, boolean shortJmp) {</span>
<span class="line-removed">556         asm.testl(cond, cond);</span>
<span class="line-removed">557         if (shortJmp) {</span>
<span class="line-removed">558             asm.jccb(AMD64Assembler.ConditionFlag.NotZero, tgt);</span>
<span class="line-removed">559         } else {</span>
<span class="line-removed">560             asm.jcc(AMD64Assembler.ConditionFlag.NotZero, tgt);</span>
<span class="line-removed">561         }</span>
<span class="line-removed">562     }</span>
<span class="line-removed">563 </span>
564     private void emitArrayLoad(AMD64MacroAssembler asm, AVXKind.AVXSize vectorSize, Register vecDst, Register arrayPtr, Register index, int offset, boolean alignedLoad) {
565         AMD64Address src = new AMD64Address(arrayPtr, index, arrayIndexScale, offset);
566         if (asm.supports(CPUFeature.AVX)) {
567             VexMoveOp loadOp = alignedLoad ? VexMoveOp.VMOVDQA32 : VexMoveOp.VMOVDQU32;
568             loadOp.emit(asm, vectorSize, vecDst, src);
569         } else {
570             // SSE
571             asm.movdqu(vecDst, src);
572         }
573     }
574 
575     /**
576      * Compares all packed bytes/words/dwords in {@code vecArray} to {@code vecCmp}. Matching values
577      * are set to all ones (0xff, 0xffff, ...), non-matching values are set to zero.
578      */
579     private static void emitVectorCompareInst(AMD64MacroAssembler asm, JavaKind kind, AVXKind.AVXSize vectorSize, Register vecArray, Register vecCmp) {
580         switch (kind) {
581             case Byte:
582                 if (asm.supports(CPUFeature.AVX)) {
583                     VexRVMOp.VPCMPEQB.emit(asm, vectorSize, vecArray, vecCmp, vecArray);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.code.ValueUtil.asRegister;
 28 import static jdk.vm.ci.code.ValueUtil.isRegister;
 29 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
 30 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.CONST;
 31 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.ILLEGAL;
 32 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 33 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
 34 
 35 import java.util.Objects;
 36 
 37 import org.graalvm.compiler.asm.Label;
 38 import org.graalvm.compiler.asm.amd64.AMD64Address;
 39 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
 40 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 41 import org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64RMOp;
<span class="line-added"> 42 import org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag;</span>
 43 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp;
 44 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMIOp;
 45 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMOp;
 46 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRVMOp;
 47 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
 48 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 49 import org.graalvm.compiler.asm.amd64.AVXKind;
 50 import org.graalvm.compiler.core.common.LIRKind;
 51 import org.graalvm.compiler.core.common.NumUtil;
 52 import org.graalvm.compiler.lir.ConstantValue;
 53 import org.graalvm.compiler.lir.LIRInstructionClass;
 54 import org.graalvm.compiler.lir.Opcode;
 55 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 56 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 57 
 58 import jdk.vm.ci.amd64.AMD64;
 59 import jdk.vm.ci.amd64.AMD64.CPUFeature;
 60 import jdk.vm.ci.amd64.AMD64Kind;
 61 import jdk.vm.ci.code.Register;
 62 import jdk.vm.ci.meta.JavaConstant;
</pre>
<hr />
<pre>
185                         new Label(),
186         };
187         Label runVectorized = new Label();
188         Label elementWiseLoop = new Label();
189         Label elementWiseFound = new Label();
190         Label elementWiseNotFound = new Label();
191         Label skipBulkVectorLoop = new Label();
192         int vectorSize = getVectorSize().getBytes() / valueKind.getByteCount();
193         int bulkSize = vectorSize * nVectors;
194         JavaKind vectorCompareKind = valueKind;
195         if (findTwoConsecutive) {
196             bulkSize /= 2;
197             vectorCompareKind = byteMode(valueKind) ? JavaKind.Char : JavaKind.Int;
198         }
199         // index = fromIndex + vectorSize (+1 if findTwoConsecutive)
200         // important: this must be the first register manipulation, since fromIndex is
201         // annotated with @Use
202         asm.leaq(index, new AMD64Address(fromIndex, vectorSize + (findTwoConsecutive ? 1 : 0)));
203 
204         // check if vector vector load is in bounds
<span class="line-modified">205         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.LessEqual, runVectorized, true);</span>

206 
207         // search range is smaller than vector size, do element-wise comparison
208 
209         // index = fromIndex (+ 1 if findTwoConsecutive)
210         asm.subq(index, vectorSize);
211         // check if enough array slots remain
<span class="line-modified">212         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.GreaterEqual, elementWiseNotFound, true);</span>

213         // compare one-by-one
214         asm.bind(elementWiseLoop);
215         // check for match
216         OperandSize cmpSize = getOpSize(getComparisonKind());
217         // address = findTwoConsecutive ? array[index - 1] : array[index]
218         AMD64Address arrayAddr = new AMD64Address(arrayPtr, index, arrayIndexScale, arrayBaseOffset - (findTwoConsecutive ? valueKind.getByteCount() : 0));
219         boolean valuesOnStack = searchValuesOnStack(searchValue);
220         if (valuesOnStack) {
221             (cmpSize == OperandSize.BYTE ? AMD64RMOp.MOVB : AMD64RMOp.MOV).emit(asm, cmpSize, cmpResult[0], arrayAddr);
222             for (int i = 0; i &lt; nValues; i++) {
223                 if (isConstant(searchValue[i])) {
224                     int imm = asConstant(searchValue[i]).asInt();
225                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, cmpResult[0], imm);
226                 } else if (isStackSlot(searchValue[i])) {
227                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], (AMD64Address) crb.asAddress(searchValue[i]));
228                 } else {
229                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], asRegister(searchValue[i]));
230                 }
<span class="line-added">231                 // TODO (yz) the preceding cmp instruction may be fused with the following jcc</span>
232                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
233             }
234         } else {
235             for (int i = 0; i &lt; nValues; i++) {
236                 if (isConstant(searchValue[i])) {
237                     int imm = asConstant(searchValue[i]).asInt();
238                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, arrayAddr, imm);
239                 } else {
240                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, asRegister(searchValue[i]), arrayAddr);
241                 }
<span class="line-added">242                 // TODO (yz) the preceding cmp instruction may be fused with the following jcc</span>
243                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
244             }
245         }
246         // adjust index
247         asm.incrementq(index, 1);
248         // continue loop
<span class="line-modified">249         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Less, elementWiseLoop, true);</span>

250 
251         asm.bind(elementWiseNotFound);
252         asm.xorq(index, index);
253 
254         if (findTwoConsecutive) {
255             asm.bind(elementWiseFound);
256             asm.decrementq(index, 1);
257         } else {
258             asm.decrementq(index, 1);
259             asm.bind(elementWiseFound);
260         }
261         asm.jmp(ret);
262 
263         // vectorized implementation
264         asm.bind(runVectorized);
265 
266         // move search values to vectors
267         for (int i = 0; i &lt; nValues; i++) {
268             // fill comparison vector with copies of the search value
269             broadcastSearchValue(crb, asm, vecCmp[i], searchValue[i], cmpResult[0], vecArray[0]);
270         }
271 
272         // do one unaligned vector comparison pass and adjust alignment afterwards
273         emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, false);
274 
275         // adjust index to vector size alignment
276         asm.leaq(cmpResult[0], new AMD64Address(arrayPtr, arrayBaseOffset));
277         if (charMode(valueKind)) {
278             asm.shrq(cmpResult[0], 1);
279         }
280         asm.addq(index, cmpResult[0]);
281         // adjust to next lower multiple of vector size
282         asm.andq(index, ~(vectorSize - 1));
283         asm.subq(index, cmpResult[0]);
284         // add bulk size
285         asm.addq(index, bulkSize);
286 
287         // check if there are enough array slots remaining for the bulk loop
<span class="line-modified">288         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Greater, skipBulkVectorLoop, true);</span>

289 
290         emitAlign(crb, asm);
291         asm.bind(bulkVectorLoop);
292         // memory-aligned bulk comparison
293         emitVectorCompare(asm, vectorCompareKind, nVectors, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, !findTwoConsecutive);
294         // adjust index
295         asm.addq(index, bulkSize);
296         // check if there are enough array slots remaining for the bulk loop
<span class="line-modified">297         asm.cmpqAndJcc(index, arrayLength, ConditionFlag.LessEqual, bulkVectorLoop, true);</span>

298 
299         asm.bind(skipBulkVectorLoop);
300         if ((findTwoConsecutive &amp;&amp; nVectors == 2) || nVectors == 1) {
301             // do last load from end of array
302             asm.movq(index, arrayLength);
303             // compare
304             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
305         } else {
306             // remove bulk offset
307             asm.subq(index, bulkSize);
308             emitAlign(crb, asm);
309             // same loop as bulkVectorLoop, with only one vector
310             asm.bind(singleVectorLoop);
311             // add vector size
312             asm.addq(index, vectorSize);
313             // check if vector load is in bounds
314             asm.cmpq(index, arrayLength);
315             // if load would be over bounds, set the load to the end of the array
316             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, index, arrayLength);
317             // compare
318             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
319             // check if there are enough array slots remaining for the loop
<span class="line-modified">320             asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Less, singleVectorLoop, true);</span>

321         }
322 
323         asm.movl(index, -1);
324         asm.jmpb(ret);
325 
326         if (findTwoConsecutive) {
327             Label vectorFound2Done = new Label();
328 
329             // vectorFound[0] and vectorFound[2] behave like the single-char case
330             asm.bind(vectorFound[2]);
331             // add static offset
332             asm.subq(index, getResultIndexDelta(2));
333             asm.jmpb(vectorFound2Done);
334 
335             asm.bind(vectorFound[0]);
336             // add static offset
337             asm.subq(index, getResultIndexDelta(0));
338             asm.bind(vectorFound2Done);
339             // find offset
340             asm.bsfq(cmpResult[0], cmpResult[0]);
</pre>
<hr />
<pre>
348 
349             Label minResult = new Label();
350             Label minResultDone = new Label();
351 
352             // in vectorFound[1] and vectorFound[3], we have to check the results 0 and 2 as well
353             if (nVectors &gt; 2) {
354                 asm.bind(vectorFound[3]);
355                 // add offset
356                 asm.subq(index, getResultIndexDelta(3));
357                 asm.jmpb(minResult);
358             }
359 
360             asm.bind(vectorFound[1]);
361             // add offset
362             asm.subq(index, getResultIndexDelta(1));
363 
364             asm.bind(minResult);
365             // find offset 0
366             asm.bsfq(cmpResult[1], cmpResult[1]);
367             // check if second result is also a match
<span class="line-modified">368             asm.testqAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.Zero, minResultDone, true);</span>

369             // find offset 1
370             asm.bsfq(cmpResult[0], cmpResult[0]);
371             asm.addq(cmpResult[0], valueKind.getByteCount());
372             // if first result is greater than second, replace it with the second result
373             asm.cmpq(cmpResult[1], cmpResult[0]);
374             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, cmpResult[1], cmpResult[0]);
375             asm.bind(minResultDone);
376             if (charMode(valueKind)) {
377                 // convert byte offset to chars
378                 asm.shrl(cmpResult[1], 1);
379             }
380             // add offset to index
381             asm.addq(index, cmpResult[1]);
382         } else {
383             Label end = new Label();
384             for (int i = 0; i &lt; nVectors; i++) {
385                 asm.bind(vectorFound[i]);
386                 // add static offset
387                 asm.subq(index, getResultIndexDelta(i));
388                 if (i &lt; nVectors - 1) {
</pre>
<hr />
<pre>
517             int base = i * nValues;
518             for (int j = 0; j &lt; nValues; j++) {
519                 emitArrayLoad(asm, getVectorSize(), vecArray[base + j], arrayPtr, index, getVectorOffset(nVectors - (i + 1)), alignedLoad);
520             }
521         }
522         // compare all loaded bytes to the search value.
523         // matching bytes are set to 0xff, non-matching bytes are set to 0x00.
524         if (!findTwoConsecutive) {
525             for (int i = 0; i &lt; nVectors; i++) {
526                 int base = i * nValues;
527                 for (int j = 0; j &lt; nValues; j++) {
528                     emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[base + j], vecCmp[j]);
529                     if ((j &amp; 1) == 1) {
530                         emitPOR(asm, getVectorSize(), vecArray[base + j - 1], vecArray[base + j]);
531                     }
532                 }
533                 if (nValues &gt; 2) {
534                     emitPOR(asm, getVectorSize(), vecArray[base], vecArray[base + 2]);
535                 }
536                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[base]);
<span class="line-modified">537                 asm.testlAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.NotZero, vectorFound[nVectors - (i + 1)], shortJmp);</span>
538             }
539         } else {
540             for (int i = 0; i &lt; nVectors; i += 2) {
541                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i], vecCmp[0]);
542                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i + 1], vecCmp[0]);
543                 emitMOVMSK(asm, getVectorSize(), cmpResult[1], vecArray[i]);
544                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[i + 1]);
<span class="line-modified">545                 asm.testlAndJcc(cmpResult[1], cmpResult[1], ConditionFlag.NotZero, vectorFound[nVectors - (i + 1)], shortJmp);</span>
<span class="line-modified">546                 asm.testlAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.NotZero, vectorFound[nVectors - (i + 2)], shortJmp);</span>
547             }
548         }
549     }
550 









551     private void emitArrayLoad(AMD64MacroAssembler asm, AVXKind.AVXSize vectorSize, Register vecDst, Register arrayPtr, Register index, int offset, boolean alignedLoad) {
552         AMD64Address src = new AMD64Address(arrayPtr, index, arrayIndexScale, offset);
553         if (asm.supports(CPUFeature.AVX)) {
554             VexMoveOp loadOp = alignedLoad ? VexMoveOp.VMOVDQA32 : VexMoveOp.VMOVDQU32;
555             loadOp.emit(asm, vectorSize, vecDst, src);
556         } else {
557             // SSE
558             asm.movdqu(vecDst, src);
559         }
560     }
561 
562     /**
563      * Compares all packed bytes/words/dwords in {@code vecArray} to {@code vecCmp}. Matching values
564      * are set to all ones (0xff, 0xffff, ...), non-matching values are set to zero.
565      */
566     private static void emitVectorCompareInst(AMD64MacroAssembler asm, JavaKind kind, AVXKind.AVXSize vectorSize, Register vecArray, Register vecCmp) {
567         switch (kind) {
568             case Byte:
569                 if (asm.supports(CPUFeature.AVX)) {
570                     VexRVMOp.VPCMPEQB.emit(asm, vectorSize, vecArray, vecCmp, vecArray);
</pre>
</td>
</tr>
</table>
<center><a href="AMD64ArrayEqualsOp.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64CCall.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>