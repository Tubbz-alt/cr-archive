<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64Move.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2011, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  */
  23 
  24 
  25 package org.graalvm.compiler.lir.amd64;
  26 
  27 import static java.lang.Double.doubleToRawLongBits;
  28 import static java.lang.Float.floatToRawIntBits;
  29 import static jdk.vm.ci.code.ValueUtil.asRegister;
  30 import static jdk.vm.ci.code.ValueUtil.isRegister;
  31 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
  32 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag.Equal;
  33 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag.NotEqual;
  34 import static org.graalvm.compiler.core.common.GraalOptions.GeneratePIC;
  35 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.COMPOSITE;
  36 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.CONST;
  37 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.HINT;
  38 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.ILLEGAL;
  39 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
  40 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
  41 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.UNINITIALIZED;
  42 import static org.graalvm.compiler.lir.LIRValueUtil.asJavaConstant;
  43 import static org.graalvm.compiler.lir.LIRValueUtil.isJavaConstant;
  44 
  45 import org.graalvm.compiler.asm.Label;
  46 import org.graalvm.compiler.asm.amd64.AMD64Address;
  47 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
  48 import org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MIOp;
  49 import org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp;
  50 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
  51 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
  52 import org.graalvm.compiler.core.common.CompressEncoding;
  53 import org.graalvm.compiler.core.common.LIRKind;
  54 import org.graalvm.compiler.core.common.NumUtil;
  55 import org.graalvm.compiler.core.common.spi.LIRKindTool;
  56 import org.graalvm.compiler.core.common.type.DataPointerConstant;
  57 import org.graalvm.compiler.debug.GraalError;
  58 import org.graalvm.compiler.lir.LIRFrameState;
  59 import org.graalvm.compiler.lir.LIRInstructionClass;
  60 import org.graalvm.compiler.lir.Opcode;
  61 import org.graalvm.compiler.lir.StandardOp.LoadConstantOp;
  62 import org.graalvm.compiler.lir.StandardOp.NullCheck;
  63 import org.graalvm.compiler.lir.StandardOp.ValueMoveOp;
  64 import org.graalvm.compiler.lir.VirtualStackSlot;
  65 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
  66 import org.graalvm.compiler.options.OptionValues;
  67 
  68 import jdk.vm.ci.amd64.AMD64;
  69 import jdk.vm.ci.amd64.AMD64Kind;
  70 import jdk.vm.ci.code.Register;
  71 import jdk.vm.ci.code.RegisterValue;
  72 import jdk.vm.ci.code.StackSlot;
  73 import jdk.vm.ci.meta.AllocatableValue;
  74 import jdk.vm.ci.meta.Constant;
  75 import jdk.vm.ci.meta.JavaConstant;
  76 import jdk.vm.ci.meta.Value;
  77 
  78 public class AMD64Move {
  79 
  80     private abstract static class AbstractMoveOp extends AMD64LIRInstruction implements ValueMoveOp {
  81         public static final LIRInstructionClass&lt;AbstractMoveOp&gt; TYPE = LIRInstructionClass.create(AbstractMoveOp.class);
  82 
  83         private AMD64Kind moveKind;
  84 
  85         protected AbstractMoveOp(LIRInstructionClass&lt;? extends AbstractMoveOp&gt; c, AMD64Kind moveKind) {
  86             super(c);
  87             this.moveKind = moveKind;
  88         }
  89 
  90         @Override
  91         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
  92             move(moveKind, crb, masm, getResult(), getInput());
  93         }
  94     }
  95 
  96     @Opcode(&quot;MOVE&quot;)
  97     public static final class MoveToRegOp extends AbstractMoveOp {
  98         public static final LIRInstructionClass&lt;MoveToRegOp&gt; TYPE = LIRInstructionClass.create(MoveToRegOp.class);
  99 
 100         @Def({REG, STACK, HINT}) protected AllocatableValue result;
 101         @Use({REG, STACK}) protected AllocatableValue input;
 102 
 103         public MoveToRegOp(AMD64Kind moveKind, AllocatableValue result, AllocatableValue input) {
 104             super(TYPE, moveKind);
 105             this.result = result;
 106             this.input = input;
 107         }
 108 
 109         @Override
 110         public AllocatableValue getInput() {
 111             return input;
 112         }
 113 
 114         @Override
 115         public AllocatableValue getResult() {
 116             return result;
 117         }
 118     }
 119 
 120     @Opcode(&quot;MOVE&quot;)
 121     public static final class MoveFromRegOp extends AbstractMoveOp {
 122         public static final LIRInstructionClass&lt;MoveFromRegOp&gt; TYPE = LIRInstructionClass.create(MoveFromRegOp.class);
 123 
 124         @Def({REG, STACK}) protected AllocatableValue result;
 125         @Use({REG, HINT}) protected AllocatableValue input;
 126 
 127         public MoveFromRegOp(AMD64Kind moveKind, AllocatableValue result, AllocatableValue input) {
 128             super(TYPE, moveKind);
 129             this.result = result;
 130             this.input = input;
 131         }
 132 
 133         @Override
 134         public AllocatableValue getInput() {
 135             return input;
 136         }
 137 
 138         @Override
 139         public AllocatableValue getResult() {
 140             return result;
 141         }
 142     }
 143 
 144     @Opcode(&quot;MOVE&quot;)
 145     public static class MoveFromConstOp extends AMD64LIRInstruction implements LoadConstantOp {
 146         public static final LIRInstructionClass&lt;MoveFromConstOp&gt; TYPE = LIRInstructionClass.create(MoveFromConstOp.class);
 147 
 148         @Def({REG, STACK}) protected AllocatableValue result;
 149         private final JavaConstant input;
 150 
 151         public MoveFromConstOp(AllocatableValue result, JavaConstant input) {
 152             super(TYPE);
 153             this.result = result;
 154             this.input = input;
 155         }
 156 
 157         @Override
 158         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 159             if (isRegister(result)) {
 160                 const2reg(crb, masm, asRegister(result), input, (AMD64Kind) result.getPlatformKind());
 161             } else {
 162                 assert isStackSlot(result);
 163                 const2stack(crb, masm, result, input);
 164             }
 165         }
 166 
 167         @Override
 168         public Constant getConstant() {
 169             return input;
 170         }
 171 
 172         @Override
 173         public AllocatableValue getResult() {
 174             return result;
 175         }
 176     }
 177 
 178     @Opcode(&quot;STACKMOVE&quot;)
 179     public static final class AMD64StackMove extends AMD64LIRInstruction implements ValueMoveOp {
 180         public static final LIRInstructionClass&lt;AMD64StackMove&gt; TYPE = LIRInstructionClass.create(AMD64StackMove.class);
 181 
 182         @Def({STACK}) protected AllocatableValue result;
 183         @Use({STACK, HINT}) protected AllocatableValue input;
 184         @Alive({OperandFlag.STACK, OperandFlag.UNINITIALIZED}) private AllocatableValue backupSlot;
 185 
 186         private Register scratch;
 187 
 188         public AMD64StackMove(AllocatableValue result, AllocatableValue input, Register scratch, AllocatableValue backupSlot) {
 189             super(TYPE);
 190             this.result = result;
 191             this.input = input;
 192             this.backupSlot = backupSlot;
 193             this.scratch = scratch;
 194         }
 195 
 196         @Override
 197         public AllocatableValue getInput() {
 198             return input;
 199         }
 200 
 201         @Override
 202         public AllocatableValue getResult() {
 203             return result;
 204         }
 205 
 206         public Register getScratchRegister() {
 207             return scratch;
 208         }
 209 
 210         public AllocatableValue getBackupSlot() {
 211             return backupSlot;
 212         }
 213 
 214         @Override
 215         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 216             AMD64Kind backupKind = (AMD64Kind) backupSlot.getPlatformKind();
 217             if (backupKind.isXMM()) {
 218                 // graal doesn&#39;t use vector values, so it&#39;s safe to backup using DOUBLE
 219                 backupKind = AMD64Kind.DOUBLE;
 220             }
 221 
 222             // backup scratch register
 223             reg2stack(backupKind, crb, masm, backupSlot, scratch);
 224             // move stack slot
 225             stack2reg((AMD64Kind) getInput().getPlatformKind(), crb, masm, scratch, getInput());
 226             reg2stack((AMD64Kind) getResult().getPlatformKind(), crb, masm, getResult(), scratch);
 227             // restore scratch register
 228             stack2reg(backupKind, crb, masm, scratch, backupSlot);
 229         }
 230     }
 231 
 232     @Opcode(&quot;MULTISTACKMOVE&quot;)
 233     public static final class AMD64MultiStackMove extends AMD64LIRInstruction {
 234         public static final LIRInstructionClass&lt;AMD64MultiStackMove&gt; TYPE = LIRInstructionClass.create(AMD64MultiStackMove.class);
 235 
 236         @Def({STACK}) protected AllocatableValue[] results;
 237         @Use({STACK}) protected Value[] inputs;
 238         @Alive({OperandFlag.STACK, OperandFlag.UNINITIALIZED}) private AllocatableValue backupSlot;
 239 
 240         private Register scratch;
 241 
 242         public AMD64MultiStackMove(AllocatableValue[] results, Value[] inputs, Register scratch, AllocatableValue backupSlot) {
 243             super(TYPE);
 244             this.results = results;
 245             this.inputs = inputs;
 246             this.backupSlot = backupSlot;
 247             this.scratch = scratch;
 248         }
 249 
 250         @Override
 251         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 252             AMD64Kind backupKind = (AMD64Kind) backupSlot.getPlatformKind();
 253             if (backupKind.isXMM()) {
 254                 // graal doesn&#39;t use vector values, so it&#39;s safe to backup using DOUBLE
 255                 backupKind = AMD64Kind.DOUBLE;
 256             }
 257 
 258             // backup scratch register
 259             move(backupKind, crb, masm, backupSlot, scratch.asValue(backupSlot.getValueKind()));
 260             for (int i = 0; i &lt; results.length; i++) {
 261                 Value input = inputs[i];
 262                 AllocatableValue result = results[i];
 263                 // move stack slot
 264                 move((AMD64Kind) input.getPlatformKind(), crb, masm, scratch.asValue(input.getValueKind()), input);
 265                 move((AMD64Kind) result.getPlatformKind(), crb, masm, result, scratch.asValue(result.getValueKind()));
 266             }
 267             // restore scratch register
 268             move(backupKind, crb, masm, scratch.asValue(backupSlot.getValueKind()), backupSlot);
 269         }
 270     }
 271 
 272     @Opcode(&quot;STACKMOVE&quot;)
 273     public static final class AMD64PushPopStackMove extends AMD64LIRInstruction implements ValueMoveOp {
 274         public static final LIRInstructionClass&lt;AMD64PushPopStackMove&gt; TYPE = LIRInstructionClass.create(AMD64PushPopStackMove.class);
 275 
 276         @Def({STACK}) protected AllocatableValue result;
 277         @Use({STACK, HINT}) protected AllocatableValue input;
 278         private final OperandSize size;
 279 
 280         public AMD64PushPopStackMove(OperandSize size, AllocatableValue result, AllocatableValue input) {
 281             super(TYPE);
 282             this.result = result;
 283             this.input = input;
 284             this.size = size;
 285         }
 286 
 287         @Override
 288         public AllocatableValue getInput() {
 289             return input;
 290         }
 291 
 292         @Override
 293         public AllocatableValue getResult() {
 294             return result;
 295         }
 296 
 297         @Override
 298         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 299             AMD64MOp.PUSH.emit(masm, size, (AMD64Address) crb.asAddress(input));
 300             AMD64MOp.POP.emit(masm, size, (AMD64Address) crb.asAddress(result));
 301         }
 302     }
 303 
 304     public static final class LeaOp extends AMD64LIRInstruction {
 305         public static final LIRInstructionClass&lt;LeaOp&gt; TYPE = LIRInstructionClass.create(LeaOp.class);
 306 
 307         @Def({REG}) protected AllocatableValue result;
 308         @Use({COMPOSITE, UNINITIALIZED}) protected AMD64AddressValue address;
 309         private final OperandSize size;
 310 
 311         public LeaOp(AllocatableValue result, AMD64AddressValue address, OperandSize size) {
 312             super(TYPE);
 313             this.result = result;
 314             this.address = address;
 315             this.size = size;
 316         }
 317 
 318         @Override
 319         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 320             if (size == OperandSize.QWORD) {
 321                 masm.leaq(asRegister(result, AMD64Kind.QWORD), address.toAddress());
 322             } else {
 323                 assert size == OperandSize.DWORD;
 324                 masm.lead(asRegister(result, AMD64Kind.DWORD), address.toAddress());
 325             }
 326         }
 327     }
 328 
 329     public static final class LeaDataOp extends AMD64LIRInstruction {
 330         public static final LIRInstructionClass&lt;LeaDataOp&gt; TYPE = LIRInstructionClass.create(LeaDataOp.class);
 331 
 332         @Def({REG}) protected AllocatableValue result;
 333         private final DataPointerConstant data;
 334 
 335         public LeaDataOp(AllocatableValue result, DataPointerConstant data) {
 336             super(TYPE);
 337             this.result = result;
 338             this.data = data;
 339         }
 340 
 341         @Override
 342         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 343             masm.leaq(asRegister(result), (AMD64Address) crb.recordDataReferenceInCode(data));
 344         }
 345     }
 346 
 347     public static final class StackLeaOp extends AMD64LIRInstruction {
 348         public static final LIRInstructionClass&lt;StackLeaOp&gt; TYPE = LIRInstructionClass.create(StackLeaOp.class);
 349 
 350         @Def({REG}) protected AllocatableValue result;
 351         @Use({STACK, UNINITIALIZED}) protected AllocatableValue slot;
 352 
 353         public StackLeaOp(AllocatableValue result, AllocatableValue slot) {
 354             super(TYPE);
 355             this.result = result;
 356             this.slot = slot;
 357             assert slot instanceof VirtualStackSlot || slot instanceof StackSlot;
 358         }
 359 
 360         @Override
 361         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 362             masm.leaq(asRegister(result, AMD64Kind.QWORD), (AMD64Address) crb.asAddress(slot));
 363         }
 364     }
 365 
 366     public static final class MembarOp extends AMD64LIRInstruction {
 367         public static final LIRInstructionClass&lt;MembarOp&gt; TYPE = LIRInstructionClass.create(MembarOp.class);
 368 
 369         private final int barriers;
 370 
 371         public MembarOp(final int barriers) {
 372             super(TYPE);
 373             this.barriers = barriers;
 374         }
 375 
 376         @Override
 377         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 378             masm.membar(barriers);
 379         }
 380     }
 381 
 382     public static final class NullCheckOp extends AMD64LIRInstruction implements NullCheck {
 383         public static final LIRInstructionClass&lt;NullCheckOp&gt; TYPE = LIRInstructionClass.create(NullCheckOp.class);
 384 
 385         @Use({COMPOSITE}) protected AMD64AddressValue address;
 386         @State protected LIRFrameState state;
 387 
 388         public NullCheckOp(AMD64AddressValue address, LIRFrameState state) {
 389             super(TYPE);
 390             this.address = address;
 391             this.state = state;
 392         }
 393 
 394         @Override
 395         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 396             crb.recordImplicitException(masm.position(), state);
 397             masm.nullCheck(address.toAddress());
 398         }
 399 
 400         @Override
 401         public Value getCheckedValue() {
 402             return address.base;
 403         }
 404 
 405         @Override
 406         public LIRFrameState getState() {
 407             return state;
 408         }
 409     }
 410 
 411     @Opcode(&quot;CAS&quot;)
 412     public static final class CompareAndSwapOp extends AMD64LIRInstruction {
 413         public static final LIRInstructionClass&lt;CompareAndSwapOp&gt; TYPE = LIRInstructionClass.create(CompareAndSwapOp.class);
 414 
 415         private final AMD64Kind accessKind;
 416 
 417         @Def protected AllocatableValue result;
 418         @Use({COMPOSITE}) protected AMD64AddressValue address;
 419         @Use protected AllocatableValue cmpValue;
 420         @Use protected AllocatableValue newValue;
 421 
 422         public CompareAndSwapOp(AMD64Kind accessKind, AllocatableValue result, AMD64AddressValue address, AllocatableValue cmpValue, AllocatableValue newValue) {
 423             super(TYPE);
 424             this.accessKind = accessKind;
 425             this.result = result;
 426             this.address = address;
 427             this.cmpValue = cmpValue;
 428             this.newValue = newValue;
 429         }
 430 
 431         @Override
 432         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 433             assert asRegister(cmpValue).equals(AMD64.rax) &amp;&amp; asRegister(result).equals(AMD64.rax);
 434 
 435             if (crb.target.isMP) {
 436                 masm.lock();
 437             }
 438             switch (accessKind) {
 439                 case BYTE:
 440                     masm.cmpxchgb(asRegister(newValue), address.toAddress());
 441                     break;
 442                 case WORD:
 443                     masm.cmpxchgw(asRegister(newValue), address.toAddress());
 444                     break;
 445                 case DWORD:
 446                     masm.cmpxchgl(asRegister(newValue), address.toAddress());
 447                     break;
 448                 case QWORD:
 449                     masm.cmpxchgq(asRegister(newValue), address.toAddress());
 450                     break;
 451                 default:
 452                     throw GraalError.shouldNotReachHere();
 453             }
 454         }
 455     }
 456 
 457     @Opcode(&quot;ATOMIC_READ_AND_ADD&quot;)
 458     public static final class AtomicReadAndAddOp extends AMD64LIRInstruction {
 459         public static final LIRInstructionClass&lt;AtomicReadAndAddOp&gt; TYPE = LIRInstructionClass.create(AtomicReadAndAddOp.class);
 460 
 461         private final AMD64Kind accessKind;
 462 
 463         @Def protected AllocatableValue result;
 464         @Alive({COMPOSITE}) protected AMD64AddressValue address;
 465         @Use protected AllocatableValue delta;
 466 
 467         public AtomicReadAndAddOp(AMD64Kind accessKind, AllocatableValue result, AMD64AddressValue address, AllocatableValue delta) {
 468             super(TYPE);
 469             this.accessKind = accessKind;
 470             this.result = result;
 471             this.address = address;
 472             this.delta = delta;
 473         }
 474 
 475         @Override
 476         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 477             move(accessKind, crb, masm, result, delta);
 478             if (crb.target.isMP) {
 479                 masm.lock();
 480             }
 481             switch (accessKind) {
 482                 case BYTE:
 483                     masm.xaddb(address.toAddress(), asRegister(result));
 484                     break;
 485                 case WORD:
 486                     masm.xaddw(address.toAddress(), asRegister(result));
 487                     break;
 488                 case DWORD:
 489                     masm.xaddl(address.toAddress(), asRegister(result));
 490                     break;
 491                 case QWORD:
 492                     masm.xaddq(address.toAddress(), asRegister(result));
 493                     break;
 494                 default:
 495                     throw GraalError.shouldNotReachHere();
 496             }
 497         }
 498     }
 499 
 500     @Opcode(&quot;ATOMIC_READ_AND_WRITE&quot;)
 501     public static final class AtomicReadAndWriteOp extends AMD64LIRInstruction {
 502         public static final LIRInstructionClass&lt;AtomicReadAndWriteOp&gt; TYPE = LIRInstructionClass.create(AtomicReadAndWriteOp.class);
 503 
 504         private final AMD64Kind accessKind;
 505 
 506         @Def protected AllocatableValue result;
 507         @Alive({COMPOSITE}) protected AMD64AddressValue address;
 508         @Use protected AllocatableValue newValue;
 509 
 510         public AtomicReadAndWriteOp(AMD64Kind accessKind, AllocatableValue result, AMD64AddressValue address, AllocatableValue newValue) {
 511             super(TYPE);
 512             this.accessKind = accessKind;
 513             this.result = result;
 514             this.address = address;
 515             this.newValue = newValue;
 516         }
 517 
 518         @Override
 519         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 520             move(accessKind, crb, masm, result, newValue);
 521             switch (accessKind) {
 522                 case BYTE:
 523                     masm.xchgb(asRegister(result), address.toAddress());
 524                     break;
 525                 case WORD:
 526                     masm.xchgw(asRegister(result), address.toAddress());
 527                     break;
 528                 case DWORD:
 529                     masm.xchgl(asRegister(result), address.toAddress());
 530                     break;
 531                 case QWORD:
 532                     masm.xchgq(asRegister(result), address.toAddress());
 533                     break;
 534                 default:
 535                     throw GraalError.shouldNotReachHere();
 536             }
 537         }
 538     }
 539 
 540     public static void move(CompilationResultBuilder crb, AMD64MacroAssembler masm, Value result, Value input) {
 541         move((AMD64Kind) result.getPlatformKind(), crb, masm, result, input);
 542     }
 543 
 544     public static void move(AMD64Kind moveKind, CompilationResultBuilder crb, AMD64MacroAssembler masm, Value result, Value input) {
 545         if (isRegister(input)) {
 546             if (isRegister(result)) {
 547                 reg2reg(moveKind, masm, result, input);
 548             } else if (isStackSlot(result)) {
 549                 reg2stack(moveKind, crb, masm, result, asRegister(input));
 550             } else {
 551                 throw GraalError.shouldNotReachHere();
 552             }
 553         } else if (isStackSlot(input)) {
 554             if (isRegister(result)) {
 555                 stack2reg(moveKind, crb, masm, asRegister(result), input);
 556             } else {
 557                 throw GraalError.shouldNotReachHere();
 558             }
 559         } else if (isJavaConstant(input)) {
 560             if (isRegister(result)) {
 561                 const2reg(crb, masm, asRegister(result), asJavaConstant(input), moveKind);
 562             } else if (isStackSlot(result)) {
 563                 const2stack(crb, masm, result, asJavaConstant(input));
 564             } else {
 565                 throw GraalError.shouldNotReachHere();
 566             }
 567         } else {
 568             throw GraalError.shouldNotReachHere();
 569         }
 570     }
 571 
 572     private static void reg2reg(AMD64Kind kind, AMD64MacroAssembler masm, Value result, Value input) {
 573         if (asRegister(input).equals(asRegister(result))) {
 574             return;
 575         }
 576         assert asRegister(result).getRegisterCategory().equals(asRegister(input).getRegisterCategory());
 577         switch (kind) {
 578             case BYTE:
 579             case WORD:
 580             case DWORD:
 581                 masm.movl(asRegister(result), asRegister(input));
 582                 break;
 583             case QWORD:
 584                 masm.movq(asRegister(result), asRegister(input));
 585                 break;
 586             case SINGLE:
 587                 masm.movflt(asRegister(result, AMD64Kind.SINGLE), asRegister(input, AMD64Kind.SINGLE));
 588                 break;
 589             case DOUBLE:
 590                 masm.movdbl(asRegister(result, AMD64Kind.DOUBLE), asRegister(input, AMD64Kind.DOUBLE));
 591                 break;
 592             default:
 593                 throw GraalError.shouldNotReachHere(&quot;kind=&quot; + kind + &quot; input=&quot; + input + &quot; result=&quot; + result);
 594         }
 595     }
 596 
 597     public static void reg2stack(AMD64Kind kind, CompilationResultBuilder crb, AMD64MacroAssembler masm, Value result, Register input) {
 598         AMD64Address dest = (AMD64Address) crb.asAddress(result);
 599         switch (kind) {
 600             case BYTE:
 601                 masm.movb(dest, input);
 602                 break;
 603             case WORD:
 604                 masm.movw(dest, input);
 605                 break;
 606             case DWORD:
 607                 masm.movl(dest, input);
 608                 break;
 609             case QWORD:
 610                 masm.movq(dest, input);
 611                 break;
 612             case SINGLE:
 613                 masm.movflt(dest, input);
 614                 break;
 615             case DOUBLE:
 616                 masm.movsd(dest, input);
 617                 break;
 618             default:
 619                 throw GraalError.shouldNotReachHere(&quot;kind=&quot; + kind + &quot; input=&quot; + input + &quot; result=&quot; + result);
 620         }
 621     }
 622 
 623     public static void stack2reg(AMD64Kind kind, CompilationResultBuilder crb, AMD64MacroAssembler masm, Register result, Value input) {
 624         AMD64Address src = (AMD64Address) crb.asAddress(input);
 625         switch (kind) {
 626             case BYTE:
 627                 masm.movsbl(result, src);
 628                 break;
 629             case WORD:
 630                 masm.movswl(result, src);
 631                 break;
 632             case DWORD:
 633                 masm.movl(result, src);
 634                 break;
 635             case QWORD:
 636                 masm.movq(result, src);
 637                 break;
 638             case SINGLE:
 639                 masm.movflt(result, src);
 640                 break;
 641             case DOUBLE:
 642                 masm.movdbl(result, src);
 643                 break;
 644             default:
 645                 throw GraalError.shouldNotReachHere(&quot;kind=&quot; + kind + &quot; input=&quot; + input + &quot; result=&quot; + result);
 646         }
 647     }
 648 
 649     public static void const2reg(CompilationResultBuilder crb, AMD64MacroAssembler masm, Register result, JavaConstant input, AMD64Kind moveKind) {
 650         /*
 651          * Note: we use the kind of the input operand (and not the kind of the result operand)
 652          * because they don&#39;t match in all cases. For example, an object constant can be loaded to a
 653          * long register when unsafe casts occurred (e.g., for a write barrier where arithmetic
 654          * operations are then performed on the pointer).
 655          */
 656         switch (input.getJavaKind().getStackKind()) {
 657             case Int:
 658                 // Do not optimize with an XOR as this instruction may be between
 659                 // a CMP and a Jcc in which case the XOR will modify the condition
 660                 // flags and interfere with the Jcc.
 661                 masm.movl(result, input.asInt());
 662 
 663                 break;
 664             case Long:
 665                 // Do not optimize with an XOR as this instruction may be between
 666                 // a CMP and a Jcc in which case the XOR will modify the condition
 667                 // flags and interfere with the Jcc.
 668                 if (input.asLong() == (int) input.asLong()) {
 669                     // Sign extended to long
 670                     masm.movslq(result, (int) input.asLong());
 671                 } else if ((input.asLong() &amp; 0xFFFFFFFFL) == input.asLong()) {
 672                     // Zero extended to long
 673                     masm.movl(result, (int) input.asLong());
 674                 } else {
 675                     masm.movq(result, input.asLong());
 676                 }
 677                 break;
 678             case Float:
 679                 // This is *not* the same as &#39;constant == 0.0f&#39; in the case where constant is -0.0f
 680                 if (Float.floatToRawIntBits(input.asFloat()) == Float.floatToRawIntBits(0.0f)) {
 681                     masm.xorps(result, result);
 682                 } else {
 683                     masm.movflt(result, (AMD64Address) crb.asFloatConstRef(input));
 684                 }
 685                 break;
 686             case Double:
 687                 // This is *not* the same as &#39;constant == 0.0d&#39; in the case where constant is -0.0d
 688                 if (Double.doubleToRawLongBits(input.asDouble()) == Double.doubleToRawLongBits(0.0d)) {
 689                     masm.xorpd(result, result);
 690                 } else {
 691                     masm.movdbl(result, (AMD64Address) crb.asDoubleConstRef(input));
 692                 }
 693                 break;
 694             case Object:
 695                 assert moveKind != null : &quot;a nun-null moveKind is required for loading an object constant&quot;;
 696                 // Do not optimize with an XOR as this instruction may be between
 697                 // a CMP and a Jcc in which case the XOR will modify the condition
 698                 // flags and interfere with the Jcc.
 699                 if (input.isNull()) {
 700                     if (moveKind == AMD64Kind.QWORD &amp;&amp; crb.mustReplaceWithUncompressedNullRegister(input)) {
 701                         masm.movq(result, crb.uncompressedNullRegister);
 702                     } else {
 703                         // Upper bits will be zeroed so this also works for narrow oops
 704                         masm.movslq(result, 0);
 705                     }
 706                 } else {
 707                     if (crb.target.inlineObjects) {
 708                         crb.recordInlineDataInCode(input);
 709                         if (moveKind == AMD64Kind.DWORD) {
 710                             // Support for narrow oops
 711                             masm.movl(result, 0xDEADDEAD, true);
 712                         } else {
 713                             masm.movq(result, 0xDEADDEADDEADDEADL, true);
 714                         }
 715                     } else {
 716                         if (moveKind == AMD64Kind.DWORD) {
 717                             // Support for narrow oops
 718                             masm.movl(result, (AMD64Address) crb.recordDataReferenceInCode(input, 0));
 719                         } else {
 720                             masm.movq(result, (AMD64Address) crb.recordDataReferenceInCode(input, 0));
 721                         }
 722                     }
 723                 }
 724                 break;
 725             default:
 726                 throw GraalError.shouldNotReachHere();
 727         }
 728     }
 729 
 730     public static boolean canMoveConst2Stack(JavaConstant input) {
 731         switch (input.getJavaKind().getStackKind()) {
 732             case Int:
 733                 break;
 734             case Long:
 735                 break;
 736             case Float:
 737                 break;
 738             case Double:
 739                 break;
 740             case Object:
 741                 if (input.isNull()) {
 742                     return true;
 743                 } else {
 744                     return false;
 745                 }
 746             default:
 747                 return false;
 748         }
 749         return true;
 750     }
 751 
 752     public static void const2stack(CompilationResultBuilder crb, AMD64MacroAssembler masm, Value result, JavaConstant input) {
 753         AMD64Address dest = (AMD64Address) crb.asAddress(result);
 754         final long imm;
 755         switch (input.getJavaKind().getStackKind()) {
 756             case Int:
 757                 imm = input.asInt();
 758                 break;
 759             case Long:
 760                 imm = input.asLong();
 761                 break;
 762             case Float:
 763                 imm = floatToRawIntBits(input.asFloat());
 764                 break;
 765             case Double:
 766                 imm = doubleToRawLongBits(input.asDouble());
 767                 break;
 768             case Object:
 769                 if (input.isNull()) {
 770                     if (crb.mustReplaceWithUncompressedNullRegister(input)) {
 771                         masm.movq(dest, crb.uncompressedNullRegister);
 772                         return;
 773                     }
 774                     imm = 0;
 775                 } else {
 776                     throw GraalError.shouldNotReachHere(&quot;Non-null object constants must be in a register&quot;);
 777                 }
 778                 break;
 779             default:
 780                 throw GraalError.shouldNotReachHere();
 781         }
 782 
 783         switch ((AMD64Kind) result.getPlatformKind()) {
 784             case BYTE:
 785                 assert NumUtil.isByte(imm) : &quot;Is not in byte range: &quot; + imm;
 786                 AMD64MIOp.MOVB.emit(masm, OperandSize.BYTE, dest, (int) imm);
 787                 break;
 788             case WORD:
 789                 assert NumUtil.isShort(imm) : &quot;Is not in short range: &quot; + imm;
 790                 AMD64MIOp.MOV.emit(masm, OperandSize.WORD, dest, (int) imm);
 791                 break;
 792             case DWORD:
 793             case SINGLE:
 794                 assert NumUtil.isInt(imm) : &quot;Is not in int range: &quot; + imm;
 795                 masm.movl(dest, (int) imm);
 796                 break;
 797             case QWORD:
 798             case DOUBLE:
 799                 masm.movlong(dest, imm);
 800                 break;
 801             default:
 802                 throw GraalError.shouldNotReachHere(&quot;Unknown result Kind: &quot; + result.getPlatformKind());
 803         }
 804     }
 805 
 806     public abstract static class PointerCompressionOp extends AMD64LIRInstruction {
 807         protected final LIRKindTool lirKindTool;
 808         protected final CompressEncoding encoding;
 809         protected final boolean nonNull;
 810 
 811         @Def({REG, HINT}) private AllocatableValue result;
 812         @Use({REG, CONST}) private Value input;
 813         @Alive({REG, ILLEGAL, UNINITIALIZED}) private AllocatableValue baseRegister;
 814 
 815         protected PointerCompressionOp(LIRInstructionClass&lt;? extends PointerCompressionOp&gt; type,
 816                         AllocatableValue result,
 817                         Value input,
 818                         AllocatableValue baseRegister,
 819                         CompressEncoding encoding,
 820                         boolean nonNull,
 821                         LIRKindTool lirKindTool) {
 822 
 823             super(type);
 824             this.result = result;
 825             this.input = input;
 826             this.baseRegister = baseRegister;
 827             this.encoding = encoding;
 828             this.nonNull = nonNull;
 829             this.lirKindTool = lirKindTool;
 830         }
 831 
 832         public static boolean hasBase(OptionValues options, CompressEncoding encoding) {
 833             return GeneratePIC.getValue(options) || encoding.hasBase();
 834         }
 835 
 836         public final Value getInput() {
 837             return input;
 838         }
 839 
 840         public final AllocatableValue getResult() {
 841             return result;
 842         }
 843 
 844         protected final Register getResultRegister() {
 845             return asRegister(result);
 846         }
 847 
 848         protected final Register getBaseRegister(CompilationResultBuilder crb) {
 849             return hasBase(crb.getOptions(), encoding) ? asRegister(baseRegister) : Register.None;
 850         }
 851 
 852         protected final int getShift() {
 853             return encoding.getShift();
 854         }
 855 
 856         /**
 857          * Emits code to move {@linkplain #getInput input} to {@link #getResult result}.
 858          */
 859         protected final void move(LIRKind kind, CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 860             AMD64Move.move((AMD64Kind) kind.getPlatformKind(), crb, masm, result, input);
 861         }
 862 
 863         /**
 864          * Emits code to uncompress the compressed oop in {@code inputAndResultReg} by left shifting
 865          * it {@code shift} bits, adding it to {@code baseReg} and storing the result back in
 866          * {@code inputAndResultReg}.
 867          */
 868         public static void emitUncompressWithBaseRegister(AMD64MacroAssembler masm, Register inputAndResultReg, Register baseReg, int shift, boolean preserveFlagsRegister) {
 869             emitUncompressWithBaseRegister(masm, inputAndResultReg, baseReg, inputAndResultReg, shift, preserveFlagsRegister);
 870         }
 871 
 872         /**
 873          * Emits code to uncompress the compressed oop in {@code inputReg} by left shifting it
 874          * {@code shift} bits, adding it to {@code baseReg} and storing the result in
 875          * {@code resultReg}.
 876          */
 877         public static void emitUncompressWithBaseRegister(AMD64MacroAssembler masm, Register resultReg, Register baseReg, Register inputReg, int shift, boolean preserveFlagsRegister) {
 878             assert !baseReg.equals(Register.None) || shift != 0 : &quot;compression not enabled&quot;;
 879             if (Scale.isScaleShiftSupported(shift)) {
 880                 AMD64Address.Scale scale = AMD64Address.Scale.fromShift(shift);
 881                 masm.leaq(resultReg, new AMD64Address(baseReg, inputReg, scale));
 882             } else {
 883                 if (preserveFlagsRegister) {
 884                     throw GraalError.shouldNotReachHere(&quot;No valid flag-effect-free instruction available to uncompress oop&quot;);
 885                 }
 886                 if (!resultReg.equals(inputReg)) {
 887                     masm.movq(resultReg, inputReg);
 888                 }
 889                 masm.shlq(resultReg, shift);
 890                 masm.addq(resultReg, baseReg);
 891             }
 892         }
 893     }
 894 
 895     public static class CompressPointerOp extends PointerCompressionOp {
 896         public static final LIRInstructionClass&lt;CompressPointerOp&gt; TYPE = LIRInstructionClass.create(CompressPointerOp.class);
 897 
 898         public CompressPointerOp(AllocatableValue result, Value input, AllocatableValue baseRegister, CompressEncoding encoding, boolean nonNull, LIRKindTool lirKindTool) {
 899             this(TYPE, result, input, baseRegister, encoding, nonNull, lirKindTool);
 900         }
 901 
 902         private CompressPointerOp(LIRInstructionClass&lt;? extends PointerCompressionOp&gt; type, AllocatableValue result, Value input,
 903                         AllocatableValue baseRegister, CompressEncoding encoding, boolean nonNull, LIRKindTool lirKindTool) {
 904 
 905             super(type, result, input, baseRegister, encoding, nonNull, lirKindTool);
 906         }
 907 
 908         @Override
 909         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 910             move(lirKindTool.getObjectKind(), crb, masm);
 911 
 912             final Register resReg = getResultRegister();
 913             final Register baseReg = getBaseRegister(crb);
 914             if (!baseReg.equals(Register.None)) {
 915                 if (!nonNull) {
 916                     masm.testq(resReg, resReg);
 917                     masm.cmovq(Equal, resReg, baseReg);
 918                 }
 919                 masm.subq(resReg, baseReg);
 920             }
 921 
 922             int shift = getShift();
 923             if (shift != 0) {
 924                 masm.shrq(resReg, shift);
 925             }
 926         }
 927     }
 928 
 929     public static class UncompressPointerOp extends PointerCompressionOp {
 930         public static final LIRInstructionClass&lt;UncompressPointerOp&gt; TYPE = LIRInstructionClass.create(UncompressPointerOp.class);
 931 
 932         public UncompressPointerOp(AllocatableValue result, Value input, AllocatableValue baseRegister, CompressEncoding encoding, boolean nonNull, LIRKindTool lirKindTool) {
 933             this(TYPE, result, input, baseRegister, encoding, nonNull, lirKindTool);
 934         }
 935 
 936         private UncompressPointerOp(LIRInstructionClass&lt;? extends PointerCompressionOp&gt; type, AllocatableValue result, Value input,
 937                         AllocatableValue baseRegister, CompressEncoding encoding, boolean nonNull, LIRKindTool lirKindTool) {
 938             super(type, result, input, baseRegister, encoding, nonNull, lirKindTool);
 939         }
 940 
 941         @Override
 942         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 943             Register baseReg = getBaseRegister(crb);
 944             int shift = getShift();
 945             Register resReg = getResultRegister();
 946             if (nonNull &amp;&amp; !baseReg.equals(Register.None) &amp;&amp; getInput() instanceof RegisterValue) {
 947                 Register inputReg = ((RegisterValue) getInput()).getRegister();
 948                 if (!inputReg.equals(resReg)) {
 949                     emitUncompressWithBaseRegister(masm, resReg, baseReg, inputReg, shift, false);
 950                     return;
 951                 }
 952             }
 953             move(lirKindTool.getNarrowOopKind(), crb, masm);
 954             emitUncompressCode(masm, resReg, shift, baseReg, nonNull);
 955         }
 956 
 957         public static void emitUncompressCode(AMD64MacroAssembler masm, Register resReg, int shift, Register baseReg, boolean nonNull) {
 958             if (nonNull) {
 959                 if (!baseReg.equals(Register.None)) {
 960                     emitUncompressWithBaseRegister(masm, resReg, baseReg, shift, false);
 961                 } else if (shift != 0) {
 962                     masm.shlq(resReg, shift);
 963                 }
 964             } else {
 965                 if (shift != 0) {
 966                     masm.shlq(resReg, shift);
 967                 }
 968 
 969                 if (!baseReg.equals(Register.None)) {
 970                     if (shift == 0) {
 971                         // if encoding.shift != 0, the flags are already set by the shlq
 972                         masm.testq(resReg, resReg);
 973                     }
 974 
 975                     Label done = new Label();
 976                     masm.jccb(Equal, done);
 977                     masm.addq(resReg, baseReg);
 978                     masm.bind(done);
 979                 }
 980             }
 981         }
 982     }
 983 
 984     private abstract static class ZeroNullConversionOp extends AMD64LIRInstruction {
 985         @Def({REG, HINT}) protected AllocatableValue result;
 986         @Use({REG}) protected AllocatableValue input;
 987 
 988         protected ZeroNullConversionOp(LIRInstructionClass&lt;? extends ZeroNullConversionOp&gt; type, AllocatableValue result, AllocatableValue input) {
 989             super(type);
 990             this.result = result;
 991             this.input = input;
 992         }
 993 
 994         @Override
 995         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 996             Register nullRegister = crb.uncompressedNullRegister;
 997             if (!nullRegister.equals(Register.None)) {
 998                 emitConversion(asRegister(result), asRegister(input), nullRegister, masm);
 999             }
1000         }
1001 
1002         protected abstract void emitConversion(Register resultRegister, Register inputRegister, Register nullRegister, AMD64MacroAssembler masm);
1003     }
1004 
1005     public static class ConvertNullToZeroOp extends ZeroNullConversionOp {
1006         public static final LIRInstructionClass&lt;ConvertNullToZeroOp&gt; TYPE = LIRInstructionClass.create(ConvertNullToZeroOp.class);
1007 
1008         public ConvertNullToZeroOp(AllocatableValue result, AllocatableValue input) {
1009             super(TYPE, result, input);
1010         }
1011 
1012         @Override
1013         protected final void emitConversion(Register resultRegister, Register inputRegister, Register nullRegister, AMD64MacroAssembler masm) {
1014             if (inputRegister.equals(resultRegister)) {
1015                 Label done = new Label();
1016                 masm.subqAndJcc(inputRegister, nullRegister, Equal, done, true);
1017                 masm.addq(inputRegister, nullRegister);
1018                 masm.bind(done);
1019             } else {
1020                 masm.subq(resultRegister, resultRegister);
1021                 masm.cmpq(inputRegister, nullRegister);
1022                 masm.cmovq(NotEqual, resultRegister, inputRegister);
1023             }
1024         }
1025     }
1026 
1027     public static class ConvertZeroToNullOp extends ZeroNullConversionOp {
1028         public static final LIRInstructionClass&lt;ConvertZeroToNullOp&gt; TYPE = LIRInstructionClass.create(ConvertZeroToNullOp.class);
1029 
1030         public ConvertZeroToNullOp(AllocatableValue result, AllocatableValue input) {
1031             super(TYPE, result, input);
1032         }
1033 
1034         @Override
1035         protected final void emitConversion(Register resultRegister, Register inputRegister, Register nullRegister, AMD64MacroAssembler masm) {
1036             if (!inputRegister.equals(resultRegister)) {
1037                 masm.movq(resultRegister, inputRegister);
1038             }
1039             masm.testq(inputRegister, inputRegister);
1040             masm.cmovq(Equal, resultRegister, nullRegister);
1041         }
1042     }
1043 }
    </pre>
  </body>
</html>