<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64MacroAssembler.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2009, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.asm.amd64;
 26 
 27 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIncDec;
 28 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmLoadAndClearUpper;
 29 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmRegToRegMoveAll;
 30 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.ADD;
 31 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.AND;
 32 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.CMP;
 33 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SUB;
 34 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.DEC;
 35 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.INC;
 36 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.DWORD;
 37 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.QWORD;
 38 import static org.graalvm.compiler.core.common.NumUtil.isByte;
 39 
 40 import java.util.function.IntConsumer;
 41 import java.util.function.Supplier;
 42 
 43 import org.graalvm.compiler.asm.Label;
 44 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
 45 import org.graalvm.compiler.core.common.NumUtil;
 46 import org.graalvm.compiler.options.OptionValues;
 47 
 48 import jdk.vm.ci.amd64.AMD64;
 49 import jdk.vm.ci.amd64.AMD64Kind;
 50 import jdk.vm.ci.code.Register;
 51 import jdk.vm.ci.code.TargetDescription;
 52 
 53 /**
 54  * This class implements commonly used X86 code patterns.
 55  */
 56 public class AMD64MacroAssembler extends AMD64Assembler {
 57 
 58     public AMD64MacroAssembler(TargetDescription target) {
 59         super(target);
 60     }
 61 
 62     public AMD64MacroAssembler(TargetDescription target, OptionValues optionValues) {
 63         super(target, optionValues);
 64     }
 65 
 66     public final void decrementq(Register reg, int value) {
 67         if (value == Integer.MIN_VALUE) {
 68             subq(reg, value);
 69             return;
 70         }
 71         if (value &lt; 0) {
 72             incrementq(reg, -value);
 73             return;
 74         }
 75         if (value == 0) {
 76             return;
 77         }
 78         if (value == 1 &amp;&amp; UseIncDec) {
 79             decq(reg);
 80         } else {
 81             subq(reg, value);
 82         }
 83     }
 84 
 85     public final void decrementq(AMD64Address dst, int value) {
 86         if (value == Integer.MIN_VALUE) {
 87             subq(dst, value);
 88             return;
 89         }
 90         if (value &lt; 0) {
 91             incrementq(dst, -value);
 92             return;
 93         }
 94         if (value == 0) {
 95             return;
 96         }
 97         if (value == 1 &amp;&amp; UseIncDec) {
 98             decq(dst);
 99         } else {
100             subq(dst, value);
101         }
102     }
103 
104     public void incrementq(Register reg, int value) {
105         if (value == Integer.MIN_VALUE) {
106             addq(reg, value);
107             return;
108         }
109         if (value &lt; 0) {
110             decrementq(reg, -value);
111             return;
112         }
113         if (value == 0) {
114             return;
115         }
116         if (value == 1 &amp;&amp; UseIncDec) {
117             incq(reg);
118         } else {
119             addq(reg, value);
120         }
121     }
122 
123     public final void incrementq(AMD64Address dst, int value) {
124         if (value == Integer.MIN_VALUE) {
125             addq(dst, value);
126             return;
127         }
128         if (value &lt; 0) {
129             decrementq(dst, -value);
130             return;
131         }
132         if (value == 0) {
133             return;
134         }
135         if (value == 1 &amp;&amp; UseIncDec) {
136             incq(dst);
137         } else {
138             addq(dst, value);
139         }
140     }
141 
142     public final void movptr(Register dst, AMD64Address src) {
143         movq(dst, src);
144     }
145 
146     public final void movptr(AMD64Address dst, Register src) {
147         movq(dst, src);
148     }
149 
150     public final void movptr(AMD64Address dst, int src) {
151         movslq(dst, src);
152     }
153 
154     public final void cmpptr(Register src1, Register src2) {
155         cmpq(src1, src2);
156     }
157 
158     public final void cmpptr(Register src1, AMD64Address src2) {
159         cmpq(src1, src2);
160     }
161 
162     public final void decrementl(Register reg) {
163         decrementl(reg, 1);
164     }
165 
166     public final void decrementl(Register reg, int value) {
167         if (value == Integer.MIN_VALUE) {
168             subl(reg, value);
169             return;
170         }
171         if (value &lt; 0) {
172             incrementl(reg, -value);
173             return;
174         }
175         if (value == 0) {
176             return;
177         }
178         if (value == 1 &amp;&amp; UseIncDec) {
179             decl(reg);
180         } else {
181             subl(reg, value);
182         }
183     }
184 
185     public final void decrementl(AMD64Address dst, int value) {
186         if (value == Integer.MIN_VALUE) {
187             subl(dst, value);
188             return;
189         }
190         if (value &lt; 0) {
191             incrementl(dst, -value);
192             return;
193         }
194         if (value == 0) {
195             return;
196         }
197         if (value == 1 &amp;&amp; UseIncDec) {
198             decl(dst);
199         } else {
200             subl(dst, value);
201         }
202     }
203 
204     public final void incrementl(Register reg, int value) {
205         if (value == Integer.MIN_VALUE) {
206             addl(reg, value);
207             return;
208         }
209         if (value &lt; 0) {
210             decrementl(reg, -value);
211             return;
212         }
213         if (value == 0) {
214             return;
215         }
216         if (value == 1 &amp;&amp; UseIncDec) {
217             incl(reg);
218         } else {
219             addl(reg, value);
220         }
221     }
222 
223     public final void incrementl(AMD64Address dst, int value) {
224         if (value == Integer.MIN_VALUE) {
225             addl(dst, value);
226             return;
227         }
228         if (value &lt; 0) {
229             decrementl(dst, -value);
230             return;
231         }
232         if (value == 0) {
233             return;
234         }
235         if (value == 1 &amp;&amp; UseIncDec) {
236             incl(dst);
237         } else {
238             addl(dst, value);
239         }
240     }
241 
242     public void movflt(Register dst, Register src) {
243         assert dst.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; src.getRegisterCategory().equals(AMD64.XMM);
244         if (UseXmmRegToRegMoveAll) {
245             if (isAVX512Register(dst) || isAVX512Register(src)) {
246                 VexMoveOp.VMOVAPS.emit(this, AVXSize.XMM, dst, src);
247             } else {
248                 movaps(dst, src);
249             }
250         } else {
251             if (isAVX512Register(dst) || isAVX512Register(src)) {
252                 VexMoveOp.VMOVSS.emit(this, AVXSize.XMM, dst, src);
253             } else {
254                 movss(dst, src);
255             }
256         }
257     }
258 
259     public void movflt(Register dst, AMD64Address src) {
260         assert dst.getRegisterCategory().equals(AMD64.XMM);
261         if (isAVX512Register(dst)) {
262             VexMoveOp.VMOVSS.emit(this, AVXSize.XMM, dst, src);
263         } else {
264             movss(dst, src);
265         }
266     }
267 
268     public void movflt(AMD64Address dst, Register src) {
269         assert src.getRegisterCategory().equals(AMD64.XMM);
270         if (isAVX512Register(src)) {
271             VexMoveOp.VMOVSS.emit(this, AVXSize.XMM, dst, src);
272         } else {
273             movss(dst, src);
274         }
275     }
276 
277     public void movdbl(Register dst, Register src) {
278         assert dst.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; src.getRegisterCategory().equals(AMD64.XMM);
279         if (UseXmmRegToRegMoveAll) {
280             if (isAVX512Register(dst) || isAVX512Register(src)) {
281                 VexMoveOp.VMOVAPD.emit(this, AVXSize.XMM, dst, src);
282             } else {
283                 movapd(dst, src);
284             }
285         } else {
286             if (isAVX512Register(dst) || isAVX512Register(src)) {
287                 VexMoveOp.VMOVSD.emit(this, AVXSize.XMM, dst, src);
288             } else {
289                 movsd(dst, src);
290             }
291         }
292     }
293 
294     public void movdbl(Register dst, AMD64Address src) {
295         assert dst.getRegisterCategory().equals(AMD64.XMM);
296         if (UseXmmLoadAndClearUpper) {
297             if (isAVX512Register(dst)) {
298                 VexMoveOp.VMOVSD.emit(this, AVXSize.XMM, dst, src);
299             } else {
300                 movsd(dst, src);
301             }
302         } else {
303             assert !isAVX512Register(dst);
304             movlpd(dst, src);
305         }
306     }
307 
308     public void movdbl(AMD64Address dst, Register src) {
309         assert src.getRegisterCategory().equals(AMD64.XMM);
310         if (isAVX512Register(src)) {
311             VexMoveOp.VMOVSD.emit(this, AVXSize.XMM, dst, src);
312         } else {
313             movsd(dst, src);
314         }
315     }
316 
317     /**
318      * Non-atomic write of a 64-bit constant to memory. Do not use if the address might be a
319      * volatile field!
320      */
321     public final void movlong(AMD64Address dst, long src) {
322         if (NumUtil.isInt(src)) {
323             AMD64MIOp.MOV.emit(this, OperandSize.QWORD, dst, (int) src);
324         } else {
325             AMD64Address high = new AMD64Address(dst.getBase(), dst.getIndex(), dst.getScale(), dst.getDisplacement() + 4);
326             movl(dst, (int) (src &amp; 0xFFFFFFFF));
327             movl(high, (int) (src &gt;&gt; 32));
328         }
329     }
330 
331     public final void setl(ConditionFlag cc, Register dst) {
332         setb(cc, dst);
333         movzbl(dst, dst);
334     }
335 
336     public final void setq(ConditionFlag cc, Register dst) {
337         setb(cc, dst);
338         movzbq(dst, dst);
339     }
340 
341     public final void flog(Register dest, Register value, boolean base10) {
342         if (base10) {
343             fldlg2();
344         } else {
345             fldln2();
346         }
347         AMD64Address tmp = trigPrologue(value);
348         fyl2x();
349         trigEpilogue(dest, tmp);
350     }
351 
352     public final void fsin(Register dest, Register value) {
353         AMD64Address tmp = trigPrologue(value);
354         fsin();
355         trigEpilogue(dest, tmp);
356     }
357 
358     public final void fcos(Register dest, Register value) {
359         AMD64Address tmp = trigPrologue(value);
360         fcos();
361         trigEpilogue(dest, tmp);
362     }
363 
364     public final void ftan(Register dest, Register value) {
365         AMD64Address tmp = trigPrologue(value);
366         fptan();
367         fstp(0); // ftan pushes 1.0 in addition to the actual result, pop
368         trigEpilogue(dest, tmp);
369     }
370 
371     public final void fpop() {
372         ffree(0);
373         fincstp();
374     }
375 
376     private AMD64Address trigPrologue(Register value) {
377         assert value.getRegisterCategory().equals(AMD64.XMM);
378         AMD64Address tmp = new AMD64Address(AMD64.rsp);
379         subq(AMD64.rsp, AMD64Kind.DOUBLE.getSizeInBytes());
380         movdbl(tmp, value);
381         fldd(tmp);
382         return tmp;
383     }
384 
385     private void trigEpilogue(Register dest, AMD64Address tmp) {
386         assert dest.getRegisterCategory().equals(AMD64.XMM);
387         fstpd(tmp);
388         movdbl(dest, tmp);
389         addq(AMD64.rsp, AMD64Kind.DOUBLE.getSizeInBytes());
390     }
391 
392     /**
393      * Emit a direct call to a fixed address, which will be patched later during code installation.
394      *
395      * @param align indicates whether the displacement bytes (offset by
396      *            {@code callDisplacementOffset}) of this call instruction should be aligned to
397      *            {@code wordSize}.
398      * @return where the actual call instruction starts.
399      */
400     public final int directCall(boolean align, int callDisplacementOffset, int wordSize) {
401         emitAlignmentForDirectCall(align, callDisplacementOffset, wordSize);
402         testAndAlign(5);
403         // After padding to mitigate JCC erratum, the displacement may be unaligned again. The
404         // previous pass is essential because JCC erratum padding may not trigger without the
405         // displacement alignment.
406         emitAlignmentForDirectCall(align, callDisplacementOffset, wordSize);
407         int beforeCall = position();
408         call();
409         return beforeCall;
410     }
411 
412     private void emitAlignmentForDirectCall(boolean align, int callDisplacementOffset, int wordSize) {
413         if (align) {
414             // make sure that the displacement word of the call ends up word aligned
415             int offset = position();
416             offset += callDisplacementOffset;
417             int modulus = wordSize;
418             if (offset % modulus != 0) {
419                 nop(modulus - offset % modulus);
420             }
421         }
422     }
423 
424     public final int indirectCall(Register callReg) {
425         int bytesToEmit = needsRex(callReg) ? 3 : 2;
426         testAndAlign(bytesToEmit);
427         int beforeCall = position();
428         call(callReg);
429         assert beforeCall + bytesToEmit == position();
430         return beforeCall;
431     }
432 
433     public final int directCall(long address, Register scratch) {
434         int bytesToEmit = needsRex(scratch) ? 13 : 12;
435         testAndAlign(bytesToEmit);
436         int beforeCall = position();
437         movq(scratch, address);
438         call(scratch);
439         assert beforeCall + bytesToEmit == position();
440         return beforeCall;
441     }
442 
443     public final int directJmp(long address, Register scratch) {
444         int bytesToEmit = needsRex(scratch) ? 13 : 12;
445         testAndAlign(bytesToEmit);
446         int beforeJmp = position();
447         movq(scratch, address);
448         jmpWithoutAlignment(scratch);
449         assert beforeJmp + bytesToEmit == position();
450         return beforeJmp;
451     }
452 
453     // This should guarantee that the alignment in AMD64Assembler.jcc methods will be not triggered.
454     private void alignFusedPair(Label branchTarget, boolean isShortJmp, int prevOpInBytes) {
455         assert prevOpInBytes &lt; 26 : &quot;Fused pair may be longer than 0x20 bytes.&quot;;
456         if (branchTarget == null) {
457             testAndAlign(prevOpInBytes + 6);
458         } else if (isShortJmp) {
459             testAndAlign(prevOpInBytes + 2);
460         } else if (!branchTarget.isBound()) {
461             testAndAlign(prevOpInBytes + 6);
462         } else {
463             long disp = branchTarget.position() - (position() + prevOpInBytes);
464             // assuming short jump first
465             if (isByte(disp - 2)) {
466                 testAndAlign(prevOpInBytes + 2);
467                 // After alignment, isByte(disp - shortSize) might not hold. Need to check
468                 // again.
469                 disp = branchTarget.position() - (position() + prevOpInBytes);
470                 if (isByte(disp - 2)) {
471                     return;
472                 }
473             }
474             testAndAlign(prevOpInBytes + 6);
475         }
476     }
477 
478     private void applyMIOpAndJcc(AMD64MIOp op, OperandSize size, Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm,
479                     IntConsumer applyBeforeFusedPair) {
480         final int bytesToEmit = getPrefixInBytes(size, src, op.srcIsByte) + OPCODE_IN_BYTES + MODRM_IN_BYTES + op.immediateSize(size);
481         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);
482         final int beforeFusedPair = position();
483         if (applyBeforeFusedPair != null) {
484             applyBeforeFusedPair.accept(beforeFusedPair);
485         }
486         op.emit(this, size, src, imm32, annotateImm);
487         assert beforeFusedPair + bytesToEmit == position();
488         jcc(cc, branchTarget, isShortJmp);
489         assert ensureWithinBoundary(beforeFusedPair);
490     }
491 
492     private void applyMIOpAndJcc(AMD64MIOp op, OperandSize size, AMD64Address src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm,
493                     IntConsumer applyBeforeFusedPair) {
494         final int bytesToEmit = getPrefixInBytes(size, src) + OPCODE_IN_BYTES + addressInBytes(src) + op.immediateSize(size);
495         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);
496         final int beforeFusedPair = position();
497         if (applyBeforeFusedPair != null) {
498             applyBeforeFusedPair.accept(beforeFusedPair);
499         }
500         op.emit(this, size, src, imm32, annotateImm);
501         assert beforeFusedPair + bytesToEmit == position();
502         jcc(cc, branchTarget, isShortJmp);
503         assert ensureWithinBoundary(beforeFusedPair);
504     }
505 
506     private int applyRMOpAndJcc(AMD64RMOp op, OperandSize size, Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
507         final int bytesToEmit = getPrefixInBytes(size, src1, op.dstIsByte, src2, op.srcIsByte) + OPCODE_IN_BYTES + MODRM_IN_BYTES;
508         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);
509         final int beforeFusedPair = position();
510         op.emit(this, size, src1, src2);
511         final int beforeJcc = position();
512         assert beforeFusedPair + bytesToEmit == beforeJcc;
513         jcc(cc, branchTarget, isShortJmp);
514         assert ensureWithinBoundary(beforeFusedPair);
515         return beforeJcc;
516     }
517 
518     private int applyRMOpAndJcc(AMD64RMOp op, OperandSize size, Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {
519         final int bytesToEmit = getPrefixInBytes(size, src1, op.dstIsByte, src2) + OPCODE_IN_BYTES + addressInBytes(src2);
520         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);
521         final int beforeFusedPair = position();
522         if (applyBeforeFusedPair != null) {
523             applyBeforeFusedPair.accept(beforeFusedPair);
524         }
525         op.emit(this, size, src1, src2);
526         final int beforeJcc = position();
527         assert beforeFusedPair + bytesToEmit == beforeJcc;
528         jcc(cc, branchTarget, isShortJmp);
529         assert ensureWithinBoundary(beforeFusedPair);
530         return beforeJcc;
531     }
532 
533     public void applyMOpAndJcc(AMD64MOp op, OperandSize size, Register dst, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
534         final int bytesToEmit = getPrefixInBytes(size, dst, op.srcIsByte) + OPCODE_IN_BYTES + MODRM_IN_BYTES;
535         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);
536         final int beforeFusedPair = position();
537         op.emit(this, size, dst);
538         assert beforeFusedPair + bytesToEmit == position();
539         jcc(cc, branchTarget, isShortJmp);
540         assert ensureWithinBoundary(beforeFusedPair);
541     }
542 
543     public final void testAndJcc(OperandSize size, Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
544         applyMIOpAndJcc(AMD64MIOp.TEST, size, src, imm32, cc, branchTarget, isShortJmp, false, null);
545     }
546 
547     public final void testlAndJcc(Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
548         applyMIOpAndJcc(AMD64MIOp.TEST, DWORD, src, imm32, cc, branchTarget, isShortJmp, false, null);
549     }
550 
551     public final void testAndJcc(OperandSize size, AMD64Address src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {
552         applyMIOpAndJcc(AMD64MIOp.TEST, size, src, imm32, cc, branchTarget, isShortJmp, false, applyBeforeFusedPair);
553     }
554 
555     public final void testAndJcc(OperandSize size, Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
556         applyRMOpAndJcc(AMD64RMOp.TEST, size, src1, src2, cc, branchTarget, isShortJmp);
557     }
558 
559     public final void testlAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
560         applyRMOpAndJcc(AMD64RMOp.TEST, DWORD, src1, src2, cc, branchTarget, isShortJmp);
561     }
562 
563     public final int testqAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
564         return applyRMOpAndJcc(AMD64RMOp.TEST, QWORD, src1, src2, cc, branchTarget, isShortJmp);
565     }
566 
567     public final void testAndJcc(OperandSize size, Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {
568         applyRMOpAndJcc(AMD64RMOp.TEST, size, src1, src2, cc, branchTarget, isShortJmp, applyBeforeFusedPair);
569     }
570 
571     public final void testbAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
572         applyRMOpAndJcc(AMD64RMOp.TESTB, OperandSize.BYTE, src1, src2, cc, branchTarget, isShortJmp);
573     }
574 
575     public final void testbAndJcc(Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
576         applyRMOpAndJcc(AMD64RMOp.TESTB, OperandSize.BYTE, src1, src2, cc, branchTarget, isShortJmp, null);
577     }
578 
579     public final void cmpAndJcc(OperandSize size, Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm, IntConsumer applyBeforeFusedPair) {
580         applyMIOpAndJcc(CMP.getMIOpcode(size, isByte(imm32)), size, src, imm32, cc, branchTarget, isShortJmp, annotateImm, applyBeforeFusedPair);
581     }
582 
583     public final void cmplAndJcc(Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
584         applyMIOpAndJcc(CMP.getMIOpcode(DWORD, isByte(imm32)), DWORD, src, imm32, cc, branchTarget, isShortJmp, false, null);
585     }
586 
587     public final void cmpqAndJcc(Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
588         applyMIOpAndJcc(CMP.getMIOpcode(QWORD, isByte(imm32)), QWORD, src, imm32, cc, branchTarget, isShortJmp, false, null);
589     }
590 
591     public final void cmpAndJcc(OperandSize size, AMD64Address src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm, IntConsumer applyBeforeFusedPair) {
592         applyMIOpAndJcc(CMP.getMIOpcode(size, NumUtil.isByte(imm32)), size, src, imm32, cc, branchTarget, isShortJmp, annotateImm, applyBeforeFusedPair);
593     }
594 
595     public final void cmpAndJcc(OperandSize size, Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
596         applyRMOpAndJcc(CMP.getRMOpcode(size), size, src1, src2, cc, branchTarget, isShortJmp);
597     }
598 
599     public final void cmplAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
600         applyRMOpAndJcc(CMP.getRMOpcode(DWORD), DWORD, src1, src2, cc, branchTarget, isShortJmp);
601     }
602 
603     public final int cmpqAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
604         return applyRMOpAndJcc(CMP.getRMOpcode(QWORD), QWORD, src1, src2, cc, branchTarget, isShortJmp);
605     }
606 
607     public final void cmpAndJcc(OperandSize size, Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {
608         applyRMOpAndJcc(CMP.getRMOpcode(size), size, src1, src2, cc, branchTarget, isShortJmp, applyBeforeFusedPair);
609     }
610 
611     public final void cmplAndJcc(Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
612         applyRMOpAndJcc(CMP.getRMOpcode(DWORD), DWORD, src1, src2, cc, branchTarget, isShortJmp, null);
613     }
614 
615     public final int cmpqAndJcc(Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
616         return applyRMOpAndJcc(CMP.getRMOpcode(QWORD), QWORD, src1, src2, cc, branchTarget, isShortJmp, null);
617     }
618 
619     public final void cmpAndJcc(OperandSize size, Register src1, Supplier&lt;AMD64Address&gt; src2, ConditionFlag cc, Label branchTarget) {
620         AMD64Address placeHolder = getPlaceholder(position());
621         final AMD64RMOp op = CMP.getRMOpcode(size);
622         final int bytesToEmit = getPrefixInBytes(size, src1, op.dstIsByte, placeHolder) + OPCODE_IN_BYTES + addressInBytes(placeHolder);
623         alignFusedPair(branchTarget, false, bytesToEmit);
624         final int beforeFusedPair = position();
625         AMD64Address src2AsAddress = src2.get();
626         op.emit(this, size, src1, src2AsAddress);
627         assert beforeFusedPair + bytesToEmit == position();
628         jcc(cc, branchTarget, false);
629         assert ensureWithinBoundary(beforeFusedPair);
630     }
631 
632     public final void andlAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
633         applyMIOpAndJcc(AND.getMIOpcode(DWORD, isByte(imm32)), DWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);
634     }
635 
636     public final void addqAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
637         applyMIOpAndJcc(ADD.getMIOpcode(QWORD, isByte(imm32)), QWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);
638     }
639 
640     public final void sublAndJcc(Register dst, Register src, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
641         applyRMOpAndJcc(SUB.getRMOpcode(DWORD), DWORD, dst, src, cc, branchTarget, isShortJmp);
642     }
643 
644     public final void subqAndJcc(Register dst, Register src, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
645         applyRMOpAndJcc(SUB.getRMOpcode(QWORD), QWORD, dst, src, cc, branchTarget, isShortJmp);
646     }
647 
648     public final void sublAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
649         applyMIOpAndJcc(SUB.getMIOpcode(DWORD, isByte(imm32)), DWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);
650     }
651 
652     public final void subqAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
653         applyMIOpAndJcc(SUB.getMIOpcode(QWORD, isByte(imm32)), QWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);
654     }
655 
656     public final void incqAndJcc(Register dst, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
657         applyMOpAndJcc(INC, QWORD, dst, cc, branchTarget, isShortJmp);
658     }
659 
660     public final void decqAndJcc(Register dst, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
661         applyMOpAndJcc(DEC, QWORD, dst, cc, branchTarget, isShortJmp);
662     }
663 }
    </pre>
  </body>
</html>