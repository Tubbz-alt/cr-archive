<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/opto/gcm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;libadt/vectset.hpp&quot;
  27 #include &quot;memory/allocation.inline.hpp&quot;
  28 #include &quot;memory/resourceArea.hpp&quot;
  29 #include &quot;opto/block.hpp&quot;
  30 #include &quot;opto/c2compiler.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/cfgnode.hpp&quot;
  33 #include &quot;opto/machnode.hpp&quot;
  34 #include &quot;opto/opcodes.hpp&quot;
  35 #include &quot;opto/phaseX.hpp&quot;
  36 #include &quot;opto/rootnode.hpp&quot;
  37 #include &quot;opto/runtime.hpp&quot;
  38 #include &quot;opto/chaitin.hpp&quot;
  39 #include &quot;runtime/deoptimization.hpp&quot;
  40 
  41 // Portions of code courtesy of Clifford Click
  42 
  43 // Optimization - Graph Style
  44 
  45 // To avoid float value underflow
  46 #define MIN_BLOCK_FREQUENCY 1.e-35f
  47 
  48 //----------------------------schedule_node_into_block-------------------------
  49 // Insert node n into block b. Look for projections of n and make sure they
  50 // are in b also.
  51 void PhaseCFG::schedule_node_into_block( Node *n, Block *b ) {
  52   // Set basic block of n, Add n to b,
  53   map_node_to_block(n, b);
  54   b-&gt;add_inst(n);
  55 
  56   // After Matching, nearly any old Node may have projections trailing it.
  57   // These are usually machine-dependent flags.  In any case, they might
  58   // float to another block below this one.  Move them up.
  59   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
  60     Node*  use  = n-&gt;fast_out(i);
  61     if (use-&gt;is_Proj()) {
  62       Block* buse = get_block_for_node(use);
  63       if (buse != b) {              // In wrong block?
  64         if (buse != NULL) {
  65           buse-&gt;find_remove(use);   // Remove from wrong block
  66         }
  67         map_node_to_block(use, b);
  68         b-&gt;add_inst(use);
  69       }
  70     }
  71   }
  72 }
  73 
  74 //----------------------------replace_block_proj_ctrl-------------------------
  75 // Nodes that have is_block_proj() nodes as their control need to use
  76 // the appropriate Region for their actual block as their control since
  77 // the projection will be in a predecessor block.
  78 void PhaseCFG::replace_block_proj_ctrl( Node *n ) {
  79   const Node *in0 = n-&gt;in(0);
  80   assert(in0 != NULL, &quot;Only control-dependent&quot;);
  81   const Node *p = in0-&gt;is_block_proj();
  82   if (p != NULL &amp;&amp; p != n) {    // Control from a block projection?
  83     assert(!n-&gt;pinned() || n-&gt;is_MachConstantBase(), &quot;only pinned MachConstantBase node is expected here&quot;);
  84     // Find trailing Region
  85     Block *pb = get_block_for_node(in0); // Block-projection already has basic block
  86     uint j = 0;
  87     if (pb-&gt;_num_succs != 1) {  // More then 1 successor?
  88       // Search for successor
  89       uint max = pb-&gt;number_of_nodes();
  90       assert( max &gt; 1, &quot;&quot; );
  91       uint start = max - pb-&gt;_num_succs;
  92       // Find which output path belongs to projection
  93       for (j = start; j &lt; max; j++) {
  94         if( pb-&gt;get_node(j) == in0 )
  95           break;
  96       }
  97       assert( j &lt; max, &quot;must find&quot; );
  98       // Change control to match head of successor basic block
  99       j -= start;
 100     }
 101     n-&gt;set_req(0, pb-&gt;_succs[j]-&gt;head());
 102   }
 103 }
 104 
 105 bool PhaseCFG::is_dominator(Node* dom_node, Node* node) {
 106   assert(is_CFG(node) &amp;&amp; is_CFG(dom_node), &quot;node and dom_node must be CFG nodes&quot;);
 107   if (dom_node == node) {
 108     return true;
 109   }
 110   Block* d = find_block_for_node(dom_node);
 111   Block* n = find_block_for_node(node);
 112   assert(n != NULL &amp;&amp; d != NULL, &quot;blocks must exist&quot;);
 113 
 114   if (d == n) {
 115     if (dom_node-&gt;is_block_start()) {
 116       return true;
 117     }
 118     if (node-&gt;is_block_start()) {
 119       return false;
 120     }
 121     if (dom_node-&gt;is_block_proj()) {
 122       return false;
 123     }
 124     if (node-&gt;is_block_proj()) {
 125       return true;
 126     }
 127 
 128     assert(is_control_proj_or_safepoint(node), &quot;node must be control projection or safepoint&quot;);
 129     assert(is_control_proj_or_safepoint(dom_node), &quot;dom_node must be control projection or safepoint&quot;);
 130 
 131     // Neither &#39;node&#39; nor &#39;dom_node&#39; is a block start or block projection.
 132     // Check if &#39;dom_node&#39; is above &#39;node&#39; in the control graph.
 133     if (is_dominating_control(dom_node, node)) {
 134       return true;
 135     }
 136 
 137 #ifdef ASSERT
 138     // If &#39;dom_node&#39; does not dominate &#39;node&#39; then &#39;node&#39; has to dominate &#39;dom_node&#39;
 139     if (!is_dominating_control(node, dom_node)) {
 140       node-&gt;dump();
 141       dom_node-&gt;dump();
 142       assert(false, &quot;neither dom_node nor node dominates the other&quot;);
 143     }
 144 #endif
 145 
 146     return false;
 147   }
 148   return d-&gt;dom_lca(n) == d;
 149 }
 150 
 151 bool PhaseCFG::is_CFG(Node* n) {
 152   return n-&gt;is_block_proj() || n-&gt;is_block_start() || is_control_proj_or_safepoint(n);
 153 }
 154 
 155 bool PhaseCFG::is_control_proj_or_safepoint(Node* n) {
 156   bool result = (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_SafePoint) || (n-&gt;is_Proj() &amp;&amp; n-&gt;as_Proj()-&gt;bottom_type() == Type::CONTROL);
 157   assert(!result || (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_SafePoint)
 158           || (n-&gt;is_Proj() &amp;&amp; n-&gt;as_Proj()-&gt;_con == 0), &quot;If control projection, it must be projection 0&quot;);
 159   return result;
 160 }
 161 
 162 Block* PhaseCFG::find_block_for_node(Node* n) {
 163   if (n-&gt;is_block_start() || n-&gt;is_block_proj()) {
 164     return get_block_for_node(n);
 165   } else {
 166     // Walk the control graph up if &#39;n&#39; is not a block start nor a block projection. In this case &#39;n&#39; must be
 167     // an unmatched control projection or a not yet matched safepoint precedence edge in the middle of a block.
 168     assert(is_control_proj_or_safepoint(n), &quot;must be control projection or safepoint&quot;);
 169     Node* ctrl = n-&gt;in(0);
 170     while (!ctrl-&gt;is_block_start()) {
 171       ctrl = ctrl-&gt;in(0);
 172     }
 173     return get_block_for_node(ctrl);
 174   }
 175 }
 176 
 177 // Walk up the control graph from &#39;n&#39; and check if &#39;dom_ctrl&#39; is found.
 178 bool PhaseCFG::is_dominating_control(Node* dom_ctrl, Node* n) {
 179   Node* ctrl = n-&gt;in(0);
 180   while (!ctrl-&gt;is_block_start()) {
 181     if (ctrl == dom_ctrl) {
 182       return true;
 183     }
 184     ctrl = ctrl-&gt;in(0);
 185   }
 186   return false;
 187 }
 188 
 189 
 190 //------------------------------schedule_pinned_nodes--------------------------
 191 // Set the basic block for Nodes pinned into blocks
 192 void PhaseCFG::schedule_pinned_nodes(VectorSet &amp;visited) {
 193   // Allocate node stack of size C-&gt;live_nodes()+8 to avoid frequent realloc
 194   GrowableArray &lt;Node*&gt; spstack(C-&gt;live_nodes() + 8);
 195   spstack.push(_root);
 196   while (spstack.is_nonempty()) {
 197     Node* node = spstack.pop();
 198     if (!visited.test_set(node-&gt;_idx)) { // Test node and flag it as visited
 199       if (node-&gt;pinned() &amp;&amp; !has_block(node)) {  // Pinned?  Nail it down!
 200         assert(node-&gt;in(0), &quot;pinned Node must have Control&quot;);
 201         // Before setting block replace block_proj control edge
 202         replace_block_proj_ctrl(node);
 203         Node* input = node-&gt;in(0);
 204         while (!input-&gt;is_block_start()) {
 205           input = input-&gt;in(0);
 206         }
 207         Block* block = get_block_for_node(input); // Basic block of controlling input
 208         schedule_node_into_block(node, block);
 209       }
 210 
 211       // If the node has precedence edges (added when CastPP nodes are
 212       // removed in final_graph_reshaping), fix the control of the
 213       // node to cover the precedence edges and remove the
 214       // dependencies.
 215       Node* n = NULL;
 216       for (uint i = node-&gt;len()-1; i &gt;= node-&gt;req(); i--) {
 217         Node* m = node-&gt;in(i);
 218         if (m == NULL) continue;
 219 
 220         // Only process precedence edges that are CFG nodes. Safepoints and control projections can be in the middle of a block
 221         if (is_CFG(m)) {
 222           node-&gt;rm_prec(i);
 223           if (n == NULL) {
 224             n = m;
 225           } else {
 226             assert(is_dominator(n, m) || is_dominator(m, n), &quot;one must dominate the other&quot;);
 227             n = is_dominator(n, m) ? m : n;
 228           }
 229         } else {
 230           assert(node-&gt;is_Mach(), &quot;sanity&quot;);
 231           assert(node-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreCM, &quot;must be StoreCM node&quot;);
 232         }
 233       }
 234       if (n != NULL) {
 235         assert(node-&gt;in(0), &quot;control should have been set&quot;);
 236         assert(is_dominator(n, node-&gt;in(0)) || is_dominator(node-&gt;in(0), n), &quot;one must dominate the other&quot;);
 237         if (!is_dominator(n, node-&gt;in(0))) {
 238           node-&gt;set_req(0, n);
 239         }
 240       }
 241 
 242       // process all inputs that are non NULL
 243       for (int i = node-&gt;req()-1; i &gt;= 0; --i) {
 244         if (node-&gt;in(i) != NULL) {
 245           spstack.push(node-&gt;in(i));
 246         }
 247       }
 248     }
 249   }
 250 }
 251 
 252 #ifdef ASSERT
 253 // Assert that new input b2 is dominated by all previous inputs.
 254 // Check this by by seeing that it is dominated by b1, the deepest
 255 // input observed until b2.
 256 static void assert_dom(Block* b1, Block* b2, Node* n, const PhaseCFG* cfg) {
 257   if (b1 == NULL)  return;
 258   assert(b1-&gt;_dom_depth &lt; b2-&gt;_dom_depth, &quot;sanity&quot;);
 259   Block* tmp = b2;
 260   while (tmp != b1 &amp;&amp; tmp != NULL) {
 261     tmp = tmp-&gt;_idom;
 262   }
 263   if (tmp != b1) {
 264     // Detected an unschedulable graph.  Print some nice stuff and die.
 265     tty-&gt;print_cr(&quot;!!! Unschedulable graph !!!&quot;);
 266     for (uint j=0; j&lt;n-&gt;len(); j++) { // For all inputs
 267       Node* inn = n-&gt;in(j); // Get input
 268       if (inn == NULL)  continue;  // Ignore NULL, missing inputs
 269       Block* inb = cfg-&gt;get_block_for_node(inn);
 270       tty-&gt;print(&quot;B%d idom=B%d depth=%2d &quot;,inb-&gt;_pre_order,
 271                  inb-&gt;_idom ? inb-&gt;_idom-&gt;_pre_order : 0, inb-&gt;_dom_depth);
 272       inn-&gt;dump();
 273     }
 274     tty-&gt;print(&quot;Failing node: &quot;);
 275     n-&gt;dump();
 276     assert(false, &quot;unscheduable graph&quot;);
 277   }
 278 }
 279 #endif
 280 
 281 static Block* find_deepest_input(Node* n, const PhaseCFG* cfg) {
 282   // Find the last input dominated by all other inputs.
 283   Block* deepb           = NULL;        // Deepest block so far
 284   int    deepb_dom_depth = 0;
 285   for (uint k = 0; k &lt; n-&gt;len(); k++) { // For all inputs
 286     Node* inn = n-&gt;in(k);               // Get input
 287     if (inn == NULL)  continue;         // Ignore NULL, missing inputs
 288     Block* inb = cfg-&gt;get_block_for_node(inn);
 289     assert(inb != NULL, &quot;must already have scheduled this input&quot;);
 290     if (deepb_dom_depth &lt; (int) inb-&gt;_dom_depth) {
 291       // The new inb must be dominated by the previous deepb.
 292       // The various inputs must be linearly ordered in the dom
 293       // tree, or else there will not be a unique deepest block.
 294       DEBUG_ONLY(assert_dom(deepb, inb, n, cfg));
 295       deepb = inb;                      // Save deepest block
 296       deepb_dom_depth = deepb-&gt;_dom_depth;
 297     }
 298   }
 299   assert(deepb != NULL, &quot;must be at least one input to n&quot;);
 300   return deepb;
 301 }
 302 
 303 
 304 //------------------------------schedule_early---------------------------------
 305 // Find the earliest Block any instruction can be placed in.  Some instructions
 306 // are pinned into Blocks.  Unpinned instructions can appear in last block in
 307 // which all their inputs occur.
 308 bool PhaseCFG::schedule_early(VectorSet &amp;visited, Node_Stack &amp;roots) {
 309   // Allocate stack with enough space to avoid frequent realloc
 310   Node_Stack nstack(roots.size() + 8);
 311   // _root will be processed among C-&gt;top() inputs
 312   roots.push(C-&gt;top(), 0);
 313   visited.set(C-&gt;top()-&gt;_idx);
 314 
 315   while (roots.size() != 0) {
 316     // Use local variables nstack_top_n &amp; nstack_top_i to cache values
 317     // on stack&#39;s top.
 318     Node* parent_node = roots.node();
 319     uint  input_index = 0;
 320     roots.pop();
 321 
 322     while (true) {
 323       if (input_index == 0) {
 324         // Fixup some control.  Constants without control get attached
 325         // to root and nodes that use is_block_proj() nodes should be attached
 326         // to the region that starts their block.
 327         const Node* control_input = parent_node-&gt;in(0);
 328         if (control_input != NULL) {
 329           replace_block_proj_ctrl(parent_node);
 330         } else {
 331           // Is a constant with NO inputs?
 332           if (parent_node-&gt;req() == 1) {
 333             parent_node-&gt;set_req(0, _root);
 334           }
 335         }
 336       }
 337 
 338       // First, visit all inputs and force them to get a block.  If an
 339       // input is already in a block we quit following inputs (to avoid
 340       // cycles). Instead we put that Node on a worklist to be handled
 341       // later (since IT&#39;S inputs may not have a block yet).
 342 
 343       // Assume all n&#39;s inputs will be processed
 344       bool done = true;
 345 
 346       while (input_index &lt; parent_node-&gt;len()) {
 347         Node* in = parent_node-&gt;in(input_index++);
 348         if (in == NULL) {
 349           continue;
 350         }
 351 
 352         int is_visited = visited.test_set(in-&gt;_idx);
 353         if (!has_block(in)) {
 354           if (is_visited) {
 355             assert(false, &quot;graph should be schedulable&quot;);
 356             return false;
 357           }
 358           // Save parent node and next input&#39;s index.
 359           nstack.push(parent_node, input_index);
 360           // Process current input now.
 361           parent_node = in;
 362           input_index = 0;
 363           // Not all n&#39;s inputs processed.
 364           done = false;
 365           break;
 366         } else if (!is_visited) {
 367           // Visit this guy later, using worklist
 368           roots.push(in, 0);
 369         }
 370       }
 371 
 372       if (done) {
 373         // All of n&#39;s inputs have been processed, complete post-processing.
 374 
 375         // Some instructions are pinned into a block.  These include Region,
 376         // Phi, Start, Return, and other control-dependent instructions and
 377         // any projections which depend on them.
 378         if (!parent_node-&gt;pinned()) {
 379           // Set earliest legal block.
 380           Block* earliest_block = find_deepest_input(parent_node, this);
 381           map_node_to_block(parent_node, earliest_block);
 382         } else {
 383           assert(get_block_for_node(parent_node) == get_block_for_node(parent_node-&gt;in(0)), &quot;Pinned Node should be at the same block as its control edge&quot;);
 384         }
 385 
 386         if (nstack.is_empty()) {
 387           // Finished all nodes on stack.
 388           // Process next node on the worklist &#39;roots&#39;.
 389           break;
 390         }
 391         // Get saved parent node and next input&#39;s index.
 392         parent_node = nstack.node();
 393         input_index = nstack.index();
 394         nstack.pop();
 395       }
 396     }
 397   }
 398   return true;
 399 }
 400 
 401 //------------------------------dom_lca----------------------------------------
 402 // Find least common ancestor in dominator tree
 403 // LCA is a current notion of LCA, to be raised above &#39;this&#39;.
 404 // As a convenient boundary condition, return &#39;this&#39; if LCA is NULL.
 405 // Find the LCA of those two nodes.
 406 Block* Block::dom_lca(Block* LCA) {
 407   if (LCA == NULL || LCA == this)  return this;
 408 
 409   Block* anc = this;
 410   while (anc-&gt;_dom_depth &gt; LCA-&gt;_dom_depth)
 411     anc = anc-&gt;_idom;           // Walk up till anc is as high as LCA
 412 
 413   while (LCA-&gt;_dom_depth &gt; anc-&gt;_dom_depth)
 414     LCA = LCA-&gt;_idom;           // Walk up till LCA is as high as anc
 415 
 416   while (LCA != anc) {          // Walk both up till they are the same
 417     LCA = LCA-&gt;_idom;
 418     anc = anc-&gt;_idom;
 419   }
 420 
 421   return LCA;
 422 }
 423 
 424 //--------------------------raise_LCA_above_use--------------------------------
 425 // We are placing a definition, and have been given a def-&gt;use edge.
 426 // The definition must dominate the use, so move the LCA upward in the
 427 // dominator tree to dominate the use.  If the use is a phi, adjust
 428 // the LCA only with the phi input paths which actually use this def.
 429 static Block* raise_LCA_above_use(Block* LCA, Node* use, Node* def, const PhaseCFG* cfg) {
 430   Block* buse = cfg-&gt;get_block_for_node(use);
 431   if (buse == NULL)    return LCA;   // Unused killing Projs have no use block
 432   if (!use-&gt;is_Phi())  return buse-&gt;dom_lca(LCA);
 433   uint pmax = use-&gt;req();       // Number of Phi inputs
 434   // Why does not this loop just break after finding the matching input to
 435   // the Phi?  Well...it&#39;s like this.  I do not have true def-use/use-def
 436   // chains.  Means I cannot distinguish, from the def-use direction, which
 437   // of many use-defs lead from the same use to the same def.  That is, this
 438   // Phi might have several uses of the same def.  Each use appears in a
 439   // different predecessor block.  But when I enter here, I cannot distinguish
 440   // which use-def edge I should find the predecessor block for.  So I find
 441   // them all.  Means I do a little extra work if a Phi uses the same value
 442   // more than once.
 443   for (uint j=1; j&lt;pmax; j++) { // For all inputs
 444     if (use-&gt;in(j) == def) {    // Found matching input?
 445       Block* pred = cfg-&gt;get_block_for_node(buse-&gt;pred(j));
 446       LCA = pred-&gt;dom_lca(LCA);
 447     }
 448   }
 449   return LCA;
 450 }
 451 
 452 //----------------------------raise_LCA_above_marks----------------------------
 453 // Return a new LCA that dominates LCA and any of its marked predecessors.
 454 // Search all my parents up to &#39;early&#39; (exclusive), looking for predecessors
 455 // which are marked with the given index.  Return the LCA (in the dom tree)
 456 // of all marked blocks.  If there are none marked, return the original
 457 // LCA.
 458 static Block* raise_LCA_above_marks(Block* LCA, node_idx_t mark, Block* early, const PhaseCFG* cfg) {
 459   Block_List worklist;
 460   worklist.push(LCA);
 461   while (worklist.size() &gt; 0) {
 462     Block* mid = worklist.pop();
 463     if (mid == early)  continue;  // stop searching here
 464 
 465     // Test and set the visited bit.
 466     if (mid-&gt;raise_LCA_visited() == mark)  continue;  // already visited
 467 
 468     // Don&#39;t process the current LCA, otherwise the search may terminate early
 469     if (mid != LCA &amp;&amp; mid-&gt;raise_LCA_mark() == mark) {
 470       // Raise the LCA.
 471       LCA = mid-&gt;dom_lca(LCA);
 472       if (LCA == early)  break;   // stop searching everywhere
 473       assert(early-&gt;dominates(LCA), &quot;early is high enough&quot;);
 474       // Resume searching at that point, skipping intermediate levels.
 475       worklist.push(LCA);
 476       if (LCA == mid)
 477         continue; // Don&#39;t mark as visited to avoid early termination.
 478     } else {
 479       // Keep searching through this block&#39;s predecessors.
 480       for (uint j = 1, jmax = mid-&gt;num_preds(); j &lt; jmax; j++) {
 481         Block* mid_parent = cfg-&gt;get_block_for_node(mid-&gt;pred(j));
 482         worklist.push(mid_parent);
 483       }
 484     }
 485     mid-&gt;set_raise_LCA_visited(mark);
 486   }
 487   return LCA;
 488 }
 489 
 490 //--------------------------memory_early_block--------------------------------
 491 // This is a variation of find_deepest_input, the heart of schedule_early.
 492 // Find the &quot;early&quot; block for a load, if we considered only memory and
 493 // address inputs, that is, if other data inputs were ignored.
 494 //
 495 // Because a subset of edges are considered, the resulting block will
 496 // be earlier (at a shallower dom_depth) than the true schedule_early
 497 // point of the node. We compute this earlier block as a more permissive
 498 // site for anti-dependency insertion, but only if subsume_loads is enabled.
 499 static Block* memory_early_block(Node* load, Block* early, const PhaseCFG* cfg) {
 500   Node* base;
 501   Node* index;
 502   Node* store = load-&gt;in(MemNode::Memory);
 503   load-&gt;as_Mach()-&gt;memory_inputs(base, index);
 504 
 505   assert(base != NodeSentinel &amp;&amp; index != NodeSentinel,
 506          &quot;unexpected base/index inputs&quot;);
 507 
 508   Node* mem_inputs[4];
 509   int mem_inputs_length = 0;
 510   if (base != NULL)  mem_inputs[mem_inputs_length++] = base;
 511   if (index != NULL) mem_inputs[mem_inputs_length++] = index;
 512   if (store != NULL) mem_inputs[mem_inputs_length++] = store;
 513 
 514   // In the comparision below, add one to account for the control input,
 515   // which may be null, but always takes up a spot in the in array.
 516   if (mem_inputs_length + 1 &lt; (int) load-&gt;req()) {
 517     // This &quot;load&quot; has more inputs than just the memory, base and index inputs.
 518     // For purposes of checking anti-dependences, we need to start
 519     // from the early block of only the address portion of the instruction,
 520     // and ignore other blocks that may have factored into the wider
 521     // schedule_early calculation.
 522     if (load-&gt;in(0) != NULL) mem_inputs[mem_inputs_length++] = load-&gt;in(0);
 523 
 524     Block* deepb           = NULL;        // Deepest block so far
 525     int    deepb_dom_depth = 0;
 526     for (int i = 0; i &lt; mem_inputs_length; i++) {
 527       Block* inb = cfg-&gt;get_block_for_node(mem_inputs[i]);
 528       if (deepb_dom_depth &lt; (int) inb-&gt;_dom_depth) {
 529         // The new inb must be dominated by the previous deepb.
 530         // The various inputs must be linearly ordered in the dom
 531         // tree, or else there will not be a unique deepest block.
 532         DEBUG_ONLY(assert_dom(deepb, inb, load, cfg));
 533         deepb = inb;                      // Save deepest block
 534         deepb_dom_depth = deepb-&gt;_dom_depth;
 535       }
 536     }
 537     early = deepb;
 538   }
 539 
 540   return early;
 541 }
 542 
 543 //--------------------------insert_anti_dependences---------------------------
 544 // A load may need to witness memory that nearby stores can overwrite.
 545 // For each nearby store, either insert an &quot;anti-dependence&quot; edge
 546 // from the load to the store, or else move LCA upward to force the
 547 // load to (eventually) be scheduled in a block above the store.
 548 //
 549 // Do not add edges to stores on distinct control-flow paths;
 550 // only add edges to stores which might interfere.
 551 //
 552 // Return the (updated) LCA.  There will not be any possibly interfering
 553 // store between the load&#39;s &quot;early block&quot; and the updated LCA.
 554 // Any stores in the updated LCA will have new precedence edges
 555 // back to the load.  The caller is expected to schedule the load
 556 // in the LCA, in which case the precedence edges will make LCM
 557 // preserve anti-dependences.  The caller may also hoist the load
 558 // above the LCA, if it is not the early block.
 559 Block* PhaseCFG::insert_anti_dependences(Block* LCA, Node* load, bool verify) {
 560   assert(load-&gt;needs_anti_dependence_check(), &quot;must be a load of some sort&quot;);
 561   assert(LCA != NULL, &quot;&quot;);
 562   DEBUG_ONLY(Block* LCA_orig = LCA);
 563 
 564   // Compute the alias index.  Loads and stores with different alias indices
 565   // do not need anti-dependence edges.
 566   int load_alias_idx = C-&gt;get_alias_index(load-&gt;adr_type());
 567 #ifdef ASSERT
 568   assert(Compile::AliasIdxTop &lt;= load_alias_idx &amp;&amp; load_alias_idx &lt; C-&gt;num_alias_types(), &quot;Invalid alias index&quot;);
 569   if (load_alias_idx == Compile::AliasIdxBot &amp;&amp; C-&gt;AliasLevel() &gt; 0 &amp;&amp;
 570       (PrintOpto || VerifyAliases ||
 571        (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose)))) {
 572     // Load nodes should not consume all of memory.
 573     // Reporting a bottom type indicates a bug in adlc.
 574     // If some particular type of node validly consumes all of memory,
 575     // sharpen the preceding &quot;if&quot; to exclude it, so we can catch bugs here.
 576     tty-&gt;print_cr(&quot;*** Possible Anti-Dependence Bug:  Load consumes all of memory.&quot;);
 577     load-&gt;dump(2);
 578     if (VerifyAliases)  assert(load_alias_idx != Compile::AliasIdxBot, &quot;&quot;);
 579   }
 580 #endif
 581 
 582   if (!C-&gt;alias_type(load_alias_idx)-&gt;is_rewritable()) {
 583     // It is impossible to spoil this load by putting stores before it,
 584     // because we know that the stores will never update the value
 585     // which &#39;load&#39; must witness.
 586     return LCA;
 587   }
 588 
 589   node_idx_t load_index = load-&gt;_idx;
 590 
 591   // Note the earliest legal placement of &#39;load&#39;, as determined by
 592   // by the unique point in the dom tree where all memory effects
 593   // and other inputs are first available.  (Computed by schedule_early.)
 594   // For normal loads, &#39;early&#39; is the shallowest place (dom graph wise)
 595   // to look for anti-deps between this load and any store.
 596   Block* early = get_block_for_node(load);
 597 
 598   // If we are subsuming loads, compute an &quot;early&quot; block that only considers
 599   // memory or address inputs. This block may be different than the
 600   // schedule_early block in that it could be at an even shallower depth in the
 601   // dominator tree, and allow for a broader discovery of anti-dependences.
 602   if (C-&gt;subsume_loads()) {
 603     early = memory_early_block(load, early, this);
 604   }
 605 
 606   ResourceArea *area = Thread::current()-&gt;resource_area();
 607   Node_List worklist_mem(area);     // prior memory state to store
 608   Node_List worklist_store(area);   // possible-def to explore
 609   Node_List worklist_visited(area); // visited mergemem nodes
 610   Node_List non_early_stores(area); // all relevant stores outside of early
 611   bool must_raise_LCA = false;
 612 
 613 #ifdef TRACK_PHI_INPUTS
 614   // %%% This extra checking fails because MergeMem nodes are not GVNed.
 615   // Provide &quot;phi_inputs&quot; to check if every input to a PhiNode is from the
 616   // original memory state.  This indicates a PhiNode for which should not
 617   // prevent the load from sinking.  For such a block, set_raise_LCA_mark
 618   // may be overly conservative.
 619   // Mechanism: count inputs seen for each Phi encountered in worklist_store.
 620   DEBUG_ONLY(GrowableArray&lt;uint&gt; phi_inputs(area, C-&gt;unique(),0,0));
 621 #endif
 622 
 623   // &#39;load&#39; uses some memory state; look for users of the same state.
 624   // Recurse through MergeMem nodes to the stores that use them.
 625 
 626   // Each of these stores is a possible definition of memory
 627   // that &#39;load&#39; needs to use.  We need to force &#39;load&#39;
 628   // to occur before each such store.  When the store is in
 629   // the same block as &#39;load&#39;, we insert an anti-dependence
 630   // edge load-&gt;store.
 631 
 632   // The relevant stores &quot;nearby&quot; the load consist of a tree rooted
 633   // at initial_mem, with internal nodes of type MergeMem.
 634   // Therefore, the branches visited by the worklist are of this form:
 635   //    initial_mem -&gt; (MergeMem -&gt;)* store
 636   // The anti-dependence constraints apply only to the fringe of this tree.
 637 
 638   Node* initial_mem = load-&gt;in(MemNode::Memory);
 639   worklist_store.push(initial_mem);
 640   worklist_visited.push(initial_mem);
 641   worklist_mem.push(NULL);
 642   while (worklist_store.size() &gt; 0) {
 643     // Examine a nearby store to see if it might interfere with our load.
 644     Node* mem   = worklist_mem.pop();
 645     Node* store = worklist_store.pop();
 646     uint op = store-&gt;Opcode();
 647 
 648     // MergeMems do not directly have anti-deps.
 649     // Treat them as internal nodes in a forward tree of memory states,
 650     // the leaves of which are each a &#39;possible-def&#39;.
 651     if (store == initial_mem    // root (exclusive) of tree we are searching
 652         || op == Op_MergeMem    // internal node of tree we are searching
 653         ) {
 654       mem = store;   // It&#39;s not a possibly interfering store.
 655       if (store == initial_mem)
 656         initial_mem = NULL;  // only process initial memory once
 657 
 658       for (DUIterator_Fast imax, i = mem-&gt;fast_outs(imax); i &lt; imax; i++) {
 659         store = mem-&gt;fast_out(i);
 660         if (store-&gt;is_MergeMem()) {
 661           // Be sure we don&#39;t get into combinatorial problems.
 662           // (Allow phis to be repeated; they can merge two relevant states.)
 663           uint j = worklist_visited.size();
 664           for (; j &gt; 0; j--) {
 665             if (worklist_visited.at(j-1) == store)  break;
 666           }
 667           if (j &gt; 0)  continue; // already on work list; do not repeat
 668           worklist_visited.push(store);
 669         }
 670         worklist_mem.push(mem);
 671         worklist_store.push(store);
 672       }
 673       continue;
 674     }
 675 
 676     if (op == Op_MachProj || op == Op_Catch)   continue;
 677     if (store-&gt;needs_anti_dependence_check())  continue;  // not really a store
 678 
 679     // Compute the alias index.  Loads and stores with different alias
 680     // indices do not need anti-dependence edges.  Wide MemBar&#39;s are
 681     // anti-dependent on everything (except immutable memories).
 682     const TypePtr* adr_type = store-&gt;adr_type();
 683     if (!C-&gt;can_alias(adr_type, load_alias_idx))  continue;
 684 
 685     // Most slow-path runtime calls do NOT modify Java memory, but
 686     // they can block and so write Raw memory.
 687     if (store-&gt;is_Mach()) {
 688       MachNode* mstore = store-&gt;as_Mach();
 689       if (load_alias_idx != Compile::AliasIdxRaw) {
 690         // Check for call into the runtime using the Java calling
 691         // convention (and from there into a wrapper); it has no
 692         // _method.  Can&#39;t do this optimization for Native calls because
 693         // they CAN write to Java memory.
 694         if (mstore-&gt;ideal_Opcode() == Op_CallStaticJava) {
 695           assert(mstore-&gt;is_MachSafePoint(), &quot;&quot;);
 696           MachSafePointNode* ms = (MachSafePointNode*) mstore;
 697           assert(ms-&gt;is_MachCallJava(), &quot;&quot;);
 698           MachCallJavaNode* mcj = (MachCallJavaNode*) ms;
 699           if (mcj-&gt;_method == NULL) {
 700             // These runtime calls do not write to Java visible memory
 701             // (other than Raw) and so do not require anti-dependence edges.
 702             continue;
 703           }
 704         }
 705         // Same for SafePoints: they read/write Raw but only read otherwise.
 706         // This is basically a workaround for SafePoints only defining control
 707         // instead of control + memory.
 708         if (mstore-&gt;ideal_Opcode() == Op_SafePoint)
 709           continue;
 710 
 711         // Check if the store is a membar on which the load is control dependent.
 712         // Inserting an anti-dependency between that membar and the load would
 713         // create a cycle that causes local scheduling to fail.
 714         if (mstore-&gt;isa_MachMemBar()) {
 715           Node* dom = load-&gt;find_exact_control(load-&gt;in(0));
 716           while (dom != NULL &amp;&amp; dom != dom-&gt;in(0) &amp;&amp; dom != mstore) {
 717             dom = dom-&gt;in(0);
 718           }
 719           if (dom == mstore) {
 720             continue;
 721           }
 722         }
 723       } else {
 724         // Some raw memory, such as the load of &quot;top&quot; at an allocation,
 725         // can be control dependent on the previous safepoint. See
 726         // comments in GraphKit::allocate_heap() about control input.
 727         // Inserting an anti-dep between such a safepoint and a use
 728         // creates a cycle, and will cause a subsequent failure in
 729         // local scheduling.  (BugId 4919904)
 730         // (%%% How can a control input be a safepoint and not a projection??)
 731         if (mstore-&gt;ideal_Opcode() == Op_SafePoint &amp;&amp; load-&gt;in(0) == mstore)
 732           continue;
 733       }
 734     }
 735 
 736     // Identify a block that the current load must be above,
 737     // or else observe that &#39;store&#39; is all the way up in the
 738     // earliest legal block for &#39;load&#39;.  In the latter case,
 739     // immediately insert an anti-dependence edge.
 740     Block* store_block = get_block_for_node(store);
 741     assert(store_block != NULL, &quot;unused killing projections skipped above&quot;);
 742 
 743     if (store-&gt;is_Phi()) {
 744       // Loop-phis need to raise load before input. (Other phis are treated
 745       // as store below.)
 746       //
 747       // &#39;load&#39; uses memory which is one (or more) of the Phi&#39;s inputs.
 748       // It must be scheduled not before the Phi, but rather before
 749       // each of the relevant Phi inputs.
 750       //
 751       // Instead of finding the LCA of all inputs to a Phi that match &#39;mem&#39;,
 752       // we mark each corresponding predecessor block and do a combined
 753       // hoisting operation later (raise_LCA_above_marks).
 754       //
 755       // Do not assert(store_block != early, &quot;Phi merging memory after access&quot;)
 756       // PhiNode may be at start of block &#39;early&#39; with backedge to &#39;early&#39;
 757       DEBUG_ONLY(bool found_match = false);
 758       for (uint j = PhiNode::Input, jmax = store-&gt;req(); j &lt; jmax; j++) {
 759         if (store-&gt;in(j) == mem) {   // Found matching input?
 760           DEBUG_ONLY(found_match = true);
 761           Block* pred_block = get_block_for_node(store_block-&gt;pred(j));
 762           if (pred_block != early) {
 763             // If any predecessor of the Phi matches the load&#39;s &quot;early block&quot;,
 764             // we do not need a precedence edge between the Phi and &#39;load&#39;
 765             // since the load will be forced into a block preceding the Phi.
 766             pred_block-&gt;set_raise_LCA_mark(load_index);
 767             assert(!LCA_orig-&gt;dominates(pred_block) ||
 768                    early-&gt;dominates(pred_block), &quot;early is high enough&quot;);
 769             must_raise_LCA = true;
 770           } else {
 771             // anti-dependent upon PHI pinned below &#39;early&#39;, no edge needed
 772             LCA = early;             // but can not schedule below &#39;early&#39;
 773           }
 774         }
 775       }
 776       assert(found_match, &quot;no worklist bug&quot;);
 777 #ifdef TRACK_PHI_INPUTS
 778 #ifdef ASSERT
 779         // This assert asks about correct handling of PhiNodes, which may not
 780         // have all input edges directly from &#39;mem&#39;. See BugId 4621264
 781         int num_mem_inputs = phi_inputs.at_grow(store-&gt;_idx,0) + 1;
 782         // Increment by exactly one even if there are multiple copies of &#39;mem&#39;
 783         // coming into the phi, because we will run this block several times
 784         // if there are several copies of &#39;mem&#39;.  (That&#39;s how DU iterators work.)
 785         phi_inputs.at_put(store-&gt;_idx, num_mem_inputs);
 786         assert(PhiNode::Input + num_mem_inputs &lt; store-&gt;req(),
 787                &quot;Expect at least one phi input will not be from original memory state&quot;);
 788 #endif //ASSERT
 789 #endif //TRACK_PHI_INPUTS
 790     } else if (store_block != early) {
 791       // &#39;store&#39; is between the current LCA and earliest possible block.
 792       // Label its block, and decide later on how to raise the LCA
 793       // to include the effect on LCA of this store.
 794       // If this store&#39;s block gets chosen as the raised LCA, we
 795       // will find him on the non_early_stores list and stick him
 796       // with a precedence edge.
 797       // (But, don&#39;t bother if LCA is already raised all the way.)
 798       if (LCA != early) {
 799         store_block-&gt;set_raise_LCA_mark(load_index);
 800         must_raise_LCA = true;
 801         non_early_stores.push(store);
 802       }
 803     } else {
 804       // Found a possibly-interfering store in the load&#39;s &#39;early&#39; block.
 805       // This means &#39;load&#39; cannot sink at all in the dominator tree.
 806       // Add an anti-dep edge, and squeeze &#39;load&#39; into the highest block.
 807       assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);
 808       if (verify) {
 809         assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 810       } else {
 811         store-&gt;add_prec(load);
 812       }
 813       LCA = early;
 814       // This turns off the process of gathering non_early_stores.
 815     }
 816   }
 817   // (Worklist is now empty; all nearby stores have been visited.)
 818 
 819   // Finished if &#39;load&#39; must be scheduled in its &#39;early&#39; block.
 820   // If we found any stores there, they have already been given
 821   // precedence edges.
 822   if (LCA == early)  return LCA;
 823 
 824   // We get here only if there are no possibly-interfering stores
 825   // in the load&#39;s &#39;early&#39; block.  Move LCA up above all predecessors
 826   // which contain stores we have noted.
 827   //
 828   // The raised LCA block can be a home to such interfering stores,
 829   // but its predecessors must not contain any such stores.
 830   //
 831   // The raised LCA will be a lower bound for placing the load,
 832   // preventing the load from sinking past any block containing
 833   // a store that may invalidate the memory state required by &#39;load&#39;.
 834   if (must_raise_LCA)
 835     LCA = raise_LCA_above_marks(LCA, load-&gt;_idx, early, this);
 836   if (LCA == early)  return LCA;
 837 
 838   // Insert anti-dependence edges from &#39;load&#39; to each store
 839   // in the non-early LCA block.
 840   // Mine the non_early_stores list for such stores.
 841   if (LCA-&gt;raise_LCA_mark() == load_index) {
 842     while (non_early_stores.size() &gt; 0) {
 843       Node* store = non_early_stores.pop();
 844       Block* store_block = get_block_for_node(store);
 845       if (store_block == LCA) {
 846         // add anti_dependence from store to load in its own block
 847         assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);
 848         if (verify) {
 849           assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 850         } else {
 851           store-&gt;add_prec(load);
 852         }
 853       } else {
 854         assert(store_block-&gt;raise_LCA_mark() == load_index, &quot;block was marked&quot;);
 855         // Any other stores we found must be either inside the new LCA
 856         // or else outside the original LCA.  In the latter case, they
 857         // did not interfere with any use of &#39;load&#39;.
 858         assert(LCA-&gt;dominates(store_block)
 859                || !LCA_orig-&gt;dominates(store_block), &quot;no stray stores&quot;);
 860       }
 861     }
 862   }
 863 
 864   // Return the highest block containing stores; any stores
 865   // within that block have been given anti-dependence edges.
 866   return LCA;
 867 }
 868 
 869 // This class is used to iterate backwards over the nodes in the graph.
 870 
 871 class Node_Backward_Iterator {
 872 
 873 private:
 874   Node_Backward_Iterator();
 875 
 876 public:
 877   // Constructor for the iterator
 878   Node_Backward_Iterator(Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg);
 879 
 880   // Postincrement operator to iterate over the nodes
 881   Node *next();
 882 
 883 private:
 884   VectorSet   &amp;_visited;
 885   Node_Stack  &amp;_stack;
 886   PhaseCFG &amp;_cfg;
 887 };
 888 
 889 // Constructor for the Node_Backward_Iterator
 890 Node_Backward_Iterator::Node_Backward_Iterator( Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg)
 891   : _visited(visited), _stack(stack), _cfg(cfg) {
 892   // The stack should contain exactly the root
 893   stack.clear();
 894   stack.push(root, root-&gt;outcnt());
 895 
 896   // Clear the visited bits
 897   visited.clear();
 898 }
 899 
 900 // Iterator for the Node_Backward_Iterator
 901 Node *Node_Backward_Iterator::next() {
 902 
 903   // If the _stack is empty, then just return NULL: finished.
 904   if ( !_stack.size() )
 905     return NULL;
 906 
 907   // I visit unvisited not-anti-dependence users first, then anti-dependent
 908   // children next. I iterate backwards to support removal of nodes.
 909   // The stack holds states consisting of 3 values:
 910   // current Def node, flag which indicates 1st/2nd pass, index of current out edge
 911   Node *self = (Node*)(((uintptr_t)_stack.node()) &amp; ~1);
 912   bool iterate_anti_dep = (((uintptr_t)_stack.node()) &amp; 1);
 913   uint idx = MIN2(_stack.index(), self-&gt;outcnt()); // Support removal of nodes.
 914   _stack.pop();
 915 
 916   // I cycle here when I am entering a deeper level of recursion.
 917   // The key variable &#39;self&#39; was set prior to jumping here.
 918   while( 1 ) {
 919 
 920     _visited.set(self-&gt;_idx);
 921 
 922     // Now schedule all uses as late as possible.
 923     const Node* src = self-&gt;is_Proj() ? self-&gt;in(0) : self;
 924     uint src_rpo = _cfg.get_block_for_node(src)-&gt;_rpo;
 925 
 926     // Schedule all nodes in a post-order visit
 927     Node *unvisited = NULL;  // Unvisited anti-dependent Node, if any
 928 
 929     // Scan for unvisited nodes
 930     while (idx &gt; 0) {
 931       // For all uses, schedule late
 932       Node* n = self-&gt;raw_out(--idx); // Use
 933 
 934       // Skip already visited children
 935       if ( _visited.test(n-&gt;_idx) )
 936         continue;
 937 
 938       // do not traverse backward control edges
 939       Node *use = n-&gt;is_Proj() ? n-&gt;in(0) : n;
 940       uint use_rpo = _cfg.get_block_for_node(use)-&gt;_rpo;
 941 
 942       if ( use_rpo &lt; src_rpo )
 943         continue;
 944 
 945       // Phi nodes always precede uses in a basic block
 946       if ( use_rpo == src_rpo &amp;&amp; use-&gt;is_Phi() )
 947         continue;
 948 
 949       unvisited = n;      // Found unvisited
 950 
 951       // Check for possible-anti-dependent
 952       // 1st pass: No such nodes, 2nd pass: Only such nodes.
 953       if (n-&gt;needs_anti_dependence_check() == iterate_anti_dep) {
 954         unvisited = n;      // Found unvisited
 955         break;
 956       }
 957     }
 958 
 959     // Did I find an unvisited not-anti-dependent Node?
 960     if (!unvisited) {
 961       if (!iterate_anti_dep) {
 962         // 2nd pass: Iterate over nodes which needs_anti_dependence_check.
 963         iterate_anti_dep = true;
 964         idx = self-&gt;outcnt();
 965         continue;
 966       }
 967       break;                  // All done with children; post-visit &#39;self&#39;
 968     }
 969 
 970     // Visit the unvisited Node.  Contains the obvious push to
 971     // indicate I&#39;m entering a deeper level of recursion.  I push the
 972     // old state onto the _stack and set a new state and loop (recurse).
 973     _stack.push((Node*)((uintptr_t)self | (uintptr_t)iterate_anti_dep), idx);
 974     self = unvisited;
 975     iterate_anti_dep = false;
 976     idx = self-&gt;outcnt();
 977   } // End recursion loop
 978 
 979   return self;
 980 }
 981 
 982 //------------------------------ComputeLatenciesBackwards----------------------
 983 // Compute the latency of all the instructions.
 984 void PhaseCFG::compute_latencies_backwards(VectorSet &amp;visited, Node_Stack &amp;stack) {
 985 #ifndef PRODUCT
 986   if (trace_opto_pipelining())
 987     tty-&gt;print(&quot;\n#---- ComputeLatenciesBackwards ----\n&quot;);
 988 #endif
 989 
 990   Node_Backward_Iterator iter((Node *)_root, visited, stack, *this);
 991   Node *n;
 992 
 993   // Walk over all the nodes from last to first
 994   while ((n = iter.next())) {
 995     // Set the latency for the definitions of this instruction
 996     partial_latency_of_defs(n);
 997   }
 998 } // end ComputeLatenciesBackwards
 999 
1000 //------------------------------partial_latency_of_defs------------------------
1001 // Compute the latency impact of this node on all defs.  This computes
1002 // a number that increases as we approach the beginning of the routine.
1003 void PhaseCFG::partial_latency_of_defs(Node *n) {
1004   // Set the latency for this instruction
1005 #ifndef PRODUCT
1006   if (trace_opto_pipelining()) {
1007     tty-&gt;print(&quot;# latency_to_inputs: node_latency[%d] = %d for node&quot;, n-&gt;_idx, get_latency_for_node(n));
1008     dump();
1009   }
1010 #endif
1011 
1012   if (n-&gt;is_Proj()) {
1013     n = n-&gt;in(0);
1014   }
1015 
1016   if (n-&gt;is_Root()) {
1017     return;
1018   }
1019 
1020   uint nlen = n-&gt;len();
1021   uint use_latency = get_latency_for_node(n);
1022   uint use_pre_order = get_block_for_node(n)-&gt;_pre_order;
1023 
1024   for (uint j = 0; j &lt; nlen; j++) {
1025     Node *def = n-&gt;in(j);
1026 
1027     if (!def || def == n) {
1028       continue;
1029     }
1030 
1031     // Walk backwards thru projections
1032     if (def-&gt;is_Proj()) {
1033       def = def-&gt;in(0);
1034     }
1035 
1036 #ifndef PRODUCT
1037     if (trace_opto_pipelining()) {
1038       tty-&gt;print(&quot;#    in(%2d): &quot;, j);
1039       def-&gt;dump();
1040     }
1041 #endif
1042 
1043     // If the defining block is not known, assume it is ok
1044     Block *def_block = get_block_for_node(def);
1045     uint def_pre_order = def_block ? def_block-&gt;_pre_order : 0;
1046 
1047     if ((use_pre_order &lt;  def_pre_order) || (use_pre_order == def_pre_order &amp;&amp; n-&gt;is_Phi())) {
1048       continue;
1049     }
1050 
1051     uint delta_latency = n-&gt;latency(j);
1052     uint current_latency = delta_latency + use_latency;
1053 
1054     if (get_latency_for_node(def) &lt; current_latency) {
1055       set_latency_for_node(def, current_latency);
1056     }
1057 
1058 #ifndef PRODUCT
1059     if (trace_opto_pipelining()) {
1060       tty-&gt;print_cr(&quot;#      %d + edge_latency(%d) == %d -&gt; %d, node_latency[%d] = %d&quot;, use_latency, j, delta_latency, current_latency, def-&gt;_idx, get_latency_for_node(def));
1061     }
1062 #endif
1063   }
1064 }
1065 
1066 //------------------------------latency_from_use-------------------------------
1067 // Compute the latency of a specific use
1068 int PhaseCFG::latency_from_use(Node *n, const Node *def, Node *use) {
1069   // If self-reference, return no latency
1070   if (use == n || use-&gt;is_Root()) {
1071     return 0;
1072   }
1073 
1074   uint def_pre_order = get_block_for_node(def)-&gt;_pre_order;
1075   uint latency = 0;
1076 
1077   // If the use is not a projection, then it is simple...
1078   if (!use-&gt;is_Proj()) {
1079 #ifndef PRODUCT
1080     if (trace_opto_pipelining()) {
1081       tty-&gt;print(&quot;#    out(): &quot;);
1082       use-&gt;dump();
1083     }
1084 #endif
1085 
1086     uint use_pre_order = get_block_for_node(use)-&gt;_pre_order;
1087 
1088     if (use_pre_order &lt; def_pre_order)
1089       return 0;
1090 
1091     if (use_pre_order == def_pre_order &amp;&amp; use-&gt;is_Phi())
1092       return 0;
1093 
1094     uint nlen = use-&gt;len();
1095     uint nl = get_latency_for_node(use);
1096 
1097     for ( uint j=0; j&lt;nlen; j++ ) {
1098       if (use-&gt;in(j) == n) {
1099         // Change this if we want local latencies
1100         uint ul = use-&gt;latency(j);
1101         uint  l = ul + nl;
1102         if (latency &lt; l) latency = l;
1103 #ifndef PRODUCT
1104         if (trace_opto_pipelining()) {
1105           tty-&gt;print_cr(&quot;#      %d + edge_latency(%d) == %d -&gt; %d, latency = %d&quot;,
1106                         nl, j, ul, l, latency);
1107         }
1108 #endif
1109       }
1110     }
1111   } else {
1112     // This is a projection, just grab the latency of the use(s)
1113     for (DUIterator_Fast jmax, j = use-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1114       uint l = latency_from_use(use, def, use-&gt;fast_out(j));
1115       if (latency &lt; l) latency = l;
1116     }
1117   }
1118 
1119   return latency;
1120 }
1121 
1122 //------------------------------latency_from_uses------------------------------
1123 // Compute the latency of this instruction relative to all of it&#39;s uses.
1124 // This computes a number that increases as we approach the beginning of the
1125 // routine.
1126 void PhaseCFG::latency_from_uses(Node *n) {
1127   // Set the latency for this instruction
1128 #ifndef PRODUCT
1129   if (trace_opto_pipelining()) {
1130     tty-&gt;print(&quot;# latency_from_outputs: node_latency[%d] = %d for node&quot;, n-&gt;_idx, get_latency_for_node(n));
1131     dump();
1132   }
1133 #endif
1134   uint latency=0;
1135   const Node *def = n-&gt;is_Proj() ? n-&gt;in(0): n;
1136 
1137   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
1138     uint l = latency_from_use(n, def, n-&gt;fast_out(i));
1139 
1140     if (latency &lt; l) latency = l;
1141   }
1142 
1143   set_latency_for_node(n, latency);
1144 }
1145 
1146 //------------------------------hoist_to_cheaper_block-------------------------
1147 // Pick a block for node self, between early and LCA, that is a cheaper
1148 // alternative to LCA.
1149 Block* PhaseCFG::hoist_to_cheaper_block(Block* LCA, Block* early, Node* self) {
1150   const double delta = 1+PROB_UNLIKELY_MAG(4);
1151   Block* least       = LCA;
1152   double least_freq  = least-&gt;_freq;
1153   uint target        = get_latency_for_node(self);
1154   uint start_latency = get_latency_for_node(LCA-&gt;head());
1155   uint end_latency   = get_latency_for_node(LCA-&gt;get_node(LCA-&gt;end_idx()));
1156   bool in_latency    = (target &lt;= start_latency);
1157   const Block* root_block = get_block_for_node(_root);
1158 
1159   // Turn off latency scheduling if scheduling is just plain off
1160   if (!C-&gt;do_scheduling())
1161     in_latency = true;
1162 
1163   // Do not hoist (to cover latency) instructions which target a
1164   // single register.  Hoisting stretches the live range of the
1165   // single register and may force spilling.
1166   MachNode* mach = self-&gt;is_Mach() ? self-&gt;as_Mach() : NULL;
1167   if (mach &amp;&amp; mach-&gt;out_RegMask().is_bound1() &amp;&amp; mach-&gt;out_RegMask().is_NotEmpty())
1168     in_latency = true;
1169 
1170 #ifndef PRODUCT
1171   if (trace_opto_pipelining()) {
1172     tty-&gt;print(&quot;# Find cheaper block for latency %d: &quot;, get_latency_for_node(self));
1173     self-&gt;dump();
1174     tty-&gt;print_cr(&quot;#   B%d: start latency for [%4d]=%d, end latency for [%4d]=%d, freq=%g&quot;,
1175       LCA-&gt;_pre_order,
1176       LCA-&gt;head()-&gt;_idx,
1177       start_latency,
1178       LCA-&gt;get_node(LCA-&gt;end_idx())-&gt;_idx,
1179       end_latency,
1180       least_freq);
1181   }
1182 #endif
1183 
1184   int cand_cnt = 0;  // number of candidates tried
1185 
1186   // Walk up the dominator tree from LCA (Lowest common ancestor) to
1187   // the earliest legal location.  Capture the least execution frequency.
1188   while (LCA != early) {
1189     LCA = LCA-&gt;_idom;         // Follow up the dominator tree
1190 
1191     if (LCA == NULL) {
1192       // Bailout without retry
1193       assert(false, &quot;graph should be schedulable&quot;);
1194       C-&gt;record_method_not_compilable(&quot;late schedule failed: LCA == NULL&quot;);
1195       return least;
1196     }
1197 
1198     // Don&#39;t hoist machine instructions to the root basic block
1199     if (mach &amp;&amp; LCA == root_block)
1200       break;
1201 
1202     uint start_lat = get_latency_for_node(LCA-&gt;head());
1203     uint end_idx   = LCA-&gt;end_idx();
1204     uint end_lat   = get_latency_for_node(LCA-&gt;get_node(end_idx));
1205     double LCA_freq = LCA-&gt;_freq;
1206 #ifndef PRODUCT
1207     if (trace_opto_pipelining()) {
1208       tty-&gt;print_cr(&quot;#   B%d: start latency for [%4d]=%d, end latency for [%4d]=%d, freq=%g&quot;,
1209         LCA-&gt;_pre_order, LCA-&gt;head()-&gt;_idx, start_lat, end_idx, end_lat, LCA_freq);
1210     }
1211 #endif
1212     cand_cnt++;
1213     if (LCA_freq &lt; least_freq              || // Better Frequency
1214         (StressGCM &amp;&amp; Compile::randomized_select(cand_cnt)) || // Should be randomly accepted in stress mode
1215          (!StressGCM                    &amp;&amp;    // Otherwise, choose with latency
1216           !in_latency                   &amp;&amp;    // No block containing latency
1217           LCA_freq &lt; least_freq * delta &amp;&amp;    // No worse frequency
1218           target &gt;= end_lat             &amp;&amp;    // within latency range
1219           !self-&gt;is_iteratively_computed() )  // But don&#39;t hoist IV increments
1220              // because they may end up above other uses of their phi forcing
1221              // their result register to be different from their input.
1222        ) {
1223       least = LCA;            // Found cheaper block
1224       least_freq = LCA_freq;
1225       start_latency = start_lat;
1226       end_latency = end_lat;
1227       if (target &lt;= start_lat)
1228         in_latency = true;
1229     }
1230   }
1231 
1232 #ifndef PRODUCT
1233   if (trace_opto_pipelining()) {
1234     tty-&gt;print_cr(&quot;#  Choose block B%d with start latency=%d and freq=%g&quot;,
1235       least-&gt;_pre_order, start_latency, least_freq);
1236   }
1237 #endif
1238 
1239   // See if the latency needs to be updated
1240   if (target &lt; end_latency) {
1241 #ifndef PRODUCT
1242     if (trace_opto_pipelining()) {
1243       tty-&gt;print_cr(&quot;#  Change latency for [%4d] from %d to %d&quot;, self-&gt;_idx, target, end_latency);
1244     }
1245 #endif
1246     set_latency_for_node(self, end_latency);
1247     partial_latency_of_defs(self);
1248   }
1249 
1250   return least;
1251 }
1252 
1253 
1254 //------------------------------schedule_late-----------------------------------
1255 // Now schedule all codes as LATE as possible.  This is the LCA in the
1256 // dominator tree of all USES of a value.  Pick the block with the least
1257 // loop nesting depth that is lowest in the dominator tree.
1258 extern const char must_clone[];
1259 void PhaseCFG::schedule_late(VectorSet &amp;visited, Node_Stack &amp;stack) {
1260 #ifndef PRODUCT
1261   if (trace_opto_pipelining())
1262     tty-&gt;print(&quot;\n#---- schedule_late ----\n&quot;);
1263 #endif
1264 
1265   Node_Backward_Iterator iter((Node *)_root, visited, stack, *this);
1266   Node *self;
1267 
1268   // Walk over all the nodes from last to first
1269   while ((self = iter.next())) {
1270     Block* early = get_block_for_node(self); // Earliest legal placement
1271 
1272     if (self-&gt;is_top()) {
1273       // Top node goes in bb #2 with other constants.
1274       // It must be special-cased, because it has no out edges.
1275       early-&gt;add_inst(self);
1276       continue;
1277     }
1278 
1279     // No uses, just terminate
1280     if (self-&gt;outcnt() == 0) {
1281       assert(self-&gt;is_MachProj(), &quot;sanity&quot;);
1282       continue;                   // Must be a dead machine projection
1283     }
1284 
1285     // If node is pinned in the block, then no scheduling can be done.
1286     if( self-&gt;pinned() )          // Pinned in block?
1287       continue;
1288 
1289     MachNode* mach = self-&gt;is_Mach() ? self-&gt;as_Mach() : NULL;
1290     if (mach) {
1291       switch (mach-&gt;ideal_Opcode()) {
1292       case Op_CreateEx:
1293         // Don&#39;t move exception creation
1294         early-&gt;add_inst(self);
1295         continue;
1296         break;
1297       case Op_CheckCastPP: {
1298         // Don&#39;t move CheckCastPP nodes away from their input, if the input
1299         // is a rawptr (5071820).
1300         Node *def = self-&gt;in(1);
1301         if (def != NULL &amp;&amp; def-&gt;bottom_type()-&gt;base() == Type::RawPtr) {
1302           early-&gt;add_inst(self);
1303 #ifdef ASSERT
1304           _raw_oops.push(def);
1305 #endif
1306           continue;
1307         }
1308         break;
1309       }
1310       default:
1311         break;
1312       }
1313     }
1314 
1315     // Gather LCA of all uses
1316     Block *LCA = NULL;
1317     {
1318       for (DUIterator_Fast imax, i = self-&gt;fast_outs(imax); i &lt; imax; i++) {
1319         // For all uses, find LCA
1320         Node* use = self-&gt;fast_out(i);
1321         LCA = raise_LCA_above_use(LCA, use, self, this);
1322       }
1323       guarantee(LCA != NULL, &quot;There must be a LCA&quot;);
1324     }  // (Hide defs of imax, i from rest of block.)
1325 
1326     // Place temps in the block of their use.  This isn&#39;t a
1327     // requirement for correctness but it reduces useless
1328     // interference between temps and other nodes.
1329     if (mach != NULL &amp;&amp; mach-&gt;is_MachTemp()) {
1330       map_node_to_block(self, LCA);
1331       LCA-&gt;add_inst(self);
1332       continue;
1333     }
1334 
1335     // Check if &#39;self&#39; could be anti-dependent on memory
1336     if (self-&gt;needs_anti_dependence_check()) {
1337       // Hoist LCA above possible-defs and insert anti-dependences to
1338       // defs in new LCA block.
1339       LCA = insert_anti_dependences(LCA, self);
1340     }
1341 
1342     if (early-&gt;_dom_depth &gt; LCA-&gt;_dom_depth) {
1343       // Somehow the LCA has moved above the earliest legal point.
1344       // (One way this can happen is via memory_early_block.)
1345       if (C-&gt;subsume_loads() == true &amp;&amp; !C-&gt;failing()) {
1346         // Retry with subsume_loads == false
1347         // If this is the first failure, the sentinel string will &quot;stick&quot;
1348         // to the Compile object, and the C2Compiler will see it and retry.
1349         C-&gt;record_failure(C2Compiler::retry_no_subsuming_loads());
1350       } else {
1351         // Bailout without retry when (early-&gt;_dom_depth &gt; LCA-&gt;_dom_depth)
1352         assert(false, &quot;graph should be schedulable&quot;);
1353         C-&gt;record_method_not_compilable(&quot;late schedule failed: incorrect graph&quot;);
1354       }
1355       return;
1356     }
1357 
1358     // If there is no opportunity to hoist, then we&#39;re done.
1359     // In stress mode, try to hoist even the single operations.
1360     bool try_to_hoist = StressGCM || (LCA != early);
1361 
1362     // Must clone guys stay next to use; no hoisting allowed.
1363     // Also cannot hoist guys that alter memory or are otherwise not
1364     // allocatable (hoisting can make a value live longer, leading to
1365     // anti and output dependency problems which are normally resolved
1366     // by the register allocator giving everyone a different register).
1367     if (mach != NULL &amp;&amp; must_clone[mach-&gt;ideal_Opcode()])
1368       try_to_hoist = false;
1369 
1370     Block* late = NULL;
1371     if (try_to_hoist) {
1372       // Now find the block with the least execution frequency.
1373       // Start at the latest schedule and work up to the earliest schedule
1374       // in the dominator tree.  Thus the Node will dominate all its uses.
1375       late = hoist_to_cheaper_block(LCA, early, self);
1376     } else {
1377       // Just use the LCA of the uses.
1378       late = LCA;
1379     }
1380 
1381     // Put the node into target block
1382     schedule_node_into_block(self, late);
1383 
1384 #ifdef ASSERT
1385     if (self-&gt;needs_anti_dependence_check()) {
1386       // since precedence edges are only inserted when we&#39;re sure they
1387       // are needed make sure that after placement in a block we don&#39;t
1388       // need any new precedence edges.
1389       verify_anti_dependences(late, self);
1390     }
1391 #endif
1392   } // Loop until all nodes have been visited
1393 
1394 } // end ScheduleLate
1395 
1396 //------------------------------GlobalCodeMotion-------------------------------
1397 void PhaseCFG::global_code_motion() {
1398   ResourceMark rm;
1399 
1400 #ifndef PRODUCT
1401   if (trace_opto_pipelining()) {
1402     tty-&gt;print(&quot;\n---- Start GlobalCodeMotion ----\n&quot;);
1403   }
1404 #endif
1405 
1406   // Initialize the node to block mapping for things on the proj_list
1407   for (uint i = 0; i &lt; _matcher.number_of_projections(); i++) {
1408     unmap_node_from_block(_matcher.get_projection(i));
1409   }
1410 
1411   // Set the basic block for Nodes pinned into blocks
1412   Arena* arena = Thread::current()-&gt;resource_area();
1413   VectorSet visited(arena);
1414   schedule_pinned_nodes(visited);
1415 
1416   // Find the earliest Block any instruction can be placed in.  Some
1417   // instructions are pinned into Blocks.  Unpinned instructions can
1418   // appear in last block in which all their inputs occur.
1419   visited.clear();
1420   Node_Stack stack(arena, (C-&gt;live_nodes() &gt;&gt; 2) + 16); // pre-grow
1421   if (!schedule_early(visited, stack)) {
1422     // Bailout without retry
1423     C-&gt;record_method_not_compilable(&quot;early schedule failed&quot;);
1424     return;
1425   }
1426 
1427   // Build Def-Use edges.
1428   // Compute the latency information (via backwards walk) for all the
1429   // instructions in the graph
1430   _node_latency = new GrowableArray&lt;uint&gt;(); // resource_area allocation
1431 
1432   if (C-&gt;do_scheduling()) {
1433     compute_latencies_backwards(visited, stack);
1434   }
1435 
1436   // Now schedule all codes as LATE as possible.  This is the LCA in the
1437   // dominator tree of all USES of a value.  Pick the block with the least
1438   // loop nesting depth that is lowest in the dominator tree.
1439   // ( visited.clear() called in schedule_late()-&gt;Node_Backward_Iterator() )
1440   schedule_late(visited, stack);
1441   if (C-&gt;failing()) {
1442     // schedule_late fails only when graph is incorrect.
1443     assert(!VerifyGraphEdges, &quot;verification should have failed&quot;);
1444     return;
1445   }
1446 
1447 #ifndef PRODUCT
1448   if (trace_opto_pipelining()) {
1449     tty-&gt;print(&quot;\n---- Detect implicit null checks ----\n&quot;);
1450   }
1451 #endif
1452 
1453   // Detect implicit-null-check opportunities.  Basically, find NULL checks
1454   // with suitable memory ops nearby.  Use the memory op to do the NULL check.
1455   // I can generate a memory op if there is not one nearby.
1456   if (C-&gt;is_method_compilation()) {
1457     // By reversing the loop direction we get a very minor gain on mpegaudio.
1458     // Feel free to revert to a forward loop for clarity.
1459     // for( int i=0; i &lt; (int)matcher._null_check_tests.size(); i+=2 ) {
1460     for (int i = _matcher._null_check_tests.size() - 2; i &gt;= 0; i -= 2) {
1461       Node* proj = _matcher._null_check_tests[i];
1462       Node* val  = _matcher._null_check_tests[i + 1];
1463       Block* block = get_block_for_node(proj);
1464       implicit_null_check(block, proj, val, C-&gt;allowed_deopt_reasons());
1465       // The implicit_null_check will only perform the transformation
1466       // if the null branch is truly uncommon, *and* it leads to an
1467       // uncommon trap.  Combined with the too_many_traps guards
1468       // above, this prevents SEGV storms reported in 6366351,
1469       // by recompiling offending methods without this optimization.
1470     }
1471   }
1472 
1473   bool block_size_threshold_ok = false;
1474   intptr_t *recalc_pressure_nodes = NULL;
1475   if (OptoRegScheduling) {
1476     for (uint i = 0; i &lt; number_of_blocks(); i++) {
1477       Block* block = get_block(i);
1478       if (block-&gt;number_of_nodes() &gt; 10) {
1479         block_size_threshold_ok = true;
1480         break;
1481       }
1482     }
1483   }
1484 
1485   // Enabling the scheduler for register pressure plus finding blocks of size to schedule for it
1486   // is key to enabling this feature.
1487   PhaseChaitin regalloc(C-&gt;unique(), *this, _matcher, true);
1488   ResourceArea live_arena(mtCompiler);      // Arena for liveness
1489   ResourceMark rm_live(&amp;live_arena);
1490   PhaseLive live(*this, regalloc._lrg_map.names(), &amp;live_arena, true);
1491   PhaseIFG ifg(&amp;live_arena);
1492   if (OptoRegScheduling &amp;&amp; block_size_threshold_ok) {
1493     regalloc.mark_ssa();
1494     Compile::TracePhase tp(&quot;computeLive&quot;, &amp;timers[_t_computeLive]);
1495     rm_live.reset_to_mark();           // Reclaim working storage
1496     IndexSet::reset_memory(C, &amp;live_arena);
1497     uint node_size = regalloc._lrg_map.max_lrg_id();
1498     ifg.init(node_size); // Empty IFG
1499     regalloc.set_ifg(ifg);
1500     regalloc.set_live(live);
1501     regalloc.gather_lrg_masks(false);    // Collect LRG masks
1502     live.compute(node_size); // Compute liveness
1503 
1504     recalc_pressure_nodes = NEW_RESOURCE_ARRAY(intptr_t, node_size);
1505     for (uint i = 0; i &lt; node_size; i++) {
1506       recalc_pressure_nodes[i] = 0;
1507     }
1508   }
1509   _regalloc = &amp;regalloc;
1510 
1511 #ifndef PRODUCT
1512   if (trace_opto_pipelining()) {
1513     tty-&gt;print(&quot;\n---- Start Local Scheduling ----\n&quot;);
1514   }
1515 #endif
1516 
1517   // Schedule locally.  Right now a simple topological sort.
1518   // Later, do a real latency aware scheduler.
1519   GrowableArray&lt;int&gt; ready_cnt(C-&gt;unique(), C-&gt;unique(), -1);
1520   visited.reset();
1521   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1522     Block* block = get_block(i);
1523     if (!schedule_local(block, ready_cnt, visited, recalc_pressure_nodes)) {
1524       if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
1525         C-&gt;record_method_not_compilable(&quot;local schedule failed&quot;);
1526       }
1527       _regalloc = NULL;
1528       return;
1529     }
1530   }
1531   _regalloc = NULL;
1532 
1533   // If we inserted any instructions between a Call and his CatchNode,
1534   // clone the instructions on all paths below the Catch.
1535   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1536     Block* block = get_block(i);
1537     call_catch_cleanup(block);
1538   }
1539 
1540 #ifndef PRODUCT
1541   if (trace_opto_pipelining()) {
1542     tty-&gt;print(&quot;\n---- After GlobalCodeMotion ----\n&quot;);
1543     for (uint i = 0; i &lt; number_of_blocks(); i++) {
1544       Block* block = get_block(i);
1545       block-&gt;dump();
1546     }
1547   }
1548 #endif
1549   // Dead.
1550   _node_latency = (GrowableArray&lt;uint&gt; *)((intptr_t)0xdeadbeef);
1551 }
1552 
1553 bool PhaseCFG::do_global_code_motion() {
1554 
1555   build_dominator_tree();
1556   if (C-&gt;failing()) {
1557     return false;
1558   }
1559 
1560   NOT_PRODUCT( C-&gt;verify_graph_edges(); )
1561 
1562   estimate_block_frequency();
1563 
1564   global_code_motion();
1565 
1566   if (C-&gt;failing()) {
1567     return false;
1568   }
1569 
1570   return true;
1571 }
1572 
1573 //------------------------------Estimate_Block_Frequency-----------------------
1574 // Estimate block frequencies based on IfNode probabilities.
1575 void PhaseCFG::estimate_block_frequency() {
1576 
1577   // Force conditional branches leading to uncommon traps to be unlikely,
1578   // not because we get to the uncommon_trap with less relative frequency,
1579   // but because an uncommon_trap typically causes a deopt, so we only get
1580   // there once.
1581   if (C-&gt;do_freq_based_layout()) {
1582     Block_List worklist;
1583     Block* root_blk = get_block(0);
1584     for (uint i = 1; i &lt; root_blk-&gt;num_preds(); i++) {
1585       Block *pb = get_block_for_node(root_blk-&gt;pred(i));
1586       if (pb-&gt;has_uncommon_code()) {
1587         worklist.push(pb);
1588       }
1589     }
1590     while (worklist.size() &gt; 0) {
1591       Block* uct = worklist.pop();
1592       if (uct == get_root_block()) {
1593         continue;
1594       }
1595       for (uint i = 1; i &lt; uct-&gt;num_preds(); i++) {
1596         Block *pb = get_block_for_node(uct-&gt;pred(i));
1597         if (pb-&gt;_num_succs == 1) {
1598           worklist.push(pb);
1599         } else if (pb-&gt;num_fall_throughs() == 2) {
1600           pb-&gt;update_uncommon_branch(uct);
1601         }
1602       }
1603     }
1604   }
1605 
1606   // Create the loop tree and calculate loop depth.
1607   _root_loop = create_loop_tree();
1608   _root_loop-&gt;compute_loop_depth(0);
1609 
1610   // Compute block frequency of each block, relative to a single loop entry.
1611   _root_loop-&gt;compute_freq();
1612 
1613   // Adjust all frequencies to be relative to a single method entry
1614   _root_loop-&gt;_freq = 1.0;
1615   _root_loop-&gt;scale_freq();
1616 
1617   // Save outmost loop frequency for LRG frequency threshold
1618   _outer_loop_frequency = _root_loop-&gt;outer_loop_freq();
1619 
1620   // force paths ending at uncommon traps to be infrequent
1621   if (!C-&gt;do_freq_based_layout()) {
1622     Block_List worklist;
1623     Block* root_blk = get_block(0);
1624     for (uint i = 1; i &lt; root_blk-&gt;num_preds(); i++) {
1625       Block *pb = get_block_for_node(root_blk-&gt;pred(i));
1626       if (pb-&gt;has_uncommon_code()) {
1627         worklist.push(pb);
1628       }
1629     }
1630     while (worklist.size() &gt; 0) {
1631       Block* uct = worklist.pop();
1632       uct-&gt;_freq = PROB_MIN;
1633       for (uint i = 1; i &lt; uct-&gt;num_preds(); i++) {
1634         Block *pb = get_block_for_node(uct-&gt;pred(i));
1635         if (pb-&gt;_num_succs == 1 &amp;&amp; pb-&gt;_freq &gt; PROB_MIN) {
1636           worklist.push(pb);
1637         }
1638       }
1639     }
1640   }
1641 
1642 #ifdef ASSERT
1643   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1644     Block* b = get_block(i);
1645     assert(b-&gt;_freq &gt;= MIN_BLOCK_FREQUENCY, &quot;Register Allocator requires meaningful block frequency&quot;);
1646   }
1647 #endif
1648 
1649 #ifndef PRODUCT
1650   if (PrintCFGBlockFreq) {
1651     tty-&gt;print_cr(&quot;CFG Block Frequencies&quot;);
1652     _root_loop-&gt;dump_tree();
1653     if (Verbose) {
1654       tty-&gt;print_cr(&quot;PhaseCFG dump&quot;);
1655       dump();
1656       tty-&gt;print_cr(&quot;Node dump&quot;);
1657       _root-&gt;dump(99999);
1658     }
1659   }
1660 #endif
1661 }
1662 
1663 //----------------------------create_loop_tree--------------------------------
1664 // Create a loop tree from the CFG
1665 CFGLoop* PhaseCFG::create_loop_tree() {
1666 
1667 #ifdef ASSERT
1668   assert(get_block(0) == get_root_block(), &quot;first block should be root block&quot;);
1669   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1670     Block* block = get_block(i);
1671     // Check that _loop field are clear...we could clear them if not.
1672     assert(block-&gt;_loop == NULL, &quot;clear _loop expected&quot;);
1673     // Sanity check that the RPO numbering is reflected in the _blocks array.
1674     // It doesn&#39;t have to be for the loop tree to be built, but if it is not,
1675     // then the blocks have been reordered since dom graph building...which
1676     // may question the RPO numbering
1677     assert(block-&gt;_rpo == i, &quot;unexpected reverse post order number&quot;);
1678   }
1679 #endif
1680 
1681   int idct = 0;
1682   CFGLoop* root_loop = new CFGLoop(idct++);
1683 
1684   Block_List worklist;
1685 
1686   // Assign blocks to loops
1687   for(uint i = number_of_blocks() - 1; i &gt; 0; i-- ) { // skip Root block
1688     Block* block = get_block(i);
1689 
1690     if (block-&gt;head()-&gt;is_Loop()) {
1691       Block* loop_head = block;
1692       assert(loop_head-&gt;num_preds() - 1 == 2, &quot;loop must have 2 predecessors&quot;);
1693       Node* tail_n = loop_head-&gt;pred(LoopNode::LoopBackControl);
1694       Block* tail = get_block_for_node(tail_n);
1695 
1696       // Defensively filter out Loop nodes for non-single-entry loops.
1697       // For all reasonable loops, the head occurs before the tail in RPO.
1698       if (i &lt;= tail-&gt;_rpo) {
1699 
1700         // The tail and (recursive) predecessors of the tail
1701         // are made members of a new loop.
1702 
1703         assert(worklist.size() == 0, &quot;nonempty worklist&quot;);
1704         CFGLoop* nloop = new CFGLoop(idct++);
1705         assert(loop_head-&gt;_loop == NULL, &quot;just checking&quot;);
1706         loop_head-&gt;_loop = nloop;
1707         // Add to nloop so push_pred() will skip over inner loops
1708         nloop-&gt;add_member(loop_head);
1709         nloop-&gt;push_pred(loop_head, LoopNode::LoopBackControl, worklist, this);
1710 
1711         while (worklist.size() &gt; 0) {
1712           Block* member = worklist.pop();
1713           if (member != loop_head) {
1714             for (uint j = 1; j &lt; member-&gt;num_preds(); j++) {
1715               nloop-&gt;push_pred(member, j, worklist, this);
1716             }
1717           }
1718         }
1719       }
1720     }
1721   }
1722 
1723   // Create a member list for each loop consisting
1724   // of both blocks and (immediate child) loops.
1725   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1726     Block* block = get_block(i);
1727     CFGLoop* lp = block-&gt;_loop;
1728     if (lp == NULL) {
1729       // Not assigned to a loop. Add it to the method&#39;s pseudo loop.
1730       block-&gt;_loop = root_loop;
1731       lp = root_loop;
1732     }
1733     if (lp == root_loop || block != lp-&gt;head()) { // loop heads are already members
1734       lp-&gt;add_member(block);
1735     }
1736     if (lp != root_loop) {
1737       if (lp-&gt;parent() == NULL) {
1738         // Not a nested loop. Make it a child of the method&#39;s pseudo loop.
1739         root_loop-&gt;add_nested_loop(lp);
1740       }
1741       if (block == lp-&gt;head()) {
1742         // Add nested loop to member list of parent loop.
1743         lp-&gt;parent()-&gt;add_member(lp);
1744       }
1745     }
1746   }
1747 
1748   return root_loop;
1749 }
1750 
1751 //------------------------------push_pred--------------------------------------
1752 void CFGLoop::push_pred(Block* blk, int i, Block_List&amp; worklist, PhaseCFG* cfg) {
1753   Node* pred_n = blk-&gt;pred(i);
1754   Block* pred = cfg-&gt;get_block_for_node(pred_n);
1755   CFGLoop *pred_loop = pred-&gt;_loop;
1756   if (pred_loop == NULL) {
1757     // Filter out blocks for non-single-entry loops.
1758     // For all reasonable loops, the head occurs before the tail in RPO.
1759     if (pred-&gt;_rpo &gt; head()-&gt;_rpo) {
1760       pred-&gt;_loop = this;
1761       worklist.push(pred);
1762     }
1763   } else if (pred_loop != this) {
1764     // Nested loop.
1765     while (pred_loop-&gt;_parent != NULL &amp;&amp; pred_loop-&gt;_parent != this) {
1766       pred_loop = pred_loop-&gt;_parent;
1767     }
1768     // Make pred&#39;s loop be a child
1769     if (pred_loop-&gt;_parent == NULL) {
1770       add_nested_loop(pred_loop);
1771       // Continue with loop entry predecessor.
1772       Block* pred_head = pred_loop-&gt;head();
1773       assert(pred_head-&gt;num_preds() - 1 == 2, &quot;loop must have 2 predecessors&quot;);
1774       assert(pred_head != head(), &quot;loop head in only one loop&quot;);
1775       push_pred(pred_head, LoopNode::EntryControl, worklist, cfg);
1776     } else {
1777       assert(pred_loop-&gt;_parent == this &amp;&amp; _parent == NULL, &quot;just checking&quot;);
1778     }
1779   }
1780 }
1781 
1782 //------------------------------add_nested_loop--------------------------------
1783 // Make cl a child of the current loop in the loop tree.
1784 void CFGLoop::add_nested_loop(CFGLoop* cl) {
1785   assert(_parent == NULL, &quot;no parent yet&quot;);
1786   assert(cl != this, &quot;not my own parent&quot;);
1787   cl-&gt;_parent = this;
1788   CFGLoop* ch = _child;
1789   if (ch == NULL) {
1790     _child = cl;
1791   } else {
1792     while (ch-&gt;_sibling != NULL) { ch = ch-&gt;_sibling; }
1793     ch-&gt;_sibling = cl;
1794   }
1795 }
1796 
1797 //------------------------------compute_loop_depth-----------------------------
1798 // Store the loop depth in each CFGLoop object.
1799 // Recursively walk the children to do the same for them.
1800 void CFGLoop::compute_loop_depth(int depth) {
1801   _depth = depth;
1802   CFGLoop* ch = _child;
1803   while (ch != NULL) {
1804     ch-&gt;compute_loop_depth(depth + 1);
1805     ch = ch-&gt;_sibling;
1806   }
1807 }
1808 
1809 //------------------------------compute_freq-----------------------------------
1810 // Compute the frequency of each block and loop, relative to a single entry
1811 // into the dominating loop head.
1812 void CFGLoop::compute_freq() {
1813   // Bottom up traversal of loop tree (visit inner loops first.)
1814   // Set loop head frequency to 1.0, then transitively
1815   // compute frequency for all successors in the loop,
1816   // as well as for each exit edge.  Inner loops are
1817   // treated as single blocks with loop exit targets
1818   // as the successor blocks.
1819 
1820   // Nested loops first
1821   CFGLoop* ch = _child;
1822   while (ch != NULL) {
1823     ch-&gt;compute_freq();
1824     ch = ch-&gt;_sibling;
1825   }
1826   assert (_members.length() &gt; 0, &quot;no empty loops&quot;);
1827   Block* hd = head();
1828   hd-&gt;_freq = 1.0;
1829   for (int i = 0; i &lt; _members.length(); i++) {
1830     CFGElement* s = _members.at(i);
1831     double freq = s-&gt;_freq;
1832     if (s-&gt;is_block()) {
1833       Block* b = s-&gt;as_Block();
1834       for (uint j = 0; j &lt; b-&gt;_num_succs; j++) {
1835         Block* sb = b-&gt;_succs[j];
1836         update_succ_freq(sb, freq * b-&gt;succ_prob(j));
1837       }
1838     } else {
1839       CFGLoop* lp = s-&gt;as_CFGLoop();
1840       assert(lp-&gt;_parent == this, &quot;immediate child&quot;);
1841       for (int k = 0; k &lt; lp-&gt;_exits.length(); k++) {
1842         Block* eb = lp-&gt;_exits.at(k).get_target();
1843         double prob = lp-&gt;_exits.at(k).get_prob();
1844         update_succ_freq(eb, freq * prob);
1845       }
1846     }
1847   }
1848 
1849   // For all loops other than the outer, &quot;method&quot; loop,
1850   // sum and normalize the exit probability. The &quot;method&quot; loop
1851   // should keep the initial exit probability of 1, so that
1852   // inner blocks do not get erroneously scaled.
1853   if (_depth != 0) {
1854     // Total the exit probabilities for this loop.
1855     double exits_sum = 0.0f;
1856     for (int i = 0; i &lt; _exits.length(); i++) {
1857       exits_sum += _exits.at(i).get_prob();
1858     }
1859 
1860     // Normalize the exit probabilities. Until now, the
1861     // probabilities estimate the possibility of exit per
1862     // a single loop iteration; afterward, they estimate
1863     // the probability of exit per loop entry.
1864     for (int i = 0; i &lt; _exits.length(); i++) {
1865       Block* et = _exits.at(i).get_target();
1866       float new_prob = 0.0f;
1867       if (_exits.at(i).get_prob() &gt; 0.0f) {
1868         new_prob = _exits.at(i).get_prob() / exits_sum;
1869       }
1870       BlockProbPair bpp(et, new_prob);
1871       _exits.at_put(i, bpp);
1872     }
1873 
1874     // Save the total, but guard against unreasonable probability,
1875     // as the value is used to estimate the loop trip count.
1876     // An infinite trip count would blur relative block
1877     // frequencies.
1878     if (exits_sum &gt; 1.0f) exits_sum = 1.0;
1879     if (exits_sum &lt; PROB_MIN) exits_sum = PROB_MIN;
1880     _exit_prob = exits_sum;
1881   }
1882 }
1883 
1884 //------------------------------succ_prob-------------------------------------
1885 // Determine the probability of reaching successor &#39;i&#39; from the receiver block.
1886 float Block::succ_prob(uint i) {
1887   int eidx = end_idx();
1888   Node *n = get_node(eidx);  // Get ending Node
1889 
1890   int op = n-&gt;Opcode();
1891   if (n-&gt;is_Mach()) {
1892     if (n-&gt;is_MachNullCheck()) {
1893       // Can only reach here if called after lcm. The original Op_If is gone,
1894       // so we attempt to infer the probability from one or both of the
1895       // successor blocks.
1896       assert(_num_succs == 2, &quot;expecting 2 successors of a null check&quot;);
1897       // If either successor has only one predecessor, then the
1898       // probability estimate can be derived using the
1899       // relative frequency of the successor and this block.
1900       if (_succs[i]-&gt;num_preds() == 2) {
1901         return _succs[i]-&gt;_freq / _freq;
1902       } else if (_succs[1-i]-&gt;num_preds() == 2) {
1903         return 1 - (_succs[1-i]-&gt;_freq / _freq);
1904       } else {
1905         // Estimate using both successor frequencies
1906         float freq = _succs[i]-&gt;_freq;
1907         return freq / (freq + _succs[1-i]-&gt;_freq);
1908       }
1909     }
1910     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1911   }
1912 
1913 
1914   // Switch on branch type
1915   switch( op ) {
1916   case Op_CountedLoopEnd:
1917   case Op_If: {
1918     assert (i &lt; 2, &quot;just checking&quot;);
1919     // Conditionals pass on only part of their frequency
1920     float prob  = n-&gt;as_MachIf()-&gt;_prob;
1921     assert(prob &gt;= 0.0 &amp;&amp; prob &lt;= 1.0, &quot;out of range probability&quot;);
1922     // If succ[i] is the FALSE branch, invert path info
1923     if( get_node(i + eidx + 1)-&gt;Opcode() == Op_IfFalse ) {
1924       return 1.0f - prob; // not taken
1925     } else {
1926       return prob; // taken
1927     }
1928   }
1929 
1930   case Op_Jump:
1931     return n-&gt;as_MachJump()-&gt;_probs[get_node(i + eidx + 1)-&gt;as_JumpProj()-&gt;_con];
1932 
1933   case Op_Catch: {
1934     const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1935     if (ci-&gt;_con == CatchProjNode::fall_through_index) {
1936       // Fall-thru path gets the lion&#39;s share.
1937       return 1.0f - PROB_UNLIKELY_MAG(5)*_num_succs;
1938     } else {
1939       // Presume exceptional paths are equally unlikely
1940       return PROB_UNLIKELY_MAG(5);
1941     }
1942   }
1943 
1944   case Op_Root:
1945   case Op_Goto:
1946     // Pass frequency straight thru to target
1947     return 1.0f;
1948 
1949   case Op_NeverBranch:
1950     return 0.0f;
1951 
1952   case Op_TailCall:
1953   case Op_TailJump:
1954   case Op_Return:
1955   case Op_Halt:
1956   case Op_Rethrow:
1957     // Do not push out freq to root block
1958     return 0.0f;
1959 
1960   default:
1961     ShouldNotReachHere();
1962   }
1963 
1964   return 0.0f;
1965 }
1966 
1967 //------------------------------num_fall_throughs-----------------------------
1968 // Return the number of fall-through candidates for a block
1969 int Block::num_fall_throughs() {
1970   int eidx = end_idx();
1971   Node *n = get_node(eidx);  // Get ending Node
1972 
1973   int op = n-&gt;Opcode();
1974   if (n-&gt;is_Mach()) {
1975     if (n-&gt;is_MachNullCheck()) {
1976       // In theory, either side can fall-thru, for simplicity sake,
1977       // let&#39;s say only the false branch can now.
1978       return 1;
1979     }
1980     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1981   }
1982 
1983   // Switch on branch type
1984   switch( op ) {
1985   case Op_CountedLoopEnd:
1986   case Op_If:
1987     return 2;
1988 
1989   case Op_Root:
1990   case Op_Goto:
1991     return 1;
1992 
1993   case Op_Catch: {
1994     for (uint i = 0; i &lt; _num_succs; i++) {
1995       const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1996       if (ci-&gt;_con == CatchProjNode::fall_through_index) {
1997         return 1;
1998       }
1999     }
2000     return 0;
2001   }
2002 
2003   case Op_Jump:
2004   case Op_NeverBranch:
2005   case Op_TailCall:
2006   case Op_TailJump:
2007   case Op_Return:
2008   case Op_Halt:
2009   case Op_Rethrow:
2010     return 0;
2011 
2012   default:
2013     ShouldNotReachHere();
2014   }
2015 
2016   return 0;
2017 }
2018 
2019 //------------------------------succ_fall_through-----------------------------
2020 // Return true if a specific successor could be fall-through target.
2021 bool Block::succ_fall_through(uint i) {
2022   int eidx = end_idx();
2023   Node *n = get_node(eidx);  // Get ending Node
2024 
2025   int op = n-&gt;Opcode();
2026   if (n-&gt;is_Mach()) {
2027     if (n-&gt;is_MachNullCheck()) {
2028       // In theory, either side can fall-thru, for simplicity sake,
2029       // let&#39;s say only the false branch can now.
2030       return get_node(i + eidx + 1)-&gt;Opcode() == Op_IfFalse;
2031     }
2032     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
2033   }
2034 
2035   // Switch on branch type
2036   switch( op ) {
2037   case Op_CountedLoopEnd:
2038   case Op_If:
2039   case Op_Root:
2040   case Op_Goto:
2041     return true;
2042 
2043   case Op_Catch: {
2044     const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
2045     return ci-&gt;_con == CatchProjNode::fall_through_index;
2046   }
2047 
2048   case Op_Jump:
2049   case Op_NeverBranch:
2050   case Op_TailCall:
2051   case Op_TailJump:
2052   case Op_Return:
2053   case Op_Halt:
2054   case Op_Rethrow:
2055     return false;
2056 
2057   default:
2058     ShouldNotReachHere();
2059   }
2060 
2061   return false;
2062 }
2063 
2064 //------------------------------update_uncommon_branch------------------------
2065 // Update the probability of a two-branch to be uncommon
2066 void Block::update_uncommon_branch(Block* ub) {
2067   int eidx = end_idx();
2068   Node *n = get_node(eidx);  // Get ending Node
2069 
2070   int op = n-&gt;as_Mach()-&gt;ideal_Opcode();
2071 
2072   assert(op == Op_CountedLoopEnd || op == Op_If, &quot;must be a If&quot;);
2073   assert(num_fall_throughs() == 2, &quot;must be a two way branch block&quot;);
2074 
2075   // Which successor is ub?
2076   uint s;
2077   for (s = 0; s &lt;_num_succs; s++) {
2078     if (_succs[s] == ub) break;
2079   }
2080   assert(s &lt; 2, &quot;uncommon successor must be found&quot;);
2081 
2082   // If ub is the true path, make the proability small, else
2083   // ub is the false path, and make the probability large
2084   bool invert = (get_node(s + eidx + 1)-&gt;Opcode() == Op_IfFalse);
2085 
2086   // Get existing probability
2087   float p = n-&gt;as_MachIf()-&gt;_prob;
2088 
2089   if (invert) p = 1.0 - p;
2090   if (p &gt; PROB_MIN) {
2091     p = PROB_MIN;
2092   }
2093   if (invert) p = 1.0 - p;
2094 
2095   n-&gt;as_MachIf()-&gt;_prob = p;
2096 }
2097 
2098 //------------------------------update_succ_freq-------------------------------
2099 // Update the appropriate frequency associated with block &#39;b&#39;, a successor of
2100 // a block in this loop.
2101 void CFGLoop::update_succ_freq(Block* b, double freq) {
2102   if (b-&gt;_loop == this) {
2103     if (b == head()) {
2104       // back branch within the loop
2105       // Do nothing now, the loop carried frequency will be
2106       // adjust later in scale_freq().
2107     } else {
2108       // simple branch within the loop
2109       b-&gt;_freq += freq;
2110     }
2111   } else if (!in_loop_nest(b)) {
2112     // branch is exit from this loop
2113     BlockProbPair bpp(b, freq);
2114     _exits.append(bpp);
2115   } else {
2116     // branch into nested loop
2117     CFGLoop* ch = b-&gt;_loop;
2118     ch-&gt;_freq += freq;
2119   }
2120 }
2121 
2122 //------------------------------in_loop_nest-----------------------------------
2123 // Determine if block b is in the receiver&#39;s loop nest.
2124 bool CFGLoop::in_loop_nest(Block* b) {
2125   int depth = _depth;
2126   CFGLoop* b_loop = b-&gt;_loop;
2127   int b_depth = b_loop-&gt;_depth;
2128   if (depth == b_depth) {
2129     return true;
2130   }
2131   while (b_depth &gt; depth) {
2132     b_loop = b_loop-&gt;_parent;
2133     b_depth = b_loop-&gt;_depth;
2134   }
2135   return b_loop == this;
2136 }
2137 
2138 //------------------------------scale_freq-------------------------------------
2139 // Scale frequency of loops and blocks by trip counts from outer loops
2140 // Do a top down traversal of loop tree (visit outer loops first.)
2141 void CFGLoop::scale_freq() {
2142   double loop_freq = _freq * trip_count();
2143   _freq = loop_freq;
2144   for (int i = 0; i &lt; _members.length(); i++) {
2145     CFGElement* s = _members.at(i);
2146     double block_freq = s-&gt;_freq * loop_freq;
2147     if (g_isnan(block_freq) || block_freq &lt; MIN_BLOCK_FREQUENCY)
2148       block_freq = MIN_BLOCK_FREQUENCY;
2149     s-&gt;_freq = block_freq;
2150   }
2151   CFGLoop* ch = _child;
2152   while (ch != NULL) {
2153     ch-&gt;scale_freq();
2154     ch = ch-&gt;_sibling;
2155   }
2156 }
2157 
2158 // Frequency of outer loop
2159 double CFGLoop::outer_loop_freq() const {
2160   if (_child != NULL) {
2161     return _child-&gt;_freq;
2162   }
2163   return _freq;
2164 }
2165 
2166 #ifndef PRODUCT
2167 //------------------------------dump_tree--------------------------------------
2168 void CFGLoop::dump_tree() const {
2169   dump();
2170   if (_child != NULL)   _child-&gt;dump_tree();
2171   if (_sibling != NULL) _sibling-&gt;dump_tree();
2172 }
2173 
2174 //------------------------------dump-------------------------------------------
2175 void CFGLoop::dump() const {
2176   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2177   tty-&gt;print(&quot;%s: %d  trip_count: %6.0f freq: %6.0f\n&quot;,
2178              _depth == 0 ? &quot;Method&quot; : &quot;Loop&quot;, _id, trip_count(), _freq);
2179   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2180   tty-&gt;print(&quot;         members:&quot;);
2181   int k = 0;
2182   for (int i = 0; i &lt; _members.length(); i++) {
2183     if (k++ &gt;= 6) {
2184       tty-&gt;print(&quot;\n              &quot;);
2185       for (int j = 0; j &lt; _depth+1; j++) tty-&gt;print(&quot;   &quot;);
2186       k = 0;
2187     }
2188     CFGElement *s = _members.at(i);
2189     if (s-&gt;is_block()) {
2190       Block *b = s-&gt;as_Block();
2191       tty-&gt;print(&quot; B%d(%6.3f)&quot;, b-&gt;_pre_order, b-&gt;_freq);
2192     } else {
2193       CFGLoop* lp = s-&gt;as_CFGLoop();
2194       tty-&gt;print(&quot; L%d(%6.3f)&quot;, lp-&gt;_id, lp-&gt;_freq);
2195     }
2196   }
2197   tty-&gt;print(&quot;\n&quot;);
2198   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2199   tty-&gt;print(&quot;         exits:  &quot;);
2200   k = 0;
2201   for (int i = 0; i &lt; _exits.length(); i++) {
2202     if (k++ &gt;= 7) {
2203       tty-&gt;print(&quot;\n              &quot;);
2204       for (int j = 0; j &lt; _depth+1; j++) tty-&gt;print(&quot;   &quot;);
2205       k = 0;
2206     }
2207     Block *blk = _exits.at(i).get_target();
2208     double prob = _exits.at(i).get_prob();
2209     tty-&gt;print(&quot; -&gt;%d@%d%%&quot;, blk-&gt;_pre_order, (int)(prob*100));
2210   }
2211   tty-&gt;print(&quot;\n&quot;);
2212 }
2213 #endif
    </pre>
  </body>
</html>