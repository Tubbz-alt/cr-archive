diff a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
@@ -3692,10 +3692,52 @@
      _shadow_space_bytes(shadow_space_bytes),
      _input_registers(input_registers),
      _output_registers(output_registers) {}
   void generate();
 
+  void spill_register(VMReg reg) {
+    assert(reg->is_reg(), "must be a register");
+    MacroAssembler* masm = _masm;
+    if (reg->is_Register()) {
+      __ push(reg->as_Register());
+    } else if (reg->is_XMMRegister()) {
+      if (UseAVX >= 3) {
+        __ subptr(rsp, 64); // bytes
+        __ evmovdqul(Address(rsp, 0), reg->as_XMMRegister(), Assembler::AVX_512bit);
+      } else if (UseAVX >= 1) {
+        __ subptr(rsp, 32);
+        __ vmovdqu(Address(rsp, 0), reg->as_XMMRegister());
+      } else {
+        __ subptr(rsp, 16);
+        __ movdqu(Address(rsp, 0), reg->as_XMMRegister());
+      }
+    } else {
+      ShouldNotReachHere();
+    }
+  }
+
+  void fill_register(VMReg reg) {
+    assert(reg->is_reg(), "must be a register");
+    MacroAssembler* masm = _masm;
+    if (reg->is_Register()) {
+      __ pop(reg->as_Register());
+    } else if (reg->is_XMMRegister()) {
+      if (UseAVX >= 3) {
+        __ evmovdqul(reg->as_XMMRegister(), Address(rsp, 0), Assembler::AVX_512bit);
+        __ addptr(rsp, 64); // bytes
+      } else if (UseAVX >= 1) {
+        __ vmovdqu(reg->as_XMMRegister(), Address(rsp, 0));
+        __ addptr(rsp, 32);
+      } else {
+        __ movdqu(reg->as_XMMRegister(), Address(rsp, 0));
+        __ addptr(rsp, 16);
+      }
+    } else {
+      ShouldNotReachHere();
+    }
+  }
+
 private:
 #ifdef ASSERT
 bool target_uses_register(VMReg reg) {
   return _input_registers.contains(reg) || _output_registers.contains(reg);
 }
@@ -3792,11 +3834,11 @@
   __ block_comment("{ L_safepoint_poll_slow_path");
   __ bind(L_safepoint_poll_slow_path);
   __ vzeroupper();
 
   if (need_spills) {
-    __ spill_register(ret_reg);
+    spill_register(ret_reg);
   }
 
   __ mov(c_rarg0, r15_thread);
   __ mov(r12, rsp); // remember sp
   __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
@@ -3804,11 +3846,11 @@
   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));
   __ mov(rsp, r12); // restore sp
   __ reinit_heapbase();
 
   if (need_spills) {
-    __ fill_register(ret_reg);
+    fill_register(ret_reg);
   }
 
   __ jmp(L_after_safepoint_poll);
   __ block_comment("} L_safepoint_poll_slow_path");
 
@@ -3817,22 +3859,22 @@
   __ block_comment("{ L_reguard");
   __ bind(L_reguard);
   __ vzeroupper();
 
   if (need_spills) {
-    __ spill_register(ret_reg);
+    spill_register(ret_reg);
   }
 
   __ mov(r12, rsp); // remember sp
   __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
   __ andptr(rsp, -16); // align stack as required by ABI
   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));
   __ mov(rsp, r12); // restore sp
   __ reinit_heapbase();
 
   if (need_spills) {
-    __ fill_register(ret_reg);
+    fill_register(ret_reg);
   }
 
   __ jmp(L_after_reguard);
 
   __ block_comment("} L_reguard");
