diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -7869,34 +7869,10 @@
   jcc(Assembler::notZero, copy_chars_loop);
 
   bind(done);
 }
 
-void MacroAssembler::spill_register(VMReg reg) {
-  assert(reg->is_reg(), "must be a register");
-  if (reg->is_Register()) {
-    push(reg->as_Register());
-  } else if (reg->is_XMMRegister()) {
-    subptr(rsp, 16); // 16 bytes
-    movdqu(Address(rsp, 0), reg->as_XMMRegister());
-  } else {
-    ShouldNotReachHere();
-  }
-}
-
-void MacroAssembler::fill_register(VMReg reg) {
-  assert(reg->is_reg(), "must be a register");
-  if (reg->is_Register()) {
-    pop(reg->as_Register());
-  } else if (reg->is_XMMRegister()) {
-    movdqu(reg->as_XMMRegister(), Address(rsp, 0));
-    addptr(rsp, 16); // 16 bytes
-  } else {
-    ShouldNotReachHere();
-  }
-}
-
 #ifdef _LP64
 void MacroAssembler::convert_f2i(Register dst, XMMRegister src) {
   Label done;
   cvttss2sil(dst, src);
   // Conversion instructions do not match JLS for overflow, underflow and NaN -> fixup in stub
diff a/src/hotspot/cpu/x86/macroAssembler_x86.hpp b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
@@ -1719,14 +1719,10 @@
 
   // Inflate byte[] array to char[].
   void byte_array_inflate(Register src, Register dst, Register len,
                           XMMRegister tmp1, Register tmp2);
 
-  // save/restore to/from stack
-  void spill_register(VMReg reg);
-  void fill_register(VMReg reg);
-
 #ifdef _LP64
   void convert_f2i(Register dst, XMMRegister src);
   void convert_d2i(Register dst, XMMRegister src);
   void convert_f2l(Register dst, XMMRegister src);
   void convert_d2l(Register dst, XMMRegister src);
diff a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
@@ -3692,10 +3692,52 @@
      _shadow_space_bytes(shadow_space_bytes),
      _input_registers(input_registers),
      _output_registers(output_registers) {}
   void generate();
 
+  void spill_register(VMReg reg) {
+    assert(reg->is_reg(), "must be a register");
+    MacroAssembler* masm = _masm;
+    if (reg->is_Register()) {
+      __ push(reg->as_Register());
+    } else if (reg->is_XMMRegister()) {
+      if (UseAVX >= 3) {
+        __ subptr(rsp, 64); // bytes
+        __ evmovdqul(Address(rsp, 0), reg->as_XMMRegister(), Assembler::AVX_512bit);
+      } else if (UseAVX >= 1) {
+        __ subptr(rsp, 32);
+        __ vmovdqu(Address(rsp, 0), reg->as_XMMRegister());
+      } else {
+        __ subptr(rsp, 16);
+        __ movdqu(Address(rsp, 0), reg->as_XMMRegister());
+      }
+    } else {
+      ShouldNotReachHere();
+    }
+  }
+
+  void fill_register(VMReg reg) {
+    assert(reg->is_reg(), "must be a register");
+    MacroAssembler* masm = _masm;
+    if (reg->is_Register()) {
+      __ pop(reg->as_Register());
+    } else if (reg->is_XMMRegister()) {
+      if (UseAVX >= 3) {
+        __ evmovdqul(reg->as_XMMRegister(), Address(rsp, 0), Assembler::AVX_512bit);
+        __ addptr(rsp, 64); // bytes
+      } else if (UseAVX >= 1) {
+        __ vmovdqu(reg->as_XMMRegister(), Address(rsp, 0));
+        __ addptr(rsp, 32);
+      } else {
+        __ movdqu(reg->as_XMMRegister(), Address(rsp, 0));
+        __ addptr(rsp, 16);
+      }
+    } else {
+      ShouldNotReachHere();
+    }
+  }
+
 private:
 #ifdef ASSERT
 bool target_uses_register(VMReg reg) {
   return _input_registers.contains(reg) || _output_registers.contains(reg);
 }
@@ -3792,11 +3834,11 @@
   __ block_comment("{ L_safepoint_poll_slow_path");
   __ bind(L_safepoint_poll_slow_path);
   __ vzeroupper();
 
   if (need_spills) {
-    __ spill_register(ret_reg);
+    spill_register(ret_reg);
   }
 
   __ mov(c_rarg0, r15_thread);
   __ mov(r12, rsp); // remember sp
   __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
@@ -3804,11 +3846,11 @@
   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));
   __ mov(rsp, r12); // restore sp
   __ reinit_heapbase();
 
   if (need_spills) {
-    __ fill_register(ret_reg);
+    fill_register(ret_reg);
   }
 
   __ jmp(L_after_safepoint_poll);
   __ block_comment("} L_safepoint_poll_slow_path");
 
@@ -3817,22 +3859,22 @@
   __ block_comment("{ L_reguard");
   __ bind(L_reguard);
   __ vzeroupper();
 
   if (need_spills) {
-    __ spill_register(ret_reg);
+    spill_register(ret_reg);
   }
 
   __ mov(r12, rsp); // remember sp
   __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
   __ andptr(rsp, -16); // align stack as required by ABI
   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));
   __ mov(rsp, r12); // restore sp
   __ reinit_heapbase();
 
   if (need_spills) {
-    __ fill_register(ret_reg);
+    fill_register(ret_reg);
   }
 
   __ jmp(L_after_reguard);
 
   __ block_comment("} L_reguard");
diff a/src/hotspot/share/opto/callGenerator.cpp b/src/hotspot/share/opto/callGenerator.cpp
--- a/src/hotspot/share/opto/callGenerator.cpp
+++ b/src/hotspot/share/opto/callGenerator.cpp
@@ -1000,11 +1000,10 @@
       if (nep->Opcode() == Op_ConP) {
         const TypeOopPtr* oop_ptr = nep->bottom_type()->is_oopptr();
         ciNativeEntryPoint* nep = oop_ptr->const_oop()->as_native_entry_point();
         return new NativeCallGenerator(callee, nep);
       } else {
-        // can this happen?
         print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),
                                "NativeEntryPoint not constant");
       }
     }
     break;
