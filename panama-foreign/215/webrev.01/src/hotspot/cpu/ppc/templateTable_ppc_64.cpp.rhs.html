<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/ppc/templateTable_ppc_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2014, 2020, Oracle and/or its affiliates. All rights reserved.
<a name="1" id="anc1"></a><span class="line-modified">   3  * Copyright (c) 2013, 2020 SAP SE. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  29 #include &quot;interpreter/interpreter.hpp&quot;
  30 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  31 #include &quot;interpreter/interp_masm.hpp&quot;
  32 #include &quot;interpreter/templateInterpreter.hpp&quot;
  33 #include &quot;interpreter/templateTable.hpp&quot;
  34 #include &quot;memory/universe.hpp&quot;
  35 #include &quot;oops/klass.inline.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added">  36 #include &quot;oops/methodData.hpp&quot;</span>
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;oops/oop.inline.hpp&quot;
  39 #include &quot;prims/methodHandles.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/synchronizer.hpp&quot;
  45 #include &quot;utilities/macros.hpp&quot;
  46 #include &quot;utilities/powerOfTwo.hpp&quot;
  47 
  48 #undef __
  49 #define __ _masm-&gt;
  50 
  51 // ============================================================================
  52 // Misc helpers
  53 
  54 // Do an oop store like *(base + index) = val OR *(base + offset) = val
  55 // (only one of both variants is possible at the same time).
  56 // Index can be noreg.
  57 // Kills:
  58 //   Rbase, Rtmp
  59 static void do_oop_store(InterpreterMacroAssembler* _masm,
  60                          Register           base,
  61                          RegisterOrConstant offset,
  62                          Register           val,         // Noreg means always null.
  63                          Register           tmp1,
  64                          Register           tmp2,
  65                          Register           tmp3,
  66                          DecoratorSet       decorators) {
  67   assert_different_registers(tmp1, tmp2, tmp3, val, base);
  68   __ store_heap_oop(val, offset, base, tmp1, tmp2, tmp3, false, decorators);
  69 }
  70 
  71 static void do_oop_load(InterpreterMacroAssembler* _masm,
  72                         Register base,
  73                         RegisterOrConstant offset,
  74                         Register dst,
  75                         Register tmp1,
  76                         Register tmp2,
  77                         DecoratorSet decorators) {
  78   assert_different_registers(base, tmp1, tmp2);
  79   assert_different_registers(dst, tmp1, tmp2);
  80   __ load_heap_oop(dst, offset, base, tmp1, tmp2, false, decorators);
  81 }
  82 
  83 // ============================================================================
  84 // Platform-dependent initialization
  85 
  86 void TemplateTable::pd_initialize() {
  87   // No ppc64 specific initialization.
  88 }
  89 
  90 Address TemplateTable::at_bcp(int offset) {
  91   // Not used on ppc.
  92   ShouldNotReachHere();
  93   return Address();
  94 }
  95 
  96 // Patches the current bytecode (ptr to it located in bcp)
  97 // in the bytecode stream with a new one.
  98 void TemplateTable::patch_bytecode(Bytecodes::Code new_bc, Register Rnew_bc, Register Rtemp, bool load_bc_into_bc_reg /*=true*/, int byte_no) {
  99   // With sharing on, may need to test method flag.
 100   if (!RewriteBytecodes) return;
 101   Label L_patch_done;
 102 
 103   switch (new_bc) {
 104     case Bytecodes::_fast_aputfield:
 105     case Bytecodes::_fast_bputfield:
 106     case Bytecodes::_fast_zputfield:
 107     case Bytecodes::_fast_cputfield:
 108     case Bytecodes::_fast_dputfield:
 109     case Bytecodes::_fast_fputfield:
 110     case Bytecodes::_fast_iputfield:
 111     case Bytecodes::_fast_lputfield:
 112     case Bytecodes::_fast_sputfield:
 113     {
 114       // We skip bytecode quickening for putfield instructions when
 115       // the put_code written to the constant pool cache is zero.
 116       // This is required so that every execution of this instruction
 117       // calls out to InterpreterRuntime::resolve_get_put to do
 118       // additional, required work.
 119       assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
 120       assert(load_bc_into_bc_reg, &quot;we use bc_reg as temp&quot;);
 121       __ get_cache_and_index_at_bcp(Rtemp /* dst = cache */, 1);
 122       // ((*(cache+indices))&gt;&gt;((1+byte_no)*8))&amp;0xFF:
 123 #if defined(VM_LITTLE_ENDIAN)
 124       __ lbz(Rnew_bc, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()) + 1 + byte_no, Rtemp);
 125 #else
 126       __ lbz(Rnew_bc, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()) + 7 - (1 + byte_no), Rtemp);
 127 #endif
 128       __ cmpwi(CCR0, Rnew_bc, 0);
 129       __ li(Rnew_bc, (unsigned int)(unsigned char)new_bc);
 130       __ beq(CCR0, L_patch_done);
 131       // __ isync(); // acquire not needed
 132       break;
 133     }
 134 
 135     default:
 136       assert(byte_no == -1, &quot;sanity&quot;);
 137       if (load_bc_into_bc_reg) {
 138         __ li(Rnew_bc, (unsigned int)(unsigned char)new_bc);
 139       }
 140   }
 141 
 142   if (JvmtiExport::can_post_breakpoint()) {
 143     Label L_fast_patch;
 144     __ lbz(Rtemp, 0, R14_bcp);
 145     __ cmpwi(CCR0, Rtemp, (unsigned int)(unsigned char)Bytecodes::_breakpoint);
 146     __ bne(CCR0, L_fast_patch);
 147     // Perform the quickening, slowly, in the bowels of the breakpoint table.
 148     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::set_original_bytecode_at), R19_method, R14_bcp, Rnew_bc);
 149     __ b(L_patch_done);
 150     __ bind(L_fast_patch);
 151   }
 152 
 153   // Patch bytecode.
 154   __ stb(Rnew_bc, 0, R14_bcp);
 155 
 156   __ bind(L_patch_done);
 157 }
 158 
 159 // ============================================================================
 160 // Individual instructions
 161 
 162 void TemplateTable::nop() {
 163   transition(vtos, vtos);
 164   // Nothing to do.
 165 }
 166 
 167 void TemplateTable::shouldnotreachhere() {
 168   transition(vtos, vtos);
 169   __ stop(&quot;shouldnotreachhere bytecode&quot;);
 170 }
 171 
 172 void TemplateTable::aconst_null() {
 173   transition(vtos, atos);
 174   __ li(R17_tos, 0);
 175 }
 176 
 177 void TemplateTable::iconst(int value) {
 178   transition(vtos, itos);
 179   assert(value &gt;= -1 &amp;&amp; value &lt;= 5, &quot;&quot;);
 180   __ li(R17_tos, value);
 181 }
 182 
 183 void TemplateTable::lconst(int value) {
 184   transition(vtos, ltos);
 185   assert(value &gt;= -1 &amp;&amp; value &lt;= 5, &quot;&quot;);
 186   __ li(R17_tos, value);
 187 }
 188 
 189 void TemplateTable::fconst(int value) {
 190   transition(vtos, ftos);
 191   static float zero = 0.0;
 192   static float one  = 1.0;
 193   static float two  = 2.0;
 194   switch (value) {
 195     default: ShouldNotReachHere();
 196     case 0: {
 197       int simm16_offset = __ load_const_optimized(R11_scratch1, (address*)&amp;zero, R0, true);
 198       __ lfs(F15_ftos, simm16_offset, R11_scratch1);
 199       break;
 200     }
 201     case 1: {
 202       int simm16_offset = __ load_const_optimized(R11_scratch1, (address*)&amp;one, R0, true);
 203       __ lfs(F15_ftos, simm16_offset, R11_scratch1);
 204       break;
 205     }
 206     case 2: {
 207       int simm16_offset = __ load_const_optimized(R11_scratch1, (address*)&amp;two, R0, true);
 208       __ lfs(F15_ftos, simm16_offset, R11_scratch1);
 209       break;
 210     }
 211   }
 212 }
 213 
 214 void TemplateTable::dconst(int value) {
 215   transition(vtos, dtos);
 216   static double zero = 0.0;
 217   static double one  = 1.0;
 218   switch (value) {
 219     case 0: {
 220       int simm16_offset = __ load_const_optimized(R11_scratch1, (address*)&amp;zero, R0, true);
 221       __ lfd(F15_ftos, simm16_offset, R11_scratch1);
 222       break;
 223     }
 224     case 1: {
 225       int simm16_offset = __ load_const_optimized(R11_scratch1, (address*)&amp;one, R0, true);
 226       __ lfd(F15_ftos, simm16_offset, R11_scratch1);
 227       break;
 228     }
 229     default: ShouldNotReachHere();
 230   }
 231 }
 232 
 233 void TemplateTable::bipush() {
 234   transition(vtos, itos);
 235   __ lbz(R17_tos, 1, R14_bcp);
 236   __ extsb(R17_tos, R17_tos);
 237 }
 238 
 239 void TemplateTable::sipush() {
 240   transition(vtos, itos);
 241   __ get_2_byte_integer_at_bcp(1, R17_tos, InterpreterMacroAssembler::Signed);
 242 }
 243 
 244 void TemplateTable::ldc(bool wide) {
 245   Register Rscratch1 = R11_scratch1,
 246            Rscratch2 = R12_scratch2,
 247            Rcpool    = R3_ARG1;
 248 
 249   transition(vtos, vtos);
 250   Label notInt, notFloat, notClass, exit;
 251 
 252   __ get_cpool_and_tags(Rcpool, Rscratch2); // Set Rscratch2 = &amp;tags.
 253   if (wide) { // Read index.
 254     __ get_2_byte_integer_at_bcp(1, Rscratch1, InterpreterMacroAssembler::Unsigned);
 255   } else {
 256     __ lbz(Rscratch1, 1, R14_bcp);
 257   }
 258 
 259   const int base_offset = ConstantPool::header_size() * wordSize;
 260   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
 261 
 262   // Get type from tags.
 263   __ addi(Rscratch2, Rscratch2, tags_offset);
 264   __ lbzx(Rscratch2, Rscratch2, Rscratch1);
 265 
 266   __ cmpwi(CCR0, Rscratch2, JVM_CONSTANT_UnresolvedClass); // Unresolved class?
 267   __ cmpwi(CCR1, Rscratch2, JVM_CONSTANT_UnresolvedClassInError); // Unresolved class in error state?
 268   __ cror(CCR0, Assembler::equal, CCR1, Assembler::equal);
 269 
 270   // Resolved class - need to call vm to get java mirror of the class.
 271   __ cmpwi(CCR1, Rscratch2, JVM_CONSTANT_Class);
 272   __ crnor(CCR0, Assembler::equal, CCR1, Assembler::equal); // Neither resolved class nor unresolved case from above?
 273   __ beq(CCR0, notClass);
 274 
 275   __ li(R4, wide ? 1 : 0);
 276   call_VM(R17_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::ldc), R4);
 277   __ push(atos);
 278   __ b(exit);
 279 
 280   __ align(32, 12);
 281   __ bind(notClass);
 282   __ addi(Rcpool, Rcpool, base_offset);
 283   __ sldi(Rscratch1, Rscratch1, LogBytesPerWord);
 284   __ cmpdi(CCR0, Rscratch2, JVM_CONSTANT_Integer);
 285   __ bne(CCR0, notInt);
 286   __ lwax(R17_tos, Rcpool, Rscratch1);
 287   __ push(itos);
 288   __ b(exit);
 289 
 290   __ align(32, 12);
 291   __ bind(notInt);
 292   __ cmpdi(CCR0, Rscratch2, JVM_CONSTANT_Float);
 293   __ bne(CCR0, notFloat);
 294   __ lfsx(F15_ftos, Rcpool, Rscratch1);
 295   __ push(ftos);
 296   __ b(exit);
 297 
 298   __ align(32, 12);
 299   // assume the tag is for condy; if not, the VM runtime will tell us
 300   __ bind(notFloat);
 301   condy_helper(exit);
 302 
 303   __ align(32, 12);
 304   __ bind(exit);
 305 }
 306 
 307 // Fast path for caching oop constants.
 308 void TemplateTable::fast_aldc(bool wide) {
 309   transition(vtos, atos);
 310 
 311   int index_size = wide ? sizeof(u2) : sizeof(u1);
 312   const Register Rscratch = R11_scratch1;
 313   Label is_null;
 314 
 315   // We are resolved if the resolved reference cache entry contains a
 316   // non-null object (CallSite, etc.)
 317   __ get_cache_index_at_bcp(Rscratch, 1, index_size);  // Load index.
 318   __ load_resolved_reference_at_index(R17_tos, Rscratch, &amp;is_null);
 319 
 320   // Convert null sentinel to NULL.
 321   int simm16_rest = __ load_const_optimized(Rscratch, Universe::the_null_sentinel_addr(), R0, true);
 322   __ ld(Rscratch, simm16_rest, Rscratch);
 323   __ cmpld(CCR0, R17_tos, Rscratch);
 324   if (VM_Version::has_isel()) {
 325     __ isel_0(R17_tos, CCR0, Assembler::equal);
 326   } else {
 327     Label not_sentinel;
 328     __ bne(CCR0, not_sentinel);
 329     __ li(R17_tos, 0);
 330     __ bind(not_sentinel);
 331   }
 332   __ verify_oop(R17_tos);
 333   __ dispatch_epilog(atos, Bytecodes::length_for(bytecode()));
 334 
 335   __ bind(is_null);
 336   __ load_const_optimized(R3_ARG1, (int)bytecode());
 337 
 338   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc);
 339 
 340   // First time invocation - must resolve first.
 341   __ call_VM(R17_tos, entry, R3_ARG1);
 342   __ verify_oop(R17_tos);
 343 }
 344 
 345 void TemplateTable::ldc2_w() {
 346   transition(vtos, vtos);
 347   Label not_double, not_long, exit;
 348 
 349   Register Rindex = R11_scratch1,
 350            Rcpool = R12_scratch2,
 351            Rtag   = R3_ARG1;
 352   __ get_cpool_and_tags(Rcpool, Rtag);
 353   __ get_2_byte_integer_at_bcp(1, Rindex, InterpreterMacroAssembler::Unsigned);
 354 
 355   const int base_offset = ConstantPool::header_size() * wordSize;
 356   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
 357   // Get type from tags.
 358   __ addi(Rcpool, Rcpool, base_offset);
 359   __ addi(Rtag, Rtag, tags_offset);
 360 
 361   __ lbzx(Rtag, Rtag, Rindex);
 362   __ sldi(Rindex, Rindex, LogBytesPerWord);
 363 
 364   __ cmpdi(CCR0, Rtag, JVM_CONSTANT_Double);
 365   __ bne(CCR0, not_double);
 366   __ lfdx(F15_ftos, Rcpool, Rindex);
 367   __ push(dtos);
 368   __ b(exit);
 369 
 370   __ bind(not_double);
 371   __ cmpdi(CCR0, Rtag, JVM_CONSTANT_Long);
 372   __ bne(CCR0, not_long);
 373   __ ldx(R17_tos, Rcpool, Rindex);
 374   __ push(ltos);
 375   __ b(exit);
 376 
 377   __ bind(not_long);
 378   condy_helper(exit);
 379 
 380   __ align(32, 12);
 381   __ bind(exit);
 382 }
 383 
 384 void TemplateTable::condy_helper(Label&amp; Done) {
 385   const Register obj   = R31;
 386   const Register off   = R11_scratch1;
 387   const Register flags = R12_scratch2;
 388   const Register rarg  = R4_ARG2;
 389   __ li(rarg, (int)bytecode());
 390   call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc), rarg);
 391   __ get_vm_result_2(flags);
 392 
 393   // VMr = obj = base address to find primitive value to push
 394   // VMr2 = flags = (tos, off) using format of CPCE::_flags
 395   __ andi(off, flags, ConstantPoolCacheEntry::field_index_mask);
 396 
 397   // What sort of thing are we loading?
 398   __ rldicl(flags, flags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
 399 
 400   switch (bytecode()) {
 401   case Bytecodes::_ldc:
 402   case Bytecodes::_ldc_w:
 403     {
 404       // tos in (itos, ftos, stos, btos, ctos, ztos)
 405       Label notInt, notFloat, notShort, notByte, notChar, notBool;
 406       __ cmplwi(CCR0, flags, itos);
 407       __ bne(CCR0, notInt);
 408       // itos
 409       __ lwax(R17_tos, obj, off);
 410       __ push(itos);
 411       __ b(Done);
 412 
 413       __ bind(notInt);
 414       __ cmplwi(CCR0, flags, ftos);
 415       __ bne(CCR0, notFloat);
 416       // ftos
 417       __ lfsx(F15_ftos, obj, off);
 418       __ push(ftos);
 419       __ b(Done);
 420 
 421       __ bind(notFloat);
 422       __ cmplwi(CCR0, flags, stos);
 423       __ bne(CCR0, notShort);
 424       // stos
 425       __ lhax(R17_tos, obj, off);
 426       __ push(stos);
 427       __ b(Done);
 428 
 429       __ bind(notShort);
 430       __ cmplwi(CCR0, flags, btos);
 431       __ bne(CCR0, notByte);
 432       // btos
 433       __ lbzx(R17_tos, obj, off);
 434       __ extsb(R17_tos, R17_tos);
 435       __ push(btos);
 436       __ b(Done);
 437 
 438       __ bind(notByte);
 439       __ cmplwi(CCR0, flags, ctos);
 440       __ bne(CCR0, notChar);
 441       // ctos
 442       __ lhzx(R17_tos, obj, off);
 443       __ push(ctos);
 444       __ b(Done);
 445 
 446       __ bind(notChar);
 447       __ cmplwi(CCR0, flags, ztos);
 448       __ bne(CCR0, notBool);
 449       // ztos
 450       __ lbzx(R17_tos, obj, off);
 451       __ push(ztos);
 452       __ b(Done);
 453 
 454       __ bind(notBool);
 455       break;
 456     }
 457 
 458   case Bytecodes::_ldc2_w:
 459     {
 460       Label notLong, notDouble;
 461       __ cmplwi(CCR0, flags, ltos);
 462       __ bne(CCR0, notLong);
 463       // ltos
 464       __ ldx(R17_tos, obj, off);
 465       __ push(ltos);
 466       __ b(Done);
 467 
 468       __ bind(notLong);
 469       __ cmplwi(CCR0, flags, dtos);
 470       __ bne(CCR0, notDouble);
 471       // dtos
 472       __ lfdx(F15_ftos, obj, off);
 473       __ push(dtos);
 474       __ b(Done);
 475 
 476       __ bind(notDouble);
 477       break;
 478     }
 479 
 480   default:
 481     ShouldNotReachHere();
 482   }
 483 
 484   __ stop(&quot;bad ldc/condy&quot;);
 485 }
 486 
 487 // Get the locals index located in the bytecode stream at bcp + offset.
 488 void TemplateTable::locals_index(Register Rdst, int offset) {
 489   __ lbz(Rdst, offset, R14_bcp);
 490 }
 491 
 492 void TemplateTable::iload() {
 493   iload_internal();
 494 }
 495 
 496 void TemplateTable::nofast_iload() {
 497   iload_internal(may_not_rewrite);
 498 }
 499 
 500 void TemplateTable::iload_internal(RewriteControl rc) {
 501   transition(vtos, itos);
 502 
 503   // Get the local value into tos
 504   const Register Rindex = R22_tmp2;
 505   locals_index(Rindex);
 506 
 507   // Rewrite iload,iload  pair into fast_iload2
 508   //         iload,caload pair into fast_icaload
 509   if (RewriteFrequentPairs &amp;&amp; rc == may_rewrite) {
 510     Label Lrewrite, Ldone;
 511     Register Rnext_byte  = R3_ARG1,
 512              Rrewrite_to = R6_ARG4,
 513              Rscratch    = R11_scratch1;
 514 
 515     // get next byte
 516     __ lbz(Rnext_byte, Bytecodes::length_for(Bytecodes::_iload), R14_bcp);
 517 
 518     // if _iload, wait to rewrite to iload2. We only want to rewrite the
 519     // last two iloads in a pair. Comparing against fast_iload means that
 520     // the next bytecode is neither an iload or a caload, and therefore
 521     // an iload pair.
 522     __ cmpwi(CCR0, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_iload);
 523     __ beq(CCR0, Ldone);
 524 
 525     __ cmpwi(CCR1, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_fast_iload);
 526     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_iload2);
 527     __ beq(CCR1, Lrewrite);
 528 
 529     __ cmpwi(CCR0, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_caload);
 530     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_icaload);
 531     __ beq(CCR0, Lrewrite);
 532 
 533     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_iload);
 534 
 535     __ bind(Lrewrite);
 536     patch_bytecode(Bytecodes::_iload, Rrewrite_to, Rscratch, false);
 537     __ bind(Ldone);
 538   }
 539 
 540   __ load_local_int(R17_tos, Rindex, Rindex);
 541 }
 542 
 543 // Load 2 integers in a row without dispatching
 544 void TemplateTable::fast_iload2() {
 545   transition(vtos, itos);
 546 
 547   __ lbz(R3_ARG1, 1, R14_bcp);
 548   __ lbz(R17_tos, Bytecodes::length_for(Bytecodes::_iload) + 1, R14_bcp);
 549 
 550   __ load_local_int(R3_ARG1, R11_scratch1, R3_ARG1);
 551   __ load_local_int(R17_tos, R12_scratch2, R17_tos);
 552   __ push_i(R3_ARG1);
 553 }
 554 
 555 void TemplateTable::fast_iload() {
 556   transition(vtos, itos);
 557   // Get the local value into tos
 558 
 559   const Register Rindex = R11_scratch1;
 560   locals_index(Rindex);
 561   __ load_local_int(R17_tos, Rindex, Rindex);
 562 }
 563 
 564 // Load a local variable type long from locals area to TOS cache register.
 565 // Local index resides in bytecodestream.
 566 void TemplateTable::lload() {
 567   transition(vtos, ltos);
 568 
 569   const Register Rindex = R11_scratch1;
 570   locals_index(Rindex);
 571   __ load_local_long(R17_tos, Rindex, Rindex);
 572 }
 573 
 574 void TemplateTable::fload() {
 575   transition(vtos, ftos);
 576 
 577   const Register Rindex = R11_scratch1;
 578   locals_index(Rindex);
 579   __ load_local_float(F15_ftos, Rindex, Rindex);
 580 }
 581 
 582 void TemplateTable::dload() {
 583   transition(vtos, dtos);
 584 
 585   const Register Rindex = R11_scratch1;
 586   locals_index(Rindex);
 587   __ load_local_double(F15_ftos, Rindex, Rindex);
 588 }
 589 
 590 void TemplateTable::aload() {
 591   transition(vtos, atos);
 592 
 593   const Register Rindex = R11_scratch1;
 594   locals_index(Rindex);
 595   __ load_local_ptr(R17_tos, Rindex, Rindex);
 596 }
 597 
 598 void TemplateTable::locals_index_wide(Register Rdst) {
 599   // Offset is 2, not 1, because Lbcp points to wide prefix code.
 600   __ get_2_byte_integer_at_bcp(2, Rdst, InterpreterMacroAssembler::Unsigned);
 601 }
 602 
 603 void TemplateTable::wide_iload() {
 604   // Get the local value into tos.
 605 
 606   const Register Rindex = R11_scratch1;
 607   locals_index_wide(Rindex);
 608   __ load_local_int(R17_tos, Rindex, Rindex);
 609 }
 610 
 611 void TemplateTable::wide_lload() {
 612   transition(vtos, ltos);
 613 
 614   const Register Rindex = R11_scratch1;
 615   locals_index_wide(Rindex);
 616   __ load_local_long(R17_tos, Rindex, Rindex);
 617 }
 618 
 619 void TemplateTable::wide_fload() {
 620   transition(vtos, ftos);
 621 
 622   const Register Rindex = R11_scratch1;
 623   locals_index_wide(Rindex);
 624   __ load_local_float(F15_ftos, Rindex, Rindex);
 625 }
 626 
 627 void TemplateTable::wide_dload() {
 628   transition(vtos, dtos);
 629 
 630   const Register Rindex = R11_scratch1;
 631   locals_index_wide(Rindex);
 632   __ load_local_double(F15_ftos, Rindex, Rindex);
 633 }
 634 
 635 void TemplateTable::wide_aload() {
 636   transition(vtos, atos);
 637 
 638   const Register Rindex = R11_scratch1;
 639   locals_index_wide(Rindex);
 640   __ load_local_ptr(R17_tos, Rindex, Rindex);
 641 }
 642 
 643 void TemplateTable::iaload() {
 644   transition(itos, itos);
 645 
 646   const Register Rload_addr = R3_ARG1,
 647                  Rarray     = R4_ARG2,
 648                  Rtemp      = R5_ARG3;
 649   __ index_check(Rarray, R17_tos /* index */, LogBytesPerInt, Rtemp, Rload_addr);
 650   __ lwa(R17_tos, arrayOopDesc::base_offset_in_bytes(T_INT), Rload_addr);
 651 }
 652 
 653 void TemplateTable::laload() {
 654   transition(itos, ltos);
 655 
 656   const Register Rload_addr = R3_ARG1,
 657                  Rarray     = R4_ARG2,
 658                  Rtemp      = R5_ARG3;
 659   __ index_check(Rarray, R17_tos /* index */, LogBytesPerLong, Rtemp, Rload_addr);
 660   __ ld(R17_tos, arrayOopDesc::base_offset_in_bytes(T_LONG), Rload_addr);
 661 }
 662 
 663 void TemplateTable::faload() {
 664   transition(itos, ftos);
 665 
 666   const Register Rload_addr = R3_ARG1,
 667                  Rarray     = R4_ARG2,
 668                  Rtemp      = R5_ARG3;
 669   __ index_check(Rarray, R17_tos /* index */, LogBytesPerInt, Rtemp, Rload_addr);
 670   __ lfs(F15_ftos, arrayOopDesc::base_offset_in_bytes(T_FLOAT), Rload_addr);
 671 }
 672 
 673 void TemplateTable::daload() {
 674   transition(itos, dtos);
 675 
 676   const Register Rload_addr = R3_ARG1,
 677                  Rarray     = R4_ARG2,
 678                  Rtemp      = R5_ARG3;
 679   __ index_check(Rarray, R17_tos /* index */, LogBytesPerLong, Rtemp, Rload_addr);
 680   __ lfd(F15_ftos, arrayOopDesc::base_offset_in_bytes(T_DOUBLE), Rload_addr);
 681 }
 682 
 683 void TemplateTable::aaload() {
 684   transition(itos, atos);
 685 
 686   // tos: index
 687   // result tos: array
 688   const Register Rload_addr = R3_ARG1,
 689                  Rarray     = R4_ARG2,
 690                  Rtemp      = R5_ARG3,
 691                  Rtemp2     = R31;
 692   __ index_check(Rarray, R17_tos /* index */, UseCompressedOops ? 2 : LogBytesPerWord, Rtemp, Rload_addr);
 693   do_oop_load(_masm, Rload_addr, arrayOopDesc::base_offset_in_bytes(T_OBJECT), R17_tos, Rtemp, Rtemp2,
 694               IS_ARRAY);
 695   __ verify_oop(R17_tos);
 696   //__ dcbt(R17_tos); // prefetch
 697 }
 698 
 699 void TemplateTable::baload() {
 700   transition(itos, itos);
 701 
 702   const Register Rload_addr = R3_ARG1,
 703                  Rarray     = R4_ARG2,
 704                  Rtemp      = R5_ARG3;
 705   __ index_check(Rarray, R17_tos /* index */, 0, Rtemp, Rload_addr);
 706   __ lbz(R17_tos, arrayOopDesc::base_offset_in_bytes(T_BYTE), Rload_addr);
 707   __ extsb(R17_tos, R17_tos);
 708 }
 709 
 710 void TemplateTable::caload() {
 711   transition(itos, itos);
 712 
 713   const Register Rload_addr = R3_ARG1,
 714                  Rarray     = R4_ARG2,
 715                  Rtemp      = R5_ARG3;
 716   __ index_check(Rarray, R17_tos /* index */, LogBytesPerShort, Rtemp, Rload_addr);
 717   __ lhz(R17_tos, arrayOopDesc::base_offset_in_bytes(T_CHAR), Rload_addr);
 718 }
 719 
 720 // Iload followed by caload frequent pair.
 721 void TemplateTable::fast_icaload() {
 722   transition(vtos, itos);
 723 
 724   const Register Rload_addr = R3_ARG1,
 725                  Rarray     = R4_ARG2,
 726                  Rtemp      = R11_scratch1;
 727 
 728   locals_index(R17_tos);
 729   __ load_local_int(R17_tos, Rtemp, R17_tos);
 730   __ index_check(Rarray, R17_tos /* index */, LogBytesPerShort, Rtemp, Rload_addr);
 731   __ lhz(R17_tos, arrayOopDesc::base_offset_in_bytes(T_CHAR), Rload_addr);
 732 }
 733 
 734 void TemplateTable::saload() {
 735   transition(itos, itos);
 736 
 737   const Register Rload_addr = R11_scratch1,
 738                  Rarray     = R12_scratch2,
 739                  Rtemp      = R3_ARG1;
 740   __ index_check(Rarray, R17_tos /* index */, LogBytesPerShort, Rtemp, Rload_addr);
 741   __ lha(R17_tos, arrayOopDesc::base_offset_in_bytes(T_SHORT), Rload_addr);
 742 }
 743 
 744 void TemplateTable::iload(int n) {
 745   transition(vtos, itos);
 746 
 747   __ lwz(R17_tos, Interpreter::local_offset_in_bytes(n), R18_locals);
 748 }
 749 
 750 void TemplateTable::lload(int n) {
 751   transition(vtos, ltos);
 752 
 753   __ ld(R17_tos, Interpreter::local_offset_in_bytes(n + 1), R18_locals);
 754 }
 755 
 756 void TemplateTable::fload(int n) {
 757   transition(vtos, ftos);
 758 
 759   __ lfs(F15_ftos, Interpreter::local_offset_in_bytes(n), R18_locals);
 760 }
 761 
 762 void TemplateTable::dload(int n) {
 763   transition(vtos, dtos);
 764 
 765   __ lfd(F15_ftos, Interpreter::local_offset_in_bytes(n + 1), R18_locals);
 766 }
 767 
 768 void TemplateTable::aload(int n) {
 769   transition(vtos, atos);
 770 
 771   __ ld(R17_tos, Interpreter::local_offset_in_bytes(n), R18_locals);
 772 }
 773 
 774 void TemplateTable::aload_0() {
 775   aload_0_internal();
 776 }
 777 
 778 void TemplateTable::nofast_aload_0() {
 779   aload_0_internal(may_not_rewrite);
 780 }
 781 
 782 void TemplateTable::aload_0_internal(RewriteControl rc) {
 783   transition(vtos, atos);
 784   // According to bytecode histograms, the pairs:
 785   //
 786   // _aload_0, _fast_igetfield
 787   // _aload_0, _fast_agetfield
 788   // _aload_0, _fast_fgetfield
 789   //
 790   // occur frequently. If RewriteFrequentPairs is set, the (slow)
 791   // _aload_0 bytecode checks if the next bytecode is either
 792   // _fast_igetfield, _fast_agetfield or _fast_fgetfield and then
 793   // rewrites the current bytecode into a pair bytecode; otherwise it
 794   // rewrites the current bytecode into _0 that doesn&#39;t do
 795   // the pair check anymore.
 796   //
 797   // Note: If the next bytecode is _getfield, the rewrite must be
 798   //       delayed, otherwise we may miss an opportunity for a pair.
 799   //
 800   // Also rewrite frequent pairs
 801   //   aload_0, aload_1
 802   //   aload_0, iload_1
 803   // These bytecodes with a small amount of code are most profitable
 804   // to rewrite.
 805 
 806   if (RewriteFrequentPairs &amp;&amp; rc == may_rewrite) {
 807 
 808     Label Lrewrite, Ldont_rewrite;
 809     Register Rnext_byte  = R3_ARG1,
 810              Rrewrite_to = R6_ARG4,
 811              Rscratch    = R11_scratch1;
 812 
 813     // Get next byte.
 814     __ lbz(Rnext_byte, Bytecodes::length_for(Bytecodes::_aload_0), R14_bcp);
 815 
 816     // If _getfield, wait to rewrite. We only want to rewrite the last two bytecodes in a pair.
 817     __ cmpwi(CCR0, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_getfield);
 818     __ beq(CCR0, Ldont_rewrite);
 819 
 820     __ cmpwi(CCR1, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_fast_igetfield);
 821     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_iaccess_0);
 822     __ beq(CCR1, Lrewrite);
 823 
 824     __ cmpwi(CCR0, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_fast_agetfield);
 825     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_aaccess_0);
 826     __ beq(CCR0, Lrewrite);
 827 
 828     __ cmpwi(CCR1, Rnext_byte, (unsigned int)(unsigned char)Bytecodes::_fast_fgetfield);
 829     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_faccess_0);
 830     __ beq(CCR1, Lrewrite);
 831 
 832     __ li(Rrewrite_to, (unsigned int)(unsigned char)Bytecodes::_fast_aload_0);
 833 
 834     __ bind(Lrewrite);
 835     patch_bytecode(Bytecodes::_aload_0, Rrewrite_to, Rscratch, false);
 836     __ bind(Ldont_rewrite);
 837   }
 838 
 839   // Do actual aload_0 (must do this after patch_bytecode which might call VM and GC might change oop).
 840   aload(0);
 841 }
 842 
 843 void TemplateTable::istore() {
 844   transition(itos, vtos);
 845 
 846   const Register Rindex = R11_scratch1;
 847   locals_index(Rindex);
 848   __ store_local_int(R17_tos, Rindex);
 849 }
 850 
 851 void TemplateTable::lstore() {
 852   transition(ltos, vtos);
 853   const Register Rindex = R11_scratch1;
 854   locals_index(Rindex);
 855   __ store_local_long(R17_tos, Rindex);
 856 }
 857 
 858 void TemplateTable::fstore() {
 859   transition(ftos, vtos);
 860 
 861   const Register Rindex = R11_scratch1;
 862   locals_index(Rindex);
 863   __ store_local_float(F15_ftos, Rindex);
 864 }
 865 
 866 void TemplateTable::dstore() {
 867   transition(dtos, vtos);
 868 
 869   const Register Rindex = R11_scratch1;
 870   locals_index(Rindex);
 871   __ store_local_double(F15_ftos, Rindex);
 872 }
 873 
 874 void TemplateTable::astore() {
 875   transition(vtos, vtos);
 876 
 877   const Register Rindex = R11_scratch1;
 878   __ pop_ptr();
 879   __ verify_oop_or_return_address(R17_tos, Rindex);
 880   locals_index(Rindex);
 881   __ store_local_ptr(R17_tos, Rindex);
 882 }
 883 
 884 void TemplateTable::wide_istore() {
 885   transition(vtos, vtos);
 886 
 887   const Register Rindex = R11_scratch1;
 888   __ pop_i();
 889   locals_index_wide(Rindex);
 890   __ store_local_int(R17_tos, Rindex);
 891 }
 892 
 893 void TemplateTable::wide_lstore() {
 894   transition(vtos, vtos);
 895 
 896   const Register Rindex = R11_scratch1;
 897   __ pop_l();
 898   locals_index_wide(Rindex);
 899   __ store_local_long(R17_tos, Rindex);
 900 }
 901 
 902 void TemplateTable::wide_fstore() {
 903   transition(vtos, vtos);
 904 
 905   const Register Rindex = R11_scratch1;
 906   __ pop_f();
 907   locals_index_wide(Rindex);
 908   __ store_local_float(F15_ftos, Rindex);
 909 }
 910 
 911 void TemplateTable::wide_dstore() {
 912   transition(vtos, vtos);
 913 
 914   const Register Rindex = R11_scratch1;
 915   __ pop_d();
 916   locals_index_wide(Rindex);
 917   __ store_local_double(F15_ftos, Rindex);
 918 }
 919 
 920 void TemplateTable::wide_astore() {
 921   transition(vtos, vtos);
 922 
 923   const Register Rindex = R11_scratch1;
 924   __ pop_ptr();
 925   __ verify_oop_or_return_address(R17_tos, Rindex);
 926   locals_index_wide(Rindex);
 927   __ store_local_ptr(R17_tos, Rindex);
 928 }
 929 
 930 void TemplateTable::iastore() {
 931   transition(itos, vtos);
 932 
 933   const Register Rindex      = R3_ARG1,
 934                  Rstore_addr = R4_ARG2,
 935                  Rarray      = R5_ARG3,
 936                  Rtemp       = R6_ARG4;
 937   __ pop_i(Rindex);
 938   __ index_check(Rarray, Rindex, LogBytesPerInt, Rtemp, Rstore_addr);
 939   __ stw(R17_tos, arrayOopDesc::base_offset_in_bytes(T_INT), Rstore_addr);
 940   }
 941 
 942 void TemplateTable::lastore() {
 943   transition(ltos, vtos);
 944 
 945   const Register Rindex      = R3_ARG1,
 946                  Rstore_addr = R4_ARG2,
 947                  Rarray      = R5_ARG3,
 948                  Rtemp       = R6_ARG4;
 949   __ pop_i(Rindex);
 950   __ index_check(Rarray, Rindex, LogBytesPerLong, Rtemp, Rstore_addr);
 951   __ std(R17_tos, arrayOopDesc::base_offset_in_bytes(T_LONG), Rstore_addr);
 952   }
 953 
 954 void TemplateTable::fastore() {
 955   transition(ftos, vtos);
 956 
 957   const Register Rindex      = R3_ARG1,
 958                  Rstore_addr = R4_ARG2,
 959                  Rarray      = R5_ARG3,
 960                  Rtemp       = R6_ARG4;
 961   __ pop_i(Rindex);
 962   __ index_check(Rarray, Rindex, LogBytesPerInt, Rtemp, Rstore_addr);
 963   __ stfs(F15_ftos, arrayOopDesc::base_offset_in_bytes(T_FLOAT), Rstore_addr);
 964   }
 965 
 966 void TemplateTable::dastore() {
 967   transition(dtos, vtos);
 968 
 969   const Register Rindex      = R3_ARG1,
 970                  Rstore_addr = R4_ARG2,
 971                  Rarray      = R5_ARG3,
 972                  Rtemp       = R6_ARG4;
 973   __ pop_i(Rindex);
 974   __ index_check(Rarray, Rindex, LogBytesPerLong, Rtemp, Rstore_addr);
 975   __ stfd(F15_ftos, arrayOopDesc::base_offset_in_bytes(T_DOUBLE), Rstore_addr);
 976   }
 977 
 978 // Pop 3 values from the stack and...
 979 void TemplateTable::aastore() {
 980   transition(vtos, vtos);
 981 
 982   Label Lstore_ok, Lis_null, Ldone;
 983   const Register Rindex    = R3_ARG1,
 984                  Rarray    = R4_ARG2,
 985                  Rscratch  = R11_scratch1,
 986                  Rscratch2 = R12_scratch2,
 987                  Rarray_klass = R5_ARG3,
 988                  Rarray_element_klass = Rarray_klass,
 989                  Rvalue_klass = R6_ARG4,
 990                  Rstore_addr = R31;    // Use register which survives VM call.
 991 
 992   __ ld(R17_tos, Interpreter::expr_offset_in_bytes(0), R15_esp); // Get value to store.
 993   __ lwz(Rindex, Interpreter::expr_offset_in_bytes(1), R15_esp); // Get index.
 994   __ ld(Rarray, Interpreter::expr_offset_in_bytes(2), R15_esp);  // Get array.
 995 
 996   __ verify_oop(R17_tos);
 997   __ index_check_without_pop(Rarray, Rindex, UseCompressedOops ? 2 : LogBytesPerWord, Rscratch, Rstore_addr);
 998   // Rindex is dead!
 999   Register Rscratch3 = Rindex;
1000 
1001   // Do array store check - check for NULL value first.
1002   __ cmpdi(CCR0, R17_tos, 0);
1003   __ beq(CCR0, Lis_null);
1004 
1005   __ load_klass(Rarray_klass, Rarray);
1006   __ load_klass(Rvalue_klass, R17_tos);
1007 
1008   // Do fast instanceof cache test.
1009   __ ld(Rarray_element_klass, in_bytes(ObjArrayKlass::element_klass_offset()), Rarray_klass);
1010 
1011   // Generate a fast subtype check. Branch to store_ok if no failure. Throw if failure.
1012   __ gen_subtype_check(Rvalue_klass /*subklass*/, Rarray_element_klass /*superklass*/, Rscratch, Rscratch2, Rscratch3, Lstore_ok);
1013 
1014   // Fell through: subtype check failed =&gt; throw an exception.
1015   __ load_dispatch_table(R11_scratch1, (address*)Interpreter::_throw_ArrayStoreException_entry);
1016   __ mtctr(R11_scratch1);
1017   __ bctr();
1018 
1019   __ bind(Lis_null);
1020   do_oop_store(_masm, Rstore_addr, arrayOopDesc::base_offset_in_bytes(T_OBJECT), noreg /* 0 */,
1021                Rscratch, Rscratch2, Rscratch3, IS_ARRAY);
1022   __ profile_null_seen(Rscratch, Rscratch2);
1023   __ b(Ldone);
1024 
1025   // Store is OK.
1026   __ bind(Lstore_ok);
1027   do_oop_store(_masm, Rstore_addr, arrayOopDesc::base_offset_in_bytes(T_OBJECT), R17_tos /* value */,
1028                Rscratch, Rscratch2, Rscratch3, IS_ARRAY | IS_NOT_NULL);
1029 
1030   __ bind(Ldone);
1031   // Adjust sp (pops array, index and value).
1032   __ addi(R15_esp, R15_esp, 3 * Interpreter::stackElementSize);
1033 }
1034 
1035 void TemplateTable::bastore() {
1036   transition(itos, vtos);
1037 
1038   const Register Rindex   = R11_scratch1,
1039                  Rarray   = R12_scratch2,
1040                  Rscratch = R3_ARG1;
1041   __ pop_i(Rindex);
1042   __ pop_ptr(Rarray);
1043   // tos: val
1044 
1045   // Need to check whether array is boolean or byte
1046   // since both types share the bastore bytecode.
1047   __ load_klass(Rscratch, Rarray);
1048   __ lwz(Rscratch, in_bytes(Klass::layout_helper_offset()), Rscratch);
1049   int diffbit = exact_log2(Klass::layout_helper_boolean_diffbit());
1050   __ testbitdi(CCR0, R0, Rscratch, diffbit);
1051   Label L_skip;
1052   __ bfalse(CCR0, L_skip);
1053   __ andi(R17_tos, R17_tos, 1);  // if it is a T_BOOLEAN array, mask the stored value to 0/1
1054   __ bind(L_skip);
1055 
1056   __ index_check_without_pop(Rarray, Rindex, 0, Rscratch, Rarray);
1057   __ stb(R17_tos, arrayOopDesc::base_offset_in_bytes(T_BYTE), Rarray);
1058 }
1059 
1060 void TemplateTable::castore() {
1061   transition(itos, vtos);
1062 
1063   const Register Rindex   = R11_scratch1,
1064                  Rarray   = R12_scratch2,
1065                  Rscratch = R3_ARG1;
1066   __ pop_i(Rindex);
1067   // tos: val
1068   // Rarray: array ptr (popped by index_check)
1069   __ index_check(Rarray, Rindex, LogBytesPerShort, Rscratch, Rarray);
1070   __ sth(R17_tos, arrayOopDesc::base_offset_in_bytes(T_CHAR), Rarray);
1071 }
1072 
1073 void TemplateTable::sastore() {
1074   castore();
1075 }
1076 
1077 void TemplateTable::istore(int n) {
1078   transition(itos, vtos);
1079   __ stw(R17_tos, Interpreter::local_offset_in_bytes(n), R18_locals);
1080 }
1081 
1082 void TemplateTable::lstore(int n) {
1083   transition(ltos, vtos);
1084   __ std(R17_tos, Interpreter::local_offset_in_bytes(n + 1), R18_locals);
1085 }
1086 
1087 void TemplateTable::fstore(int n) {
1088   transition(ftos, vtos);
1089   __ stfs(F15_ftos, Interpreter::local_offset_in_bytes(n), R18_locals);
1090 }
1091 
1092 void TemplateTable::dstore(int n) {
1093   transition(dtos, vtos);
1094   __ stfd(F15_ftos, Interpreter::local_offset_in_bytes(n + 1), R18_locals);
1095 }
1096 
1097 void TemplateTable::astore(int n) {
1098   transition(vtos, vtos);
1099 
1100   __ pop_ptr();
1101   __ verify_oop_or_return_address(R17_tos, R11_scratch1);
1102   __ std(R17_tos, Interpreter::local_offset_in_bytes(n), R18_locals);
1103 }
1104 
1105 void TemplateTable::pop() {
1106   transition(vtos, vtos);
1107 
1108   __ addi(R15_esp, R15_esp, Interpreter::stackElementSize);
1109 }
1110 
1111 void TemplateTable::pop2() {
1112   transition(vtos, vtos);
1113 
1114   __ addi(R15_esp, R15_esp, Interpreter::stackElementSize * 2);
1115 }
1116 
1117 void TemplateTable::dup() {
1118   transition(vtos, vtos);
1119 
1120   __ ld(R11_scratch1, Interpreter::stackElementSize, R15_esp);
1121   __ push_ptr(R11_scratch1);
1122 }
1123 
1124 void TemplateTable::dup_x1() {
1125   transition(vtos, vtos);
1126 
1127   Register Ra = R11_scratch1,
1128            Rb = R12_scratch2;
1129   // stack: ..., a, b
1130   __ ld(Rb, Interpreter::stackElementSize,     R15_esp);
1131   __ ld(Ra, Interpreter::stackElementSize * 2, R15_esp);
1132   __ std(Rb, Interpreter::stackElementSize * 2, R15_esp);
1133   __ std(Ra, Interpreter::stackElementSize,     R15_esp);
1134   __ push_ptr(Rb);
1135   // stack: ..., b, a, b
1136 }
1137 
1138 void TemplateTable::dup_x2() {
1139   transition(vtos, vtos);
1140 
1141   Register Ra = R11_scratch1,
1142            Rb = R12_scratch2,
1143            Rc = R3_ARG1;
1144 
1145   // stack: ..., a, b, c
1146   __ ld(Rc, Interpreter::stackElementSize,     R15_esp);  // load c
1147   __ ld(Ra, Interpreter::stackElementSize * 3, R15_esp);  // load a
1148   __ std(Rc, Interpreter::stackElementSize * 3, R15_esp); // store c in a
1149   __ ld(Rb, Interpreter::stackElementSize * 2, R15_esp);  // load b
1150   // stack: ..., c, b, c
1151   __ std(Ra, Interpreter::stackElementSize * 2, R15_esp); // store a in b
1152   // stack: ..., c, a, c
1153   __ std(Rb, Interpreter::stackElementSize,     R15_esp); // store b in c
1154   __ push_ptr(Rc);                                        // push c
1155   // stack: ..., c, a, b, c
1156 }
1157 
1158 void TemplateTable::dup2() {
1159   transition(vtos, vtos);
1160 
1161   Register Ra = R11_scratch1,
1162            Rb = R12_scratch2;
1163   // stack: ..., a, b
1164   __ ld(Rb, Interpreter::stackElementSize,     R15_esp);
1165   __ ld(Ra, Interpreter::stackElementSize * 2, R15_esp);
1166   __ push_2ptrs(Ra, Rb);
1167   // stack: ..., a, b, a, b
1168 }
1169 
1170 void TemplateTable::dup2_x1() {
1171   transition(vtos, vtos);
1172 
1173   Register Ra = R11_scratch1,
1174            Rb = R12_scratch2,
1175            Rc = R3_ARG1;
1176   // stack: ..., a, b, c
1177   __ ld(Rc, Interpreter::stackElementSize,     R15_esp);
1178   __ ld(Rb, Interpreter::stackElementSize * 2, R15_esp);
1179   __ std(Rc, Interpreter::stackElementSize * 2, R15_esp);
1180   __ ld(Ra, Interpreter::stackElementSize * 3, R15_esp);
1181   __ std(Ra, Interpreter::stackElementSize,     R15_esp);
1182   __ std(Rb, Interpreter::stackElementSize * 3, R15_esp);
1183   // stack: ..., b, c, a
1184   __ push_2ptrs(Rb, Rc);
1185   // stack: ..., b, c, a, b, c
1186 }
1187 
1188 void TemplateTable::dup2_x2() {
1189   transition(vtos, vtos);
1190 
1191   Register Ra = R11_scratch1,
1192            Rb = R12_scratch2,
1193            Rc = R3_ARG1,
1194            Rd = R4_ARG2;
1195   // stack: ..., a, b, c, d
1196   __ ld(Rb, Interpreter::stackElementSize * 3, R15_esp);
1197   __ ld(Rd, Interpreter::stackElementSize,     R15_esp);
1198   __ std(Rb, Interpreter::stackElementSize,     R15_esp);  // store b in d
1199   __ std(Rd, Interpreter::stackElementSize * 3, R15_esp);  // store d in b
1200   __ ld(Ra, Interpreter::stackElementSize * 4, R15_esp);
1201   __ ld(Rc, Interpreter::stackElementSize * 2, R15_esp);
1202   __ std(Ra, Interpreter::stackElementSize * 2, R15_esp);  // store a in c
1203   __ std(Rc, Interpreter::stackElementSize * 4, R15_esp);  // store c in a
1204   // stack: ..., c, d, a, b
1205   __ push_2ptrs(Rc, Rd);
1206   // stack: ..., c, d, a, b, c, d
1207 }
1208 
1209 void TemplateTable::swap() {
1210   transition(vtos, vtos);
1211   // stack: ..., a, b
1212 
1213   Register Ra = R11_scratch1,
1214            Rb = R12_scratch2;
1215   // stack: ..., a, b
1216   __ ld(Rb, Interpreter::stackElementSize,     R15_esp);
1217   __ ld(Ra, Interpreter::stackElementSize * 2, R15_esp);
1218   __ std(Rb, Interpreter::stackElementSize * 2, R15_esp);
1219   __ std(Ra, Interpreter::stackElementSize,     R15_esp);
1220   // stack: ..., b, a
1221 }
1222 
1223 void TemplateTable::iop2(Operation op) {
1224   transition(itos, itos);
1225 
1226   Register Rscratch = R11_scratch1;
1227 
1228   __ pop_i(Rscratch);
1229   // tos  = number of bits to shift
1230   // Rscratch = value to shift
1231   switch (op) {
1232     case  add:   __ add(R17_tos, Rscratch, R17_tos); break;
1233     case  sub:   __ sub(R17_tos, Rscratch, R17_tos); break;
1234     case  mul:   __ mullw(R17_tos, Rscratch, R17_tos); break;
1235     case  _and:  __ andr(R17_tos, Rscratch, R17_tos); break;
1236     case  _or:   __ orr(R17_tos, Rscratch, R17_tos); break;
1237     case  _xor:  __ xorr(R17_tos, Rscratch, R17_tos); break;
1238     case  shl:   __ rldicl(R17_tos, R17_tos, 0, 64-5); __ slw(R17_tos, Rscratch, R17_tos); break;
1239     case  shr:   __ rldicl(R17_tos, R17_tos, 0, 64-5); __ sraw(R17_tos, Rscratch, R17_tos); break;
1240     case  ushr:  __ rldicl(R17_tos, R17_tos, 0, 64-5); __ srw(R17_tos, Rscratch, R17_tos); break;
1241     default:     ShouldNotReachHere();
1242   }
1243 }
1244 
1245 void TemplateTable::lop2(Operation op) {
1246   transition(ltos, ltos);
1247 
1248   Register Rscratch = R11_scratch1;
1249   __ pop_l(Rscratch);
1250   switch (op) {
1251     case  add:   __ add(R17_tos, Rscratch, R17_tos); break;
1252     case  sub:   __ sub(R17_tos, Rscratch, R17_tos); break;
1253     case  _and:  __ andr(R17_tos, Rscratch, R17_tos); break;
1254     case  _or:   __ orr(R17_tos, Rscratch, R17_tos); break;
1255     case  _xor:  __ xorr(R17_tos, Rscratch, R17_tos); break;
1256     default:     ShouldNotReachHere();
1257   }
1258 }
1259 
1260 void TemplateTable::idiv() {
1261   transition(itos, itos);
1262 
1263   Label Lnormal, Lexception, Ldone;
1264   Register Rdividend = R11_scratch1; // Used by irem.
1265 
1266   __ addi(R0, R17_tos, 1);
1267   __ cmplwi(CCR0, R0, 2);
1268   __ bgt(CCR0, Lnormal); // divisor &lt;-1 or &gt;1
1269 
1270   __ cmpwi(CCR1, R17_tos, 0);
1271   __ beq(CCR1, Lexception); // divisor == 0
1272 
1273   __ pop_i(Rdividend);
1274   __ mullw(R17_tos, Rdividend, R17_tos); // div by +/-1
1275   __ b(Ldone);
1276 
1277   __ bind(Lexception);
1278   __ load_dispatch_table(R11_scratch1, (address*)Interpreter::_throw_ArithmeticException_entry);
1279   __ mtctr(R11_scratch1);
1280   __ bctr();
1281 
1282   __ align(32, 12);
1283   __ bind(Lnormal);
1284   __ pop_i(Rdividend);
1285   __ divw(R17_tos, Rdividend, R17_tos); // Can&#39;t divide minint/-1.
1286   __ bind(Ldone);
1287 }
1288 
1289 void TemplateTable::irem() {
1290   transition(itos, itos);
1291 
1292   __ mr(R12_scratch2, R17_tos);
1293   idiv();
1294   __ mullw(R17_tos, R17_tos, R12_scratch2);
1295   __ subf(R17_tos, R17_tos, R11_scratch1); // Dividend set by idiv.
1296 }
1297 
1298 void TemplateTable::lmul() {
1299   transition(ltos, ltos);
1300 
1301   __ pop_l(R11_scratch1);
1302   __ mulld(R17_tos, R11_scratch1, R17_tos);
1303 }
1304 
1305 void TemplateTable::ldiv() {
1306   transition(ltos, ltos);
1307 
1308   Label Lnormal, Lexception, Ldone;
1309   Register Rdividend = R11_scratch1; // Used by lrem.
1310 
1311   __ addi(R0, R17_tos, 1);
1312   __ cmpldi(CCR0, R0, 2);
1313   __ bgt(CCR0, Lnormal); // divisor &lt;-1 or &gt;1
1314 
1315   __ cmpdi(CCR1, R17_tos, 0);
1316   __ beq(CCR1, Lexception); // divisor == 0
1317 
1318   __ pop_l(Rdividend);
1319   __ mulld(R17_tos, Rdividend, R17_tos); // div by +/-1
1320   __ b(Ldone);
1321 
1322   __ bind(Lexception);
1323   __ load_dispatch_table(R11_scratch1, (address*)Interpreter::_throw_ArithmeticException_entry);
1324   __ mtctr(R11_scratch1);
1325   __ bctr();
1326 
1327   __ align(32, 12);
1328   __ bind(Lnormal);
1329   __ pop_l(Rdividend);
1330   __ divd(R17_tos, Rdividend, R17_tos); // Can&#39;t divide minint/-1.
1331   __ bind(Ldone);
1332 }
1333 
1334 void TemplateTable::lrem() {
1335   transition(ltos, ltos);
1336 
1337   __ mr(R12_scratch2, R17_tos);
1338   ldiv();
1339   __ mulld(R17_tos, R17_tos, R12_scratch2);
1340   __ subf(R17_tos, R17_tos, R11_scratch1); // Dividend set by ldiv.
1341 }
1342 
1343 void TemplateTable::lshl() {
1344   transition(itos, ltos);
1345 
1346   __ rldicl(R17_tos, R17_tos, 0, 64-6); // Extract least significant bits.
1347   __ pop_l(R11_scratch1);
1348   __ sld(R17_tos, R11_scratch1, R17_tos);
1349 }
1350 
1351 void TemplateTable::lshr() {
1352   transition(itos, ltos);
1353 
1354   __ rldicl(R17_tos, R17_tos, 0, 64-6); // Extract least significant bits.
1355   __ pop_l(R11_scratch1);
1356   __ srad(R17_tos, R11_scratch1, R17_tos);
1357 }
1358 
1359 void TemplateTable::lushr() {
1360   transition(itos, ltos);
1361 
1362   __ rldicl(R17_tos, R17_tos, 0, 64-6); // Extract least significant bits.
1363   __ pop_l(R11_scratch1);
1364   __ srd(R17_tos, R11_scratch1, R17_tos);
1365 }
1366 
1367 void TemplateTable::fop2(Operation op) {
1368   transition(ftos, ftos);
1369 
1370   switch (op) {
1371     case add: __ pop_f(F0_SCRATCH); __ fadds(F15_ftos, F0_SCRATCH, F15_ftos); break;
1372     case sub: __ pop_f(F0_SCRATCH); __ fsubs(F15_ftos, F0_SCRATCH, F15_ftos); break;
1373     case mul: __ pop_f(F0_SCRATCH); __ fmuls(F15_ftos, F0_SCRATCH, F15_ftos); break;
1374     case div: __ pop_f(F0_SCRATCH); __ fdivs(F15_ftos, F0_SCRATCH, F15_ftos); break;
1375     case rem:
1376       __ pop_f(F1_ARG1);
1377       __ fmr(F2_ARG2, F15_ftos);
1378       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem));
1379       __ fmr(F15_ftos, F1_RET);
1380       break;
1381 
1382     default: ShouldNotReachHere();
1383   }
1384 }
1385 
1386 void TemplateTable::dop2(Operation op) {
1387   transition(dtos, dtos);
1388 
1389   switch (op) {
1390     case add: __ pop_d(F0_SCRATCH); __ fadd(F15_ftos, F0_SCRATCH, F15_ftos); break;
1391     case sub: __ pop_d(F0_SCRATCH); __ fsub(F15_ftos, F0_SCRATCH, F15_ftos); break;
1392     case mul: __ pop_d(F0_SCRATCH); __ fmul(F15_ftos, F0_SCRATCH, F15_ftos); break;
1393     case div: __ pop_d(F0_SCRATCH); __ fdiv(F15_ftos, F0_SCRATCH, F15_ftos); break;
1394     case rem:
1395       __ pop_d(F1_ARG1);
1396       __ fmr(F2_ARG2, F15_ftos);
1397       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem));
1398       __ fmr(F15_ftos, F1_RET);
1399       break;
1400 
1401     default: ShouldNotReachHere();
1402   }
1403 }
1404 
1405 // Negate the value in the TOS cache.
1406 void TemplateTable::ineg() {
1407   transition(itos, itos);
1408 
1409   __ neg(R17_tos, R17_tos);
1410 }
1411 
1412 // Negate the value in the TOS cache.
1413 void TemplateTable::lneg() {
1414   transition(ltos, ltos);
1415 
1416   __ neg(R17_tos, R17_tos);
1417 }
1418 
1419 void TemplateTable::fneg() {
1420   transition(ftos, ftos);
1421 
1422   __ fneg(F15_ftos, F15_ftos);
1423 }
1424 
1425 void TemplateTable::dneg() {
1426   transition(dtos, dtos);
1427 
1428   __ fneg(F15_ftos, F15_ftos);
1429 }
1430 
1431 // Increments a local variable in place.
1432 void TemplateTable::iinc() {
1433   transition(vtos, vtos);
1434 
1435   const Register Rindex     = R11_scratch1,
1436                  Rincrement = R0,
1437                  Rvalue     = R12_scratch2;
1438 
1439   locals_index(Rindex);              // Load locals index from bytecode stream.
1440   __ lbz(Rincrement, 2, R14_bcp);    // Load increment from the bytecode stream.
1441   __ extsb(Rincrement, Rincrement);
1442 
1443   __ load_local_int(Rvalue, Rindex, Rindex); // Puts address of local into Rindex.
1444 
1445   __ add(Rvalue, Rincrement, Rvalue);
1446   __ stw(Rvalue, 0, Rindex);
1447 }
1448 
1449 void TemplateTable::wide_iinc() {
1450   transition(vtos, vtos);
1451 
1452   Register Rindex       = R11_scratch1,
1453            Rlocals_addr = Rindex,
1454            Rincr        = R12_scratch2;
1455   locals_index_wide(Rindex);
1456   __ get_2_byte_integer_at_bcp(4, Rincr, InterpreterMacroAssembler::Signed);
1457   __ load_local_int(R17_tos, Rlocals_addr, Rindex);
1458   __ add(R17_tos, Rincr, R17_tos);
1459   __ stw(R17_tos, 0, Rlocals_addr);
1460 }
1461 
1462 void TemplateTable::convert() {
1463   // %%%%% Factor this first part accross platforms
1464 #ifdef ASSERT
1465   TosState tos_in  = ilgl;
1466   TosState tos_out = ilgl;
1467   switch (bytecode()) {
1468     case Bytecodes::_i2l: // fall through
1469     case Bytecodes::_i2f: // fall through
1470     case Bytecodes::_i2d: // fall through
1471     case Bytecodes::_i2b: // fall through
1472     case Bytecodes::_i2c: // fall through
1473     case Bytecodes::_i2s: tos_in = itos; break;
1474     case Bytecodes::_l2i: // fall through
1475     case Bytecodes::_l2f: // fall through
1476     case Bytecodes::_l2d: tos_in = ltos; break;
1477     case Bytecodes::_f2i: // fall through
1478     case Bytecodes::_f2l: // fall through
1479     case Bytecodes::_f2d: tos_in = ftos; break;
1480     case Bytecodes::_d2i: // fall through
1481     case Bytecodes::_d2l: // fall through
1482     case Bytecodes::_d2f: tos_in = dtos; break;
1483     default             : ShouldNotReachHere();
1484   }
1485   switch (bytecode()) {
1486     case Bytecodes::_l2i: // fall through
1487     case Bytecodes::_f2i: // fall through
1488     case Bytecodes::_d2i: // fall through
1489     case Bytecodes::_i2b: // fall through
1490     case Bytecodes::_i2c: // fall through
1491     case Bytecodes::_i2s: tos_out = itos; break;
1492     case Bytecodes::_i2l: // fall through
1493     case Bytecodes::_f2l: // fall through
1494     case Bytecodes::_d2l: tos_out = ltos; break;
1495     case Bytecodes::_i2f: // fall through
1496     case Bytecodes::_l2f: // fall through
1497     case Bytecodes::_d2f: tos_out = ftos; break;
1498     case Bytecodes::_i2d: // fall through
1499     case Bytecodes::_l2d: // fall through
1500     case Bytecodes::_f2d: tos_out = dtos; break;
1501     default             : ShouldNotReachHere();
1502   }
1503   transition(tos_in, tos_out);
1504 #endif
1505 
1506   // Conversion
1507   Label done;
1508   switch (bytecode()) {
1509     case Bytecodes::_i2l:
1510       __ extsw(R17_tos, R17_tos);
1511       break;
1512 
1513     case Bytecodes::_l2i:
1514       // Nothing to do, we&#39;ll continue to work with the lower bits.
1515       break;
1516 
1517     case Bytecodes::_i2b:
1518       __ extsb(R17_tos, R17_tos);
1519       break;
1520 
1521     case Bytecodes::_i2c:
1522       __ rldicl(R17_tos, R17_tos, 0, 64-2*8);
1523       break;
1524 
1525     case Bytecodes::_i2s:
1526       __ extsh(R17_tos, R17_tos);
1527       break;
1528 
1529     case Bytecodes::_i2d:
1530       __ extsw(R17_tos, R17_tos);
1531     case Bytecodes::_l2d:
1532       __ move_l_to_d();
1533       __ fcfid(F15_ftos, F15_ftos);
1534       break;
1535 
1536     case Bytecodes::_i2f:
1537       __ extsw(R17_tos, R17_tos);
1538       __ move_l_to_d();
1539       if (VM_Version::has_fcfids()) { // fcfids is &gt;= Power7 only
1540         // Comment: alternatively, load with sign extend could be done by lfiwax.
1541         __ fcfids(F15_ftos, F15_ftos);
1542       } else {
1543         __ fcfid(F15_ftos, F15_ftos);
1544         __ frsp(F15_ftos, F15_ftos);
1545       }
1546       break;
1547 
1548     case Bytecodes::_l2f:
1549       if (VM_Version::has_fcfids()) { // fcfids is &gt;= Power7 only
1550         __ move_l_to_d();
1551         __ fcfids(F15_ftos, F15_ftos);
1552       } else {
1553         // Avoid rounding problem when result should be 0x3f800001: need fixup code before fcfid+frsp.
1554         __ mr(R3_ARG1, R17_tos);
1555         __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::l2f));
1556         __ fmr(F15_ftos, F1_RET);
1557       }
1558       break;
1559 
1560     case Bytecodes::_f2d:
1561       // empty
1562       break;
1563 
1564     case Bytecodes::_d2f:
1565       __ frsp(F15_ftos, F15_ftos);
1566       break;
1567 
1568     case Bytecodes::_d2i:
1569     case Bytecodes::_f2i:
1570       __ fcmpu(CCR0, F15_ftos, F15_ftos);
1571       __ li(R17_tos, 0); // 0 in case of NAN
1572       __ bso(CCR0, done);
1573       __ fctiwz(F15_ftos, F15_ftos);
1574       __ move_d_to_l();
1575       break;
1576 
1577     case Bytecodes::_d2l:
1578     case Bytecodes::_f2l:
1579       __ fcmpu(CCR0, F15_ftos, F15_ftos);
1580       __ li(R17_tos, 0); // 0 in case of NAN
1581       __ bso(CCR0, done);
1582       __ fctidz(F15_ftos, F15_ftos);
1583       __ move_d_to_l();
1584       break;
1585 
1586     default: ShouldNotReachHere();
1587   }
1588   __ bind(done);
1589 }
1590 
1591 // Long compare
1592 void TemplateTable::lcmp() {
1593   transition(ltos, itos);
1594 
1595   const Register Rscratch = R11_scratch1;
1596   __ pop_l(Rscratch); // first operand, deeper in stack
1597 
1598   __ cmpd(CCR0, Rscratch, R17_tos); // compare
1599   __ mfcr(R17_tos); // set bit 32..33 as follows: &lt;: 0b10, =: 0b00, &gt;: 0b01
1600   __ srwi(Rscratch, R17_tos, 30);
1601   __ srawi(R17_tos, R17_tos, 31);
1602   __ orr(R17_tos, Rscratch, R17_tos); // set result as follows: &lt;: -1, =: 0, &gt;: 1
1603 }
1604 
1605 // fcmpl/fcmpg and dcmpl/dcmpg bytecodes
1606 // unordered_result == -1 =&gt; fcmpl or dcmpl
1607 // unordered_result ==  1 =&gt; fcmpg or dcmpg
1608 void TemplateTable::float_cmp(bool is_float, int unordered_result) {
1609   const FloatRegister Rfirst  = F0_SCRATCH,
1610                       Rsecond = F15_ftos;
1611   const Register Rscratch = R11_scratch1;
1612 
1613   if (is_float) {
1614     __ pop_f(Rfirst);
1615   } else {
1616     __ pop_d(Rfirst);
1617   }
1618 
1619   Label Lunordered, Ldone;
1620   __ fcmpu(CCR0, Rfirst, Rsecond); // compare
1621   if (unordered_result) {
1622     __ bso(CCR0, Lunordered);
1623   }
1624   __ mfcr(R17_tos); // set bit 32..33 as follows: &lt;: 0b10, =: 0b00, &gt;: 0b01
1625   __ srwi(Rscratch, R17_tos, 30);
1626   __ srawi(R17_tos, R17_tos, 31);
1627   __ orr(R17_tos, Rscratch, R17_tos); // set result as follows: &lt;: -1, =: 0, &gt;: 1
1628   if (unordered_result) {
1629     __ b(Ldone);
1630     __ bind(Lunordered);
1631     __ load_const_optimized(R17_tos, unordered_result);
1632   }
1633   __ bind(Ldone);
1634 }
1635 
1636 // Branch_conditional which takes TemplateTable::Condition.
1637 void TemplateTable::branch_conditional(ConditionRegister crx, TemplateTable::Condition cc, Label&amp; L, bool invert) {
1638   bool positive = false;
1639   Assembler::Condition cond = Assembler::equal;
1640   switch (cc) {
1641     case TemplateTable::equal:         positive = true ; cond = Assembler::equal  ; break;
1642     case TemplateTable::not_equal:     positive = false; cond = Assembler::equal  ; break;
1643     case TemplateTable::less:          positive = true ; cond = Assembler::less   ; break;
1644     case TemplateTable::less_equal:    positive = false; cond = Assembler::greater; break;
1645     case TemplateTable::greater:       positive = true ; cond = Assembler::greater; break;
1646     case TemplateTable::greater_equal: positive = false; cond = Assembler::less   ; break;
1647     default: ShouldNotReachHere();
1648   }
1649   int bo = (positive != invert) ? Assembler::bcondCRbiIs1 : Assembler::bcondCRbiIs0;
1650   int bi = Assembler::bi0(crx, cond);
1651   __ bc(bo, bi, L);
1652 }
1653 
1654 void TemplateTable::branch(bool is_jsr, bool is_wide) {
1655 
1656   // Note: on SPARC, we use InterpreterMacroAssembler::if_cmp also.
1657   __ verify_thread();
1658 
1659   const Register Rscratch1    = R11_scratch1,
1660                  Rscratch2    = R12_scratch2,
1661                  Rscratch3    = R3_ARG1,
1662                  R4_counters  = R4_ARG2,
1663                  bumped_count = R31,
1664                  Rdisp        = R22_tmp2;
1665 
1666   __ profile_taken_branch(Rscratch1, bumped_count);
1667 
1668   // Get (wide) offset.
1669   if (is_wide) {
1670     __ get_4_byte_integer_at_bcp(1, Rdisp, InterpreterMacroAssembler::Signed);
1671   } else {
1672     __ get_2_byte_integer_at_bcp(1, Rdisp, InterpreterMacroAssembler::Signed);
1673   }
1674 
1675   // --------------------------------------------------------------------------
1676   // Handle all the JSR stuff here, then exit.
1677   // It&#39;s much shorter and cleaner than intermingling with the
1678   // non-JSR normal-branch stuff occurring below.
1679   if (is_jsr) {
1680     // Compute return address as bci in Otos_i.
1681     __ ld(Rscratch1, in_bytes(Method::const_offset()), R19_method);
1682     __ addi(Rscratch2, R14_bcp, -in_bytes(ConstMethod::codes_offset()) + (is_wide ? 5 : 3));
1683     __ subf(R17_tos, Rscratch1, Rscratch2);
1684 
1685     // Bump bcp to target of JSR.
1686     __ add(R14_bcp, Rdisp, R14_bcp);
1687     // Push returnAddress for &quot;ret&quot; on stack.
1688     __ push_ptr(R17_tos);
1689     // And away we go!
1690     __ dispatch_next(vtos, 0 ,true);
1691     return;
1692   }
1693 
1694   // --------------------------------------------------------------------------
1695   // Normal (non-jsr) branch handling
1696 
1697   // Bump bytecode pointer by displacement (take the branch).
1698   __ add(R14_bcp, Rdisp, R14_bcp); // Add to bc addr.
1699 
1700   const bool increment_invocation_counter_for_backward_branches = UseCompiler &amp;&amp; UseLoopCounter;
1701   if (increment_invocation_counter_for_backward_branches) {
1702     Label Lforward;
1703 
1704     // Check branch direction.
1705     __ cmpdi(CCR0, Rdisp, 0);
1706     __ bgt(CCR0, Lforward);
1707 
1708     __ get_method_counters(R19_method, R4_counters, Lforward);
1709 
1710     if (TieredCompilation) {
1711       Label Lno_mdo, Loverflow;
1712       const int increment = InvocationCounter::count_increment;
1713       if (ProfileInterpreter) {
1714         Register Rmdo = Rscratch1;
1715 
1716         // If no method data exists, go to profile_continue.
1717         __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);
1718         __ cmpdi(CCR0, Rmdo, 0);
1719         __ beq(CCR0, Lno_mdo);
1720 
1721         // Increment backedge counter in the MDO.
1722         const int mdo_bc_offs = in_bytes(MethodData::backedge_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
1723         __ lwz(Rscratch2, mdo_bc_offs, Rmdo);
1724         __ lwz(Rscratch3, in_bytes(MethodData::backedge_mask_offset()), Rmdo);
1725         __ addi(Rscratch2, Rscratch2, increment);
1726         __ stw(Rscratch2, mdo_bc_offs, Rmdo);
1727         if (UseOnStackReplacement) {
1728           __ and_(Rscratch3, Rscratch2, Rscratch3);
1729           __ bne(CCR0, Lforward);
1730           __ b(Loverflow);
1731         } else {
1732           __ b(Lforward);
1733         }
1734       }
1735 
1736       // If there&#39;s no MDO, increment counter in method.
1737       const int mo_bc_offs = in_bytes(MethodCounters::backedge_counter_offset()) + in_bytes(InvocationCounter::counter_offset());
1738       __ bind(Lno_mdo);
1739       __ lwz(Rscratch2, mo_bc_offs, R4_counters);
1740       __ lwz(Rscratch3, in_bytes(MethodCounters::backedge_mask_offset()), R4_counters);
1741       __ addi(Rscratch2, Rscratch2, increment);
1742       __ stw(Rscratch2, mo_bc_offs, R4_counters);
1743       if (UseOnStackReplacement) {
1744         __ and_(Rscratch3, Rscratch2, Rscratch3);
1745         __ bne(CCR0, Lforward);
1746       } else {
1747         __ b(Lforward);
1748       }
1749       __ bind(Loverflow);
1750 
1751       // Notify point for loop, pass branch bytecode.
1752       __ subf(R4_ARG2, Rdisp, R14_bcp); // Compute branch bytecode (previous bcp).
1753       __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);
1754 
1755       // Was an OSR adapter generated?
1756       __ cmpdi(CCR0, R3_RET, 0);
1757       __ beq(CCR0, Lforward);
1758 
1759       // Has the nmethod been invalidated already?
1760       __ lbz(R0, nmethod::state_offset(), R3_RET);
1761       __ cmpwi(CCR0, R0, nmethod::in_use);
1762       __ bne(CCR0, Lforward);
1763 
1764       // Migrate the interpreter frame off of the stack.
1765       // We can use all registers because we will not return to interpreter from this point.
1766 
1767       // Save nmethod.
1768       const Register osr_nmethod = R31;
1769       __ mr(osr_nmethod, R3_RET);
1770       __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R11_scratch1);
1771       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin), R16_thread);
1772       __ reset_last_Java_frame();
1773       // OSR buffer is in ARG1.
1774 
1775       // Remove the interpreter frame.
1776       __ merge_frames(/*top_frame_sp*/ R21_sender_SP, /*return_pc*/ R0, R11_scratch1, R12_scratch2);
1777 
1778       // Jump to the osr code.
1779       __ ld(R11_scratch1, nmethod::osr_entry_point_offset(), osr_nmethod);
1780       __ mtlr(R0);
1781       __ mtctr(R11_scratch1);
1782       __ bctr();
1783 
1784     } else {
1785 
1786       const Register invoke_ctr = Rscratch1;
1787       // Update Backedge branch separately from invocations.
1788       __ increment_backedge_counter(R4_counters, invoke_ctr, Rscratch2, Rscratch3);
1789 
1790       if (ProfileInterpreter) {
1791         __ test_invocation_counter_for_mdp(invoke_ctr, R4_counters, Rscratch2, Lforward);
1792         if (UseOnStackReplacement) {
1793           __ test_backedge_count_for_osr(bumped_count, R4_counters, R14_bcp, Rdisp, Rscratch2);
1794         }
1795       } else {
1796         if (UseOnStackReplacement) {
1797           __ test_backedge_count_for_osr(invoke_ctr, R4_counters, R14_bcp, Rdisp, Rscratch2);
1798         }
1799       }
1800     }
1801 
1802     __ bind(Lforward);
1803   }
1804   __ dispatch_next(vtos, 0, true);
1805 }
1806 
1807 // Helper function for if_cmp* methods below.
1808 // Factored out common compare and branch code.
1809 void TemplateTable::if_cmp_common(Register Rfirst, Register Rsecond, Register Rscratch1, Register Rscratch2, Condition cc, bool is_jint, bool cmp0) {
1810   Label Lnot_taken;
1811   // Note: The condition code we get is the condition under which we
1812   // *fall through*! So we have to inverse the CC here.
1813 
1814   if (is_jint) {
1815     if (cmp0) {
1816       __ cmpwi(CCR0, Rfirst, 0);
1817     } else {
1818       __ cmpw(CCR0, Rfirst, Rsecond);
1819     }
1820   } else {
1821     if (cmp0) {
1822       __ cmpdi(CCR0, Rfirst, 0);
1823     } else {
1824       __ cmpd(CCR0, Rfirst, Rsecond);
1825     }
1826   }
1827   branch_conditional(CCR0, cc, Lnot_taken, /*invert*/ true);
1828 
1829   // Conition is false =&gt; Jump!
1830   branch(false, false);
1831 
1832   // Condition is not true =&gt; Continue.
1833   __ align(32, 12);
1834   __ bind(Lnot_taken);
1835   __ profile_not_taken_branch(Rscratch1, Rscratch2);
1836 }
1837 
1838 // Compare integer values with zero and fall through if CC holds, branch away otherwise.
1839 void TemplateTable::if_0cmp(Condition cc) {
1840   transition(itos, vtos);
1841 
1842   if_cmp_common(R17_tos, noreg, R11_scratch1, R12_scratch2, cc, true, true);
1843 }
1844 
1845 // Compare integer values and fall through if CC holds, branch away otherwise.
1846 //
1847 // Interface:
1848 //  - Rfirst: First operand  (older stack value)
1849 //  - tos:    Second operand (younger stack value)
1850 void TemplateTable::if_icmp(Condition cc) {
1851   transition(itos, vtos);
1852 
1853   const Register Rfirst  = R0,
1854                  Rsecond = R17_tos;
1855 
1856   __ pop_i(Rfirst);
1857   if_cmp_common(Rfirst, Rsecond, R11_scratch1, R12_scratch2, cc, true, false);
1858 }
1859 
1860 void TemplateTable::if_nullcmp(Condition cc) {
1861   transition(atos, vtos);
1862 
1863   if_cmp_common(R17_tos, noreg, R11_scratch1, R12_scratch2, cc, false, true);
1864 }
1865 
1866 void TemplateTable::if_acmp(Condition cc) {
1867   transition(atos, vtos);
1868 
1869   const Register Rfirst  = R0,
1870                  Rsecond = R17_tos;
1871 
1872   __ pop_ptr(Rfirst);
1873   if_cmp_common(Rfirst, Rsecond, R11_scratch1, R12_scratch2, cc, false, false);
1874 }
1875 
1876 void TemplateTable::ret() {
1877   locals_index(R11_scratch1);
1878   __ load_local_ptr(R17_tos, R11_scratch1, R11_scratch1);
1879 
1880   __ profile_ret(vtos, R17_tos, R11_scratch1, R12_scratch2);
1881 
1882   __ ld(R11_scratch1, in_bytes(Method::const_offset()), R19_method);
1883   __ add(R11_scratch1, R17_tos, R11_scratch1);
1884   __ addi(R14_bcp, R11_scratch1, in_bytes(ConstMethod::codes_offset()));
1885   __ dispatch_next(vtos, 0, true);
1886 }
1887 
1888 void TemplateTable::wide_ret() {
1889   transition(vtos, vtos);
1890 
1891   const Register Rindex = R3_ARG1,
1892                  Rscratch1 = R11_scratch1,
1893                  Rscratch2 = R12_scratch2;
1894 
1895   locals_index_wide(Rindex);
1896   __ load_local_ptr(R17_tos, R17_tos, Rindex);
1897   __ profile_ret(vtos, R17_tos, Rscratch1, R12_scratch2);
1898   // Tos now contains the bci, compute the bcp from that.
1899   __ ld(Rscratch1, in_bytes(Method::const_offset()), R19_method);
1900   __ addi(Rscratch2, R17_tos, in_bytes(ConstMethod::codes_offset()));
1901   __ add(R14_bcp, Rscratch1, Rscratch2);
1902   __ dispatch_next(vtos, 0, true);
1903 }
1904 
1905 void TemplateTable::tableswitch() {
1906   transition(itos, vtos);
1907 
1908   Label Ldispatch, Ldefault_case;
1909   Register Rlow_byte         = R3_ARG1,
1910            Rindex            = Rlow_byte,
1911            Rhigh_byte        = R4_ARG2,
1912            Rdef_offset_addr  = R5_ARG3, // is going to contain address of default offset
1913            Rscratch1         = R11_scratch1,
1914            Rscratch2         = R12_scratch2,
1915            Roffset           = R6_ARG4;
1916 
1917   // Align bcp.
1918   __ addi(Rdef_offset_addr, R14_bcp, BytesPerInt);
1919   __ clrrdi(Rdef_offset_addr, Rdef_offset_addr, log2_long((jlong)BytesPerInt));
1920 
1921   // Load lo &amp; hi.
1922   __ get_u4(Rlow_byte, Rdef_offset_addr, BytesPerInt, InterpreterMacroAssembler::Unsigned);
1923   __ get_u4(Rhigh_byte, Rdef_offset_addr, 2 *BytesPerInt, InterpreterMacroAssembler::Unsigned);
1924 
1925   // Check for default case (=index outside [low,high]).
1926   __ cmpw(CCR0, R17_tos, Rlow_byte);
1927   __ cmpw(CCR1, R17_tos, Rhigh_byte);
1928   __ blt(CCR0, Ldefault_case);
1929   __ bgt(CCR1, Ldefault_case);
1930 
1931   // Lookup dispatch offset.
1932   __ sub(Rindex, R17_tos, Rlow_byte);
1933   __ extsw(Rindex, Rindex);
1934   __ profile_switch_case(Rindex, Rhigh_byte /* scratch */, Rscratch1, Rscratch2);
1935   __ sldi(Rindex, Rindex, LogBytesPerInt);
1936   __ addi(Rindex, Rindex, 3 * BytesPerInt);
1937 #if defined(VM_LITTLE_ENDIAN)
1938   __ lwbrx(Roffset, Rdef_offset_addr, Rindex);
1939   __ extsw(Roffset, Roffset);
1940 #else
1941   __ lwax(Roffset, Rdef_offset_addr, Rindex);
1942 #endif
1943   __ b(Ldispatch);
1944 
1945   __ bind(Ldefault_case);
1946   __ profile_switch_default(Rhigh_byte, Rscratch1);
1947   __ get_u4(Roffset, Rdef_offset_addr, 0, InterpreterMacroAssembler::Signed);
1948 
1949   __ bind(Ldispatch);
1950 
1951   __ add(R14_bcp, Roffset, R14_bcp);
1952   __ dispatch_next(vtos, 0, true);
1953 }
1954 
1955 void TemplateTable::lookupswitch() {
1956   transition(itos, itos);
1957   __ stop(&quot;lookupswitch bytecode should have been rewritten&quot;);
1958 }
1959 
1960 // Table switch using linear search through cases.
1961 // Bytecode stream format:
1962 // Bytecode (1) | 4-byte padding | default offset (4) | count (4) | value/offset pair1 (8) | value/offset pair2 (8) | ...
1963 // Note: Everything is big-endian format here.
1964 void TemplateTable::fast_linearswitch() {
1965   transition(itos, vtos);
1966 
1967   Label Lloop_entry, Lsearch_loop, Lcontinue_execution, Ldefault_case;
1968   Register Rcount           = R3_ARG1,
1969            Rcurrent_pair    = R4_ARG2,
1970            Rdef_offset_addr = R5_ARG3, // Is going to contain address of default offset.
1971            Roffset          = R31,     // Might need to survive C call.
1972            Rvalue           = R12_scratch2,
1973            Rscratch         = R11_scratch1,
1974            Rcmp_value       = R17_tos;
1975 
1976   // Align bcp.
1977   __ addi(Rdef_offset_addr, R14_bcp, BytesPerInt);
1978   __ clrrdi(Rdef_offset_addr, Rdef_offset_addr, log2_long((jlong)BytesPerInt));
1979 
1980   // Setup loop counter and limit.
1981   __ get_u4(Rcount, Rdef_offset_addr, BytesPerInt, InterpreterMacroAssembler::Unsigned);
1982   __ addi(Rcurrent_pair, Rdef_offset_addr, 2 * BytesPerInt); // Rcurrent_pair now points to first pair.
1983 
1984   __ mtctr(Rcount);
1985   __ cmpwi(CCR0, Rcount, 0);
1986   __ bne(CCR0, Lloop_entry);
1987 
1988   // Default case
1989   __ bind(Ldefault_case);
1990   __ get_u4(Roffset, Rdef_offset_addr, 0, InterpreterMacroAssembler::Signed);
1991   if (ProfileInterpreter) {
1992     __ profile_switch_default(Rdef_offset_addr, Rcount/* scratch */);
1993   }
1994   __ b(Lcontinue_execution);
1995 
1996   // Next iteration
1997   __ bind(Lsearch_loop);
1998   __ bdz(Ldefault_case);
1999   __ addi(Rcurrent_pair, Rcurrent_pair, 2 * BytesPerInt);
2000   __ bind(Lloop_entry);
2001   __ get_u4(Rvalue, Rcurrent_pair, 0, InterpreterMacroAssembler::Unsigned);
2002   __ cmpw(CCR0, Rvalue, Rcmp_value);
2003   __ bne(CCR0, Lsearch_loop);
2004 
2005   // Found, load offset.
2006   __ get_u4(Roffset, Rcurrent_pair, BytesPerInt, InterpreterMacroAssembler::Signed);
2007   // Calculate case index and profile
2008   __ mfctr(Rcurrent_pair);
2009   if (ProfileInterpreter) {
2010     __ sub(Rcurrent_pair, Rcount, Rcurrent_pair);
2011     __ profile_switch_case(Rcurrent_pair, Rcount /*scratch*/, Rdef_offset_addr/*scratch*/, Rscratch);
2012   }
2013 
2014   __ bind(Lcontinue_execution);
2015   __ add(R14_bcp, Roffset, R14_bcp);
2016   __ dispatch_next(vtos, 0, true);
2017 }
2018 
2019 // Table switch using binary search (value/offset pairs are ordered).
2020 // Bytecode stream format:
2021 // Bytecode (1) | 4-byte padding | default offset (4) | count (4) | value/offset pair1 (8) | value/offset pair2 (8) | ...
2022 // Note: Everything is big-endian format here. So on little endian machines, we have to revers offset and count and cmp value.
2023 void TemplateTable::fast_binaryswitch() {
2024 
2025   transition(itos, vtos);
2026   // Implementation using the following core algorithm: (copied from Intel)
2027   //
2028   // int binary_search(int key, LookupswitchPair* array, int n) {
2029   //   // Binary search according to &quot;Methodik des Programmierens&quot; by
2030   //   // Edsger W. Dijkstra and W.H.J. Feijen, Addison Wesley Germany 1985.
2031   //   int i = 0;
2032   //   int j = n;
2033   //   while (i+1 &lt; j) {
2034   //     // invariant P: 0 &lt;= i &lt; j &lt;= n and (a[i] &lt;= key &lt; a[j] or Q)
2035   //     // with      Q: for all i: 0 &lt;= i &lt; n: key &lt; a[i]
2036   //     // where a stands for the array and assuming that the (inexisting)
2037   //     // element a[n] is infinitely big.
2038   //     int h = (i + j) &gt;&gt; 1;
2039   //     // i &lt; h &lt; j
2040   //     if (key &lt; array[h].fast_match()) {
2041   //       j = h;
2042   //     } else {
2043   //       i = h;
2044   //     }
2045   //   }
2046   //   // R: a[i] &lt;= key &lt; a[i+1] or Q
2047   //   // (i.e., if key is within array, i is the correct index)
2048   //   return i;
2049   // }
2050 
2051   // register allocation
2052   const Register Rkey     = R17_tos;          // already set (tosca)
2053   const Register Rarray   = R3_ARG1;
2054   const Register Ri       = R4_ARG2;
2055   const Register Rj       = R5_ARG3;
2056   const Register Rh       = R6_ARG4;
2057   const Register Rscratch = R11_scratch1;
2058 
2059   const int log_entry_size = 3;
2060   const int entry_size = 1 &lt;&lt; log_entry_size;
2061 
2062   Label found;
2063 
2064   // Find Array start,
2065   __ addi(Rarray, R14_bcp, 3 * BytesPerInt);
2066   __ clrrdi(Rarray, Rarray, log2_long((jlong)BytesPerInt));
2067 
2068   // initialize i &amp; j
2069   __ li(Ri,0);
2070   __ get_u4(Rj, Rarray, -BytesPerInt, InterpreterMacroAssembler::Unsigned);
2071 
2072   // and start.
2073   Label entry;
2074   __ b(entry);
2075 
2076   // binary search loop
2077   { Label loop;
2078     __ bind(loop);
2079     // int h = (i + j) &gt;&gt; 1;
2080     __ srdi(Rh, Rh, 1);
2081     // if (key &lt; array[h].fast_match()) {
2082     //   j = h;
2083     // } else {
2084     //   i = h;
2085     // }
2086     __ sldi(Rscratch, Rh, log_entry_size);
2087 #if defined(VM_LITTLE_ENDIAN)
2088     __ lwbrx(Rscratch, Rscratch, Rarray);
2089 #else
2090     __ lwzx(Rscratch, Rscratch, Rarray);
2091 #endif
2092 
2093     // if (key &lt; current value)
2094     //   Rh = Rj
2095     // else
2096     //   Rh = Ri
2097     Label Lgreater;
2098     __ cmpw(CCR0, Rkey, Rscratch);
2099     __ bge(CCR0, Lgreater);
2100     __ mr(Rj, Rh);
2101     __ b(entry);
2102     __ bind(Lgreater);
2103     __ mr(Ri, Rh);
2104 
2105     // while (i+1 &lt; j)
2106     __ bind(entry);
2107     __ addi(Rscratch, Ri, 1);
2108     __ cmpw(CCR0, Rscratch, Rj);
2109     __ add(Rh, Ri, Rj); // start h = i + j &gt;&gt; 1;
2110 
2111     __ blt(CCR0, loop);
2112   }
2113 
2114   // End of binary search, result index is i (must check again!).
2115   Label default_case;
2116   Label continue_execution;
2117   if (ProfileInterpreter) {
2118     __ mr(Rh, Ri);              // Save index in i for profiling.
2119   }
2120   // Ri = value offset
2121   __ sldi(Ri, Ri, log_entry_size);
2122   __ add(Ri, Ri, Rarray);
2123   __ get_u4(Rscratch, Ri, 0, InterpreterMacroAssembler::Unsigned);
2124 
2125   Label not_found;
2126   // Ri = offset offset
2127   __ cmpw(CCR0, Rkey, Rscratch);
2128   __ beq(CCR0, not_found);
2129   // entry not found -&gt; j = default offset
2130   __ get_u4(Rj, Rarray, -2 * BytesPerInt, InterpreterMacroAssembler::Unsigned);
2131   __ b(default_case);
2132 
2133   __ bind(not_found);
2134   // entry found -&gt; j = offset
2135   __ profile_switch_case(Rh, Rj, Rscratch, Rkey);
2136   __ get_u4(Rj, Ri, BytesPerInt, InterpreterMacroAssembler::Unsigned);
2137 
2138   if (ProfileInterpreter) {
2139     __ b(continue_execution);
2140   }
2141 
2142   __ bind(default_case); // fall through (if not profiling)
2143   __ profile_switch_default(Ri, Rscratch);
2144 
2145   __ bind(continue_execution);
2146 
2147   __ extsw(Rj, Rj);
2148   __ add(R14_bcp, Rj, R14_bcp);
2149   __ dispatch_next(vtos, 0 , true);
2150 }
2151 
2152 void TemplateTable::_return(TosState state) {
2153   transition(state, state);
2154   assert(_desc-&gt;calls_vm(),
2155          &quot;inconsistent calls_vm information&quot;); // call in remove_activation
2156 
2157   if (_desc-&gt;bytecode() == Bytecodes::_return_register_finalizer) {
2158 
2159     Register Rscratch     = R11_scratch1,
2160              Rklass       = R12_scratch2,
2161              Rklass_flags = Rklass;
2162     Label Lskip_register_finalizer;
2163 
2164     // Check if the method has the FINALIZER flag set and call into the VM to finalize in this case.
2165     assert(state == vtos, &quot;only valid state&quot;);
2166     __ ld(R17_tos, 0, R18_locals);
2167 
2168     // Load klass of this obj.
2169     __ load_klass(Rklass, R17_tos);
2170     __ lwz(Rklass_flags, in_bytes(Klass::access_flags_offset()), Rklass);
2171     __ testbitdi(CCR0, R0, Rklass_flags, exact_log2(JVM_ACC_HAS_FINALIZER));
2172     __ bfalse(CCR0, Lskip_register_finalizer);
2173 
2174     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::register_finalizer), R17_tos /* obj */);
2175 
2176     __ align(32, 12);
2177     __ bind(Lskip_register_finalizer);
2178   }
2179 
2180   if (_desc-&gt;bytecode() != Bytecodes::_return_register_finalizer) {
2181     Label no_safepoint;
2182     __ ld(R11_scratch1, in_bytes(Thread::polling_page_offset()), R16_thread);
2183     __ andi_(R11_scratch1, R11_scratch1, SafepointMechanism::poll_bit());
2184     __ beq(CCR0, no_safepoint);
2185     __ push(state);
2186     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_safepoint));
2187     __ pop(state);
2188     __ bind(no_safepoint);
2189   }
2190 
2191   // Move the result value into the correct register and remove memory stack frame.
2192   __ remove_activation(state, /* throw_monitor_exception */ true);
2193   // Restoration of lr done by remove_activation.
2194   switch (state) {
2195     // Narrow result if state is itos but result type is smaller.
2196     // Need to narrow in the return bytecode rather than in generate_return_entry
2197     // since compiled code callers expect the result to already be narrowed.
2198     case itos: __ narrow(R17_tos); /* fall through */
2199     case ltos:
2200     case atos: __ mr(R3_RET, R17_tos); break;
2201     case ftos:
2202     case dtos: __ fmr(F1_RET, F15_ftos); break;
2203     case vtos: // This might be a constructor. Final fields (and volatile fields on PPC64) need
2204                // to get visible before the reference to the object gets stored anywhere.
2205                __ membar(Assembler::StoreStore); break;
2206     default  : ShouldNotReachHere();
2207   }
2208   __ blr();
2209 }
2210 
2211 // ============================================================================
2212 // Constant pool cache access
2213 //
2214 // Memory ordering:
2215 //
2216 // Like done in C++ interpreter, we load the fields
2217 //   - _indices
2218 //   - _f12_oop
2219 // acquired, because these are asked if the cache is already resolved. We don&#39;t
2220 // want to float loads above this check.
2221 // See also comments in ConstantPoolCacheEntry::bytecode_1(),
2222 // ConstantPoolCacheEntry::bytecode_2() and ConstantPoolCacheEntry::f1();
2223 
2224 // Call into the VM if call site is not yet resolved
2225 //
2226 // Input regs:
2227 //   - None, all passed regs are outputs.
2228 //
2229 // Returns:
2230 //   - Rcache:  The const pool cache entry that contains the resolved result.
2231 //   - Rresult: Either noreg or output for f1/f2.
2232 //
2233 // Kills:
2234 //   - Rscratch
2235 void TemplateTable::resolve_cache_and_index(int byte_no, Register Rcache, Register Rscratch, size_t index_size) {
2236 
2237   __ get_cache_and_index_at_bcp(Rcache, 1, index_size);
2238   Label Lresolved, Ldone, L_clinit_barrier_slow;
2239 
2240   Bytecodes::Code code = bytecode();
2241   switch (code) {
2242     case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2243     case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2244     default:
2245       break;
2246   }
2247 
2248   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
2249   // We are resolved if the indices offset contains the current bytecode.
2250 #if defined(VM_LITTLE_ENDIAN)
2251   __ lbz(Rscratch, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()) + byte_no + 1, Rcache);
2252 #else
2253   __ lbz(Rscratch, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()) + 7 - (byte_no + 1), Rcache);
2254 #endif
2255   // Acquire by cmp-br-isync (see below).
2256   __ cmpdi(CCR0, Rscratch, (int)code);
2257   __ beq(CCR0, Lresolved);
2258 
2259   // Class initialization barrier slow path lands here as well.
2260   __ bind(L_clinit_barrier_slow);
2261 
2262   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2263   __ li(R4_ARG2, code);
2264   __ call_VM(noreg, entry, R4_ARG2, true);
2265 
2266   // Update registers with resolved info.
2267   __ get_cache_and_index_at_bcp(Rcache, 1, index_size);
2268   __ b(Ldone);
2269 
2270   __ bind(Lresolved);
2271   __ isync(); // Order load wrt. succeeding loads.
2272 
2273   // Class initialization barrier for static methods
2274   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; bytecode() == Bytecodes::_invokestatic) {
2275     const Register method = Rscratch;
2276     const Register klass  = Rscratch;
2277 
2278     __ load_resolved_method_at_index(byte_no, Rcache, method);
2279     __ load_method_holder(klass, method);
2280     __ clinit_barrier(klass, R16_thread, NULL /*L_fast_path*/, &amp;L_clinit_barrier_slow);
2281   }
2282 
2283   __ bind(Ldone);
2284 }
2285 
2286 // Load the constant pool cache entry at field accesses into registers.
2287 // The Rcache and Rindex registers must be set before call.
2288 // Input:
2289 //   - Rcache, Rindex
2290 // Output:
2291 //   - Robj, Roffset, Rflags
2292 void TemplateTable::load_field_cp_cache_entry(Register Robj,
2293                                               Register Rcache,
2294                                               Register Rindex /* unused on PPC64 */,
2295                                               Register Roffset,
2296                                               Register Rflags,
2297                                               bool is_static = false) {
2298   assert_different_registers(Rcache, Rflags, Roffset);
2299   // assert(Rindex == noreg, &quot;parameter not used on PPC64&quot;);
2300 
2301   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2302   __ ld(Rflags, in_bytes(cp_base_offset) + in_bytes(ConstantPoolCacheEntry::flags_offset()), Rcache);
2303   __ ld(Roffset, in_bytes(cp_base_offset) + in_bytes(ConstantPoolCacheEntry::f2_offset()), Rcache);
2304   if (is_static) {
2305     __ ld(Robj, in_bytes(cp_base_offset) + in_bytes(ConstantPoolCacheEntry::f1_offset()), Rcache);
2306     __ ld(Robj, in_bytes(Klass::java_mirror_offset()), Robj);
2307     __ resolve_oop_handle(Robj);
2308     // Acquire not needed here. Following access has an address dependency on this value.
2309   }
2310 }
2311 
2312 // Load the constant pool cache entry at invokes into registers.
2313 // Resolve if necessary.
2314 
2315 // Input Registers:
2316 //   - None, bcp is used, though
2317 //
2318 // Return registers:
2319 //   - Rmethod       (f1 field or f2 if invokevirtual)
2320 //   - Ritable_index (f2 field)
2321 //   - Rflags        (flags field)
2322 //
2323 // Kills:
2324 //   - R21
2325 //
2326 void TemplateTable::load_invoke_cp_cache_entry(int byte_no,
2327                                                Register Rmethod,
2328                                                Register Ritable_index,
2329                                                Register Rflags,
2330                                                bool is_invokevirtual,
2331                                                bool is_invokevfinal,
2332                                                bool is_invokedynamic) {
2333 
2334   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2335   // Determine constant pool cache field offsets.
2336   assert(is_invokevirtual == (byte_no == f2_byte), &quot;is_invokevirtual flag redundant&quot;);
2337   const int method_offset = in_bytes(cp_base_offset + (is_invokevirtual ? ConstantPoolCacheEntry::f2_offset() : ConstantPoolCacheEntry::f1_offset()));
2338   const int flags_offset  = in_bytes(cp_base_offset + ConstantPoolCacheEntry::flags_offset());
2339   // Access constant pool cache fields.
2340   const int index_offset  = in_bytes(cp_base_offset + ConstantPoolCacheEntry::f2_offset());
2341 
2342   Register Rcache = R21_tmp1; // Note: same register as R21_sender_SP.
2343 
2344   if (is_invokevfinal) {
2345     assert(Ritable_index == noreg, &quot;register not used&quot;);
2346     // Already resolved.
2347     __ get_cache_and_index_at_bcp(Rcache, 1);
2348   } else {
2349     resolve_cache_and_index(byte_no, Rcache, /* temp */ Rmethod, is_invokedynamic ? sizeof(u4) : sizeof(u2));
2350   }
2351 
2352   __ ld(Rmethod, method_offset, Rcache);
2353   __ ld(Rflags, flags_offset, Rcache);
2354 
2355   if (Ritable_index != noreg) {
2356     __ ld(Ritable_index, index_offset, Rcache);
2357   }
2358 }
2359 
2360 // ============================================================================
2361 // Field access
2362 
2363 // Volatile variables demand their effects be made known to all CPU&#39;s
2364 // in order. Store buffers on most chips allow reads &amp; writes to
2365 // reorder; the JMM&#39;s ReadAfterWrite.java test fails in -Xint mode
2366 // without some kind of memory barrier (i.e., it&#39;s not sufficient that
2367 // the interpreter does not reorder volatile references, the hardware
2368 // also must not reorder them).
2369 //
2370 // According to the new Java Memory Model (JMM):
2371 // (1) All volatiles are serialized wrt to each other. ALSO reads &amp;
2372 //     writes act as aquire &amp; release, so:
2373 // (2) A read cannot let unrelated NON-volatile memory refs that
2374 //     happen after the read float up to before the read. It&#39;s OK for
2375 //     non-volatile memory refs that happen before the volatile read to
2376 //     float down below it.
2377 // (3) Similar a volatile write cannot let unrelated NON-volatile
2378 //     memory refs that happen BEFORE the write float down to after the
2379 //     write. It&#39;s OK for non-volatile memory refs that happen after the
2380 //     volatile write to float up before it.
2381 //
2382 // We only put in barriers around volatile refs (they are expensive),
2383 // not _between_ memory refs (that would require us to track the
2384 // flavor of the previous memory refs). Requirements (2) and (3)
2385 // require some barriers before volatile stores and after volatile
2386 // loads. These nearly cover requirement (1) but miss the
2387 // volatile-store-volatile-load case.  This final case is placed after
2388 // volatile-stores although it could just as well go before
2389 // volatile-loads.
2390 
2391 // The registers cache and index expected to be set before call.
2392 // Correct values of the cache and index registers are preserved.
2393 // Kills:
2394 //   Rcache (if has_tos)
2395 //   Rscratch
2396 void TemplateTable::jvmti_post_field_access(Register Rcache, Register Rscratch, bool is_static, bool has_tos) {
2397 
2398   assert_different_registers(Rcache, Rscratch);
2399 
2400   if (JvmtiExport::can_post_field_access()) {
2401     ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2402     Label Lno_field_access_post;
2403 
2404     // Check if post field access in enabled.
2405     int offs = __ load_const_optimized(Rscratch, JvmtiExport::get_field_access_count_addr(), R0, true);
2406     __ lwz(Rscratch, offs, Rscratch);
2407 
2408     __ cmpwi(CCR0, Rscratch, 0);
2409     __ beq(CCR0, Lno_field_access_post);
2410 
2411     // Post access enabled - do it!
2412     __ addi(Rcache, Rcache, in_bytes(cp_base_offset));
2413     if (is_static) {
2414       __ li(R17_tos, 0);
2415     } else {
2416       if (has_tos) {
2417         // The fast bytecode versions have obj ptr in register.
2418         // Thus, save object pointer before call_VM() clobbers it
2419         // put object on tos where GC wants it.
2420         __ push_ptr(R17_tos);
2421       } else {
2422         // Load top of stack (do not pop the value off the stack).
2423         __ ld(R17_tos, Interpreter::expr_offset_in_bytes(0), R15_esp);
2424       }
2425       __ verify_oop(R17_tos);
2426     }
2427     // tos:   object pointer or NULL if static
2428     // cache: cache entry pointer
2429     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), R17_tos, Rcache);
2430     if (!is_static &amp;&amp; has_tos) {
2431       // Restore object pointer.
2432       __ pop_ptr(R17_tos);
2433       __ verify_oop(R17_tos);
2434     } else {
2435       // Cache is still needed to get class or obj.
2436       __ get_cache_and_index_at_bcp(Rcache, 1);
2437     }
2438 
2439     __ align(32, 12);
2440     __ bind(Lno_field_access_post);
2441   }
2442 }
2443 
2444 // kills R11_scratch1
2445 void TemplateTable::pop_and_check_object(Register Roop) {
2446   Register Rtmp = R11_scratch1;
2447 
2448   assert_different_registers(Rtmp, Roop);
2449   __ pop_ptr(Roop);
2450   // For field access must check obj.
2451   __ null_check_throw(Roop, -1, Rtmp);
2452   __ verify_oop(Roop);
2453 }
2454 
2455 // PPC64: implement volatile loads as fence-store-acquire.
2456 void TemplateTable::getfield_or_static(int byte_no, bool is_static, RewriteControl rc) {
2457   transition(vtos, vtos);
2458 
2459   Label Lacquire, Lisync;
2460 
2461   const Register Rcache        = R3_ARG1,
2462                  Rclass_or_obj = R22_tmp2,
2463                  Roffset       = R23_tmp3,
2464                  Rflags        = R31,
2465                  Rbtable       = R5_ARG3,
2466                  Rbc           = R6_ARG4,
2467                  Rscratch      = R12_scratch2;
2468 
2469   static address field_branch_table[number_of_states],
2470                  static_branch_table[number_of_states];
2471 
2472   address* branch_table = (is_static || rc == may_not_rewrite) ? static_branch_table : field_branch_table;
2473 
2474   // Get field offset.
2475   resolve_cache_and_index(byte_no, Rcache, Rscratch, sizeof(u2));
2476 
2477   // JVMTI support
2478   jvmti_post_field_access(Rcache, Rscratch, is_static, false);
2479 
2480   // Load after possible GC.
2481   load_field_cp_cache_entry(Rclass_or_obj, Rcache, noreg, Roffset, Rflags, is_static);
2482 
2483   // Load pointer to branch table.
2484   __ load_const_optimized(Rbtable, (address)branch_table, Rscratch);
2485 
2486   // Get volatile flag.
2487   __ rldicl(Rscratch, Rflags, 64-ConstantPoolCacheEntry::is_volatile_shift, 63); // Extract volatile bit.
2488   // Note: sync is needed before volatile load on PPC64.
2489 
2490   // Check field type.
2491   __ rldicl(Rflags, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
2492 
2493 #ifdef ASSERT
2494   Label LFlagInvalid;
2495   __ cmpldi(CCR0, Rflags, number_of_states);
2496   __ bge(CCR0, LFlagInvalid);
2497 #endif
2498 
2499   // Load from branch table and dispatch (volatile case: one instruction ahead).
2500   __ sldi(Rflags, Rflags, LogBytesPerWord);
2501   __ cmpwi(CCR6, Rscratch, 1); // Volatile?
2502   if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2503     __ sldi(Rscratch, Rscratch, exact_log2(BytesPerInstWord)); // Volatile ? size of 1 instruction : 0.
2504   }
2505   __ ldx(Rbtable, Rbtable, Rflags);
2506 
2507   // Get the obj from stack.
2508   if (!is_static) {
2509     pop_and_check_object(Rclass_or_obj); // Kills R11_scratch1.
2510   } else {
2511     __ verify_oop(Rclass_or_obj);
2512   }
2513 
2514   if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2515     __ subf(Rbtable, Rscratch, Rbtable); // Point to volatile/non-volatile entry point.
2516   }
2517   __ mtctr(Rbtable);
2518   __ bctr();
2519 
2520 #ifdef ASSERT
2521   __ bind(LFlagInvalid);
2522   __ stop(&quot;got invalid flag&quot;);
2523 #endif
2524 
2525   if (!is_static &amp;&amp; rc == may_not_rewrite) {
2526     // We reuse the code from is_static.  It&#39;s jumped to via the table above.
2527     return;
2528   }
2529 
2530 #ifdef ASSERT
2531   // __ bind(Lvtos);
2532   address pc_before_fence = __ pc();
2533   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2534   assert(__ pc() - pc_before_fence == (ptrdiff_t)BytesPerInstWord, &quot;must be single instruction&quot;);
2535   assert(branch_table[vtos] == 0, &quot;can&#39;t compute twice&quot;);
2536   branch_table[vtos] = __ pc(); // non-volatile_entry point
2537   __ stop(&quot;vtos unexpected&quot;);
2538 #endif
2539 
2540   __ align(32, 28, 28); // Align load.
2541   // __ bind(Ldtos);
2542   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2543   assert(branch_table[dtos] == 0, &quot;can&#39;t compute twice&quot;);
2544   branch_table[dtos] = __ pc(); // non-volatile_entry point
2545   __ lfdx(F15_ftos, Rclass_or_obj, Roffset);
2546   __ push(dtos);
2547   if (!is_static &amp;&amp; rc == may_rewrite) {
2548     patch_bytecode(Bytecodes::_fast_dgetfield, Rbc, Rscratch);
2549   }
2550   {
2551     Label acquire_double;
2552     __ beq(CCR6, acquire_double); // Volatile?
2553     __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2554 
2555     __ bind(acquire_double);
2556     __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
2557     __ beq_predict_taken(CCR0, Lisync);
2558     __ b(Lisync); // In case of NAN.
2559   }
2560 
2561   __ align(32, 28, 28); // Align load.
2562   // __ bind(Lftos);
2563   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2564   assert(branch_table[ftos] == 0, &quot;can&#39;t compute twice&quot;);
2565   branch_table[ftos] = __ pc(); // non-volatile_entry point
2566   __ lfsx(F15_ftos, Rclass_or_obj, Roffset);
2567   __ push(ftos);
2568   if (!is_static &amp;&amp; rc == may_rewrite) {
2569     patch_bytecode(Bytecodes::_fast_fgetfield, Rbc, Rscratch);
2570   }
2571   {
2572     Label acquire_float;
2573     __ beq(CCR6, acquire_float); // Volatile?
2574     __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2575 
2576     __ bind(acquire_float);
2577     __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
2578     __ beq_predict_taken(CCR0, Lisync);
2579     __ b(Lisync); // In case of NAN.
2580   }
2581 
2582   __ align(32, 28, 28); // Align load.
2583   // __ bind(Litos);
2584   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2585   assert(branch_table[itos] == 0, &quot;can&#39;t compute twice&quot;);
2586   branch_table[itos] = __ pc(); // non-volatile_entry point
2587   __ lwax(R17_tos, Rclass_or_obj, Roffset);
2588   __ push(itos);
2589   if (!is_static &amp;&amp; rc == may_rewrite) {
2590     patch_bytecode(Bytecodes::_fast_igetfield, Rbc, Rscratch);
2591   }
2592   __ beq(CCR6, Lacquire); // Volatile?
2593   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2594 
2595   __ align(32, 28, 28); // Align load.
2596   // __ bind(Lltos);
2597   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2598   assert(branch_table[ltos] == 0, &quot;can&#39;t compute twice&quot;);
2599   branch_table[ltos] = __ pc(); // non-volatile_entry point
2600   __ ldx(R17_tos, Rclass_or_obj, Roffset);
2601   __ push(ltos);
2602   if (!is_static &amp;&amp; rc == may_rewrite) {
2603     patch_bytecode(Bytecodes::_fast_lgetfield, Rbc, Rscratch);
2604   }
2605   __ beq(CCR6, Lacquire); // Volatile?
2606   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2607 
2608   __ align(32, 28, 28); // Align load.
2609   // __ bind(Lbtos);
2610   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2611   assert(branch_table[btos] == 0, &quot;can&#39;t compute twice&quot;);
2612   branch_table[btos] = __ pc(); // non-volatile_entry point
2613   __ lbzx(R17_tos, Rclass_or_obj, Roffset);
2614   __ extsb(R17_tos, R17_tos);
2615   __ push(btos);
2616   if (!is_static &amp;&amp; rc == may_rewrite) {
2617     patch_bytecode(Bytecodes::_fast_bgetfield, Rbc, Rscratch);
2618   }
2619   __ beq(CCR6, Lacquire); // Volatile?
2620   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2621 
2622   __ align(32, 28, 28); // Align load.
2623   // __ bind(Lztos); (same code as btos)
2624   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2625   assert(branch_table[ztos] == 0, &quot;can&#39;t compute twice&quot;);
2626   branch_table[ztos] = __ pc(); // non-volatile_entry point
2627   __ lbzx(R17_tos, Rclass_or_obj, Roffset);
2628   __ push(ztos);
2629   if (!is_static &amp;&amp; rc == may_rewrite) {
2630     // use btos rewriting, no truncating to t/f bit is needed for getfield.
2631     patch_bytecode(Bytecodes::_fast_bgetfield, Rbc, Rscratch);
2632   }
2633   __ beq(CCR6, Lacquire); // Volatile?
2634   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2635 
2636   __ align(32, 28, 28); // Align load.
2637   // __ bind(Lctos);
2638   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2639   assert(branch_table[ctos] == 0, &quot;can&#39;t compute twice&quot;);
2640   branch_table[ctos] = __ pc(); // non-volatile_entry point
2641   __ lhzx(R17_tos, Rclass_or_obj, Roffset);
2642   __ push(ctos);
2643   if (!is_static &amp;&amp; rc == may_rewrite) {
2644     patch_bytecode(Bytecodes::_fast_cgetfield, Rbc, Rscratch);
2645   }
2646   __ beq(CCR6, Lacquire); // Volatile?
2647   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2648 
2649   __ align(32, 28, 28); // Align load.
2650   // __ bind(Lstos);
2651   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2652   assert(branch_table[stos] == 0, &quot;can&#39;t compute twice&quot;);
2653   branch_table[stos] = __ pc(); // non-volatile_entry point
2654   __ lhax(R17_tos, Rclass_or_obj, Roffset);
2655   __ push(stos);
2656   if (!is_static &amp;&amp; rc == may_rewrite) {
2657     patch_bytecode(Bytecodes::_fast_sgetfield, Rbc, Rscratch);
2658   }
2659   __ beq(CCR6, Lacquire); // Volatile?
2660   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2661 
2662   __ align(32, 28, 28); // Align load.
2663   // __ bind(Latos);
2664   __ fence(); // Volatile entry point (one instruction before non-volatile_entry point).
2665   assert(branch_table[atos] == 0, &quot;can&#39;t compute twice&quot;);
2666   branch_table[atos] = __ pc(); // non-volatile_entry point
2667   do_oop_load(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, /* nv temp */ Rflags, IN_HEAP);
2668   __ verify_oop(R17_tos);
2669   __ push(atos);
2670   //__ dcbt(R17_tos); // prefetch
2671   if (!is_static &amp;&amp; rc == may_rewrite) {
2672     patch_bytecode(Bytecodes::_fast_agetfield, Rbc, Rscratch);
2673   }
2674   __ beq(CCR6, Lacquire); // Volatile?
2675   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2676 
2677   __ align(32, 12);
2678   __ bind(Lacquire);
2679   __ twi_0(R17_tos);
2680   __ bind(Lisync);
2681   __ isync(); // acquire
2682 
2683 #ifdef ASSERT
2684   for (int i = 0; i&lt;number_of_states; ++i) {
2685     assert(branch_table[i], &quot;get initialization&quot;);
2686     //tty-&gt;print_cr(&quot;get: %s_branch_table[%d] = 0x%llx (opcode 0x%llx)&quot;,
2687     //              is_static ? &quot;static&quot; : &quot;field&quot;, i, branch_table[i], *((unsigned int*)branch_table[i]));
2688   }
2689 #endif
2690 }
2691 
2692 void TemplateTable::getfield(int byte_no) {
2693   getfield_or_static(byte_no, false);
2694 }
2695 
2696 void TemplateTable::nofast_getfield(int byte_no) {
2697   getfield_or_static(byte_no, false, may_not_rewrite);
2698 }
2699 
2700 void TemplateTable::getstatic(int byte_no) {
2701   getfield_or_static(byte_no, true);
2702 }
2703 
2704 // The registers cache and index expected to be set before call.
2705 // The function may destroy various registers, just not the cache and index registers.
2706 void TemplateTable::jvmti_post_field_mod(Register Rcache, Register Rscratch, bool is_static) {
2707 
2708   assert_different_registers(Rcache, Rscratch, R6_ARG4);
2709 
2710   if (JvmtiExport::can_post_field_modification()) {
2711     Label Lno_field_mod_post;
2712 
2713     // Check if post field access in enabled.
2714     int offs = __ load_const_optimized(Rscratch, JvmtiExport::get_field_modification_count_addr(), R0, true);
2715     __ lwz(Rscratch, offs, Rscratch);
2716 
2717     __ cmpwi(CCR0, Rscratch, 0);
2718     __ beq(CCR0, Lno_field_mod_post);
2719 
2720     // Do the post
2721     ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2722     const Register Robj = Rscratch;
2723 
2724     __ addi(Rcache, Rcache, in_bytes(cp_base_offset));
2725     if (is_static) {
2726       // Life is simple. Null out the object pointer.
2727       __ li(Robj, 0);
2728     } else {
2729       // In case of the fast versions, value lives in registers =&gt; put it back on tos.
2730       int offs = Interpreter::expr_offset_in_bytes(0);
2731       Register base = R15_esp;
2732       switch(bytecode()) {
2733         case Bytecodes::_fast_aputfield: __ push_ptr(); offs+= Interpreter::stackElementSize; break;
2734         case Bytecodes::_fast_iputfield: // Fall through
2735         case Bytecodes::_fast_bputfield: // Fall through
2736         case Bytecodes::_fast_zputfield: // Fall through
2737         case Bytecodes::_fast_cputfield: // Fall through
2738         case Bytecodes::_fast_sputfield: __ push_i(); offs+=  Interpreter::stackElementSize; break;
2739         case Bytecodes::_fast_lputfield: __ push_l(); offs+=2*Interpreter::stackElementSize; break;
2740         case Bytecodes::_fast_fputfield: __ push_f(); offs+=  Interpreter::stackElementSize; break;
2741         case Bytecodes::_fast_dputfield: __ push_d(); offs+=2*Interpreter::stackElementSize; break;
2742         default: {
2743           offs = 0;
2744           base = Robj;
2745           const Register Rflags = Robj;
2746           Label is_one_slot;
2747           // Life is harder. The stack holds the value on top, followed by the
2748           // object. We don&#39;t know the size of the value, though; it could be
2749           // one or two words depending on its type. As a result, we must find
2750           // the type to determine where the object is.
2751           __ ld(Rflags, in_bytes(ConstantPoolCacheEntry::flags_offset()), Rcache); // Big Endian
2752           __ rldicl(Rflags, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
2753 
2754           __ cmpwi(CCR0, Rflags, ltos);
2755           __ cmpwi(CCR1, Rflags, dtos);
2756           __ addi(base, R15_esp, Interpreter::expr_offset_in_bytes(1));
2757           __ crnor(CCR0, Assembler::equal, CCR1, Assembler::equal);
2758           __ beq(CCR0, is_one_slot);
2759           __ addi(base, R15_esp, Interpreter::expr_offset_in_bytes(2));
2760           __ bind(is_one_slot);
2761           break;
2762         }
2763       }
2764       __ ld(Robj, offs, base);
2765       __ verify_oop(Robj);
2766     }
2767 
2768     __ addi(R6_ARG4, R15_esp, Interpreter::expr_offset_in_bytes(0));
2769     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), Robj, Rcache, R6_ARG4);
2770     __ get_cache_and_index_at_bcp(Rcache, 1);
2771 
2772     // In case of the fast versions, value lives in registers =&gt; put it back on tos.
2773     switch(bytecode()) {
2774       case Bytecodes::_fast_aputfield: __ pop_ptr(); break;
2775       case Bytecodes::_fast_iputfield: // Fall through
2776       case Bytecodes::_fast_bputfield: // Fall through
2777       case Bytecodes::_fast_zputfield: // Fall through
2778       case Bytecodes::_fast_cputfield: // Fall through
2779       case Bytecodes::_fast_sputfield: __ pop_i(); break;
2780       case Bytecodes::_fast_lputfield: __ pop_l(); break;
2781       case Bytecodes::_fast_fputfield: __ pop_f(); break;
2782       case Bytecodes::_fast_dputfield: __ pop_d(); break;
2783       default: break; // Nothin&#39; to do.
2784     }
2785 
2786     __ align(32, 12);
2787     __ bind(Lno_field_mod_post);
2788   }
2789 }
2790 
2791 // PPC64: implement volatile stores as release-store (return bytecode contains an additional release).
2792 void TemplateTable::putfield_or_static(int byte_no, bool is_static, RewriteControl rc) {
2793   Label Lvolatile;
2794 
2795   const Register Rcache        = R5_ARG3,  // Do not use ARG1/2 (causes trouble in jvmti_post_field_mod).
2796                  Rclass_or_obj = R31,      // Needs to survive C call.
2797                  Roffset       = R22_tmp2, // Needs to survive C call.
2798                  Rflags        = R3_ARG1,
2799                  Rbtable       = R4_ARG2,
2800                  Rscratch      = R11_scratch1,
2801                  Rscratch2     = R12_scratch2,
2802                  Rscratch3     = R6_ARG4,
2803                  Rbc           = Rscratch3;
2804   const ConditionRegister CR_is_vol = CCR2; // Non-volatile condition register (survives runtime call in do_oop_store).
2805 
2806   static address field_rw_branch_table[number_of_states],
2807                  field_norw_branch_table[number_of_states],
2808                  static_branch_table[number_of_states];
2809 
2810   address* branch_table = is_static ? static_branch_table :
2811     (rc == may_rewrite ? field_rw_branch_table : field_norw_branch_table);
2812 
2813   // Stack (grows up):
2814   //  value
2815   //  obj
2816 
2817   // Load the field offset.
2818   resolve_cache_and_index(byte_no, Rcache, Rscratch, sizeof(u2));
2819   jvmti_post_field_mod(Rcache, Rscratch, is_static);
2820   load_field_cp_cache_entry(Rclass_or_obj, Rcache, noreg, Roffset, Rflags, is_static);
2821 
2822   // Load pointer to branch table.
2823   __ load_const_optimized(Rbtable, (address)branch_table, Rscratch);
2824 
2825   // Get volatile flag.
2826   __ rldicl(Rscratch, Rflags, 64-ConstantPoolCacheEntry::is_volatile_shift, 63); // Extract volatile bit.
2827 
2828   // Check the field type.
2829   __ rldicl(Rflags, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
2830 
2831 #ifdef ASSERT
2832   Label LFlagInvalid;
2833   __ cmpldi(CCR0, Rflags, number_of_states);
2834   __ bge(CCR0, LFlagInvalid);
2835 #endif
2836 
2837   // Load from branch table and dispatch (volatile case: one instruction ahead).
2838   __ sldi(Rflags, Rflags, LogBytesPerWord);
2839   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2840     __ cmpwi(CR_is_vol, Rscratch, 1);  // Volatile?
2841   }
2842   __ sldi(Rscratch, Rscratch, exact_log2(BytesPerInstWord)); // Volatile? size of instruction 1 : 0.
2843   __ ldx(Rbtable, Rbtable, Rflags);
2844 
2845   __ subf(Rbtable, Rscratch, Rbtable); // Point to volatile/non-volatile entry point.
2846   __ mtctr(Rbtable);
2847   __ bctr();
2848 
2849 #ifdef ASSERT
2850   __ bind(LFlagInvalid);
2851   __ stop(&quot;got invalid flag&quot;);
2852 
2853   // __ bind(Lvtos);
2854   address pc_before_release = __ pc();
2855   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2856   assert(__ pc() - pc_before_release == (ptrdiff_t)BytesPerInstWord, &quot;must be single instruction&quot;);
2857   assert(branch_table[vtos] == 0, &quot;can&#39;t compute twice&quot;);
2858   branch_table[vtos] = __ pc(); // non-volatile_entry point
2859   __ stop(&quot;vtos unexpected&quot;);
2860 #endif
2861 
2862   __ align(32, 28, 28); // Align pop.
2863   // __ bind(Ldtos);
2864   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2865   assert(branch_table[dtos] == 0, &quot;can&#39;t compute twice&quot;);
2866   branch_table[dtos] = __ pc(); // non-volatile_entry point
2867   __ pop(dtos);
2868   if (!is_static) {
2869     pop_and_check_object(Rclass_or_obj);  // Kills R11_scratch1.
2870   }
2871   __ stfdx(F15_ftos, Rclass_or_obj, Roffset);
2872   if (!is_static &amp;&amp; rc == may_rewrite) {
2873     patch_bytecode(Bytecodes::_fast_dputfield, Rbc, Rscratch, true, byte_no);
2874   }
2875   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2876     __ beq(CR_is_vol, Lvolatile); // Volatile?
2877   }
2878   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2879 
2880   __ align(32, 28, 28); // Align pop.
2881   // __ bind(Lftos);
2882   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2883   assert(branch_table[ftos] == 0, &quot;can&#39;t compute twice&quot;);
2884   branch_table[ftos] = __ pc(); // non-volatile_entry point
2885   __ pop(ftos);
2886   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1.
2887   __ stfsx(F15_ftos, Rclass_or_obj, Roffset);
2888   if (!is_static &amp;&amp; rc == may_rewrite) {
2889     patch_bytecode(Bytecodes::_fast_fputfield, Rbc, Rscratch, true, byte_no);
2890   }
2891   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2892     __ beq(CR_is_vol, Lvolatile); // Volatile?
2893   }
2894   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2895 
2896   __ align(32, 28, 28); // Align pop.
2897   // __ bind(Litos);
2898   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2899   assert(branch_table[itos] == 0, &quot;can&#39;t compute twice&quot;);
2900   branch_table[itos] = __ pc(); // non-volatile_entry point
2901   __ pop(itos);
2902   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1.
2903   __ stwx(R17_tos, Rclass_or_obj, Roffset);
2904   if (!is_static &amp;&amp; rc == may_rewrite) {
2905     patch_bytecode(Bytecodes::_fast_iputfield, Rbc, Rscratch, true, byte_no);
2906   }
2907   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2908     __ beq(CR_is_vol, Lvolatile); // Volatile?
2909   }
2910   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2911 
2912   __ align(32, 28, 28); // Align pop.
2913   // __ bind(Lltos);
2914   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2915   assert(branch_table[ltos] == 0, &quot;can&#39;t compute twice&quot;);
2916   branch_table[ltos] = __ pc(); // non-volatile_entry point
2917   __ pop(ltos);
2918   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1.
2919   __ stdx(R17_tos, Rclass_or_obj, Roffset);
2920   if (!is_static &amp;&amp; rc == may_rewrite) {
2921     patch_bytecode(Bytecodes::_fast_lputfield, Rbc, Rscratch, true, byte_no);
2922   }
2923   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2924     __ beq(CR_is_vol, Lvolatile); // Volatile?
2925   }
2926   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2927 
2928   __ align(32, 28, 28); // Align pop.
2929   // __ bind(Lbtos);
2930   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2931   assert(branch_table[btos] == 0, &quot;can&#39;t compute twice&quot;);
2932   branch_table[btos] = __ pc(); // non-volatile_entry point
2933   __ pop(btos);
2934   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1.
2935   __ stbx(R17_tos, Rclass_or_obj, Roffset);
2936   if (!is_static &amp;&amp; rc == may_rewrite) {
2937     patch_bytecode(Bytecodes::_fast_bputfield, Rbc, Rscratch, true, byte_no);
2938   }
2939   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2940     __ beq(CR_is_vol, Lvolatile); // Volatile?
2941   }
2942   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2943 
2944   __ align(32, 28, 28); // Align pop.
2945   // __ bind(Lztos);
2946   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2947   assert(branch_table[ztos] == 0, &quot;can&#39;t compute twice&quot;);
2948   branch_table[ztos] = __ pc(); // non-volatile_entry point
2949   __ pop(ztos);
2950   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1.
2951   __ andi(R17_tos, R17_tos, 0x1);
2952   __ stbx(R17_tos, Rclass_or_obj, Roffset);
2953   if (!is_static &amp;&amp; rc == may_rewrite) {
2954     patch_bytecode(Bytecodes::_fast_zputfield, Rbc, Rscratch, true, byte_no);
2955   }
2956   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2957     __ beq(CR_is_vol, Lvolatile); // Volatile?
2958   }
2959   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2960 
2961   __ align(32, 28, 28); // Align pop.
2962   // __ bind(Lctos);
2963   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2964   assert(branch_table[ctos] == 0, &quot;can&#39;t compute twice&quot;);
2965   branch_table[ctos] = __ pc(); // non-volatile_entry point
2966   __ pop(ctos);
2967   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1..
2968   __ sthx(R17_tos, Rclass_or_obj, Roffset);
2969   if (!is_static &amp;&amp; rc == may_rewrite) {
2970     patch_bytecode(Bytecodes::_fast_cputfield, Rbc, Rscratch, true, byte_no);
2971   }
2972   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2973     __ beq(CR_is_vol, Lvolatile); // Volatile?
2974   }
2975   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2976 
2977   __ align(32, 28, 28); // Align pop.
2978   // __ bind(Lstos);
2979   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2980   assert(branch_table[stos] == 0, &quot;can&#39;t compute twice&quot;);
2981   branch_table[stos] = __ pc(); // non-volatile_entry point
2982   __ pop(stos);
2983   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // Kills R11_scratch1.
2984   __ sthx(R17_tos, Rclass_or_obj, Roffset);
2985   if (!is_static &amp;&amp; rc == may_rewrite) {
2986     patch_bytecode(Bytecodes::_fast_sputfield, Rbc, Rscratch, true, byte_no);
2987   }
2988   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2989     __ beq(CR_is_vol, Lvolatile); // Volatile?
2990   }
2991   __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
2992 
2993   __ align(32, 28, 28); // Align pop.
2994   // __ bind(Latos);
2995   __ release(); // Volatile entry point (one instruction before non-volatile_entry point).
2996   assert(branch_table[atos] == 0, &quot;can&#39;t compute twice&quot;);
2997   branch_table[atos] = __ pc(); // non-volatile_entry point
2998   __ pop(atos);
2999   if (!is_static) { pop_and_check_object(Rclass_or_obj); } // kills R11_scratch1
3000   do_oop_store(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, Rscratch2, Rscratch3, IN_HEAP);
3001   if (!is_static &amp;&amp; rc == may_rewrite) {
3002     patch_bytecode(Bytecodes::_fast_aputfield, Rbc, Rscratch, true, byte_no);
3003   }
3004   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
3005     __ beq(CR_is_vol, Lvolatile); // Volatile?
3006     __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
3007 
3008     __ align(32, 12);
3009     __ bind(Lvolatile);
3010     __ fence();
3011   }
3012   // fallthru: __ b(Lexit);
3013 
3014 #ifdef ASSERT
3015   for (int i = 0; i&lt;number_of_states; ++i) {
3016     assert(branch_table[i], &quot;put initialization&quot;);
3017     //tty-&gt;print_cr(&quot;put: %s_branch_table[%d] = 0x%llx (opcode 0x%llx)&quot;,
3018     //              is_static ? &quot;static&quot; : &quot;field&quot;, i, branch_table[i], *((unsigned int*)branch_table[i]));
3019   }
3020 #endif
3021 }
3022 
3023 void TemplateTable::putfield(int byte_no) {
3024   putfield_or_static(byte_no, false);
3025 }
3026 
3027 void TemplateTable::nofast_putfield(int byte_no) {
3028   putfield_or_static(byte_no, false, may_not_rewrite);
3029 }
3030 
3031 void TemplateTable::putstatic(int byte_no) {
3032   putfield_or_static(byte_no, true);
3033 }
3034 
3035 // See SPARC. On PPC64, we have a different jvmti_post_field_mod which does the job.
3036 void TemplateTable::jvmti_post_fast_field_mod() {
3037   __ should_not_reach_here();
3038 }
3039 
3040 void TemplateTable::fast_storefield(TosState state) {
3041   transition(state, vtos);
3042 
3043   const Register Rcache        = R5_ARG3,  // Do not use ARG1/2 (causes trouble in jvmti_post_field_mod).
3044                  Rclass_or_obj = R31,      // Needs to survive C call.
3045                  Roffset       = R22_tmp2, // Needs to survive C call.
3046                  Rflags        = R3_ARG1,
3047                  Rscratch      = R11_scratch1,
3048                  Rscratch2     = R12_scratch2,
3049                  Rscratch3     = R4_ARG2;
3050   const ConditionRegister CR_is_vol = CCR2; // Non-volatile condition register (survives runtime call in do_oop_store).
3051 
3052   // Constant pool already resolved =&gt; Load flags and offset of field.
3053   __ get_cache_and_index_at_bcp(Rcache, 1);
3054   jvmti_post_field_mod(Rcache, Rscratch, false /* not static */);
3055   load_field_cp_cache_entry(noreg, Rcache, noreg, Roffset, Rflags, false);
3056 
3057   // Get the obj and the final store addr.
3058   pop_and_check_object(Rclass_or_obj); // Kills R11_scratch1.
3059 
3060   // Get volatile flag.
3061   __ rldicl_(Rscratch, Rflags, 64-ConstantPoolCacheEntry::is_volatile_shift, 63); // Extract volatile bit.
3062   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) { __ cmpdi(CR_is_vol, Rscratch, 1); }
3063   {
3064     Label LnotVolatile;
3065     __ beq(CCR0, LnotVolatile);
3066     __ release();
3067     __ align(32, 12);
3068     __ bind(LnotVolatile);
3069   }
3070 
3071   // Do the store and fencing.
3072   switch(bytecode()) {
3073     case Bytecodes::_fast_aputfield:
3074       // Store into the field.
3075       do_oop_store(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, Rscratch2, Rscratch3, IN_HEAP);
3076       break;
3077 
3078     case Bytecodes::_fast_iputfield:
3079       __ stwx(R17_tos, Rclass_or_obj, Roffset);
3080       break;
3081 
3082     case Bytecodes::_fast_lputfield:
3083       __ stdx(R17_tos, Rclass_or_obj, Roffset);
3084       break;
3085 
3086     case Bytecodes::_fast_zputfield:
3087       __ andi(R17_tos, R17_tos, 0x1);  // boolean is true if LSB is 1
3088       // fall through to bputfield
3089     case Bytecodes::_fast_bputfield:
3090       __ stbx(R17_tos, Rclass_or_obj, Roffset);
3091       break;
3092 
3093     case Bytecodes::_fast_cputfield:
3094     case Bytecodes::_fast_sputfield:
3095       __ sthx(R17_tos, Rclass_or_obj, Roffset);
3096       break;
3097 
3098     case Bytecodes::_fast_fputfield:
3099       __ stfsx(F15_ftos, Rclass_or_obj, Roffset);
3100       break;
3101 
3102     case Bytecodes::_fast_dputfield:
3103       __ stfdx(F15_ftos, Rclass_or_obj, Roffset);
3104       break;
3105 
3106     default: ShouldNotReachHere();
3107   }
3108 
3109   if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
3110     Label LVolatile;
3111     __ beq(CR_is_vol, LVolatile);
3112     __ dispatch_epilog(vtos, Bytecodes::length_for(bytecode()));
3113 
3114     __ align(32, 12);
3115     __ bind(LVolatile);
3116     __ fence();
3117   }
3118 }
3119 
3120 void TemplateTable::fast_accessfield(TosState state) {
3121   transition(atos, state);
3122 
3123   Label LisVolatile;
3124   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
3125 
3126   const Register Rcache        = R3_ARG1,
3127                  Rclass_or_obj = R17_tos,
3128                  Roffset       = R22_tmp2,
3129                  Rflags        = R23_tmp3,
3130                  Rscratch      = R12_scratch2;
3131 
3132   // Constant pool already resolved. Get the field offset.
3133   __ get_cache_and_index_at_bcp(Rcache, 1);
3134   load_field_cp_cache_entry(noreg, Rcache, noreg, Roffset, Rflags, false);
3135 
3136   // JVMTI support
3137   jvmti_post_field_access(Rcache, Rscratch, false, true);
3138 
3139   // Get the load address.
3140   __ null_check_throw(Rclass_or_obj, -1, Rscratch);
3141 
3142   // Get volatile flag.
3143   __ rldicl_(Rscratch, Rflags, 64-ConstantPoolCacheEntry::is_volatile_shift, 63); // Extract volatile bit.
3144   __ bne(CCR0, LisVolatile);
3145 
3146   switch(bytecode()) {
3147     case Bytecodes::_fast_agetfield:
3148     {
3149       do_oop_load(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, /* nv temp */ Rflags, IN_HEAP);
3150       __ verify_oop(R17_tos);
3151       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3152 
3153       __ bind(LisVolatile);
3154       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3155       do_oop_load(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, /* nv temp */ Rflags, IN_HEAP);
3156       __ verify_oop(R17_tos);
3157       __ twi_0(R17_tos);
3158       __ isync();
3159       break;
3160     }
3161     case Bytecodes::_fast_igetfield:
3162     {
3163       __ lwax(R17_tos, Rclass_or_obj, Roffset);
3164       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3165 
3166       __ bind(LisVolatile);
3167       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3168       __ lwax(R17_tos, Rclass_or_obj, Roffset);
3169       __ twi_0(R17_tos);
3170       __ isync();
3171       break;
3172     }
3173     case Bytecodes::_fast_lgetfield:
3174     {
3175       __ ldx(R17_tos, Rclass_or_obj, Roffset);
3176       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3177 
3178       __ bind(LisVolatile);
3179       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3180       __ ldx(R17_tos, Rclass_or_obj, Roffset);
3181       __ twi_0(R17_tos);
3182       __ isync();
3183       break;
3184     }
3185     case Bytecodes::_fast_bgetfield:
3186     {
3187       __ lbzx(R17_tos, Rclass_or_obj, Roffset);
3188       __ extsb(R17_tos, R17_tos);
3189       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3190 
3191       __ bind(LisVolatile);
3192       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3193       __ lbzx(R17_tos, Rclass_or_obj, Roffset);
3194       __ twi_0(R17_tos);
3195       __ extsb(R17_tos, R17_tos);
3196       __ isync();
3197       break;
3198     }
3199     case Bytecodes::_fast_cgetfield:
3200     {
3201       __ lhzx(R17_tos, Rclass_or_obj, Roffset);
3202       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3203 
3204       __ bind(LisVolatile);
3205       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3206       __ lhzx(R17_tos, Rclass_or_obj, Roffset);
3207       __ twi_0(R17_tos);
3208       __ isync();
3209       break;
3210     }
3211     case Bytecodes::_fast_sgetfield:
3212     {
3213       __ lhax(R17_tos, Rclass_or_obj, Roffset);
3214       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3215 
3216       __ bind(LisVolatile);
3217       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3218       __ lhax(R17_tos, Rclass_or_obj, Roffset);
3219       __ twi_0(R17_tos);
3220       __ isync();
3221       break;
3222     }
3223     case Bytecodes::_fast_fgetfield:
3224     {
3225       __ lfsx(F15_ftos, Rclass_or_obj, Roffset);
3226       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3227 
3228       __ bind(LisVolatile);
3229       Label Ldummy;
3230       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3231       __ lfsx(F15_ftos, Rclass_or_obj, Roffset);
3232       __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
3233       __ bne_predict_not_taken(CCR0, Ldummy);
3234       __ bind(Ldummy);
3235       __ isync();
3236       break;
3237     }
3238     case Bytecodes::_fast_dgetfield:
3239     {
3240       __ lfdx(F15_ftos, Rclass_or_obj, Roffset);
3241       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()));
3242 
3243       __ bind(LisVolatile);
3244       Label Ldummy;
3245       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3246       __ lfdx(F15_ftos, Rclass_or_obj, Roffset);
3247       __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
3248       __ bne_predict_not_taken(CCR0, Ldummy);
3249       __ bind(Ldummy);
3250       __ isync();
3251       break;
3252     }
3253     default: ShouldNotReachHere();
3254   }
3255 }
3256 
3257 void TemplateTable::fast_xaccess(TosState state) {
3258   transition(vtos, state);
3259 
3260   Label LisVolatile;
3261   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
3262   const Register Rcache        = R3_ARG1,
3263                  Rclass_or_obj = R17_tos,
3264                  Roffset       = R22_tmp2,
3265                  Rflags        = R23_tmp3,
3266                  Rscratch      = R12_scratch2;
3267 
3268   __ ld(Rclass_or_obj, 0, R18_locals);
3269 
3270   // Constant pool already resolved. Get the field offset.
3271   __ get_cache_and_index_at_bcp(Rcache, 2);
3272   load_field_cp_cache_entry(noreg, Rcache, noreg, Roffset, Rflags, false);
3273 
3274   // JVMTI support not needed, since we switch back to single bytecode as soon as debugger attaches.
3275 
3276   // Needed to report exception at the correct bcp.
3277   __ addi(R14_bcp, R14_bcp, 1);
3278 
3279   // Get the load address.
3280   __ null_check_throw(Rclass_or_obj, -1, Rscratch);
3281 
3282   // Get volatile flag.
3283   __ rldicl_(Rscratch, Rflags, 64-ConstantPoolCacheEntry::is_volatile_shift, 63); // Extract volatile bit.
3284   __ bne(CCR0, LisVolatile);
3285 
3286   switch(state) {
3287   case atos:
3288     {
3289       do_oop_load(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, /* nv temp */ Rflags, IN_HEAP);
3290       __ verify_oop(R17_tos);
3291       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()) - 1); // Undo bcp increment.
3292 
3293       __ bind(LisVolatile);
3294       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3295       do_oop_load(_masm, Rclass_or_obj, Roffset, R17_tos, Rscratch, /* nv temp */ Rflags, IN_HEAP);
3296       __ verify_oop(R17_tos);
3297       __ twi_0(R17_tos);
3298       __ isync();
3299       break;
3300     }
3301   case itos:
3302     {
3303       __ lwax(R17_tos, Rclass_or_obj, Roffset);
3304       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()) - 1); // Undo bcp increment.
3305 
3306       __ bind(LisVolatile);
3307       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3308       __ lwax(R17_tos, Rclass_or_obj, Roffset);
3309       __ twi_0(R17_tos);
3310       __ isync();
3311       break;
3312     }
3313   case ftos:
3314     {
3315       __ lfsx(F15_ftos, Rclass_or_obj, Roffset);
3316       __ dispatch_epilog(state, Bytecodes::length_for(bytecode()) - 1); // Undo bcp increment.
3317 
3318       __ bind(LisVolatile);
3319       Label Ldummy;
3320       if (support_IRIW_for_not_multiple_copy_atomic_cpu) { __ fence(); }
3321       __ lfsx(F15_ftos, Rclass_or_obj, Roffset);
3322       __ fcmpu(CCR0, F15_ftos, F15_ftos); // Acquire by cmp-br-isync.
3323       __ bne_predict_not_taken(CCR0, Ldummy);
3324       __ bind(Ldummy);
3325       __ isync();
3326       break;
3327     }
3328   default: ShouldNotReachHere();
3329   }
3330   __ addi(R14_bcp, R14_bcp, -1);
3331 }
3332 
3333 // ============================================================================
3334 // Calls
3335 
3336 // Common code for invoke
3337 //
3338 // Input:
3339 //   - byte_no
3340 //
3341 // Output:
3342 //   - Rmethod:        The method to invoke next or i-klass (invokeinterface).
3343 //   - Rret_addr:      The return address to return to.
3344 //   - Rindex:         MethodType (invokehandle), CallSite obj (invokedynamic) or Method (invokeinterface)
3345 //   - Rrecv:          Cache for &quot;this&quot; pointer, might be noreg if static call.
3346 //   - Rflags:         Method flags from const pool cache.
3347 //
3348 //  Kills:
3349 //   - Rscratch1
3350 //
3351 void TemplateTable::prepare_invoke(int byte_no,
3352                                    Register Rmethod,  // linked method (or i-klass)
3353                                    Register Rret_addr,// return address
3354                                    Register Rindex,   // itable index, MethodType, Method, etc.
3355                                    Register Rrecv,    // If caller wants to see it.
3356                                    Register Rflags,   // If caller wants to test it.
3357                                    Register Rscratch
3358                                    ) {
3359   // Determine flags.
3360   const Bytecodes::Code code = bytecode();
3361   const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;
3362   const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;
3363   const bool is_invokehandle     = code == Bytecodes::_invokehandle;
3364   const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;
3365   const bool is_invokespecial    = code == Bytecodes::_invokespecial;
3366   const bool load_receiver       = (Rrecv != noreg);
3367   assert(load_receiver == (code != Bytecodes::_invokestatic &amp;&amp; code != Bytecodes::_invokedynamic), &quot;&quot;);
3368 
3369   assert_different_registers(Rmethod, Rindex, Rflags, Rscratch);
3370   assert_different_registers(Rmethod, Rrecv, Rflags, Rscratch);
3371   assert_different_registers(Rret_addr, Rscratch);
3372 
3373   load_invoke_cp_cache_entry(byte_no, Rmethod, Rindex, Rflags, is_invokevirtual, false, is_invokedynamic);
3374 
3375   // Saving of SP done in call_from_interpreter.
3376 
3377   // Maybe push &quot;appendix&quot; to arguments.
3378   if (is_invokedynamic || is_invokehandle) {
3379     Label Ldone;
3380     __ rldicl_(R0, Rflags, 64-ConstantPoolCacheEntry::has_appendix_shift, 63);
3381     __ beq(CCR0, Ldone);
3382     // Push &quot;appendix&quot; (MethodType, CallSite, etc.).
3383     // This must be done before we get the receiver,
3384     // since the parameter_size includes it.
3385     __ load_resolved_reference_at_index(Rscratch, Rindex);
3386     __ verify_oop(Rscratch);
3387     __ push_ptr(Rscratch);
3388     __ bind(Ldone);
3389   }
3390 
3391   // Load receiver if needed (after appendix is pushed so parameter size is correct).
3392   if (load_receiver) {
3393     const Register Rparam_count = Rscratch;
3394     __ andi(Rparam_count, Rflags, ConstantPoolCacheEntry::parameter_size_mask);
3395     __ load_receiver(Rparam_count, Rrecv);
3396     __ verify_oop(Rrecv);
3397   }
3398 
3399   // Get return address.
3400   {
3401     Register Rtable_addr = Rscratch;
3402     Register Rret_type = Rret_addr;
3403     address table_addr = (address) Interpreter::invoke_return_entry_table_for(code);
3404 
3405     // Get return type. It&#39;s coded into the upper 4 bits of the lower half of the 64 bit value.
3406     __ rldicl(Rret_type, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
3407     __ load_dispatch_table(Rtable_addr, (address*)table_addr);
3408     __ sldi(Rret_type, Rret_type, LogBytesPerWord);
3409     // Get return address.
3410     __ ldx(Rret_addr, Rtable_addr, Rret_type);
3411   }
3412 }
3413 
3414 // Helper for virtual calls. Load target out of vtable and jump off!
3415 // Kills all passed registers.
3416 void TemplateTable::generate_vtable_call(Register Rrecv_klass, Register Rindex, Register Rret, Register Rtemp) {
3417 
3418   assert_different_registers(Rrecv_klass, Rtemp, Rret);
3419   const Register Rtarget_method = Rindex;
3420 
3421   // Get target method &amp; entry point.
3422   const int base = in_bytes(Klass::vtable_start_offset());
3423   // Calc vtable addr scale the vtable index by 8.
3424   __ sldi(Rindex, Rindex, exact_log2(vtableEntry::size_in_bytes()));
3425   // Load target.
3426   __ addi(Rrecv_klass, Rrecv_klass, base + vtableEntry::method_offset_in_bytes());
3427   __ ldx(Rtarget_method, Rindex, Rrecv_klass);
3428   // Argument and return type profiling.
3429   __ profile_arguments_type(Rtarget_method, Rrecv_klass /* scratch1 */, Rtemp /* scratch2 */, true);
3430   __ call_from_interpreter(Rtarget_method, Rret, Rrecv_klass /* scratch1 */, Rtemp /* scratch2 */);
3431 }
3432 
3433 // Virtual or final call. Final calls are rewritten on the fly to run through &quot;fast_finalcall&quot; next time.
3434 void TemplateTable::invokevirtual(int byte_no) {
3435   transition(vtos, vtos);
3436 
3437   Register Rtable_addr = R11_scratch1,
3438            Rret_type = R12_scratch2,
3439            Rret_addr = R5_ARG3,
3440            Rflags = R22_tmp2, // Should survive C call.
3441            Rrecv = R3_ARG1,
3442            Rrecv_klass = Rrecv,
3443            Rvtableindex_or_method = R31, // Should survive C call.
3444            Rnum_params = R4_ARG2,
3445            Rnew_bc = R6_ARG4;
3446 
3447   Label LnotFinal;
3448 
3449   load_invoke_cp_cache_entry(byte_no, Rvtableindex_or_method, noreg, Rflags, /*virtual*/ true, false, false);
3450 
3451   __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_vfinal_shift);
3452   __ bfalse(CCR0, LnotFinal);
3453 
3454   if (RewriteBytecodes &amp;&amp; !UseSharedSpaces &amp;&amp; !DumpSharedSpaces) {
3455     patch_bytecode(Bytecodes::_fast_invokevfinal, Rnew_bc, R12_scratch2);
3456   }
3457   invokevfinal_helper(Rvtableindex_or_method, Rflags, R11_scratch1, R12_scratch2);
3458 
3459   __ align(32, 12);
3460   __ bind(LnotFinal);
3461   // Load &quot;this&quot; pointer (receiver).
3462   __ rldicl(Rnum_params, Rflags, 64, 48);
3463   __ load_receiver(Rnum_params, Rrecv);
3464   __ verify_oop(Rrecv);
3465 
3466   // Get return type. It&#39;s coded into the upper 4 bits of the lower half of the 64 bit value.
3467   __ rldicl(Rret_type, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
3468   __ load_dispatch_table(Rtable_addr, Interpreter::invoke_return_entry_table());
3469   __ sldi(Rret_type, Rret_type, LogBytesPerWord);
3470   __ ldx(Rret_addr, Rret_type, Rtable_addr);
3471   __ null_check_throw(Rrecv, oopDesc::klass_offset_in_bytes(), R11_scratch1);
3472   __ load_klass(Rrecv_klass, Rrecv);
3473   __ verify_klass_ptr(Rrecv_klass);
3474   __ profile_virtual_call(Rrecv_klass, R11_scratch1, R12_scratch2, false);
3475 
3476   generate_vtable_call(Rrecv_klass, Rvtableindex_or_method, Rret_addr, R11_scratch1);
3477 }
3478 
3479 void TemplateTable::fast_invokevfinal(int byte_no) {
3480   transition(vtos, vtos);
3481 
3482   assert(byte_no == f2_byte, &quot;use this argument&quot;);
3483   Register Rflags  = R22_tmp2,
3484            Rmethod = R31;
3485   load_invoke_cp_cache_entry(byte_no, Rmethod, noreg, Rflags, /*virtual*/ true, /*is_invokevfinal*/ true, false);
3486   invokevfinal_helper(Rmethod, Rflags, R11_scratch1, R12_scratch2);
3487 }
3488 
3489 void TemplateTable::invokevfinal_helper(Register Rmethod, Register Rflags, Register Rscratch1, Register Rscratch2) {
3490 
3491   assert_different_registers(Rmethod, Rflags, Rscratch1, Rscratch2);
3492 
3493   // Load receiver from stack slot.
3494   Register Rrecv = Rscratch2;
3495   Register Rnum_params = Rrecv;
3496 
3497   __ ld(Rnum_params, in_bytes(Method::const_offset()), Rmethod);
3498   __ lhz(Rnum_params /* number of params */, in_bytes(ConstMethod::size_of_parameters_offset()), Rnum_params);
3499 
3500   // Get return address.
3501   Register Rtable_addr = Rscratch1,
3502            Rret_addr   = Rflags,
3503            Rret_type   = Rret_addr;
3504   // Get return type. It&#39;s coded into the upper 4 bits of the lower half of the 64 bit value.
3505   __ rldicl(Rret_type, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);
3506   __ load_dispatch_table(Rtable_addr, Interpreter::invoke_return_entry_table());
3507   __ sldi(Rret_type, Rret_type, LogBytesPerWord);
3508   __ ldx(Rret_addr, Rret_type, Rtable_addr);
3509 
3510   // Load receiver and receiver NULL check.
3511   __ load_receiver(Rnum_params, Rrecv);
3512   __ null_check_throw(Rrecv, -1, Rscratch1);
3513 
3514   __ profile_final_call(Rrecv, Rscratch1);
3515   // Argument and return type profiling.
3516   __ profile_arguments_type(Rmethod, Rscratch1, Rscratch2, true);
3517 
3518   // Do the call.
3519   __ call_from_interpreter(Rmethod, Rret_addr, Rscratch1, Rscratch2);
3520 }
3521 
3522 void TemplateTable::invokespecial(int byte_no) {
3523   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3524   transition(vtos, vtos);
3525 
3526   Register Rtable_addr = R3_ARG1,
3527            Rret_addr   = R4_ARG2,
3528            Rflags      = R5_ARG3,
3529            Rreceiver   = R6_ARG4,
3530            Rmethod     = R31;
3531 
3532   prepare_invoke(byte_no, Rmethod, Rret_addr, noreg, Rreceiver, Rflags, R11_scratch1);
3533 
3534   // Receiver NULL check.
3535   __ null_check_throw(Rreceiver, -1, R11_scratch1);
3536 
3537   __ profile_call(R11_scratch1, R12_scratch2);
3538   // Argument and return type profiling.
3539   __ profile_arguments_type(Rmethod, R11_scratch1, R12_scratch2, false);
3540   __ call_from_interpreter(Rmethod, Rret_addr, R11_scratch1, R12_scratch2);
3541 }
3542 
3543 void TemplateTable::invokestatic(int byte_no) {
3544   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3545   transition(vtos, vtos);
3546 
3547   Register Rtable_addr = R3_ARG1,
3548            Rret_addr   = R4_ARG2,
3549            Rflags      = R5_ARG3;
3550 
3551   prepare_invoke(byte_no, R19_method, Rret_addr, noreg, noreg, Rflags, R11_scratch1);
3552 
3553   __ profile_call(R11_scratch1, R12_scratch2);
3554   // Argument and return type profiling.
3555   __ profile_arguments_type(R19_method, R11_scratch1, R12_scratch2, false);
3556   __ call_from_interpreter(R19_method, Rret_addr, R11_scratch1, R12_scratch2);
3557 }
3558 
3559 void TemplateTable::invokeinterface_object_method(Register Rrecv_klass,
3560                                                   Register Rret,
3561                                                   Register Rflags,
3562                                                   Register Rmethod,
3563                                                   Register Rtemp1,
3564                                                   Register Rtemp2) {
3565 
3566   assert_different_registers(Rmethod, Rret, Rrecv_klass, Rflags, Rtemp1, Rtemp2);
3567   Label LnotFinal;
3568 
3569   // Check for vfinal.
3570   __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_vfinal_shift);
3571   __ bfalse(CCR0, LnotFinal);
3572 
3573   Register Rscratch = Rflags; // Rflags is dead now.
3574 
3575   // Final call case.
3576   __ profile_final_call(Rtemp1, Rscratch);
3577   // Argument and return type profiling.
3578   __ profile_arguments_type(Rmethod, Rscratch, Rrecv_klass /* scratch */, true);
3579   // Do the final call - the index (f2) contains the method.
3580   __ call_from_interpreter(Rmethod, Rret, Rscratch, Rrecv_klass /* scratch */);
3581 
3582   // Non-final callc case.
3583   __ bind(LnotFinal);
3584   __ profile_virtual_call(Rrecv_klass, Rtemp1, Rscratch, false);
3585   generate_vtable_call(Rrecv_klass, Rmethod, Rret, Rscratch);
3586 }
3587 
3588 void TemplateTable::invokeinterface(int byte_no) {
3589   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3590   transition(vtos, vtos);
3591 
3592   const Register Rscratch1        = R11_scratch1,
3593                  Rscratch2        = R12_scratch2,
3594                  Rmethod          = R6_ARG4,
3595                  Rmethod2         = R9_ARG7,
3596                  Rinterface_klass = R5_ARG3,
3597                  Rret_addr        = R8_ARG6,
3598                  Rindex           = R10_ARG8,
3599                  Rreceiver        = R3_ARG1,
3600                  Rrecv_klass      = R4_ARG2,
3601                  Rflags           = R7_ARG5;
3602 
3603   prepare_invoke(byte_no, Rinterface_klass, Rret_addr, Rmethod, Rreceiver, Rflags, Rscratch1);
3604 
3605   // First check for Object case, then private interface method,
3606   // then regular interface method.
3607 
3608   // Get receiver klass - this is also a null check
3609   __ null_check_throw(Rreceiver, oopDesc::klass_offset_in_bytes(), Rscratch2);
3610   __ load_klass(Rrecv_klass, Rreceiver);
3611 
3612   // Check corner case object method.
3613   // Special case of invokeinterface called for virtual method of
3614   // java.lang.Object. See ConstantPoolCacheEntry::set_method() for details:
3615   // The invokeinterface was rewritten to a invokevirtual, hence we have
3616   // to handle this corner case.
3617 
3618   Label LnotObjectMethod, Lthrow_ame;
3619   __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_forced_virtual_shift);
3620   __ bfalse(CCR0, LnotObjectMethod);
3621   invokeinterface_object_method(Rrecv_klass, Rret_addr, Rflags, Rmethod, Rscratch1, Rscratch2);
3622   __ bind(LnotObjectMethod);
3623 
3624   // Check for private method invocation - indicated by vfinal
3625   Label LnotVFinal, L_no_such_interface, L_subtype;
3626 
3627   __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_vfinal_shift);
3628   __ bfalse(CCR0, LnotVFinal);
3629 
3630   __ check_klass_subtype(Rrecv_klass, Rinterface_klass, Rscratch1, Rscratch2, L_subtype);
3631   // If we get here the typecheck failed
3632   __ b(L_no_such_interface);
3633   __ bind(L_subtype);
3634 
3635   // do the call
3636 
3637   Register Rscratch = Rflags; // Rflags is dead now.
3638 
3639   __ profile_final_call(Rscratch1, Rscratch);
3640   __ profile_arguments_type(Rmethod, Rscratch, Rrecv_klass /* scratch */, true);
3641 
3642   __ call_from_interpreter(Rmethod, Rret_addr, Rscratch, Rrecv_klass /* scratch */);
3643 
3644   __ bind(LnotVFinal);
3645 
3646   __ lookup_interface_method(Rrecv_klass, Rinterface_klass, noreg, noreg, Rscratch1, Rscratch2,
3647                              L_no_such_interface, /*return_method=*/false);
3648 
3649   __ profile_virtual_call(Rrecv_klass, Rscratch1, Rscratch2, false);
3650 
3651   // Find entry point to call.
3652 
3653   // Get declaring interface class from method
3654   __ load_method_holder(Rinterface_klass, Rmethod);
3655 
3656   // Get itable index from method
3657   __ lwa(Rindex, in_bytes(Method::itable_index_offset()), Rmethod);
3658   __ subfic(Rindex, Rindex, Method::itable_index_max);
3659 
3660   __ lookup_interface_method(Rrecv_klass, Rinterface_klass, Rindex, Rmethod2, Rscratch1, Rscratch2,
3661                              L_no_such_interface);
3662 
3663   __ cmpdi(CCR0, Rmethod2, 0);
3664   __ beq(CCR0, Lthrow_ame);
3665   // Found entry. Jump off!
3666   // Argument and return type profiling.
3667   __ profile_arguments_type(Rmethod2, Rscratch1, Rscratch2, true);
3668   __ call_from_interpreter(Rmethod2, Rret_addr, Rscratch1, Rscratch2);
3669 
3670   // Vtable entry was NULL =&gt; Throw abstract method error.
3671   __ bind(Lthrow_ame);
3672   // Pass arguments for generating a verbose error message.
3673   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodErrorVerbose),
3674           Rrecv_klass, Rmethod);
3675 
3676   // Interface was not found =&gt; Throw incompatible class change error.
3677   __ bind(L_no_such_interface);
3678   // Pass arguments for generating a verbose error message.
3679   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeErrorVerbose),
3680           Rrecv_klass, Rinterface_klass);
3681   DEBUG_ONLY( __ should_not_reach_here(); )
3682 }
3683 
3684 void TemplateTable::invokedynamic(int byte_no) {
3685   transition(vtos, vtos);
3686 
3687   const Register Rret_addr = R3_ARG1,
3688                  Rflags    = R4_ARG2,
3689                  Rmethod   = R22_tmp2,
3690                  Rscratch1 = R11_scratch1,
3691                  Rscratch2 = R12_scratch2;
3692 
3693   prepare_invoke(byte_no, Rmethod, Rret_addr, Rscratch1, noreg, Rflags, Rscratch2);
3694 
3695   // Profile this call.
3696   __ profile_call(Rscratch1, Rscratch2);
3697 
3698   // Off we go. With the new method handles, we don&#39;t jump to a method handle
3699   // entry any more. Instead, we pushed an &quot;appendix&quot; in prepare invoke, which happens
3700   // to be the callsite object the bootstrap method returned. This is passed to a
3701   // &quot;link&quot; method which does the dispatch (Most likely just grabs the MH stored
3702   // inside the callsite and does an invokehandle).
3703   // Argument and return type profiling.
3704   __ profile_arguments_type(Rmethod, Rscratch1, Rscratch2, false);
3705   __ call_from_interpreter(Rmethod, Rret_addr, Rscratch1 /* scratch1 */, Rscratch2 /* scratch2 */);
3706 }
3707 
3708 void TemplateTable::invokehandle(int byte_no) {
3709   transition(vtos, vtos);
3710 
3711   const Register Rret_addr = R3_ARG1,
3712                  Rflags    = R4_ARG2,
3713                  Rrecv     = R5_ARG3,
3714                  Rmethod   = R22_tmp2,
3715                  Rscratch1 = R11_scratch1,
3716                  Rscratch2 = R12_scratch2;
3717 
3718   prepare_invoke(byte_no, Rmethod, Rret_addr, Rscratch1, Rrecv, Rflags, Rscratch2);
3719   __ verify_method_ptr(Rmethod);
3720   __ null_check_throw(Rrecv, -1, Rscratch2);
3721 
3722   __ profile_final_call(Rrecv, Rscratch1);
3723 
3724   // Still no call from handle =&gt; We call the method handle interpreter here.
3725   // Argument and return type profiling.
3726   __ profile_arguments_type(Rmethod, Rscratch1, Rscratch2, true);
3727   __ call_from_interpreter(Rmethod, Rret_addr, Rscratch1 /* scratch1 */, Rscratch2 /* scratch2 */);
3728 }
3729 
3730 // =============================================================================
3731 // Allocation
3732 
3733 // Puts allocated obj ref onto the expression stack.
3734 void TemplateTable::_new() {
3735   transition(vtos, atos);
3736 
3737   Label Lslow_case,
3738         Ldone;
3739 
3740   const Register RallocatedObject = R17_tos,
3741                  RinstanceKlass   = R9_ARG7,
3742                  Rscratch         = R11_scratch1,
3743                  Roffset          = R8_ARG6,
3744                  Rinstance_size   = Roffset,
3745                  Rcpool           = R4_ARG2,
3746                  Rtags            = R3_ARG1,
3747                  Rindex           = R5_ARG3;
3748 
3749   // --------------------------------------------------------------------------
3750   // Check if fast case is possible.
3751 
3752   // Load pointers to const pool and const pool&#39;s tags array.
3753   __ get_cpool_and_tags(Rcpool, Rtags);
3754   // Load index of constant pool entry.
3755   __ get_2_byte_integer_at_bcp(1, Rindex, InterpreterMacroAssembler::Unsigned);
3756 
3757   // Note: compared to other architectures, PPC&#39;s implementation always goes
3758   // to the slow path if TLAB is used and fails.
3759   if (UseTLAB) {
3760     // Make sure the class we&#39;re about to instantiate has been resolved
3761     // This is done before loading instanceKlass to be consistent with the order
3762     // how Constant Pool is updated (see ConstantPoolCache::klass_at_put).
3763     __ addi(Rtags, Rtags, Array&lt;u1&gt;::base_offset_in_bytes());
3764     __ lbzx(Rtags, Rindex, Rtags);
3765 
3766     __ cmpdi(CCR0, Rtags, JVM_CONSTANT_Class);
3767     __ bne(CCR0, Lslow_case);
3768 
3769     // Get instanceKlass
3770     __ sldi(Roffset, Rindex, LogBytesPerWord);
3771     __ load_resolved_klass_at_offset(Rcpool, Roffset, RinstanceKlass);
3772 
3773     // Make sure klass is fully initialized and get instance_size.
3774     __ lbz(Rscratch, in_bytes(InstanceKlass::init_state_offset()), RinstanceKlass);
3775     __ lwz(Rinstance_size, in_bytes(Klass::layout_helper_offset()), RinstanceKlass);
3776 
3777     __ cmpdi(CCR1, Rscratch, InstanceKlass::fully_initialized);
3778     // Make sure klass does not have has_finalizer, or is abstract, or interface or java/lang/Class.
3779     __ andi_(R0, Rinstance_size, Klass::_lh_instance_slow_path_bit); // slow path bit equals 0?
3780 
3781     __ crnand(CCR0, Assembler::equal, CCR1, Assembler::equal); // slow path bit set or not fully initialized?
3782     __ beq(CCR0, Lslow_case);
3783 
3784     // --------------------------------------------------------------------------
3785     // Fast case:
3786     // Allocate the instance.
3787     // 1) Try to allocate in the TLAB.
3788     // 2) If the above fails (or is not applicable), go to a slow case (creates a new TLAB, etc.).
3789 
3790     Register RoldTopValue = RallocatedObject; // Object will be allocated here if it fits.
3791     Register RnewTopValue = R6_ARG4;
3792     Register RendValue    = R7_ARG5;
3793 
3794     // Check if we can allocate in the TLAB.
3795     __ ld(RoldTopValue, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
3796     __ ld(RendValue,    in_bytes(JavaThread::tlab_end_offset()), R16_thread);
3797 
3798     __ add(RnewTopValue, Rinstance_size, RoldTopValue);
3799 
3800     // If there is enough space, we do not CAS and do not clear.
3801     __ cmpld(CCR0, RnewTopValue, RendValue);
3802     __ bgt(CCR0, Lslow_case);
3803 
3804     __ std(RnewTopValue, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
3805 
3806     if (!ZeroTLAB) {
3807       // --------------------------------------------------------------------------
3808       // Init1: Zero out newly allocated memory.
3809       // Initialize remaining object fields.
3810       Register Rbase = Rtags;
3811       __ addi(Rinstance_size, Rinstance_size, 7 - (int)sizeof(oopDesc));
3812       __ addi(Rbase, RallocatedObject, sizeof(oopDesc));
3813       __ srdi(Rinstance_size, Rinstance_size, 3);
3814 
3815       // Clear out object skipping header. Takes also care of the zero length case.
3816       __ clear_memory_doubleword(Rbase, Rinstance_size);
3817     }
3818 
3819     // --------------------------------------------------------------------------
3820     // Init2: Initialize the header: mark, klass
3821     // Init mark.
3822     if (UseBiasedLocking) {
3823       __ ld(Rscratch, in_bytes(Klass::prototype_header_offset()), RinstanceKlass);
3824     } else {
3825       __ load_const_optimized(Rscratch, markWord::prototype().value(), R0);
3826     }
3827     __ std(Rscratch, oopDesc::mark_offset_in_bytes(), RallocatedObject);
3828 
3829     // Init klass.
3830     __ store_klass_gap(RallocatedObject);
3831     __ store_klass(RallocatedObject, RinstanceKlass, Rscratch); // klass (last for cms)
3832 
3833     // Check and trigger dtrace event.
3834     SkipIfEqualZero::skip_to_label_if_equal_zero(_masm, Rscratch, &amp;DTraceAllocProbes, Ldone);
3835     __ push(atos);
3836     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc));
3837     __ pop(atos);
3838 
3839     __ b(Ldone);
3840   }
3841 
3842   // --------------------------------------------------------------------------
3843   // slow case
3844   __ bind(Lslow_case);
3845   call_VM(R17_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), Rcpool, Rindex);
3846 
3847   // continue
3848   __ bind(Ldone);
3849 
3850   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3851   __ membar(Assembler::StoreStore);
3852 }
3853 
3854 void TemplateTable::newarray() {
3855   transition(itos, atos);
3856 
3857   __ lbz(R4, 1, R14_bcp);
3858   __ extsw(R5, R17_tos);
3859   call_VM(R17_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray), R4, R5 /* size */);
3860 
3861   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3862   __ membar(Assembler::StoreStore);
3863 }
3864 
3865 void TemplateTable::anewarray() {
3866   transition(itos, atos);
3867 
3868   __ get_constant_pool(R4);
3869   __ get_2_byte_integer_at_bcp(1, R5, InterpreterMacroAssembler::Unsigned);
3870   __ extsw(R6, R17_tos); // size
3871   call_VM(R17_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::anewarray), R4 /* pool */, R5 /* index */, R6 /* size */);
3872 
3873   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3874   __ membar(Assembler::StoreStore);
3875 }
3876 
3877 // Allocate a multi dimensional array
3878 void TemplateTable::multianewarray() {
3879   transition(vtos, atos);
3880 
3881   Register Rptr = R31; // Needs to survive C call.
3882 
3883   // Put ndims * wordSize into frame temp slot
3884   __ lbz(Rptr, 3, R14_bcp);
3885   __ sldi(Rptr, Rptr, Interpreter::logStackElementSize);
3886   // Esp points past last_dim, so set to R4 to first_dim address.
3887   __ add(R4, Rptr, R15_esp);
3888   call_VM(R17_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), R4 /* first_size_address */);
3889   // Pop all dimensions off the stack.
3890   __ add(R15_esp, Rptr, R15_esp);
3891 
3892   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3893   __ membar(Assembler::StoreStore);
3894 }
3895 
3896 void TemplateTable::arraylength() {
3897   transition(atos, itos);
3898 
3899   __ verify_oop(R17_tos);
3900   __ null_check_throw(R17_tos, arrayOopDesc::length_offset_in_bytes(), R11_scratch1);
3901   __ lwa(R17_tos, arrayOopDesc::length_offset_in_bytes(), R17_tos);
3902 }
3903 
3904 // ============================================================================
3905 // Typechecks
3906 
3907 void TemplateTable::checkcast() {
3908   transition(atos, atos);
3909 
3910   Label Ldone, Lis_null, Lquicked, Lresolved;
3911   Register Roffset         = R6_ARG4,
3912            RobjKlass       = R4_ARG2,
3913            RspecifiedKlass = R5_ARG3, // Generate_ClassCastException_verbose_handler will read value from this register.
3914            Rcpool          = R11_scratch1,
3915            Rtags           = R12_scratch2;
3916 
3917   // Null does not pass.
3918   __ cmpdi(CCR0, R17_tos, 0);
3919   __ beq(CCR0, Lis_null);
3920 
3921   // Get constant pool tag to find out if the bytecode has already been &quot;quickened&quot;.
3922   __ get_cpool_and_tags(Rcpool, Rtags);
3923 
3924   __ get_2_byte_integer_at_bcp(1, Roffset, InterpreterMacroAssembler::Unsigned);
3925 
3926   __ addi(Rtags, Rtags, Array&lt;u1&gt;::base_offset_in_bytes());
3927   __ lbzx(Rtags, Rtags, Roffset);
3928 
3929   __ cmpdi(CCR0, Rtags, JVM_CONSTANT_Class);
3930   __ beq(CCR0, Lquicked);
3931 
3932   // Call into the VM to &quot;quicken&quot; instanceof.
3933   __ push_ptr();  // for GC
3934   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
3935   __ get_vm_result_2(RspecifiedKlass);
3936   __ pop_ptr();   // Restore receiver.
3937   __ b(Lresolved);
3938 
3939   // Extract target class from constant pool.
3940   __ bind(Lquicked);
3941   __ sldi(Roffset, Roffset, LogBytesPerWord);
3942   __ load_resolved_klass_at_offset(Rcpool, Roffset, RspecifiedKlass);
3943 
3944   // Do the checkcast.
3945   __ bind(Lresolved);
3946   // Get value klass in RobjKlass.
3947   __ load_klass(RobjKlass, R17_tos);
3948   // Generate a fast subtype check. Branch to cast_ok if no failure. Return 0 if failure.
3949   __ gen_subtype_check(RobjKlass, RspecifiedKlass, /*3 temp regs*/ Roffset, Rcpool, Rtags, /*target if subtype*/ Ldone);
3950 
3951   // Not a subtype; so must throw exception
3952   // Target class oop is in register R6_ARG4 == RspecifiedKlass by convention.
3953   __ load_dispatch_table(R11_scratch1, (address*)Interpreter::_throw_ClassCastException_entry);
3954   __ mtctr(R11_scratch1);
3955   __ bctr();
3956 
3957   // Profile the null case.
3958   __ align(32, 12);
3959   __ bind(Lis_null);
3960   __ profile_null_seen(R11_scratch1, Rtags); // Rtags used as scratch.
3961 
3962   __ align(32, 12);
3963   __ bind(Ldone);
3964 }
3965 
3966 // Output:
3967 //   - tos == 0: Obj was null or not an instance of class.
3968 //   - tos == 1: Obj was an instance of class.
3969 void TemplateTable::instanceof() {
3970   transition(atos, itos);
3971 
3972   Label Ldone, Lis_null, Lquicked, Lresolved;
3973   Register Roffset         = R6_ARG4,
3974            RobjKlass       = R4_ARG2,
3975            RspecifiedKlass = R5_ARG3,
3976            Rcpool          = R11_scratch1,
3977            Rtags           = R12_scratch2;
3978 
3979   // Null does not pass.
3980   __ cmpdi(CCR0, R17_tos, 0);
3981   __ beq(CCR0, Lis_null);
3982 
3983   // Get constant pool tag to find out if the bytecode has already been &quot;quickened&quot;.
3984   __ get_cpool_and_tags(Rcpool, Rtags);
3985 
3986   __ get_2_byte_integer_at_bcp(1, Roffset, InterpreterMacroAssembler::Unsigned);
3987 
3988   __ addi(Rtags, Rtags, Array&lt;u1&gt;::base_offset_in_bytes());
3989   __ lbzx(Rtags, Rtags, Roffset);
3990 
3991   __ cmpdi(CCR0, Rtags, JVM_CONSTANT_Class);
3992   __ beq(CCR0, Lquicked);
3993 
3994   // Call into the VM to &quot;quicken&quot; instanceof.
3995   __ push_ptr();  // for GC
3996   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
3997   __ get_vm_result_2(RspecifiedKlass);
3998   __ pop_ptr();   // Restore receiver.
3999   __ b(Lresolved);
4000 
4001   // Extract target class from constant pool.
4002   __ bind(Lquicked);
4003   __ sldi(Roffset, Roffset, LogBytesPerWord);
4004   __ load_resolved_klass_at_offset(Rcpool, Roffset, RspecifiedKlass);
4005 
4006   // Do the checkcast.
4007   __ bind(Lresolved);
4008   // Get value klass in RobjKlass.
4009   __ load_klass(RobjKlass, R17_tos);
4010   // Generate a fast subtype check. Branch to cast_ok if no failure. Return 0 if failure.
4011   __ li(R17_tos, 1);
4012   __ gen_subtype_check(RobjKlass, RspecifiedKlass, /*3 temp regs*/ Roffset, Rcpool, Rtags, /*target if subtype*/ Ldone);
4013   __ li(R17_tos, 0);
4014 
4015   if (ProfileInterpreter) {
4016     __ b(Ldone);
4017   }
4018 
4019   // Profile the null case.
4020   __ align(32, 12);
4021   __ bind(Lis_null);
4022   __ profile_null_seen(Rcpool, Rtags); // Rcpool and Rtags used as scratch.
4023 
4024   __ align(32, 12);
4025   __ bind(Ldone);
4026 }
4027 
4028 // =============================================================================
4029 // Breakpoints
4030 
4031 void TemplateTable::_breakpoint() {
4032   transition(vtos, vtos);
4033 
4034   // Get the unpatched byte code.
4035   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::get_original_bytecode_at), R19_method, R14_bcp);
4036   __ mr(R31, R3_RET);
4037 
4038   // Post the breakpoint event.
4039   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::_breakpoint), R19_method, R14_bcp);
4040 
4041   // Complete the execution of original bytecode.
4042   __ dispatch_Lbyte_code(vtos, R31, Interpreter::normal_table(vtos));
4043 }
4044 
4045 // =============================================================================
4046 // Exceptions
4047 
4048 void TemplateTable::athrow() {
4049   transition(atos, vtos);
4050 
4051   // Exception oop is in tos
4052   __ verify_oop(R17_tos);
4053 
4054   __ null_check_throw(R17_tos, -1, R11_scratch1);
4055 
4056   // Throw exception interpreter entry expects exception oop to be in R3.
4057   __ mr(R3_RET, R17_tos);
4058   __ load_dispatch_table(R11_scratch1, (address*)Interpreter::throw_exception_entry());
4059   __ mtctr(R11_scratch1);
4060   __ bctr();
4061 }
4062 
4063 // =============================================================================
4064 // Synchronization
4065 // Searches the basic object lock list on the stack for a free slot
4066 // and uses it to lock the obect in tos.
4067 //
4068 // Recursive locking is enabled by exiting the search if the same
4069 // object is already found in the list. Thus, a new basic lock obj lock
4070 // is allocated &quot;higher up&quot; in the stack and thus is found first
4071 // at next monitor exit.
4072 void TemplateTable::monitorenter() {
4073   transition(atos, vtos);
4074 
4075   __ verify_oop(R17_tos);
4076 
4077   Register Rcurrent_monitor  = R11_scratch1,
4078            Rcurrent_obj      = R12_scratch2,
4079            Robj_to_lock      = R17_tos,
4080            Rscratch1         = R3_ARG1,
4081            Rscratch2         = R4_ARG2,
4082            Rscratch3         = R5_ARG3,
4083            Rcurrent_obj_addr = R6_ARG4;
4084 
4085   // ------------------------------------------------------------------------------
4086   // Null pointer exception.
4087   __ null_check_throw(Robj_to_lock, -1, R11_scratch1);
4088 
4089   // Try to acquire a lock on the object.
4090   // Repeat until succeeded (i.e., until monitorenter returns true).
4091 
4092   // ------------------------------------------------------------------------------
4093   // Find a free slot in the monitor block.
4094   Label Lfound, Lexit, Lallocate_new;
4095   ConditionRegister found_free_slot = CCR0,
4096                     found_same_obj  = CCR1,
4097                     reached_limit   = CCR6;
4098   {
4099     Label Lloop;
4100     Register Rlimit = Rcurrent_monitor;
4101 
4102     // Set up search loop - start with topmost monitor.
4103     __ add(Rcurrent_obj_addr, BasicObjectLock::obj_offset_in_bytes(), R26_monitor);
4104 
4105     __ ld(Rlimit, 0, R1_SP);
4106     __ addi(Rlimit, Rlimit, - (frame::ijava_state_size + frame::interpreter_frame_monitor_size_in_bytes() - BasicObjectLock::obj_offset_in_bytes())); // Monitor base
4107 
4108     // Check if any slot is present =&gt; short cut to allocation if not.
4109     __ cmpld(reached_limit, Rcurrent_obj_addr, Rlimit);
4110     __ bgt(reached_limit, Lallocate_new);
4111 
4112     // Pre-load topmost slot.
4113     __ ld(Rcurrent_obj, 0, Rcurrent_obj_addr);
4114     __ addi(Rcurrent_obj_addr, Rcurrent_obj_addr, frame::interpreter_frame_monitor_size() * wordSize);
4115     // The search loop.
4116     __ bind(Lloop);
4117     // Found free slot?
4118     __ cmpdi(found_free_slot, Rcurrent_obj, 0);
4119     // Is this entry for same obj? If so, stop the search and take the found
4120     // free slot or allocate a new one to enable recursive locking.
4121     __ cmpd(found_same_obj, Rcurrent_obj, Robj_to_lock);
4122     __ cmpld(reached_limit, Rcurrent_obj_addr, Rlimit);
4123     __ beq(found_free_slot, Lexit);
4124     __ beq(found_same_obj, Lallocate_new);
4125     __ bgt(reached_limit, Lallocate_new);
4126     // Check if last allocated BasicLockObj reached.
4127     __ ld(Rcurrent_obj, 0, Rcurrent_obj_addr);
4128     __ addi(Rcurrent_obj_addr, Rcurrent_obj_addr, frame::interpreter_frame_monitor_size() * wordSize);
4129     // Next iteration if unchecked BasicObjectLocks exist on the stack.
4130     __ b(Lloop);
4131   }
4132 
4133   // ------------------------------------------------------------------------------
4134   // Check if we found a free slot.
4135   __ bind(Lexit);
4136 
4137   __ addi(Rcurrent_monitor, Rcurrent_obj_addr, -(frame::interpreter_frame_monitor_size() * wordSize) - BasicObjectLock::obj_offset_in_bytes());
4138   __ addi(Rcurrent_obj_addr, Rcurrent_obj_addr, - frame::interpreter_frame_monitor_size() * wordSize);
4139   __ b(Lfound);
4140 
4141   // We didn&#39;t find a free BasicObjLock =&gt; allocate one.
4142   __ align(32, 12);
4143   __ bind(Lallocate_new);
4144   __ add_monitor_to_stack(false, Rscratch1, Rscratch2);
4145   __ mr(Rcurrent_monitor, R26_monitor);
4146   __ addi(Rcurrent_obj_addr, R26_monitor, BasicObjectLock::obj_offset_in_bytes());
4147 
4148   // ------------------------------------------------------------------------------
4149   // We now have a slot to lock.
4150   __ bind(Lfound);
4151 
4152   // Increment bcp to point to the next bytecode, so exception handling for async. exceptions work correctly.
4153   // The object has already been poped from the stack, so the expression stack looks correct.
4154   __ addi(R14_bcp, R14_bcp, 1);
4155 
4156   __ std(Robj_to_lock, 0, Rcurrent_obj_addr);
4157   __ lock_object(Rcurrent_monitor, Robj_to_lock);
4158 
4159   // Check if there&#39;s enough space on the stack for the monitors after locking.
4160   // This emits a single store.
4161   __ generate_stack_overflow_check(0);
4162 
4163   // The bcp has already been incremented. Just need to dispatch to next instruction.
4164   __ dispatch_next(vtos);
4165 }
4166 
4167 void TemplateTable::monitorexit() {
4168   transition(atos, vtos);
4169   __ verify_oop(R17_tos);
4170 
4171   Register Rcurrent_monitor  = R11_scratch1,
4172            Rcurrent_obj      = R12_scratch2,
4173            Robj_to_lock      = R17_tos,
4174            Rcurrent_obj_addr = R3_ARG1,
4175            Rlimit            = R4_ARG2;
4176   Label Lfound, Lillegal_monitor_state;
4177 
4178   // Check corner case: unbalanced monitorEnter / Exit.
4179   __ ld(Rlimit, 0, R1_SP);
4180   __ addi(Rlimit, Rlimit, - (frame::ijava_state_size + frame::interpreter_frame_monitor_size_in_bytes())); // Monitor base
4181 
4182   // Null pointer check.
4183   __ null_check_throw(Robj_to_lock, -1, R11_scratch1);
4184 
4185   __ cmpld(CCR0, R26_monitor, Rlimit);
4186   __ bgt(CCR0, Lillegal_monitor_state);
4187 
4188   // Find the corresponding slot in the monitors stack section.
4189   {
4190     Label Lloop;
4191 
4192     // Start with topmost monitor.
4193     __ addi(Rcurrent_obj_addr, R26_monitor, BasicObjectLock::obj_offset_in_bytes());
4194     __ addi(Rlimit, Rlimit, BasicObjectLock::obj_offset_in_bytes());
4195     __ ld(Rcurrent_obj, 0, Rcurrent_obj_addr);
4196     __ addi(Rcurrent_obj_addr, Rcurrent_obj_addr, frame::interpreter_frame_monitor_size() * wordSize);
4197 
4198     __ bind(Lloop);
4199     // Is this entry for same obj?
4200     __ cmpd(CCR0, Rcurrent_obj, Robj_to_lock);
4201     __ beq(CCR0, Lfound);
4202 
4203     // Check if last allocated BasicLockObj reached.
4204 
4205     __ ld(Rcurrent_obj, 0, Rcurrent_obj_addr);
4206     __ cmpld(CCR0, Rcurrent_obj_addr, Rlimit);
4207     __ addi(Rcurrent_obj_addr, Rcurrent_obj_addr, frame::interpreter_frame_monitor_size() * wordSize);
4208 
4209     // Next iteration if unchecked BasicObjectLocks exist on the stack.
4210     __ ble(CCR0, Lloop);
4211   }
4212 
4213   // Fell through without finding the basic obj lock =&gt; throw up!
4214   __ bind(Lillegal_monitor_state);
4215   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_illegal_monitor_state_exception));
4216   __ should_not_reach_here();
4217 
4218   __ align(32, 12);
4219   __ bind(Lfound);
4220   __ addi(Rcurrent_monitor, Rcurrent_obj_addr,
4221           -(frame::interpreter_frame_monitor_size() * wordSize) - BasicObjectLock::obj_offset_in_bytes());
4222   __ unlock_object(Rcurrent_monitor);
4223 }
4224 
4225 // ============================================================================
4226 // Wide bytecodes
4227 
4228 // Wide instructions. Simply redirects to the wide entry point for that instruction.
4229 void TemplateTable::wide() {
4230   transition(vtos, vtos);
4231 
4232   const Register Rtable = R11_scratch1,
4233                  Rindex = R12_scratch2,
4234                  Rtmp   = R0;
4235 
4236   __ lbz(Rindex, 1, R14_bcp);
4237 
4238   __ load_dispatch_table(Rtable, Interpreter::_wentry_point);
4239 
4240   __ slwi(Rindex, Rindex, LogBytesPerWord);
4241   __ ldx(Rtmp, Rtable, Rindex);
4242   __ mtctr(Rtmp);
4243   __ bctr();
4244   // Note: the bcp increment step is part of the individual wide bytecode implementations.
4245 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>