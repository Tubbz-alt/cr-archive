<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/interpreter/bytecodeInterpreter.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2002, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 // no precompiled headers
  26 #include &quot;classfile/vmSymbols.hpp&quot;
  27 #include &quot;gc/shared/collectedHeap.hpp&quot;
  28 #include &quot;gc/shared/threadLocalAllocBuffer.inline.hpp&quot;
  29 #include &quot;interpreter/bytecodeHistogram.hpp&quot;
  30 #include &quot;interpreter/bytecodeInterpreter.hpp&quot;
  31 #include &quot;interpreter/bytecodeInterpreter.inline.hpp&quot;
  32 #include &quot;interpreter/bytecodeInterpreterProfiling.hpp&quot;
  33 #include &quot;interpreter/interpreter.hpp&quot;
  34 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  35 #include &quot;logging/log.hpp&quot;
  36 #include &quot;memory/resourceArea.hpp&quot;
  37 #include &quot;memory/universe.hpp&quot;
  38 #include &quot;oops/constantPool.inline.hpp&quot;
  39 #include &quot;oops/cpCache.inline.hpp&quot;
  40 #include &quot;oops/method.inline.hpp&quot;
  41 #include &quot;oops/methodCounters.hpp&quot;
  42 #include &quot;oops/objArrayKlass.hpp&quot;
  43 #include &quot;oops/objArrayOop.inline.hpp&quot;
  44 #include &quot;oops/oop.inline.hpp&quot;
  45 #include &quot;oops/typeArrayOop.inline.hpp&quot;
  46 #include &quot;prims/jvmtiExport.hpp&quot;
  47 #include &quot;prims/jvmtiThreadState.hpp&quot;
  48 #include &quot;runtime/atomic.hpp&quot;
  49 #include &quot;runtime/biasedLocking.hpp&quot;
  50 #include &quot;runtime/frame.inline.hpp&quot;
  51 #include &quot;runtime/handles.inline.hpp&quot;
  52 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  53 #include &quot;runtime/orderAccess.hpp&quot;
  54 #include &quot;runtime/sharedRuntime.hpp&quot;
  55 #include &quot;runtime/threadCritical.hpp&quot;
  56 #include &quot;utilities/exceptions.hpp&quot;
  57 
  58 // no precompiled headers
  59 #ifdef CC_INTERP
  60 
  61 /*
  62  * USELABELS - If using GCC, then use labels for the opcode dispatching
  63  * rather -then a switch statement. This improves performance because it
  64  * gives us the opportunity to have the instructions that calculate the
  65  * next opcode to jump to be intermixed with the rest of the instructions
  66  * that implement the opcode (see UPDATE_PC_AND_TOS_AND_CONTINUE macro).
  67  */
  68 #undef USELABELS
  69 #ifdef __GNUC__
  70 /*
  71    ASSERT signifies debugging. It is much easier to step thru bytecodes if we
  72    don&#39;t use the computed goto approach.
  73 */
  74 #ifndef ASSERT
  75 #define USELABELS
  76 #endif
  77 #endif
  78 
  79 #undef CASE
  80 #ifdef USELABELS
  81 #define CASE(opcode) opc ## opcode
  82 #define DEFAULT opc_default
  83 #else
  84 #define CASE(opcode) case Bytecodes:: opcode
  85 #define DEFAULT default
  86 #endif
  87 
  88 /*
  89  * PREFETCH_OPCCODE - Some compilers do better if you prefetch the next
  90  * opcode before going back to the top of the while loop, rather then having
  91  * the top of the while loop handle it. This provides a better opportunity
  92  * for instruction scheduling. Some compilers just do this prefetch
  93  * automatically. Some actually end up with worse performance if you
  94  * force the prefetch. Solaris gcc seems to do better, but cc does worse.
  95  */
  96 #undef PREFETCH_OPCCODE
  97 #define PREFETCH_OPCCODE
  98 
  99 /*
 100   Interpreter safepoint: it is expected that the interpreter will have no live
 101   handles of its own creation live at an interpreter safepoint. Therefore we
 102   run a HandleMarkCleaner and trash all handles allocated in the call chain
 103   since the JavaCalls::call_helper invocation that initiated the chain.
 104   There really shouldn&#39;t be any handles remaining to trash but this is cheap
 105   in relation to a safepoint.
 106 */
 107 #define SAFEPOINT                                                                 \
 108     {                                                                             \
 109        /* zap freed handles rather than GC&#39;ing them */                            \
 110        HandleMarkCleaner __hmc(THREAD);                                           \
 111        CALL_VM(SafepointMechanism::block_if_requested(THREAD), handle_exception); \
 112     }
 113 
 114 /*
 115  * VM_JAVA_ERROR - Macro for throwing a java exception from
 116  * the interpreter loop. Should really be a CALL_VM but there
 117  * is no entry point to do the transition to vm so we just
 118  * do it by hand here.
 119  */
 120 #define VM_JAVA_ERROR_NO_JUMP(name, msg, note_a_trap)                             \
 121     DECACHE_STATE();                                                              \
 122     SET_LAST_JAVA_FRAME();                                                        \
 123     {                                                                             \
 124        InterpreterRuntime::note_a_trap(THREAD, istate-&gt;method(), BCI());          \
 125        ThreadInVMfromJava trans(THREAD);                                          \
 126        Exceptions::_throw_msg(THREAD, __FILE__, __LINE__, name, msg);             \
 127     }                                                                             \
 128     RESET_LAST_JAVA_FRAME();                                                      \
 129     CACHE_STATE();
 130 
 131 // Normal throw of a java error.
 132 #define VM_JAVA_ERROR(name, msg, note_a_trap)                                     \
 133     VM_JAVA_ERROR_NO_JUMP(name, msg, note_a_trap)                                 \
 134     goto handle_exception;
 135 
 136 #ifdef PRODUCT
 137 #define DO_UPDATE_INSTRUCTION_COUNT(opcode)
 138 #else
 139 #define DO_UPDATE_INSTRUCTION_COUNT(opcode)                                                          \
 140 {                                                                                                    \
 141     BytecodeCounter::_counter_value++;                                                               \
 142     BytecodeHistogram::_counters[(Bytecodes::Code)opcode]++;                                         \
 143     if (StopInterpreterAt &amp;&amp; StopInterpreterAt == BytecodeCounter::_counter_value) os::breakpoint(); \
 144     if (TraceBytecodes) {                                                                            \
 145       CALL_VM((void)InterpreterRuntime::trace_bytecode(THREAD, 0,                    \
 146                                         topOfStack[Interpreter::expr_index_at(1)],   \
 147                                         topOfStack[Interpreter::expr_index_at(2)]),  \
 148                                         handle_exception);                           \
 149     }                                                                                \
 150 }
 151 #endif
 152 
 153 #undef DEBUGGER_SINGLE_STEP_NOTIFY
 154 #ifdef VM_JVMTI
 155 /* NOTE: (kbr) This macro must be called AFTER the PC has been
 156    incremented. JvmtiExport::at_single_stepping_point() may cause a
 157    breakpoint opcode to get inserted at the current PC to allow the
 158    debugger to coalesce single-step events.
 159 
 160    As a result if we call at_single_stepping_point() we refetch opcode
 161    to get the current opcode. This will override any other prefetching
 162    that might have occurred.
 163 */
 164 #define DEBUGGER_SINGLE_STEP_NOTIFY()                                            \
 165 {                                                                                \
 166       if (_jvmti_interp_events) {                                                \
 167         if (JvmtiExport::should_post_single_step()) {                            \
 168           DECACHE_STATE();                                                       \
 169           SET_LAST_JAVA_FRAME();                                                 \
 170           ThreadInVMfromJava trans(THREAD);                                      \
 171           JvmtiExport::at_single_stepping_point(THREAD,                          \
 172                                           istate-&gt;method(),                      \
 173                                           pc);                                   \
 174           RESET_LAST_JAVA_FRAME();                                               \
 175           CACHE_STATE();                                                         \
 176           if (THREAD-&gt;pop_frame_pending() &amp;&amp;                                     \
 177               !THREAD-&gt;pop_frame_in_process()) {                                 \
 178             goto handle_Pop_Frame;                                               \
 179           }                                                                      \
 180           if (THREAD-&gt;jvmti_thread_state() &amp;&amp;                                    \
 181               THREAD-&gt;jvmti_thread_state()-&gt;is_earlyret_pending()) {             \
 182             goto handle_Early_Return;                                            \
 183           }                                                                      \
 184           opcode = *pc;                                                          \
 185         }                                                                        \
 186       }                                                                          \
 187 }
 188 #else
 189 #define DEBUGGER_SINGLE_STEP_NOTIFY()
 190 #endif
 191 
 192 /*
 193  * CONTINUE - Macro for executing the next opcode.
 194  */
 195 #undef CONTINUE
 196 #ifdef USELABELS
 197 // Have to do this dispatch this way in C++ because otherwise gcc complains about crossing an
 198 // initialization (which is is the initialization of the table pointer...)
 199 #define DISPATCH(opcode) goto *(void*)dispatch_table[opcode]
 200 #define CONTINUE {                              \
 201         opcode = *pc;                           \
 202         DO_UPDATE_INSTRUCTION_COUNT(opcode);    \
 203         DEBUGGER_SINGLE_STEP_NOTIFY();          \
 204         DISPATCH(opcode);                       \
 205     }
 206 #else
 207 #ifdef PREFETCH_OPCCODE
 208 #define CONTINUE {                              \
 209         opcode = *pc;                           \
 210         DO_UPDATE_INSTRUCTION_COUNT(opcode);    \
 211         DEBUGGER_SINGLE_STEP_NOTIFY();          \
 212         continue;                               \
 213     }
 214 #else
 215 #define CONTINUE {                              \
 216         DO_UPDATE_INSTRUCTION_COUNT(opcode);    \
 217         DEBUGGER_SINGLE_STEP_NOTIFY();          \
 218         continue;                               \
 219     }
 220 #endif
 221 #endif
 222 
 223 
 224 #define UPDATE_PC(opsize) {pc += opsize; }
 225 /*
 226  * UPDATE_PC_AND_TOS - Macro for updating the pc and topOfStack.
 227  */
 228 #undef UPDATE_PC_AND_TOS
 229 #define UPDATE_PC_AND_TOS(opsize, stack) \
 230     {pc += opsize; MORE_STACK(stack); }
 231 
 232 /*
 233  * UPDATE_PC_AND_TOS_AND_CONTINUE - Macro for updating the pc and topOfStack,
 234  * and executing the next opcode. It&#39;s somewhat similar to the combination
 235  * of UPDATE_PC_AND_TOS and CONTINUE, but with some minor optimizations.
 236  */
 237 #undef UPDATE_PC_AND_TOS_AND_CONTINUE
 238 #ifdef USELABELS
 239 #define UPDATE_PC_AND_TOS_AND_CONTINUE(opsize, stack) {         \
 240         pc += opsize; opcode = *pc; MORE_STACK(stack);          \
 241         DO_UPDATE_INSTRUCTION_COUNT(opcode);                    \
 242         DEBUGGER_SINGLE_STEP_NOTIFY();                          \
 243         DISPATCH(opcode);                                       \
 244     }
 245 
 246 #define UPDATE_PC_AND_CONTINUE(opsize) {                        \
 247         pc += opsize; opcode = *pc;                             \
 248         DO_UPDATE_INSTRUCTION_COUNT(opcode);                    \
 249         DEBUGGER_SINGLE_STEP_NOTIFY();                          \
 250         DISPATCH(opcode);                                       \
 251     }
 252 #else
 253 #ifdef PREFETCH_OPCCODE
 254 #define UPDATE_PC_AND_TOS_AND_CONTINUE(opsize, stack) {         \
 255         pc += opsize; opcode = *pc; MORE_STACK(stack);          \
 256         DO_UPDATE_INSTRUCTION_COUNT(opcode);                    \
 257         DEBUGGER_SINGLE_STEP_NOTIFY();                          \
 258         goto do_continue;                                       \
 259     }
 260 
 261 #define UPDATE_PC_AND_CONTINUE(opsize) {                        \
 262         pc += opsize; opcode = *pc;                             \
 263         DO_UPDATE_INSTRUCTION_COUNT(opcode);                    \
 264         DEBUGGER_SINGLE_STEP_NOTIFY();                          \
 265         goto do_continue;                                       \
 266     }
 267 #else
 268 #define UPDATE_PC_AND_TOS_AND_CONTINUE(opsize, stack) { \
 269         pc += opsize; MORE_STACK(stack);                \
 270         DO_UPDATE_INSTRUCTION_COUNT(opcode);            \
 271         DEBUGGER_SINGLE_STEP_NOTIFY();                  \
 272         goto do_continue;                               \
 273     }
 274 
 275 #define UPDATE_PC_AND_CONTINUE(opsize) {                \
 276         pc += opsize;                                   \
 277         DO_UPDATE_INSTRUCTION_COUNT(opcode);            \
 278         DEBUGGER_SINGLE_STEP_NOTIFY();                  \
 279         goto do_continue;                               \
 280     }
 281 #endif /* PREFETCH_OPCCODE */
 282 #endif /* USELABELS */
 283 
 284 // About to call a new method, update the save the adjusted pc and return to frame manager
 285 #define UPDATE_PC_AND_RETURN(opsize)  \
 286    DECACHE_TOS();                     \
 287    istate-&gt;set_bcp(pc+opsize);        \
 288    return;
 289 
 290 
 291 #define METHOD istate-&gt;method()
 292 #define GET_METHOD_COUNTERS(res)    \
 293   res = METHOD-&gt;method_counters();  \
 294   if (res == NULL) {                \
 295     CALL_VM(res = InterpreterRuntime::build_method_counters(THREAD, METHOD), handle_exception); \
 296   }
 297 
 298 #define OSR_REQUEST(res, branch_pc) \
 299             CALL_VM(res=InterpreterRuntime::frequency_counter_overflow(THREAD, branch_pc), handle_exception);
 300 /*
 301  * For those opcodes that need to have a GC point on a backwards branch
 302  */
 303 
 304 // Backedge counting is kind of strange. The asm interpreter will increment
 305 // the backedge counter as a separate counter but it does it&#39;s comparisons
 306 // to the sum (scaled) of invocation counter and backedge count to make
 307 // a decision. Seems kind of odd to sum them together like that
 308 
 309 // skip is delta from current bcp/bci for target, branch_pc is pre-branch bcp
 310 
 311 
 312 #define DO_BACKEDGE_CHECKS(skip, branch_pc)                                                         \
 313     if ((skip) &lt;= 0) {                                                                              \
 314       MethodCounters* mcs;                                                                          \
 315       GET_METHOD_COUNTERS(mcs);                                                                     \
 316       if (UseLoopCounter) {                                                                         \
 317         bool do_OSR = UseOnStackReplacement;                                                        \
 318         mcs-&gt;backedge_counter()-&gt;increment();                                                       \
 319         if (ProfileInterpreter) {                                                                   \
 320           BI_PROFILE_GET_OR_CREATE_METHOD_DATA(handle_exception);                                   \
 321           /* Check for overflow against MDO count. */                                               \
 322           do_OSR = do_OSR                                                                           \
 323             &amp;&amp; (mdo_last_branch_taken_count &gt;= (uint)InvocationCounter::InterpreterBackwardBranchLimit)\
 324             /* When ProfileInterpreter is on, the backedge_count comes     */                       \
 325             /* from the methodDataOop, which value does not get reset on   */                       \
 326             /* the call to frequency_counter_overflow(). To avoid          */                       \
 327             /* excessive calls to the overflow routine while the method is */                       \
 328             /* being compiled, add a second test to make sure the overflow */                       \
 329             /* function is called only once every overflow_frequency.      */                       \
 330             &amp;&amp; (!(mdo_last_branch_taken_count &amp; 1023));                                             \
 331         } else {                                                                                    \
 332           /* check for overflow of backedge counter */                                              \
 333           do_OSR = do_OSR                                                                           \
 334             &amp;&amp; mcs-&gt;invocation_counter()-&gt;reached_InvocationLimit(mcs-&gt;backedge_counter());         \
 335         }                                                                                           \
 336         if (do_OSR) {                                                                               \
 337           nmethod* osr_nmethod;                                                                     \
 338           OSR_REQUEST(osr_nmethod, branch_pc);                                                      \
 339           if (osr_nmethod != NULL &amp;&amp; osr_nmethod-&gt;is_in_use()) {                                    \
 340             intptr_t* buf;                                                                          \
 341             /* Call OSR migration with last java frame only, no checks. */                          \
 342             CALL_VM_NAKED_LJF(buf=SharedRuntime::OSR_migration_begin(THREAD));                      \
 343             istate-&gt;set_msg(do_osr);                                                                \
 344             istate-&gt;set_osr_buf((address)buf);                                                      \
 345             istate-&gt;set_osr_entry(osr_nmethod-&gt;osr_entry());                                        \
 346             return;                                                                                 \
 347           }                                                                                         \
 348         }                                                                                           \
 349       }  /* UseCompiler ... */                                                                      \
 350       SAFEPOINT;                                                                                    \
 351     }
 352 
 353 /*
 354  * For those opcodes that need to have a GC point on a backwards branch
 355  */
 356 
 357 /*
 358  * Macros for caching and flushing the interpreter state. Some local
 359  * variables need to be flushed out to the frame before we do certain
 360  * things (like pushing frames or becomming gc safe) and some need to
 361  * be recached later (like after popping a frame). We could use one
 362  * macro to cache or decache everything, but this would be less then
 363  * optimal because we don&#39;t always need to cache or decache everything
 364  * because some things we know are already cached or decached.
 365  */
 366 #undef DECACHE_TOS
 367 #undef CACHE_TOS
 368 #undef CACHE_PREV_TOS
 369 #define DECACHE_TOS()    istate-&gt;set_stack(topOfStack);
 370 
 371 #define CACHE_TOS()      topOfStack = (intptr_t *)istate-&gt;stack();
 372 
 373 #undef DECACHE_PC
 374 #undef CACHE_PC
 375 #define DECACHE_PC()    istate-&gt;set_bcp(pc);
 376 #define CACHE_PC()      pc = istate-&gt;bcp();
 377 #define CACHE_CP()      cp = istate-&gt;constants();
 378 #define CACHE_LOCALS()  locals = istate-&gt;locals();
 379 #undef CACHE_FRAME
 380 #define CACHE_FRAME()
 381 
 382 // BCI() returns the current bytecode-index.
 383 #undef  BCI
 384 #define BCI()           ((int)(intptr_t)(pc - (intptr_t)istate-&gt;method()-&gt;code_base()))
 385 
 386 /*
 387  * CHECK_NULL - Macro for throwing a NullPointerException if the object
 388  * passed is a null ref.
 389  * On some architectures/platforms it should be possible to do this implicitly
 390  */
 391 #undef CHECK_NULL
 392 #define CHECK_NULL(obj_)                                                                         \
 393         if ((obj_) == NULL) {                                                                    \
 394           VM_JAVA_ERROR(vmSymbols::java_lang_NullPointerException(), NULL, note_nullCheck_trap); \
 395         }                                                                                        \
 396         VERIFY_OOP(obj_)
 397 
 398 #define VMdoubleConstZero() 0.0
 399 #define VMdoubleConstOne() 1.0
 400 #define VMlongConstZero() (max_jlong-max_jlong)
 401 #define VMlongConstOne() ((max_jlong-max_jlong)+1)
 402 
 403 /*
 404  * Alignment
 405  */
 406 #define VMalignWordUp(val)          (((uintptr_t)(val) + 3) &amp; ~3)
 407 
 408 // Decache the interpreter state that interpreter modifies directly (i.e. GC is indirect mod)
 409 #define DECACHE_STATE() DECACHE_PC(); DECACHE_TOS();
 410 
 411 // Reload interpreter state after calling the VM or a possible GC
 412 #define CACHE_STATE()   \
 413         CACHE_TOS();    \
 414         CACHE_PC();     \
 415         CACHE_CP();     \
 416         CACHE_LOCALS();
 417 
 418 // Call the VM with last java frame only.
 419 #define CALL_VM_NAKED_LJF(func)                                    \
 420         DECACHE_STATE();                                           \
 421         SET_LAST_JAVA_FRAME();                                     \
 422         func;                                                      \
 423         RESET_LAST_JAVA_FRAME();                                   \
 424         CACHE_STATE();
 425 
 426 // Call the VM. Don&#39;t check for pending exceptions.
 427 #define CALL_VM_NOCHECK(func)                                      \
 428         CALL_VM_NAKED_LJF(func)                                    \
 429         if (THREAD-&gt;pop_frame_pending() &amp;&amp;                         \
 430             !THREAD-&gt;pop_frame_in_process()) {                     \
 431           goto handle_Pop_Frame;                                   \
 432         }                                                          \
 433         if (THREAD-&gt;jvmti_thread_state() &amp;&amp;                        \
 434             THREAD-&gt;jvmti_thread_state()-&gt;is_earlyret_pending()) { \
 435           goto handle_Early_Return;                                \
 436         }
 437 
 438 // Call the VM and check for pending exceptions
 439 #define CALL_VM(func, label) {                                     \
 440           CALL_VM_NOCHECK(func);                                   \
 441           if (THREAD-&gt;has_pending_exception()) goto label;         \
 442         }
 443 
 444 /*
 445  * BytecodeInterpreter::run(interpreterState istate)
 446  * BytecodeInterpreter::runWithChecks(interpreterState istate)
 447  *
 448  * The real deal. This is where byte codes actually get interpreted.
 449  * Basically it&#39;s a big while loop that iterates until we return from
 450  * the method passed in.
 451  *
 452  * The runWithChecks is used if JVMTI is enabled.
 453  *
 454  */
 455 #if defined(VM_JVMTI)
 456 void
 457 BytecodeInterpreter::runWithChecks(interpreterState istate) {
 458 #else
 459 void
 460 BytecodeInterpreter::run(interpreterState istate) {
 461 #endif
 462 
 463   // In order to simplify some tests based on switches set at runtime
 464   // we invoke the interpreter a single time after switches are enabled
 465   // and set simpler to to test variables rather than method calls or complex
 466   // boolean expressions.
 467 
 468   static int initialized = 0;
 469   static int checkit = 0;
 470   static intptr_t* c_addr = NULL;
 471   static intptr_t  c_value;
 472 
 473   if (checkit &amp;&amp; *c_addr != c_value) {
 474     os::breakpoint();
 475   }
 476 #ifdef VM_JVMTI
 477   static bool _jvmti_interp_events = 0;
 478 #endif
 479 
 480   static int _compiling;  // (UseCompiler || CountCompiledCalls)
 481 
 482 #ifdef ASSERT
 483   if (istate-&gt;_msg != initialize) {
 484     assert(labs(istate-&gt;_stack_base - istate-&gt;_stack_limit) == (istate-&gt;_method-&gt;max_stack() + 1), &quot;bad stack limit&quot;);
 485     IA32_ONLY(assert(istate-&gt;_stack_limit == istate-&gt;_thread-&gt;last_Java_sp() + 1, &quot;wrong&quot;));
 486   }
 487   // Verify linkages.
 488   interpreterState l = istate;
 489   do {
 490     assert(l == l-&gt;_self_link, &quot;bad link&quot;);
 491     l = l-&gt;_prev_link;
 492   } while (l != NULL);
 493   // Screwups with stack management usually cause us to overwrite istate
 494   // save a copy so we can verify it.
 495   interpreterState orig = istate;
 496 #endif
 497 
 498   intptr_t*        topOfStack = (intptr_t *)istate-&gt;stack(); /* access with STACK macros */
 499   address          pc = istate-&gt;bcp();
 500   jubyte opcode;
 501   intptr_t*        locals = istate-&gt;locals();
 502   ConstantPoolCache*    cp = istate-&gt;constants(); // method()-&gt;constants()-&gt;cache()
 503 #ifdef LOTS_OF_REGS
 504   JavaThread*      THREAD = istate-&gt;thread();
 505 #else
 506 #undef THREAD
 507 #define THREAD istate-&gt;thread()
 508 #endif
 509 
 510 #ifdef USELABELS
 511   const static void* const opclabels_data[256] = {
 512 /* 0x00 */ &amp;&amp;opc_nop,     &amp;&amp;opc_aconst_null,&amp;&amp;opc_iconst_m1,&amp;&amp;opc_iconst_0,
 513 /* 0x04 */ &amp;&amp;opc_iconst_1,&amp;&amp;opc_iconst_2,   &amp;&amp;opc_iconst_3, &amp;&amp;opc_iconst_4,
 514 /* 0x08 */ &amp;&amp;opc_iconst_5,&amp;&amp;opc_lconst_0,   &amp;&amp;opc_lconst_1, &amp;&amp;opc_fconst_0,
 515 /* 0x0C */ &amp;&amp;opc_fconst_1,&amp;&amp;opc_fconst_2,   &amp;&amp;opc_dconst_0, &amp;&amp;opc_dconst_1,
 516 
 517 /* 0x10 */ &amp;&amp;opc_bipush, &amp;&amp;opc_sipush, &amp;&amp;opc_ldc,    &amp;&amp;opc_ldc_w,
 518 /* 0x14 */ &amp;&amp;opc_ldc2_w, &amp;&amp;opc_iload,  &amp;&amp;opc_lload,  &amp;&amp;opc_fload,
 519 /* 0x18 */ &amp;&amp;opc_dload,  &amp;&amp;opc_aload,  &amp;&amp;opc_iload_0,&amp;&amp;opc_iload_1,
 520 /* 0x1C */ &amp;&amp;opc_iload_2,&amp;&amp;opc_iload_3,&amp;&amp;opc_lload_0,&amp;&amp;opc_lload_1,
 521 
 522 /* 0x20 */ &amp;&amp;opc_lload_2,&amp;&amp;opc_lload_3,&amp;&amp;opc_fload_0,&amp;&amp;opc_fload_1,
 523 /* 0x24 */ &amp;&amp;opc_fload_2,&amp;&amp;opc_fload_3,&amp;&amp;opc_dload_0,&amp;&amp;opc_dload_1,
 524 /* 0x28 */ &amp;&amp;opc_dload_2,&amp;&amp;opc_dload_3,&amp;&amp;opc_aload_0,&amp;&amp;opc_aload_1,
 525 /* 0x2C */ &amp;&amp;opc_aload_2,&amp;&amp;opc_aload_3,&amp;&amp;opc_iaload, &amp;&amp;opc_laload,
 526 
 527 /* 0x30 */ &amp;&amp;opc_faload,  &amp;&amp;opc_daload,  &amp;&amp;opc_aaload,  &amp;&amp;opc_baload,
 528 /* 0x34 */ &amp;&amp;opc_caload,  &amp;&amp;opc_saload,  &amp;&amp;opc_istore,  &amp;&amp;opc_lstore,
 529 /* 0x38 */ &amp;&amp;opc_fstore,  &amp;&amp;opc_dstore,  &amp;&amp;opc_astore,  &amp;&amp;opc_istore_0,
 530 /* 0x3C */ &amp;&amp;opc_istore_1,&amp;&amp;opc_istore_2,&amp;&amp;opc_istore_3,&amp;&amp;opc_lstore_0,
 531 
 532 /* 0x40 */ &amp;&amp;opc_lstore_1,&amp;&amp;opc_lstore_2,&amp;&amp;opc_lstore_3,&amp;&amp;opc_fstore_0,
 533 /* 0x44 */ &amp;&amp;opc_fstore_1,&amp;&amp;opc_fstore_2,&amp;&amp;opc_fstore_3,&amp;&amp;opc_dstore_0,
 534 /* 0x48 */ &amp;&amp;opc_dstore_1,&amp;&amp;opc_dstore_2,&amp;&amp;opc_dstore_3,&amp;&amp;opc_astore_0,
 535 /* 0x4C */ &amp;&amp;opc_astore_1,&amp;&amp;opc_astore_2,&amp;&amp;opc_astore_3,&amp;&amp;opc_iastore,
 536 
 537 /* 0x50 */ &amp;&amp;opc_lastore,&amp;&amp;opc_fastore,&amp;&amp;opc_dastore,&amp;&amp;opc_aastore,
 538 /* 0x54 */ &amp;&amp;opc_bastore,&amp;&amp;opc_castore,&amp;&amp;opc_sastore,&amp;&amp;opc_pop,
 539 /* 0x58 */ &amp;&amp;opc_pop2,   &amp;&amp;opc_dup,    &amp;&amp;opc_dup_x1, &amp;&amp;opc_dup_x2,
 540 /* 0x5C */ &amp;&amp;opc_dup2,   &amp;&amp;opc_dup2_x1,&amp;&amp;opc_dup2_x2,&amp;&amp;opc_swap,
 541 
 542 /* 0x60 */ &amp;&amp;opc_iadd,&amp;&amp;opc_ladd,&amp;&amp;opc_fadd,&amp;&amp;opc_dadd,
 543 /* 0x64 */ &amp;&amp;opc_isub,&amp;&amp;opc_lsub,&amp;&amp;opc_fsub,&amp;&amp;opc_dsub,
 544 /* 0x68 */ &amp;&amp;opc_imul,&amp;&amp;opc_lmul,&amp;&amp;opc_fmul,&amp;&amp;opc_dmul,
 545 /* 0x6C */ &amp;&amp;opc_idiv,&amp;&amp;opc_ldiv,&amp;&amp;opc_fdiv,&amp;&amp;opc_ddiv,
 546 
 547 /* 0x70 */ &amp;&amp;opc_irem, &amp;&amp;opc_lrem, &amp;&amp;opc_frem,&amp;&amp;opc_drem,
 548 /* 0x74 */ &amp;&amp;opc_ineg, &amp;&amp;opc_lneg, &amp;&amp;opc_fneg,&amp;&amp;opc_dneg,
 549 /* 0x78 */ &amp;&amp;opc_ishl, &amp;&amp;opc_lshl, &amp;&amp;opc_ishr,&amp;&amp;opc_lshr,
 550 /* 0x7C */ &amp;&amp;opc_iushr,&amp;&amp;opc_lushr,&amp;&amp;opc_iand,&amp;&amp;opc_land,
 551 
 552 /* 0x80 */ &amp;&amp;opc_ior, &amp;&amp;opc_lor,&amp;&amp;opc_ixor,&amp;&amp;opc_lxor,
 553 /* 0x84 */ &amp;&amp;opc_iinc,&amp;&amp;opc_i2l,&amp;&amp;opc_i2f, &amp;&amp;opc_i2d,
 554 /* 0x88 */ &amp;&amp;opc_l2i, &amp;&amp;opc_l2f,&amp;&amp;opc_l2d, &amp;&amp;opc_f2i,
 555 /* 0x8C */ &amp;&amp;opc_f2l, &amp;&amp;opc_f2d,&amp;&amp;opc_d2i, &amp;&amp;opc_d2l,
 556 
 557 /* 0x90 */ &amp;&amp;opc_d2f,  &amp;&amp;opc_i2b,  &amp;&amp;opc_i2c,  &amp;&amp;opc_i2s,
 558 /* 0x94 */ &amp;&amp;opc_lcmp, &amp;&amp;opc_fcmpl,&amp;&amp;opc_fcmpg,&amp;&amp;opc_dcmpl,
 559 /* 0x98 */ &amp;&amp;opc_dcmpg,&amp;&amp;opc_ifeq, &amp;&amp;opc_ifne, &amp;&amp;opc_iflt,
 560 /* 0x9C */ &amp;&amp;opc_ifge, &amp;&amp;opc_ifgt, &amp;&amp;opc_ifle, &amp;&amp;opc_if_icmpeq,
 561 
 562 /* 0xA0 */ &amp;&amp;opc_if_icmpne,&amp;&amp;opc_if_icmplt,&amp;&amp;opc_if_icmpge,  &amp;&amp;opc_if_icmpgt,
 563 /* 0xA4 */ &amp;&amp;opc_if_icmple,&amp;&amp;opc_if_acmpeq,&amp;&amp;opc_if_acmpne,  &amp;&amp;opc_goto,
 564 /* 0xA8 */ &amp;&amp;opc_jsr,      &amp;&amp;opc_ret,      &amp;&amp;opc_tableswitch,&amp;&amp;opc_lookupswitch,
 565 /* 0xAC */ &amp;&amp;opc_ireturn,  &amp;&amp;opc_lreturn,  &amp;&amp;opc_freturn,    &amp;&amp;opc_dreturn,
 566 
 567 /* 0xB0 */ &amp;&amp;opc_areturn,     &amp;&amp;opc_return,         &amp;&amp;opc_getstatic,    &amp;&amp;opc_putstatic,
 568 /* 0xB4 */ &amp;&amp;opc_getfield,    &amp;&amp;opc_putfield,       &amp;&amp;opc_invokevirtual,&amp;&amp;opc_invokespecial,
 569 /* 0xB8 */ &amp;&amp;opc_invokestatic,&amp;&amp;opc_invokeinterface,&amp;&amp;opc_invokedynamic,&amp;&amp;opc_new,
 570 /* 0xBC */ &amp;&amp;opc_newarray,    &amp;&amp;opc_anewarray,      &amp;&amp;opc_arraylength,  &amp;&amp;opc_athrow,
 571 
 572 /* 0xC0 */ &amp;&amp;opc_checkcast,   &amp;&amp;opc_instanceof,     &amp;&amp;opc_monitorenter, &amp;&amp;opc_monitorexit,
 573 /* 0xC4 */ &amp;&amp;opc_wide,        &amp;&amp;opc_multianewarray, &amp;&amp;opc_ifnull,       &amp;&amp;opc_ifnonnull,
 574 /* 0xC8 */ &amp;&amp;opc_goto_w,      &amp;&amp;opc_jsr_w,          &amp;&amp;opc_breakpoint,   &amp;&amp;opc_default,
 575 /* 0xCC */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 576 
 577 /* 0xD0 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 578 /* 0xD4 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 579 /* 0xD8 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 580 /* 0xDC */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 581 
 582 /* 0xE0 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,         &amp;&amp;opc_default,
 583 /* 0xE4 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_fast_aldc,    &amp;&amp;opc_fast_aldc_w,
 584 /* 0xE8 */ &amp;&amp;opc_return_register_finalizer,
 585                               &amp;&amp;opc_invokehandle,   &amp;&amp;opc_default,      &amp;&amp;opc_default,
 586 /* 0xEC */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,         &amp;&amp;opc_default,
 587 
 588 /* 0xF0 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 589 /* 0xF4 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 590 /* 0xF8 */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default,
 591 /* 0xFC */ &amp;&amp;opc_default,     &amp;&amp;opc_default,        &amp;&amp;opc_default,      &amp;&amp;opc_default
 592   };
 593   uintptr_t *dispatch_table = (uintptr_t*)&amp;opclabels_data[0];
 594 #endif /* USELABELS */
 595 
 596 #ifdef ASSERT
 597   // this will trigger a VERIFY_OOP on entry
 598   if (istate-&gt;msg() != initialize &amp;&amp; ! METHOD-&gt;is_static()) {
 599     oop rcvr = LOCALS_OBJECT(0);
 600     VERIFY_OOP(rcvr);
 601   }
 602 #endif
 603 
 604   /* QQQ this should be a stack method so we don&#39;t know actual direction */
 605   guarantee(istate-&gt;msg() == initialize ||
 606          topOfStack &gt;= istate-&gt;stack_limit() &amp;&amp;
 607          topOfStack &lt; istate-&gt;stack_base(),
 608          &quot;Stack top out of range&quot;);
 609 
 610 #ifdef CC_INTERP_PROFILE
 611   // MethodData&#39;s last branch taken count.
 612   uint mdo_last_branch_taken_count = 0;
 613 #else
 614   const uint mdo_last_branch_taken_count = 0;
 615 #endif
 616 
 617   switch (istate-&gt;msg()) {
 618     case initialize: {
 619       if (initialized++) ShouldNotReachHere(); // Only one initialize call.
 620       _compiling = (UseCompiler || CountCompiledCalls);
 621 #ifdef VM_JVMTI
 622       _jvmti_interp_events = JvmtiExport::can_post_interpreter_events();
 623 #endif
 624       return;
 625     }
 626     break;
 627     case method_entry: {
 628       THREAD-&gt;set_do_not_unlock();
 629       // count invocations
 630       assert(initialized, &quot;Interpreter not initialized&quot;);
 631       if (_compiling) {
 632         MethodCounters* mcs;
 633         GET_METHOD_COUNTERS(mcs);
 634 #if COMPILER2_OR_JVMCI
 635         if (ProfileInterpreter) {
 636           METHOD-&gt;increment_interpreter_invocation_count(THREAD);
 637         }
 638 #endif
 639         mcs-&gt;invocation_counter()-&gt;increment();
 640         if (mcs-&gt;invocation_counter()-&gt;reached_InvocationLimit(mcs-&gt;backedge_counter())) {
 641           CALL_VM((void)InterpreterRuntime::frequency_counter_overflow(THREAD, NULL), handle_exception);
 642           // We no longer retry on a counter overflow.
 643         }
 644         // Get or create profile data. Check for pending (async) exceptions.
 645         BI_PROFILE_GET_OR_CREATE_METHOD_DATA(handle_exception);
 646         SAFEPOINT;
 647       }
 648 
 649       if ((istate-&gt;_stack_base - istate-&gt;_stack_limit) != istate-&gt;method()-&gt;max_stack() + 1) {
 650         // initialize
 651         os::breakpoint();
 652       }
 653 
 654       // Lock method if synchronized.
 655       if (METHOD-&gt;is_synchronized()) {
 656         // oop rcvr = locals[0].j.r;
 657         oop rcvr;
 658         if (METHOD-&gt;is_static()) {
 659           rcvr = METHOD-&gt;constants()-&gt;pool_holder()-&gt;java_mirror();
 660         } else {
 661           rcvr = LOCALS_OBJECT(0);
 662           VERIFY_OOP(rcvr);
 663         }
 664         // The initial monitor is ours for the taking.
 665         // Monitor not filled in frame manager any longer as this caused race condition with biased locking.
 666         BasicObjectLock* mon = &amp;istate-&gt;monitor_base()[-1];
 667         mon-&gt;set_obj(rcvr);
 668         bool success = false;
 669         uintptr_t epoch_mask_in_place = markWord::epoch_mask_in_place;
 670         markWord mark = rcvr-&gt;mark();
 671         intptr_t hash = (intptr_t) markWord::no_hash;
 672         // Implies UseBiasedLocking.
 673         if (mark.has_bias_pattern()) {
 674           uintptr_t thread_ident;
 675           uintptr_t anticipated_bias_locking_value;
 676           thread_ident = (uintptr_t)istate-&gt;thread();
 677           anticipated_bias_locking_value =
 678             ((rcvr-&gt;klass()-&gt;prototype_header().value() | thread_ident) ^ mark.value()) &amp;
 679             ~(markWord::age_mask_in_place);
 680 
 681           if (anticipated_bias_locking_value == 0) {
 682             // Already biased towards this thread, nothing to do.
 683             if (PrintBiasedLockingStatistics) {
 684               (* BiasedLocking::biased_lock_entry_count_addr())++;
 685             }
 686             success = true;
 687           } else if ((anticipated_bias_locking_value &amp; markWord::biased_lock_mask_in_place) != 0) {
 688             // Try to revoke bias.
 689             markWord header = rcvr-&gt;klass()-&gt;prototype_header();
 690             if (hash != markWord::no_hash) {
 691               header = header.copy_set_hash(hash);
 692             }
 693             if (rcvr-&gt;cas_set_mark(header, mark) == mark) {
 694               if (PrintBiasedLockingStatistics)
 695                 (*BiasedLocking::revoked_lock_entry_count_addr())++;
 696             }
 697           } else if ((anticipated_bias_locking_value &amp; epoch_mask_in_place) != 0) {
 698             // Try to rebias.
 699             markWord new_header( (intptr_t) rcvr-&gt;klass()-&gt;prototype_header().value() | thread_ident);
 700             if (hash != markWord::no_hash) {
 701               new_header = new_header.copy_set_hash(hash);
 702             }
 703             if (rcvr-&gt;cas_set_mark(new_header, mark) == mark) {
 704               if (PrintBiasedLockingStatistics) {
 705                 (* BiasedLocking::rebiased_lock_entry_count_addr())++;
 706               }
 707             } else {
 708               CALL_VM(InterpreterRuntime::monitorenter(THREAD, mon), handle_exception);
 709             }
 710             success = true;
 711           } else {
 712             // Try to bias towards thread in case object is anonymously biased.
 713             markWord header(mark.value() &amp;
 714                             (markWord::biased_lock_mask_in_place |
 715                              markWord::age_mask_in_place | epoch_mask_in_place));
 716             if (hash != markWord::no_hash) {
 717               header = header.copy_set_hash(hash);
 718             }
 719             markWord new_header(header.value() | thread_ident);
 720             // Debugging hint.
 721             DEBUG_ONLY(mon-&gt;lock()-&gt;set_displaced_header(markWord((uintptr_t) 0xdeaddead));)
 722             if (rcvr-&gt;cas_set_mark(new_header, header) == header) {
 723               if (PrintBiasedLockingStatistics) {
 724                 (* BiasedLocking::anonymously_biased_lock_entry_count_addr())++;
 725               }
 726             } else {
 727               CALL_VM(InterpreterRuntime::monitorenter(THREAD, mon), handle_exception);
 728             }
 729             success = true;
 730           }
 731         }
 732 
 733         // Traditional lightweight locking.
 734         if (!success) {
 735           markWord displaced = rcvr-&gt;mark().set_unlocked();
 736           mon-&gt;lock()-&gt;set_displaced_header(displaced);
 737           bool call_vm = UseHeavyMonitors;
 738           if (call_vm || rcvr-&gt;cas_set_mark(markWord::from_pointer(mon), displaced) != displaced) {
 739             // Is it simple recursive case?
 740             if (!call_vm &amp;&amp; THREAD-&gt;is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {
 741               mon-&gt;lock()-&gt;set_displaced_header(markWord::from_pointer(NULL));
 742             } else {
 743               CALL_VM(InterpreterRuntime::monitorenter(THREAD, mon), handle_exception);
 744             }
 745           }
 746         }
 747       }
 748       THREAD-&gt;clr_do_not_unlock();
 749 
 750       // Notify jvmti
 751 #ifdef VM_JVMTI
 752       if (_jvmti_interp_events) {
 753         // Whenever JVMTI puts a thread in interp_only_mode, method
 754         // entry/exit events are sent for that thread to track stack depth.
 755         if (THREAD-&gt;is_interp_only_mode()) {
 756           CALL_VM(InterpreterRuntime::post_method_entry(THREAD),
 757                   handle_exception);
 758         }
 759       }
 760 #endif /* VM_JVMTI */
 761 
 762       goto run;
 763     }
 764 
 765     case popping_frame: {
 766       // returned from a java call to pop the frame, restart the call
 767       // clear the message so we don&#39;t confuse ourselves later
 768       assert(THREAD-&gt;pop_frame_in_process(), &quot;wrong frame pop state&quot;);
 769       istate-&gt;set_msg(no_request);
 770       if (_compiling) {
 771         // Set MDX back to the ProfileData of the invoke bytecode that will be
 772         // restarted.
 773         SET_MDX(NULL);
 774         BI_PROFILE_GET_OR_CREATE_METHOD_DATA(handle_exception);
 775       }
 776       THREAD-&gt;clr_pop_frame_in_process();
 777       goto run;
 778     }
 779 
 780     case method_resume: {
 781       if ((istate-&gt;_stack_base - istate-&gt;_stack_limit) != istate-&gt;method()-&gt;max_stack() + 1) {
 782         // resume
 783         os::breakpoint();
 784       }
 785       // returned from a java call, continue executing.
 786       if (THREAD-&gt;pop_frame_pending() &amp;&amp; !THREAD-&gt;pop_frame_in_process()) {
 787         goto handle_Pop_Frame;
 788       }
 789       if (THREAD-&gt;jvmti_thread_state() &amp;&amp;
 790           THREAD-&gt;jvmti_thread_state()-&gt;is_earlyret_pending()) {
 791         goto handle_Early_Return;
 792       }
 793 
 794       if (THREAD-&gt;has_pending_exception()) goto handle_exception;
 795       // Update the pc by the saved amount of the invoke bytecode size
 796       UPDATE_PC(istate-&gt;bcp_advance());
 797 
 798       if (_compiling) {
 799         // Get or create profile data. Check for pending (async) exceptions.
 800         BI_PROFILE_GET_OR_CREATE_METHOD_DATA(handle_exception);
 801       }
 802       goto run;
 803     }
 804 
 805     case deopt_resume2: {
 806       // Returned from an opcode that will reexecute. Deopt was
 807       // a result of a PopFrame request.
 808       //
 809 
 810       if (_compiling) {
 811         // Get or create profile data. Check for pending (async) exceptions.
 812         BI_PROFILE_GET_OR_CREATE_METHOD_DATA(handle_exception);
 813       }
 814       goto run;
 815     }
 816 
 817     case deopt_resume: {
 818       // Returned from an opcode that has completed. The stack has
 819       // the result all we need to do is skip across the bytecode
 820       // and continue (assuming there is no exception pending)
 821       //
 822       // compute continuation length
 823       //
 824       // Note: it is possible to deopt at a return_register_finalizer opcode
 825       // because this requires entering the vm to do the registering. While the
 826       // opcode is complete we can&#39;t advance because there are no more opcodes
 827       // much like trying to deopt at a poll return. In that has we simply
 828       // get out of here
 829       //
 830       if ( Bytecodes::code_at(METHOD, pc) == Bytecodes::_return_register_finalizer) {
 831         // this will do the right thing even if an exception is pending.
 832         goto handle_return;
 833       }
 834       UPDATE_PC(Bytecodes::length_at(METHOD, pc));
 835       if (THREAD-&gt;has_pending_exception()) goto handle_exception;
 836 
 837       if (_compiling) {
 838         // Get or create profile data. Check for pending (async) exceptions.
 839         BI_PROFILE_GET_OR_CREATE_METHOD_DATA(handle_exception);
 840       }
 841       goto run;
 842     }
 843     case got_monitors: {
 844       // continue locking now that we have a monitor to use
 845       // we expect to find newly allocated monitor at the &quot;top&quot; of the monitor stack.
 846       oop lockee = STACK_OBJECT(-1);
 847       VERIFY_OOP(lockee);
 848       // derefing&#39;s lockee ought to provoke implicit null check
 849       // find a free monitor
 850       BasicObjectLock* entry = (BasicObjectLock*) istate-&gt;stack_base();
 851       assert(entry-&gt;obj() == NULL, &quot;Frame manager didn&#39;t allocate the monitor&quot;);
 852       entry-&gt;set_obj(lockee);
 853       bool success = false;
 854       uintptr_t epoch_mask_in_place = markWord::epoch_mask_in_place;
 855 
 856       markWord mark = lockee-&gt;mark();
 857       intptr_t hash = (intptr_t) markWord::no_hash;
 858       // implies UseBiasedLocking
 859       if (mark.has_bias_pattern()) {
 860         uintptr_t thread_ident;
 861         uintptr_t anticipated_bias_locking_value;
 862         thread_ident = (uintptr_t)istate-&gt;thread();
 863         anticipated_bias_locking_value =
 864           ((lockee-&gt;klass()-&gt;prototype_header().value() | thread_ident) ^ mark.value()) &amp;
 865           ~(markWord::age_mask_in_place);
 866 
 867         if  (anticipated_bias_locking_value == 0) {
 868           // already biased towards this thread, nothing to do
 869           if (PrintBiasedLockingStatistics) {
 870             (* BiasedLocking::biased_lock_entry_count_addr())++;
 871           }
 872           success = true;
 873         } else if ((anticipated_bias_locking_value &amp; markWord::biased_lock_mask_in_place) != 0) {
 874           // try revoke bias
 875           markWord header = lockee-&gt;klass()-&gt;prototype_header();
 876           if (hash != markWord::no_hash) {
 877             header = header.copy_set_hash(hash);
 878           }
 879           if (lockee-&gt;cas_set_mark(header, mark) == mark) {
 880             if (PrintBiasedLockingStatistics) {
 881               (*BiasedLocking::revoked_lock_entry_count_addr())++;
 882             }
 883           }
 884         } else if ((anticipated_bias_locking_value &amp; epoch_mask_in_place) !=0) {
 885           // try rebias
 886           markWord new_header( (intptr_t) lockee-&gt;klass()-&gt;prototype_header().value() | thread_ident);
 887           if (hash != markWord::no_hash) {
 888             new_header = new_header.copy_set_hash(hash);
 889           }
 890           if (lockee-&gt;cas_set_mark(new_header, mark) == mark) {
 891             if (PrintBiasedLockingStatistics) {
 892               (* BiasedLocking::rebiased_lock_entry_count_addr())++;
 893             }
 894           } else {
 895             CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);
 896           }
 897           success = true;
 898         } else {
 899           // try to bias towards thread in case object is anonymously biased
 900           markWord header(mark.value() &amp; (markWord::biased_lock_mask_in_place |
 901                                           markWord::age_mask_in_place | epoch_mask_in_place));
 902           if (hash != markWord::no_hash) {
 903             header = header.copy_set_hash(hash);
 904           }
 905           markWord new_header(header.value() | thread_ident);
 906           // debugging hint
 907           DEBUG_ONLY(entry-&gt;lock()-&gt;set_displaced_header(markWord((uintptr_t) 0xdeaddead));)
 908           if (lockee-&gt;cas_set_mark(new_header, header) == header) {
 909             if (PrintBiasedLockingStatistics) {
 910               (* BiasedLocking::anonymously_biased_lock_entry_count_addr())++;
 911             }
 912           } else {
 913             CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);
 914           }
 915           success = true;
 916         }
 917       }
 918 
 919       // traditional lightweight locking
 920       if (!success) {
 921         markWord displaced = lockee-&gt;mark().set_unlocked();
 922         entry-&gt;lock()-&gt;set_displaced_header(displaced);
 923         bool call_vm = UseHeavyMonitors;
 924         if (call_vm || lockee-&gt;cas_set_mark(markWord::from_pointer(entry), displaced) != displaced) {
 925           // Is it simple recursive case?
 926           if (!call_vm &amp;&amp; THREAD-&gt;is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {
 927             entry-&gt;lock()-&gt;set_displaced_header(markWord::from_pointer(NULL));
 928           } else {
 929             CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);
 930           }
 931         }
 932       }
 933       UPDATE_PC_AND_TOS(1, -1);
 934       goto run;
 935     }
 936     default: {
 937       fatal(&quot;Unexpected message from frame manager&quot;);
 938     }
 939   }
 940 
 941 run:
 942 
 943   DO_UPDATE_INSTRUCTION_COUNT(*pc)
 944   DEBUGGER_SINGLE_STEP_NOTIFY();
 945 #ifdef PREFETCH_OPCCODE
 946   opcode = *pc;  /* prefetch first opcode */
 947 #endif
 948 
 949 #ifndef USELABELS
 950   while (1)
 951 #endif
 952   {
 953 #ifndef PREFETCH_OPCCODE
 954       opcode = *pc;
 955 #endif
 956       // Seems like this happens twice per opcode. At worst this is only
 957       // need at entry to the loop.
 958       // DEBUGGER_SINGLE_STEP_NOTIFY();
 959       /* Using this labels avoids double breakpoints when quickening and
 960        * when returing from transition frames.
 961        */
 962   opcode_switch:
 963       assert(istate == orig, &quot;Corrupted istate&quot;);
 964       /* QQQ Hmm this has knowledge of direction, ought to be a stack method */
 965       assert(topOfStack &gt;= istate-&gt;stack_limit(), &quot;Stack overrun&quot;);
 966       assert(topOfStack &lt; istate-&gt;stack_base(), &quot;Stack underrun&quot;);
 967 
 968 #ifdef USELABELS
 969       DISPATCH(opcode);
 970 #else
 971       switch (opcode)
 972 #endif
 973       {
 974       CASE(_nop):
 975           UPDATE_PC_AND_CONTINUE(1);
 976 
 977           /* Push miscellaneous constants onto the stack. */
 978 
 979       CASE(_aconst_null):
 980           SET_STACK_OBJECT(NULL, 0);
 981           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
 982 
 983 #undef  OPC_CONST_n
 984 #define OPC_CONST_n(opcode, const_type, value)                          \
 985       CASE(opcode):                                                     \
 986           SET_STACK_ ## const_type(value, 0);                           \
 987           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
 988 
 989           OPC_CONST_n(_iconst_m1,   INT,       -1);
 990           OPC_CONST_n(_iconst_0,    INT,        0);
 991           OPC_CONST_n(_iconst_1,    INT,        1);
 992           OPC_CONST_n(_iconst_2,    INT,        2);
 993           OPC_CONST_n(_iconst_3,    INT,        3);
 994           OPC_CONST_n(_iconst_4,    INT,        4);
 995           OPC_CONST_n(_iconst_5,    INT,        5);
 996           OPC_CONST_n(_fconst_0,    FLOAT,      0.0);
 997           OPC_CONST_n(_fconst_1,    FLOAT,      1.0);
 998           OPC_CONST_n(_fconst_2,    FLOAT,      2.0);
 999 
1000 #undef  OPC_CONST2_n
1001 #define OPC_CONST2_n(opcname, value, key, kind)                         \
1002       CASE(_##opcname):                                                 \
1003       {                                                                 \
1004           SET_STACK_ ## kind(VM##key##Const##value(), 1);               \
1005           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);                         \
1006       }
1007          OPC_CONST2_n(dconst_0, Zero, double, DOUBLE);
1008          OPC_CONST2_n(dconst_1, One,  double, DOUBLE);
1009          OPC_CONST2_n(lconst_0, Zero, long, LONG);
1010          OPC_CONST2_n(lconst_1, One,  long, LONG);
1011 
1012          /* Load constant from constant pool: */
1013 
1014           /* Push a 1-byte signed integer value onto the stack. */
1015       CASE(_bipush):
1016           SET_STACK_INT((jbyte)(pc[1]), 0);
1017           UPDATE_PC_AND_TOS_AND_CONTINUE(2, 1);
1018 
1019           /* Push a 2-byte signed integer constant onto the stack. */
1020       CASE(_sipush):
1021           SET_STACK_INT((int16_t)Bytes::get_Java_u2(pc + 1), 0);
1022           UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
1023 
1024           /* load from local variable */
1025 
1026       CASE(_aload):
1027           VERIFY_OOP(LOCALS_OBJECT(pc[1]));
1028           SET_STACK_OBJECT(LOCALS_OBJECT(pc[1]), 0);
1029           UPDATE_PC_AND_TOS_AND_CONTINUE(2, 1);
1030 
1031       CASE(_iload):
1032       CASE(_fload):
1033           SET_STACK_SLOT(LOCALS_SLOT(pc[1]), 0);
1034           UPDATE_PC_AND_TOS_AND_CONTINUE(2, 1);
1035 
1036       CASE(_lload):
1037           SET_STACK_LONG_FROM_ADDR(LOCALS_LONG_AT(pc[1]), 1);
1038           UPDATE_PC_AND_TOS_AND_CONTINUE(2, 2);
1039 
1040       CASE(_dload):
1041           SET_STACK_DOUBLE_FROM_ADDR(LOCALS_DOUBLE_AT(pc[1]), 1);
1042           UPDATE_PC_AND_TOS_AND_CONTINUE(2, 2);
1043 
1044 #undef  OPC_LOAD_n
1045 #define OPC_LOAD_n(num)                                                 \
1046       CASE(_aload_##num):                                               \
1047           VERIFY_OOP(LOCALS_OBJECT(num));                               \
1048           SET_STACK_OBJECT(LOCALS_OBJECT(num), 0);                      \
1049           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);                         \
1050                                                                         \
1051       CASE(_iload_##num):                                               \
1052       CASE(_fload_##num):                                               \
1053           SET_STACK_SLOT(LOCALS_SLOT(num), 0);                          \
1054           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);                         \
1055                                                                         \
1056       CASE(_lload_##num):                                               \
1057           SET_STACK_LONG_FROM_ADDR(LOCALS_LONG_AT(num), 1);             \
1058           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);                         \
1059       CASE(_dload_##num):                                               \
1060           SET_STACK_DOUBLE_FROM_ADDR(LOCALS_DOUBLE_AT(num), 1);         \
1061           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1062 
1063           OPC_LOAD_n(0);
1064           OPC_LOAD_n(1);
1065           OPC_LOAD_n(2);
1066           OPC_LOAD_n(3);
1067 
1068           /* store to a local variable */
1069 
1070       CASE(_astore):
1071           astore(topOfStack, -1, locals, pc[1]);
1072           UPDATE_PC_AND_TOS_AND_CONTINUE(2, -1);
1073 
1074       CASE(_istore):
1075       CASE(_fstore):
1076           SET_LOCALS_SLOT(STACK_SLOT(-1), pc[1]);
1077           UPDATE_PC_AND_TOS_AND_CONTINUE(2, -1);
1078 
1079       CASE(_lstore):
1080           SET_LOCALS_LONG(STACK_LONG(-1), pc[1]);
1081           UPDATE_PC_AND_TOS_AND_CONTINUE(2, -2);
1082 
1083       CASE(_dstore):
1084           SET_LOCALS_DOUBLE(STACK_DOUBLE(-1), pc[1]);
1085           UPDATE_PC_AND_TOS_AND_CONTINUE(2, -2);
1086 
1087       CASE(_wide): {
1088           uint16_t reg = Bytes::get_Java_u2(pc + 2);
1089 
1090           opcode = pc[1];
1091 
1092           // Wide and it&#39;s sub-bytecode are counted as separate instructions. If we
1093           // don&#39;t account for this here, the bytecode trace skips the next bytecode.
1094           DO_UPDATE_INSTRUCTION_COUNT(opcode);
1095 
1096           switch(opcode) {
1097               case Bytecodes::_aload:
1098                   VERIFY_OOP(LOCALS_OBJECT(reg));
1099                   SET_STACK_OBJECT(LOCALS_OBJECT(reg), 0);
1100                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, 1);
1101 
1102               case Bytecodes::_iload:
1103               case Bytecodes::_fload:
1104                   SET_STACK_SLOT(LOCALS_SLOT(reg), 0);
1105                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, 1);
1106 
1107               case Bytecodes::_lload:
1108                   SET_STACK_LONG_FROM_ADDR(LOCALS_LONG_AT(reg), 1);
1109                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, 2);
1110 
1111               case Bytecodes::_dload:
1112                   SET_STACK_DOUBLE_FROM_ADDR(LOCALS_LONG_AT(reg), 1);
1113                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, 2);
1114 
1115               case Bytecodes::_astore:
1116                   astore(topOfStack, -1, locals, reg);
1117                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, -1);
1118 
1119               case Bytecodes::_istore:
1120               case Bytecodes::_fstore:
1121                   SET_LOCALS_SLOT(STACK_SLOT(-1), reg);
1122                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, -1);
1123 
1124               case Bytecodes::_lstore:
1125                   SET_LOCALS_LONG(STACK_LONG(-1), reg);
1126                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, -2);
1127 
1128               case Bytecodes::_dstore:
1129                   SET_LOCALS_DOUBLE(STACK_DOUBLE(-1), reg);
1130                   UPDATE_PC_AND_TOS_AND_CONTINUE(4, -2);
1131 
1132               case Bytecodes::_iinc: {
1133                   int16_t offset = (int16_t)Bytes::get_Java_u2(pc+4);
1134                   // Be nice to see what this generates.... QQQ
1135                   SET_LOCALS_INT(LOCALS_INT(reg) + offset, reg);
1136                   UPDATE_PC_AND_CONTINUE(6);
1137               }
1138               case Bytecodes::_ret:
1139                   // Profile ret.
1140                   BI_PROFILE_UPDATE_RET(/*bci=*/((int)(intptr_t)(LOCALS_ADDR(reg))));
1141                   // Now, update the pc.
1142                   pc = istate-&gt;method()-&gt;code_base() + (intptr_t)(LOCALS_ADDR(reg));
1143                   UPDATE_PC_AND_CONTINUE(0);
1144               default:
1145                   VM_JAVA_ERROR(vmSymbols::java_lang_InternalError(), &quot;undefined opcode&quot;, note_no_trap);
1146           }
1147       }
1148 
1149 
1150 #undef  OPC_STORE_n
1151 #define OPC_STORE_n(num)                                                \
1152       CASE(_astore_##num):                                              \
1153           astore(topOfStack, -1, locals, num);                          \
1154           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);                        \
1155       CASE(_istore_##num):                                              \
1156       CASE(_fstore_##num):                                              \
1157           SET_LOCALS_SLOT(STACK_SLOT(-1), num);                         \
1158           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1159 
1160           OPC_STORE_n(0);
1161           OPC_STORE_n(1);
1162           OPC_STORE_n(2);
1163           OPC_STORE_n(3);
1164 
1165 #undef  OPC_DSTORE_n
1166 #define OPC_DSTORE_n(num)                                               \
1167       CASE(_dstore_##num):                                              \
1168           SET_LOCALS_DOUBLE(STACK_DOUBLE(-1), num);                     \
1169           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -2);                        \
1170       CASE(_lstore_##num):                                              \
1171           SET_LOCALS_LONG(STACK_LONG(-1), num);                         \
1172           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -2);
1173 
1174           OPC_DSTORE_n(0);
1175           OPC_DSTORE_n(1);
1176           OPC_DSTORE_n(2);
1177           OPC_DSTORE_n(3);
1178 
1179           /* stack pop, dup, and insert opcodes */
1180 
1181 
1182       CASE(_pop):                /* Discard the top item on the stack */
1183           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1184 
1185 
1186       CASE(_pop2):               /* Discard the top 2 items on the stack */
1187           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -2);
1188 
1189 
1190       CASE(_dup):               /* Duplicate the top item on the stack */
1191           dup(topOfStack);
1192           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1193 
1194       CASE(_dup2):              /* Duplicate the top 2 items on the stack */
1195           dup2(topOfStack);
1196           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1197 
1198       CASE(_dup_x1):    /* insert top word two down */
1199           dup_x1(topOfStack);
1200           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1201 
1202       CASE(_dup_x2):    /* insert top word three down  */
1203           dup_x2(topOfStack);
1204           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1205 
1206       CASE(_dup2_x1):   /* insert top 2 slots three down */
1207           dup2_x1(topOfStack);
1208           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1209 
1210       CASE(_dup2_x2):   /* insert top 2 slots four down */
1211           dup2_x2(topOfStack);
1212           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1213 
1214       CASE(_swap): {        /* swap top two elements on the stack */
1215           swap(topOfStack);
1216           UPDATE_PC_AND_CONTINUE(1);
1217       }
1218 
1219           /* Perform various binary integer operations */
1220 
1221 #undef  OPC_INT_BINARY
1222 #define OPC_INT_BINARY(opcname, opname, test)                           \
1223       CASE(_i##opcname):                                                \
1224           if (test &amp;&amp; (STACK_INT(-1) == 0)) {                           \
1225               VM_JAVA_ERROR(vmSymbols::java_lang_ArithmeticException(), \
1226                             &quot;/ by zero&quot;, note_div0Check_trap);          \
1227           }                                                             \
1228           SET_STACK_INT(VMint##opname(STACK_INT(-2),                    \
1229                                       STACK_INT(-1)),                   \
1230                                       -2);                              \
1231           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);                        \
1232       CASE(_l##opcname):                                                \
1233       {                                                                 \
1234           if (test) {                                                   \
1235             jlong l1 = STACK_LONG(-1);                                  \
1236             if (VMlongEqz(l1)) {                                        \
1237               VM_JAVA_ERROR(vmSymbols::java_lang_ArithmeticException(), \
1238                             &quot;/ by long zero&quot;, note_div0Check_trap);     \
1239             }                                                           \
1240           }                                                             \
1241           /* First long at (-1,-2) next long at (-3,-4) */              \
1242           SET_STACK_LONG(VMlong##opname(STACK_LONG(-3),                 \
1243                                         STACK_LONG(-1)),                \
1244                                         -3);                            \
1245           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -2);                        \
1246       }
1247 
1248       OPC_INT_BINARY(add, Add, 0);
1249       OPC_INT_BINARY(sub, Sub, 0);
1250       OPC_INT_BINARY(mul, Mul, 0);
1251       OPC_INT_BINARY(and, And, 0);
1252       OPC_INT_BINARY(or,  Or,  0);
1253       OPC_INT_BINARY(xor, Xor, 0);
1254       OPC_INT_BINARY(div, Div, 1);
1255       OPC_INT_BINARY(rem, Rem, 1);
1256 
1257 
1258       /* Perform various binary floating number operations */
1259       /* On some machine/platforms/compilers div zero check can be implicit */
1260 
1261 #undef  OPC_FLOAT_BINARY
1262 #define OPC_FLOAT_BINARY(opcname, opname)                                  \
1263       CASE(_d##opcname): {                                                 \
1264           SET_STACK_DOUBLE(VMdouble##opname(STACK_DOUBLE(-3),              \
1265                                             STACK_DOUBLE(-1)),             \
1266                                             -3);                           \
1267           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -2);                           \
1268       }                                                                    \
1269       CASE(_f##opcname):                                                   \
1270           SET_STACK_FLOAT(VMfloat##opname(STACK_FLOAT(-2),                 \
1271                                           STACK_FLOAT(-1)),                \
1272                                           -2);                             \
1273           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1274 
1275 
1276      OPC_FLOAT_BINARY(add, Add);
1277      OPC_FLOAT_BINARY(sub, Sub);
1278      OPC_FLOAT_BINARY(mul, Mul);
1279      OPC_FLOAT_BINARY(div, Div);
1280      OPC_FLOAT_BINARY(rem, Rem);
1281 
1282       /* Shift operations
1283        * Shift left int and long: ishl, lshl
1284        * Logical shift right int and long w/zero extension: iushr, lushr
1285        * Arithmetic shift right int and long w/sign extension: ishr, lshr
1286        */
1287 
1288 #undef  OPC_SHIFT_BINARY
1289 #define OPC_SHIFT_BINARY(opcname, opname)                               \
1290       CASE(_i##opcname):                                                \
1291          SET_STACK_INT(VMint##opname(STACK_INT(-2),                     \
1292                                      STACK_INT(-1)),                    \
1293                                      -2);                               \
1294          UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);                         \
1295       CASE(_l##opcname):                                                \
1296       {                                                                 \
1297          SET_STACK_LONG(VMlong##opname(STACK_LONG(-2),                  \
1298                                        STACK_INT(-1)),                  \
1299                                        -2);                             \
1300          UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);                         \
1301       }
1302 
1303       OPC_SHIFT_BINARY(shl, Shl);
1304       OPC_SHIFT_BINARY(shr, Shr);
1305       OPC_SHIFT_BINARY(ushr, Ushr);
1306 
1307      /* Increment local variable by constant */
1308       CASE(_iinc):
1309       {
1310           // locals[pc[1]].j.i += (jbyte)(pc[2]);
1311           SET_LOCALS_INT(LOCALS_INT(pc[1]) + (jbyte)(pc[2]), pc[1]);
1312           UPDATE_PC_AND_CONTINUE(3);
1313       }
1314 
1315      /* negate the value on the top of the stack */
1316 
1317       CASE(_ineg):
1318          SET_STACK_INT(VMintNeg(STACK_INT(-1)), -1);
1319          UPDATE_PC_AND_CONTINUE(1);
1320 
1321       CASE(_fneg):
1322          SET_STACK_FLOAT(VMfloatNeg(STACK_FLOAT(-1)), -1);
1323          UPDATE_PC_AND_CONTINUE(1);
1324 
1325       CASE(_lneg):
1326       {
1327          SET_STACK_LONG(VMlongNeg(STACK_LONG(-1)), -1);
1328          UPDATE_PC_AND_CONTINUE(1);
1329       }
1330 
1331       CASE(_dneg):
1332       {
1333          SET_STACK_DOUBLE(VMdoubleNeg(STACK_DOUBLE(-1)), -1);
1334          UPDATE_PC_AND_CONTINUE(1);
1335       }
1336 
1337       /* Conversion operations */
1338 
1339       CASE(_i2f):       /* convert top of stack int to float */
1340          SET_STACK_FLOAT(VMint2Float(STACK_INT(-1)), -1);
1341          UPDATE_PC_AND_CONTINUE(1);
1342 
1343       CASE(_i2l):       /* convert top of stack int to long */
1344       {
1345           // this is ugly QQQ
1346           jlong r = VMint2Long(STACK_INT(-1));
1347           MORE_STACK(-1); // Pop
1348           SET_STACK_LONG(r, 1);
1349 
1350           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1351       }
1352 
1353       CASE(_i2d):       /* convert top of stack int to double */
1354       {
1355           // this is ugly QQQ (why cast to jlong?? )
1356           jdouble r = (jlong)STACK_INT(-1);
1357           MORE_STACK(-1); // Pop
1358           SET_STACK_DOUBLE(r, 1);
1359 
1360           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1361       }
1362 
1363       CASE(_l2i):       /* convert top of stack long to int */
1364       {
1365           jint r = VMlong2Int(STACK_LONG(-1));
1366           MORE_STACK(-2); // Pop
1367           SET_STACK_INT(r, 0);
1368           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1369       }
1370 
1371       CASE(_l2f):   /* convert top of stack long to float */
1372       {
1373           jlong r = STACK_LONG(-1);
1374           MORE_STACK(-2); // Pop
1375           SET_STACK_FLOAT(VMlong2Float(r), 0);
1376           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1377       }
1378 
1379       CASE(_l2d):       /* convert top of stack long to double */
1380       {
1381           jlong r = STACK_LONG(-1);
1382           MORE_STACK(-2); // Pop
1383           SET_STACK_DOUBLE(VMlong2Double(r), 1);
1384           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1385       }
1386 
1387       CASE(_f2i):  /* Convert top of stack float to int */
1388           SET_STACK_INT(SharedRuntime::f2i(STACK_FLOAT(-1)), -1);
1389           UPDATE_PC_AND_CONTINUE(1);
1390 
1391       CASE(_f2l):  /* convert top of stack float to long */
1392       {
1393           jlong r = SharedRuntime::f2l(STACK_FLOAT(-1));
1394           MORE_STACK(-1); // POP
1395           SET_STACK_LONG(r, 1);
1396           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1397       }
1398 
1399       CASE(_f2d):  /* convert top of stack float to double */
1400       {
1401           jfloat f;
1402           jdouble r;
1403           f = STACK_FLOAT(-1);
1404           r = (jdouble) f;
1405           MORE_STACK(-1); // POP
1406           SET_STACK_DOUBLE(r, 1);
1407           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1408       }
1409 
1410       CASE(_d2i): /* convert top of stack double to int */
1411       {
1412           jint r1 = SharedRuntime::d2i(STACK_DOUBLE(-1));
1413           MORE_STACK(-2);
1414           SET_STACK_INT(r1, 0);
1415           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1416       }
1417 
1418       CASE(_d2f): /* convert top of stack double to float */
1419       {
1420           jfloat r1 = VMdouble2Float(STACK_DOUBLE(-1));
1421           MORE_STACK(-2);
1422           SET_STACK_FLOAT(r1, 0);
1423           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1424       }
1425 
1426       CASE(_d2l): /* convert top of stack double to long */
1427       {
1428           jlong r1 = SharedRuntime::d2l(STACK_DOUBLE(-1));
1429           MORE_STACK(-2);
1430           SET_STACK_LONG(r1, 1);
1431           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 2);
1432       }
1433 
1434       CASE(_i2b):
1435           SET_STACK_INT(VMint2Byte(STACK_INT(-1)), -1);
1436           UPDATE_PC_AND_CONTINUE(1);
1437 
1438       CASE(_i2c):
1439           SET_STACK_INT(VMint2Char(STACK_INT(-1)), -1);
1440           UPDATE_PC_AND_CONTINUE(1);
1441 
1442       CASE(_i2s):
1443           SET_STACK_INT(VMint2Short(STACK_INT(-1)), -1);
1444           UPDATE_PC_AND_CONTINUE(1);
1445 
1446       /* comparison operators */
1447 
1448 
1449 #define COMPARISON_OP(name, comparison)                                      \
1450       CASE(_if_icmp##name): {                                                \
1451           const bool cmp = (STACK_INT(-2) comparison STACK_INT(-1));         \
1452           int skip = cmp                                                     \
1453                       ? (int16_t)Bytes::get_Java_u2(pc + 1) : 3;             \
1454           address branch_pc = pc;                                            \
1455           /* Profile branch. */                                              \
1456           BI_PROFILE_UPDATE_BRANCH(/*is_taken=*/cmp);                        \
1457           UPDATE_PC_AND_TOS(skip, -2);                                       \
1458           DO_BACKEDGE_CHECKS(skip, branch_pc);                               \
1459           CONTINUE;                                                          \
1460       }                                                                      \
1461       CASE(_if##name): {                                                     \
1462           const bool cmp = (STACK_INT(-1) comparison 0);                     \
1463           int skip = cmp                                                     \
1464                       ? (int16_t)Bytes::get_Java_u2(pc + 1) : 3;             \
1465           address branch_pc = pc;                                            \
1466           /* Profile branch. */                                              \
1467           BI_PROFILE_UPDATE_BRANCH(/*is_taken=*/cmp);                        \
1468           UPDATE_PC_AND_TOS(skip, -1);                                       \
1469           DO_BACKEDGE_CHECKS(skip, branch_pc);                               \
1470           CONTINUE;                                                          \
1471       }
1472 
1473 #define COMPARISON_OP2(name, comparison)                                     \
1474       COMPARISON_OP(name, comparison)                                        \
1475       CASE(_if_acmp##name): {                                                \
1476           const bool cmp = (STACK_OBJECT(-2) comparison STACK_OBJECT(-1));   \
1477           int skip = cmp                                                     \
1478                        ? (int16_t)Bytes::get_Java_u2(pc + 1) : 3;            \
1479           address branch_pc = pc;                                            \
1480           /* Profile branch. */                                              \
1481           BI_PROFILE_UPDATE_BRANCH(/*is_taken=*/cmp);                        \
1482           UPDATE_PC_AND_TOS(skip, -2);                                       \
1483           DO_BACKEDGE_CHECKS(skip, branch_pc);                               \
1484           CONTINUE;                                                          \
1485       }
1486 
1487 #define NULL_COMPARISON_NOT_OP(name)                                         \
1488       CASE(_if##name): {                                                     \
1489           const bool cmp = (!(STACK_OBJECT(-1) == NULL));                    \
1490           int skip = cmp                                                     \
1491                       ? (int16_t)Bytes::get_Java_u2(pc + 1) : 3;             \
1492           address branch_pc = pc;                                            \
1493           /* Profile branch. */                                              \
1494           BI_PROFILE_UPDATE_BRANCH(/*is_taken=*/cmp);                        \
1495           UPDATE_PC_AND_TOS(skip, -1);                                       \
1496           DO_BACKEDGE_CHECKS(skip, branch_pc);                               \
1497           CONTINUE;                                                          \
1498       }
1499 
1500 #define NULL_COMPARISON_OP(name)                                             \
1501       CASE(_if##name): {                                                     \
1502           const bool cmp = ((STACK_OBJECT(-1) == NULL));                     \
1503           int skip = cmp                                                     \
1504                       ? (int16_t)Bytes::get_Java_u2(pc + 1) : 3;             \
1505           address branch_pc = pc;                                            \
1506           /* Profile branch. */                                              \
1507           BI_PROFILE_UPDATE_BRANCH(/*is_taken=*/cmp);                        \
1508           UPDATE_PC_AND_TOS(skip, -1);                                       \
1509           DO_BACKEDGE_CHECKS(skip, branch_pc);                               \
1510           CONTINUE;                                                          \
1511       }
1512       COMPARISON_OP(lt, &lt;);
1513       COMPARISON_OP(gt, &gt;);
1514       COMPARISON_OP(le, &lt;=);
1515       COMPARISON_OP(ge, &gt;=);
1516       COMPARISON_OP2(eq, ==);  /* include ref comparison */
1517       COMPARISON_OP2(ne, !=);  /* include ref comparison */
1518       NULL_COMPARISON_OP(null);
1519       NULL_COMPARISON_NOT_OP(nonnull);
1520 
1521       /* Goto pc at specified offset in switch table. */
1522 
1523       CASE(_tableswitch): {
1524           jint* lpc  = (jint*)VMalignWordUp(pc+1);
1525           int32_t  key  = STACK_INT(-1);
1526           int32_t  low  = Bytes::get_Java_u4((address)&amp;lpc[1]);
1527           int32_t  high = Bytes::get_Java_u4((address)&amp;lpc[2]);
1528           int32_t  skip;
1529           key -= low;
1530           if (((uint32_t) key &gt; (uint32_t)(high - low))) {
1531             key = -1;
1532             skip = Bytes::get_Java_u4((address)&amp;lpc[0]);
1533           } else {
1534             skip = Bytes::get_Java_u4((address)&amp;lpc[key + 3]);
1535           }
1536           // Profile switch.
1537           BI_PROFILE_UPDATE_SWITCH(/*switch_index=*/key);
1538           // Does this really need a full backedge check (osr)?
1539           address branch_pc = pc;
1540           UPDATE_PC_AND_TOS(skip, -1);
1541           DO_BACKEDGE_CHECKS(skip, branch_pc);
1542           CONTINUE;
1543       }
1544 
1545       /* Goto pc whose table entry matches specified key. */
1546 
1547       CASE(_lookupswitch): {
1548           jint* lpc  = (jint*)VMalignWordUp(pc+1);
1549           int32_t  key  = STACK_INT(-1);
1550           int32_t  skip = Bytes::get_Java_u4((address) lpc); /* default amount */
1551           // Remember index.
1552           int      index = -1;
1553           int      newindex = 0;
1554           int32_t  npairs = Bytes::get_Java_u4((address) &amp;lpc[1]);
1555           while (--npairs &gt;= 0) {
1556             lpc += 2;
1557             if (key == (int32_t)Bytes::get_Java_u4((address)lpc)) {
1558               skip = Bytes::get_Java_u4((address)&amp;lpc[1]);
1559               index = newindex;
1560               break;
1561             }
1562             newindex += 1;
1563           }
1564           // Profile switch.
1565           BI_PROFILE_UPDATE_SWITCH(/*switch_index=*/index);
1566           address branch_pc = pc;
1567           UPDATE_PC_AND_TOS(skip, -1);
1568           DO_BACKEDGE_CHECKS(skip, branch_pc);
1569           CONTINUE;
1570       }
1571 
1572       CASE(_fcmpl):
1573       CASE(_fcmpg):
1574       {
1575           SET_STACK_INT(VMfloatCompare(STACK_FLOAT(-2),
1576                                         STACK_FLOAT(-1),
1577                                         (opcode == Bytecodes::_fcmpl ? -1 : 1)),
1578                         -2);
1579           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1580       }
1581 
1582       CASE(_dcmpl):
1583       CASE(_dcmpg):
1584       {
1585           int r = VMdoubleCompare(STACK_DOUBLE(-3),
1586                                   STACK_DOUBLE(-1),
1587                                   (opcode == Bytecodes::_dcmpl ? -1 : 1));
1588           MORE_STACK(-4); // Pop
1589           SET_STACK_INT(r, 0);
1590           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1591       }
1592 
1593       CASE(_lcmp):
1594       {
1595           int r = VMlongCompare(STACK_LONG(-3), STACK_LONG(-1));
1596           MORE_STACK(-4);
1597           SET_STACK_INT(r, 0);
1598           UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);
1599       }
1600 
1601 
1602       /* Return from a method */
1603 
1604       CASE(_areturn):
1605       CASE(_ireturn):
1606       CASE(_freturn):
1607       {
1608           // Allow a safepoint before returning to frame manager.
1609           SAFEPOINT;
1610 
1611           goto handle_return;
1612       }
1613 
1614       CASE(_lreturn):
1615       CASE(_dreturn):
1616       {
1617           // Allow a safepoint before returning to frame manager.
1618           SAFEPOINT;
1619           goto handle_return;
1620       }
1621 
1622       CASE(_return_register_finalizer): {
1623 
1624           oop rcvr = LOCALS_OBJECT(0);
1625           VERIFY_OOP(rcvr);
1626           if (rcvr-&gt;klass()-&gt;has_finalizer()) {
1627             CALL_VM(InterpreterRuntime::register_finalizer(THREAD, rcvr), handle_exception);
1628           }
1629           goto handle_return;
1630       }
1631       CASE(_return): {
1632 
1633           // Allow a safepoint before returning to frame manager.
1634           SAFEPOINT;
1635           goto handle_return;
1636       }
1637 
1638       /* Array access byte-codes */
1639 
1640       /* Every array access byte-code starts out like this */
1641 //        arrayOopDesc* arrObj = (arrayOopDesc*)STACK_OBJECT(arrayOff);
1642 #define ARRAY_INTRO(arrayOff)                                                  \
1643       arrayOop arrObj = (arrayOop)STACK_OBJECT(arrayOff);                      \
1644       jint     index  = STACK_INT(arrayOff + 1);                               \
1645       char message[jintAsStringSize];                                          \
1646       CHECK_NULL(arrObj);                                                      \
1647       if ((uint32_t)index &gt;= (uint32_t)arrObj-&gt;length()) {                     \
1648           sprintf(message, &quot;%d&quot;, index);                                       \
1649           VM_JAVA_ERROR(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), \
1650                         message, note_rangeCheck_trap);                        \
1651       }
1652 
1653       /* 32-bit loads. These handle conversion from &lt; 32-bit types */
1654 #define ARRAY_LOADTO32(T, T2, format, stackRes, extra)                                \
1655       {                                                                               \
1656           ARRAY_INTRO(-2);                                                            \
1657           (void)extra;                                                                \
1658           SET_ ## stackRes(*(T2 *)(((address) arrObj-&gt;base(T)) + index * sizeof(T2)), \
1659                            -2);                                                       \
1660           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);                                      \
1661       }
1662 
1663       /* 64-bit loads */
1664 #define ARRAY_LOADTO64(T,T2, stackRes, extra)                                              \
1665       {                                                                                    \
1666           ARRAY_INTRO(-2);                                                                 \
1667           SET_ ## stackRes(*(T2 *)(((address) arrObj-&gt;base(T)) + index * sizeof(T2)), -1); \
1668           (void)extra;                                                                     \
1669           UPDATE_PC_AND_CONTINUE(1);                                                       \
1670       }
1671 
1672       CASE(_iaload):
1673           ARRAY_LOADTO32(T_INT, jint,   &quot;%d&quot;,   STACK_INT, 0);
1674       CASE(_faload):
1675           ARRAY_LOADTO32(T_FLOAT, jfloat, &quot;%f&quot;,   STACK_FLOAT, 0);
1676       CASE(_aaload): {
1677           ARRAY_INTRO(-2);
1678           SET_STACK_OBJECT(((objArrayOop) arrObj)-&gt;obj_at(index), -2);
1679           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1680       }
1681       CASE(_baload):
1682           ARRAY_LOADTO32(T_BYTE, jbyte,  &quot;%d&quot;,   STACK_INT, 0);
1683       CASE(_caload):
1684           ARRAY_LOADTO32(T_CHAR,  jchar, &quot;%d&quot;,   STACK_INT, 0);
1685       CASE(_saload):
1686           ARRAY_LOADTO32(T_SHORT, jshort, &quot;%d&quot;,   STACK_INT, 0);
1687       CASE(_laload):
1688           ARRAY_LOADTO64(T_LONG, jlong, STACK_LONG, 0);
1689       CASE(_daload):
1690           ARRAY_LOADTO64(T_DOUBLE, jdouble, STACK_DOUBLE, 0);
1691 
1692       /* 32-bit stores. These handle conversion to &lt; 32-bit types */
1693 #define ARRAY_STOREFROM32(T, T2, format, stackSrc, extra)                            \
1694       {                                                                              \
1695           ARRAY_INTRO(-3);                                                           \
1696           (void)extra;                                                               \
1697           *(T2 *)(((address) arrObj-&gt;base(T)) + index * sizeof(T2)) = stackSrc( -1); \
1698           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -3);                                     \
1699       }
1700 
1701       /* 64-bit stores */
1702 #define ARRAY_STOREFROM64(T, T2, stackSrc, extra)                                    \
1703       {                                                                              \
1704           ARRAY_INTRO(-4);                                                           \
1705           (void)extra;                                                               \
1706           *(T2 *)(((address) arrObj-&gt;base(T)) + index * sizeof(T2)) = stackSrc( -1); \
1707           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -4);                                     \
1708       }
1709 
1710       CASE(_iastore):
1711           ARRAY_STOREFROM32(T_INT, jint,   &quot;%d&quot;,   STACK_INT, 0);
1712       CASE(_fastore):
1713           ARRAY_STOREFROM32(T_FLOAT, jfloat, &quot;%f&quot;,   STACK_FLOAT, 0);
1714       /*
1715        * This one looks different because of the assignability check
1716        */
1717       CASE(_aastore): {
1718           oop rhsObject = STACK_OBJECT(-1);
1719           VERIFY_OOP(rhsObject);
1720           ARRAY_INTRO( -3);
1721           // arrObj, index are set
1722           if (rhsObject != NULL) {
1723             /* Check assignability of rhsObject into arrObj */
1724             Klass* rhsKlass = rhsObject-&gt;klass(); // EBX (subclass)
1725             Klass* elemKlass = ObjArrayKlass::cast(arrObj-&gt;klass())-&gt;element_klass(); // superklass EAX
1726             //
1727             // Check for compatibilty. This check must not GC!!
1728             // Seems way more expensive now that we must dispatch
1729             //
1730             if (rhsKlass != elemKlass &amp;&amp; !rhsKlass-&gt;is_subtype_of(elemKlass)) { // ebx-&gt;is...
1731               // Decrement counter if subtype check failed.
1732               BI_PROFILE_SUBTYPECHECK_FAILED(rhsKlass);
1733               VM_JAVA_ERROR(vmSymbols::java_lang_ArrayStoreException(), &quot;&quot;, note_arrayCheck_trap);
1734             }
1735             // Profile checkcast with null_seen and receiver.
1736             BI_PROFILE_UPDATE_CHECKCAST(/*null_seen=*/false, rhsKlass);
1737           } else {
1738             // Profile checkcast with null_seen and receiver.
1739             BI_PROFILE_UPDATE_CHECKCAST(/*null_seen=*/true, NULL);
1740           }
1741           ((objArrayOop) arrObj)-&gt;obj_at_put(index, rhsObject);
1742           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -3);
1743       }
1744       CASE(_bastore): {
1745           ARRAY_INTRO(-3);
1746           int item = STACK_INT(-1);
1747           // if it is a T_BOOLEAN array, mask the stored value to 0/1
1748           if (arrObj-&gt;klass() == Universe::boolArrayKlassObj()) {
1749             item &amp;= 1;
1750           } else {
1751             assert(arrObj-&gt;klass() == Universe::byteArrayKlassObj(),
1752                    &quot;should be byte array otherwise&quot;);
1753           }
1754           ((typeArrayOop)arrObj)-&gt;byte_at_put(index, item);
1755           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -3);
1756       }
1757       CASE(_castore):
1758           ARRAY_STOREFROM32(T_CHAR, jchar,  &quot;%d&quot;,   STACK_INT, 0);
1759       CASE(_sastore):
1760           ARRAY_STOREFROM32(T_SHORT, jshort, &quot;%d&quot;,   STACK_INT, 0);
1761       CASE(_lastore):
1762           ARRAY_STOREFROM64(T_LONG, jlong, STACK_LONG, 0);
1763       CASE(_dastore):
1764           ARRAY_STOREFROM64(T_DOUBLE, jdouble, STACK_DOUBLE, 0);
1765 
1766       CASE(_arraylength):
1767       {
1768           arrayOop ary = (arrayOop) STACK_OBJECT(-1);
1769           CHECK_NULL(ary);
1770           SET_STACK_INT(ary-&gt;length(), -1);
1771           UPDATE_PC_AND_CONTINUE(1);
1772       }
1773 
1774       /* monitorenter and monitorexit for locking/unlocking an object */
1775 
1776       CASE(_monitorenter): {
1777         oop lockee = STACK_OBJECT(-1);
1778         // derefing&#39;s lockee ought to provoke implicit null check
1779         CHECK_NULL(lockee);
1780         // find a free monitor or one already allocated for this object
1781         // if we find a matching object then we need a new monitor
1782         // since this is recursive enter
1783         BasicObjectLock* limit = istate-&gt;monitor_base();
1784         BasicObjectLock* most_recent = (BasicObjectLock*) istate-&gt;stack_base();
1785         BasicObjectLock* entry = NULL;
1786         while (most_recent != limit ) {
1787           if (most_recent-&gt;obj() == NULL) entry = most_recent;
1788           else if (most_recent-&gt;obj() == lockee) break;
1789           most_recent++;
1790         }
1791         if (entry != NULL) {
1792           entry-&gt;set_obj(lockee);
1793           int success = false;
1794           uintptr_t epoch_mask_in_place = markWord::epoch_mask_in_place;
1795 
1796           markWord mark = lockee-&gt;mark();
1797           intptr_t hash = (intptr_t) markWord::no_hash;
1798           // implies UseBiasedLocking
1799           if (mark.has_bias_pattern()) {
1800             uintptr_t thread_ident;
1801             uintptr_t anticipated_bias_locking_value;
1802             thread_ident = (uintptr_t)istate-&gt;thread();
1803             anticipated_bias_locking_value =
1804               ((lockee-&gt;klass()-&gt;prototype_header().value() | thread_ident) ^ mark.value()) &amp;
1805               ~(markWord::age_mask_in_place);
1806 
1807             if  (anticipated_bias_locking_value == 0) {
1808               // already biased towards this thread, nothing to do
1809               if (PrintBiasedLockingStatistics) {
1810                 (* BiasedLocking::biased_lock_entry_count_addr())++;
1811               }
1812               success = true;
1813             }
1814             else if ((anticipated_bias_locking_value &amp; markWord::biased_lock_mask_in_place) != 0) {
1815               // try revoke bias
1816               markWord header = lockee-&gt;klass()-&gt;prototype_header();
1817               if (hash != markWord::no_hash) {
1818                 header = header.copy_set_hash(hash);
1819               }
1820               if (lockee-&gt;cas_set_mark(header, mark) == mark) {
1821                 if (PrintBiasedLockingStatistics)
1822                   (*BiasedLocking::revoked_lock_entry_count_addr())++;
1823               }
1824             }
1825             else if ((anticipated_bias_locking_value &amp; epoch_mask_in_place) !=0) {
1826               // try rebias
1827               markWord new_header( (intptr_t) lockee-&gt;klass()-&gt;prototype_header().value() | thread_ident);
1828               if (hash != markWord::no_hash) {
1829                 new_header = new_header.copy_set_hash(hash);
1830               }
1831               if (lockee-&gt;cas_set_mark(new_header, mark) == mark) {
1832                 if (PrintBiasedLockingStatistics)
1833                   (* BiasedLocking::rebiased_lock_entry_count_addr())++;
1834               }
1835               else {
1836                 CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);
1837               }
1838               success = true;
1839             }
1840             else {
1841               // try to bias towards thread in case object is anonymously biased
1842               markWord header(mark.value() &amp; (markWord::biased_lock_mask_in_place |
1843                                               markWord::age_mask_in_place |
1844                                               epoch_mask_in_place));
1845               if (hash != markWord::no_hash) {
1846                 header = header.copy_set_hash(hash);
1847               }
1848               markWord new_header(header.value() | thread_ident);
1849               // debugging hint
1850               DEBUG_ONLY(entry-&gt;lock()-&gt;set_displaced_header(markWord((uintptr_t) 0xdeaddead));)
1851               if (lockee-&gt;cas_set_mark(new_header, header) == header) {
1852                 if (PrintBiasedLockingStatistics)
1853                   (* BiasedLocking::anonymously_biased_lock_entry_count_addr())++;
1854               }
1855               else {
1856                 CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);
1857               }
1858               success = true;
1859             }
1860           }
1861 
1862           // traditional lightweight locking
1863           if (!success) {
1864             markWord displaced = lockee-&gt;mark().set_unlocked();
1865             entry-&gt;lock()-&gt;set_displaced_header(displaced);
1866             bool call_vm = UseHeavyMonitors;
1867             if (call_vm || lockee-&gt;cas_set_mark(markWord::from_pointer(entry), displaced) != displaced) {
1868               // Is it simple recursive case?
1869               if (!call_vm &amp;&amp; THREAD-&gt;is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {
1870                 entry-&gt;lock()-&gt;set_displaced_header(markWord::from_pointer(NULL));
1871               } else {
1872                 CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);
1873               }
1874             }
1875           }
1876           UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1877         } else {
1878           istate-&gt;set_msg(more_monitors);
1879           UPDATE_PC_AND_RETURN(0); // Re-execute
1880         }
1881       }
1882 
1883       CASE(_monitorexit): {
1884         oop lockee = STACK_OBJECT(-1);
1885         CHECK_NULL(lockee);
1886         // derefing&#39;s lockee ought to provoke implicit null check
1887         // find our monitor slot
1888         BasicObjectLock* limit = istate-&gt;monitor_base();
1889         BasicObjectLock* most_recent = (BasicObjectLock*) istate-&gt;stack_base();
1890         while (most_recent != limit ) {
1891           if ((most_recent)-&gt;obj() == lockee) {
1892             BasicLock* lock = most_recent-&gt;lock();
1893             markWord header = lock-&gt;displaced_header();
1894             most_recent-&gt;set_obj(NULL);
1895             if (!lockee-&gt;mark().has_bias_pattern()) {
1896               bool call_vm = UseHeavyMonitors;
1897               // If it isn&#39;t recursive we either must swap old header or call the runtime
1898               if (header.to_pointer() != NULL || call_vm) {
1899                 markWord old_header = markWord::encode(lock);
1900                 if (call_vm || lockee-&gt;cas_set_mark(header, old_header) != old_header) {
1901                   // restore object for the slow case
1902                   most_recent-&gt;set_obj(lockee);
1903                   CALL_VM(InterpreterRuntime::monitorexit(THREAD, most_recent), handle_exception);
1904                 }
1905               }
1906             }
1907             UPDATE_PC_AND_TOS_AND_CONTINUE(1, -1);
1908           }
1909           most_recent++;
1910         }
1911         // Need to throw illegal monitor state exception
1912         CALL_VM(InterpreterRuntime::throw_illegal_monitor_state_exception(THREAD), handle_exception);
1913         ShouldNotReachHere();
1914       }
1915 
1916       /* All of the non-quick opcodes. */
1917 
1918       /* -Set clobbersCpIndex true if the quickened opcode clobbers the
1919        *  constant pool index in the instruction.
1920        */
1921       CASE(_getfield):
1922       CASE(_getstatic):
1923         {
1924           u2 index;
1925           ConstantPoolCacheEntry* cache;
1926           index = Bytes::get_native_u2(pc+1);
1927 
1928           // QQQ Need to make this as inlined as possible. Probably need to
1929           // split all the bytecode cases out so c++ compiler has a chance
1930           // for constant prop to fold everything possible away.
1931 
1932           cache = cp-&gt;entry_at(index);
1933           if (!cache-&gt;is_resolved((Bytecodes::Code)opcode)) {
1934             CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),
1935                     handle_exception);
1936             cache = cp-&gt;entry_at(index);
1937           }
1938 
1939 #ifdef VM_JVMTI
1940           if (_jvmti_interp_events) {
1941             int *count_addr;
1942             oop obj;
1943             // Check to see if a field modification watch has been set
1944             // before we take the time to call into the VM.
1945             count_addr = (int *)JvmtiExport::get_field_access_count_addr();
1946             if ( *count_addr &gt; 0 ) {
1947               if ((Bytecodes::Code)opcode == Bytecodes::_getstatic) {
1948                 obj = (oop)NULL;
1949               } else {
1950                 obj = (oop) STACK_OBJECT(-1);
1951                 VERIFY_OOP(obj);
1952               }
1953               CALL_VM(InterpreterRuntime::post_field_access(THREAD,
1954                                           obj,
1955                                           cache),
1956                                           handle_exception);
1957             }
1958           }
1959 #endif /* VM_JVMTI */
1960 
1961           oop obj;
1962           if ((Bytecodes::Code)opcode == Bytecodes::_getstatic) {
1963             Klass* k = cache-&gt;f1_as_klass();
1964             obj = k-&gt;java_mirror();
1965             MORE_STACK(1);  // Assume single slot push
1966           } else {
1967             obj = (oop) STACK_OBJECT(-1);
1968             CHECK_NULL(obj);
1969           }
1970 
1971           //
1972           // Now store the result on the stack
1973           //
1974           TosState tos_type = cache-&gt;flag_state();
1975           int field_offset = cache-&gt;f2_as_index();
1976           if (cache-&gt;is_volatile()) {
1977             if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
1978               OrderAccess::fence();
1979             }
1980             if (tos_type == atos) {
1981               VERIFY_OOP(obj-&gt;obj_field_acquire(field_offset));
1982               SET_STACK_OBJECT(obj-&gt;obj_field_acquire(field_offset), -1);
1983             } else if (tos_type == itos) {
1984               SET_STACK_INT(obj-&gt;int_field_acquire(field_offset), -1);
1985             } else if (tos_type == ltos) {
1986               SET_STACK_LONG(obj-&gt;long_field_acquire(field_offset), 0);
1987               MORE_STACK(1);
1988             } else if (tos_type == btos || tos_type == ztos) {
1989               SET_STACK_INT(obj-&gt;byte_field_acquire(field_offset), -1);
1990             } else if (tos_type == ctos) {
1991               SET_STACK_INT(obj-&gt;char_field_acquire(field_offset), -1);
1992             } else if (tos_type == stos) {
1993               SET_STACK_INT(obj-&gt;short_field_acquire(field_offset), -1);
1994             } else if (tos_type == ftos) {
1995               SET_STACK_FLOAT(obj-&gt;float_field_acquire(field_offset), -1);
1996             } else {
1997               SET_STACK_DOUBLE(obj-&gt;double_field_acquire(field_offset), 0);
1998               MORE_STACK(1);
1999             }
2000           } else {
2001             if (tos_type == atos) {
2002               VERIFY_OOP(obj-&gt;obj_field(field_offset));
2003               SET_STACK_OBJECT(obj-&gt;obj_field(field_offset), -1);
2004             } else if (tos_type == itos) {
2005               SET_STACK_INT(obj-&gt;int_field(field_offset), -1);
2006             } else if (tos_type == ltos) {
2007               SET_STACK_LONG(obj-&gt;long_field(field_offset), 0);
2008               MORE_STACK(1);
2009             } else if (tos_type == btos || tos_type == ztos) {
2010               SET_STACK_INT(obj-&gt;byte_field(field_offset), -1);
2011             } else if (tos_type == ctos) {
2012               SET_STACK_INT(obj-&gt;char_field(field_offset), -1);
2013             } else if (tos_type == stos) {
2014               SET_STACK_INT(obj-&gt;short_field(field_offset), -1);
2015             } else if (tos_type == ftos) {
2016               SET_STACK_FLOAT(obj-&gt;float_field(field_offset), -1);
2017             } else {
2018               SET_STACK_DOUBLE(obj-&gt;double_field(field_offset), 0);
2019               MORE_STACK(1);
2020             }
2021           }
2022 
2023           UPDATE_PC_AND_CONTINUE(3);
2024          }
2025 
2026       CASE(_putfield):
2027       CASE(_putstatic):
2028         {
2029           u2 index = Bytes::get_native_u2(pc+1);
2030           ConstantPoolCacheEntry* cache = cp-&gt;entry_at(index);
2031           if (!cache-&gt;is_resolved((Bytecodes::Code)opcode)) {
2032             CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),
2033                     handle_exception);
2034             cache = cp-&gt;entry_at(index);
2035           }
2036 
2037 #ifdef VM_JVMTI
2038           if (_jvmti_interp_events) {
2039             int *count_addr;
2040             oop obj;
2041             // Check to see if a field modification watch has been set
2042             // before we take the time to call into the VM.
2043             count_addr = (int *)JvmtiExport::get_field_modification_count_addr();
2044             if ( *count_addr &gt; 0 ) {
2045               if ((Bytecodes::Code)opcode == Bytecodes::_putstatic) {
2046                 obj = (oop)NULL;
2047               }
2048               else {
2049                 if (cache-&gt;is_long() || cache-&gt;is_double()) {
2050                   obj = (oop) STACK_OBJECT(-3);
2051                 } else {
2052                   obj = (oop) STACK_OBJECT(-2);
2053                 }
2054                 VERIFY_OOP(obj);
2055               }
2056 
2057               CALL_VM(InterpreterRuntime::post_field_modification(THREAD,
2058                                           obj,
2059                                           cache,
2060                                           (jvalue *)STACK_SLOT(-1)),
2061                                           handle_exception);
2062             }
2063           }
2064 #endif /* VM_JVMTI */
2065 
2066           // QQQ Need to make this as inlined as possible. Probably need to split all the bytecode cases
2067           // out so c++ compiler has a chance for constant prop to fold everything possible away.
2068 
2069           oop obj;
2070           int count;
2071           TosState tos_type = cache-&gt;flag_state();
2072 
2073           count = -1;
2074           if (tos_type == ltos || tos_type == dtos) {
2075             --count;
2076           }
2077           if ((Bytecodes::Code)opcode == Bytecodes::_putstatic) {
2078             Klass* k = cache-&gt;f1_as_klass();
2079             obj = k-&gt;java_mirror();
2080           } else {
2081             --count;
2082             obj = (oop) STACK_OBJECT(count);
2083             CHECK_NULL(obj);
2084           }
2085 
2086           //
2087           // Now store the result
2088           //
2089           int field_offset = cache-&gt;f2_as_index();
2090           if (cache-&gt;is_volatile()) {
2091             if (tos_type == itos) {
2092               obj-&gt;release_int_field_put(field_offset, STACK_INT(-1));
2093             } else if (tos_type == atos) {
2094               VERIFY_OOP(STACK_OBJECT(-1));
2095               obj-&gt;release_obj_field_put(field_offset, STACK_OBJECT(-1));
2096             } else if (tos_type == btos) {
2097               obj-&gt;release_byte_field_put(field_offset, STACK_INT(-1));
2098             } else if (tos_type == ztos) {
2099               int bool_field = STACK_INT(-1);  // only store LSB
2100               obj-&gt;release_byte_field_put(field_offset, (bool_field &amp; 1));
2101             } else if (tos_type == ltos) {
2102               obj-&gt;release_long_field_put(field_offset, STACK_LONG(-1));
2103             } else if (tos_type == ctos) {
2104               obj-&gt;release_char_field_put(field_offset, STACK_INT(-1));
2105             } else if (tos_type == stos) {
2106               obj-&gt;release_short_field_put(field_offset, STACK_INT(-1));
2107             } else if (tos_type == ftos) {
2108               obj-&gt;release_float_field_put(field_offset, STACK_FLOAT(-1));
2109             } else {
2110               obj-&gt;release_double_field_put(field_offset, STACK_DOUBLE(-1));
2111             }
2112             OrderAccess::storeload();
2113           } else {
2114             if (tos_type == itos) {
2115               obj-&gt;int_field_put(field_offset, STACK_INT(-1));
2116             } else if (tos_type == atos) {
2117               VERIFY_OOP(STACK_OBJECT(-1));
2118               obj-&gt;obj_field_put(field_offset, STACK_OBJECT(-1));
2119             } else if (tos_type == btos) {
2120               obj-&gt;byte_field_put(field_offset, STACK_INT(-1));
2121             } else if (tos_type == ztos) {
2122               int bool_field = STACK_INT(-1);  // only store LSB
2123               obj-&gt;byte_field_put(field_offset, (bool_field &amp; 1));
2124             } else if (tos_type == ltos) {
2125               obj-&gt;long_field_put(field_offset, STACK_LONG(-1));
2126             } else if (tos_type == ctos) {
2127               obj-&gt;char_field_put(field_offset, STACK_INT(-1));
2128             } else if (tos_type == stos) {
2129               obj-&gt;short_field_put(field_offset, STACK_INT(-1));
2130             } else if (tos_type == ftos) {
2131               obj-&gt;float_field_put(field_offset, STACK_FLOAT(-1));
2132             } else {
2133               obj-&gt;double_field_put(field_offset, STACK_DOUBLE(-1));
2134             }
2135           }
2136 
2137           UPDATE_PC_AND_TOS_AND_CONTINUE(3, count);
2138         }
2139 
2140       CASE(_new): {
2141         u2 index = Bytes::get_Java_u2(pc+1);
2142         ConstantPool* constants = istate-&gt;method()-&gt;constants();
2143         if (!constants-&gt;tag_at(index).is_unresolved_klass()) {
2144           // Make sure klass is initialized and doesn&#39;t have a finalizer
2145           Klass* entry = constants-&gt;resolved_klass_at(index);
2146           InstanceKlass* ik = InstanceKlass::cast(entry);
2147           if (ik-&gt;is_initialized() &amp;&amp; ik-&gt;can_be_fastpath_allocated() ) {
2148             size_t obj_size = ik-&gt;size_helper();
2149             oop result = NULL;
2150             // If the TLAB isn&#39;t pre-zeroed then we&#39;ll have to do it
2151             bool need_zero = !ZeroTLAB;
2152             if (UseTLAB) {
2153               result = (oop) THREAD-&gt;tlab().allocate(obj_size);
2154             }
2155             // Disable non-TLAB-based fast-path, because profiling requires that all
2156             // allocations go through InterpreterRuntime::_new() if THREAD-&gt;tlab().allocate
2157             // returns NULL.
2158 #ifndef CC_INTERP_PROFILE
2159             if (result == NULL) {
2160               need_zero = true;
2161               // Try allocate in shared eden
2162             retry:
2163               HeapWord* compare_to = *Universe::heap()-&gt;top_addr();
2164               HeapWord* new_top = compare_to + obj_size;
2165               if (new_top &lt;= *Universe::heap()-&gt;end_addr()) {
2166                 if (Atomic::cmpxchg(Universe::heap()-&gt;top_addr(), compare_to, new_top) != compare_to) {
2167                   goto retry;
2168                 }
2169                 result = (oop) compare_to;
2170               }
2171             }
2172 #endif
2173             if (result != NULL) {
2174               // Initialize object (if nonzero size and need) and then the header
2175               if (need_zero ) {
2176                 HeapWord* to_zero = cast_from_oop&lt;HeapWord*&gt;(result) + sizeof(oopDesc) / oopSize;
2177                 obj_size -= sizeof(oopDesc) / oopSize;
2178                 if (obj_size &gt; 0 ) {
2179                   memset(to_zero, 0, obj_size * HeapWordSize);
2180                 }
2181               }
2182               if (UseBiasedLocking) {
2183                 result-&gt;set_mark(ik-&gt;prototype_header());
2184               } else {
2185                 result-&gt;set_mark(markWord::prototype());
2186               }
2187               result-&gt;set_klass_gap(0);
2188               result-&gt;set_klass(ik);
2189               // Must prevent reordering of stores for object initialization
2190               // with stores that publish the new object.
2191               OrderAccess::storestore();
2192               SET_STACK_OBJECT(result, 0);
2193               UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
2194             }
2195           }
2196         }
2197         // Slow case allocation
2198         CALL_VM(InterpreterRuntime::_new(THREAD, METHOD-&gt;constants(), index),
2199                 handle_exception);
2200         // Must prevent reordering of stores for object initialization
2201         // with stores that publish the new object.
2202         OrderAccess::storestore();
2203         SET_STACK_OBJECT(THREAD-&gt;vm_result(), 0);
2204         THREAD-&gt;set_vm_result(NULL);
2205         UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
2206       }
2207       CASE(_anewarray): {
2208         u2 index = Bytes::get_Java_u2(pc+1);
2209         jint size = STACK_INT(-1);
2210         CALL_VM(InterpreterRuntime::anewarray(THREAD, METHOD-&gt;constants(), index, size),
2211                 handle_exception);
2212         // Must prevent reordering of stores for object initialization
2213         // with stores that publish the new object.
2214         OrderAccess::storestore();
2215         SET_STACK_OBJECT(THREAD-&gt;vm_result(), -1);
2216         THREAD-&gt;set_vm_result(NULL);
2217         UPDATE_PC_AND_CONTINUE(3);
2218       }
2219       CASE(_multianewarray): {
2220         jint dims = *(pc+3);
2221         jint size = STACK_INT(-1);
2222         // stack grows down, dimensions are up!
2223         jint *dimarray =
2224                    (jint*)&amp;topOfStack[dims * Interpreter::stackElementWords+
2225                                       Interpreter::stackElementWords-1];
2226         //adjust pointer to start of stack element
2227         CALL_VM(InterpreterRuntime::multianewarray(THREAD, dimarray),
2228                 handle_exception);
2229         // Must prevent reordering of stores for object initialization
2230         // with stores that publish the new object.
2231         OrderAccess::storestore();
2232         SET_STACK_OBJECT(THREAD-&gt;vm_result(), -dims);
2233         THREAD-&gt;set_vm_result(NULL);
2234         UPDATE_PC_AND_TOS_AND_CONTINUE(4, -(dims-1));
2235       }
2236       CASE(_checkcast):
2237           if (STACK_OBJECT(-1) != NULL) {
2238             VERIFY_OOP(STACK_OBJECT(-1));
2239             u2 index = Bytes::get_Java_u2(pc+1);
2240             // Constant pool may have actual klass or unresolved klass. If it is
2241             // unresolved we must resolve it.
2242             if (METHOD-&gt;constants()-&gt;tag_at(index).is_unresolved_klass()) {
2243               CALL_VM(InterpreterRuntime::quicken_io_cc(THREAD), handle_exception);
2244             }
2245             Klass* klassOf = (Klass*) METHOD-&gt;constants()-&gt;resolved_klass_at(index);
2246             Klass* objKlass = STACK_OBJECT(-1)-&gt;klass(); // ebx
2247             //
2248             // Check for compatibilty. This check must not GC!!
2249             // Seems way more expensive now that we must dispatch.
2250             //
2251             if (objKlass != klassOf &amp;&amp; !objKlass-&gt;is_subtype_of(klassOf)) {
2252               // Decrement counter at checkcast.
2253               BI_PROFILE_SUBTYPECHECK_FAILED(objKlass);
2254               ResourceMark rm(THREAD);
2255               char* message = SharedRuntime::generate_class_cast_message(
2256                 objKlass, klassOf);
2257               VM_JAVA_ERROR(vmSymbols::java_lang_ClassCastException(), message, note_classCheck_trap);
2258             }
2259             // Profile checkcast with null_seen and receiver.
2260             BI_PROFILE_UPDATE_CHECKCAST(/*null_seen=*/false, objKlass);
2261           } else {
2262             // Profile checkcast with null_seen and receiver.
2263             BI_PROFILE_UPDATE_CHECKCAST(/*null_seen=*/true, NULL);
2264           }
2265           UPDATE_PC_AND_CONTINUE(3);
2266 
2267       CASE(_instanceof):
2268           if (STACK_OBJECT(-1) == NULL) {
2269             SET_STACK_INT(0, -1);
2270             // Profile instanceof with null_seen and receiver.
2271             BI_PROFILE_UPDATE_INSTANCEOF(/*null_seen=*/true, NULL);
2272           } else {
2273             VERIFY_OOP(STACK_OBJECT(-1));
2274             u2 index = Bytes::get_Java_u2(pc+1);
2275             // Constant pool may have actual klass or unresolved klass. If it is
2276             // unresolved we must resolve it.
2277             if (METHOD-&gt;constants()-&gt;tag_at(index).is_unresolved_klass()) {
2278               CALL_VM(InterpreterRuntime::quicken_io_cc(THREAD), handle_exception);
2279             }
2280             Klass* klassOf = (Klass*) METHOD-&gt;constants()-&gt;resolved_klass_at(index);
2281             Klass* objKlass = STACK_OBJECT(-1)-&gt;klass();
2282             //
2283             // Check for compatibilty. This check must not GC!!
2284             // Seems way more expensive now that we must dispatch.
2285             //
2286             if ( objKlass == klassOf || objKlass-&gt;is_subtype_of(klassOf)) {
2287               SET_STACK_INT(1, -1);
2288             } else {
2289               SET_STACK_INT(0, -1);
2290               // Decrement counter at checkcast.
2291               BI_PROFILE_SUBTYPECHECK_FAILED(objKlass);
2292             }
2293             // Profile instanceof with null_seen and receiver.
2294             BI_PROFILE_UPDATE_INSTANCEOF(/*null_seen=*/false, objKlass);
2295           }
2296           UPDATE_PC_AND_CONTINUE(3);
2297 
2298       CASE(_ldc_w):
2299       CASE(_ldc):
2300         {
2301           u2 index;
2302           bool wide = false;
2303           int incr = 2; // frequent case
2304           if (opcode == Bytecodes::_ldc) {
2305             index = pc[1];
2306           } else {
2307             index = Bytes::get_Java_u2(pc+1);
2308             incr = 3;
2309             wide = true;
2310           }
2311 
2312           ConstantPool* constants = METHOD-&gt;constants();
2313           switch (constants-&gt;tag_at(index).value()) {
2314           case JVM_CONSTANT_Integer:
2315             SET_STACK_INT(constants-&gt;int_at(index), 0);
2316             break;
2317 
2318           case JVM_CONSTANT_Float:
2319             SET_STACK_FLOAT(constants-&gt;float_at(index), 0);
2320             break;
2321 
2322           case JVM_CONSTANT_String:
2323             {
2324               oop result = constants-&gt;resolved_references()-&gt;obj_at(index);
2325               if (result == NULL) {
2326                 CALL_VM(InterpreterRuntime::resolve_ldc(THREAD, (Bytecodes::Code) opcode), handle_exception);
2327                 SET_STACK_OBJECT(THREAD-&gt;vm_result(), 0);
2328                 THREAD-&gt;set_vm_result(NULL);
2329               } else {
2330                 VERIFY_OOP(result);
2331                 SET_STACK_OBJECT(result, 0);
2332               }
2333             break;
2334             }
2335 
2336           case JVM_CONSTANT_Class:
2337             VERIFY_OOP(constants-&gt;resolved_klass_at(index)-&gt;java_mirror());
2338             SET_STACK_OBJECT(constants-&gt;resolved_klass_at(index)-&gt;java_mirror(), 0);
2339             break;
2340 
2341           case JVM_CONSTANT_UnresolvedClass:
2342           case JVM_CONSTANT_UnresolvedClassInError:
2343             CALL_VM(InterpreterRuntime::ldc(THREAD, wide), handle_exception);
2344             SET_STACK_OBJECT(THREAD-&gt;vm_result(), 0);
2345             THREAD-&gt;set_vm_result(NULL);
2346             break;
2347 
2348           case JVM_CONSTANT_Dynamic:
2349             {
2350               CALL_VM(InterpreterRuntime::resolve_ldc(THREAD, (Bytecodes::Code) opcode), handle_exception);
2351               oop result = THREAD-&gt;vm_result();
2352               VERIFY_OOP(result);
2353 
2354               jvalue value;
2355               BasicType type = java_lang_boxing_object::get_value(result, &amp;value);
2356               switch (type) {
2357               case T_FLOAT:   SET_STACK_FLOAT(value.f, 0); break;
2358               case T_INT:     SET_STACK_INT(value.i, 0); break;
2359               case T_SHORT:   SET_STACK_INT(value.s, 0); break;
2360               case T_BYTE:    SET_STACK_INT(value.b, 0); break;
2361               case T_CHAR:    SET_STACK_INT(value.c, 0); break;
2362               case T_BOOLEAN: SET_STACK_INT(value.z, 0); break;
2363               default:  ShouldNotReachHere();
2364               }
2365 
2366               break;
2367             }
2368 
2369           default:  ShouldNotReachHere();
2370           }
2371           UPDATE_PC_AND_TOS_AND_CONTINUE(incr, 1);
2372         }
2373 
2374       CASE(_ldc2_w):
2375         {
2376           u2 index = Bytes::get_Java_u2(pc+1);
2377 
2378           ConstantPool* constants = METHOD-&gt;constants();
2379           switch (constants-&gt;tag_at(index).value()) {
2380 
2381           case JVM_CONSTANT_Long:
2382              SET_STACK_LONG(constants-&gt;long_at(index), 1);
2383             break;
2384 
2385           case JVM_CONSTANT_Double:
2386              SET_STACK_DOUBLE(constants-&gt;double_at(index), 1);
2387             break;
2388 
2389           case JVM_CONSTANT_Dynamic:
2390             {
2391               CALL_VM(InterpreterRuntime::resolve_ldc(THREAD, (Bytecodes::Code) opcode), handle_exception);
2392               oop result = THREAD-&gt;vm_result();
2393               VERIFY_OOP(result);
2394 
2395               jvalue value;
2396               BasicType type = java_lang_boxing_object::get_value(result, &amp;value);
2397               switch (type) {
2398               case T_DOUBLE: SET_STACK_DOUBLE(value.d, 1); break;
2399               case T_LONG:   SET_STACK_LONG(value.j, 1); break;
2400               default:  ShouldNotReachHere();
2401               }
2402 
2403               break;
2404             }
2405 
2406           default:  ShouldNotReachHere();
2407           }
2408           UPDATE_PC_AND_TOS_AND_CONTINUE(3, 2);
2409         }
2410 
2411       CASE(_fast_aldc_w):
2412       CASE(_fast_aldc): {
2413         u2 index;
2414         int incr;
2415         if (opcode == Bytecodes::_fast_aldc) {
2416           index = pc[1];
2417           incr = 2;
2418         } else {
2419           index = Bytes::get_native_u2(pc+1);
2420           incr = 3;
2421         }
2422 
2423         // We are resolved if the resolved_references array contains a non-null object (CallSite, etc.)
2424         // This kind of CP cache entry does not need to match the flags byte, because
2425         // there is a 1-1 relation between bytecode type and CP entry type.
2426         ConstantPool* constants = METHOD-&gt;constants();
2427         oop result = constants-&gt;resolved_references()-&gt;obj_at(index);
2428         if (result == NULL) {
2429           CALL_VM(InterpreterRuntime::resolve_ldc(THREAD, (Bytecodes::Code) opcode),
2430                   handle_exception);
2431           result = THREAD-&gt;vm_result();
2432         }
2433         if (result == Universe::the_null_sentinel())
2434           result = NULL;
2435 
2436         VERIFY_OOP(result);
2437         SET_STACK_OBJECT(result, 0);
2438         UPDATE_PC_AND_TOS_AND_CONTINUE(incr, 1);
2439       }
2440 
2441       CASE(_invokedynamic): {
2442 
2443         u4 index = Bytes::get_native_u4(pc+1);
2444         ConstantPoolCacheEntry* cache = cp-&gt;constant_pool()-&gt;invokedynamic_cp_cache_entry_at(index);
2445 
2446         // We are resolved if the resolved_references array contains a non-null object (CallSite, etc.)
2447         // This kind of CP cache entry does not need to match the flags byte, because
2448         // there is a 1-1 relation between bytecode type and CP entry type.
2449         if (! cache-&gt;is_resolved((Bytecodes::Code) opcode)) {
2450           CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),
2451                   handle_exception);
2452           cache = cp-&gt;constant_pool()-&gt;invokedynamic_cp_cache_entry_at(index);
2453         }
2454 
2455         Method* method = cache-&gt;f1_as_method();
2456         if (VerifyOops) method-&gt;verify();
2457 
2458         if (cache-&gt;has_appendix()) {
2459           constantPoolHandle cp(THREAD, METHOD-&gt;constants());
2460           SET_STACK_OBJECT(cache-&gt;appendix_if_resolved(cp), 0);
2461           MORE_STACK(1);
2462         }
2463 
2464         istate-&gt;set_msg(call_method);
2465         istate-&gt;set_callee(method);
2466         istate-&gt;set_callee_entry_point(method-&gt;from_interpreted_entry());
2467         istate-&gt;set_bcp_advance(5);
2468 
2469         // Invokedynamic has got a call counter, just like an invokestatic -&gt; increment!
2470         BI_PROFILE_UPDATE_CALL();
2471 
2472         UPDATE_PC_AND_RETURN(0); // I&#39;ll be back...
2473       }
2474 
2475       CASE(_invokehandle): {
2476 
2477         u2 index = Bytes::get_native_u2(pc+1);
2478         ConstantPoolCacheEntry* cache = cp-&gt;entry_at(index);
2479 
2480         if (! cache-&gt;is_resolved((Bytecodes::Code) opcode)) {
2481           CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),
2482                   handle_exception);
2483           cache = cp-&gt;entry_at(index);
2484         }
2485 
2486         Method* method = cache-&gt;f1_as_method();
2487         if (VerifyOops) method-&gt;verify();
2488 
2489         if (cache-&gt;has_appendix()) {
2490           constantPoolHandle cp(THREAD, METHOD-&gt;constants());
2491           SET_STACK_OBJECT(cache-&gt;appendix_if_resolved(cp), 0);
2492           MORE_STACK(1);
2493         }
2494 
2495         istate-&gt;set_msg(call_method);
2496         istate-&gt;set_callee(method);
2497         istate-&gt;set_callee_entry_point(method-&gt;from_interpreted_entry());
2498         istate-&gt;set_bcp_advance(3);
2499 
2500         // Invokehandle has got a call counter, just like a final call -&gt; increment!
2501         BI_PROFILE_UPDATE_FINALCALL();
2502 
2503         UPDATE_PC_AND_RETURN(0); // I&#39;ll be back...
2504       }
2505 
2506       CASE(_invokeinterface): {
2507         u2 index = Bytes::get_native_u2(pc+1);
2508 
2509         // QQQ Need to make this as inlined as possible. Probably need to split all the bytecode cases
2510         // out so c++ compiler has a chance for constant prop to fold everything possible away.
2511 
2512         ConstantPoolCacheEntry* cache = cp-&gt;entry_at(index);
2513         if (!cache-&gt;is_resolved((Bytecodes::Code)opcode)) {
2514           CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),
2515                   handle_exception);
2516           cache = cp-&gt;entry_at(index);
2517         }
2518 
2519         istate-&gt;set_msg(call_method);
2520 
2521         // Special case of invokeinterface called for virtual method of
2522         // java.lang.Object.  See cpCache.cpp for details.
2523         Method* callee = NULL;
2524         if (cache-&gt;is_forced_virtual()) {
2525           CHECK_NULL(STACK_OBJECT(-(cache-&gt;parameter_size())));
2526           if (cache-&gt;is_vfinal()) {
2527             callee = cache-&gt;f2_as_vfinal_method();
2528             // Profile &#39;special case of invokeinterface&#39; final call.
2529             BI_PROFILE_UPDATE_FINALCALL();
2530           } else {
2531             // Get receiver.
2532             int parms = cache-&gt;parameter_size();
2533             // Same comments as invokevirtual apply here.
2534             oop rcvr = STACK_OBJECT(-parms);
2535             VERIFY_OOP(rcvr);
2536             Klass* rcvrKlass = rcvr-&gt;klass();
2537             callee = (Method*) rcvrKlass-&gt;method_at_vtable(cache-&gt;f2_as_index());
2538             // Profile &#39;special case of invokeinterface&#39; virtual call.
2539             BI_PROFILE_UPDATE_VIRTUALCALL(rcvrKlass);
2540           }
2541         } else if (cache-&gt;is_vfinal()) {
2542           // private interface method invocations
2543           //
2544           // Ensure receiver class actually implements
2545           // the resolved interface class. The link resolver
2546           // does this, but only for the first time this
2547           // interface is being called.
2548           int parms = cache-&gt;parameter_size();
2549           oop rcvr = STACK_OBJECT(-parms);
2550           CHECK_NULL(rcvr);
2551           Klass* recv_klass = rcvr-&gt;klass();
2552           Klass* resolved_klass = cache-&gt;f1_as_klass();
2553           if (!recv_klass-&gt;is_subtype_of(resolved_klass)) {
2554             ResourceMark rm(THREAD);
2555             char buf[200];
2556             jio_snprintf(buf, sizeof(buf), &quot;Class %s does not implement the requested interface %s&quot;,
2557               recv_klass-&gt;external_name(),
2558               resolved_klass-&gt;external_name());
2559             VM_JAVA_ERROR(vmSymbols::java_lang_IncompatibleClassChangeError(), buf, note_no_trap);
2560           }
2561           callee = cache-&gt;f2_as_vfinal_method();
2562         }
2563         if (callee != NULL) {
2564           istate-&gt;set_callee(callee);
2565           istate-&gt;set_callee_entry_point(callee-&gt;from_interpreted_entry());
2566 #ifdef VM_JVMTI
2567           if (JvmtiExport::can_post_interpreter_events() &amp;&amp; THREAD-&gt;is_interp_only_mode()) {
2568             istate-&gt;set_callee_entry_point(callee-&gt;interpreter_entry());
2569           }
2570 #endif /* VM_JVMTI */
2571           istate-&gt;set_bcp_advance(5);
2572           UPDATE_PC_AND_RETURN(0); // I&#39;ll be back...
2573         }
2574 
2575         // this could definitely be cleaned up QQQ
2576         Method *interface_method = cache-&gt;f2_as_interface_method();
2577         InstanceKlass* iclass = interface_method-&gt;method_holder();
2578 
2579         // get receiver
2580         int parms = cache-&gt;parameter_size();
2581         oop rcvr = STACK_OBJECT(-parms);
2582         CHECK_NULL(rcvr);
2583         InstanceKlass* int2 = (InstanceKlass*) rcvr-&gt;klass();
2584 
2585         // Receiver subtype check against resolved interface klass (REFC).
2586         {
2587           Klass* refc = cache-&gt;f1_as_klass();
2588           itableOffsetEntry* scan;
2589           for (scan = (itableOffsetEntry*) int2-&gt;start_of_itable();
2590                scan-&gt;interface_klass() != NULL;
2591                scan++) {
2592             if (scan-&gt;interface_klass() == refc) {
2593               break;
2594             }
2595           }
2596           // Check that the entry is non-null.  A null entry means
2597           // that the receiver class doesn&#39;t implement the
2598           // interface, and wasn&#39;t the same as when the caller was
2599           // compiled.
2600           if (scan-&gt;interface_klass() == NULL) {
2601             VM_JAVA_ERROR(vmSymbols::java_lang_IncompatibleClassChangeError(), &quot;&quot;, note_no_trap);
2602           }
2603         }
2604 
2605         itableOffsetEntry* ki = (itableOffsetEntry*) int2-&gt;start_of_itable();
2606         int i;
2607         for ( i = 0 ; i &lt; int2-&gt;itable_length() ; i++, ki++ ) {
2608           if (ki-&gt;interface_klass() == iclass) break;
2609         }
2610         // If the interface isn&#39;t found, this class doesn&#39;t implement this
2611         // interface. The link resolver checks this but only for the first
2612         // time this interface is called.
2613         if (i == int2-&gt;itable_length()) {
2614           CALL_VM(InterpreterRuntime::throw_IncompatibleClassChangeErrorVerbose(THREAD, rcvr-&gt;klass(), iclass),
2615                   handle_exception);
2616         }
2617         int mindex = interface_method-&gt;itable_index();
2618 
2619         itableMethodEntry* im = ki-&gt;first_method_entry(rcvr-&gt;klass());
2620         callee = im[mindex].method();
2621         if (callee == NULL) {
2622           CALL_VM(InterpreterRuntime::throw_AbstractMethodErrorVerbose(THREAD, rcvr-&gt;klass(), interface_method),
2623                   handle_exception);
2624         }
2625 
2626         // Profile virtual call.
2627         BI_PROFILE_UPDATE_VIRTUALCALL(rcvr-&gt;klass());
2628 
2629         istate-&gt;set_callee(callee);
2630         istate-&gt;set_callee_entry_point(callee-&gt;from_interpreted_entry());
2631 #ifdef VM_JVMTI
2632         if (JvmtiExport::can_post_interpreter_events() &amp;&amp; THREAD-&gt;is_interp_only_mode()) {
2633           istate-&gt;set_callee_entry_point(callee-&gt;interpreter_entry());
2634         }
2635 #endif /* VM_JVMTI */
2636         istate-&gt;set_bcp_advance(5);
2637         UPDATE_PC_AND_RETURN(0); // I&#39;ll be back...
2638       }
2639 
2640       CASE(_invokevirtual):
2641       CASE(_invokespecial):
2642       CASE(_invokestatic): {
2643         u2 index = Bytes::get_native_u2(pc+1);
2644 
2645         ConstantPoolCacheEntry* cache = cp-&gt;entry_at(index);
2646         // QQQ Need to make this as inlined as possible. Probably need to split all the bytecode cases
2647         // out so c++ compiler has a chance for constant prop to fold everything possible away.
2648 
2649         if (!cache-&gt;is_resolved((Bytecodes::Code)opcode)) {
2650           CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),
2651                   handle_exception);
2652           cache = cp-&gt;entry_at(index);
2653         }
2654 
2655         istate-&gt;set_msg(call_method);
2656         {
2657           Method* callee;
2658           if ((Bytecodes::Code)opcode == Bytecodes::_invokevirtual) {
2659             CHECK_NULL(STACK_OBJECT(-(cache-&gt;parameter_size())));
2660             if (cache-&gt;is_vfinal()) {
2661               callee = cache-&gt;f2_as_vfinal_method();
2662               // Profile final call.
2663               BI_PROFILE_UPDATE_FINALCALL();
2664             } else {
2665               // get receiver
2666               int parms = cache-&gt;parameter_size();
2667               // this works but needs a resourcemark and seems to create a vtable on every call:
2668               // Method* callee = rcvr-&gt;klass()-&gt;vtable()-&gt;method_at(cache-&gt;f2_as_index());
2669               //
2670               // this fails with an assert
2671               // InstanceKlass* rcvrKlass = InstanceKlass::cast(STACK_OBJECT(-parms)-&gt;klass());
2672               // but this works
2673               oop rcvr = STACK_OBJECT(-parms);
2674               VERIFY_OOP(rcvr);
2675               Klass* rcvrKlass = rcvr-&gt;klass();
2676               /*
2677                 Executing this code in java.lang.String:
2678                     public String(char value[]) {
2679                           this.count = value.length;
2680                           this.value = (char[])value.clone();
2681                      }
2682 
2683                  a find on rcvr-&gt;klass() reports:
2684                  {type array char}{type array class}
2685                   - klass: {other class}
2686 
2687                   but using InstanceKlass::cast(STACK_OBJECT(-parms)-&gt;klass()) causes in assertion failure
2688                   because rcvr-&gt;klass()-&gt;is_instance_klass() == 0
2689                   However it seems to have a vtable in the right location. Huh?
2690                   Because vtables have the same offset for ArrayKlass and InstanceKlass.
2691               */
2692               callee = (Method*) rcvrKlass-&gt;method_at_vtable(cache-&gt;f2_as_index());
2693               // Profile virtual call.
2694               BI_PROFILE_UPDATE_VIRTUALCALL(rcvrKlass);
2695             }
2696           } else {
2697             if ((Bytecodes::Code)opcode == Bytecodes::_invokespecial) {
2698               CHECK_NULL(STACK_OBJECT(-(cache-&gt;parameter_size())));
2699             }
2700             callee = cache-&gt;f1_as_method();
2701 
2702             // Profile call.
2703             BI_PROFILE_UPDATE_CALL();
2704           }
2705 
2706           istate-&gt;set_callee(callee);
2707           istate-&gt;set_callee_entry_point(callee-&gt;from_interpreted_entry());
2708 #ifdef VM_JVMTI
2709           if (JvmtiExport::can_post_interpreter_events() &amp;&amp; THREAD-&gt;is_interp_only_mode()) {
2710             istate-&gt;set_callee_entry_point(callee-&gt;interpreter_entry());
2711           }
2712 #endif /* VM_JVMTI */
2713           istate-&gt;set_bcp_advance(3);
2714           UPDATE_PC_AND_RETURN(0); // I&#39;ll be back...
2715         }
2716       }
2717 
2718       /* Allocate memory for a new java object. */
2719 
2720       CASE(_newarray): {
2721         BasicType atype = (BasicType) *(pc+1);
2722         jint size = STACK_INT(-1);
2723         CALL_VM(InterpreterRuntime::newarray(THREAD, atype, size),
2724                 handle_exception);
2725         // Must prevent reordering of stores for object initialization
2726         // with stores that publish the new object.
2727         OrderAccess::storestore();
2728         SET_STACK_OBJECT(THREAD-&gt;vm_result(), -1);
2729         THREAD-&gt;set_vm_result(NULL);
2730 
2731         UPDATE_PC_AND_CONTINUE(2);
2732       }
2733 
2734       /* Throw an exception. */
2735 
2736       CASE(_athrow): {
2737           oop except_oop = STACK_OBJECT(-1);
2738           CHECK_NULL(except_oop);
2739           // set pending_exception so we use common code
2740           THREAD-&gt;set_pending_exception(except_oop, NULL, 0);
2741           goto handle_exception;
2742       }
2743 
2744       /* goto and jsr. They are exactly the same except jsr pushes
2745        * the address of the next instruction first.
2746        */
2747 
2748       CASE(_jsr): {
2749           /* push bytecode index on stack */
2750           SET_STACK_ADDR(((address)pc - (intptr_t)(istate-&gt;method()-&gt;code_base()) + 3), 0);
2751           MORE_STACK(1);
2752           /* FALL THROUGH */
2753       }
2754 
2755       CASE(_goto):
2756       {
2757           int16_t offset = (int16_t)Bytes::get_Java_u2(pc + 1);
2758           // Profile jump.
2759           BI_PROFILE_UPDATE_JUMP();
2760           address branch_pc = pc;
2761           UPDATE_PC(offset);
2762           DO_BACKEDGE_CHECKS(offset, branch_pc);
2763           CONTINUE;
2764       }
2765 
2766       CASE(_jsr_w): {
2767           /* push return address on the stack */
2768           SET_STACK_ADDR(((address)pc - (intptr_t)(istate-&gt;method()-&gt;code_base()) + 5), 0);
2769           MORE_STACK(1);
2770           /* FALL THROUGH */
2771       }
2772 
2773       CASE(_goto_w):
2774       {
2775           int32_t offset = Bytes::get_Java_u4(pc + 1);
2776           // Profile jump.
2777           BI_PROFILE_UPDATE_JUMP();
2778           address branch_pc = pc;
2779           UPDATE_PC(offset);
2780           DO_BACKEDGE_CHECKS(offset, branch_pc);
2781           CONTINUE;
2782       }
2783 
2784       /* return from a jsr or jsr_w */
2785 
2786       CASE(_ret): {
2787           // Profile ret.
2788           BI_PROFILE_UPDATE_RET(/*bci=*/((int)(intptr_t)(LOCALS_ADDR(pc[1]))));
2789           // Now, update the pc.
2790           pc = istate-&gt;method()-&gt;code_base() + (intptr_t)(LOCALS_ADDR(pc[1]));
2791           UPDATE_PC_AND_CONTINUE(0);
2792       }
2793 
2794       /* debugger breakpoint */
2795 
2796       CASE(_breakpoint): {
2797           Bytecodes::Code original_bytecode;
2798           DECACHE_STATE();
2799           SET_LAST_JAVA_FRAME();
2800           original_bytecode = InterpreterRuntime::get_original_bytecode_at(THREAD,
2801                               METHOD, pc);
2802           RESET_LAST_JAVA_FRAME();
2803           CACHE_STATE();
2804           if (THREAD-&gt;has_pending_exception()) goto handle_exception;
2805             CALL_VM(InterpreterRuntime::_breakpoint(THREAD, METHOD, pc),
2806                                                     handle_exception);
2807 
2808           opcode = (jubyte)original_bytecode;
2809           goto opcode_switch;
2810       }
2811 
2812       DEFAULT:
2813           fatal(&quot;Unimplemented opcode %d = %s&quot;, opcode,
2814                 Bytecodes::name((Bytecodes::Code)opcode));
2815           goto finish;
2816 
2817       } /* switch(opc) */
2818 
2819 
2820 #ifdef USELABELS
2821     check_for_exception:
2822 #endif
2823     {
2824       if (!THREAD-&gt;has_pending_exception()) {
2825         CONTINUE;
2826       }
2827       /* We will be gcsafe soon, so flush our state. */
2828       DECACHE_PC();
2829       goto handle_exception;
2830     }
2831   do_continue: ;
2832 
2833   } /* while (1) interpreter loop */
2834 
2835 
2836   // An exception exists in the thread state see whether this activation can handle it
2837   handle_exception: {
2838 
2839     HandleMarkCleaner __hmc(THREAD);
2840     Handle except_oop(THREAD, THREAD-&gt;pending_exception());
2841     // Prevent any subsequent HandleMarkCleaner in the VM
2842     // from freeing the except_oop handle.
2843     HandleMark __hm(THREAD);
2844 
2845     THREAD-&gt;clear_pending_exception();
2846     assert(except_oop() != NULL, &quot;No exception to process&quot;);
2847     intptr_t continuation_bci;
2848     // expression stack is emptied
2849     topOfStack = istate-&gt;stack_base() - Interpreter::stackElementWords;
2850     CALL_VM(continuation_bci = (intptr_t)InterpreterRuntime::exception_handler_for_exception(THREAD, except_oop()),
2851             handle_exception);
2852 
2853     except_oop = Handle(THREAD, THREAD-&gt;vm_result());
2854     THREAD-&gt;set_vm_result(NULL);
2855     if (continuation_bci &gt;= 0) {
2856       // Place exception on top of stack
2857       SET_STACK_OBJECT(except_oop(), 0);
2858       MORE_STACK(1);
2859       pc = METHOD-&gt;code_base() + continuation_bci;
2860       if (log_is_enabled(Info, exceptions)) {
2861         ResourceMark rm(THREAD);
2862         stringStream tempst;
2863         tempst.print(&quot;interpreter method &lt;%s&gt;\n&quot;
2864                      &quot; at bci %d, continuing at %d for thread &quot; INTPTR_FORMAT,
2865                      METHOD-&gt;print_value_string(),
2866                      (int)(istate-&gt;bcp() - METHOD-&gt;code_base()),
2867                      (int)continuation_bci, p2i(THREAD));
2868         Exceptions::log_exception(except_oop, tempst.as_string());
2869       }
2870       // for AbortVMOnException flag
2871       Exceptions::debug_check_abort(except_oop);
2872 
2873       // Update profiling data.
2874       BI_PROFILE_ALIGN_TO_CURRENT_BCI();
2875       goto run;
2876     }
2877     if (log_is_enabled(Info, exceptions)) {
2878       ResourceMark rm;
2879       stringStream tempst;
2880       tempst.print(&quot;interpreter method &lt;%s&gt;\n&quot;
2881              &quot; at bci %d, unwinding for thread &quot; INTPTR_FORMAT,
2882              METHOD-&gt;print_value_string(),
2883              (int)(istate-&gt;bcp() - METHOD-&gt;code_base()),
2884              p2i(THREAD));
2885       Exceptions::log_exception(except_oop, tempst.as_string());
2886     }
2887     // for AbortVMOnException flag
2888     Exceptions::debug_check_abort(except_oop);
2889 
2890     // No handler in this activation, unwind and try again
2891     THREAD-&gt;set_pending_exception(except_oop(), NULL, 0);
2892     goto handle_return;
2893   }  // handle_exception:
2894 
2895   // Return from an interpreter invocation with the result of the interpretation
2896   // on the top of the Java Stack (or a pending exception)
2897 
2898   handle_Pop_Frame: {
2899 
2900     // We don&#39;t really do anything special here except we must be aware
2901     // that we can get here without ever locking the method (if sync).
2902     // Also we skip the notification of the exit.
2903 
2904     istate-&gt;set_msg(popping_frame);
2905     // Clear pending so while the pop is in process
2906     // we don&#39;t start another one if a call_vm is done.
2907     THREAD-&gt;clr_pop_frame_pending();
2908     // Let interpreter (only) see the we&#39;re in the process of popping a frame
2909     THREAD-&gt;set_pop_frame_in_process();
2910 
2911     goto handle_return;
2912 
2913   } // handle_Pop_Frame
2914 
2915   // ForceEarlyReturn ends a method, and returns to the caller with a return value
2916   // given by the invoker of the early return.
2917   handle_Early_Return: {
2918 
2919     istate-&gt;set_msg(early_return);
2920 
2921     // Clear expression stack.
2922     topOfStack = istate-&gt;stack_base() - Interpreter::stackElementWords;
2923 
2924     JvmtiThreadState *ts = THREAD-&gt;jvmti_thread_state();
2925 
2926     // Push the value to be returned.
2927     switch (istate-&gt;method()-&gt;result_type()) {
2928       case T_BOOLEAN:
2929       case T_SHORT:
2930       case T_BYTE:
2931       case T_CHAR:
2932       case T_INT:
2933         SET_STACK_INT(ts-&gt;earlyret_value().i, 0);
2934         MORE_STACK(1);
2935         break;
2936       case T_LONG:
2937         SET_STACK_LONG(ts-&gt;earlyret_value().j, 1);
2938         MORE_STACK(2);
2939         break;
2940       case T_FLOAT:
2941         SET_STACK_FLOAT(ts-&gt;earlyret_value().f, 0);
2942         MORE_STACK(1);
2943         break;
2944       case T_DOUBLE:
2945         SET_STACK_DOUBLE(ts-&gt;earlyret_value().d, 1);
2946         MORE_STACK(2);
2947         break;
2948       case T_ARRAY:
2949       case T_OBJECT:
2950         SET_STACK_OBJECT(ts-&gt;earlyret_oop(), 0);
2951         MORE_STACK(1);
2952         break;
2953     }
2954 
2955     ts-&gt;clr_earlyret_value();
2956     ts-&gt;set_earlyret_oop(NULL);
2957     ts-&gt;clr_earlyret_pending();
2958 
2959     // Fall through to handle_return.
2960 
2961   } // handle_Early_Return
2962 
2963   handle_return: {
2964     // A storestore barrier is required to order initialization of
2965     // final fields with publishing the reference to the object that
2966     // holds the field. Without the barrier the value of final fields
2967     // can be observed to change.
2968     OrderAccess::storestore();
2969 
2970     DECACHE_STATE();
2971 
2972     bool suppress_error = istate-&gt;msg() == popping_frame || istate-&gt;msg() == early_return;
2973     bool suppress_exit_event = THREAD-&gt;has_pending_exception() || istate-&gt;msg() == popping_frame;
2974     Handle original_exception(THREAD, THREAD-&gt;pending_exception());
2975     Handle illegal_state_oop(THREAD, NULL);
2976 
2977     // We&#39;d like a HandleMark here to prevent any subsequent HandleMarkCleaner
2978     // in any following VM entries from freeing our live handles, but illegal_state_oop
2979     // isn&#39;t really allocated yet and so doesn&#39;t become live until later and
2980     // in unpredicatable places. Instead we must protect the places where we enter the
2981     // VM. It would be much simpler (and safer) if we could allocate a real handle with
2982     // a NULL oop in it and then overwrite the oop later as needed. This isn&#39;t
2983     // unfortunately isn&#39;t possible.
2984 
2985     THREAD-&gt;clear_pending_exception();
2986 
2987     //
2988     // As far as we are concerned we have returned. If we have a pending exception
2989     // that will be returned as this invocation&#39;s result. However if we get any
2990     // exception(s) while checking monitor state one of those IllegalMonitorStateExceptions
2991     // will be our final result (i.e. monitor exception trumps a pending exception).
2992     //
2993 
2994     // If we never locked the method (or really passed the point where we would have),
2995     // there is no need to unlock it (or look for other monitors), since that
2996     // could not have happened.
2997 
2998     if (THREAD-&gt;do_not_unlock()) {
2999 
3000       // Never locked, reset the flag now because obviously any caller must
3001       // have passed their point of locking for us to have gotten here.
3002 
3003       THREAD-&gt;clr_do_not_unlock();
3004     } else {
3005       // At this point we consider that we have returned. We now check that the
3006       // locks were properly block structured. If we find that they were not
3007       // used properly we will return with an illegal monitor exception.
3008       // The exception is checked by the caller not the callee since this
3009       // checking is considered to be part of the invocation and therefore
3010       // in the callers scope (JVM spec 8.13).
3011       //
3012       // Another weird thing to watch for is if the method was locked
3013       // recursively and then not exited properly. This means we must
3014       // examine all the entries in reverse time(and stack) order and
3015       // unlock as we find them. If we find the method monitor before
3016       // we are at the initial entry then we should throw an exception.
3017       // It is not clear the template based interpreter does this
3018       // correctly
3019 
3020       BasicObjectLock* base = istate-&gt;monitor_base();
3021       BasicObjectLock* end = (BasicObjectLock*) istate-&gt;stack_base();
3022       bool method_unlock_needed = METHOD-&gt;is_synchronized();
3023       // We know the initial monitor was used for the method don&#39;t check that
3024       // slot in the loop
3025       if (method_unlock_needed) base--;
3026 
3027       // Check all the monitors to see they are unlocked. Install exception if found to be locked.
3028       while (end &lt; base) {
3029         oop lockee = end-&gt;obj();
3030         if (lockee != NULL) {
3031           BasicLock* lock = end-&gt;lock();
3032           markWord header = lock-&gt;displaced_header();
3033           end-&gt;set_obj(NULL);
3034 
3035           if (!lockee-&gt;mark().has_bias_pattern()) {
3036             // If it isn&#39;t recursive we either must swap old header or call the runtime
3037             if (header.to_pointer() != NULL) {
3038               markWord old_header = markWord::encode(lock);
3039               if (lockee-&gt;cas_set_mark(header, old_header) != old_header) {
3040                 // restore object for the slow case
3041                 end-&gt;set_obj(lockee);
3042                 {
3043                   // Prevent any HandleMarkCleaner from freeing our live handles
3044                   HandleMark __hm(THREAD);
3045                   CALL_VM_NOCHECK(InterpreterRuntime::monitorexit(THREAD, end));
3046                 }
3047               }
3048             }
3049           }
3050           // One error is plenty
3051           if (illegal_state_oop() == NULL &amp;&amp; !suppress_error) {
3052             {
3053               // Prevent any HandleMarkCleaner from freeing our live handles
3054               HandleMark __hm(THREAD);
3055               CALL_VM_NOCHECK(InterpreterRuntime::throw_illegal_monitor_state_exception(THREAD));
3056             }
3057             assert(THREAD-&gt;has_pending_exception(), &quot;Lost our exception!&quot;);
3058             illegal_state_oop = Handle(THREAD, THREAD-&gt;pending_exception());
3059             THREAD-&gt;clear_pending_exception();
3060           }
3061         }
3062         end++;
3063       }
3064       // Unlock the method if needed
3065       if (method_unlock_needed) {
3066         if (base-&gt;obj() == NULL) {
3067           // The method is already unlocked this is not good.
3068           if (illegal_state_oop() == NULL &amp;&amp; !suppress_error) {
3069             {
3070               // Prevent any HandleMarkCleaner from freeing our live handles
3071               HandleMark __hm(THREAD);
3072               CALL_VM_NOCHECK(InterpreterRuntime::throw_illegal_monitor_state_exception(THREAD));
3073             }
3074             assert(THREAD-&gt;has_pending_exception(), &quot;Lost our exception!&quot;);
3075             illegal_state_oop = Handle(THREAD, THREAD-&gt;pending_exception());
3076             THREAD-&gt;clear_pending_exception();
3077           }
3078         } else {
3079           //
3080           // The initial monitor is always used for the method
3081           // However if that slot is no longer the oop for the method it was unlocked
3082           // and reused by something that wasn&#39;t unlocked!
3083           //
3084           // deopt can come in with rcvr dead because c2 knows
3085           // its value is preserved in the monitor. So we can&#39;t use locals[0] at all
3086           // and must use first monitor slot.
3087           //
3088           oop rcvr = base-&gt;obj();
3089           if (rcvr == NULL) {
3090             if (!suppress_error) {
3091               VM_JAVA_ERROR_NO_JUMP(vmSymbols::java_lang_NullPointerException(), &quot;&quot;, note_nullCheck_trap);
3092               illegal_state_oop = Handle(THREAD, THREAD-&gt;pending_exception());
3093               THREAD-&gt;clear_pending_exception();
3094             }
3095           } else if (UseHeavyMonitors) {
3096             {
3097               // Prevent any HandleMarkCleaner from freeing our live handles.
3098               HandleMark __hm(THREAD);
3099               CALL_VM_NOCHECK(InterpreterRuntime::monitorexit(THREAD, base));
3100             }
3101             if (THREAD-&gt;has_pending_exception()) {
3102               if (!suppress_error) illegal_state_oop = Handle(THREAD, THREAD-&gt;pending_exception());
3103               THREAD-&gt;clear_pending_exception();
3104             }
3105           } else {
3106             BasicLock* lock = base-&gt;lock();
3107             markWord header = lock-&gt;displaced_header();
3108             base-&gt;set_obj(NULL);
3109 
3110             if (!rcvr-&gt;mark().has_bias_pattern()) {
3111               base-&gt;set_obj(NULL);
3112               // If it isn&#39;t recursive we either must swap old header or call the runtime
3113               if (header.to_pointer() != NULL) {
3114                 markWord old_header = markWord::encode(lock);
3115                 if (rcvr-&gt;cas_set_mark(header, old_header) != old_header) {
3116                   // restore object for the slow case
3117                   base-&gt;set_obj(rcvr);
3118                   {
3119                     // Prevent any HandleMarkCleaner from freeing our live handles
3120                     HandleMark __hm(THREAD);
3121                     CALL_VM_NOCHECK(InterpreterRuntime::monitorexit(THREAD, base));
3122                   }
3123                   if (THREAD-&gt;has_pending_exception()) {
3124                     if (!suppress_error) illegal_state_oop = Handle(THREAD, THREAD-&gt;pending_exception());
3125                     THREAD-&gt;clear_pending_exception();
3126                   }
3127                 }
3128               }
3129             }
3130           }
3131         }
3132       }
3133     }
3134     // Clear the do_not_unlock flag now.
3135     THREAD-&gt;clr_do_not_unlock();
3136 
3137     //
3138     // Notify jvmti/jvmdi
3139     //
3140     // NOTE: we do not notify a method_exit if we have a pending exception,
3141     // including an exception we generate for unlocking checks.  In the former
3142     // case, JVMDI has already been notified by our call for the exception handler
3143     // and in both cases as far as JVMDI is concerned we have already returned.
3144     // If we notify it again JVMDI will be all confused about how many frames
3145     // are still on the stack (4340444).
3146     //
3147     // NOTE Further! It turns out the the JVMTI spec in fact expects to see
3148     // method_exit events whenever we leave an activation unless it was done
3149     // for popframe. This is nothing like jvmdi. However we are passing the
3150     // tests at the moment (apparently because they are jvmdi based) so rather
3151     // than change this code and possibly fail tests we will leave it alone
3152     // (with this note) in anticipation of changing the vm and the tests
3153     // simultaneously.
3154 
3155 
3156     //
3157     suppress_exit_event = suppress_exit_event || illegal_state_oop() != NULL;
3158 
3159 
3160 
3161 #ifdef VM_JVMTI
3162       if (_jvmti_interp_events) {
3163         // Whenever JVMTI puts a thread in interp_only_mode, method
3164         // entry/exit events are sent for that thread to track stack depth.
3165         if ( !suppress_exit_event &amp;&amp; THREAD-&gt;is_interp_only_mode() ) {
3166           {
3167             // Prevent any HandleMarkCleaner from freeing our live handles
3168             HandleMark __hm(THREAD);
3169             CALL_VM_NOCHECK(InterpreterRuntime::post_method_exit(THREAD));
3170           }
3171         }
3172       }
3173 #endif /* VM_JVMTI */
3174 
3175     //
3176     // See if we are returning any exception
3177     // A pending exception that was pending prior to a possible popping frame
3178     // overrides the popping frame.
3179     //
3180     assert(!suppress_error || (suppress_error &amp;&amp; illegal_state_oop() == NULL), &quot;Error was not suppressed&quot;);
3181     if (illegal_state_oop() != NULL || original_exception() != NULL) {
3182       // Inform the frame manager we have no result.
3183       istate-&gt;set_msg(throwing_exception);
3184       if (illegal_state_oop() != NULL)
3185         THREAD-&gt;set_pending_exception(illegal_state_oop(), NULL, 0);
3186       else
3187         THREAD-&gt;set_pending_exception(original_exception(), NULL, 0);
3188       UPDATE_PC_AND_RETURN(0);
3189     }
3190 
3191     if (istate-&gt;msg() == popping_frame) {
3192       // Make it simpler on the assembly code and set the message for the frame pop.
3193       // returns
3194       if (istate-&gt;prev() == NULL) {
3195         // We must be returning to a deoptimized frame (because popframe only happens between
3196         // two interpreted frames). We need to save the current arguments in C heap so that
3197         // the deoptimized frame when it restarts can copy the arguments to its expression
3198         // stack and re-execute the call. We also have to notify deoptimization that this
3199         // has occurred and to pick the preserved args copy them to the deoptimized frame&#39;s
3200         // java expression stack. Yuck.
3201         //
3202         THREAD-&gt;popframe_preserve_args(in_ByteSize(METHOD-&gt;size_of_parameters() * wordSize),
3203                                 LOCALS_SLOT(METHOD-&gt;size_of_parameters() - 1));
3204         THREAD-&gt;set_popframe_condition_bit(JavaThread::popframe_force_deopt_reexecution_bit);
3205       }
3206     } else {
3207       istate-&gt;set_msg(return_from_method);
3208     }
3209 
3210     // Normal return
3211     // Advance the pc and return to frame manager
3212     UPDATE_PC_AND_RETURN(1);
3213   } /* handle_return: */
3214 
3215 // This is really a fatal error return
3216 
3217 finish:
3218   DECACHE_TOS();
3219   DECACHE_PC();
3220 
3221   return;
3222 }
3223 
3224 /*
3225  * All the code following this point is only produced once and is not present
3226  * in the JVMTI version of the interpreter
3227 */
3228 
3229 #ifndef VM_JVMTI
3230 
3231 // This constructor should only be used to contruct the object to signal
3232 // interpreter initialization. All other instances should be created by
3233 // the frame manager.
3234 BytecodeInterpreter::BytecodeInterpreter(messages msg) {
3235   if (msg != initialize) ShouldNotReachHere();
3236   _msg = msg;
3237   _self_link = this;
3238   _prev_link = NULL;
3239 }
3240 
3241 // Inline static functions for Java Stack and Local manipulation
3242 
3243 // The implementations are platform dependent. We have to worry about alignment
3244 // issues on some machines which can change on the same platform depending on
3245 // whether it is an LP64 machine also.
3246 address BytecodeInterpreter::stack_slot(intptr_t *tos, int offset) {
3247   return (address) tos[Interpreter::expr_index_at(-offset)];
3248 }
3249 
3250 jint BytecodeInterpreter::stack_int(intptr_t *tos, int offset) {
3251   return *((jint*) &amp;tos[Interpreter::expr_index_at(-offset)]);
3252 }
3253 
3254 jfloat BytecodeInterpreter::stack_float(intptr_t *tos, int offset) {
3255   return *((jfloat *) &amp;tos[Interpreter::expr_index_at(-offset)]);
3256 }
3257 
3258 oop BytecodeInterpreter::stack_object(intptr_t *tos, int offset) {
3259   return cast_to_oop(tos [Interpreter::expr_index_at(-offset)]);
3260 }
3261 
3262 jdouble BytecodeInterpreter::stack_double(intptr_t *tos, int offset) {
3263   return ((VMJavaVal64*) &amp;tos[Interpreter::expr_index_at(-offset)])-&gt;d;
3264 }
3265 
3266 jlong BytecodeInterpreter::stack_long(intptr_t *tos, int offset) {
3267   return ((VMJavaVal64 *) &amp;tos[Interpreter::expr_index_at(-offset)])-&gt;l;
3268 }
3269 
3270 // only used for value types
3271 void BytecodeInterpreter::set_stack_slot(intptr_t *tos, address value,
3272                                                         int offset) {
3273   *((address *)&amp;tos[Interpreter::expr_index_at(-offset)]) = value;
3274 }
3275 
3276 void BytecodeInterpreter::set_stack_int(intptr_t *tos, int value,
3277                                                        int offset) {
3278   *((jint *)&amp;tos[Interpreter::expr_index_at(-offset)]) = value;
3279 }
3280 
3281 void BytecodeInterpreter::set_stack_float(intptr_t *tos, jfloat value,
3282                                                          int offset) {
3283   *((jfloat *)&amp;tos[Interpreter::expr_index_at(-offset)]) = value;
3284 }
3285 
3286 void BytecodeInterpreter::set_stack_object(intptr_t *tos, oop value,
3287                                                           int offset) {
3288   *((oop *)&amp;tos[Interpreter::expr_index_at(-offset)]) = value;
3289 }
3290 
3291 // needs to be platform dep for the 32 bit platforms.
3292 void BytecodeInterpreter::set_stack_double(intptr_t *tos, jdouble value,
3293                                                           int offset) {
3294   ((VMJavaVal64*)&amp;tos[Interpreter::expr_index_at(-offset)])-&gt;d = value;
3295 }
3296 
3297 void BytecodeInterpreter::set_stack_double_from_addr(intptr_t *tos,
3298                                               address addr, int offset) {
3299   (((VMJavaVal64*)&amp;tos[Interpreter::expr_index_at(-offset)])-&gt;d =
3300                         ((VMJavaVal64*)addr)-&gt;d);
3301 }
3302 
3303 void BytecodeInterpreter::set_stack_long(intptr_t *tos, jlong value,
3304                                                         int offset) {
3305   ((VMJavaVal64*)&amp;tos[Interpreter::expr_index_at(-offset+1)])-&gt;l = 0xdeedbeeb;
3306   ((VMJavaVal64*)&amp;tos[Interpreter::expr_index_at(-offset)])-&gt;l = value;
3307 }
3308 
3309 void BytecodeInterpreter::set_stack_long_from_addr(intptr_t *tos,
3310                                             address addr, int offset) {
3311   ((VMJavaVal64*)&amp;tos[Interpreter::expr_index_at(-offset+1)])-&gt;l = 0xdeedbeeb;
3312   ((VMJavaVal64*)&amp;tos[Interpreter::expr_index_at(-offset)])-&gt;l =
3313                         ((VMJavaVal64*)addr)-&gt;l;
3314 }
3315 
3316 // Locals
3317 
3318 address BytecodeInterpreter::locals_slot(intptr_t* locals, int offset) {
3319   return (address)locals[Interpreter::local_index_at(-offset)];
3320 }
3321 jint BytecodeInterpreter::locals_int(intptr_t* locals, int offset) {
3322   return (jint)locals[Interpreter::local_index_at(-offset)];
3323 }
3324 jfloat BytecodeInterpreter::locals_float(intptr_t* locals, int offset) {
3325   return (jfloat)locals[Interpreter::local_index_at(-offset)];
3326 }
3327 oop BytecodeInterpreter::locals_object(intptr_t* locals, int offset) {
3328   return cast_to_oop(locals[Interpreter::local_index_at(-offset)]);
3329 }
3330 jdouble BytecodeInterpreter::locals_double(intptr_t* locals, int offset) {
3331   return ((VMJavaVal64*)&amp;locals[Interpreter::local_index_at(-(offset+1))])-&gt;d;
3332 }
3333 jlong BytecodeInterpreter::locals_long(intptr_t* locals, int offset) {
3334   return ((VMJavaVal64*)&amp;locals[Interpreter::local_index_at(-(offset+1))])-&gt;l;
3335 }
3336 
3337 // Returns the address of locals value.
3338 address BytecodeInterpreter::locals_long_at(intptr_t* locals, int offset) {
3339   return ((address)&amp;locals[Interpreter::local_index_at(-(offset+1))]);
3340 }
3341 address BytecodeInterpreter::locals_double_at(intptr_t* locals, int offset) {
3342   return ((address)&amp;locals[Interpreter::local_index_at(-(offset+1))]);
3343 }
3344 
3345 // Used for local value or returnAddress
3346 void BytecodeInterpreter::set_locals_slot(intptr_t *locals,
3347                                    address value, int offset) {
3348   *((address*)&amp;locals[Interpreter::local_index_at(-offset)]) = value;
3349 }
3350 void BytecodeInterpreter::set_locals_int(intptr_t *locals,
3351                                    jint value, int offset) {
3352   *((jint *)&amp;locals[Interpreter::local_index_at(-offset)]) = value;
3353 }
3354 void BytecodeInterpreter::set_locals_float(intptr_t *locals,
3355                                    jfloat value, int offset) {
3356   *((jfloat *)&amp;locals[Interpreter::local_index_at(-offset)]) = value;
3357 }
3358 void BytecodeInterpreter::set_locals_object(intptr_t *locals,
3359                                    oop value, int offset) {
3360   *((oop *)&amp;locals[Interpreter::local_index_at(-offset)]) = value;
3361 }
3362 void BytecodeInterpreter::set_locals_double(intptr_t *locals,
3363                                    jdouble value, int offset) {
3364   ((VMJavaVal64*)&amp;locals[Interpreter::local_index_at(-(offset+1))])-&gt;d = value;
3365 }
3366 void BytecodeInterpreter::set_locals_long(intptr_t *locals,
3367                                    jlong value, int offset) {
3368   ((VMJavaVal64*)&amp;locals[Interpreter::local_index_at(-(offset+1))])-&gt;l = value;
3369 }
3370 void BytecodeInterpreter::set_locals_double_from_addr(intptr_t *locals,
3371                                    address addr, int offset) {
3372   ((VMJavaVal64*)&amp;locals[Interpreter::local_index_at(-(offset+1))])-&gt;d = ((VMJavaVal64*)addr)-&gt;d;
3373 }
3374 void BytecodeInterpreter::set_locals_long_from_addr(intptr_t *locals,
3375                                    address addr, int offset) {
3376   ((VMJavaVal64*)&amp;locals[Interpreter::local_index_at(-(offset+1))])-&gt;l = ((VMJavaVal64*)addr)-&gt;l;
3377 }
3378 
3379 void BytecodeInterpreter::astore(intptr_t* tos,    int stack_offset,
3380                           intptr_t* locals, int locals_offset) {
3381   intptr_t value = tos[Interpreter::expr_index_at(-stack_offset)];
3382   locals[Interpreter::local_index_at(-locals_offset)] = value;
3383 }
3384 
3385 
3386 void BytecodeInterpreter::copy_stack_slot(intptr_t *tos, int from_offset,
3387                                    int to_offset) {
3388   tos[Interpreter::expr_index_at(-to_offset)] =
3389                       (intptr_t)tos[Interpreter::expr_index_at(-from_offset)];
3390 }
3391 
3392 void BytecodeInterpreter::dup(intptr_t *tos) {
3393   copy_stack_slot(tos, -1, 0);
3394 }
3395 void BytecodeInterpreter::dup2(intptr_t *tos) {
3396   copy_stack_slot(tos, -2, 0);
3397   copy_stack_slot(tos, -1, 1);
3398 }
3399 
3400 void BytecodeInterpreter::dup_x1(intptr_t *tos) {
3401   /* insert top word two down */
3402   copy_stack_slot(tos, -1, 0);
3403   copy_stack_slot(tos, -2, -1);
3404   copy_stack_slot(tos, 0, -2);
3405 }
3406 
3407 void BytecodeInterpreter::dup_x2(intptr_t *tos) {
3408   /* insert top word three down  */
3409   copy_stack_slot(tos, -1, 0);
3410   copy_stack_slot(tos, -2, -1);
3411   copy_stack_slot(tos, -3, -2);
3412   copy_stack_slot(tos, 0, -3);
3413 }
3414 void BytecodeInterpreter::dup2_x1(intptr_t *tos) {
3415   /* insert top 2 slots three down */
3416   copy_stack_slot(tos, -1, 1);
3417   copy_stack_slot(tos, -2, 0);
3418   copy_stack_slot(tos, -3, -1);
3419   copy_stack_slot(tos, 1, -2);
3420   copy_stack_slot(tos, 0, -3);
3421 }
3422 void BytecodeInterpreter::dup2_x2(intptr_t *tos) {
3423   /* insert top 2 slots four down */
3424   copy_stack_slot(tos, -1, 1);
3425   copy_stack_slot(tos, -2, 0);
3426   copy_stack_slot(tos, -3, -1);
3427   copy_stack_slot(tos, -4, -2);
3428   copy_stack_slot(tos, 1, -3);
3429   copy_stack_slot(tos, 0, -4);
3430 }
3431 
3432 
3433 void BytecodeInterpreter::swap(intptr_t *tos) {
3434   // swap top two elements
3435   intptr_t val = tos[Interpreter::expr_index_at(1)];
3436   // Copy -2 entry to -1
3437   copy_stack_slot(tos, -2, -1);
3438   // Store saved -1 entry into -2
3439   tos[Interpreter::expr_index_at(2)] = val;
3440 }
3441 // --------------------------------------------------------------------------------
3442 // Non-product code
3443 #ifndef PRODUCT
3444 
3445 const char* BytecodeInterpreter::C_msg(BytecodeInterpreter::messages msg) {
3446   switch (msg) {
3447      case BytecodeInterpreter::no_request:  return(&quot;no_request&quot;);
3448      case BytecodeInterpreter::initialize:  return(&quot;initialize&quot;);
3449      // status message to C++ interpreter
3450      case BytecodeInterpreter::method_entry:  return(&quot;method_entry&quot;);
3451      case BytecodeInterpreter::method_resume:  return(&quot;method_resume&quot;);
3452      case BytecodeInterpreter::got_monitors:  return(&quot;got_monitors&quot;);
3453      case BytecodeInterpreter::rethrow_exception:  return(&quot;rethrow_exception&quot;);
3454      // requests to frame manager from C++ interpreter
3455      case BytecodeInterpreter::call_method:  return(&quot;call_method&quot;);
3456      case BytecodeInterpreter::return_from_method:  return(&quot;return_from_method&quot;);
3457      case BytecodeInterpreter::more_monitors:  return(&quot;more_monitors&quot;);
3458      case BytecodeInterpreter::throwing_exception:  return(&quot;throwing_exception&quot;);
3459      case BytecodeInterpreter::popping_frame:  return(&quot;popping_frame&quot;);
3460      case BytecodeInterpreter::do_osr:  return(&quot;do_osr&quot;);
3461      // deopt
3462      case BytecodeInterpreter::deopt_resume:  return(&quot;deopt_resume&quot;);
3463      case BytecodeInterpreter::deopt_resume2:  return(&quot;deopt_resume2&quot;);
3464      default: return(&quot;BAD MSG&quot;);
3465   }
3466 }
3467 void
3468 BytecodeInterpreter::print() {
3469   tty-&gt;print_cr(&quot;thread: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_thread);
3470   tty-&gt;print_cr(&quot;bcp: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_bcp);
3471   tty-&gt;print_cr(&quot;locals: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_locals);
3472   tty-&gt;print_cr(&quot;constants: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_constants);
3473   {
3474     ResourceMark rm;
3475     char *method_name = _method-&gt;name_and_sig_as_C_string();
3476     tty-&gt;print_cr(&quot;method: &quot; INTPTR_FORMAT &quot;[ %s ]&quot;,  (uintptr_t) this-&gt;_method, method_name);
3477   }
3478   tty-&gt;print_cr(&quot;mdx: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_mdx);
3479   tty-&gt;print_cr(&quot;stack: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_stack);
3480   tty-&gt;print_cr(&quot;msg: %s&quot;, C_msg(this-&gt;_msg));
3481   tty-&gt;print_cr(&quot;result_to_call._callee: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_result._to_call._callee);
3482   tty-&gt;print_cr(&quot;result_to_call._callee_entry_point: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_result._to_call._callee_entry_point);
3483   tty-&gt;print_cr(&quot;result_to_call._bcp_advance: %d &quot;, this-&gt;_result._to_call._bcp_advance);
3484   tty-&gt;print_cr(&quot;osr._osr_buf: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_result._osr._osr_buf);
3485   tty-&gt;print_cr(&quot;osr._osr_entry: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_result._osr._osr_entry);
3486   tty-&gt;print_cr(&quot;prev_link: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_prev_link);
3487   tty-&gt;print_cr(&quot;native_mirror: &quot; INTPTR_FORMAT, (uintptr_t) p2i(this-&gt;_oop_temp));
3488   tty-&gt;print_cr(&quot;stack_base: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_stack_base);
3489   tty-&gt;print_cr(&quot;stack_limit: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_stack_limit);
3490   tty-&gt;print_cr(&quot;monitor_base: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_monitor_base);
3491 #if !defined(ZERO) &amp;&amp; defined(PPC)
3492   tty-&gt;print_cr(&quot;last_Java_fp: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_last_Java_fp);
3493 #endif // !ZERO
3494   tty-&gt;print_cr(&quot;self_link: &quot; INTPTR_FORMAT, (uintptr_t) this-&gt;_self_link);
3495 }
3496 
3497 extern &quot;C&quot; {
3498   void PI(uintptr_t arg) {
3499     ((BytecodeInterpreter*)arg)-&gt;print();
3500   }
3501 }
3502 #endif // PRODUCT
3503 
3504 #endif // JVMTI
3505 #endif // CC_INTERP
    </pre>
  </body>
</html>