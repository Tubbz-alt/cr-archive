<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2016, 2019, Red Hat, Inc. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahTraversalGC.hpp&quot;
 32 #include &quot;logging/logStream.hpp&quot;
 33 #include &quot;runtime/orderAccess.hpp&quot;
 34 
 35 ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :
 36   _heap(heap),
 37   _mutator_free_bitmap(max_regions, mtGC),
 38   _collector_free_bitmap(max_regions, mtGC),
 39   _max(max_regions)
 40 {
 41   clear_internal();
 42 }
 43 
 44 void ShenandoahFreeSet::increase_used(size_t num_bytes) {
 45   assert_heaplock_owned_by_current_thread();
 46   _used += num_bytes;
 47 
 48   assert(_used &lt;= _capacity, &quot;must not use more than we have: used: &quot; SIZE_FORMAT
 49          &quot;, capacity: &quot; SIZE_FORMAT &quot;, num_bytes: &quot; SIZE_FORMAT, _used, _capacity, num_bytes);
 50 }
 51 
 52 bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {
 53   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 54           idx, _max, _mutator_leftmost, _mutator_rightmost);
 55   return _mutator_free_bitmap.at(idx);
 56 }
 57 
 58 bool ShenandoahFreeSet::is_collector_free(size_t idx) const {
 59   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 60           idx, _max, _collector_leftmost, _collector_rightmost);
 61   return _collector_free_bitmap.at(idx);
 62 }
 63 
 64 HeapWord* ShenandoahFreeSet::allocate_single(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 65   // Scan the bitmap looking for a first fit.
 66   //
 67   // Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,
 68   // we would find the region to allocate at right away.
 69   //
 70   // Allocations are biased: new application allocs go to beginning of the heap, and GC allocs
 71   // go to the end. This makes application allocation faster, because we would clear lots
 72   // of regions from the beginning most of the time.
 73   //
 74   // Free set maintains mutator and collector views, and normally they allocate in their views only,
 75   // unless we special cases for stealing and mixed allocations.
 76 
 77   switch (req.type()) {
 78     case ShenandoahAllocRequest::_alloc_tlab:
 79     case ShenandoahAllocRequest::_alloc_shared: {
 80 
 81       // Try to allocate in the mutator view
 82       for (size_t idx = _mutator_leftmost; idx &lt;= _mutator_rightmost; idx++) {
 83         if (is_mutator_free(idx)) {
 84           HeapWord* result = try_allocate_in(_heap-&gt;get_region(idx), req, in_new_region);
 85           if (result != NULL) {
 86             return result;
 87           }
 88         }
 89       }
 90 
 91       // There is no recovery. Mutator does not touch collector view at all.
 92       break;
 93     }
 94     case ShenandoahAllocRequest::_alloc_gclab:
 95     case ShenandoahAllocRequest::_alloc_shared_gc: {
 96       // size_t is unsigned, need to dodge underflow when _leftmost = 0
 97 
 98       // Fast-path: try to allocate in the collector view first
 99       for (size_t c = _collector_rightmost + 1; c &gt; _collector_leftmost; c--) {
100         size_t idx = c - 1;
101         if (is_collector_free(idx)) {
102           HeapWord* result = try_allocate_in(_heap-&gt;get_region(idx), req, in_new_region);
103           if (result != NULL) {
104             return result;
105           }
106         }
107       }
108 
109       // No dice. Can we borrow space from mutator view?
110       if (!ShenandoahEvacReserveOverflow) {
111         return NULL;
112       }
113 
114       // Try to steal the empty region from the mutator view
115       for (size_t c = _mutator_rightmost + 1; c &gt; _mutator_leftmost; c--) {
116         size_t idx = c - 1;
117         if (is_mutator_free(idx)) {
118           ShenandoahHeapRegion* r = _heap-&gt;get_region(idx);
119           if (can_allocate_from(r)) {
120             flip_to_gc(r);
121             HeapWord *result = try_allocate_in(r, req, in_new_region);
122             if (result != NULL) {
123               return result;
124             }
125           }
126         }
127       }
128 
129       // Try to mix the allocation into the mutator view:
130       if (ShenandoahAllowMixedAllocs) {
131         for (size_t c = _mutator_rightmost + 1; c &gt; _mutator_leftmost; c--) {
132           size_t idx = c - 1;
133           if (is_mutator_free(idx)) {
134             HeapWord* result = try_allocate_in(_heap-&gt;get_region(idx), req, in_new_region);
135             if (result != NULL) {
136               return result;
137             }
138           }
139         }
140       }
141       break;
142     }
143     default:
144       ShouldNotReachHere();
145   }
146 
147   return NULL;
148 }
149 
150 HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
151   assert (!has_no_alloc_capacity(r), &quot;Performance: should avoid full regions on this path: &quot; SIZE_FORMAT, r-&gt;region_number());
152 
153   if (_heap-&gt;is_concurrent_root_in_progress() &amp;&amp;
154       r-&gt;is_trash()) {
155     return NULL;
156   }
157 
158   try_recycle_trashed(r);
159 
160   in_new_region = r-&gt;is_empty();
161 
162   HeapWord* result = NULL;
163   size_t size = req.size();
164 
165   if (ShenandoahElasticTLAB &amp;&amp; req.is_lab_alloc()) {
166     size_t free = align_down(r-&gt;free() &gt;&gt; LogHeapWordSize, MinObjAlignment);
167     if (size &gt; free) {
168       size = free;
169     }
170     if (size &gt;= req.min_size()) {
171       result = r-&gt;allocate(size, req.type());
172       assert (result != NULL, &quot;Allocation must succeed: free &quot; SIZE_FORMAT &quot;, actual &quot; SIZE_FORMAT, free, size);
173     }
174   } else {
175     result = r-&gt;allocate(size, req.type());
176   }
177 
178   if (result != NULL) {
179     // Allocation successful, bump stats:
180     if (req.is_mutator_alloc()) {
181       increase_used(size * HeapWordSize);
182     }
183 
184     // Record actual allocation size
185     req.set_actual_size(size);
186 
187     if (req.is_gc_alloc()) {
188       r-&gt;set_update_watermark(r-&gt;top());
189       if (_heap-&gt;is_concurrent_traversal_in_progress()) {
190         // Traversal needs to traverse through GC allocs. Adjust TAMS to the new top
191         // so that these allocations appear below TAMS, and thus get traversed.
192         // See top of shenandoahTraversal.cpp for an explanation.
193         _heap-&gt;marking_context()-&gt;capture_top_at_mark_start(r);
194         _heap-&gt;traversal_gc()-&gt;traversal_set()-&gt;add_region_check_for_duplicates(r);
195         OrderAccess::fence();
196       }
197     }
198   }
199 
200   if (result == NULL || has_no_alloc_capacity(r)) {
201     // Region cannot afford this or future allocations. Retire it.
202     //
203     // While this seems a bit harsh, especially in the case when this large allocation does not
204     // fit, but the next small one would, we are risking to inflate scan times when lots of
205     // almost-full regions precede the fully-empty region where we want allocate the entire TLAB.
206     // TODO: Record first fully-empty region, and use that for large allocations
207 
208     // Record the remainder as allocation waste
209     if (req.is_mutator_alloc()) {
210       size_t waste = r-&gt;free();
211       if (waste &gt; 0) {
212         increase_used(waste);
213         _heap-&gt;notify_mutator_alloc_words(waste &gt;&gt; LogHeapWordSize, true);
214       }
215     }
216 
217     size_t num = r-&gt;region_number();
218     _collector_free_bitmap.clear_bit(num);
219     _mutator_free_bitmap.clear_bit(num);
220     // Touched the bounds? Need to update:
221     if (touches_bounds(num)) {
222       adjust_bounds();
223     }
224     assert_bounds();
225   }
226   return result;
227 }
228 
229 bool ShenandoahFreeSet::touches_bounds(size_t num) const {
230   return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;
231 }
232 
233 void ShenandoahFreeSet::recompute_bounds() {
234   // Reset to the most pessimistic case:
235   _mutator_rightmost = _max - 1;
236   _mutator_leftmost = 0;
237   _collector_rightmost = _max - 1;
238   _collector_leftmost = 0;
239 
240   // ...and adjust from there
241   adjust_bounds();
242 }
243 
244 void ShenandoahFreeSet::adjust_bounds() {
245   // Rewind both mutator bounds until the next bit.
246   while (_mutator_leftmost &lt; _max &amp;&amp; !is_mutator_free(_mutator_leftmost)) {
247     _mutator_leftmost++;
248   }
249   while (_mutator_rightmost &gt; 0 &amp;&amp; !is_mutator_free(_mutator_rightmost)) {
250     _mutator_rightmost--;
251   }
252   // Rewind both collector bounds until the next bit.
253   while (_collector_leftmost &lt; _max &amp;&amp; !is_collector_free(_collector_leftmost)) {
254     _collector_leftmost++;
255   }
256   while (_collector_rightmost &gt; 0 &amp;&amp; !is_collector_free(_collector_rightmost)) {
257     _collector_rightmost--;
258   }
259 }
260 
261 HeapWord* ShenandoahFreeSet::allocate_contiguous(ShenandoahAllocRequest&amp; req) {
262   assert_heaplock_owned_by_current_thread();
263 
264   size_t words_size = req.size();
265   size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);
266 
267   // No regions left to satisfy allocation, bye.
268   if (num &gt; mutator_count()) {
269     return NULL;
270   }
271 
272   // Find the continuous interval of $num regions, starting from $beg and ending in $end,
273   // inclusive. Contiguous allocations are biased to the beginning.
274 
275   size_t beg = _mutator_leftmost;
276   size_t end = beg;
277 
278   while (true) {
279     if (end &gt;= _max) {
280       // Hit the end, goodbye
281       return NULL;
282     }
283 
284     // If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.
285     // If region is not completely free, the current [beg; end] is useless, and we may fast-forward.
286     if (!is_mutator_free(end) || !can_allocate_from(_heap-&gt;get_region(end))) {
287       end++;
288       beg = end;
289       continue;
290     }
291 
292     if ((end - beg + 1) == num) {
293       // found the match
294       break;
295     }
296 
297     end++;
298   };
299 
300   size_t remainder = words_size &amp; ShenandoahHeapRegion::region_size_words_mask();
301 
302   // Initialize regions:
303   for (size_t i = beg; i &lt;= end; i++) {
304     ShenandoahHeapRegion* r = _heap-&gt;get_region(i);
305     try_recycle_trashed(r);
306 
307     assert(i == beg || _heap-&gt;get_region(i-1)-&gt;region_number() + 1 == r-&gt;region_number(), &quot;Should be contiguous&quot;);
308     assert(r-&gt;is_empty(), &quot;Should be empty&quot;);
309 
310     if (i == beg) {
311       r-&gt;make_humongous_start();
312     } else {
313       r-&gt;make_humongous_cont();
314     }
315 
316     // Trailing region may be non-full, record the remainder there
317     size_t used_words;
318     if ((i == end) &amp;&amp; (remainder != 0)) {
319       used_words = remainder;
320     } else {
321       used_words = ShenandoahHeapRegion::region_size_words();
322     }
323 
324     r-&gt;set_top(r-&gt;bottom() + used_words);
325     r-&gt;reset_alloc_metadata_to_shared();
326 
327     _mutator_free_bitmap.clear_bit(r-&gt;region_number());
328   }
329 
330   // While individual regions report their true use, all humongous regions are
331   // marked used in the free set.
332   increase_used(ShenandoahHeapRegion::region_size_bytes() * num);
333 
334   if (remainder != 0) {
335     // Record this remainder as allocation waste
336     _heap-&gt;notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);
337   }
338 
339   // Allocated at left/rightmost? Move the bounds appropriately.
340   if (beg == _mutator_leftmost || end == _mutator_rightmost) {
341     adjust_bounds();
342   }
343   assert_bounds();
344 
345   req.set_actual_size(words_size);
346   return _heap-&gt;get_region(beg)-&gt;bottom();
347 }
348 
349 bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {
350   return r-&gt;is_empty() || (r-&gt;is_trash() &amp;&amp; !_heap-&gt;is_concurrent_root_in_progress());
351 }
352 
353 size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {
354   if (r-&gt;is_trash()) {
355     // This would be recycled on allocation path
356     return ShenandoahHeapRegion::region_size_bytes();
357   } else {
358     return r-&gt;free();
359   }
360 }
361 
362 bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {
363   return alloc_capacity(r) == 0;
364 }
365 
366 void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion *r) {
367   if (r-&gt;is_trash()) {
368     _heap-&gt;decrease_used(r-&gt;used());
369     r-&gt;recycle();
370   }
371 }
372 
373 void ShenandoahFreeSet::recycle_trash() {
374   // lock is not reentrable, check we don&#39;t have it
375   assert_heaplock_not_owned_by_current_thread();
376 
377   for (size_t i = 0; i &lt; _heap-&gt;num_regions(); i++) {
378     ShenandoahHeapRegion* r = _heap-&gt;get_region(i);
379     if (r-&gt;is_trash()) {
380       ShenandoahHeapLocker locker(_heap-&gt;lock());
381       try_recycle_trashed(r);
382     }
383     SpinPause(); // allow allocators to take the lock
384   }
385 }
386 
387 void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {
388   size_t idx = r-&gt;region_number();
389 
390   assert(_mutator_free_bitmap.at(idx), &quot;Should be in mutator view&quot;);
391   assert(can_allocate_from(r), &quot;Should not be allocated&quot;);
392 
393   _mutator_free_bitmap.clear_bit(idx);
394   _collector_free_bitmap.set_bit(idx);
395   _collector_leftmost = MIN2(idx, _collector_leftmost);
396   _collector_rightmost = MAX2(idx, _collector_rightmost);
397 
398   _capacity -= alloc_capacity(r);
399 
400   if (touches_bounds(idx)) {
401     adjust_bounds();
402   }
403   assert_bounds();
404 }
405 
406 void ShenandoahFreeSet::clear() {
407   assert_heaplock_owned_by_current_thread();
408   clear_internal();
409 }
410 
411 void ShenandoahFreeSet::clear_internal() {
412   _mutator_free_bitmap.clear();
413   _collector_free_bitmap.clear();
414   _mutator_leftmost = _max;
415   _mutator_rightmost = 0;
416   _collector_leftmost = _max;
417   _collector_rightmost = 0;
418   _capacity = 0;
419   _used = 0;
420 }
421 
422 void ShenandoahFreeSet::rebuild() {
423   assert_heaplock_owned_by_current_thread();
424   clear();
425 
426   for (size_t idx = 0; idx &lt; _heap-&gt;num_regions(); idx++) {
427     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
428     if (region-&gt;is_alloc_allowed() || region-&gt;is_trash()) {
429       assert(!region-&gt;is_cset(), &quot;Shouldn&#39;t be adding those to the free set&quot;);
430 
431       // Do not add regions that would surely fail allocation
432       if (has_no_alloc_capacity(region)) continue;
433 
434       _capacity += alloc_capacity(region);
435       assert(_used &lt;= _capacity, &quot;must not use more than we have&quot;);
436 
437       assert(!is_mutator_free(idx), &quot;We are about to add it, it shouldn&#39;t be there already&quot;);
438       _mutator_free_bitmap.set_bit(idx);
439     }
440   }
441 
442   // Evac reserve: reserve trailing space for evacuations
443   size_t to_reserve = _heap-&gt;max_capacity() / 100 * ShenandoahEvacReserve;
444   size_t reserved = 0;
445 
446   for (size_t idx = _heap-&gt;num_regions() - 1; idx &gt; 0; idx--) {
447     if (reserved &gt;= to_reserve) break;
448 
449     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
450     if (_mutator_free_bitmap.at(idx) &amp;&amp; can_allocate_from(region)) {
451       _mutator_free_bitmap.clear_bit(idx);
452       _collector_free_bitmap.set_bit(idx);
453       size_t ac = alloc_capacity(region);
454       _capacity -= ac;
455       reserved += ac;
456     }
457   }
458 
459   recompute_bounds();
460   assert_bounds();
461 }
462 
463 void ShenandoahFreeSet::log_status() {
464   assert_heaplock_owned_by_current_thread();
465 
466   LogTarget(Info, gc, ergo) lt;
467   if (lt.is_enabled()) {
468     ResourceMark rm;
469     LogStream ls(lt);
470 
471     {
472       size_t last_idx = 0;
473       size_t max = 0;
474       size_t max_contig = 0;
475       size_t empty_contig = 0;
476 
477       size_t total_used = 0;
478       size_t total_free = 0;
479 
480       for (size_t idx = _mutator_leftmost; idx &lt;= _mutator_rightmost; idx++) {
481         if (is_mutator_free(idx)) {
482           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
483           size_t free = alloc_capacity(r);
484 
485           max = MAX2(max, free);
486 
487           if (r-&gt;is_empty() &amp;&amp; (last_idx + 1 == idx)) {
488             empty_contig++;
489           } else {
490             empty_contig = 0;
491           }
492 
493           total_used += r-&gt;used();
494           total_free += free;
495 
496           max_contig = MAX2(max_contig, empty_contig);
497           last_idx = idx;
498         }
499       }
500 
501       size_t max_humongous = max_contig * ShenandoahHeapRegion::region_size_bytes();
502       size_t free = capacity() - used();
503 
504       ls.print(&quot;Free: &quot; SIZE_FORMAT &quot;%s (&quot; SIZE_FORMAT &quot; regions), Max regular: &quot; SIZE_FORMAT &quot;%s, Max humongous: &quot; SIZE_FORMAT &quot;%s, &quot;,
505                byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),
506                mutator_count(),
507                byte_size_in_proper_unit(max),           proper_unit_for_byte_size(max),
508                byte_size_in_proper_unit(max_humongous), proper_unit_for_byte_size(max_humongous)
509       );
510 
511       size_t frag_ext;
512       if (free &gt; 0) {
513         frag_ext = 100 - (100 * max_humongous / free);
514       } else {
515         frag_ext = 0;
516       }
517       ls.print(&quot;External frag: &quot; SIZE_FORMAT &quot;%%, &quot;, frag_ext);
518 
519       size_t frag_int;
520       if (mutator_count() &gt; 0) {
521         frag_int = (100 * (total_used / mutator_count()) / ShenandoahHeapRegion::region_size_bytes());
522       } else {
523         frag_int = 0;
524       }
525       ls.print(&quot;Internal frag: &quot; SIZE_FORMAT &quot;%%&quot;, frag_int);
526       ls.cr();
527     }
528 
529     {
530       size_t max = 0;
531       size_t total_free = 0;
532 
533       for (size_t idx = _collector_leftmost; idx &lt;= _collector_rightmost; idx++) {
534         if (is_collector_free(idx)) {
535           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
536           size_t free = alloc_capacity(r);
537           max = MAX2(max, free);
538           total_free += free;
539         }
540       }
541 
542       ls.print_cr(&quot;Evacuation Reserve: &quot; SIZE_FORMAT &quot;%s (&quot; SIZE_FORMAT &quot; regions), Max regular: &quot; SIZE_FORMAT &quot;%s&quot;,
543                   byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),
544                   collector_count(),
545                   byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));
546     }
547   }
548 }
549 
550 HeapWord* ShenandoahFreeSet::allocate(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
551   assert_heaplock_owned_by_current_thread();
552   assert_bounds();
553 
554   if (req.size() &gt; ShenandoahHeapRegion::humongous_threshold_words()) {
555     switch (req.type()) {
556       case ShenandoahAllocRequest::_alloc_shared:
557       case ShenandoahAllocRequest::_alloc_shared_gc:
558         in_new_region = true;
559         return allocate_contiguous(req);
560       case ShenandoahAllocRequest::_alloc_gclab:
561       case ShenandoahAllocRequest::_alloc_tlab:
562         in_new_region = false;
563         assert(false, &quot;Trying to allocate TLAB larger than the humongous threshold: &quot; SIZE_FORMAT &quot; &gt; &quot; SIZE_FORMAT,
564                req.size(), ShenandoahHeapRegion::humongous_threshold_words());
565         return NULL;
566       default:
567         ShouldNotReachHere();
568         return NULL;
569     }
570   } else {
571     return allocate_single(req, in_new_region);
572   }
573 }
574 
575 size_t ShenandoahFreeSet::unsafe_peek_free() const {
576   // Deliberately not locked, this method is unsafe when free set is modified.
577 
578   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
579     if (index &lt; _max &amp;&amp; is_mutator_free(index)) {
580       ShenandoahHeapRegion* r = _heap-&gt;get_region(index);
581       if (r-&gt;free() &gt;= MinTLABSize) {
582         return r-&gt;free();
583       }
584     }
585   }
586 
587   // It appears that no regions left
588   return 0;
589 }
590 
591 void ShenandoahFreeSet::print_on(outputStream* out) const {
592   out-&gt;print_cr(&quot;Mutator Free Set: &quot; SIZE_FORMAT &quot;&quot;, mutator_count());
593   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
594     if (is_mutator_free(index)) {
595       _heap-&gt;get_region(index)-&gt;print_on(out);
596     }
597   }
598   out-&gt;print_cr(&quot;Collector Free Set: &quot; SIZE_FORMAT &quot;&quot;, collector_count());
599   for (size_t index = _collector_leftmost; index &lt;= _collector_rightmost; index++) {
600     if (is_collector_free(index)) {
601       _heap-&gt;get_region(index)-&gt;print_on(out);
602     }
603   }
604 }
605 
606 #ifdef ASSERT
607 void ShenandoahFreeSet::assert_heaplock_owned_by_current_thread() const {
608   _heap-&gt;assert_heaplock_owned_by_current_thread();
609 }
610 
611 void ShenandoahFreeSet::assert_heaplock_not_owned_by_current_thread() const {
612   _heap-&gt;assert_heaplock_not_owned_by_current_thread();
613 }
614 
615 void ShenandoahFreeSet::assert_bounds() const {
616   // Performance invariants. Failing these would not break the free set, but performance
617   // would suffer.
618   assert (_mutator_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_leftmost,  _max);
619   assert (_mutator_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_rightmost, _max);
620 
621   assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _mutator_leftmost);
622   assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _mutator_rightmost);
623 
624   size_t beg_off = _mutator_free_bitmap.get_next_one_offset(0);
625   size_t end_off = _mutator_free_bitmap.get_next_one_offset(_mutator_rightmost + 1);
626   assert (beg_off &gt;= _mutator_leftmost, &quot;free regions before the leftmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT, beg_off, _mutator_leftmost);
627   assert (end_off == _max,      &quot;free regions past the rightmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT,  end_off, _mutator_rightmost);
628 
629   assert (_collector_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_leftmost,  _max);
630   assert (_collector_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_rightmost, _max);
631 
632   assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _collector_leftmost);
633   assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _collector_rightmost);
634 
635   beg_off = _collector_free_bitmap.get_next_one_offset(0);
636   end_off = _collector_free_bitmap.get_next_one_offset(_collector_rightmost + 1);
637   assert (beg_off &gt;= _collector_leftmost, &quot;free regions before the leftmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT, beg_off, _collector_leftmost);
638   assert (end_off == _max,      &quot;free regions past the rightmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT,  end_off, _collector_rightmost);
639 }
640 #endif
    </pre>
  </body>
</html>