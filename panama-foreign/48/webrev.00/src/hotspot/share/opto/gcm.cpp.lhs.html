<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/opto/gcm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;libadt/vectset.hpp&quot;
  27 #include &quot;memory/allocation.inline.hpp&quot;
  28 #include &quot;memory/resourceArea.hpp&quot;
  29 #include &quot;opto/block.hpp&quot;
  30 #include &quot;opto/c2compiler.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/cfgnode.hpp&quot;
  33 #include &quot;opto/machnode.hpp&quot;
  34 #include &quot;opto/opcodes.hpp&quot;
  35 #include &quot;opto/phaseX.hpp&quot;
  36 #include &quot;opto/rootnode.hpp&quot;
  37 #include &quot;opto/runtime.hpp&quot;
  38 #include &quot;opto/chaitin.hpp&quot;
  39 #include &quot;runtime/deoptimization.hpp&quot;
  40 
  41 // Portions of code courtesy of Clifford Click
  42 
  43 // Optimization - Graph Style
  44 
  45 // To avoid float value underflow
  46 #define MIN_BLOCK_FREQUENCY 1.e-35f
  47 
  48 //----------------------------schedule_node_into_block-------------------------
  49 // Insert node n into block b. Look for projections of n and make sure they
  50 // are in b also.
  51 void PhaseCFG::schedule_node_into_block( Node *n, Block *b ) {
  52   // Set basic block of n, Add n to b,
  53   map_node_to_block(n, b);
  54   b-&gt;add_inst(n);
  55 
  56   // After Matching, nearly any old Node may have projections trailing it.
  57   // These are usually machine-dependent flags.  In any case, they might
  58   // float to another block below this one.  Move them up.
  59   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
  60     Node*  use  = n-&gt;fast_out(i);
  61     if (use-&gt;is_Proj()) {
  62       Block* buse = get_block_for_node(use);
  63       if (buse != b) {              // In wrong block?
  64         if (buse != NULL) {
  65           buse-&gt;find_remove(use);   // Remove from wrong block
  66         }
  67         map_node_to_block(use, b);
  68         b-&gt;add_inst(use);
  69       }
  70     }
  71   }
  72 }
  73 
  74 //----------------------------replace_block_proj_ctrl-------------------------
  75 // Nodes that have is_block_proj() nodes as their control need to use
  76 // the appropriate Region for their actual block as their control since
  77 // the projection will be in a predecessor block.
  78 void PhaseCFG::replace_block_proj_ctrl( Node *n ) {
  79   const Node *in0 = n-&gt;in(0);
  80   assert(in0 != NULL, &quot;Only control-dependent&quot;);
  81   const Node *p = in0-&gt;is_block_proj();
  82   if (p != NULL &amp;&amp; p != n) {    // Control from a block projection?
  83     assert(!n-&gt;pinned() || n-&gt;is_MachConstantBase(), &quot;only pinned MachConstantBase node is expected here&quot;);
  84     // Find trailing Region
  85     Block *pb = get_block_for_node(in0); // Block-projection already has basic block
  86     uint j = 0;
  87     if (pb-&gt;_num_succs != 1) {  // More then 1 successor?
  88       // Search for successor
  89       uint max = pb-&gt;number_of_nodes();
  90       assert( max &gt; 1, &quot;&quot; );
  91       uint start = max - pb-&gt;_num_succs;
  92       // Find which output path belongs to projection
  93       for (j = start; j &lt; max; j++) {
  94         if( pb-&gt;get_node(j) == in0 )
  95           break;
  96       }
  97       assert( j &lt; max, &quot;must find&quot; );
  98       // Change control to match head of successor basic block
  99       j -= start;
 100     }
 101     n-&gt;set_req(0, pb-&gt;_succs[j]-&gt;head());
 102   }
 103 }
 104 
 105 bool PhaseCFG::is_dominator(Node* dom_node, Node* node) {
 106   assert(is_CFG(node) &amp;&amp; is_CFG(dom_node), &quot;node and dom_node must be CFG nodes&quot;);
 107   if (dom_node == node) {
 108     return true;
 109   }
 110   Block* d = find_block_for_node(dom_node);
 111   Block* n = find_block_for_node(node);
 112   assert(n != NULL &amp;&amp; d != NULL, &quot;blocks must exist&quot;);
 113 
 114   if (d == n) {
 115     if (dom_node-&gt;is_block_start()) {
 116       return true;
 117     }
 118     if (node-&gt;is_block_start()) {
 119       return false;
 120     }
 121     if (dom_node-&gt;is_block_proj()) {
 122       return false;
 123     }
 124     if (node-&gt;is_block_proj()) {
 125       return true;
 126     }
 127 
 128     assert(is_control_proj_or_safepoint(node), &quot;node must be control projection or safepoint&quot;);
 129     assert(is_control_proj_or_safepoint(dom_node), &quot;dom_node must be control projection or safepoint&quot;);
 130 
 131     // Neither &#39;node&#39; nor &#39;dom_node&#39; is a block start or block projection.
 132     // Check if &#39;dom_node&#39; is above &#39;node&#39; in the control graph.
 133     if (is_dominating_control(dom_node, node)) {
 134       return true;
 135     }
 136 
 137 #ifdef ASSERT
 138     // If &#39;dom_node&#39; does not dominate &#39;node&#39; then &#39;node&#39; has to dominate &#39;dom_node&#39;
 139     if (!is_dominating_control(node, dom_node)) {
 140       node-&gt;dump();
 141       dom_node-&gt;dump();
 142       assert(false, &quot;neither dom_node nor node dominates the other&quot;);
 143     }
 144 #endif
 145 
 146     return false;
 147   }
 148   return d-&gt;dom_lca(n) == d;
 149 }
 150 
 151 bool PhaseCFG::is_CFG(Node* n) {
 152   return n-&gt;is_block_proj() || n-&gt;is_block_start() || is_control_proj_or_safepoint(n);
 153 }
 154 
 155 bool PhaseCFG::is_control_proj_or_safepoint(Node* n) {
 156   bool result = (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_SafePoint) || (n-&gt;is_Proj() &amp;&amp; n-&gt;as_Proj()-&gt;bottom_type() == Type::CONTROL);
 157   assert(!result || (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_SafePoint)
 158           || (n-&gt;is_Proj() &amp;&amp; n-&gt;as_Proj()-&gt;_con == 0), &quot;If control projection, it must be projection 0&quot;);
 159   return result;
 160 }
 161 
 162 Block* PhaseCFG::find_block_for_node(Node* n) {
 163   if (n-&gt;is_block_start() || n-&gt;is_block_proj()) {
 164     return get_block_for_node(n);
 165   } else {
 166     // Walk the control graph up if &#39;n&#39; is not a block start nor a block projection. In this case &#39;n&#39; must be
 167     // an unmatched control projection or a not yet matched safepoint precedence edge in the middle of a block.
 168     assert(is_control_proj_or_safepoint(n), &quot;must be control projection or safepoint&quot;);
 169     Node* ctrl = n-&gt;in(0);
 170     while (!ctrl-&gt;is_block_start()) {
 171       ctrl = ctrl-&gt;in(0);
 172     }
 173     return get_block_for_node(ctrl);
 174   }
 175 }
 176 
 177 // Walk up the control graph from &#39;n&#39; and check if &#39;dom_ctrl&#39; is found.
 178 bool PhaseCFG::is_dominating_control(Node* dom_ctrl, Node* n) {
 179   Node* ctrl = n-&gt;in(0);
 180   while (!ctrl-&gt;is_block_start()) {
 181     if (ctrl == dom_ctrl) {
 182       return true;
 183     }
 184     ctrl = ctrl-&gt;in(0);
 185   }
 186   return false;
 187 }
 188 
 189 
 190 //------------------------------schedule_pinned_nodes--------------------------
 191 // Set the basic block for Nodes pinned into blocks
 192 void PhaseCFG::schedule_pinned_nodes(VectorSet &amp;visited) {
 193   // Allocate node stack of size C-&gt;live_nodes()+8 to avoid frequent realloc
 194   GrowableArray &lt;Node*&gt; spstack(C-&gt;live_nodes() + 8);
 195   spstack.push(_root);
 196   while (spstack.is_nonempty()) {
 197     Node* node = spstack.pop();
 198     if (!visited.test_set(node-&gt;_idx)) { // Test node and flag it as visited
 199       if (node-&gt;pinned() &amp;&amp; !has_block(node)) {  // Pinned?  Nail it down!
 200         assert(node-&gt;in(0), &quot;pinned Node must have Control&quot;);
 201         // Before setting block replace block_proj control edge
 202         replace_block_proj_ctrl(node);
 203         Node* input = node-&gt;in(0);
 204         while (!input-&gt;is_block_start()) {
 205           input = input-&gt;in(0);
 206         }
 207         Block* block = get_block_for_node(input); // Basic block of controlling input
 208         schedule_node_into_block(node, block);
 209       }
 210 
 211       // If the node has precedence edges (added when CastPP nodes are
 212       // removed in final_graph_reshaping), fix the control of the
 213       // node to cover the precedence edges and remove the
 214       // dependencies.
 215       Node* n = NULL;
 216       for (uint i = node-&gt;len()-1; i &gt;= node-&gt;req(); i--) {
 217         Node* m = node-&gt;in(i);
 218         if (m == NULL) continue;
 219 
 220         // Only process precedence edges that are CFG nodes. Safepoints and control projections can be in the middle of a block
 221         if (is_CFG(m)) {
 222           node-&gt;rm_prec(i);
 223           if (n == NULL) {
 224             n = m;
 225           } else {
 226             assert(is_dominator(n, m) || is_dominator(m, n), &quot;one must dominate the other&quot;);
 227             n = is_dominator(n, m) ? m : n;
 228           }
 229         } else {
 230           assert(node-&gt;is_Mach(), &quot;sanity&quot;);
 231           assert(node-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreCM, &quot;must be StoreCM node&quot;);
 232         }
 233       }
 234       if (n != NULL) {
 235         assert(node-&gt;in(0), &quot;control should have been set&quot;);
 236         assert(is_dominator(n, node-&gt;in(0)) || is_dominator(node-&gt;in(0), n), &quot;one must dominate the other&quot;);
 237         if (!is_dominator(n, node-&gt;in(0))) {
 238           node-&gt;set_req(0, n);
 239         }
 240       }
 241 
 242       // process all inputs that are non NULL
 243       for (int i = node-&gt;req()-1; i &gt;= 0; --i) {
 244         if (node-&gt;in(i) != NULL) {
 245           spstack.push(node-&gt;in(i));
 246         }
 247       }
 248     }
 249   }
 250 }
 251 
 252 #ifdef ASSERT
 253 // Assert that new input b2 is dominated by all previous inputs.
 254 // Check this by by seeing that it is dominated by b1, the deepest
 255 // input observed until b2.
 256 static void assert_dom(Block* b1, Block* b2, Node* n, const PhaseCFG* cfg) {
 257   if (b1 == NULL)  return;
 258   assert(b1-&gt;_dom_depth &lt; b2-&gt;_dom_depth, &quot;sanity&quot;);
 259   Block* tmp = b2;
 260   while (tmp != b1 &amp;&amp; tmp != NULL) {
 261     tmp = tmp-&gt;_idom;
 262   }
 263   if (tmp != b1) {
 264     // Detected an unschedulable graph.  Print some nice stuff and die.
 265     tty-&gt;print_cr(&quot;!!! Unschedulable graph !!!&quot;);
 266     for (uint j=0; j&lt;n-&gt;len(); j++) { // For all inputs
 267       Node* inn = n-&gt;in(j); // Get input
 268       if (inn == NULL)  continue;  // Ignore NULL, missing inputs
 269       Block* inb = cfg-&gt;get_block_for_node(inn);
 270       tty-&gt;print(&quot;B%d idom=B%d depth=%2d &quot;,inb-&gt;_pre_order,
 271                  inb-&gt;_idom ? inb-&gt;_idom-&gt;_pre_order : 0, inb-&gt;_dom_depth);
 272       inn-&gt;dump();
 273     }
 274     tty-&gt;print(&quot;Failing node: &quot;);
 275     n-&gt;dump();
 276     assert(false, &quot;unscheduable graph&quot;);
 277   }
 278 }
 279 #endif
 280 
 281 static Block* find_deepest_input(Node* n, const PhaseCFG* cfg) {
 282   // Find the last input dominated by all other inputs.
 283   Block* deepb           = NULL;        // Deepest block so far
 284   int    deepb_dom_depth = 0;
 285   for (uint k = 0; k &lt; n-&gt;len(); k++) { // For all inputs
 286     Node* inn = n-&gt;in(k);               // Get input
 287     if (inn == NULL)  continue;         // Ignore NULL, missing inputs
 288     Block* inb = cfg-&gt;get_block_for_node(inn);
 289     assert(inb != NULL, &quot;must already have scheduled this input&quot;);
 290     if (deepb_dom_depth &lt; (int) inb-&gt;_dom_depth) {
 291       // The new inb must be dominated by the previous deepb.
 292       // The various inputs must be linearly ordered in the dom
 293       // tree, or else there will not be a unique deepest block.
 294       DEBUG_ONLY(assert_dom(deepb, inb, n, cfg));
 295       deepb = inb;                      // Save deepest block
 296       deepb_dom_depth = deepb-&gt;_dom_depth;
 297     }
 298   }
 299   assert(deepb != NULL, &quot;must be at least one input to n&quot;);
 300   return deepb;
 301 }
 302 
 303 
 304 //------------------------------schedule_early---------------------------------
 305 // Find the earliest Block any instruction can be placed in.  Some instructions
 306 // are pinned into Blocks.  Unpinned instructions can appear in last block in
 307 // which all their inputs occur.
 308 bool PhaseCFG::schedule_early(VectorSet &amp;visited, Node_Stack &amp;roots) {
 309   // Allocate stack with enough space to avoid frequent realloc
 310   Node_Stack nstack(roots.size() + 8);
 311   // _root will be processed among C-&gt;top() inputs
 312   roots.push(C-&gt;top(), 0);
 313   visited.set(C-&gt;top()-&gt;_idx);
 314 
 315   while (roots.size() != 0) {
 316     // Use local variables nstack_top_n &amp; nstack_top_i to cache values
 317     // on stack&#39;s top.
 318     Node* parent_node = roots.node();
 319     uint  input_index = 0;
 320     roots.pop();
 321 
 322     while (true) {
 323       if (input_index == 0) {
 324         // Fixup some control.  Constants without control get attached
 325         // to root and nodes that use is_block_proj() nodes should be attached
 326         // to the region that starts their block.
 327         const Node* control_input = parent_node-&gt;in(0);
 328         if (control_input != NULL) {
 329           replace_block_proj_ctrl(parent_node);
 330         } else {
 331           // Is a constant with NO inputs?
 332           if (parent_node-&gt;req() == 1) {
 333             parent_node-&gt;set_req(0, _root);
 334           }
 335         }
 336       }
 337 
 338       // First, visit all inputs and force them to get a block.  If an
 339       // input is already in a block we quit following inputs (to avoid
 340       // cycles). Instead we put that Node on a worklist to be handled
 341       // later (since IT&#39;S inputs may not have a block yet).
 342 
 343       // Assume all n&#39;s inputs will be processed
 344       bool done = true;
 345 
 346       while (input_index &lt; parent_node-&gt;len()) {
 347         Node* in = parent_node-&gt;in(input_index++);
 348         if (in == NULL) {
 349           continue;
 350         }
 351 
 352         int is_visited = visited.test_set(in-&gt;_idx);
 353         if (!has_block(in)) {
 354           if (is_visited) {
 355             assert(false, &quot;graph should be schedulable&quot;);
 356             return false;
 357           }
 358           // Save parent node and next input&#39;s index.
 359           nstack.push(parent_node, input_index);
 360           // Process current input now.
 361           parent_node = in;
 362           input_index = 0;
 363           // Not all n&#39;s inputs processed.
 364           done = false;
 365           break;
 366         } else if (!is_visited) {
 367           // Visit this guy later, using worklist
 368           roots.push(in, 0);
 369         }
 370       }
 371 
 372       if (done) {
 373         // All of n&#39;s inputs have been processed, complete post-processing.
 374 
 375         // Some instructions are pinned into a block.  These include Region,
 376         // Phi, Start, Return, and other control-dependent instructions and
 377         // any projections which depend on them.
 378         if (!parent_node-&gt;pinned()) {
 379           // Set earliest legal block.
 380           Block* earliest_block = find_deepest_input(parent_node, this);
 381           map_node_to_block(parent_node, earliest_block);
 382         } else {
 383           assert(get_block_for_node(parent_node) == get_block_for_node(parent_node-&gt;in(0)), &quot;Pinned Node should be at the same block as its control edge&quot;);
 384         }
 385 
 386         if (nstack.is_empty()) {
 387           // Finished all nodes on stack.
 388           // Process next node on the worklist &#39;roots&#39;.
 389           break;
 390         }
 391         // Get saved parent node and next input&#39;s index.
 392         parent_node = nstack.node();
 393         input_index = nstack.index();
 394         nstack.pop();
 395       }
 396     }
 397   }
 398   return true;
 399 }
 400 
 401 //------------------------------dom_lca----------------------------------------
 402 // Find least common ancestor in dominator tree
 403 // LCA is a current notion of LCA, to be raised above &#39;this&#39;.
 404 // As a convenient boundary condition, return &#39;this&#39; if LCA is NULL.
 405 // Find the LCA of those two nodes.
 406 Block* Block::dom_lca(Block* LCA) {
 407   if (LCA == NULL || LCA == this)  return this;
 408 
 409   Block* anc = this;
 410   while (anc-&gt;_dom_depth &gt; LCA-&gt;_dom_depth)
 411     anc = anc-&gt;_idom;           // Walk up till anc is as high as LCA
 412 
 413   while (LCA-&gt;_dom_depth &gt; anc-&gt;_dom_depth)
 414     LCA = LCA-&gt;_idom;           // Walk up till LCA is as high as anc
 415 
 416   while (LCA != anc) {          // Walk both up till they are the same
 417     LCA = LCA-&gt;_idom;
 418     anc = anc-&gt;_idom;
 419   }
 420 
 421   return LCA;
 422 }
 423 
 424 //--------------------------raise_LCA_above_use--------------------------------
 425 // We are placing a definition, and have been given a def-&gt;use edge.
 426 // The definition must dominate the use, so move the LCA upward in the
 427 // dominator tree to dominate the use.  If the use is a phi, adjust
 428 // the LCA only with the phi input paths which actually use this def.
 429 static Block* raise_LCA_above_use(Block* LCA, Node* use, Node* def, const PhaseCFG* cfg) {
 430   Block* buse = cfg-&gt;get_block_for_node(use);
 431   if (buse == NULL)    return LCA;   // Unused killing Projs have no use block
 432   if (!use-&gt;is_Phi())  return buse-&gt;dom_lca(LCA);
 433   uint pmax = use-&gt;req();       // Number of Phi inputs
 434   // Why does not this loop just break after finding the matching input to
 435   // the Phi?  Well...it&#39;s like this.  I do not have true def-use/use-def
 436   // chains.  Means I cannot distinguish, from the def-use direction, which
 437   // of many use-defs lead from the same use to the same def.  That is, this
 438   // Phi might have several uses of the same def.  Each use appears in a
 439   // different predecessor block.  But when I enter here, I cannot distinguish
 440   // which use-def edge I should find the predecessor block for.  So I find
 441   // them all.  Means I do a little extra work if a Phi uses the same value
 442   // more than once.
 443   for (uint j=1; j&lt;pmax; j++) { // For all inputs
 444     if (use-&gt;in(j) == def) {    // Found matching input?
 445       Block* pred = cfg-&gt;get_block_for_node(buse-&gt;pred(j));
 446       LCA = pred-&gt;dom_lca(LCA);
 447     }
 448   }
 449   return LCA;
 450 }
 451 
 452 //----------------------------raise_LCA_above_marks----------------------------
 453 // Return a new LCA that dominates LCA and any of its marked predecessors.
 454 // Search all my parents up to &#39;early&#39; (exclusive), looking for predecessors
 455 // which are marked with the given index.  Return the LCA (in the dom tree)
 456 // of all marked blocks.  If there are none marked, return the original
 457 // LCA.
 458 static Block* raise_LCA_above_marks(Block* LCA, node_idx_t mark, Block* early, const PhaseCFG* cfg) {
 459   Block_List worklist;
 460   worklist.push(LCA);
 461   while (worklist.size() &gt; 0) {
 462     Block* mid = worklist.pop();
 463     if (mid == early)  continue;  // stop searching here
 464 
 465     // Test and set the visited bit.
 466     if (mid-&gt;raise_LCA_visited() == mark)  continue;  // already visited
 467 
 468     // Don&#39;t process the current LCA, otherwise the search may terminate early
 469     if (mid != LCA &amp;&amp; mid-&gt;raise_LCA_mark() == mark) {
 470       // Raise the LCA.
 471       LCA = mid-&gt;dom_lca(LCA);
 472       if (LCA == early)  break;   // stop searching everywhere
 473       assert(early-&gt;dominates(LCA), &quot;early is high enough&quot;);
 474       // Resume searching at that point, skipping intermediate levels.
 475       worklist.push(LCA);
 476       if (LCA == mid)
 477         continue; // Don&#39;t mark as visited to avoid early termination.
 478     } else {
 479       // Keep searching through this block&#39;s predecessors.
 480       for (uint j = 1, jmax = mid-&gt;num_preds(); j &lt; jmax; j++) {
 481         Block* mid_parent = cfg-&gt;get_block_for_node(mid-&gt;pred(j));
 482         worklist.push(mid_parent);
 483       }
 484     }
 485     mid-&gt;set_raise_LCA_visited(mark);
 486   }
 487   return LCA;
 488 }
 489 
 490 //--------------------------memory_early_block--------------------------------
 491 // This is a variation of find_deepest_input, the heart of schedule_early.
 492 // Find the &quot;early&quot; block for a load, if we considered only memory and
 493 // address inputs, that is, if other data inputs were ignored.
 494 //
 495 // Because a subset of edges are considered, the resulting block will
 496 // be earlier (at a shallower dom_depth) than the true schedule_early
 497 // point of the node. We compute this earlier block as a more permissive
 498 // site for anti-dependency insertion, but only if subsume_loads is enabled.
 499 static Block* memory_early_block(Node* load, Block* early, const PhaseCFG* cfg) {
 500   Node* base;
 501   Node* index;
 502   Node* store = load-&gt;in(MemNode::Memory);
 503   load-&gt;as_Mach()-&gt;memory_inputs(base, index);
 504 
 505   assert(base != NodeSentinel &amp;&amp; index != NodeSentinel,
 506          &quot;unexpected base/index inputs&quot;);
 507 
 508   Node* mem_inputs[4];
 509   int mem_inputs_length = 0;
 510   if (base != NULL)  mem_inputs[mem_inputs_length++] = base;
 511   if (index != NULL) mem_inputs[mem_inputs_length++] = index;
 512   if (store != NULL) mem_inputs[mem_inputs_length++] = store;
 513 
 514   // In the comparision below, add one to account for the control input,
 515   // which may be null, but always takes up a spot in the in array.
 516   if (mem_inputs_length + 1 &lt; (int) load-&gt;req()) {
 517     // This &quot;load&quot; has more inputs than just the memory, base and index inputs.
 518     // For purposes of checking anti-dependences, we need to start
 519     // from the early block of only the address portion of the instruction,
 520     // and ignore other blocks that may have factored into the wider
 521     // schedule_early calculation.
 522     if (load-&gt;in(0) != NULL) mem_inputs[mem_inputs_length++] = load-&gt;in(0);
 523 
 524     Block* deepb           = NULL;        // Deepest block so far
 525     int    deepb_dom_depth = 0;
 526     for (int i = 0; i &lt; mem_inputs_length; i++) {
 527       Block* inb = cfg-&gt;get_block_for_node(mem_inputs[i]);
 528       if (deepb_dom_depth &lt; (int) inb-&gt;_dom_depth) {
 529         // The new inb must be dominated by the previous deepb.
 530         // The various inputs must be linearly ordered in the dom
 531         // tree, or else there will not be a unique deepest block.
 532         DEBUG_ONLY(assert_dom(deepb, inb, load, cfg));
 533         deepb = inb;                      // Save deepest block
 534         deepb_dom_depth = deepb-&gt;_dom_depth;
 535       }
 536     }
 537     early = deepb;
 538   }
 539 
 540   return early;
 541 }
 542 
 543 //--------------------------insert_anti_dependences---------------------------
 544 // A load may need to witness memory that nearby stores can overwrite.
 545 // For each nearby store, either insert an &quot;anti-dependence&quot; edge
 546 // from the load to the store, or else move LCA upward to force the
 547 // load to (eventually) be scheduled in a block above the store.
 548 //
 549 // Do not add edges to stores on distinct control-flow paths;
 550 // only add edges to stores which might interfere.
 551 //
 552 // Return the (updated) LCA.  There will not be any possibly interfering
 553 // store between the load&#39;s &quot;early block&quot; and the updated LCA.
 554 // Any stores in the updated LCA will have new precedence edges
 555 // back to the load.  The caller is expected to schedule the load
 556 // in the LCA, in which case the precedence edges will make LCM
 557 // preserve anti-dependences.  The caller may also hoist the load
 558 // above the LCA, if it is not the early block.
 559 Block* PhaseCFG::insert_anti_dependences(Block* LCA, Node* load, bool verify) {
 560   assert(load-&gt;needs_anti_dependence_check(), &quot;must be a load of some sort&quot;);
 561   assert(LCA != NULL, &quot;&quot;);
 562   DEBUG_ONLY(Block* LCA_orig = LCA);
 563 
 564   // Compute the alias index.  Loads and stores with different alias indices
 565   // do not need anti-dependence edges.
 566   int load_alias_idx = C-&gt;get_alias_index(load-&gt;adr_type());
 567 #ifdef ASSERT
 568   assert(Compile::AliasIdxTop &lt;= load_alias_idx &amp;&amp; load_alias_idx &lt; C-&gt;num_alias_types(), &quot;Invalid alias index&quot;);
 569   if (load_alias_idx == Compile::AliasIdxBot &amp;&amp; C-&gt;AliasLevel() &gt; 0 &amp;&amp;
 570       (PrintOpto || VerifyAliases ||
 571        (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose)))) {
 572     // Load nodes should not consume all of memory.
 573     // Reporting a bottom type indicates a bug in adlc.
 574     // If some particular type of node validly consumes all of memory,
 575     // sharpen the preceding &quot;if&quot; to exclude it, so we can catch bugs here.
 576     tty-&gt;print_cr(&quot;*** Possible Anti-Dependence Bug:  Load consumes all of memory.&quot;);
 577     load-&gt;dump(2);
 578     if (VerifyAliases)  assert(load_alias_idx != Compile::AliasIdxBot, &quot;&quot;);
 579   }
 580 #endif
 581 
 582   if (!C-&gt;alias_type(load_alias_idx)-&gt;is_rewritable()) {
 583     // It is impossible to spoil this load by putting stores before it,
 584     // because we know that the stores will never update the value
 585     // which &#39;load&#39; must witness.
 586     return LCA;
 587   }
 588 
 589   node_idx_t load_index = load-&gt;_idx;
 590 
 591   // Note the earliest legal placement of &#39;load&#39;, as determined by
 592   // by the unique point in the dom tree where all memory effects
 593   // and other inputs are first available.  (Computed by schedule_early.)
 594   // For normal loads, &#39;early&#39; is the shallowest place (dom graph wise)
 595   // to look for anti-deps between this load and any store.
 596   Block* early = get_block_for_node(load);
 597 
 598   // If we are subsuming loads, compute an &quot;early&quot; block that only considers
 599   // memory or address inputs. This block may be different than the
 600   // schedule_early block in that it could be at an even shallower depth in the
 601   // dominator tree, and allow for a broader discovery of anti-dependences.
 602   if (C-&gt;subsume_loads()) {
 603     early = memory_early_block(load, early, this);
 604   }
 605 
 606   ResourceArea *area = Thread::current()-&gt;resource_area();
 607   Node_List worklist_mem(area);     // prior memory state to store
 608   Node_List worklist_store(area);   // possible-def to explore
 609   Node_List worklist_visited(area); // visited mergemem nodes
 610   Node_List non_early_stores(area); // all relevant stores outside of early
 611   bool must_raise_LCA = false;
 612 
 613 #ifdef TRACK_PHI_INPUTS
 614   // %%% This extra checking fails because MergeMem nodes are not GVNed.
 615   // Provide &quot;phi_inputs&quot; to check if every input to a PhiNode is from the
 616   // original memory state.  This indicates a PhiNode for which should not
 617   // prevent the load from sinking.  For such a block, set_raise_LCA_mark
 618   // may be overly conservative.
 619   // Mechanism: count inputs seen for each Phi encountered in worklist_store.
 620   DEBUG_ONLY(GrowableArray&lt;uint&gt; phi_inputs(area, C-&gt;unique(),0,0));
 621 #endif
 622 
 623   // &#39;load&#39; uses some memory state; look for users of the same state.
 624   // Recurse through MergeMem nodes to the stores that use them.
 625 
 626   // Each of these stores is a possible definition of memory
 627   // that &#39;load&#39; needs to use.  We need to force &#39;load&#39;
 628   // to occur before each such store.  When the store is in
 629   // the same block as &#39;load&#39;, we insert an anti-dependence
 630   // edge load-&gt;store.
 631 
 632   // The relevant stores &quot;nearby&quot; the load consist of a tree rooted
 633   // at initial_mem, with internal nodes of type MergeMem.
 634   // Therefore, the branches visited by the worklist are of this form:
 635   //    initial_mem -&gt; (MergeMem -&gt;)* store
 636   // The anti-dependence constraints apply only to the fringe of this tree.
 637 
 638   Node* initial_mem = load-&gt;in(MemNode::Memory);
 639   worklist_store.push(initial_mem);
 640   worklist_visited.push(initial_mem);
 641   worklist_mem.push(NULL);
 642   while (worklist_store.size() &gt; 0) {
 643     // Examine a nearby store to see if it might interfere with our load.
 644     Node* mem   = worklist_mem.pop();
 645     Node* store = worklist_store.pop();
 646     uint op = store-&gt;Opcode();
 647 
 648     // MergeMems do not directly have anti-deps.
 649     // Treat them as internal nodes in a forward tree of memory states,
 650     // the leaves of which are each a &#39;possible-def&#39;.
 651     if (store == initial_mem    // root (exclusive) of tree we are searching
 652         || op == Op_MergeMem    // internal node of tree we are searching
 653         ) {
 654       mem = store;   // It&#39;s not a possibly interfering store.
 655       if (store == initial_mem)
 656         initial_mem = NULL;  // only process initial memory once
 657 
 658       for (DUIterator_Fast imax, i = mem-&gt;fast_outs(imax); i &lt; imax; i++) {
 659         store = mem-&gt;fast_out(i);
 660         if (store-&gt;is_MergeMem()) {
 661           // Be sure we don&#39;t get into combinatorial problems.
 662           // (Allow phis to be repeated; they can merge two relevant states.)
 663           uint j = worklist_visited.size();
 664           for (; j &gt; 0; j--) {
 665             if (worklist_visited.at(j-1) == store)  break;
 666           }
 667           if (j &gt; 0)  continue; // already on work list; do not repeat
 668           worklist_visited.push(store);
 669         }
 670         worklist_mem.push(mem);
 671         worklist_store.push(store);
 672       }
 673       continue;
 674     }
 675 
 676     if (op == Op_MachProj || op == Op_Catch)   continue;
 677     if (store-&gt;needs_anti_dependence_check())  continue;  // not really a store
 678 
 679     // Compute the alias index.  Loads and stores with different alias
 680     // indices do not need anti-dependence edges.  Wide MemBar&#39;s are
 681     // anti-dependent on everything (except immutable memories).
 682     const TypePtr* adr_type = store-&gt;adr_type();
 683     if (!C-&gt;can_alias(adr_type, load_alias_idx))  continue;
 684 
 685     // Most slow-path runtime calls do NOT modify Java memory, but
 686     // they can block and so write Raw memory.
 687     if (store-&gt;is_Mach()) {
 688       MachNode* mstore = store-&gt;as_Mach();
 689       if (load_alias_idx != Compile::AliasIdxRaw) {
 690         // Check for call into the runtime using the Java calling
 691         // convention (and from there into a wrapper); it has no
 692         // _method.  Can&#39;t do this optimization for Native calls because
 693         // they CAN write to Java memory.
 694         if (mstore-&gt;ideal_Opcode() == Op_CallStaticJava) {
 695           assert(mstore-&gt;is_MachSafePoint(), &quot;&quot;);
 696           MachSafePointNode* ms = (MachSafePointNode*) mstore;
 697           assert(ms-&gt;is_MachCallJava(), &quot;&quot;);
 698           MachCallJavaNode* mcj = (MachCallJavaNode*) ms;
 699           if (mcj-&gt;_method == NULL) {
 700             // These runtime calls do not write to Java visible memory
 701             // (other than Raw) and so do not require anti-dependence edges.
 702             continue;
 703           }
 704         }
 705         // Same for SafePoints: they read/write Raw but only read otherwise.
 706         // This is basically a workaround for SafePoints only defining control
 707         // instead of control + memory.
 708         if (mstore-&gt;ideal_Opcode() == Op_SafePoint)
 709           continue;
<a name="1" id="anc1"></a>












 710       } else {
 711         // Some raw memory, such as the load of &quot;top&quot; at an allocation,
 712         // can be control dependent on the previous safepoint. See
 713         // comments in GraphKit::allocate_heap() about control input.
 714         // Inserting an anti-dep between such a safepoint and a use
 715         // creates a cycle, and will cause a subsequent failure in
 716         // local scheduling.  (BugId 4919904)
 717         // (%%% How can a control input be a safepoint and not a projection??)
 718         if (mstore-&gt;ideal_Opcode() == Op_SafePoint &amp;&amp; load-&gt;in(0) == mstore)
 719           continue;
 720       }
 721     }
 722 
 723     // Identify a block that the current load must be above,
 724     // or else observe that &#39;store&#39; is all the way up in the
 725     // earliest legal block for &#39;load&#39;.  In the latter case,
 726     // immediately insert an anti-dependence edge.
 727     Block* store_block = get_block_for_node(store);
 728     assert(store_block != NULL, &quot;unused killing projections skipped above&quot;);
 729 
 730     if (store-&gt;is_Phi()) {
 731       // Loop-phis need to raise load before input. (Other phis are treated
 732       // as store below.)
 733       //
 734       // &#39;load&#39; uses memory which is one (or more) of the Phi&#39;s inputs.
 735       // It must be scheduled not before the Phi, but rather before
 736       // each of the relevant Phi inputs.
 737       //
 738       // Instead of finding the LCA of all inputs to a Phi that match &#39;mem&#39;,
 739       // we mark each corresponding predecessor block and do a combined
 740       // hoisting operation later (raise_LCA_above_marks).
 741       //
 742       // Do not assert(store_block != early, &quot;Phi merging memory after access&quot;)
 743       // PhiNode may be at start of block &#39;early&#39; with backedge to &#39;early&#39;
 744       DEBUG_ONLY(bool found_match = false);
 745       for (uint j = PhiNode::Input, jmax = store-&gt;req(); j &lt; jmax; j++) {
 746         if (store-&gt;in(j) == mem) {   // Found matching input?
 747           DEBUG_ONLY(found_match = true);
 748           Block* pred_block = get_block_for_node(store_block-&gt;pred(j));
 749           if (pred_block != early) {
 750             // If any predecessor of the Phi matches the load&#39;s &quot;early block&quot;,
 751             // we do not need a precedence edge between the Phi and &#39;load&#39;
 752             // since the load will be forced into a block preceding the Phi.
 753             pred_block-&gt;set_raise_LCA_mark(load_index);
 754             assert(!LCA_orig-&gt;dominates(pred_block) ||
 755                    early-&gt;dominates(pred_block), &quot;early is high enough&quot;);
 756             must_raise_LCA = true;
 757           } else {
 758             // anti-dependent upon PHI pinned below &#39;early&#39;, no edge needed
 759             LCA = early;             // but can not schedule below &#39;early&#39;
 760           }
 761         }
 762       }
 763       assert(found_match, &quot;no worklist bug&quot;);
 764 #ifdef TRACK_PHI_INPUTS
 765 #ifdef ASSERT
 766         // This assert asks about correct handling of PhiNodes, which may not
 767         // have all input edges directly from &#39;mem&#39;. See BugId 4621264
 768         int num_mem_inputs = phi_inputs.at_grow(store-&gt;_idx,0) + 1;
 769         // Increment by exactly one even if there are multiple copies of &#39;mem&#39;
 770         // coming into the phi, because we will run this block several times
 771         // if there are several copies of &#39;mem&#39;.  (That&#39;s how DU iterators work.)
 772         phi_inputs.at_put(store-&gt;_idx, num_mem_inputs);
 773         assert(PhiNode::Input + num_mem_inputs &lt; store-&gt;req(),
 774                &quot;Expect at least one phi input will not be from original memory state&quot;);
 775 #endif //ASSERT
 776 #endif //TRACK_PHI_INPUTS
 777     } else if (store_block != early) {
 778       // &#39;store&#39; is between the current LCA and earliest possible block.
 779       // Label its block, and decide later on how to raise the LCA
 780       // to include the effect on LCA of this store.
 781       // If this store&#39;s block gets chosen as the raised LCA, we
 782       // will find him on the non_early_stores list and stick him
 783       // with a precedence edge.
 784       // (But, don&#39;t bother if LCA is already raised all the way.)
 785       if (LCA != early) {
 786         store_block-&gt;set_raise_LCA_mark(load_index);
 787         must_raise_LCA = true;
 788         non_early_stores.push(store);
 789       }
 790     } else {
 791       // Found a possibly-interfering store in the load&#39;s &#39;early&#39; block.
 792       // This means &#39;load&#39; cannot sink at all in the dominator tree.
 793       // Add an anti-dep edge, and squeeze &#39;load&#39; into the highest block.
 794       assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);
 795       if (verify) {
 796         assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 797       } else {
 798         store-&gt;add_prec(load);
 799       }
 800       LCA = early;
 801       // This turns off the process of gathering non_early_stores.
 802     }
 803   }
 804   // (Worklist is now empty; all nearby stores have been visited.)
 805 
 806   // Finished if &#39;load&#39; must be scheduled in its &#39;early&#39; block.
 807   // If we found any stores there, they have already been given
 808   // precedence edges.
 809   if (LCA == early)  return LCA;
 810 
 811   // We get here only if there are no possibly-interfering stores
 812   // in the load&#39;s &#39;early&#39; block.  Move LCA up above all predecessors
 813   // which contain stores we have noted.
 814   //
 815   // The raised LCA block can be a home to such interfering stores,
 816   // but its predecessors must not contain any such stores.
 817   //
 818   // The raised LCA will be a lower bound for placing the load,
 819   // preventing the load from sinking past any block containing
 820   // a store that may invalidate the memory state required by &#39;load&#39;.
 821   if (must_raise_LCA)
 822     LCA = raise_LCA_above_marks(LCA, load-&gt;_idx, early, this);
 823   if (LCA == early)  return LCA;
 824 
 825   // Insert anti-dependence edges from &#39;load&#39; to each store
 826   // in the non-early LCA block.
 827   // Mine the non_early_stores list for such stores.
 828   if (LCA-&gt;raise_LCA_mark() == load_index) {
 829     while (non_early_stores.size() &gt; 0) {
 830       Node* store = non_early_stores.pop();
 831       Block* store_block = get_block_for_node(store);
 832       if (store_block == LCA) {
 833         // add anti_dependence from store to load in its own block
 834         assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);
 835         if (verify) {
 836           assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 837         } else {
 838           store-&gt;add_prec(load);
 839         }
 840       } else {
 841         assert(store_block-&gt;raise_LCA_mark() == load_index, &quot;block was marked&quot;);
 842         // Any other stores we found must be either inside the new LCA
 843         // or else outside the original LCA.  In the latter case, they
 844         // did not interfere with any use of &#39;load&#39;.
 845         assert(LCA-&gt;dominates(store_block)
 846                || !LCA_orig-&gt;dominates(store_block), &quot;no stray stores&quot;);
 847       }
 848     }
 849   }
 850 
 851   // Return the highest block containing stores; any stores
 852   // within that block have been given anti-dependence edges.
 853   return LCA;
 854 }
 855 
 856 // This class is used to iterate backwards over the nodes in the graph.
 857 
 858 class Node_Backward_Iterator {
 859 
 860 private:
 861   Node_Backward_Iterator();
 862 
 863 public:
 864   // Constructor for the iterator
 865   Node_Backward_Iterator(Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg);
 866 
 867   // Postincrement operator to iterate over the nodes
 868   Node *next();
 869 
 870 private:
 871   VectorSet   &amp;_visited;
 872   Node_Stack  &amp;_stack;
 873   PhaseCFG &amp;_cfg;
 874 };
 875 
 876 // Constructor for the Node_Backward_Iterator
 877 Node_Backward_Iterator::Node_Backward_Iterator( Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg)
 878   : _visited(visited), _stack(stack), _cfg(cfg) {
 879   // The stack should contain exactly the root
 880   stack.clear();
 881   stack.push(root, root-&gt;outcnt());
 882 
 883   // Clear the visited bits
 884   visited.clear();
 885 }
 886 
 887 // Iterator for the Node_Backward_Iterator
 888 Node *Node_Backward_Iterator::next() {
 889 
 890   // If the _stack is empty, then just return NULL: finished.
 891   if ( !_stack.size() )
 892     return NULL;
 893 
 894   // I visit unvisited not-anti-dependence users first, then anti-dependent
 895   // children next. I iterate backwards to support removal of nodes.
 896   // The stack holds states consisting of 3 values:
 897   // current Def node, flag which indicates 1st/2nd pass, index of current out edge
 898   Node *self = (Node*)(((uintptr_t)_stack.node()) &amp; ~1);
 899   bool iterate_anti_dep = (((uintptr_t)_stack.node()) &amp; 1);
 900   uint idx = MIN2(_stack.index(), self-&gt;outcnt()); // Support removal of nodes.
 901   _stack.pop();
 902 
 903   // I cycle here when I am entering a deeper level of recursion.
 904   // The key variable &#39;self&#39; was set prior to jumping here.
 905   while( 1 ) {
 906 
 907     _visited.set(self-&gt;_idx);
 908 
 909     // Now schedule all uses as late as possible.
 910     const Node* src = self-&gt;is_Proj() ? self-&gt;in(0) : self;
 911     uint src_rpo = _cfg.get_block_for_node(src)-&gt;_rpo;
 912 
 913     // Schedule all nodes in a post-order visit
 914     Node *unvisited = NULL;  // Unvisited anti-dependent Node, if any
 915 
 916     // Scan for unvisited nodes
 917     while (idx &gt; 0) {
 918       // For all uses, schedule late
 919       Node* n = self-&gt;raw_out(--idx); // Use
 920 
 921       // Skip already visited children
 922       if ( _visited.test(n-&gt;_idx) )
 923         continue;
 924 
 925       // do not traverse backward control edges
 926       Node *use = n-&gt;is_Proj() ? n-&gt;in(0) : n;
 927       uint use_rpo = _cfg.get_block_for_node(use)-&gt;_rpo;
 928 
 929       if ( use_rpo &lt; src_rpo )
 930         continue;
 931 
 932       // Phi nodes always precede uses in a basic block
 933       if ( use_rpo == src_rpo &amp;&amp; use-&gt;is_Phi() )
 934         continue;
 935 
 936       unvisited = n;      // Found unvisited
 937 
 938       // Check for possible-anti-dependent
 939       // 1st pass: No such nodes, 2nd pass: Only such nodes.
 940       if (n-&gt;needs_anti_dependence_check() == iterate_anti_dep) {
 941         unvisited = n;      // Found unvisited
 942         break;
 943       }
 944     }
 945 
 946     // Did I find an unvisited not-anti-dependent Node?
 947     if (!unvisited) {
 948       if (!iterate_anti_dep) {
 949         // 2nd pass: Iterate over nodes which needs_anti_dependence_check.
 950         iterate_anti_dep = true;
 951         idx = self-&gt;outcnt();
 952         continue;
 953       }
 954       break;                  // All done with children; post-visit &#39;self&#39;
 955     }
 956 
 957     // Visit the unvisited Node.  Contains the obvious push to
 958     // indicate I&#39;m entering a deeper level of recursion.  I push the
 959     // old state onto the _stack and set a new state and loop (recurse).
 960     _stack.push((Node*)((uintptr_t)self | (uintptr_t)iterate_anti_dep), idx);
 961     self = unvisited;
 962     iterate_anti_dep = false;
 963     idx = self-&gt;outcnt();
 964   } // End recursion loop
 965 
 966   return self;
 967 }
 968 
 969 //------------------------------ComputeLatenciesBackwards----------------------
 970 // Compute the latency of all the instructions.
 971 void PhaseCFG::compute_latencies_backwards(VectorSet &amp;visited, Node_Stack &amp;stack) {
 972 #ifndef PRODUCT
 973   if (trace_opto_pipelining())
 974     tty-&gt;print(&quot;\n#---- ComputeLatenciesBackwards ----\n&quot;);
 975 #endif
 976 
 977   Node_Backward_Iterator iter((Node *)_root, visited, stack, *this);
 978   Node *n;
 979 
 980   // Walk over all the nodes from last to first
 981   while ((n = iter.next())) {
 982     // Set the latency for the definitions of this instruction
 983     partial_latency_of_defs(n);
 984   }
 985 } // end ComputeLatenciesBackwards
 986 
 987 //------------------------------partial_latency_of_defs------------------------
 988 // Compute the latency impact of this node on all defs.  This computes
 989 // a number that increases as we approach the beginning of the routine.
 990 void PhaseCFG::partial_latency_of_defs(Node *n) {
 991   // Set the latency for this instruction
 992 #ifndef PRODUCT
 993   if (trace_opto_pipelining()) {
 994     tty-&gt;print(&quot;# latency_to_inputs: node_latency[%d] = %d for node&quot;, n-&gt;_idx, get_latency_for_node(n));
 995     dump();
 996   }
 997 #endif
 998 
 999   if (n-&gt;is_Proj()) {
1000     n = n-&gt;in(0);
1001   }
1002 
1003   if (n-&gt;is_Root()) {
1004     return;
1005   }
1006 
1007   uint nlen = n-&gt;len();
1008   uint use_latency = get_latency_for_node(n);
1009   uint use_pre_order = get_block_for_node(n)-&gt;_pre_order;
1010 
1011   for (uint j = 0; j &lt; nlen; j++) {
1012     Node *def = n-&gt;in(j);
1013 
1014     if (!def || def == n) {
1015       continue;
1016     }
1017 
1018     // Walk backwards thru projections
1019     if (def-&gt;is_Proj()) {
1020       def = def-&gt;in(0);
1021     }
1022 
1023 #ifndef PRODUCT
1024     if (trace_opto_pipelining()) {
1025       tty-&gt;print(&quot;#    in(%2d): &quot;, j);
1026       def-&gt;dump();
1027     }
1028 #endif
1029 
1030     // If the defining block is not known, assume it is ok
1031     Block *def_block = get_block_for_node(def);
1032     uint def_pre_order = def_block ? def_block-&gt;_pre_order : 0;
1033 
1034     if ((use_pre_order &lt;  def_pre_order) || (use_pre_order == def_pre_order &amp;&amp; n-&gt;is_Phi())) {
1035       continue;
1036     }
1037 
1038     uint delta_latency = n-&gt;latency(j);
1039     uint current_latency = delta_latency + use_latency;
1040 
1041     if (get_latency_for_node(def) &lt; current_latency) {
1042       set_latency_for_node(def, current_latency);
1043     }
1044 
1045 #ifndef PRODUCT
1046     if (trace_opto_pipelining()) {
1047       tty-&gt;print_cr(&quot;#      %d + edge_latency(%d) == %d -&gt; %d, node_latency[%d] = %d&quot;, use_latency, j, delta_latency, current_latency, def-&gt;_idx, get_latency_for_node(def));
1048     }
1049 #endif
1050   }
1051 }
1052 
1053 //------------------------------latency_from_use-------------------------------
1054 // Compute the latency of a specific use
1055 int PhaseCFG::latency_from_use(Node *n, const Node *def, Node *use) {
1056   // If self-reference, return no latency
1057   if (use == n || use-&gt;is_Root()) {
1058     return 0;
1059   }
1060 
1061   uint def_pre_order = get_block_for_node(def)-&gt;_pre_order;
1062   uint latency = 0;
1063 
1064   // If the use is not a projection, then it is simple...
1065   if (!use-&gt;is_Proj()) {
1066 #ifndef PRODUCT
1067     if (trace_opto_pipelining()) {
1068       tty-&gt;print(&quot;#    out(): &quot;);
1069       use-&gt;dump();
1070     }
1071 #endif
1072 
1073     uint use_pre_order = get_block_for_node(use)-&gt;_pre_order;
1074 
1075     if (use_pre_order &lt; def_pre_order)
1076       return 0;
1077 
1078     if (use_pre_order == def_pre_order &amp;&amp; use-&gt;is_Phi())
1079       return 0;
1080 
1081     uint nlen = use-&gt;len();
1082     uint nl = get_latency_for_node(use);
1083 
1084     for ( uint j=0; j&lt;nlen; j++ ) {
1085       if (use-&gt;in(j) == n) {
1086         // Change this if we want local latencies
1087         uint ul = use-&gt;latency(j);
1088         uint  l = ul + nl;
1089         if (latency &lt; l) latency = l;
1090 #ifndef PRODUCT
1091         if (trace_opto_pipelining()) {
1092           tty-&gt;print_cr(&quot;#      %d + edge_latency(%d) == %d -&gt; %d, latency = %d&quot;,
1093                         nl, j, ul, l, latency);
1094         }
1095 #endif
1096       }
1097     }
1098   } else {
1099     // This is a projection, just grab the latency of the use(s)
1100     for (DUIterator_Fast jmax, j = use-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1101       uint l = latency_from_use(use, def, use-&gt;fast_out(j));
1102       if (latency &lt; l) latency = l;
1103     }
1104   }
1105 
1106   return latency;
1107 }
1108 
1109 //------------------------------latency_from_uses------------------------------
1110 // Compute the latency of this instruction relative to all of it&#39;s uses.
1111 // This computes a number that increases as we approach the beginning of the
1112 // routine.
1113 void PhaseCFG::latency_from_uses(Node *n) {
1114   // Set the latency for this instruction
1115 #ifndef PRODUCT
1116   if (trace_opto_pipelining()) {
1117     tty-&gt;print(&quot;# latency_from_outputs: node_latency[%d] = %d for node&quot;, n-&gt;_idx, get_latency_for_node(n));
1118     dump();
1119   }
1120 #endif
1121   uint latency=0;
1122   const Node *def = n-&gt;is_Proj() ? n-&gt;in(0): n;
1123 
1124   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
1125     uint l = latency_from_use(n, def, n-&gt;fast_out(i));
1126 
1127     if (latency &lt; l) latency = l;
1128   }
1129 
1130   set_latency_for_node(n, latency);
1131 }
1132 
1133 //------------------------------hoist_to_cheaper_block-------------------------
1134 // Pick a block for node self, between early and LCA, that is a cheaper
1135 // alternative to LCA.
1136 Block* PhaseCFG::hoist_to_cheaper_block(Block* LCA, Block* early, Node* self) {
1137   const double delta = 1+PROB_UNLIKELY_MAG(4);
1138   Block* least       = LCA;
1139   double least_freq  = least-&gt;_freq;
1140   uint target        = get_latency_for_node(self);
1141   uint start_latency = get_latency_for_node(LCA-&gt;head());
1142   uint end_latency   = get_latency_for_node(LCA-&gt;get_node(LCA-&gt;end_idx()));
1143   bool in_latency    = (target &lt;= start_latency);
1144   const Block* root_block = get_block_for_node(_root);
1145 
1146   // Turn off latency scheduling if scheduling is just plain off
1147   if (!C-&gt;do_scheduling())
1148     in_latency = true;
1149 
1150   // Do not hoist (to cover latency) instructions which target a
1151   // single register.  Hoisting stretches the live range of the
1152   // single register and may force spilling.
1153   MachNode* mach = self-&gt;is_Mach() ? self-&gt;as_Mach() : NULL;
1154   if (mach &amp;&amp; mach-&gt;out_RegMask().is_bound1() &amp;&amp; mach-&gt;out_RegMask().is_NotEmpty())
1155     in_latency = true;
1156 
1157 #ifndef PRODUCT
1158   if (trace_opto_pipelining()) {
1159     tty-&gt;print(&quot;# Find cheaper block for latency %d: &quot;, get_latency_for_node(self));
1160     self-&gt;dump();
1161     tty-&gt;print_cr(&quot;#   B%d: start latency for [%4d]=%d, end latency for [%4d]=%d, freq=%g&quot;,
1162       LCA-&gt;_pre_order,
1163       LCA-&gt;head()-&gt;_idx,
1164       start_latency,
1165       LCA-&gt;get_node(LCA-&gt;end_idx())-&gt;_idx,
1166       end_latency,
1167       least_freq);
1168   }
1169 #endif
1170 
1171   int cand_cnt = 0;  // number of candidates tried
1172 
1173   // Walk up the dominator tree from LCA (Lowest common ancestor) to
1174   // the earliest legal location.  Capture the least execution frequency.
1175   while (LCA != early) {
1176     LCA = LCA-&gt;_idom;         // Follow up the dominator tree
1177 
1178     if (LCA == NULL) {
1179       // Bailout without retry
1180       assert(false, &quot;graph should be schedulable&quot;);
1181       C-&gt;record_method_not_compilable(&quot;late schedule failed: LCA == NULL&quot;);
1182       return least;
1183     }
1184 
1185     // Don&#39;t hoist machine instructions to the root basic block
1186     if (mach &amp;&amp; LCA == root_block)
1187       break;
1188 
1189     uint start_lat = get_latency_for_node(LCA-&gt;head());
1190     uint end_idx   = LCA-&gt;end_idx();
1191     uint end_lat   = get_latency_for_node(LCA-&gt;get_node(end_idx));
1192     double LCA_freq = LCA-&gt;_freq;
1193 #ifndef PRODUCT
1194     if (trace_opto_pipelining()) {
1195       tty-&gt;print_cr(&quot;#   B%d: start latency for [%4d]=%d, end latency for [%4d]=%d, freq=%g&quot;,
1196         LCA-&gt;_pre_order, LCA-&gt;head()-&gt;_idx, start_lat, end_idx, end_lat, LCA_freq);
1197     }
1198 #endif
1199     cand_cnt++;
1200     if (LCA_freq &lt; least_freq              || // Better Frequency
1201         (StressGCM &amp;&amp; Compile::randomized_select(cand_cnt)) || // Should be randomly accepted in stress mode
1202          (!StressGCM                    &amp;&amp;    // Otherwise, choose with latency
1203           !in_latency                   &amp;&amp;    // No block containing latency
1204           LCA_freq &lt; least_freq * delta &amp;&amp;    // No worse frequency
1205           target &gt;= end_lat             &amp;&amp;    // within latency range
1206           !self-&gt;is_iteratively_computed() )  // But don&#39;t hoist IV increments
1207              // because they may end up above other uses of their phi forcing
1208              // their result register to be different from their input.
1209        ) {
1210       least = LCA;            // Found cheaper block
1211       least_freq = LCA_freq;
1212       start_latency = start_lat;
1213       end_latency = end_lat;
1214       if (target &lt;= start_lat)
1215         in_latency = true;
1216     }
1217   }
1218 
1219 #ifndef PRODUCT
1220   if (trace_opto_pipelining()) {
1221     tty-&gt;print_cr(&quot;#  Choose block B%d with start latency=%d and freq=%g&quot;,
1222       least-&gt;_pre_order, start_latency, least_freq);
1223   }
1224 #endif
1225 
1226   // See if the latency needs to be updated
1227   if (target &lt; end_latency) {
1228 #ifndef PRODUCT
1229     if (trace_opto_pipelining()) {
1230       tty-&gt;print_cr(&quot;#  Change latency for [%4d] from %d to %d&quot;, self-&gt;_idx, target, end_latency);
1231     }
1232 #endif
1233     set_latency_for_node(self, end_latency);
1234     partial_latency_of_defs(self);
1235   }
1236 
1237   return least;
1238 }
1239 
1240 
1241 //------------------------------schedule_late-----------------------------------
1242 // Now schedule all codes as LATE as possible.  This is the LCA in the
1243 // dominator tree of all USES of a value.  Pick the block with the least
1244 // loop nesting depth that is lowest in the dominator tree.
1245 extern const char must_clone[];
1246 void PhaseCFG::schedule_late(VectorSet &amp;visited, Node_Stack &amp;stack) {
1247 #ifndef PRODUCT
1248   if (trace_opto_pipelining())
1249     tty-&gt;print(&quot;\n#---- schedule_late ----\n&quot;);
1250 #endif
1251 
1252   Node_Backward_Iterator iter((Node *)_root, visited, stack, *this);
1253   Node *self;
1254 
1255   // Walk over all the nodes from last to first
1256   while ((self = iter.next())) {
1257     Block* early = get_block_for_node(self); // Earliest legal placement
1258 
1259     if (self-&gt;is_top()) {
1260       // Top node goes in bb #2 with other constants.
1261       // It must be special-cased, because it has no out edges.
1262       early-&gt;add_inst(self);
1263       continue;
1264     }
1265 
1266     // No uses, just terminate
1267     if (self-&gt;outcnt() == 0) {
1268       assert(self-&gt;is_MachProj(), &quot;sanity&quot;);
1269       continue;                   // Must be a dead machine projection
1270     }
1271 
1272     // If node is pinned in the block, then no scheduling can be done.
1273     if( self-&gt;pinned() )          // Pinned in block?
1274       continue;
1275 
1276     MachNode* mach = self-&gt;is_Mach() ? self-&gt;as_Mach() : NULL;
1277     if (mach) {
1278       switch (mach-&gt;ideal_Opcode()) {
1279       case Op_CreateEx:
1280         // Don&#39;t move exception creation
1281         early-&gt;add_inst(self);
1282         continue;
1283         break;
1284       case Op_CheckCastPP: {
1285         // Don&#39;t move CheckCastPP nodes away from their input, if the input
1286         // is a rawptr (5071820).
1287         Node *def = self-&gt;in(1);
1288         if (def != NULL &amp;&amp; def-&gt;bottom_type()-&gt;base() == Type::RawPtr) {
1289           early-&gt;add_inst(self);
1290 #ifdef ASSERT
1291           _raw_oops.push(def);
1292 #endif
1293           continue;
1294         }
1295         break;
1296       }
1297       default:
1298         break;
1299       }
1300     }
1301 
1302     // Gather LCA of all uses
1303     Block *LCA = NULL;
1304     {
1305       for (DUIterator_Fast imax, i = self-&gt;fast_outs(imax); i &lt; imax; i++) {
1306         // For all uses, find LCA
1307         Node* use = self-&gt;fast_out(i);
1308         LCA = raise_LCA_above_use(LCA, use, self, this);
1309       }
1310       guarantee(LCA != NULL, &quot;There must be a LCA&quot;);
1311     }  // (Hide defs of imax, i from rest of block.)
1312 
1313     // Place temps in the block of their use.  This isn&#39;t a
1314     // requirement for correctness but it reduces useless
1315     // interference between temps and other nodes.
1316     if (mach != NULL &amp;&amp; mach-&gt;is_MachTemp()) {
1317       map_node_to_block(self, LCA);
1318       LCA-&gt;add_inst(self);
1319       continue;
1320     }
1321 
1322     // Check if &#39;self&#39; could be anti-dependent on memory
1323     if (self-&gt;needs_anti_dependence_check()) {
1324       // Hoist LCA above possible-defs and insert anti-dependences to
1325       // defs in new LCA block.
1326       LCA = insert_anti_dependences(LCA, self);
1327     }
1328 
1329     if (early-&gt;_dom_depth &gt; LCA-&gt;_dom_depth) {
1330       // Somehow the LCA has moved above the earliest legal point.
1331       // (One way this can happen is via memory_early_block.)
1332       if (C-&gt;subsume_loads() == true &amp;&amp; !C-&gt;failing()) {
1333         // Retry with subsume_loads == false
1334         // If this is the first failure, the sentinel string will &quot;stick&quot;
1335         // to the Compile object, and the C2Compiler will see it and retry.
1336         C-&gt;record_failure(C2Compiler::retry_no_subsuming_loads());
1337       } else {
1338         // Bailout without retry when (early-&gt;_dom_depth &gt; LCA-&gt;_dom_depth)
1339         assert(false, &quot;graph should be schedulable&quot;);
1340         C-&gt;record_method_not_compilable(&quot;late schedule failed: incorrect graph&quot;);
1341       }
1342       return;
1343     }
1344 
1345     // If there is no opportunity to hoist, then we&#39;re done.
1346     // In stress mode, try to hoist even the single operations.
1347     bool try_to_hoist = StressGCM || (LCA != early);
1348 
1349     // Must clone guys stay next to use; no hoisting allowed.
1350     // Also cannot hoist guys that alter memory or are otherwise not
1351     // allocatable (hoisting can make a value live longer, leading to
1352     // anti and output dependency problems which are normally resolved
1353     // by the register allocator giving everyone a different register).
1354     if (mach != NULL &amp;&amp; must_clone[mach-&gt;ideal_Opcode()])
1355       try_to_hoist = false;
1356 
1357     Block* late = NULL;
1358     if (try_to_hoist) {
1359       // Now find the block with the least execution frequency.
1360       // Start at the latest schedule and work up to the earliest schedule
1361       // in the dominator tree.  Thus the Node will dominate all its uses.
1362       late = hoist_to_cheaper_block(LCA, early, self);
1363     } else {
1364       // Just use the LCA of the uses.
1365       late = LCA;
1366     }
1367 
1368     // Put the node into target block
1369     schedule_node_into_block(self, late);
1370 
1371 #ifdef ASSERT
1372     if (self-&gt;needs_anti_dependence_check()) {
1373       // since precedence edges are only inserted when we&#39;re sure they
1374       // are needed make sure that after placement in a block we don&#39;t
1375       // need any new precedence edges.
1376       verify_anti_dependences(late, self);
1377     }
1378 #endif
1379   } // Loop until all nodes have been visited
1380 
1381 } // end ScheduleLate
1382 
1383 //------------------------------GlobalCodeMotion-------------------------------
1384 void PhaseCFG::global_code_motion() {
1385   ResourceMark rm;
1386 
1387 #ifndef PRODUCT
1388   if (trace_opto_pipelining()) {
1389     tty-&gt;print(&quot;\n---- Start GlobalCodeMotion ----\n&quot;);
1390   }
1391 #endif
1392 
1393   // Initialize the node to block mapping for things on the proj_list
1394   for (uint i = 0; i &lt; _matcher.number_of_projections(); i++) {
1395     unmap_node_from_block(_matcher.get_projection(i));
1396   }
1397 
1398   // Set the basic block for Nodes pinned into blocks
1399   Arena* arena = Thread::current()-&gt;resource_area();
1400   VectorSet visited(arena);
1401   schedule_pinned_nodes(visited);
1402 
1403   // Find the earliest Block any instruction can be placed in.  Some
1404   // instructions are pinned into Blocks.  Unpinned instructions can
1405   // appear in last block in which all their inputs occur.
1406   visited.clear();
1407   Node_Stack stack(arena, (C-&gt;live_nodes() &gt;&gt; 2) + 16); // pre-grow
1408   if (!schedule_early(visited, stack)) {
1409     // Bailout without retry
1410     C-&gt;record_method_not_compilable(&quot;early schedule failed&quot;);
1411     return;
1412   }
1413 
1414   // Build Def-Use edges.
1415   // Compute the latency information (via backwards walk) for all the
1416   // instructions in the graph
1417   _node_latency = new GrowableArray&lt;uint&gt;(); // resource_area allocation
1418 
1419   if (C-&gt;do_scheduling()) {
1420     compute_latencies_backwards(visited, stack);
1421   }
1422 
1423   // Now schedule all codes as LATE as possible.  This is the LCA in the
1424   // dominator tree of all USES of a value.  Pick the block with the least
1425   // loop nesting depth that is lowest in the dominator tree.
1426   // ( visited.clear() called in schedule_late()-&gt;Node_Backward_Iterator() )
1427   schedule_late(visited, stack);
1428   if (C-&gt;failing()) {
1429     return;
1430   }
1431 
1432 #ifndef PRODUCT
1433   if (trace_opto_pipelining()) {
1434     tty-&gt;print(&quot;\n---- Detect implicit null checks ----\n&quot;);
1435   }
1436 #endif
1437 
1438   // Detect implicit-null-check opportunities.  Basically, find NULL checks
1439   // with suitable memory ops nearby.  Use the memory op to do the NULL check.
1440   // I can generate a memory op if there is not one nearby.
1441   if (C-&gt;is_method_compilation()) {
1442     // By reversing the loop direction we get a very minor gain on mpegaudio.
1443     // Feel free to revert to a forward loop for clarity.
1444     // for( int i=0; i &lt; (int)matcher._null_check_tests.size(); i+=2 ) {
1445     for (int i = _matcher._null_check_tests.size() - 2; i &gt;= 0; i -= 2) {
1446       Node* proj = _matcher._null_check_tests[i];
1447       Node* val  = _matcher._null_check_tests[i + 1];
1448       Block* block = get_block_for_node(proj);
1449       implicit_null_check(block, proj, val, C-&gt;allowed_deopt_reasons());
1450       // The implicit_null_check will only perform the transformation
1451       // if the null branch is truly uncommon, *and* it leads to an
1452       // uncommon trap.  Combined with the too_many_traps guards
1453       // above, this prevents SEGV storms reported in 6366351,
1454       // by recompiling offending methods without this optimization.
1455     }
1456   }
1457 
1458   bool block_size_threshold_ok = false;
1459   intptr_t *recalc_pressure_nodes = NULL;
1460   if (OptoRegScheduling) {
1461     for (uint i = 0; i &lt; number_of_blocks(); i++) {
1462       Block* block = get_block(i);
1463       if (block-&gt;number_of_nodes() &gt; 10) {
1464         block_size_threshold_ok = true;
1465         break;
1466       }
1467     }
1468   }
1469 
1470   // Enabling the scheduler for register pressure plus finding blocks of size to schedule for it
1471   // is key to enabling this feature.
1472   PhaseChaitin regalloc(C-&gt;unique(), *this, _matcher, true);
1473   ResourceArea live_arena(mtCompiler);      // Arena for liveness
1474   ResourceMark rm_live(&amp;live_arena);
1475   PhaseLive live(*this, regalloc._lrg_map.names(), &amp;live_arena, true);
1476   PhaseIFG ifg(&amp;live_arena);
1477   if (OptoRegScheduling &amp;&amp; block_size_threshold_ok) {
1478     regalloc.mark_ssa();
1479     Compile::TracePhase tp(&quot;computeLive&quot;, &amp;timers[_t_computeLive]);
1480     rm_live.reset_to_mark();           // Reclaim working storage
1481     IndexSet::reset_memory(C, &amp;live_arena);
1482     uint node_size = regalloc._lrg_map.max_lrg_id();
1483     ifg.init(node_size); // Empty IFG
1484     regalloc.set_ifg(ifg);
1485     regalloc.set_live(live);
1486     regalloc.gather_lrg_masks(false);    // Collect LRG masks
1487     live.compute(node_size); // Compute liveness
1488 
1489     recalc_pressure_nodes = NEW_RESOURCE_ARRAY(intptr_t, node_size);
1490     for (uint i = 0; i &lt; node_size; i++) {
1491       recalc_pressure_nodes[i] = 0;
1492     }
1493   }
1494   _regalloc = &amp;regalloc;
1495 
1496 #ifndef PRODUCT
1497   if (trace_opto_pipelining()) {
1498     tty-&gt;print(&quot;\n---- Start Local Scheduling ----\n&quot;);
1499   }
1500 #endif
1501 
1502   // Schedule locally.  Right now a simple topological sort.
1503   // Later, do a real latency aware scheduler.
1504   GrowableArray&lt;int&gt; ready_cnt(C-&gt;unique(), C-&gt;unique(), -1);
1505   visited.reset();
1506   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1507     Block* block = get_block(i);
1508     if (!schedule_local(block, ready_cnt, visited, recalc_pressure_nodes)) {
1509       if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
1510         C-&gt;record_method_not_compilable(&quot;local schedule failed&quot;);
1511       }
1512       _regalloc = NULL;
1513       return;
1514     }
1515   }
1516   _regalloc = NULL;
1517 
1518   // If we inserted any instructions between a Call and his CatchNode,
1519   // clone the instructions on all paths below the Catch.
1520   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1521     Block* block = get_block(i);
1522     call_catch_cleanup(block);
1523   }
1524 
1525 #ifndef PRODUCT
1526   if (trace_opto_pipelining()) {
1527     tty-&gt;print(&quot;\n---- After GlobalCodeMotion ----\n&quot;);
1528     for (uint i = 0; i &lt; number_of_blocks(); i++) {
1529       Block* block = get_block(i);
1530       block-&gt;dump();
1531     }
1532   }
1533 #endif
1534   // Dead.
1535   _node_latency = (GrowableArray&lt;uint&gt; *)((intptr_t)0xdeadbeef);
1536 }
1537 
1538 bool PhaseCFG::do_global_code_motion() {
1539 
1540   build_dominator_tree();
1541   if (C-&gt;failing()) {
1542     return false;
1543   }
1544 
1545   NOT_PRODUCT( C-&gt;verify_graph_edges(); )
1546 
1547   estimate_block_frequency();
1548 
1549   global_code_motion();
1550 
1551   if (C-&gt;failing()) {
1552     return false;
1553   }
1554 
1555   return true;
1556 }
1557 
1558 //------------------------------Estimate_Block_Frequency-----------------------
1559 // Estimate block frequencies based on IfNode probabilities.
1560 void PhaseCFG::estimate_block_frequency() {
1561 
1562   // Force conditional branches leading to uncommon traps to be unlikely,
1563   // not because we get to the uncommon_trap with less relative frequency,
1564   // but because an uncommon_trap typically causes a deopt, so we only get
1565   // there once.
1566   if (C-&gt;do_freq_based_layout()) {
1567     Block_List worklist;
1568     Block* root_blk = get_block(0);
1569     for (uint i = 1; i &lt; root_blk-&gt;num_preds(); i++) {
1570       Block *pb = get_block_for_node(root_blk-&gt;pred(i));
1571       if (pb-&gt;has_uncommon_code()) {
1572         worklist.push(pb);
1573       }
1574     }
1575     while (worklist.size() &gt; 0) {
1576       Block* uct = worklist.pop();
1577       if (uct == get_root_block()) {
1578         continue;
1579       }
1580       for (uint i = 1; i &lt; uct-&gt;num_preds(); i++) {
1581         Block *pb = get_block_for_node(uct-&gt;pred(i));
1582         if (pb-&gt;_num_succs == 1) {
1583           worklist.push(pb);
1584         } else if (pb-&gt;num_fall_throughs() == 2) {
1585           pb-&gt;update_uncommon_branch(uct);
1586         }
1587       }
1588     }
1589   }
1590 
1591   // Create the loop tree and calculate loop depth.
1592   _root_loop = create_loop_tree();
1593   _root_loop-&gt;compute_loop_depth(0);
1594 
1595   // Compute block frequency of each block, relative to a single loop entry.
1596   _root_loop-&gt;compute_freq();
1597 
1598   // Adjust all frequencies to be relative to a single method entry
1599   _root_loop-&gt;_freq = 1.0;
1600   _root_loop-&gt;scale_freq();
1601 
1602   // Save outmost loop frequency for LRG frequency threshold
1603   _outer_loop_frequency = _root_loop-&gt;outer_loop_freq();
1604 
1605   // force paths ending at uncommon traps to be infrequent
1606   if (!C-&gt;do_freq_based_layout()) {
1607     Block_List worklist;
1608     Block* root_blk = get_block(0);
1609     for (uint i = 1; i &lt; root_blk-&gt;num_preds(); i++) {
1610       Block *pb = get_block_for_node(root_blk-&gt;pred(i));
1611       if (pb-&gt;has_uncommon_code()) {
1612         worklist.push(pb);
1613       }
1614     }
1615     while (worklist.size() &gt; 0) {
1616       Block* uct = worklist.pop();
1617       uct-&gt;_freq = PROB_MIN;
1618       for (uint i = 1; i &lt; uct-&gt;num_preds(); i++) {
1619         Block *pb = get_block_for_node(uct-&gt;pred(i));
1620         if (pb-&gt;_num_succs == 1 &amp;&amp; pb-&gt;_freq &gt; PROB_MIN) {
1621           worklist.push(pb);
1622         }
1623       }
1624     }
1625   }
1626 
1627 #ifdef ASSERT
1628   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1629     Block* b = get_block(i);
1630     assert(b-&gt;_freq &gt;= MIN_BLOCK_FREQUENCY, &quot;Register Allocator requires meaningful block frequency&quot;);
1631   }
1632 #endif
1633 
1634 #ifndef PRODUCT
1635   if (PrintCFGBlockFreq) {
1636     tty-&gt;print_cr(&quot;CFG Block Frequencies&quot;);
1637     _root_loop-&gt;dump_tree();
1638     if (Verbose) {
1639       tty-&gt;print_cr(&quot;PhaseCFG dump&quot;);
1640       dump();
1641       tty-&gt;print_cr(&quot;Node dump&quot;);
1642       _root-&gt;dump(99999);
1643     }
1644   }
1645 #endif
1646 }
1647 
1648 //----------------------------create_loop_tree--------------------------------
1649 // Create a loop tree from the CFG
1650 CFGLoop* PhaseCFG::create_loop_tree() {
1651 
1652 #ifdef ASSERT
1653   assert(get_block(0) == get_root_block(), &quot;first block should be root block&quot;);
1654   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1655     Block* block = get_block(i);
1656     // Check that _loop field are clear...we could clear them if not.
1657     assert(block-&gt;_loop == NULL, &quot;clear _loop expected&quot;);
1658     // Sanity check that the RPO numbering is reflected in the _blocks array.
1659     // It doesn&#39;t have to be for the loop tree to be built, but if it is not,
1660     // then the blocks have been reordered since dom graph building...which
1661     // may question the RPO numbering
1662     assert(block-&gt;_rpo == i, &quot;unexpected reverse post order number&quot;);
1663   }
1664 #endif
1665 
1666   int idct = 0;
1667   CFGLoop* root_loop = new CFGLoop(idct++);
1668 
1669   Block_List worklist;
1670 
1671   // Assign blocks to loops
1672   for(uint i = number_of_blocks() - 1; i &gt; 0; i-- ) { // skip Root block
1673     Block* block = get_block(i);
1674 
1675     if (block-&gt;head()-&gt;is_Loop()) {
1676       Block* loop_head = block;
1677       assert(loop_head-&gt;num_preds() - 1 == 2, &quot;loop must have 2 predecessors&quot;);
1678       Node* tail_n = loop_head-&gt;pred(LoopNode::LoopBackControl);
1679       Block* tail = get_block_for_node(tail_n);
1680 
1681       // Defensively filter out Loop nodes for non-single-entry loops.
1682       // For all reasonable loops, the head occurs before the tail in RPO.
1683       if (i &lt;= tail-&gt;_rpo) {
1684 
1685         // The tail and (recursive) predecessors of the tail
1686         // are made members of a new loop.
1687 
1688         assert(worklist.size() == 0, &quot;nonempty worklist&quot;);
1689         CFGLoop* nloop = new CFGLoop(idct++);
1690         assert(loop_head-&gt;_loop == NULL, &quot;just checking&quot;);
1691         loop_head-&gt;_loop = nloop;
1692         // Add to nloop so push_pred() will skip over inner loops
1693         nloop-&gt;add_member(loop_head);
1694         nloop-&gt;push_pred(loop_head, LoopNode::LoopBackControl, worklist, this);
1695 
1696         while (worklist.size() &gt; 0) {
1697           Block* member = worklist.pop();
1698           if (member != loop_head) {
1699             for (uint j = 1; j &lt; member-&gt;num_preds(); j++) {
1700               nloop-&gt;push_pred(member, j, worklist, this);
1701             }
1702           }
1703         }
1704       }
1705     }
1706   }
1707 
1708   // Create a member list for each loop consisting
1709   // of both blocks and (immediate child) loops.
1710   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1711     Block* block = get_block(i);
1712     CFGLoop* lp = block-&gt;_loop;
1713     if (lp == NULL) {
1714       // Not assigned to a loop. Add it to the method&#39;s pseudo loop.
1715       block-&gt;_loop = root_loop;
1716       lp = root_loop;
1717     }
1718     if (lp == root_loop || block != lp-&gt;head()) { // loop heads are already members
1719       lp-&gt;add_member(block);
1720     }
1721     if (lp != root_loop) {
1722       if (lp-&gt;parent() == NULL) {
1723         // Not a nested loop. Make it a child of the method&#39;s pseudo loop.
1724         root_loop-&gt;add_nested_loop(lp);
1725       }
1726       if (block == lp-&gt;head()) {
1727         // Add nested loop to member list of parent loop.
1728         lp-&gt;parent()-&gt;add_member(lp);
1729       }
1730     }
1731   }
1732 
1733   return root_loop;
1734 }
1735 
1736 //------------------------------push_pred--------------------------------------
1737 void CFGLoop::push_pred(Block* blk, int i, Block_List&amp; worklist, PhaseCFG* cfg) {
1738   Node* pred_n = blk-&gt;pred(i);
1739   Block* pred = cfg-&gt;get_block_for_node(pred_n);
1740   CFGLoop *pred_loop = pred-&gt;_loop;
1741   if (pred_loop == NULL) {
1742     // Filter out blocks for non-single-entry loops.
1743     // For all reasonable loops, the head occurs before the tail in RPO.
1744     if (pred-&gt;_rpo &gt; head()-&gt;_rpo) {
1745       pred-&gt;_loop = this;
1746       worklist.push(pred);
1747     }
1748   } else if (pred_loop != this) {
1749     // Nested loop.
1750     while (pred_loop-&gt;_parent != NULL &amp;&amp; pred_loop-&gt;_parent != this) {
1751       pred_loop = pred_loop-&gt;_parent;
1752     }
1753     // Make pred&#39;s loop be a child
1754     if (pred_loop-&gt;_parent == NULL) {
1755       add_nested_loop(pred_loop);
1756       // Continue with loop entry predecessor.
1757       Block* pred_head = pred_loop-&gt;head();
1758       assert(pred_head-&gt;num_preds() - 1 == 2, &quot;loop must have 2 predecessors&quot;);
1759       assert(pred_head != head(), &quot;loop head in only one loop&quot;);
1760       push_pred(pred_head, LoopNode::EntryControl, worklist, cfg);
1761     } else {
1762       assert(pred_loop-&gt;_parent == this &amp;&amp; _parent == NULL, &quot;just checking&quot;);
1763     }
1764   }
1765 }
1766 
1767 //------------------------------add_nested_loop--------------------------------
1768 // Make cl a child of the current loop in the loop tree.
1769 void CFGLoop::add_nested_loop(CFGLoop* cl) {
1770   assert(_parent == NULL, &quot;no parent yet&quot;);
1771   assert(cl != this, &quot;not my own parent&quot;);
1772   cl-&gt;_parent = this;
1773   CFGLoop* ch = _child;
1774   if (ch == NULL) {
1775     _child = cl;
1776   } else {
1777     while (ch-&gt;_sibling != NULL) { ch = ch-&gt;_sibling; }
1778     ch-&gt;_sibling = cl;
1779   }
1780 }
1781 
1782 //------------------------------compute_loop_depth-----------------------------
1783 // Store the loop depth in each CFGLoop object.
1784 // Recursively walk the children to do the same for them.
1785 void CFGLoop::compute_loop_depth(int depth) {
1786   _depth = depth;
1787   CFGLoop* ch = _child;
1788   while (ch != NULL) {
1789     ch-&gt;compute_loop_depth(depth + 1);
1790     ch = ch-&gt;_sibling;
1791   }
1792 }
1793 
1794 //------------------------------compute_freq-----------------------------------
1795 // Compute the frequency of each block and loop, relative to a single entry
1796 // into the dominating loop head.
1797 void CFGLoop::compute_freq() {
1798   // Bottom up traversal of loop tree (visit inner loops first.)
1799   // Set loop head frequency to 1.0, then transitively
1800   // compute frequency for all successors in the loop,
1801   // as well as for each exit edge.  Inner loops are
1802   // treated as single blocks with loop exit targets
1803   // as the successor blocks.
1804 
1805   // Nested loops first
1806   CFGLoop* ch = _child;
1807   while (ch != NULL) {
1808     ch-&gt;compute_freq();
1809     ch = ch-&gt;_sibling;
1810   }
1811   assert (_members.length() &gt; 0, &quot;no empty loops&quot;);
1812   Block* hd = head();
1813   hd-&gt;_freq = 1.0;
1814   for (int i = 0; i &lt; _members.length(); i++) {
1815     CFGElement* s = _members.at(i);
1816     double freq = s-&gt;_freq;
1817     if (s-&gt;is_block()) {
1818       Block* b = s-&gt;as_Block();
1819       for (uint j = 0; j &lt; b-&gt;_num_succs; j++) {
1820         Block* sb = b-&gt;_succs[j];
1821         update_succ_freq(sb, freq * b-&gt;succ_prob(j));
1822       }
1823     } else {
1824       CFGLoop* lp = s-&gt;as_CFGLoop();
1825       assert(lp-&gt;_parent == this, &quot;immediate child&quot;);
1826       for (int k = 0; k &lt; lp-&gt;_exits.length(); k++) {
1827         Block* eb = lp-&gt;_exits.at(k).get_target();
1828         double prob = lp-&gt;_exits.at(k).get_prob();
1829         update_succ_freq(eb, freq * prob);
1830       }
1831     }
1832   }
1833 
1834   // For all loops other than the outer, &quot;method&quot; loop,
1835   // sum and normalize the exit probability. The &quot;method&quot; loop
1836   // should keep the initial exit probability of 1, so that
1837   // inner blocks do not get erroneously scaled.
1838   if (_depth != 0) {
1839     // Total the exit probabilities for this loop.
1840     double exits_sum = 0.0f;
1841     for (int i = 0; i &lt; _exits.length(); i++) {
1842       exits_sum += _exits.at(i).get_prob();
1843     }
1844 
1845     // Normalize the exit probabilities. Until now, the
1846     // probabilities estimate the possibility of exit per
1847     // a single loop iteration; afterward, they estimate
1848     // the probability of exit per loop entry.
1849     for (int i = 0; i &lt; _exits.length(); i++) {
1850       Block* et = _exits.at(i).get_target();
1851       float new_prob = 0.0f;
1852       if (_exits.at(i).get_prob() &gt; 0.0f) {
1853         new_prob = _exits.at(i).get_prob() / exits_sum;
1854       }
1855       BlockProbPair bpp(et, new_prob);
1856       _exits.at_put(i, bpp);
1857     }
1858 
1859     // Save the total, but guard against unreasonable probability,
1860     // as the value is used to estimate the loop trip count.
1861     // An infinite trip count would blur relative block
1862     // frequencies.
1863     if (exits_sum &gt; 1.0f) exits_sum = 1.0;
1864     if (exits_sum &lt; PROB_MIN) exits_sum = PROB_MIN;
1865     _exit_prob = exits_sum;
1866   }
1867 }
1868 
1869 //------------------------------succ_prob-------------------------------------
1870 // Determine the probability of reaching successor &#39;i&#39; from the receiver block.
1871 float Block::succ_prob(uint i) {
1872   int eidx = end_idx();
1873   Node *n = get_node(eidx);  // Get ending Node
1874 
1875   int op = n-&gt;Opcode();
1876   if (n-&gt;is_Mach()) {
1877     if (n-&gt;is_MachNullCheck()) {
1878       // Can only reach here if called after lcm. The original Op_If is gone,
1879       // so we attempt to infer the probability from one or both of the
1880       // successor blocks.
1881       assert(_num_succs == 2, &quot;expecting 2 successors of a null check&quot;);
1882       // If either successor has only one predecessor, then the
1883       // probability estimate can be derived using the
1884       // relative frequency of the successor and this block.
1885       if (_succs[i]-&gt;num_preds() == 2) {
1886         return _succs[i]-&gt;_freq / _freq;
1887       } else if (_succs[1-i]-&gt;num_preds() == 2) {
1888         return 1 - (_succs[1-i]-&gt;_freq / _freq);
1889       } else {
1890         // Estimate using both successor frequencies
1891         float freq = _succs[i]-&gt;_freq;
1892         return freq / (freq + _succs[1-i]-&gt;_freq);
1893       }
1894     }
1895     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1896   }
1897 
1898 
1899   // Switch on branch type
1900   switch( op ) {
1901   case Op_CountedLoopEnd:
1902   case Op_If: {
1903     assert (i &lt; 2, &quot;just checking&quot;);
1904     // Conditionals pass on only part of their frequency
1905     float prob  = n-&gt;as_MachIf()-&gt;_prob;
1906     assert(prob &gt;= 0.0 &amp;&amp; prob &lt;= 1.0, &quot;out of range probability&quot;);
1907     // If succ[i] is the FALSE branch, invert path info
1908     if( get_node(i + eidx + 1)-&gt;Opcode() == Op_IfFalse ) {
1909       return 1.0f - prob; // not taken
1910     } else {
1911       return prob; // taken
1912     }
1913   }
1914 
1915   case Op_Jump:
1916     return n-&gt;as_MachJump()-&gt;_probs[get_node(i + eidx + 1)-&gt;as_JumpProj()-&gt;_con];
1917 
1918   case Op_Catch: {
1919     const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1920     if (ci-&gt;_con == CatchProjNode::fall_through_index) {
1921       // Fall-thru path gets the lion&#39;s share.
1922       return 1.0f - PROB_UNLIKELY_MAG(5)*_num_succs;
1923     } else {
1924       // Presume exceptional paths are equally unlikely
1925       return PROB_UNLIKELY_MAG(5);
1926     }
1927   }
1928 
1929   case Op_Root:
1930   case Op_Goto:
1931     // Pass frequency straight thru to target
1932     return 1.0f;
1933 
1934   case Op_NeverBranch:
1935     return 0.0f;
1936 
1937   case Op_TailCall:
1938   case Op_TailJump:
1939   case Op_Return:
1940   case Op_Halt:
1941   case Op_Rethrow:
1942     // Do not push out freq to root block
1943     return 0.0f;
1944 
1945   default:
1946     ShouldNotReachHere();
1947   }
1948 
1949   return 0.0f;
1950 }
1951 
1952 //------------------------------num_fall_throughs-----------------------------
1953 // Return the number of fall-through candidates for a block
1954 int Block::num_fall_throughs() {
1955   int eidx = end_idx();
1956   Node *n = get_node(eidx);  // Get ending Node
1957 
1958   int op = n-&gt;Opcode();
1959   if (n-&gt;is_Mach()) {
1960     if (n-&gt;is_MachNullCheck()) {
1961       // In theory, either side can fall-thru, for simplicity sake,
1962       // let&#39;s say only the false branch can now.
1963       return 1;
1964     }
1965     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1966   }
1967 
1968   // Switch on branch type
1969   switch( op ) {
1970   case Op_CountedLoopEnd:
1971   case Op_If:
1972     return 2;
1973 
1974   case Op_Root:
1975   case Op_Goto:
1976     return 1;
1977 
1978   case Op_Catch: {
1979     for (uint i = 0; i &lt; _num_succs; i++) {
1980       const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1981       if (ci-&gt;_con == CatchProjNode::fall_through_index) {
1982         return 1;
1983       }
1984     }
1985     return 0;
1986   }
1987 
1988   case Op_Jump:
1989   case Op_NeverBranch:
1990   case Op_TailCall:
1991   case Op_TailJump:
1992   case Op_Return:
1993   case Op_Halt:
1994   case Op_Rethrow:
1995     return 0;
1996 
1997   default:
1998     ShouldNotReachHere();
1999   }
2000 
2001   return 0;
2002 }
2003 
2004 //------------------------------succ_fall_through-----------------------------
2005 // Return true if a specific successor could be fall-through target.
2006 bool Block::succ_fall_through(uint i) {
2007   int eidx = end_idx();
2008   Node *n = get_node(eidx);  // Get ending Node
2009 
2010   int op = n-&gt;Opcode();
2011   if (n-&gt;is_Mach()) {
2012     if (n-&gt;is_MachNullCheck()) {
2013       // In theory, either side can fall-thru, for simplicity sake,
2014       // let&#39;s say only the false branch can now.
2015       return get_node(i + eidx + 1)-&gt;Opcode() == Op_IfFalse;
2016     }
2017     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
2018   }
2019 
2020   // Switch on branch type
2021   switch( op ) {
2022   case Op_CountedLoopEnd:
2023   case Op_If:
2024   case Op_Root:
2025   case Op_Goto:
2026     return true;
2027 
2028   case Op_Catch: {
2029     const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
2030     return ci-&gt;_con == CatchProjNode::fall_through_index;
2031   }
2032 
2033   case Op_Jump:
2034   case Op_NeverBranch:
2035   case Op_TailCall:
2036   case Op_TailJump:
2037   case Op_Return:
2038   case Op_Halt:
2039   case Op_Rethrow:
2040     return false;
2041 
2042   default:
2043     ShouldNotReachHere();
2044   }
2045 
2046   return false;
2047 }
2048 
2049 //------------------------------update_uncommon_branch------------------------
2050 // Update the probability of a two-branch to be uncommon
2051 void Block::update_uncommon_branch(Block* ub) {
2052   int eidx = end_idx();
2053   Node *n = get_node(eidx);  // Get ending Node
2054 
2055   int op = n-&gt;as_Mach()-&gt;ideal_Opcode();
2056 
2057   assert(op == Op_CountedLoopEnd || op == Op_If, &quot;must be a If&quot;);
2058   assert(num_fall_throughs() == 2, &quot;must be a two way branch block&quot;);
2059 
2060   // Which successor is ub?
2061   uint s;
2062   for (s = 0; s &lt;_num_succs; s++) {
2063     if (_succs[s] == ub) break;
2064   }
2065   assert(s &lt; 2, &quot;uncommon successor must be found&quot;);
2066 
2067   // If ub is the true path, make the proability small, else
2068   // ub is the false path, and make the probability large
2069   bool invert = (get_node(s + eidx + 1)-&gt;Opcode() == Op_IfFalse);
2070 
2071   // Get existing probability
2072   float p = n-&gt;as_MachIf()-&gt;_prob;
2073 
2074   if (invert) p = 1.0 - p;
2075   if (p &gt; PROB_MIN) {
2076     p = PROB_MIN;
2077   }
2078   if (invert) p = 1.0 - p;
2079 
2080   n-&gt;as_MachIf()-&gt;_prob = p;
2081 }
2082 
2083 //------------------------------update_succ_freq-------------------------------
2084 // Update the appropriate frequency associated with block &#39;b&#39;, a successor of
2085 // a block in this loop.
2086 void CFGLoop::update_succ_freq(Block* b, double freq) {
2087   if (b-&gt;_loop == this) {
2088     if (b == head()) {
2089       // back branch within the loop
2090       // Do nothing now, the loop carried frequency will be
2091       // adjust later in scale_freq().
2092     } else {
2093       // simple branch within the loop
2094       b-&gt;_freq += freq;
2095     }
2096   } else if (!in_loop_nest(b)) {
2097     // branch is exit from this loop
2098     BlockProbPair bpp(b, freq);
2099     _exits.append(bpp);
2100   } else {
2101     // branch into nested loop
2102     CFGLoop* ch = b-&gt;_loop;
2103     ch-&gt;_freq += freq;
2104   }
2105 }
2106 
2107 //------------------------------in_loop_nest-----------------------------------
2108 // Determine if block b is in the receiver&#39;s loop nest.
2109 bool CFGLoop::in_loop_nest(Block* b) {
2110   int depth = _depth;
2111   CFGLoop* b_loop = b-&gt;_loop;
2112   int b_depth = b_loop-&gt;_depth;
2113   if (depth == b_depth) {
2114     return true;
2115   }
2116   while (b_depth &gt; depth) {
2117     b_loop = b_loop-&gt;_parent;
2118     b_depth = b_loop-&gt;_depth;
2119   }
2120   return b_loop == this;
2121 }
2122 
2123 //------------------------------scale_freq-------------------------------------
2124 // Scale frequency of loops and blocks by trip counts from outer loops
2125 // Do a top down traversal of loop tree (visit outer loops first.)
2126 void CFGLoop::scale_freq() {
2127   double loop_freq = _freq * trip_count();
2128   _freq = loop_freq;
2129   for (int i = 0; i &lt; _members.length(); i++) {
2130     CFGElement* s = _members.at(i);
2131     double block_freq = s-&gt;_freq * loop_freq;
2132     if (g_isnan(block_freq) || block_freq &lt; MIN_BLOCK_FREQUENCY)
2133       block_freq = MIN_BLOCK_FREQUENCY;
2134     s-&gt;_freq = block_freq;
2135   }
2136   CFGLoop* ch = _child;
2137   while (ch != NULL) {
2138     ch-&gt;scale_freq();
2139     ch = ch-&gt;_sibling;
2140   }
2141 }
2142 
2143 // Frequency of outer loop
2144 double CFGLoop::outer_loop_freq() const {
2145   if (_child != NULL) {
2146     return _child-&gt;_freq;
2147   }
2148   return _freq;
2149 }
2150 
2151 #ifndef PRODUCT
2152 //------------------------------dump_tree--------------------------------------
2153 void CFGLoop::dump_tree() const {
2154   dump();
2155   if (_child != NULL)   _child-&gt;dump_tree();
2156   if (_sibling != NULL) _sibling-&gt;dump_tree();
2157 }
2158 
2159 //------------------------------dump-------------------------------------------
2160 void CFGLoop::dump() const {
2161   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2162   tty-&gt;print(&quot;%s: %d  trip_count: %6.0f freq: %6.0f\n&quot;,
2163              _depth == 0 ? &quot;Method&quot; : &quot;Loop&quot;, _id, trip_count(), _freq);
2164   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2165   tty-&gt;print(&quot;         members:&quot;);
2166   int k = 0;
2167   for (int i = 0; i &lt; _members.length(); i++) {
2168     if (k++ &gt;= 6) {
2169       tty-&gt;print(&quot;\n              &quot;);
2170       for (int j = 0; j &lt; _depth+1; j++) tty-&gt;print(&quot;   &quot;);
2171       k = 0;
2172     }
2173     CFGElement *s = _members.at(i);
2174     if (s-&gt;is_block()) {
2175       Block *b = s-&gt;as_Block();
2176       tty-&gt;print(&quot; B%d(%6.3f)&quot;, b-&gt;_pre_order, b-&gt;_freq);
2177     } else {
2178       CFGLoop* lp = s-&gt;as_CFGLoop();
2179       tty-&gt;print(&quot; L%d(%6.3f)&quot;, lp-&gt;_id, lp-&gt;_freq);
2180     }
2181   }
2182   tty-&gt;print(&quot;\n&quot;);
2183   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2184   tty-&gt;print(&quot;         exits:  &quot;);
2185   k = 0;
2186   for (int i = 0; i &lt; _exits.length(); i++) {
2187     if (k++ &gt;= 7) {
2188       tty-&gt;print(&quot;\n              &quot;);
2189       for (int j = 0; j &lt; _depth+1; j++) tty-&gt;print(&quot;   &quot;);
2190       k = 0;
2191     }
2192     Block *blk = _exits.at(i).get_target();
2193     double prob = _exits.at(i).get_prob();
2194     tty-&gt;print(&quot; -&gt;%d@%d%%&quot;, blk-&gt;_pre_order, (int)(prob*100));
2195   }
2196   tty-&gt;print(&quot;\n&quot;);
2197 }
2198 #endif
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>