<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/sharedRuntime.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="mutexLocker.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/sharedRuntime.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  40 #include &quot;gc/shared/barrierSet.hpp&quot;
  41 #include &quot;gc/shared/gcLocker.inline.hpp&quot;
  42 #include &quot;interpreter/interpreter.hpp&quot;
  43 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  44 #include &quot;jfr/jfrEvents.hpp&quot;
  45 #include &quot;logging/log.hpp&quot;
  46 #include &quot;memory/metaspaceShared.hpp&quot;
  47 #include &quot;memory/resourceArea.hpp&quot;
  48 #include &quot;memory/universe.hpp&quot;
  49 #include &quot;oops/klass.hpp&quot;
  50 #include &quot;oops/method.inline.hpp&quot;
  51 #include &quot;oops/objArrayKlass.hpp&quot;
  52 #include &quot;oops/oop.inline.hpp&quot;
  53 #include &quot;prims/forte.hpp&quot;
  54 #include &quot;prims/jvmtiExport.hpp&quot;
  55 #include &quot;prims/methodHandles.hpp&quot;
  56 #include &quot;prims/nativeLookup.hpp&quot;
  57 #include &quot;runtime/arguments.hpp&quot;
  58 #include &quot;runtime/atomic.hpp&quot;
  59 #include &quot;runtime/biasedLocking.hpp&quot;
<span class="line-removed">  60 #include &quot;runtime/compilationPolicy.hpp&quot;</span>
  61 #include &quot;runtime/frame.inline.hpp&quot;
  62 #include &quot;runtime/handles.inline.hpp&quot;
  63 #include &quot;runtime/init.hpp&quot;
  64 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  65 #include &quot;runtime/java.hpp&quot;
  66 #include &quot;runtime/javaCalls.hpp&quot;
  67 #include &quot;runtime/sharedRuntime.hpp&quot;
  68 #include &quot;runtime/stubRoutines.hpp&quot;
  69 #include &quot;runtime/vframe.inline.hpp&quot;
  70 #include &quot;runtime/vframeArray.hpp&quot;
  71 #include &quot;utilities/copy.hpp&quot;
  72 #include &quot;utilities/dtrace.hpp&quot;
  73 #include &quot;utilities/events.hpp&quot;
  74 #include &quot;utilities/hashtable.inline.hpp&quot;
  75 #include &quot;utilities/macros.hpp&quot;
  76 #include &quot;utilities/xmlstream.hpp&quot;
  77 #ifdef COMPILER1
  78 #include &quot;c1/c1_Runtime1.hpp&quot;
  79 #endif
  80 #if INCLUDE_TSAN
</pre>
<hr />
<pre>
 751 
 752 void SharedRuntime::throw_StackOverflowError_common(JavaThread* thread, bool delayed) {
 753   // We avoid using the normal exception construction in this case because
 754   // it performs an upcall to Java, and we&#39;re already out of stack space.
 755   Thread* THREAD = thread;
 756   Klass* k = SystemDictionary::StackOverflowError_klass();
 757   oop exception_oop = InstanceKlass::cast(k)-&gt;allocate_instance(CHECK);
 758   if (delayed) {
 759     java_lang_Throwable::set_message(exception_oop,
 760                                      Universe::delayed_stack_overflow_error_message());
 761   }
 762   Handle exception (thread, exception_oop);
 763   if (StackTraceInThrowable) {
 764     java_lang_Throwable::fill_in_stack_trace(exception);
 765   }
 766   // Increment counter for hs_err file reporting
 767   Atomic::inc(&amp;Exceptions::_stack_overflow_errors);
 768   throw_and_post_jvmti_exception(thread, exception);
 769 }
 770 
<span class="line-removed"> 771 #if INCLUDE_JVMCI</span>
<span class="line-removed"> 772 address SharedRuntime::deoptimize_for_implicit_exception(JavaThread* thread, address pc, CompiledMethod* nm, int deopt_reason) {</span>
<span class="line-removed"> 773   assert(deopt_reason &gt; Deoptimization::Reason_none &amp;&amp; deopt_reason &lt; Deoptimization::Reason_LIMIT, &quot;invalid deopt reason&quot;);</span>
<span class="line-removed"> 774   thread-&gt;set_jvmci_implicit_exception_pc(pc);</span>
<span class="line-removed"> 775   thread-&gt;set_pending_deoptimization(Deoptimization::make_trap_request((Deoptimization::DeoptReason)deopt_reason, Deoptimization::Action_reinterpret));</span>
<span class="line-removed"> 776   return (SharedRuntime::deopt_blob()-&gt;implicit_exception_uncommon_trap());</span>
<span class="line-removed"> 777 }</span>
<span class="line-removed"> 778 #endif // INCLUDE_JVMCI</span>
<span class="line-removed"> 779 </span>
 780 address SharedRuntime::continuation_for_implicit_exception(JavaThread* thread,
 781                                                            address pc,
<span class="line-modified"> 782                                                            SharedRuntime::ImplicitExceptionKind exception_kind)</span>
 783 {
 784   address target_pc = NULL;
 785 
 786   if (Interpreter::contains(pc)) {
 787 #ifdef CC_INTERP
 788     // C++ interpreter doesn&#39;t throw implicit exceptions
 789     ShouldNotReachHere();
 790 #else
 791     switch (exception_kind) {
 792       case IMPLICIT_NULL:           return Interpreter::throw_NullPointerException_entry();
 793       case IMPLICIT_DIVIDE_BY_ZERO: return Interpreter::throw_ArithmeticException_entry();
 794       case STACK_OVERFLOW:          return Interpreter::throw_StackOverflowError_entry();
 795       default:                      ShouldNotReachHere();
 796     }
 797 #endif // !CC_INTERP
 798   } else {
 799     switch (exception_kind) {
 800       case STACK_OVERFLOW: {
 801         // Stack overflow only occurs upon frame setup; the callee is
 802         // going to be unwound. Dispatch to a shared runtime stub
</pre>
<hr />
<pre>
 863           // Otherwise, it&#39;s a compiled method.  Consult its exception handlers.
 864           CompiledMethod* cm = (CompiledMethod*)cb;
 865           if (cm-&gt;inlinecache_check_contains(pc)) {
 866             // exception happened inside inline-cache check code
 867             // =&gt; the nmethod is not yet active (i.e., the frame
 868             // is not set up yet) =&gt; use return address pushed by
 869             // caller =&gt; don&#39;t push another return address
 870             Events::log_exception(thread, &quot;NullPointerException in IC check &quot; INTPTR_FORMAT, p2i(pc));
 871             return StubRoutines::throw_NullPointerException_at_call_entry();
 872           }
 873 
 874           if (cm-&gt;method()-&gt;is_method_handle_intrinsic()) {
 875             // exception happened inside MH dispatch code, similar to a vtable stub
 876             Events::log_exception(thread, &quot;NullPointerException in MH adapter &quot; INTPTR_FORMAT, p2i(pc));
 877             return StubRoutines::throw_NullPointerException_at_call_entry();
 878           }
 879 
 880 #ifndef PRODUCT
 881           _implicit_null_throws++;
 882 #endif
<span class="line-modified"> 883 #if INCLUDE_JVMCI</span>
<span class="line-removed"> 884           if (cm-&gt;is_compiled_by_jvmci() &amp;&amp; cm-&gt;pc_desc_at(pc) != NULL) {</span>
<span class="line-removed"> 885             // If there&#39;s no PcDesc then we&#39;ll die way down inside of</span>
<span class="line-removed"> 886             // deopt instead of just getting normal error reporting,</span>
<span class="line-removed"> 887             // so only go there if it will succeed.</span>
<span class="line-removed"> 888             return deoptimize_for_implicit_exception(thread, pc, cm, Deoptimization::Reason_null_check);</span>
<span class="line-removed"> 889           } else {</span>
<span class="line-removed"> 890 #endif // INCLUDE_JVMCI</span>
<span class="line-removed"> 891           assert (cm-&gt;is_nmethod(), &quot;Expect nmethod&quot;);</span>
<span class="line-removed"> 892           target_pc = ((nmethod*)cm)-&gt;continuation_for_implicit_exception(pc);</span>
<span class="line-removed"> 893 #if INCLUDE_JVMCI</span>
<span class="line-removed"> 894           }</span>
<span class="line-removed"> 895 #endif // INCLUDE_JVMCI</span>
 896           // If there&#39;s an unexpected fault, target_pc might be NULL,
 897           // in which case we want to fall through into the normal
 898           // error handling code.
 899         }
 900 
 901         break; // fall through
 902       }
 903 
 904 
 905       case IMPLICIT_DIVIDE_BY_ZERO: {
 906         CompiledMethod* cm = CodeCache::find_compiled(pc);
 907         guarantee(cm != NULL, &quot;must have containing compiled method for implicit division-by-zero exceptions&quot;);
 908 #ifndef PRODUCT
 909         _implicit_div0_throws++;
 910 #endif
<span class="line-modified"> 911 #if INCLUDE_JVMCI</span>
<span class="line-removed"> 912         if (cm-&gt;is_compiled_by_jvmci() &amp;&amp; cm-&gt;pc_desc_at(pc) != NULL) {</span>
<span class="line-removed"> 913           return deoptimize_for_implicit_exception(thread, pc, cm, Deoptimization::Reason_div0_check);</span>
<span class="line-removed"> 914         } else {</span>
<span class="line-removed"> 915 #endif // INCLUDE_JVMCI</span>
<span class="line-removed"> 916         target_pc = cm-&gt;continuation_for_implicit_exception(pc);</span>
<span class="line-removed"> 917 #if INCLUDE_JVMCI</span>
<span class="line-removed"> 918         }</span>
<span class="line-removed"> 919 #endif // INCLUDE_JVMCI</span>
 920         // If there&#39;s an unexpected fault, target_pc might be NULL,
 921         // in which case we want to fall through into the normal
 922         // error handling code.
 923         break; // fall through
 924       }
 925 
 926       default: ShouldNotReachHere();
 927     }
 928 
 929     assert(exception_kind == IMPLICIT_NULL || exception_kind == IMPLICIT_DIVIDE_BY_ZERO, &quot;wrong implicit exception kind&quot;);
 930 
 931     if (exception_kind == IMPLICIT_NULL) {
 932 #ifndef PRODUCT
 933       // for AbortVMOnException flag
 934       Exceptions::debug_check_abort(&quot;java.lang.NullPointerException&quot;);
 935 #endif //PRODUCT
 936       Events::log_exception(thread, &quot;Implicit null exception at &quot; INTPTR_FORMAT &quot; to &quot; INTPTR_FORMAT, p2i(pc), p2i(target_pc));
 937     } else {
 938 #ifndef PRODUCT
 939       // for AbortVMOnException flag
</pre>
<hr />
<pre>
1041       (char *) sig-&gt;bytes(), sig-&gt;utf8_length());
1042   return 0;
1043 JRT_END
1044 
1045 #if INCLUDE_TSAN
1046 
1047 JRT_LEAF(void, SharedRuntime::verify_oop_index(oopDesc* obj, int index))
1048   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1049   assert(index &gt;= 0, &quot;index is less than 0&quot;);
1050   int obj_size_in_bytes = obj-&gt;size() * HeapWordSize;
1051   assert(index &lt; obj_size_in_bytes, &quot;index %d &gt;= obj size %d&quot;, index, obj_size_in_bytes);
1052 JRT_END
1053 
1054 // TSAN: method entry callback from interpreter
1055 // (1) In order to have the line numbers in the call stack, we use the caller
1056 //     address instead of the method that&#39;s being called. This also matches
1057 //     the entry/exit convention that TSAN uses for C++.
1058 // We use JRT_ENTRY since call_VM_leaf doesn&#39;t set _last_Java_sp that we need.
1059 JRT_ENTRY(void, SharedRuntime::tsan_interp_method_entry(JavaThread *thread))
1060   DEBUG_ONLY(NoSafepointVerifier nsv;)
<span class="line-removed">1061   DEBUG_ONLY(NoAllocVerifier nav;)</span>
1062   DEBUG_ONLY(NoHandleMark nhm;)
1063   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1064 
1065   RegisterMap unused_reg_map(thread, false);
1066 
1067   // These asserts should be removed once
1068   // we support more than just the interpreter for TSAN.
1069   assert(!thread-&gt;last_frame().is_compiled_frame(),
1070          &quot;Current frame should not be a compiled frame&quot;);
1071   const frame sender = thread-&gt;last_frame().real_sender(&amp;unused_reg_map);
1072   assert(!sender.is_compiled_frame(), &quot;Sender should not be a compiled frame&quot;);
1073 
1074   jmethodID jmethod_id = 0;
1075   u2 bci = 0;
1076   // TODO: is (0, 0) really the best we can do
1077   // when the sender isn&#39;t an interpreted frame?
1078   if (sender.is_interpreted_frame()) {
1079     jmethod_id = sender.interpreter_frame_method()-&gt;find_jmethod_id_or_null();
1080     bci = sender.interpreter_frame_bci();
1081   }
1082   __tsan_func_entry(tsan_code_location(jmethod_id, bci));
1083 JRT_END
1084 
1085 // TSAN: method exit callback from interpreter
1086 JRT_LEAF(void, SharedRuntime::tsan_interp_method_exit())
1087   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1088   __tsan_func_exit();
1089 JRT_END
1090 
1091 void SharedRuntime::tsan_oop_lock(Thread* thread, oop obj) {
1092   DEBUG_ONLY(NoSafepointVerifier nsv;)
1093   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1094   assert(thread != NULL, &quot;null thread&quot;);
1095   assert(obj != NULL, &quot;null oop&quot;);
1096   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1097 
1098   TsanOopMap::add_oop(obj);
<span class="line-modified">1099   __tsan_java_mutex_lock((julong)(address)obj);</span>
1100 }
1101 
1102 void SharedRuntime::tsan_oop_unlock(Thread *thread, oop obj) {
1103   DEBUG_ONLY(NoSafepointVerifier nsv;)
1104   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1105   assert(thread != NULL, &quot;null thread&quot;);
1106   assert(obj != NULL, &quot;null oop&quot;);
1107   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1108   assert(TsanOopMap::exists(obj), &quot;oop seen in unlock but not tracked&quot;);
1109 
<span class="line-modified">1110   __tsan_java_mutex_unlock((julong)(address)obj);</span>
1111 }
1112 
1113 void SharedRuntime::tsan_oop_rec_lock(Thread* thread, oop obj, int rec) {
1114   DEBUG_ONLY(NoSafepointVerifier nsv;)
1115   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1116   assert(thread != NULL, &quot;null thread&quot;);
1117   assert(obj != NULL, &quot;null oop&quot;);
1118   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1119 
1120   TsanOopMap::add_oop(obj);
<span class="line-modified">1121   __tsan_java_mutex_lock_rec((julong)(address)obj, rec);</span>
1122 }
1123 
1124 int SharedRuntime::tsan_oop_rec_unlock(Thread *thread, oop obj) {
1125   DEBUG_ONLY(NoSafepointVerifier nsv;)
1126   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1127   assert(thread != NULL, &quot;null thread&quot;);
1128   assert(obj != NULL, &quot;null oop&quot;);
1129   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1130   assert(TsanOopMap::exists(obj), &quot;oop seen in unlock but not tracked&quot;);
1131 
<span class="line-modified">1132   return __tsan_java_mutex_unlock_rec((julong)(address)obj);</span>
1133 }
1134 
1135 JRT_LEAF(void, SharedRuntime::tsan_interp_lock(JavaThread* thread,
1136                                                BasicObjectLock* elem))
1137   DEBUG_ONLY(thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);)
1138   assert(elem != NULL, &quot;null elem&quot;);
1139 
1140   oop obj = elem-&gt;obj();
1141   tsan_oop_lock(thread, obj);
1142 
1143   assert(obj == elem-&gt;obj(), &quot;oop changed&quot;);
1144   DEBUG_ONLY(thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);)
1145 JRT_END
1146 
1147 JRT_LEAF(void, SharedRuntime::tsan_interp_unlock(JavaThread* thread,
1148                                                  BasicObjectLock* elem))
1149   DEBUG_ONLY(thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);)
1150   assert(elem != NULL, &quot;null elem&quot;);
1151 
1152   oop obj = elem-&gt;obj();
</pre>
<hr />
<pre>
1210 TSAN_MEMORY_ACCESS(write1)
1211 TSAN_MEMORY_ACCESS(write2)
1212 TSAN_MEMORY_ACCESS(write4)
1213 TSAN_MEMORY_ACCESS(write8)
1214 
1215 #endif // INCLUDE_TSAN
1216 
1217 // Finds receiver, CallInfo (i.e. receiver method), and calling bytecode)
1218 // for a call current in progress, i.e., arguments has been pushed on stack
1219 // put callee has not been invoked yet.  Used by: resolve virtual/static,
1220 // vtable updates, etc.  Caller frame must be compiled.
1221 Handle SharedRuntime::find_callee_info(JavaThread* thread, Bytecodes::Code&amp; bc, CallInfo&amp; callinfo, TRAPS) {
1222   ResourceMark rm(THREAD);
1223 
1224   // last java frame on stack (which includes native call frames)
1225   vframeStream vfst(thread, true);  // Do not skip and javaCalls
1226 
1227   return find_callee_info_helper(thread, vfst, bc, callinfo, THREAD);
1228 }
1229 
<span class="line-modified">1230 methodHandle SharedRuntime::extract_attached_method(vframeStream&amp; vfst) {</span>
1231   CompiledMethod* caller = vfst.nm();
1232 
1233   nmethodLocker caller_lock(caller);
1234 
1235   address pc = vfst.frame_pc();
1236   { // Get call instruction under lock because another thread may be busy patching it.
1237     CompiledICLocker ic_locker(caller);
1238     return caller-&gt;attached_method_before_pc(pc);
1239   }
1240   return NULL;
1241 }
1242 
1243 // Finds receiver, CallInfo (i.e. receiver method), and calling bytecode
1244 // for a call current in progress, i.e., arguments has been pushed on stack
1245 // but callee has not been invoked yet.  Caller frame must be compiled.
1246 Handle SharedRuntime::find_callee_info_helper(JavaThread* thread,
1247                                               vframeStream&amp; vfst,
1248                                               Bytecodes::Code&amp; bc,
1249                                               CallInfo&amp; callinfo, TRAPS) {
1250   Handle receiver;
1251   Handle nullHandle;  //create a handy null handle for exception returns
1252 
1253   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
1254 
1255   // Find caller and bci from vframe
1256   methodHandle caller(THREAD, vfst.method());
1257   int          bci   = vfst.bci();
1258 
1259   Bytecode_invoke bytecode(caller, bci);
1260   int bytecode_index = bytecode.index();
1261   bc = bytecode.invoke_code();
1262 
<span class="line-modified">1263   methodHandle attached_method = extract_attached_method(vfst);</span>
1264   if (attached_method.not_null()) {
<span class="line-modified">1265     methodHandle callee = bytecode.static_target(CHECK_NH);</span>
1266     vmIntrinsics::ID id = callee-&gt;intrinsic_id();
1267     // When VM replaces MH.invokeBasic/linkTo* call with a direct/virtual call,
1268     // it attaches statically resolved method to the call site.
1269     if (MethodHandles::is_signature_polymorphic(id) &amp;&amp;
1270         MethodHandles::is_signature_polymorphic_intrinsic(id)) {
1271       bc = MethodHandles::signature_polymorphic_intrinsic_bytecode(id);
1272 
1273       // Adjust invocation mode according to the attached method.
1274       switch (bc) {
1275         case Bytecodes::_invokevirtual:
1276           if (attached_method-&gt;method_holder()-&gt;is_interface()) {
1277             bc = Bytecodes::_invokeinterface;
1278           }
1279           break;
1280         case Bytecodes::_invokeinterface:
1281           if (!attached_method-&gt;method_holder()-&gt;is_interface()) {
1282             bc = Bytecodes::_invokevirtual;
1283           }
1284           break;
1285         case Bytecodes::_invokehandle:
</pre>
<hr />
<pre>
1293       }
1294     }
1295   }
1296 
1297   assert(bc != Bytecodes::_illegal, &quot;not initialized&quot;);
1298 
1299   bool has_receiver = bc != Bytecodes::_invokestatic &amp;&amp;
1300                       bc != Bytecodes::_invokedynamic &amp;&amp;
1301                       bc != Bytecodes::_invokehandle;
1302 
1303   // Find receiver for non-static call
1304   if (has_receiver) {
1305     // This register map must be update since we need to find the receiver for
1306     // compiled frames. The receiver might be in a register.
1307     RegisterMap reg_map2(thread);
1308     frame stubFrame   = thread-&gt;last_frame();
1309     // Caller-frame is a compiled frame
1310     frame callerFrame = stubFrame.sender(&amp;reg_map2);
1311 
1312     if (attached_method.is_null()) {
<span class="line-modified">1313       methodHandle callee = bytecode.static_target(CHECK_NH);</span>
<span class="line-modified">1314       if (callee.is_null()) {</span>
1315         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1316       }
1317     }
1318 
1319     // Retrieve from a compiled argument list
1320     receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));
1321 
1322     if (receiver.is_null()) {
1323       THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
1324     }
1325   }
1326 
1327   // Resolve method
1328   if (attached_method.not_null()) {
1329     // Parameterized by attached method.
1330     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);
1331   } else {
1332     // Parameterized by bytecode.
1333     constantPoolHandle constants(THREAD, caller-&gt;constants());
1334     LinkResolver::resolve_invoke(callinfo, receiver, constants, bytecode_index, bc, CHECK_NH);
1335   }
1336 
1337 #ifdef ASSERT
1338   // Check that the receiver klass is of the right subtype and that it is initialized for virtual calls
1339   if (has_receiver) {
1340     assert(receiver.not_null(), &quot;should have thrown exception&quot;);
1341     Klass* receiver_klass = receiver-&gt;klass();
1342     Klass* rk = NULL;
1343     if (attached_method.not_null()) {
1344       // In case there&#39;s resolved method attached, use its holder during the check.
1345       rk = attached_method-&gt;method_holder();
1346     } else {
1347       // Klass is already loaded.
1348       constantPoolHandle constants(THREAD, caller-&gt;constants());
1349       rk = constants-&gt;klass_ref_at(bytecode_index, CHECK_NH);
1350     }
1351     Klass* static_receiver_klass = rk;
<span class="line-removed">1352     methodHandle callee = callinfo.selected_method();</span>
1353     assert(receiver_klass-&gt;is_subtype_of(static_receiver_klass),
1354            &quot;actual receiver must be subclass of static receiver klass&quot;);
1355     if (receiver_klass-&gt;is_instance_klass()) {
1356       if (InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized()) {
1357         tty-&gt;print_cr(&quot;ERROR: Klass not yet initialized!!&quot;);
1358         receiver_klass-&gt;print();
1359       }
1360       assert(!InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized(), &quot;receiver_klass must be initialized&quot;);
1361     }
1362   }
1363 #endif
1364 
1365   return receiver;
1366 }
1367 
1368 methodHandle SharedRuntime::find_callee_method(JavaThread* thread, TRAPS) {
1369   ResourceMark rm(THREAD);
1370   // We need first to check if any Java activations (compiled, interpreted)
1371   // exist on the stack since last JavaCall.  If not, we need
1372   // to get the target method from the JavaCall wrapper.
1373   vframeStream vfst(thread, true);  // Do not skip any javaCalls
1374   methodHandle callee_method;
1375   if (vfst.at_end()) {
1376     // No Java frames were found on stack since we did the JavaCall.
1377     // Hence the stack can only contain an entry_frame.  We need to
1378     // find the target method from the stub frame.
1379     RegisterMap reg_map(thread, false);
1380     frame fr = thread-&gt;last_frame();
1381     assert(fr.is_runtime_frame(), &quot;must be a runtimeStub&quot;);
1382     fr = fr.sender(&amp;reg_map);
1383     assert(fr.is_entry_frame(), &quot;must be&quot;);
1384     // fr is now pointing to the entry frame.
1385     callee_method = methodHandle(THREAD, fr.entry_frame_call_wrapper()-&gt;callee_method());
1386   } else {
1387     Bytecodes::Code bc;
1388     CallInfo callinfo;
1389     find_callee_info_helper(thread, vfst, bc, callinfo, CHECK_(methodHandle()));
<span class="line-modified">1390     callee_method = callinfo.selected_method();</span>
1391   }
1392   assert(callee_method()-&gt;is_method(), &quot;must be&quot;);
1393   return callee_method;
1394 }
1395 
1396 // Resolves a call.
1397 methodHandle SharedRuntime::resolve_helper(JavaThread *thread,
1398                                            bool is_virtual,
1399                                            bool is_optimized, TRAPS) {
1400   methodHandle callee_method;
1401   callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);
1402   if (JvmtiExport::can_hotswap_or_post_breakpoint()) {
1403     int retry_count = 0;
1404     while (!HAS_PENDING_EXCEPTION &amp;&amp; callee_method-&gt;is_old() &amp;&amp;
1405            callee_method-&gt;method_holder() != SystemDictionary::Object_klass()) {
1406       // If has a pending exception then there is no need to re-try to
1407       // resolve this method.
1408       // If the method has been redefined, we need to try again.
1409       // Hack: we have no way to update the vtables of arrays, so don&#39;t
1410       // require that java.lang.Object has been updated.
</pre>
<hr />
<pre>
1456                      CHECK_false);
1457   } else {
1458     // static call
1459     CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);
1460   }
1461 
1462   // grab lock, check for deoptimization and potentially patch caller
1463   {
1464     CompiledICLocker ml(caller_nm);
1465 
1466     // Lock blocks for safepoint during which both nmethods can change state.
1467 
1468     // Now that we are ready to patch if the Method* was redefined then
1469     // don&#39;t update call site and let the caller retry.
1470     // Don&#39;t update call site if callee nmethod was unloaded or deoptimized.
1471     // Don&#39;t update call site if callee nmethod was replaced by an other nmethod
1472     // which may happen when multiply alive nmethod (tiered compilation)
1473     // will be supported.
1474     if (!callee_method-&gt;is_old() &amp;&amp;
1475         (callee == NULL || (callee-&gt;is_in_use() &amp;&amp; callee_method-&gt;code() == callee))) {

1476 #ifdef ASSERT
1477       // We must not try to patch to jump to an already unloaded method.
1478       if (dest_entry_point != 0) {
1479         CodeBlob* cb = CodeCache::find_blob(dest_entry_point);
1480         assert((cb != NULL) &amp;&amp; cb-&gt;is_compiled() &amp;&amp; (((CompiledMethod*)cb) == callee),
1481                &quot;should not call unloaded nmethod&quot;);
1482       }
1483 #endif
1484       if (is_virtual) {
1485         CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
1486         if (inline_cache-&gt;is_clean()) {
1487           if (!inline_cache-&gt;set_to_monomorphic(virtual_call_info)) {
1488             return false;
1489           }
1490         }
1491       } else {






1492         CompiledStaticCall* ssc = caller_nm-&gt;compiledStaticCall_before(caller_frame.pc());
1493         if (ssc-&gt;is_clean()) ssc-&gt;set(static_call_info);
1494       }
1495     }
1496   } // unlock CompiledICLocker
1497   return true;
1498 }
1499 
1500 // Resolves a call.  The compilers generate code for calls that go here
1501 // and are patched with the real destination of the call.
1502 methodHandle SharedRuntime::resolve_sub_helper(JavaThread *thread,
1503                                                bool is_virtual,
1504                                                bool is_optimized, TRAPS) {
1505 
1506   ResourceMark rm(thread);
1507   RegisterMap cbl_map(thread, false);
1508   frame caller_frame = thread-&gt;last_frame().sender(&amp;cbl_map);
1509 
1510   CodeBlob* caller_cb = caller_frame.cb();
1511   guarantee(caller_cb != NULL &amp;&amp; caller_cb-&gt;is_compiled(), &quot;must be called from compiled method&quot;);
1512   CompiledMethod* caller_nm = caller_cb-&gt;as_compiled_method_or_null();
1513 
1514   // make sure caller is not getting deoptimized
1515   // and removed before we are done with it.
1516   // CLEANUP - with lazy deopt shouldn&#39;t need this lock
1517   nmethodLocker caller_lock(caller_nm);
1518 
1519   // determine call info &amp; receiver
1520   // note: a) receiver is NULL for static calls
1521   //       b) an exception is thrown if receiver is NULL for non-static calls
1522   CallInfo call_info;
1523   Bytecodes::Code invoke_code = Bytecodes::_illegal;
1524   Handle receiver = find_callee_info(thread, invoke_code,
1525                                      call_info, CHECK_(methodHandle()));
<span class="line-modified">1526   methodHandle callee_method = call_info.selected_method();</span>
1527 
1528   assert((!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokestatic ) ||
1529          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokespecial) ||
1530          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokehandle ) ||
1531          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokedynamic) ||
1532          ( is_virtual &amp;&amp; invoke_code != Bytecodes::_invokestatic ), &quot;inconsistent bytecode&quot;);
1533 
1534   assert(caller_nm-&gt;is_alive() &amp;&amp; !caller_nm-&gt;is_unloading(), &quot;It should be alive&quot;);
1535 
1536 #ifndef PRODUCT
1537   // tracing/debugging/statistics
1538   int *addr = (is_optimized) ? (&amp;_resolve_opt_virtual_ctr) :
1539                 (is_virtual) ? (&amp;_resolve_virtual_ctr) :
1540                                (&amp;_resolve_static_ctr);
1541   Atomic::inc(addr);
1542 
1543   if (TraceCallFixup) {
1544     ResourceMark rm(thread);
1545     tty-&gt;print(&quot;resolving %s%s (%s) call to&quot;,
1546       (is_optimized) ? &quot;optimized &quot; : &quot;&quot;, (is_virtual) ? &quot;virtual&quot; : &quot;static&quot;,
1547       Bytecodes::name(invoke_code));
1548     callee_method-&gt;print_short_name(tty);
1549     tty-&gt;print_cr(&quot; at pc: &quot; INTPTR_FORMAT &quot; to code: &quot; INTPTR_FORMAT,
1550                   p2i(caller_frame.pc()), p2i(callee_method-&gt;code()));
1551   }
1552 #endif
1553 
<span class="line-modified">1554   // Do not patch call site for static call to another class</span>
<span class="line-modified">1555   // when the class is not fully initialized.</span>
<span class="line-modified">1556   if (invoke_code == Bytecodes::_invokestatic) {</span>
<span class="line-modified">1557     if (!callee_method-&gt;method_holder()-&gt;is_initialized() &amp;&amp;</span>






1558         callee_method-&gt;method_holder() != caller_nm-&gt;method()-&gt;method_holder()) {
1559       assert(callee_method-&gt;method_holder()-&gt;is_linked(), &quot;must be&quot;);
1560       return callee_method;
<span class="line-removed">1561     } else {</span>
<span class="line-removed">1562       assert(callee_method-&gt;method_holder()-&gt;is_initialized() ||</span>
<span class="line-removed">1563              callee_method-&gt;method_holder()-&gt;is_reentrant_initialization(thread),</span>
<span class="line-removed">1564              &quot;invalid class initialization state for invoke_static&quot;);</span>
1565     }
1566   }
1567 
1568   // JSR 292 key invariant:
1569   // If the resolved method is a MethodHandle invoke target, the call
1570   // site must be a MethodHandle call site, because the lambda form might tail-call
1571   // leaving the stack in a state unknown to either caller or callee
1572   // TODO detune for now but we might need it again
1573 //  assert(!callee_method-&gt;is_compiled_lambda_form() ||
1574 //         caller_nm-&gt;is_method_handle_return(caller_frame.pc()), &quot;must be MH call site&quot;);
1575 
1576   // Compute entry points. This might require generation of C2I converter
1577   // frames, so we cannot be holding any locks here. Furthermore, the
1578   // computation of the entry points is independent of patching the call.  We
1579   // always return the entry-point, but we only patch the stub if the call has
1580   // not been deoptimized.  Return values: For a virtual call this is an
1581   // (cached_oop, destination address) pair. For a static call/optimized
1582   // virtual this is just a destination address.
1583 
1584   // Patching IC caches may fail if we run out if transition stubs.
</pre>
<hr />
<pre>
1625   // 6243940 We might end up in here if the callee is deoptimized
1626   // as we race to call it.  We don&#39;t want to take a safepoint if
1627   // the caller was interpreted because the caller frame will look
1628   // interpreted to the stack walkers and arguments are now
1629   // &quot;compiled&quot; so it is much better to make this transition
1630   // invisible to the stack walking code. The i2c path will
1631   // place the callee method in the callee_target. It is stashed
1632   // there because if we try and find the callee by normal means a
1633   // safepoint is possible and have trouble gc&#39;ing the compiled args.
1634   RegisterMap reg_map(thread, false);
1635   frame stub_frame = thread-&gt;last_frame();
1636   assert(stub_frame.is_runtime_frame(), &quot;sanity check&quot;);
1637   frame caller_frame = stub_frame.sender(&amp;reg_map);
1638 
1639   if (caller_frame.is_interpreted_frame() ||
1640       caller_frame.is_entry_frame()) {
1641     Method* callee = thread-&gt;callee_target();
1642     guarantee(callee != NULL &amp;&amp; callee-&gt;is_method(), &quot;bad handshake&quot;);
1643     thread-&gt;set_vm_result_2(callee);
1644     thread-&gt;set_callee_target(NULL);
<span class="line-modified">1645     return callee-&gt;get_c2i_entry();</span>












1646   }
1647 
1648   // Must be compiled to compiled path which is safe to stackwalk
1649   methodHandle callee_method;
1650   JRT_BLOCK
1651     // Force resolving of caller (if we called from compiled frame)
1652     callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);
1653     thread-&gt;set_vm_result_2(callee_method());
1654   JRT_BLOCK_END
1655   // return compiled code entry point after potential safepoints
1656   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);
1657   return callee_method-&gt;verified_code_entry();
1658 JRT_END
1659 
1660 // Handle abstract method call
1661 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_abstract(JavaThread* thread))
1662   // Verbose error message for AbstractMethodError.
1663   // Get the called method from the invoke bytecode.
1664   vframeStream vfst(thread, true);
1665   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
<span class="line-modified">1666   methodHandle caller(vfst.method());</span>
1667   Bytecode_invoke invoke(caller, vfst.bci());
1668   DEBUG_ONLY( invoke.verify(); )
1669 
1670   // Find the compiled caller frame.
1671   RegisterMap reg_map(thread);
1672   frame stubFrame = thread-&gt;last_frame();
1673   assert(stubFrame.is_runtime_frame(), &quot;must be&quot;);
1674   frame callerFrame = stubFrame.sender(&amp;reg_map);
1675   assert(callerFrame.is_compiled_frame(), &quot;must be&quot;);
1676 
1677   // Install exception and return forward entry.
1678   address res = StubRoutines::throw_AbstractMethodError_entry();
1679   JRT_BLOCK
<span class="line-modified">1680     methodHandle callee = invoke.static_target(thread);</span>
1681     if (!callee.is_null()) {
1682       oop recv = callerFrame.retrieve_receiver(&amp;reg_map);
1683       Klass *recv_klass = (recv != NULL) ? recv-&gt;klass() : NULL;
1684       LinkResolver::throw_abstract_method_error(callee, recv_klass, thread);
1685       res = StubRoutines::forward_exception_entry();
1686     }
1687   JRT_BLOCK_END
1688   return res;
1689 JRT_END
1690 
1691 
1692 // resolve a static call and patch code
1693 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
1694   methodHandle callee_method;
1695   JRT_BLOCK
1696     callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);
1697     thread-&gt;set_vm_result_2(callee_method());
1698   JRT_BLOCK_END
1699   // return compiled code entry point after potential safepoints
1700   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);
</pre>
<hr />
<pre>
1824   // don&#39;t have vtable entries (vtable_index &lt; 0) and we&#39;d blow up. So we force a
1825   // reresolution of the  call site (as if we did a handle_wrong_method and not an
1826   // plain ic_miss) and the site will be converted to an optimized virtual call site
1827   // never to miss again. I don&#39;t believe C2 will produce code like this but if it
1828   // did this would still be the correct thing to do for it too, hence no ifdef.
1829   //
1830   if (call_info.resolved_method()-&gt;can_be_statically_bound()) {
1831     methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));
1832     if (TraceCallFixup) {
1833       RegisterMap reg_map(thread, false);
1834       frame caller_frame = thread-&gt;last_frame().sender(&amp;reg_map);
1835       ResourceMark rm(thread);
1836       tty-&gt;print(&quot;converting IC miss to reresolve (%s) call to&quot;, Bytecodes::name(bc));
1837       callee_method-&gt;print_short_name(tty);
1838       tty-&gt;print_cr(&quot; from pc: &quot; INTPTR_FORMAT, p2i(caller_frame.pc()));
1839       tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1840     }
1841     return callee_method;
1842   }
1843 
<span class="line-modified">1844   methodHandle callee_method = call_info.selected_method();</span>
1845 
1846 #ifndef PRODUCT
1847   Atomic::inc(&amp;_ic_miss_ctr);
1848 
1849   // Statistics &amp; Tracing
1850   if (TraceCallFixup) {
1851     ResourceMark rm(thread);
1852     tty-&gt;print(&quot;IC miss (%s) call to&quot;, Bytecodes::name(bc));
1853     callee_method-&gt;print_short_name(tty);
1854     tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1855   }
1856 
1857   if (ICMissHistogram) {
1858     MutexLocker m(VMStatistic_lock);
1859     RegisterMap reg_map(thread, false);
1860     frame f = thread-&gt;last_frame().real_sender(&amp;reg_map);// skip runtime stub
1861     // produce statistics under the lock
1862     trace_ic_miss(f.pc());
1863   }
1864 #endif
</pre>
<hr />
<pre>
2076       }
2077       // assert is too strong could also be resolve destinations.
2078       // assert(InlineCacheBuffer::contains(destination) || VtableStubs::contains(destination), &quot;must be&quot;);
2079     }
2080   } else {
2081     if (TraceCallFixup) {
2082       tty-&gt;print(&quot;already patched callsite at &quot; INTPTR_FORMAT &quot; to compiled code for&quot;, p2i(caller_pc));
2083       moop-&gt;print_short_name(tty);
2084       tty-&gt;print_cr(&quot; to &quot; INTPTR_FORMAT, p2i(entry_point));
2085     }
2086   }
2087   return false;
2088 }
2089 
2090 // ---------------------------------------------------------------------------
2091 // We are calling the interpreter via a c2i. Normally this would mean that
2092 // we were called by a compiled method. However we could have lost a race
2093 // where we went int -&gt; i2c -&gt; c2i and so the caller could in fact be
2094 // interpreted. If the caller is compiled we attempt to patch the caller
2095 // so he no longer calls into the interpreter.
<span class="line-modified">2096 IRT_LEAF(void, SharedRuntime::fixup_callers_callsite(Method* method, address caller_pc))</span>
2097   Method* moop(method);
2098 
2099   address entry_point = moop-&gt;from_compiled_entry_no_trampoline();
2100 
2101   // It&#39;s possible that deoptimization can occur at a call site which hasn&#39;t
2102   // been resolved yet, in which case this function will be called from
2103   // an nmethod that has been patched for deopt and we can ignore the
2104   // request for a fixup.
2105   // Also it is possible that we lost a race in that from_compiled_entry
2106   // is now back to the i2c in that case we don&#39;t need to patch and if
2107   // we did we&#39;d leap into space because the callsite needs to use
2108   // &quot;to interpreter&quot; stub in order to load up the Method*. Don&#39;t
2109   // ask me how I know this...
2110 
2111   CodeBlob* cb = CodeCache::find_blob(caller_pc);
2112   if (cb == NULL || !cb-&gt;is_compiled() || entry_point == moop-&gt;get_c2i_entry()) {
2113     return;
2114   }
2115 
2116   // The check above makes sure this is a nmethod.
</pre>
<hr />
<pre>
2145       // there. If you&#39;re lucky you&#39;ll get the assert in the bugid, if not you&#39;ve
2146       // just made a call site that could be megamorphic into a monomorphic site
2147       // for the rest of its life! Just another racing bug in the life of
2148       // fixup_callers_callsite ...
2149       //
2150       RelocIterator iter(nm, call-&gt;instruction_address(), call-&gt;next_instruction_address());
2151       iter.next();
2152       assert(iter.has_current(), &quot;must have a reloc at java call site&quot;);
2153       relocInfo::relocType typ = iter.reloc()-&gt;type();
2154       if (typ != relocInfo::static_call_type &amp;&amp;
2155            typ != relocInfo::opt_virtual_call_type &amp;&amp;
2156            typ != relocInfo::static_stub_type) {
2157         return;
2158       }
2159       address destination = call-&gt;destination();
2160       if (should_fixup_call_destination(destination, entry_point, caller_pc, moop, cb)) {
2161         call-&gt;set_destination_mt_safe(entry_point);
2162       }
2163     }
2164   }
<span class="line-modified">2165 IRT_END</span>
2166 
2167 
2168 // same as JVM_Arraycopy, but called directly from compiled code
2169 JRT_ENTRY(void, SharedRuntime::slow_arraycopy_C(oopDesc* src,  jint src_pos,
2170                                                 oopDesc* dest, jint dest_pos,
2171                                                 jint length,
2172                                                 JavaThread* thread)) {
2173 #ifndef PRODUCT
2174   _slow_array_copy_ctr++;
2175 #endif
2176   // Check if we have null pointers
2177   if (src == NULL || dest == NULL) {
2178     THROW(vmSymbols::java_lang_NullPointerException());
2179   }
2180   // Do the copy.  The casts to arrayOop are necessary to the copy_array API,
2181   // even though the copy_array API also performs dynamic checks to ensure
2182   // that src and dest are truly arrays (and are conformable).
2183   // The copy_array mechanism is awkward and could be removed, but
2184   // the compilers don&#39;t call this function except as a last resort,
2185   // so it probably doesn&#39;t matter.
</pre>
<hr />
<pre>
2199   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
2200   Bytecode_checkcast cc(vfst.method(), vfst.method()-&gt;bcp_from(vfst.bci()));
2201   constantPoolHandle cpool(thread, vfst.method()-&gt;constants());
2202   Klass* target_klass = ConstantPool::klass_at_if_loaded(cpool, cc.index());
2203   Symbol* target_klass_name = NULL;
2204   if (target_klass == NULL) {
2205     // This klass should be resolved, but just in case, get the name in the klass slot.
2206     target_klass_name = cpool-&gt;klass_name_at(cc.index());
2207   }
2208   return generate_class_cast_message(caster_klass, target_klass, target_klass_name);
2209 }
2210 
2211 
2212 // The caller of generate_class_cast_message() (or one of its callers)
2213 // must use a ResourceMark in order to correctly free the result.
2214 char* SharedRuntime::generate_class_cast_message(
2215     Klass* caster_klass, Klass* target_klass, Symbol* target_klass_name) {
2216   const char* caster_name = caster_klass-&gt;external_name();
2217 
2218   assert(target_klass != NULL || target_klass_name != NULL, &quot;one must be provided&quot;);
<span class="line-modified">2219   const char* target_name = target_klass == NULL ? target_klass_name-&gt;as_C_string() :</span>
2220                                                    target_klass-&gt;external_name();
2221 
2222   size_t msglen = strlen(caster_name) + strlen(&quot;class &quot;) + strlen(&quot; cannot be cast to class &quot;) + strlen(target_name) + 1;
2223 
2224   const char* caster_klass_description = &quot;&quot;;
2225   const char* target_klass_description = &quot;&quot;;
2226   const char* klass_separator = &quot;&quot;;
2227   if (target_klass != NULL &amp;&amp; caster_klass-&gt;module() == target_klass-&gt;module()) {
2228     caster_klass_description = caster_klass-&gt;joint_in_module_of_loader(target_klass);
2229   } else {
2230     caster_klass_description = caster_klass-&gt;class_in_module_of_loader();
2231     target_klass_description = (target_klass != NULL) ? target_klass-&gt;class_in_module_of_loader() : &quot;&quot;;
2232     klass_separator = (target_klass != NULL) ? &quot;; &quot; : &quot;&quot;;
2233   }
2234 
2235   // add 3 for parenthesis and preceeding space
2236   msglen += strlen(caster_klass_description) + strlen(target_klass_description) + strlen(klass_separator) + 3;
2237 
2238   char* message = NEW_RESOURCE_ARRAY_RETURN_NULL(char, msglen);
2239   if (message == NULL) {
</pre>
<hr />
<pre>
2258 JRT_END
2259 
2260 
2261 // Handles the uncommon case in locking, i.e., contention or an inflated lock.
2262 JRT_BLOCK_ENTRY(void, SharedRuntime::complete_monitor_locking_C(oopDesc* _obj, BasicLock* lock, JavaThread* thread))
2263   if (!SafepointSynchronize::is_synchronizing()) {
2264     // Only try quick_enter() if we&#39;re not trying to reach a safepoint
2265     // so that the calling thread reaches the safepoint more quickly.
2266     if (ObjectSynchronizer::quick_enter(_obj, thread, lock)) return;
2267   }
2268   // NO_ASYNC required because an async exception on the state transition destructor
2269   // would leave you with the lock held and it would never be released.
2270   // The normal monitorenter NullPointerException is thrown without acquiring a lock
2271   // and the model is that an exception implies the method failed.
2272   JRT_BLOCK_NO_ASYNC
2273   oop obj(_obj);
2274   if (PrintBiasedLockingStatistics) {
2275     Atomic::inc(BiasedLocking::slow_path_entry_count_addr());
2276   }
2277   Handle h_obj(THREAD, obj);
<span class="line-modified">2278   if (UseBiasedLocking) {</span>
<span class="line-removed">2279     // Retry fast entry if bias is revoked to avoid unnecessary inflation</span>
<span class="line-removed">2280     ObjectSynchronizer::fast_enter(h_obj, lock, true, CHECK);</span>
<span class="line-removed">2281   } else {</span>
<span class="line-removed">2282     ObjectSynchronizer::slow_enter(h_obj, lock, CHECK);</span>
<span class="line-removed">2283   }</span>
2284   assert(!HAS_PENDING_EXCEPTION, &quot;Should have no exception here&quot;);
2285   JRT_BLOCK_END
2286 JRT_END
2287 
2288 // Handles the uncommon cases of monitor unlocking in compiled code
2289 JRT_LEAF(void, SharedRuntime::complete_monitor_unlocking_C(oopDesc* _obj, BasicLock* lock, JavaThread * THREAD))
2290    oop obj(_obj);
2291   assert(JavaThread::current() == THREAD, &quot;invariant&quot;);
2292   // I&#39;m not convinced we need the code contained by MIGHT_HAVE_PENDING anymore
2293   // testing was unable to ever fire the assert that guarded it so I have removed it.
2294   assert(!HAS_PENDING_EXCEPTION, &quot;Do we need code below anymore?&quot;);
2295 #undef MIGHT_HAVE_PENDING
2296 #ifdef MIGHT_HAVE_PENDING
2297   // Save and restore any pending_exception around the exception mark.
2298   // While the slow_exit must not throw an exception, we could come into
2299   // this routine with one set.
2300   oop pending_excep = NULL;
2301   const char* pending_file;
2302   int pending_line;
2303   if (HAS_PENDING_EXCEPTION) {
2304     pending_excep = PENDING_EXCEPTION;
2305     pending_file  = THREAD-&gt;exception_file();
2306     pending_line  = THREAD-&gt;exception_line();
2307     CLEAR_PENDING_EXCEPTION;
2308   }
2309 #endif /* MIGHT_HAVE_PENDING */
2310 
2311   {
2312     // Exit must be non-blocking, and therefore no exceptions can be thrown.
2313     EXCEPTION_MARK;
<span class="line-modified">2314     ObjectSynchronizer::slow_exit(obj, lock, THREAD);</span>
2315   }
2316 
2317 #ifdef MIGHT_HAVE_PENDING
2318   if (pending_excep != NULL) {
2319     THREAD-&gt;set_pending_exception(pending_excep, pending_file, pending_line);
2320   }
2321 #endif /* MIGHT_HAVE_PENDING */
2322 JRT_END
2323 
2324 #ifndef PRODUCT
2325 
2326 void SharedRuntime::print_statistics() {
2327   ttyLocker ttyl;
2328   if (xtty != NULL)  xtty-&gt;head(&quot;statistics type=&#39;SharedRuntime&#39;&quot;);
2329 
2330   if (_throw_null_ctr) tty-&gt;print_cr(&quot;%5d implicit null throw&quot;, _throw_null_ctr);
2331 
2332   SharedRuntime::print_ic_miss_histogram();
2333 
2334   if (CountRemovableExceptions) {
</pre>
<hr />
<pre>
2414     double rest = sum;
2415     double percent = sum / 100;
2416     for (i = 0; i &lt;= N; i++) {
2417       rest -= histo[i];
2418       tty-&gt;print_cr(&quot;%4d: %7d (%5.1f%%)&quot;, i, histo[i], histo[i] / percent);
2419     }
2420     tty-&gt;print_cr(&quot;rest: %7d (%5.1f%%))&quot;, (int)rest, rest / percent);
2421     tty-&gt;print_cr(&quot;(avg. %s = %3.1f, max = %d)&quot;, name, weighted_sum / sum, n);
2422   }
2423 
2424   void print_histogram() {
2425     tty-&gt;print_cr(&quot;\nHistogram of call arity (incl. rcvr, calls to compiled methods only):&quot;);
2426     print_histogram_helper(_max_arity, _arity_histogram, &quot;arity&quot;);
2427     tty-&gt;print_cr(&quot;\nSame for parameter size (in words):&quot;);
2428     print_histogram_helper(_max_size, _size_histogram, &quot;size&quot;);
2429     tty-&gt;cr();
2430   }
2431 
2432  public:
2433   MethodArityHistogram() {
<span class="line-modified">2434     MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
2435     _max_arity = _max_size = 0;
2436     for (int i = 0; i &lt; MAX_ARITY; i++) _arity_histogram[i] = _size_histogram[i] = 0;
2437     CodeCache::nmethods_do(add_method_to_histogram);
2438     print_histogram();
2439   }
2440 };
2441 
2442 int MethodArityHistogram::_arity_histogram[MethodArityHistogram::MAX_ARITY];
2443 int MethodArityHistogram::_size_histogram[MethodArityHistogram::MAX_ARITY];
2444 int MethodArityHistogram::_max_arity;
2445 int MethodArityHistogram::_max_size;
2446 
2447 void SharedRuntime::print_call_statistics(int comp_total) {
2448   tty-&gt;print_cr(&quot;Calls from compiled code:&quot;);
2449   int total  = _nof_normal_calls + _nof_interface_calls + _nof_static_calls;
2450   int mono_c = _nof_normal_calls - _nof_optimized_calls - _nof_megamorphic_calls;
2451   int mono_i = _nof_interface_calls - _nof_optimized_interface_calls - _nof_megamorphic_interface_calls;
2452   tty-&gt;print_cr(&quot;\t%9d   (%4.1f%%) total non-inlined   &quot;, total, percent(total, total));
2453   tty-&gt;print_cr(&quot;\t%9d   (%4.1f%%) virtual calls       &quot;, _nof_normal_calls, percent(_nof_normal_calls, total));
2454   tty-&gt;print_cr(&quot;\t  %9d  (%3.0f%%)   inlined          &quot;, _nof_inlined_calls, percent(_nof_inlined_calls, _nof_normal_calls));
</pre>
<hr />
<pre>
2629 
2630  private:
2631 
2632 #ifndef PRODUCT
2633   static int _lookups; // number of calls to lookup
2634   static int _buckets; // number of buckets checked
2635   static int _equals;  // number of buckets checked with matching hash
2636   static int _hits;    // number of successful lookups
2637   static int _compact; // number of equals calls with compact signature
2638 #endif
2639 
2640   AdapterHandlerEntry* bucket(int i) {
2641     return (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::bucket(i);
2642   }
2643 
2644  public:
2645   AdapterHandlerTable()
2646     : BasicHashtable&lt;mtCode&gt;(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
2647 
2648   // Create a new entry suitable for insertion in the table
<span class="line-modified">2649   AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry) {</span>
2650     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::new_entry(fingerprint-&gt;compute_hash());
<span class="line-modified">2651     entry-&gt;init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);</span>
2652     if (DumpSharedSpaces) {
2653       ((CDSAdapterHandlerEntry*)entry)-&gt;init();
2654     }
2655     return entry;
2656   }
2657 
2658   // Insert an entry into the table
2659   void add(AdapterHandlerEntry* entry) {
2660     int index = hash_to_index(entry-&gt;hash());
2661     add_entry(index, entry);
2662   }
2663 
2664   void free_entry(AdapterHandlerEntry* entry) {
2665     entry-&gt;deallocate();
2666     BasicHashtable&lt;mtCode&gt;::free_entry(entry);
2667   }
2668 
2669   // Find a entry with the same fingerprint if it exists
2670   AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {
2671     NOT_PRODUCT(_lookups++);
</pre>
<hr />
<pre>
2780 }
2781 
2782 void AdapterHandlerLibrary::initialize() {
2783   if (_adapters != NULL) return;
2784   _adapters = new AdapterHandlerTable();
2785 
2786   // Create a special handler for abstract methods.  Abstract methods
2787   // are never compiled so an i2c entry is somewhat meaningless, but
2788   // throw AbstractMethodError just in case.
2789   // Pass wrong_method_abstract for the c2i transitions to return
2790   // AbstractMethodError for invalid invocations.
2791   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
2792   _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),
2793                                                               StubRoutines::throw_AbstractMethodError_entry(),
2794                                                               wrong_method_abstract, wrong_method_abstract);
2795 }
2796 
2797 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
2798                                                       address i2c_entry,
2799                                                       address c2i_entry,
<span class="line-modified">2800                                                       address c2i_unverified_entry) {</span>
<span class="line-modified">2801   return _adapters-&gt;new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);</span>

2802 }
2803 
2804 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle&amp; method) {
2805   AdapterHandlerEntry* entry = get_adapter0(method);
<span class="line-modified">2806   if (method-&gt;is_shared()) {</span>
2807     // See comments around Method::link_method()
2808     MutexLocker mu(AdapterHandlerLibrary_lock);
2809     if (method-&gt;adapter() == NULL) {
2810       method-&gt;update_adapter_trampoline(entry);
2811     }
2812     address trampoline = method-&gt;from_compiled_entry();
2813     if (*(int*)trampoline == 0) {
2814       CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
2815       MacroAssembler _masm(&amp;buffer);
2816       SharedRuntime::generate_trampoline(&amp;_masm, entry-&gt;get_c2i_entry());
2817       assert(*(int*)trampoline != 0, &quot;Instruction(s) for trampoline must not be encoded as zeros.&quot;);

2818 
2819       if (PrintInterpreter) {
2820         Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
2821       }
2822     }
2823   }
2824 
2825   return entry;
2826 }
2827 
2828 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle&amp; method) {
2829   // Use customized signature handler.  Need to lock around updates to
2830   // the AdapterHandlerTable (it is not safe for concurrent readers
2831   // and a single writer: this could be fixed if it becomes a
2832   // problem).
2833 
2834   ResourceMark rm;
2835 
2836   NOT_PRODUCT(int insts_size);
2837   AdapterBlob* new_adapter = NULL;
</pre>
<hr />
<pre>
2957     jio_snprintf(blob_id,
2958                  sizeof(blob_id),
2959                  &quot;%s(%s)@&quot; PTR_FORMAT,
2960                  new_adapter-&gt;name(),
2961                  fingerprint-&gt;as_string(),
2962                  new_adapter-&gt;content_begin());
2963     Forte::register_stub(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
2964 
2965     if (JvmtiExport::should_post_dynamic_code_generated()) {
2966       JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
2967     }
2968   }
2969   return entry;
2970 }
2971 
2972 address AdapterHandlerEntry::base_address() {
2973   address base = _i2c_entry;
2974   if (base == NULL)  base = _c2i_entry;
2975   assert(base &lt;= _c2i_entry || _c2i_entry == NULL, &quot;&quot;);
2976   assert(base &lt;= _c2i_unverified_entry || _c2i_unverified_entry == NULL, &quot;&quot;);

2977   return base;
2978 }
2979 
2980 void AdapterHandlerEntry::relocate(address new_base) {
2981   address old_base = base_address();
2982   assert(old_base != NULL, &quot;&quot;);
2983   ptrdiff_t delta = new_base - old_base;
2984   if (_i2c_entry != NULL)
2985     _i2c_entry += delta;
2986   if (_c2i_entry != NULL)
2987     _c2i_entry += delta;
2988   if (_c2i_unverified_entry != NULL)
2989     _c2i_unverified_entry += delta;


2990   assert(base_address() == new_base, &quot;&quot;);
2991 }
2992 
2993 
2994 void AdapterHandlerEntry::deallocate() {
2995   delete _fingerprint;
2996 #ifdef ASSERT
<span class="line-modified">2997   if (_saved_code) FREE_C_HEAP_ARRAY(unsigned char, _saved_code);</span>
2998 #endif
2999 }
3000 
3001 
3002 #ifdef ASSERT
3003 // Capture the code before relocation so that it can be compared
3004 // against other versions.  If the code is captured after relocation
3005 // then relative instructions won&#39;t be equivalent.
3006 void AdapterHandlerEntry::save_code(unsigned char* buffer, int length) {
3007   _saved_code = NEW_C_HEAP_ARRAY(unsigned char, length, mtCode);
3008   _saved_code_length = length;
3009   memcpy(_saved_code, buffer, length);
3010 }
3011 
3012 
3013 bool AdapterHandlerEntry::compare_code(unsigned char* buffer, int length) {
3014   if (length != _saved_code_length) {
3015     return false;
3016   }
3017 
3018   return (memcmp(buffer, _saved_code, length) == 0) ? true : false;
3019 }
3020 #endif
3021 
3022 
3023 /**
3024  * Create a native wrapper for this native method.  The wrapper converts the
3025  * Java-compiled calling convention to the native convention, handles
3026  * arguments, and transitions to native.  On return from the native we transition
3027  * back to java blocking if a safepoint is in progress.
3028  */
3029 void AdapterHandlerLibrary::create_native_wrapper(const methodHandle&amp; method) {
3030   ResourceMark rm;
3031   nmethod* nm = NULL;

3032 
3033   assert(method-&gt;is_native(), &quot;must be native&quot;);
3034   assert(method-&gt;is_method_handle_intrinsic() ||
3035          method-&gt;has_native_function(), &quot;must have something valid to call!&quot;);
3036 





3037   {
3038     // Perform the work while holding the lock, but perform any printing outside the lock
3039     MutexLocker mu(AdapterHandlerLibrary_lock);
3040     // See if somebody beat us to it
3041     if (method-&gt;code() != NULL) {
3042       return;
3043     }
3044 
3045     const int compile_id = CompileBroker::assign_compile_id(method, CompileBroker::standard_entry_bci);
3046     assert(compile_id &gt; 0, &quot;Must generate native wrapper&quot;);
3047 
3048 
3049     ResourceMark rm;
3050     BufferBlob*  buf = buffer_blob(); // the temporary code buffer in CodeCache
3051     if (buf != NULL) {
3052       CodeBuffer buffer(buf);
3053       double locs_buf[20];
3054       buffer.insts()-&gt;initialize_shared_locs((relocInfo*)locs_buf, sizeof(locs_buf) / sizeof(relocInfo));
3055       MacroAssembler _masm(&amp;buffer);
3056 
</pre>
<hr />
<pre>
3061       VMRegPair*   regs = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);
3062       int i=0;
3063       if (!method-&gt;is_static())  // Pass in receiver first
3064         sig_bt[i++] = T_OBJECT;
3065       SignatureStream ss(method-&gt;signature());
3066       for (; !ss.at_return_type(); ss.next()) {
3067         sig_bt[i++] = ss.type();  // Collect remaining bits of signature
3068         if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
3069           sig_bt[i++] = T_VOID;   // Longs &amp; doubles take 2 Java slots
3070       }
3071       assert(i == total_args_passed, &quot;&quot;);
3072       BasicType ret_type = ss.type();
3073 
3074       // Now get the compiled-Java layout as input (or output) arguments.
3075       // NOTE: Stubs for compiled entry points of method handle intrinsics
3076       // are just trampolines so the argument registers must be outgoing ones.
3077       const bool is_outgoing = method-&gt;is_method_handle_intrinsic();
3078       int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, is_outgoing);
3079 
3080       // Generate the compiled-to-native wrapper code
<span class="line-modified">3081       nm = SharedRuntime::generate_native_wrapper(&amp;_masm, method, compile_id, sig_bt, regs, ret_type);</span>
3082 
3083       if (nm != NULL) {
<span class="line-modified">3084         method-&gt;set_code(method, nm);</span>





3085 
3086         DirectiveSet* directive = DirectivesStack::getDefaultDirective(CompileBroker::compiler(CompLevel_simple));
3087         if (directive-&gt;PrintAssemblyOption) {
3088           nm-&gt;print_code();
3089         }
3090         DirectivesStack::release(directive);
3091       }
3092     }
3093   } // Unlock AdapterHandlerLibrary_lock
3094 
3095 
3096   // Install the generated code.
3097   if (nm != NULL) {
3098     const char *msg = method-&gt;is_static() ? &quot;(static)&quot; : &quot;&quot;;
3099     CompileTask::print_ul(nm, msg);
3100     if (PrintCompilation) {
3101       ttyLocker ttyl;
3102       CompileTask::print(tty, nm, msg);
3103     }
3104     nm-&gt;post_compiled_method_load_event();
</pre>
<hr />
<pre>
3136 JRT_END
3137 
3138 // -------------------------------------------------------------------------
3139 // Java-Java calling convention
3140 // (what you use when Java calls Java)
3141 
3142 //------------------------------name_for_receiver----------------------------------
3143 // For a given signature, return the VMReg for parameter 0.
3144 VMReg SharedRuntime::name_for_receiver() {
3145   VMRegPair regs;
3146   BasicType sig_bt = T_OBJECT;
3147   (void) java_calling_convention(&amp;sig_bt, &amp;regs, 1, true);
3148   // Return argument 0 register.  In the LP64 build pointers
3149   // take 2 registers, but the VM wants only the &#39;main&#39; name.
3150   return regs.first();
3151 }
3152 
3153 VMRegPair *SharedRuntime::find_callee_arguments(Symbol* sig, bool has_receiver, bool has_appendix, int* arg_size) {
3154   // This method is returning a data structure allocating as a
3155   // ResourceObject, so do not put any ResourceMarks in here.
<span class="line-removed">3156   char *s = sig-&gt;as_C_string();</span>
<span class="line-removed">3157   int len = (int)strlen(s);</span>
<span class="line-removed">3158   s++; len--;                   // Skip opening paren</span>
3159 
3160   BasicType *sig_bt = NEW_RESOURCE_ARRAY(BasicType, 256);
3161   VMRegPair *regs = NEW_RESOURCE_ARRAY(VMRegPair, 256);
3162   int cnt = 0;
3163   if (has_receiver) {
3164     sig_bt[cnt++] = T_OBJECT; // Receiver is argument 0; not in signature
3165   }
3166 
<span class="line-modified">3167   while (*s != &#39;)&#39;) {          // Find closing right paren</span>
<span class="line-modified">3168     switch (*s++) {            // Switch on signature character</span>
<span class="line-modified">3169     case &#39;B&#39;: sig_bt[cnt++] = T_BYTE;    break;</span>
<span class="line-modified">3170     case &#39;C&#39;: sig_bt[cnt++] = T_CHAR;    break;</span>
<span class="line-modified">3171     case &#39;D&#39;: sig_bt[cnt++] = T_DOUBLE;  sig_bt[cnt++] = T_VOID; break;</span>
<span class="line-removed">3172     case &#39;F&#39;: sig_bt[cnt++] = T_FLOAT;   break;</span>
<span class="line-removed">3173     case &#39;I&#39;: sig_bt[cnt++] = T_INT;     break;</span>
<span class="line-removed">3174     case &#39;J&#39;: sig_bt[cnt++] = T_LONG;    sig_bt[cnt++] = T_VOID; break;</span>
<span class="line-removed">3175     case &#39;S&#39;: sig_bt[cnt++] = T_SHORT;   break;</span>
<span class="line-removed">3176     case &#39;Z&#39;: sig_bt[cnt++] = T_BOOLEAN; break;</span>
<span class="line-removed">3177     case &#39;V&#39;: sig_bt[cnt++] = T_VOID;    break;</span>
<span class="line-removed">3178     case &#39;L&#39;:                   // Oop</span>
<span class="line-removed">3179       while (*s++ != &#39;;&#39;);   // Skip signature</span>
<span class="line-removed">3180       sig_bt[cnt++] = T_OBJECT;</span>
<span class="line-removed">3181       break;</span>
<span class="line-removed">3182     case &#39;[&#39;: {                 // Array</span>
<span class="line-removed">3183       do {                      // Skip optional size</span>
<span class="line-removed">3184         while (*s &gt;= &#39;0&#39; &amp;&amp; *s &lt;= &#39;9&#39;) s++;</span>
<span class="line-removed">3185       } while (*s++ == &#39;[&#39;);   // Nested arrays?</span>
<span class="line-removed">3186       // Skip element type</span>
<span class="line-removed">3187       if (s[-1] == &#39;L&#39;)</span>
<span class="line-removed">3188         while (*s++ != &#39;;&#39;); // Skip signature</span>
<span class="line-removed">3189       sig_bt[cnt++] = T_ARRAY;</span>
<span class="line-removed">3190       break;</span>
<span class="line-removed">3191     }</span>
<span class="line-removed">3192     default : ShouldNotReachHere();</span>
<span class="line-removed">3193     }</span>
3194   }
3195 
3196   if (has_appendix) {
3197     sig_bt[cnt++] = T_OBJECT;
3198   }
3199 
3200   assert(cnt &lt; 256, &quot;grow table size&quot;);
3201 
3202   int comp_args_on_stack;
3203   comp_args_on_stack = java_calling_convention(sig_bt, regs, cnt, true);
3204 
3205   // the calling convention doesn&#39;t count out_preserve_stack_slots so
3206   // we must add that in to get &quot;true&quot; stack offsets.
3207 
3208   if (comp_args_on_stack) {
3209     for (int i = 0; i &lt; cnt; i++) {
3210       VMReg reg1 = regs[i].first();
3211       if (reg1-&gt;is_stack()) {
3212         // Yuck
3213         reg1 = reg1-&gt;bias(out_preserve_stack_slots());
</pre>
<hr />
<pre>
3269   int max_locals = moop-&gt;max_locals();
3270   // Allocate temp buffer, 1 word per local &amp; 2 per active monitor
3271   int buf_size_words = max_locals + active_monitor_count * BasicObjectLock::size();
3272   intptr_t *buf = NEW_C_HEAP_ARRAY(intptr_t,buf_size_words, mtCode);
3273 
3274   // Copy the locals.  Order is preserved so that loading of longs works.
3275   // Since there&#39;s no GC I can copy the oops blindly.
3276   assert(sizeof(HeapWord)==sizeof(intptr_t), &quot;fix this code&quot;);
3277   Copy::disjoint_words((HeapWord*)fr.interpreter_frame_local_at(max_locals-1),
3278                        (HeapWord*)&amp;buf[0],
3279                        max_locals);
3280 
3281   // Inflate locks.  Copy the displaced headers.  Be careful, there can be holes.
3282   int i = max_locals;
3283   for (BasicObjectLock *kptr2 = fr.interpreter_frame_monitor_end();
3284        kptr2 &lt; fr.interpreter_frame_monitor_begin();
3285        kptr2 = fr.next_monitor_in_interpreter_frame(kptr2) ) {
3286     if (kptr2-&gt;obj() != NULL) {         // Avoid &#39;holes&#39; in the monitor array
3287       BasicLock *lock = kptr2-&gt;lock();
3288       // Inflate so the displaced header becomes position-independent
<span class="line-modified">3289       if (lock-&gt;displaced_header()-&gt;is_unlocked())</span>
3290         ObjectSynchronizer::inflate_helper(kptr2-&gt;obj());
3291       // Now the displaced header is free to move
<span class="line-modified">3292       buf[i++] = (intptr_t)lock-&gt;displaced_header();</span>
3293       buf[i++] = cast_from_oop&lt;intptr_t&gt;(kptr2-&gt;obj());
3294     }
3295   }
3296   assert(i - max_locals == active_monitor_count*2, &quot;found the expected number of monitors&quot;);
3297 
3298   return buf;
3299 JRT_END
3300 
3301 JRT_LEAF(void, SharedRuntime::OSR_migration_end( intptr_t* buf) )
3302   FREE_C_HEAP_ARRAY(intptr_t, buf);
3303 JRT_END
3304 
3305 bool AdapterHandlerLibrary::contains(const CodeBlob* b) {
3306   AdapterHandlerTableIterator iter(_adapters);
3307   while (iter.has_next()) {
3308     AdapterHandlerEntry* a = iter.next();
3309     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) return true;
3310   }
3311   return false;
3312 }
3313 
3314 void AdapterHandlerLibrary::print_handler_on(outputStream* st, const CodeBlob* b) {
3315   AdapterHandlerTableIterator iter(_adapters);
3316   while (iter.has_next()) {
3317     AdapterHandlerEntry* a = iter.next();
3318     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) {
3319       st-&gt;print(&quot;Adapter for signature: &quot;);
3320       a-&gt;print_adapter_on(tty);
3321       return;
3322     }
3323   }
3324   assert(false, &quot;Should have found handler&quot;);
3325 }
3326 
3327 void AdapterHandlerEntry::print_adapter_on(outputStream* st) const {
<span class="line-modified">3328   st-&gt;print_cr(&quot;AHE@&quot; INTPTR_FORMAT &quot;: %s i2c: &quot; INTPTR_FORMAT &quot; c2i: &quot; INTPTR_FORMAT &quot; c2iUV: &quot; INTPTR_FORMAT,</span>
<span class="line-modified">3329                p2i(this), fingerprint()-&gt;as_string(),</span>
<span class="line-modified">3330                p2i(get_i2c_entry()), p2i(get_c2i_entry()), p2i(get_c2i_unverified_entry()));</span>
<span class="line-modified">3331 </span>










3332 }
3333 
3334 #if INCLUDE_CDS
3335 
3336 void CDSAdapterHandlerEntry::init() {
3337   assert(DumpSharedSpaces, &quot;used during dump time only&quot;);
3338   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
3339   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
3340 };
3341 
3342 #endif // INCLUDE_CDS
3343 
3344 
3345 #ifndef PRODUCT
3346 
3347 void AdapterHandlerLibrary::print_statistics() {
3348   _adapters-&gt;print_statistics();
3349 }
3350 
3351 #endif /* PRODUCT */
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  40 #include &quot;gc/shared/barrierSet.hpp&quot;
  41 #include &quot;gc/shared/gcLocker.inline.hpp&quot;
  42 #include &quot;interpreter/interpreter.hpp&quot;
  43 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  44 #include &quot;jfr/jfrEvents.hpp&quot;
  45 #include &quot;logging/log.hpp&quot;
  46 #include &quot;memory/metaspaceShared.hpp&quot;
  47 #include &quot;memory/resourceArea.hpp&quot;
  48 #include &quot;memory/universe.hpp&quot;
  49 #include &quot;oops/klass.hpp&quot;
  50 #include &quot;oops/method.inline.hpp&quot;
  51 #include &quot;oops/objArrayKlass.hpp&quot;
  52 #include &quot;oops/oop.inline.hpp&quot;
  53 #include &quot;prims/forte.hpp&quot;
  54 #include &quot;prims/jvmtiExport.hpp&quot;
  55 #include &quot;prims/methodHandles.hpp&quot;
  56 #include &quot;prims/nativeLookup.hpp&quot;
  57 #include &quot;runtime/arguments.hpp&quot;
  58 #include &quot;runtime/atomic.hpp&quot;
  59 #include &quot;runtime/biasedLocking.hpp&quot;

  60 #include &quot;runtime/frame.inline.hpp&quot;
  61 #include &quot;runtime/handles.inline.hpp&quot;
  62 #include &quot;runtime/init.hpp&quot;
  63 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  64 #include &quot;runtime/java.hpp&quot;
  65 #include &quot;runtime/javaCalls.hpp&quot;
  66 #include &quot;runtime/sharedRuntime.hpp&quot;
  67 #include &quot;runtime/stubRoutines.hpp&quot;
  68 #include &quot;runtime/vframe.inline.hpp&quot;
  69 #include &quot;runtime/vframeArray.hpp&quot;
  70 #include &quot;utilities/copy.hpp&quot;
  71 #include &quot;utilities/dtrace.hpp&quot;
  72 #include &quot;utilities/events.hpp&quot;
  73 #include &quot;utilities/hashtable.inline.hpp&quot;
  74 #include &quot;utilities/macros.hpp&quot;
  75 #include &quot;utilities/xmlstream.hpp&quot;
  76 #ifdef COMPILER1
  77 #include &quot;c1/c1_Runtime1.hpp&quot;
  78 #endif
  79 #if INCLUDE_TSAN
</pre>
<hr />
<pre>
 750 
 751 void SharedRuntime::throw_StackOverflowError_common(JavaThread* thread, bool delayed) {
 752   // We avoid using the normal exception construction in this case because
 753   // it performs an upcall to Java, and we&#39;re already out of stack space.
 754   Thread* THREAD = thread;
 755   Klass* k = SystemDictionary::StackOverflowError_klass();
 756   oop exception_oop = InstanceKlass::cast(k)-&gt;allocate_instance(CHECK);
 757   if (delayed) {
 758     java_lang_Throwable::set_message(exception_oop,
 759                                      Universe::delayed_stack_overflow_error_message());
 760   }
 761   Handle exception (thread, exception_oop);
 762   if (StackTraceInThrowable) {
 763     java_lang_Throwable::fill_in_stack_trace(exception);
 764   }
 765   // Increment counter for hs_err file reporting
 766   Atomic::inc(&amp;Exceptions::_stack_overflow_errors);
 767   throw_and_post_jvmti_exception(thread, exception);
 768 }
 769 









 770 address SharedRuntime::continuation_for_implicit_exception(JavaThread* thread,
 771                                                            address pc,
<span class="line-modified"> 772                                                            ImplicitExceptionKind exception_kind)</span>
 773 {
 774   address target_pc = NULL;
 775 
 776   if (Interpreter::contains(pc)) {
 777 #ifdef CC_INTERP
 778     // C++ interpreter doesn&#39;t throw implicit exceptions
 779     ShouldNotReachHere();
 780 #else
 781     switch (exception_kind) {
 782       case IMPLICIT_NULL:           return Interpreter::throw_NullPointerException_entry();
 783       case IMPLICIT_DIVIDE_BY_ZERO: return Interpreter::throw_ArithmeticException_entry();
 784       case STACK_OVERFLOW:          return Interpreter::throw_StackOverflowError_entry();
 785       default:                      ShouldNotReachHere();
 786     }
 787 #endif // !CC_INTERP
 788   } else {
 789     switch (exception_kind) {
 790       case STACK_OVERFLOW: {
 791         // Stack overflow only occurs upon frame setup; the callee is
 792         // going to be unwound. Dispatch to a shared runtime stub
</pre>
<hr />
<pre>
 853           // Otherwise, it&#39;s a compiled method.  Consult its exception handlers.
 854           CompiledMethod* cm = (CompiledMethod*)cb;
 855           if (cm-&gt;inlinecache_check_contains(pc)) {
 856             // exception happened inside inline-cache check code
 857             // =&gt; the nmethod is not yet active (i.e., the frame
 858             // is not set up yet) =&gt; use return address pushed by
 859             // caller =&gt; don&#39;t push another return address
 860             Events::log_exception(thread, &quot;NullPointerException in IC check &quot; INTPTR_FORMAT, p2i(pc));
 861             return StubRoutines::throw_NullPointerException_at_call_entry();
 862           }
 863 
 864           if (cm-&gt;method()-&gt;is_method_handle_intrinsic()) {
 865             // exception happened inside MH dispatch code, similar to a vtable stub
 866             Events::log_exception(thread, &quot;NullPointerException in MH adapter &quot; INTPTR_FORMAT, p2i(pc));
 867             return StubRoutines::throw_NullPointerException_at_call_entry();
 868           }
 869 
 870 #ifndef PRODUCT
 871           _implicit_null_throws++;
 872 #endif
<span class="line-modified"> 873           target_pc = cm-&gt;continuation_for_implicit_null_exception(pc);</span>












 874           // If there&#39;s an unexpected fault, target_pc might be NULL,
 875           // in which case we want to fall through into the normal
 876           // error handling code.
 877         }
 878 
 879         break; // fall through
 880       }
 881 
 882 
 883       case IMPLICIT_DIVIDE_BY_ZERO: {
 884         CompiledMethod* cm = CodeCache::find_compiled(pc);
 885         guarantee(cm != NULL, &quot;must have containing compiled method for implicit division-by-zero exceptions&quot;);
 886 #ifndef PRODUCT
 887         _implicit_div0_throws++;
 888 #endif
<span class="line-modified"> 889         target_pc = cm-&gt;continuation_for_implicit_div0_exception(pc);</span>








 890         // If there&#39;s an unexpected fault, target_pc might be NULL,
 891         // in which case we want to fall through into the normal
 892         // error handling code.
 893         break; // fall through
 894       }
 895 
 896       default: ShouldNotReachHere();
 897     }
 898 
 899     assert(exception_kind == IMPLICIT_NULL || exception_kind == IMPLICIT_DIVIDE_BY_ZERO, &quot;wrong implicit exception kind&quot;);
 900 
 901     if (exception_kind == IMPLICIT_NULL) {
 902 #ifndef PRODUCT
 903       // for AbortVMOnException flag
 904       Exceptions::debug_check_abort(&quot;java.lang.NullPointerException&quot;);
 905 #endif //PRODUCT
 906       Events::log_exception(thread, &quot;Implicit null exception at &quot; INTPTR_FORMAT &quot; to &quot; INTPTR_FORMAT, p2i(pc), p2i(target_pc));
 907     } else {
 908 #ifndef PRODUCT
 909       // for AbortVMOnException flag
</pre>
<hr />
<pre>
1011       (char *) sig-&gt;bytes(), sig-&gt;utf8_length());
1012   return 0;
1013 JRT_END
1014 
1015 #if INCLUDE_TSAN
1016 
1017 JRT_LEAF(void, SharedRuntime::verify_oop_index(oopDesc* obj, int index))
1018   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1019   assert(index &gt;= 0, &quot;index is less than 0&quot;);
1020   int obj_size_in_bytes = obj-&gt;size() * HeapWordSize;
1021   assert(index &lt; obj_size_in_bytes, &quot;index %d &gt;= obj size %d&quot;, index, obj_size_in_bytes);
1022 JRT_END
1023 
1024 // TSAN: method entry callback from interpreter
1025 // (1) In order to have the line numbers in the call stack, we use the caller
1026 //     address instead of the method that&#39;s being called. This also matches
1027 //     the entry/exit convention that TSAN uses for C++.
1028 // We use JRT_ENTRY since call_VM_leaf doesn&#39;t set _last_Java_sp that we need.
1029 JRT_ENTRY(void, SharedRuntime::tsan_interp_method_entry(JavaThread *thread))
1030   DEBUG_ONLY(NoSafepointVerifier nsv;)

1031   DEBUG_ONLY(NoHandleMark nhm;)
1032   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1033 
1034   RegisterMap unused_reg_map(thread, false);
1035 
1036   // These asserts should be removed once
1037   // we support more than just the interpreter for TSAN.
1038   assert(!thread-&gt;last_frame().is_compiled_frame(),
1039          &quot;Current frame should not be a compiled frame&quot;);
1040   const frame sender = thread-&gt;last_frame().real_sender(&amp;unused_reg_map);
1041   assert(!sender.is_compiled_frame(), &quot;Sender should not be a compiled frame&quot;);
1042 
1043   jmethodID jmethod_id = 0;
1044   u2 bci = 0;
1045   // TODO: is (0, 0) really the best we can do
1046   // when the sender isn&#39;t an interpreted frame?
1047   if (sender.is_interpreted_frame()) {
1048     jmethod_id = sender.interpreter_frame_method()-&gt;find_jmethod_id_or_null();
1049     bci = sender.interpreter_frame_bci();
1050   }
1051   __tsan_func_entry(tsan_code_location(jmethod_id, bci));
1052 JRT_END
1053 
1054 // TSAN: method exit callback from interpreter
1055 JRT_LEAF(void, SharedRuntime::tsan_interp_method_exit())
1056   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1057   __tsan_func_exit();
1058 JRT_END
1059 
1060 void SharedRuntime::tsan_oop_lock(Thread* thread, oop obj) {
1061   DEBUG_ONLY(NoSafepointVerifier nsv;)
1062   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1063   assert(thread != NULL, &quot;null thread&quot;);
1064   assert(obj != NULL, &quot;null oop&quot;);
1065   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1066 
1067   TsanOopMap::add_oop(obj);
<span class="line-modified">1068   __tsan_java_mutex_lock((julong)(oopDesc*)obj);</span>
1069 }
1070 
1071 void SharedRuntime::tsan_oop_unlock(Thread *thread, oop obj) {
1072   DEBUG_ONLY(NoSafepointVerifier nsv;)
1073   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1074   assert(thread != NULL, &quot;null thread&quot;);
1075   assert(obj != NULL, &quot;null oop&quot;);
1076   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1077   assert(TsanOopMap::exists(obj), &quot;oop seen in unlock but not tracked&quot;);
1078 
<span class="line-modified">1079   __tsan_java_mutex_unlock((julong)(oopDesc*)obj);</span>
1080 }
1081 
1082 void SharedRuntime::tsan_oop_rec_lock(Thread* thread, oop obj, int rec) {
1083   DEBUG_ONLY(NoSafepointVerifier nsv;)
1084   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1085   assert(thread != NULL, &quot;null thread&quot;);
1086   assert(obj != NULL, &quot;null oop&quot;);
1087   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1088 
1089   TsanOopMap::add_oop(obj);
<span class="line-modified">1090   __tsan_java_mutex_lock_rec((julong)(oopDesc*)obj, rec);</span>
1091 }
1092 
1093 int SharedRuntime::tsan_oop_rec_unlock(Thread *thread, oop obj) {
1094   DEBUG_ONLY(NoSafepointVerifier nsv;)
1095   assert(ThreadSanitizer, &quot;Need -XX:+ThreadSanitizer&quot;);
1096   assert(thread != NULL, &quot;null thread&quot;);
1097   assert(obj != NULL, &quot;null oop&quot;);
1098   assert(oopDesc::is_oop(obj), &quot;invalid oop&quot;);
1099   assert(TsanOopMap::exists(obj), &quot;oop seen in unlock but not tracked&quot;);
1100 
<span class="line-modified">1101   return __tsan_java_mutex_unlock_rec((julong)(oopDesc*)obj);</span>
1102 }
1103 
1104 JRT_LEAF(void, SharedRuntime::tsan_interp_lock(JavaThread* thread,
1105                                                BasicObjectLock* elem))
1106   DEBUG_ONLY(thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);)
1107   assert(elem != NULL, &quot;null elem&quot;);
1108 
1109   oop obj = elem-&gt;obj();
1110   tsan_oop_lock(thread, obj);
1111 
1112   assert(obj == elem-&gt;obj(), &quot;oop changed&quot;);
1113   DEBUG_ONLY(thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);)
1114 JRT_END
1115 
1116 JRT_LEAF(void, SharedRuntime::tsan_interp_unlock(JavaThread* thread,
1117                                                  BasicObjectLock* elem))
1118   DEBUG_ONLY(thread-&gt;last_frame().interpreter_frame_verify_monitor(elem);)
1119   assert(elem != NULL, &quot;null elem&quot;);
1120 
1121   oop obj = elem-&gt;obj();
</pre>
<hr />
<pre>
1179 TSAN_MEMORY_ACCESS(write1)
1180 TSAN_MEMORY_ACCESS(write2)
1181 TSAN_MEMORY_ACCESS(write4)
1182 TSAN_MEMORY_ACCESS(write8)
1183 
1184 #endif // INCLUDE_TSAN
1185 
1186 // Finds receiver, CallInfo (i.e. receiver method), and calling bytecode)
1187 // for a call current in progress, i.e., arguments has been pushed on stack
1188 // put callee has not been invoked yet.  Used by: resolve virtual/static,
1189 // vtable updates, etc.  Caller frame must be compiled.
1190 Handle SharedRuntime::find_callee_info(JavaThread* thread, Bytecodes::Code&amp; bc, CallInfo&amp; callinfo, TRAPS) {
1191   ResourceMark rm(THREAD);
1192 
1193   // last java frame on stack (which includes native call frames)
1194   vframeStream vfst(thread, true);  // Do not skip and javaCalls
1195 
1196   return find_callee_info_helper(thread, vfst, bc, callinfo, THREAD);
1197 }
1198 
<span class="line-modified">1199 Method* SharedRuntime::extract_attached_method(vframeStream&amp; vfst) {</span>
1200   CompiledMethod* caller = vfst.nm();
1201 
1202   nmethodLocker caller_lock(caller);
1203 
1204   address pc = vfst.frame_pc();
1205   { // Get call instruction under lock because another thread may be busy patching it.
1206     CompiledICLocker ic_locker(caller);
1207     return caller-&gt;attached_method_before_pc(pc);
1208   }
1209   return NULL;
1210 }
1211 
1212 // Finds receiver, CallInfo (i.e. receiver method), and calling bytecode
1213 // for a call current in progress, i.e., arguments has been pushed on stack
1214 // but callee has not been invoked yet.  Caller frame must be compiled.
1215 Handle SharedRuntime::find_callee_info_helper(JavaThread* thread,
1216                                               vframeStream&amp; vfst,
1217                                               Bytecodes::Code&amp; bc,
1218                                               CallInfo&amp; callinfo, TRAPS) {
1219   Handle receiver;
1220   Handle nullHandle;  //create a handy null handle for exception returns
1221 
1222   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
1223 
1224   // Find caller and bci from vframe
1225   methodHandle caller(THREAD, vfst.method());
1226   int          bci   = vfst.bci();
1227 
1228   Bytecode_invoke bytecode(caller, bci);
1229   int bytecode_index = bytecode.index();
1230   bc = bytecode.invoke_code();
1231 
<span class="line-modified">1232   methodHandle attached_method(THREAD, extract_attached_method(vfst));</span>
1233   if (attached_method.not_null()) {
<span class="line-modified">1234     Method* callee = bytecode.static_target(CHECK_NH);</span>
1235     vmIntrinsics::ID id = callee-&gt;intrinsic_id();
1236     // When VM replaces MH.invokeBasic/linkTo* call with a direct/virtual call,
1237     // it attaches statically resolved method to the call site.
1238     if (MethodHandles::is_signature_polymorphic(id) &amp;&amp;
1239         MethodHandles::is_signature_polymorphic_intrinsic(id)) {
1240       bc = MethodHandles::signature_polymorphic_intrinsic_bytecode(id);
1241 
1242       // Adjust invocation mode according to the attached method.
1243       switch (bc) {
1244         case Bytecodes::_invokevirtual:
1245           if (attached_method-&gt;method_holder()-&gt;is_interface()) {
1246             bc = Bytecodes::_invokeinterface;
1247           }
1248           break;
1249         case Bytecodes::_invokeinterface:
1250           if (!attached_method-&gt;method_holder()-&gt;is_interface()) {
1251             bc = Bytecodes::_invokevirtual;
1252           }
1253           break;
1254         case Bytecodes::_invokehandle:
</pre>
<hr />
<pre>
1262       }
1263     }
1264   }
1265 
1266   assert(bc != Bytecodes::_illegal, &quot;not initialized&quot;);
1267 
1268   bool has_receiver = bc != Bytecodes::_invokestatic &amp;&amp;
1269                       bc != Bytecodes::_invokedynamic &amp;&amp;
1270                       bc != Bytecodes::_invokehandle;
1271 
1272   // Find receiver for non-static call
1273   if (has_receiver) {
1274     // This register map must be update since we need to find the receiver for
1275     // compiled frames. The receiver might be in a register.
1276     RegisterMap reg_map2(thread);
1277     frame stubFrame   = thread-&gt;last_frame();
1278     // Caller-frame is a compiled frame
1279     frame callerFrame = stubFrame.sender(&amp;reg_map2);
1280 
1281     if (attached_method.is_null()) {
<span class="line-modified">1282       Method* callee = bytecode.static_target(CHECK_NH);</span>
<span class="line-modified">1283       if (callee == NULL) {</span>
1284         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1285       }
1286     }
1287 
1288     // Retrieve from a compiled argument list
1289     receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));
1290 
1291     if (receiver.is_null()) {
1292       THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
1293     }
1294   }
1295 
1296   // Resolve method
1297   if (attached_method.not_null()) {
1298     // Parameterized by attached method.
1299     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);
1300   } else {
1301     // Parameterized by bytecode.
1302     constantPoolHandle constants(THREAD, caller-&gt;constants());
1303     LinkResolver::resolve_invoke(callinfo, receiver, constants, bytecode_index, bc, CHECK_NH);
1304   }
1305 
1306 #ifdef ASSERT
1307   // Check that the receiver klass is of the right subtype and that it is initialized for virtual calls
1308   if (has_receiver) {
1309     assert(receiver.not_null(), &quot;should have thrown exception&quot;);
1310     Klass* receiver_klass = receiver-&gt;klass();
1311     Klass* rk = NULL;
1312     if (attached_method.not_null()) {
1313       // In case there&#39;s resolved method attached, use its holder during the check.
1314       rk = attached_method-&gt;method_holder();
1315     } else {
1316       // Klass is already loaded.
1317       constantPoolHandle constants(THREAD, caller-&gt;constants());
1318       rk = constants-&gt;klass_ref_at(bytecode_index, CHECK_NH);
1319     }
1320     Klass* static_receiver_klass = rk;

1321     assert(receiver_klass-&gt;is_subtype_of(static_receiver_klass),
1322            &quot;actual receiver must be subclass of static receiver klass&quot;);
1323     if (receiver_klass-&gt;is_instance_klass()) {
1324       if (InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized()) {
1325         tty-&gt;print_cr(&quot;ERROR: Klass not yet initialized!!&quot;);
1326         receiver_klass-&gt;print();
1327       }
1328       assert(!InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized(), &quot;receiver_klass must be initialized&quot;);
1329     }
1330   }
1331 #endif
1332 
1333   return receiver;
1334 }
1335 
1336 methodHandle SharedRuntime::find_callee_method(JavaThread* thread, TRAPS) {
1337   ResourceMark rm(THREAD);
1338   // We need first to check if any Java activations (compiled, interpreted)
1339   // exist on the stack since last JavaCall.  If not, we need
1340   // to get the target method from the JavaCall wrapper.
1341   vframeStream vfst(thread, true);  // Do not skip any javaCalls
1342   methodHandle callee_method;
1343   if (vfst.at_end()) {
1344     // No Java frames were found on stack since we did the JavaCall.
1345     // Hence the stack can only contain an entry_frame.  We need to
1346     // find the target method from the stub frame.
1347     RegisterMap reg_map(thread, false);
1348     frame fr = thread-&gt;last_frame();
1349     assert(fr.is_runtime_frame(), &quot;must be a runtimeStub&quot;);
1350     fr = fr.sender(&amp;reg_map);
1351     assert(fr.is_entry_frame(), &quot;must be&quot;);
1352     // fr is now pointing to the entry frame.
1353     callee_method = methodHandle(THREAD, fr.entry_frame_call_wrapper()-&gt;callee_method());
1354   } else {
1355     Bytecodes::Code bc;
1356     CallInfo callinfo;
1357     find_callee_info_helper(thread, vfst, bc, callinfo, CHECK_(methodHandle()));
<span class="line-modified">1358     callee_method = methodHandle(THREAD, callinfo.selected_method());</span>
1359   }
1360   assert(callee_method()-&gt;is_method(), &quot;must be&quot;);
1361   return callee_method;
1362 }
1363 
1364 // Resolves a call.
1365 methodHandle SharedRuntime::resolve_helper(JavaThread *thread,
1366                                            bool is_virtual,
1367                                            bool is_optimized, TRAPS) {
1368   methodHandle callee_method;
1369   callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);
1370   if (JvmtiExport::can_hotswap_or_post_breakpoint()) {
1371     int retry_count = 0;
1372     while (!HAS_PENDING_EXCEPTION &amp;&amp; callee_method-&gt;is_old() &amp;&amp;
1373            callee_method-&gt;method_holder() != SystemDictionary::Object_klass()) {
1374       // If has a pending exception then there is no need to re-try to
1375       // resolve this method.
1376       // If the method has been redefined, we need to try again.
1377       // Hack: we have no way to update the vtables of arrays, so don&#39;t
1378       // require that java.lang.Object has been updated.
</pre>
<hr />
<pre>
1424                      CHECK_false);
1425   } else {
1426     // static call
1427     CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);
1428   }
1429 
1430   // grab lock, check for deoptimization and potentially patch caller
1431   {
1432     CompiledICLocker ml(caller_nm);
1433 
1434     // Lock blocks for safepoint during which both nmethods can change state.
1435 
1436     // Now that we are ready to patch if the Method* was redefined then
1437     // don&#39;t update call site and let the caller retry.
1438     // Don&#39;t update call site if callee nmethod was unloaded or deoptimized.
1439     // Don&#39;t update call site if callee nmethod was replaced by an other nmethod
1440     // which may happen when multiply alive nmethod (tiered compilation)
1441     // will be supported.
1442     if (!callee_method-&gt;is_old() &amp;&amp;
1443         (callee == NULL || (callee-&gt;is_in_use() &amp;&amp; callee_method-&gt;code() == callee))) {
<span class="line-added">1444       NoSafepointVerifier nsv;</span>
1445 #ifdef ASSERT
1446       // We must not try to patch to jump to an already unloaded method.
1447       if (dest_entry_point != 0) {
1448         CodeBlob* cb = CodeCache::find_blob(dest_entry_point);
1449         assert((cb != NULL) &amp;&amp; cb-&gt;is_compiled() &amp;&amp; (((CompiledMethod*)cb) == callee),
1450                &quot;should not call unloaded nmethod&quot;);
1451       }
1452 #endif
1453       if (is_virtual) {
1454         CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
1455         if (inline_cache-&gt;is_clean()) {
1456           if (!inline_cache-&gt;set_to_monomorphic(virtual_call_info)) {
1457             return false;
1458           }
1459         }
1460       } else {
<span class="line-added">1461         if (VM_Version::supports_fast_class_init_checks() &amp;&amp;</span>
<span class="line-added">1462             invoke_code == Bytecodes::_invokestatic &amp;&amp;</span>
<span class="line-added">1463             callee_method-&gt;needs_clinit_barrier() &amp;&amp;</span>
<span class="line-added">1464             callee != NULL &amp;&amp; (callee-&gt;is_compiled_by_jvmci() || callee-&gt;is_aot())) {</span>
<span class="line-added">1465           return true; // skip patching for JVMCI or AOT code</span>
<span class="line-added">1466         }</span>
1467         CompiledStaticCall* ssc = caller_nm-&gt;compiledStaticCall_before(caller_frame.pc());
1468         if (ssc-&gt;is_clean()) ssc-&gt;set(static_call_info);
1469       }
1470     }
1471   } // unlock CompiledICLocker
1472   return true;
1473 }
1474 
1475 // Resolves a call.  The compilers generate code for calls that go here
1476 // and are patched with the real destination of the call.
1477 methodHandle SharedRuntime::resolve_sub_helper(JavaThread *thread,
1478                                                bool is_virtual,
1479                                                bool is_optimized, TRAPS) {
1480 
1481   ResourceMark rm(thread);
1482   RegisterMap cbl_map(thread, false);
1483   frame caller_frame = thread-&gt;last_frame().sender(&amp;cbl_map);
1484 
1485   CodeBlob* caller_cb = caller_frame.cb();
1486   guarantee(caller_cb != NULL &amp;&amp; caller_cb-&gt;is_compiled(), &quot;must be called from compiled method&quot;);
1487   CompiledMethod* caller_nm = caller_cb-&gt;as_compiled_method_or_null();
1488 
1489   // make sure caller is not getting deoptimized
1490   // and removed before we are done with it.
1491   // CLEANUP - with lazy deopt shouldn&#39;t need this lock
1492   nmethodLocker caller_lock(caller_nm);
1493 
1494   // determine call info &amp; receiver
1495   // note: a) receiver is NULL for static calls
1496   //       b) an exception is thrown if receiver is NULL for non-static calls
1497   CallInfo call_info;
1498   Bytecodes::Code invoke_code = Bytecodes::_illegal;
1499   Handle receiver = find_callee_info(thread, invoke_code,
1500                                      call_info, CHECK_(methodHandle()));
<span class="line-modified">1501   methodHandle callee_method(THREAD, call_info.selected_method());</span>
1502 
1503   assert((!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokestatic ) ||
1504          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokespecial) ||
1505          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokehandle ) ||
1506          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokedynamic) ||
1507          ( is_virtual &amp;&amp; invoke_code != Bytecodes::_invokestatic ), &quot;inconsistent bytecode&quot;);
1508 
1509   assert(caller_nm-&gt;is_alive() &amp;&amp; !caller_nm-&gt;is_unloading(), &quot;It should be alive&quot;);
1510 
1511 #ifndef PRODUCT
1512   // tracing/debugging/statistics
1513   int *addr = (is_optimized) ? (&amp;_resolve_opt_virtual_ctr) :
1514                 (is_virtual) ? (&amp;_resolve_virtual_ctr) :
1515                                (&amp;_resolve_static_ctr);
1516   Atomic::inc(addr);
1517 
1518   if (TraceCallFixup) {
1519     ResourceMark rm(thread);
1520     tty-&gt;print(&quot;resolving %s%s (%s) call to&quot;,
1521       (is_optimized) ? &quot;optimized &quot; : &quot;&quot;, (is_virtual) ? &quot;virtual&quot; : &quot;static&quot;,
1522       Bytecodes::name(invoke_code));
1523     callee_method-&gt;print_short_name(tty);
1524     tty-&gt;print_cr(&quot; at pc: &quot; INTPTR_FORMAT &quot; to code: &quot; INTPTR_FORMAT,
1525                   p2i(caller_frame.pc()), p2i(callee_method-&gt;code()));
1526   }
1527 #endif
1528 
<span class="line-modified">1529   if (invoke_code == Bytecodes::_invokestatic) {</span>
<span class="line-modified">1530     assert(callee_method-&gt;method_holder()-&gt;is_initialized() ||</span>
<span class="line-modified">1531            callee_method-&gt;method_holder()-&gt;is_reentrant_initialization(thread),</span>
<span class="line-modified">1532            &quot;invalid class initialization state for invoke_static&quot;);</span>
<span class="line-added">1533     if (!VM_Version::supports_fast_class_init_checks() &amp;&amp; callee_method-&gt;needs_clinit_barrier()) {</span>
<span class="line-added">1534       // In order to keep class initialization check, do not patch call</span>
<span class="line-added">1535       // site for static call when the class is not fully initialized.</span>
<span class="line-added">1536       // Proper check is enforced by call site re-resolution on every invocation.</span>
<span class="line-added">1537       //</span>
<span class="line-added">1538       // When fast class initialization checks are supported (VM_Version::supports_fast_class_init_checks() == true),</span>
1539       // explicit class initialization check is put in nmethod entry (VEP).
1540       assert(callee_method-&gt;method_holder()-&gt;is_linked(), &quot;must be&quot;);
1541       return callee_method;




1542     }
1543   }
1544 
1545   // JSR 292 key invariant:
1546   // If the resolved method is a MethodHandle invoke target, the call
1547   // site must be a MethodHandle call site, because the lambda form might tail-call
1548   // leaving the stack in a state unknown to either caller or callee
1549   // TODO detune for now but we might need it again
1550 //  assert(!callee_method-&gt;is_compiled_lambda_form() ||
1551 //         caller_nm-&gt;is_method_handle_return(caller_frame.pc()), &quot;must be MH call site&quot;);
1552 
1553   // Compute entry points. This might require generation of C2I converter
1554   // frames, so we cannot be holding any locks here. Furthermore, the
1555   // computation of the entry points is independent of patching the call.  We
1556   // always return the entry-point, but we only patch the stub if the call has
1557   // not been deoptimized.  Return values: For a virtual call this is an
1558   // (cached_oop, destination address) pair. For a static call/optimized
1559   // virtual this is just a destination address.
1560 
1561   // Patching IC caches may fail if we run out if transition stubs.
</pre>
<hr />
<pre>
1602   // 6243940 We might end up in here if the callee is deoptimized
1603   // as we race to call it.  We don&#39;t want to take a safepoint if
1604   // the caller was interpreted because the caller frame will look
1605   // interpreted to the stack walkers and arguments are now
1606   // &quot;compiled&quot; so it is much better to make this transition
1607   // invisible to the stack walking code. The i2c path will
1608   // place the callee method in the callee_target. It is stashed
1609   // there because if we try and find the callee by normal means a
1610   // safepoint is possible and have trouble gc&#39;ing the compiled args.
1611   RegisterMap reg_map(thread, false);
1612   frame stub_frame = thread-&gt;last_frame();
1613   assert(stub_frame.is_runtime_frame(), &quot;sanity check&quot;);
1614   frame caller_frame = stub_frame.sender(&amp;reg_map);
1615 
1616   if (caller_frame.is_interpreted_frame() ||
1617       caller_frame.is_entry_frame()) {
1618     Method* callee = thread-&gt;callee_target();
1619     guarantee(callee != NULL &amp;&amp; callee-&gt;is_method(), &quot;bad handshake&quot;);
1620     thread-&gt;set_vm_result_2(callee);
1621     thread-&gt;set_callee_target(NULL);
<span class="line-modified">1622     if (caller_frame.is_entry_frame() &amp;&amp; VM_Version::supports_fast_class_init_checks()) {</span>
<span class="line-added">1623       // Bypass class initialization checks in c2i when caller is in native.</span>
<span class="line-added">1624       // JNI calls to static methods don&#39;t have class initialization checks.</span>
<span class="line-added">1625       // Fast class initialization checks are present in c2i adapters and call into</span>
<span class="line-added">1626       // SharedRuntime::handle_wrong_method() on the slow path.</span>
<span class="line-added">1627       //</span>
<span class="line-added">1628       // JVM upcalls may land here as well, but there&#39;s a proper check present in</span>
<span class="line-added">1629       // LinkResolver::resolve_static_call (called from JavaCalls::call_static),</span>
<span class="line-added">1630       // so bypassing it in c2i adapter is benign.</span>
<span class="line-added">1631       return callee-&gt;get_c2i_no_clinit_check_entry();</span>
<span class="line-added">1632     } else {</span>
<span class="line-added">1633       return callee-&gt;get_c2i_entry();</span>
<span class="line-added">1634     }</span>
1635   }
1636 
1637   // Must be compiled to compiled path which is safe to stackwalk
1638   methodHandle callee_method;
1639   JRT_BLOCK
1640     // Force resolving of caller (if we called from compiled frame)
1641     callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);
1642     thread-&gt;set_vm_result_2(callee_method());
1643   JRT_BLOCK_END
1644   // return compiled code entry point after potential safepoints
1645   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);
1646   return callee_method-&gt;verified_code_entry();
1647 JRT_END
1648 
1649 // Handle abstract method call
1650 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_abstract(JavaThread* thread))
1651   // Verbose error message for AbstractMethodError.
1652   // Get the called method from the invoke bytecode.
1653   vframeStream vfst(thread, true);
1654   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
<span class="line-modified">1655   methodHandle caller(thread, vfst.method());</span>
1656   Bytecode_invoke invoke(caller, vfst.bci());
1657   DEBUG_ONLY( invoke.verify(); )
1658 
1659   // Find the compiled caller frame.
1660   RegisterMap reg_map(thread);
1661   frame stubFrame = thread-&gt;last_frame();
1662   assert(stubFrame.is_runtime_frame(), &quot;must be&quot;);
1663   frame callerFrame = stubFrame.sender(&amp;reg_map);
1664   assert(callerFrame.is_compiled_frame(), &quot;must be&quot;);
1665 
1666   // Install exception and return forward entry.
1667   address res = StubRoutines::throw_AbstractMethodError_entry();
1668   JRT_BLOCK
<span class="line-modified">1669     methodHandle callee(thread, invoke.static_target(thread));</span>
1670     if (!callee.is_null()) {
1671       oop recv = callerFrame.retrieve_receiver(&amp;reg_map);
1672       Klass *recv_klass = (recv != NULL) ? recv-&gt;klass() : NULL;
1673       LinkResolver::throw_abstract_method_error(callee, recv_klass, thread);
1674       res = StubRoutines::forward_exception_entry();
1675     }
1676   JRT_BLOCK_END
1677   return res;
1678 JRT_END
1679 
1680 
1681 // resolve a static call and patch code
1682 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
1683   methodHandle callee_method;
1684   JRT_BLOCK
1685     callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);
1686     thread-&gt;set_vm_result_2(callee_method());
1687   JRT_BLOCK_END
1688   // return compiled code entry point after potential safepoints
1689   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);
</pre>
<hr />
<pre>
1813   // don&#39;t have vtable entries (vtable_index &lt; 0) and we&#39;d blow up. So we force a
1814   // reresolution of the  call site (as if we did a handle_wrong_method and not an
1815   // plain ic_miss) and the site will be converted to an optimized virtual call site
1816   // never to miss again. I don&#39;t believe C2 will produce code like this but if it
1817   // did this would still be the correct thing to do for it too, hence no ifdef.
1818   //
1819   if (call_info.resolved_method()-&gt;can_be_statically_bound()) {
1820     methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));
1821     if (TraceCallFixup) {
1822       RegisterMap reg_map(thread, false);
1823       frame caller_frame = thread-&gt;last_frame().sender(&amp;reg_map);
1824       ResourceMark rm(thread);
1825       tty-&gt;print(&quot;converting IC miss to reresolve (%s) call to&quot;, Bytecodes::name(bc));
1826       callee_method-&gt;print_short_name(tty);
1827       tty-&gt;print_cr(&quot; from pc: &quot; INTPTR_FORMAT, p2i(caller_frame.pc()));
1828       tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1829     }
1830     return callee_method;
1831   }
1832 
<span class="line-modified">1833   methodHandle callee_method(thread, call_info.selected_method());</span>
1834 
1835 #ifndef PRODUCT
1836   Atomic::inc(&amp;_ic_miss_ctr);
1837 
1838   // Statistics &amp; Tracing
1839   if (TraceCallFixup) {
1840     ResourceMark rm(thread);
1841     tty-&gt;print(&quot;IC miss (%s) call to&quot;, Bytecodes::name(bc));
1842     callee_method-&gt;print_short_name(tty);
1843     tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1844   }
1845 
1846   if (ICMissHistogram) {
1847     MutexLocker m(VMStatistic_lock);
1848     RegisterMap reg_map(thread, false);
1849     frame f = thread-&gt;last_frame().real_sender(&amp;reg_map);// skip runtime stub
1850     // produce statistics under the lock
1851     trace_ic_miss(f.pc());
1852   }
1853 #endif
</pre>
<hr />
<pre>
2065       }
2066       // assert is too strong could also be resolve destinations.
2067       // assert(InlineCacheBuffer::contains(destination) || VtableStubs::contains(destination), &quot;must be&quot;);
2068     }
2069   } else {
2070     if (TraceCallFixup) {
2071       tty-&gt;print(&quot;already patched callsite at &quot; INTPTR_FORMAT &quot; to compiled code for&quot;, p2i(caller_pc));
2072       moop-&gt;print_short_name(tty);
2073       tty-&gt;print_cr(&quot; to &quot; INTPTR_FORMAT, p2i(entry_point));
2074     }
2075   }
2076   return false;
2077 }
2078 
2079 // ---------------------------------------------------------------------------
2080 // We are calling the interpreter via a c2i. Normally this would mean that
2081 // we were called by a compiled method. However we could have lost a race
2082 // where we went int -&gt; i2c -&gt; c2i and so the caller could in fact be
2083 // interpreted. If the caller is compiled we attempt to patch the caller
2084 // so he no longer calls into the interpreter.
<span class="line-modified">2085 JRT_LEAF(void, SharedRuntime::fixup_callers_callsite(Method* method, address caller_pc))</span>
2086   Method* moop(method);
2087 
2088   address entry_point = moop-&gt;from_compiled_entry_no_trampoline();
2089 
2090   // It&#39;s possible that deoptimization can occur at a call site which hasn&#39;t
2091   // been resolved yet, in which case this function will be called from
2092   // an nmethod that has been patched for deopt and we can ignore the
2093   // request for a fixup.
2094   // Also it is possible that we lost a race in that from_compiled_entry
2095   // is now back to the i2c in that case we don&#39;t need to patch and if
2096   // we did we&#39;d leap into space because the callsite needs to use
2097   // &quot;to interpreter&quot; stub in order to load up the Method*. Don&#39;t
2098   // ask me how I know this...
2099 
2100   CodeBlob* cb = CodeCache::find_blob(caller_pc);
2101   if (cb == NULL || !cb-&gt;is_compiled() || entry_point == moop-&gt;get_c2i_entry()) {
2102     return;
2103   }
2104 
2105   // The check above makes sure this is a nmethod.
</pre>
<hr />
<pre>
2134       // there. If you&#39;re lucky you&#39;ll get the assert in the bugid, if not you&#39;ve
2135       // just made a call site that could be megamorphic into a monomorphic site
2136       // for the rest of its life! Just another racing bug in the life of
2137       // fixup_callers_callsite ...
2138       //
2139       RelocIterator iter(nm, call-&gt;instruction_address(), call-&gt;next_instruction_address());
2140       iter.next();
2141       assert(iter.has_current(), &quot;must have a reloc at java call site&quot;);
2142       relocInfo::relocType typ = iter.reloc()-&gt;type();
2143       if (typ != relocInfo::static_call_type &amp;&amp;
2144            typ != relocInfo::opt_virtual_call_type &amp;&amp;
2145            typ != relocInfo::static_stub_type) {
2146         return;
2147       }
2148       address destination = call-&gt;destination();
2149       if (should_fixup_call_destination(destination, entry_point, caller_pc, moop, cb)) {
2150         call-&gt;set_destination_mt_safe(entry_point);
2151       }
2152     }
2153   }
<span class="line-modified">2154 JRT_END</span>
2155 
2156 
2157 // same as JVM_Arraycopy, but called directly from compiled code
2158 JRT_ENTRY(void, SharedRuntime::slow_arraycopy_C(oopDesc* src,  jint src_pos,
2159                                                 oopDesc* dest, jint dest_pos,
2160                                                 jint length,
2161                                                 JavaThread* thread)) {
2162 #ifndef PRODUCT
2163   _slow_array_copy_ctr++;
2164 #endif
2165   // Check if we have null pointers
2166   if (src == NULL || dest == NULL) {
2167     THROW(vmSymbols::java_lang_NullPointerException());
2168   }
2169   // Do the copy.  The casts to arrayOop are necessary to the copy_array API,
2170   // even though the copy_array API also performs dynamic checks to ensure
2171   // that src and dest are truly arrays (and are conformable).
2172   // The copy_array mechanism is awkward and could be removed, but
2173   // the compilers don&#39;t call this function except as a last resort,
2174   // so it probably doesn&#39;t matter.
</pre>
<hr />
<pre>
2188   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
2189   Bytecode_checkcast cc(vfst.method(), vfst.method()-&gt;bcp_from(vfst.bci()));
2190   constantPoolHandle cpool(thread, vfst.method()-&gt;constants());
2191   Klass* target_klass = ConstantPool::klass_at_if_loaded(cpool, cc.index());
2192   Symbol* target_klass_name = NULL;
2193   if (target_klass == NULL) {
2194     // This klass should be resolved, but just in case, get the name in the klass slot.
2195     target_klass_name = cpool-&gt;klass_name_at(cc.index());
2196   }
2197   return generate_class_cast_message(caster_klass, target_klass, target_klass_name);
2198 }
2199 
2200 
2201 // The caller of generate_class_cast_message() (or one of its callers)
2202 // must use a ResourceMark in order to correctly free the result.
2203 char* SharedRuntime::generate_class_cast_message(
2204     Klass* caster_klass, Klass* target_klass, Symbol* target_klass_name) {
2205   const char* caster_name = caster_klass-&gt;external_name();
2206 
2207   assert(target_klass != NULL || target_klass_name != NULL, &quot;one must be provided&quot;);
<span class="line-modified">2208   const char* target_name = target_klass == NULL ? target_klass_name-&gt;as_klass_external_name() :</span>
2209                                                    target_klass-&gt;external_name();
2210 
2211   size_t msglen = strlen(caster_name) + strlen(&quot;class &quot;) + strlen(&quot; cannot be cast to class &quot;) + strlen(target_name) + 1;
2212 
2213   const char* caster_klass_description = &quot;&quot;;
2214   const char* target_klass_description = &quot;&quot;;
2215   const char* klass_separator = &quot;&quot;;
2216   if (target_klass != NULL &amp;&amp; caster_klass-&gt;module() == target_klass-&gt;module()) {
2217     caster_klass_description = caster_klass-&gt;joint_in_module_of_loader(target_klass);
2218   } else {
2219     caster_klass_description = caster_klass-&gt;class_in_module_of_loader();
2220     target_klass_description = (target_klass != NULL) ? target_klass-&gt;class_in_module_of_loader() : &quot;&quot;;
2221     klass_separator = (target_klass != NULL) ? &quot;; &quot; : &quot;&quot;;
2222   }
2223 
2224   // add 3 for parenthesis and preceeding space
2225   msglen += strlen(caster_klass_description) + strlen(target_klass_description) + strlen(klass_separator) + 3;
2226 
2227   char* message = NEW_RESOURCE_ARRAY_RETURN_NULL(char, msglen);
2228   if (message == NULL) {
</pre>
<hr />
<pre>
2247 JRT_END
2248 
2249 
2250 // Handles the uncommon case in locking, i.e., contention or an inflated lock.
2251 JRT_BLOCK_ENTRY(void, SharedRuntime::complete_monitor_locking_C(oopDesc* _obj, BasicLock* lock, JavaThread* thread))
2252   if (!SafepointSynchronize::is_synchronizing()) {
2253     // Only try quick_enter() if we&#39;re not trying to reach a safepoint
2254     // so that the calling thread reaches the safepoint more quickly.
2255     if (ObjectSynchronizer::quick_enter(_obj, thread, lock)) return;
2256   }
2257   // NO_ASYNC required because an async exception on the state transition destructor
2258   // would leave you with the lock held and it would never be released.
2259   // The normal monitorenter NullPointerException is thrown without acquiring a lock
2260   // and the model is that an exception implies the method failed.
2261   JRT_BLOCK_NO_ASYNC
2262   oop obj(_obj);
2263   if (PrintBiasedLockingStatistics) {
2264     Atomic::inc(BiasedLocking::slow_path_entry_count_addr());
2265   }
2266   Handle h_obj(THREAD, obj);
<span class="line-modified">2267   ObjectSynchronizer::enter(h_obj, lock, CHECK);</span>





2268   assert(!HAS_PENDING_EXCEPTION, &quot;Should have no exception here&quot;);
2269   JRT_BLOCK_END
2270 JRT_END
2271 
2272 // Handles the uncommon cases of monitor unlocking in compiled code
2273 JRT_LEAF(void, SharedRuntime::complete_monitor_unlocking_C(oopDesc* _obj, BasicLock* lock, JavaThread * THREAD))
2274    oop obj(_obj);
2275   assert(JavaThread::current() == THREAD, &quot;invariant&quot;);
2276   // I&#39;m not convinced we need the code contained by MIGHT_HAVE_PENDING anymore
2277   // testing was unable to ever fire the assert that guarded it so I have removed it.
2278   assert(!HAS_PENDING_EXCEPTION, &quot;Do we need code below anymore?&quot;);
2279 #undef MIGHT_HAVE_PENDING
2280 #ifdef MIGHT_HAVE_PENDING
2281   // Save and restore any pending_exception around the exception mark.
2282   // While the slow_exit must not throw an exception, we could come into
2283   // this routine with one set.
2284   oop pending_excep = NULL;
2285   const char* pending_file;
2286   int pending_line;
2287   if (HAS_PENDING_EXCEPTION) {
2288     pending_excep = PENDING_EXCEPTION;
2289     pending_file  = THREAD-&gt;exception_file();
2290     pending_line  = THREAD-&gt;exception_line();
2291     CLEAR_PENDING_EXCEPTION;
2292   }
2293 #endif /* MIGHT_HAVE_PENDING */
2294 
2295   {
2296     // Exit must be non-blocking, and therefore no exceptions can be thrown.
2297     EXCEPTION_MARK;
<span class="line-modified">2298     ObjectSynchronizer::exit(obj, lock, THREAD);</span>
2299   }
2300 
2301 #ifdef MIGHT_HAVE_PENDING
2302   if (pending_excep != NULL) {
2303     THREAD-&gt;set_pending_exception(pending_excep, pending_file, pending_line);
2304   }
2305 #endif /* MIGHT_HAVE_PENDING */
2306 JRT_END
2307 
2308 #ifndef PRODUCT
2309 
2310 void SharedRuntime::print_statistics() {
2311   ttyLocker ttyl;
2312   if (xtty != NULL)  xtty-&gt;head(&quot;statistics type=&#39;SharedRuntime&#39;&quot;);
2313 
2314   if (_throw_null_ctr) tty-&gt;print_cr(&quot;%5d implicit null throw&quot;, _throw_null_ctr);
2315 
2316   SharedRuntime::print_ic_miss_histogram();
2317 
2318   if (CountRemovableExceptions) {
</pre>
<hr />
<pre>
2398     double rest = sum;
2399     double percent = sum / 100;
2400     for (i = 0; i &lt;= N; i++) {
2401       rest -= histo[i];
2402       tty-&gt;print_cr(&quot;%4d: %7d (%5.1f%%)&quot;, i, histo[i], histo[i] / percent);
2403     }
2404     tty-&gt;print_cr(&quot;rest: %7d (%5.1f%%))&quot;, (int)rest, rest / percent);
2405     tty-&gt;print_cr(&quot;(avg. %s = %3.1f, max = %d)&quot;, name, weighted_sum / sum, n);
2406   }
2407 
2408   void print_histogram() {
2409     tty-&gt;print_cr(&quot;\nHistogram of call arity (incl. rcvr, calls to compiled methods only):&quot;);
2410     print_histogram_helper(_max_arity, _arity_histogram, &quot;arity&quot;);
2411     tty-&gt;print_cr(&quot;\nSame for parameter size (in words):&quot;);
2412     print_histogram_helper(_max_size, _size_histogram, &quot;size&quot;);
2413     tty-&gt;cr();
2414   }
2415 
2416  public:
2417   MethodArityHistogram() {
<span class="line-modified">2418     MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
2419     _max_arity = _max_size = 0;
2420     for (int i = 0; i &lt; MAX_ARITY; i++) _arity_histogram[i] = _size_histogram[i] = 0;
2421     CodeCache::nmethods_do(add_method_to_histogram);
2422     print_histogram();
2423   }
2424 };
2425 
2426 int MethodArityHistogram::_arity_histogram[MethodArityHistogram::MAX_ARITY];
2427 int MethodArityHistogram::_size_histogram[MethodArityHistogram::MAX_ARITY];
2428 int MethodArityHistogram::_max_arity;
2429 int MethodArityHistogram::_max_size;
2430 
2431 void SharedRuntime::print_call_statistics(int comp_total) {
2432   tty-&gt;print_cr(&quot;Calls from compiled code:&quot;);
2433   int total  = _nof_normal_calls + _nof_interface_calls + _nof_static_calls;
2434   int mono_c = _nof_normal_calls - _nof_optimized_calls - _nof_megamorphic_calls;
2435   int mono_i = _nof_interface_calls - _nof_optimized_interface_calls - _nof_megamorphic_interface_calls;
2436   tty-&gt;print_cr(&quot;\t%9d   (%4.1f%%) total non-inlined   &quot;, total, percent(total, total));
2437   tty-&gt;print_cr(&quot;\t%9d   (%4.1f%%) virtual calls       &quot;, _nof_normal_calls, percent(_nof_normal_calls, total));
2438   tty-&gt;print_cr(&quot;\t  %9d  (%3.0f%%)   inlined          &quot;, _nof_inlined_calls, percent(_nof_inlined_calls, _nof_normal_calls));
</pre>
<hr />
<pre>
2613 
2614  private:
2615 
2616 #ifndef PRODUCT
2617   static int _lookups; // number of calls to lookup
2618   static int _buckets; // number of buckets checked
2619   static int _equals;  // number of buckets checked with matching hash
2620   static int _hits;    // number of successful lookups
2621   static int _compact; // number of equals calls with compact signature
2622 #endif
2623 
2624   AdapterHandlerEntry* bucket(int i) {
2625     return (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::bucket(i);
2626   }
2627 
2628  public:
2629   AdapterHandlerTable()
2630     : BasicHashtable&lt;mtCode&gt;(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
2631 
2632   // Create a new entry suitable for insertion in the table
<span class="line-modified">2633   AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {</span>
2634     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::new_entry(fingerprint-&gt;compute_hash());
<span class="line-modified">2635     entry-&gt;init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);</span>
2636     if (DumpSharedSpaces) {
2637       ((CDSAdapterHandlerEntry*)entry)-&gt;init();
2638     }
2639     return entry;
2640   }
2641 
2642   // Insert an entry into the table
2643   void add(AdapterHandlerEntry* entry) {
2644     int index = hash_to_index(entry-&gt;hash());
2645     add_entry(index, entry);
2646   }
2647 
2648   void free_entry(AdapterHandlerEntry* entry) {
2649     entry-&gt;deallocate();
2650     BasicHashtable&lt;mtCode&gt;::free_entry(entry);
2651   }
2652 
2653   // Find a entry with the same fingerprint if it exists
2654   AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {
2655     NOT_PRODUCT(_lookups++);
</pre>
<hr />
<pre>
2764 }
2765 
2766 void AdapterHandlerLibrary::initialize() {
2767   if (_adapters != NULL) return;
2768   _adapters = new AdapterHandlerTable();
2769 
2770   // Create a special handler for abstract methods.  Abstract methods
2771   // are never compiled so an i2c entry is somewhat meaningless, but
2772   // throw AbstractMethodError just in case.
2773   // Pass wrong_method_abstract for the c2i transitions to return
2774   // AbstractMethodError for invalid invocations.
2775   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
2776   _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),
2777                                                               StubRoutines::throw_AbstractMethodError_entry(),
2778                                                               wrong_method_abstract, wrong_method_abstract);
2779 }
2780 
2781 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
2782                                                       address i2c_entry,
2783                                                       address c2i_entry,
<span class="line-modified">2784                                                       address c2i_unverified_entry,</span>
<span class="line-modified">2785                                                       address c2i_no_clinit_check_entry) {</span>
<span class="line-added">2786   return _adapters-&gt;new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);</span>
2787 }
2788 
2789 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle&amp; method) {
2790   AdapterHandlerEntry* entry = get_adapter0(method);
<span class="line-modified">2791   if (entry != NULL &amp;&amp; method-&gt;is_shared()) {</span>
2792     // See comments around Method::link_method()
2793     MutexLocker mu(AdapterHandlerLibrary_lock);
2794     if (method-&gt;adapter() == NULL) {
2795       method-&gt;update_adapter_trampoline(entry);
2796     }
2797     address trampoline = method-&gt;from_compiled_entry();
2798     if (*(int*)trampoline == 0) {
2799       CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
2800       MacroAssembler _masm(&amp;buffer);
2801       SharedRuntime::generate_trampoline(&amp;_masm, entry-&gt;get_c2i_entry());
2802       assert(*(int*)trampoline != 0, &quot;Instruction(s) for trampoline must not be encoded as zeros.&quot;);
<span class="line-added">2803       _masm.flush();</span>
2804 
2805       if (PrintInterpreter) {
2806         Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
2807       }
2808     }
2809   }
2810 
2811   return entry;
2812 }
2813 
2814 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle&amp; method) {
2815   // Use customized signature handler.  Need to lock around updates to
2816   // the AdapterHandlerTable (it is not safe for concurrent readers
2817   // and a single writer: this could be fixed if it becomes a
2818   // problem).
2819 
2820   ResourceMark rm;
2821 
2822   NOT_PRODUCT(int insts_size);
2823   AdapterBlob* new_adapter = NULL;
</pre>
<hr />
<pre>
2943     jio_snprintf(blob_id,
2944                  sizeof(blob_id),
2945                  &quot;%s(%s)@&quot; PTR_FORMAT,
2946                  new_adapter-&gt;name(),
2947                  fingerprint-&gt;as_string(),
2948                  new_adapter-&gt;content_begin());
2949     Forte::register_stub(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
2950 
2951     if (JvmtiExport::should_post_dynamic_code_generated()) {
2952       JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
2953     }
2954   }
2955   return entry;
2956 }
2957 
2958 address AdapterHandlerEntry::base_address() {
2959   address base = _i2c_entry;
2960   if (base == NULL)  base = _c2i_entry;
2961   assert(base &lt;= _c2i_entry || _c2i_entry == NULL, &quot;&quot;);
2962   assert(base &lt;= _c2i_unverified_entry || _c2i_unverified_entry == NULL, &quot;&quot;);
<span class="line-added">2963   assert(base &lt;= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, &quot;&quot;);</span>
2964   return base;
2965 }
2966 
2967 void AdapterHandlerEntry::relocate(address new_base) {
2968   address old_base = base_address();
2969   assert(old_base != NULL, &quot;&quot;);
2970   ptrdiff_t delta = new_base - old_base;
2971   if (_i2c_entry != NULL)
2972     _i2c_entry += delta;
2973   if (_c2i_entry != NULL)
2974     _c2i_entry += delta;
2975   if (_c2i_unverified_entry != NULL)
2976     _c2i_unverified_entry += delta;
<span class="line-added">2977   if (_c2i_no_clinit_check_entry != NULL)</span>
<span class="line-added">2978     _c2i_no_clinit_check_entry += delta;</span>
2979   assert(base_address() == new_base, &quot;&quot;);
2980 }
2981 
2982 
2983 void AdapterHandlerEntry::deallocate() {
2984   delete _fingerprint;
2985 #ifdef ASSERT
<span class="line-modified">2986   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);</span>
2987 #endif
2988 }
2989 
2990 
2991 #ifdef ASSERT
2992 // Capture the code before relocation so that it can be compared
2993 // against other versions.  If the code is captured after relocation
2994 // then relative instructions won&#39;t be equivalent.
2995 void AdapterHandlerEntry::save_code(unsigned char* buffer, int length) {
2996   _saved_code = NEW_C_HEAP_ARRAY(unsigned char, length, mtCode);
2997   _saved_code_length = length;
2998   memcpy(_saved_code, buffer, length);
2999 }
3000 
3001 
3002 bool AdapterHandlerEntry::compare_code(unsigned char* buffer, int length) {
3003   if (length != _saved_code_length) {
3004     return false;
3005   }
3006 
3007   return (memcmp(buffer, _saved_code, length) == 0) ? true : false;
3008 }
3009 #endif
3010 
3011 
3012 /**
3013  * Create a native wrapper for this native method.  The wrapper converts the
3014  * Java-compiled calling convention to the native convention, handles
3015  * arguments, and transitions to native.  On return from the native we transition
3016  * back to java blocking if a safepoint is in progress.
3017  */
3018 void AdapterHandlerLibrary::create_native_wrapper(const methodHandle&amp; method) {
3019   ResourceMark rm;
3020   nmethod* nm = NULL;
<span class="line-added">3021   address critical_entry = NULL;</span>
3022 
3023   assert(method-&gt;is_native(), &quot;must be native&quot;);
3024   assert(method-&gt;is_method_handle_intrinsic() ||
3025          method-&gt;has_native_function(), &quot;must have something valid to call!&quot;);
3026 
<span class="line-added">3027   if (CriticalJNINatives &amp;&amp; !method-&gt;is_method_handle_intrinsic()) {</span>
<span class="line-added">3028     // We perform the I/O with transition to native before acquiring AdapterHandlerLibrary_lock.</span>
<span class="line-added">3029     critical_entry = NativeLookup::lookup_critical_entry(method);</span>
<span class="line-added">3030   }</span>
<span class="line-added">3031 </span>
3032   {
3033     // Perform the work while holding the lock, but perform any printing outside the lock
3034     MutexLocker mu(AdapterHandlerLibrary_lock);
3035     // See if somebody beat us to it
3036     if (method-&gt;code() != NULL) {
3037       return;
3038     }
3039 
3040     const int compile_id = CompileBroker::assign_compile_id(method, CompileBroker::standard_entry_bci);
3041     assert(compile_id &gt; 0, &quot;Must generate native wrapper&quot;);
3042 
3043 
3044     ResourceMark rm;
3045     BufferBlob*  buf = buffer_blob(); // the temporary code buffer in CodeCache
3046     if (buf != NULL) {
3047       CodeBuffer buffer(buf);
3048       double locs_buf[20];
3049       buffer.insts()-&gt;initialize_shared_locs((relocInfo*)locs_buf, sizeof(locs_buf) / sizeof(relocInfo));
3050       MacroAssembler _masm(&amp;buffer);
3051 
</pre>
<hr />
<pre>
3056       VMRegPair*   regs = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);
3057       int i=0;
3058       if (!method-&gt;is_static())  // Pass in receiver first
3059         sig_bt[i++] = T_OBJECT;
3060       SignatureStream ss(method-&gt;signature());
3061       for (; !ss.at_return_type(); ss.next()) {
3062         sig_bt[i++] = ss.type();  // Collect remaining bits of signature
3063         if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
3064           sig_bt[i++] = T_VOID;   // Longs &amp; doubles take 2 Java slots
3065       }
3066       assert(i == total_args_passed, &quot;&quot;);
3067       BasicType ret_type = ss.type();
3068 
3069       // Now get the compiled-Java layout as input (or output) arguments.
3070       // NOTE: Stubs for compiled entry points of method handle intrinsics
3071       // are just trampolines so the argument registers must be outgoing ones.
3072       const bool is_outgoing = method-&gt;is_method_handle_intrinsic();
3073       int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, is_outgoing);
3074 
3075       // Generate the compiled-to-native wrapper code
<span class="line-modified">3076       nm = SharedRuntime::generate_native_wrapper(&amp;_masm, method, compile_id, sig_bt, regs, ret_type, critical_entry);</span>
3077 
3078       if (nm != NULL) {
<span class="line-modified">3079         {</span>
<span class="line-added">3080           MutexLocker pl(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-added">3081           if (nm-&gt;make_in_use()) {</span>
<span class="line-added">3082             method-&gt;set_code(method, nm);</span>
<span class="line-added">3083           }</span>
<span class="line-added">3084         }</span>
3085 
3086         DirectiveSet* directive = DirectivesStack::getDefaultDirective(CompileBroker::compiler(CompLevel_simple));
3087         if (directive-&gt;PrintAssemblyOption) {
3088           nm-&gt;print_code();
3089         }
3090         DirectivesStack::release(directive);
3091       }
3092     }
3093   } // Unlock AdapterHandlerLibrary_lock
3094 
3095 
3096   // Install the generated code.
3097   if (nm != NULL) {
3098     const char *msg = method-&gt;is_static() ? &quot;(static)&quot; : &quot;&quot;;
3099     CompileTask::print_ul(nm, msg);
3100     if (PrintCompilation) {
3101       ttyLocker ttyl;
3102       CompileTask::print(tty, nm, msg);
3103     }
3104     nm-&gt;post_compiled_method_load_event();
</pre>
<hr />
<pre>
3136 JRT_END
3137 
3138 // -------------------------------------------------------------------------
3139 // Java-Java calling convention
3140 // (what you use when Java calls Java)
3141 
3142 //------------------------------name_for_receiver----------------------------------
3143 // For a given signature, return the VMReg for parameter 0.
3144 VMReg SharedRuntime::name_for_receiver() {
3145   VMRegPair regs;
3146   BasicType sig_bt = T_OBJECT;
3147   (void) java_calling_convention(&amp;sig_bt, &amp;regs, 1, true);
3148   // Return argument 0 register.  In the LP64 build pointers
3149   // take 2 registers, but the VM wants only the &#39;main&#39; name.
3150   return regs.first();
3151 }
3152 
3153 VMRegPair *SharedRuntime::find_callee_arguments(Symbol* sig, bool has_receiver, bool has_appendix, int* arg_size) {
3154   // This method is returning a data structure allocating as a
3155   // ResourceObject, so do not put any ResourceMarks in here.



3156 
3157   BasicType *sig_bt = NEW_RESOURCE_ARRAY(BasicType, 256);
3158   VMRegPair *regs = NEW_RESOURCE_ARRAY(VMRegPair, 256);
3159   int cnt = 0;
3160   if (has_receiver) {
3161     sig_bt[cnt++] = T_OBJECT; // Receiver is argument 0; not in signature
3162   }
3163 
<span class="line-modified">3164   for (SignatureStream ss(sig); !ss.at_return_type(); ss.next()) {</span>
<span class="line-modified">3165     BasicType type = ss.type();</span>
<span class="line-modified">3166     sig_bt[cnt++] = type;</span>
<span class="line-modified">3167     if (is_double_word_type(type))</span>
<span class="line-modified">3168       sig_bt[cnt++] = T_VOID;</span>






















3169   }
3170 
3171   if (has_appendix) {
3172     sig_bt[cnt++] = T_OBJECT;
3173   }
3174 
3175   assert(cnt &lt; 256, &quot;grow table size&quot;);
3176 
3177   int comp_args_on_stack;
3178   comp_args_on_stack = java_calling_convention(sig_bt, regs, cnt, true);
3179 
3180   // the calling convention doesn&#39;t count out_preserve_stack_slots so
3181   // we must add that in to get &quot;true&quot; stack offsets.
3182 
3183   if (comp_args_on_stack) {
3184     for (int i = 0; i &lt; cnt; i++) {
3185       VMReg reg1 = regs[i].first();
3186       if (reg1-&gt;is_stack()) {
3187         // Yuck
3188         reg1 = reg1-&gt;bias(out_preserve_stack_slots());
</pre>
<hr />
<pre>
3244   int max_locals = moop-&gt;max_locals();
3245   // Allocate temp buffer, 1 word per local &amp; 2 per active monitor
3246   int buf_size_words = max_locals + active_monitor_count * BasicObjectLock::size();
3247   intptr_t *buf = NEW_C_HEAP_ARRAY(intptr_t,buf_size_words, mtCode);
3248 
3249   // Copy the locals.  Order is preserved so that loading of longs works.
3250   // Since there&#39;s no GC I can copy the oops blindly.
3251   assert(sizeof(HeapWord)==sizeof(intptr_t), &quot;fix this code&quot;);
3252   Copy::disjoint_words((HeapWord*)fr.interpreter_frame_local_at(max_locals-1),
3253                        (HeapWord*)&amp;buf[0],
3254                        max_locals);
3255 
3256   // Inflate locks.  Copy the displaced headers.  Be careful, there can be holes.
3257   int i = max_locals;
3258   for (BasicObjectLock *kptr2 = fr.interpreter_frame_monitor_end();
3259        kptr2 &lt; fr.interpreter_frame_monitor_begin();
3260        kptr2 = fr.next_monitor_in_interpreter_frame(kptr2) ) {
3261     if (kptr2-&gt;obj() != NULL) {         // Avoid &#39;holes&#39; in the monitor array
3262       BasicLock *lock = kptr2-&gt;lock();
3263       // Inflate so the displaced header becomes position-independent
<span class="line-modified">3264       if (lock-&gt;displaced_header().is_unlocked())</span>
3265         ObjectSynchronizer::inflate_helper(kptr2-&gt;obj());
3266       // Now the displaced header is free to move
<span class="line-modified">3267       buf[i++] = (intptr_t)lock-&gt;displaced_header().value();</span>
3268       buf[i++] = cast_from_oop&lt;intptr_t&gt;(kptr2-&gt;obj());
3269     }
3270   }
3271   assert(i - max_locals == active_monitor_count*2, &quot;found the expected number of monitors&quot;);
3272 
3273   return buf;
3274 JRT_END
3275 
3276 JRT_LEAF(void, SharedRuntime::OSR_migration_end( intptr_t* buf) )
3277   FREE_C_HEAP_ARRAY(intptr_t, buf);
3278 JRT_END
3279 
3280 bool AdapterHandlerLibrary::contains(const CodeBlob* b) {
3281   AdapterHandlerTableIterator iter(_adapters);
3282   while (iter.has_next()) {
3283     AdapterHandlerEntry* a = iter.next();
3284     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) return true;
3285   }
3286   return false;
3287 }
3288 
3289 void AdapterHandlerLibrary::print_handler_on(outputStream* st, const CodeBlob* b) {
3290   AdapterHandlerTableIterator iter(_adapters);
3291   while (iter.has_next()) {
3292     AdapterHandlerEntry* a = iter.next();
3293     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) {
3294       st-&gt;print(&quot;Adapter for signature: &quot;);
3295       a-&gt;print_adapter_on(tty);
3296       return;
3297     }
3298   }
3299   assert(false, &quot;Should have found handler&quot;);
3300 }
3301 
3302 void AdapterHandlerEntry::print_adapter_on(outputStream* st) const {
<span class="line-modified">3303   st-&gt;print(&quot;AHE@&quot; INTPTR_FORMAT &quot;: %s&quot;, p2i(this), fingerprint()-&gt;as_string());</span>
<span class="line-modified">3304   if (get_i2c_entry() != NULL) {</span>
<span class="line-modified">3305     st-&gt;print(&quot; i2c: &quot; INTPTR_FORMAT, p2i(get_i2c_entry()));</span>
<span class="line-modified">3306   }</span>
<span class="line-added">3307   if (get_c2i_entry() != NULL) {</span>
<span class="line-added">3308     st-&gt;print(&quot; c2i: &quot; INTPTR_FORMAT, p2i(get_c2i_entry()));</span>
<span class="line-added">3309   }</span>
<span class="line-added">3310   if (get_c2i_unverified_entry() != NULL) {</span>
<span class="line-added">3311     st-&gt;print(&quot; c2iUV: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));</span>
<span class="line-added">3312   }</span>
<span class="line-added">3313   if (get_c2i_no_clinit_check_entry() != NULL) {</span>
<span class="line-added">3314     st-&gt;print(&quot; c2iNCI: &quot; INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));</span>
<span class="line-added">3315   }</span>
<span class="line-added">3316   st-&gt;cr();</span>
3317 }
3318 
3319 #if INCLUDE_CDS
3320 
3321 void CDSAdapterHandlerEntry::init() {
3322   assert(DumpSharedSpaces, &quot;used during dump time only&quot;);
3323   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
3324   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
3325 };
3326 
3327 #endif // INCLUDE_CDS
3328 
3329 
3330 #ifndef PRODUCT
3331 
3332 void AdapterHandlerLibrary::print_statistics() {
3333   _adapters-&gt;print_statistics();
3334 }
3335 
3336 #endif /* PRODUCT */
</pre>
</td>
</tr>
</table>
<center><a href="mutexLocker.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>