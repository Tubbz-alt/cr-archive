<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/templateTable_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_x86_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../../os_cpu/linux_x86/os_linux_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/templateTable_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
2828 // (3) Similar a volatile write cannot let unrelated NON-volatile
2829 //     memory refs that happen BEFORE the write float down to after the
2830 //     write.  It&#39;s OK for non-volatile memory refs that happen after the
2831 //     volatile write to float up before it.
2832 //
2833 // We only put in barriers around volatile refs (they are expensive),
2834 // not _between_ memory refs (that would require us to track the
2835 // flavor of the previous memory refs).  Requirements (2) and (3)
2836 // require some barriers before volatile stores and after volatile
2837 // loads.  These nearly cover requirement (1) but miss the
2838 // volatile-store-volatile-load case.  This final case is placed after
2839 // volatile-stores although it could just as well go before
2840 // volatile-loads.
2841 
2842 void TemplateTable::volatile_barrier(Assembler::Membar_mask_bits order_constraint ) {
2843   // Helper function to insert a is-volatile test and memory barrier
2844   __ membar(order_constraint);
2845 }
2846 
2847 void TemplateTable::resolve_cache_and_index(int byte_no,
<span class="line-modified">2848                                             Register Rcache,</span>
2849                                             Register index,
2850                                             size_t index_size) {
2851   const Register temp = rbx;
<span class="line-modified">2852   assert_different_registers(Rcache, index, temp);</span>
2853 

2854   Label resolved;
2855 
2856   Bytecodes::Code code = bytecode();
2857   switch (code) {
2858   case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2859   case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2860   default: break;
2861   }
2862 
2863   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
<span class="line-modified">2864   __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);</span>
2865   __ cmpl(temp, code);  // have we resolved this bytecode?
2866   __ jcc(Assembler::equal, resolved);
2867 
2868   // resolve first time through


2869   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2870   __ movl(temp, code);
2871   __ call_VM(noreg, entry, temp);
2872   // Update registers with resolved info
<span class="line-modified">2873   __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);</span>

2874   __ bind(resolved);












2875 }
2876 
2877 // The cache and index registers must be set before call
2878 void TemplateTable::load_field_cp_cache_entry(Register obj,
2879                                               Register cache,
2880                                               Register index,
2881                                               Register off,
2882                                               Register flags,
2883                                               bool is_static = false) {
2884   assert_different_registers(cache, index, flags, off);
2885 
2886   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2887   // Field offset
2888   __ movptr(off, Address(cache, index, Address::times_ptr,
2889                          in_bytes(cp_base_offset +
2890                                   ConstantPoolCacheEntry::f2_offset())));
2891   // Flags
2892   __ movl(flags, Address(cache, index, Address::times_ptr,
2893                          in_bytes(cp_base_offset +
2894                                   ConstantPoolCacheEntry::flags_offset())));
</pre>
<hr />
<pre>
2923     );
2924   }
2925 }
2926 
2927 void TemplateTable::load_invoke_cp_cache_entry(int byte_no,
2928                                                Register method,
2929                                                Register itable_index,
2930                                                Register flags,
2931                                                bool is_invokevirtual,
2932                                                bool is_invokevfinal, /*unused*/
2933                                                bool is_invokedynamic) {
2934   // setup registers
2935   const Register cache = rcx;
2936   const Register index = rdx;
2937   assert_different_registers(method, flags);
2938   assert_different_registers(method, cache, index);
2939   assert_different_registers(itable_index, flags);
2940   assert_different_registers(itable_index, cache, index);
2941   // determine constant pool cache field offsets
2942   assert(is_invokevirtual == (byte_no == f2_byte), &quot;is_invokevirtual flag redundant&quot;);
<span class="line-removed">2943   const int method_offset = in_bytes(</span>
<span class="line-removed">2944     ConstantPoolCache::base_offset() +</span>
<span class="line-removed">2945       ((byte_no == f2_byte)</span>
<span class="line-removed">2946        ? ConstantPoolCacheEntry::f2_offset()</span>
<span class="line-removed">2947        : ConstantPoolCacheEntry::f1_offset()));</span>
2948   const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +
2949                                     ConstantPoolCacheEntry::flags_offset());
2950   // access constant pool cache fields
2951   const int index_offset = in_bytes(ConstantPoolCache::base_offset() +
2952                                     ConstantPoolCacheEntry::f2_offset());
2953 
2954   size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));
2955   resolve_cache_and_index(byte_no, cache, index, index_size);
<span class="line-modified">2956     __ movptr(method, Address(cache, index, Address::times_ptr, method_offset));</span>
2957 
2958   if (itable_index != noreg) {
2959     // pick up itable or appendix index from f2 also:
2960     __ movptr(itable_index, Address(cache, index, Address::times_ptr, index_offset));
2961   }
2962   __ movl(flags, Address(cache, index, Address::times_ptr, flags_offset));
2963 }
2964 
2965 // The registers cache and index expected to be set before call.
2966 // Correct values of the cache and index registers are preserved.
2967 void TemplateTable::jvmti_post_field_access(Register cache,
2968                                             Register index,
2969                                             bool is_static,
2970                                             bool has_tos) {
2971   if (JvmtiExport::can_post_field_access()) {
2972     // Check to see if a field access watch has been set before we take
2973     // the time to call into the VM.
2974     Label L1;
2975     assert_different_registers(cache, index, rax);
2976     __ mov32(rax, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
</pre>
<hr />
<pre>
3138   __ jcc(Assembler::notEqual, notFloat);
3139   // ftos
3140 
3141   __ access_load_at(T_FLOAT, IN_HEAP, noreg /* ftos */, field, noreg, noreg);
3142   __ push(ftos);
3143   TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3144       field, rdx, SharedRuntime::tsan_read4, ftos));
3145   // Rewrite bytecode to be faster
3146   if (!is_static &amp;&amp; rc == may_rewrite) {
3147     patch_bytecode(Bytecodes::_fast_fgetfield, bc, rbx);
3148   }
3149   __ jmp(Done);
3150 
3151   __ bind(notFloat);
3152 #ifdef ASSERT
3153   Label notDouble;
3154   __ cmpl(flags, dtos);
3155   __ jcc(Assembler::notEqual, notDouble);
3156 #endif
3157   // dtos
<span class="line-modified">3158   __ access_load_at(T_DOUBLE, IN_HEAP, noreg /* dtos */, field, noreg, noreg);</span>

3159   __ push(dtos);
3160   TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3161       field, rdx, SharedRuntime::tsan_read8, dtos));
3162   // Rewrite bytecode to be faster
3163   if (!is_static &amp;&amp; rc == may_rewrite) {
3164     patch_bytecode(Bytecodes::_fast_dgetfield, bc, rbx);
3165   }
3166 #ifdef ASSERT
3167   __ jmp(Done);
3168 
3169   __ bind(notDouble);
3170   __ stop(&quot;Bad state&quot;);
3171 #endif
3172 
3173   __ bind(Done);
3174   // [jk] not needed currently
3175   // volatile_barrier(Assembler::Membar_mask_bits(Assembler::LoadLoad |
3176   //                                              Assembler::LoadStore));
3177 }
3178 
</pre>
<hr />
<pre>
3423     if (!is_static) pop_and_check_object(obj);
3424     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3425         field, rdx, SharedRuntime::tsan_write2, stos));
3426     __ access_store_at(T_SHORT, IN_HEAP, field, rax, noreg, noreg);
3427     if (!is_static &amp;&amp; rc == may_rewrite) {
3428       patch_bytecode(Bytecodes::_fast_sputfield, bc, rbx, true, byte_no);
3429     }
3430     __ jmp(Done);
3431   }
3432 
3433   __ bind(notShort);
3434   __ cmpl(flags, ltos);
3435   __ jcc(Assembler::notEqual, notLong);
3436 
3437   // ltos
3438   {
3439     __ pop(ltos);
3440     if (!is_static) pop_and_check_object(obj);
3441     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3442         field, rdx, SharedRuntime::tsan_write8, ltos));
<span class="line-modified">3443     __ access_store_at(T_LONG, IN_HEAP, field, noreg /* ltos*/, noreg, noreg);</span>

3444 #ifdef _LP64
3445     if (!is_static &amp;&amp; rc == may_rewrite) {
3446       patch_bytecode(Bytecodes::_fast_lputfield, bc, rbx, true, byte_no);
3447     }
3448 #endif // _LP64
3449     __ jmp(Done);
3450   }
3451 
3452   __ bind(notLong);
3453   __ cmpl(flags, ftos);
3454   __ jcc(Assembler::notEqual, notFloat);
3455 
3456   // ftos
3457   {
3458     __ pop(ftos);
3459     if (!is_static) pop_and_check_object(obj);
3460     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3461         field, rdx, SharedRuntime::tsan_write4, ftos));
3462     __ access_store_at(T_FLOAT, IN_HEAP, field, noreg /* ftos */, noreg, noreg);
3463     if (!is_static &amp;&amp; rc == may_rewrite) {
3464       patch_bytecode(Bytecodes::_fast_fputfield, bc, rbx, true, byte_no);
3465     }
3466     __ jmp(Done);
3467   }
3468 
3469   __ bind(notFloat);
3470 #ifdef ASSERT
3471   Label notDouble;
3472   __ cmpl(flags, dtos);
3473   __ jcc(Assembler::notEqual, notDouble);
3474 #endif
3475 
3476   // dtos
3477   {
3478     __ pop(dtos);
3479     if (!is_static) pop_and_check_object(obj);
3480     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3481         field, rdx, SharedRuntime::tsan_write8, dtos));
<span class="line-modified">3482     __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg /* dtos */, noreg, noreg);</span>

3483     if (!is_static &amp;&amp; rc == may_rewrite) {
3484       patch_bytecode(Bytecodes::_fast_dputfield, bc, rbx, true, byte_no);
3485     }
3486   }
3487 
3488 #ifdef ASSERT
3489   __ jmp(Done);
3490 
3491   __ bind(notDouble);
3492   __ stop(&quot;Bad state&quot;);
3493 #endif
3494 
3495   __ bind(Done);
3496 }
3497 
3498 void TemplateTable::putfield(int byte_no) {
3499   putfield_or_static(byte_no, false);
3500 }
3501 
3502 void TemplateTable::nofast_putfield(int byte_no) {
</pre>
<hr />
<pre>
4031   __ load_klass(rdx, rcx);
4032 
4033   Label no_such_method;
4034 
4035   // Preserve method for throw_AbstractMethodErrorVerbose.
4036   __ mov(rcx, rbx);
4037   // Receiver subtype check against REFC.
4038   // Superklass in rax. Subklass in rdx. Blows rcx, rdi.
4039   __ lookup_interface_method(// inputs: rec. class, interface, itable index
4040                              rdx, rax, noreg,
4041                              // outputs: scan temp. reg, scan temp. reg
4042                              rbcp, rlocals,
4043                              no_such_interface,
4044                              /*return_method=*/false);
4045 
4046   // profile this call
4047   __ restore_bcp(); // rbcp was destroyed by receiver type check
4048   __ profile_virtual_call(rdx, rbcp, rlocals);
4049 
4050   // Get declaring interface class from method, and itable index
<span class="line-modified">4051   __ movptr(rax, Address(rbx, Method::const_offset()));</span>
<span class="line-removed">4052   __ movptr(rax, Address(rax, ConstMethod::constants_offset()));</span>
<span class="line-removed">4053   __ movptr(rax, Address(rax, ConstantPool::pool_holder_offset_in_bytes()));</span>
4054   __ movl(rbx, Address(rbx, Method::itable_index_offset()));
4055   __ subl(rbx, Method::itable_index_max);
4056   __ negl(rbx);
4057 
4058   // Preserve recvKlass for throw_AbstractMethodErrorVerbose.
4059   __ mov(rlocals, rdx);
4060   __ lookup_interface_method(// inputs: rec. class, interface, itable index
4061                              rlocals, rax, rbx,
4062                              // outputs: method, scan temp. reg
4063                              rbx, rbcp,
4064                              no_such_interface);
4065 
4066   // rbx: Method* to call
4067   // rcx: receiver
4068   // Check for abstract method error
4069   // Note: This should be done more efficiently via a throw_abstract_method_error
4070   //       interpreter entry point and a conditional jump to it in case of a null
4071   //       method.
4072   __ testptr(rbx, rbx);
4073   __ jcc(Assembler::zero, no_such_method);
</pre>
<hr />
<pre>
4172 
4173 void TemplateTable::_new() {
4174   transition(vtos, atos);
4175   __ get_unsigned_2_byte_index_at_bcp(rdx, 1);
4176   Label slow_case;
4177   Label slow_case_no_pop;
4178   Label done;
4179   Label initialize_header;
4180   Label initialize_object;  // including clearing the fields
4181 
4182   __ get_cpool_and_tags(rcx, rax);
4183 
4184   // Make sure the class we&#39;re about to instantiate has been resolved.
4185   // This is done before loading InstanceKlass to be consistent with the order
4186   // how Constant Pool is updated (see ConstantPool::klass_at_put)
4187   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
4188   __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);
4189   __ jcc(Assembler::notEqual, slow_case_no_pop);
4190 
4191   // get InstanceKlass
<span class="line-modified">4192   __ load_resolved_klass_at_index(rcx, rdx, rcx);</span>
4193   __ push(rcx);  // save the contexts of klass for initializing the header
4194 
4195   // make sure klass is initialized &amp; doesn&#39;t have finalizer
4196   // make sure klass is fully initialized
4197   __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
4198   __ jcc(Assembler::notEqual, slow_case);
4199 
4200   // get instance_size in InstanceKlass (scaled to a count of bytes)
4201   __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));
4202   // test to see if it has a finalizer or is malformed in some way
4203   __ testl(rdx, Klass::_lh_instance_slow_path_bit);
4204   __ jcc(Assembler::notZero, slow_case);
4205 
4206   // Allocate the instance:
4207   //  If TLAB is enabled:
4208   //    Try to allocate in the TLAB.
4209   //    If fails, go to the slow path.
4210   //  Else If inline contiguous allocations are enabled:
4211   //    Try to allocate in eden.
4212   //    If fails due to heap end, go to slow path.
</pre>
<hr />
<pre>
4268     // rdx must be &gt; 0, no extra check needed here
4269 #endif
4270 
4271     // initialize remaining object fields: rdx was a multiple of 8
4272     { Label loop;
4273     __ bind(loop);
4274     __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);
4275     NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));
4276     __ decrement(rdx);
4277     __ jcc(Assembler::notZero, loop);
4278     }
4279 
4280     // initialize object header only.
4281     __ bind(initialize_header);
4282     if (UseBiasedLocking) {
4283       __ pop(rcx);   // get saved klass back in the register.
4284       __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));
4285       __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);
4286     } else {
4287       __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),
<span class="line-modified">4288                 (intptr_t)markOopDesc::prototype()); // header</span>
4289       __ pop(rcx);   // get saved klass back in the register.
4290     }
4291 #ifdef _LP64
4292     __ xorl(rsi, rsi); // use zero reg to clear memory (shorter code)
4293     __ store_klass_gap(rax, rsi);  // zero klass gap for compressed oops
4294 #endif
4295     __ store_klass(rax, rcx);  // klass
4296 
4297     {
4298       SkipIfEqual skip_if(_masm, &amp;DTraceAllocProbes, 0);
4299       // Trigger dtrace event for fastpath
4300       __ push(atos);
4301       __ call_VM_leaf(
4302            CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);
4303       __ pop(atos);
4304     }
4305 
4306     TSAN_RUNTIME_ONLY(
4307       // return value of new oop is in rax.
4308       __ push(atos);
</pre>
<hr />
<pre>
4374   __ jcc(Assembler::equal, quicked);
4375   __ push(atos); // save receiver for result, and for GC
4376   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4377 
4378   // vm_result_2 has metadata result
4379 #ifndef _LP64
4380   // borrow rdi from locals
4381   __ get_thread(rdi);
4382   __ get_vm_result_2(rax, rdi);
4383   __ restore_locals();
4384 #else
4385   __ get_vm_result_2(rax, r15_thread);
4386 #endif
4387 
4388   __ pop_ptr(rdx); // restore receiver
4389   __ jmpb(resolved);
4390 
4391   // Get superklass in rax and subklass in rbx
4392   __ bind(quicked);
4393   __ mov(rdx, rax); // Save object in rdx; rax needed for subtype check
<span class="line-modified">4394   __ load_resolved_klass_at_index(rcx, rbx, rax);</span>
4395 
4396   __ bind(resolved);
4397   __ load_klass(rbx, rdx);
4398 
4399   // Generate subtype check.  Blows rcx, rdi.  Object in rdx.
4400   // Superklass in rax.  Subklass in rbx.
4401   __ gen_subtype_check(rbx, ok_is_subtype);
4402 
4403   // Come here on failure
4404   __ push_ptr(rdx);
4405   // object is at TOS
4406   __ jump(ExternalAddress(Interpreter::_throw_ClassCastException_entry));
4407 
4408   // Come here on success
4409   __ bind(ok_is_subtype);
4410   __ mov(rax, rdx); // Restore object in rdx
4411 
4412   // Collect counts on whether this check-cast sees NULLs a lot or not.
4413   if (ProfileInterpreter) {
4414     __ jmp(done);
</pre>
<hr />
<pre>
4440   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4441   // vm_result_2 has metadata result
4442 
4443 #ifndef _LP64
4444   // borrow rdi from locals
4445   __ get_thread(rdi);
4446   __ get_vm_result_2(rax, rdi);
4447   __ restore_locals();
4448 #else
4449   __ get_vm_result_2(rax, r15_thread);
4450 #endif
4451 
4452   __ pop_ptr(rdx); // restore receiver
4453   __ verify_oop(rdx);
4454   __ load_klass(rdx, rdx);
4455   __ jmpb(resolved);
4456 
4457   // Get superklass in rax and subklass in rdx
4458   __ bind(quicked);
4459   __ load_klass(rdx, rax);
<span class="line-modified">4460   __ load_resolved_klass_at_index(rcx, rbx, rax);</span>
4461 
4462   __ bind(resolved);
4463 
4464   // Generate subtype check.  Blows rcx, rdi
4465   // Superklass in rax.  Subklass in rdx.
4466   __ gen_subtype_check(rdx, ok_is_subtype);
4467 
4468   // Come here on failure
4469   __ xorl(rax, rax);
4470   __ jmpb(done);
4471   // Come here on success
4472   __ bind(ok_is_subtype);
4473   __ movl(rax, 1);
4474 
4475   // Collect counts on whether this test sees NULLs a lot or not.
4476   if (ProfileInterpreter) {
4477     __ jmp(done);
4478     __ bind(is_null);
4479     __ profile_null_seen(rcx);
4480   } else {
</pre>
</td>
<td>
<hr />
<pre>
2828 // (3) Similar a volatile write cannot let unrelated NON-volatile
2829 //     memory refs that happen BEFORE the write float down to after the
2830 //     write.  It&#39;s OK for non-volatile memory refs that happen after the
2831 //     volatile write to float up before it.
2832 //
2833 // We only put in barriers around volatile refs (they are expensive),
2834 // not _between_ memory refs (that would require us to track the
2835 // flavor of the previous memory refs).  Requirements (2) and (3)
2836 // require some barriers before volatile stores and after volatile
2837 // loads.  These nearly cover requirement (1) but miss the
2838 // volatile-store-volatile-load case.  This final case is placed after
2839 // volatile-stores although it could just as well go before
2840 // volatile-loads.
2841 
2842 void TemplateTable::volatile_barrier(Assembler::Membar_mask_bits order_constraint ) {
2843   // Helper function to insert a is-volatile test and memory barrier
2844   __ membar(order_constraint);
2845 }
2846 
2847 void TemplateTable::resolve_cache_and_index(int byte_no,
<span class="line-modified">2848                                             Register cache,</span>
2849                                             Register index,
2850                                             size_t index_size) {
2851   const Register temp = rbx;
<span class="line-modified">2852   assert_different_registers(cache, index, temp);</span>
2853 
<span class="line-added">2854   Label L_clinit_barrier_slow;</span>
2855   Label resolved;
2856 
2857   Bytecodes::Code code = bytecode();
2858   switch (code) {
2859   case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2860   case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2861   default: break;
2862   }
2863 
2864   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
<span class="line-modified">2865   __ get_cache_and_index_and_bytecode_at_bcp(cache, index, temp, byte_no, 1, index_size);</span>
2866   __ cmpl(temp, code);  // have we resolved this bytecode?
2867   __ jcc(Assembler::equal, resolved);
2868 
2869   // resolve first time through
<span class="line-added">2870   // Class initialization barrier slow path lands here as well.</span>
<span class="line-added">2871   __ bind(L_clinit_barrier_slow);</span>
2872   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2873   __ movl(temp, code);
2874   __ call_VM(noreg, entry, temp);
2875   // Update registers with resolved info
<span class="line-modified">2876   __ get_cache_and_index_at_bcp(cache, index, 1, index_size);</span>
<span class="line-added">2877 </span>
2878   __ bind(resolved);
<span class="line-added">2879 </span>
<span class="line-added">2880   // Class initialization barrier for static methods</span>
<span class="line-added">2881   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; bytecode() == Bytecodes::_invokestatic) {</span>
<span class="line-added">2882     const Register method = temp;</span>
<span class="line-added">2883     const Register klass  = temp;</span>
<span class="line-added">2884     const Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);</span>
<span class="line-added">2885     assert(thread != noreg, &quot;x86_32 not supported&quot;);</span>
<span class="line-added">2886 </span>
<span class="line-added">2887     __ load_resolved_method_at_index(byte_no, method, cache, index);</span>
<span class="line-added">2888     __ load_method_holder(klass, method);</span>
<span class="line-added">2889     __ clinit_barrier(klass, thread, NULL /*L_fast_path*/, &amp;L_clinit_barrier_slow);</span>
<span class="line-added">2890   }</span>
2891 }
2892 
2893 // The cache and index registers must be set before call
2894 void TemplateTable::load_field_cp_cache_entry(Register obj,
2895                                               Register cache,
2896                                               Register index,
2897                                               Register off,
2898                                               Register flags,
2899                                               bool is_static = false) {
2900   assert_different_registers(cache, index, flags, off);
2901 
2902   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2903   // Field offset
2904   __ movptr(off, Address(cache, index, Address::times_ptr,
2905                          in_bytes(cp_base_offset +
2906                                   ConstantPoolCacheEntry::f2_offset())));
2907   // Flags
2908   __ movl(flags, Address(cache, index, Address::times_ptr,
2909                          in_bytes(cp_base_offset +
2910                                   ConstantPoolCacheEntry::flags_offset())));
</pre>
<hr />
<pre>
2939     );
2940   }
2941 }
2942 
2943 void TemplateTable::load_invoke_cp_cache_entry(int byte_no,
2944                                                Register method,
2945                                                Register itable_index,
2946                                                Register flags,
2947                                                bool is_invokevirtual,
2948                                                bool is_invokevfinal, /*unused*/
2949                                                bool is_invokedynamic) {
2950   // setup registers
2951   const Register cache = rcx;
2952   const Register index = rdx;
2953   assert_different_registers(method, flags);
2954   assert_different_registers(method, cache, index);
2955   assert_different_registers(itable_index, flags);
2956   assert_different_registers(itable_index, cache, index);
2957   // determine constant pool cache field offsets
2958   assert(is_invokevirtual == (byte_no == f2_byte), &quot;is_invokevirtual flag redundant&quot;);





2959   const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +
2960                                     ConstantPoolCacheEntry::flags_offset());
2961   // access constant pool cache fields
2962   const int index_offset = in_bytes(ConstantPoolCache::base_offset() +
2963                                     ConstantPoolCacheEntry::f2_offset());
2964 
2965   size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));
2966   resolve_cache_and_index(byte_no, cache, index, index_size);
<span class="line-modified">2967   __ load_resolved_method_at_index(byte_no, method, cache, index);</span>
2968 
2969   if (itable_index != noreg) {
2970     // pick up itable or appendix index from f2 also:
2971     __ movptr(itable_index, Address(cache, index, Address::times_ptr, index_offset));
2972   }
2973   __ movl(flags, Address(cache, index, Address::times_ptr, flags_offset));
2974 }
2975 
2976 // The registers cache and index expected to be set before call.
2977 // Correct values of the cache and index registers are preserved.
2978 void TemplateTable::jvmti_post_field_access(Register cache,
2979                                             Register index,
2980                                             bool is_static,
2981                                             bool has_tos) {
2982   if (JvmtiExport::can_post_field_access()) {
2983     // Check to see if a field access watch has been set before we take
2984     // the time to call into the VM.
2985     Label L1;
2986     assert_different_registers(cache, index, rax);
2987     __ mov32(rax, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
</pre>
<hr />
<pre>
3149   __ jcc(Assembler::notEqual, notFloat);
3150   // ftos
3151 
3152   __ access_load_at(T_FLOAT, IN_HEAP, noreg /* ftos */, field, noreg, noreg);
3153   __ push(ftos);
3154   TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3155       field, rdx, SharedRuntime::tsan_read4, ftos));
3156   // Rewrite bytecode to be faster
3157   if (!is_static &amp;&amp; rc == may_rewrite) {
3158     patch_bytecode(Bytecodes::_fast_fgetfield, bc, rbx);
3159   }
3160   __ jmp(Done);
3161 
3162   __ bind(notFloat);
3163 #ifdef ASSERT
3164   Label notDouble;
3165   __ cmpl(flags, dtos);
3166   __ jcc(Assembler::notEqual, notDouble);
3167 #endif
3168   // dtos
<span class="line-modified">3169   // MO_RELAXED: for the case of volatile field, in fact it adds no extra work for the underlying implementation</span>
<span class="line-added">3170   __ access_load_at(T_DOUBLE, IN_HEAP | MO_RELAXED, noreg /* dtos */, field, noreg, noreg);</span>
3171   __ push(dtos);
3172   TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3173       field, rdx, SharedRuntime::tsan_read8, dtos));
3174   // Rewrite bytecode to be faster
3175   if (!is_static &amp;&amp; rc == may_rewrite) {
3176     patch_bytecode(Bytecodes::_fast_dgetfield, bc, rbx);
3177   }
3178 #ifdef ASSERT
3179   __ jmp(Done);
3180 
3181   __ bind(notDouble);
3182   __ stop(&quot;Bad state&quot;);
3183 #endif
3184 
3185   __ bind(Done);
3186   // [jk] not needed currently
3187   // volatile_barrier(Assembler::Membar_mask_bits(Assembler::LoadLoad |
3188   //                                              Assembler::LoadStore));
3189 }
3190 
</pre>
<hr />
<pre>
3435     if (!is_static) pop_and_check_object(obj);
3436     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3437         field, rdx, SharedRuntime::tsan_write2, stos));
3438     __ access_store_at(T_SHORT, IN_HEAP, field, rax, noreg, noreg);
3439     if (!is_static &amp;&amp; rc == may_rewrite) {
3440       patch_bytecode(Bytecodes::_fast_sputfield, bc, rbx, true, byte_no);
3441     }
3442     __ jmp(Done);
3443   }
3444 
3445   __ bind(notShort);
3446   __ cmpl(flags, ltos);
3447   __ jcc(Assembler::notEqual, notLong);
3448 
3449   // ltos
3450   {
3451     __ pop(ltos);
3452     if (!is_static) pop_and_check_object(obj);
3453     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3454         field, rdx, SharedRuntime::tsan_write8, ltos));
<span class="line-modified">3455     // MO_RELAXED: generate atomic store for the case of volatile field (important for x86_32)</span>
<span class="line-added">3456     __ access_store_at(T_LONG, IN_HEAP | MO_RELAXED, field, noreg /* ltos*/, noreg, noreg);</span>
3457 #ifdef _LP64
3458     if (!is_static &amp;&amp; rc == may_rewrite) {
3459       patch_bytecode(Bytecodes::_fast_lputfield, bc, rbx, true, byte_no);
3460     }
3461 #endif // _LP64
3462     __ jmp(Done);
3463   }
3464 
3465   __ bind(notLong);
3466   __ cmpl(flags, ftos);
3467   __ jcc(Assembler::notEqual, notFloat);
3468 
3469   // ftos
3470   {
3471     __ pop(ftos);
3472     if (!is_static) pop_and_check_object(obj);
3473     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3474         field, rdx, SharedRuntime::tsan_write4, ftos));
3475     __ access_store_at(T_FLOAT, IN_HEAP, field, noreg /* ftos */, noreg, noreg);
3476     if (!is_static &amp;&amp; rc == may_rewrite) {
3477       patch_bytecode(Bytecodes::_fast_fputfield, bc, rbx, true, byte_no);
3478     }
3479     __ jmp(Done);
3480   }
3481 
3482   __ bind(notFloat);
3483 #ifdef ASSERT
3484   Label notDouble;
3485   __ cmpl(flags, dtos);
3486   __ jcc(Assembler::notEqual, notDouble);
3487 #endif
3488 
3489   // dtos
3490   {
3491     __ pop(dtos);
3492     if (!is_static) pop_and_check_object(obj);
3493     TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
3494         field, rdx, SharedRuntime::tsan_write8, dtos));
<span class="line-modified">3495     // MO_RELAXED: for the case of volatile field, in fact it adds no extra work for the underlying implementation</span>
<span class="line-added">3496     __ access_store_at(T_DOUBLE, IN_HEAP | MO_RELAXED, field, noreg /* dtos */, noreg, noreg);</span>
3497     if (!is_static &amp;&amp; rc == may_rewrite) {
3498       patch_bytecode(Bytecodes::_fast_dputfield, bc, rbx, true, byte_no);
3499     }
3500   }
3501 
3502 #ifdef ASSERT
3503   __ jmp(Done);
3504 
3505   __ bind(notDouble);
3506   __ stop(&quot;Bad state&quot;);
3507 #endif
3508 
3509   __ bind(Done);
3510 }
3511 
3512 void TemplateTable::putfield(int byte_no) {
3513   putfield_or_static(byte_no, false);
3514 }
3515 
3516 void TemplateTable::nofast_putfield(int byte_no) {
</pre>
<hr />
<pre>
4045   __ load_klass(rdx, rcx);
4046 
4047   Label no_such_method;
4048 
4049   // Preserve method for throw_AbstractMethodErrorVerbose.
4050   __ mov(rcx, rbx);
4051   // Receiver subtype check against REFC.
4052   // Superklass in rax. Subklass in rdx. Blows rcx, rdi.
4053   __ lookup_interface_method(// inputs: rec. class, interface, itable index
4054                              rdx, rax, noreg,
4055                              // outputs: scan temp. reg, scan temp. reg
4056                              rbcp, rlocals,
4057                              no_such_interface,
4058                              /*return_method=*/false);
4059 
4060   // profile this call
4061   __ restore_bcp(); // rbcp was destroyed by receiver type check
4062   __ profile_virtual_call(rdx, rbcp, rlocals);
4063 
4064   // Get declaring interface class from method, and itable index
<span class="line-modified">4065   __ load_method_holder(rax, rbx);</span>


4066   __ movl(rbx, Address(rbx, Method::itable_index_offset()));
4067   __ subl(rbx, Method::itable_index_max);
4068   __ negl(rbx);
4069 
4070   // Preserve recvKlass for throw_AbstractMethodErrorVerbose.
4071   __ mov(rlocals, rdx);
4072   __ lookup_interface_method(// inputs: rec. class, interface, itable index
4073                              rlocals, rax, rbx,
4074                              // outputs: method, scan temp. reg
4075                              rbx, rbcp,
4076                              no_such_interface);
4077 
4078   // rbx: Method* to call
4079   // rcx: receiver
4080   // Check for abstract method error
4081   // Note: This should be done more efficiently via a throw_abstract_method_error
4082   //       interpreter entry point and a conditional jump to it in case of a null
4083   //       method.
4084   __ testptr(rbx, rbx);
4085   __ jcc(Assembler::zero, no_such_method);
</pre>
<hr />
<pre>
4184 
4185 void TemplateTable::_new() {
4186   transition(vtos, atos);
4187   __ get_unsigned_2_byte_index_at_bcp(rdx, 1);
4188   Label slow_case;
4189   Label slow_case_no_pop;
4190   Label done;
4191   Label initialize_header;
4192   Label initialize_object;  // including clearing the fields
4193 
4194   __ get_cpool_and_tags(rcx, rax);
4195 
4196   // Make sure the class we&#39;re about to instantiate has been resolved.
4197   // This is done before loading InstanceKlass to be consistent with the order
4198   // how Constant Pool is updated (see ConstantPool::klass_at_put)
4199   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
4200   __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);
4201   __ jcc(Assembler::notEqual, slow_case_no_pop);
4202 
4203   // get InstanceKlass
<span class="line-modified">4204   __ load_resolved_klass_at_index(rcx, rcx, rdx);</span>
4205   __ push(rcx);  // save the contexts of klass for initializing the header
4206 
4207   // make sure klass is initialized &amp; doesn&#39;t have finalizer
4208   // make sure klass is fully initialized
4209   __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
4210   __ jcc(Assembler::notEqual, slow_case);
4211 
4212   // get instance_size in InstanceKlass (scaled to a count of bytes)
4213   __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));
4214   // test to see if it has a finalizer or is malformed in some way
4215   __ testl(rdx, Klass::_lh_instance_slow_path_bit);
4216   __ jcc(Assembler::notZero, slow_case);
4217 
4218   // Allocate the instance:
4219   //  If TLAB is enabled:
4220   //    Try to allocate in the TLAB.
4221   //    If fails, go to the slow path.
4222   //  Else If inline contiguous allocations are enabled:
4223   //    Try to allocate in eden.
4224   //    If fails due to heap end, go to slow path.
</pre>
<hr />
<pre>
4280     // rdx must be &gt; 0, no extra check needed here
4281 #endif
4282 
4283     // initialize remaining object fields: rdx was a multiple of 8
4284     { Label loop;
4285     __ bind(loop);
4286     __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);
4287     NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));
4288     __ decrement(rdx);
4289     __ jcc(Assembler::notZero, loop);
4290     }
4291 
4292     // initialize object header only.
4293     __ bind(initialize_header);
4294     if (UseBiasedLocking) {
4295       __ pop(rcx);   // get saved klass back in the register.
4296       __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));
4297       __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);
4298     } else {
4299       __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),
<span class="line-modified">4300                 (intptr_t)markWord::prototype().value()); // header</span>
4301       __ pop(rcx);   // get saved klass back in the register.
4302     }
4303 #ifdef _LP64
4304     __ xorl(rsi, rsi); // use zero reg to clear memory (shorter code)
4305     __ store_klass_gap(rax, rsi);  // zero klass gap for compressed oops
4306 #endif
4307     __ store_klass(rax, rcx);  // klass
4308 
4309     {
4310       SkipIfEqual skip_if(_masm, &amp;DTraceAllocProbes, 0);
4311       // Trigger dtrace event for fastpath
4312       __ push(atos);
4313       __ call_VM_leaf(
4314            CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);
4315       __ pop(atos);
4316     }
4317 
4318     TSAN_RUNTIME_ONLY(
4319       // return value of new oop is in rax.
4320       __ push(atos);
</pre>
<hr />
<pre>
4386   __ jcc(Assembler::equal, quicked);
4387   __ push(atos); // save receiver for result, and for GC
4388   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4389 
4390   // vm_result_2 has metadata result
4391 #ifndef _LP64
4392   // borrow rdi from locals
4393   __ get_thread(rdi);
4394   __ get_vm_result_2(rax, rdi);
4395   __ restore_locals();
4396 #else
4397   __ get_vm_result_2(rax, r15_thread);
4398 #endif
4399 
4400   __ pop_ptr(rdx); // restore receiver
4401   __ jmpb(resolved);
4402 
4403   // Get superklass in rax and subklass in rbx
4404   __ bind(quicked);
4405   __ mov(rdx, rax); // Save object in rdx; rax needed for subtype check
<span class="line-modified">4406   __ load_resolved_klass_at_index(rax, rcx, rbx);</span>
4407 
4408   __ bind(resolved);
4409   __ load_klass(rbx, rdx);
4410 
4411   // Generate subtype check.  Blows rcx, rdi.  Object in rdx.
4412   // Superklass in rax.  Subklass in rbx.
4413   __ gen_subtype_check(rbx, ok_is_subtype);
4414 
4415   // Come here on failure
4416   __ push_ptr(rdx);
4417   // object is at TOS
4418   __ jump(ExternalAddress(Interpreter::_throw_ClassCastException_entry));
4419 
4420   // Come here on success
4421   __ bind(ok_is_subtype);
4422   __ mov(rax, rdx); // Restore object in rdx
4423 
4424   // Collect counts on whether this check-cast sees NULLs a lot or not.
4425   if (ProfileInterpreter) {
4426     __ jmp(done);
</pre>
<hr />
<pre>
4452   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4453   // vm_result_2 has metadata result
4454 
4455 #ifndef _LP64
4456   // borrow rdi from locals
4457   __ get_thread(rdi);
4458   __ get_vm_result_2(rax, rdi);
4459   __ restore_locals();
4460 #else
4461   __ get_vm_result_2(rax, r15_thread);
4462 #endif
4463 
4464   __ pop_ptr(rdx); // restore receiver
4465   __ verify_oop(rdx);
4466   __ load_klass(rdx, rdx);
4467   __ jmpb(resolved);
4468 
4469   // Get superklass in rax and subklass in rdx
4470   __ bind(quicked);
4471   __ load_klass(rdx, rax);
<span class="line-modified">4472   __ load_resolved_klass_at_index(rax, rcx, rbx);</span>
4473 
4474   __ bind(resolved);
4475 
4476   // Generate subtype check.  Blows rcx, rdi
4477   // Superklass in rax.  Subklass in rdx.
4478   __ gen_subtype_check(rdx, ok_is_subtype);
4479 
4480   // Come here on failure
4481   __ xorl(rax, rax);
4482   __ jmpb(done);
4483   // Come here on success
4484   __ bind(ok_is_subtype);
4485   __ movl(rax, 1);
4486 
4487   // Collect counts on whether this test sees NULLs a lot or not.
4488   if (ProfileInterpreter) {
4489     __ jmp(done);
4490     __ bind(is_null);
4491     __ profile_null_seen(rcx);
4492   } else {
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_x86_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../../os_cpu/linux_x86/os_linux_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>