<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/os_cpu/bsd_zero/atomic_bsd_zero.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../bsd_x86/vm_version_bsd_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="orderAccess_bsd_zero.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os_cpu/bsd_zero/atomic_bsd_zero.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
143 
144       if (__kernel_cmpxchg (prev, prev + add_value, ptr) == 0)
145         return prev + add_value;
146     }
147 }
148 
149 /* Atomically write VALUE into `*PTR&#39; and returns the previous
150    contents of `*PTR&#39;.  */
151 static inline int arm_lock_test_and_set(int newval, volatile int *ptr) {
152   for (;;) {
153       // Loop until a __kernel_cmpxchg succeeds.
154       int prev = *ptr;
155 
156       if (__kernel_cmpxchg (prev, newval, ptr) == 0)
157         return prev;
158     }
159 }
160 #endif // ARM
161 
162 template&lt;size_t byte_size&gt;
<span class="line-modified">163 struct Atomic::PlatformAdd</span>
<span class="line-modified">164   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;</span>
<span class="line-modified">165 {</span>
<span class="line-modified">166   template&lt;typename I, typename D&gt;</span>
<span class="line-modified">167   D add_and_fetch(I add_value, D volatile* dest, atomic_memory_order order) const;</span>



168 };
169 
170 template&lt;&gt;
<span class="line-modified">171 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">172 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
173                                                atomic_memory_order order) const {
174   STATIC_ASSERT(4 == sizeof(I));
175   STATIC_ASSERT(4 == sizeof(D));
176 
177 #ifdef ARM
<span class="line-modified">178   return add_using_helper&lt;int&gt;(arm_add_and_fetch, add_value, dest);</span>
179 #else
180 #ifdef M68K
<span class="line-modified">181   return add_using_helper&lt;int&gt;(m68k_add_and_fetch, add_value, dest);</span>
182 #else
183   return __sync_add_and_fetch(dest, add_value);
184 #endif // M68K
185 #endif // ARM
186 }
187 
188 template&lt;&gt;
<span class="line-modified">189 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">190 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
191                                                atomic_memory_order order) const {
192   STATIC_ASSERT(8 == sizeof(I));
193   STATIC_ASSERT(8 == sizeof(D));
194 
195   return __sync_add_and_fetch(dest, add_value);
196 }
197 
198 template&lt;&gt;
199 template&lt;typename T&gt;
<span class="line-modified">200 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-modified">201                                              T volatile* dest,</span>
202                                              atomic_memory_order order) const {
203   STATIC_ASSERT(4 == sizeof(T));
204 #ifdef ARM
<span class="line-modified">205   return xchg_using_helper&lt;int&gt;(arm_lock_test_and_set, exchange_value, dest);</span>
206 #else
207 #ifdef M68K
<span class="line-modified">208   return xchg_using_helper&lt;int&gt;(m68k_lock_test_and_set, exchange_value, dest);</span>
209 #else
210   // __sync_lock_test_and_set is a bizarrely named atomic exchange
211   // operation.  Note that some platforms only support this with the
212   // limitation that the only valid value to store is the immediate
213   // constant 1.  There is a test for this in JNI_CreateJavaVM().
214   T result = __sync_lock_test_and_set (dest, exchange_value);
215   // All atomic operations are expected to be full memory barriers
216   // (see atomic.hpp). However, __sync_lock_test_and_set is not
217   // a full memory barrier, but an acquire barrier. Hence, this added
218   // barrier.
219   __sync_synchronize();
220   return result;
221 #endif // M68K
222 #endif // ARM
223 }
224 
225 template&lt;&gt;
226 template&lt;typename T&gt;
<span class="line-modified">227 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-modified">228                                              T volatile* dest,</span>
229                                              atomic_memory_order order) const {
230   STATIC_ASSERT(8 == sizeof(T));
231   T result = __sync_lock_test_and_set (dest, exchange_value);
232   __sync_synchronize();
233   return result;
234 }
235 
236 // No direct support for cmpxchg of bytes; emulate using int.
237 template&lt;&gt;
238 struct Atomic::PlatformCmpxchg&lt;1&gt; : Atomic::CmpxchgByteUsingInt {};
239 
240 template&lt;&gt;
241 template&lt;typename T&gt;
<span class="line-modified">242 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-removed">243                                                 T volatile* dest,</span>
244                                                 T compare_value,

245                                                 atomic_memory_order order) const {
246   STATIC_ASSERT(4 == sizeof(T));
247 #ifdef ARM
<span class="line-modified">248   return cmpxchg_using_helper&lt;int&gt;(arm_compare_and_swap, exchange_value, dest, compare_value);</span>
249 #else
250 #ifdef M68K
<span class="line-modified">251   return cmpxchg_using_helper&lt;int&gt;(m68k_compare_and_swap, exchange_value, dest, compare_value);</span>
252 #else
253   return __sync_val_compare_and_swap(dest, compare_value, exchange_value);
254 #endif // M68K
255 #endif // ARM
256 }
257 
258 template&lt;&gt;
259 template&lt;typename T&gt;
<span class="line-modified">260 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-removed">261                                                 T volatile* dest,</span>
262                                                 T compare_value,

263                                                 atomic_memory_order order) const {
264   STATIC_ASSERT(8 == sizeof(T));
265   return __sync_val_compare_and_swap(dest, compare_value, exchange_value);
266 }
267 
268 template&lt;&gt;
269 template&lt;typename T&gt;
270 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
271   STATIC_ASSERT(8 == sizeof(T));
272   volatile int64_t dest;
273   os::atomic_copy64(reinterpret_cast&lt;const volatile int64_t*&gt;(src), reinterpret_cast&lt;volatile int64_t*&gt;(&amp;dest));
274   return PrimitiveConversions::cast&lt;T&gt;(dest);
275 }
276 
277 template&lt;&gt;
278 template&lt;typename T&gt;
<span class="line-modified">279 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T store_value,</span>
<span class="line-modified">280                                                  T volatile* dest) const {</span>
281   STATIC_ASSERT(8 == sizeof(T));
282   os::atomic_copy64(reinterpret_cast&lt;const volatile int64_t*&gt;(&amp;store_value), reinterpret_cast&lt;volatile int64_t*&gt;(dest));
283 }
284 
285 #endif // OS_CPU_BSD_ZERO_ATOMIC_BSD_ZERO_HPP
</pre>
</td>
<td>
<hr />
<pre>
143 
144       if (__kernel_cmpxchg (prev, prev + add_value, ptr) == 0)
145         return prev + add_value;
146     }
147 }
148 
149 /* Atomically write VALUE into `*PTR&#39; and returns the previous
150    contents of `*PTR&#39;.  */
151 static inline int arm_lock_test_and_set(int newval, volatile int *ptr) {
152   for (;;) {
153       // Loop until a __kernel_cmpxchg succeeds.
154       int prev = *ptr;
155 
156       if (__kernel_cmpxchg (prev, newval, ptr) == 0)
157         return prev;
158     }
159 }
160 #endif // ARM
161 
162 template&lt;size_t byte_size&gt;
<span class="line-modified">163 struct Atomic::PlatformAdd {</span>
<span class="line-modified">164   template&lt;typename D, typename I&gt;</span>
<span class="line-modified">165   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;</span>
<span class="line-modified">166 </span>
<span class="line-modified">167   template&lt;typename D, typename I&gt;</span>
<span class="line-added">168   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {</span>
<span class="line-added">169     return add_and_fetch(dest, add_value, order) - add_value;</span>
<span class="line-added">170   }</span>
171 };
172 
173 template&lt;&gt;
<span class="line-modified">174 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">175 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
176                                                atomic_memory_order order) const {
177   STATIC_ASSERT(4 == sizeof(I));
178   STATIC_ASSERT(4 == sizeof(D));
179 
180 #ifdef ARM
<span class="line-modified">181   return add_using_helper&lt;int&gt;(arm_add_and_fetch, dest, add_value);</span>
182 #else
183 #ifdef M68K
<span class="line-modified">184   return add_using_helper&lt;int&gt;(m68k_add_and_fetch, dest, add_value);</span>
185 #else
186   return __sync_add_and_fetch(dest, add_value);
187 #endif // M68K
188 #endif // ARM
189 }
190 
191 template&lt;&gt;
<span class="line-modified">192 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">193 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
194                                                atomic_memory_order order) const {
195   STATIC_ASSERT(8 == sizeof(I));
196   STATIC_ASSERT(8 == sizeof(D));
197 
198   return __sync_add_and_fetch(dest, add_value);
199 }
200 
201 template&lt;&gt;
202 template&lt;typename T&gt;
<span class="line-modified">203 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">204                                              T exchange_value,</span>
205                                              atomic_memory_order order) const {
206   STATIC_ASSERT(4 == sizeof(T));
207 #ifdef ARM
<span class="line-modified">208   return xchg_using_helper&lt;int&gt;(arm_lock_test_and_set, dest, exchange_value);</span>
209 #else
210 #ifdef M68K
<span class="line-modified">211   return xchg_using_helper&lt;int&gt;(m68k_lock_test_and_set, dest, exchange_value);</span>
212 #else
213   // __sync_lock_test_and_set is a bizarrely named atomic exchange
214   // operation.  Note that some platforms only support this with the
215   // limitation that the only valid value to store is the immediate
216   // constant 1.  There is a test for this in JNI_CreateJavaVM().
217   T result = __sync_lock_test_and_set (dest, exchange_value);
218   // All atomic operations are expected to be full memory barriers
219   // (see atomic.hpp). However, __sync_lock_test_and_set is not
220   // a full memory barrier, but an acquire barrier. Hence, this added
221   // barrier.
222   __sync_synchronize();
223   return result;
224 #endif // M68K
225 #endif // ARM
226 }
227 
228 template&lt;&gt;
229 template&lt;typename T&gt;
<span class="line-modified">230 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">231                                              T exchange_value,</span>
232                                              atomic_memory_order order) const {
233   STATIC_ASSERT(8 == sizeof(T));
234   T result = __sync_lock_test_and_set (dest, exchange_value);
235   __sync_synchronize();
236   return result;
237 }
238 
239 // No direct support for cmpxchg of bytes; emulate using int.
240 template&lt;&gt;
241 struct Atomic::PlatformCmpxchg&lt;1&gt; : Atomic::CmpxchgByteUsingInt {};
242 
243 template&lt;&gt;
244 template&lt;typename T&gt;
<span class="line-modified">245 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>

246                                                 T compare_value,
<span class="line-added">247                                                 T exchange_value,</span>
248                                                 atomic_memory_order order) const {
249   STATIC_ASSERT(4 == sizeof(T));
250 #ifdef ARM
<span class="line-modified">251   return cmpxchg_using_helper&lt;int&gt;(arm_compare_and_swap, dest, compare_value, exchange_value);</span>
252 #else
253 #ifdef M68K
<span class="line-modified">254   return cmpxchg_using_helper&lt;int&gt;(m68k_compare_and_swap, dest, compare_value, exchange_value);</span>
255 #else
256   return __sync_val_compare_and_swap(dest, compare_value, exchange_value);
257 #endif // M68K
258 #endif // ARM
259 }
260 
261 template&lt;&gt;
262 template&lt;typename T&gt;
<span class="line-modified">263 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>

264                                                 T compare_value,
<span class="line-added">265                                                 T exchange_value,</span>
266                                                 atomic_memory_order order) const {
267   STATIC_ASSERT(8 == sizeof(T));
268   return __sync_val_compare_and_swap(dest, compare_value, exchange_value);
269 }
270 
271 template&lt;&gt;
272 template&lt;typename T&gt;
273 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
274   STATIC_ASSERT(8 == sizeof(T));
275   volatile int64_t dest;
276   os::atomic_copy64(reinterpret_cast&lt;const volatile int64_t*&gt;(src), reinterpret_cast&lt;volatile int64_t*&gt;(&amp;dest));
277   return PrimitiveConversions::cast&lt;T&gt;(dest);
278 }
279 
280 template&lt;&gt;
281 template&lt;typename T&gt;
<span class="line-modified">282 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">283                                                  T store_value) const {</span>
284   STATIC_ASSERT(8 == sizeof(T));
285   os::atomic_copy64(reinterpret_cast&lt;const volatile int64_t*&gt;(&amp;store_value), reinterpret_cast&lt;volatile int64_t*&gt;(dest));
286 }
287 
288 #endif // OS_CPU_BSD_ZERO_ATOMIC_BSD_ZERO_HPP
</pre>
</td>
</tr>
</table>
<center><a href="../bsd_x86/vm_version_bsd_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="orderAccess_bsd_zero.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>