<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/os_cpu/linux_aarch64/atomic_linux_aarch64.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
  4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  5  *
  6  * This code is free software; you can redistribute it and/or modify it
  7  * under the terms of the GNU General Public License version 2 only, as
  8  * published by the Free Software Foundation.
  9  *
 10  * This code is distributed in the hope that it will be useful, but WITHOUT
 11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #ifndef OS_CPU_LINUX_AARCH64_ATOMIC_LINUX_AARCH64_HPP
 27 #define OS_CPU_LINUX_AARCH64_ATOMIC_LINUX_AARCH64_HPP
 28 
 29 #include &quot;runtime/vm_version.hpp&quot;
 30 
 31 // Implementation of class atomic
 32 // Note that memory_order_conservative requires a full barrier after atomic stores.
 33 // See https://patchwork.kernel.org/patch/3575821/
 34 
 35 template&lt;size_t byte_size&gt;
 36 struct Atomic::PlatformAdd {
 37   template&lt;typename D, typename I&gt;
 38   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const {
 39     D res = __atomic_add_fetch(dest, add_value, __ATOMIC_RELEASE);
 40     FULL_MEM_BARRIER;
 41     return res;
 42   }
 43 
 44   template&lt;typename D, typename I&gt;
 45   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {
 46     return add_and_fetch(dest, add_value, order) - add_value;
 47   }
 48 };
 49 
 50 template&lt;size_t byte_size&gt;
 51 template&lt;typename T&gt;
 52 inline T Atomic::PlatformXchg&lt;byte_size&gt;::operator()(T volatile* dest,
 53                                                      T exchange_value,
 54                                                      atomic_memory_order order) const {
 55   STATIC_ASSERT(byte_size == sizeof(T));
 56   T res = __atomic_exchange_n(dest, exchange_value, __ATOMIC_RELEASE);
 57   FULL_MEM_BARRIER;
 58   return res;
 59 }
 60 
 61 // __attribute__((unused)) on dest is to get rid of spurious GCC warnings.
 62 template&lt;size_t byte_size&gt;
 63 template&lt;typename T&gt;
 64 inline T Atomic::PlatformCmpxchg&lt;byte_size&gt;::operator()(T volatile* dest __attribute__((unused)),
 65                                                         T compare_value,
 66                                                         T exchange_value,
 67                                                         atomic_memory_order order) const {
 68   STATIC_ASSERT(byte_size == sizeof(T));
 69   if (order == memory_order_relaxed) {
 70     T value = compare_value;
 71     __atomic_compare_exchange(dest, &amp;value, &amp;exchange_value, /*weak*/false,
 72                               __ATOMIC_RELAXED, __ATOMIC_RELAXED);
 73     return value;
 74   } else {
 75     T value = compare_value;
 76     FULL_MEM_BARRIER;
 77     __atomic_compare_exchange(dest, &amp;value, &amp;exchange_value, /*weak*/false,
 78                               __ATOMIC_RELAXED, __ATOMIC_RELAXED);
 79     FULL_MEM_BARRIER;
 80     return value;
 81   }
 82 }
 83 
 84 template&lt;size_t byte_size&gt;
 85 struct Atomic::PlatformOrderedLoad&lt;byte_size, X_ACQUIRE&gt;
 86 {
 87   template &lt;typename T&gt;
 88   T operator()(const volatile T* p) const { T data; __atomic_load(const_cast&lt;T*&gt;(p), &amp;data, __ATOMIC_ACQUIRE); return data; }
 89 };
 90 
 91 template&lt;size_t byte_size&gt;
 92 struct Atomic::PlatformOrderedStore&lt;byte_size, RELEASE_X&gt;
 93 {
 94   template &lt;typename T&gt;
 95   void operator()(volatile T* p, T v) const { __atomic_store(const_cast&lt;T*&gt;(p), &amp;v, __ATOMIC_RELEASE); }
 96 };
 97 
 98 template&lt;size_t byte_size&gt;
 99 struct Atomic::PlatformOrderedStore&lt;byte_size, RELEASE_X_FENCE&gt;
100 {
101   template &lt;typename T&gt;
102   void operator()(volatile T* p, T v) const { release_store(p, v); OrderAccess::fence(); }
103 };
104 
105 #endif // OS_CPU_LINUX_AARCH64_ATOMIC_LINUX_AARCH64_HPP
    </pre>
  </body>
</html>