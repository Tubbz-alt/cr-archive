diff a/src/hotspot/os_cpu/linux_x86/linux_x86_32.s b/src/hotspot/os_cpu/linux_x86/linux_x86_32.s
--- a/src/hotspot/os_cpu/linux_x86/linux_x86_32.s
+++ b/src/hotspot/os_cpu/linux_x86/linux_x86_32.s
@@ -1,6 +1,6 @@
-# 
+#
 # Copyright (c) 2004, 2017, Oracle and/or its affiliates. All rights reserved.
 # DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 #
 # This code is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License version 2 only, as
@@ -17,19 +17,19 @@
 # Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 #
 # Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 # or visit www.oracle.com if you need additional information or have any
 # questions.
-# 
+#
+
 
-	
         # NOTE WELL!  The _Copy functions are called directly
 	# from server-compiler-generated code via CallLeafNoFP,
 	# which means that they *must* either not use floating
 	# point or use it in the same manner as does the server
 	# compiler.
-	
+
         .globl _Copy_conjoint_bytes
         .globl _Copy_arrayof_conjoint_bytes
         .globl _Copy_conjoint_jshorts_atomic
 	.globl _Copy_arrayof_conjoint_jshorts
         .globl _Copy_conjoint_jints_atomic
@@ -172,11 +172,11 @@
         movl     8+ 8(%esp),%edi      # to
         cmpl     %esi,%edi
         leal     -1(%esi,%ecx),%eax   # from + count - 1
         jbe      acb_CopyRight
         cmpl     %eax,%edi
-        jbe      acb_CopyLeft 
+        jbe      acb_CopyLeft
         # copy from low to high
 acb_CopyRight:
         cmpl     $3,%ecx
         jbe      5f
 1:      movl     %ecx,%eax
@@ -260,11 +260,11 @@
         movl     8+ 8(%esp),%edi      # to
         cmpl     %esi,%edi
         leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2
         jbe      cs_CopyRight
         cmpl     %eax,%edi
-        jbe      cs_CopyLeft 
+        jbe      cs_CopyLeft
         # copy from low to high
 cs_CopyRight:
         # align source address at dword address boundary
         movl     %esi,%eax            # original from
         andl     $3,%eax              # either 0 or 2
@@ -281,11 +281,11 @@
         jz       4f                   # no dwords to move
         cmpl     $32,%ecx
         jbe      2f                   # <= 32 dwords
         # copy aligned dwords
         rep;     smovl
-        jmp      4f 
+        jmp      4f
         # copy aligned dwords
 2:      subl     %esi,%edi
         .p2align 4,,15
 3:      movl     (%esi),%edx
         movl     %edx,(%edi,%esi,1)
@@ -347,23 +347,23 @@
         movl     8+ 8(%esp),%edi      # to
         cmpl     %esi,%edi
         leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2
         jbe      acs_CopyRight
         cmpl     %eax,%edi
-        jbe      acs_CopyLeft 
+        jbe      acs_CopyLeft
 acs_CopyRight:
         movl     %ecx,%eax            # word count
         sarl     %ecx                 # dword count
         jz       4f                   # no dwords to move
         cmpl     $32,%ecx
         jbe      2f                   # <= 32 dwords
         # copy aligned dwords
         rep;     smovl
-        jmp      4f 
+        jmp      4f
         # copy aligned dwords
         .space 5
-2:      subl     %esi,%edi 
+2:      subl     %esi,%edi
         .p2align 4,,15
 3:      movl     (%esi),%edx
         movl     %edx,(%edi,%esi,1)
         addl     $4,%esi
         subl     $1,%ecx
@@ -426,11 +426,11 @@
         movl     8+ 8(%esp),%edi      # to
         cmpl     %esi,%edi
         leal     -4(%esi,%ecx,4),%eax # from + count*4 - 4
         jbe      ci_CopyRight
         cmpl     %eax,%edi
-        jbe      ci_CopyLeft 
+        jbe      ci_CopyLeft
 ci_CopyRight:
         cmpl     $32,%ecx
         jbe      2f                   # <= 32 dwords
         rep;     smovl
         popl     %edi
@@ -469,11 +469,11 @@
         rep;     smovl
         cld
         popl     %edi
         popl     %esi
         ret
-	
+
         # Support for void Copy::conjoint_jlongs_atomic(jlong* from,
         #                                               jlong* to,
         #                                               size_t count)
         #
         # 32-bit
@@ -535,19 +535,19 @@
         movl     %ecx,%eax
         sarl     %ecx
         je       5f
         cmpl     $33,%ecx
         jae      3f
-1:      subl     %esi,%edi 
+1:      subl     %esi,%edi
         .p2align 4,,15
 2:      movl     (%esi),%edx
         movl     %edx,(%edi,%esi,1)
         addl     $4,%esi
         subl     $1,%ecx
         jnz      2b
         addl     %esi,%edi
-        jmp      5f 
+        jmp      5f
 3:      smovl # align to 8 bytes, we know we are 4 byte aligned to start
         subl     $1,%ecx
 4:      .p2align 4,,15
         movq     0(%esi),%mm0
         addl     $64,%edi
@@ -610,13 +610,13 @@
         popl     %edi
         popl     %esi
         ret
 
 
-        # Support for jlong Atomic::cmpxchg(jlong exchange_value,
-        #                                   volatile jlong* dest,
-        #                                   jlong compare_value)
+        # Support for jlong Atomic::cmpxchg(volatile jlong* dest,
+        #                                   jlong compare_value,
+        #                                   jlong exchange_value)
         #
         .p2align 4,,15
 	.type    _Atomic_cmpxchg_long,@function
 _Atomic_cmpxchg_long:
                                    #  8(%esp) : return PC
@@ -641,6 +641,5 @@
         movl     4(%esp), %eax   # src
         fildll    (%eax)
         movl     8(%esp), %eax   # dest
         fistpll   (%eax)
         ret
-
