<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/os_cpu/linux_x86/atomic_linux_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_LINUX_X86_ATOMIC_LINUX_X86_HPP
 26 #define OS_CPU_LINUX_X86_ATOMIC_LINUX_X86_HPP
 27 
 28 // Implementation of class atomic
 29 
 30 template&lt;size_t byte_size&gt;
 31 struct Atomic::PlatformAdd
 32   : Atomic::FetchAndAdd&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;
 33 {
 34   template&lt;typename I, typename D&gt;
 35   D fetch_and_add(I add_value, D volatile* dest, atomic_memory_order order) const;
 36 };
 37 
 38 template&lt;&gt;
 39 template&lt;typename I, typename D&gt;
 40 inline D Atomic::PlatformAdd&lt;4&gt;::fetch_and_add(I add_value, D volatile* dest,
 41                                                atomic_memory_order order) const {
 42   STATIC_ASSERT(4 == sizeof(I));
 43   STATIC_ASSERT(4 == sizeof(D));
 44   D old_value;
 45   __asm__ volatile (  &quot;lock xaddl %0,(%2)&quot;
 46                     : &quot;=r&quot; (old_value)
 47                     : &quot;0&quot; (add_value), &quot;r&quot; (dest)
 48                     : &quot;cc&quot;, &quot;memory&quot;);
 49   return old_value;
 50 }
 51 
 52 template&lt;&gt;
 53 template&lt;typename T&gt;
 54 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T exchange_value,
 55                                              T volatile* dest,
 56                                              atomic_memory_order order) const {
 57   STATIC_ASSERT(4 == sizeof(T));
 58   __asm__ volatile (  &quot;xchgl (%2),%0&quot;
 59                     : &quot;=r&quot; (exchange_value)
 60                     : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
 61                     : &quot;memory&quot;);
 62   return exchange_value;
 63 }
 64 
 65 template&lt;&gt;
 66 template&lt;typename T&gt;
 67 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T exchange_value,
 68                                                 T volatile* dest,
 69                                                 T compare_value,
 70                                                 atomic_memory_order /* order */) const {
 71   STATIC_ASSERT(1 == sizeof(T));
 72   __asm__ volatile (&quot;lock cmpxchgb %1,(%3)&quot;
 73                     : &quot;=a&quot; (exchange_value)
 74                     : &quot;q&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 75                     : &quot;cc&quot;, &quot;memory&quot;);
 76   return exchange_value;
 77 }
 78 
 79 template&lt;&gt;
 80 template&lt;typename T&gt;
 81 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T exchange_value,
 82                                                 T volatile* dest,
 83                                                 T compare_value,
 84                                                 atomic_memory_order /* order */) const {
 85   STATIC_ASSERT(4 == sizeof(T));
 86   __asm__ volatile (&quot;lock cmpxchgl %1,(%3)&quot;
 87                     : &quot;=a&quot; (exchange_value)
 88                     : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 89                     : &quot;cc&quot;, &quot;memory&quot;);
 90   return exchange_value;
 91 }
 92 
 93 #ifdef AMD64
 94 
 95 template&lt;&gt;
 96 template&lt;typename I, typename D&gt;
 97 inline D Atomic::PlatformAdd&lt;8&gt;::fetch_and_add(I add_value, D volatile* dest,
 98                                                atomic_memory_order order) const {
 99   STATIC_ASSERT(8 == sizeof(I));
100   STATIC_ASSERT(8 == sizeof(D));
101   D old_value;
102   __asm__ __volatile__ (&quot;lock xaddq %0,(%2)&quot;
103                         : &quot;=r&quot; (old_value)
104                         : &quot;0&quot; (add_value), &quot;r&quot; (dest)
105                         : &quot;cc&quot;, &quot;memory&quot;);
106   return old_value;
107 }
108 
109 template&lt;&gt;
110 template&lt;typename T&gt;
111 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T exchange_value, T volatile* dest,
112                                              atomic_memory_order order) const {
113   STATIC_ASSERT(8 == sizeof(T));
114   __asm__ __volatile__ (&quot;xchgq (%2),%0&quot;
115                         : &quot;=r&quot; (exchange_value)
116                         : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
117                         : &quot;memory&quot;);
118   return exchange_value;
119 }
120 
121 template&lt;&gt;
122 template&lt;typename T&gt;
123 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,
124                                                 T volatile* dest,
125                                                 T compare_value,
126                                                 atomic_memory_order /* order */) const {
127   STATIC_ASSERT(8 == sizeof(T));
128   __asm__ __volatile__ (&quot;lock cmpxchgq %1,(%3)&quot;
129                         : &quot;=a&quot; (exchange_value)
130                         : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
131                         : &quot;cc&quot;, &quot;memory&quot;);
132   return exchange_value;
133 }
134 
135 #else // !AMD64
136 
137 extern &quot;C&quot; {
138   // defined in linux_x86.s
139   int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);
140   void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);
141 }
142 
143 template&lt;&gt;
144 template&lt;typename T&gt;
145 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,
146                                                 T volatile* dest,
147                                                 T compare_value,
148                                                 atomic_memory_order order) const {
149   STATIC_ASSERT(8 == sizeof(T));
150   return cmpxchg_using_helper&lt;int64_t&gt;(_Atomic_cmpxchg_long, exchange_value, dest, compare_value);
151 }
152 
153 template&lt;&gt;
154 template&lt;typename T&gt;
155 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
156   STATIC_ASSERT(8 == sizeof(T));
157   volatile int64_t dest;
158   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(src), reinterpret_cast&lt;volatile int64_t*&gt;(&amp;dest));
159   return PrimitiveConversions::cast&lt;T&gt;(dest);
160 }
161 
162 template&lt;&gt;
163 template&lt;typename T&gt;
164 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T store_value,
165                                                  T volatile* dest) const {
166   STATIC_ASSERT(8 == sizeof(T));
167   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(&amp;store_value), reinterpret_cast&lt;volatile int64_t*&gt;(dest));
168 }
169 
170 #endif // AMD64
171 
172 #endif // OS_CPU_LINUX_X86_ATOMIC_LINUX_X86_HPP
    </pre>
  </body>
</html>