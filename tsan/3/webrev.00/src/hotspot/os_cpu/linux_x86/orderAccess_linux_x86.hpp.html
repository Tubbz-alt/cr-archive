<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/os_cpu/linux_x86/orderAccess_linux_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
 1 /*
 2  * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
 3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 4  *
 5  * This code is free software; you can redistribute it and/or modify it
 6  * under the terms of the GNU General Public License version 2 only, as
 7  * published by the Free Software Foundation.
 8  *
 9  * This code is distributed in the hope that it will be useful, but WITHOUT
10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
12  * version 2 for more details (a copy is included in the LICENSE file that
13  * accompanied this code).
14  *
15  * You should have received a copy of the GNU General Public License version
16  * 2 along with this work; if not, write to the Free Software Foundation,
17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
18  *
19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
20  * or visit www.oracle.com if you need additional information or have any
21  * questions.
22  *
23  */
24 
25 #ifndef OS_CPU_LINUX_X86_ORDERACCESS_LINUX_X86_HPP
26 #define OS_CPU_LINUX_X86_ORDERACCESS_LINUX_X86_HPP
27 
28 // Included in orderAccess.hpp header file.
29 
30 // Compiler version last used for testing: gcc 4.8.2
31 // Please update this information when this file changes
32 
33 // Implementation of class OrderAccess.
34 
35 // A compiler barrier, forcing the C++ compiler to invalidate all memory assumptions
36 static inline void compiler_barrier() {
37   __asm__ volatile (&quot;&quot; : : : &quot;memory&quot;);
38 }
39 
40 inline void OrderAccess::loadload()   { compiler_barrier(); }
41 inline void OrderAccess::storestore() { compiler_barrier(); }
42 inline void OrderAccess::loadstore()  { compiler_barrier(); }
43 inline void OrderAccess::storeload()  { fence();            }
44 
45 inline void OrderAccess::acquire()    { compiler_barrier(); }
46 inline void OrderAccess::release()    { compiler_barrier(); }
47 
48 inline void OrderAccess::fence() {
49    // always use locked addl since mfence is sometimes expensive
50 #ifdef AMD64
51   __asm__ volatile (&quot;lock; addl $0,0(%%rsp)&quot; : : : &quot;cc&quot;, &quot;memory&quot;);
52 #else
53   __asm__ volatile (&quot;lock; addl $0,0(%%esp)&quot; : : : &quot;cc&quot;, &quot;memory&quot;);
54 #endif
55   compiler_barrier();
56 }
57 
58 inline void OrderAccess::cross_modify_fence() {
59   int idx = 0;
60 #ifdef AMD64
61   __asm__ volatile (&quot;cpuid &quot; : &quot;+a&quot; (idx) : : &quot;ebx&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot;);
62 #else
63   // On some x86 systems EBX is a reserved register that cannot be
64   // clobbered, so we must protect it around the CPUID.
65   __asm__ volatile (&quot;xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx &quot; : &quot;+a&quot; (idx) : : &quot;esi&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot;);
66 #endif
67 }
68 
69 #endif // OS_CPU_LINUX_X86_ORDERACCESS_LINUX_X86_HPP
    </pre>
  </body>
</html>