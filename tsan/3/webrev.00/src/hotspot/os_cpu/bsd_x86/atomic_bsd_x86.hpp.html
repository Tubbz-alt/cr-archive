<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/os_cpu/bsd_x86/atomic_bsd_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
 26 #define OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
 27 
 28 // Implementation of class atomic
 29 
 30 template&lt;size_t byte_size&gt;
 31 struct Atomic::PlatformAdd {
 32   template&lt;typename D, typename I&gt;
 33   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order /* order */) const;
 34 
 35   template&lt;typename D, typename I&gt;
 36   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const {
 37     return fetch_and_add(dest, add_value, order) + add_value;
 38   }
 39 };
 40 
 41 template&lt;&gt;
 42 template&lt;typename D, typename I&gt;
 43 inline D Atomic::PlatformAdd&lt;4&gt;::fetch_and_add(D volatile* dest, I add_value,
 44                                                atomic_memory_order /* order */) const {
 45   STATIC_ASSERT(4 == sizeof(I));
 46   STATIC_ASSERT(4 == sizeof(D));
 47   D old_value;
 48   __asm__ volatile (  &quot;lock xaddl %0,(%2)&quot;
 49                     : &quot;=r&quot; (old_value)
 50                     : &quot;0&quot; (add_value), &quot;r&quot; (dest)
 51                     : &quot;cc&quot;, &quot;memory&quot;);
 52   return old_value;
 53 }
 54 
 55 template&lt;&gt;
 56 template&lt;typename T&gt;
 57 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,
 58                                              T exchange_value,
 59                                              atomic_memory_order /* order */) const {
 60   STATIC_ASSERT(4 == sizeof(T));
 61   __asm__ volatile (  &quot;xchgl (%2),%0&quot;
 62                     : &quot;=r&quot; (exchange_value)
 63                     : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
 64                     : &quot;memory&quot;);
 65   return exchange_value;
 66 }
 67 
 68 template&lt;&gt;
 69 template&lt;typename T&gt;
 70 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,
 71                                                 T compare_value,
 72                                                 T exchange_value,
 73                                                 atomic_memory_order /* order */) const {
 74   STATIC_ASSERT(1 == sizeof(T));
 75   __asm__ volatile (  &quot;lock cmpxchgb %1,(%3)&quot;
 76                     : &quot;=a&quot; (exchange_value)
 77                     : &quot;q&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 78                     : &quot;cc&quot;, &quot;memory&quot;);
 79   return exchange_value;
 80 }
 81 
 82 template&lt;&gt;
 83 template&lt;typename T&gt;
 84 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,
 85                                                 T compare_value,
 86                                                 T exchange_value,
 87                                                 atomic_memory_order /* order */) const {
 88   STATIC_ASSERT(4 == sizeof(T));
 89   __asm__ volatile (  &quot;lock cmpxchgl %1,(%3)&quot;
 90                     : &quot;=a&quot; (exchange_value)
 91                     : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 92                     : &quot;cc&quot;, &quot;memory&quot;);
 93   return exchange_value;
 94 }
 95 
 96 #ifdef AMD64
 97 template&lt;&gt;
 98 template&lt;typename D, typename I&gt;
 99 inline D Atomic::PlatformAdd&lt;8&gt;::fetch_and_add(D volatile* dest, I add_value,
100                                                atomic_memory_order /* order */) const {
101   STATIC_ASSERT(8 == sizeof(I));
102   STATIC_ASSERT(8 == sizeof(D));
103   D old_value;
104   __asm__ __volatile__ (  &quot;lock xaddq %0,(%2)&quot;
105                         : &quot;=r&quot; (old_value)
106                         : &quot;0&quot; (add_value), &quot;r&quot; (dest)
107                         : &quot;cc&quot;, &quot;memory&quot;);
108   return old_value;
109 }
110 
111 template&lt;&gt;
112 template&lt;typename T&gt;
113 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T volatile* dest,
114                                              T exchange_value,
115                                              atomic_memory_order /* order */) const {
116   STATIC_ASSERT(8 == sizeof(T));
117   __asm__ __volatile__ (&quot;xchgq (%2),%0&quot;
118                         : &quot;=r&quot; (exchange_value)
119                         : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
120                         : &quot;memory&quot;);
121   return exchange_value;
122 }
123 
124 template&lt;&gt;
125 template&lt;typename T&gt;
126 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,
127                                                 T compare_value,
128                                                 T exchange_value,
129                                                 atomic_memory_order /* order */) const {
130   STATIC_ASSERT(8 == sizeof(T));
131   __asm__ __volatile__ (  &quot;lock cmpxchgq %1,(%3)&quot;
132                         : &quot;=a&quot; (exchange_value)
133                         : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
134                         : &quot;cc&quot;, &quot;memory&quot;);
135   return exchange_value;
136 }
137 
138 #else // !AMD64
139 
140 extern &quot;C&quot; {
141   // defined in bsd_x86.s
142   int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);
143   void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);
144 }
145 
146 template&lt;&gt;
147 template&lt;typename T&gt;
148 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,
149                                                 T compare_value,
150                                                 T exchange_value,
151                                                 atomic_memory_order /* order */) const {
152   STATIC_ASSERT(8 == sizeof(T));
153   return cmpxchg_using_helper&lt;int64_t&gt;(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);
154 }
155 
156 template&lt;&gt;
157 template&lt;typename T&gt;
158 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
159   STATIC_ASSERT(8 == sizeof(T));
160   volatile int64_t dest;
161   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(src), reinterpret_cast&lt;volatile int64_t*&gt;(&amp;dest));
162   return PrimitiveConversions::cast&lt;T&gt;(dest);
163 }
164 
165 template&lt;&gt;
166 template&lt;typename T&gt;
167 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
168                                                  T store_value) const {
169   STATIC_ASSERT(8 == sizeof(T));
170   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(&amp;store_value), reinterpret_cast&lt;volatile int64_t*&gt;(dest));
171 }
172 
173 #endif // AMD64
174 
175 template&lt;&gt;
176 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
177 {
178   template &lt;typename T&gt;
179   void operator()(volatile T* p, T v) const {
180     __asm__ volatile (  &quot;xchgb (%2),%0&quot;
181                       : &quot;=q&quot; (v)
182                       : &quot;0&quot; (v), &quot;r&quot; (p)
183                       : &quot;memory&quot;);
184   }
185 };
186 
187 template&lt;&gt;
188 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
189 {
190   template &lt;typename T&gt;
191   void operator()(volatile T* p, T v) const {
192     __asm__ volatile (  &quot;xchgw (%2),%0&quot;
193                       : &quot;=r&quot; (v)
194                       : &quot;0&quot; (v), &quot;r&quot; (p)
195                       : &quot;memory&quot;);
196   }
197 };
198 
199 template&lt;&gt;
200 struct Atomic::PlatformOrderedStore&lt;4, RELEASE_X_FENCE&gt;
201 {
202   template &lt;typename T&gt;
203   void operator()(volatile T* p, T v) const {
204     __asm__ volatile (  &quot;xchgl (%2),%0&quot;
205                       : &quot;=r&quot; (v)
206                       : &quot;0&quot; (v), &quot;r&quot; (p)
207                       : &quot;memory&quot;);
208   }
209 };
210 
211 #ifdef AMD64
212 template&lt;&gt;
213 struct Atomic::PlatformOrderedStore&lt;8, RELEASE_X_FENCE&gt;
214 {
215   template &lt;typename T&gt;
216   void operator()(volatile T* p, T v) const {
217     __asm__ volatile (  &quot;xchgq (%2), %0&quot;
218                       : &quot;=r&quot; (v)
219                       : &quot;0&quot; (v), &quot;r&quot; (p)
220                       : &quot;memory&quot;);
221   }
222 };
223 #endif // AMD64
224 
225 #endif // OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
    </pre>
  </body>
</html>