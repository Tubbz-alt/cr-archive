<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/os_cpu/bsd_x86/orderAccess_bsd_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_BSD_X86_ORDERACCESS_BSD_X86_HPP
 26 #define OS_CPU_BSD_X86_ORDERACCESS_BSD_X86_HPP
 27 
 28 // Included in orderAccess.hpp header file.
 29 
 30 // Compiler version last used for testing: clang 5.1
 31 // Please update this information when this file changes
 32 
 33 // A compiler barrier, forcing the C++ compiler to invalidate all memory assumptions
 34 static inline void compiler_barrier() {
 35   __asm__ volatile (&quot;&quot; : : : &quot;memory&quot;);
 36 }
 37 
 38 // x86 is TSO and hence only needs a fence for storeload
 39 // However, a compiler barrier is still needed to prevent reordering
 40 // between volatile and non-volatile memory accesses.
 41 
 42 // Implementation of class OrderAccess.
 43 
 44 inline void OrderAccess::loadload()   { compiler_barrier(); }
 45 inline void OrderAccess::storestore() { compiler_barrier(); }
 46 inline void OrderAccess::loadstore()  { compiler_barrier(); }
 47 inline void OrderAccess::storeload()  { fence();            }
 48 
 49 inline void OrderAccess::acquire()    { compiler_barrier(); }
 50 inline void OrderAccess::release()    { compiler_barrier(); }
 51 
 52 inline void OrderAccess::fence() {
 53   // always use locked addl since mfence is sometimes expensive
 54 #ifdef AMD64
 55   __asm__ volatile (&quot;lock; addl $0,0(%%rsp)&quot; : : : &quot;cc&quot;, &quot;memory&quot;);
 56 #else
 57   __asm__ volatile (&quot;lock; addl $0,0(%%esp)&quot; : : : &quot;cc&quot;, &quot;memory&quot;);
 58 #endif
 59   compiler_barrier();
 60 }
 61 
 62 template&lt;&gt;
 63 struct OrderAccess::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
 64 {
 65   template &lt;typename T&gt;
 66   void operator()(T v, volatile T* p) const {
 67     __asm__ volatile (  &quot;xchgb (%2),%0&quot;
 68                       : &quot;=q&quot; (v)
 69                       : &quot;0&quot; (v), &quot;r&quot; (p)
 70                       : &quot;memory&quot;);
 71   }
 72 };
 73 
 74 template&lt;&gt;
 75 struct OrderAccess::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
 76 {
 77   template &lt;typename T&gt;
 78   void operator()(T v, volatile T* p) const {
 79     __asm__ volatile (  &quot;xchgw (%2),%0&quot;
 80                       : &quot;=r&quot; (v)
 81                       : &quot;0&quot; (v), &quot;r&quot; (p)
 82                       : &quot;memory&quot;);
 83   }
 84 };
 85 
 86 template&lt;&gt;
 87 struct OrderAccess::PlatformOrderedStore&lt;4, RELEASE_X_FENCE&gt;
 88 {
 89   template &lt;typename T&gt;
 90   void operator()(T v, volatile T* p) const {
 91     __asm__ volatile (  &quot;xchgl (%2),%0&quot;
 92                       : &quot;=r&quot; (v)
 93                       : &quot;0&quot; (v), &quot;r&quot; (p)
 94                       : &quot;memory&quot;);
 95   }
 96 };
 97 
 98 #ifdef AMD64
 99 template&lt;&gt;
100 struct OrderAccess::PlatformOrderedStore&lt;8, RELEASE_X_FENCE&gt;
101 {
102   template &lt;typename T&gt;
103   void operator()(T v, volatile T* p) const {
104     __asm__ volatile (  &quot;xchgq (%2), %0&quot;
105                       : &quot;=r&quot; (v)
106                       : &quot;0&quot; (v), &quot;r&quot; (p)
107                       : &quot;memory&quot;);
108   }
109 };
110 #endif // AMD64
111 
112 #endif // OS_CPU_BSD_X86_ORDERACCESS_BSD_X86_HPP
    </pre>
  </body>
</html>