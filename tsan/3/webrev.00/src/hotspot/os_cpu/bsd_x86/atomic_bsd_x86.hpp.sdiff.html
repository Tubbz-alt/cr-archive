<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/os_cpu/bsd_x86/atomic_bsd_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../aix_ppc/os_aix_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="bsd_x86_32.s.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os_cpu/bsd_x86/atomic_bsd_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
 26 #define OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
 27 
 28 // Implementation of class atomic
 29 
 30 template&lt;size_t byte_size&gt;
<span class="line-modified"> 31 struct Atomic::PlatformAdd</span>
<span class="line-modified"> 32   : Atomic::FetchAndAdd&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;</span>
<span class="line-modified"> 33 {</span>
<span class="line-modified"> 34   template&lt;typename I, typename D&gt;</span>
<span class="line-modified"> 35   D fetch_and_add(I add_value, D volatile* dest, atomic_memory_order /* order */) const;</span>



 36 };
 37 
 38 template&lt;&gt;
<span class="line-modified"> 39 template&lt;typename I, typename D&gt;</span>
<span class="line-modified"> 40 inline D Atomic::PlatformAdd&lt;4&gt;::fetch_and_add(I add_value, D volatile* dest,</span>
 41                                                atomic_memory_order /* order */) const {
 42   STATIC_ASSERT(4 == sizeof(I));
 43   STATIC_ASSERT(4 == sizeof(D));
 44   D old_value;
 45   __asm__ volatile (  &quot;lock xaddl %0,(%2)&quot;
 46                     : &quot;=r&quot; (old_value)
 47                     : &quot;0&quot; (add_value), &quot;r&quot; (dest)
 48                     : &quot;cc&quot;, &quot;memory&quot;);
 49   return old_value;
 50 }
 51 
 52 template&lt;&gt;
 53 template&lt;typename T&gt;
<span class="line-modified"> 54 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-modified"> 55                                              T volatile* dest,</span>
 56                                              atomic_memory_order /* order */) const {
 57   STATIC_ASSERT(4 == sizeof(T));
 58   __asm__ volatile (  &quot;xchgl (%2),%0&quot;
 59                     : &quot;=r&quot; (exchange_value)
 60                     : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
 61                     : &quot;memory&quot;);
 62   return exchange_value;
 63 }
 64 
 65 template&lt;&gt;
 66 template&lt;typename T&gt;
<span class="line-modified"> 67 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T exchange_value,</span>
<span class="line-removed"> 68                                                 T volatile* dest,</span>
 69                                                 T compare_value,

 70                                                 atomic_memory_order /* order */) const {
 71   STATIC_ASSERT(1 == sizeof(T));
 72   __asm__ volatile (  &quot;lock cmpxchgb %1,(%3)&quot;
 73                     : &quot;=a&quot; (exchange_value)
 74                     : &quot;q&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 75                     : &quot;cc&quot;, &quot;memory&quot;);
 76   return exchange_value;
 77 }
 78 
 79 template&lt;&gt;
 80 template&lt;typename T&gt;
<span class="line-modified"> 81 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-removed"> 82                                                 T volatile* dest,</span>
 83                                                 T compare_value,

 84                                                 atomic_memory_order /* order */) const {
 85   STATIC_ASSERT(4 == sizeof(T));
 86   __asm__ volatile (  &quot;lock cmpxchgl %1,(%3)&quot;
 87                     : &quot;=a&quot; (exchange_value)
 88                     : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 89                     : &quot;cc&quot;, &quot;memory&quot;);
 90   return exchange_value;
 91 }
 92 
 93 #ifdef AMD64
 94 template&lt;&gt;
<span class="line-modified"> 95 template&lt;typename I, typename D&gt;</span>
<span class="line-modified"> 96 inline D Atomic::PlatformAdd&lt;8&gt;::fetch_and_add(I add_value, D volatile* dest,</span>
 97                                                atomic_memory_order /* order */) const {
 98   STATIC_ASSERT(8 == sizeof(I));
 99   STATIC_ASSERT(8 == sizeof(D));
100   D old_value;
101   __asm__ __volatile__ (  &quot;lock xaddq %0,(%2)&quot;
102                         : &quot;=r&quot; (old_value)
103                         : &quot;0&quot; (add_value), &quot;r&quot; (dest)
104                         : &quot;cc&quot;, &quot;memory&quot;);
105   return old_value;
106 }
107 
108 template&lt;&gt;
109 template&lt;typename T&gt;
<span class="line-modified">110 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-modified">111                                              T volatile* dest,</span>
112                                              atomic_memory_order /* order */) const {
113   STATIC_ASSERT(8 == sizeof(T));
114   __asm__ __volatile__ (&quot;xchgq (%2),%0&quot;
115                         : &quot;=r&quot; (exchange_value)
116                         : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
117                         : &quot;memory&quot;);
118   return exchange_value;
119 }
120 
121 template&lt;&gt;
122 template&lt;typename T&gt;
<span class="line-modified">123 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-removed">124                                                 T volatile* dest,</span>
125                                                 T compare_value,

126                                                 atomic_memory_order /* order */) const {
127   STATIC_ASSERT(8 == sizeof(T));
128   __asm__ __volatile__ (  &quot;lock cmpxchgq %1,(%3)&quot;
129                         : &quot;=a&quot; (exchange_value)
130                         : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
131                         : &quot;cc&quot;, &quot;memory&quot;);
132   return exchange_value;
133 }
134 
135 #else // !AMD64
136 
137 extern &quot;C&quot; {
138   // defined in bsd_x86.s
139   int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);
140   void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);
141 }
142 
143 template&lt;&gt;
144 template&lt;typename T&gt;
<span class="line-modified">145 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-removed">146                                                 T volatile* dest,</span>
147                                                 T compare_value,

148                                                 atomic_memory_order /* order */) const {
149   STATIC_ASSERT(8 == sizeof(T));
<span class="line-modified">150   return cmpxchg_using_helper&lt;int64_t&gt;(_Atomic_cmpxchg_long, exchange_value, dest, compare_value);</span>
151 }
152 
153 template&lt;&gt;
154 template&lt;typename T&gt;
155 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
156   STATIC_ASSERT(8 == sizeof(T));
157   volatile int64_t dest;
158   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(src), reinterpret_cast&lt;volatile int64_t*&gt;(&amp;dest));
159   return PrimitiveConversions::cast&lt;T&gt;(dest);
160 }
161 
162 template&lt;&gt;
163 template&lt;typename T&gt;
<span class="line-modified">164 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T store_value,</span>
<span class="line-modified">165                                                  T volatile* dest) const {</span>
166   STATIC_ASSERT(8 == sizeof(T));
167   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(&amp;store_value), reinterpret_cast&lt;volatile int64_t*&gt;(dest));
168 }
169 
170 #endif // AMD64
171 


















































172 #endif // OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
</pre>
</td>
<td>
<hr />
<pre>
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
 26 #define OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
 27 
 28 // Implementation of class atomic
 29 
 30 template&lt;size_t byte_size&gt;
<span class="line-modified"> 31 struct Atomic::PlatformAdd {</span>
<span class="line-modified"> 32   template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 33   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order /* order */) const;</span>
<span class="line-modified"> 34 </span>
<span class="line-modified"> 35   template&lt;typename D, typename I&gt;</span>
<span class="line-added"> 36   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const {</span>
<span class="line-added"> 37     return fetch_and_add(dest, add_value, order) + add_value;</span>
<span class="line-added"> 38   }</span>
 39 };
 40 
 41 template&lt;&gt;
<span class="line-modified"> 42 template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 43 inline D Atomic::PlatformAdd&lt;4&gt;::fetch_and_add(D volatile* dest, I add_value,</span>
 44                                                atomic_memory_order /* order */) const {
 45   STATIC_ASSERT(4 == sizeof(I));
 46   STATIC_ASSERT(4 == sizeof(D));
 47   D old_value;
 48   __asm__ volatile (  &quot;lock xaddl %0,(%2)&quot;
 49                     : &quot;=r&quot; (old_value)
 50                     : &quot;0&quot; (add_value), &quot;r&quot; (dest)
 51                     : &quot;cc&quot;, &quot;memory&quot;);
 52   return old_value;
 53 }
 54 
 55 template&lt;&gt;
 56 template&lt;typename T&gt;
<span class="line-modified"> 57 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-modified"> 58                                              T exchange_value,</span>
 59                                              atomic_memory_order /* order */) const {
 60   STATIC_ASSERT(4 == sizeof(T));
 61   __asm__ volatile (  &quot;xchgl (%2),%0&quot;
 62                     : &quot;=r&quot; (exchange_value)
 63                     : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
 64                     : &quot;memory&quot;);
 65   return exchange_value;
 66 }
 67 
 68 template&lt;&gt;
 69 template&lt;typename T&gt;
<span class="line-modified"> 70 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>

 71                                                 T compare_value,
<span class="line-added"> 72                                                 T exchange_value,</span>
 73                                                 atomic_memory_order /* order */) const {
 74   STATIC_ASSERT(1 == sizeof(T));
 75   __asm__ volatile (  &quot;lock cmpxchgb %1,(%3)&quot;
 76                     : &quot;=a&quot; (exchange_value)
 77                     : &quot;q&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 78                     : &quot;cc&quot;, &quot;memory&quot;);
 79   return exchange_value;
 80 }
 81 
 82 template&lt;&gt;
 83 template&lt;typename T&gt;
<span class="line-modified"> 84 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>

 85                                                 T compare_value,
<span class="line-added"> 86                                                 T exchange_value,</span>
 87                                                 atomic_memory_order /* order */) const {
 88   STATIC_ASSERT(4 == sizeof(T));
 89   __asm__ volatile (  &quot;lock cmpxchgl %1,(%3)&quot;
 90                     : &quot;=a&quot; (exchange_value)
 91                     : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
 92                     : &quot;cc&quot;, &quot;memory&quot;);
 93   return exchange_value;
 94 }
 95 
 96 #ifdef AMD64
 97 template&lt;&gt;
<span class="line-modified"> 98 template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 99 inline D Atomic::PlatformAdd&lt;8&gt;::fetch_and_add(D volatile* dest, I add_value,</span>
100                                                atomic_memory_order /* order */) const {
101   STATIC_ASSERT(8 == sizeof(I));
102   STATIC_ASSERT(8 == sizeof(D));
103   D old_value;
104   __asm__ __volatile__ (  &quot;lock xaddq %0,(%2)&quot;
105                         : &quot;=r&quot; (old_value)
106                         : &quot;0&quot; (add_value), &quot;r&quot; (dest)
107                         : &quot;cc&quot;, &quot;memory&quot;);
108   return old_value;
109 }
110 
111 template&lt;&gt;
112 template&lt;typename T&gt;
<span class="line-modified">113 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">114                                              T exchange_value,</span>
115                                              atomic_memory_order /* order */) const {
116   STATIC_ASSERT(8 == sizeof(T));
117   __asm__ __volatile__ (&quot;xchgq (%2),%0&quot;
118                         : &quot;=r&quot; (exchange_value)
119                         : &quot;0&quot; (exchange_value), &quot;r&quot; (dest)
120                         : &quot;memory&quot;);
121   return exchange_value;
122 }
123 
124 template&lt;&gt;
125 template&lt;typename T&gt;
<span class="line-modified">126 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>

127                                                 T compare_value,
<span class="line-added">128                                                 T exchange_value,</span>
129                                                 atomic_memory_order /* order */) const {
130   STATIC_ASSERT(8 == sizeof(T));
131   __asm__ __volatile__ (  &quot;lock cmpxchgq %1,(%3)&quot;
132                         : &quot;=a&quot; (exchange_value)
133                         : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest)
134                         : &quot;cc&quot;, &quot;memory&quot;);
135   return exchange_value;
136 }
137 
138 #else // !AMD64
139 
140 extern &quot;C&quot; {
141   // defined in bsd_x86.s
142   int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);
143   void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);
144 }
145 
146 template&lt;&gt;
147 template&lt;typename T&gt;
<span class="line-modified">148 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>

149                                                 T compare_value,
<span class="line-added">150                                                 T exchange_value,</span>
151                                                 atomic_memory_order /* order */) const {
152   STATIC_ASSERT(8 == sizeof(T));
<span class="line-modified">153   return cmpxchg_using_helper&lt;int64_t&gt;(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);</span>
154 }
155 
156 template&lt;&gt;
157 template&lt;typename T&gt;
158 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
159   STATIC_ASSERT(8 == sizeof(T));
160   volatile int64_t dest;
161   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(src), reinterpret_cast&lt;volatile int64_t*&gt;(&amp;dest));
162   return PrimitiveConversions::cast&lt;T&gt;(dest);
163 }
164 
165 template&lt;&gt;
166 template&lt;typename T&gt;
<span class="line-modified">167 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">168                                                  T store_value) const {</span>
169   STATIC_ASSERT(8 == sizeof(T));
170   _Atomic_move_long(reinterpret_cast&lt;const volatile int64_t*&gt;(&amp;store_value), reinterpret_cast&lt;volatile int64_t*&gt;(dest));
171 }
172 
173 #endif // AMD64
174 
<span class="line-added">175 template&lt;&gt;</span>
<span class="line-added">176 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;</span>
<span class="line-added">177 {</span>
<span class="line-added">178   template &lt;typename T&gt;</span>
<span class="line-added">179   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">180     __asm__ volatile (  &quot;xchgb (%2),%0&quot;</span>
<span class="line-added">181                       : &quot;=q&quot; (v)</span>
<span class="line-added">182                       : &quot;0&quot; (v), &quot;r&quot; (p)</span>
<span class="line-added">183                       : &quot;memory&quot;);</span>
<span class="line-added">184   }</span>
<span class="line-added">185 };</span>
<span class="line-added">186 </span>
<span class="line-added">187 template&lt;&gt;</span>
<span class="line-added">188 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;</span>
<span class="line-added">189 {</span>
<span class="line-added">190   template &lt;typename T&gt;</span>
<span class="line-added">191   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">192     __asm__ volatile (  &quot;xchgw (%2),%0&quot;</span>
<span class="line-added">193                       : &quot;=r&quot; (v)</span>
<span class="line-added">194                       : &quot;0&quot; (v), &quot;r&quot; (p)</span>
<span class="line-added">195                       : &quot;memory&quot;);</span>
<span class="line-added">196   }</span>
<span class="line-added">197 };</span>
<span class="line-added">198 </span>
<span class="line-added">199 template&lt;&gt;</span>
<span class="line-added">200 struct Atomic::PlatformOrderedStore&lt;4, RELEASE_X_FENCE&gt;</span>
<span class="line-added">201 {</span>
<span class="line-added">202   template &lt;typename T&gt;</span>
<span class="line-added">203   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">204     __asm__ volatile (  &quot;xchgl (%2),%0&quot;</span>
<span class="line-added">205                       : &quot;=r&quot; (v)</span>
<span class="line-added">206                       : &quot;0&quot; (v), &quot;r&quot; (p)</span>
<span class="line-added">207                       : &quot;memory&quot;);</span>
<span class="line-added">208   }</span>
<span class="line-added">209 };</span>
<span class="line-added">210 </span>
<span class="line-added">211 #ifdef AMD64</span>
<span class="line-added">212 template&lt;&gt;</span>
<span class="line-added">213 struct Atomic::PlatformOrderedStore&lt;8, RELEASE_X_FENCE&gt;</span>
<span class="line-added">214 {</span>
<span class="line-added">215   template &lt;typename T&gt;</span>
<span class="line-added">216   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">217     __asm__ volatile (  &quot;xchgq (%2), %0&quot;</span>
<span class="line-added">218                       : &quot;=r&quot; (v)</span>
<span class="line-added">219                       : &quot;0&quot; (v), &quot;r&quot; (p)</span>
<span class="line-added">220                       : &quot;memory&quot;);</span>
<span class="line-added">221   }</span>
<span class="line-added">222 };</span>
<span class="line-added">223 #endif // AMD64</span>
<span class="line-added">224 </span>
225 #endif // OS_CPU_BSD_X86_ATOMIC_BSD_X86_HPP
</pre>
</td>
</tr>
</table>
<center><a href="../aix_ppc/os_aix_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="bsd_x86_32.s.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>