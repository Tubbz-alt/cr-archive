<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/os_cpu/aix_ppc/atomic_aix_ppc.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * Copyright (c) 2012, 2019 SAP SE. All rights reserved.
  4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  5  *
  6  * This code is free software; you can redistribute it and/or modify it
  7  * under the terms of the GNU General Public License version 2 only, as
  8  * published by the Free Software Foundation.
  9  *
 10  * This code is distributed in the hope that it will be useful, but WITHOUT
 11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #ifndef OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
 27 #define OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
 28 
 29 #ifndef PPC64
 30 #error &quot;Atomic currently only implemented for PPC64&quot;
 31 #endif
 32 
<a name="1" id="anc1"></a><span class="line-added"> 33 #include &quot;orderAccess_aix_ppc.hpp&quot;</span>
 34 #include &quot;utilities/debug.hpp&quot;
 35 
 36 // Implementation of class atomic
 37 
 38 //
 39 // machine barrier instructions:
 40 //
 41 // - sync            two-way memory barrier, aka fence
 42 // - lwsync          orders  Store|Store,
 43 //                            Load|Store,
 44 //                            Load|Load,
 45 //                   but not Store|Load
 46 // - eieio           orders memory accesses for device memory (only)
 47 // - isync           invalidates speculatively executed instructions
 48 //                   From the POWER ISA 2.06 documentation:
 49 //                    &quot;[...] an isync instruction prevents the execution of
 50 //                   instructions following the isync until instructions
 51 //                   preceding the isync have completed, [...]&quot;
 52 //                   From IBM&#39;s AIX assembler reference:
 53 //                    &quot;The isync [...] instructions causes the processor to
 54 //                   refetch any instructions that might have been fetched
 55 //                   prior to the isync instruction. The instruction isync
 56 //                   causes the processor to wait for all previous instructions
 57 //                   to complete. Then any instructions already fetched are
 58 //                   discarded and instruction processing continues in the
 59 //                   environment established by the previous instructions.&quot;
 60 //
 61 // semantic barrier instructions:
 62 // (as defined in orderAccess.hpp)
 63 //
 64 // - release         orders Store|Store,       (maps to lwsync)
 65 //                           Load|Store
 66 // - acquire         orders  Load|Store,       (maps to lwsync)
 67 //                           Load|Load
 68 // - fence           orders Store|Store,       (maps to sync)
 69 //                           Load|Store,
 70 //                           Load|Load,
 71 //                          Store|Load
 72 //
 73 
 74 inline void pre_membar(atomic_memory_order order) {
 75   switch (order) {
 76     case memory_order_relaxed:
 77     case memory_order_acquire: break;
 78     case memory_order_release:
 79     case memory_order_acq_rel: __asm__ __volatile__ (&quot;lwsync&quot; : : : &quot;memory&quot;); break;
 80     default /*conservative*/ : __asm__ __volatile__ (&quot;sync&quot;   : : : &quot;memory&quot;); break;
 81   }
 82 }
 83 
 84 inline void post_membar(atomic_memory_order order) {
 85   switch (order) {
 86     case memory_order_relaxed:
 87     case memory_order_release: break;
 88     case memory_order_acquire:
 89     case memory_order_acq_rel: __asm__ __volatile__ (&quot;isync&quot;  : : : &quot;memory&quot;); break;
 90     default /*conservative*/ : __asm__ __volatile__ (&quot;sync&quot;   : : : &quot;memory&quot;); break;
 91   }
 92 }
 93 
 94 
 95 template&lt;size_t byte_size&gt;
<a name="2" id="anc2"></a><span class="line-modified"> 96 struct Atomic::PlatformAdd {</span>
<span class="line-modified"> 97   template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 98   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;</span>
<span class="line-modified"> 99 </span>
<span class="line-modified">100   template&lt;typename D, typename I&gt;</span>
<span class="line-added">101   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {</span>
<span class="line-added">102     return add_and_fetch(dest, add_value, order) - add_value;</span>
<span class="line-added">103   }</span>
104 };
105 
106 template&lt;&gt;
<a name="3" id="anc3"></a><span class="line-modified">107 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">108 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
109                                                atomic_memory_order order) const {
110   STATIC_ASSERT(4 == sizeof(I));
111   STATIC_ASSERT(4 == sizeof(D));
112 
113   D result;
114 
115   pre_membar(order);
116 
117   __asm__ __volatile__ (
118     &quot;1: lwarx   %0,  0, %2    \n&quot;
119     &quot;   add     %0, %0, %1    \n&quot;
120     &quot;   stwcx.  %0,  0, %2    \n&quot;
121     &quot;   bne-    1b            \n&quot;
122     : /*%0*/&quot;=&amp;r&quot; (result)
123     : /*%1*/&quot;r&quot; (add_value), /*%2*/&quot;r&quot; (dest)
124     : &quot;cc&quot;, &quot;memory&quot; );
125 
126   post_membar(order);
127 
128   return result;
129 }
130 
131 
132 template&lt;&gt;
<a name="4" id="anc4"></a><span class="line-modified">133 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">134 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
135                                                atomic_memory_order order) const {
136   STATIC_ASSERT(8 == sizeof(I));
137   STATIC_ASSERT(8 == sizeof(D));
138 
139   D result;
140 
141   pre_membar(order);
142 
143   __asm__ __volatile__ (
144     &quot;1: ldarx   %0,  0, %2    \n&quot;
145     &quot;   add     %0, %0, %1    \n&quot;
146     &quot;   stdcx.  %0,  0, %2    \n&quot;
147     &quot;   bne-    1b            \n&quot;
148     : /*%0*/&quot;=&amp;r&quot; (result)
149     : /*%1*/&quot;r&quot; (add_value), /*%2*/&quot;r&quot; (dest)
150     : &quot;cc&quot;, &quot;memory&quot; );
151 
152   post_membar(order);
153 
154   return result;
155 }
156 
157 template&lt;&gt;
158 template&lt;typename T&gt;
<a name="5" id="anc5"></a><span class="line-modified">159 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">160                                              T exchange_value,</span>
161                                              atomic_memory_order order) const {
162   // Note that xchg doesn&#39;t necessarily do an acquire
163   // (see synchronizer.cpp).
164 
165   T old_value;
166   const uint64_t zero = 0;
167 
168   pre_membar(order);
169 
170   __asm__ __volatile__ (
171     /* atomic loop */
172     &quot;1:                                                 \n&quot;
173     &quot;   lwarx   %[old_value], %[dest], %[zero]          \n&quot;
174     &quot;   stwcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
175     &quot;   bne-    1b                                      \n&quot;
176     /* exit */
177     &quot;2:                                                 \n&quot;
178     /* out */
179     : [old_value]       &quot;=&amp;r&quot;   (old_value),
180                         &quot;=m&quot;    (*dest)
181     /* in */
182     : [dest]            &quot;b&quot;     (dest),
183       [zero]            &quot;r&quot;     (zero),
184       [exchange_value]  &quot;r&quot;     (exchange_value),
185                         &quot;m&quot;     (*dest)
186     /* clobber */
187     : &quot;cc&quot;,
188       &quot;memory&quot;
189     );
190 
191   post_membar(order);
192 
193   return old_value;
194 }
195 
196 template&lt;&gt;
197 template&lt;typename T&gt;
<a name="6" id="anc6"></a><span class="line-modified">198 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">199                                              T exchange_value,</span>
200                                              atomic_memory_order order) const {
201   STATIC_ASSERT(8 == sizeof(T));
202   // Note that xchg doesn&#39;t necessarily do an acquire
203   // (see synchronizer.cpp).
204 
205   T old_value;
206   const uint64_t zero = 0;
207 
208   pre_membar(order);
209 
210   __asm__ __volatile__ (
211     /* atomic loop */
212     &quot;1:                                                 \n&quot;
213     &quot;   ldarx   %[old_value], %[dest], %[zero]          \n&quot;
214     &quot;   stdcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
215     &quot;   bne-    1b                                      \n&quot;
216     /* exit */
217     &quot;2:                                                 \n&quot;
218     /* out */
219     : [old_value]       &quot;=&amp;r&quot;   (old_value),
220                         &quot;=m&quot;    (*dest)
221     /* in */
222     : [dest]            &quot;b&quot;     (dest),
223       [zero]            &quot;r&quot;     (zero),
224       [exchange_value]  &quot;r&quot;     (exchange_value),
225                         &quot;m&quot;     (*dest)
226     /* clobber */
227     : &quot;cc&quot;,
228       &quot;memory&quot;
229     );
230 
231   post_membar(order);
232 
233   return old_value;
234 }
235 
236 template&lt;&gt;
237 template&lt;typename T&gt;
<a name="7" id="anc7"></a><span class="line-modified">238 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>

239                                                 T compare_value,
<a name="8" id="anc8"></a><span class="line-added">240                                                 T exchange_value,</span>
241                                                 atomic_memory_order order) const {
242   STATIC_ASSERT(1 == sizeof(T));
243 
244   // Note that cmpxchg guarantees a two-way memory barrier across
245   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
246   // specified otherwise (see atomic.hpp).
247 
248   // Using 32 bit internally.
249   volatile int *dest_base = (volatile int*)((uintptr_t)dest &amp; ~3);
250 
251 #ifdef VM_LITTLE_ENDIAN
252   const unsigned int shift_amount        = ((uintptr_t)dest &amp; 3) * 8;
253 #else
254   const unsigned int shift_amount        = ((~(uintptr_t)dest) &amp; 3) * 8;
255 #endif
256   const unsigned int masked_compare_val  = ((unsigned int)(unsigned char)compare_value),
257                      masked_exchange_val = ((unsigned int)(unsigned char)exchange_value),
258                      xor_value           = (masked_compare_val ^ masked_exchange_val) &lt;&lt; shift_amount;
259 
260   unsigned int old_value, value32;
261 
262   pre_membar(order);
263 
264   __asm__ __volatile__ (
265     /* simple guard */
266     &quot;   lbz     %[old_value], 0(%[dest])                  \n&quot;
267     &quot;   cmpw    %[masked_compare_val], %[old_value]       \n&quot;
268     &quot;   bne-    2f                                        \n&quot;
269     /* atomic loop */
270     &quot;1:                                                   \n&quot;
271     &quot;   lwarx   %[value32], 0, %[dest_base]               \n&quot;
272     /* extract byte and compare */
273     &quot;   srd     %[old_value], %[value32], %[shift_amount] \n&quot;
274     &quot;   clrldi  %[old_value], %[old_value], 56            \n&quot;
275     &quot;   cmpw    %[masked_compare_val], %[old_value]       \n&quot;
276     &quot;   bne-    2f                                        \n&quot;
277     /* replace byte and try to store */
278     &quot;   xor     %[value32], %[xor_value], %[value32]      \n&quot;
279     &quot;   stwcx.  %[value32], 0, %[dest_base]               \n&quot;
280     &quot;   bne-    1b                                        \n&quot;
281     /* exit */
282     &quot;2:                                                   \n&quot;
283     /* out */
284     : [old_value]           &quot;=&amp;r&quot;   (old_value),
285       [value32]             &quot;=&amp;r&quot;   (value32),
286                             &quot;=m&quot;    (*dest),
287                             &quot;=m&quot;    (*dest_base)
288     /* in */
289     : [dest]                &quot;b&quot;     (dest),
290       [dest_base]           &quot;b&quot;     (dest_base),
291       [shift_amount]        &quot;r&quot;     (shift_amount),
292       [masked_compare_val]  &quot;r&quot;     (masked_compare_val),
293       [xor_value]           &quot;r&quot;     (xor_value),
294                             &quot;m&quot;     (*dest),
295                             &quot;m&quot;     (*dest_base)
296     /* clobber */
297     : &quot;cc&quot;,
298       &quot;memory&quot;
299     );
300 
301   post_membar(order);
302 
303   return PrimitiveConversions::cast&lt;T&gt;((unsigned char)old_value);
304 }
305 
306 template&lt;&gt;
307 template&lt;typename T&gt;
<a name="9" id="anc9"></a><span class="line-modified">308 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>

309                                                 T compare_value,
<a name="10" id="anc10"></a><span class="line-added">310                                                 T exchange_value,</span>
311                                                 atomic_memory_order order) const {
312   STATIC_ASSERT(4 == sizeof(T));
313 
314   // Note that cmpxchg guarantees a two-way memory barrier across
315   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
316   // specified otherwise (see atomic.hpp).
317 
318   T old_value;
319   const uint64_t zero = 0;
320 
321   pre_membar(order);
322 
323   __asm__ __volatile__ (
324     /* simple guard */
325     &quot;   lwz     %[old_value], 0(%[dest])                \n&quot;
326     &quot;   cmpw    %[compare_value], %[old_value]          \n&quot;
327     &quot;   bne-    2f                                      \n&quot;
328     /* atomic loop */
329     &quot;1:                                                 \n&quot;
330     &quot;   lwarx   %[old_value], %[dest], %[zero]          \n&quot;
331     &quot;   cmpw    %[compare_value], %[old_value]          \n&quot;
332     &quot;   bne-    2f                                      \n&quot;
333     &quot;   stwcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
334     &quot;   bne-    1b                                      \n&quot;
335     /* exit */
336     &quot;2:                                                 \n&quot;
337     /* out */
338     : [old_value]       &quot;=&amp;r&quot;   (old_value),
339                         &quot;=m&quot;    (*dest)
340     /* in */
341     : [dest]            &quot;b&quot;     (dest),
342       [zero]            &quot;r&quot;     (zero),
343       [compare_value]   &quot;r&quot;     (compare_value),
344       [exchange_value]  &quot;r&quot;     (exchange_value),
345                         &quot;m&quot;     (*dest)
346     /* clobber */
347     : &quot;cc&quot;,
348       &quot;memory&quot;
349     );
350 
351   post_membar(order);
352 
353   return old_value;
354 }
355 
356 template&lt;&gt;
357 template&lt;typename T&gt;
<a name="11" id="anc11"></a><span class="line-modified">358 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>

359                                                 T compare_value,
<a name="12" id="anc12"></a><span class="line-added">360                                                 T exchange_value,</span>
361                                                 atomic_memory_order order) const {
362   STATIC_ASSERT(8 == sizeof(T));
363 
364   // Note that cmpxchg guarantees a two-way memory barrier across
365   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
366   // specified otherwise (see atomic.hpp).
367 
368   T old_value;
369   const uint64_t zero = 0;
370 
371   pre_membar(order);
372 
373   __asm__ __volatile__ (
374     /* simple guard */
375     &quot;   ld      %[old_value], 0(%[dest])                \n&quot;
376     &quot;   cmpd    %[compare_value], %[old_value]          \n&quot;
377     &quot;   bne-    2f                                      \n&quot;
378     /* atomic loop */
379     &quot;1:                                                 \n&quot;
380     &quot;   ldarx   %[old_value], %[dest], %[zero]          \n&quot;
381     &quot;   cmpd    %[compare_value], %[old_value]          \n&quot;
382     &quot;   bne-    2f                                      \n&quot;
383     &quot;   stdcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
384     &quot;   bne-    1b                                      \n&quot;
385     /* exit */
386     &quot;2:                                                 \n&quot;
387     /* out */
388     : [old_value]       &quot;=&amp;r&quot;   (old_value),
389                         &quot;=m&quot;    (*dest)
390     /* in */
391     : [dest]            &quot;b&quot;     (dest),
392       [zero]            &quot;r&quot;     (zero),
393       [compare_value]   &quot;r&quot;     (compare_value),
394       [exchange_value]  &quot;r&quot;     (exchange_value),
395                         &quot;m&quot;     (*dest)
396     /* clobber */
397     : &quot;cc&quot;,
398       &quot;memory&quot;
399     );
400 
401   post_membar(order);
402 
403   return old_value;
404 }
405 
<a name="13" id="anc13"></a><span class="line-added">406 template&lt;size_t byte_size&gt;</span>
<span class="line-added">407 struct Atomic::PlatformOrderedLoad&lt;byte_size, X_ACQUIRE&gt; {</span>
<span class="line-added">408   template &lt;typename T&gt;</span>
<span class="line-added">409   T operator()(const volatile T* p) const {</span>
<span class="line-added">410     T t = Atomic::load(p);</span>
<span class="line-added">411     // Use twi-isync for load_acquire (faster than lwsync).</span>
<span class="line-added">412     __asm__ __volatile__ (&quot;twi 0,%0,0\n isync\n&quot; : : &quot;r&quot; (t) : &quot;memory&quot;);</span>
<span class="line-added">413     return t;</span>
<span class="line-added">414   }</span>
<span class="line-added">415 };</span>
<span class="line-added">416 </span>
417 #endif // OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
<a name="14" id="anc14"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="14" type="hidden" />
</body>
</html>