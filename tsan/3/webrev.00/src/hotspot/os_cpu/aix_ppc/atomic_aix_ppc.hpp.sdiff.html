<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/os_cpu/aix_ppc/atomic_aix_ppc.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../../os/windows/version.rc.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="orderAccess_aix_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os_cpu/aix_ppc/atomic_aix_ppc.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #ifndef OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
 27 #define OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
 28 
 29 #ifndef PPC64
 30 #error &quot;Atomic currently only implemented for PPC64&quot;
 31 #endif
 32 

 33 #include &quot;utilities/debug.hpp&quot;
 34 
 35 // Implementation of class atomic
 36 
 37 //
 38 // machine barrier instructions:
 39 //
 40 // - sync            two-way memory barrier, aka fence
 41 // - lwsync          orders  Store|Store,
 42 //                            Load|Store,
 43 //                            Load|Load,
 44 //                   but not Store|Load
 45 // - eieio           orders memory accesses for device memory (only)
 46 // - isync           invalidates speculatively executed instructions
 47 //                   From the POWER ISA 2.06 documentation:
 48 //                    &quot;[...] an isync instruction prevents the execution of
 49 //                   instructions following the isync until instructions
 50 //                   preceding the isync have completed, [...]&quot;
 51 //                   From IBM&#39;s AIX assembler reference:
 52 //                    &quot;The isync [...] instructions causes the processor to
</pre>
<hr />
<pre>
 75     case memory_order_relaxed:
 76     case memory_order_acquire: break;
 77     case memory_order_release:
 78     case memory_order_acq_rel: __asm__ __volatile__ (&quot;lwsync&quot; : : : &quot;memory&quot;); break;
 79     default /*conservative*/ : __asm__ __volatile__ (&quot;sync&quot;   : : : &quot;memory&quot;); break;
 80   }
 81 }
 82 
 83 inline void post_membar(atomic_memory_order order) {
 84   switch (order) {
 85     case memory_order_relaxed:
 86     case memory_order_release: break;
 87     case memory_order_acquire:
 88     case memory_order_acq_rel: __asm__ __volatile__ (&quot;isync&quot;  : : : &quot;memory&quot;); break;
 89     default /*conservative*/ : __asm__ __volatile__ (&quot;sync&quot;   : : : &quot;memory&quot;); break;
 90   }
 91 }
 92 
 93 
 94 template&lt;size_t byte_size&gt;
<span class="line-modified"> 95 struct Atomic::PlatformAdd</span>
<span class="line-modified"> 96   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;</span>
<span class="line-modified"> 97 {</span>
<span class="line-modified"> 98   template&lt;typename I, typename D&gt;</span>
<span class="line-modified"> 99   D add_and_fetch(I add_value, D volatile* dest, atomic_memory_order order) const;</span>



100 };
101 
102 template&lt;&gt;
<span class="line-modified">103 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">104 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
105                                                atomic_memory_order order) const {
106   STATIC_ASSERT(4 == sizeof(I));
107   STATIC_ASSERT(4 == sizeof(D));
108 
109   D result;
110 
111   pre_membar(order);
112 
113   __asm__ __volatile__ (
114     &quot;1: lwarx   %0,  0, %2    \n&quot;
115     &quot;   add     %0, %0, %1    \n&quot;
116     &quot;   stwcx.  %0,  0, %2    \n&quot;
117     &quot;   bne-    1b            \n&quot;
118     : /*%0*/&quot;=&amp;r&quot; (result)
119     : /*%1*/&quot;r&quot; (add_value), /*%2*/&quot;r&quot; (dest)
120     : &quot;cc&quot;, &quot;memory&quot; );
121 
122   post_membar(order);
123 
124   return result;
125 }
126 
127 
128 template&lt;&gt;
<span class="line-modified">129 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">130 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
131                                                atomic_memory_order order) const {
132   STATIC_ASSERT(8 == sizeof(I));
133   STATIC_ASSERT(8 == sizeof(D));
134 
135   D result;
136 
137   pre_membar(order);
138 
139   __asm__ __volatile__ (
140     &quot;1: ldarx   %0,  0, %2    \n&quot;
141     &quot;   add     %0, %0, %1    \n&quot;
142     &quot;   stdcx.  %0,  0, %2    \n&quot;
143     &quot;   bne-    1b            \n&quot;
144     : /*%0*/&quot;=&amp;r&quot; (result)
145     : /*%1*/&quot;r&quot; (add_value), /*%2*/&quot;r&quot; (dest)
146     : &quot;cc&quot;, &quot;memory&quot; );
147 
148   post_membar(order);
149 
150   return result;
151 }
152 
153 template&lt;&gt;
154 template&lt;typename T&gt;
<span class="line-modified">155 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-modified">156                                              T volatile* dest,</span>
157                                              atomic_memory_order order) const {
158   // Note that xchg doesn&#39;t necessarily do an acquire
159   // (see synchronizer.cpp).
160 
161   T old_value;
162   const uint64_t zero = 0;
163 
164   pre_membar(order);
165 
166   __asm__ __volatile__ (
167     /* atomic loop */
168     &quot;1:                                                 \n&quot;
169     &quot;   lwarx   %[old_value], %[dest], %[zero]          \n&quot;
170     &quot;   stwcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
171     &quot;   bne-    1b                                      \n&quot;
172     /* exit */
173     &quot;2:                                                 \n&quot;
174     /* out */
175     : [old_value]       &quot;=&amp;r&quot;   (old_value),
176                         &quot;=m&quot;    (*dest)
177     /* in */
178     : [dest]            &quot;b&quot;     (dest),
179       [zero]            &quot;r&quot;     (zero),
180       [exchange_value]  &quot;r&quot;     (exchange_value),
181                         &quot;m&quot;     (*dest)
182     /* clobber */
183     : &quot;cc&quot;,
184       &quot;memory&quot;
185     );
186 
187   post_membar(order);
188 
189   return old_value;
190 }
191 
192 template&lt;&gt;
193 template&lt;typename T&gt;
<span class="line-modified">194 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-modified">195                                              T volatile* dest,</span>
196                                              atomic_memory_order order) const {
197   STATIC_ASSERT(8 == sizeof(T));
198   // Note that xchg doesn&#39;t necessarily do an acquire
199   // (see synchronizer.cpp).
200 
201   T old_value;
202   const uint64_t zero = 0;
203 
204   pre_membar(order);
205 
206   __asm__ __volatile__ (
207     /* atomic loop */
208     &quot;1:                                                 \n&quot;
209     &quot;   ldarx   %[old_value], %[dest], %[zero]          \n&quot;
210     &quot;   stdcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
211     &quot;   bne-    1b                                      \n&quot;
212     /* exit */
213     &quot;2:                                                 \n&quot;
214     /* out */
215     : [old_value]       &quot;=&amp;r&quot;   (old_value),
216                         &quot;=m&quot;    (*dest)
217     /* in */
218     : [dest]            &quot;b&quot;     (dest),
219       [zero]            &quot;r&quot;     (zero),
220       [exchange_value]  &quot;r&quot;     (exchange_value),
221                         &quot;m&quot;     (*dest)
222     /* clobber */
223     : &quot;cc&quot;,
224       &quot;memory&quot;
225     );
226 
227   post_membar(order);
228 
229   return old_value;
230 }
231 
232 template&lt;&gt;
233 template&lt;typename T&gt;
<span class="line-modified">234 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T exchange_value,</span>
<span class="line-removed">235                                                 T volatile* dest,</span>
236                                                 T compare_value,

237                                                 atomic_memory_order order) const {
238   STATIC_ASSERT(1 == sizeof(T));
239 
240   // Note that cmpxchg guarantees a two-way memory barrier across
241   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
242   // specified otherwise (see atomic.hpp).
243 
244   // Using 32 bit internally.
245   volatile int *dest_base = (volatile int*)((uintptr_t)dest &amp; ~3);
246 
247 #ifdef VM_LITTLE_ENDIAN
248   const unsigned int shift_amount        = ((uintptr_t)dest &amp; 3) * 8;
249 #else
250   const unsigned int shift_amount        = ((~(uintptr_t)dest) &amp; 3) * 8;
251 #endif
252   const unsigned int masked_compare_val  = ((unsigned int)(unsigned char)compare_value),
253                      masked_exchange_val = ((unsigned int)(unsigned char)exchange_value),
254                      xor_value           = (masked_compare_val ^ masked_exchange_val) &lt;&lt; shift_amount;
255 
256   unsigned int old_value, value32;
</pre>
<hr />
<pre>
284     /* in */
285     : [dest]                &quot;b&quot;     (dest),
286       [dest_base]           &quot;b&quot;     (dest_base),
287       [shift_amount]        &quot;r&quot;     (shift_amount),
288       [masked_compare_val]  &quot;r&quot;     (masked_compare_val),
289       [xor_value]           &quot;r&quot;     (xor_value),
290                             &quot;m&quot;     (*dest),
291                             &quot;m&quot;     (*dest_base)
292     /* clobber */
293     : &quot;cc&quot;,
294       &quot;memory&quot;
295     );
296 
297   post_membar(order);
298 
299   return PrimitiveConversions::cast&lt;T&gt;((unsigned char)old_value);
300 }
301 
302 template&lt;&gt;
303 template&lt;typename T&gt;
<span class="line-modified">304 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-removed">305                                                 T volatile* dest,</span>
306                                                 T compare_value,

307                                                 atomic_memory_order order) const {
308   STATIC_ASSERT(4 == sizeof(T));
309 
310   // Note that cmpxchg guarantees a two-way memory barrier across
311   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
312   // specified otherwise (see atomic.hpp).
313 
314   T old_value;
315   const uint64_t zero = 0;
316 
317   pre_membar(order);
318 
319   __asm__ __volatile__ (
320     /* simple guard */
321     &quot;   lwz     %[old_value], 0(%[dest])                \n&quot;
322     &quot;   cmpw    %[compare_value], %[old_value]          \n&quot;
323     &quot;   bne-    2f                                      \n&quot;
324     /* atomic loop */
325     &quot;1:                                                 \n&quot;
326     &quot;   lwarx   %[old_value], %[dest], %[zero]          \n&quot;
</pre>
<hr />
<pre>
334     : [old_value]       &quot;=&amp;r&quot;   (old_value),
335                         &quot;=m&quot;    (*dest)
336     /* in */
337     : [dest]            &quot;b&quot;     (dest),
338       [zero]            &quot;r&quot;     (zero),
339       [compare_value]   &quot;r&quot;     (compare_value),
340       [exchange_value]  &quot;r&quot;     (exchange_value),
341                         &quot;m&quot;     (*dest)
342     /* clobber */
343     : &quot;cc&quot;,
344       &quot;memory&quot;
345     );
346 
347   post_membar(order);
348 
349   return old_value;
350 }
351 
352 template&lt;&gt;
353 template&lt;typename T&gt;
<span class="line-modified">354 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-removed">355                                                 T volatile* dest,</span>
356                                                 T compare_value,

357                                                 atomic_memory_order order) const {
358   STATIC_ASSERT(8 == sizeof(T));
359 
360   // Note that cmpxchg guarantees a two-way memory barrier across
361   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
362   // specified otherwise (see atomic.hpp).
363 
364   T old_value;
365   const uint64_t zero = 0;
366 
367   pre_membar(order);
368 
369   __asm__ __volatile__ (
370     /* simple guard */
371     &quot;   ld      %[old_value], 0(%[dest])                \n&quot;
372     &quot;   cmpd    %[compare_value], %[old_value]          \n&quot;
373     &quot;   bne-    2f                                      \n&quot;
374     /* atomic loop */
375     &quot;1:                                                 \n&quot;
376     &quot;   ldarx   %[old_value], %[dest], %[zero]          \n&quot;
</pre>
<hr />
<pre>
382     &quot;2:                                                 \n&quot;
383     /* out */
384     : [old_value]       &quot;=&amp;r&quot;   (old_value),
385                         &quot;=m&quot;    (*dest)
386     /* in */
387     : [dest]            &quot;b&quot;     (dest),
388       [zero]            &quot;r&quot;     (zero),
389       [compare_value]   &quot;r&quot;     (compare_value),
390       [exchange_value]  &quot;r&quot;     (exchange_value),
391                         &quot;m&quot;     (*dest)
392     /* clobber */
393     : &quot;cc&quot;,
394       &quot;memory&quot;
395     );
396 
397   post_membar(order);
398 
399   return old_value;
400 }
401 











402 #endif // OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
</pre>
</td>
<td>
<hr />
<pre>
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #ifndef OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
 27 #define OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
 28 
 29 #ifndef PPC64
 30 #error &quot;Atomic currently only implemented for PPC64&quot;
 31 #endif
 32 
<span class="line-added"> 33 #include &quot;orderAccess_aix_ppc.hpp&quot;</span>
 34 #include &quot;utilities/debug.hpp&quot;
 35 
 36 // Implementation of class atomic
 37 
 38 //
 39 // machine barrier instructions:
 40 //
 41 // - sync            two-way memory barrier, aka fence
 42 // - lwsync          orders  Store|Store,
 43 //                            Load|Store,
 44 //                            Load|Load,
 45 //                   but not Store|Load
 46 // - eieio           orders memory accesses for device memory (only)
 47 // - isync           invalidates speculatively executed instructions
 48 //                   From the POWER ISA 2.06 documentation:
 49 //                    &quot;[...] an isync instruction prevents the execution of
 50 //                   instructions following the isync until instructions
 51 //                   preceding the isync have completed, [...]&quot;
 52 //                   From IBM&#39;s AIX assembler reference:
 53 //                    &quot;The isync [...] instructions causes the processor to
</pre>
<hr />
<pre>
 76     case memory_order_relaxed:
 77     case memory_order_acquire: break;
 78     case memory_order_release:
 79     case memory_order_acq_rel: __asm__ __volatile__ (&quot;lwsync&quot; : : : &quot;memory&quot;); break;
 80     default /*conservative*/ : __asm__ __volatile__ (&quot;sync&quot;   : : : &quot;memory&quot;); break;
 81   }
 82 }
 83 
 84 inline void post_membar(atomic_memory_order order) {
 85   switch (order) {
 86     case memory_order_relaxed:
 87     case memory_order_release: break;
 88     case memory_order_acquire:
 89     case memory_order_acq_rel: __asm__ __volatile__ (&quot;isync&quot;  : : : &quot;memory&quot;); break;
 90     default /*conservative*/ : __asm__ __volatile__ (&quot;sync&quot;   : : : &quot;memory&quot;); break;
 91   }
 92 }
 93 
 94 
 95 template&lt;size_t byte_size&gt;
<span class="line-modified"> 96 struct Atomic::PlatformAdd {</span>
<span class="line-modified"> 97   template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 98   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;</span>
<span class="line-modified"> 99 </span>
<span class="line-modified">100   template&lt;typename D, typename I&gt;</span>
<span class="line-added">101   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {</span>
<span class="line-added">102     return add_and_fetch(dest, add_value, order) - add_value;</span>
<span class="line-added">103   }</span>
104 };
105 
106 template&lt;&gt;
<span class="line-modified">107 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">108 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
109                                                atomic_memory_order order) const {
110   STATIC_ASSERT(4 == sizeof(I));
111   STATIC_ASSERT(4 == sizeof(D));
112 
113   D result;
114 
115   pre_membar(order);
116 
117   __asm__ __volatile__ (
118     &quot;1: lwarx   %0,  0, %2    \n&quot;
119     &quot;   add     %0, %0, %1    \n&quot;
120     &quot;   stwcx.  %0,  0, %2    \n&quot;
121     &quot;   bne-    1b            \n&quot;
122     : /*%0*/&quot;=&amp;r&quot; (result)
123     : /*%1*/&quot;r&quot; (add_value), /*%2*/&quot;r&quot; (dest)
124     : &quot;cc&quot;, &quot;memory&quot; );
125 
126   post_membar(order);
127 
128   return result;
129 }
130 
131 
132 template&lt;&gt;
<span class="line-modified">133 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">134 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
135                                                atomic_memory_order order) const {
136   STATIC_ASSERT(8 == sizeof(I));
137   STATIC_ASSERT(8 == sizeof(D));
138 
139   D result;
140 
141   pre_membar(order);
142 
143   __asm__ __volatile__ (
144     &quot;1: ldarx   %0,  0, %2    \n&quot;
145     &quot;   add     %0, %0, %1    \n&quot;
146     &quot;   stdcx.  %0,  0, %2    \n&quot;
147     &quot;   bne-    1b            \n&quot;
148     : /*%0*/&quot;=&amp;r&quot; (result)
149     : /*%1*/&quot;r&quot; (add_value), /*%2*/&quot;r&quot; (dest)
150     : &quot;cc&quot;, &quot;memory&quot; );
151 
152   post_membar(order);
153 
154   return result;
155 }
156 
157 template&lt;&gt;
158 template&lt;typename T&gt;
<span class="line-modified">159 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">160                                              T exchange_value,</span>
161                                              atomic_memory_order order) const {
162   // Note that xchg doesn&#39;t necessarily do an acquire
163   // (see synchronizer.cpp).
164 
165   T old_value;
166   const uint64_t zero = 0;
167 
168   pre_membar(order);
169 
170   __asm__ __volatile__ (
171     /* atomic loop */
172     &quot;1:                                                 \n&quot;
173     &quot;   lwarx   %[old_value], %[dest], %[zero]          \n&quot;
174     &quot;   stwcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
175     &quot;   bne-    1b                                      \n&quot;
176     /* exit */
177     &quot;2:                                                 \n&quot;
178     /* out */
179     : [old_value]       &quot;=&amp;r&quot;   (old_value),
180                         &quot;=m&quot;    (*dest)
181     /* in */
182     : [dest]            &quot;b&quot;     (dest),
183       [zero]            &quot;r&quot;     (zero),
184       [exchange_value]  &quot;r&quot;     (exchange_value),
185                         &quot;m&quot;     (*dest)
186     /* clobber */
187     : &quot;cc&quot;,
188       &quot;memory&quot;
189     );
190 
191   post_membar(order);
192 
193   return old_value;
194 }
195 
196 template&lt;&gt;
197 template&lt;typename T&gt;
<span class="line-modified">198 inline T Atomic::PlatformXchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">199                                              T exchange_value,</span>
200                                              atomic_memory_order order) const {
201   STATIC_ASSERT(8 == sizeof(T));
202   // Note that xchg doesn&#39;t necessarily do an acquire
203   // (see synchronizer.cpp).
204 
205   T old_value;
206   const uint64_t zero = 0;
207 
208   pre_membar(order);
209 
210   __asm__ __volatile__ (
211     /* atomic loop */
212     &quot;1:                                                 \n&quot;
213     &quot;   ldarx   %[old_value], %[dest], %[zero]          \n&quot;
214     &quot;   stdcx.  %[exchange_value], %[dest], %[zero]     \n&quot;
215     &quot;   bne-    1b                                      \n&quot;
216     /* exit */
217     &quot;2:                                                 \n&quot;
218     /* out */
219     : [old_value]       &quot;=&amp;r&quot;   (old_value),
220                         &quot;=m&quot;    (*dest)
221     /* in */
222     : [dest]            &quot;b&quot;     (dest),
223       [zero]            &quot;r&quot;     (zero),
224       [exchange_value]  &quot;r&quot;     (exchange_value),
225                         &quot;m&quot;     (*dest)
226     /* clobber */
227     : &quot;cc&quot;,
228       &quot;memory&quot;
229     );
230 
231   post_membar(order);
232 
233   return old_value;
234 }
235 
236 template&lt;&gt;
237 template&lt;typename T&gt;
<span class="line-modified">238 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>

239                                                 T compare_value,
<span class="line-added">240                                                 T exchange_value,</span>
241                                                 atomic_memory_order order) const {
242   STATIC_ASSERT(1 == sizeof(T));
243 
244   // Note that cmpxchg guarantees a two-way memory barrier across
245   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
246   // specified otherwise (see atomic.hpp).
247 
248   // Using 32 bit internally.
249   volatile int *dest_base = (volatile int*)((uintptr_t)dest &amp; ~3);
250 
251 #ifdef VM_LITTLE_ENDIAN
252   const unsigned int shift_amount        = ((uintptr_t)dest &amp; 3) * 8;
253 #else
254   const unsigned int shift_amount        = ((~(uintptr_t)dest) &amp; 3) * 8;
255 #endif
256   const unsigned int masked_compare_val  = ((unsigned int)(unsigned char)compare_value),
257                      masked_exchange_val = ((unsigned int)(unsigned char)exchange_value),
258                      xor_value           = (masked_compare_val ^ masked_exchange_val) &lt;&lt; shift_amount;
259 
260   unsigned int old_value, value32;
</pre>
<hr />
<pre>
288     /* in */
289     : [dest]                &quot;b&quot;     (dest),
290       [dest_base]           &quot;b&quot;     (dest_base),
291       [shift_amount]        &quot;r&quot;     (shift_amount),
292       [masked_compare_val]  &quot;r&quot;     (masked_compare_val),
293       [xor_value]           &quot;r&quot;     (xor_value),
294                             &quot;m&quot;     (*dest),
295                             &quot;m&quot;     (*dest_base)
296     /* clobber */
297     : &quot;cc&quot;,
298       &quot;memory&quot;
299     );
300 
301   post_membar(order);
302 
303   return PrimitiveConversions::cast&lt;T&gt;((unsigned char)old_value);
304 }
305 
306 template&lt;&gt;
307 template&lt;typename T&gt;
<span class="line-modified">308 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>

309                                                 T compare_value,
<span class="line-added">310                                                 T exchange_value,</span>
311                                                 atomic_memory_order order) const {
312   STATIC_ASSERT(4 == sizeof(T));
313 
314   // Note that cmpxchg guarantees a two-way memory barrier across
315   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
316   // specified otherwise (see atomic.hpp).
317 
318   T old_value;
319   const uint64_t zero = 0;
320 
321   pre_membar(order);
322 
323   __asm__ __volatile__ (
324     /* simple guard */
325     &quot;   lwz     %[old_value], 0(%[dest])                \n&quot;
326     &quot;   cmpw    %[compare_value], %[old_value]          \n&quot;
327     &quot;   bne-    2f                                      \n&quot;
328     /* atomic loop */
329     &quot;1:                                                 \n&quot;
330     &quot;   lwarx   %[old_value], %[dest], %[zero]          \n&quot;
</pre>
<hr />
<pre>
338     : [old_value]       &quot;=&amp;r&quot;   (old_value),
339                         &quot;=m&quot;    (*dest)
340     /* in */
341     : [dest]            &quot;b&quot;     (dest),
342       [zero]            &quot;r&quot;     (zero),
343       [compare_value]   &quot;r&quot;     (compare_value),
344       [exchange_value]  &quot;r&quot;     (exchange_value),
345                         &quot;m&quot;     (*dest)
346     /* clobber */
347     : &quot;cc&quot;,
348       &quot;memory&quot;
349     );
350 
351   post_membar(order);
352 
353   return old_value;
354 }
355 
356 template&lt;&gt;
357 template&lt;typename T&gt;
<span class="line-modified">358 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>

359                                                 T compare_value,
<span class="line-added">360                                                 T exchange_value,</span>
361                                                 atomic_memory_order order) const {
362   STATIC_ASSERT(8 == sizeof(T));
363 
364   // Note that cmpxchg guarantees a two-way memory barrier across
365   // the cmpxchg, so it&#39;s really a a &#39;fence_cmpxchg_fence&#39; if not
366   // specified otherwise (see atomic.hpp).
367 
368   T old_value;
369   const uint64_t zero = 0;
370 
371   pre_membar(order);
372 
373   __asm__ __volatile__ (
374     /* simple guard */
375     &quot;   ld      %[old_value], 0(%[dest])                \n&quot;
376     &quot;   cmpd    %[compare_value], %[old_value]          \n&quot;
377     &quot;   bne-    2f                                      \n&quot;
378     /* atomic loop */
379     &quot;1:                                                 \n&quot;
380     &quot;   ldarx   %[old_value], %[dest], %[zero]          \n&quot;
</pre>
<hr />
<pre>
386     &quot;2:                                                 \n&quot;
387     /* out */
388     : [old_value]       &quot;=&amp;r&quot;   (old_value),
389                         &quot;=m&quot;    (*dest)
390     /* in */
391     : [dest]            &quot;b&quot;     (dest),
392       [zero]            &quot;r&quot;     (zero),
393       [compare_value]   &quot;r&quot;     (compare_value),
394       [exchange_value]  &quot;r&quot;     (exchange_value),
395                         &quot;m&quot;     (*dest)
396     /* clobber */
397     : &quot;cc&quot;,
398       &quot;memory&quot;
399     );
400 
401   post_membar(order);
402 
403   return old_value;
404 }
405 
<span class="line-added">406 template&lt;size_t byte_size&gt;</span>
<span class="line-added">407 struct Atomic::PlatformOrderedLoad&lt;byte_size, X_ACQUIRE&gt; {</span>
<span class="line-added">408   template &lt;typename T&gt;</span>
<span class="line-added">409   T operator()(const volatile T* p) const {</span>
<span class="line-added">410     T t = Atomic::load(p);</span>
<span class="line-added">411     // Use twi-isync for load_acquire (faster than lwsync).</span>
<span class="line-added">412     __asm__ __volatile__ (&quot;twi 0,%0,0\n isync\n&quot; : : &quot;r&quot; (t) : &quot;memory&quot;);</span>
<span class="line-added">413     return t;</span>
<span class="line-added">414   }</span>
<span class="line-added">415 };</span>
<span class="line-added">416 </span>
417 #endif // OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP
</pre>
</td>
</tr>
</table>
<center><a href="../../os/windows/version.rc.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="orderAccess_aix_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>