<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../solaris_x86/vm_version_solaris_x86.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="orderAccess_windows_x86.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 25,10 ***</span>
<span class="line-new-header">--- 25,21 ---</span>
  #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
  #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
  
  #include &quot;runtime/os.hpp&quot;
  
<span class="line-added">+ // Note that in MSVC, volatile memory accesses are explicitly</span>
<span class="line-added">+ // guaranteed to have acquire release semantics (w.r.t. compiler</span>
<span class="line-added">+ // reordering) and therefore does not even need a compiler barrier</span>
<span class="line-added">+ // for normal acquire release accesses. And all generalized</span>
<span class="line-added">+ // bound calls like release_store go through Atomic::load</span>
<span class="line-added">+ // and Atomic::store which do volatile memory accesses.</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }</span>
<span class="line-added">+ </span>
  // The following alternative implementations are needed because
  // Windows 95 doesn&#39;t support (some of) the corresponding Windows NT
  // calls. Furthermore, these versions allow inlining in the caller.
  // (More precisely: The documentation for InterlockedExchange says
  // it is supported for Windows 95. However, when single-stepping
</pre>
<hr />
<pre>
<span class="line-old-header">*** 41,56 ***</span>
  // this becomes a performance problem.
  
  #pragma warning(disable: 4035) // Disables warnings reporting missing return statement
  
  template&lt;size_t byte_size&gt;
<span class="line-modified">! struct Atomic::PlatformAdd</span>
<span class="line-modified">!   : Atomic::AddAndFetch&lt;Atomic::PlatformAdd&lt;byte_size&gt; &gt;</span>
<span class="line-modified">! {</span>
<span class="line-modified">!   template&lt;typename I, typename D&gt;</span>
<span class="line-modified">!   D add_and_fetch(I add_value, D volatile* dest, atomic_memory_order order) const;</span>
  };
  
  #ifdef AMD64
  template&lt;&gt;
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
                                                 atomic_memory_order order) const {
<span class="line-modified">!   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, add_value, dest);</span>
  }
  
  template&lt;&gt;
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
                                                 atomic_memory_order order) const {
<span class="line-modified">!   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, add_value, dest);</span>
  }
  
  #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
    template&lt;&gt;                                                            \
    template&lt;typename T&gt;                                                  \
<span class="line-modified">!   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T exchange_value, \</span>
<span class="line-modified">!                                                       T volatile* dest, \</span>
                                                        atomic_memory_order order) const { \
      STATIC_ASSERT(ByteSize == sizeof(T));                               \
<span class="line-modified">!     return xchg_using_helper&lt;StubType&gt;(StubName, exchange_value, dest); \</span>
    }
  
  DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)
  DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)
  
  #undef DEFINE_STUB_XCHG
  
<span class="line-modified">! #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)               \</span>
<span class="line-modified">!   template&lt;&gt;                                                            \</span>
<span class="line-modified">!   template&lt;typename T&gt;                                                  \</span>
<span class="line-modified">!   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T exchange_value, \</span>
<span class="line-modified">!                                                          T volatile* dest, \</span>
<span class="line-modified">!                                                          T compare_value, \</span>
                                                           atomic_memory_order order) const { \
<span class="line-modified">!     STATIC_ASSERT(ByteSize == sizeof(T));                               \</span>
<span class="line-modified">!     return cmpxchg_using_helper&lt;StubType&gt;(StubName, exchange_value, dest, compare_value); \</span>
    }
  
  DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)
  DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)
  DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)
<span class="line-new-header">--- 52,59 ---</span>
  // this becomes a performance problem.
  
  #pragma warning(disable: 4035) // Disables warnings reporting missing return statement
  
  template&lt;size_t byte_size&gt;
<span class="line-modified">! struct Atomic::PlatformAdd {</span>
<span class="line-modified">!   template&lt;typename D, typename I&gt;</span>
<span class="line-modified">!   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;</span>
<span class="line-modified">! </span>
<span class="line-modified">!   template&lt;typename D, typename I&gt;</span>
<span class="line-added">+   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {</span>
<span class="line-added">+     return add_and_fetch(dest, add_value, order) - add_value;</span>
<span class="line-added">+   }</span>
  };
  
  #ifdef AMD64
  template&lt;&gt;
<span class="line-modified">! template&lt;typename D, typename I&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
                                                 atomic_memory_order order) const {
<span class="line-modified">!   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, dest, add_value);</span>
  }
  
  template&lt;&gt;
<span class="line-modified">! template&lt;typename D, typename I&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
                                                 atomic_memory_order order) const {
<span class="line-modified">!   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, dest, add_value);</span>
  }
  
  #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \
    template&lt;&gt;                                                            \
    template&lt;typename T&gt;                                                  \
<span class="line-modified">!   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \</span>
<span class="line-modified">!                                                       T exchange_value, \</span>
                                                        atomic_memory_order order) const { \
      STATIC_ASSERT(ByteSize == sizeof(T));                               \
<span class="line-modified">!     return xchg_using_helper&lt;StubType&gt;(StubName, dest, exchange_value); \</span>
    }
  
  DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)
  DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)
  
  #undef DEFINE_STUB_XCHG
  
<span class="line-modified">! #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \</span>
<span class="line-modified">!   template&lt;&gt;                                                               \</span>
<span class="line-modified">!   template&lt;typename T&gt;                                                     \</span>
<span class="line-modified">!   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \</span>
<span class="line-modified">!                                                          T compare_value,  \</span>
<span class="line-modified">!                                                          T exchange_value, \</span>
                                                           atomic_memory_order order) const { \
<span class="line-modified">!     STATIC_ASSERT(ByteSize == sizeof(T));                                  \</span>
<span class="line-modified">!     return cmpxchg_using_helper&lt;StubType&gt;(StubName, dest, compare_value, exchange_value); \</span>
    }
  
  DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)
  DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)
  DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 98,12 ***</span>
  #undef DEFINE_STUB_CMPXCHG
  
  #else // !AMD64
  
  template&lt;&gt;
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(I add_value, D volatile* dest,</span>
                                                 atomic_memory_order order) const {
    STATIC_ASSERT(4 == sizeof(I));
    STATIC_ASSERT(4 == sizeof(D));
    __asm {
      mov edx, dest;
<span class="line-new-header">--- 112,12 ---</span>
  #undef DEFINE_STUB_CMPXCHG
  
  #else // !AMD64
  
  template&lt;&gt;
<span class="line-modified">! template&lt;typename D, typename I&gt;</span>
<span class="line-modified">! inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
                                                 atomic_memory_order order) const {
    STATIC_ASSERT(4 == sizeof(I));
    STATIC_ASSERT(4 == sizeof(D));
    __asm {
      mov edx, dest;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 114,12 ***</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-modified">!                                              T volatile* dest,</span>
                                               atomic_memory_order order) const {
    STATIC_ASSERT(4 == sizeof(T));
    // alternative for InterlockedExchange
    __asm {
      mov eax, exchange_value;
<span class="line-new-header">--- 128,12 ---</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">!                                              T exchange_value,</span>
                                               atomic_memory_order order) const {
    STATIC_ASSERT(4 == sizeof(T));
    // alternative for InterlockedExchange
    __asm {
      mov eax, exchange_value;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 128,13 ***</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T exchange_value,</span>
<span class="line-removed">-                                                 T volatile* dest,</span>
                                                  T compare_value,
                                                  atomic_memory_order order) const {
    STATIC_ASSERT(1 == sizeof(T));
    // alternative for InterlockedCompareExchange
    __asm {
      mov edx, dest
<span class="line-new-header">--- 142,13 ---</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>
                                                  T compare_value,
<span class="line-added">+                                                 T exchange_value,</span>
                                                  atomic_memory_order order) const {
    STATIC_ASSERT(1 == sizeof(T));
    // alternative for InterlockedCompareExchange
    __asm {
      mov edx, dest
</pre>
<hr />
<pre>
<span class="line-old-header">*** 144,13 ***</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T exchange_value,</span>
<span class="line-removed">-                                                 T volatile* dest,</span>
                                                  T compare_value,
                                                  atomic_memory_order order) const {
    STATIC_ASSERT(4 == sizeof(T));
    // alternative for InterlockedCompareExchange
    __asm {
      mov edx, dest
<span class="line-new-header">--- 158,13 ---</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>
                                                  T compare_value,
<span class="line-added">+                                                 T exchange_value,</span>
                                                  atomic_memory_order order) const {
    STATIC_ASSERT(4 == sizeof(T));
    // alternative for InterlockedCompareExchange
    __asm {
      mov edx, dest
</pre>
<hr />
<pre>
<span class="line-old-header">*** 160,13 ***</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T exchange_value,</span>
<span class="line-removed">-                                                 T volatile* dest,</span>
                                                  T compare_value,
                                                  atomic_memory_order order) const {
    STATIC_ASSERT(8 == sizeof(T));
    int32_t ex_lo  = (int32_t)exchange_value;
    int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );
    int32_t cmp_lo = (int32_t)compare_value;
<span class="line-new-header">--- 174,13 ---</span>
    }
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>
                                                  T compare_value,
<span class="line-added">+                                                 T exchange_value,</span>
                                                  atomic_memory_order order) const {
    STATIC_ASSERT(8 == sizeof(T));
    int32_t ex_lo  = (int32_t)exchange_value;
    int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );
    int32_t cmp_lo = (int32_t)compare_value;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 200,12 ***</span>
    return dest;
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline void Atomic::PlatformStore&lt;8&gt;::operator()(T store_value,</span>
<span class="line-modified">!                                                  T volatile* dest) const {</span>
    STATIC_ASSERT(8 == sizeof(T));
    volatile T* src = &amp;store_value;
    __asm {
      mov eax, src
      fild     qword ptr [eax]
<span class="line-new-header">--- 214,12 ---</span>
    return dest;
  }
  
  template&lt;&gt;
  template&lt;typename T&gt;
<span class="line-modified">! inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-modified">!                                                  T store_value) const {</span>
    STATIC_ASSERT(8 == sizeof(T));
    volatile T* src = &amp;store_value;
    __asm {
      mov eax, src
      fild     qword ptr [eax]
</pre>
<hr />
<pre>
<span class="line-old-header">*** 216,6 ***</span>
<span class="line-new-header">--- 230,47 ---</span>
  
  #endif // AMD64
  
  #pragma warning(default: 4035) // Enables warnings reporting missing return statement
  
<span class="line-added">+ #ifndef AMD64</span>
<span class="line-added">+ template&lt;&gt;</span>
<span class="line-added">+ struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;</span>
<span class="line-added">+ {</span>
<span class="line-added">+   template &lt;typename T&gt;</span>
<span class="line-added">+   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">+     __asm {</span>
<span class="line-added">+       mov edx, p;</span>
<span class="line-added">+       mov al, v;</span>
<span class="line-added">+       xchg al, byte ptr [edx];</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ template&lt;&gt;</span>
<span class="line-added">+ struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;</span>
<span class="line-added">+ {</span>
<span class="line-added">+   template &lt;typename T&gt;</span>
<span class="line-added">+   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">+     __asm {</span>
<span class="line-added">+       mov edx, p;</span>
<span class="line-added">+       mov ax, v;</span>
<span class="line-added">+       xchg ax, word ptr [edx];</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ template&lt;&gt;</span>
<span class="line-added">+ struct Atomic::PlatformOrderedStore&lt;4, RELEASE_X_FENCE&gt;</span>
<span class="line-added">+ {</span>
<span class="line-added">+   template &lt;typename T&gt;</span>
<span class="line-added">+   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">+     __asm {</span>
<span class="line-added">+       mov edx, p;</span>
<span class="line-added">+       mov eax, v;</span>
<span class="line-added">+       xchg eax, dword ptr [edx];</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ };</span>
<span class="line-added">+ #endif // AMD64</span>
<span class="line-added">+ </span>
  #endif // OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
</pre>
<center><a href="../solaris_x86/vm_version_solaris_x86.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="orderAccess_windows_x86.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>