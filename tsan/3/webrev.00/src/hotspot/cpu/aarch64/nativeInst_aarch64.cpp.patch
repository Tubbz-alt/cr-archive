diff a/src/hotspot/cpu/aarch64/nativeInst_aarch64.cpp b/src/hotspot/cpu/aarch64/nativeInst_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/nativeInst_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/nativeInst_aarch64.cpp
@@ -28,10 +28,11 @@
 #include "code/compiledIC.hpp"
 #include "memory/resourceArea.hpp"
 #include "nativeInst_aarch64.hpp"
 #include "oops/oop.inline.hpp"
 #include "runtime/handles.hpp"
+#include "runtime/orderAccess.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/stubRoutines.hpp"
 #include "utilities/ostream.hpp"
 #ifdef COMPILER1
 #include "c1/c1_Runtime1.hpp"
@@ -230,11 +231,15 @@
 void NativeCall::insert(address code_pos, address entry) { Unimplemented(); }
 
 //-------------------------------------------------------------------
 
 void NativeMovConstReg::verify() {
-  // make sure code pattern is actually mov reg64, imm64 instructions
+  if (! (nativeInstruction_at(instruction_address())->is_movz() ||
+        is_adrp_at(instruction_address()) ||
+        is_ldr_literal_at(instruction_address())) ) {
+    fatal("should be MOVZ or ADRP or LDR (literal)");
+  }
 }
 
 
 intptr_t NativeMovConstReg::data() const {
   // das(uint64_t(instruction_address()),2);
@@ -281,12 +286,10 @@
                 p2i(instruction_address()), data());
 }
 
 //-------------------------------------------------------------------
 
-address NativeMovRegMem::instruction_address() const      { return addr_at(instruction_offset); }
-
 int NativeMovRegMem::offset() const  {
   address pc = instruction_address();
   unsigned insn = *(unsigned*)pc;
   if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
     address addr = MacroAssembler::target_addr_for_insn(pc);
@@ -326,13 +329,18 @@
 address NativeJump::jump_destination() const          {
   address dest = MacroAssembler::target_addr_for_insn(instruction_address());
 
   // We use jump to self as the unresolved address which the inline
   // cache code (and relocs) know about
+  // As a special case we also use sequence movptr(r,0); br(r);
+  // i.e. jump to 0 when we need leave space for a wide immediate
+  // load
 
-  // return -1 if jump to self
-  dest = (dest == (address) this) ? (address) -1 : dest;
+  // return -1 if jump to self or to 0
+  if ((dest == (address)this) || dest == 0) {
+    dest = (address) -1;
+  }
   return dest;
 }
 
 void NativeJump::set_jump_destination(address dest) {
   // We use jump to self as the unresolved address which the inline
@@ -350,13 +358,17 @@
   NativeMovConstReg* move = nativeMovConstReg_at(instruction_address());
   address dest = (address) move->data();
 
   // We use jump to self as the unresolved address which the inline
   // cache code (and relocs) know about
+  // As a special case we also use jump to 0 when first generating
+  // a general jump
 
-  // return -1 if jump to self
-  dest = (dest == (address) this) ? (address) -1 : dest;
+  // return -1 if jump to self or to 0
+  if ((dest == (address)this) || dest == 0) {
+    dest = (address) -1;
+  }
   return dest;
 }
 
 void NativeGeneralJump::set_jump_destination(address dest) {
   NativeMovConstReg* move = nativeMovConstReg_at(instruction_address());
