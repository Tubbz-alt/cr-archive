<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64_log.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
<span class="line-modified">   3  * Copyright (c) 2014, 2015, Red Hat Inc. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #ifndef CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
  27 #define CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
  28 
  29 #include &quot;asm/assembler.hpp&quot;


  30 
  31 // MacroAssembler extends Assembler by frequently used macros.
  32 //
  33 // Instructions for which a &#39;better&#39; code sequence exists depending
  34 // on arguments should also go in here.
  35 
  36 class MacroAssembler: public Assembler {
  37   friend class LIR_Assembler;
  38 
  39  public:
  40   using Assembler::mov;
  41   using Assembler::movi;
  42 
  43  protected:
  44 
  45   // Support for VM calls
  46   //
  47   // This is the base routine called by the different versions of call_VM_leaf. The interpreter
  48   // may customize this version by overriding it for its purposes (e.g., to save/restore
  49   // additional registers when doing a VM call).
</pre>
<hr />
<pre>
  62 
  63   // This is the base routine called by the different versions of call_VM. The interpreter
  64   // may customize this version by overriding it for its purposes (e.g., to save/restore
  65   // additional registers when doing a VM call).
  66   //
  67   // If no java_thread register is specified (noreg) than rthread will be used instead. call_VM_base
  68   // returns the register which contains the thread upon return. If a thread register has been
  69   // specified, the return value will correspond to that register. If no last_java_sp is specified
  70   // (noreg) than rsp will be used instead.
  71   virtual void call_VM_base(           // returns the register containing the thread upon return
  72     Register oop_result,               // where an oop-result ends up if any; use noreg otherwise
  73     Register java_thread,              // the thread if computed before     ; use noreg otherwise
  74     Register last_java_sp,             // to set up last_Java_frame in stubs; use noreg otherwise
  75     address  entry_point,              // the entry point
  76     int      number_of_arguments,      // the number of arguments (w/o thread) to pop after the call
  77     bool     check_exceptions          // whether to check for pending exceptions after return
  78   );
  79 
  80   void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions = true);
  81 
<span class="line-modified">  82   // True if an XOR can be used to expand narrow klass references.</span>
<span class="line-modified">  83   bool use_XOR_for_compressed_class_base;</span>









  84 
  85  public:
<span class="line-modified">  86   MacroAssembler(CodeBuffer* code) : Assembler(code) {</span>
<span class="line-removed">  87     use_XOR_for_compressed_class_base</span>
<span class="line-removed">  88       = (operand_valid_for_logical_immediate(false /*is32*/,</span>
<span class="line-removed">  89                                              (uint64_t)Universe::narrow_klass_base())</span>
<span class="line-removed">  90          &amp;&amp; ((uint64_t)Universe::narrow_klass_base()</span>
<span class="line-removed">  91              &gt; (1UL &lt;&lt; log2_intptr(Universe::narrow_klass_range()))));</span>
<span class="line-removed">  92   }</span>
  93 
  94  // These routines should emit JVMTI PopFrame and ForceEarlyReturn handling code.
  95  // The implementation is only non-empty for the InterpreterMacroAssembler,
  96  // as only the interpreter handles PopFrame and ForceEarlyReturn requests.
  97  virtual void check_and_handle_popframe(Register java_thread);
  98  virtual void check_and_handle_earlyret(Register java_thread);
  99 
 100   void safepoint_poll(Label&amp; slow_path);
 101   void safepoint_poll_acquire(Label&amp; slow_path);
 102 
 103   // Biased locking support
 104   // lock_reg and obj_reg must be loaded up with the appropriate values.
 105   // swap_reg is killed.
 106   // tmp_reg must be supplied and must not be rscratch1 or rscratch2
 107   // Optional slow case is for implementations (interpreter and C1) which branch to
 108   // slow case directly. Leaves condition codes set for C2&#39;s Fast_Lock node.
 109   // Returns offset of first potentially-faulting instruction for null
 110   // check info (currently consumed only by C1). If
 111   // swap_reg_contains_mark is true then returns -1 as it is assumed
 112   // the calling code has already passed any potential faults.
</pre>
<hr />
<pre>
 115                            bool swap_reg_contains_mark,
 116                            Label&amp; done, Label* slow_case = NULL,
 117                            BiasedLockingCounters* counters = NULL);
 118   void biased_locking_exit (Register obj_reg, Register temp_reg, Label&amp; done);
 119 
 120 
 121   // Helper functions for statistics gathering.
 122   // Unconditional atomic increment.
 123   void atomic_incw(Register counter_addr, Register tmp, Register tmp2);
 124   void atomic_incw(Address counter_addr, Register tmp1, Register tmp2, Register tmp3) {
 125     lea(tmp1, counter_addr);
 126     atomic_incw(tmp1, tmp2, tmp3);
 127   }
 128   // Load Effective Address
 129   void lea(Register r, const Address &amp;a) {
 130     InstructionMark im(this);
 131     code_section()-&gt;relocate(inst_mark(), a.rspec());
 132     a.lea(this, r);
 133   }
 134 














 135   void addmw(Address a, Register incr, Register scratch) {
 136     ldrw(scratch, a);
 137     addw(scratch, scratch, incr);
 138     strw(scratch, a);
 139   }
 140 
 141   // Add constant to memory word
 142   void addmw(Address a, int imm, Register scratch) {
 143     ldrw(scratch, a);
 144     if (imm &gt; 0)
 145       addw(scratch, scratch, (unsigned)imm);
 146     else
 147       subw(scratch, scratch, (unsigned)-imm);
 148     strw(scratch, a);
 149   }
 150 
 151   void bind(Label&amp; L) {
 152     Assembler::bind(L);
 153     code()-&gt;clear_last_insn();
 154   }
 155 
 156   void membar(Membar_mask_bits order_constraint);
 157 
 158   using Assembler::ldr;
 159   using Assembler::str;
 160 
 161   void ldr(Register Rx, const Address &amp;adr);
 162   void ldrw(Register Rw, const Address &amp;adr);
 163   void str(Register Rx, const Address &amp;adr);
 164   void strw(Register Rx, const Address &amp;adr);
 165 
 166   // Frame creation and destruction shared between JITs.
 167   void build_frame(int framesize);
 168   void remove_frame(int framesize);
 169 
 170   virtual void _call_Unimplemented(address call_site) {
 171     mov(rscratch2, call_site);
<span class="line-removed"> 172     haltsim();</span>
 173   }
 174 
 175 #define call_Unimplemented() _call_Unimplemented((address)__PRETTY_FUNCTION__)
 176 
<span class="line-removed"> 177   virtual void notify(int type);</span>
<span class="line-removed"> 178 </span>
 179   // aliases defined in AARCH64 spec
 180 
 181   template&lt;class T&gt;
 182   inline void cmpw(Register Rd, T imm)  { subsw(zr, Rd, imm); }
 183 
 184   inline void cmp(Register Rd, unsigned char imm8)  { subs(zr, Rd, imm8); }
 185   inline void cmp(Register Rd, unsigned imm) __attribute__ ((deprecated));
 186 
 187   inline void cmnw(Register Rd, unsigned imm) { addsw(zr, Rd, imm); }
 188   inline void cmn(Register Rd, unsigned imm) { adds(zr, Rd, imm); }
 189 
 190   void cset(Register Rd, Assembler::Condition cond) {
 191     csinc(Rd, zr, zr, ~cond);
 192   }
 193   void csetw(Register Rd, Assembler::Condition cond) {
 194     csincw(Rd, zr, zr, ~cond);
 195   }
 196 
 197   void cneg(Register Rd, Register Rn, Assembler::Condition cond) {
 198     csneg(Rd, Rn, Rn, ~cond);
</pre>
<hr />
<pre>
 427       nop();                                                                  \
 428     Assembler::INSN(Rd, Rn, Rm, Ra);                                          \
 429   }
 430 
 431   WRAP(madd) WRAP(msub) WRAP(maddw) WRAP(msubw)
 432   WRAP(smaddl) WRAP(smsubl) WRAP(umaddl) WRAP(umsubl)
 433 #undef WRAP
 434 
 435 
 436   // macro assembly operations needed for aarch64
 437 
 438   // first two private routines for loading 32 bit or 64 bit constants
 439 private:
 440 
 441   void mov_immediate64(Register dst, u_int64_t imm64);
 442   void mov_immediate32(Register dst, u_int32_t imm32);
 443 
 444   int push(unsigned int bitset, Register stack);
 445   int pop(unsigned int bitset, Register stack);
 446 



 447   void mov(Register dst, Address a);
 448 
 449 public:
 450   void push(RegSet regs, Register stack) { if (regs.bits()) push(regs.bits(), stack); }
 451   void pop(RegSet regs, Register stack) { if (regs.bits()) pop(regs.bits(), stack); }
 452 



 453   // Push and pop everything that might be clobbered by a native
 454   // runtime call except rscratch1 and rscratch2.  (They are always
 455   // scratch, so we don&#39;t have to protect them.)  Only save the lower
 456   // 64 bits of each vector register.
 457   void push_call_clobbered_registers();
 458   void pop_call_clobbered_registers();
 459 
 460   // now mov instructions for loading absolute addresses and 32 or
 461   // 64 bit integers
 462 
 463   inline void mov(Register dst, address addr)
 464   {
 465     mov_immediate64(dst, (u_int64_t)addr);
 466   }
 467 
 468   inline void mov(Register dst, u_int64_t imm64)
 469   {
 470     mov_immediate64(dst, imm64);
 471   }
 472 
</pre>
<hr />
<pre>
 590     return target_addr_for_insn(insn_addr, insn);
 591   }
 592 
 593   // Required platform-specific helpers for Label::patch_instructions.
 594   // They _shadow_ the declarations in AbstractAssembler, which are undefined.
 595   static int pd_patch_instruction_size(address branch, address target);
 596   static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {
 597     pd_patch_instruction_size(branch, target);
 598   }
 599   static address pd_call_destination(address branch) {
 600     return target_addr_for_insn(branch);
 601   }
 602 #ifndef PRODUCT
 603   static void pd_print_patched_instruction(address branch);
 604 #endif
 605 
 606   static int patch_oop(address insn_addr, address o);
 607   static int patch_narrow_klass(address insn_addr, narrowKlass n);
 608 
 609   address emit_trampoline_stub(int insts_call_instruction_offset, address target);

 610 
 611   // The following 4 methods return the offset of the appropriate move instruction
 612 
 613   // Support for fast byte/short loading with zero extension (depending on particular CPU)
 614   int load_unsigned_byte(Register dst, Address src);
 615   int load_unsigned_short(Register dst, Address src);
 616 
 617   // Support for fast byte/short loading with sign extension (depending on particular CPU)
 618   int load_signed_byte(Register dst, Address src);
 619   int load_signed_short(Register dst, Address src);
 620 
 621   int load_signed_byte32(Register dst, Address src);
 622   int load_signed_short32(Register dst, Address src);
 623 
 624   // Support for sign-extension (hi:lo = extend_sign(lo))
 625   void extend_sign(Register hi, Register lo);
 626 
 627   // Load and store values by size and signed-ness
 628   void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2 = noreg);
 629   void store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2 = noreg);
</pre>
<hr />
<pre>
 769 
 770   void set_last_Java_frame(Register last_java_sp,
 771                            Register last_java_fp,
 772                            Register last_java_pc,
 773                            Register scratch);
 774 
 775   void reset_last_Java_frame(Register thread);
 776 
 777   // thread in the default location (rthread)
 778   void reset_last_Java_frame(bool clear_fp);
 779 
 780   // Stores
 781   void store_check(Register obj);                // store check for obj - register is destroyed afterwards
 782   void store_check(Register obj, Address dst);   // same as above, dst is exact store location (reg. is destroyed)
 783 
 784   void resolve_jobject(Register value, Register thread, Register tmp);
 785 
 786   // C &#39;boolean&#39; to Java boolean: x == 0 ? 0 : 1
 787   void c2bool(Register x);
 788 


 789   // oop manipulations
 790   void load_klass(Register dst, Register src);
 791   void store_klass(Register dst, Register src);
 792   void cmp_klass(Register oop, Register trial_klass, Register tmp);
 793 
 794   void resolve_oop_handle(Register result, Register tmp = r5);
 795   void load_mirror(Register dst, Register method, Register tmp = r5);
 796 
 797   void access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
 798                       Register tmp1, Register tmp_thread);
 799 
 800   void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
 801                        Register tmp1, Register tmp_thread);
 802 
 803   // Resolves obj for access. Result is placed in the same register.
 804   // All other registers are preserved.
 805   void resolve(DecoratorSet decorators, Register obj);
 806 
 807   void load_heap_oop(Register dst, Address src, Register tmp1 = noreg,
 808                      Register thread_tmp = noreg, DecoratorSet decorators = 0);
</pre>
<hr />
<pre>
 907   // The rest of the type check; must be wired to a corresponding fast path.
 908   // It does not repeat the fast path logic, so don&#39;t use it standalone.
 909   // The temp_reg and temp2_reg can be noreg, if no temps are available.
 910   // Updates the sub&#39;s secondary super cache as necessary.
 911   // If set_cond_codes, condition codes will be Z on success, NZ on failure.
 912   void check_klass_subtype_slow_path(Register sub_klass,
 913                                      Register super_klass,
 914                                      Register temp_reg,
 915                                      Register temp2_reg,
 916                                      Label* L_success,
 917                                      Label* L_failure,
 918                                      bool set_cond_codes = false);
 919 
 920   // Simplified, combined version, good for typical uses.
 921   // Falls through on failure.
 922   void check_klass_subtype(Register sub_klass,
 923                            Register super_klass,
 924                            Register temp_reg,
 925                            Label&amp; L_success);
 926 





 927   Address argument_address(RegisterOrConstant arg_slot, int extra_slot_offset = 0);
 928 
 929 
 930   // Debugging
 931 
 932   // only if +VerifyOops
 933   void verify_oop(Register reg, const char* s = &quot;broken oop&quot;);
 934   void verify_oop_addr(Address addr, const char * s = &quot;broken oop addr&quot;);
 935 
 936 // TODO: verify method and klass metadata (compare against vptr?)
 937   void _verify_method_ptr(Register reg, const char * msg, const char * file, int line) {}
 938   void _verify_klass_ptr(Register reg, const char * msg, const char * file, int line){}
 939 
 940 #define verify_method_ptr(reg) _verify_method_ptr(reg, &quot;broken method &quot; #reg, __FILE__, __LINE__)
 941 #define verify_klass_ptr(reg) _verify_klass_ptr(reg, &quot;broken klass &quot; #reg, __FILE__, __LINE__)
 942 
 943   // only if +VerifyFPU
 944   void verify_FPU(int stack_depth, const char* s = &quot;illegal FPU state&quot;);
 945 
 946   // prints msg, dumps registers and stops execution
</pre>
<hr />
<pre>
1159   Address form_address(Register Rd, Register base, long byte_offset, int shift);
1160 
1161   // Return true iff an address is within the 48-bit AArch64 address
1162   // space.
1163   bool is_valid_AArch64_address(address a) {
1164     return ((uint64_t)a &gt;&gt; 48) == 0;
1165   }
1166 
1167   // Load the base of the cardtable byte map into reg.
1168   void load_byte_map_base(Register reg);
1169 
1170   // Prolog generator routines to support switch between x86 code and
1171   // generated ARM code
1172 
1173   // routine to generate an x86 prolog for a stub function which
1174   // bootstraps into the generated ARM code which directly follows the
1175   // stub
1176   //
1177 
1178   public:
<span class="line-removed">1179   // enum used for aarch64--x86 linkage to define return type of x86 function</span>
<span class="line-removed">1180   enum ret_type { ret_type_void, ret_type_integral, ret_type_float, ret_type_double};</span>
<span class="line-removed">1181 </span>
<span class="line-removed">1182 #ifdef BUILTIN_SIM</span>
<span class="line-removed">1183   void c_stub_prolog(int gp_arg_count, int fp_arg_count, int ret_type, address *prolog_ptr = NULL);</span>
<span class="line-removed">1184 #else</span>
<span class="line-removed">1185   void c_stub_prolog(int gp_arg_count, int fp_arg_count, int ret_type) { }</span>
<span class="line-removed">1186 #endif</span>
<span class="line-removed">1187 </span>
<span class="line-removed">1188   // special version of call_VM_leaf_base needed for aarch64 simulator</span>
<span class="line-removed">1189   // where we need to specify both the gp and fp arg counts and the</span>
<span class="line-removed">1190   // return type so that the linkage routine from aarch64 to x86 and</span>
<span class="line-removed">1191   // back knows which aarch64 registers to copy to x86 registers and</span>
<span class="line-removed">1192   // which x86 result register to copy back to an aarch64 register</span>
<span class="line-removed">1193 </span>
<span class="line-removed">1194   void call_VM_leaf_base1(</span>
<span class="line-removed">1195     address  entry_point,             // the entry point</span>
<span class="line-removed">1196     int      number_of_gp_arguments,  // the number of gp reg arguments to pass</span>
<span class="line-removed">1197     int      number_of_fp_arguments,  // the number of fp reg arguments to pass</span>
<span class="line-removed">1198     ret_type type,                    // the return type for the call</span>
<span class="line-removed">1199     Label*   retaddr = NULL</span>
<span class="line-removed">1200   );</span>
1201 
1202   void ldr_constant(Register dest, const Address &amp;const_addr) {
1203     if (NearCpool) {
1204       ldr(dest, const_addr);
1205     } else {
1206       unsigned long offset;
1207       adrp(dest, InternalAddress(const_addr.target()), offset);
1208       ldr(dest, Address(dest, offset));
1209     }
1210   }
1211 
1212   address read_polling_page(Register r, address page, relocInfo::relocType rtype);
1213   address read_polling_page(Register r, relocInfo::relocType rtype);
1214   void get_polling_page(Register dest, address page, relocInfo::relocType rtype);
1215 
1216   // CRC32 code for java.util.zip.CRC32::updateBytes() instrinsic.
1217   void update_byte_crc32(Register crc, Register val, Register table);
1218   void update_word_crc32(Register crc, Register v, Register tmp,
1219         Register table0, Register table1, Register table2, Register table3,
1220         bool upper = false);
</pre>
<hr />
<pre>
1343     } else {
1344       ldrw(Rx, spill_address(4, offset));
1345     }
1346   }
1347   void unspill(FloatRegister Vx, SIMD_RegVariant T, int offset) {
1348     ldr(Vx, T, spill_address(1 &lt;&lt; (int)T, offset));
1349   }
1350   void spill_copy128(int src_offset, int dst_offset,
1351                      Register tmp1=rscratch1, Register tmp2=rscratch2) {
1352     if (src_offset &lt; 512 &amp;&amp; (src_offset &amp; 7) == 0 &amp;&amp;
1353         dst_offset &lt; 512 &amp;&amp; (dst_offset &amp; 7) == 0) {
1354       ldp(tmp1, tmp2, Address(sp, src_offset));
1355       stp(tmp1, tmp2, Address(sp, dst_offset));
1356     } else {
1357       unspill(tmp1, true, src_offset);
1358       spill(tmp1, true, dst_offset);
1359       unspill(tmp1, true, src_offset+8);
1360       spill(tmp1, true, dst_offset+8);
1361     }
1362   }



1363 };
1364 
1365 #ifdef ASSERT
1366 inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }
1367 #endif
1368 
1369 /**
1370  * class SkipIfEqual:
1371  *
1372  * Instantiating this class will result in assembly code being output that will
1373  * jump around any code emitted between the creation of the instance and it&#39;s
1374  * automatic destruction at the end of a scope block, depending on the value of
1375  * the flag passed to the constructor, which will be checked at run-time.
1376  */
1377 class SkipIfEqual {
1378  private:
1379   MacroAssembler* _masm;
1380   Label _label;
1381 
1382  public:
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
<span class="line-modified">   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #ifndef CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
  27 #define CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
  28 
  29 #include &quot;asm/assembler.hpp&quot;
<span class="line-added">  30 #include &quot;oops/compressedOops.hpp&quot;</span>
<span class="line-added">  31 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  32 
  33 // MacroAssembler extends Assembler by frequently used macros.
  34 //
  35 // Instructions for which a &#39;better&#39; code sequence exists depending
  36 // on arguments should also go in here.
  37 
  38 class MacroAssembler: public Assembler {
  39   friend class LIR_Assembler;
  40 
  41  public:
  42   using Assembler::mov;
  43   using Assembler::movi;
  44 
  45  protected:
  46 
  47   // Support for VM calls
  48   //
  49   // This is the base routine called by the different versions of call_VM_leaf. The interpreter
  50   // may customize this version by overriding it for its purposes (e.g., to save/restore
  51   // additional registers when doing a VM call).
</pre>
<hr />
<pre>
  64 
  65   // This is the base routine called by the different versions of call_VM. The interpreter
  66   // may customize this version by overriding it for its purposes (e.g., to save/restore
  67   // additional registers when doing a VM call).
  68   //
  69   // If no java_thread register is specified (noreg) than rthread will be used instead. call_VM_base
  70   // returns the register which contains the thread upon return. If a thread register has been
  71   // specified, the return value will correspond to that register. If no last_java_sp is specified
  72   // (noreg) than rsp will be used instead.
  73   virtual void call_VM_base(           // returns the register containing the thread upon return
  74     Register oop_result,               // where an oop-result ends up if any; use noreg otherwise
  75     Register java_thread,              // the thread if computed before     ; use noreg otherwise
  76     Register last_java_sp,             // to set up last_Java_frame in stubs; use noreg otherwise
  77     address  entry_point,              // the entry point
  78     int      number_of_arguments,      // the number of arguments (w/o thread) to pop after the call
  79     bool     check_exceptions          // whether to check for pending exceptions after return
  80   );
  81 
  82   void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions = true);
  83 
<span class="line-modified">  84   enum KlassDecodeMode {</span>
<span class="line-modified">  85     KlassDecodeNone,</span>
<span class="line-added">  86     KlassDecodeZero,</span>
<span class="line-added">  87     KlassDecodeXor,</span>
<span class="line-added">  88     KlassDecodeMovk</span>
<span class="line-added">  89   };</span>
<span class="line-added">  90 </span>
<span class="line-added">  91   KlassDecodeMode klass_decode_mode();</span>
<span class="line-added">  92 </span>
<span class="line-added">  93  private:</span>
<span class="line-added">  94   static KlassDecodeMode _klass_decode_mode;</span>
  95 
  96  public:
<span class="line-modified">  97   MacroAssembler(CodeBuffer* code) : Assembler(code) {}</span>






  98 
  99  // These routines should emit JVMTI PopFrame and ForceEarlyReturn handling code.
 100  // The implementation is only non-empty for the InterpreterMacroAssembler,
 101  // as only the interpreter handles PopFrame and ForceEarlyReturn requests.
 102  virtual void check_and_handle_popframe(Register java_thread);
 103  virtual void check_and_handle_earlyret(Register java_thread);
 104 
 105   void safepoint_poll(Label&amp; slow_path);
 106   void safepoint_poll_acquire(Label&amp; slow_path);
 107 
 108   // Biased locking support
 109   // lock_reg and obj_reg must be loaded up with the appropriate values.
 110   // swap_reg is killed.
 111   // tmp_reg must be supplied and must not be rscratch1 or rscratch2
 112   // Optional slow case is for implementations (interpreter and C1) which branch to
 113   // slow case directly. Leaves condition codes set for C2&#39;s Fast_Lock node.
 114   // Returns offset of first potentially-faulting instruction for null
 115   // check info (currently consumed only by C1). If
 116   // swap_reg_contains_mark is true then returns -1 as it is assumed
 117   // the calling code has already passed any potential faults.
</pre>
<hr />
<pre>
 120                            bool swap_reg_contains_mark,
 121                            Label&amp; done, Label* slow_case = NULL,
 122                            BiasedLockingCounters* counters = NULL);
 123   void biased_locking_exit (Register obj_reg, Register temp_reg, Label&amp; done);
 124 
 125 
 126   // Helper functions for statistics gathering.
 127   // Unconditional atomic increment.
 128   void atomic_incw(Register counter_addr, Register tmp, Register tmp2);
 129   void atomic_incw(Address counter_addr, Register tmp1, Register tmp2, Register tmp3) {
 130     lea(tmp1, counter_addr);
 131     atomic_incw(tmp1, tmp2, tmp3);
 132   }
 133   // Load Effective Address
 134   void lea(Register r, const Address &amp;a) {
 135     InstructionMark im(this);
 136     code_section()-&gt;relocate(inst_mark(), a.rspec());
 137     a.lea(this, r);
 138   }
 139 
<span class="line-added"> 140   /* Sometimes we get misaligned loads and stores, usually from Unsafe</span>
<span class="line-added"> 141      accesses, and these can exceed the offset range. */</span>
<span class="line-added"> 142   Address legitimize_address(const Address &amp;a, int size, Register scratch) {</span>
<span class="line-added"> 143     if (a.getMode() == Address::base_plus_offset) {</span>
<span class="line-added"> 144       if (! Address::offset_ok_for_immed(a.offset(), exact_log2(size))) {</span>
<span class="line-added"> 145         block_comment(&quot;legitimize_address {&quot;);</span>
<span class="line-added"> 146         lea(scratch, a);</span>
<span class="line-added"> 147         block_comment(&quot;} legitimize_address&quot;);</span>
<span class="line-added"> 148         return Address(scratch);</span>
<span class="line-added"> 149       }</span>
<span class="line-added"> 150     }</span>
<span class="line-added"> 151     return a;</span>
<span class="line-added"> 152   }</span>
<span class="line-added"> 153 </span>
 154   void addmw(Address a, Register incr, Register scratch) {
 155     ldrw(scratch, a);
 156     addw(scratch, scratch, incr);
 157     strw(scratch, a);
 158   }
 159 
 160   // Add constant to memory word
 161   void addmw(Address a, int imm, Register scratch) {
 162     ldrw(scratch, a);
 163     if (imm &gt; 0)
 164       addw(scratch, scratch, (unsigned)imm);
 165     else
 166       subw(scratch, scratch, (unsigned)-imm);
 167     strw(scratch, a);
 168   }
 169 
 170   void bind(Label&amp; L) {
 171     Assembler::bind(L);
 172     code()-&gt;clear_last_insn();
 173   }
 174 
 175   void membar(Membar_mask_bits order_constraint);
 176 
 177   using Assembler::ldr;
 178   using Assembler::str;
 179 
 180   void ldr(Register Rx, const Address &amp;adr);
 181   void ldrw(Register Rw, const Address &amp;adr);
 182   void str(Register Rx, const Address &amp;adr);
 183   void strw(Register Rx, const Address &amp;adr);
 184 
 185   // Frame creation and destruction shared between JITs.
 186   void build_frame(int framesize);
 187   void remove_frame(int framesize);
 188 
 189   virtual void _call_Unimplemented(address call_site) {
 190     mov(rscratch2, call_site);

 191   }
 192 
 193 #define call_Unimplemented() _call_Unimplemented((address)__PRETTY_FUNCTION__)
 194 


 195   // aliases defined in AARCH64 spec
 196 
 197   template&lt;class T&gt;
 198   inline void cmpw(Register Rd, T imm)  { subsw(zr, Rd, imm); }
 199 
 200   inline void cmp(Register Rd, unsigned char imm8)  { subs(zr, Rd, imm8); }
 201   inline void cmp(Register Rd, unsigned imm) __attribute__ ((deprecated));
 202 
 203   inline void cmnw(Register Rd, unsigned imm) { addsw(zr, Rd, imm); }
 204   inline void cmn(Register Rd, unsigned imm) { adds(zr, Rd, imm); }
 205 
 206   void cset(Register Rd, Assembler::Condition cond) {
 207     csinc(Rd, zr, zr, ~cond);
 208   }
 209   void csetw(Register Rd, Assembler::Condition cond) {
 210     csincw(Rd, zr, zr, ~cond);
 211   }
 212 
 213   void cneg(Register Rd, Register Rn, Assembler::Condition cond) {
 214     csneg(Rd, Rn, Rn, ~cond);
</pre>
<hr />
<pre>
 443       nop();                                                                  \
 444     Assembler::INSN(Rd, Rn, Rm, Ra);                                          \
 445   }
 446 
 447   WRAP(madd) WRAP(msub) WRAP(maddw) WRAP(msubw)
 448   WRAP(smaddl) WRAP(smsubl) WRAP(umaddl) WRAP(umsubl)
 449 #undef WRAP
 450 
 451 
 452   // macro assembly operations needed for aarch64
 453 
 454   // first two private routines for loading 32 bit or 64 bit constants
 455 private:
 456 
 457   void mov_immediate64(Register dst, u_int64_t imm64);
 458   void mov_immediate32(Register dst, u_int32_t imm32);
 459 
 460   int push(unsigned int bitset, Register stack);
 461   int pop(unsigned int bitset, Register stack);
 462 
<span class="line-added"> 463   int push_fp(unsigned int bitset, Register stack);</span>
<span class="line-added"> 464   int pop_fp(unsigned int bitset, Register stack);</span>
<span class="line-added"> 465 </span>
 466   void mov(Register dst, Address a);
 467 
 468 public:
 469   void push(RegSet regs, Register stack) { if (regs.bits()) push(regs.bits(), stack); }
 470   void pop(RegSet regs, Register stack) { if (regs.bits()) pop(regs.bits(), stack); }
 471 
<span class="line-added"> 472   void push_fp(RegSet regs, Register stack) { if (regs.bits()) push_fp(regs.bits(), stack); }</span>
<span class="line-added"> 473   void pop_fp(RegSet regs, Register stack) { if (regs.bits()) pop_fp(regs.bits(), stack); }</span>
<span class="line-added"> 474 </span>
 475   // Push and pop everything that might be clobbered by a native
 476   // runtime call except rscratch1 and rscratch2.  (They are always
 477   // scratch, so we don&#39;t have to protect them.)  Only save the lower
 478   // 64 bits of each vector register.
 479   void push_call_clobbered_registers();
 480   void pop_call_clobbered_registers();
 481 
 482   // now mov instructions for loading absolute addresses and 32 or
 483   // 64 bit integers
 484 
 485   inline void mov(Register dst, address addr)
 486   {
 487     mov_immediate64(dst, (u_int64_t)addr);
 488   }
 489 
 490   inline void mov(Register dst, u_int64_t imm64)
 491   {
 492     mov_immediate64(dst, imm64);
 493   }
 494 
</pre>
<hr />
<pre>
 612     return target_addr_for_insn(insn_addr, insn);
 613   }
 614 
 615   // Required platform-specific helpers for Label::patch_instructions.
 616   // They _shadow_ the declarations in AbstractAssembler, which are undefined.
 617   static int pd_patch_instruction_size(address branch, address target);
 618   static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {
 619     pd_patch_instruction_size(branch, target);
 620   }
 621   static address pd_call_destination(address branch) {
 622     return target_addr_for_insn(branch);
 623   }
 624 #ifndef PRODUCT
 625   static void pd_print_patched_instruction(address branch);
 626 #endif
 627 
 628   static int patch_oop(address insn_addr, address o);
 629   static int patch_narrow_klass(address insn_addr, narrowKlass n);
 630 
 631   address emit_trampoline_stub(int insts_call_instruction_offset, address target);
<span class="line-added"> 632   void emit_static_call_stub();</span>
 633 
 634   // The following 4 methods return the offset of the appropriate move instruction
 635 
 636   // Support for fast byte/short loading with zero extension (depending on particular CPU)
 637   int load_unsigned_byte(Register dst, Address src);
 638   int load_unsigned_short(Register dst, Address src);
 639 
 640   // Support for fast byte/short loading with sign extension (depending on particular CPU)
 641   int load_signed_byte(Register dst, Address src);
 642   int load_signed_short(Register dst, Address src);
 643 
 644   int load_signed_byte32(Register dst, Address src);
 645   int load_signed_short32(Register dst, Address src);
 646 
 647   // Support for sign-extension (hi:lo = extend_sign(lo))
 648   void extend_sign(Register hi, Register lo);
 649 
 650   // Load and store values by size and signed-ness
 651   void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2 = noreg);
 652   void store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2 = noreg);
</pre>
<hr />
<pre>
 792 
 793   void set_last_Java_frame(Register last_java_sp,
 794                            Register last_java_fp,
 795                            Register last_java_pc,
 796                            Register scratch);
 797 
 798   void reset_last_Java_frame(Register thread);
 799 
 800   // thread in the default location (rthread)
 801   void reset_last_Java_frame(bool clear_fp);
 802 
 803   // Stores
 804   void store_check(Register obj);                // store check for obj - register is destroyed afterwards
 805   void store_check(Register obj, Address dst);   // same as above, dst is exact store location (reg. is destroyed)
 806 
 807   void resolve_jobject(Register value, Register thread, Register tmp);
 808 
 809   // C &#39;boolean&#39; to Java boolean: x == 0 ? 0 : 1
 810   void c2bool(Register x);
 811 
<span class="line-added"> 812   void load_method_holder(Register holder, Register method);</span>
<span class="line-added"> 813 </span>
 814   // oop manipulations
 815   void load_klass(Register dst, Register src);
 816   void store_klass(Register dst, Register src);
 817   void cmp_klass(Register oop, Register trial_klass, Register tmp);
 818 
 819   void resolve_oop_handle(Register result, Register tmp = r5);
 820   void load_mirror(Register dst, Register method, Register tmp = r5);
 821 
 822   void access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
 823                       Register tmp1, Register tmp_thread);
 824 
 825   void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
 826                        Register tmp1, Register tmp_thread);
 827 
 828   // Resolves obj for access. Result is placed in the same register.
 829   // All other registers are preserved.
 830   void resolve(DecoratorSet decorators, Register obj);
 831 
 832   void load_heap_oop(Register dst, Address src, Register tmp1 = noreg,
 833                      Register thread_tmp = noreg, DecoratorSet decorators = 0);
</pre>
<hr />
<pre>
 932   // The rest of the type check; must be wired to a corresponding fast path.
 933   // It does not repeat the fast path logic, so don&#39;t use it standalone.
 934   // The temp_reg and temp2_reg can be noreg, if no temps are available.
 935   // Updates the sub&#39;s secondary super cache as necessary.
 936   // If set_cond_codes, condition codes will be Z on success, NZ on failure.
 937   void check_klass_subtype_slow_path(Register sub_klass,
 938                                      Register super_klass,
 939                                      Register temp_reg,
 940                                      Register temp2_reg,
 941                                      Label* L_success,
 942                                      Label* L_failure,
 943                                      bool set_cond_codes = false);
 944 
 945   // Simplified, combined version, good for typical uses.
 946   // Falls through on failure.
 947   void check_klass_subtype(Register sub_klass,
 948                            Register super_klass,
 949                            Register temp_reg,
 950                            Label&amp; L_success);
 951 
<span class="line-added"> 952   void clinit_barrier(Register klass,</span>
<span class="line-added"> 953                       Register thread,</span>
<span class="line-added"> 954                       Label* L_fast_path = NULL,</span>
<span class="line-added"> 955                       Label* L_slow_path = NULL);</span>
<span class="line-added"> 956 </span>
 957   Address argument_address(RegisterOrConstant arg_slot, int extra_slot_offset = 0);
 958 
 959 
 960   // Debugging
 961 
 962   // only if +VerifyOops
 963   void verify_oop(Register reg, const char* s = &quot;broken oop&quot;);
 964   void verify_oop_addr(Address addr, const char * s = &quot;broken oop addr&quot;);
 965 
 966 // TODO: verify method and klass metadata (compare against vptr?)
 967   void _verify_method_ptr(Register reg, const char * msg, const char * file, int line) {}
 968   void _verify_klass_ptr(Register reg, const char * msg, const char * file, int line){}
 969 
 970 #define verify_method_ptr(reg) _verify_method_ptr(reg, &quot;broken method &quot; #reg, __FILE__, __LINE__)
 971 #define verify_klass_ptr(reg) _verify_klass_ptr(reg, &quot;broken klass &quot; #reg, __FILE__, __LINE__)
 972 
 973   // only if +VerifyFPU
 974   void verify_FPU(int stack_depth, const char* s = &quot;illegal FPU state&quot;);
 975 
 976   // prints msg, dumps registers and stops execution
</pre>
<hr />
<pre>
1189   Address form_address(Register Rd, Register base, long byte_offset, int shift);
1190 
1191   // Return true iff an address is within the 48-bit AArch64 address
1192   // space.
1193   bool is_valid_AArch64_address(address a) {
1194     return ((uint64_t)a &gt;&gt; 48) == 0;
1195   }
1196 
1197   // Load the base of the cardtable byte map into reg.
1198   void load_byte_map_base(Register reg);
1199 
1200   // Prolog generator routines to support switch between x86 code and
1201   // generated ARM code
1202 
1203   // routine to generate an x86 prolog for a stub function which
1204   // bootstraps into the generated ARM code which directly follows the
1205   // stub
1206   //
1207 
1208   public:






















1209 
1210   void ldr_constant(Register dest, const Address &amp;const_addr) {
1211     if (NearCpool) {
1212       ldr(dest, const_addr);
1213     } else {
1214       unsigned long offset;
1215       adrp(dest, InternalAddress(const_addr.target()), offset);
1216       ldr(dest, Address(dest, offset));
1217     }
1218   }
1219 
1220   address read_polling_page(Register r, address page, relocInfo::relocType rtype);
1221   address read_polling_page(Register r, relocInfo::relocType rtype);
1222   void get_polling_page(Register dest, address page, relocInfo::relocType rtype);
1223 
1224   // CRC32 code for java.util.zip.CRC32::updateBytes() instrinsic.
1225   void update_byte_crc32(Register crc, Register val, Register table);
1226   void update_word_crc32(Register crc, Register v, Register tmp,
1227         Register table0, Register table1, Register table2, Register table3,
1228         bool upper = false);
</pre>
<hr />
<pre>
1351     } else {
1352       ldrw(Rx, spill_address(4, offset));
1353     }
1354   }
1355   void unspill(FloatRegister Vx, SIMD_RegVariant T, int offset) {
1356     ldr(Vx, T, spill_address(1 &lt;&lt; (int)T, offset));
1357   }
1358   void spill_copy128(int src_offset, int dst_offset,
1359                      Register tmp1=rscratch1, Register tmp2=rscratch2) {
1360     if (src_offset &lt; 512 &amp;&amp; (src_offset &amp; 7) == 0 &amp;&amp;
1361         dst_offset &lt; 512 &amp;&amp; (dst_offset &amp; 7) == 0) {
1362       ldp(tmp1, tmp2, Address(sp, src_offset));
1363       stp(tmp1, tmp2, Address(sp, dst_offset));
1364     } else {
1365       unspill(tmp1, true, src_offset);
1366       spill(tmp1, true, dst_offset);
1367       unspill(tmp1, true, src_offset+8);
1368       spill(tmp1, true, dst_offset+8);
1369     }
1370   }
<span class="line-added">1371 </span>
<span class="line-added">1372   void cache_wb(Address line);</span>
<span class="line-added">1373   void cache_wbsync(bool is_pre);</span>
1374 };
1375 
1376 #ifdef ASSERT
1377 inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }
1378 #endif
1379 
1380 /**
1381  * class SkipIfEqual:
1382  *
1383  * Instantiating this class will result in assembly code being output that will
1384  * jump around any code emitted between the creation of the instance and it&#39;s
1385  * automatic destruction at the end of a scope block, depending on the value of
1386  * the flag passed to the constructor, which will be checked at run-time.
1387  */
1388 class SkipIfEqual {
1389  private:
1390   MacroAssembler* _masm;
1391   Label _label;
1392 
1393  public:
</pre>
</td>
</tr>
</table>
<center><a href="macroAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64_log.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>